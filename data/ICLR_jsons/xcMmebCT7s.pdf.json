{
    "abstractText": "Discovering mutations enhancing protein\u2013protein interactions (PPIs) is critical for advancing biomedical research and developing improved therapeutics. While machine learning approaches have substantially advanced the field, they often struggle to generalize beyond training data in practical scenarios. The contributions of this work are three-fold. First, we construct PPIRef, the largest and non-redundant dataset of 3D protein\u2013protein interactions, enabling effective largescale learning. Second, we leverage PPIRef to pre-train PPIformer, a new SE(3)equivariant model generalizing across diverse protein-binder variants. We finetune PPIformer to predict effects of mutations on protein\u2013protein interactions via a thermodynamically motivated adjustment of the pre-training loss function. Finally, we demonstrate the enhanced generalization of our new PPIformer approach by outperforming other state-of-the-art methods on the new non-leaking splits of the standard labeled PPI mutational data and independent case studies optimizing a human antibody against SARS-CoV-2 and increasing staphylokinase thrombolytic activity.",
    "authors": [],
    "id": "SP:342b77bca55960b86f0e8c45207ace282b8df4d3",
    "references": [
        {
            "authors": [
                "Kyle A. Barlow",
                "Shane \u00d3 Conch\u00fair",
                "Samuel Thompson",
                "Pooja Suresh",
                "James E. Lucas",
                "Markus Heinonen",
                "Tanja Kortemme"
            ],
            "title": "Flex ddg: Rosetta ensemble-based estimation of changes in protein\u2013protein binding affinity upon mutation",
            "venue": "The Journal of Physical Chemistry B,",
            "year": 2018
        },
        {
            "authors": [
                "Prasad V Burra",
                "Ying Zhang",
                "Adam Godzik",
                "Boguslaw Stec"
            ],
            "title": "Global distribution of conformational states derived from redundant models in the pdb points to non-uniqueness of the protein structure",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2009
        },
        {
            "authors": [
                "Shanshan Cheng",
                "Yang Zhang",
                "Charles L Brooks"
            ],
            "title": "Pcalign: a method to quantify physicochemical similarity of protein-protein interfaces",
            "venue": "BMC bioinformatics,",
            "year": 2015
        },
        {
            "authors": [
                "Justas Dauparas",
                "Ivan Anishchenko",
                "Nathaniel Bennett",
                "Hua Bai",
                "Robert J Ragotte",
                "Lukas F Milles",
                "Basile IM Wicky",
                "Alexis Courbet",
                "Rob J de Haas",
                "Neville Bethel"
            ],
            "title": "Robust deep learning\u2013 based protein sequence design using proteinmpnn",
            "year": 2022
        },
        {
            "authors": [
                "Yves Dehouck",
                "Jean Marc Kwasigroch",
                "Marianne Rooman",
                "Dimitri Gilis"
            ],
            "title": "Beatmusic: prediction of changes in protein\u2013protein binding affinity on mutations",
            "venue": "Nucleic acids research,",
            "year": 2013
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Eli J. Draizen",
                "Luis Felipe R. Murillo",
                "John Readey",
                "Cameron Mura",
                "Philip E. Bourne"
            ],
            "title": "Prop3d: A flexible, python-based platform for machine learning with protein structural properties and biophysical data",
            "venue": "bioRxiv,",
            "year": 2022
        },
        {
            "authors": [
                "Richard Evans",
                "Michael O\u2019Neill",
                "Alexander Pritzel",
                "Natasha Antropova",
                "Andrew Senior",
                "Tim Green",
                "Augustin \u017d\u0131\u0301dek",
                "Russ Bates",
                "Sam Blackwell",
                "Jason Yim"
            ],
            "title": "Protein complex prediction with alphafold-multimer",
            "year": 2021
        },
        {
            "authors": [
                "Valery L Feigin",
                "Michael Brainin",
                "Bo Norrving",
                "Sheila Martins",
                "Ralph L Sacco",
                "Werner Hacke",
                "Marc Fisher",
                "Jeyaraj Pandian",
                "Patrice Lindsay"
            ],
            "title": "World stroke organization (wso): global stroke fact sheet 2022",
            "venue": "International Journal of Stroke,",
            "year": 2022
        },
        {
            "authors": [
                "Matthias Fey",
                "Jan Eric Lenssen"
            ],
            "title": "Fast graph representation learning with pytorch geometric",
            "venue": "arXiv preprint arXiv:1903.02428,",
            "year": 2019
        },
        {
            "authors": [
                "Pablo Gainza",
                "Freyr Sverrisson",
                "Frederico Monti",
                "Emanuele Rodola",
                "D Boscaini",
                "MM Bronstein",
                "BE Correia"
            ],
            "title": "Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning",
            "venue": "Nature Methods,",
            "year": 2020
        },
        {
            "authors": [
                "Octavian-Eugen Ganea",
                "Xinyuan Huang",
                "Charlotte Bunne",
                "Yatao Bian",
                "Regina Barzilay",
                "Tommi Jaakkola",
                "Andreas Krause"
            ],
            "title": "Independent se (3)-equivariant models for end-to-end rigid protein docking",
            "venue": "arXiv preprint arXiv:2111.07786,",
            "year": 2021
        },
        {
            "authors": [
                "Mu Gao",
                "Jeffrey Skolnick"
            ],
            "title": "ialign: a method for the structural comparison of protein\u2013protein",
            "venue": "interfaces. Bioinformatics,",
            "year": 2010
        },
        {
            "authors": [
                "Mu Gao",
                "Jeffrey Skolnick"
            ],
            "title": "Structural space of protein\u2013protein interfaces is degenerate, close to complete, and highly connected",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2010
        },
        {
            "authors": [
                "Cunliang Geng",
                "Anna Vangone",
                "Gert E Folkers",
                "Li C Xue",
                "Alexandre MJJ Bonvin"
            ],
            "title": "isee: Interface structure, evolution, and energy-based machine learning predictor of binding affinity changes upon mutations",
            "venue": "Proteins: Structure, Function, and Bioinformatics,",
            "year": 2019
        },
        {
            "authors": [
                "Cunliang Geng",
                "Li C Xue",
                "Jorge Roel-Touris",
                "Alexandre MJJ Bonvin"
            ],
            "title": "Finding the \u03b4\u03b4g spot: Are predictors of binding affinity changes upon mutations in protein\u2013protein interactions ready for it? Wiley Interdisciplinary Reviews: Computational Molecular Science, 9(5):e1410, 2019b",
            "year": 2019
        },
        {
            "authors": [
                "Chloe Hsu",
                "Robert Verkuil",
                "Jason Liu",
                "Zeming Lin",
                "Brian Hie",
                "Tom Sercu",
                "Adam Lerer",
                "Alexander Rives"
            ],
            "title": "Learning inverse folding from millions of predicted structures. bioRxiv, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Andrei A Ivanov",
                "Fadlo R Khuri",
                "Haian Fu"
            ],
            "title": "Targeting protein\u2013protein interactions as an anticancer strategy",
            "venue": "Trends in pharmacological sciences,",
            "year": 2013
        },
        {
            "authors": [
                "Arian R Jamasb",
                "Ramon Vi\u00f1as",
                "Eric J Ma",
                "Charlie Harris",
                "Kexin Huang",
                "Dominic Hall",
                "Pietro Li\u00f3",
                "Tom L Blundell"
            ],
            "title": "Graphein-a python library for geometric deep learning and network analysis on protein structures and interaction",
            "year": 2020
        },
        {
            "authors": [
                "Justina Jankauskait\u0117",
                "Brian Jim\u00e9nez-Garc\u0131\u0301a",
                "Justas Dapk\u016bnas",
                "Juan Fern\u00e1ndez-Recio",
                "Iain H Moal"
            ],
            "title": "Skempi 2.0: an updated benchmark of changes in protein\u2013protein binding energy, kinetics and thermodynamics upon mutation",
            "year": 2019
        },
        {
            "authors": [
                "Wengong Jin",
                "Siranush Sarkizova",
                "Xun Chen",
                "Nir Hacohen",
                "Caroline Uhler"
            ],
            "title": "Unsupervised protein-ligand binding energy prediction via neural euler\u2019s rotation equation",
            "venue": "arXiv preprint arXiv:2301.10814,",
            "year": 2023
        },
        {
            "authors": [
                "Bowen Jing",
                "Stephan Eismann",
                "Patricia Suriana",
                "Raphael JL Townshend",
                "Ron Dror"
            ],
            "title": "Learning from protein structure with geometric vector perceptrons",
            "venue": "arXiv preprint arXiv:2009.01411,",
            "year": 2020
        },
        {
            "authors": [
                "John Jumper",
                "Richard Evans",
                "Alexander Pritzel",
                "Tim Green",
                "Michael Figurnov",
                "Olaf Ronneberger",
                "Kathryn Tunyasuvunakool",
                "Russ Bates",
                "Augustin \u017d\u0131\u0301dek",
                "Anna Potapenko"
            ],
            "title": "Highly accurate protein structure prediction with alphafold",
            "year": 2021
        },
        {
            "authors": [
                "George Em Karniadakis",
                "Ioannis G Kevrekidis",
                "Lu Lu",
                "Paris Perdikaris",
                "Sifan Wang",
                "Liu Yang"
            ],
            "title": "Physics-informed machine learning",
            "venue": "Nature Reviews Physics,",
            "year": 2021
        },
        {
            "authors": [
                "Panagiotis L Kastritis",
                "Alexandre MJJ Bonvin"
            ],
            "title": "On the binding affinity of macromolecular interactions: daring to ask why proteins interact",
            "venue": "Journal of The Royal Society Interface,",
            "year": 2012
        },
        {
            "authors": [
                "Mohamed Amine Ketata",
                "Cedrik Laue",
                "Ruslan Mammadov",
                "Hannes St\u00e4rk",
                "Menghua Wu",
                "Gabriele Corso",
                "C\u00e9line Marquet",
                "Regina Barzilay",
                "Tommi S Jaakkola"
            ],
            "title": "Diffdock-pp: Rigid proteinprotein docking with diffusion models",
            "venue": "arXiv preprint arXiv:2304.03889,",
            "year": 2023
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Yves Laroche",
                "Stephane Heymans",
                "Sophie Capaert",
                "Frans De Cock",
                "Eddy Demarsin",
                "D\u00e9sir\u00e9 Collen"
            ],
            "title": "Recombinant staphylokinase variants with reduced antigenicity due to elimination of b-lymphocyte epitopes",
            "venue": "Blood, The Journal of the American Society of Hematology,",
            "year": 2000
        },
        {
            "authors": [
                "Julia Koehler Leman",
                "Brian D Weitzner",
                "Steven M Lewis",
                "Jared Adolf-Bryfogle",
                "Nawsad Alam",
                "Rebecca F Alford",
                "Melanie Aprahamian",
                "David Baker",
                "Kyle A Barlow",
                "Patrick Barth"
            ],
            "title": "Macromolecular modeling and design in rosetta: recent methods and frameworks",
            "venue": "Nature methods,",
            "year": 2020
        },
        {
            "authors": [
                "Yi-Lun Liao",
                "Tess Smidt"
            ],
            "title": "Equiformer: Equivariant graph attention transformer for 3d atomistic graphs",
            "venue": "arXiv preprint arXiv:2206.11990,",
            "year": 2022
        },
        {
            "authors": [
                "Yi-Lun Liao",
                "Brandon Wood",
                "Abhishek Das",
                "Tess Smidt"
            ],
            "title": "Equiformerv2: Improved equivariant transformer for scaling to higher-degree representations",
            "venue": "arXiv preprint arXiv:2306.12059,",
            "year": 2023
        },
        {
            "authors": [
                "Zeming Lin",
                "Halil Akin",
                "Roshan Rao",
                "Brian Hie",
                "Zhongkai Zhu",
                "Wenting Lu",
                "Nikita Smetanin",
                "Robert Verkuil",
                "Ori Kabeli",
                "Yaniv Shmueli"
            ],
            "title": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
            "year": 2023
        },
        {
            "authors": [
                "Xianggen Liu",
                "Yunan Luo",
                "Pengyong Li",
                "Sen Song",
                "Jian Peng"
            ],
            "title": "Deep geometric representations for modeling effects of mutations on protein-protein binding affinity",
            "venue": "PLoS computational biology,",
            "year": 2021
        },
        {
            "authors": [
                "Haiying Lu",
                "Qiaodan Zhou",
                "Jun He",
                "Zhongliang Jiang",
                "Cheng Peng",
                "Rongsheng Tong",
                "Jianyou Shi"
            ],
            "title": "Recent advances in the development of protein\u2013protein interactions modulators: mechanisms and clinical trials",
            "venue": "Signal transduction and targeted therapy,",
            "year": 2020
        },
        {
            "authors": [
                "Shitong Luo",
                "Yufeng Su",
                "Zuofan Wu",
                "Chenpeng Su",
                "Jian Peng",
                "Jianzhu Ma"
            ],
            "title": "Rotamer density estimator is an unsupervised learner of the effect of mutations on protein-protein interaction",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Anthony Marchand",
                "Alexandra K Van Hall-Beauvais",
                "Bruno E Correia"
            ],
            "title": "Computational design of novel protein\u2013protein interactions\u2013an overview on methodological approaches and applications",
            "venue": "Current Opinion in Structural Biology,",
            "year": 2022
        },
        {
            "authors": [
                "Joshua Meier",
                "Roshan Rao",
                "Robert Verkuil",
                "Jason Liu",
                "Tom Sercu",
                "Alex Rives"
            ],
            "title": "Language models enable zero-shot prediction of the effects of mutations on protein function",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Claudio Mirabello",
                "Bj\u00f6rn Wallner"
            ],
            "title": "Topology independent structural matching discovers novel templates for protein",
            "venue": "interfaces. Bioinformatics,",
            "year": 2018
        },
        {
            "authors": [
                "Alex Morehead",
                "Chen Chen",
                "Ada Sedova",
                "Jianlin Cheng"
            ],
            "title": "Dips-plus: The enhanced database of interacting protein structures for interface prediction",
            "venue": "arXiv preprint arXiv:2106.04362,",
            "year": 2021
        },
        {
            "authors": [
                "Dmitri Nikitin",
                "Jan Mican",
                "Martin Toul",
                "David Bednar",
                "Michaela Peskova",
                "Patricia Kittova",
                "Sandra Thalerova",
                "Jan Vitecek",
                "Jiri Damborsky",
                "Robert Mikulik"
            ],
            "title": "Computer-aided engineering of staphylokinase toward enhanced affinity and selectivity for plasmin",
            "venue": "Computational and structural biotechnology journal,",
            "year": 2022
        },
        {
            "authors": [
                "Swagata Pahari",
                "Gen Li",
                "Adithya Krishna Murthy",
                "Siqi Liang",
                "Robert Fragoza",
                "Haiyuan Yu",
                "Emil Alexov"
            ],
            "title": "Saambe-3d: predicting effect of mutations on protein\u2013protein interactions",
            "venue": "International journal of molecular sciences,",
            "year": 2020
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, highperformance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Michael Remmert",
                "Andreas Biegert",
                "Andreas Hauser",
                "Johannes S\u00f6ding"
            ],
            "title": "Hhblits: lightning-fast iterative protein sequence searching by hmm-hmm alignment",
            "venue": "Nature Methods,",
            "year": 2012
        },
        {
            "authors": [
                "Alexander Rives",
                "Joshua Meier",
                "Tom Sercu",
                "Siddharth Goyal",
                "Zeming Lin",
                "Jason Liu",
                "Demi Guo",
                "Myle Ott",
                "C Lawrence Zitnick",
                "Jerry Ma"
            ],
            "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2021
        },
        {
            "authors": [
                "Carlos HM Rodrigues",
                "Douglas EV Pires",
                "David B Ascher"
            ],
            "title": "mmcsm-ppi: predicting the effects of multiple point mutations on protein\u2013protein interactions",
            "venue": "Nucleic Acids Research,",
            "year": 2021
        },
        {
            "authors": [
                "Leo Scheller",
                "Tobias Strittmatter",
                "David Fuchs",
                "Daniel Bojar",
                "Martin Fussenegger"
            ],
            "title": "Generalized extracellular molecule sensor platform for programming cellular behavior",
            "venue": "Nature chemical biology,",
            "year": 2018
        },
        {
            "authors": [
                "Joost Schymkowitz",
                "Jesper Borg",
                "Francois Stricher",
                "Robby Nys",
                "Frederic Rousseau",
                "Luis Serrano"
            ],
            "title": "The foldx web server: an online force field",
            "venue": "Nucleic acids research,",
            "year": 2005
        },
        {
            "authors": [
                "Sisi Shan",
                "Shitong Luo",
                "Ziqing Yang",
                "Junxian Hong",
                "Yufeng Su",
                "Fan Ding",
                "Lili Fu",
                "Chenyu Li",
                "Peng Chen",
                "Jianzhu Ma"
            ],
            "title": "Deep learning guided optimization of human antibody against sars-cov-2 variants with broad neutralization",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2022
        },
        {
            "authors": [
                "Woong-Hee Shin",
                "Keiko Kumazawa",
                "Kenichiro Imai",
                "Takatsugu Hirokawa",
                "Daisuke Kihara"
            ],
            "title": "Quantitative comparison of protein-protein interaction interface using physicochemical featurebased descriptors of surface patches",
            "venue": "Frontiers in Molecular Biosciences,",
            "year": 2023
        },
        {
            "authors": [
                "Woong-Hee Shin",
                "Keiko Kumazawa",
                "Kenichiro Imai",
                "Takatsugu Hirokawa",
                "Daisuke Kihara"
            ],
            "title": "Quantitative comparison of protein-protein interaction interface using physicochemical featurebased descriptors of surface patches",
            "venue": "Frontiers in Molecular Biosciences,",
            "year": 2023
        },
        {
            "authors": [
                "Raghav Shroff",
                "Austin W Cole",
                "Barrett R Morrow",
                "Daniel J Diaz",
                "Isaac Donnell",
                "Jimmy Gollihar",
                "Andrew D Ellington",
                "Ross Thyer"
            ],
            "title": "A structure-based deep learning framework for protein",
            "venue": "engineering. bioRxiv,",
            "year": 2019
        },
        {
            "authors": [
                "Valentina Sora",
                "Adrian Otamendi Laspiur",
                "Kristine Degn",
                "Matteo Arnaudi",
                "Mattia Utichi",
                "Ludovica Beltrame",
                "Dayana De Menezes",
                "Matteo Orlandi",
                "Ulrik Kristoffer Stoltze",
                "Olga Rigina",
                "Peter Wad Sackett",
                "Karin Wadt",
                "Kjeld Schmiegelow",
                "Matteo Tiberti",
                "Elena Papaleo"
            ],
            "title": "Rosettaddgprediction for high-throughput mutational scans: From stability to binding",
            "venue": "Protein Science,",
            "year": 2023
        },
        {
            "authors": [
                "Martin Steinegger",
                "Johannes S\u00f6ding"
            ],
            "title": "Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets",
            "venue": "Nature biotechnology,",
            "year": 2017
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jon Shlens",
                "Zbigniew Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Raphael Townshend",
                "Rishi Bedi",
                "Patricia Suriana",
                "Ron Dror"
            ],
            "title": "End-to-end learning on 3d protein structure for interface prediction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Michel van Kempen",
                "Stephanie S Kim",
                "Charlotte Tumescheit",
                "Milot Mirdita",
                "Jeongjae Lee",
                "Cameron LM Gilchrist",
                "Johannes S\u00f6ding",
                "Martin Steinegger"
            ],
            "title": "Fast and accurate protein structure search with foldseek",
            "venue": "Nature Biotechnology,",
            "year": 2023
        },
        {
            "authors": [
                "Thom Vreven",
                "Iain H Moal",
                "Anna Vangone",
                "Brian G Pierce",
                "Panagiotis L Kastritis",
                "Mieczyslaw Torchala",
                "Raphael Chaleil",
                "Brian Jim\u00e9nez-Garc\u0131\u0301a",
                "Paul A Bates",
                "Juan Fernandez-Recio"
            ],
            "title": "Updates to the integrated protein\u2013protein interaction benchmarks: docking benchmark version 5 and affinity benchmark version 2",
            "venue": "Journal of molecular biology,",
            "year": 2015
        },
        {
            "authors": [
                "Menglun Wang",
                "Zixuan Cang",
                "Guo-Wei Wei"
            ],
            "title": "A topology-based network tree for the prediction of protein\u2013protein binding affinity changes following mutation",
            "venue": "Nature Machine Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Joseph L Watson",
                "David Juergens",
                "Nathaniel R Bennett",
                "Brian L Trippe",
                "Jason Yim",
                "Helen E Eisenach",
                "Woody Ahern",
                "Andrew J Borst",
                "Robert J Ragotte",
                "Lukas F Milles"
            ],
            "title": "De novo design of protein structure and function with rfdiffusion",
            "venue": "Nature, pp",
            "year": 2023
        },
        {
            "authors": [
                "Peng Xiong",
                "Chengxin Zhang",
                "Wei Zheng",
                "Yang Zhang"
            ],
            "title": "Bindprofx: assessing mutation-induced binding affinity change by protein interface profiles with pseudo-counts",
            "venue": "Journal of molecular biology,",
            "year": 2017
        },
        {
            "authors": [
                "Jason Yim",
                "Brian L Trippe",
                "Valentin De Bortoli",
                "Emile Mathieu",
                "Arnaud Doucet",
                "Regina Barzilay",
                "Tommi Jaakkola"
            ],
            "title": "Se (3) diffusion model with application to protein backbone generation",
            "venue": "arXiv preprint arXiv:2302.02277,",
            "year": 2023
        },
        {
            "authors": [
                "Chengxin Zhang",
                "Morgan Shine",
                "Anna Marie Pyle",
                "Yang Zhang"
            ],
            "title": "Us-align: universal structure alignments of proteins, nucleic acids, and macromolecular complexes",
            "venue": "Nature methods,",
            "year": 2022
        },
        {
            "authors": [
                "Yang Zhang",
                "Jeffrey Skolnick"
            ],
            "title": "Scoring function for automated assessment of protein structure template quality",
            "venue": "Proteins: Structure, Function, and Bioinformatics,",
            "year": 2004
        },
        {
            "authors": [
                "Zuobai Zhang",
                "Minghao Xu",
                "Vijil Chenthamarakshan",
                "Aur\u00e9lie Lozano",
                "Payel Das",
                "Jian Tang"
            ],
            "title": "Enhancing protein language models with structure-based encoder and pre-training",
            "venue": "arXiv preprint arXiv:2303.06275,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The goal of this work is to develop a practically-reliable method for designing protein\u2013protein interactions (PPIs). We focus on predicting binding affinity changes of protein complexes upon mutations. This problem, also referred to as \u2206\u2206G prediction, is the central challenge of protein binder design (Marchand et al., 2022). The discovery of mutations increasing binding affinity unlocks application areas of tremendous importance, most notably in healthcare and biotechnology. Interactions between proteins play a crucial role in mechanisms of various diseases including cancer and neurodegenerative diseases (Lu et al., 2020; Ivanov et al., 2013). Simultaneously, they offer potential pathways for the action of protein-based therapeutics in addressing other medical conditions, such as stroke, which stands as a leading cause of disability and mortality worldwide (Feigin et al., 2022; Nikitin et al., 2022). Furthermore, the design of PPIs also extends its relevance to biotechnological applications, including the development of bio-sensors (Scheller et al., 2018; Langan et al., 2019).\nWhile machine learning methods for designing protein\u2013protein interactions have been developed for more than a decade, their generalization beyond training data is hindered by multiple challenges. First, training datasets for protein\u2013protein interactions suffer from a severe redundancy and biases. These inherent imperfections are difficult to identify and rectify using available algorithmic tools due to their low scalability. Second, the approaches employed for train-test splitting of both largescale unlabeled PPIs and small mutational libraries introduce data leakage, i.e. that an interaction with a near-duplicate 3D structure appears both in the training and test set. As a result, performance estimates of machine learning models do not accurately reflect their real-world generalization capabilities. Besides that, commonly employed evaluation metrics do not fully capture practically important performance criteria. Finally, the design and training of the models for predicting binding affinity changes upon mutations are often prone to overfitting, as they do not fully capture the right granularity of the protein complex representation and the appropriate inductive biases.\nIn this work, we make a step towards better generalizable machine learning for the design of protein\u2013 protein interactions. The contribution of our work is threefold. First, we exhaustively mine Protein Data Bank to establish a novel complete and non-redundant dataset of 3D structures of protein\u2013protein interactions, which we name PPIRef. Our dataset exceeds the other alternatives of its\nkind in terms of both size and quality. To achieve higher quality, we develop iDist, a scalable algorithm for comparing the 3D structures of protein\u2013protein interfaces, and apply it to cluster and debias our dataset. Furthermore, we apply iDist to evaluate the redundancy of the other datasets and to illustrate a data leakage issue inherent in existing data splits. Second, we develop PPIFORMER \u2013 an SE(3)-equivariant transformer for modeling the coarse-grain structures of PPIs. Utilizing PPIRef, we pre-train PPIFORMER to generalize across diverse variants of protein binders. We propose and analyze the data representation and regularization techniques to maximize the generalization capabilities of our model. Finally, we fine-tune PPIFORMER on the task of predicting binding affinity changes of protein\u2013protein interactions upon mutations (\u2206\u2206G). To achieve effective transfer learning, we utilize a slightly modified pre-training loss function inspired by the thermodynamic definition of \u2206\u2206G. Further, we create a higher-quality data split of existing mutational data and rethink the evaluation metrics. We demonstrate that our method outperforms state-of-the-art machine learning models on multiple test sets which include the case studies on designing human antibodies targeting SARS-CoV-2 and the staphylokinase thrombolytic.\nIn summary, our work introduces a new state-of-the-art machine learning model for designing protein\u2013protein interactions, as well as the new largest and non-redundant dataset of protein\u2013protein interactions along with the algorithm for analyzing large-scale PPI data.\n2 RELATED WORK\nPredicting effects of mutations on protein\u2013 protein interactions. The task of predicting the effects of mutations on protein-protein interactions (\u2206\u2206G) has been studied for more than a decade (Geng et al., 2019b). Traditionally, \u2206\u2206G predictors relied on physics-based simulations and statistical potentials (Barlow et al., 2018; Xiong et al., 2017; Dehouck et al., 2013; Schymkowitz et al., 2005). In contrast, more recent machine learning approaches (Rodrigues et al., 2021; Pahari et al., 2020; Wang et al., 2020; Geng et al., 2019a) pri-\nmarily rely on handcrafted descriptors of protein-protein interfaces. The latest generation of the methods employs end-to-end deep learning from protein complexes, claiming to surpass computationally intensive force field simulations in terms of predictive performance (Luo et al., 2023; Shan et al., 2022; Liu et al., 2021). In this study, we revisit this claim and show that the performance of the state-of-the-art methods may be overestimated due to leaking evaluation data and overfitting.\nSelf-supervised learning for protein design. Collecting annotated mutational data is resourceintensive, prompting exploration into self-supervised learning to reduce dependency on small labeled datasets. Notably, protein language models have demonstrated competitive performance without any supervision by utilizing raw inferred amino acid type probabilities (Meier et al., 2021). Similarly, protein inverse folding methods were shown to effectively predict the effects of single-point mutations on protein complex stability (Hsu et al., 2022). Likewise, predicting missing amino acids in full-atomic protein structures was shown to be effective for detecting mutation hotspots (Shroff et al., 2019). Recently, Zhang et al. (2023) have demonstrated enhanced predictive performance on multiple downstream problems achieved through self-supervised pre-training on synthetic tasks on protein structures. Some of the latest binding \u2206\u2206G predictors utilize pre-training from monomeric protein structures by learning to reconstruct native side chain rotamer angles (Luo et al., 2023; Liu et al., 2021). In this study, we introduce a self-supervised model pre-trained on 3D structures of protein\u2013protein interactions for the downstream engineering of protein binders.\nDatasets of protein\u2013protein interactions. Historically, datasets of protein-protein interactions have been falling under two categories: sets of hundreds of small curated task-specific examples (Jankauskaite\u0307 et al., 2019; Vreven et al., 2015), and larger collections of thousands of unannotated interactions, potentially containing biases and structural redundancy (Morehead et al., 2021; Evans et al., 2021; Townshend et al., 2019; Gainza et al., 2020). In this work, we construct a new dataset that is one order of magnitude larger than existing alternatives, falling into the second category.\nWe further refine our dataset by removing nearly duplicated entries using our new scalable algorithm. This results in the largest available and non-redundant dataset of protein-protein interaction structures. Our algorithm for comparing protein-protein interactions contrasts with existing methods that rely on computationally intensive alignment procedures (Shin et al., 2023b; Mirabello & Wallner, 2018; Cheng et al., 2015; Gao & Skolnick, 2010a). Instead, we design our algorithm to enable large-scale retrieval of similar protein-protein interfaces by approximating their structural alignment. Our approach stands in line with prior work on the efficient retrieval of similar protein sequences (Steinegger & So\u0308ding, 2017), and more recently, monomeric protein structures (van Kempen et al., 2023)."
        },
        {
            "heading": "3 PPIREF: NEW LARGE DATASET OF PROTEIN\u2013PROTEIN INTERACTIONS",
            "text": "The Protein Data Bank (PDB) is a massive resource of over 200,000 experimentally obtained protein 3D structures. The space of protein\u2013protein interactions in PDB is hypothesized to cover nearly all physically plausible interfaces in terms of geometric similarity (Gao & Skolnick, 2010b). Nevertheless, it comes at the expense of a heavy structural redundancy given by the highly modular anatomy of many proteins and their complexes (Draizen et al., 2022; Burra et al., 2009). To the best of our knowledge, there have been no attempts to quantitatively assess the redundancy of the large protein\u2013protein interaction space represented in PDB and construct a balanced subset suitable for large-scale learning. We start this section by introducing the approximate iDist algorithm enabling fast detection of near duplicate protein\u2013protein interfaces (Section 3.1). Using iDist, we assess the effective size of existing PPI datasets (Section 3.2) and propose a new, largest non-redundant PPI dataset, called PPIRef (Section 3.3)."
        },
        {
            "heading": "3.1 IDIST: NEW EFFICIENT APPROACH FOR PROTEIN\u2013PROTEIN INTERFACE DEDUPLICATION",
            "text": "Existing algorithms to determine the similarity between protein-protein interfaces rely on structural alignment procedures (Gao & Skolnick, 2010a; Mirabello & Wallner, 2018; Shin et al., 2023a). However, since finding an optimal alignment between PPIs is computationally heavy, alignmentbased approaches do not scale to large datasets. Therefore, we develop a reliable approximation of the well established algorithm iAlign, which adapts the well-known TM-score to the domain of PPIs (Gao & Skolnick, 2010a). Our iDist algorithm computes SE(3)-invariant vector representations of protein\u2013protein interfaces via message passing between residues, which in turn enables efficient detection of near-duplicates using the Euclidean distance with a threshold estimated to approximate iAlign (Figure 3). The complete algorithm is described in Appendix A.\nTo evaluate the performance of iDist, we benchmark it against iAlign. We start by sampling 100 PDB codes from the DIPS dataset (Townshend et al., 2019) of protein-protein interactions and extract the corresponding 1,646 PPIs. Subsequently, we calculate all 2,709,316 pairwise similarities between these PPIs using both the exact iAlign struc-\ntural alignment algorithm and our efficient iDist approximation. Employing 128 CPUs in parallel, iAlign computations took 2 hours, while iDist required 15 seconds being around 480 times faster. Next, we estimate the quality of the approximation on the task of retrieving near-duplicate PPI interfaces. Using iAlign-defined ground-truth duplicates, iDist demonstrates a precision of 99% and a recall of 97%. These evaluation results confirm that iDist enables efficient deduplication of extensive PPI data within a reasonable timeframe, which is not achievable with other algorithms. The scala-\nbility of the method enabled us to analyze existing PPI datasets and their respective data splits used by recent machine-learning approaches. Please refer to Appendix A for the details of the evaluation of iDist."
        },
        {
            "heading": "3.2 LIMITATIONS OF EXISTING PROTEIN\u2013PROTEIN INTERACTION DATASETS",
            "text": "We apply iDist to assess the composition of DIPS \u2013 the state-of-the-art dataset of protein\u2013protein interactions comprising approximately 40,000 entries (Morehead et al., 2021; Townshend et al., 2019). We construct a near-duplicate network of DIPS by connecting two interfaces if they are detected as duplicates by iDist. This results in a graph with 8.5K components, where the largest component comprises 36% of the interfaces. Notably, relaxing iDist duplicate detection threshold 1.5 times results in 84% of the interfaces forming a single component, indicating the high connectivity of the PPI space in DIPS. After iteratively removing entries with at least one near-duplicate, the dataset size drops to 22% of its initial size. The aforementioned observations are in agreement with the hypothesis of Gao & Skolnick (2010a) suggesting high redundancy and connectivity of the PPI space in PDB. Finally, we analyze the existing data splits of DIPS. We estimate the leakage ratio of a split by finding the percentage of test interactions having near-duplicates in a training fold. We find that the split based on protein sequence similarity (not 3D structure as in our case) used for the validation of protein of protein\u2013protein docking models (Ketata et al., 2023; Ganea et al., 2021) has near duplicates in the training data for 53% of test examples, while the random split from Morehead et al. (2021) has 88% of such leaks between training and test data. Figure 1 illustrates an example of such a leak: the left interface is present in the training fold of the original split (Ganea et al., 2021), while the interface on the right-hand side is in the test fold."
        },
        {
            "heading": "3.3 PPIREF: NEW LARGE DATASETS OF PROTEIN\u2013PROTEIN INTERACTIONS",
            "text": "We address the redundancy of existing PPI datasets by building a novel dataset of structurally distinct 3D protein-protein interfaces. We call the new dataset PPIRef. We start by exhaustively mining all 202,380 entries from the Protein Data Bank as of June 20, 2023. Subsequently, we extract all putative interactions by finding all pairs of protein chains that have a contact between heavy atoms in the range of at most 10A\u030a. This procedure results in 837,241 hypothetical PPIs, further referred to as the raw PPIRef800K. Further, we apply well-established criteria (Appendix A, Townshend et al. (2019)) to select only biophysically proper interactions. This filtering results in 322,454 protein\u2013 protein interactions comprising the vanilla version of PPIRef, which we name PPIRef300K. Finally, we iteratively filter out near-duplicate entries by applying the iDist algorithm. This de-duplication results in the final non-redundant dataset comprising 45,553 PPIs, which we call PPIRef50K (or simply PPIRef) in this paper. Table 1 illustrates that our dataset exceeds the sizes and effective sizes of the representative alternatives DIPS (Townshend et al., 2019), DIPS-Plus (Morehead et al., 2021) and the one used to train the MaSIF-search model (Gainza et al., 2020)."
        },
        {
            "heading": "4 LEARNING TO DESIGN PROTEIN-PROTEIN INTERACTIONS",
            "text": "Predicting differences in binding energies (\u2206\u2206G) of protein complexes upon amino-acid substitutions is a core problem in the design of protein-protein interactions. However, the task of \u2206\u2206G prediction is challenging for machine learning due to the scarcity of available annotated datasets, covering only several hundred protein\u2013protein interactions (Jankauskaite\u0307 et al., 2019). In this section, we first discuss our coarse-grained representations of proteins that enable modeling protein complexes and their mutants in a way robust to overfitting (Section 4.1). Second, we introduce the PPIFORMER model architecture, which efficiently operates on the representations through its SE(3)-equivariance (Section 4.2). Third, we describe our masking procedure and a loss function that enables us to pre-train PPIformer on 3D PPIs from the PPIRef dataset (Section 4.3). Finally, we demonstrate how the pre-trained PPIFORMER can be seamlessly fine-tuned to predict \u2206\u2206G values leveraging masked modeling and a thermodynamically inspired loss function (Section 4.4)."
        },
        {
            "heading": "4.1 REPRESENTATION OF PROTEIN\u2013PROTEIN COMPLEXES",
            "text": "In a living cell, proteins continually undergo thermal fluctuations and change their precise shapes. This fact is particularly manifested on protein surfaces, where the precise atomic structure is flexible\nHidden representation\nA\nB 1C78 1. Construct\nmasked graph\nInput PPI Input representation Probabilities\nInput mutant\n2. Encode 3. Classify\nSelf-supervised pre-training BothDownstream task fine-tuning\nFigure 2: Overview of PPIFORMER. A single pre-training step starts with randomly sampling a protein-protein interaction c (in this example, the staphylokinase dimer A-B from the PDB entry 1C78). Next, randomly selected residues M are masked to obtain the masked interaction c\\M . After that, the interaction is converted into a graph representation (G,X,E,F0,F1) with masked nodes M (black circles). The model subsequently learns to classify the types of masked amino acids by acquiring SE(3)-invariant hidden representation H of the whole interface via the encoder f and classifier g. On the downstream task of \u2206\u2206G prediction, mutated amino acids are masked, and the probabilities of possible substitutions PM,: are inferred with the pre-trained model. Finally, the \u2206\u2206G is estimated based on the predicted probabilities via log odds.\ndue to interactions with other molecules. As a result, protein\u2013protein interfaces may be highly flexible (Kastritis & Bonvin, 2013). Nevertheless, the available crystal structures from the Protein Data Bank only represent their rigid, energetically favorable, states (Jin et al., 2023). Therefore, we aim to develop representations of protein complexes robust to atom fluctuations, as well as wellsuited for modeling mutated interface variants. In this section, we define a coarse residue-level representation to allow for sufficient flexibility of the interfaces, which nevertheless captures the major aspects of the interaction patterns.\nMore specifically, consider a protein\u2013protein complex (or interface) c \u2208 AN of N residues in the alphabet of amino acidsA = {1, . . . , 20}. We consider the order of residues in c arbitrary, ensuring permutation invariance, the property critical for representing the interfaces (Mirabello & Wallner, 2018). Next, we represent the complex c as a k-NN graph G, where nodes represent the individual residues and edges are based on the proximity of corresponding alpha-carbon (C\u03b1) atoms. The graph is augmented by node-level and pair-wise features X,E,F0,F1. In detail, matrix X \u2208 RN\u00d73 contains the coordinates of alpha-carbons of all residues. Next, all residue nodes are put into semantic relation by pair-wise binary edge features E \u2208 {0, 1}N\u00d7N with 0 if residues come from the same protein partner and 1 otherwise. Finally, each node is associated with two kinds of features: type-0 F0 \u2208 RN\u00d720\u00d71, also referred to as scalars, and type-1 vectors F1 \u2208 RN\u00d71\u00d73. Features F0 capture the one-hot representations of wild-type amino acids c, while vectors F1 are defined as virtual beta-carbon orientations calculated from the backbone geometry using ideal angle and bond length definitions (Dauparas et al., 2022). Please note that F0 are invariant with respect to rototranslations and F1 are equivariant. Collectively, coordinates X and virtual-beta carbon directions F1 capture the complete geometry of the protein backbones involved in the complex. Our representation is agnostic to precise angles of side-chain rotamers, implicitly modeling their flexibility. The schematic illustration is provided in Figure 2 (Input representation)."
        },
        {
            "heading": "4.2 PPIFORMER MODEL",
            "text": "In order to effectively learn from protein\u2013protein complexes or interfaces (G,X,E,F0,F1), respecting permutation invariance of amino acids and arbitrary coordinate systems of Protein Data Bank entries, we define PPIFORMER, an SE(3)-equivariant architecture designed to learn from protein-protein interactions. The model consists of an encoder f and classifier g such that g(f(G,X,E,F0,F1)) yields a probability matrix P \u2208 [0, 1]N\u00d7|A|, where Pi,j defines the probability of amino acid type j at residue i. Intuitively, matrix P captures the likelihood of amino acids to occur at different positions in a protein complex depending on their structural context. Further,\nwe use probabilities to estimate the type of a masked amino acid i \u2264 N as p(c\u0302i = j|c\\M ) := Pi,j , which we discuss in the following subsection.\nThe core of our architecture is comprised of Equiformer SE(3)-equivariant graph attention blocks f (l) (Liao et al., 2023; Liao & Smidt, 2022). Each of the L blocks (or layers) updates equivariant features of different types associated with all amino acids via message passing with an equivariant attention mechanism with K heads. In detail, the input to lth block is an original graph G with coordinates X and edge features E along with a set of node feature matrices H(l)k \u2208 RN\u00d7dk\u00d7(2k+1) of different types k \u2265 0 from the previous layer. Here, dk \u2265 0 is the hyper-parameter defining the number of type-k hidden features shared across all blocks, and 2k + 1 is their corresponding theoretical dimension. The input node features for the first layer are set to F0,F1. The output of each block is given by the updated node features of different equivariance types. Internally, all blocks lift hidden representations up to the hidden degree deg \u2265 0 which is an additional hyper-parameter. Taking the type-0 outputs of the final layer leads to invariant amino acid embeddings:\nH (1) 0 ,H (1) 1 , . . . ,H (1) deg = f (0)(G,X,E,F0,F1), (1)\nH (l+1) 0 ,H (l+1) 1 , . . . ,H (l+1) deg = f (l)(G,X,E,H (l) 0 ,H (l) 1 , . . . ,H (l) deg), (2)\nH := H (L) 0 . (3)\nCollectively, we term the composition of transformer blocks f (l) as the encoder\nf : G,X,E,F0,F1 7\u2192 H (4)\nwith the property of SE(3)-invariance for any rotation R \u2208 SO(3) and translation t \u2208 R3:\nf(G,X,E,F0,F1) = f(G,XR+ 1t T ,E,F0,F1). (5)\nTo further estimate the probabilities of masked amino acids discussed below, we apply a 1-layer classifier with the softmax activation g : RN\u00d7d0\u00d71 \u2192 [0, 1]N\u00d7|A| on top of masked node embeddings."
        },
        {
            "heading": "4.3 3D EQUIVARIANT SELF-SUPERVISED PRE-TRAINING FROM UNLABELED",
            "text": "PROTEIN\u2013PROTEIN INTERACTIONS\nIn this section, we describe how we leverage a large amount of unlabeled protein\u2013protein interfaces from PPIRef to train our model for predicting the effects of mutations on binding affinity. We first present the pre-training method used to effectively train PPIFORMER to capture the space of native variants of binding interfaces. Then we discuss the fine-tuning of our model to predict the effects of mutations via log odds ratios. Finally, we demonstrate that the proposed combination of masked pre-training and log-odds fine-tuning is well-justified thermodynamically.\nStructural masking of protein\u2013protein interfaces. The paradigm of masked modeling has proven to be an effective way of pre-training from protein sequences (Lin et al., 2023). Nevertheless, while the masking of amino acids in a protein sequence is straightforward by introducing a special token, the masking of structural fragments is not obvious. Here, we leverage our flexible coarse-grained protein\u2013protein complex representation to define masking by simple change in the feature representation.\nHaving a protein complex or interface c \u2208 AN and a mask M \u2282 {1, . . . , N}, we define the masked structure c\\M \u2208 (A \u222a {0})N by setting amino acid classes at all masked positions M to zeros. Consequently, when constructing one-hot scalar features F0 from c\\M , we set corresponding rows to zeroes. Note that vector features F1 do not require any masking since they do not contain any information about the types of amino acids, benefiting from using virtual beta-carbons instead of real ones. Additionally, the glycine amino acid lacking the beta-carbon atom does not need special handling.\nLoss for masked modeling of protein\u2013protein interfaces. Having a protein\u2013protein interaction and a random mask M \u2282 {1, . . . , N}, we follow a traditional cross-entropy loss for training the model to predict native amino acids in a masked version c\\M \u2208 (A \u222a {0})N of the native complex\nc \u2208 AN . To additionally increase the generalization capabilities of PPIFORMER to capture potentially unseen or mutated interfaces, we further employ two regularization measures. First, we apply label smoothing to force the model not to be overly confident in wild-type amino acids, so that it can be more flexible towards unseen variants (Szegedy et al., 2016). Second, we weight the loss inversely to the prior distribution of amino acids in protein\u2013protein interfaces to remove the bias towards overrepresented amino acids such as leucine. The overall pre-training loss is defined as\nLmasking = \u2212 \u2211 i\u2208M wci\n[ (1\u2212 \u03f5) \u00b7 log p(c\u0302i = ci|c\\M ) +\n\u2211 j\u2208A\\ci log p(c\u0302i = cj |c\\M ) \u00b7 \u03f5 |A|\n] . (6)\nHere, p(c\u0302i = ci|c\\M ) is the probability of predicting the wild-type amino acid type of a masked residue i \u2208 M . Next, the sum over all other amino acid types j \u2208 A \\ ci is the label smoothing regularization term where 0 < \u03f5 < 1 is the smoothing hyper-parameter. Further, wci is the weighting factor corresponding to the wild-type amino acid ci at position i. Finally, the sum over i \u2208 M corresponds to the loss being evaluated over all masked residues."
        },
        {
            "heading": "4.4 TRANSFER LEARNING FOR PREDICTING THE EFFECTS OF MUTATIONS ON",
            "text": "PROTEIN\u2013PROTEIN INTERACTIONS\nPredicting the effects of mutations on binding affinity (\u2206\u2206G) is a central task in designing protein\u2013 protein interactions. Nevertheless, collecting \u2206\u2206G annotations is expensive and time-consuming. As a result, the labeled mutational data for binding affinity changes are scarce, not exceeding several thousand annotations (Jankauskaite\u0307 et al., 2019). Therefore, in our work, we aim to leverage the pretrained PPIFORMER model for scoring mutations with minimal supervision.\nThermodynamic motivation. From the thermodynamic perspective, binding energy change \u2206\u2206G (or alternatively denoted as \u2206\u2206Gwt\u2192mut) can be decomposed as follows:\n\u2206\u2206G = \u2206Gmut \u2212\u2206Gwt = RT (log (Kwt)\u2212 log (Kmut)), (7) where \u2206Gmut and \u2206Gwt are binding energies of mutated and wild-type complexes, respectively (Kastritis & Bonvin, 2013). The R, T > 0 terms are gas and temperature environmental constants, and Kwt and Kmut denote the equilibrium constants of protein\u2013protein interactions, i.e. the ratios of the concentration of the complexes formed when proteins interact to the concentrations of the individual proteins. The form of \u2206\u2206G introduces symmetries into the problem of its estimation. For example, predicting the effect of a reversed mutation \u2206\u2206Gmut\u2192wt should satisfy the antisymmetry property \u2206\u2206Gwt\u2192mut = \u2212\u2206\u2206Gmut\u2192wt. Available machine learning predictors either ignore the symmetry (Liu et al., 2021) or require two forward passes to estimate the quantity twice, for both directions, and combine the predictions to enforce the antisymmetry as (\u2206\u2206Gwt\u2192mut \u2212 (\u2212\u2206\u2206Gmut\u2192wt))/2 (Luo et al., 2023).\nPredicting effects of mutations on binding energy via the log odds ratio. Here, in line with physics-informed machine learning (Karniadakis et al., 2021), we leverage the thermodynamic interpretation of \u2206\u2206G to adapt the pre-training cross-entropy for the downstream \u2206\u2206G finetuning. Having a complex c \u2208 AN and its mutant m \u2208 AN , with the substitutions of residues M \u2282 {1, . . . , N} such that ci \u0338= mi for all i \u2208M , we estimate its binding energy change as:\n\u2206\u0302\u2206G = \u2211 i\u2208M log p(c\u0302i = ci|c\\M )\u2212 \u2211 i\u2208M log p(c\u0302i = mi|c\\M ), (8)\nwhere the p terms are PPIFORMER output probabilities. The \u2206\u0302\u2206G prediction is a log odds ratio, used by Meier et al. (2021) for zero-shot predictions on protein sequences. Intuitively, the predicted binding energy change upon mutation is negative (increased affinity) if the predicted likelihood of the mutated structure is higher than the likelihood of the native structure. When simultaneously decomposing Equation (7) and Equation (8), we can observe that ln (Kwt) is estimated as\u2211\ni\u2208M log p(c\u0302i = ci|c\\M ), and ln (Kmut) is estimated as \u2211\ni\u2208M log p(c\u0302i = mi|c\\M ). Considering that the estimate of the wild type likelihood is identical to the pre-training loss (Equation (6)) up to regularizations, we reason that during pre-training PPIFORMER learns the correlates of \u2206G values, whereas during fine-tuning it refines them to \u2206\u2206G predictions."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we describe our protocol for benchmarking the generalization on the task of \u2206\u2206G prediction and present our results. We begin by introducing the evaluation datasets, metrics, and baseline methods (Section 5.1). Next, we show that our approach outperforms state-of-the-art machine learning methods in designing protein-protein interactions distinct from the training data (Section 5.2). Additionally, we show the benefits of our new PPIRef dataset and key PPIFORMER components through the ablation studies in Appendix C."
        },
        {
            "heading": "5.1 EVALUATION PROTOCOL",
            "text": "Datasets. To fine-tune PPIFORMER for \u2206\u2206G prediction we use the largest available labeled dataset, SKEMPI v2.0 containing 7085 mutations (Jankauskaite\u0307 et al., 2019). Prior works (Luo et al., 2023; Liu et al., 2021) primarily validate the performance of models on PDB-disjoint splits of the dataset. However, we find such approach not appropriate to measure the generalization capacity of predictors due to a high ratio of leakages. Therefore, we construct a new non-leaking cross-validation split and set aside 5 PPI outliers to obtain 5 distinct test folds. Further, to simulate practical protein-protein interaction design scenarios, we perform additional evaluation on two independent case studies. These case studies assess the capability of the model to retrieve mutations optimizing the human P36-5D2 antibody against SARS-CoV-2 and increasing the thrombolytic activity of staphylokinase. Please refer to Appendix B.2 for the details.\nMetrics. To evaluate the capabilities of models in prioritizing favorable mutations, we use the Spearman correlation coefficient between predicted and ground-truth \u2206\u2206G values. To evaluate the performance in detecting stabilizing mutations, we calculate precision and recall with respect to negative \u2206\u2206G values. We also report additional metrics to enable comparison with prior work (Luo et al., 2023). However, in Appendix B.3, we emphasize that these metrics can be misleading when selecting a model for a practical application. For the independent retrieval case studies, we report predicted ranks for all favorable mutations and evaluate the retrieval using precision at k metrics (P@k). We provide the details in Appendix B.3.\nBaselines. We evaluate the performance of PPIformer against the state-of-the-art represenatives of four main categories of available methods. Specifically, Flex ddG (Barlow et al., 2018) Rosettabased (Leman et al., 2020) protocol is the most advanced traditional force-field simulator (the same protocol is used in a recent RosettaDDGPrediction toolbox (Sora et al., 2023)). The baseline machine learning methods are: RDE-Network (Luo et al., 2023) pre-trained on unlabeled monomer protein structures and fine-tuned for \u2206\u2206G prediction on SKEMPI v2.0; ESM-IF (Hsu et al., 2022) unsupervised predictor of \u2206\u2206G trained for inverse folding on experimental and AlphaFold (Jumper et al., 2021) structures; and MSA Transformer (Rao et al., 2021), an evolutionary baseline. A more detailed description of the methods and their settings is provided in Appendix B.4."
        },
        {
            "heading": "5.2 COMPARISON WITH THE STATE OF THE ART",
            "text": "Prediction of \u2206\u2206G on held-out test cases. As shown in table Table 2, on 5 challenging test PPIs from SKEMPI v2.0, PPIFORMER confidently outperforms all machine-learning baselines on 6 out of 7 evaluation metrics, being second-best in terms of recall. We achieve a 173% relative improvement in mutation ranking compared to the state-of-the-art supervised RDE-Network, as measured\nby Spearman correlation. Importantly, this non-leaking evaluation reveals that traditional force field simulators, represented by state-of-the-art Flex ddG, still outperform machine learning methods in terms of prediction accuracy. However, in terms of speed, they may not be applicable in typical real-world large-scale mutational screenings, being 5 orders of magnitude slower (Appendix B.4).\nOptimization of a human antibody against SARS-CoV-2. Within a pool of 494 candidate single-point mutations of a human antibody, our model detects 2 mutations out of 5 (using a 10% threshold as defined by Luo et al. (2023)) that are known to be effective against SARS-CoV-2 (Table 3). The best among other methods detects 3 out of 5 mutations. However, PPIFORMER achieves superior performance when considering the ranks of all 5 mutations collectively, with the maximum rank for the favorable mutation not exceeding 21.46%. Notably, unlike other methods, PPIFORMER assigns the best rank to one of the 5 favorable mutations (P@1 = 100). Since there are no groundtruth annotations for all the candidates, and the annotated ones were pre-selected by DDGPred (Shan et al., 2022), this observation is particularly significant, as the evaluation of worse ranked candidates may not be exactly correct due to the missing ground truth annotations.\nEngineering staphylokinase for enhanced thrombolytic activity. Within a pool of 80 mutations, PPIFORMER precisely prioritizes 2 out of 6 strongly favorable mutations with the top-2 ranked candidates (Table 4). In contrast, the second-best method, RDE-Network, identifies one mutation and assigns it a worse ranking (top-4). MSA Transformer provides accurate top-1 prediction but strongly loses performance on higher cutoffs and detecting two-fold activity-increasing mutations."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this work, we have built PPIRef \u2013 the largest and non-redundant dataset for self-supervised learning of protein-protein interactions. Using this data, we have pre-trained a new model, PPIFORMER, designed to represent protein-protein interactions. Subsequently, we fine-tuned the model for the target task of discovering mutations enhancing protein\u2013protein interactions. We have shown that our model effectively generalizes to unseen protein\u2013protein interfaces outperforming state-of-theart machine learning methods. This work opens up the possibility of training large-scale foundation models for designing protein-protein interactions."
        },
        {
            "heading": "A DETAILS OF THE IDIST ALGORITHM AND PPIREF DATASET",
            "text": "Algorithm 1: iDistEMBED Input: Protein\u2013protein interface I of N residues from C chains. Output: vector representation of the interface zI . /* Get coordinates, features, and partner information */ 1 X \u2208 RN,3,F \u2208 RN,d,p \u2208 {1, . . . , C}N \u2190 get residues(I) /* Embed residues */ 2 for i\u2190 1 to N do 3 Jintra \u2190 {j \u2208 {1, . . . , N} | pi = pj} 4 Jinter \u2190 {j \u2208 {1, . . . ,M} | pi \u0338= pj} 5 mintra \u2190 1|Jintra| \u2211 j\u2208Jintra fj \u00b7 e\u2212 \u2225xi\u2212xj\u2225 2 2 \u03b1\n6 minter \u2190 1|Jinter| \u2211 j\u2208Jinter fj \u00b7 e\u2212 \u2225xi\u2212xj\u2225 2 2 \u03b1 7 hi \u2190 12 fi + 14mintra \u2212 14minter /* Embed interface */ 8 for c\u2190 1 to C do 9 Jc \u2190 {j \u2208 {1, . . . , N} | pj = c}\n10 zI \u2190 1|C| \u2211C c=1 1 |Jc| \u2211\nj\u2208Jc hj 11 return zI\nAlgorithm 2: iDist Input: Two protein\u2013protein interfaces I and J . Output: Distance \u2265 0.\n1 zI \u2190 iDistEMBED(I) 2 zJ \u2190 iDistEMBED(J ) 3 return \u2225zI \u2212 zJ \u22252\niDist algorithm. Algorithms 1 and 2 outline iDist, a fast method for detecting near-duplicate protein\u2013protein interfaces. Algorithm 1 details the conversion of a protein-protein interface I into a vector representation zI . In the first step, the features of the interface X,F and p are extracted. The residue coordinates X are determined by the positions of the C\u03b1 atoms. Next, the residue vector features F are initialized with simple 20-dimensional one-hot encodings of amino acids. We also experiment with ESM-1b features but obtain slightly lower performance. We observe that the reduced performance is attributed to ESM-1b biasing the comparison towards entire chains rather than interfaces (Rives et al., 2021). Finally, each residue is associated with a label indicating the chain it belongs to, forming the vector p.\nIn the following step, detailed in lines 2-7, the hidden representations hi for each residue i are constructed. Each residue receives messages from other residues within the same chain Jintra as well as from the other chain Jinter. Inspired by (Dauparas et al., 2022), the messages are represented by exponential radial basis functions (with \u03b1 set to 16). Each node averages intra- and inter-messages into contact patterns mintra and minter. The representation hi is then obtained by averaging the difference between mintra and minter, followed by averaging with the initial features.\nLastly, in steps 8-11, the interface representation zI is derived by averaging the hidden features across the individual chains and then across the interaction. As described in Algorithm 2, iDist then simply computes the Euclidean distance between two representations zI and zJ to compare two interfaces I and J . The outlined procedure can be seen as the implicit approximation of the structural alignment due to the SE(3)-invariance of the algorithm.\niDist evaluation and threshold selection. In order, to evaluate the approximation performance of iDist, we compare it against iAlign (Gao & Skolnick, 2010a), the adaptation of TM-score to protein\u2013 protein interfaces (Zhang & Skolnick, 2004). Figure 3 illustrates two main modes of joint probability distribution of the methods. Under iAlign(I,J ) < 0.7 and iDist(I,J ) > 0.04, the interfaces I and J are typically unrelated with more rare cases of sharing similar structural patterns. In contrast, under iAlign(I,J ) > 0.7 and iAlign(I,J ) < 0.04, the interfaces are near-duplicated. The analysis suggests the threshold of 0.04 for detecting near duplicates with iDist. Under this threshold, iDist achieves 0.99% precision and 0.97% recall with respect to iAlign.\nFurther, for analyzing the composition of DIPS we employ the same iDist threshold of 0.04. Nevertheless, we re-calibrate it to 0.03 for the interfaces in PPIRef which are constructed based on the 10A\u030a cutoff on C\u03b1 distances rather than 6A\u030a.\nConstruction of PPIRef50K. To filter out proper protein\u2013protein interactions from the raw PPIRef800K in order to obtain PPIRef300K, we apply three standard filtering criteria (Townshend et al., 2019). In particular, an interaction passes the criteria if: (i) the structure determination method is \u201cx-ray diffraction\u201d or \u201celectron microscopy\u201d, (ii) the crystallographical resolution is at most 3.5A\u030a, and (iii) the buried surface area (BSA) of the interface is at least 500A\u030a2. As a result, 99% of putative interactions satisfy the method criterion, 79% \u2013 resolution, and 47% \u2013 BSA."
        },
        {
            "heading": "B EXPERIMENTAL SETUP",
            "text": "B.1 IMPLEMENTATION DETAILS\nWe implement PPIFORMER in PyTorch leveraging PyTorch Geometric, PyTorch Lightning, Graphein, and a publicly available implementation of Equiformer1 (Paszke et al., 2019; Fey & Lenssen, 2019; Falcon & The PyTorch Lightning team, 2019; Jamasb et al., 2020). We train our model on four AMD MI250X GPUs in parallel in a distributed data parallel (DDP) mode. Our best model was trained for 32 epochs of dynamic masking in 22 hours.\n1https://github.com/lucidrains/equiformer-pytorch\nWe pre-train PPIFORMER by sampling mini-batches of protein\u2013protein interfaces using Adam optimizer with the default \u03b21=0.9, \u03b22 = 0.999 (Kingma & Ba, 2014). We partially explore the grid of hyper-parameters given by Table 5, and select the best model according to the performance on zero-shot \u2206\u2206G inference on the training set of SKEMPI v2.0. We further fine-tune the model with the learning rate of 3 \u00b7 10\u22124 and sampling 32 mutations per GPU in a single training step. We employ the three-fold cross-validation setup discussed in the next section, and ensemble three corresponding fine-tuned models for test predictions. We observe that dividing C\u03b1 by 4, as proposed by Watson et al. (2023), increases the rate of convergence.\nB.2 TEST DATASETS The\nparagraph was significantly expanded for higher clarity. SKEMPI v2.0 split. In contrast to previous works (Luo et al., 2023; Shan et al., 2022; Liu et al., 2021), we do not create a PDB-disjoint split of SKEMPI v2.0 (based on the #Pdb column) to evaluate the performance of the models. We found that this approach results in at least two kinds of data leakage, limiting the evaluation of generalization towards unseen binders. To illustrate the leakage, we analyze the most recent cross-validation split used to train and test the RDE-Network model (Luo et al., 2023). First, we find that every third test mutation is, in fact, a mutation of the training PPI with a different PDB code in the test set (but with the same Protein 1 and Protein 2 values). Second, more than half of the test PDB codes violate the original hold-out separation proposed by the authors of SKEMPI v2.0. Specifically, Jankauskaite\u0307 et al. (2019) performed automated clustering of PPIs followed by manual inspection in order to split the entries into independent groups for the evaluation of machine learning generalization, resulting in the Hold out proteins column. We observe that 57% of test PPIs in the PDB-disjoint split in (Luo et al., 2023) violate the proposed grouping, i.e. they have a homologous training PPI, with the same Hold out proteins value.\nFor example, the first two PPIs present in the SKEMPI v2.0 dataset, 1CSE E I and 1ACB E I, are interactions between the same human T-cell receptor and HLA-A with the viral peptide. These complexes share almost identical 3D structure and are both labeled with the same 6 mutations with slightly different \u2206\u2206G values. As a result, they have identical Protein 1 and Protein 2 values and, consequently, also belong to the same Hold out proteins group. Nevertheless, in the RDE-Network split, they are separated into different folds, exemplifying the data leakage in a random PDB-disjoint split.\nIn contrast, to ensure the effective evaluation of generalization, we split the SKEMPI v2.0 dataset into 3 cross-validation folds based on the Hold out proteins feature, as originally proposed by the dataset authors. Additionally, we stratify the \u2206\u2206G distribution across the folds to ensure a balance in labels. Before performing the cross-validation split, we set aside 5 distinct PPIs to form 5 test folds. We consider a PPI distinct if it does not share interacting partners and does not have a homologous binding site (Jankauskaite\u0307 et al., 2019) with any other PPI in SKEMPI v2.02 (and, as a result, has a distinct Hold out proteins value). Additionally, we ensure that for each of the 5 selected interactions, both negative and positive \u2206\u2206G labels are present. Finally, the maximum iAlign IS-score between all PPIs in the test folds and all PPIs in train-validation folds is 0.22, confirming the intended structural independence of the test set. Please refer to Table 6 for details on the constructed test set.\nOptimization of a human antibody against SARS-CoV-2. We utilize the benchmark of Luo et al. (2023) to test the capabilities of the models to optimize a human antibody against SARS-CoV-2. Specifically, the goal is to retrieve 5 mutations on the heavy chain CDR region of the antibody that are known to enhance the neutralization effectiveness within a pool of all exhaustive 494 single-point mutations on the interface.\nEngineering staphylokinase for enhanced thrombolytic activity. We assess the potential of the methods to enhance the thrombolytic activity of the staphylokinase protein (SAK). Staphylokinase, known for its cost-effectiveness and safety as a thrombolytic agent, faces a significant limitation in its widespread clinical application due to its low affinity to plasmin (Nikitin et al., 2022). In this\n2Figure 2 in the original SKEMPI v2.0 paper illustrates such PPIs by connected components consisting of two nodes (proteins), or more than two nodes but with two unique ones, where uniqueness is defined by the \u201cShare common binding site\u201d edges.\nstudy, we leverage 80 thrombolytic activity labels associated with SAK mutations located at the binding interface with plasmin (Laroche et al., 2000).\nSpecifically, we evaluate the \u2206\u2206G predictions on the 1BUI structure from PDB, which contains the trimer consisting of plasmin-activated staphylokinase bound to another plasmin substrate. Unlike in the case of the SARS-CoV-2 benchmark, all 80 binary labels for SAK-plasmin have experimentally measured effects, among which 28 mutations are favorable. Additionally, 6 of them introduce at least two-fold thrombolytic activity improvement, constituting even more practically-significant targets for retrieval. Besides that, 24 out of 80 mutations on SAK are multi-point, which provides a more general setup for the evaluation than the SARS-CoV-2 benchmark. Note that while the quantity measured for SAK is the activity, what we are estimating is the \u2206\u2206G for the SAK-plasmin interaction. Since the affinity of the complex is the main activity bottleneck, these two quantities were shown to be highly correlated (Nikitin et al., 2022).\nB.3 EVALUATION METRICS\nIn a practical binder design scenario (Nikitin et al., 2022; Shan et al., 2022), the primary objectives typically include (i) prioritizing mutations with lower \u2206\u2206G values, and more specifically, (ii) identifying stabilizing mutations (with negative \u2206\u2206G) from a pool of candidates. To address (i), we evaluate models using the Spearman correlation coefficient between the ground-truth and predicted \u2206\u2206G values. For (ii), we calculate precision and recall with respect to negative \u2206\u2206G signs. To enable comparisons with other methods, we also report metrics utilized in previous work: Pearson correlation coefficient, mean absolute error (MAE), root mean squared error (RMSE), and area under the receiver operating characteristic (ROC AUC). However, we stress that these metrics may be misleading when selecting a model for a practical application. Pearson correlation may not capture the non-linear scoring capabilities of a method and is sensitive to outlying predictions, insignificant in high ranks of the scoring. MAE and RMSE are not invariant to monotonic transformations of predictions, and ROC AUC may overemphasise the performance on destabilizing mutations. On the independent SARS-CoV-2 and SAK engineering case studies, we report scores for each of the favorable mutations following Luo et al. (2023). In addition, we evaluate more systematic precision at 1 (P@1) and precisions at k percent of the total pool of mutations (P@k%) for k \u2208 {5, 10}.\nB.4 BASELINES\nFlex ddG (Barlow et al., 2018). Flex ddG is a Rosetta (Leman et al., 2020) protocol which predicts \u2206\u2206G by estimating the change in binding free energies between wild-type and mutant using force field simulations. For each mutation, the \u2206\u2206G prediction is obtained by averaging the output from 5 runs of ddG-no backrub control, a number shown to be optimal by Barlow et al. (2018). Since, on average the prediction of a single \u2206\u2206G on the SKEMPI v2.0 test folds requires approximately 1 CPU hour (on Intel Xeon Gold 6130), we do not evaluate a complete ddG-backrub protocol. When using the ddG-backrub parameters suggested by Barlow et al. (2018), the running time further increases in the orders of magnitude, making the method impractical when compared to other baseline methods. For comparison, on the same data PPIFORMER requires 73 GPU milliseconds per mutation (using AMD MI250X GPU with the batch size of 1).\nMSA Transformer (Rao et al., 2021) MSA Transformer is an unsupervised sequence-based baseline, trained in a self-supervised way on diverse multiple sequence alignments (MSA). We use the pre-trained model provided by Rao et al. (2021) and build MSAs against UniRef30 database using HHblits algorithm (Remmert et al., 2012) with the same parameters as were used to train MSA Transformer.\nESM-IF (Hsu et al., 2022). ESM-IF, an inverse folding model, has been demonstrated by Hsu et al. (2022) to effectively generalize for predicting the stabilities of SKEMPI complexes upon singlepoint mutations. To predict \u2206G, the authors compute the average log-likelihood of amino acids within a mutated chain, conditioned on the complex structure. To predict \u2206\u2206G, we subtract the averaged likelihood of a mutant chain from that of the wildtype chain. To account for simultaneous mutations across multiple chains, we perform an ESM-IF forward pass for each chain in a complex and average their likelihoods to obtain the final result.\nRDE-Network (Luo et al., 2023). For the evaluation on SKEMPI-independent case studies, we use pre-trained model weights as provided by Luo et al. (2023). To validate the performance of RDE-Network on our 5 test folds of SKEMPI v2.0, we exclude all test examples from cross-validation and retrain the model with the same hyperparameters as used by Luo et al. (2023). Since, the exclusion of data points affects the sampling of training batches, and may, therefore, negatively affect the performance, we use 3 different random seeds and choose the final model according to the minimal validation loss."
        },
        {
            "heading": "C ABLATIONS",
            "text": "Self-supervised pre-training. This section evaluates the effects of the components proposed to enhance the generalization capabilities of PPIFORMER. First of all, Figure 4 (Top) illustrates that PPIFORMER achieves a notable performance without any supervised fine-tuning (per-PPI \u03c1Spearman = 0.21 and precision = 35% on 5,643 mutations from SKEMPI v2.0). Next, we discuss the importance\nof the key PPIFORMER components by analyzing the model performance under their change. First of all, the best pre-training is achieved when sampling protein\u2013protein interactions from our PPIRef50K dataset, both on scoring mutations and detecting the stabilizing ones (a3). We observe that training from redundant PPIRef300K decreases the performance by introducing biases into the model (a4). Finally, training from raw putative PPIs (a5) achieves the worst performance while training from DIPS or DIPS-Plus underperforms PPIRef (a1, a2).\nFurther, we discuss the effect of three key components of PPIFORMER: data representation, loss and masking strategy. First, the 80%10%10% masking proposed by Devlin et al. (2018) leads to better performance (b1), as well as masking only residues from the same chain (b2), which better corresponds to practical binder design scenarios. Next, as discussed in Section 4.1, we represent the orientations of residues with a single virtual beta-carbon vector per residue. This is in contrast with the more widely adopted utilization of three vectors per residue describing the complete frame of an amino acid by including the direction to a real C\u03b2 and the directions to alpha-carbons of neighboring residues (Jing et al., 2020) or, alternatively, two orthonormalized vectors in the directions of neighboring N and C atoms along the protein backbone (Watson et al., 2023; Yim et al., 2023). We observe that a single virtual beta carbon leads to better generalization compared to extra full-frame representations (c1, c2). Finally, the regularization by label smoothing and amino acid class weighting has the strongest positive impact on the generalization capabilities of PPIFORMER (d1-d3).\nFine-tuning on \u2206\u2206G labels. We further ablate the importance of our self-supervised pre-training for the \u2206\u2206G fine-tuning. Figure 6 illustrates the crucial significance of the latter. The pre-trained PPIFORMER surpasses the randomly initialized model with a large margin of 0.08 absolute difference on per-PPI Spearman correlation. Note that the pre-trained zero-shot predictor (red curve at step 0) achieves performance competitive to the full-trained model without pre-training (green curve, step 400).\nD ADDITIONAL RESULTS\nIn this section, we provide a more detailed comparison of our method with the state-of-the-art approaches on the SKEMPI v2.0 test set. Table 6 demonstrates that PPIFORMER achieves the best or the secondbest scores on all five protein\u2013protein interactions, outperforming the state-of-the-art methods."
        }
    ],
    "year": 2023
}