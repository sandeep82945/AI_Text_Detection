{
    "abstractText": "Goal representation affects the performance of Hierarchical Reinforcement Learning (HRL) algorithms by decomposing the complex learning problem into easier subtasks. Recent studies show that representations that preserve temporally abstract environment dynamics are successful in solving difficult problems and provide theoretical guarantees for optimality. These methods however cannot scale to tasks where environment dynamics increase in complexity i.e. the temporally abstract transition relations depend on larger number of variables. On the other hand, other efforts have tried to use spatial abstraction to mitigate the previous issues. Their limitations include scalability to high dimensional environments and dependency on prior knowledge. In this paper, we propose a novel three-layer HRL algorithm that introduces, at different levels of the hierarchy, both a spatial and a temporal goal abstraction. We provide a theoretical study of the regret bounds of the learned policies. We evaluate the approach on complex continuous control tasks, demonstrating the effectiveness of spatial and temporal abstractions learned by this approach.",
    "authors": [],
    "id": "SP:d283596615f2b910ee5701aafbffdcaefb8f2724",
    "references": [
        {
            "authors": [
                "David Abel",
                "Nate Umbanhowar",
                "Khimya Khetarpal",
                "Dilip Arumugam",
                "Doina Precup",
                "Michael Littman"
            ],
            "title": "Value preserving state-action abstractions",
            "venue": "Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Andrew G. Barto",
                "Sridhar Mahadevan"
            ],
            "title": "Recent advances in hierarchical reinforcement learning",
            "venue": "Discrete Event Dynamic Systems,",
            "year": 2003
        },
        {
            "authors": [
                "Patrick Cousot",
                "Radhia Cousot"
            ],
            "title": "Abstract interpretation frameworks",
            "venue": "J. Log. Comput.,",
            "year": 1992
        },
        {
            "authors": [
                "Peter Dayan",
                "Geoffrey E Hinton"
            ],
            "title": "Feudal reinforcement learning",
            "venue": "In NeurIPS,",
            "year": 1992
        },
        {
            "authors": [
                "Yan Duan",
                "Xi Chen",
                "Rein Houthooft",
                "John Schulman",
                "Pieter Abbeel"
            ],
            "title": "Benchmarking deep reinforcement learning for continuous control",
            "venue": "Proceedings of The 33rd International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Ben Eysenbach",
                "Ruslan Salakhutdinov",
                "Sergey Levine"
            ],
            "title": "Search on the replay buffer: Bridging planning and reinforcement learning",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "authors": [
                "Scott Fujimoto",
                "Herke van Hoof",
                "David Meger"
            ],
            "title": "Addressing function approximation error in actor-critic methods",
            "venue": "Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Marta Garnelo",
                "Murray Shanahan"
            ],
            "title": "Reconciling deep learning with symbolic artificial intelligence: representing objects and relations",
            "venue": "Current Opinion in Behavioral Sciences,",
            "year": 2019
        },
        {
            "authors": [
                "Timon Gehr",
                "Matthew Mirman",
                "Dana Drachsler-Cohen",
                "Petar Tsankov",
                "Swarat Chaudhuri",
                "Martin T. Vechev"
            ],
            "title": "AI2: safety and robustness certification of neural networks with abstract interpretation",
            "venue": "In IEEE Symposium on Security and Privacy,",
            "year": 2018
        },
        {
            "authors": [
                "Dibya Ghosh",
                "Abhishek Gupta",
                "Sergey Levine"
            ],
            "title": "Learning actionable representations with goal conditioned policies",
            "venue": "ICLR",
            "year": 2019
        },
        {
            "authors": [
                "Le\u00f3n Illanes",
                "Xi Yan",
                "Rodrigo Toro Icarte",
                "Sheila A. McIlraith"
            ],
            "title": "Symbolic plans as high-level instructions for reinforcement learning",
            "venue": "In AAAI,",
            "year": 2020
        },
        {
            "authors": [
                "Tejas D Kulkarni",
                "Karthik Narasimhan",
                "Ardavan Saeedi",
                "Josh Tenenbaum"
            ],
            "title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation",
            "venue": "In NeurIPS,",
            "year": 2016
        },
        {
            "authors": [
                "Siyuan Li",
                "Lulu Zheng",
                "Jianhao Wang",
                "Chongjie Zhang"
            ],
            "title": "Learning subgoal representations with slow dynamics",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Daoming Lyu",
                "Fangkai Yang",
                "Bo Liu",
                "Steven Gustafson"
            ],
            "title": "SDRL: interpretable and data-efficient deep reinforcement learning leveraging symbolic planning",
            "venue": "In AAAI,",
            "year": 2019
        },
        {
            "authors": [
                "Ofir Nachum",
                "Shixiang Gu",
                "Honglak Lee",
                "Sergey Levine"
            ],
            "title": "Data-efficient hierarchical reinforcement learning",
            "venue": "NeurIPS",
            "year": 2018
        },
        {
            "authors": [
                "Ofir Nachum",
                "Shixiang Gu",
                "Honglak Lee",
                "Sergey Levine"
            ],
            "title": "Near-optimal representation learning for hierarchical reinforcement learning",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "X. Rival",
                "K. Yi"
            ],
            "title": "Introduction to Static Analysis: An Abstract Interpretation Perspective",
            "year": 2020
        },
        {
            "authors": [
                "Nikolay Savinov",
                "Anton Raichuk",
                "Damien Vincent",
                "Rapha\u00ebl Marinier",
                "Marc Pollefeys",
                "Timothy P. Lillicrap",
                "Sylvain Gelly"
            ],
            "title": "Episodic curiosity through reachability",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Richard S. Sutton",
                "Andrew G. Barto"
            ],
            "title": "Reinforcement Learning: an Introduction",
            "year": 1998
        },
        {
            "authors": [
                "Richard S. Sutton",
                "Doina Precup",
                "Satinder Singh"
            ],
            "title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning",
            "venue": "Artificial Intelligence,",
            "year": 1999
        },
        {
            "authors": [
                "Emanuel Todorov",
                "Tom Erez",
                "Yuval Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,",
            "year": 2012
        },
        {
            "authors": [
                "Alexander Sasha Vezhnevets",
                "Simon Osindero",
                "Tom Schaul",
                "Nicolas Heess",
                "Max Jaderberg",
                "David Silver",
                "Koray Kavukcuoglu"
            ],
            "title": "Feudal networks for hierarchical reinforcement learning",
            "year": 2017
        },
        {
            "authors": [
                "Shiqi Wang",
                "Kexin Pei",
                "Justin Whitehouse",
                "Junfeng Yang",
                "Suman Jana"
            ],
            "title": "Formal security analysis of neural networks using symbolic intervals",
            "venue": "In USENIX Security Symposium,",
            "year": 2018
        },
        {
            "authors": [
                "Christopher J.C.H. Watkins",
                "Peter Dayan"
            ],
            "title": "Technical note: q -learning",
            "venue": "Mach. Learn.,",
            "year": 1992
        },
        {
            "authors": [
                "Mehdi Zadem",
                "Sergio Mover",
                "Sao Mai Nguyen"
            ],
            "title": "Goal space abstraction in hierarchical reinforcement learning via set-based reachability analysis, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Tianren Zhang",
                "Shangqi Guo",
                "Tian Tan",
                "Xiaolin Hu",
                "Feng Chen"
            ],
            "title": "Adjacency constraint for efficient hierarchical reinforcement learning",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Zhang"
            ],
            "title": "The implementation of our algorithm is adapted from the work",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Goal-conditioned Hierarchical Reinforcement Learning (HRL) (Dayan & Hinton, 1992) tackles task complexity by introducing a temporal abstraction over learned behaviours, effectively decomposing a large and difficult task into several simpler subtasks. Recent works (Vezhnevets et al., 2017; Kulkarni et al., 2016; Nachum et al., 2019; Zhang et al., 2023; Li et al., 2021) have shown that learning an abstract goal representations is key to proposing semantically meaningful subgoals and to solving more complex tasks. In particular, representations that capture environment dynamics over an abstract temporal scale have been shown to provide interesting properties with regards to bounding the suboptimality of learned policies under abstract goal spaces (Nachum et al., 2019; Abel et al., 2020), as well as efficiently handling continuous control problems.\nHowever, temporal abstractions that capture aspects of the environment dynamics (Ghosh et al., 2019; Savinov et al., 2019; Eysenbach et al., 2019; Zhang et al., 2023; Li et al., 2021) still cannot scale to environments where the pairwise state reachability relation is complex. For instance, Zhang et al. (2023) compute a k-reachability relation for a subset of the environment\u2019s states defined with an oracle (e.g., the oracle selects only the x, y dimensions). While sampling reachable goals is useful to drive efficiency, the learned k-adjacency relation is difficult to learn for higher dimensions. This situation typically happens when temporally abstract relations take into account more variables in the state space. The main limitation of these approaches is the lack of a spatial abstraction to generalise such relations over states.\nAlternatively, other works (Kulkarni et al., 2016; Illanes et al., 2020; Garnelo & Shanahan, 2019; Zadem et al., 2023) have studied various forms of spatial abstractions for goal spaces. These abstractions effectively group states with similar roles in sets to construct a discrete goal space. The advantage of such representation is a smaller size exploration space that expresses large and longhorizon tasks. In contrast to the algorithms that require varying levels of prior knowledge (Kulkarni et al., 2016; Lyu et al., 2019; Illanes et al., 2020), GARA (Zadem et al., 2023) gradually learns such spatial abstractions by considering reachability relations between sets of states. We refer to this abstraction as reachability-aware abstraction. While such representation is efficient for lowdimensional tasks, scalability remains an issue due to the lack of a temporal abstraction mechanism.\nWhat is challenging about scaling GARA\u2019s approach to more complex environments is exactly what makes the set-based representation effective: the low-level agent must learn how to reach a set of states that, especially in the initial phases of the algorithm when the abstraction is coarser, may be \u201cfar\u201d away. We tackle this problem introducing a new agent in the hierarchy that introduces a temporal abstraction. Such an agent learns to select intermediate subgoals that: can be reached from a state s executing the low-level agent; and helps constructing a trajectory from s to a goal abstract state.\nIn this paper, we propose a three-layer HRL algorithm that achieves both a temporal and spatial abstraction for capturing the environment dynamics. We motivate the use of temporal abstraction as the key factor that can scale the abstraction proposed in Zadem et al. (2023), and the reachabilityaware spatial abstraction as a way to efficiently represent goals in complex tasks. We complement the approach by adding theoretical guarantees on the bounds of suboptimality of policies learned under this abstraction. Our approach is empirically evaluated on a set of challenging continuous control tasks. Our work presents the following contributions:\n(1) A novel Feudal HRL algorithm, STAR, to learn online subgoal representations and policies. STAR consists of 3 agents: the high-level agent selects regions in the abstract reachabilityaware goal space, the middle-level agent selects concrete subgoals that help reaching abstract goals, and the low-level agent learns to take actions in the environment (Section 3).\n(2) Provide a theoretical motivation for using reachability-aware goal representations, showing a suboptimality bound on the learned policy and that our algorithm progressively improves the reachability-aware abstraction. (Section 4)\n(3) Empirically show that STAR successfully combines both temporal and spatial abstraction for more efficient learning, and that the reachability-aware abstraction scales to tasks with more complex dynamics. (Section 5)."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Building on ideas for introducing hierarchy in Reinforcement Learning (Sutton et al., 1999; Barto & Mahadevan, 2003; Dayan & Hinton, 1992), recent advances have managed to considerably elevate HRL algorithms to tackle complex continuous control environments. Nachum et al. (2018) introduces a two-level hierarchy that sample goals from a pre-defined oracle on the state space. This approach provides a good basis for HRL algorithms as it is generic and addresses non-stationary learning but may still be suboptimal as the goal sampling is unconstrained in the oracle.\nTo remedy this, Ghosh et al. (2019); Savinov et al. (2019); Eysenbach et al. (2019); Zhang et al. (2023); Li et al. (2021) learn different goal representations that try to capture the environment dynamics. This idea has been validated under different theoretical formulations (Nachum et al., 2019; Abel et al., 2020; Li et al., 2021). In particular, Li et al. (2021) learns a latent representation based on slow-dynamics in the state space. The idea is that meaningful temporally abstract relations (over k steps) are expressed by state features that slowly change over time. However, these features may not be always sufficient to capture all the critical information about dynamics. Both Savinov et al. (2019) and Zhang et al. (2023) use k-step reachability relations as a characterisation for environment dynamics. Their idea is to learn if goals (mappings of states in an embedding / oracle) reach a potential goal in k steps. These relations are later used to drive the sampling of goals that can be reached, resulting in more efficient learning. However, such learned pairwise relations are binary and lack the information of which goals can be reached by applying a specific policy. Additionally, without any spatial abstraction, it can be difficult to learn these relations for a complex transition relation (e.g. a relation that require monitoring more that few variables).\nTo introduce spatial abstraction, we study the work by Zadem et al. (2023) in which the authors introduce GARA, a spatial abstraction for the goal space that captures richer information from k step reachability relations. This algorithm progressively learns a discretisation of the state space that serves as an abstract goal space. This abstraction generalizes reachability relations over sets of states, greatly reducing the difficulty of the learning problem. This approach however was only validated on a 4-dimensional environment and suffers from scalability issues as the abstraction is learned concurrently with the hierarchical policies. As GARA starts learning from a coarse abstraction (composed of a small number of large sets), the goal set is often distant from the current state,\nthus it can be difficult to learn meaningful policies that manage to reach a desired goal set. Under such circumstances, the approach is blocked targeting a hard to reach goal and cannot improve as it lacks any mechanism to propose easier, more granular subgoals. To alleviate this discrepancy, we introduce a new agent in the hierarchy of GARA that applies a temporal abstraction (Sutton et al., 1999). Our intuition is to synthesise between the discrete goal chosen from the top-down process and the temporal transitions allowed by the current low-level policy from the bottom-up process, through a mid-level agent that acts as an intelligent tutoring system and learns to select intermediate goals g \u2208 S that: (a) can be reached from a state s executing the current low-level agent; and (b) helps constructing a trajectory from s to a goal abstract state G \u2208 G."
        },
        {
            "heading": "3 SPATIO-TEMPORAL ABSTRACTION VIA REACHABILITY",
            "text": "We consider a goal-conditioned Markov Decision Process (S,A, P, rext) as environment, where S \u2286 Rn is a continuous state space, A is an action space, P (st+1|st, at) is a probabilistic transition function, and rext : S \u00d7S \u2192 R is a parameterised reward function, defined as the negative distance to the task goal g\u2217 \u2208 S , i.e rext(s, g\u2217) = \u2212\u2225g\u2217 \u2212 s\u22252. The multi-task reinforcement learning problem consists in learning a goal conditioned policy \u03c0 to sample at each time step t an action a \u223c \u03c0(st | g\u2217), so as to maximizes the expected future reward. The spatial goal abstraction is modeled by a set-based abstraction defined by a function N : S \u2192 2S that maps concrete states to sets of states (i.e., \u2200s \u2208 S,N (s) \u2286 S). We write GN to refer to the range of the abstraction N , which is intuitively the abstract goal space. We further drop the subscript (i.e., write G) when N is clear from the context and denote elements of G with the upper case letter G. We propose a HRL algorithm, Spatio-Temporal Abstraction via Reachability (STAR), that learns, at the same time, a spatial goal abstraction N and policies at multiple time scales. The STAR algorithm, shown in Figure 1, has two main components: a 3-levels Feudal HRL algorithm (enclosed in the red dashed lines); and an abstraction refinement component (shown in the blue solid lines). STAR runs the Feudal HRL algorithm and the abstraction refinement in a feedback loop, refining the abstraction N at the end of every learning episode. Observe that the high-level agent (called Navigator) samples a goal G from the abstract goal space G, and that such a goal can be difficult to reach from the current state. The first intuition of the algorithm is that the middle-level agent (Manager) achieves a temporal-abstraction of the Controller actions needed to reach a, possibly very far, goal G by sampling an intermediate subgoal g \u2208 S of a difficulty level adapted to the current non-optimal policy. The second intuition is that the algorithm learns the abstraction iteratively. Every refinement obtains a finer abstraction N \u2032 from N . Intuitively, N \u2032 will split at least a goal G1 \u2208 GN in two goals G\u20321, G \u2032\u2032\n1 \u2208 GN \u2032 if there are different states in G1 (i.e., sa, sb \u2208 G1) that cannot reach the same target G2 \u2208 G when applying the same low-level policy. While we will define such reachability property precisely later, intuitively the refinement separates goal states that \u201dbehave differently\u201d under the same low level policy (i.e., N \u2032 would represent more faithfully the environment dynamic).\nWe first explain how STAR uses the different hierarchy\u2019s levels to address the challenges of learning policies when the abstraction is coarse (Section 3.1), and then formalize a refinement that obtains a reachability aware abstraction (Section 3.2)."
        },
        {
            "heading": "3.1 A 3-LEVEL HIERARCHICAL ALGORITHM FOR TEMPORAL ABSTRACTION",
            "text": "The feudal architecture of STAR is composed of a hierarchy with three agents:\n1. Navigator: the highest-level agent samples an abstract goal G \u2208 G that should help to reach the task goal g\u2217 from the current agent\u2019s state (Gt+k \u223c \u03c0Nav(st, g\u2217)).\n2. Manager: the mid-level agent is conditioned by the navigator goal G and picks subgoals in the state space (gt+l \u223c \u03c0Man(st, Gt+k)).\n3. Controller: the low-level policy is goal-conditioned by the Manager\u2019s subgoal g and samples actions to reach given goal (a \u223c \u03c0Cont(st, gt+l)). 1\nThe agents work at different time scales: the Navigator selects a goal Gt+k every k steps, the Manager selects a goal gt+l every l steps (with k multiple of l), and the Controller selects an action to execute at every step. Intuitively, the Navigator\u2019s role in this architecture is to select an abstract goal Gt+k (i.e. a set of states that are similar and of interest to solve the task) from the current state st. However, the initial abstraction G of the state space is very coarse (i.e., the abstract regions contains several states and still do not represent the agent\u2019s dynamic). This means that learning a flat policy that reaches Gt+k from st is very challenging. The Manager samples subgoals gt+l from S that are intermediate, easier targets to learn to reach for the Controller, instead of a possibly far away state that might prove too difficult to reach. Intuitively, the Manager implements a temporal abstraction mechanism. The structure of STAR guides the agent through the large stategoal abstraction, while it also allows the agent to learn manageable low-level policies.\nWe set the reward at each level of the hierarchy following the above intuitions. The Navigator receives the external environment reward after k steps for learning to reach the task goal g\u2217:\nrNavigator(st) := max x\u2208N (st)\nrext(x, g \u2217),\nThe max in this reward is computed over observed exploration data. The Manager receives a reward expressing the distance between the Controller\u2019s state st and the target abstract state Gt+k:\nrManager(st, Gt+k) := \u2212\u2225st \u2212 Center(Gt+k)\u22252, where Center(Gt+k) is the center of the goal Gt+k. Specifically, since the goal Gt+k is a set of states, the Manager\u2019s reward uses the Euclidean distance between st and the center of the set. Such reward allow the Manager to learn how to sample subgoals that help the agent to reach Gt+k. Finally, the Controller is rewarded with respect to the Euclidean distance between subgoals sampled by the Manager and the reached state:\nrController(st, gt+l) := \u2212\u2225gt+l \u2212 st\u22252. This reward allows the Controller to learn how to reach intermediate subgoal states gt+l."
        },
        {
            "heading": "3.2 REFINING N VIA REACHABILITY ANALYSIS",
            "text": "While we follow the high level description of the refinement procedure of the GARA (Zadem et al., 2023) algorithm, we adapt it to our notation and to the new theoretical results on the refinement we present later (which holds for both GARA and STAR). Furthermore, in the rest of this Section and in Section 4, we will assume a 2-level hierarchy, where \u03c0Nav is the high-level policy (e.g., \u03c0high), and \u03c0low is the hierarchical policy obtained composing \u03c0Man and \u03c0Cont.\nWe first define the k-step reachability relation for a goal-conditioned policy \u03c0NLow :\nRk\u03c0NLow (Gi, Gj) := s\u2032 \u2208 S | s \u2208 Gi, s kGGGGGGGGGGGA\u03c0NLow(., Gj) s\u2032  ,\n1In the following, we use the upper-case G letter for goals in G and the lower-case g for subgoals in S.\nwhere s k GGGGGGGGGGGA\n\u03c0NLow(., Gj) s\u2032 means that s can reach s\u2032 by executing the policy \u03c0NLow(.,Gj) (targeting Gj)\nin k steps. In other words, Rk\u03c0NLow (Gi, Gj) is the set of states reached when starting in any state in Gi and applying the policy \u03c0NLow(., Gj) for k steps.\nThe algorithm uses the notion of reachability property among a pair of abstract goals:\nDefinition 1 (Pairwise Reachability Property) Let N : S \u2192 2S be a set-based abstraction and Gi, Gj \u2208 GN . N satisfies the pairwise reachability property for (Gi, Gj) if Rk\u03c0\u2217NLow (Gi, Gj) \u2286 Gj .\nThe algorithm decides to refine the abstract representation after an episode of the HRL algorithm. Let E := {G0, . . . , Gn} be the list of goals visited in the last episode. The refinement algorithm analyzes all the pairs (Gi, Gi+1) \u2208 E , for 0 \u2264 i < n, and refines N in a new abstraction N \u2032 \u201dsplitting\u201dGi if it does not satisfy the pairwise reachability property. Each refinement obtains a new, finer abstraction N \u2032 where the reachability property is respected in one more goal. We formalize when an abstraction N \u2032 refines an abstraction N with respect to the reachability property as follows:\nDefinition 2 (Pairwise Reachability-Aware Refinement) Let N : S \u2192 2S and N \u2032 : S \u2192 2S be two set-based abstractions such that there exists Gi \u2208 GN , GN \u2032 = (GN \\ {Gi}) \u222a {G\u20321, G\u20322}, G\u20321 \u222a G\u20322 = Gi, and G\u20321 \u2229 G\u20322 = \u2205. N \u2032 refines N (written as N \u2032 \u227a N ) if, for some Gj \u2208 GN , N \u2032 satisfies the pairwise reachability property for (G\u20321, Gj), while N does not satisfy the pairwise reachability property for (Gi, Gj)."
        },
        {
            "heading": "3.2.1 REFINEMENT COMPUTATION",
            "text": "We implement the refinement similarly to GARA. We represent an abstraction N directly with the set of abstract states GN := {G1, . . . , Gn}, a partition of the state space S (i.e., all the sets in GN are disjoint and their union is S). We represent each G \u2208 GN as a multi-dimensional interval (i.e., a hyper-rectangle). We (i) train a neural network Fk predicting the k-step reachability from each partition in GN ; and (ii) for eachGi, Gi+1 \u2208 E , we check pairwise reachability from FK ; and (iii) if pairwise reachability does not hold, we compute a refinement N \u2032 of N . We approximate the reachability relation with a forward model, a fully connected feed forward neural network Fk : S \u00d7 GN \u2192 S. Fk(st, Gj), is trained from the replay buffer and predicts the state st+k the agent would reach in k steps starting from a state st when applying the low level policy \u03c0NLow(s,Gj) conditioned on the goal Gj . We avoid the non-stationarity issue of the lowerlevel policy \u03c0NLow by computing the refinement only when \u03c0NLow is stable. See Appendix C for more details. We use the forward model Fk to evaluate the policy\u2019s stability in the regions visited in E by computing the Mean Squared Error of the forward model predictions on a batch of data from the replay buffer, and consider the model stable only if the error remains smaller than a parameter \u03c3 over a window of time.\nChecking the reachability property for (Gi, Gj) amounts to computing the output of Fk(s,Gj), for all states s \u2208 Gi, i.e., Rk\u03c0NLow (Gi, Gj) := {Fk(s,Gj) | s \u2208 Gi}, and checking if R k \u03c0NLow\n(Gi, Gj) \u2286 Gj . Technically, we compute an over-approximation R\u0303k\u03c0NLow (Gi, Gj) \u2287 R k \u03c0NLow\n(Gi, Gj) with the Ai2 (Gehr et al., 2018) tool. Ai2 uses abstract interpretation (Cousot & Cousot, 1992; Rival & Yi, 2020) to compute such an over-approximation: the tool starts with a set-based representation (e.g., intervals, zonotopes, . . . ) of the neural network input, the set Gi, and then computes the set of outputs layer-by-layer. In Fk, each layer applies first a linear transformation (i.e. for the weights and biases) and then a piece-wise ReLU activation function. For each layer, Ai2 computes first an over-approximation of the linear transformation (i.e., it applies the linear transformation to the input set), and then uses this result to compute an over-approximation of the ReLU activation function, producing the new input set to use in the next layer. Such computations work on abstract domains: for example, one can over-approximate a linear transformation applied to an interval by computing the transformation, and then a convex-hull to obtain the output in the form of an interval. The over-approximations for ReLU activation functions are described in detail in Gehr et al. (2018).\nWe split the set Gi when it does not satisfy the reachability property w.r.t. Gi+1. Since Gi is an interval, we implement the algorithm from (Zadem et al., 2023; Wang et al., 2018) that: (i) stops if\nGi satisfies the reachability property or none of the states in Gi reach Gj ; and otherwise (ii) splits Gi in two intervals, G\u2032i and G \u2032\u2032 i , calling the algorithm recursively on both intervals. This results in two sets of intervals, subsets ofGi, that either satisfy the reachability property or not. Such intervals are the new intervals replacing Gi in the new abstraction (see Appendix F)."
        },
        {
            "heading": "4 THEORETICAL PROPERTIES OF THE REFINEMENT",
            "text": "In this section, we motivate the adoption of the goal-space abstraction and the reachabilityaware refinement showing that: (i) there exists a bound on the sub-optimality of policies trained with a reachability-aware abstraction; and (ii) the reachability-aware refinement gradually finds a reachability-aware abstraction. Our results apply to both STAR and GARA (Zadem et al., 2023), and all the proofs are available in the Appendix A.\nThe theoretical results hold under the assumptions that the environment M is deterministic and the reward signal rext is bounded in the environment. Consequently, we assume that the distance separating a state s \u2208 S from all the states s\u2032 \u2208 S that s can reach in one step is bounded. Thus, there is an upper bound Rmax := max\ns,s\u2032\u2208S,\u2211a\u2208A P (s\u2032|s,a)\u22650\u2225s\u2212 s\u2032\u22252 on the reward signal. Let \u03c0\u2217 be the optimal hierarchical policy composed by a high-level policy g \u223c \u03c0\u2217high(s, g\u2217) that samples g \u2208 S, and a low-level policy a \u223c \u03c0\u2217low(s, g) that samples actions a \u2208 A. Since the environment is deterministic, there exists an optimal high-level trajectory containing the goals sampled with \u03c0\u2217high and an optimal low-level trajectory containing all the visited states:\nT \u2217high := {g0, g1, . . . , gm}, T \u2217Low := {s0, s1, . . . , sm\u00b7k}, with si\u00b7k = gi, for 0 \u2264 i \u2264 m. Let N : S \u2192 2S be a set-based abstraction. We write \u03c0\u2217N for the optimal hierarchical policy obtained with the abstraction N . We write T \u2217NHigh and T \u2217NLow for the optimal high- and low-level trajectories respectively. Below, we provide an upper bound on the difference between the optimal hierarchical policy \u03c0\u2217 and the optimal hierarchical policy \u03c0\u2217N when N is a reachability-aware.\nDefinition 3 (Reachability-Aware Abstraction) Let N : S \u2192 2S be a set-based abstraction, \u03c0\u2217N the corresponding optimal hierarchical policy, and T \u2217high the optimal high-level trajectory from \u03c0\u2217high. N is a reachability-aware abstraction with respect to T \u2217high if:\n1. States are contained in their abstraction: \u2200s \u2208 S, s \u2208 N (s). 2. The abstractions of the goals in the optimal trajectory are disjoint:\n\u2200gi, gj \u2208 T \u2217high, (gi \u0338= gj \u2192 N (gi) \u2229N (gj) = \u2205).\n3. The abstractions of each consecutive goals in the optimal trajectory satisfy the pairwise reachability property:\n\u2200gi, gi+1 \u2208 T \u2217high, Rk\u03c0\u2217NLow (N (gi),N (gi+1)) \u2286 N (gi+1).\n4. The reward in the final abstract goal N (gm) is bounded: \u2203\u03f5 > 0,\u2200s \u2208 N (gm).|rext(gm)\u2212 rext(s)| \u2264 \u03f5.\nTheorem 1 (Sub-optimal Learning) Let M be a deterministic environment with task goal g\u2217 \u2208 S and rext(s) = \u2212\u2225g\u2217 \u2212 s\u22252. Let N : S \u2192 2S be a reachability-aware abstraction with respect to T \u2217high. Then, for s0 \u2208 T \u2217Low and s\u20320 \u2208 T \u2217NLow we have that:\n|V\u03c0\u2217(s0)\u2212 V\u03c0\u2217N (s \u2032 0)| \u2264  mk2\u2211 i=0 \u03b3ii+ mk\u2211 i=mk2 \u03b3i(mk \u2212 i)  \u00b7 2Rmax + 1\u2212 \u03b3mk+1 1\u2212 \u03b3 \u03f5, (1)\nwhere V\u03c0(s) is the value function for a policy \u03c0 (Sutton & Barto, 1998). Moreover, if there exists a B \u2265 \u03f5 > 0 such that for all 1 \u2264 i \u2264 m, max\nx,y\u2208N (gi) \u2225x \u2212 y\u2225 \u2264 B, then\n\u2200si \u2208 T \u2217Low and \u2200s\u2032i \u2208 T \u2217NLow we have that:\n|V\u03c0\u2217Low(s0)\u2212 V\u03c0\u2217NLow (s \u2032 0)| \u2264 1\u2212 \u03b3mk+1 1\u2212 \u03b3 (kRmax +B). (2)\nEquation (1) in the above theorem provides a bound on the sub-optimality of the hierarchical policy when trained under a set-based reachability-aware abstraction N . Intuitively, the worst trajectory T \u2217NLow , starting from s\u20320 \u2208 N (s0), can progressively deviate from T \u2217Low as i increases. When i \u2265 mk2 , the trajectories progressively converge around N (gm). Equation (2) defines a tighter upper bound when there is a boundB on the maximum distance between two states in each abstract goal in T \u2217NHigh . In practice, the existence of the bound B is valid when the state space is bounded. In this case, the deviation of the two trajectories is independent from i and is stable across time.\nLemma 1 Let N and N \u2032 be two set-based abstractions such that N \u2032 \u227a N and N satisfies the Conditions (1), and (4) of a reachability-aware abstraction (Definition 3). Also, let Gi \u2208 T \u2217NHigh (note that Gi \u2208 GN ), and Gi be the goal refined in N \u2032. Then, the abstraction N \u2032 satisfies the following:\n1. \u2203 gi \u2208 T \u2217high such that N does not satisfy the reachability property for (N (gi),N (gi+1)), while N \u2032 does for (N \u2032(gi),N (gi+1)).\n2. If there exists gj \u2208 T \u2217high such that gj \u2208 N (gi), then gj \u0338\u2208 N \u2032(gi)).\nTheorem 2 Given an initial set-based abstraction N and assuming N (gm) satisfies the Conditions (1), and (4) of Definition 3, we compute a reachability-aware abstraction after applying a finite number of reachability-aware refinements.\nTheorem 2 follows from Lemma 1 and shows that applying the reachability-aware refinement for a finite number of times we compute a reachability-aware abstraction. In practice, the assumption that N (gm) verifies criteria 1 and 4 of Def.3 is reasonable since the goal g\u2217 is known in the environment. That is to say, N (gm) could correspond to a region whose center is g\u2217 and radius is Rmax."
        },
        {
            "heading": "5 EXPERIMENTAL EVALUATION",
            "text": "In this section we answer the following research questions: 1) Do the spatial and temporal abstraction of STAR allow for more data-efficient learning? 2) Does the reachability-aware abstraction scale to more complex environments compared to a more concrete reachability relation? 3) How does the learned abstraction decompose the environment to allow for learning successful policies?"
        },
        {
            "heading": "5.1 ENVIRONMENT SETUP",
            "text": "We evaluate our approach on a set of challenging tasks in the Ant environments (Fig.2) adapted from Duan et al. (2016) and popularised by Nachum et al. (2018). The Ant is a simulated quadrupedal robot whose body is composed of a torso and four legs attached by joints. Furthermore, each leg is split into two parts by a joint in the middle to allow bending. The observable space of the Ant is composed of the positions, orientations and velocities of the torso as well as the angles and angular velocities in each joint. Overall the state space is comprised of 30 dimensions. The actions correspond to applying forces on actuators in the joint. This actions space is continuous and 8- dimensional. We propose the following tasks:\n1. Ant Maze: in this task, the ant must navigate a \u2019\u2283\u2019-shaped maze to reach the exit positioned at the top left.\n2. Ant Fall: the environment is composed of two raised platforms seperated by a chasm. The ant starts on one of the platforms and must safely cross to the exit without falling. A movable block can be push into the chasm to serve as a bridge. Besides the precise maneuvers required by the ant, falling into the chasm is a very likely yet irreversible mistake.\n3. Ant Maze Cam: this is a more challenging version of Ant Maze. The upper half of the maze is fully blocked by an additional obstacle that can only be opened when the ant looks at the camera (in yellow in Fig. 2c) when on the red spot. The exit remains unchanged.\nWe note that these tasks are hierarchical in nature as reaching the goal requires correctly controlling the ant to be able to move (low-level) and then navigating to the exit (high-level). Moreover, in both Ant Maze Cam and Ant Fall, no reward is attributed to the intermediate behaviour that unlocks the path to the exit (looking at the camera, pushing the block). Under such circumstances, the underlying dynamics are more complex making the reachability relations more difficult to extract."
        },
        {
            "heading": "5.2 COMPARATIVE ANALYSIS",
            "text": "We compare STAR with the following algorithms:\n1. GARA (Zadem et al., 2023): this algorithm learns a spatial abstraction via reachability analysis using a two-level hierarchical policy.\n2. HIRO (Nachum et al., 2018): this algorithm relies on a Manager to sample goals directly from the state space S and learns how to achieve them using the controller.\n3. HRAC (Zhang et al., 2023): adopting the same architecture as HIRO, this approach tries to approximate a reachability relation between goals in an abstract space and use it to sample rewarding reachable goals. The reachability relation is derived from measuring the shortest transition distance between states in the environment.\n4. LESSON (Li et al., 2021): a HRL algorithm that learns a latent goal representations based on slow dynamics in the environment. The latent space learns from features that are slow to change over k steps, in order to capture a temporal abstraction.\nIn line with HIRO and HRAC, STAR relies on an oracle \u03c8(s) that transforms the observations of the high-level agents (Manager and Navigator). In practice \u03c8() corresponds to a feature selection applied to states. In contrast, LESSON learns a latent goal space without an oracle. In Ant Maze \u03c8(s) = (x, y), in Ant Fall \u03c8(s) = (x, y, z), and in Ant Maze Cam, \u03c8(s) = (x, y, \u03b8x, \u03b8y, \u03b8z).\nFig.3 shows that STAR outperforms all of the state-of-art approaches by reaching a higher success rate with less timesteps. In particular GARA, operating only under a spatial abstraction mechanism is unable to solve Ant Maze, the easiest task in this analysis. HIRO on the other hand learns less efficient policies due to it lacking a spatial abstraction component. These results show that STAR, which combines temporal and spatial abstractions, is a more efficient approach.\nTo discuss the second research question, we first observe that, while the high-level dynamics of Ant Maze can be captured by the x, y dimensions, the dynamics of Ant Fall require all the x, y, z dimensions (z expresses if the ant is safely crossing above the pit or if it has fallen), and Ant Maze Cam requires x, y, \u03b8x, \u03b8y, and \u03b8z (the orientation angles are necessary to unlock the access to the upper part of the maze). Fig.3 shows that HRAC is unable to capture meaningful relations between subgoals and fails at solving either Ant Fall or Ant Maze Cam due to the increased complexity in capturing the high-level task dynamic. Similarly, LESSON is unable to learn a good subgoal representation using slow dynamics in Ant Maze Cam. In fact, \u03b8x,\u03b8y , and \u03b8z are features that do not respect the LESSONS\u2019 slowness objective (i.e., they can vary rapidly across k steps). As a results, the goal abstraction in LESSON may overlook them, losing critical information in the process. Instead, STAR is capable of abstracting these dimensions and converging to a successful policy. We remark that STAR, similarly to HRAC, can be seen as extensions of HIRO with the addition of a reachability-based component that improves goal representation. However, the results shown in Fig. 3 highlight how the addition of the reachability information in HRAC is even detrimental for the performance when the number of features in the oracle increases (e.g., on Ant Fall and Ant Maze Cam). Instead, the STAR\u2019s reachability-aware spatial abstraction and intermediate temporal abstraction allow the algorithm to scale to more complex tasks."
        },
        {
            "heading": "5.3 REPRESENTATION ANALYSIS",
            "text": "We answer the third research question examining the progress of the STAR\u2019s Navigator agent at different timesteps during learning when solving the Ant Maze. From Fig.4 we can see that, progressively, the ant explores trajectories leading to the goal of the task. Additionally, the frequency of visiting goals in the difficult areas of the maze (e.g., the tight corners) is higher, and these goals are eventually refined later in the training, jibing with the configuration of the obstabcles. Note that the Navigator\u2019s trajectory at 3M timesteps sticks close to the obstacles and pass through the maze\u2019s opening, resembling an optimal trajectory. This study provides some insight on how STAR gradually refines the goal abstraction to identify successful trajectories in the environment. In particular, STAR learns a more precise abstraction in bottleneck areas where only a few subset of states manage to reach the next goal. We provide the representation analysis on Ant Fall in Appendix B."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we propose a novel goal-conditioned HRL algorithm, STAR, that combines spatial and temporal abstractions. The spatial representation groups states with similar environment dynamics, and the temporal abstraction compensates for the non-optimality of the low-level policy, allowing online learning of both the policies and the representations. STAR\u2019s high-level agent learns a coarse, although precise, spatial approximation of the environment dynamics that, differently from other algorithms using reachability for goal representation, scales to more complex continuous control environments. STAR outperforms the existing algorithms that use either one of the abstractions and that struggle to learn meaningful representations. Here, we further provide a theoretical justification for reachability-aware abstractions, showing that we can use such representations to learn sub-optimal policies and learn them with a sequence of refinements. In the future, we plan to extend our approach to stochastic environments, which would require a different reachability property, and non-Markovian environments, which would require adding a history component to the spatial abstraction."
        },
        {
            "heading": "A PROOFS OF THEOREMS",
            "text": ""
        },
        {
            "heading": "A.1 PROOF OF THEOREM 1:",
            "text": "In this part of the proof, we will study the optimality of the hierarchical policies across the last k steps of the trajectory. First, we consider the optimal trajectory for a flat (non-hierarchical policy) in the environment described by M :\nT \u2217 := {s0, s1, . . . , sM}, where sM = g\u2217\nThe optimal hierarchical policy \u03c0\u2217 composed by \u03c0\u2217High and \u03c0 \u2217 Low samples a goal g \u2208 S every k steps (where k is a parameter). The following optimal trajectories are thus:\nT \u2217High := {g0, g1, . . . , gm}, where gm = g\u2217\nT \u2217Low := {s0, s1, . . . , smk}, where sn\u00b7k = gn, n \u2208 [[0, . . . ,m]] and smk = g\u2217 2\nThe optimal hierarchical policy \u03c0\u2217N under the abstraction N (defined by Def.3) yields the optimal trajectories:\nT \u2217NHigh := {N (g0),N (g1), . . . ,N (gm)} T \u2217NLow := {s\u20320, s\u20321, . . . , s\u2032mk}, where s\u2032n\u00b7k \u2208 N (gn), n \u2208 [[0, . . . ,m]] and s\u2032mk \u2208 N (gm)\nWe make the distinction between s and s\u2032 since \u03c0\u2217Low and \u03c0 \u2217 NLow do not necessarily follow the same trajectory. Whereas \u03c0\u2217Low is conditioned by goals that act as points in the state space, \u03c0 \u2217 NLow is conditioned by a whole set of points. Moving forward, we specify the following rewards functions for the hierarchical policies learning without abstraction and with the abstraction N :\nFor \u03c0\u2217, rHigh(s, g\u2217) = rext(s) = \u2212\u2225g\u2217 \u2212 s\u22252 and rLow(s, g) = \u2212\u2225g \u2212 s\u22252 For \u03c0\u2217N , rNHigh(s, g\n\u2217) = max x\u2208N (s) rext(x, g \u2217) = max x\u2208N (s) \u2212\u2225g\u2217\u2212x\u22252 and rNLow(s,N (g)) = 1x\u2208N (g)(s).\nIntuitively, rNHigh(s) is computed over the closest point to g \u2217 in N (s). rNLow(s,N (g)) is a binary reward that expresses if the agent reaches its abstract goal. This reward is chosen in this proof for its generality; it simply expresses that the agent should reach N (g). In practice it is sparse and difficult to optimise (it is replaced in STAR by the negative distance to the center of N (g)). A suboptimality bound will be derived for the last k steps of the trajectory, and by induction will be proven for the rest. Given criteria.4:\n\u2200s \u2208 N (gm).|rext(gm)\u2212 rext(s)| \u2264 \u03f5 we can claim that:\n|V\u03c0\u2217Low(smk)\u2212 V\u03c0\u2217NLow (s \u2032 mk)| \u2264 \u03f5\nTo derive the upper bound on suboptimality, we will proceed by computing V\u03c0\u2217NLow on the worst possible optimal trajectory T \u2217NLow . This gives:\n|V\u03c0\u2217Low(smk\u22121)\u2212 V\u03c0\u2217NLow (s \u2032 mk\u22121)| = |rext(smk\u22121) + \u03b3rext(smk)\u2212 rext(s\u2032mk\u22121)\u2212 \u03b3rext(s\u2032mk)|\n\u2264 |rext(smk\u22121)\u2212 rext(s\u2032mk\u22121)|+ \u03b3|rext(smk)\u2212 rext(s\u2032mk)| By triangle inequality.\nThe second term in the inequality corresponds exactly to \u03b3|V\u03c0\u2217Low(smk) \u2212 V\u03c0\u2217NLow (s \u2032 mk)| which is bounded by \u03b3\u03f5. The first term can be expanded as such:\n|rext(smk\u22121)\u2212 rext(s\u2032mk\u22121)| = rext(smk\u22121)\u2212 rext(s\u2032mk\u22121)( since rext(smk\u22121) \u2265 rext(s\u2032mk\u22121)) = \u2212\u2225g\u2217 \u2212 smk\u22121\u22252 + \u2225g\u2217 \u2212 s\u2032mk\u22121\u22252 \u2264 \u2212\u2225g\u2217 \u2212 smk\u22121\u22252 + \u2225g\u2217 \u2212 smk\u22121\u22252 + \u2225smk\u22121 \u2212 s\u2032mk\u22121\u22252 \u2264 \u2225smk\u22121 \u2212 s\u2032mk\u22121\u22252\n2k is chosen as a divisor of M to simplify the notations.\nSince the abstraction guarantees that starting from N (gm\u22121), a state s\u2032 \u2208 N (gm) is reached with \u03c0\u2217NLow in k steps, then the state s \u2032 mk should at the worst, be reachable in one step from s \u2032 mk\u22121. In addition, the state s\u2032mk\u22121 should at the worst be reachable from s \u2032 (m\u22121)k in k \u2212 1 steps.\nIn other words, the worst state s\u2032mk\u22121 is the farthest state from gm that reaches s \u2032 mk in 1 step and is reached by s\u2032(m\u22121)k in k \u2212 1 steps. We will bound \u2225smk\u22121 \u2212 s\u2032mk\u22121\u22252 using the two reachability relations.\nOn one hand, since smk\u22121 also reaches gm = g\u2217 in 1 step then \u2225gmk \u2212 smk\u22121\u22252 \u2264 Rmax. Then:\n\u2225smk\u22121 \u2212 s\u2032mk\u22121\u22252 \u2264 \u2225smk\u22121 \u2212 gm\u22252 + \u2225gm \u2212 s\u2032mk\u22252+\u2225s\u2032mk \u2212 s\u2032mk\u22121\u22252 \u2264 2Rmax + \u03f5\nOn the other hand, gm\u22121 = s(m\u22121)k reaches smk\u22121 in k \u2212 1 steps, and then \u2225gm\u22121 \u2212 smk\u22121\u22252 \u2264 (k \u2212 1)Rmax. Similarly, we could have:\n\u2225smk\u22121 \u2212 s\u2032mk\u22121\u22252 \u2264 \u2225smk\u22121 \u2212 gm\u22121\u22252 + \u2225gm\u22121 \u2212 s\u2032(m\u22121)k\u22252+\u2225s\u2032(m\u22121)k \u2212 s\u2032mk\u22121\u22252 \u2264 2(k \u2212 1)Rmax + \u2225gm\u22121 \u2212 s\u2032(m\u22121)k\u22252\nThis results in two distinct bounds for \u2225smk\u22121 \u2212 s\u2032mk\u22121\u22252 origination from a forward reachability relation, and a backwards reachability relation. This provides the following bound:\n|V\u03c0\u2217Low(smk\u22121)\u2212 V\u03c0\u2217NLow (s \u2032 mk\u22121)| \u2264 min(2Rmax + \u03f5, 2(k \u2212 1)Rmax + \u2225gm\u22121 \u2212 s\u2032(m\u22121)k\u22252) + \u03b3\u03f5\nBounding \u2225gm\u22121 \u2212 s\u2032(m\u22121)k\u22252 depends on the nature of the abstraction, and more specifically if the abstract sets can be bounded or not. Starting from the more general case, Def.3 guarantees that s\u2032(m\u22121)k is reachable from s \u2032 0 in m.(k\u2212 1) steps. To simplify the computation, can also assume that \u2225s0 \u2212 s\u20320\u22252 \u2264 \u03f5. (In practice, usually s0 = s\u20320). Also, s(m\u22121)k is reachable from s0 in m.(k \u2212 1) steps. Thus :\n\u2225gm\u22121 \u2212 s\u2032(m\u22121)k\u22252 = \u2225gm\u22121 \u2212 s0 + s0 \u2212 s\u20320 + s\u20320 \u2212 s\u2032(m\u22121)k\u22252 \u2264 2k(m\u2212 1)Rmax + \u03f5\nThen:\n|V\u03c0\u2217Low(smk\u22121)\u2212 V\u03c0\u2217NLow (s \u2032 mk\u22121)| \u2264 min(2Rmax + \u03f5, 2(mk \u2212 1)Rmax + \u03f5) + \u03b3\u03f5\n\u2264 2min(1, (mk \u2212 1))Rmax + (\u03b3 + 1)\u03f5 \u2264 2Rmax + (\u03b3 + 1)\u03f5\nWith an iteration process the above reasoning can be extended: \u2200i \u2208 {0, . . . ,mk}:\n|rext(si)\u2212 rext(s\u2032i)| = rext(si)\u2212 rext(s\u2032i)( since rext(si) \u2265 rext(s\u2032i)) = \u2212\u2225g\u2217 \u2212 si\u22252 + \u2225g\u2217 \u2212 s\u2032i\u22252 \u2264 \u2212\u2225g\u2217 \u2212 si\u22252 + \u2225g\u2217 \u2212 si\u22252 + \u2225si \u2212 s\u2032i\u22252 \u2264 \u2225si \u2212 s\u2032i\u22252 \u2264 min(2(mk \u2212 i)Rmax + \u03f5, 2iRmax + \u03f5) \u2264 2min(mk \u2212 i, i)Rmax + \u03f5\nwith min(mk \u2212 i, i) = i if i \u2264 mk2 else min(mk \u2212 i, i) = mk \u2212 i.\nFinally the bound can be computed as:\n|V\u03c0\u2217Low(s0)\u2212 V\u03c0\u2217NLow (s \u2032 0)| = mk\u2211 i=0 \u03b3i(rext(si)\u2212 rext(s\u2032i))\n\u2264 mk\u2211 i=0 \u03b3i\u2225si \u2212 s\u2032i\u22252\n\u2264 mk\u2211 i=0 \u03b3i(2min(mk \u2212 i, i)Rmax + \u03f5) \u2264 mk 2\u2211\ni=0\n\u03b3i(2iRmax + \u03f5) + mk\u2211 i=mk2 \u03b3i(2(mk \u2212 i)Rmax + \u03f5)\n\u2264 mk 2\u2211\ni=0\n\u03b3i(2iRmax) + mk\u2211 i=mk2 \u03b3i(2(mk \u2212 i)Rmax) + 1\u2212 \u03b3mk+1 1\u2212 \u03b3 \u03f5\n\u2264 ( mk 2\u2211\ni=0\n\u03b3ii+ mk\u2211 i=mk2 \u03b3i(mk \u2212 i))2Rmax + 1\u2212 \u03b3mk+1 1\u2212 \u03b3 \u03f5\nWe will now examine the case if \u2203B \u2265 \u03f5 > 0 such that \u2200i \u2208 {1, . . . ,m},maxx,y\u2208N (gi)\u2225x\u2212 y\u2225 \u2264 B. Following the reasoning established before, we can claim that:\n|rext(si)\u2212 rext(s\u2032i)| \u2264 \u2225si \u2212 s\u2032i\u22252 \u2264 min(2iRmax +B, 2(k \u2212 i)Rmax +B)\n\u2264 2(k 2 Rmax) +B \u2264 kRmax +B\nUltimately,\n|V\u03c0\u2217Low(s0)\u2212 V\u03c0\u2217NLow (s \u2032 0)| = mk\u2211 i=0 \u03b3i(rext(si)\u2212 rext(s\u2032i))\n\u2264 1\u2212 \u03b3 mk+1\n1\u2212 \u03b3 (kRmax +B)"
        },
        {
            "heading": "A.2 PROOF OF LEMMA 1 AND THEOREM 2:",
            "text": "By definition of N \u2032, since Gi \u2208 T \u2217NHigh then \u2203gi \u2208 Gi (since the reward for the high-level agent is rNHigh(Gi) = max\ns\u2208Gi rext(s)) such that gi reaches a state in Ni+1: To prove this we can first consider\nthe coarsest abstraction N : S \u2192 GN \u2286 2S s.t GN = {G0, G1} with G1 = N (gm) (assumed to satisfy criteria 1 and 4) and G0 = N (g0) = \u00b7 \u00b7 \u00b7 = N (gm\u22121). In this case, the goal in question gi is in N (gm\u22121) and by definition reaches gm \u2208 G1. This corresponds to gm\u22121. Thus, the process of refinement splits G0 such as G0 = G\u2032 \u222a G\u2032\u2032, G\u2032 \u2229 G\u2032\u2032 = \u2205 and identifies G\u2032 such that gm\u22121 \u2208 G\u2032 (since G\u2032 satisfies the reachability property for (G\u2032, Gi+1)) and G\u2032\u2032 = N \u2032(g0) = \u00b7 \u00b7 \u00b7 = N \u2032(gm\u22122). Additionally, G\u2032, G\u2032\u2032 and G1 are disjoint by definition of the refinement.\nBy induction, the lemma is proven. The induction also leads to a reachability-aware abstraction proving theorem 2."
        },
        {
            "heading": "B REPRESENTATION ANALYSIS IN ANT FALL",
            "text": "Similarly to the study established in 5.3, we provide the results obtained in Ant Fall (visualizing the 5 dimensional representation for Ant Maze Cam is less feasible).\nFig.5 shows that after 1M steps the ant gradually learns to approach the area nearing the movable block (colored in yellow in the figure). Around 2M steps, the ant starts succeeding at crossing the pit by pushing the block forward. By 3M steps, the Navigator has refined the abstraction and identified as goal the bridge-like region (immediately at the back of the movable block) that it targets most frequently to maximise its success to cross the chasm.\nThis representation further validates our hypothesis regarding the ability the reachability-aware abstraction to identify areas of interest in the environment and orient the learning of the agent towards optimal behaviour."
        },
        {
            "heading": "C ADDRESSING NON-STATIONARITY",
            "text": "Since STAR is a 3-level HRL algorithm, non-stationarity can affect the learning of the Manager as well as the Navigator.\nTo solve this problem in the Managaer, we incorporate the policy correction approach from Nachum et al. (2018) which adequately relabels subgoals in the replay buffer that are unadapted to the current policy of the agent.\nAdditionally, we address non-stationarity in the Navigator level by only refining the abstraction when the controller\u2019s policy is stable. Precisely, reachability analysis for (Gi, Gj) is only engaged when the controller\u2019s policy \u03c0NLow(s \u2208 Gi, Gj) has roughly converged to a deterministic behaviour and is not randomly taking actions. Since the forward model is trained on data generated by \u03c0NLow , it could be used as a proxy for assessing behaviour stability. Every time the transition (Gi, Gj) is explored, the error error(Fk(s \u2208 Gi, Gj)) is evaluated and stored. Finally, we infer the stability of learned policies from the progress of the error in the forward model during a window of time; if the last 10 evaluated errors satisfy error(Fk(s \u2208 Gi, Gj)) < \u03c3 then the policy is considered stable and reachability analysis is conducted."
        },
        {
            "heading": "D NAVIGATOR POLICY LEARNING",
            "text": ""
        },
        {
            "heading": "D.1 POLICY TRAINING",
            "text": "The Navigator\u2019s policy mainly uses Q-learning (Watkins & Dayan, 1992) to learn a policy \u03c0Nav for goal sampling. This choice is adapted to the discrete and small nature of the problem that the Navigator is expected to solve. More precisely, both the state and action spaces of the Navigator correspond to G since the Navigator is in a state Gt and has to select a next partition Gt+k to target. Additionally, we use the learned reachability relations to orient the exploration of the agent towards reachable goals. This is done by restricting the exploration to sample reachable goals when\npossible. Formally, at a goalGi, the Q-values are computed only goalsGj thatGi is known to reach (following reachability analysis)."
        },
        {
            "heading": "D.2 HANDLING A GROWING STATE SPACE",
            "text": "Following a refinement of the goal space G, a new goal space G\u2032 is formed such that |G\u2032| > |G|. To handle a growing state space, the Navigator transfer the Q-table computed on the state space G to the new state space G\u2032. Assuming that Gi is the goal that was refined in G, then G\u2032i1 and G\u2032i2 are the newly created goals with G \u2032 i1 reaching G\u2032i+1 = Gi+1. In that case, Q(G \u2032 i1 , G\u2032i+1) = max G\u2208G\u2032 Q(G\u2032, G\u2032i+1) and Q(G \u2032 i2 , G\u2032i+1) = min G\u2208G\u2032 Q(G\u2032, G\u2032i+1). Additionally, \u2200G\u2032 \u2208 G, s.t G\u2032 \u0338="
        },
        {
            "heading": "G\u2032i1 and G",
            "text": "\u2032 \u0338= G\u2032i2 , Q(G\u2032, G\u2032i1) = Q(G\u2032, G\u2032i2) = Q(G\u2032, Gi) (here G = G\u2032). Inversely, \u2200G\u2032 \u2208 G, s.t G\u2032 \u0338= G\u2032i+1 = Gi+1, Q(G\u2032i1 , G\u2032) = Q(G\u2032i2 , G\u2032) = Q(Gi, G\u2032) (here G = G\u2032). In summary, the new Q-tables modifies the Q-values between the refined goal and its target while preserving the rest of the values.\nD.3 INITIALISATION OF THE ABSTRACTION\nThe algorithm start without an abstract goal. After initial exploration, and once the controller\u2019s policy is stable (see C for more details), we split S into a set of visited states and a set of unvisited ones. Consequently the initial abstraction becomes G = G0 \u222a Gc0, with G0 the set of visited states and Gc0 the complement of G0 in S, representing unvisited states. This resulting abstraction is still coarse (e.g., partitions the state once space over the x and y dimensions) and is mainly useful to identify an abstract state enclosing a neighborhood of the initial state."
        },
        {
            "heading": "E ENVIRONMENT DETAILS",
            "text": "The implementation adapts the same environments (Ant Maze, Ant Fall) used in Nachum et al. (2018) with the addition of Ant Maze Cam. All of the environments use the Mujoco physics simulator (Todorov et al., 2012). In all the following setups, the training episode ends atmax timesteps = 500. The reward signal is dense and corresponds to the negative Euclidean distance to the goal g\u2217 scaled by a factor of 0.1. Success is achieved when this distance is less than a fixed threshold of 5.\n1. Ant Maze: The maze is composed of immovable blocks everywhere except in (0, 0), (8, 0), (16, 0), (16, 8), (16, 16), (8, 16), (0, 16). The agent is initializes at position (0, 0). At each episode, a target position is sampled uniformly from g\u2217x \u223c [\u22124, 20], g\u2217y \u223c [\u22124, 20]. At evaluation time, the agent is only evaluated for a fixed exit at (0, 16).\n2. Ant Fall: the agent is initialized on an elevated platform of height 4. Immovable blocks are placed everywhere except at (\u22128, 0), (0, 0), (\u22128, 8), (0, 8), (\u22128, 16), (0, 16), (\u22128, 24), (0, 24). A pit is within range [\u22124, 12]\u00d7 [12, 20]. A movable block is placed at (8, 8). The agent is initialized at position (0, 0, 4.5). At each episode, the target position is fixed to (g\u2217x, g \u2217 y , g \u2217 z) = (0, 27, 4.5). The\nant has to push the block into the pit and use it as a bridge to cross to the second platform. At evaluation time, the agent is only evaluated for a fixed exit at (0, 27, 4.5).\n3. Ant Maze Cam: This maze is similar to Ant Maze with the addition of a new block at (16, 8) effectively closing any passage to the top half of the maze. For the block to be removed, the ant needs to navigate to the area in the range [16, 20]\u00d7 [0, 8] where a camera is placed. In the area, the orientation ori(\u03b8x, \u03b8y, \u03b8z) needs to be negative simulating an identification process by the camera. ori(\u03b8x, \u03b8y, \u03b8z) is a function that projects the orientation on the xy plane."
        },
        {
            "heading": "F THE REACHABILITY ANALYSIS",
            "text": "The reachability analysis in STAR closely follows the process detailed in Zadem et al. (2023). First, we approximate the k-step reachability relations between states with a neural network model: Fk. More precisely, Fk(st, Gj) predicts the state s\u2032t+k reached after k steps when starting from st and targeting the abstract goal Gj .\nTo generalise these approximations from state-wise relations to set-wise relations, we rely on off-theshelf neural network reachability analysis techniques. Specifically, we use Ai2 (Gehr et al. (2018)) to compute the output of a neural network given a set of inputs. Consider the reachabilty analysis of a transition (Gi, Gj), Ai2 computes an over-approximation of the output set of the forward model; R\u0303k\u03c0NLow\n(Gi, Gj) = {s\u2032t+k = Fk(st, Gj), st \u2208 Gi}. This over-approximation is efficiently computed layer-by-layer in the neural network using operations on abstract domains. In practice, our abstract goals are represented as disjoint hyperrectangles since they are expressive in our applications and simple to analyse.\nThe algorithm checks if R\u0303k\u03c0NLow (Gi, Gj) \u2286 Gj , i.e. the reached set of states is inside the abstract goal. If the inclusion is valid, then the reachability property is verified in (Gi, Gj) and no splitting is required. Similarly, if R\u0303k\u03c0NLow (Gi, Gj) \u2229Gj = \u2205 then the reachability property cannot be respected for any subset in Gi and no refinement occurs. Otherwise, the algorithm splits the starting set Gi in two subsets across a dimension of the hyperrectangle. Each split is recursively tested through reachability analysis and searched accordingly until a subset that respects reachability is found or a maximal splitting depth is reached.\nAbstraction Refinement: As detailed in section F, an important step of our algorithm when conducting reachability analysis on transition (Gi, Gj) is to verify if R\u0303k\u03c0NLow (Gi, Gj) \u2286 Gj . In practice, since R\u0303k is an over-approximation, we can expect to have estimation errors in such a way that verifying the precise inclusion is very difficult. We rely on a heuristic that checks if V (R\u0303k\u03c0NLow\n(Gi,Gj)\u2229Gj) V (Gj)\n\u2265 \u03c41, with V () the volume of a hyperrectangle and \u03c41 a predefined threshold. This heuristic roughly translates to checking if most of the approximated reached states are indeed in the abstract goal. Similarly, to check if R\u0303k\u03c0NLow (Gi, Gj) \u2229 Gj = \u2205, we verify V (R\u0303k\u03c0NLow\n(Gi,Gj)\u2229Gj) V (Gj)\n\u2264 \u03c42 with \u03c42 a preset parameter. That is to say, we check that most of the abstract goal is not reached. Finally, we set a minimum volume ratio of a split compared to Gi."
        },
        {
            "heading": "G HYPERPARAMETERS",
            "text": ""
        },
        {
            "heading": "G.1 MANAGER-CONTROLLER NETWORKS",
            "text": "The implementation of our algorithm is adapted from the work of Zhang et al. (2023). The Manager and Controller use the same architecture and hyperparameters as HRAC (albeit with a different goal space). It should also be noted that the policy correction method introduced in HIRO, is similarly used in HRAC and STAR. Both the Manager and Controller use TD3 (Fujimoto et al., 2018) for learning policies with the same architecture and hyperparameters as in Zhang et al. (2023). The hyperparameters for the Manager and Controller networks are in table 1."
        },
        {
            "heading": "G.2 THE NAVIGATOR TRAINING",
            "text": "The Navigator uses an \u03f5-greedy exploration policy with \u03f50 = 0.99 and \u03f5min = 0.01 with a linear decay of a factor of 0.000001. The learning rate of the Navigator is 0.01 for Ant Maze and Ant Maze Cam and 0.005 for Ant Fall. The Navigator\u2019s experience is stored in a buffer of size 100000 (this buffer is used to train Fk). The Navigator\u2019s actions frequency is k = 30."
        },
        {
            "heading": "G.3 FORWARD MODEL",
            "text": "For the forward model we use a fully connected neural network with MSE loss. The network is of size (32, 32). We use the ADAM optimiser. This neural network is updated every episode for transitions (Gi, Gj) \u2208 E if this transition has been sampled for a defined minimal number of Navigator steps (to acquire sufficient data). Table.2 shows the hyperparameters of Fk."
        },
        {
            "heading": "G.4 REACHABILITY ANLYSIS",
            "text": "Table.3 shows the hyperparametes of the reachability analysis."
        },
        {
            "heading": "H STAR\u2019S PSEUDO-CODE",
            "text": "Algorithm 1 STAR Input: Learning environment E. Output: Computes \u03c0Nav, \u03c0Man and \u03c0Cont\n1: DNavigator \u2190 \u2205, DManager \u2190 \u2205, DController \u2190 \u2205, G \u2190 S 2: for t \u2264 max timesteps do 3: E \u2190 \u2205 4: sinit \u2190 initial state from E, st \u2190 sinit 5: Gs \u2190 G \u2208 G such that st \u2208 G 6: Gd \u223c \u03c0Nav(Gs, g\u2217) 7: gt \u223c \u03c0Man(st, Gd) 8: while true do 9: E \u2190 E \u222a {(Gs, Gd)}\n10: at \u223c \u03c0Cont(st, gt) 11: (st+1, r ext t , done)\u2190 execute the action at at st in E 12: rController = \u2212\u2225gt \u2212 st\u22252 13: Update \u03c0Cont 14: if not done then 15: st \u2190 st+1, t\u2190 t+ 1 16: if t mod l = 0 then 17: DManager \u2190 (st\u2212l, Gd, gt\u2212l, st, rManager, done) 18: Update \u03c0Man 19: gt \u223c \u03c0Man(st, Gd) 20: if t mod k = 0 then 21: Update DNavigator \u2190 (st\u2212k, Gd, st, rNavigator, done) 22: Update \u03c0Nav 23: Gs \u2190 G \u2208 G such that st \u2208 G 24: Gd \u223c \u03c0Nav(st, gexit) 25: else 26: Update Fk with the data from DNavigator 27: G \u2190 Refine(G, E ,Fk) 28: break the while loop and start a new episode"
        }
    ],
    "year": 2023
}