{
    "abstractText": "Offline reinforcement learning (RL), which aims to fully explore offline datasets for training without interaction with environments, has attracted growing recent attention. A major challenge for the real-world application of offline RL stems from the robustness against state observation perturbations, e.g., as a result of sensor errors or adversarial attacks. Unlike online robust RL, agents cannot be adversarially trained in the offline setting. In this work, we propose Diffusion Model-Based Predictor (DMBP) in a new framework that recovers the actual states with conditional diffusion models for state-based RL tasks. To mitigate the error accumulation issue in model-based estimation resulting from the classical training of conventional diffusion models, we propose a non-Markovian training objective to minimize the sum entropy of denoised states in RL trajectory. Experiments on standard benchmark problems demonstrate that DMBP can significantly enhance the robustness of existing offline RL algorithms against different scales of random noises and adversarial attacks on state observations. Further, the proposed framework can effectively deal with incomplete state observations with random combinations of multiple unobserved dimensions in the test. Our implementation is available at https://github.com/zhyang2226/DMBP",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhihe Yang"
        },
        {
            "affiliations": [],
            "name": "Yunjian Xu"
        }
    ],
    "id": "SP:2dd7b3127ee6e392b9d0223f7a2334127f247718",
    "references": [
        {
            "authors": [
                "Anurag Ajay",
                "Yilun Du",
                "Abhi Gupta",
                "Joshua Tenenbaum",
                "Tommi Jaakkola",
                "Pulkit Agrawal"
            ],
            "title": "Is conditional generative modeling all you need for decision-making",
            "venue": "arXiv preprint arXiv:2211.15657,",
            "year": 2022
        },
        {
            "authors": [
                "Gaon An",
                "Seungyong Moon",
                "Jang-Hyun Kim",
                "Hyun Oh Song"
            ],
            "title": "Uncertainty-based offline reinforcement learning with diversified Q-ensemble",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "David M Chan",
                "Roshan Rao",
                "Forrest Huang",
                "John F Canny"
            ],
            "title": "t-sne-cuda: Gpu-accelerated t-sne and its applications to modern data",
            "venue": "In 2018 30th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD),",
            "year": 2018
        },
        {
            "authors": [
                "Hyungjin Chung",
                "Byeongsu Sim",
                "Jong Chul Ye"
            ],
            "title": "Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Giulio Franzese",
                "Simone Rossi",
                "Lixuan Yang",
                "Alessandro Finamore",
                "Dario Rossi",
                "Maurizio Filippone",
                "Pietro Michiardi"
            ],
            "title": "How much is enough? a study on diffusion times in score-based generative models",
            "venue": "arXiv preprint arXiv:2206.05173,",
            "year": 2022
        },
        {
            "authors": [
                "Justin Fu",
                "Aviral Kumar",
                "Ofir Nachum",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "D4RL: Datasets for deep data-driven reinforcement learning",
            "venue": "arXiv preprint arXiv:2004.07219,",
            "year": 2020
        },
        {
            "authors": [
                "Scott Fujimoto",
                "Shixiang Shane Gu"
            ],
            "title": "A minimalist approach to offline reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Scott Fujimoto",
                "David Meger",
                "Doina Precup"
            ],
            "title": "Off-policy deep reinforcement learning without exploration",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Adam Gleave",
                "Michael Dennis",
                "Cody Wild",
                "Neel Kant",
                "Sergey Levine",
                "Stuart Russell"
            ],
            "title": "Adversarial policies: Attacking deep reinforcement learning",
            "year": 1905
        },
        {
            "authors": [
                "Chin Pang Ho",
                "Marek Petrik",
                "Wolfram Wiesemann"
            ],
            "title": "Fast Bellman updates for robust MDPs",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Michael Janner",
                "Justin Fu",
                "Marvin Zhang",
                "Sergey Levine"
            ],
            "title": "When to trust your model: Model-based policy optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Michael Janner",
                "Yilun Du",
                "Joshua B. Tenenbaum",
                "Sergey Levine"
            ],
            "title": "Planning with diffusion for flexible behavior synthesis",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Aviral Kumar",
                "Justin Fu",
                "Matthew Soh",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "Stabilizing off-policy Q-learning via bootstrapping error reduction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Aviral Kumar",
                "Aurick Zhou",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "Conservative Q-learning for offline reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Sergey Levine",
                "Aviral Kumar",
                "George Tucker",
                "Justin Fu"
            ],
            "title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems",
            "venue": "arXiv preprint arXiv:2005.01643,",
            "year": 2020
        },
        {
            "authors": [
                "Zhixuan Liang",
                "Yao Mu",
                "Mingyu Ding",
                "Fei Ni",
                "Masayoshi Tomizuka",
                "Ping Luo"
            ],
            "title": "Adaptdiffuser: Diffusion models as adaptive self-evolving planners",
            "venue": "arXiv preprint arXiv:2302.01877,",
            "year": 2023
        },
        {
            "authors": [
                "Yen-Chen Lin",
                "Ming-Yu Liu",
                "Min Sun",
                "Jia-Bin Huang"
            ],
            "title": "Detecting adversarial attacks on neural network policies with visual foresight",
            "venue": "arXiv preprint arXiv:1710.00814,",
            "year": 2017
        },
        {
            "authors": [
                "Michael L Littman"
            ],
            "title": "Markov games as a framework for multi-agent reinforcement learning",
            "venue": "In Machine Learning Proceedings",
            "year": 1994
        },
        {
            "authors": [
                "Andreas Lugmayr",
                "Martin Danelljan",
                "Andres Romero",
                "Fisher Yu",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Repaint: Inpainting using denoising diffusion probabilistic models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jiafei Lyu",
                "Xiaoteng Ma",
                "Xiu Li",
                "Zongqing Lu"
            ],
            "title": "Mildly conservative Q-learning for offline reinforcement learning",
            "venue": "arXiv preprint arXiv:2206.04745,",
            "year": 2022
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Andrei A Rusu",
                "Joel Veness",
                "Marc G Bellemare",
                "Alex Graves",
                "Martin Riedmiller",
                "Andreas K Fidjeland",
                "Georg Ostrovski"
            ],
            "title": "Human-level control through deep reinforcement learning",
            "venue": "Nature, 518(7540):529\u2013533,",
            "year": 2015
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kishan Panaganti",
                "Zaiyan Xu",
                "Dileep Kalathil",
                "Mohammad Ghavamzadeh"
            ],
            "title": "Robust reinforcement learning using offline data",
            "venue": "arXiv preprint arXiv:2208.05129,",
            "year": 2022
        },
        {
            "authors": [
                "Lerrel Pinto",
                "James Davidson",
                "Rahul Sukthankar",
                "Abhinav Gupta"
            ],
            "title": "Robust adversarial reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "18th International Conference,",
            "year": 2015
        },
        {
            "authors": [
                "Aurko Roy",
                "Huan Xu",
                "Sebastian Pokutta"
            ],
            "title": "Reinforcement learning under model mismatch",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Thomas Hubert",
                "Karen Simonyan",
                "Laurent Sifre",
                "Simon Schmitt",
                "Arthur Guez",
                "Edward Lockhart",
                "Demis Hassabis",
                "Thore Graepel"
            ],
            "title": "Mastering Atari, Go, chess and shogi by planning with a learned model",
            "year": 2020
        },
        {
            "authors": [
                "Qianli Shen",
                "Yan Li",
                "Haoming Jiang",
                "Zhaoran Wang",
                "Tuo Zhao"
            ],
            "title": "Deep reinforcement learning with robust and smooth policy",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "David Silver",
                "Julian Schrittwieser",
                "Karen Simonyan",
                "Ioannis Antonoglou",
                "Aja Huang",
                "Arthur Guez",
                "Thomas Hubert",
                "Lucas Baker",
                "Matthew Lai",
                "Adrian Bolton"
            ],
            "title": "Mastering the game of Go without human knowledge",
            "year": 2017
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "arXiv preprint arXiv:2010.02502,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "arXiv preprint arXiv:2011.13456,",
            "year": 2020
        },
        {
            "authors": [
                "Yanchao Sun",
                "Ruijie Zheng",
                "Yongyuan Liang",
                "Furong Huang"
            ],
            "title": "Who is the strongest enemy? Towards optimal and efficient evasion attacks in deep RL",
            "venue": "arXiv preprint arXiv:2106.05087,",
            "year": 2021
        },
        {
            "authors": [
                "Kai Liang Tan",
                "Yasaman Esfandiari",
                "Xian Yeow Lee",
                "Soumik Sarkar"
            ],
            "title": "Robustifying reinforcement learning agents via action space adversarial training",
            "venue": "American Control Conference (ACC),",
            "year": 2020
        },
        {
            "authors": [
                "Chen Tessler",
                "Yonathan Efroni",
                "Shie Mannor"
            ],
            "title": "Action robust reinforcement learning and applications in continuous control",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton"
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research,",
            "year": 2008
        },
        {
            "authors": [
                "Zhendong Wang",
                "Jonathan J Hunt",
                "Mingyuan Zhou"
            ],
            "title": "Diffusion policies as an expressive policy class for offline reinforcement learning",
            "venue": "arXiv preprint arXiv:2208.06193,",
            "year": 2022
        },
        {
            "authors": [
                "Yifan Wu",
                "George Tucker",
                "Ofir Nachum"
            ],
            "title": "Behavior regularized offline reinforcement learning",
            "venue": "arXiv preprint arXiv:1911.11361,",
            "year": 2019
        },
        {
            "authors": [
                "Huan Xu",
                "Shie Mannor"
            ],
            "title": "The robustness-performance tradeoff in Markov decision processes",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2006
        },
        {
            "authors": [
                "Huan Xu",
                "Shie Mannor"
            ],
            "title": "Distributionally robust Markov decision processes",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2010
        },
        {
            "authors": [
                "Mengdi Xu",
                "Zuxin Liu",
                "Peide Huang",
                "Wenhao Ding",
                "Zhepeng Cen",
                "Bo Li",
                "Ding Zhao"
            ],
            "title": "Trustworthy reinforcement learning against intrinsic vulnerabilities: Robustness, safety, and generalizability",
            "venue": "arXiv preprint arXiv:2209.08025,",
            "year": 2022
        },
        {
            "authors": [
                "Rui Yang",
                "Chenjia Bai",
                "Xiaoteng Ma",
                "Zhaoran Wang",
                "Chongjie Zhang",
                "Lei Han"
            ],
            "title": "RORL: Robust offline reinforcement learning via conservative smoothing",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Chenlu Ye",
                "Rui Yang",
                "Quanquan Gu",
                "Tong Zhang"
            ],
            "title": "Corruption-robust offline reinforcement learning with general function approximation",
            "venue": "arXiv preprint arXiv:2310.14550,",
            "year": 2023
        },
        {
            "authors": [
                "Pengqian Yu",
                "Huan Xu"
            ],
            "title": "Distributionally robust counterpart in Markov decision processes",
            "venue": "IEEE Transactions on Automatic Control,",
            "year": 2015
        },
        {
            "authors": [
                "Tianhe Yu",
                "Garrett Thomas",
                "Lantao Yu",
                "Stefano Ermon",
                "James Y Zou",
                "Sergey Levine",
                "Chelsea Finn",
                "Tengyu Ma"
            ],
            "title": "MOPO: Model-based offline policy optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tianhe Yu",
                "Aviral Kumar",
                "Rafael Rafailov",
                "Aravind Rajeswaran",
                "Sergey Levine",
                "Chelsea Finn"
            ],
            "title": "COMBO: Conservative offline model-based policy optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Huan Zhang",
                "Hongge Chen",
                "Chaowei Xiao",
                "Bo Li",
                "Mingyan Liu",
                "Duane Boning",
                "Cho-Jui Hsieh"
            ],
            "title": "Robust deep reinforcement learning against adversarial perturbations on state observations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Huan Zhang",
                "Hongge Chen",
                "Duane Boning",
                "Cho-Jui Hsieh"
            ],
            "title": "Robust reinforcement learning on state observations with learned optimal adversary",
            "venue": "arXiv preprint arXiv:2101.08452,",
            "year": 2021
        },
        {
            "authors": [
                "Qinsheng Zhang",
                "Molei Tao",
                "Yongxin Chen"
            ],
            "title": "gddim: Generalized denoising diffusion implicit models",
            "venue": "arXiv preprint arXiv:2206.05564,",
            "year": 2022
        },
        {
            "authors": [
                "Xuezhou Zhang",
                "Yiding Chen",
                "Xiaojin Zhu",
                "Wen Sun"
            ],
            "title": "Corruption-robust offline reinforcement learning",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "p\u03b8(s\u0303t ) below. Following Ho"
            ],
            "title": "2020), we adopt the variational lower bound (VLB) to optimize the negative log-likelihood: LVLB",
            "year": 2020
        },
        {
            "authors": [
                "Ho"
            ],
            "title": "2020), for the first term in the last line of Eq. 8, we let p\u03b8(s\u0303",
            "venue": "LVLB \u2265 Lentropy (Ho et al.,",
            "year": 2020
        },
        {
            "authors": [
                "Song"
            ],
            "title": "2022a), which aim to reduce the number of reverse denoising steps required during the testing phase. The proposed methods will be subject to verification in our future work",
            "year": 2022
        },
        {
            "authors": [
                "Yang"
            ],
            "title": "self-adaptive diffusion start timestep to make DMBP more adaptive for environments with varying noise scales. Another limitation of this work is the lack of validation of DMBP\u2019s performance with normalized state observations (as in Zhang et al",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Reinforcement learning (RL) has been proven to be a powerful tool for high-dimensional decisionmaking problems under uncertainty (Mnih et al., 2015; Silver et al., 2017; Schrittwieser et al., 2020). However, its trial-and-error learning manner requires frequent interactions with the environment, which can be expensive and/or dangerous in a variety of real-world applications (Levine et al., 2020). A widely adopted solution is to build up a simulator for policy training, which is costly and may fail due to the discrepancy between the simulator and reality. As a promising alternative that has received growing attention, offline RL fully explores offline datasets and requires no interaction with the environments in the training process.\nA major challenge of offline training is on the robustness against perturbation on state observations, which may result from sensor errors, adversarial attacks, and mismatches between statistic datasets and the real environment. For example, GPS signal errors can lead to inaccurate positioning of autonomous vehicles, and position sensor errors can lead to erroneous estimation of robot arm postures. The robustness of the trained policy against state perturbations is vital for preventing agents from catastrophic movements. In online settings, various adversarial training methods have been proposed to robustly handle the mismatch between observed and actual states (Zhang et al., 2020; 2021; Sun et al., 2021). These methods are not directly applicable in offline training.\nA classical approach against perturbed state observation is to train robust policies against worst-case disturbances (see the left subplot in Figure 1), which may lead to over-conservatism (Zhang et al., 2020; 2021). In a pioneering work (Yang et al., 2022), the authors propose an alternative approach that adopts the conservative smoothing method to smoothen the Q-value and regularize the policy, preventing the agent from taking catastrophic movements under adversarial attacks in the test. The performance of the aforementioned approach may decay quickly with the increasing noise scale, especially in complicated environments with high-dimensional action and state spaces.\n\u2217Corresponding author\nFor online image-based deep RL, Lin et al. (2017) propose a model-based approach to \u201cdenoise\u201d the observations by predicting the actual states. They construct a multiple-layer perceptron (MLP) neural network to detect the adversarial attack on image-based observations and predict the original states for decision-making in Atari games. For state-based RL tasks, similar MLP-based prediction methods have been used as data augmentation in online (Janner et al., 2019) and offline (Yu et al., 2020; 2021) settings instead of denoising tools. In general, MLP-based prediction methods cannot be applied to complicated state-based tasks (like Mujoco) which are sensitive to observation noise and prone to error accumulation.\nRecently, diffusion-based generative models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020b) are widely used in offline RL/decision-making problems as trajectory generators (Janner et al., 2022; Ajay et al., 2022; Liang et al., 2023) and behavior cloners (Wang et al., 2022). We note that the potential of diffusion models to facilitate decision making via state denoising has not been fully explored.\nTowards this end, we propose a new framework that predicts the actual states against observation perturbations for state based offline RL, which is referred to as Diffusion Model-Based Predictor (DMBP). Different from the aforementioned works, the proposed approach utilizes diffusion models as noise reduction tools rather than generation models, and can therefore enhance the robustness of existing offline RL algorithms against different scales of perturbations on state observations.\nA diagram of the proposed approach is shown in the right subplot of Figure 1. Given the pastestimated state trajectory, last-step agent-generated action, and the noised current state from the environment, DMBP utilizes a conditioned diffusion model to estimate the current state by reversely denoising data. To mitigate the error accumulation issue in state estimation, we propose a new non-Markovian loss function that minimizes the sum entropy of denoised states over the RL trajectory (cf. Section 4). In order to well capture the relationship between the noised current state and the denoised state trajectory (especially the last RL timestep denoised state), we propose an Unet-MLP neural network structure to predict noise information (cf. Appendix B.1). The output of DMBP is an estimation of the current state, which is fed into an offline RL algorithm to generate the action. To our knowledge, this is the first state denoising framework for offline RL against observation perturbations in state-based tasks.\nThe proposed framework has several advantages over existing offline RL methods against noisy observations. First, with an objective of recovering the actual state, DMBP can significantly strengthen the robustness of existing offline RL algorithms against different scales of random noises and adversarial attacks. The proposed approach does not lead to over-conservative policies, compared with counterparts that train robust policies against worst-case (or adversarial) perturbations.\nFurther, by virtue of the capability of diffusion models to infill the missing regions (i.e., image inpainting), DMBP facilitates the decision making under incomplete state observations with random combinations of multiple unobserved dimensions in the test. Such a situation is common in reality, for example, when robots continue to work with compromised sensors."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": "Robust RL. Robust RL can be categorized into two taxonomies: training-time and testing-time robustness. Training-time robust RL involves perturbations during the training process, while evaluating the agent in a clean environment (Zhang et al., 2022b; Ye et al., 2023). Conversely, testing-time robust RL focuses on training the agent with unperturbed datasets or environments and then testing its performance in the presence of disturbances (Yang et al., 2022; Panaganti et al., 2022). Our work primarily aims at enhancing the testing-time robustness of existing offline RL algorithms.\nTesting-time robust RL formulations can generally be divided into three categories (Xu et al., 2022). i) Uncertain observations: In online settings, Zhang et al. (2020) propose a state-adversarial Markov decision process (SA-MDP) framework, which is advanded by Zhang et al. (2021); Sun et al. (2021) that adopt neural networks to simulate worst-case observation attacks for the training of more robust policies. In offline settings, Yang et al. (2022) utilize the conservative smoothing method to make the agent take similar actions when the perturbations on state observation are relatively small. ii) Uncertain actions: Tessler et al. (2019) explore the training of robust policies against two types of action uncertainties, i.e., occasional and constant adversarial perturbations. Tan et al. (2020) utilize adversarial training on actions to enhance the robustness against action perturbations. iii) Uncertain transitions and rewards: The computation of optimal policies against uncertain environment parameters has been explored under the robust Markov Decision Process (MDP) (Xu & Mannor, 2006; Roy et al., 2017; Ho et al., 2018) and the distributionally robust MDP frameworks (Xu & Mannor, 2010; Yu & Xu, 2015). In online RL settings, Pinto et al. (2017) and Gleave et al. (2019) train the agent under adversarial model uncertainty through a two-player Markov game approach (Littman, 1994). For offline RL training, Panaganti et al. (2022) propose a dual reformulated robust Bellman operator to deal with the uncertain transition probability.\nFor models in the first two categories, the true states and transition probabilities of the environments are not influenced by the action, which is not the case for the robust approaches against model uncertainties developed in the third category. Our work belongs to the first category.\nDiffusion models in offline RL. The diffusion model was originally proposed as an iterative denoising procedure for image generation in computer vision (Sohl-Dickstein et al., 2015; Ho et al., 2020). Recently, diffusion model has been adopted in decision-making for state-based tasks. Diffuser (Janner et al., 2022) and Decision Diffuser (Ajay et al., 2022) utilize the conditional diffusion model as a trajectory generator to facilitate the decision making of the agent. Wang et al. (2022) propose the Diffusion-QL algorithm that adopts the diffusion model to regulate the policy not to be far away from the one used in datasets, in a similar spirit to Fujimoto et al. (2019); Wu et al. (2019). Different from the aforementioned works, the proposed approach utilizes the diffusion model as a denoiser (against state observation perturbations) rather than a generator, for robust offline training of RL agents."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "Offline RL. RL tasks are generally modeled as Markovian Decision Processes (MDP) in the form of M = (S,A, r, P, \u03b3, d0), where S is the state space, A is the action space, r : S \u00d7A \u2192 R represents the reward function, P is the model dynamics, \u03b3 \u2208 [0, 1) is the discount factor, and d0 \u2208 \u2206(S) is the distribution of initial state s0 (the set of the probability distribution over X is denoted as \u2206(X )). P (s\u2032|s,a) : S \u00d7A \u2192 \u2206(S) represents the transition function from state s to s\u2032 when taking action a. The state-action-reward transitions over trajectory are recorded as \u03c4 := (st,at, rt)t\u22650.The goal of RL is to learn a policy \u03c0\u03d5 that maximizes the expectation of the cumulated discounted reward R(\u03c4 ) = \u2211\u221e t=0 \u03b3 tr(st,at), denoted by \u03c0\u2217\u03d5 = argmax\u03c0 Es0\u223cd0,a\u223c\u03c0[R(\u03c4 )].\nA commonly used iteration method for state-based tasks is under the actor-critic framework, where the Q-value of a policy is defined asQ\u03c0(st,at) := Ea\u223c\u03c0[ \u2211\u221e i=t \u03b3\n(i\u2212t)r(si,ai)] and is modeled using neural networks (recorded as Q\u03c8(st,at)). To approach an optimal policy, the temporal difference (TD) method is adopted to update the critic Q-value by minimizing the TD loss: LTD(\u03c8) := E(s,a,r,s\u2032)\u2208D[(r + \u03b3maxa\u2032\u2208AQ\u03c8(s\u2032,a\u2032) \u2212 Q\u03c8(s,a))2]. The actor is updated by Lactor(\u03d5) := Es\u2208D,a\u223c\u03c0\u03d5(\u00b7|s)[\u2212Q(s,a)], where the dataset D records historical interactions between agent and environment, and is continuously updated in the alternate training of the actor and the critic. In offline RL settings, the training is performed on a statistic dataset D\u03bd := {(s,a, r, s\u2032)}, which is obtained from a behavior policy \u03c0\u03bd without any interaction with the environment.\nDirect adoption of the actor-critic approach may lead to a severe distributional shift between the trained policy \u03c0\u03d5 and the behavior policy \u03c0\u03bd due to the over-estimation of the Q-value of actions unseen in datasets. To mitigate this issue, policy regularization has been adopted to update the actor through constrained policy loss (Wu et al., 2019; Kumar et al., 2019; Fujimoto et al., 2019; Fujimoto & Gu, 2021; Wang et al., 2022): L(\u03d5) := Ld(\u03d5) + \u03b1actorLactor(\u03d5), where Ld(\u03d5) is the behavior cloning loss representing the nominal distance between the trained policy and the behavior policy, and \u03b1actor is the coefficient for the Q-value term. Alternatively, conservative Q estimation updates the critic through minimizing the constrained Q-value loss (Kumar et al., 2020; An et al., 2021; Lyu et al., 2022; Yang et al., 2022): L(\u03c8) := Lq(\u03c8) + \u03b1criticLTD(\u03c8), where Lq(\u03c8) is the penalty on Q-value for out-of-distribution actions, and \u03b1critic is the coefficient for the TD loss term.\nDiffusion model. Diffusion based generative models have been widely used for synthesizing highquality images from text descriptions. The forward process, i.e., the noising process, is a Markov chain that gradually adds Gaussian noise to data according to a variance schedule \u03b21, . . . , \u03b2K :\nq(x1:K | x0) := K\u220f k=1 q(xk | xk\u22121), q(xk | xk\u22121) := N (xk; \u221a 1\u2212 \u03b2kxk\u22121, \u03b2kI).\nThe reverse process, i.e., the denoising process, is a Markov chain with learned Gaussian transitions that usually starts at p(xK) = N (xK ; 0, I):\np\u03b8(x0:K) := p(xK) K\u220f k=1 p\u03b8(xk\u22121 | xk), p\u03b8(xk\u22121 | xk) := N (xk\u22121;\u00b5\u03b8(xk, k),\u03a3\u03b8(xk, k)).\nHo et al. (2020) derive a simplified surrogate loss for the reverse process denoising: Ldenoise(\u03b8) := Ek\u223c[1,K],\u03f5\u223cN (0,I)[\u2225\u03f5\u03b8(xk, k)\u2212 \u03f5\u22252]. (1)\nThe Gaussian noise \u03f5, which perturbs the original data x0 to noised data xk, is estimated through the neural network based predictor \u03f5\u03b8(xk, k). xk\u22121 is sampled from the reverse process as \u00b5\u03b8(xk, k) and \u03a3\u03b8(xk, k) are functions of \u03f5\u03b8(xk, k). It is straightforward to extend diffusion models to conditional ones with p\u03b8(xt\u22121 | xt, c) (conditioned on information c), where the noise prediction is given by \u03f5\u03b8(xk, k, c)."
        },
        {
            "heading": "4 DIFFUSION MODEL BASED PREDICTOR",
            "text": "We express the perturbed version of the original state s as s\u0303, where Bd(s, \u03f5) := {s\u0303 : d(s, s\u0303) \u2264 \u03f5} is the perturbation set and the metric d(\u00b7, \u00b7) is based on \u2113p norm, as in Shen et al. (2020). An adversarial attack on state s is introduced in Yang et al. (2022): s\u0303\u2217 = argmaxs\u0303\u2208Bd(s,\u03f5)D(\u03c0\u03d5(\u00b7|s)\u2225\u03c0\u03d5(\u00b7|s\u0303)), where D(\u00b7\u2225\u00b7) is the divergence of two distributions. The targets of both works are to minimize the smoothness regularizer for the policy: R\u03c0s = Es\u2208D maxs\u0303\u2208Bd(s,\u03f5)D(\u03c0(\u00b7|s)\u2225\u03c0(\u00b7|s\u0303)), and to minimize the smoothness regularizer for the value function: RVs = Es\u2208D,a\u223c\u03c0maxs\u0303\u2208Bd(s,\u03f5)(Q(s,a)\u2212 Q(s\u0303,a)), against the perturbations on state observations. We remark that we do not normalize the state observations when applying the perturbations as in Shen et al. (2020); Sun et al. (2021), in contrast to Zhang et al. (2020); Yang et al. (2022).\nIn Section 4.1, we propose DMBP to recover the actual state for decision-making (which is fundamentally different from the technical approaches in aforementioned works). In Section 4.2, we propose a new non-Markovian loss function to mitigate error accumulation. In Section 4.3, we apply DMBP to RL tasks under incomplete state observations with unobserved dimension(s)."
        },
        {
            "heading": "4.1 CONDITIONAL DIFFUSION FOR PREDICTING REAL STATE",
            "text": "As there are two timesteps involved in our framework, we use superscripts i, k \u2208 {1, . . .K} to denote diffusion timestep and subscript t \u2208 {1, . . . , T} to denote trajectory timestep in RL tasks. DMBP is inspired by the diffusion model framework originally proposed for image generations (Ho et al., 2020). As the proposed framework essentially deals with information with small to medium scale noises instead of generating data from pure noise, we redesign the variance schedule as:\n\u03b2i = 1\u2212 \u03b1i = e\u2212 b i+a+c, \u03b1\u0304k = k\u220f i=1 \u03b1i, \u03b2\u0303i = 1\u2212 \u03b1\u0304i\u22121 1\u2212 \u03b1\u0304i \u03b2i,\nwhere a, b, c are hyperparameters (cf. Appendix B.2). The redesigned variance schedule restricts the noise scale to be small in the diffusion process and limits the total number of diffusion timesteps K for predictor training. We use the conditional diffusion model to obtain the denoised state s\u0302t from the noised state s\u0303t, with the condition on last step action at\u22121 and the previously denoised state trajectory \u03c4 s\u0302t\u22121 := {s\u03021, s\u03022, ..., s\u0302t\u22121}. The denoised state s\u0302t is sampled from the reverse denoising process, which can be expressed as a Markov chain:\ns\u0302t \u223c p\u03b8(s\u03030:kt | at\u22121, \u03c4 s\u0302t\u22121) = fk(s\u0303t) k\u220f i=1 p\u03b8(s\u0303 i\u22121 t | s\u0303it,at\u22121, \u03c4 s\u0302t\u22121), (2)\nwhere fk(s\u0303t) = \u221a \u03b1\u0304ks\u0303t. The transitions p\u03b8(s\u0303i\u22121t | s\u0303it,at\u22121, \u03c4 s\u0302t\u22121) can be modeled using Gaussian distribution N (s\u0303i\u22121t ;\u00b5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302t\u22121, i),\u03a3\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302t\u22121, i)), with the following mean and variance (Ho et al., 2020):\n\u00b5\u03b8(s\u0303 i t,at\u22121, \u03c4 s\u0302 t\u22121, i) = \u221a \u03b1i(1\u2212 \u03b1\u0304i\u22121) 1\u2212 \u03b1\u0304i s\u0303it + \u221a \u03b1\u0304i\u22121\u03b2i 1\u2212 \u03b1\u0304i s\u0303 0(i) t , \u03a3\u03b8(s\u0303 i t,at\u22121, \u03c4 s\u0302 t\u22121, i) = \u03b2\u0303iI .\nHere, s\u03030(i)t is the state directly recovered from the current diffusion step noise prediction, which is given by\ns\u0303 0(i) t = 1\u221a \u03b1\u0304i [s\u0303it \u2212 \u221a 1\u2212 \u03b1\u0304i\u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302t\u22121, i)]. (3)\nThe reverse diffusion chain is given by\ns\u0303i\u22121t | s\u0303it = s\u0303it\u221a \u03b1i \u2212 \u03b2i\u221a \u03b1i(1\u2212 \u03b1\u0304i) \u03f5\u03b8(s\u0303 i t,at\u22121, \u03c4 s\u0302 t\u22121, i) +\n\u221a \u03b2\u0303i\u03f5, (4)\nwhere \u03f5 \u223c N (0, I) and is set to be 0 at the final denoising step (i = 1). For the final step denoising output of Eq. 4 s\u03030t (i.e., the output of DMBP), we refer it to as s\u0302t. s\u0302t can be used for decision-making by any offline-trained agent according to at = \u03c0\u03d5(\u00b7 | s\u0302t). s\u0302t is stored in the trajectory cache \u03c4 s\u0302t , and the pair of (\u03c4 s\u0302t , at) will be utilized for the next step denoising. In practice, on account of the stochasticity involved in the diffusion process, we denoise the state 50 times in parallel and take the average value as the final output s\u0302t to prevent the denoised state from falling out of the distribution.\nWe find that directly inputting state trajectories and action into neural networks leads to poor noise estimation (cf. Appendix C for ablation study on network structure), partially due to the fact that s\u0302t\u22121 is more closely related to s\u0303it than s\u0302j with j < t\u2212 1, and that this information cannot be well captured by neural networks. Therefore, we first extract the information from the trajectory with U-net (Ronneberger et al., 2015; Janner et al., 2022) (recorded as U\u03be(s\u0303it, \u03c4 s\u0302 t\u22121)), and then utilize an MLP-based neural network to predict the noise through \u03f5\u03b8(U\u03be(s\u0303it, \u03c4 s\u0302 t\u22121), s\u0303 i t,at\u22121, s\u0302t\u22121, i), which is represented by \u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302 t\u22121, i) for notational convenience. See Appendix B.1 for details."
        },
        {
            "heading": "4.2 NON-MARKOVIAN LOSS FUNCTION",
            "text": "The accuracy of the current denoising result s\u0302t is highly dependent on the accuracy of the diffusion condition \u03c4 s\u0302t\u22121. A straightforward adoption of the loss function 1 in the denoising diffusion probabilistic model (DDPM) (Ho et al., 2020) may lead to severe error accumulation in online testing, due to the mismatch between the training-process noise prediction \u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s t\u22121, i) and the testing-process noise prediction \u03f5\u03b8(s\u0303it+1,at, \u03c4 s\u0302 t\u22121, i). To mitigate error accumulation and enhance the robustness of DMBP, we propose a non-Markovian training objective to minimize the sum entropy of denoised states over the RL trajectory \u03c4 :\nLentropy = T\u2211 t=2 Est\u2208\u03c4 ,q(st) [ \u2212 logP (s\u0302t | at\u22121, \u03c4 s\u0302t\u22121) ] , (5)\nwhere P (s\u0302t | at\u22121, \u03c4 s\u0302t\u22121) = p\u03b8(s\u03030t | at\u22121, \u03c4 s\u0302t\u22121) is the distribution of state after denoising at RL timestep t. Following the setting in (Ho et al., 2020), we establish a closed-form expression of the training objective that minimizes Eq. 5, which can be simplified as (cf. the details in Appendix A):\nLsimple(\u03b8) = Es1\u223cd0,\u03f5it\u223cN (0,I),i\u223cUK [ T\u2211 t=2 \u2225\u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302t\u22121, i)\u2212 \u03f5it\u22252 ] , (6)\nwhere UK is the uniform distribution over discrete set {1, 2, . . . ,K}, and the noised states for all terms are sampled through:\ns\u0303it = \u221a \u03b1\u0304ist + \u221a 1\u2212 \u03b1\u0304i\u03f5it, \u03f5it \u223c N (0, I).\nFor computational convenience, we further simplify Eq. 6 and derive our non-Markovian loss function by sampling the partial trajectory (st\u2212N ,at\u2212N , st\u2212N+1, . . . , st+M\u22121) from the offline dataset D\u03bd (N is the condition trajectory length and M is the sample trajectory length):\nL(\u03b8) = Ei\u223cUK ,\u03f5t\u223cN (0,I),(st\u2212N ,...,st+M\u22121)\u2208D\u03bd [ \u2225\u03f5\u03b8(s\u0303it,at\u22121, \u03c4 st\u22121, i)\u2212 \u03f5it\u22252\ufe38 \ufe37\ufe37 \ufe38\nLt\n+\nt+M\u22121\u2211 m=t+1\n\u2225\u03f5\u03b8(s\u0303im,am\u22121, \u03c4 s\u0306m\u22121, i)\u2212 \u03f5im\u22252\ufe38 \ufe37\ufe37 \ufe38 Lm\n] ,\n(7)\nwhere the state trajectory condition for the predictor \u03f5\u03b8 inLt is the original \u03c4 st\u22121 = {st\u2212N , . . . , st\u22121} from the offline dataset D\u03bd , and the state trajectory condition in Lm can be expressed as \u03c4 s\u0306m\u22121 = {s\u0306j | j \u2208 {m\u2212N, . . . ,m\u2212 1}}, with:\ns\u0306j =  sj1\u221a \u03b1\u0304i [ s\u0303ij \u2212 \u221a 1\u2212 \u03b1\u0304i\u03f5\u03b8(s\u0303ij ,aj\u22121, \u03c4 s\u0306j\u22121, i) ] if j < t, otherwise (j \u2208 {t, . . . , t+M \u2212 2}).\nDifferent from the loss function in Ho et al. (2020) that concerns only single-step diffusion accuracy (for data generation under conditions on ground-truth states), the proposed non-Markovian loss function trades off between the diffusion accuracy at the current RL timestep and the condition shift in a long RL time horizon (to avoid error accumulation)."
        },
        {
            "heading": "4.3 DIFFUSION BASED STATE INFILLING FOR UNOBSERVED DIMENSION",
            "text": "Inspired by the application of diffusion models in image inpainting (Lugmayr et al., 2022), we propose a state infilling procedure for DMBP facilitated decision making, which is shown to work well on state-based RL tasks with incomplete state observations (cf. Section 5.2).\nAlgorithm 1 Diffusion based state infilling for DMBP Require: sKt \u223c N (0, I), s\u0301t,at\u22121, \u03c4 s\u0302t\u22121,m\n1: for i = K, . . . , 1 do 2: for u = 1, . . . , U do 3: \u03f5 \u223c N (0, I) if i > 1, else \u03f5 = 0 4: s\u0301i\u22121t,known = \u221a \u03b1\u0304is\u0301t + \u221a 1\u2212 \u03b1\u0304i\u03f5 5: z \u223c N (0, I) if i > 1, else z = 0 6: \u03f5pred = \u03f5\u03b8(s\u0301it,at\u22121, \u03c4 s\u0302t\u22121, i) 7: s\u0301i\u22121t,unknown = 1\u221a \u03b1i s\u0301it \u2212 \u03b2i\u221a \u03b1i(1\u2212\u03b1\u0304i) \u03f5pred + \u221a \u03b2iz 8: s\u0301i\u22121t = m\u2299 s\u0301i\u22121t,known + (1\u2212m)\u2299 s\u0301 i\u22121 t,unknown\n9: if u < U and i > 1 then 10: s\u0301it \u223c N ( \u221a 1\u2212 \u03b2i\u22121s\u0301i\u22121t , \u03b2i\u22121I) 11: end if 12: end for 13: end for 14: return s\u0302t = s\u03010t We denote the ground truth state as st, the unobserved state information as (1\u2212m)\u2299 st, and the observed state information as s\u0301t = m \u2299 st, which is incomplete with some masked dimensions. Given the recovered state trajectory \u03c4 s\u0302t\u22121, agent generated action at\u22121, and the known information of the current state s\u0301t, DMBP aims to recover the original state s\u0302t for decision making. Following the inpainting method of Lugmayr et al. (2022), DMBP infills the missing state information through Algorithm 1. For each diffusion timestep, the known region of the state is determined from the forward process (noising process) in line 4, and the unknown region of the state is determined from the reverse process (denoising process) in line 7. To avoid the disharmony of the forward and reverse process generated information, the combined information (in line 8) takes one diffusion forward step in line 10, which is called \u201cresampling\u201d. The resampling is performed by U times for one diffusion timestep. More resampling times may lead to more accurate and harmonious diffusion information at the cost of higher computational load.\n5 EXPERIMENTS\nWe evaluate the proposed DMBP together with several state-of-the-art baseline offline RL algorithms on D4RL Gym benchmark (Fu et al., 2020) against different types of attacks on state observations. The baseline algorithms include Batch Constrained deep Q-learning (BCQ) (Fujimoto et al., 2019), Conservative Q-Learning (CQL) (Kumar et al., 2020), TD3 with Behavior Cloning (TD3+BC) (Fujimoto & Gu, 2021), Diffusion Q-Learning (Diffusion QL) (Wang et al., 2022), and Robust Offline Reinforcement Learning (RORL) (Yang et al., 2022).\nWe train DMBP for 300 epochs (1000 gradient steps with a batch size of 256 for each epoch) with hyperparameters defined in Appendix B.2. We train the baseline algorithms with the corresponding suggested hyperparameters in specific environments and datasets. We perform two tests, on robustness against noised state observations (in Section 5.1) and on robustness against incomplete state observations with unobserved dimension(s) (in Section 5.2). We utilize the same DMBP for all baseline algorithms (cf. the framework in Figure 1), and benchmark their performance against the original baseline algorithms (without DMBP). We present partial results on the D4RL Mujoco benchmark, and the results of other datasets (including medium-expert, medium, and full-replay) and other benchmarks (including Adroit and Franka Kitchen) can be found in Appendix D."
        },
        {
            "heading": "5.1 ROBUSTNESS AGAINST NOISED STATE OBSERVATIONS",
            "text": "Firstly, we evaluate the performance of DMBP in a basic setting with Gaussian noises of standard deviation \u03ba: s\u0303t = st + \u03ba \u00b7 N (0, I). The evaluation results in Table 1 indicate that DMBP can significantly enhance the robustness of all baseline algorithms, especially on the dataset \u201cmediumreplay\u201d, where DMBP strengthened baseline algorithms achieve similar scores as in the corresponding noise-free cases. To demonstrate the powerful denoising effect of DMBP, we visualize a partial trajectory of \"hopper\" during the test in Figure 2.\nWe consider three additional types of noise attacks that are commonly used on state observations, where DMBP-strengthened algorithms also outperform the corresponding baselines (cf. Table 2): i) Uniform random noise distributed inside the \u2113\u221e ball with the norm of \u03ba: s\u0303t = st + \u03ba \u00b7 U(\u2212I, I). ii) Maximum action-difference (adversarial) attack: The noises are selected inside the \u2113\u221e ball with the norm of \u03ba, such that s\u0303t = st + argmaxs\u0303\u2208Bd(s,\u03ba)D(\u03c0\u03d5(\u00b7|s)\u2225\u03c0\u03d5(\u00b7|s\u0303)). Among 20 samples of s\u0303t in the ball, we choose the one with the largest \u2225\u03c0\u03d5(\u00b7|s)\u2212 \u03c0\u03d5(\u00b7|s\u0303)\u22252. iii) Minimum Q-value (adversarial) attack: The noises are selected inside the \u2113\u221e ball with the norm of \u03ba such that, s\u0303t = st + argmins\u0303\u2208Bd(s,\u03ba)Q(s\u0303t, \u03c0\u03d5(\u00b7|s\u0303)). Again, we sample 20 times and choose the one with the minimum Q to be the perturbed state s\u0303t.\nThe latter two adversarial attacks have been considered in the literature (Pinto et al., 2017; Zhang et al., 2020; Yang et al., 2022). For fair comparison, when we use DMBP against adversarial attacks, we first sample 20 noised states s\u0303t and denoise them using DMBP, and then choose the denoised states s\u0302t with the maximum action difference or the minimum Q-value as the perturbed state."
        },
        {
            "heading": "5.2 ROBUSTNESS AGAINST INCOMPLETE STATE OBSERVATIONS WITH UNOBSERVED DIMENSION",
            "text": "We utilize DMBP to recover the missing state information for decision-making. In D4RL benchmark problems, we mask some dimensional state information that cannot be observed by the tested policy (i.e., the masked dimensions of the state are set as 0 for t \u2208 {2, 3, . . . , T}). The baseline algorithms make decisions based on the observed (incomplete) states, and DMBP-improved counterparts take actions according to the recovered states. For each dimension of the state, we make the dimension unobserved and conduct 10 tests. When multiple state dimensions cannot be observed, we randomly select 30 groups of dimensions and conduct 10 tests on each group. The experiment results of CQL, RORL with offline \u201cexpert\u201d and \u201cmedium-replay\u201d datasets are shown in Figure 3. DMBP significantly enhances the performance of all baseline algorithms by accurately predicting the missing state information. On \u201cmedium-replay\u201d datasets, the DMBP strengthened algorithms incur little performance degradation in masked environments, compared with that achieved in the original environments with complete and accurate observations."
        },
        {
            "heading": "5.3 ABLATION STUDY",
            "text": "In Figure 4, we conduct ablation studies on dataset \"hopper-expert-v2\", where algorithms are more prone to error accumulation than in other datasets/environments, to demonstrate the efficacy of the proposed non-Markovian loss function and evaluate the impact of the non-Markovian sampling length (M in Eq. 7). We utilize pre-trained Diffusion QL for decision-making to evaluate the performance of DMBP under the framework in Figure 1. Other hyperparameters and DMBP training follow the basic settings in Appendix B.2 and D.1, respectively.\nWhen M = 1, the DMBP training objective 7 reduces to the classical training objective of diffusion models in Eq. 1 (i.e. LM = 0 in Eq. 7). From the second to the fourth subplots of Figure 4, we observe that the direct adoption of classical conditional diffusion models suffers from severe error accumulation as training proceeds. The proposed non-Markovian training objective significantly enhances the robustness of the baseline RL algorithm against state observation perturbations, especially when the noise scale is large. When M is no less than 6, the performance of DMBP remains almost the same. To expedite the computation, we set M = 6 for the \"hopper\" environment. More ablations studies on neural network structure and condition trajectory lengths (N ) can be found in Appendix C."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this work, we propose the first framework of state-denoising for offline RL against observation perturbations in state-based tasks. Leveraging conditional diffusion models, we develop Diffusion Model-Based Predictor (DMBP) to recover the actual state for decision-making. To reduce the error accumulation during test, we propose a new non-Markovian loss function that minimizes the sum entropy of denoised states along the trajectory. Experiments on D4RL benchmarks demonstrate that the proposed DMBP can significantly enhance the robustness of existing offline RL algorithms against different scales of random noises and even adversarial attacks. The proposed framework is shown to be able to effectively deal with the cases of incomplete state observations (with multiple unobserved dimensions) for state-based RL tasks."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported in part by the General Research Fund (GRF) project 14200720 of the Hong Kong University Grants Committee and the National Natural Science Foundation of China (NSFC) Project 62073273. The authors would like to thank the anonymous reviewers for valuable discussion."
        },
        {
            "heading": "A DERIVATION OF THE TRAINING OBJECTIVE IN EQ. 7",
            "text": "Following the standard setting of diffusion models with Gaussian distributed noised states (SohlDickstein et al., 2015; Ho et al., 2020), below we derive the non-Markovian training objective via conditional diffusion models.\nSuppose that the initial state is perfectly known for the agent, i.e., s1 is known and does not need to be estimated. \u03c4 s\u0302t\u22121 := {s\u0302j | 1 \u2264 j \u2264 t\u2212 1} represents the denoised state trajectory. Our learning objective is to minimize the cross-entropy of the denoised states along the RL trajectory:\nLentropy = T\u2211 t=2 Eq(st) [ \u2212 log p\u03b8(s\u03030t | at\u22121, \u03c4 s\u0302t\u22121) ] .\nWe denote the state in the forward diffusion process as sit and the state in the reverse diffusion process as s\u0303it. We refer p\u03b8(s\u0303 0 t | at\u22121, \u03c4 s\u0302t\u22121) to as p\u03b8(s\u03030t ) below. Following Ho et al. (2020), we adopt the variational lower bound (VLB) to optimize the negative log-likelihood:\nLVLB = T\u2211 t=2 Eq(st) [ \u2212 log p\u03b8(s\u03030t ) +DKL(q(s1:Kt | s0t )\u2225p\u03b8(s\u03031:Kt | s\u03030t ,at\u22121, \u03c4 s\u0302t\u22121) ] =\nT\u2211 t=2 { \u2212Eq [ log p\u03b8(s\u0303 0 t ) ] + Es1:Kt \u223cq(s1:Kt |s0t ) [ log q(s1:Kt | s0t ) p\u03b8(s\u03031:Kt | s\u03030t ,at\u22121, \u03c4 s\u0302t\u22121) ]}\n= T\u2211 t=2 { \u2212Eq [ log p\u03b8(s\u0303 0 t ) ] + Es1:Kt \u223cq(s1:Kt |s0t ) [ log q(s1:Kt | s0t ) p\u03b8(s\u03030:Kt | s\u03030t ,at\u22121, \u03c4 s\u0302t\u22121)p\u03b8(s\u03030t ) ]}\n= T\u2211 t=2 Es1:Kt \u223cq(s1:Kt |s0t ) [ log q(s1:Kt | s0t ) p\u03b8(s\u03030:Kt | s\u03030t ,at\u22121, \u03c4 s\u0302t\u22121) ]\n= T\u2211 t=2 Es1:Kt \u223cq(s1:Kt |s0t )\n[ log\n\u220fK i=1 q(s i t | si\u22121t )\np\u03b8(s\u0303Kt ) \u220fK i=1 p\u03b8(s\u0303 i\u22121 t | s\u0303it,at\u22121, \u03c4 s\u0302t\u22121)\n]\n= T\u2211 t=2 Es1:Kt \u223cq(s1:Kt |s0t ) [ \u2212 log p\u03b8(s\u03030t | s\u03031t ,at\u22121, \u03c4 s\u0302t\u22121) + K\u2211 i=2 log q(si\u22121t | sit, s0t ) p\u03b8(s\u0303 i\u22121 t | s\u0303it,at\u22121, \u03c4 s\u0302t\u22121)\n+ log q(sKt | s0t ) p\u03b8(s\u0303Kt ) ] =\nT\u2211 t=2 Es1:Kt \u223cq(s1:Kt |s0t ) [ \u2212 log p\u03b8(s\u03030t | s\u03031t ,at\u22121, \u03c4 s\u0302t\u22121)\n+ K\u2211 i=2 DKL ( q(si\u22121t | sit, s0t ) \u2225 p\u03b8(s\u0303i\u22121t | s\u0303it,at\u22121, \u03c4 s\u0302t\u22121) ) +DKL ( q(sKt | s0t ) \u2225 p\u03b8(s\u0303Kt ) ) ] ,\n(8) where DKL(\u00b7\u2225\u00b7) represents the Kullback-Leibler divergence and it is guaranteed that LVLB \u2265 Lentropy (Ho et al., 2020). As in Ho et al. (2020), for the first term in the last line of Eq. 8, we let\np\u03b8(s\u0303 0 t | s\u03031t ,at\u22121, \u03c4 s\u0302t\u22121) = N ( s\u03031t\u221a \u03b11 \u2212 \u03b21\u221a \u03b11(1\u2212 \u03b1\u03041) \u03f5\u03b8(s\u0303 1 t ,at\u22121, \u03c4 s\u0302 t\u22121, 1) , \u03b21\u03a3 ) . (9)\nTo directly produce the noised state under Gaussian distributed perturbations, we obtain from the forward diffusion process that\nsit = \u221a \u03b1\u0304ist + \u221a 1\u2212 \u03b1\u0304i\u03f5it, \u03f5it \u223c N (0, I), i \u2208 {1, 2, . . . ,K}. (10)\nFor the expression in Eq. 9, we have\n\u2212 log p\u03b8(s\u03030t | s\u03031t ,at\u22121, \u03c4 s\u0302t\u22121) = log [ (2\u03c0) d 2 | \u03b21\u03a3 | 1 2 ] + 1\n2 \u2225s\u03030t \u2212 s\u03031t\u221a \u03b11\n+ \u03b21\u221a\n\u03b11(1\u2212 \u03b1\u03041) \u03f5\u03b8(s\u0303\n1 t ,at\u22121, \u03c4 s\u0302 t\u22121, 1)\u22252(\u03b21\u03a3)\u22121\n= 1 2 log [ (2\u03c0)d | \u03b21\u03a3 | ] + 1 2\u03b11 \u2225\u03f5\u03b8(s\u03031t ,at\u22121, \u03c4 s\u0302t\u22121, 1)\u2212 \u03f5i=1t \u22252(\u03a3)\u22121 ,\n(11)\nwhere the first equality follows from Eq. 9, and the second equality holds due to Eq. 10.\nFor the second term in the last line of Eq. 8, we let (Ho et al., 2020) q(si\u22121t | sit, s0t ) = N (\u221a\n\u03b1i(1\u2212 \u03b1\u0304i\u22121) 1\u2212 \u03b1\u0304i sit + \u221a \u03b1\u0304i\u22121\u03b2i 1\u2212 \u03b1\u0304i s0t , 1\u2212 \u03b1\u0304i\u22121 1\u2212 \u03b1\u0304i \u03b2i\u03a3\n) , (12)\nand\np\u03b8(s\u0303 i\u22121 t | s\u0303it,at\u22121, \u03c4 s\u0302t\u22121) = N ( s\u0303it\u221a \u03b1i \u2212 \u03b2i\u221a \u03b1i(1\u2212 \u03b1\u0304i) \u03f5\u03b8(s\u0303 i t,at\u22121, \u03c4 s\u0302 t\u22121, i) , 1\u2212 \u03b1\u0304i\u22121 1\u2212 \u03b1\u0304i \u03b2i\u03a3 ) .\n(13)\nUtilizing the Kullback-Leibler divergence for Gaussian distributions in Eq. 12 and Eq. 13, we obtain DKL ( q(si\u22121t | sit, s0t ) \u2225 p\u03b8(s\u0303i\u22121t | s\u0303it,at\u22121, \u03c4 s\u0302t\u22121) ) = 1\n2 \u2225 \u221a \u03b1i(1\u2212 \u03b1\u0304i\u22121) 1\u2212 \u03b1\u0304i sit + \u221a \u03b1\u0304i\u22121\u03b2i 1\u2212 \u03b1\u0304i s0t \u2212 s\u0303it\u221a \u03b1i + \u03b2i\u221a \u03b1i(1\u2212 \u03b1\u0304i) \u03f5\u03b8(s\u0303 i t,at\u22121, \u03c4 s\u0302 t\u22121, i)\u22252( 1\u2212\u03b1\u0304i\u22121 1\u2212\u03b1\u0304i \u03b2i\u03a3 )\u22121 =\n\u03b2i 2\u03b1i(1\u2212 \u03b1\u0304i\u22121) \u2225\u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302t\u22121, i)\u2212 \u03f5it\u22252\u03a3\u22121 ,\n(14) where the last equality follows from Eq. 10.\nAs in (Ho et al., 2020), for the third term in the last line of Eq. 8, we let q(sKt | s0t ) = N (\u221a \u03b1\u0304Ks 0 t , (1\u2212 \u03b1\u0304K)\u03a3 ) . (15)\nIn a similar spirit to the pure Gaussian distributed noise assumption made in Ho et al. (2020), we assume that the observed noised state follows a Gaussian distribution, i.e.,\np\u03b8(s\u0303 K t ) = N ( ms0t , n\u03a3 ) . (16)\nAccording to Eq. 15 and Eq. 16, we obtain the KL divergence of two Gaussian distributions\nDKL ( q(sKt | s0t ) \u2225 p\u03b8(s\u0303Kt ) ) = 1\n2\n[ \u2225( \u221a \u03b1\u0304K \u2212m)s0t\u22252(n\u03a3)\u22121 + d 1\u2212 \u03b1\u0304K n \u2212 d\u2212 d log 1\u2212 \u03b1\u0304K n ] ,\n(17) which is a constant and does not depend on the neural network parameters \u03f5\u03b8.\nSubstituting Eqs. 11 14 17 to the last line in Eq. 8, we can express LVLB as\nLVLB = T\u2211 t=2 Es1\u223cd0,\u03f5it\u223cN (0,I)\n[ Ct +\nK\u2211 i=1 \u03b3i\u2225\u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302t\u22121, i)\u2212 \u03f5it\u22252\u03a3\u22121\n]\n= Es1\u223cd0,\u03f5it\u223cN (0,I) [ T\u2211 t=2 Ct + T\u2211 t=2 K\u2211 i=1 \u03b3i\u2225\u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302t\u22121, i)\u2212 \u03f5it\u22252\u03a3\u22121 ] ,\n(18)\nwhere Ct is a constant for t \u2208 {1, 2, . . . , T \u2212 1}:\nCt = 1\n2\n[ \u2225( \u221a \u03b1\u0304K \u2212m)s0t\u22252(n\u03a3)\u22121 + d 1\u2212 \u03b1\u0304K n \u2212 d\u2212 d log 1\u2212 \u03b1\u0304K n + log [ (2\u03c0)d | \u03b21\u03a3 | ]] ,\nand the coefficients for the norm terms are given by\n\u03b3i =  1 2\u03b11 \u03b2i\n2\u03b1i(1\u2212 \u03b1\u0304i\u22121)\nif i = 1,\notherwise (i \u2208 {2, 3, . . . ,K}).\nFollowing the setting in Ho et al. (2020), we ignore the constant term and the coefficients. The training objective in Eq. 18 can be simplified as\nLsimple(\u03b8) = Es1\u223cd0,\u03f5it\u223cN (0,I) [ T\u2211 t=2 K\u2211 i=1 \u2225\u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302t\u22121, i)\u2212 \u03f5it\u22252 ]\n= Es1\u223cd0,\u03f5it\u223cN (0,I) [ K\u2211 i=1 T\u2211 t=2 \u2225\u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302t\u22121, i)\u2212 \u03f5it\u22252 ]\n= Es1\u223cd0,\u03f5it\u223cN (0,I),i\u223cUK [ T\u2211 t=2 \u2225\u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302t\u22121, i)\u2212 \u03f5it\u22252 ] ,\n(19)\nwhere UK is the uniform distribution over discrete set {1, 2, . . . ,K}, and for \u03c4 s\u0302t\u22121 := {s\u0302j | j \u2264 t\u22121}, we have\ns\u0302j =  s1 fk(s\u0303 k j )\nk\u220f i=1 p\u03b8(s\u0303 i\u22121 j | s\u0303 i j ,aj\u22121, \u03c4 s\u0302 j\u22121)\nif j = 1,\notherwise (j \u2208 {2, . . . , t\u2212 1}).\nFrom 19, it is clear that the simplified target is to make \u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302 t\u22121, i) as close as possible to \u03f5 i t for any t. From the forward noising process in Eq. 10, we have the following approximation\ns\u0302t = s\u0303 0(i) t \u2248 s\u0303it\u221a \u03b1\u0304i\n\u2212 \u221a \u03b1\u0304i\u221a\n1\u2212 \u03b1\u0304i \u03f5\u03b8(s\u0303\ni t,at\u22121, \u03c4 s\u0302 t\u22121, i), t \u2208 {2, . . . , T \u2212 1}. (20)\nTherefore, Eq. 19 can be expressed as: Lsimple(\u03b8) = Es1\u223cd0,\u03f5it\u223cN (0,I),i\u223cUK [ \u2225\u03f5\u03b8(s\u0303i2,a1, s1, i)\u2212 \u03f5i2\u22252\ufe38 \ufe37\ufe37 \ufe38\nterm1\n+\nT\u2211 t=3 \u2225\u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302t\u22121, i)\u2212 \u03f5it\u22252\ufe38 \ufe37\ufe37 \ufe38 term2 ] ,\n(21)\nwhere \u03c4 s\u0302t\u22121 := {s\u0302j | j \u2264 t \u2212 1}, and s\u0302t along the RL trajectory can be calculated through Eq. 20 iteratively from t = 2 to t = T . However, this is time-consuming as the sampling process needs to be looped for (T \u2212 2) times. In addition, the historical state trajectories multiple RL timesteps ago have little impact on the current denoising results. To expedite the computation, we extract a partial trajectory with a window size of N +M (N is the condition trajectory length and M is the sample trajectory length) in Eq. 21, and set the training objective as in Eq. 7:\nL(\u03b8) = Ei\u223cUK ,\u03f5t\u223cN (0,I),(st\u2212N ,...,st+M\u22121)\u2208D\u03bd [ \u2225\u03f5\u03b8(s\u0303it,at\u22121, \u03c4 st\u22121, i)\u2212 \u03f5it\u22252\ufe38 \ufe37\ufe37 \ufe38\nLt\n+\nt+M\u22121\u2211 m=t+1\n\u2225\u03f5\u03b8(s\u0303im,am\u22121, \u03c4 s\u0306m\u22121, i)\u2212 \u03f5im\u22252\ufe38 \ufe37\ufe37 \ufe38 Lm\n] ,\nwhere the state trajectory condition for the predictor \u03f5\u03b8 inLt is the original \u03c4 st\u22121 = {st\u2212N , . . . , st\u22121} from the offline dataset D\u03bd , and the state trajectory condition in Lm can be expressed as \u03c4 s\u0306m\u22121 = {s\u0306j | j \u2208 {m\u2212N, . . . ,m\u2212 1}}, with:\ns\u0306j =  sj1\u221a \u03b1\u0304i [ s\u0303ij \u2212 \u221a 1\u2212 \u03b1\u0304i\u03f5\u03b8(s\u0303ij ,aj\u22121, \u03c4 s\u0306j\u22121, i) ] if j < t, otherwise (j \u2208 {t, . . . , t+M \u2212 2}).\nB IMPLEMENTATION DETAILS"
        },
        {
            "heading": "B.1 DMBP NETWORK STRUCTURE",
            "text": "We implement DMBP based on classical diffusion models (Ho et al., 2020). As discussed in Section 4.1, the naive adoption of MLP neural network structure, which stacks all inputs in one dimension, leads to poor estimation of noise, partially due to the fact that s\u0302t\u22121 is more closely related to s\u0303it than s\u0302j with j < t\u2212 1, and that this information cannot be well captured by the MLP structure. Therefore, we first extract the information from the trajectory with U-net (Ronneberger et al., 2015; Janner et al., 2022) (see \"Information Extraction\" block in Figure 5). Then, the output of U-net U\u03be(s\u0303 i t, \u03c4 s\u0302 t\u22121) is flattened on one dimension and fed into \"Noise Prediction\" block together with the noised current state s\u0303it, last-step agent generated action at\u22121, last-step denoised state s\u0302t\u22121, and the diffusion timestep i.\nIt should be mentioned that directly inputting state and action to neural networks may lead to poor estimation performance, partially due to the much higher dimensionality of state than that of action. We utilize MLP-based neural networks as encoders to match the dimensions of action and states. The final output of our Unet-MLP neural network is recorded as \u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302 t\u22121, i). Detailed hyperparameter settings can be found in Appendix B.2, and ablation studies on network structure can be found in Appendix C."
        },
        {
            "heading": "B.2 DMBP HYPERPARAMETERS AND TRAINING",
            "text": "As discussed in Section 4.1, the proposed framework essentially deals with information with small to medium scale noises instead of generating data from pure noise. We redesign the variance schedule to restrict the noise scale in the diffusion process and limit the total number of diffusion timesteps K for predictor training. The generic neural network and the variance schedule hyperparameters can be found in Table 3.\nSince action and state dimensions vary greatly in different environments, we choose different numbers of neurons in the proposed MLP-based encoders in various environments. The corresponding hyperparameter is recorded as the embedded dimension \u03b6, and the action/state encoder can be\nexpressed as FC(2\u03b6, 12\u03b6) with Mish activations. Besides, for different benchmark environments and datasets, we choose different condition trajectory lengths (N ) and sample trajectory lengths (M ) to achieve the best performance (cf. Table 4).\nAlgorithm 2 DMBP algorithm Require: offline dataset Dv , initialized noise predictor \u03f5\u03b8\n1: for each iteration do 2: Sample trajectory mini-batch B = {(st\u2212N ,at\u2212N , st\u2212N+1, . . . , st+M\u22121)} \u223c Dv 3: Sample uniformly distributed diffusion timestep i \u223c {1, 2, . . . ,K} 4: Sample random Gaussian noise \u03f5it \u223c N (0, I) 5: Produce noised state through s\u0303it = \u221a \u03b1\u0304ist + \u221a 1\u2212 \u03b1\u0304i\u03f5it 6: Get Trajectory \u03c4 s\u0302t\u22121 = {st\u2212N , . . . , st\u22121} 7: Lt = \u2225\u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302t\u22121, i)\u2212 \u03f5it\u22252 8: Recover the noised state through s\u0306t = 1\u221a\u03b1\u0304i [ s\u0303it \u2212 \u221a 1\u2212 \u03b1\u0304i\u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302t\u22121, i)\n] 9: \u03c4 s\u0302t = POP(PUSH(\u03c4 s\u0302 t\u22121, s\u0306t), st\u2212N )\n10: for m = t+ 1, . . . , t+M \u2212 1 do 11: Sample \u03f5im \u223c N (0, I) 12: s\u0303im = \u221a \u03b1\u0304ism + \u221a 1\u2212 \u03b1\u0304i\u03f5im 13: Lm = \u2225\u03f5\u03b8(s\u0303im,am\u22121, \u03c4 s\u0302m\u22121, i)\u2212 \u03f5im\u22252 14: s\u0306m =\n1\u221a \u03b1\u0304i [ s\u0303im \u2212 \u221a 1\u2212 \u03b1\u0304i\u03f5\u03b8(s\u0303im,am\u22121, \u03c4 s\u0302m\u22121, i) ] 15: \u03c4 s\u0302m = POP(PUSH(\u03c4 s\u0302 m\u22121, s\u0306m), s\u0306m\u2212N ) 16: end for 17: Update noise predictor \u03f5\u03b8 by minimizing \u2211t+M\u22121 n=t Ln 18: end for 19: return \u03f5\u03b8"
        },
        {
            "heading": "C ADDITIONAL ABLATION STUDIES",
            "text": "As introduced in Section 5.3, our proposed non-Markovian training objective is essential for avoiding error accumulation in model-based estimation. To demonstrate the efficacy of the proposed approach, we visualize partial trajectory of \"hopper\" during the test in Figure 6.\nWhen the proposed non-Markovian loss function is adopted to train DMBP (see the third subplot of Figure 6), DMBP can give accurate predictions on the original states in a long RL trajectory. Even though prediction errors exist for some RL timesteps, such errors do not collapse future prediction results. However, when the classical Markovian loss function is used to train DMBP (in the fourth subplot of Figure 6), prediction error accumulates, leading to severe prediction errors and significant deviation of the predicted trajectory (away from the actual one).\nWe further conduct ablation studies on the condition trajectory length (N ) in Figure 7. When N = 1, the noise prediction \u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302 t\u22121, i) decays to \u03f5\u03b8(s\u0303 i t,at\u22121, s\u0302t\u22121, i), i.e., the state condition for DMBP reduces to only the last step denoised state s\u0302t\u22121 (instead of the trajectory \u03c4 s\u0302t\u22121). Theoretically, this is enough for predicting the noise information as reinforcement learning problems are always modeled as Markov decision processes (MDP), and most model-based methods follow this assumption (Lin et al., 2017; Janner et al., 2019; Yu et al., 2020; 2021). However, we observe that DMBP performs poorly in the conventional MDP training setting with N = 1. On the contrary, larger condition trajectory length N leads to better prediction of the noised states. This is partially due to the fact that when predicting the current state (s\u0302t), the prediction error of the previous RL timestep denoised state (s\u0302t\u22121) can be compensated by the information extracted on the state trajectory (\u03c4 s\u0302t\u22121). We set N = 4 for the \"hopper\" environment.\nLastly, we conduct the ablation studies on our proposed Unet-MLP structure. As discussed in Section 4.1, directly inputting state trajectories and action into MLP networks leads to poor noise estimation. In Figure 8, we compare the performance of DMBP with MLP network structure and DMBP with the proposed Unet-MLP network structure. For the MLP network, we stack all information in one dimension as input, and the network hyperparameters are set to be the same as the \"Noise Prediction\" module of our Unet-MLP structure (cf. Figure 5). For the Unet-MLP network, we follow the basic settings in Appendix B. We observe that the MLP network converges faster but achieves much worse denoising performance than Unet-MLP."
        },
        {
            "heading": "D EXPERIMENTAL DETAILS",
            "text": ""
        },
        {
            "heading": "D.1 EXPERIMENT SETTINGS",
            "text": "For the baseline RL algorithms (including BCQ (Fujimoto et al., 2019), CQL (Kumar et al., 2020), TD3+BC (Fujimoto & Gu, 2021), Diffusion QL (Wang et al., 2022), and RORL (Yang et al., 2022)), we train them with the suggested hyperparameters for specific environments and datasets. For the sake of fair comparison, we set checkpoints every 10 epochs (1000 gradient steps for each epoch) during the training process of each algorithm, and randomly select 5 from the last 10 checkpoint policies for robustness evaluation.\nAs for the training process of DMBP, we follow the hyperparameter settings in Appendix B.2. We train DMBP for 300 epochs and set the checkpoints every 10 epochs. Analogously, we randomly select 5 from the last 10 checkpoint DMBPs and pair them randomly with the previously selected policies for robustness evaluation.\nWhen DMBP is adopted to improve the robustness against noised state observations (denoising tasks introduced in Section 5.1), we utilize different diffusion start timesteps (k) (see Eq. 2) for different types and scales of noises (cf. Table 5). When DMBP is used for improving the robustness of policy against incomplete state observations with unobserved dimension(s) (the infilling tasks introduced in Section 5.2), we set the diffusion start timestep to be 100 and the resampling times to be 2 for all benchmark experiments."
        },
        {
            "heading": "D.2 ADDITIONAL EXPERIMENTAL RESULTS ON MUJOCO",
            "text": "We provide more experimental results in addition to the robustness evaluation of DMBP against noised state observation (in Section 5.1) and against incomplete state observations with unobserved dimensions (in Section 5.2).\nFollowing the basic setting with Gaussian noises defined in Section 5.1, the full robustness evaluation results of five baseline algorithms (BCQ, CQL, TD3+BC, Diffusion QL, and RORL) on the Mujoco domain with and without DMBP are presented in Table 6, where we include the training datasets \"medium-expert\", \"medium\", and \"full-replay\". In Table 7, we further provide full evaluation results of three other types of noise attacks that have been commonly adopted on state observations (including uniform random noise, maximum action-difference attack, and minimum Q-value attack). It is clear that DMBP greatly enhances the robustness of all baseline algorithms against different types of attacks on state observation for all environments and datasets. Almost all baseline algorithms (without robustness consideration) strengthened by DMBP outperform the SOTA robust offline RL algorithm RORL.\nTo better demonstrate the robustness enhancement of DMBP on the baseline algorithms under different noise scales, we present the robustness evaluation of CQL and RORL against four different attacks in Figure 9.\nFor the robustness evaluation against incomplete state observations with unobserved dimensions (see experiment settings in Section 5.2), we perform the robustness evaluation test under three baseline RL algorithms, BCQ, TD3+BC, and Diffusion QL, on halfcheetah (cf. Figure 10), hopper (cf. Figure 11), and walker2d (cf. Figure 12); the training datasets are chosen to be \"medium-expert\", \"medium\" and \"full-replay\". The evaluation results highlight the robust adaptability of DMBP, i.e., its compatibility with a wide range of offline RL algorithms and diverse datasets."
        },
        {
            "heading": "D.3 EXPERIMENTAL RESULTS ON ADROIT",
            "text": "We further evaluate DMBP in the Adroit domain, where agents are required to control a 24-DoF robotic hand to manipulate a pen, a hammer, a door, and a ball. The action and state spaces in the Adroit domain have much higher dimensions than those in the Mujoco domain, and most of the baseline algorithms perform poorly on \"human\" and \"cloned\" datasets. Therefore, we only present the robustness evaluation results on the \"expert\" dataset as shown in Table 8. Again, DMBP significantly enhances the robustness of all baseline algorithms. Stronger robustness enhancement is observed for \"policy regularization\" offline RL algorithms (i.e., BCQ and Diffusion QL), compared to \"Q-value constrain\" offline RL algorithms (i.e., CQL and RORL).\nFor the robustness evaluation against in-complete state observations (cf. Figure 13), we mask more state dimensions (ranging from 2 to 20), and the evaluation results demonstrate that DMBP performs well even when roughly half of the state dimensions are masked, especially in \"hammer\" and \"door\" environments."
        },
        {
            "heading": "D.4 EXPERIMENTAL RESULTS ON FRANKA KITCHEN",
            "text": "To demonstrate the efficacy of DMBP, we conduct experiments on the Franka Kitchen domain, where agents are required to control a 9-DoF Franka robot in a kitchen environment to finish multiple tasks on common household items. It is noticed that the Franka Kitchen domain has sparse reward environments, i.e., agent scores 25 when it finishes a given task and scores 0 otherwise. Therefore, it is challenging to train an agent in such environments, and it becomes extremely difficult for an agent to obtain any rewards with perturbed state observations. We present the robustness evaluation results of DMBP against noised state observations in Table 9 and against incomplete state observations with unobserved dimensions in Figure 14. We note that DMBP significantly enhances the robustness of all baseline algorithms."
        },
        {
            "heading": "D.5 COMPUTATIONAL COST",
            "text": "In Table 10, we compare the computational cost of DMBP for multiple baseline algorithms on a single machine with one GPU (NVIDIA RTX4090 24GB) and one CPU (AMD Ryzen 9 7950X). For each algorithm, we perform the training on halfcheetah-expert-v2 dataset, and measure the average epoch time (1000 gradient steps for each epoch) and the GPU memory usage over 10 epochs. Although the training of DMBP requires more computing time per epoch, DMBP expedites the convergence (in 200-300 epochs) of the baseline algorithms (which usually converge in 1000-3000 epochs), i.e., the total training time of DMBP is similar to that of the baseline algorithms in comparison.\nWe also perform experiments to measure the decision time consumption for baseline RL algorithms and the DMBP improved ones (see Table 11). We compare the total runtime over 1000 RL timesteps of the baseline algorithms (trained on halfcheetah-expert-v2 dataset) with and without the DMBP denoising framework. For the start diffusion timestep of DMBP in denoising tasks, we select two values of k = 1 and k = 5. DMBP consumes more decision time within an acceptable range.\nTo expedite the training process of DMBP, one can optimize the training schedule by adjusting the number of diffusion steps (Chung et al., 2022; Franzese et al., 2022) and the design of noise scale (Nichol & Dhariwal, 2021) through learning, which have been proven effective in speeding up the convergence rate. Analogously, to expedite the testing process of DMBP, one can employ implicit sampling techniques, such as Song et al. (2020a); Zhang et al. (2022a), which aim to reduce the number of reverse denoising steps required during the testing phase. The proposed methods will be subject to verification in our future work."
        },
        {
            "heading": "D.6 EXPERIMENTAL RESULT ANALYSIS",
            "text": "In our experiments, we find that DMBP achieves better robustness enhancement performance when it is combined with offline RL algorithms based on policy regularization (BCQ, TD3BC, Diffusion QL), as opposed to those based on conservative Q estimation (CQL, RORL). Among the baseline algorithms we evaluate, Diffusion QL demonstrates the best performance when paired with DMBP, while the least improvement exhibits for RORL. For the purpose of understanding the underlying mechanisms, we employ t-SNE (Van der Maaten & Hinton, 2008; Chan et al., 2018) to visualize the distributions of states and actions encountered during the training and testing processes of these offline RL algorithms.\nAs shown in Figure 15, we train various baseline offline RL algorithms using the halfcheetah-expertv2 dataset. The visualization depicts the distributions of states and actions in the dataset, as well as during the testing process without any perturbations, across 20 epochs (each epoch consisting of 1000 RL timesteps). The test results indicate that policy regularization based algorithms tend to replicate the policies observed in the dataset during the testing process. They also exhibit a preference for accessing states that are present in the dataset. This phenomenon is particularly evident in the case of Diffusion QL, where the training and testing sets largely overlap. For conservative Q based algorithms, on the other hand, the overlap between the training and testing sets is less significant (particularly for RORL). By the nature of the Diffusion Models, DMBP has a tendency to recover states that frequently appear in the dataset. This explains why DMBP exhibits better robustness enhancement performance when paired with policy regularization based offline RL algorithms.\nWe note that DMBP demonstrates a certain level of generalization ability for unseen states and actions in the dataset. We visualize the distributions of states and actions for the agent in the clean environments, the agent in the noised environments with the assistance of DMBP, and the agent in the noised environments without the assistance of DMBP (see Figure 16).1 There is a significant overlap between the state and action distributions obtained by the agent in the clean environment (the green scatter plot) and those from the agent assisted by DMBP in the noisy environment (the red scatter plot). This indicates that DMBP is able to accurately infer the original states during testing, even in cases where some states are not observed in the training set. Without the assistance of DMBP, the distribution of actions taken by the agent in the noisy environment, as well as the distribution of states encountered during the process (the purple scatter plot), are completely different from those observed in the clean environment."
        },
        {
            "heading": "E LIMITATIONS",
            "text": "One limitation of our present work is that the noise scale related hyperparameter (i.e., the diffusion start timestep k in Eq. 2) is manually defined when DMBP is deployed for online tests in environments with different scales of noises. In future work, we will consider self-adaptive diffusion start timestep to make DMBP more adaptive for environments with varying noise scales.\nAnother limitation of this work is the lack of validation of DMBP\u2019s performance with normalized state observations (as in Zhang et al. (2020); Yang et al. (2022)). Without normalizing the state observation, adding noise of the same scale may introduce a bias in the corruption\u2019s effect on different dimensions. In future work, we will validate and optimize the performance of DMBP with normalized state observations.\n1Note that the recorded states during the testing process in the noised environment are the original states other than the noised ones."
        }
    ],
    "title": "DMBP: DIFFUSION MODEL-BASED PREDICTOR FOR RO- BUST OFFLINE REINFORCEMENT LEARNING AGAINST STATE OBSERVATION PERTURBATIONS",
    "year": 2024
}