{
    "abstractText": "We study the problem of procedure planning in instructional videos, which aims to make a goal-oriented sequence of action steps given partial visual state observations. The motivation of this problem is to learn a structured and plannable state and action space. Recent works succeeded in sequence modeling of steps with only sequence-level annotations accessible during training, which overlooked the roles of states in the procedures. In this work, we point out that State CHangEs MAtter (SCHEMA) for procedure planning in instructional videos. We aim to establish a more structured state space by investigating the causal relations between steps and states in procedures. Specifically, we explicitly represent each step as state changes and track the state changes in procedures. For step representation, we leveraged the commonsense knowledge in large language models (LLMs) to describe the state changes of steps via our designed chain-of-thought prompting. For state change tracking, we align visual state observations with language state descriptions via cross-modal contrastive learning, and explicitly model the intermediate states of the procedure using LLM-generated state descriptions. Experiments on CrossTask, COIN, and NIV benchmark datasets demonstrate that our proposed SCHEMA model achieves state-of-the-art performance and obtains explainable visualizations.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yulei Niu"
        },
        {
            "affiliations": [],
            "name": "Wenliang Guo"
        },
        {
            "affiliations": [],
            "name": "Long Chen"
        },
        {
            "affiliations": [],
            "name": "Xudong Lin"
        },
        {
            "affiliations": [],
            "name": "Shih-Fu Chang"
        }
    ],
    "id": "SP:82996b7761926f58453700725e93b4062e07f619",
    "references": [
        {
            "authors": [
                "Yazan Abu Farha",
                "Juergen Gall"
            ],
            "title": "Uncertainty-aware anticipation of activities",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops,",
            "year": 2019
        },
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Piotr Bojanowski",
                "Nishant Agrawal",
                "Josef Sivic",
                "Ivan Laptev",
                "Simon Lacoste-Julien"
            ],
            "title": "Unsupervised learning from narrated instruction videos",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Ivan Laptev",
                "Josef Sivic",
                "Simon Lacoste-Julien"
            ],
            "title": "Joint discovery of object states and manipulation actions",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Jing Bi",
                "Jiebo Luo",
                "Chenliang Xu"
            ],
            "title": "Procedure planning in instructional videos via contextual modeling and model-based policy learning",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Mario Bollini",
                "Stefanie Tellex",
                "Tyler Thompson",
                "Nicholas Roy",
                "Daniela Rus"
            ],
            "title": "Interpreting and executing recipes with a cooking robot",
            "venue": "In Experimental Robotics: The 13th International Symposium on Experimental Robotics,",
            "year": 2013
        },
        {
            "authors": [
                "Anthony Brohan",
                "Yevgen Chebotar",
                "Chelsea Finn",
                "Karol Hausman",
                "Alexander Herzog",
                "Daniel Ho",
                "Julian Ibarz",
                "Alex Irpan",
                "Eric Jang",
                "Ryan Julian"
            ],
            "title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "venue": "In 6th Annual Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Chien-Yi Chang",
                "De-An Huang",
                "Danfei Xu",
                "Ehsan Adeli",
                "Li Fei-Fei",
                "Juan Carlos Niebles"
            ],
            "title": "Procedure planning in instructional videos",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Nikita Dvornik",
                "Isma Hadji",
                "Ran Zhang",
                "Konstantinos G Derpanis",
                "Animesh Garg",
                "Richard P Wildes",
                "Allan D Jepson"
            ],
            "title": "Stepformer: Self-supervised step discovery and localization in instructional videos",
            "venue": "arXiv preprint arXiv:2304.13265,",
            "year": 2023
        },
        {
            "authors": [
                "Kiana Ehsani",
                "Hessam Bagherinezhad",
                "Joseph Redmon",
                "Roozbeh Mottaghi",
                "Ali Farhadi"
            ],
            "title": "Who let the dogs out? modeling dog behavior from visual data",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Peter Jansen"
            ],
            "title": "Visually-grounded planning without vision: Language models infer detailed plans from high-level instructions",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Oscar Koller",
                "Hermann Ney",
                "Richard Bowden"
            ],
            "title": "Deep hand: How to train a cnn on 1 million hand images when your data is continuous and weakly labelled",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Oscar Koller",
                "Sepehr Zargaran",
                "Hermann Ney"
            ],
            "title": "Re-sign: Re-aligned end-to-end sequence modelling with deep recurrent cnn-hmms",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Hilde Kuehne",
                "Ali Arslan",
                "Thomas Serre"
            ],
            "title": "The language of actions: Recovering the syntax and semantics of goal-directed human activities",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Mingchen Li",
                "Lifu Huang"
            ],
            "title": "Understand the dynamic world: An end-to-end knowledge informed framework for open domain entity state tracking",
            "venue": "arXiv preprint arXiv:2304.13854,",
            "year": 2023
        },
        {
            "authors": [
                "Xudong Lin",
                "Fabio Petroni",
                "Gedas Bertasius",
                "Marcus Rohrbach",
                "Shih-Fu Chang",
                "Lorenzo Torresani"
            ],
            "title": "Learning to recognize procedural activities with distant supervision",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Sachit Menon",
                "Carl Vondrick"
            ],
            "title": "Visual classification via description from large language models",
            "venue": "arXiv preprint arXiv:2210.07183,",
            "year": 2022
        },
        {
            "authors": [
                "Antoine Miech",
                "Dimitri Zhukov",
                "Jean-Baptiste Alayrac",
                "Makarand Tapaswi",
                "Ivan Laptev",
                "Josef Sivic"
            ],
            "title": "Howto100m: Learning a text-video embedding by watching hundred million narrated video clips",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Antoine Miech",
                "Jean-Baptiste Alayrac",
                "Lucas Smaira",
                "Ivan Laptev",
                "Josef Sivic",
                "Andrew Zisserman"
            ],
            "title": "End-to-end learning of visual representations from uncurated instructional videos",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Bhavana Dalvi Mishra",
                "Lifu Huang",
                "Niket Tandon",
                "Wen-tau Yih",
                "Peter Clark"
            ],
            "title": "Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension",
            "venue": "arXiv preprint arXiv:1805.06975,",
            "year": 2018
        },
        {
            "authors": [
                "Sheshera Mysore",
                "Zach Jensen",
                "Edward Kim",
                "Kevin Huang",
                "Haw-Shiuan Chang",
                "Emma Strubell",
                "Jeffrey Flanigan",
                "Andrew McCallum",
                "Elsa Olivetti"
            ],
            "title": "The materials science procedural text corpus: Annotating materials synthesis procedures with shallow semantic structures",
            "venue": "arXiv preprint arXiv:1905.06939,",
            "year": 2019
        },
        {
            "authors": [
                "Taichi Nishimura",
                "Atsushi Hashimoto",
                "Yoshitaka Ushiku",
                "Hirotaka Kameko",
                "Shinsuke Mori"
            ],
            "title": "State-aware video procedural captioning",
            "venue": "In Proceedings of the 29th ACM International Conference on Multimedia,",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Richard",
                "Hilde Kuehne",
                "Juergen Gall"
            ],
            "title": "Weakly supervised action learning with rnn based fine-to-coarse modeling",
            "venue": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Alexander Richard",
                "Hilde Kuehne",
                "Ahsan Iqbal",
                "Juergen Gall"
            ],
            "title": "Neuralnetwork-viterbi: A framework for weakly supervised video learning",
            "venue": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Marcus Rohrbach",
                "Anna Rohrbach",
                "Michaela Regneri",
                "Sikandar Amin",
                "Mykhaylo Andriluka",
                "Manfred Pinkal",
                "Bernt Schiele"
            ],
            "title": "Recognizing fine-grained and composite activities using hand-centric features and script",
            "year": 2016
        },
        {
            "authors": [
                "Nirat Saini",
                "Hanyu Wang",
                "Archana Swaminathan",
                "Vinoj Jayasundara",
                "Bo He",
                "Kamal Gupta",
                "Abhinav Shrivastava"
            ],
            "title": "Chop & learn: Recognizing and generating object-state compositions",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Keisuke Shirai",
                "Atsushi Hashimoto",
                "Taichi Nishimura",
                "Hirotaka Kameko",
                "Shuhei Kurita",
                "Yoshitaka Ushiku",
                "Shinsuke Mori"
            ],
            "title": "Visual recipe flow: A dataset for learning visual state changes of objects with recipe flows",
            "venue": "In Proceedings of the 29th International Conference on Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Tom\u00e1\u0161 Sou\u010dek",
                "Jean-Baptiste Alayrac",
                "Antoine Miech",
                "Ivan Laptev",
                "Josef Sivic"
            ],
            "title": "Look for the change: Learning object states and state-modifying actions from untrimmed web videos",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Tom\u00e1\u0161 Sou\u010dek",
                "Jean-Baptiste Alayrac",
                "Antoine Miech",
                "Ivan Laptev",
                "Josef Sivic"
            ],
            "title": "Multi-task learning of object state changes from uncurated videos",
            "venue": "arXiv preprint arXiv:2211.13500,",
            "year": 2022
        },
        {
            "authors": [
                "Tom\u00e1\u0161 Sou\u010dek",
                "Dima Damen",
                "Michael Wray",
                "Ivan Laptev",
                "Josef Sivic"
            ],
            "title": "Genhowto: Learning to generate actions and state transformations from instructional videos",
            "venue": "arXiv preprint arXiv:2312.07322,",
            "year": 2023
        },
        {
            "authors": [
                "Aravind Srinivas",
                "Allan Jabri",
                "Pieter Abbeel",
                "Sergey Levine",
                "Chelsea Finn"
            ],
            "title": "Universal planning networks: Learning generalizable representations for visuomotor control",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Jiankai Sun",
                "De-An Huang",
                "Bo Lu",
                "Yun-Hui Liu",
                "Bolei Zhou",
                "Animesh Garg"
            ],
            "title": "Plate: Visuallygrounded planning with transformers in procedural tasks",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Niket Tandon",
                "Bhavana Dalvi Mishra",
                "Joel Grus",
                "Wen-tau Yih",
                "Antoine Bosselut",
                "Peter Clark"
            ],
            "title": "Reasoning about actions and state changes by injecting commonsense knowledge",
            "venue": "arXiv preprint arXiv:1808.10012,",
            "year": 2018
        },
        {
            "authors": [
                "Niket Tandon",
                "Keisuke Sakaguchi",
                "Bhavana Dalvi Mishra",
                "Dheeraj Rajagopal",
                "Peter Clark",
                "Michal Guerquin",
                "Kyle Richardson",
                "Eduard Hovy"
            ],
            "title": "A dataset for tracking entities in open domain procedural text",
            "year": 2011
        },
        {
            "authors": [
                "Yansong Tang",
                "Dajun Ding",
                "Yongming Rao",
                "Yu Zheng",
                "Danyang Zhang",
                "Lili Zhao",
                "Jiwen Lu",
                "Jie Zhou"
            ],
            "title": "Coin: A large-scale dataset for comprehensive instructional video analysis",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Stefanie Tellex",
                "Thomas Kollar",
                "Steven Dickerson",
                "Matthew Walter",
                "Ashis Banerjee",
                "Seth Teller",
                "Nicholas Roy"
            ],
            "title": "Understanding natural language commands for robotic navigation and mobile manipulation",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2011
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Andrew Viterbi"
            ],
            "title": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm",
            "venue": "IEEE transactions on Information Theory,",
            "year": 1967
        },
        {
            "authors": [
                "An-Lan Wang",
                "Kun-Yu Lin",
                "Jia-Run Du",
                "Jingke Meng",
                "Wei-Shi Zheng"
            ],
            "title": "Event-guided procedure planning from instructional videos with text supervision",
            "venue": "arXiv preprint arXiv:2308.08885,",
            "year": 2023
        },
        {
            "authors": [
                "Hanlin Wang",
                "Yilu Wu",
                "Sheng Guo",
                "Limin Wang"
            ],
            "title": "Pdpp: Projected diffusion for procedure planning in instructional videos",
            "venue": "arXiv preprint arXiv:2303.14676,",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Xueqing Wu",
                "Sha Li",
                "Heng Ji"
            ],
            "title": "Openpi-c: A better benchmark and stronger baseline for openvocabulary state tracking",
            "venue": "arXiv preprint arXiv:2306.00887,",
            "year": 2023
        },
        {
            "authors": [
                "Zihui Xue",
                "Kumar Ashutosh",
                "Kristen Grauman"
            ],
            "title": "Learning object state changes in videos: An open-world perspective",
            "venue": "arXiv preprint arXiv:2312.11782,",
            "year": 2023
        },
        {
            "authors": [
                "Zi Yang",
                "Eric Nyberg"
            ],
            "title": "Leveraging procedural knowledge for task-oriented search",
            "venue": "In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2015
        },
        {
            "authors": [
                "Andy Zeng",
                "Maria Attarian",
                "Brian Ichter",
                "Krzysztof Choromanski",
                "Adrian Wong",
                "Stefan Welker",
                "Federico Tombari",
                "Aveek Purohit",
                "Michael Ryoo",
                "Vikas Sindhwani"
            ],
            "title": "Socratic models: Composing zero-shot multimodal reasoning with language",
            "venue": "arXiv preprint arXiv:2204.00598,",
            "year": 2022
        },
        {
            "authors": [
                "Li Zhang",
                "Qing Lyu",
                "Chris Callison-Burch"
            ],
            "title": "Reasoning about goals, steps, and temporal ordering with wikihow",
            "venue": "arXiv preprint arXiv:2009.07690,",
            "year": 2020
        },
        {
            "authors": [
                "Li Zhang",
                "Hainiu Xu",
                "Abhinav Kommula",
                "Niket Tandon",
                "Chris Callison-Burch"
            ],
            "title": "Openpi2. 0: An improved dataset for entity tracking in texts",
            "venue": "arXiv preprint arXiv:2305.14603,",
            "year": 2023
        },
        {
            "authors": [
                "He Zhao",
                "Isma Hadji",
                "Nikita Dvornik",
                "Konstantinos G Derpanis",
                "Richard P Wildes",
                "Allan D Jepson"
            ],
            "title": "P3iv: Probabilistic procedure planning from instructional videos with weak supervision",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Luowei Zhou",
                "Chenliang Xu",
                "Jason J Corso"
            ],
            "title": "Towards automatic learning of procedures from web instructional videos",
            "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Dimitri Zhukov",
                "Jean-Baptiste Alayrac",
                "Ramazan Gokberk Cinbis",
                "David Fouhey",
                "Ivan Laptev",
                "Josef Sivic"
            ],
            "title": "Cross-task weakly supervised learning from instructional videos",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Humans are natural experts in procedure planning, i.e., arranging a sequence of instruction steps to achieve a specific goal. Procedure planning is an essential and fundamental reasoning ability for embodied AI systems and is crucial in complicated real-world problems like robotic navigation (Tellex et al., 2011; Jansen, 2020; Brohan et al., 2022). Instruction steps in procedural tasks are commonly state-modifying actions that induce state changes of objects. For example, for the task of \u201cgrilling steak\u201d, a raw steak would be first topped with pepper after \u201cseasoning the steak\u201d, then placed on the grill before \u201cclosing the lid\u201d, and become cooked pieces after \u201ccutting the steak\u201d. These before-states and after-states reflect fine-grained information like shape, color, size, and location of entities. Therefore, the planning agents need to figure out both the temporal relations between action steps and the causal relations between steps and states.\nInstructional videos are natural resources for learning procedural activities from daily tasks. Chang et al. (2020) proposed the problem of procedure planning in instructional videos, which is to produce a sequence of action steps given the visual observations of start and goal states, as shown in Figure 1 (a). The motivation for this problem is to learn a structured and plannable state and action space (Chang et al., 2020). While earlier works (Chang et al., 2020; Sun et al., 2022; Bi et al., 2021) utilized the full annotations of step sequences and intermediate states as supervision (Figure 1(b)), recent works (Zhao et al., 2022; Wang et al., 2023b) achieved promising results with weaker supervision, where only step sequence annotations are available during training (Figure 1(c)). The weaker supervision setting reduces the expensive cost of video annotations and verifies the necessity of plannable action space. However, as the intermediate visual states are excluded during both training and evaluation, how to comprehensively represent the state information remains an open question.\nThis paper aims to establish a more structured state space for procedure planning by investigating the causal relations between steps and states in procedures. We first ask: how do humans recognize and\nCode: https://github.com/WenliangGuo/SCHEMA\nunderstand and distinguish steps in procedures? Instead of solely focusing on the action information, humans would track state changes in the procedures by leveraging their commonsense knowledge, which is more informative than only looking at the actions. For example, the steak is cooked and as a whole before \u201ccutting\u201d for \u201cgrilling steak\u201d, and becomes cooked pieces after the step. Previous NLP studies have demonstrated the helpfulness of state change modeling in various reasoning tasks, including automatic execution of biology experiments (Mysore et al., 2019), cooking recipes (Bollini et al., 2013), and daily activities (Yang & Nyberg, 2015). Recent studies further explicitly track state changes of entities in procedure texts (Mishra et al., 2018; Tandon et al., 2018; 2020; Zhang et al., 2023; Wu et al., 2023; Li & Huang, 2023). The success of state changes modeling motivates us to investigate the causal relations between steps and states for procedure planning.\nIn this work, we achieve this goal by representing each state-modifying step as state changes. The cores of our method are step representation and state change tracking. For step representation, motivated by the success of large language models (LLMs) in visual recognition (Menon & Vondrick, 2022), we leveraged LLMs to describe the state changes of each step. Specifically, we asked LLMs (e.g., GPT-3.5) to describe the states before and after each step with our designed chain-of-thought prompts (Sec. 3.2). For state changes tracking, as shown in Figure 1(d), we align the visual state observations with language state descriptions via cross-modal contrastive learning. Intuitively, the start visual state should be aligned with the before-state descriptions of the first step, while the goal visual state should be aligned with the after-state descriptions of the last step. As the language descriptions are more discriminative than visual states, we expect the multi-modal alignment to learn a more structured state space. We also take state descriptions as supervisions of intermediate visual states. Finally, the step prediction model is trained in a masked token prediction manner.\nOur main contributions are summarized as follows:\n\u2022 We pointed out that State CHangEs MAtter (SCHEMA) for procedure planning in instructional videos, and proposed a new representation of steps in procedural videos as state changes.\n\u2022 We proposed to track state changes by aligning visual state observations with LLMs-generated language descriptions for a more structured state space and represent mid-states via descriptions.\n\u2022 Our extensive experiments on CrossTask, COIN, and NIV datasets demonstrated the quality of state description generation and the effectiveness of our method."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Procedure Planning (Chang et al., 2020; Zhang et al., 2020) is an essential and fundamental problem for embodied agents. In this work, we followed Chang et al.\u2019s (Chang et al., 2020) formulation of procedure planning. Recent works proposed different approaches for sequence generation, e.g., autoregressive Transformers (Sun et al., 2022), policy learning (Bi et al., 2021), probabilistic modeling (Bi et al., 2021), and diffusion models (Wang et al., 2023b). Interestingly, Zhao et al. (2022) used only language instructions as supervision for procedures and did not require full annotations of intermediate visual states, which highlights the importance of sequence generation for procedure planning. These methods commonly formulated the problem of procedure planning as conditional sequence generation, and the visual observations of states are treated as conditional inputs. However, the motivation of procedure planning is to align the state-modifying actions with their associated state changes, and expect the agents to understand how the state changes given the actions.\nInstructional Videos Analysis. Instructional videos have been a good data source to obtain data for procedural activities (Rohrbach et al., 2016; Kuehne et al., 2014; Zhou et al., 2018; Zhukov et al., 2019; Tang et al., 2019; Miech et al., 2019). Existing research on this topic usually tackles understanding the step-task structures in instructional videos, where the step/task annotation can be either obtained from manual annotation on a relatively small set of videos (Zhou et al., 2018; Zhukov et al., 2019; Tang et al., 2019) or through weak/distant supervision on large unlabeled video data (Miech et al., 2019; 2020; Dvornik et al., 2023; Lin et al., 2022). For example, StepFormer (Dvornik et al., 2023) is a transformer decoder trained with video subtitles (mostly from automatic speech recognition) for discovering and localizing steps in instructional videos. Another recent research (Souc\u030cek et al., 2022a) tackles a more fine-grained understanding of instructional videos, which learns to identify state-modifying actions via self-supervision. However, such approaches require training on the large collection of noisy unlabeled videos, which is expensive and inaccurate enough (Souc\u030cek et al., 2022a).\nTracking State Changes is an essential reasoning ability in complex tasks like question answering and planning. Recent work has made continuous progress on explicitly tracking entity state changes in procedural texts (Mishra et al., 2018; Tandon et al., 2018; 2020; Zhang et al., 2023; Wu et al., 2023; Li & Huang, 2023). Some work in the CV area also investigated the relations between actions and states in videos (Alayrac et al., 2017; Souc\u030cek et al., 2022a;b; Nishimura et al., 2021; Shirai et al., 2022; Xue et al., 2023; Souc\u030cek et al., 2023; Saini et al., 2023). Especially, Nishimura et al. (2021) focused on the video procedural captioning task and proposed to model material state transition from visual observation, which introduces a visual simulator modified from a natural language understanding simulator. Shirai et al. (2022) established a multimodal dataset for object state change prediction, which consists of image pairs as state changes and workflow of receipt text as an action graph. However, the object category is limited to food or tools for cooking. Considering the similarity between procedural texts and instructional videos it is natural to explore state changes in instructional videos. In this work, we investigate state changes in procedural videos for procedure planning."
        },
        {
            "heading": "3 SCHEMA: STATE CHANGES MATTER",
            "text": "In this section, we introduce the details of our proposed framework, State CHangEs MAtter (SCHEMA). We first introduce the background of procedure planning in Sec. 3.1, and present our method in Sec. 3.2\u223c3.4. Specifically, we first provide the details of step representation in Sec. 3.2, model architecture in Sec. 3.3, and training and inference in Sec. 3.4."
        },
        {
            "heading": "3.1 PROBLEM FORMULATION",
            "text": "We follow Chang et al. (2020)\u2019s formulation of procedure planning in instructional videos. As shown in Figure 1(a), given the visual observations of start state s0 and goal state sT , the task is to plan a procedure, i.e., a sequence of action steps \u03c0\u0302 = a1:T , which can transform the state from s0 to sT . The procedure planning problem can be formulated as p(a1:T |s0, sT ). The motivation of this task is to learn a structured and plannable state and action space. For the training supervision, earlier works used full annotations of procedures including both action steps a1:T and their associated visual states, i.e., the states before and after the step, which are annotated as timestamps of videos (Figure 1(b)). Zhao et al. (2022) proposed to use weaker supervision\nwhere only the action step annotations a1:T are available (Figure 1(c)), which reduces the expensive annotation cost for videos. Recent works under this setting show that the plannable action space can be established by conditional sequence generation (Zhao et al., 2022; Wang et al., 2023b). However, there remain open questions about the role of structured state space: why are intermediate states optional for procedure planning? Are there any better representations for steps and visual states?"
        },
        {
            "heading": "3.2 STEP REPRESENTATION AS STATE CHANGES IN LANGUAGE",
            "text": "Our goal is to construct a more structured state space by investigating the causal relations between steps and states in procedures. Motivated by the success of state changes modeling in various reasoning tasks (Bollini et al., 2013; Yang & Nyberg, 2015; Mysore et al., 2019), we represent steps as their before-states and after-states. The state changes can be represented by visual observations or language descriptions. We observed that visual scenes in instructional videos are diverse and noisy, and the details are hard to capture if the object is far from the camera. In addition, the intermediate visual states may not be available due to the high cost of video annotations. Therefore, we represent state changes as discriminative and discrete language descriptions.\nMotivated by Menon & Vondrick (2022), we leveraged large language models (LLMs), such as GPT-3.5 (Brown et al., 2020), to generate language descriptions of states based on their commonsense knowledge. In short, we fed each action step with its high-level task goal to the LLMs, and query several descriptions about the associate states before and after the action step. A baseline prompting following Menon & Vondrick (2022) for state descriptions is:\nQ: What are useful features for distinguishing the states before and after [step] for [goal] in a frame? A: There are several useful visual features to tell the state changes before and after [step] for [goal]:\nHowever, we empirically found that this prompting does not work well for state descriptions, as LLMs disregard the commonsense knowledge behind the step. For example, given the action step \u201cadd onion\u201d and the task \u201cmake kimchi fried rice\u201d, the before-state description is \u201cthe onion was uncut and unchopped\u201d, which is incorrect because the onion should have been cut.\nChain-of-thought Prompting. Aware of the misalignment between action and states, we proposed a chain-of-thought prompting (Wei et al., 2022) strategy to first describe the details of action steps and then describe the state changes according to the details of the steps. Our prompt is designed as:\nFirst, describe the details of [step] for [goal] with one verb. Second, use 3 sentences to describe the status changes of\nobjects before and after [step], avoiding using [verb].\nwhere \u201c[verb]\u201d is the action name (e.g., \u201cpour\u201d) to increase the description diversity. We also provide several examples as context (see appendix). We fixed the number of descriptions as 3 as we empirically found that one or two descriptions cannot cover all the objects and attributes, while more than three descriptions are redundant. Figure 2 illustrates two examples of the generated descriptions based on our prompts. Overall, the step and state descriptions contain more details about attributes, locations, and relations of objects. In the following, for the step name Ai, we denote its step description as dsi , before-state descriptions as {dbi1, \u00b7 \u00b7 \u00b7 , dbiK}, and after-state descriptions as {dai1, \u00b7 \u00b7 \u00b7 , daiK}, where K=3 in our implementation."
        },
        {
            "heading": "3.3 ARCHITECTURE",
            "text": "Figure 3 illustrates the overview of our SCHEMA pipeline. Overall, we break up the procedure planning problem p(a1:T |s0, sT ) into two subproblems, i.e., mid-state prediction and step prediction. Midstate prediction is to estimate the intermediate states s1:(T\u22121) given s0 and sT , i.e., p(s1:(T\u22121)|s0, sT ). Step prediction is to predict the step sequence given the full states, i.e., p(a1:T |s0:T ). We formulate the procedure planning problem as:\np(a1:T |s0, sT ) = \u222b\np(a1:T |s0:T )\ufe38 \ufe37\ufe37 \ufe38 step prediction p(s1:(T\u22121)|s0, sT )\ufe38 \ufe37\ufe37 \ufe38 mid-state prediction ds1:(T\u22121). (1)"
        },
        {
            "heading": "3.3.1 STATE REPRESENTATION",
            "text": "We align visual observations with language descriptions of the same states to learn a structure state space, which will be introduced in Sec. 3.4.\nState encoder. The state encoder takes the video frame as input and outputs its embedding. The state encoder fencs consists of a fixed pre-trained visual feature extractor f\nvis and a trainable projection (two-layer FFN) fprojs . The embedding for state s is obtained by s enc = fencs (s) = f proj s (f vis(s)).\nDescription encoder. Similar to the state encoder, the description encoder fencd consists of a fixed language feature extractor f lang and a trainable projection fprojd . The description encoder takes description d as input and outputs its embedding denc = fencd (d) = f proj d (f lang(d))."
        },
        {
            "heading": "3.3.2 MID-STATE PREDICTION",
            "text": "State Decoder. The state decoder fdecs is an non-autoregressive Transformer (Vaswani et al., 2017). The state decoder predicts the intermediate states s1:(T\u22121) given the start state s0 and the goal state sT . The query for the state decoder is denoted as Qs = [senc0 + p0, p1, \u00b7 \u00b7 \u00b7 , pT\u22121, sencT + pT ], where pi denotes the i-th positional embedding. The state decoder also takes the collection of state descriptions Ds = {db11, \u00b7 \u00b7 \u00b7 , dbCK , da11, \u00b7 \u00b7 \u00b7 , daCK} as the external memory M , where C is the number of step classes and M = fencd (Ds). The external memory interacts with the decoder via cross-attention. The state decoding process to obtain the embeddings s\u0302deci is denoted as:\ns\u0302dec1 , \u00b7 \u00b7 \u00b7 , s\u0302decT\u22121 = fdecs (Qs,M). (2)"
        },
        {
            "heading": "3.3.3 STEP PREDICTION",
            "text": "Step Decoder. The step decoder fdeca is a Transformer model with a similar architecture as the state decoder fdecs . The query combines state embeddings and positional embeddings, denoted as Qa = [senc0 + q0, q1, s\u0302 dec 1 + p2, \u00b7 \u00b7 \u00b7 , s\u0302decT\u22121 + q2T\u22122, q2T\u22121, sencT + q2T ] where qi denotes the\ni-th positional embedding. Similar to the state decoder fdecs , the step decoder f dec a also takes M = fencd (Ds) as the external memory. The step decoding process is denoted as:\na\u0302dec1 , \u00b7 \u00b7 \u00b7 , a\u0302decT = fdeca (Qa,M), (3)\nwhere a\u0302dec1 , \u00b7 \u00b7 \u00b7 , a\u0302decT are the estimated action embeddings. A two-layer feed-forward network (FFN) f clsa is built on top of a\u0302 dec as the step classifier to predict the logits of steps, i.e., a\u0302 = f clsa (a\u0302 dec).\nIn addition to capturing the task information, we establish a task classifier that takes the visual features of start and end states as input and outputs a vector to represent the task information (Wang et al., 2023b;a). The task feature is added to the queries Qs and Qa, where we omitted it for simplicity."
        },
        {
            "heading": "3.4 TRAINING AND INFERENCE",
            "text": "The training process consists of three parts: (1) state space learning that aligns visual observations with language descriptions, (2) masked state modeling for mid-state prediction, and (3) masked step modeling for step prediction. For simplicity, we define the losses with one procedure.\nState Space Learning. Although vision-language models like CLIP (Radford et al., 2021) are pre-trained for vision-language alignment, they cannot be directly used for our problem as the pretraining is not designed for fine-grained state understanding. Therefore, we train two additional projections fprojs and f proj d on top of the visual encoder and language encoder. The added projections also allow us to align other visual features with language features. Given the start state s0 (or end state sT ) and a step label ai, the similarity between s0 (or sT ) and each step Ai is calculated by sim(s0, Ai) = \u2211K j=1 < s enc 0 , d enc,b ij > and sim(sT , Ai) = \u2211K j=1 < s enc T , d enc,a ij >, where < \u00b7, \u00b7 > denotes the dot product. Figure 4 (a) illustrates the idea of structured state space via vision-language alignment. Specifically, we regard the language descriptions with the same state as positive samples, and take descriptions of the other states as negative samples. We define the contrastive loss as:\nLencs = \u2212 log exp(sim(s0, Aa1))\u2211C i=1 exp(sim(s0, Ai))\ufe38 \ufe37\ufe37 \ufe38\nstart state\n\u2212 log exp(sim(sT , AaT ))\u2211C i=1 exp(sim(sT , Ai))\ufe38 \ufe37\ufe37 \ufe38\nend state\n(4)\nMasked State Modeling. The mid-state prediction process can be regarded as a masked state modeling problem, where the intermediate states are masked from the state sequence, and the state decoder recovers the masked states. Since the annotations of intermediate states in videos are not available, we instead use LLMs-generated state descriptions as guidance. In a procedure (s0, a1, s1, \u00b7 \u00b7 \u00b7 , sT\u22121, aT , sT ) where the mid-state st is the after-state for action at and before-state for action at+1, we use the before-state descriptions of at and after-state descriptions of at+1 as the supervision for st. Figure 4(b) illustrates the idea of mid-state learning. We average the description embeddings as the target embedding s\u0302dect for st, and calculate the mean squared error between s\u0302 dec t\nand sdect as the mid-state prediction loss:\nsdect = 1\n2K ( K\u2211 j=1 denc,aat,j + d enc,b at+1,j ), L dec s = T\u22121\u2211 t=1 (s\u0302dect \u2212 sdect )2. (5)\nMasked Step Modeling. Similar to mid-state estimation, the step prediction process can also be regarded as a masked step modeling problem, where the steps are masked from the state-action sequences. The loss is the cross-entropy between ground-truth answers at and predictions a\u0302dect , i.e., Ldeca = \u2211T t=1 \u2212at log a\u0302dect . The final loss combines the above losses, i.e., L = Lencs + Ldecs + Ldeca .\nInference. The non-autoregressive Transformer model may make insufficient use of the temporal relation information among the action steps, i.e., action co-occurrence frequencies. Inspired by the success of Viterbi algorithm(Viterbi, 1967) in sequential labeling works (Koller et al., 2016; 2017; Richard et al., 2017; 2018), we follow Zhao et al. (2022) and conduct the Viterbi algorithm for post-processing during inference. For Viterbi, we obtained the emission matrix based on the probability distribution over [a\u0302dec1 , \u00b7 \u00b7 \u00b7 , a\u0302decT ], and estimated the transition matrix based on action co-occurrence frequencies in the training set. Different from Zhao et al. (2022) that applied Viterbi to probabilistic modeling, we applied Viterbi to deterministic modeling. Specifically, instead of sampling 1,500 generated sequences to estimate the emission matrix (Zhao et al., 2022), we run the feedforwarding only once and use the single predicted probability matrix as the emission matrix, which is simpler and more efficient."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EVALUATION PROTOCOL",
            "text": "Datasets. We evaluate our SCHEMA method on three benchmark instruction video datasets, CrossTask (Zhukov et al., 2019), and COIN (Tang et al., 2019), and NIV (Alayrac et al., 2016). The CrossTask dataset consists of 2,750 videos from 18 tasks depicting 133 actions, with an average of 7.6 actions per video. The COIN dataset contains 11,827 videos from 180 tasks, with an average of 3.6 actions per video. The NIV dataset contains 150 videos with an average of 9.5 actions per video. Following previous works (Chang et al., 2020; Bi et al., 2021; Sun et al., 2022), we randomly select 70% of the videos in each task as the training set and take the others as the test set. We extract all the step sequences at:(t+T\u22121) in the videos as procedures with the time horizon T as 3 or 4.\nFeature Extractors. As our goal for state space learning is to align visual observation and language descriptions, we tried CLIP (Radford et al., 2021) ViT-L/14 model as visual encoder and its associated pretrained Transformer as language encoder. We also follow recent works and use the S3D network (Miech et al., 2019) pretrained on the HowTo100M dataset (Miech et al., 2020) as the visual encoder, and add two projections for vision-language alignment (Sec. 3.4). We empirically found\nthat video-based pre-trained features perform better than image-based pre-trained features. In the following, we used the pretrained S3D model as visual encoder.\nMetrics. Following previous works (Chang et al., 2020; Bi et al., 2021; Sun et al., 2022; Zhao et al., 2022), we evaluate the models on three metrics. (1) Success Rate (SR) is the most strict metric that regards a procedure as a success if all the predicted action steps in the procedure match the ground-truth steps. (2) mean Accuracy (mAcc) calculates the average accuracy of the predicted actions at each step. (3) mean Intersection over Union (mIoU) is the least strict metric that calculates the overlap between the predicted procedure and ground-truth plan, obtained by |{at}\u2229{a\u0302t}||{at}\u222a{a\u0302t}| , where {a\u0302t} is the set of predicted actions and {at} is the set of ground truths. Baselines. We follow previous works and consider the following baseline methods for comparisons. The recent baselines are (1) PlaTe (Sun et al., 2022), which extends DNN and uses a Transformerbased architecture; (2) Ext-GAIL (Bi et al., 2021), which uses reinforcement learning for procedure planning; (3) P3IV (Zhao et al., 2022), which is the first to use weak supervision and proposed a generative adversarial framework; (4) PDPP (Wang et al., 2023b), which is a diffusion-based model for sequence distribution modeling; and (5) EGPP (Wang et al., 2023a), which extracts event information for procedure planning. Details of other earlier baselines can be found in appendix."
        },
        {
            "heading": "4.2 RESULTS",
            "text": "Comparisons with Other Methods. Tables 1 and 3 show the comparisons between our method and others on CrossTask and COIN datasets. Overall, our proposed method outperforms other methods by large margins on all the datasets and metrics. Specifically, for T = 3 on CrossTask, our method outperforms P3IV by over 8% (31.83 vs. 23.34) on the sequence-level metric SR and outperforms EGPP by over 5% (31.83 vs. 26.40). The improvements are consistent with longer procedures (i.e., T = 4, 5, 6), and other two step-level metrics mAcc and mIoU. We also found that both P3IV and EGPP didn\u2019t work well on COIN compared to their performances on CrossTask. Specifically, P3IV outperforms DDN by large margins on CrossTask (e.g., 23.34 vs. 12.18 for SR with T = 3). However, the improvements on COIN become marginal, especially for longer procedures (i.e., 11.32 vs. 11.13 for SR with T = 4). The similar results are observed on EGPP, As comparisons, the SR of our SEPP is consistently larger than P3IV by over 16% for T = 3 and 10% for T = 4. The improvements of mACC and mIoU are also significant. These results demonstrate the better generality and effectiveness of our method on different datasets compared to P3IV and EGPP.\nAn exception case is the recent work PDPP (Wang et al., 2023b) which achieves higher performance on both datasets. However, we argued that they define the start state and end state differently.\nSpecifically, previous works define states as a 2-second window around the start/end time, while PDPP defines the window after the start time and before the end time. Such a definition is more likely to access step information especially for short-term actions, leading to unfair comparisons with other methods. We further compared our method with PDPP under both conventional setting and their setting. The results on Table 2 show that our method outperforms PDPP with T = 3 and T = 4 under both settings, and the main improvement of PDPP comes from the different setting with a small T (e.g., \u223c11% increase of SR on T =3). An interesting observation is that the benefits of different settings to PDPP become marginal with a larger T , which may be credited to their diffusion model.\nAblation Studies. We first conduct ablation studies on CrossTask to validate the effect of two key components, state alignment (Eq. 4 and Figure 4(a)) and mid-state prediction (Sec. 3.3.2). As shown in Table 4, visual-language alignment improves SR by 1.4\u223c2.4% ((c) vs. (a), (d) vs. (b)) for T = 3 on CrossTask. Also, the mid-state prediction module also improves the performance, and the improvements become larger with state alignment (i.e., +0.69% on SR w/o state alignment vs. +1.68% on SR with state alignment). The entire combination (d) achieves the best results. These results verified the impacts of state space learning and mid-state prediction. More ablation studies are in the appendix.\nQualitative Results. Figure 5 illustrates examples of state justifications, i.e., how the model aligns visual state observation with language descriptions. We retrieve top-5 similar descriptions from the corpus of state descriptions. Overall, the retrieved descriptions match the image well, and most of the top similar descriptions are aligned with the visual observations, which improved the explainable state understanding. More visualization results are in the appendix."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we pointed out that State CHangEs MAtter (SCHEMA) for procedure planning in instructional videos, and proposed to represent steps as state changes and track state changes in procedural videos. We leveraged large language models (i.e., GPT-3.5) to generate descriptions of state changes, and align the visual states with language descriptions for a more structured state space. We further decompose the procedure planning into two subproblems, mid-state prediction and step prediction. Extensive experiments further verified that our proposed state representation can promote procedure planning. In the future, potential directions are to establish a benchmark dataset to explicitly track state changes in instructional videos, and to investigate the roles of state changes tracking in other procedural learning tasks like pre-training and future step forecasting. In addition, multimodal procedural planning would be a practical and challenging problem that generates coherent textual plans and visual plans, reflecting the state changes in multiple modalities."
        },
        {
            "heading": "ACKNOWLEDGEMENT",
            "text": "This work is supported by U.S. DARPA KAIROS Program No. FA8750-19-2-1004. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein."
        },
        {
            "heading": "C FURTHER DISCUSSIONS",
            "text": "In this section, we provide further discussions with related works and insights that are not included in the main paper due to page limits.\nLimitations. The limitations of our method are as follows. First, the model may fail to identify state changes if they are not explicitly shown in the video. This is a general challenge for procedure planning models, as the details of objects may be hard to recognize if they are far from the camera. Although we tried to tackle this challenge by associating non-visible state changes with language descriptions, which is learned from the paired vision-text training data, there is no guarantee that the non-visible state changes issue can be totally removed. A potential direction is to generalize the task by taking long-term videos as the input visual observations, so that the model can infer the state changes from video history or ASR narrations. Second, the quality of state descriptions relies on LLMs, which would be a bottleneck of state representation. We will explore more effective and reliable methods to leverage LLMs\u2019 commonsense knowledge, or utilize generative vision-language models (VLMs) for state description generation. Third, our approach is under a close-vocabulary setting. We will explore the open-vocabulary setting in the future.\nDifferences from other mid-state prediction methods. Differences from existing mid-state prediction methods: (1) Compared to PlaTe (Sun et al., 2022) and Ext-GAIL (Bi et al., 2021). First, PlaTe and Ext-GAIL are under the full supervision setting (i.e., visual observations of intermediate states are annotated, shown in Figure 1(b)) while ours is under the weak supervision setting (i.e., mid-state annotations are not available). Second, PlaTe formulates mid-state prediction as P (st|st\u22121, at\u22121, sT ) conditioned on last state st\u22121, last action at\u22121, and end state sT , Ext-GAIL formulates mid-state prediction as P (st|st\u22121, at\u22121, zc) where zc is context information learned from s0 and sT , while we formulate mid-state prediction as P (s1:(T\u22121)|s0, sT ) conditioned on start state s0 and end state sT . Third, PlaTe represents mid-states using extremely low-dimensional (actually 4-d) features in their implementation, which is hard to learn and represent mid-state information. The role of their mid-state predictor is questionable.\n(2) Compared to methods under the weak supervision settings. Recent works like P3IV (Zhao et al., 2022) consider the weak supervision where mid-states are not annotated, and they didn\u2019t predict mid-states in their pipeline. Differently, we leverage LLMs\u2019 commonsense knowledge to transform step into state change descriptions. Therefore, our work rethinks the role and representation style of mid-states, and we expect future works to further investigate the state representation.\nComparisons with related works. Some recent works explored how to extract commonsense information via text descriptions using language models. For example, Socratic Models (Zeng et al., 2022) (SMs) use language models in a zero-shot setting to solve downstream multimodal tasks. The similarity between ours and SMs is that we both leveraged LLMs and prompting to obtain commonsense knowledge for other modalities and multimodal tasks, especially video-related tasks and planning tasks. The main differences are: (1) Goal. SMs aim to embrace the heterogeneity of pretrained models through structured Socratic dialogue, while our goal is to represent steps as state changes via LLMs descriptions for procedure planning. (2) Task. SMs focus on zero-shot multimodal tasks without further training, while we focus on procedure learning in instructional videos with weak supervision and further training; (3) Framework. SMs deliver a zero-shot modular framework that composes multimodal models and LLMs and makes sample-specific API calling, while we only used LLMs once to obtain the generic language descriptions and train another planning model.\nViterbi alrogithm. We use Viterbi for post-processing during inference. We use transition matrix in Viterbi to include the temporal ordering knowledge in the training data, i.e., action co-occurrence frequencies. We empirically found that the training priors help with success rate increase. Our emission matrix estimation is a new contribution to deterministic modeling for procedure planning. Instead of sampling 1,500 generated sequences to estimate the emission matrix (Zhao et al., 2022), we run the feedforwarding only once and use the single predicted probability matrix as the emission matrix, which is simple and more time-efficient. This will be a useful tool for future deterministic modeling works.\nHallucination of LLMs. In the main paper, we observed the hallucination of LLMs using the baseline from Menon & Vondrick (2022), e.g., given the action step \u201cadd onion\u201d and the task goal \u201cmake kimchi fried rice\u201d, the generated description of the state before adding onion is \u201cthe onion was uncut and unchopped\u201d, which is incorrect because the onion should have been cut before being added to the rice. We proposed a chain-of-thought prompting method to first describe more details of the steps and then describe the state changes based on the detailed step descriptions. According to our manual checkup, most of the descriptions are reasonable and match human knowledge. We observed a few failure cases. One common problem is that LLMs may combine two steps as one. For example, for the step \u201ccut strawberries\u201d and task \"make French strawberry cake\", one of the after-state descriptions is \u201cThe strawberries are on the cake\u201d which is incorrect. The reason is that LLMs mix two steps \u201ccut strawberries\u201d and \u201cadd strawberries\u201d to generate the after-state descriptions. A potential solution is to require the LLMs to (1) distinguish different steps by feeding other steps into the prompt as references, (2) consider the temporal relations between steps.\nGeneralization capabilities. Our method can handle variations in action steps, object states, and environmental conditions. For variations in action steps, we evaluated our method with variant lengths of action steps, ranging from 3 to 6. According to the experimental results, our method consistently outperforms state-of-the-art models. For object states and environmental conditions, our evaluation benchmark instructional video datasets cover various topics and domains, including different environments like cooking, housework, and car repairing, and different objects like fruits, drinks, and household items. For example, CrossTask covers 133 types of steps in 18 tasks, and COIN covers 778 types of steps in 180 tasks. The evaluation on these two datasets can reflect the models\u2019 generality on various object states and environmental conditions.\nPotential benefit and drawback of mid-states. The potential benefit of mid-states is the explainability, e.g., extending language-only procedures to multi-modal procedures by adding intermediate visual states. The extension can be realized by (a) retrieving images or video clips from a predefined corpus of images or videos based on feature similarities; (b) generative images via text-to-image generation models (e.g., Stable Diffusion) based on the associated language descriptions. A potential drawback is the impact on the uncertainty modeling, i.e., generating multiple procedures given the same start and goal states. As our motivation is to reduce the high variance of visual observations, language descriptions as the supervision of mid-states would also decrease the uncertainty of sequences. The future direction is to combine mid-states and language descriptions with probabilistic modeling.\nNoisy visual observations. Visual scenes of states are diverse and may have low qualities (i.e., incomplete or noisy) in instructional videos. Considering the high variance of visual observations, we proposed to represent visual states using LLMs descriptions and align the visual observations with language descriptions. We expect the discriminative language description to reduce the variance of visual states and add LLM commonsense knowledge for representation learning.\nScaling up to larger dataset. Different from existing benchmark datasets for procedure planning in instructional videos, larger datasets would have the following requirements: (1) Open-vocabulary setting. Large-scale real-word datasets are often open-vocabulary. For the state description generation process, since LLM is designed for open-vocabulary sequence generation, our generation part can be easily applied to larger datasets. For the step classifier, we can replace the fixed-vocabulary classifier (i.e., two-layer FFN) with a similarity calculation module, i.e., calculating the similarity between output embeddings of step decoder and language step descriptions. (2) Efficient training. Training on large-scale datasets often requires efficient training like distributed computing. Since our state decoder and step decoder are encoder-only Transformer architecture, it is easy to extend the training pipeline to distributed computing. (3) Long-term prediction. As shown in the comparison results with state-of-the-art methods, long-term prediction is still challenging for procedure planning model, but is an essential ability for larger datasets. One potential extension way is to treat our model as an adaptor to transform visual observations into tokens as the inputs to generative model like LLMs or VLMs for long-term sequence generation, which is a popular way to adapt LLMs or VLMs to downstream applications."
        }
    ],
    "title": "SCHEMA: STATE CHANGES MATTER FOR PROCEDURE PLANNING IN INSTRUCTIONAL VIDEOS",
    "year": 2024
}