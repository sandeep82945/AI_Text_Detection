{
    "abstractText": "With the rapid growth of large language models (LLMs), there is increasing demand for memory and computation for LLMs. Recent efforts on post-training pruning of LLMs aim to reduce the model size and computation, yet the performance is still sub-optimal. In this paper, we present a plug-and-play solution for post-training pruning of LLMs. The proposed solution has two innovative components: 1) Relative Importance and Activations (RIA), a new pruning metric that jointly considers the weight and activations efficiently on LLMs; and 2) Channel Permutation, a new approach to maximally preserve important weights under N:M sparsity. The proposed two components can be readily combined to further enhance the N:M structured pruned LLMs. Our empirical experiments show that RIA alone can already surpass all existing post-training pruning methods on prevalent LLMs, e.g., LLaMA ranging from 7B to 65B. Furthermore, N:M structured pruning with channel permutation can even outperform the original LLaMA2 70B on zero-shot tasks, together with practical speed-up on specific hardware.",
    "authors": [],
    "id": "SP:e3cf7de7b7f839735e0e061247c768ff56a173b9",
    "references": [
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Utku Evci",
                "Trevor Gale",
                "Jacob Menick",
                "Pablo Samuel Castro",
                "Erich Elsen"
            ],
            "title": "Rigging the lottery: Making all tickets winners",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Elias Frantar",
                "Dan Alistarh"
            ],
            "title": "Spdy: Accurate pruning with speedup guarantees",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Elias Frantar",
                "Dan Alistarh"
            ],
            "title": "Sparsegpt: Massive language models can be accurately pruned in one-shot",
            "year": 2023
        },
        {
            "authors": [
                "Elias Frantar",
                "Saleh Ashkboos",
                "Torsten Hoefler",
                "Dan Alistarh"
            ],
            "title": "Gptq: Accurate post-training quantization for generative pre-trained transformers",
            "venue": "arXiv preprint arXiv:2210.17323,",
            "year": 2022
        },
        {
            "authors": [
                "Leo Gao",
                "Jonathan Tow",
                "Stella Biderman",
                "Sid Black",
                "Anthony DiPofi",
                "Charles Foster",
                "Laurence Golding",
                "Jeffrey Hsu",
                "Kyle McDonell",
                "Niklas Muennighoff",
                "Jason Phang",
                "Laria Reynolds",
                "Eric Tang",
                "Anish Thite",
                "Ben Wang",
                "Kevin Wang",
                "Andy Zou"
            ],
            "title": "A framework for few-shot language model evaluation",
            "venue": "URL https://doi.org/10.5281/zenodo",
            "year": 2021
        },
        {
            "authors": [
                "Song Han",
                "Jeff Pool",
                "John Tran",
                "William Dally"
            ],
            "title": "Learning both weights and connections for efficient neural network",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Babak Hassibi",
                "David G Stork",
                "Gregory J Wolff"
            ],
            "title": "Optimal brain surgeon and general network pruning",
            "venue": "In IEEE international conference on neural networks,",
            "year": 1993
        },
        {
            "authors": [
                "Duc NM Hoang",
                "Shiwei Liu",
                "Radu Marculescu",
                "Zhangyang Wang"
            ],
            "title": "Revisiting pruning at initialization through the lens of ramanujan graph",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Itay Hubara",
                "Brian Chmiel",
                "Moshe Island",
                "Ron Banner",
                "Joseph Naor",
                "Daniel Soudry"
            ],
            "title": "Accelerated sparse neural training: A provable and efficient method to find n: m transposable masks",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Harold W Kuhn"
            ],
            "title": "The hungarian method for the assignment problem",
            "venue": "Naval Research Logistics Quarterly,",
            "year": 1955
        },
        {
            "authors": [
                "Yann LeCun",
                "John Denker",
                "Sara Solla"
            ],
            "title": "Optimal brain damage",
            "venue": "Advances in neural information processing systems,",
            "year": 1989
        },
        {
            "authors": [
                "Namhoon Lee",
                "Thalaiyasingam Ajanthan",
                "Philip HS Torr"
            ],
            "title": "Snip: Single-shot network pruning based on connection sensitivity",
            "venue": "arXiv preprint arXiv:1810.02340,",
            "year": 2018
        },
        {
            "authors": [
                "Jiajun Li",
                "Ahmed Louri"
            ],
            "title": "Adaprune: An accelerator-aware pruning technique for sustainable cnn accelerators",
            "venue": "IEEE Transactions on Sustainable Computing,",
            "year": 2021
        },
        {
            "authors": [
                "Ji Lin",
                "Jiaming Tang",
                "Haotian Tang",
                "Shang Yang",
                "Xingyu Dang",
                "Song Han"
            ],
            "title": "Awq: Activation-aware weight quantization for llm compression and acceleration",
            "venue": "arXiv preprint arXiv:2306.00978,",
            "year": 2023
        },
        {
            "authors": [
                "Shiwei Liu",
                "Tianlong Chen",
                "Xiaohan Chen",
                "Zahra Atashgahi",
                "Lu Yin",
                "Huanyu Kou",
                "Li Shen",
                "Mykola Pechenizkiy",
                "Zhangyang Wang",
                "Decebal Constantin Mocanu"
            ],
            "title": "Sparse training via boosting pruning plasticity with neuroregeneration",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zichang Liu",
                "Jue Wang",
                "Tri Dao",
                "Tianyi Zhou",
                "Binhang Yuan",
                "Zhao Song",
                "Anshumali Shrivastava",
                "Ce Zhang",
                "Yuandong Tian",
                "Christopher Re"
            ],
            "title": "Deja vu: Contextual sparsity for efficient llms at inference time",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Xinyin Ma",
                "Gongfan Fang",
                "Xinchao Wang"
            ],
            "title": "Llm-pruner: On the structural pruning of large language models",
            "venue": "arXiv preprint arXiv:2305.11627,",
            "year": 2023
        },
        {
            "authors": [
                "Mitchell P. Marcus",
                "Beatrice Santorini",
                "Mary Ann Marcinkiewicz"
            ],
            "title": "Building a large annotated corpus of English: The Penn Treebank",
            "venue": "Computational Linguistics,",
            "year": 1993
        },
        {
            "authors": [
                "Stephen Merity",
                "Caiming Xiong",
                "James Bradbury",
                "Richard Socher"
            ],
            "title": "Pointer sentinel mixture models, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Asit Mishra",
                "Jorge Albericio Latorre",
                "Jeff Pool",
                "Darko Stosic",
                "Dusan Stosic",
                "Ganesh Venkatesh",
                "Chong Yu",
                "Paulius Micikevicius"
            ],
            "title": "Accelerating sparse deep neural networks",
            "venue": "arXiv preprint arXiv:2104.08378,",
            "year": 2021
        },
        {
            "authors": [
                "Decebal Constantin Mocanu",
                "Elena Mocanu",
                "Peter Stone",
                "Phuong H Nguyen",
                "Madeleine Gibescu",
                "Antonio Liotta"
            ],
            "title": "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science",
            "venue": "Nature communications,",
            "year": 2018
        },
        {
            "authors": [
                "Jeff Pool",
                "Chong Yu"
            ],
            "title": "Channel permutations for n: M sparsity",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "year": 2019
        },
        {
            "authors": [
                "Victor Sanh",
                "Thomas Wolf",
                "Alexander Rush"
            ],
            "title": "Movement pruning: Adaptive sparsity by finetuning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Mingjie Sun",
                "Zhuang Liu",
                "Anna Bair",
                "J Zico Kolter"
            ],
            "title": "A simple and effective pruning approach for large language models",
            "venue": "arXiv preprint arXiv:2306.11695,",
            "year": 2023
        },
        {
            "authors": [
                "Philippe Tillet",
                "Hsiang-Tsung Kung",
                "David Cox"
            ],
            "title": "Triton: an intermediate language and compiler for tiled neural network computations",
            "venue": "In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages,",
            "year": 2019
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "arXiv preprint arXiv:2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Guangxuan Xiao",
                "Ji Lin",
                "Mickael Seznec",
                "Hao Wu",
                "Julien Demouth",
                "Song Han"
            ],
            "title": "Smoothquant: Accurate and efficient post-training quantization for large language models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Geng Yuan",
                "Xiaolong Ma",
                "Wei Niu",
                "Zhengang Li",
                "Zhenglun Kong",
                "Ning Liu",
                "Yifan Gong",
                "Zheng Zhan",
                "Chaoyang He",
                "Qing Jin"
            ],
            "title": "Mest: Accurate and fast memory-economic sparse training framework on the edge",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models",
            "venue": "arXiv preprint arXiv:2205.01068,",
            "year": 2022
        },
        {
            "authors": [
                "Y Zhang",
                "J Zhao",
                "W Wu",
                "A Muscoloni",
                "CV Cannistraci"
            ],
            "title": "Epitopological sparse ultra-deep learning: A brain-network topological theory carves communities in sparse and percolated hyperbolic anns",
            "year": 2023
        },
        {
            "authors": [
                "Yuxin Zhang",
                "Mingbao Lin",
                "Zhihang Lin",
                "Yiting Luo",
                "Ke Li",
                "Fei Chao",
                "Yongjian Wu",
                "Rongrong Ji"
            ],
            "title": "Learning best combination for efficient n: M sparsity",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Michael Zhu",
                "Suyog Gupta"
            ],
            "title": "To prune, or not to prune: exploring the efficacy of pruning",
            "year": 2021
        },
        {
            "authors": [
                "pruner (Ma"
            ],
            "title": "2023), they introduce a skip-layer pruning strategy which skips the pruning process for the first several layers and the last few layers. To ensure a fair comparison, we implement a uniform skip strategy for all algorithms listed in Table 10. Additionally, for a more comprehensive evaluation, we conduct tests on pruning all layers of the model",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "With the rapid growth of large language models (LLMs), there is increasing demand for memory and computation for LLMs. Recent efforts on post-training pruning of LLMs aim to reduce the model size and computation, yet the performance is still sub-optimal. In this paper, we present a plug-and-play solution for post-training pruning of LLMs. The proposed solution has two innovative components: 1) Relative Importance and Activations (RIA), a new pruning metric that jointly considers the weight and activations efficiently on LLMs; and 2) Channel Permutation, a new approach to maximally preserve important weights under N:M sparsity. The proposed two components can be readily combined to further enhance the N:M structured pruned LLMs. Our empirical experiments show that RIA alone can already surpass all existing post-training pruning methods on prevalent LLMs, e.g., LLaMA ranging from 7B to 65B. Furthermore, N:M structured pruning with channel permutation can even outperform the original LLaMA2 70B on zero-shot tasks, together with practical speed-up on specific hardware."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Recent research on Large Language Models (LLMs) has garnered significant interest. These LLMs, characterized by their vast number of parameters, have exhibited remarkable proficiency across a wide range of tasks. However, deploying such models poses challenges due to their substantial size, computational demands, and execution time. To address this, several methods for network compression have been explored, including model quantization (Frantar et al., 2022; Xiao et al., 2023; Lin et al., 2023) which is the most popular and mature method, and network sparsity (LeCun et al., 1989; Hassibi et al., 1993; Mocanu et al., 2018; Sun et al., 2023; Frantar & Alistarh, 2023).\nUnlike quantization techniques, which adjust the precision of weights or activations to compress the network, network sparsity primarily targets the elimination of redundant or useless weights within models. Despite its potential, the exploration of sparsity in LLMs remains somewhat limited. Generally, there are three primary ways to induce sparsity in neural networks: 1) sparse training (Lee et al., 2018; Mocanu et al., 2018; Evci et al., 2020; Sanh et al., 2020; Yuan et al., 2021; Hoang et al., 2022; Zhang et al., 2023); 2) pruning-aware training (Han et al., 2015; Liu et al., 2021); and 3) posttraining pruning (PTP) (Hassibi et al., 1993; Li & Louri, 2021; Frantar & Alistarh, 2023; Sun et al., 2023). However, both sparse training and during-training pruning require multiple rounds of iterative training, which is computationally costly and time-consuming, especially for LLMs. Therefore, one-shot PTP on a well-pre-trained model represents a more reasonable approach for LLMs.\nThe primary challenge associated with post-training pruning lies in the substantial performance degradation compared to dense models. Current methods, like SparseGPT (Frantar & Alistarh, 2023) and Wanda (Sun et al., 2023), exhibit promising results in unstructured pruning. Nonetheless, to achieve practical speed-up, it is favored to conduct N:M structured pruning, which can be supported on specific hardware with sparse matrix multiplications (Mishra et al., 2021). These prior methods (Sun et al., 2023; Frantar & Alistarh, 2023) under the N:M sparsity suffer from significant performance drops, thereby restricting their applications in practice. Recently, (Pool & Yu, 2021) has introduced an input channel permutation method using the greedy search, which can boost the performance under the N:M sparsity. However, the greedy search is time-consuming on LLMs and thus can be hardly applied.\nIn this paper, we introduce a plug-and-play post-training pruning method for LLMs. Specifically, the method comprises two key components. First, we introduce Relative Importance and Activation (RIA), a new pruning metric for LLM pruning. We show that prior pruning metrics (Frantar & Alistarh, 2023; Sun et al., 2023) tend to prune away entire channels of network weights, which is undesired for neither unstructured nor N:M structured pruning. Instead, RIA jointly considers the input and output channels of weight, together with the activation information, and it turns out to effectively mitigate such issues. Second, we also consider a better way to turn LLMs into N:M sparsity. Unlike existing methods that directly convert the weight matrix to N:M sparsity, we propose channel permutation to properly permute the weight channels so as to maximally preserve the important weights under N:M structures. Finally, the proposed RIA and channel permutation can be readily combined, leading to an efficient and plug-and-play approach for the real-world acceleration of sparse LLMs inference. We decide to call our method \u201cplug-and-play\u201d because in the zeroshot experiment, it demonstrated performance similar to original dense model without need any additional finetining or retraining whereas, the other post-training pruning method such as Sparsegpt and Wanda are not \u201cPlug-and-play\u201d because they don\u2019t have channel permutation.\nExtensive evaluation on open-sourced LLMs (e.g., LLaMA (Touvron et al., 2023a), LLaMA-2 (Touvron et al., 2023b), and OPT (Zhang et al., 2022a)) demonstrates that RIA outperforms SOTA oneshot post-training pruning (PTP) methods in both unstructured sparsity and N:M sparsity scenario. Additionally, channel permutation can be efficiently scaled to LLMs with over 70B parameters within 2 hours, yet recover a lot of the performance drop caused by N:M constraint and even outperforms 3 out of 5 datasets in comparison to the dense models involved in the article. By employing RIA and channel permutation, LLMs can undergo a smooth transition into N:M constraints. This ensures compatibility with hardware requirements while maintaining the performance of the pruned model intact."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Post-Training Pruning. PTP methods trace their roots to OBD (LeCun et al., 1989), which employs the Hessian Matrix for weight pruning. OBS (Hassibi et al., 1993) further refines the approach by adjusting remaining weights to minimize loss changes during pruning. The advent of Large Language Models (LLMs) has sparked interest in leveraging the Hessian Matrix for pruning, exemplified by works like AdaPrune (Li & Louri, 2021) and Iterative Adaprune (Frantar & Alistarh, 2022) targeting BERT (Devlin et al., 2018). However, weight reconstruction via Hessian Matrix inverse incurs substantial computational complexity, at O(N4). SparseGPT (Frantar & Alistarh, 2023) reduces this complexity to O(N3), while Wanda (Sun et al., 2023) leverages input activations for efficient one-shot PTP with reduced pruning time and comparable performance to SparseGPT. Our proposed method, Relative Importance and Activations (RIA) inherits the time-saving advantages of Wanda while simultaneously enhancing the performance of the pruned LLMs. Note that the method proposed in this article is designed for application without retraining and finetuning, which differentiates it from techniques like Dejavu (Liu et al., 2023) that require additional training steps. Consequently, our comparisons are focused on methods that do not involve retraining, such as Magnitude Pruning (Zhu & Gupta, 2017), Wanda (Sun et al., 2023), and SparseGPT (Frantar & Alistarh, 2023).\nN:M Sparsity. Recently, NVIDIA has introduced N:M constraint sparsity (Mishra et al., 2021) as a method to compress neural network models while preserving hardware efficiency. This N:M sparsity constraint stipulates that at least N out of every contiguous M element must be set to zero, thereby accelerating matrix-multiply-accumulate instructions. For instance, a 2:4 constraint ratio results in 50% sparsity, effectively doubling the model\u2019s inference speed when utilizing the NVIDIA Ampere GPU architecture. However, directly applying SOTA unstructured pruning methods to meet the N:M sparsity often leads to a noticeable decline in performance, as demonstrated in Table 5. Some approaches (Hubara et al., 2021; Zhou et al., 2021; Zhang et al., 2022b) suggest fine-tuning pruned models to recover capacity, but this is prohibitively expensive for Large Language Models (LLMs).\nMatrix Permutation. N:M sparsity primarily targets the application of sparsity in the input channel dimension. (Pool & Yu, 2021) presents a permutation method to identify the optimal permutation of input channels through an exhaustive greedy search and an escape phase to navigate local minima. However, this greedy approach becomes impractically time-consuming when applied to LLMs due to their extensive linear layers. In this study, we leverage the specific characteristics of LLMs and\npropose a Channel Permutation strategy. This approach transforms the permutation problem into a well-known combinatorial optimization problem, the linear sum assignment (LSA). We efficiently solve this problem using the Hungarian algorithm, resulting in a significant reduction in computation time."
        },
        {
            "heading": "3 LLM PRUNING WITH RELATIVE IMPORTANCE AND ACTIVATIONS",
            "text": ""
        },
        {
            "heading": "3.1 POST-TRAINING PRUNING: PRELIMINARIES",
            "text": "Post-training pruning (PTP) typically starts from the pre-trained network, removes redundant parameters, and does not need end-to-end fine-tuning. Unlike training-based pruning methods (Mocanu et al., 2018; Sanh et al., 2020; Zhang et al., 2023), PTP is fast, resource-saving, and therefore preferred for compressing LLMs (Frantar & Alistarh, 2023; Sun et al., 2023). PTP is currently prevalent in unstructured pruning and N:M structured pruning, which is also the main focus of this paper. It is less applied in structured pruning due to a larger performance drop.\nA common approach to achieve PTP is layer-wise pruning, i.e., minimizing the discrepancy square error between the dense and pruned model layer-by-layer recursively. Specifically, we denote the input of the l-th linear layer as Xl, and weight Wl \u2208 Rr\u00d7c, where r and c represents the number of output and input channels respectively. Our primary goal is to find the pruning mask Ml \u2208 {0, 1}r\u00d7c that minimizes the \u21132 distance error between the original and pruned layer. Therefore, the objective can be formally expressed as follows:\nargmin Ml\n||WlXl \u2212 (Ml \u2299 Wl) \u00b7 Xl||22, s.t.. ||Ml||0 \u2264 k, (1)\nwhere k represents the number of remaining weights determined by the pruning ratio, and \u2225 \u00b7 \u22250 is the \u21130-norm (i.e., the number of non-zero elements). To solve Ml in Equation 1, there are various pruning metrics, e.g., magnitude-based pruning that mask the weight below a certain threshold. Nonetheless, we show that these prior pruning metrics have intrinsic drawbacks, as discussed in the following section. Optionally, the weight Wl in Equation 1 can also be reconstructed via closedform updates (Frantar & Alistarh, 2023); see Appendix D for more discussions."
        },
        {
            "heading": "3.2 RELATIVE IMPORTANCE: A NEW PRUNING METRIC",
            "text": "We present relative importance (dubbed as RI), a new metric for LLM pruning. We find that prevalent PTP methods tend to suffer from channel corruption, i,e., the entire input or output channel is pruned away. This is akin to node removal in network science terms, and it significantly diminishes the performance of Large Language Models (LLMs), in a manner similar to structured pruning. To illustrate this, Figure 1 shows a linear layer with W \u2208 R4\u00d74. Magnitude-based pruning with 50% unstructured sparsity will corrupt W4\u2217, i,e, the 4-th output channel. In practice, we find similar issues also exist in other prevalent pruning metrics, e.g., Wanda (Sun et al., 2023) prunes around 600 channels pruned out of 5120 channels, with more than 10% channels corrupted. Given that welltrained LLMs contain unique information in the input and output channels, it is critical to avoid channel corruption in post-training pruning. We explain the phenomenon of channel corruption in detail in Appendix E.\nTo mitigate such issues, the proposed relative importance (RI) aims to re-evaluate the importance of each weight element Wij based on all connections that originate from the input neuron i or lead to the output neuron j. Specifically, the relative importance for Wij can be calculated as:\nRIij = |Wij |\u2211 |W\u2217j | + |Wij |\u2211 |Wi\u2217| , (2)\nwhere \u2211 |W\u2217j | sums over the absolute values of input channel j, and similarly \u2211\n|Wi\u2217| for the sum of oinputput channels i. The resulting score RIij offers insight into the relative importance of weight Wij in the context of its connections to neurons i and j."
        },
        {
            "heading": "3.3 INCORPORATING ACTIVATIONS INTO RELATIVE IMPORTANCE",
            "text": "The proposed relative importance can be further combined with activations to better assess the weight significance, dubbed as relative importance and activation (RIA). Recent findings (Xiao et al., 2023) show that the occurrence of activation outliers has become a well-known issue in quantizing LLMs. Our visualizations on LLaMA-7B and LLaMA-13B also confirm these outliers; see Figure 7 and Figure 8 for details.\nMoreover, we find that activation outliers persist regardless of the dataset or parts of the model. To see this, we calculate the Spearman\u2019s Rank Correlation Coefficient of activations between different datasets. From Figure 2, it can be found that pair-wise correlations of activations exhibit positive values and similar trends across different layers and Transformer modules. We offer evidence that the Spearman Rank correlation between pairs of the activations of different datasets is positive. This positivity is a necessary condition to incorporate the activation into our RIA formula. And indeed it is always satisfied.\nBuilt upon Equation 2, for each element Wij , RIA further combines \u21132-norm of activations ||Xi||2 as follows:\nRIAij = RIij \u00d7 (||Xi||2)a = ( |Wij |\u2211 |W\u2217j | + |Wij |\u2211 |Wi\u2217| )\u00d7 (||Xi||2)a, (3)\nwhere a power factor a is introduced to control the strength of activations. Our empirical results show that a = 0.5 works generally well for different LLMs and datasets."
        },
        {
            "heading": "4 TURNING INTO N:M SPARSITY",
            "text": "This section studies how to turn LLMs into N:M sparsity with the pruning metric. N:M sparsity is usually favored by post-training pruning given its practical speed-up on specific hardware. Unlike existing solutions that directly convert to the N:M format, we propose channel permutation, a new approach that better leverages the pruning metrics by efficiently permuting the weight channels. Channel permutation can work seamlessly with the RIA metric in Section 3, as well as other prevalent methods such as (Frantar & Alistarh, 2023; Sun et al., 2023)."
        },
        {
            "heading": "4.1 N:M STRUCTURED PRUNING: FORMULATION",
            "text": "We begin by revisiting unstructured PTP in Equation 1. Without loss of generality, we denote the weight importance score as S \u2208 Rr\u00d7c, which can be either RIA or other pruning metrics. It is an\nNP-hard problem (Frantar & Alistarh, 2023) to find the optimal M \u2208 {0, 1}r\u00d7c in Equation 1. A common surrogate is to maximize the sum of retained weight importance scores as follows:\nargmax M\n\u2211 M \u2299 S, s.t. ||M||0 \u2264 k. (4)\nSimilarly, for N:M sparsity, every N out of M contiguous elements is zero along each output channel, and the objective is re-formulated as:\nargmax M r\u2211 i=0 c m\u2211 k=0 \u2211 (M \u2299 S)i,km:(k+1)m, s.t. ||Mi,km:(k+1)m||0 \u2264 m\u2212 n. (5)\nA simple solution to Equation 5 in existing works (Frantar & Alistarh, 2023; Sun et al., 2023) is directly setting the mask Mi,km:(k+1)m of top N elements in Si,km:(k+1)m to 1, and 0 otherwise. Nonetheless, this usually leads to sub-optimal solutions. As illustrated in Figure 3, some input channels with similar scores, either large or small, might get stuck in the same weight block Wkm:(k+1)m,\u2217. As a consequence, some large weights might be pruned by mistake while small weights might be preserved instead."
        },
        {
            "heading": "4.2 CHANNEL PERMUTATION FOR IMPROVED N:M SPARSITY",
            "text": "To address the aforementioned challenge, we present a new channel permutation (CP) approach that gives better N:M structures. Note that permuting the input channels of weight can lead to different importance scores S. We thus introduce an additional column permutation matrix P for S, such that the sum of retained weight importance can be further maximized:\nargmax M,P r\u2211 i=0 c m\u2211 k=0 \u2211 (M \u2299 (S P))i,km:(k+1)m, s.t. ||Mi,km:(k+1)m||0 \u2264 m\u2212 n. (6)\nFor ease of presentation in the following, we denote the block of the weight matrix as W\u2217,km:(k+1)m \u2208 Rr\u00d7m, where k \u2208 {1, ...,K} and K is the number of blocks. N:M pruning thus occurs row-wisely within each block, i.e., only n values are preserved out of m elements for Wi,km:(k+1)m. Based on the notation, the proposed channel permutation mainly includes the following two steps.\nStep 1: Heuristic Channel Allocation. We first calculate the sum of weight importance for each input channel. These channels are then sorted to be allocated in K blocks. To maximally retain the important channels in each block with N:M sparsity, we use a heuristic allocation strategy. Given K blocks, we alternatively allocate every top-K input channel into each block, and this process is repeated for m times until all channels are allocated. An example is illustrated in Figure 3. Given 8 input channels and 4 output channels, there are 2 blocks under 2:4 sparsity. The top-1 and and top-2 channels are allocated to block 1 and block 2, and similarly for the rest input channels. Given such a heuristic channel allocation strategy, it can be found that there is a significant enhancement in the sum of retained weight importance scores compared to direct N:M pruning.\nStep 2: Linear Sum Assignment. In the next, we show the heuristic allocation strategy can be further refined. The refining process can be formulated as a linear sum assignment (LSA) problem, which can be efficiently solved by the Hungarian algorithm (Kuhn, 1955). To see this, we can take out one allocated channel from each block, and thus there are K channels to be filled back to K blocks. It is thus a traditional linear sum assignment problem to find a better one-by-one matching between the K channels and K blocks, such that the weight importance sum in Equation 6 can be further improved. From Figure 3, LSA further improves the score sum by 0.4, with the top-1 and top-2, top-3 and top-4 channels swapped from the heuristic channel allocation.\nRemarks. Note that the permutation of weight matrices does not affect the output of LLMs. For dense layers, the input activations need to be simultaneously permuted, which can be done by permuting the output channels of the previous layer. The exception lies in the residual connection, which can be done with an efficient permutation operator. More details on implementing channel permutation and Hungarian algorithm are listed in F."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "5.1 SETUP",
            "text": "We evaluate the proposed approach on three popular LLMs: LLaMA 7B-65B (Touvron et al., 2023a), LLaMA2 7B-70B (Touvron et al., 2023b), and OPT 1.3B-13B (Zhang et al., 2022a). We use the public checkpoint of the involved models in the HuggingFace Transformers library 1. We utilize 3 NVIDIA A100 GPUs each equipped with 80GB memory. For each model under consideration, we apply uniform pruning to all linear layers, with the exception of embeddings and the head. Specifically, in each self-attention module, there are four linear layers, while each MLP module contains three linear layers for LLaMA model families and two for OPT. All the evaluations are conducted with the same code to make sure the comparison is fair. The detailed setting of tasks, metrics, baseline methods, and calibration data can be found in Appendix B."
        },
        {
            "heading": "5.2 UNSTRUCTURED PRUNING",
            "text": "Main Results. As highlighted in Table 1, RIA consistently outperforms Wanda and SparseGPT across all scenarios. Notably, our method achieves a 50% improvement in preventing performance drop of the Dense model in comparison to Sparsegpt (16% in LLaMA and LLaMA2 model family), and 17% improvement in preventing performance drop in comparison to Wanda (13% in LLaMA and LLaMA2 model family). It is essential to note that as the model size increases, the performance gap between models pruned by RIA and the original dense models diminishes significantly.\n1https://huggingface.co/meta-llama, https://huggingface.co/facebook\nAblation Studies. To thoroughly evaluate the influence of each element within our RIA equation, we undertook an ablation test, with the outcomes presented in Table 2. We disassembled our formula into several distinct components for closer scrutiny: ||X||2: \u21132-norm of activations; |W|: weight magnitude; |W|in: weight magnitude normalized by the input channels; |W|out: weight magnitude normalized by the output channels; RI: stands for relative importance which is the combination of |W|in and |W|out; and RIA (a = 1.0) and\nRIA (a = 0.5) combines relative importance with input activations with different values of a. The selection of a is shown in Figure 10.\nAs illustrated in Table 2, normalizing the weight magnitude through either input or output channels offers a substantial performance boost over merely considering weight magnitude. Interestingly, utilizing Relative Importance alone can match or, in instances like LLaMA-30B, even outperform SparseGPT. A distinguishing feature of Relative Importance is its reliance solely on weight information, eliminating the need for calibration data. In contrast, both Wanda and SparseGPT necessitate calibration data to derive input activations or Hessian\nmatrices. The table also showcases enhancements brought about by the other equation components of RIA.\nSparsity. In Figure 4(a), we examine the effects of varying sparsity levels, ranging from 0.1 to 0.6, on the performance of the LLaMA 65b model. The PPL curves clearly demonstrate that Magnitude pruning is particularly sensitive to increased sparsity levels, with a 60% sparsity level resulting in significant model degradation. In contrast, the SparseGPT, Wanda, and RIA models exhibit more robust performance across all tested sparsity levels, with RIA (green) consistently outperforming the others at every level of sparsity.\nCalibration data. SparseGPT, Wanda, and RIA all require calibration data for obtaining either input activations or the Hessian matrix. To assess the robustness of our algorithm concerning the calibration data, we conducted a sensitivity test involving variations in the type of calibration datasets and the number of calibration samples. The influence of calibration datasets is presented in Table 7. Our aim here is to assess the impact of the number of calibration samples.\nAs illustrated in Figure 4(b), an example of the LLaMA30B model, SparseGPT appears to rely on a larger number of calibration samples, while both Wanda and RIA demonstrate robust performance across varying sample sizes. Notably, RIA consistently outperforms the other methods in all cases.\nZero-shot performance. In Table 3, we present the zero-shot performance of unstructured 50% sparsity of the models pruned with magnitude, Wanda, sparsegpt, and RIA on LLaMA2-70b model. We report the average performance across these datasets in the last column. As shown in the table, RIA gets the best performance on 3/5 datasets and also achieves the best average performance across the 5 datasets."
        },
        {
            "heading": "5.3 N:M STRUCTURED PRUNING",
            "text": "While RIA aims to explore the upper bounds of performance achievable through one-shot PTP methods, combining it with the N:M constraint seeks the real inference speed by aligning with the present GPU hardware environment. In this subsection, we assess how Channel Permutation (CP) can enhance the performance of one-shot PTP when incorporated with the N:M constraint.\nMain Results. In this comparison, we assess the performance of unstructured 50% sparsity, 2:4 constraint sparsity, and 4:8 constraint sparsity for Magnitude, Wanda, SparseGPT, and RIA. Addi-\ntionally, we provide the performance results when applying Channel Permutation (CP) to each of these methods. Direct using step 1 of CP (CP w/o LSA) is also displayed in the table, serving as an ablation test in comparison to the complete one. We present the Perplexity of each pruned model on the Wikitext2 dataset, maintaining the same settings as in Section 5.1.\nAs presented in Table 4, RIA consistently delivers superior performance across all semi-structured sparsity patterns when employing one-shot PTP. Importantly, when utilizing merely CP, every method\u2014with the exception of Magnitude\u2014already exhibits a significant improvement in performance. With the incorperation of LSA, the performance is further improved. This highlights our motivation to group the input channels based on their sorted indices, ensuring that similar scaling channels don\u2019t end up in the same block.\nZero-shot Performance. In Table 5, we present the zero-shot performance of the N:M constraint models pruned using RIA and Wanda. The table\u2019s last column also provides the average performance across these datasets. As the table indicates, while there\u2019s a performance decline on the Hellaswag and ARC-C datasets, the case of RIA (2:4+CP) performs even surpasses the dense model with a large amplitude on BoolQ, MNLI, and RTE datasets. This highlights the observation that large\nlanguage models can be prone to overfitting, resulting in an abundance of redundant, unnecessary, or potentially harmful elements. It\u2019s noteworthy that with the incorporation of CP, there\u2019s a remarkable improvement in performance for both Wanda and RIA."
        },
        {
            "heading": "5.4 RUNNING TIME ANALYSIS",
            "text": "Pruning and permutation running time. We present the actual running times of each algorithm on the largest model involved in this article, LLaMA2-70b. The relative complexity analysis can be found in Appendix G.\n\u2022 Pruning time. We test each algorithm with 128 calibration data. For SparseGPT, the pruning time amounts to 5756 seconds (approximately 1.5 hours). In contrast, Wanda and RIA demonstrate significantly reduced runtimes, with times of approximately 611 seconds (approximately 10 minutes) and 627 seconds (approximately 10 minutes)\n\u2022 Channel permutation time. For a comparative analysis of execution duration, we present the processing time of a single matrix constructed using our algorithm for the N:M sparsity. This is compared against the greedy method and its variants, which employ escaping strategies to circumvent getting trapped in local minima, as discussed in (Pool & Yu, 2021). The results for different dimensions are provided in Table 12.\nInference acceleration. We assess the inference acceleration offered by sparsity in Large Language Models (LLMs). In a manner akin to SparseGPT (Frantar & Alistarh, 2023), we present data for both the unstructured sparsity and 2:4 sparsity acceleration on CPU and GPU relative to the dense model across various components. For CPU-based inference, the torch. sparse package is employed to manage the sparse matrix and execute sparse matrix multiplication. Our observations indicate an inference speed acceleration ranging between 1.8\u00d7 and 2.0\u00d7. In addition, we evaluate the speed acceleration observed on the GPU when applying a 2:4 constraint. Theoretically, employing an N:M sparsity can yield up to a 2\u00d7 speedup when contrasted with dense models. We conducted tests on the Nvidia Tesla A100, utilizing the cuSPARSELt library for Sparse Matrix-Matrix Multiplication (SpMM) (Mishra et al., 2021) with N:M sparse matrices. For unstructured sparsity, we treated it as a dense matrix and assessed the actual inference speed. Comprehensive acceleration metrics for each module are outlined in Table 6 and the acceleration of each layer is about 1.6\u00d7. It\u2019s worth mentioning that the overall speed enhancement for the entire model might be slightly reduced due to inherent overhead factors."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this article, we have introduced two novel methods, RIA and Channel Permutation, that together establish an effective plug-and-play pipeline for post-training pruning and inference acceleration of large language models. RIA incorporates Relative Importance and the feature of input activations that create a criterion to prune the weights of LLMs. Through extensive experiments on prominent LLMs like LLaMA, LLaMA2, and OPT across varying model sizes, we have demonstrated that RIA consistently outperforms existing SOTA one-shot pruning techniques SparseGPT and Wanda, which sets a new benchmark for post-training pruning performance. Furthermore, Channel Permutation successfully reduces the performance drop when adapting the model to N:M constraint by reframing the input channel permutation problem as a combinatorial optimization task and solving it efficiently with the Hungarian algorithm. Together, RIA and Channel Permutation form a seamless \u201dplugand-play\u201d method, enabling effective one-shot post-training pruning for all current large language models. Furthermore, this method is hardware-friendly, ensuring enhanced inference acceleration."
        },
        {
            "heading": "A MOTIVATION OF CHANNEL PERMUTATION",
            "text": "Based on the results presented in section 5, it is evident that after applying channel permutation, the performance of N:M constraint models significantly improves in comparison to executing N:M sparsity directly. This improvement can be demonstrated through empirical observations made within Large Language Models (LLMs). As shown in Figure 5, pruning with 50% unstructured sparsity versus direct N:M con-\nstraint sparsity (2:4 in the figure) produces distinct variances in the distribution of input channels\u2019 retained degree. The N:M constraint, by its nature, results in a more uniform pruning pattern. With an example of the first row of Figure 3, when directly pruning with 2:4 constraint sparsity, if the input channels with universally larger scores get stuck in the same block, some of the weights will be \u201cwrongly\u201d pruned. Conversely, the block with universally smaller scaling scores will \u201cwrongly\u201d preserve some weights. One diminishes while the other grows, resulting in a more uniform pruning, a smaller variance of retained degree distribution of input channels, and therefore a lower total sum scores after pruning. In contrast, unstructured pruning takes on a broader perspective, pruning weights within their global context. This expansive methodology inherently introduces more variability, leading to a larger variance in the retained degree of input channels after pruning. The channel permutation method proposed in this article tackles this challenge smoothly by distributing similar scaling input channels into different blocks, allowing weights that should be retained to be preserved as much as possible. The post-channel-permutation distribution (2:4 + CP) of input channels\u2019 retained degrees can be seen in Figure 5, closely mirroring the results of unstructured pruning."
        },
        {
            "heading": "B DETAILED EXPERIMENTAL SETTINGS",
            "text": "Tasks and Metrics. We mainly evaluate language modeling and zero-shot classification. Our initial assessment of language modeling involves a comprehensive evaluation of perplexity (PPL), where lower values indicate better performance. This evaluation is conducted on the test set of Wikitext2 (Merity et al., 2016). For further evaluation, we also conduct experiments on zero-shot classification to assess the ability of the sparse model to correctly classify objects or data points into categories it has never seen during training across five common-sense datasets: Hellaswag, BoolQ, ARC-Challenge, MNLI, and RTE, in comparison to the performance of the dense model. We run the experiments with the public GitHub benchmark EleutherAI/lm-evaluation-harness (Gao et al., 2021).\nBaselines. For unstructured pruning, we compare with: 1) magnitude pruning (Zhu & Gupta, 2017), the most prevalent pruning approach; and two more recent state-of-the-art works on LLM pruning: 2) SparseGPT (Frantar & Alistarh, 2023) and 3) Wanda (Sun et al., 2023). These methods can be evaluated both on unstructured pruning and N:M structured pruning in the following sections.\nCalibration Data. We employ 128 samples from the C4 dataset (Raffel et al., 2019) for all models, and each sample contains 2048 tokens. This also aligns with the settings in baseline methods for a fair comparison. Note that we also discuss the choice of calibration data across different datasets, and more details can be found in Appendix C."
        },
        {
            "heading": "C SENSITIVITY TEST ON CALIBRATION DATASETS",
            "text": "Figure 2 illustrates that different datasets exhibit varying data distributions, which in turn impact the input activations. Consequently, we conducted experiments employing different calibration datasets to evaluate the robustness of the pruned model. Specifically, we utilized three different\ndatasets\u2014Wikitext2, C4, and PTB (Marcus et al., 1993)\u2014with 128 samples, each containing 2048 tokens, for calibration purposes, and evaluated the performance on Wikitext2. RIA is compared with Wanda and SparseGPT, with the results summarized in Table 7. RIA surpasses other algorithms in the majority of scenarios, with the exception of when using identical calibration and evaluation datasets for PTB and wikitext2. This exception can be primarily attributed to the weight reconstruction process. A distribution that is more congruent tends to yield enhanced performance through reconstruction. We therefore explored integrating the reconstruction approach with both Wanda and RIA in Appendix D. Furthermore, looking at the results across different calibration datasets, the results of RIA are more stable which indicates that RIA is more robust to the calibration data."
        },
        {
            "heading": "D WEIGHT RECONSTRUCTION",
            "text": "Following the approach of OBS (Hassibi et al., 1993), which replaces the fine-tuning process with weight reconstruction using calibration data, this method has gained traction in LLMs post-pruning as an alternative to fine-tuning and retraining. SparseGPT (Frantar & Alistarh, 2023) extends this idea by permitting partial updates, thereby reducing computational complexity. The specific formula for this is detailed in (Frantar & Alistarh, 2023). This weight reconstruction technique is essentially a tool embraced by all PTP methods that revise the objective function of equation 1 to 7.\nargmin Ml,W\u0302l\n||Wl \u00b7 Xl \u2212 (Ml \u2299 W\u0302l) \u00b7 Xl||22 (7)\nIn this formulation, W\u0302l represents the weight matrix after undergoing the reconstruction process. In this section, we evaluate Wanda and RIA based on their adoption of weight reconstruction and juxtapose their performance with that of SparseGPT. Our findings are presented in Table 8. We omit the results from PTB as their PPLs are excessively high, rendering them of no reference value. It\u2019s evident that RIA+rec outperforms other reconstruction-based algorithms. However, except for wikitext2 is used both for calibration and evaluation, the reconstruction does not appear to enhance performance. Therefore, we have chosen not to employ this weight reconstruction approach in our main text."
        },
        {
            "heading": "E CHANNEL CORRUPTIONS AND STRUCTURED PRUNING",
            "text": "Evidence of Channel Corruption We mentioned in the main text that there exist channel corruptions when applying Wanda to prune LLMs. We offer the evidence here with the Figure. As illustrated in Figure 6, our analysis of the Wanda model shows that approximately 10% of channels exhibit corruption. This evidence directly supports our claim and provides a clearer understanding of the phenomenon.\nDetrimental Impact of channel corruption on Model Performance To investigate the impact of complete channel pruning on model performance, we conducted experiments using LLM-pruner (Ma et al., 2023), a SOTA structured pruning method in LLMs. These experiments were designed to demonstrate the influence of channel corruption. Additionally, we employed pruning based solely on activation as a means to evaluate the network. This approach serves not only as a test of structured pruning but also as an ablation study for our RIA method. We compare the structured pruning methods with the semi-\nstructured pruning method Wanda, RIA, and RIA incorporated with channel permutation. In LLMpruner (Ma et al., 2023), they introduce a skip-layer pruning strategy which skips the pruning process for the first several layers and the last few layers. To ensure a fair comparison, we implement a uniform skip strategy for all algorithms listed in Table 10. Additionally, for a more comprehensive evaluation, we conduct tests on pruning all layers of the model as detailed in Table 11.\nOur results indicate: 1) The performance of LLM-Pruner(perplexity) notably increases at already a low sparsity level of 25%. This contrasts sharply with semi-structured pruning methods, highlighting the detrimental impact of channel corruption on model effectiveness. Meanwhile, in all levels of sparsity, RIA demonstrates superior performance in comparison to Wanda. Additionally, channel permutation enhances performance when transitioning the model to semi-structured sparsity. This finding aligns with the conclusions presented in the main text.\n2) RIA incorporates the LLM-Pruner\u2019s approach, which involves skipping 6 out of 32 layers, leading to lower perplexity scores (PPLs). However, the trade-off between performance and the increased inference speed due to sparsity requires further exploration. We aim to continue investigating this issue promptly for this study.\nEnd-to-End inference latency of N:M sparsity and structured sparsity. We here offer the inference latency of the structured pruned model with 50% sparsity and 2:4 sparsity model. Our experiment is on LLaMA2-7b, the sequence length of the input is 12. The deployment is on 2 Nvidia A100 80GB in parallel on Ubuntu 22.04.1 system. Here, we offer the inference latency with varying batch sizes.\n1 8 16 64 dense 281.13 ms 547.82 ms 1011.81ms 3742.76 ms structured 50% sparsity 238.92 ms 326.14 ms 616.13 ms 2181.69 ms 2:4 sparsity 225.60 ms 357.48 ms 731.81 ms 2495.41 ms\nbatches, both structured 50% sparsity and 2:4 sparsity models do not significantly boost inference speed. However, as the batch size grows, the acceleration for 2:4 sparsity approximates 1.5x, whereas the structured 50% sparsity tends to reach about 1.7x acceleration.\nIn conclusion, given the performance and the inference latency analysis, although structured pruning could provide a faster acceleration than N:M sparsity with the same sparsity level, the performance drop is unignorable even when the sparsity is very low. Therefore, in the current situation, the N:M constraint sparsity is the only sparsity pattern that strikes the balance between performance and acceleration speed.\nF IMPLEMENTATION DETAILS OF CHANNEL PERMUTATION\nFrom a perspective of computational efficiency, directly implementing permutation on input vectors introduces extra runtime overhead for extracting the permuted index and applying it to the input vectors. However, an alternative approach can be considered. Given that each input serves as the output of the preceding layer, we can permute the output channels of the previous layers\u2019 weights to serve as the permuted index for the next layers\u2019 input channels. This eliminates the need to permute the input index separately, resulting in time savings.\nHowever, two key considerations must be taken into account:\na) For the Q, K, and V projection layers within a single module, they must be treated as a unified matrix, concatenating them into a single entity that shares the same input channel indices. This ensures that the input index permutation remains identical across these layers, avoiding the need for additional permutations. In the case of LLaMA, the MLP module presents a similar scenario, as it contains parallel layers, namely, down proj and gate proj, which also need to be concatenated into a single matrix.\nb) Another important factor to note is that when dealing with models featuring residual connections, this strategy cannot be applied. This is due to the fact that inputs in such modules also originate from the previous module. As a result, permutations must be applied to the input activations at the beginning of each module. However, this process doesn\u2019t significantly impact execution time. In fact, we\u2019ve modified the RMS normalization layer \u2014 which typically follows the Residual connection \u2014 using Triton (Tillet et al., 2019). This ensures the output channel indices of this layer align with the input indices of the subsequent layers. In our tests, the time taken for this adjustment is negligible."
        },
        {
            "heading": "G COMPLEXITY ANALYSIS OF RIA AND CHANNEL PERMUTATION",
            "text": "RIA. We provide a summary of the computational complexity of various Post-Training Pruning (PTP) algorithms. For SparseGPT, the time complexity is approximately O(d3hidden) (Frantar & Alistarh, 2023), whereas both RIA and Wanda exhibit similar time complexities of approximately O(d2hidden) (Sun et al., 2023).\nChannel Permutation The process of channel reallocation to obtain the sorted input channel indices is straightforward. Computation of the score matrix S involves determining the outcome of placing each objection into every box and summing the results while processing the 2:4 constraint within each block. Consequently, the time complexity for this computation is O (( c M\n)2 \u00d7 r \u00d7M), where r\u00d7M represents the computational complexity for the N:M constraint within each block. The time complexity of the Hungarian algorithm is O (( c M )3) .\nThe greedy method (Pool & Yu, 2021) is challenging to implement in LLMs due to its prohibitive running time, as shown in Table 12. We conducted only a single experiment for performance comparison on LLaMA2-13b, which took us 3 days. We tested the greedy method with 100 escape iterations to handle the permutation, the PPL on wikitext2 with a permuted 2:4 constraint is 8.01 which is just comparable to our CP method (7.99). However, the CP\u2019s execution time was just 30 minutes, making it possible to be applied in the LLMs."
        },
        {
            "heading": "H HUNGARIAN ALGORITHM",
            "text": "Given a bipartite graph with N left vertices and N right vertices, depicted by matrix S, where Sij represents the weight between the left vertex i and the right vertex j, the algorithm aims to identify the ideal matching that minimizes the aggregate weight. This goal is captured in the subsequent objective function:\nmin N\u2211 i=1 N\u2211 j=1 Sij \u00d7 Xij . (8)\nIn this Equation, Xij is a binary determinant that showcases whether the left vertex i is paired with the right vertex j. In our scenario, the initial first group of input indices is treated as vertex i, with the incomplete boxes acting as vertex j. This transition into an LSA problem is seamlessly facilitated by the Hungarian algorithm, which subsequently derives the optimal permutation. After rearranging the first indices of the blocks, we apply the same procedure to the subsequent groups in sequence.\nI INTEGRATION OF POST-TRAINING PRUNING WITH QUANTIZATION METHODS\nOur experiments utilized the LLaMA2-7b model, with a focus on two quantization methods: GPTQ (Frantar et al., 2022) and AWQ (Lin et al., 2023). The calibration was performed using the C4 dataset, and evaluations were done on Wikitext2. We present our results in Table 13.\nOur findings suggest that this integration can lead to both reducing memory and inference acceleration with only a slight detriment to performance.\nWe have identified two primary strategies for merging post-training pruning with quantization: first pruning then quantizing (a), and first quantizing then pruning (b). Our findings indicate a preference for (a) pruning before quantizing. This is potentially because for (b), conventional block-wise quantization relies merely on the min and max weights within a block, and pruning can lead to a reduction in these extremes, thereby potentially aiding quantization. In contrast, (a) quantizing prior to pruning adversely affects the computation of weight importance for pruning, resulting in diminished performance."
        },
        {
            "heading": "J THE EXPLANATION OF PREVENTING PERFORMANCE DROP FROM DENSE",
            "text": "The primary goal of post-training pruning methods is to minimize the performance drop compared to the dense model. Therefore, designing metrics that offer a fair evaluation of new methods against this baseline (dense performance) is crucial. Consider two post-training pruning methods, A and B, with perplexities P(A) and P(B), respectively. Let P(D) represent the baseline perplexity of the dense network. The effectiveness of method B in preventing performance degradation compared to method A can be quantified as P (A)\u2212P (B)P (A)\u2212P (D) . This formula allows for a fair assessment of the new method relative to previous approaches, taking into account the performance of the dense model in the posttraining pruning context. Using this calculation, the RIA method demonstrates a 17% improvement over Wanda and a 50% improvement over SparseGPT in preventing performance drops from the dense model, consistently across all tested models."
        }
    ],
    "title": "PLUG-AND-PLAY: AN EFFICIENT POST-TRAINING PRUNING METHOD FOR LARGE LANGUAGE MODELS",
    "year": 2023
}