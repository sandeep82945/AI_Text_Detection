{
    "abstractText": "Natural language to code generation is an important application area of LLMS and has received wide attention from the community. The majority of relevant studies have exclusively concentrated on increasing the quantity and functional correctness of training sets while disregarding other stylistic elements of programs. More recently, data quality has garnered a lot of interest and multiple works have showcased its importance for improving performance. In this work, we investigate data quality for code and find that making the code more structured and readable leads to improved code generation performance of the system. We build a novel data-cleaning pipeline that uses these principles to transform existing programs by 1.) renaming variables, 2.) modularizing and decomposing complex code into smaller helper sub-functions, and 3.) inserting natural-language based planning annotations. We evaluate our approach on two challenging algorithmic code generation benchmarks and find that fine-tuning CODELLAMA-7B on our transformed programs improves the performance by up to 30% compared to fine-tuning on the original dataset. Additionally, we demonstrate improved performance from using a smaller amount of higher-quality data, finding that a model fine-tuned on the entire original dataset is outperformed by a model trained on one-eighth of our cleaned dataset. Even in comparison to closed-source models, our models outperform the much larger ALPHACODE models (Li et al., 2022).",
    "authors": [],
    "id": "SP:574a9cd8ef84d955afc2d0ac936cd2a43c9c1f09",
    "references": [
        {
            "authors": [
                "Ramakrishna Bairi",
                "Atharv Sonwane",
                "Aditya Kanade",
                "Vageesh D C",
                "Arun Iyer",
                "Suresh Parthasarathy",
                "Sriram Rajamani",
                "B. Ashok",
                "Shashank Shet"
            ],
            "title": "Codeplan: Repositorylevel coding using llms and planning",
            "venue": "In Neural Information Processing Systems Workshop on Foundation Models for Decision Making (FMDM-NeurIPS),",
            "year": 2023
        },
        {
            "authors": [
                "Yihan Cao",
                "Yanbin Kang",
                "Lichao Sun"
            ],
            "title": "Instruction mining: High-quality instruction data selection for large language models",
            "venue": "arXiv preprint arXiv:2307.06290,",
            "year": 2023
        },
        {
            "authors": [
                "Bei Chen",
                "Fengji Zhang",
                "Anh Nguyen",
                "Daoguang Zan",
                "Zeqi Lin",
                "Jian-Guang Lou",
                "Weizhu Chen"
            ],
            "title": "Codet: Code generation with generated tests",
            "venue": "arXiv preprint arXiv:2207.10397,",
            "year": 2022
        },
        {
            "authors": [
                "Hao Chen",
                "Yiming Zhang",
                "Qi Zhang",
                "Hantao Yang",
                "Xiaomeng Hu",
                "Xuetao Ma",
                "Yifan Yanggong",
                "Junbo Zhao"
            ],
            "title": "Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning",
            "venue": "arXiv preprint arXiv:2305.09246,",
            "year": 2023
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harri Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "venue": "arXiv preprint arXiv:2107.03374,",
            "year": 2021
        },
        {
            "authors": [
                "Wenhu Chen",
                "Xueguang Ma",
                "Xinyi Wang",
                "William W Cohen"
            ],
            "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "venue": "arXiv preprint arXiv:2211.12588,",
            "year": 2022
        },
        {
            "authors": [
                "Xinyun Chen",
                "Maxwell Lin",
                "Nathanael Sch\u00e4rli",
                "Denny Zhou"
            ],
            "title": "Teaching large language models to self-debug",
            "venue": "arXiv preprint arXiv:2304.05128,",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023",
            "venue": "URL https: //lmsys.org/blog/2023-03-30-vicuna/",
            "year": 2023
        },
        {
            "authors": [
                "Yao Fu",
                "Hao Peng",
                "Litu Ou",
                "Ashish Sabharwal",
                "Tushar Khot"
            ],
            "title": "Specializing smaller language models towards multi-step reasoning",
            "venue": "arXiv preprint arXiv:2301.12726,",
            "year": 2023
        },
        {
            "authors": [
                "Patrick Haluptzok",
                "Matthew Bowers",
                "Adam Tauman Kalai"
            ],
            "title": "Language models can teach themselves to program better",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Steven Basart",
                "Saurav Kadavath",
                "Mantas Mazeika",
                "Akul Arora",
                "Ethan Guo",
                "Collin Burns",
                "Samir Puranik",
                "Horace He",
                "Dawn Song",
                "Jacob Steinhardt"
            ],
            "title": "Measuring coding challenge competence with apps",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Xinyi Hou",
                "Yanjie Zhao",
                "Yue Liu",
                "Zhou Yang",
                "Kailong Wang",
                "Li Li",
                "Xiapu Luo",
                "David Lo",
                "John Grundy",
                "Haoyu Wang"
            ],
            "title": "Large language models for software engineering: A systematic literature review",
            "venue": "arXiv preprint arXiv:2308.10620,",
            "year": 2023
        },
        {
            "authors": [
                "Naman Jain",
                "Skanda Vaidyanath",
                "Arun Iyer",
                "Nagarajan Natarajan",
                "Suresh Parthasarathy",
                "Sriram Rajamani",
                "Rahul Sharma"
            ],
            "title": "Jigsaw: Large language models meet program synthesis",
            "venue": "ICSE",
            "year": 2022
        },
        {
            "authors": [
                "Xue Jiang",
                "Yihong Dong",
                "Lecheng Wang",
                "Qiwei Shang",
                "Ge Li"
            ],
            "title": "Self-planning code generation with large language model",
            "venue": "arXiv preprint arXiv:2303.06689,",
            "year": 2023
        },
        {
            "authors": [
                "Darren Key",
                "Wen-Ding Li",
                "Kevin Ellis"
            ],
            "title": "I speak, you verify: Toward trustworthy neural program synthesis",
            "venue": "arXiv preprint arXiv:2210.00848,",
            "year": 2022
        },
        {
            "authors": [
                "Sumith Kulal",
                "Panupong Pasupat",
                "Kartik Chandra",
                "Mina Lee",
                "Oded Padon",
                "Alex Aiken",
                "Percy S Liang"
            ],
            "title": "Spoc: Search-based pseudocode to code",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Woosuk Kwon",
                "Zhuohan Li",
                "Siyuan Zhuang",
                "Ying Sheng",
                "Lianmin Zheng",
                "Cody Hao Yu",
                "Joseph E. Gonzalez",
                "Hao Zhang",
                "Ion Stoica"
            ],
            "title": "Efficient memory management for large language model serving with pagedattention",
            "venue": "In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles,",
            "year": 2023
        },
        {
            "authors": [
                "Yuhang Lai",
                "Chengxi Li",
                "Yiming Wang",
                "Tianyi Zhang",
                "Ruiqi Zhong",
                "Luke Zettlemoyer",
                "Scott Wen tau Yih",
                "Daniel Fried",
                "Sida Wang",
                "Tao Yu"
            ],
            "title": "Ds-1000: A natural and reliable benchmark for data science code",
            "venue": "generation. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Hung Le",
                "Yue Wang",
                "Akhilesh Deepak Gotmare",
                "Silvio Savarese",
                "Steven Hoi"
            ],
            "title": "CodeRL: Mastering code generation through pretrained models and deep reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jierui Li",
                "Szymon Tworkowski",
                "Yingying Wu",
                "Raymond Mooney"
            ],
            "title": "Explaining competitive-level programming solutions using llms",
            "venue": "arXiv preprint arXiv:2307.05337,",
            "year": 2023
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Jack Hessel",
                "Youngjae Yu",
                "Xiang Ren",
                "Kai-Wei Chang",
                "Yejin Choi"
            ],
            "title": "Symbolic chain-of-thought distillation: Small models can also \u201cthink\u201d step-by-step",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Xin-Ye Li",
                "Jiang-Tian Xue",
                "Zheng Xie",
                "Ming Li"
            ],
            "title": "Think outside the code: Brainstorming boosts large language models in code generation",
            "venue": "arXiv preprint arXiv:2305.10679,",
            "year": 2023
        },
        {
            "authors": [
                "Yuanzhi Li",
                "S\u00e9bastien Bubeck",
                "Ronen Eldan",
                "Allie Del Giorno",
                "Suriya Gunasekar",
                "Yin Tat Lee"
            ],
            "title": "Textbooks are all you need ii: phi-1.5 technical report",
            "venue": "arXiv preprint arXiv:2309.05463,",
            "year": 2023
        },
        {
            "authors": [
                "Yujia Li",
                "David Choi",
                "Junyoung Chung",
                "Nate Kushman",
                "Julian Schrittwieser",
                "R\u00e9mi Leblond",
                "Tom Eccles",
                "James Keeling",
                "Felix Gimeno",
                "Agustin Dal Lago"
            ],
            "title": "Competition-level code generation with alphacode",
            "year": 2022
        },
        {
            "authors": [
                "Chao Liu",
                "Xuanlin Bao",
                "Hongyu Zhang",
                "Neng Zhang",
                "Haibo Hu",
                "Xiaohong Zhang",
                "Meng Yan"
            ],
            "title": "Improving chatgpt prompt for code generation",
            "venue": "arXiv preprint arXiv:2305.08360,",
            "year": 2023
        },
        {
            "authors": [
                "Jiate Liu",
                "Yiqin Zhu",
                "Kaiwen Xiao",
                "Qiang Fu",
                "Xiao Han",
                "Wei Yang",
                "Deheng Ye"
            ],
            "title": "Rltf: Reinforcement learning from unit test feedback, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Haipeng Luo",
                "Qingfeng Sun",
                "Can Xu",
                "Pu Zhao",
                "Jianguang Lou",
                "Chongyang Tao",
                "Xiubo Geng",
                "Qingwei Lin",
                "Shifeng Chen",
                "Dongmei Zhang"
            ],
            "title": "Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct",
            "venue": "arXiv preprint arXiv:2308.09583,",
            "year": 2023
        },
        {
            "authors": [
                "Ziyang Luo",
                "Can Xu",
                "Pu Zhao",
                "Qingfeng Sun",
                "Xiubo Geng",
                "Wenxiang Hu",
                "Chongyang Tao",
                "Jing Ma",
                "Qingwei Lin",
                "Daxin Jiang"
            ],
            "title": "Wizardcoder: Empowering code large language models with evol-instruct",
            "venue": "arXiv preprint arXiv:2306.08568,",
            "year": 2023
        },
        {
            "authors": [
                "Lucie Charlotte Magister",
                "Jonathan Mallinson",
                "Jakub Adamek",
                "Eric Malmi",
                "Aliaksei Severyn"
            ],
            "title": "Teaching small language models to reason",
            "venue": "arXiv preprint arXiv:2212.08410,",
            "year": 2022
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Shishir G. Patil",
                "Tianjun Zhang",
                "Xin Wang",
                "Joseph E. Gonzalez"
            ],
            "title": "Gorilla: Large language model connected with massive apis",
            "venue": "arXiv preprint arXiv:2305.15334,",
            "year": 2023
        },
        {
            "authors": [
                "Baptiste Rozi\u00e8re",
                "Jonas Gehring",
                "Fabian Gloeckle",
                "Sten Sootla",
                "Itai Gat",
                "Xiaoqing Ellen Tan",
                "Yossi Adi",
                "Jingyu Liu",
                "Tal Remez",
                "J\u00e9r\u00e9my Rapin"
            ],
            "title": "Code llama: Open foundation models for code",
            "venue": "arXiv preprint arXiv:2308.12950,",
            "year": 2023
        },
        {
            "authors": [
                "Melanie Sclar",
                "Peter West",
                "Sachin Kumar",
                "Yulia Tsvetkov",
                "Yejin Choi"
            ],
            "title": "Referee: Referencefree sentence summarization with sharper controllability through symbolic knowledge distillation",
            "venue": "arXiv preprint arXiv:2210.13800,",
            "year": 2022
        },
        {
            "authors": [
                "Noah Shinn",
                "Federico Cassano",
                "Ashwin Gopinath",
                "Karthik R Narasimhan",
                "Shunyu Yao"
            ],
            "title": "Reflexion: Language agents with verbal reinforcement learning",
            "venue": "In Thirty-seventh Conference on Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Parshin Shojaee",
                "Aneesh Jain",
                "Sindhu Tipirneni",
                "Chandan K. Reddy"
            ],
            "title": "Execution-based code generation using deep reinforcement learning",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Disha Shrivastava",
                "Hugo Larochelle",
                "Daniel Tarlow"
            ],
            "title": "Repository-level prompt generation for large language models of code",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto"
            ],
            "title": "Stanford alpaca: An instruction-following llama model, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Lei Wang",
                "Wanyu Xu",
                "Yihuai Lan",
                "Zhiqiang Hu",
                "Yunshi Lan",
                "Roy Ka-Wei Lee",
                "Ee-Peng Lim"
            ],
            "title": "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models",
            "venue": "arXiv preprint arXiv:2305.04091,",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560,",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Peter West",
                "Chandra Bhagavatula",
                "Jack Hessel",
                "Jena Hwang",
                "Liwei Jiang",
                "Ronan Le Bras",
                "Ximing Lu",
                "Sean Welleck",
                "Yejin Choi"
            ],
            "title": "Symbolic knowledge distillation: from general language models to commonsense models",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Can Xu",
                "Qingfeng Sun",
                "Kai Zheng",
                "Xiubo Geng",
                "Pu Zhao",
                "Jiazhan Feng",
                "Chongyang Tao",
                "Daxin Jiang"
            ],
            "title": "Wizardlm: Empowering large language models to follow complex instructions",
            "venue": "arXiv preprint arXiv:2304.12244,",
            "year": 2023
        },
        {
            "authors": [
                "Canada",
                "July"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "doi: 10.18653/v1/2023.acl-long.",
            "year": 2023
        },
        {
            "authors": [
                "Xiang Yue",
                "Xingwei Qu",
                "Ge Zhang",
                "Yao Fu",
                "Wenhao Huang",
                "Huan Sun",
                "Yu Su",
                "Wenhu Chen"
            ],
            "title": "Mammoth: Building math generalist models through hybrid instruction tuning",
            "venue": "arXiv preprint arXiv:2309.05653,",
            "year": 2023
        },
        {
            "authors": [
                "Eric Zelikman",
                "Yuhuai Wu",
                "Jesse Mu",
                "Noah Goodman"
            ],
            "title": "STar: Bootstrapping reasoning with reasoning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Eric Zelikman",
                "Qian Huang",
                "Gabriel Poesia",
                "Noah D Goodman",
                "Nick Haber"
            ],
            "title": "Parsel: A (de) compositional framework for algorithmic reasoning with language models",
            "venue": "arXiv preprint arXiv:2212.10561,",
            "year": 2023
        },
        {
            "authors": [
                "Fengji Zhang",
                "Bei Chen",
                "Yue Zhang",
                "Jin Liu",
                "Daoguang Zan",
                "Yi Mao",
                "Jian-Guang Lou",
                "Weizhu Chen"
            ],
            "title": "Repocoder: Repository-level code completion through iterative retrieval and generation",
            "venue": "arXiv preprint arXiv:2303.12570,",
            "year": 2023
        },
        {
            "authors": [
                "Kexun Zhang",
                "Danqing Wang",
                "Jingtao Xia",
                "William Yang Wang",
                "Lei Li"
            ],
            "title": "Algo: Synthesizing algorithmic programs with generated oracle verifiers",
            "venue": "arXiv preprint arXiv:2305.14591,",
            "year": 2023
        },
        {
            "authors": [
                "Shun Zhang",
                "Zhenfang Chen",
                "Yikang Shen",
                "Mingyu Ding",
                "Joshua B Tenenbaum",
                "Chuang Gan"
            ],
            "title": "Planning with large language models for code generation",
            "venue": "arXiv preprint arXiv:2303.05510,",
            "year": 2023
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Wei-Lin Chiang",
                "Ying Sheng",
                "Siyuan Zhuang",
                "Zhanghao Wu",
                "Yonghao Zhuang",
                "Zi Lin",
                "Zhuohan Li",
                "Dacheng Li",
                "Eric. P Xing",
                "Hao Zhang",
                "Joseph E. Gonzalez",
                "Ion Stoica"
            ],
            "title": "Judging llm-as-a-judge with mt-bench and chatbot arena, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Daniel Fried Zhiruo Wang",
                "Shuyan Zhou",
                "Graham Neubig"
            ],
            "title": "Execution-based evaluation for open-domain code generation",
            "year": 2022
        },
        {
            "authors": [
                "Chunting Zhou",
                "Pengfei Liu",
                "Puxin Xu",
                "Srini Iyer",
                "Jiao Sun",
                "Yuning Mao",
                "Xuezhe Ma",
                "Avia Efrat",
                "Ping Yu",
                "Lili Yu"
            ],
            "title": "Lima: Less is more for alignment",
            "venue": "arXiv preprint arXiv:2305.11206,",
            "year": 2023
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Claire Cui",
                "Olivier Bousquet",
                "Quoc Le"
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "arXiv preprint arXiv:2205.10625,",
            "year": 2022
        },
        {
            "authors": [
                "Terry Yue Zhuo"
            ],
            "title": "Large language models are state-of-the-art evaluators of code generation",
            "venue": "arXiv preprint arXiv:2304.14317,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Natural language to code generation has witnessed considerable advances in recent years with the advent of large language models (LLMS for brevity). These advances primarily arise from training on large web-scale data and are measured based on the functional correctness of the programs. Thus, other aspects like readability, structuring, and styling and how they affect training and data quality are largely ignored by these works. On the flip side, many recent works have demonstrated the effectiveness of training on higher quality data during both pre-training (Li et al., 2023d) and fine-tuning (Zhou et al., 2023; Cao et al., 2023) phases. Even within the code-generation domain, Gunasekar et al. (2023) demonstrated the benefits of training on a \u201ctextbook\u201d quality dataset, generated synthetically using the GPT-3.5-TURBO model (Ouyang et al., 2022). However, these works do not provide an understanding of the factors that actually improve the data quality.\nIn this work, we show that using programs following good programming practices and allowing for more readability leads to improved code generation performance compared to using programs that do not follow these practices. We use these insights to build a novel automated code datacleaning pipeline that transforms programs while maintaining functional correctness using inputoutput examples. In contrast to prior works that curate high quality datasets by directly generating new data using LLMS, here we translate existing datasets into their parallel cleaned versions while identifying attributes that actually improve data quality.\nWe use LLMS to perform the transformations used in our data-cleaning approach. We demonstrate that instruction-tuned models can take a user-identified attribute of data quality as a natural language instruction and perform the transformation accurately. Our approach leverages the disparity in difficulty between generating a solution and editing an existing one. Therefore, it is particularly effective in domains where the existing model struggles to generate a correct solution but can effectively edit a given solution. We perform our data-cleaning transformations in three iterations: 1) renaming variables 2) modularizing complex code into subfunctions, and 3) adding planning annotations.\nFigure 1 provides an overview of our approach. Notice that the variable renaming step at the top adjusts the variable names to be contextually relevant (e.g. a to root_u and d to graph). The modularization step (depicted on the right) identifies and decomposes the original program into several smaller subfunctions such as find_root, merge_trees, build_graph, etc. It then implements these subroutines and assembles the modular program. Finally, our planning step (depicted at the bottom) constructs a plan by summarizing functions in a top-down fashion (starting from the main).\nWe evaluate our approach in a niche, yet challenging, domain of algorithmic code generation. The goal is to generate a program for a given problem statement. The task is challenging because it requires both high-level algorithmic reasoning and low-level coding and is evaluated using a strict functional correctness metric. We use two well-known algorithmic code generation benchmarks, namely APPS (Hendrycks et al., 2021) and CODE-CONTESTS (Li et al., 2022). We transform the corresponding programs in the training sets and obtain parallel datasets from our cleaning approach. Additionally, we utilize input-output examples to maintain functional equivalence between the original and transformed programs. We qualitatively analyze the generated dataset and find that it uses smaller helper sub-functions, each often implementing a standard algorithm or key program functionality. and provide more in depth findings in Section 4.1. We further assess the impact of the\ntransformed datasets on the performance on our downstream code generation task. We fine-tune CODELLAMA-7B model on the various collected datasets. Our findings reveal that the model finetuned on our modularized dataset outperforms the model fine-tuned on the functionally equivalent original dataset by up to 30%. Beyond, performance improvement, we also demonstrate that improving data quality improves the data efficiency. In particular, a model fine-tuned on the entire original dataset is outperformed by a model trained on just one-eighth of our cleaned dataset.\nWe next study improving planning in a supervised learning setup similar to prior works (Fu et al., 2023; Li et al., 2023b). While we observe limited improvements in planning, we disentangle planning vs coding capabilities and find that our fine-tuned model is capable of using gold-annotated plans, extracted from the ground-truth solutions to accurately generate solutions for the complex programs. This highlights planning for complex problems remaining a key bottleneck that does not seem to improve by merely increasing training datasets. Finally, in comparison to existing baselines, our fine-tuned models outperform the larger ALPHACODE (Li et al., 2022) models."
        },
        {
            "heading": "2 METHODOLOGY",
            "text": "In this section, we present our general data transformation approach and then instantiate it for performing code data cleaning."
        },
        {
            "heading": "2.1 TRANSFORMATIONS FOR DATA CLEANING",
            "text": "Given a dataset D consisting of N instances di, such that, D = {di}Ni=1. To achieve a desired data cleaning specification, the user additionally provides a data-cleaning instruction I, which highlights an attribute that needs to be modified. Optionally, we also use an oracle equivalence checker (O) which ensures that the transformed data instance d\u0303i is consistent with the original input based on some desired metric. For example, we can use edit-distance or functional equivalence based on input-output examples as our oracle checker.\nWe use a pre-trained language model (denoted by M) to generate the transformed instance (d\u0303i) by prompting the model with the transformation instruction (I) and the original answer (y). We can perform either zero-shot or few-shot prompting for performing the data cleaning operation. Finally, we extract the instance d\u0303i generated by M, and apply our oracle equivalence checker (O) to ensure consistency with the original data. If O(d\u0303i,di) = 0, i.e., the oracle reports a failure, we reject the generated output and retry the example within a sampling budget.\nWhile our transformation approach does not provide any guarantees about the quality of the performed transformation and relies on LLMS, we empirically observe that instruction-tuned LLMS can perform various unstructured data cleaning steps quite effectively. We provide a detailed analysis of the generated outputs for our algorithmic code generation setting in Section 4.1. Finally, in accordance with existing literature on prompting LLMS, we found that using simple and precise, low-level instructions improves the performance and accuracy of the models in performing the operations. Thus, for complex data cleaning operations (refactoring), we find improvements by breaking it down and performing multiple operations iteratively (renaming followed by modularization)."
        },
        {
            "heading": "2.2 CODE DATA-CLEANING",
            "text": "We apply our transformations-based data cleaning approach to programming data. Coding requires both \u2013 low-level programming and high-level reasoning or planning skills. Therefore, we propose a three-step cleaning pipeline that improves the readability and program structuring targeting the lowlevel coding skills and inserts natural-language based plans data targeting the high-level reasoning skills. Our steps are detailed below.\n1. Rename variables. This step renames the variables in the program, making them descriptive and easier to follow. Figure 1 top provides an example of this transformation.\n2. Modularize functions. Problem decomposition has been identified as a key approach for improving the reasoning capabilities of models (Zhou et al., 2022; Wang et al., 2023). We identify program decompositions and transform the program by extracting their functionality into smaller helper functions. Figure 1 right provides an example of this transformation.\n3. Plan annotations. This step summarizes the helper functions in the already modularized program and prepends it to the programs in the form of a natural language plan. These natural language descriptions are analogous to prompting approaches that are used for solving reasoning problems like chain-of-thought prompting (Wei et al., 2022), parsel (Zelikman et al., 2023), etc. Figure 1 bottom provides an example of this transformation.\nAdditionally, while performing these transformations, we use the test cases provided in the dataset to construct our oracle equivalence checker (O). It ensures that our transformed programs maintain functional equivalence to the original program."
        },
        {
            "heading": "3 EXPERIMENTAL SETUP",
            "text": "In this section, we detail our experimental setup and implementation. Section 3.1 outlines the benchmarks and metrics used for the algorithmic code generation task, while Sections 3.2 and 3.3 delve into the specifics of our code cleaning approach and fine-tuning experiments respectively."
        },
        {
            "heading": "3.1 BENCHMARKS",
            "text": "We use two standard algorithmic code generation benchmarks, APPS and CODE-CONTESTS. The benchmarks provide a collection of problem statements described in natural language and corresponding test cases. The goal is to generate a program that successfully solves the problem. The evaluation is performed using a strict functional correctness metric.\nAPPS (Hendrycks et al., 2021). This benchmark includes 10,000 problems, evenly split between training and test sets. It is sourced from multiple open-access competitive programming websites. It is further divided into APPS-INTRODUCTORY, APPS-INTERVIEW, and APPS-COMPETITION subsets based on problem difficulty. In this study, we only consider problems sourced from a subset of the competition websites based on the number of test cases provided.\nCODE-CONTESTS (Li et al., 2022). This benchmark includes 13,328 problems in the training set and 165 problems in the test set. We only use a subset of the training split that includes python solutions satisfying the provided test cases. Additionally, since the training set provides over a hundred solutions per problem, we perform near-deduplication on the solutions and limit them to a maximum of 25 solutions per problem.\nTable 1 and Appendix A provide further details about our final datasets.\nMetrics. We assess the code generation performance of the models using the PASS@K metric (Kulal et al., 2019; Chen et al., 2021), which evaluates the functional correctness of generated programs. For each problem, we generate N solutions (where N \u2265 2K) and compute the expected number of scenarios in which the problem is solved at least once when sub-selecting a random sample of K solutions. We vary K in {1, 10, 25} for APPS dataset and {1, 10, 100} for the CODE-CONTESTS benchmark. We present more details about sampling hyperparameters in Appendix A."
        },
        {
            "heading": "3.2 DATA TRANSFORMATIONS",
            "text": "We apply our data transformation approach on the APPS and CODE-CONTESTS datasets. Unless specified otherwise, we use GPT-3.5-TURBO as our default language model M to perform the transformations and use a default temperature 0.3. In case of failure, we retry up to 5 iterations. We obtain three parallel datasets at the end of our cleaning process, one for each of renaming, modularization, and planning (note that the transformations are applied sequentially). Table 2 provides a\nsummary of the generated datasets along with the instructions used to generate them. We provide complete details about the transformations in Appendix B.\nWe also simulate a simple direct synthetic data generation approach somewhat similar to Gunasekar et al. (2023). Specifically, we generate solutions for the training problems using the GPT-3.5-TURBO model. We use in-context learning with the two-shot prompt examples selected from our Dmodular dataset. To ensure diverse solutions, we use three distinct few-shot examples and generate eight solutions for every prompt at a temperature of 0.5. Additionally, we filter the solutions for correctness based on the ground truth test cases provided in the dataset to ensure we are not training on incorrect programs. Since it resembles a distillation-like setup, we refer to this dataset as Ddistill."
        },
        {
            "heading": "3.3 EXPERIMENT DETAILS",
            "text": "To evaluate the quality of the transformed datasets, we measure how they impact the test benchmark accuracy. We study both in-context learning and fine-tuning using examples from our datasets.\nModels. We use the CODELLAMA-7B model (Rozi\u00e8re et al., 2023) in all our experiments (referred as CL-7B ahead). We use the model checkpoint from huggingface1 and perform batched inference through VLLM (Kwon et al., 2023), necessary for computing the PASS@K metric. We also present the numbers from CODE-DAVINCI-002 and GPT-3.5-TURBO whenever available.\nIn-context learning. We select two question-answer pairs from the Doriginal and Dmodular training sets as our in-context learning example. For a fair comparison between the two evaluations, we use the same problem and corresponding solutions from the two datasets as examples. The examples are combined with appropriate delimiters and the model is then prompted with a new problem. Note that these in-context learning examples increase the sequence length by over 2,000 tokens and considerably slow the inference.\nFine-Tuning. We perform full fine-tuning over the base CL-7B model on the different datasets. We train the models for two epochs on the APPS dataset and one epoch on the CODE-CONTESTS dataset using a 5e\u22125 learning rate and an effective batch size of 256 on 4 A6000 GPUs."
        },
        {
            "heading": "4 EXPERIMENTAL RESULTS",
            "text": "We present our experimental results in this section. Section 4.1 first provides a qualitative overview of the transformed programs and the remaining section presents the code generation results."
        },
        {
            "heading": "4.1 ANALYSIS OF THE TRANSFORMED PROGRAMS",
            "text": "Data statistics. For the CODE-CONTESTS dataset, out of 98,582 programs extracted from the original dataset (Doriginal), we can successfully transform 92,675 (94.0%) into our modularized dataset (Dmodular). We obtain similar success rates for the APPS dataset (details deferred to the appendix). On the contrary, the distilled dataset (Ddistill), which is constructed by generating solutions directly using GPT-3.5-TURBO only finds a correct solution for about 50% of the problems.\nAnalysis of the transformed programs. We find that our transformation approach decomposes the original programs by inserting three new functions on a median (\u223c2.6 functions on average).To get a better understanding of the decomposition, we cluster the functions using their function names and signatures. We find that these helper functions often implement key program logic, standard\n1https://huggingface.co/codellama/CodeLlama-7b-hf 2Model generations were obtained from Chen et al. (2022a)\nalgorithms, and utilities like handling inputs, outputs, and orchestrating the main function. Interestingly, we also find that the helper functions are often reused across problems, with small variations in implementations. For example, the top five most frequent helper functions, dfs, build_graph, gcd, dp, and binary_search occur in about 3-8% of the problems. Additionally, we qualitatively analyze a hundred random samples from Doriginal and Dmodular datasets to determine the quality of performed transformations. Figures 4 to 11 in the appendix provide examples of such transformations. We find that most of the transformations are meaningful. They improve the readability of the programs and also find suitable decomposition for the program logic encoded in the control flow (see Figures 4, 5, 6, 14 as examples). However, in some cases, the generated helper functions can have improper names (calculate_max_colors in Figure 11) or complex implementations copied directly from the original program (count_sequences in Figure 12). Additionally, for simpler programs (Figure 13), the entire program functionality can be implemented in a single function and the decomposition does not provide any extra information. Finally, we use GPT-4 as judge (Zheng et al., 2023) evaluation to quantitatively assess the transformations in regards to their meaningfulness and about the consistency of original and transformed programs. Appendix C.1 presents the comprehensive setup. We find that over 99% of the transformations are regarded as helpful of which only 3-5% of examples are judged as can do better. Similarly, 99.4% of the transformed programs are judged as consistent with the original programs. More detailed evaluation results in Table 6.\nUnlike, generated code, we cannot constrain or check the generated natural language plans. Thus, we find that sometimes the plans can be imprecise and vary in detail. While using a stronger pretrained model like GPT-4 could alleviate some of these issues, we believe this will be a good avenue for applying something analogous to process supervision (Lightman et al., 2023)."
        },
        {
            "heading": "4.2 MAIN RESULTS",
            "text": "Tables 3 and 4 provide our primary results on APPS and CODE-CONTESTS datasets respectively. We defer the results for the APPS-COMPETITION subset to Appendix C and highlight our findings below."
        },
        {
            "heading": "4.2.1 EFFECT OF MODULARIZATION",
            "text": "We find that our data-cleaning approach improves the performance of the model on both APPS and CODE-CONTESTS datasets in both in-context learning and fine-tuning settings.\nIn-context Learning. We first evaluate the performance of the model when provided with parallel two-shot in-context learning examples from Doriginal and Dmodular datasets each. We find that the\n3Result sourced from Li et al. (2022) 4Result sourced from Zhang et al. (2023b) 5Result sourced from Li et al. (2023c)\nCODE-CONTESTS PASS@10 PASS@25 PASS@100\nIn-context Learning CL-7B + Doriginal 5.1 6.5 7.2 CL-7B + Dmodular 4.9 6.6 9.3 -0.2 +0.1 +2.1 Fine-tuning CL-7B + Doriginal 5 6.4 10.9 CL-7B + Dmodular 6.1 8.3 12.4 +1.1 +1.9 +1.5 CL-7B + Dplanning 5.3 7.0 10.8 CL-7B + Drename 4.7 6.3 10.5 Closed models ALPHACODE-9B 3 5.0 7.0 10.0 ALPHACODE-41B3 5.0 7.0 10.0 CODE-DAVINCI-002 4 3.0 - 7.5 GPT-3.5-TURBO5 - - 18.2 + BRAINSTORM5 - - 29.3\nTable 4: Result on the CODE-CONTESTS dataset. Similar to findings on the APPS dataset, we find that our data cleaning approach generally improves the performance with modularization working particularly well while planning and renaming providing marginal to no improvements.\nCODE-CONTESTS-PLAN\nPASS@10 PASS@25 PASS@100\nCL-7B + Doriginal 6.5 9.5 15.0 CL-7B + Dmodular 8.8 11.8 17.8 CL-7B + Dplanning 6.9 10.5 15.4 CL-7B + DGTplan 17.9 22.3 28.1\n+9.1 +10.5 +11.3\nTable 5: Effect of using ground-truth plans. We disentangle the high-level reasoning vs coding capabilities by extracting ground-truth plans from solutions corresponding to the test problems. We find significant improvement in the performance on the CODE-CONTESTS-PLAN dataset, indicating that the model trained on the Dplanning dataset while incapable of building correct plans, can follow such plans accurately.\nPASS@1 improves from 14.2 to 17.5 (a 23% relative improvement) on the APPS-INTRODUCTORY dataset and PASS@100 improves from 7.2 to 9.3 (a 29% relative improvement) on the CODECONTESTS dataset. These results indicate that more readablity and better-structured coding is helpful to the model in solving more problems.\nFine-tuning. Next, we fine-tune the model on the Doriginal and Dmodular datasets and again find strong performance improvements from our transformation approach. Specifically, on the APPSINTRODUCTORY dataset, the PASS@1 improves from 18.7 to 22.7 (a 23% relative improvement). Similarly, the CODE-CONTESTS dataset PASS@25 metric improves from 6.4 to 8.4 (30% relative improvement). These results cement our above findings about the effect of cleaning the data.\nInterestingly, we also note that fine-tuning only provides modest improvements over the in-context learning performance. We hypothesize that this is due to the challenging nature of our task. 6"
        },
        {
            "heading": "4.2.2 EFFECT OF PLANNING ANNOTATIONS",
            "text": "Prior work has demonstrated considerable successes in improving reasoning in LLMS (Yue et al., 2023; Magister et al., 2022; Fu et al., 2023) by performing supervised learning on natural language reasoning or planning steps. We perform similar experiment, fine-tuning the model on Dplanning dataset consisting of plans generated by our approach on top of Dmodular. We find that planning only provides a modest improvement over the Dmodular dataset (PASS@25 improved from 42.6 to 43.9 on the APPS-INTRODUCTORY dataset) or often no improvements at all.\nUpon inspection of the generated solutions, we find that often the generated plans are imprecise or incorrect, highlighting that planning still remains a bottleneck. To disentangle the high-level planning from the coding component, we analyze the performance of the model when provided with ground-truth plans on the CODE-CONTESTS dataset. We extract these ground-truth plans by applying our data transformation approach on the test set (similar to how Dplanning training set was created). Table 5 provides results on this subset of 109 problems from the CODE-CONTESTS dataset for which we were able to extract the ground truth plans (since some problems don\u2019t have a valid python solutions). While our model trained on the Dplanning dataset is incapable of synthesizing new plans, it can follow the generated plans correctly. All metrics improve significantly, e.g. PASS@100 improving from 17.8 to 28.1, well over the performance of GPT-3.5-TURBO, a much larger model!\n6Note that the in-context learning examples insert over 2,000 additional tokens to the sequence prefix and lead to a significantly slower generation speed compared to the fine-tuned models.\nOur mixed results raise critical questions for future work on improving planning in LLMS. In particular, poor performance might be attributed to any imprecision in automatically generated plans. Future data curation techniques that filter or augment this imprecision would be valueable. Alternatively, the supervised learning paradigm followed in this work might be insufficient for models to generalize planning in complex domains. Future work can explore alternative learning algorithms, possibly over our modularization approach which naturally decomposes programs."
        },
        {
            "heading": "4.2.3 ABLATIONS",
            "text": "Effect of data size. Beyond improving the quality of the resulting model, data quality is also attributed to improving the data efficiency. We evaluate this aspect by fine-tuning our model on different fractions of Doriginal and Dmodular datasets and find similar results. Figure 3 presents the performance of the model as a function of training set size. As shown in the figure, training on just one-eighth of Dmodular dataset achieves similar PASS@1 as fine-tuning on the entire Doriginal. Effect of renaming. We use variable renaming as an intermediate step in our cleaning process. We evaluate the performance of the model fine-tuned only on the Drename dataset and find that renaming provides some performance improvements when compared to fine-tuning on Doriginal dataset. For example, PASS@1 improved from 17.2 to 19.1 on APPS-INTRODUCTORY. However, renaming still performs worse in comparison to fine-tuning on the Dmodular. This highlights that beyond just readable code, functional decomposition is also a key aspect of improving our performance.\nCleaning Transformations vs Distillation. We compare our transformation approach with a direct distillation baseline where we directly generate solutions using GPT-3.5-TURBO, referred to as the Ddistill dataset7. This corresponds to various LLM instruction or fine-tuning approaches (Xu et al., 2023; Li et al., 2023b) providing a strong baseline for data cleaning. On the APPS-INTRODUCTORY dataset, we find that fine-tuning on the Dmodular dataset achieves better performance compared to the Ddistill dataset demonstrating the advantage of cleaning over the generation baseline. Choice of transformation model. To evaluate how the choice of transformation model affects performance, we use the GPT-4-TURBO model to transform on a subset of the training set (detailed setup in Appendix C.3). GPT-4-TURBO, a stronger model, performs the transformations successfully and the resulting model trained on this version of the modularized dataset achieves even higher accuracy. For instance, PASS@10 improves from 33.0 when using Dmodular constructed with GPT3.5-TURBO to 34.3 when using the Dmodular constructed with GPT-4-TURBO (full results in Table 8)."
        },
        {
            "heading": "4.2.4 COMPARISON TO OTHER BASELINES",
            "text": "Beyond CL-7B, fine-tuned models outperform strong baselines like ALPHACODE on the CODECONTESTS dataset but still lag behind larger CODE-DAVINCI-002 and GPT-3.5-TURBO models.\n7Note that we generate these solutions using in-context examples from the Dmodular dataset"
        },
        {
            "heading": "4.2.5 CASE STUDY OF GENERATED MODULARIZED PROGRAM",
            "text": "Figure 2 provides an example of a program correctly generated by a model fine-tuned on our Dmodular dataset. The problem requires removing rows and columns containing cells with certain attributes (i.e., if the cell is white) The modularized solution correctly identifies the steps required to solve the problem and implements them as separate helper functions, providing readable code."
        },
        {
            "heading": "5 RELATED WORK",
            "text": "Instruction tuning. Instruction tuning refers to the process of finetuning a base pretrained LLM to perform general-purpose tasks and follow instructions. Recent works, Zhou et al. (2023); Cao et al. (2023); Chen et al. (2023a) have demonstrated that a small high-quality instruction corpus is sufficient for achieving good instruction tuning performance. Here, we perform task-specific finetuning of LLMS and observe similar performance improvements.\nSynthetic data for LLMS. Recent works have explored using synthetic datasets for generalpurpose or task-specific finetuning of LLMS. These approaches work by generating synthetic datasets from a strong LLM (like GPT-3.5-TURBO or GPT-4) using a set of existing tasks (Taori et al., 2023; Chiang et al., 2023) or generating new tasks using self-instruct (Wang et al., 2022) or evol-instruct (Xu et al., 2023) approaches. This has been also applied for task-specific finetuning \u2013 in common-sense reasoning (West et al., 2022), text-summarization (Sclar et al., 2022), mathematical reasoning (Luo et al., 2023a; Yue et al., 2023), tool use (Patil et al., 2023), coding (Luo et al., 2023b), and general-purpose reasoning Li et al. (2023b); Zelikman et al. (2022).\nMore specifically, Yue et al. (2023) curates diverse corpus of mathematics problems with chain-ofthought or program-of-thought (Chen et al., 2022b) annotations for mathematical reasoning analogous to our plans. Gunasekar et al. (2023) proposed pre-training models on programming \u201ctextbooks\u201d generated synthetically from GPT-3.5-TURBO. Haluptzok et al. (2023) similarly generates programming puzzles and corresponding solutions from language models. Our work also studies curating synthetic data for code-generation space. However, instead of directly generating data using LLMS, we identify good programming patterns and clean existing datasets using them.\nAlgorithmic Code Generation. Code generation is a broad domain and is covered in Appendix D. We only discuss pertinent algorithmic code generation works here. Hendrycks et al. (2021) released the APPS dataset while Li et al. (2022) released the CODE-CONTESTS dataset with the ALPHACODE models. Zhang et al. (2023c) proposed a lookahead-search-based decoding algorithm for improving reasoning in LLMS and is orthogonal to our work. Chen et al. (2022a); Zhang et al. (2023b) proposed CODET and ALGO, that use generated tests (using LLM or brute-force solution) to re-rank the generated solutions. Zelikman et al. (2023) proposed the PARSEL approach which used the CODE-DAVINCI-002 model to first generate a plan in their high-level problem-specification language and then generate a program using it. Li et al. (2023a) also study disentangling the planning and code generation capabilities for closed source LLMS, similar to our experiments on open models."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "Traditionally, data quality has primarily been linked to functional correctness, ignoring the rich stylistic aspects differing across programs. In this work, we demonstrate that stylistic aspects like readability, and program structuring actually impact the performance of the trained model on downstream tasks and thus also contribute to data quality. Next, we proposed a novel data-cleaning pipeline demonstrating that LLMS can be used for transforming existing datasets to improve their quality based on user-instructions and oracle equivalence checker. While our evaluations focused on the algorithmic code generation task, we believe that this approach would also be useful for other domains for improving data quality as well. In particular, even in the absence of oracle checkers like test cases, we believe that there is an opportunity to use learned \u201coracles\u201d for ensuring consistency and quality in other domains akin to how observed in Sclar et al. (2022). Finally, beyond improving algorithmic code generation, we believe our modularization approach can also be useful for general software engineering use cases (test generation, verification, etc.) where modularity is beneficial."
        },
        {
            "heading": "A EXPERIMENTAL SETUP",
            "text": "APPS benchmark. Since some of the problems in the APPS dataset are sourced from websites that provide insufficient or absent test cases, we filter the problems from those platforms. Specifically, we only retain problems from the codeforces, codechef, and atcoder competition websites. This also removes the disparity/domain-shift between the training and test splits which has been observed as an issue in the APPS dataset in prior works (Section 4.1 in Li et al. (2023c)). While we considerably reduced the size of our training set, our test set is still quite close to the test set containing around 3800 problems instead of the default 5000.\nCODE-CONTESTS benchmark. The original CODE-CONTESTS benchmark consists of 13,328 problems in the training dataset. We restrict the dataset to only problems with valid python solutions that pass the test cases. Next, since the original dataset provides over 100 solutions per problem, we perform minhash-based deduplication on the solutions (hash size=64, num bands=60, band size=5) from gaoya8 and retain a maximum of 25 solutions per problem. This results in about 7k problems in the training set spanning about 98.5k solutions. We do not perform any filtering on the test set.\nAdditionally, we note that some of the provided solutions in both APPS and CODE-CONTESTS datasets do not pass the test cases. These cases are sometimes caused by incorrect programs, often because of scraping solutions with the wrong language. However, more often problems in these datasets support multiple correct solutions (for instance solutions to a problem can return a list of elements in any order). The ground truth test cases only check for a single correct solution and thus result in many solutions failing the test cases. We retain such samples for the smaller APPS dataset and instead check whether the transformed program behavior is similar to the original program.\nMetrics We use the PASS@K to perform our evaluations. We perform nucleus sampling using VLLM with p = 0.95. We outline the default sampling configurations used for computing the metrics\n1. PASS@1 - We use a sampling budget (N ) = 10 and temperature = 0.1. 2. PASS@10 - We use a sampling budget (N ) = 50 and temperature = 0.6. 3. PASS@25 - We use a sampling budget (N ) = 50 and temperature = 0.6. 4. PASS@100 - We use a sampling budget (N ) = 200 and temperature = 0.8.\nFinetuning details We finetune the CODELLAMA-7B model using deepspeed huggingface trainer. We use the following training configuration for our main experiments -\nTraining Parameters Values LR 5e\u22125 Epochs 1 or 2 depending on the dataset Batch Size 256 (combing grad. accumulation) Dtype bf16\n8https://github.com/serega/gaoya"
        },
        {
            "heading": "B CODE TRANSFORMATIONS IMPLEMENTATION",
            "text": "We implement our code transformation approach using zero-shot prompting with GPT-3.5-TURBO model. After transformation, we extract the generated code and evaluate its functional correctness using the provided test cases. In case the program does not pass, we retry the process with up to a maximum of 5 attempts. In our experience, instruction-tuned models can follow precise commands and transform programs very well."
        },
        {
            "heading": "B.1 RENAMING",
            "text": "We use the following prompt to perform renaming."
        },
        {
            "heading": "QUESTION :",
            "text": "{ p r o b l e m _ s t a t e m e n t }\nANSWER: \u2018 \u2018 \u2018 py thon { s o l u t i o n } \u2018 \u2018 \u2018 Rename t h e v a r i a b l e s i n t h e program t o be d e s c r i p t i v e , mean ing fu l , and c o n s i s t e n t . Do n o t change t h e o r i g i n a l s e m a n t i c s o f t h e program . E n c l o s e t h e program w i t h i n b a c k t i c k s a s shown above and remember t o use d e s c r i p t i v e v a r i a b l e names ."
        },
        {
            "heading": "B.2 MODULARIZATION",
            "text": "Unlike renaming, we perform two rounds of modularization in case the generated program consists of long function implementations (hinting that the function can be decomposed further). We use the following prompt to perform the first round of modularization"
        },
        {
            "heading": "QUESTION :",
            "text": "{ p r o b l e m _ s t a t e m e n t }\nANSWER: \u2018 \u2018 \u2018 py thon { r e n a m e d _ s o l u t i o n } \u2018 \u2018 \u2018 R e f a c t o r t h e above program . Fol low t h e g u i d e l i n e s * make t h e program more modular wi th s m a l l e r and m e a n i n g f u l h e l p e r f u n c t i o n s * good d e s c r i p t i v e names f o r t h e h e l p e r f u n c t i o n s * have an e n t r y f u n c t i o n c a l l e d \u2018 main ( ) \u2018 * \u2018 main ( ) \u2018 i s c a l l e d i n s i d e \u2018 i f __name__ == \u2019 __main__ \u2019 \u2018\nDo n o t change t h e o r i g i n a l s e m a n t i c s o f t h e program s i g n i f i c a n t l y and no need t o pe r fo rm o p t i m i z a t i o n s . E n c l o s e t h e program w i t h i n b a c k t i c k s as shown above\nNext, in case the modularized program contains a function with the number of lines greater than 20, we further prompt the model while signaling which functions to further decompose. This occurs in about 20-40% of modularized solutions and we use the following prompt."
        },
        {
            "heading": "QUESTION :",
            "text": "{ p r o b l e m _ s t a t e m e n t }\nANSWER: \u2018 \u2018 \u2018 py thon { m o d u l a r i z e d _ s o l u t i o n } \u2018 \u2018 \u2018 R e f a c t o r t h e above program by m o d u l a r i z i n g i t and b r e a k i n g down long and complex f u n c t i o n s i n t o s m a l l e r m e a n i n g f u l h e l p e r f u n c t i o n s . P a r t i c u l a r l y r e f a c t o r and decompose t h e f o l l o w i n g f u n c t i o n ( s ) i n t o s m a l l e r h e l p e r f u n c t i o n s \u2212 { f u n c t i o n _ n a m e s _ s t r i n g } Only r e t u r n t h e r e f a c t o r e d program e n c l o s e d i n b a c k t i c k s as shown above . \" \" \""
        },
        {
            "heading": "B.3 PLANNING",
            "text": "We use the following prompt to generate natural language plans"
        },
        {
            "heading": "QUESTION :",
            "text": "{ p r o b l e m _ s t a t e m e n t }\nANSWER: \u2018 \u2018 \u2018 py thon { m o d u l a r i z e d _ s o l u t i o n } \u2018 \u2018 \u2018 G e n e r a t e a summary f o r t h e f o l l o w i n g f u n c t i o n s and c l a s s e s i n t h e program w i t h i n f o u r l i n e s each . The summaries s h o u l d be d e s c r i p t i v e and h e l p f u l f o r u n d e r s t a n d i n g t h e program ( however y e t c o n c i s e i n f o u r l i n e s ) . The f u n c t i o n s and c l a s s e s a r e \u2212 { l i s t _ o f _ f u n c t i o n _ n a m e s } Fol low t h e p r o v i d e d f o r m a t f o r t h e summaries w h i l e b e i n g i n f o r m a t i v e and c o n c i s e . E n c l o s e t h e\ns i g n a t u r e s i n b a c k t i c k s as shown above ."
        },
        {
            "heading": "C ADDITIONAL RESULTS",
            "text": ""
        },
        {
            "heading": "C.1 GPT-4 JUDGE EVALUATION FOR THE TRANSFORMATIONS",
            "text": "We here present some quantitative evidence of the improvements made from our transformation approach. However, since the transformations are free-form code generation, we rely on using GPT-4 as judge, an evaluation approach gaining popularity for evaluating free-form language outputs (Zheng et al., 2023; Zhuo, 2023). Specifically, we ask the language model to answer whether the modularized refactored code has better variable names, better function decomposition, and is consistent with the original program. The model can provide answers on a key of 1-3 from comparison questions and 0-1 for the consistency question. The following prompt depicts our approach"
        },
        {
            "heading": "SYSTEM PROMPT:",
            "text": "P l e a s e a c t a s an i m p a r t i a l j u d g e and e v a l u a t e t h e code r e f a c t o r i n g below . You need t o e v a l u a t e whe the r t h e r e f a c t o r e d program u s e s b e t t e r and c o r r e c t v a r i a b l e names , r e f a c t o r s t h e\ni m p l e m e n t a t i o n i n t o c o r r e c t s m a l l e r h e l p e r f u n c t i o n s and c o n s i s t e n c y wi th t h e o r i g i n a l program . Your e v a l u a t i o n s h o u l d be based on c o r r e c t n e s and h e l p f u l n e s s o f t h e r e f a c t o r i n g i n b e t t e r u n d e r s t a n d i n g t h e problem and a l s o i f i t i s s t i l l c o n s i s t e n t w i th t h e o r i g i n a l program , i . e . i t f o l l o w s s i m i l a r program l o g i c and a l g o r i t h m .\n* For e v a l u a t i n g v a r i a b l e names and f u n c t i o n d e c o m p o s i t i o n , p l e a s e g i v e a s c o r e from 1 t o 3 where 1 means t h e r e f a c t o r i n g i s n o t h e l p f u l a t a l l , 2 means t h e r e f a c t o r i n g i s somewhat h e l p f u l and 3 means t h e r e f a c t o r i n g i s ve ry h e l p f u l . Example f o r m a t\nV a r i a b l e names r e a s o n i n g : [ [ r e a s o n i n g f o r t h e v a r i a b l e names s c o r e , o f t e n a s s e s s i n g whe the r t h e v a r i a b l e names a r e more d e s c r i p t i v e and m e a n i n g f u l and c o r r e c t l y r e f l e c t t h e v a r i a b l e \u2019 s p u r p o s e ] ] V a r i a b l e names : [ [ 1 ] ] o r [ [ 2 ] ] o r [ [ 3 ] ]\nF u n c t i o n d e c o m p o s i t i o n r e a s o n i n g : [ [ r e a s o n i n g f o r t h e d e c o m p o s i t i o n s c o r e , o f t e n a s s e s s i n g whe the r some f u n c t i o n i s t o o long , p o s s i b i l i t y t o pe r fo rm f u r t h e r a b s t r a c t i o n s , c h o i c e o f a b s t r a c t i o n s , h e l p e r f u n c t i o n names ] ] F u n c t i o n d e c o m p o s i t i o n : [ [ 1 ] ] o r [ [ 2 ] ] o r [ [ 3 ] ]\n* For e v a l u a t i n g c o n s i s t e n c y , p l e a s e g i v e a s c o r e o f 0 i f t h e r e f a c t o r e d program i s n o t c o n s i s t e n t w i th t h e o r i g i n a l program and 1 i f i t i s c o n s i s t e n t . Example f o r m a t\nC o n s i s t e n c y r e a s o n i n g : [ [ r e a s o n i n g f o r t h e c o n s i s t e n c y s c o r e , o f t e n a s s e s s i n g whe the r t h e r e f a c t o r e d program f o l l o w s s i m i l a r program l o g i c and a l g o r i t h m as t h e o r i g i n a l program ] ] C o n s i s t e n c y : [ [ 0 ] ] o r [ [ 1 ] ]"
        },
        {
            "heading": "QUESTION :",
            "text": "{ p r o b l e m _ s t a t e m e n t }"
        },
        {
            "heading": "ORIGINAL SOLUTION :",
            "text": "{ s o l u t i o n }"
        },
        {
            "heading": "REFACTORED SOLUTION :",
            "text": "{ s o l u t i o n }\nWhile this evaluation might portray certain subtle biases, we believe it still provides us a signal to assess the quality of the transformations. To reduce costs, we the GPT-4 as judge evaluation to 1000 problems in the APPS dataset 9. GPT-4 followed the proposed format for 998 solutions and we present results on them in Table 6. Results demonstrate that most of the applied transformations are meaningful while remaining consistent with the original ground truth solutions.\n9Our prompt spans around 1.5-2k tokens including problem, original, and refactored programs leading to high costs\nTo get better insights into GPT-4 evaluations, we look at the examples which receive lower scores. The scores and associated reasoning appear meaningful. For example, in Figure 14, the modularized program is already significantly more readable than original renamed program. However, GPT-4 identifies that the calculate_permutation_even and calculate_permutation_even helper functions are virtually the same and can be abstracted further. Note that this transformation is an artifiact of the fact that original program consisted of same program logic distributed across two far apart if conditions. Similarly, in Figure 15, GPT-4 identified some unmodified variable names like t while acknowledging other improvements such as sky to heights giving it a rating of 2. The rating of 1 is provided when the transformation does not modify any variable names or does not decompose existing functions, as evidenced by score distributed, a rare occurrence. Indeed, often the examples marked with a rating 2 actually improve upon orignal code in non-trivial ways.10"
        },
        {
            "heading": "C.2 APPS-COMPETITION RESULTS",
            "text": "We present the results on APPS-COMPETITION dataset here."
        },
        {
            "heading": "C.3 ABLATION ON CHOICE OF MODEL",
            "text": "We use GPT-3.5-TURBO as the default model for performing the transformations in the main experiments since it provides a nice balance between the accuracy and cost of performing the transformations. Here, to demonstrate the generality of our approach we perform an ablation by replacing the transformation model with GPT-4-TURBO. Since this model is about 8-10x more expensive than GPT-3.5-TURBO, we perform this ablation on a subset of 5k programs sampled from the dataset.\nExperimental setup. We repeat the renaming and modularization steps described in Section 3.2 using the GPT-4-TURBO model. We call the resulting transformed dataset as D4modular. Next, fairly compare the resulting dataset with the original and modularized dataset generated using GPT3.5-TURBO, we sample the corresponding parallel original and transformed programs and call them Doriginal and D3.5modular datasets.\n10Curiously, GPT-4 sometimes returned a rating of 2.5 instead of integer 2 or 3. We rounded it to 2, thus making our evaluation harsher!"
        },
        {
            "heading": "D ADDITIONAL RELATED WORK",
            "text": "Code LLMS have been used for multiple domains in various lines of approaches. Here, we present a few key approaches and recommend the reader to Hou et al. (2023) for a detailed survey. Chen et al. (2021) released the CODE-DAVINCI-002 model and evaluate it for code generation. Since then, LLMS have been used for a variety of domains such as data science (Jain et al.; Lai et al., 2022; Yin et al., 2023), apis (Zhiruo Wang & Neubig, 2022; Patil et al., 2023), and repositories (Zhang et al., 2023a; Bairi et al., 2023; Shrivastava et al., 2023). (Le et al., 2022; Shojaee et al., 2023; Liu et al., 2023b) use reinforcement learning with compilation/execution feedback to fine-tune code LLMS for (algorithmic) code generation task.\nOther works have approached code generation from different fronts, exploring planning (Jiang et al., 2023), repair (Chen et al., 2023b; Shinn et al., 2023), constrain generation (Key et al., 2022), and prompt optimization (Liu et al., 2023a).\nE EXAMPLES OF TRANSFORMED PROGRAM"
        }
    ],
    "year": 2023
}