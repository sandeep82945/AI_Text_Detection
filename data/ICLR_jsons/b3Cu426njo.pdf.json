{
    "abstractText": "Relying on prior knowledge accumulated from related tasks, meta-learning offers a powerful approach to learning a novel task from limited training data. Recent approaches parameterize the prior with a family of probability density functions or recurrent neural networks, whose parameters can be optimized by utilizing validation data from the observed tasks. While these approaches have appealing empirical performance, the expressiveness of their prior is relatively low, which limits the generalization and interpretation of meta-learning. Aiming at expressive yet meaningful priors, this contribution puts forth a novel prior representation model that leverages the notion of algorithm unrolling. The key idea is to unroll the proximal gradient descent steps, where learnable piecewise linear functions are developed to approximate the desired proximal operators within tight theoretical error bounds established for both smooth and non-smooth proximal functions. The resultant multi-block neural network not only broadens the scope of learnable priors, but also enhances interpretability from an optimization viewpoint. Numerical tests conducted on few-shot learning datasets demonstrate markedly improved performance with flexible, visualizable, and understandable priors.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yilang Zhang"
        },
        {
            "affiliations": [],
            "name": "Georgios B. Giannakis"
        }
    ],
    "id": "SP:89cd77fb7f12d0c5e3de38a7d124a118c57994d8",
    "references": [
        {
            "authors": [
                "Momin Abbas",
                "Quan Xiao",
                "Lisha Chen",
                "Pin-Yu Chen",
                "Tianyi Chen"
            ],
            "title": "Sharp-MAML: Sharpnessaware model-agnostic meta learning",
            "venue": "In Proc. Int. Conf. Machine Learn.,",
            "year": 2022
        },
        {
            "authors": [
                "Milad Abdollahzadeh",
                "Touba Malekzadeh",
                "Ngai-Man (Man) Cheung"
            ],
            "title": "Revisit multimodal metalearning through the lens of multi-task learning",
            "venue": "In Proc. Adv. Neural Info. Process. Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Han Altae-Tran",
                "Bharath Ramsundar",
                "Aneesh S. Pappu",
                "Vijay Pande"
            ],
            "title": "Low data drug discovery with one-shot learning",
            "venue": "ACS Central Science,",
            "year": 2017
        },
        {
            "authors": [
                "Marcin Andrychowicz",
                "Misha Denil",
                "Sergio G\u00f3mez",
                "Matthew W Hoffman",
                "David Pfau",
                "Tom Schaul",
                "Brendan Shillingford",
                "Nando de Freitas"
            ],
            "title": "Learning to learn by gradient descent by gradient descent",
            "venue": "In Proc. Adv. Neural Info. Process. Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Sungyong Baik",
                "Seokil Hong",
                "Kyoung Mu Lee"
            ],
            "title": "Learning to forget for meta-learning",
            "venue": "In Proc. Conf. Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Sungyong Baik",
                "Janghoon Choi",
                "Heewon Kim",
                "Dohee Cho",
                "Jaesik Min",
                "Kyoung Mu Lee"
            ],
            "title": "Metalearning with task-adaptive loss function for few-shot learning",
            "venue": "In Proc. Int. Conf. Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Samy Bengio",
                "Yoshua Bengio",
                "Jocelyn Cloutier"
            ],
            "title": "On the search for new learning rules for anns",
            "venue": "Neural Processing Letters,",
            "year": 1995
        },
        {
            "authors": [
                "Luca Bertinetto",
                "Joao F. Henriques",
                "Philip Torr",
                "Andrea Vedaldi"
            ],
            "title": "Meta-learning with differentiable closed-form solvers",
            "venue": "In Proc. Int. Conf. Learn. Represention,",
            "year": 2019
        },
        {
            "authors": [
                "Ignasi Clavera",
                "Anusha Nagabandi",
                "Simin Liu",
                "Ronald S. Fearing",
                "Pieter Abbeel",
                "Sergey Levine",
                "Chelsea Finn"
            ],
            "title": "Learning to adapt in dynamic, real-world environments through metareinforcement learning",
            "venue": "In Proc. Int. Conf. Learn. Represention,",
            "year": 2019
        },
        {
            "authors": [
                "Alireza Fallah",
                "Aryan Mokhtari",
                "Asuman Ozdaglar"
            ],
            "title": "On the convergence theory of gradientbased model-agnostic meta-learning algorithms",
            "venue": "In Proc. Int. Conf. Artif. Intel. and Stats.,",
            "year": 2020
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "In Proc. Int. Conf. Machine Learn.,",
            "year": 2017
        },
        {
            "authors": [
                "Chelsea Finn",
                "Kelvin Xu",
                "Sergey Levine"
            ],
            "title": "Probabilistic model-agnostic meta-learning",
            "venue": "In Proc. Adv. Neural Info. Process. Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Sebastian Flennerhag",
                "Andrei A. Rusu",
                "Razvan Pascanu",
                "Francesco Visin",
                "Hujun Yin",
                "Raia Hadsell"
            ],
            "title": "Meta-learning with warped gradient descent",
            "venue": "In Proc. Int. Conf. Learn. Represention,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Gordon",
                "John Bronskill",
                "Matthias Bauer",
                "Sebastian Nowozin",
                "Richard Turner"
            ],
            "title": "Metalearning probabilistic inference for prediction",
            "venue": "In Proc. Int. Conf. Learn. Represention,",
            "year": 2019
        },
        {
            "authors": [
                "Erin Grant",
                "Chelsea Finn",
                "Sergey Levine",
                "Trevor Darrell",
                "Thomas Griffiths"
            ],
            "title": "Recasting gradientbased meta-learning as hierarchical bayes",
            "venue": "In Proc. Int. Conf. Learn. Represention,",
            "year": 2018
        },
        {
            "authors": [
                "Karol Gregor",
                "Yann LeCun"
            ],
            "title": "Learning fast approximations of sparse coding",
            "venue": "In Proc. Int. Conf. Machine Learn.,",
            "year": 2010
        },
        {
            "authors": [
                "R\u00e9mi Gribonval",
                "Mila Nikolova"
            ],
            "title": "A characterization of proximity operators",
            "venue": "J. Mathematical Imaging and Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Jiatao Gu",
                "Yong Wang",
                "Yun Chen",
                "Victor O.K. Li",
                "Kyunghyun Cho"
            ],
            "title": "Meta-learning for lowresource neural machine translation",
            "venue": "In Proc. Empirical Methods in Natural Language Process.,",
            "year": 2018
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proc. Conf. Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "A. Steven Younger",
                "Peter R. Conwell"
            ],
            "title": "Learning to learn using gradient descent",
            "venue": "In Prof. Intl. Conf. Artif. Neural Networks,",
            "year": 2001
        },
        {
            "authors": [
                "Seyed Amir Hossein Hosseini",
                "Burhaneddin Yaman",
                "Steen Moeller",
                "Mingyi Hong",
                "Mehmet Ak\u00e7akaya"
            ],
            "title": "Dense recurrent neural networks for accelerated mri: History-cognizant unrolling of optimization algorithms",
            "venue": "IEEE J. Sel. Topics Sig. Process.,",
            "year": 2020
        },
        {
            "authors": [
                "Jeremy Howard",
                "Sebastian Ruder"
            ],
            "title": "Universal language model fine-tuning for text classification",
            "venue": "arXiv preprint arXiv:1801.06146,",
            "year": 2018
        },
        {
            "authors": [
                "Samuel Hurault",
                "Arthur Leclaire",
                "Nicolas Papadakis"
            ],
            "title": "Proximal denoiser for convergent plugand-play optimization with nonconvex regularization",
            "venue": "In Proc. Int. Conf. Machine Learn.,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiyi Ji",
                "Jason D Lee",
                "Yingbin Liang",
                "H. Vincent Poor"
            ],
            "title": "Convergence of meta-learning with task-specific adaptation over partial parameters",
            "venue": "In Proc. Adv. Neural Info. Process. Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiyi Ji",
                "Junjie Yang",
                "Yingbin Liang"
            ],
            "title": "Theoretical convergence of multi-step model-agnostic meta-learning",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2022
        },
        {
            "authors": [
                "Kwonjoon Lee",
                "Subhransu Maji",
                "Avinash Ravichandran",
                "Stefano Soatto"
            ],
            "title": "Meta-learning with differentiable convex optimization",
            "venue": "In Proc. Conf. Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Yoonho Lee",
                "Seungjin Choi"
            ],
            "title": "Gradient-based meta-learning with learned layerwise metric and subspace",
            "venue": "In Proc. Int. Conf. Machine Learn.,",
            "year": 2018
        },
        {
            "authors": [
                "Ke Li",
                "Jitendra Malik"
            ],
            "title": "Learning to optimize",
            "venue": "In Proc. Int. Conf. Learn. Represention,",
            "year": 2017
        },
        {
            "authors": [
                "Mao Li",
                "Yingyi Ma",
                "Xinhua Zhang"
            ],
            "title": "Proximal mapping for deep regularization",
            "venue": "In Proc. Adv. Neural Info. Process. Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yuelong Li",
                "Mohammad Tofighi",
                "Junyi Geng",
                "Vishal Monga",
                "Yonina C. Eldar"
            ],
            "title": "Efficient and interpretable deep blind image deblurring via algorithm unrolling",
            "venue": "IEEE T. Comput. Imaging,",
            "year": 2020
        },
        {
            "authors": [
                "Zhenguo Li",
                "Fengwei Zhou",
                "Fei Chen",
                "Hang Li"
            ],
            "title": "Meta-sgd: Learning to learn quickly for fewshot learning",
            "venue": "arXiv preprint arXiv:1707.09835,",
            "year": 2017
        },
        {
            "authors": [
                "Morteza Mardani",
                "Qingyun Sun",
                "David Donoho",
                "Vardan Papyan",
                "Hatef Monajemi",
                "Shreyas Vasanawala",
                "John Pauly"
            ],
            "title": "Neural proximal gradient descent for compressive imaging",
            "venue": "In Proc. Adv. Neural Info. Process. Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Nikhil Mishra",
                "Mostafa Rohaninejad",
                "Xi Chen",
                "Pieter Abbeel"
            ],
            "title": "A simple neural attentive metalearner",
            "venue": "In Proc. Int. Conf. Learn. Represention,",
            "year": 2018
        },
        {
            "authors": [
                "Vishal Monga",
                "Yuelong Li",
                "Yonina C. Eldar"
            ],
            "title": "Algorithm unrolling: Interpretable, efficient deep learning for signal and image processing",
            "venue": "IEEE Sig. Process. Mag.,",
            "year": 2021
        },
        {
            "authors": [
                "Cuong Nguyen",
                "Thanh-Toan Do",
                "Gustavo Carneiro"
            ],
            "title": "Uncertainty in model-agnostic metalearning using variational inference",
            "venue": "In Proc. Winter Conf. App. Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Aniruddh Raghu",
                "Maithra Raghu",
                "Samy Bengio",
                "Oriol Vinyals"
            ],
            "title": "Rapid learning or feature reuse? towards understanding the effectiveness of maml",
            "venue": "In Proc. Int. Conf. Learn. Represention,",
            "year": 2020
        },
        {
            "authors": [
                "Aravind Rajeswaran",
                "Chelsea Finn",
                "Sham M Kakade",
                "Sergey Levine"
            ],
            "title": "Meta-learning with implicit gradients",
            "venue": "In Proc. Adv. Neural Info. Process. Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Sachin Ravi",
                "Alex Beatson"
            ],
            "title": "Amortized bayesian meta-learning",
            "venue": "In Proc. Int. Conf. Learn. Represention,",
            "year": 2019
        },
        {
            "authors": [
                "Sachin Ravi",
                "Hugo Larochelle"
            ],
            "title": "Optimization as a model for few-shot learning",
            "venue": "In Proc. Int. Conf. Learn. Represention,",
            "year": 2017
        },
        {
            "authors": [
                "Mengye Ren",
                "Sachin Ravi",
                "Eleni Triantafillou",
                "Jake Snell",
                "Kevin Swersky",
                "Josh B. Tenenbaum",
                "Hugo Larochelle",
                "Richard S. Zemel"
            ],
            "title": "Meta-learning for semi-supervised few-shot classification",
            "venue": "In Proc. Int. Conf. Learn. Represention,",
            "year": 2018
        },
        {
            "authors": [
                "Adam Santoro",
                "Sergey Bartunov",
                "Matthew Botvinick",
                "Daan Wierstra",
                "Timothy Lillicrap"
            ],
            "title": "Metalearning with memory-augmented neural networks",
            "venue": "In Proc. Int. Conf. Machine Learn.,",
            "year": 2016
        },
        {
            "authors": [
                "J. Schmidhuber"
            ],
            "title": "A neural network that embeds its own meta-levels",
            "venue": "In IEEE Intl. Conf. on Neural Networks, pp. 407\u2013412",
            "year": 1993
        },
        {
            "authors": [
                "David Silver",
                "Aja Huang",
                "Chris J Maddison",
                "Arthur Guez",
                "Laurent Sifre",
                "George Van Den Driessche",
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Veda Panneershelvam",
                "Marc Lanctot"
            ],
            "title": "Mastering the game of go with deep neural networks and tree",
            "venue": "search. nature,",
            "year": 2016
        },
        {
            "authors": [
                "Jake Snell",
                "Kevin Swersky",
                "Richard Zemel"
            ],
            "title": "Prototypical networks for few-shot learning",
            "venue": "In Proc. Adv. Neural Info. Process. Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Flood Sung",
                "Yongxin Yang",
                "Li Zhang",
                "Tao Xiang",
                "Philip H.S. Torr",
                "Timothy M. Hospedales"
            ],
            "title": "Learning to compare: Relation network for few-shot learning",
            "venue": "In Proc. Conf. Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Hongduan Tian",
                "Bo Liu",
                "Xiao-Tong Yuan",
                "Qingshan Liu"
            ],
            "title": "Meta-learning with network pruning",
            "venue": "In Proc. European Conf. Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Hongduan Tian",
                "Bo Liu",
                "Xiao-Tong Yuan",
                "Qingshan Liu"
            ],
            "title": "Meta-learning with network pruning",
            "venue": "In Proc. European Conf. Computer Vision, pp",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Proc. Adv. Neural Info. Process. Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Charles Blundell",
                "Timothy Lillicrap",
                "koray kavukcuoglu",
                "Daan Wierstra"
            ],
            "title": "Matching networks for one shot learning",
            "venue": "In Proc. Adv. Neural Info. Process. Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Lianzhe Wang",
                "Shiji Zhou",
                "Shanghang Zhang",
                "Xu Chu",
                "Heng Chang",
                "Wenwu Zhu"
            ],
            "title": "Improving generalization of meta-learning with inverted regularization at inner-level",
            "venue": "In Proc. Conf. Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Yilang Zhang",
                "Bingcong Li",
                "Shijian Gao",
                "Georgios B. Giannakis"
            ],
            "title": "Scalable bayesian metalearning through generalized implicit gradients",
            "venue": "In Proc. AAAI Conf. Artif. Intel.,",
            "year": 2023
        },
        {
            "authors": [
                "\u2212 x)dx"
            ],
            "title": "Compared to non-expansive operators, proximal operators induced by non-convex regularizers have gained popularity in recent years thanks to their enhanced expressiveness, and their convergence guarantees (Hurault et al., 2022). Our method fails precisely within this category. Additionally, the PLFs should be monotone to qualify as a valid proximal operator; see e.g., (Gribonval",
            "venue": "In practice,",
            "year": 2020
        },
        {
            "authors": [],
            "title": "(xn,yn)}n=1, it is possible to enhance the accuracy of x\u0302 by learning a more efficient optimization rule. In (Gregor & LeCun, 2010), the two steps (60) of ISTA for each k = 1",
            "year": 2010
        },
        {
            "authors": [
                "Beatson",
                "Nguyen"
            ],
            "title": "Flennerhag et al., 2020), and implicit Gaussian (Baik et al., 2020",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "While deep learning has achieved documented success in a broad range of applications (Silver et al., 2016; He et al., 2016; Vaswani et al., 2017), it often requires huge data records to train large-scale and high-capacity models. In contrast, human intelligence is capable of identifying new objects or concepts from merely a few samples. How to incorporate this ability into \u201cmachine intelligence\u201d has garnered great attention and interest in a number of domains, especially when data are scarce or costly to collect. Examples of such applications include drug molecule discovery (Altae-Tran et al., 2017), low-resource machine translation (Gu et al., 2018), and robotics (Clavera et al., 2019).\nMotivated by the fact that humans acquire new knowledge efficiently from past experiences, a principled framework has been investigated to mimic this ability of humans, known as learning-to-learn or meta-learning (Thrun & Pratt, 1998). Meta-learning aims to identify a task-invariant prior from a class of (partially) related tasks, which can be used to facilitate the learning of new tasks from the same class. The underlying assumption of meta-learning is that all tasks of interest are linked through their data distribution or latent problem structure. Thus, task-invariant common prior knowledge can be acquired as an inductive bias, and thereby transferred to new tasks (Thrun & Pratt, 1998). By doing so, even a couple of training data can suffice for learning a new task.\nConventional meta-learning methods rely on prescribed criteria to extract the prior; see e.g., (Schmidhuber, 1993; Bengio et al., 1995). With recent advances of deep learning, these handcrafted approaches have been replaced by data-driven ones, where a meta-learner captures the prior information across tasks, while a base-learner utilizes this prior to aid per-task learning. The desired prior is encoded in the base-learner parameters shared across tasks, and can be learned by optimizing a loss over the given tasks. Early attempts to this end utilize a neural network (NN) to represent the prior (Santoro et al., 2016; Mishra et al., 2018; Ravi & Larochelle, 2017). The base-learner employs e.g., recurrent neural networks (RNNs) with input training data per task, and output parameters for\nthe task-specific model. However, the choices of the NNs heavily depend on the task-specific model, and the black-box nature of NNs makes them susceptible to poor interpretability and reliability.\nAs opposed to model-based meta-learning, model-agnostic meta-learning (MAML) extracts the prior without presuming the task-specific model beforehand (Finn et al., 2017). MAML resorts to an iterative optimizer to obtain the per-task model parameters. The prior information is reflected in the initialization of the model parameters, which is shared across tasks. Building upon MAML, various optimization-based meta-learning algorithms have been investigated to further improve its performance; see e.g., (Li et al., 2017; Bertinetto et al., 2019; Lee et al., 2019). Convergence guarantees have also been established to gain insights about these methods (Fallah et al., 2020; Ji et al., 2020; 2022). Interestingly, (Grant et al., 2018) pointed out that the initialization learned in MAML is approximately tantamount to the mean of a Gaussian prior probability density function (pdf) over the model parameters. This motivates well Bayesian formulations of meta-learning to further quantify the uncertainty in model parameters (Finn et al., 2018; Ravi & Beatson, 2019; Nguyen et al., 2020; Zhang et al., 2023). Nevertheless, the priors learned by these MAML-variants are confined to specific pdfs, including the Gaussian and degenerate ones. As a result, generalizing optimization-based meta-learning to practical domains that may require sophisticated priors is challenging.\nThis work advocates a novel meta-learning approach termed MetaProxNet that offers sufficient prior expressiveness, while maintaining the highly desirable interpretability. Our contribution is fourfold.\ni) A prior representation framework is introduced using the algorithm unrolling technique. The novel framework overcomes the interpretability challenge and breaks the expressiveness bottleneck, thus enabling one to meta-learn complicated yet interpretable priors.\nii) Instead of employing a fixed proximal operator induced by a certain prior pdf, piecewise linear functions (PLFs) are developed to learn further generalized priors.\niii) Theoretical analysis provides tight PGD error bounds between the learnable PLFs and the optimal proximal operators, which can be readily minimized under mild conditions.\niv) Numerical tests compare MetaProxNet with state-of-the-art methods having different priors, and confirm superiority of MetaProxNet. PLFs are visualized to depict the explainable prior."
        },
        {
            "heading": "2 PROBLEM SETUP",
            "text": "Meta-learning extracts task-invariant prior information from a collection of relevant tasks to aid the learning of new tasks, even if only a small number of training data are available. Formally, let t = 1, . . . , T index the aforementioned relevant tasks, each with corresponding dataset Dt := {(xnt , ynt )} Nt n=1 comprising Nt input-output data pairs. Set Dt is formed with a training subset Dtrnt \u2282 Dt and a validation subset Dvalt := Dt \\ Dtrnt . Likewise, a new task (with subscript \u22c6) will comprise a training subset Dtrn\u22c6 , and a test input xtst\u22c6 , for which the corresponding output ytst\u22c6 is to be predicted. Typically, |Dtrn\u22c6 | is rather small compared to what is required in supervised deep learning tasks. Due to the limited training data, directly learning the new task by optimizing its task-specific model over Dtrn\u22c6 is infeasible. However, since T can be considerably large, one prudent remedy is to leverage the cumulative prior knowledge across other related tasks.\nLet \u03b8t \u2208 Rd denote the task-specific model parameter for task t, and \u03b8 \u2208 RD the prior parameter shared across tasks. The prior can be learned via empirical risk minimization (ERM) alternating between i) base-learner optimization per t that estimates \u03b8t using Dtrnt and \u03b8; and, ii) meta-learner optimization that updates the estimate of \u03b8 using {Dvalt }Tt=1. This nested structure can be intrinsically characterized by a bilevel optimization problem\nmin \u03b8 T\u2211 t=1 L(\u03b8\u2217t (\u03b8);Dvalt ) (1a) s.to \u03b8\u2217t (\u03b8) = argmin \u03b8t L(\u03b8t;Dtrnt ) +R(\u03b8t;\u03b8), \u2200t (1b)\nwhere L is the loss function assessing the performance of the model, and R is the regularizer that captures the task-invariant prior. From the Bayesian viewpoint, L(\u03b8t;Dtrnt ) and R(\u03b8t;\u03b8) in (1b) are typically selected to be the negative log-likehood (nll) \u2212 log p(ytrnt |\u03b8t;Xtrnt ), and negative logprior (nlp) \u2212 log p(\u03b8t;\u03b8), where matrix Xtrnt is formed by all input vectors in Dtrnt , and ytrnt is the vector collecting their corresponding outputs. Hence, (1b) can be interpreted as the maximum a posteriori (MAP) estimator \u03b8\u2217t (\u03b8) = argmax\u03b8t p(\u03b8t|y trn t ;X trn t ,\u03b8) upon invoking Bayes rule.\nIt is worth stressing that R(\u03b8t;\u03b8) is instrumental in learning task t, when |Dtrnt | is small. Without it, an over-parameterized model such as a deep NN could easily overfit Dtrnt . Moreover, it is generally infeasible to reach the global minimum \u03b8\u2217t , especially with a highly non-convex optimization involved in learning the task-specific model. Thus, a practical alternative is to rely on a suboptimal solution \u03b8\u0302t obtained by a parameterized base-learner B. Then, problem (1) boils down to\nmin \u03b8 T\u2211 t=1 L(\u03b8\u0302t(\u03b8);Dvalt ) (2a)\ns.to \u03b8\u0302t(\u03b8) = B(Dtrnt ;\u03b8), \u2200t. (2b)\nDepending on the choices of B, meta-learning approaches can be either NN-based or optimizationbased ones. The former typically employ an RNN to learn the mapping from Dtrnt to \u03b8\u0302 \u2217 t , using the premise that the recurrent cells of an RNN correspond to the iterations for optimizing (1b) (Ravi & Larochelle, 2017). However, there is no analytical guarantee regarding the convergence of this \u201cRNN-based optimization,\u201d and it is also hard to specify what priors have been learned by these RNNs. In contrast, the optimization-based approaches solve (1b) through an iterative optimizer, with R being the nlp term linked with a preselected pdf. For example, it has been reported in (Grant et al., 2018) that the optimization strategy adopted by MAML (Finn et al., 2017) corresponds up to an implicit Gaussian pdf p(\u03b8t;\u03b8) = N (\u03b8,Qt), where Qt is associated with the hyperparameters of B. Besides implicit prior pdfs, their explicit counterparts have also been investigated; see e.g., isotropic Gaussian (Rajeswaran et al., 2019), and diagonal Gaussian (Ravi & Beatson, 2019) examples."
        },
        {
            "heading": "3 INTERPRETABLE AND GENERALIZED PRIORS USING UNROLLED NNS",
            "text": "Existing meta-learning algorithms rely on either a blackbox NN or a preselected pdf (such as a Gaussian one) to parameterize the prior. However, the NN often lacks interpretability and the chosen pdf can have limited expressiveness. Consider for instance a preselected Gaussian prior pdf, which is inherently unimodal, symmetric, log-concave, and infinitely differentiable by definition. Such a prior may not be well-suited for tasks with multimodal or asymmetric parametric pdfs; see App. I for a case study. To enhance the prior expressiveness as well as offer the desired interpretability, our key idea is to learn a data-driven regularizer R, which dynamically adjusts its form to fit the provided tasks. This learnable R is effected by an unrolled NN, which drives our base-learner B."
        },
        {
            "heading": "3.1 PRIOR REPRESENTATION VIA ALGORITHM UNROLLING",
            "text": "Algorithm unrolling was introduced in (Gregor & LeCun, 2010) to learn the optimal update rule for the reconstruction of sparse signals from their low-dimensional linear measurements. In particular, algorithm unrolling involves unfolding the iterations of an optimization algorithm to create repeating blocks of an NN. In doing so, the desired prior is parameterized using learnable weights of the NN; see App J for a brief introduction. Following this work, several unrolling methods have been reported to learn interpretable priors for natural and medical signals, especially for images (Monga et al., 2021). Algorithm unrolling is also adopted here, but for a different purpose. While earlier efforts focus on learning the prior for a single task in the (transformed) signal space X \u2286 Rdim(xnt ), here it is employed for task-invariant prior extraction in the model parameter space \u0398t \u2286 Rd; that is, the prior we aim to learn is p(\u03b8t), \u2200t rather than p(xnt ) for t given. The widely adopted convolutional (C)NNs, which exhibit remarkable effectiveness in representing priors for 2-dimensional images, may not fit well with the 1-dimensional \u03b8t. A better alternative will be sought after the ensuing discussion that links prior representation with proximal function learning.\nTo solve the regularized problem (1b), we consider unrolling the proximal gradient descent (PGD) algorithm (Parikh et al., 2014), which allows one to \u201cdivide and conquer\u201d the objective function by separately optimizing L and R. Each PGD iteration indexed by k includes two steps: i) optimization of L(\u03b8k\u22121t ;Dtrnt ) wrt \u03b8 k\u22121 t using GD, with the update represented by an auxiliary variable z k t \u2208 Rd; and ii) optimization of R(\u03b8k\u22121t ;\u03b8) using zkt to update \u03b8 k\u22121 t . An upshot of the PGD algorithm is that it only requires L(\u03b8t; \u00b7) to be differentiable wrt \u03b8t, while R(\u03b8t; \u00b7) can be non-differentiable and even discontinuous. Thus, the expanded choices of R broaden the range of representable priors. The\nAlgorithm 1: Vanilla PGD algorithm for solving (1b) Input: Dtrnt , hyperparameters \u03b8, step size \u03b1, and maximum iteration K. Initialization: initialize \u03b80t according to \u03b8, and z0t = \u03b8 0 t .\nsteps of PGD are summarized in Algorithm 1, where the so-termed proximal operator is\nproxR,\u03b1(z) := argmin \u03b8t\n1\n2\u03b1 \u2225\u03b8t \u2212 z\u222522 +R(\u03b8t;\u03b8). (3)\nFor a broad range of R, their corresponding proxR,\u03b1 has an analytical form. One well-known example is the indicator function R = IS for some set S, which is discontinuous and non-differentiable. However, it corresponds to a well-defined proxR,\u03b1, namely the projection operator PS onto set S. Using algorithm unrolling, our idea is to search for the unknown optimal regularizing function R\u2217 (i.e., the one minimizing (1)) through learning its corresponding proximal operator proxR\u2217,\u03b1 with an unrolled NN. In particular, each PGD iteration indexed by k is replaced by a block consisting of a data consistency (DC) module, and a learnable NN-based \u02c7proxk. While the former ensures that the task-specific estimate \u03b8\u030c k\u22121 t of the unrolled NN is consistent with Dtrnt (by minimizing L(\u03b8\u030ck\u22121t ;Dtrnt ) wrt \u03b8\u030c k\u22121 t ), the latter looks for the optimal per-step prior that calibrates \u03b8\u030c k\u22121 t . The pipeline of this unrolled NN is illustrated in Fig. 1, where the DC module can be either a na\u0131\u0308ve GD as in line 4 of Algorithm 1, or, a data-driven rule such as GD with a learnable \u03b1. Let us for simplicity adopt the na\u0131\u0308ve GD as DC module, which aligns with MAML (Finn et al., 2017), and can be readily generalized to other iterative descent rules (Li et al., 2017; Lee & Choi, 2018; Park & Oliva, 2019; Flennerhag et al., 2020). The typical choice for each \u02c7proxk is an NN. Although p(\u03b8t;\u03b8) may not be available since the NN mapping is nonlinear, it can serve as a generalized prior, if properly scaled.\nUnlike previous works (Mardani et al., 2018; Hosseini et al., 2020) that model { \u02c7proxk}Kk=1 with 2- dimensional convolutions, here the input and output of \u02c7proxk are both 1-dimensional vectors in Rd; cf. (3). Our motivation comes from the two most widely-used priors in optimization-based metalearning. The first prior is the diagonal Gaussian one with R(\u03b8t;\u03b8) = 12 (\u03b8t\u2212\u03b8\ninit)\u22a4 diag(\u03bb)(\u03b8t\u2212 \u03b8init), where \u03b8init = \u03b80t is the task-invariant initialization of (1b), and \u03b8 := [\u03b8\ninit\u22a4,\u03bb\u22a4]\u22a4 is the vector parameterizing R (Ravi & Beatson, 2019; Rajeswaran et al., 2019; Nguyen et al., 2020). It can be easily verified that proxR,\u03b1(z) = (z\u2212 \u03b8 init)/(1d + \u03b1\u03bb) + \u03b8 init, with / being the elementwise division and 1d \u2208 Rd denoting the constant vector of all 1\u2019s. The second example is the shifted sparse prior that shares a pre-defined portion of \u03b8t across tasks (Raghu et al., 2020; Bertinetto et al., 2019; Lee et al., 2019). Here, we consider its variant R(\u03b8t;\u03b8) = \u2225\u039b(\u03b8t \u2212 \u03b8init)\u22251 that can be learned (Tian et al., 2020b). This results in proxR,\u03b1(z) = S\u03b1\u03bb(z\u2212 \u03b8\ninit) + \u03b8init, where S\u03b1\u03bb is the element-wise shrinkage (a.k.a. soft-thresholding) operator such that its i-th element\n[S\u03b1\u03bb(z)]i := S\u03b1\u03bbi(zi) :=  zi + \u03b1\u03bbi, zi < \u2212\u03b1\u03bbi 0, \u2212\u03b1\u03bbi \u2264 zi < \u03b1\u03bbi zi \u2212 \u03b1\u03bbi, zi \u2265 \u03b1\u03bbi .\nFor notational simplicity, denote by shifted vectors \u03b8\u0304kt := \u03b8 k t \u2212 \u03b8 init, z\u0304kt := z k t \u2212 \u03b8 init, shifted loss L\u0304(\u03b8; \u00b7) := L(\u03b8+\u03b8init; \u00b7), and shifted proximal operator \u00afproxR,\u03b1(z) := proxR,\u03b1(z+\u03b8 init)\u2212\u03b8init.\nThe PGD iteration can be thus reformulated as z\u0304kt = \u03b8\u0304 k\u22121 t \u2212 \u03b1\u2207\u03b8\u0304k\u22121t L\u0304(\u03b8\u0304 k\u22121 t ;Dtrnt ) (4a)\n\u03b8\u0304 k t = \u00afproxR,\u03b1(z\u0304 k t ), k = 1, . . . ,K (4b)\nwith initialization \u03b8\u03040t = z\u0304 0 t = 0d and output \u03b8\u0302t = \u03b8\u0304 K t + \u03b8 init. Further, the \u00afproxR,\u03b1(z) operator of the forgoing two examples reduces to z/(1d + \u03b1\u03bb) and S\u03b1\u03bb(z), respectively.\nInspired by the fact that \u00afproxR,\u03b1(z) of both examples belongs to the family of piecewise linear functions (PLFs), the fresh idea is to parameterize the shifted per-step \u02c7\u0304proxk(z;\u03b8) := \u02c7proxk(z + \u03b8init)\u2212\u03b8init of the unrolled NN using learnable PLFs. We first show that the wanted \u02c7\u0304proxk : Rd 7\u2192 Rd can be effectively decomposed and thus simplified under the following assumption that is widely adopted in meta-learning (Ravi & Beatson, 2019; Rajeswaran et al., 2019; Nguyen et al., 2020). Assumption 3.1. The optimal regularizer R\u2217 factorizes across its input dimensions; that is, R\u2217(\u03b8t;\u03b8) = \u2211d i=1 R\u2217i ([\u03b8t]i;\u03b8).\nWith Assumption 3.1 in effect, an immediate result is the element-wise proximal operator\n[proxR\u2217,\u03b1(z)]i = argmin [\u03b8t]i\n1\n2\u03b1 \u2225\u03b8t \u2212 z\u222522 + d\u2211 i=1 R\u2217i ([\u03b8t]i;\u03b8)\n= argmin [\u03b8t]i\n1\n2\u03b1 ([\u03b8t]i \u2212 zi)2 +R\u2217i ([\u03b8t]i;\u03b8) := proxR\u2217i ,\u03b1(zi), i = 1, . . . , d. (5)\nThis observation suggests that we can alternatively model the dimension-wise decomposition \u02c7\u0304proxki := [ \u02c7\u0304prox k]i for each i = 1, . . . , d, with a handy 1-dimensional PLF\n\u02c7\u0304proxki (zi) =  \u03c8ki,0(\u03b6 k i,1\u2212zi)+\u03c8 k i,1(zi\u2212\u03b6 k i,0) \u03b6ki,1\u2212\u03b6ki,0 , zi < \u03b6 k i,1 \u03b6ki,c\u22121 \u2264 zi < \u03b6ki,c\u03c8ki,c\u22121(\u03b6ki,c\u2212zi)+\u03c8ki,c(zi\u2212\u03b6ki,c\u22121) \u03b6ki,c\u2212\u03b6ki,c\u22121 , and c = 2, . . . C \u2212 1 \u03c8ki,C(\u03b6 k i,C+1\u2212zi)+\u03c8 k i,C+1(zi\u2212\u03b6 k i,C)\n\u03b6ki,C+1\u2212\u03b6ki,C , zi \u2265 \u03b6ki,C\u22121\n(6)\nwhere C \u2265 1 is a pre-selected constant indicating the total number of pieces, and {(\u03b6ki,c, \u03c8ki,c)}Cc=0 are the learnable control points parametrizing \u02c7\u0304proxki . To ensure \u02c7\u0304prox k i is a valid function, we further require \u03b6ki,0 \u2264 . . . \u2264 \u03b6ki,C for \u2200i, k. To this end, the problem of finding a proper task-invariant prior p(\u03b8t;\u03b8) boils down to learning the parameters of PLFs that are shared across tasks. Comparison of the pdf-based and PLF-based proximal operators can be visualized in Fig. 2."
        },
        {
            "heading": "3.2 PRIOR LEARNING VIA ALTERNATING OPTIMIZATION",
            "text": "Building upon the unrolling-based prior information representation, we are ready to elucidate how the prior can be learned by alternately optimizing the meta-learner and base-learner. We term the proposed method as meta-learning via proximal networks (MetaProxNet).\nLet r and k denote iteration indices for (1a) and (1b), respectively. For notational brevity, define vectors \u03b6k := [\u03b6k1,0, . . . , \u03b6 k d,C ] \u22a4 and \u03c8k := [\u03c8k1,0, . . . , \u03c8 k d,C ]\n\u22a4 of the PLF control points, and \u03b8r the concatenation of \u03b8init,r, \u03b61,r, . . . , \u03b6K,r,\u03c81,r, . . . ,\u03c8K,r in the r-th iteration of (1a). Given\nAlgorithm 2: MetaProxNet algorithm Input: {Dt}Tt=1, step sizes \u03b1 and \u03b2, batch size B, and maximum iterations K and R. Initialization: randomly initialize \u03b80.\n1 for r = 1, . . . , R do 2 Randomly sample a mini-batch T r \u2282 {1, . . . , T} of cardinality B; 3 for t \u2208 T r do 4 Initialize \u02c7\u0304\u03b80t = \u02c7\u0304z 0 t = 0d; 5 for k = 1, . . . ,K do 6 Descend \u02c7\u0304zkt (\u03b8\nr\u22121) = \u02c7\u0304\u03b8k\u22121t (\u03b8 r\u22121)\u2212 \u03b1\u2207\u02c7\u0304\u03b8k\u22121t L\u0304( \u02c7\u0304\u03b8k\u22121t (\u03b8 r\u22121);Dtrnt );\n7 Update \u02c7\u0304\u03b8kt (\u03b8 r\u22121) = \u02c7\u0304proxk(\u02c7\u0304zkt (\u03b8 r\u22121); \u03b6k,r,\u03c8k,r); 8 end 9 Shift \u03b8\u0302t(\u03b8r\u22121) = \u02c7\u0304\u03b8Kt (\u03b8\nr\u22121) + \u03b8init,r; 10 end 11 Update \u03b8r = \u03b8r\u22121 \u2212 \u03b2 1B \u2211 t\u2208T r \u2207\u03b8r\u22121L(\u03b8\u0302t(\u03b8\nr\u22121);Dvalt ); 12 end\nOutput: \u03b8\u0302 = \u03b8R.\n{Dtrnt }Tt=1, the goal of (1b) is to learn the task-specific estimate \u03b8\u0302t(\u03b8 r) that depends on \u03b8r per task t. This can leverage the current base-learner estimate B(\u00b7;\u03b8r), which is the unrolled multi-block NN of our MetaProxNet. In the k-th block, its DC module and PLFs optimize (1b) through\n\u02c7\u0304zkt (\u03b8 r) = \u02c7\u0304\u03b8k\u22121t (\u03b8 r)\u2212 \u03b1\u2207\u02c7\u0304\u03b8k\u22121t L\u0304( \u02c7\u0304\u03b8k\u22121t (\u03b8 r);Dtrnt ) (7a) \u02c7\u0304\u03b8kt (\u03b8 r) = \u02c7\u0304proxk(\u02c7\u0304zkt (\u03b8 r); \u03b6k,r,\u03c8k,r), k = 1, . . . ,K. (7b)\nwhere \u02c7\u0304zkt and \u02c7\u0304\u03b8kt denote the shifted iterative variables of the unrolled NN as in (4).\nAfter obtaining \u03b8\u0302t(\u03b8r) = \u02c7\u0304\u03b8Kt (\u03b8 r)+\u03b8init,r, the next step is to optimize (1a) by updating \u03b8r. A popular strategy is the mini-batch stochastic GD (SGD). Specifically, a subset T r \u2282 {1, . . . , T} of tasks are randomly selected to assess the performance of \u03b8r on Dvalt , which yields a loss L(\u03b8\u0302t(\u03b8\nr);Dvalt ) for \u2200t \u2208 T r. Then, \u03b8r+1 is reached by descending the averaged loss with step size \u03b2, that is\n\u03b8r+1 = \u03b8r \u2212 \u03b2 1 |T r| \u2211 t\u2208T r \u2207\u03b8rL(\u03b8\u0302t(\u03b8r);Dvalt ). (8)\nThe step-by-step pseudo-codes for our novel MetaProxNet approach are listed under Algorithm 2.\nIn practice however, simultaneously optimizing both {\u03b6k}Kk=1 and {\u03c8 k}Kk=1 incurs cumbersome gradient computations due to the entangled structure of (1). To relieve this burden, we fix the former by uniformly partitioning a closed interval [\u2212A,A], while optimizing only the latter. In other words, we let \u03b6ki,c = ( 2c C \u2212 1)A, \u2200c, i, k, where A > 0 is a pre-selected constant that is sufficiently large; see Assumption A.3. In fact, this setup can be viewed as a uniform discretization of the continuous variable \u03b6ki \u2208 R on [\u2212A,A]. Non-uniform discretization can be alternatively sought, if p(\u03b6ki ) or its estimate is available a priori."
        },
        {
            "heading": "3.3 ERROR BOUNDS FOR PLF-BASED PROXIMAL OPERATOR",
            "text": "Having introduced how to model and learn priors using unrolled NNs, this subsection analyzes the performance by bounding the approximation error on \u03b8\u0302t induced by replacing the unknown optimal \u00afproxR\u2217,\u03b1 with the learned PLF-based \u02c7\u0304prox\nk. Sharp bounds will be separately established for smooth and non-smooth \u00afproxR\u2217,\u03b1 operators under mild conditions. Utilizing these bounds, a quantitative criterion will be provided for choosing the hyperparameter C. All proofs and technical assumptions can be found in Apps. A-C. Smooth \u00afproxR\u2217,\u03b1 \u2208 C1([\u2212A,A]d) will be first considered. The following theorem offers an upper bound for the normalized error on (shifted) \u03b8\u0302t. Theorem 3.2 (Finite-step PGD error for smooth proximal operators). Consider \u02c7\u0304proxk defined by (6) with fixed \u03b6ki,c = ( 2c C \u2212 1)A, and let \u03a8 := [\u03c8 1, . . . ,\u03c8K ] denote the matrix parameterizing { \u02c7\u0304proxk}Kk=1. Let \u03b8\u0304 K t and \u02c7\u0304\u03b8Kt be the K-step PGD outputs using \u00afproxR\u2217,\u03b1 \u2208 C1([\u2212A,A]d) and\n\u02c7\u0304proxk, respectively. Under mild assumptions, it holds for t = 1, . . . , T that\nmin \u03a8 1\u221a d \u2225\u2225\u03b8\u0304Kt \u2212 \u02c7\u0304\u03b8Kt (\u03a8)\u2225\u22252 = O( 1C2 ). (9) This bound is tight when \u03c8ki,0 = \u00afproxR\u2217i ,\u03b1(\u2212A) and \u03c8 k i,C = \u00afproxR\u2217i ,\u03b1(A), \u2200k, i.\nTheorem 3.2 asserts that by optimizing over \u03a8 of the PLFs, \u02c7\u0304proxk can approximate any smooth \u00afproxR\u2217,\u03b1 with K-step PGD error in the order O( 1C2 ). In other words, an \u03f5-approximant \u02c7\u0304\u03b8Kt of \u03b8\u0304 K t can be obtained upon choosing C = \u2126( 1\u221a \u03f5 ) and optimizing \u03a8. The tightness of the bound implies that there exists at least one \u00afproxR\u2217,\u03b1 that reaches the upper bound when enforcing the first and last control points of each PLF to align with the desired \u00afproxR\u2217i ,\u03b1 operator.\nUnfortunately, directly optimizing the left-hand side of (9) is impossible, because the optimal \u00afproxR\u2217,\u03b1 corresponding to the oracle prior p(\u03b8t;\u03b8\n\u2217) is unknown. A feasible alternative is to perform the ERM in (1) by leveraging the datasets {Dt}Tt=1 generated with \u03b8t \u223c p(\u03b8t;\u03b8\n\u2217). As a result, the (unknown) optimal PLF parameters \u03a8\u2217 = argmin\u03a8 \u2225\u2225\u03b8\u0304Kt \u2212 \u02c7\u0304\u03b8Kt (\u03a8)\u2225\u22252, and the sub-optimal estimate \u03a8\u0302 obtained by solving (1), satisfy the inequality\n1\u221a d \u2225\u2225\u03b8\u0304Kt \u2212 \u02c7\u0304\u03b8Kt (\u03a8\u0302)\u2225\u22252 \u2264 1\u221ad\u2225\u2225\u03b8\u0304Kt \u2212 \u02c7\u0304\u03b8Kt (\u03a8\u2217)\u2225\u22252 + 1\u221ad\u2225\u2225\u02c7\u0304\u03b8Kt (\u03a8\u0302)\u2212 \u02c7\u0304\u03b8Kt (\u03a8\u2217)\u2225\u22252. (10) The extra error 1\u221a\nd \u2225\u2225\u02c7\u0304\u03b8Kt (\u03a8\u0302)\u2212 \u02c7\u0304\u03b8Kt (\u03a8\u2217)\u2225\u22252 can be further bounded in linear order O( 1\u221ad\u2225\u03a8\u0302\u2212\u03a8\u2217\u22251) of the normalized ERM error; see App. C for further elaboration.\nAside from smooth ones, non-smooth \u00afproxR\u2217,\u03b1 has gained attention in various PGD-guided applications. The next theorem forgoes the smooth assumption to yield a more generic but looser bound.\nTheorem 3.3 (Finite-step PGD error for continuous proximal operators). Consider the notational conventions of Theorem 3.2 with continuous \u00afproxR\u2217,\u03b1 \u2208 C0([\u2212A,A]d). Under mild assumptions, it holds for t = 1, . . . , T that\nmin \u03a8 1\u221a d \u2225\u2225\u03b8\u0304Kt \u2212 \u02c7\u0304\u03b8Kt (\u03a8)\u2225\u22252 = O( 1C ). (11) This bound is tight when \u03c8ki,0 = \u00afproxR\u2217i ,\u03b1(\u2212A) and \u03c8 k i,C = \u00afproxR\u2217i ,\u03b1(A), \u2200k, i.\nCompared to the smooth case, the error bound in Theorem 3.3 has an order of O( 1C ). This implies that by selecting C = \u2126( 1\u03f5 ), operator \u02c7\u0304prox\nk can approximate any continuous \u00afproxR\u2217,\u03b1 with normalized K-step PGD error no larger than \u03f5. This increased order implies that one can easily expand the range of learnable priors with a larger C. Moreover, the discussion following (10) regarding the sub-optimality of \u03a8\u0302, applies to Theorem 3.3 too, and it is deferred to App. C."
        },
        {
            "heading": "4 NUMERICAL TESTS",
            "text": "In this section, numerical tests are presented on several meta-learning benchmark datasets to evaluate the empirical performance of MetaProxNet. Hyperparameters and datasets are described in App. E. All experiments are run on a server with RTX A5000 GPU, and our codes are available online at https://github.com/zhangyilang/MetaProxNet."
        },
        {
            "heading": "4.1 COMPARISON OF META-LEARNING METHODS HAVING DIFFERENT PRIORS",
            "text": "The first test is on few-shot classification datasets miniImageNet (Vinyals et al., 2016) and TieredImageNet (Ren et al., 2018), where \u201cshot\u201d signifies the per-class number of labeled training data for each t. The default model is a standard 4-layer CNN (Vinyals et al., 2016), each layer comprising a 3 \u00d7 3 convolution operation of 64 channels, a batch normalization, a ReLU activation, and a 2 \u00d7 2 max pooling. A linear regressor with softmax is appended to perform classification.\nTo demonstrate the superiority of unrolling-based priors over the RNN-based and handcrafted ones, we first compare MetaProxNet against several state-of-the-art meta-learning methods. As discussed in Sec. 3.1, our MetaProxNet can be readily integrated with other optimization-based meta-learning\nmethods through a simple substitution of the DC module. Tab. 1 lists the performance of MetaProxNet assessed using 1, 000 random new tasks, with MAML (Finn et al., 2017) and MetaCurvature (MC) (Park & Oliva, 2019) serving as backbones. For an apples-to-apples comparison, methods that use different models (e.g., residual networks) or pretrained feature extractors are not included in the table. It is seen that our MetaProxNet performs competitively in terms of classification accuracy when compared to state-of-the-art meta-learning methods. This empirically confirms the effectiveness of MetaProxNet. Additional discussions regarding the efficiency of MetaProxNet and extra tests with tied weights can be found in the Apps. F and G.\n4.2 ABLATION TESTS\nAblation tests are also carried out to investigate the essential reason for the performance gain of MetaProxNet. Evidently, MetaProxNet+MAML differs from its backbone MAML in two key aspects: task-level optimization algorithm (PGD vs. GD) and prior (unrolled-NN based vs. Gaussian). To assess which of the two contributes more to the performance gain of MetaProxNet, the ablation tests compare three methods: i) MAML that employs GD and Gaussian prior; ii) a variant with PGD and Gaussian prior; and, iii) MetaProxNet+MAML that utilizes PGD and an unrolled-NN based prior. To avoid overfitting in MAML, the models for all methods are fixed to a 4-layer 32-channel CNN. Tab. 2 lists the\nperformance of the three methods. It is seen that the PGD baseline and MAML exhibit comparable performance, while MetaProxNet outperforms both in all 4 tests. This reveals that the key factor contributing to MetaProxNet\u2019s success is the more expressive prior relative to PGD."
        },
        {
            "heading": "4.3 IMPACT OF HYPEREPARAMETER C",
            "text": "Numerical tests are also carried out to verify the theoretical analysis in Sec. 3.3, which upper bounds the \u2113-2 error between two PGD optimization outputs: one using the optimal prior and the other using a PLF-induced prior. Specifically, Theorems 3.2 and 3.3 state that this \u2113-2 error bounds will reduce as C increases, thus offering a better calibrated \u03b8\u0302t. To examine the qualities of \u03b8\u0302t with different\nC, Fig. 3 depicts the test accuracies of MetaProxNet+MAML on 5-class 1-shot miniImageNet as a function of C. It can be observed that the accuracy improves with C increasing, which corroborates with our theories. Moreover, C = 5 suffices to achieve satisfactory performance, while larger values of C only have a minor impact on MetaProxNet\u2019s empirical performance. This suggests that the constants hidden within the error bounds O( 1C ) and O( 1 C2 ) can be small enough in practice. To avoid potential overfitting of priors, we set C = 5 in all the tests."
        },
        {
            "heading": "4.4 INTERPRETING UNROLLING-BASED PRIORS BY VISUALIZING THE LEARNED PLFS",
            "text": "From an optimization viewpoint, the learned PLFs correspond to an implicit prior pdf that generally comes with no analytical expression. These PLFs can be visualized to further understand the behavior of the unrolled NN. Figs. 4a and 4b respectively depict the averaged \u02c7\u0304proxki for i\u2019s that correspond to the first and last CNN layers. The visualization showcases that the averaged PLF for the first layer is similar to the soft shrinkage function S\u03b1\u03bbi of the sparse prior mentioned in Sec. 3.1, while the last layer tends to have a linear PLF, which resembles that of a Gaussian prior.\nIn practice, the visualization of the PLFs can be utilized to examine the impact of the prior when updating model parameters, thus guiding the model training process. In Fig. 4, the acquired PLFs keep shallow layer weights being sparse around the initial value \u03b8init (that is, less updated) when k is small, while deep layers can be updated freely along its gradient directions. This suggests, when fine-tuning a pre-trained large-scale model on a specific task, it is advisable to freeze the weights of the embedding function and exclusively train the last few layers with a relatively large step size in the initial epochs. Once these deep layers have attained sufficient training, one can then gradually unfreeze the shallow layers and proceed with fine-tuning the entire model. This learned update strategy closely aligns with the widely adopted \u201cgradual unfreezing\u201d training approach for fine-tuning large-scale models, which has been proven effective in various practical applications; see e.g., (Howard & Ruder, 2018)."
        },
        {
            "heading": "5 CONCLUSIONS AND OUTLOOK",
            "text": "A novel prior information representation approach was pursued in this work using algorithm unrolling to learn more flexible and generalized priors. Under this framework, a meta-learning method termed MetaProxNet was developed with learnable PLFs effecting an implicit prior. The learned prior enjoys interpretability from an optimization vantage point, and can be well explained by visualizing its PLFs. Further, performance analysis established that the PLFs are capable of fitting smooth/continuous proximal functions with a proper selection of C. Numerical tests further corroborated empirically the superiority of MetaProxNet relative to meta-learning alternatives in prior representation and learning.\nOur future research agenda includes exciting themes on i) investigating various optimizers besides PGD; ii) implementing MetaProxNet with more complicated backbones and DC modules; and, iii) establishing bilevel convergence guarantees for MetaProxNet."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported by NSF grants 2102312, 2103256, 2128593, 2126052, and 2212318."
        },
        {
            "heading": "A PROOF OF THEOREM 3.2.",
            "text": "Smooth \u00afproxR\u2217,\u03b1 \u2208 C1([\u2212A,A]d) will be first considered under the following four technical assumptions. Assumption A.1. \u00afproxR\u2217,\u03b1 \u2208 C1([\u2212A,A]d) has G1-Lipschitz gradient on [\u2212A,A]d. Assumption A.2. L\u0304 \u2208 C1([\u2212A,A]d) has G2-Lipschitz gradient on [\u2212A,A]d. Assumption A.3. Constant A is sufficiently large so that \u02c7\u0304zkt , z\u0304k\u2217t \u2208 [\u2212A,A]d, \u2200t, k, where z\u0304k\u2217t is the PGD auxiliary variable generated with \u00afproxR\u2217,\u03b1.\nAssumption A.4. Operator \u00afproxR\u2217,\u03b1 \u2208 C0([\u2212A,A]d) is L-Lipschitz on [\u2212A,A]d. Remark A.5 (Mild assumptions). In Assumption A.1 and A.2, the optimal \u00afproxR\u2217,\u03b1 and the loss L\u0304 are only assumed to be Lipschitz smooth on the compact subset [\u2212A,A]d \u2282 Rd, without imposing any strong premise regarding their convexity or Lipschitz continuity. For Assumption A.3, the existence of such an A can be easily guaranteed when e.g., task-level step size \u03b1 \u2264 2/G2, and level sets {\u03b8t | L\u0304(\u03b8t;Dtrnt ) \u2264 L\u0304(0d;Dtrnt )}, {\u03b8t | R\u030c(\u03b8t;\u03b8) \u2264 R\u030c(0d;\u03b8)} and {\u03b8t | R\u2217(\u03b8t) \u2264 R\u2217(0d)} are bounded. In addition, Assumption A.4 can be readily satisfied as well. For example, when R\u2217 has GRLipschitz gradient on [\u2212A,A]d and \u03b1 < 1/GR, it follows from the stationary condition of (3) that 1\u03b1 (proxR\u2217,\u03b1(z) \u2212 z) + \u2207R\n\u2217(proxR\u2217,\u03b1(z)) = 0. Hence, it holds for \u2200z, z\u2032 \u2208 [\u2212A,A]d that \u2225z\u2212z\u2032\u22252 \u2265 \u2223\u2223\u2225proxR\u2217,\u03b1(z)\u2212proxR\u2217,\u03b1(z\u2032)\u22252\u2212\u03b1\u2225\u2207R\u2217(proxR\u2217,\u03b1(z))\u2212\u2207R\u2217(proxR\u2217,\u03b1(z\u2032))\u22252\u2223\u2223 = (1 \u2212 \u03b1GR)\u2225 proxR\u2217,\u03b1(z) \u2212 proxR\u2217,\u03b1(z\u2032)\u22252. In other words, the Lipschitz constant in this case is upper bounded by L \u2264 1/(1\u2212 \u03b1GR).\nTo prove Theorem 3.2, we first show a lemma that is important for bounding the error | \u02c7\u0304proxki \u2212 \u00afproxR\u2217i ,\u03b1| on [\u2212A,A].\nLemma A.6. Let f \u2208 C1(R) : R 7\u2192 R be a function with G-Lipschitz gradient. For \u2200\u03b61, \u03b62 \u2208 R and \u03b61 \u0338= \u03b62, define\nf\u0302(z) := (\u03b62 \u2212 z)f(\u03b61) + (z \u2212 \u03b61)f(\u03b62)\n\u03b62 \u2212 \u03b61 . (12)\nIt then holds for \u2200\u03b3 \u2208 [0, 1] that\u2223\u2223f((1\u2212 \u03b3)\u03b61 + \u03b3\u03b62)\u2212 f\u0302((1\u2212 \u03b3)\u03b61 + \u03b3\u03b62)\u2223\u2223 \u2264 G 8 (\u03b62 \u2212 \u03b61)2. (13) Proof. For notational convenience, let g(\u03b3) := \u2223\u2223f((1\u2212 \u03b3)\u03b61 + \u03b3\u03b62)\u2212 f\u0302((1\u2212 \u03b3)\u03b61 + \u03b3\u03b62)\u2223\u2223. Using the definition of f\u0302 and g, it can be easily verified for \u2200\u03b3 \u2208 (0, 1) that g \u2208 C0(R) and g(\u03b3) \u2265 g(0) = g(1) = 0. (14)\nTherefore, there exits at least one maximizer \u03b3\u2217 = argmax\u03b3\u2208(0,1) g(\u03b3) inside the open interval (0, 1). For brevity, define the corresponding \u03b6\u2217 = (1\u2212 \u03b3\u2217)\u03b61 + \u03b3\u2217\u03b62. Through Fermat\u2019s stationary point theorem (a.k.a. interior extremum theorem), it turns out that g\u2032(\u03b3\u2217) = 0, which implies\n(\u03b61 \u2212 \u03b62)f \u2032(\u03b6\u2217) = (\u03b61 \u2212 \u03b62)f\u0302 \u2032(\u03b6\u2217). (15) Since \u03b61 \u0338= \u03b62, we obtain\nf \u2032(\u03b6\u2217) = f\u0302 \u2032(\u03b6\u2217). (16)\nNext, we discuss the following two possible cases of \u03b3\u2217.\nCaes i) \u03b3\u2217 \u2208 (0, 1/2] It follows from (12), (16) and the Lipschitzness of f \u2032 that\u2223\u2223f \u2032((1\u2212 \u03b3)\u03b61 + \u03b3\u03b62)\u2212 f\u0302 \u2032((1\u2212 \u03b3)\u03b61 + \u03b3\u03b62)\u2223\u2223 = \u2223\u2223f \u2032((1\u2212 \u03b3)\u03b61 + \u03b3\u03b62)\u2212 f\u0302 \u2032(\u03b6\u2217)\u2223\u2223\n= \u2223\u2223f \u2032((1\u2212 \u03b3)\u03b61 + \u03b3\u03b62)\u2212 f \u2032(\u03b6\u2217)\u2223\u2223 \u2264 G|(1\u2212 \u03b3)\u03b61 + \u03b3\u03b62 \u2212 \u03b6\u2217| = G|\u03b3 \u2212 \u03b3\u2217||\u03b62 \u2212 \u03b61|. (17)\nAs a result, it holds for \u2200\u03b3 \u2208 [0, 1] that\ng(\u03b3) \u2264 g(\u03b3\u2217) (a)= \u2223\u2223\u2223\u2223\u2223 \u222b \u03b3\u2217 0 [ (\u03b62 \u2212 \u03b61)f \u2032 ( (1\u2212 \u03b3)\u03b61 + \u03b3\u03b62 ) \u2212 (\u03b62 \u2212 \u03b61)f\u0302 \u2032 ( (1\u2212 \u03b3)\u03b61 + \u03b3\u03b62 )] d\u03b3 \u2223\u2223\u2223\u2223\u2223 \u2264 |\u03b62 \u2212 \u03b61| \u222b \u03b3\u2217 0\n\u2223\u2223\u2223f \u2032((1\u2212 \u03b3)\u03b61 + \u03b3\u03b62)\u2212 f\u0302 \u2032((1\u2212 \u03b3)\u03b61 + \u03b3\u03b62)\u2223\u2223\u2223 d\u03b3 (b)\n\u2264 G|\u03b62 \u2212 \u03b61|2 \u222b \u03b3\u2217 0 (\u03b3\u2217 \u2212 \u03b3)d\u03b3 = G(\u03b62 \u2212 \u03b61)2 \u03b3\u22172 2 (c) \u2264 G 8 (\u03b62 \u2212 \u03b61)2 (18)\nwhere (a) uses the fact that f(\u03b61) = f\u0302(\u03b61), (b) is from (17), and (c) is due to \u03b3\u2217 \u2264 1/2.\nCase ii) \u03b3\u2217 \u2208 [1/2, 1) Likewise, we can also have\u2223\u2223f \u2032(\u03b7\u03b61 + (1\u2212 \u03b7)\u03b62)\u2212 f\u0302 \u2032(\u03b7\u03b61 + (1\u2212 \u03b7)\u03b62)\u2223\u2223 = \u2223\u2223f \u2032(\u03b7\u03b61 + (1\u2212 \u03b7)\u03b62)\u2212 f\u0302 \u2032(\u03b6\u2217)\u2223\u2223\n= \u2223\u2223f \u2032(\u03b7\u03b61 + (1\u2212 \u03b7)\u03b62)\u2212 f \u2032(\u03b6\u2217)\u2223\u2223 \u2264 G|\u03b7\u03b61 + (1\u2212 \u03b7)\u03b62 \u2212 \u03b6\u2217| = G|1\u2212 \u03b7 \u2212 \u03b3\u2217||\u03b62 \u2212 \u03b61|. (19)\nIt then holds for \u2200\u03b3 \u2208 [0, 1] that g(\u03b3) \u2264 g(\u03b3\u2217) = \u2223\u2223\u2223\u2223\u222b 1 \u03b3\u2217 [ (\u03b62 \u2212 \u03b61)f \u2032 ( (1\u2212 \u03b3)\u03b61 + \u03b3\u03b62 ) \u2212 (\u03b62 \u2212 \u03b61)f\u0302 \u2032 ( (1\u2212 \u03b3)\u03b61 + \u03b3\u03b62 )] d\u03b3 \u2223\u2223\u2223\u2223 \u2264 |\u03b62 \u2212 \u03b61| \u222b 1 \u03b3\u2217\n\u2223\u2223\u2223f \u2032((1\u2212 \u03b3)\u03b61 + \u03b3\u03b62)\u2212 f\u0302 \u2032((1\u2212 \u03b3)\u03b61 + \u03b3\u03b62)\u2223\u2223\u2223 d\u03b3 (a) = |\u03b62 \u2212 \u03b61| \u222b 1\u2212\u03b3\u2217 0\n\u2223\u2223\u2223f \u2032(\u03b7\u03b61 + (1\u2212 \u03b7)\u03b62)\u2212 f\u0302 \u2032(\u03b7\u03b61 + (1\u2212 \u03b7)\u03b62)\u2223\u2223\u2223 d\u03b7 (b)\n\u2264 G|\u03b62 \u2212 \u03b61|2 \u222b 1\u2212\u03b3\u2217 0 (1\u2212 \u03b7 \u2212 \u03b3\u2217)d\u03b7 = G(\u03b62 \u2212 \u03b61)2 (1\u2212 \u03b3\u2217)2\n2\n\u2264 G 8 (\u03b62 \u2212 \u03b61)2 (20)\nwhere (a) follows by the substitution of integral variable \u03b3 = 1\u2212 \u03b7, and (b) uses (19). Combining these two cases with (14) yields the desired conclusion.\nThe next theorem bounds the per-step error | \u02c7\u0304proxki \u2212 \u00afproxR\u2217,\u03b1| utilizing Lemma A.6.\nTheorem A.7 (Per-step error for smooth proximal operator). Consider \u02c7\u0304proxk defined by (6) with fixed \u03b6ki,c = ( 2c C \u2212 1)A. Define \u03c8 k i := [\u03c8 k i,0, . . . , \u03c8 k i,C ]\n\u22a4 the vector parameterizing \u02c7\u0304proxki . Then under Assumptions 3.1 and A.1, it holds for i = 1, . . . , d and k = 1, . . . ,K that\nmin \u03c8ki max z\u2208[\u2212A,A] \u2223\u2223 \u00afproxR\u2217i ,\u03b1(z)\u2212 \u02c7\u0304proxki (z;\u03c8ki )\u2223\u2223 \u2264 G1A22C2 . (21) This bound is tight with the additional constraints that \u03c8ki,0 = \u00afproxR\u2217i ,\u03b1(\u2212A) and \u03c8 k i,C = \u00afproxR\u2217i ,\u03b1(A), \u2200k, i.\nProof. Define \u03c8\u0303i := [ \u00afproxR\u2217i ,\u03b1(\u2212A), \u00afproxR\u2217i ,\u03b1(\u2212A + 2 CA), . . . , \u00afproxR\u2217i ,\u03b1(A)] \u22a4 to be the vector collecting the proximal function values at the partition points \u03b6ki,c = ( 2c C \u2212 1)A. It then follows that\nmin \u03c8ki max z\u2208[\u2212A,A]\n| \u00afproxR\u2217i ,\u03b1(z)\u2212 \u02c7\u0304prox k i (z;\u03c8 k i )| \u2264 max z\u2208[\u2212A,A] | \u00afproxR\u2217i ,\u03b1(z)\u2212 \u02c7\u0304prox k i (z; \u03c8\u0303i)|. (22)\nNext, applying Lemma A.6 to each piece of \u02c7\u0304proxki , it holds for \u2200\u03b3 \u2208 [0, 1] and c = 1, . . . , C that\u2223\u2223proxRi,\u03b1 ((1\u2212\u03b3)\u03b6ki,c\u22121+\u03b3\u03b6ki,c)\u2212 \u02c7\u0304proxki ((1\u2212\u03b3)\u03b6ki,c\u22121+\u03b3\u03b6ki,c; \u03c8\u0303i)\u2223\u2223 \u2264 G18 (\u03b6ki,c\u2212\u03b6ki,c\u22121)2 = G1A22C2 . (23) By noticing that \u222aCc=1{(1\u2212\u03b3)\u03b6ki,c\u22121+\u03b3\u03b6ki,c | \u03b3 \u2208 [0, 1]} = [\u03b60i , \u03b6Ci ] = [\u2212A,A], we obtain from (23) that\n| \u00afproxR\u2217i ,\u03b1(z)\u2212 \u02c7\u0304proxi(z; \u03c8\u0303i)| \u2264 G1A\n2\n2C2 , \u2200z \u2208 [\u2212A,A]. (24)\nRelating (22) to (24) leads to the desired error bound (21).\nFor later use, we define distance dist([a, b];\u03c8i) := max\nz\u2208[a,b] \u2223\u2223 \u00afproxR\u2217i ,\u03b1(z)\u2212 \u02c7\u0304proxki (z;\u03c8i)\u2223\u2223 (25) To illustrate this bound is tight with the additional constraints stated in Theorem A.7, a specific example will be constructed to show that the upper bound can be actually attained. To be more specific, it will be shown that for any given C \u2265 1, there exists a \u00afproxR\u2217i ,\u03b1 that satisfies Assumption A.1 and reaches the right side of (21), with the minimizer exactly being\n\u03c8\u0303i = \u03c8 \u2217 i := argmin\n\u03c8i: \u03c8i,0= \u00afproxR\u2217 i ,\u03b1(\u2212A)\n\u03c8i,C= \u00afproxR\u2217 i ,\u03b1(A)\ndist([\u2212A,A];\u03c8i). (26)\nFor simplicity, we drop the superscript k to write \u03b6i,c = ( 2cC \u2212 1)A in the sequel. Consider the following proximal function\n\u00afproxR\u2217i ,\u03b1(z) =  0, z < \u2212A G1 2 (z \u2212 \u03b6i,c) 2 + 2G1A 2 C2 c, \u03b6i,c \u2264 z < \u03b6i,c+1, c = 0, 2, . . . , 2\u230a C 2 \u230b \u2212G12 (z \u2212 \u03b6i,c+1) 2 + 2G1A 2 C2 (c+ 1), \u03b6i,c \u2264 z < \u03b6i,c+1, c = 1, 3, . . . , 2\u230a C+1 2 \u230b \u2212 1\n2G1A(1\u2212 2C \u230a C 2 \u230b)(z \u2212A) +\n2G1A 2\nC2 , z \u2265 A\n.\n(27) It can be verified that this function satisfies Assumption A.1 by showing that\n\u00afprox\u2032R\u2217i ,\u03b1(z) =  0, z < \u2212A G1(z \u2212 \u03b6i,c), \u03b6i,c \u2264 z < \u03b6i,c+1, c = 0, 2, . . . , 2\u230aC2 \u230b G1(\u03b6i,c+1 \u2212 z), \u03b6i,c \u2264 z < \u03b6i,c+1, c = 1, 3, . . . , 2\u230aC+12 \u230b \u2212 1 2G1A(1\u2212 2C \u230a C 2 \u230b), z \u2265 A\n(28)\nis continuous, and the second-order derivate | \u00afprox\u2032\u2032R\u2217i ,\u03b1(z)| = { G1, \u2212A \u2264 z < A 0, otherwise\n(29)\nis bounded by G1.\nIn such case, the c-th element of \u03c8\u0303i is \u03c8\u0303i,c = \u00afproxR\u2217i ,\u03b1(\u03b6i,c) = 2G1A\n2\nC2 c. It then follows from (6) that\n\u02c7\u0304proxki (z; \u03c8\u0303i) = G1A\nC (z +A). (30)\nAs a result, one can have\ndist([\u2212A,A]; \u03c8\u0303i) = dist([\u03b6i,c\u22121, \u03b6i,c]; \u03c8\u0303i) = G1A\n2\n2C2 , (31)\nwhere the maximum (hidden inside dist; cf. (25)) is attained at z = \u03b6i,c\u22121+\u03b6i,c2 for each c = 1, . . . , C.\nWhat remains now is to prove (26), which relies on the mathematical induction of C. The proof starts with the base case that C = 1. With the two extra constraints in effect, we already have\n\u03c8\u2217i = argmin \u03c8i: \u03c8i,0= \u00afproxR\u2217\ni ,\u03b1(\u2212A)\n\u03c8i,C= \u00afproxR\u2217 i ,\u03b1(A)\ndist([\u2212A,A];\u03c8i) = [ \u00afproxR\u2217i ,\u03b1(\u2212A), \u00afproxR\u2217i ,\u03b1(A)] \u22a4 = \u03c8\u0303i (32)\nand the minimum value dist([\u2212A,A];\u03c8\u2217i ) = G1A 2 2C2 . Now, with the inductive hypothesis that (26) holds for C = 1, . . . , C \u2032 (C \u2032 \u2265 1), we now prove that (26) is also true for C = C \u2032 +1. Without loss of generality, assume C \u2032 is even so that C \u2032 +1 is odd. A similar analysis can be readily carried out when C \u2032 is odd.\nNext, we discuss the following three possible cases to determine the optimal \u03c8\u2217i that minimizes dist([\u2212A,A];\u03c8i). In particular, it will be proved that the minimum distance of G1A 2\n2C2 can be reached only in the first case, where the induction hypothesis for C = C \u2032 + 1 holds.\nCase i) [\u03c8\u2217i ]C\u2032 = [\u03c8\u0303i]C\u2032 = \u00afproxR\u2217i ,\u03b1(\u03b6i,C\u2032). With this additional condition, it holds that\n\u03c8\u2217i = argmin \u03c8i: \u03c8i,0= \u00afproxR\u2217\ni ,\u03b1(\u2212A)\n\u03c8i,C= \u00afproxR\u2217 i ,\u03b1(A)\n\u03c8i,C\u2032= \u00afproxR\u2217 i ,\u03b1(\u03b6i,C\u2032 )\ndist([\u2212A,A];\u03c8i)\n= argmin \u03c8i: \u03c8i,0= \u00afproxR\u2217\ni ,\u03b1(\u2212A)\n\u03c8i,C= \u00afproxR\u2217 i ,\u03b1(A)\n\u03c8i,C\u2032= \u00afproxR\u2217 i ,\u03b1(\u03b6i,C\u2032 )\nmax { dist([\u2212A, \u03b6i,C\u2032 ];\u03c8i), dist([\u03b6i,C\u2032 , A];\u03c8i) } (33)\nwhere the last equality follows from the definition (25).\nBy applying the base inductive case (i.e., C = 1) on the interval [\u03b6i,C\u2032 , A] that contains one piece of \u02c7\u0304proxki (note that \u03b6i,C\u2032+1 = \u03b6i,C = A by definition), it follows that\nmin \u03c8i: \u03c8i,C\u2032= \u00afproxR\u2217\ni ,\u03b1(\u03b6i,C\u2032 )\n\u03c8i,C= \u00afproxR\u2217 i ,\u03b1(\u03b6i,C)\ndist([\u03b6i,C\u2032 , A];\u03c8i) = G1(A\u2212 \u03b6i,C\u2032)2 8 = G1A 2 2C2 , (34)\nwhere the second equality utilizes that {\u03b6i,c}Cc=0 uniformly partition [\u2212A,A] so that A \u2212 \u03b6i,C\u2032 = \u03b6i,C\u2032+1 \u2212 \u03b6i,C\u2032 = 2AC . Moreover, using the inductive hypothesis for C = C \u2032 on the interval [\u2212A, \u03b6i,C\u2032 ] containing C \u2032 pieces of \u02c7\u0304proxki , we obtain\nmin \u03c8i: \u03c8i,0= \u00afproxR\u2217\ni ,\u03b1(\u2212A)\n\u03c8i,C\u2032= \u00afproxR\u2217 i ,\u03b1(\u03b6i,C\u2032 )\ndist([\u2212A, \u03b6i,C\u2032 ];\u03c8i) = G1(\u03b6i,C\u2032 +A)\n2\n8C \u20322 = G1A\n2\n2C2 (35)\nand that the first C \u2032 elements of the minimizer of (35) are equal to [\u03c8\u0303i]1:C\u2032 .\nTherefore, combining (33)-(35) we arrive at\n\u03c8\u2217i = [[\u03c8\u0303i] \u22a4 1:C\u2032 , \u00afproxR\u2217i ,\u03b1(A)] \u22a4 = \u03c8\u0303i, (36)\nwhich indicates that the inductive hypothesis (26) also holds for C = C \u2032 + 1.\nNext, we show that the minimum distance of (26) in the following two cases is larger than G1A 2\n2C2 .\nCase ii) \u03c8\u2217i,C\u2032 > \u03c8\u0303i,C\u2032 = \u00afproxR\u2217i ,\u03b1(\u03b6i,C\u2032). According to (27), (30) and that C \u2032 is even, one can easily verify that\n\u00afproxR\u2217i ,\u03b1(z)\u2212 \u02c7\u0304prox k i (z; \u03c8\u0303i) \u2265 0, z \u2208 [\u03b6i,C\u2032\u22121, \u03b6i,C\u2032 ] (37a)\n\u00afproxR\u2217i ,\u03b1(z)\u2212 \u02c7\u0304prox k i (z; \u03c8\u0303i) \u2264 0, z \u2208 [\u03b6i,C\u2032 , A]. (37b)\nSince \u03c8\u2217i,C\u2032 > \u03c8\u0303i,C\u2032 and \u03c8 \u2217 i,C = \u03c8\u0303i,C , it follows from the definition (6) that\n\u02c7\u0304proxki (z;\u03c8 \u2217 i ) > \u02c7\u0304prox k i (z; \u03c8\u0303i), z \u2208 [\u03b6i,C\u2032 , A). (38)\nThen, it holds that G1A 2\n2C2 = dist([\u03b6i,C\u2032 , A]; \u03c8\u0303i) = max\nz\u2208[\u03b6i,C\u2032 ,A] \u2223\u2223 \u00afproxR\u2217i ,\u03b1(z)\u2212 \u02c7\u0304proxki (z; \u03c8\u0303i)\u2223\u2223 (a) =\n\u2223\u2223\u2223 \u00afproxR\u2217i ,\u03b1(\u03b6i,C\u2032 +A2 )\u2212 \u02c7\u0304proxki (\u03b6i,C\u2032 +A2 ; \u03c8\u0303i)\u2223\u2223\u2223 (b) = \u02c7\u0304proxki (\u03b6i,C\u2032 +A 2 ; \u03c8\u0303i ) \u2212 \u00afproxR\u2217i ,\u03b1 (\u03b6i,C\u2032 +A 2\n) (c) < \u02c7\u0304proxki (\u03b6i,C\u2032 +A 2 ;\u03c8\u2217i ) \u2212 \u00afproxR\u2217i ,\u03b1 (\u03b6i,C\u2032 +A 2\n) \u2264 max z\u2208[\u03b6i,C\u2032 ,A]\n\u2223\u2223 \u00afproxR\u2217i ,\u03b1(z)\u2212 \u02c7\u0304proxki (z;\u03c8\u2217i )\u2223\u2223 = dist([\u03b6i,C\u2032 , A];\u03c8\u2217i ), (39) where (a) uses that (31) is achieved at z = \u03b6i,c\u22121+\u03b6i,c2 , (b) follows from (37b), and (c) is due to (38).\nIn other words, if \u03c8\u2217i,C\u2032 > \u03c8\u0303i,C\u2032 , it must hold that\ndist([\u03b60i , \u03b6 C i ];\u03c8 \u2217 i ) >\nG1(\u03b6 C i \u2212 \u03b60i )2\n8C2\n, which is larger than that of case i). Therefore, the optimal \u03c8\u2217i must satisfy \u03c8 \u2217 i,C\u2032 \u2264 \u03c8\u0303i,C\u2032 .\nCase iii) \u03c8\u2217i,C\u2032 < \u03c8\u0303i,C\u2032 = \u00afproxR\u2217i ,\u03b1(\u03b6i,C\u2032). Again with (31), one can easily get\nG1A 2\n2C2 = dist([\u03b6i,C\u2032\u22121, \u03b6i,C\u2032 ]; \u03c8\u0303i)\n= \u00afproxR\u2217i ,\u03b1 (\u03b6i,C\u2032\u22121 + \u03b6i,C\u2032 2 ) \u2212 \u02c7\u0304proxki (\u03b6i,C\u2032\u22121 + \u03b6i,C\u2032 2 ; \u03c8\u0303i ) . (40)\nRecall from the definition (6) that \u02c7\u0304proxki (z;\u03c8i), z \u2208 [\u03b6i,C\u2032\u22121, \u03b6i,C\u2032 ] is defined as the line segment connecting points (\u03b6i,C\u2032\u22121, \u03c8i,C\u2032\u22121) and (\u03b6i,C\u2032 , \u03c8i,C\u2032). To ensure dist([\u03b6i,C\u2032\u22121, \u03b6i,C\u2032 ];\u03c8\u2217i ) \u2264 G1A 2\n2C2 , a necessary condition is \u02c7\u0304prox k i\n( \u03b6i,C\u2032\u22121+\u03b6i,C\u2032\n2 ;\u03c8 \u2217 i ) \u2265 \u02c7\u0304proxki ( \u03b6i,C\u2032\u22121+\u03b6i,C\u2032 2 ; \u03c8\u0303i ) ; cf. (37a).\nSince we have \u03c8\u2217i,C\u2032 < \u03c8\u0303i,C\u2032 in this case, it must hold that \u03c8 \u2217 i,C\u2032\u22121 > \u03c8\u0303i,C\u2032\u22121. By applying this analysis recursively, one can proceed to obtain a series of necessary conditions of dist([\u2212A,A];\u03c8\u2217i ) \u2264 G1A 2 2C2 , which are (recall that C \u2032 is presumed even)\n\u03c8\u2217i,C\u2032\u22121 > \u03c8\u0303i,C\u2032\u22121, \u03c8 \u2217 i,C\u2032\u22122 < \u03c8\u0303i,C\u2032\u22122, (41)\n. . . , (42)\n\u03c8\u2217i,1 > \u03c8\u0303i,1, \u03c8 \u2217 i,0 < \u03c8\u0303i,0. (43)\nThis contradicts with the constraint that \u03c8\u2217i,0 = \u00afproxR\u2217i ,\u03b1(\u2212A) = \u03c8\u0303i,0; cf. (26). That is to say, requiring \u03c8\u2217i,C\u2032 < \u03c8\u0303i,C\u2032 will lead to dist([\u2212A,A];\u03c8 \u2217 i ) > G1A 2 2C2 , which is not optimal.\nTo the end, through the three cases we conclude that the minimizer \u03c8\u2217i must satisfy \u03c8 \u2217 i,C\u2032 = \u03c8\u0303i,C\u2032 , which implies (26) holds for C = C \u2032 + 1; see (36). The proof is thus completed.\nBuilding upon the per-step error bound established in Theorem A.7, the K-step cumulative error bound will next be proved. In particular, the following theorem offers an upper bound for the normalized error on (shifted) \u03b8\u0302t.\nTheorem A.8 (Formal statement: finite-step PGD error for smooth proximal operators). Consider \u02c7\u0304proxk defined by (6) with fixed \u03b6ki,c = ( 2c C \u2212 1)A. Define \u03a8 := [\u03c8 1, . . . ,\u03c8K ] the matrix parameterizing { \u02c7\u0304proxk}Kk=1. Let \u03b8\u0304 K t and \u02c7\u0304\u03b8Kt be the K-step PGD outputs using \u00afproxR\u2217,\u03b1 and \u02c7\u0304prox k,\nrespectively. With Assumptions 3.1 and A.1-A.4 in effect, it holds for t = 1, . . . , T that\nmin \u03a8 1\u221a d \u2225\u2225\u03b8\u0304Kt \u2212 \u02c7\u0304\u03b8Kt (\u03a8)\u2225\u22252 = O( 1C2 ). (44) This bound is tight with the additional constraints that \u03c8ki,0 = \u00afproxR\u2217i ,\u03b1(\u2212A) and \u03c8 k i,C = \u00afproxR\u2217i ,\u03b1(A), \u2200k, i.\nProof. For notational compactness, define \u03c8\u0303 k\n:= argmin\u03c8k max\u2212A1d\u2aafz\u2aafA1d \u2225 \u00afproxR\u2217,\u03b1(z) \u2212 \u02c7\u0304proxk(z;\u03c8k)\u22252 and \u03a8\u0303 := [\u03c8\u0303 1 , . . . , \u03c8\u0303 K ]. Since both \u00afproxR\u2217,\u03b1 and \u02c7\u0304prox k are factorable across the dimensions of their inputs, we know that \u03c8\u0303 k is the concatenation of the minimizers\n\u03c8\u0303 k\ni = argmin \u03c8ki max z\u2208[\u2212A,A]\n\u2225 \u00afproxR\u2217i ,\u03b1(z)\u2212 \u02c7\u0304prox k i (z;\u03c8 k i )\u22252, i = 1, . . . , d\nfor each dimension. It then holds for k = 1, . . . ,K that\nmin \u03a8 \u2225\u2225\u03b8\u0304kt \u2212 \u02c7\u0304\u03b8kt (\u03a8)\u2225\u22252 \u2264\n\u2225\u2225\u03b8\u0304kt \u2212 \u02c7\u0304\u03b8kt (\u03a8\u0303)\u2225\u22252 = \u2225\u2225 \u00afproxR\u2217,\u03b1(z\u0304kt )\u2212 \u02c7\u0304proxki (\u02c7\u0304zkt ; \u03c8\u0303k)\u2225\u22252 \u2264\n\u2225\u2225 \u00afproxR\u2217,\u03b1(z\u0304kt )\u2212 \u00afproxR\u2217,\u03b1(\u02c7\u0304zkt )\u2225\u22252 + \u2225\u2225 \u00afproxR\u2217,\u03b1(\u02c7\u0304zkt )\u2212 \u02c7\u0304proxki (\u02c7\u0304zkt ; \u03c8\u0303k)\u2225\u22252 (a)\n\u2264 \u2225\u2225 \u00afproxR\u2217,\u03b1(z\u0304kt )\u2212 \u00afproxR\u2217,\u03b1(\u02c7\u0304zkt )\u2225\u22252 + \u221a dA2G1 2C2\n(b) \u2264 L \u2225\u2225z\u0304kt \u2212 \u02c7\u0304zkt (\u03a8\u0303)\u2225\u22252 + \u221a dA2G1 2C2\n(c) \u2264 L \u2225\u2225\u03b8\u0304k\u22121t \u2212 \u02c7\u0304\u03b8k\u22121t (\u03a8\u0303)\u2225\u22252 + \u03b1L\u2225\u2225\u2207\u03b8\u0304k\u22121t L\u0304(\u03b8\u0304k\u22121t ;Dtrnt )\u2212\u2207\u02c7\u0304\u03b8k\u22121t L\u0304(\u02c7\u0304\u03b8k\u22121t ;Dtrnt )\u2225\u22252 + \u221a dA2G1 2C2\n(d) \u2264 L(1 + \u03b1G2) \u2225\u2225\u03b8\u0304k\u22121t \u2212 \u02c7\u0304\u03b8k\u22121t (\u03a8\u0303)\u2225\u22252 + \u221a dA2G1 2C2\n(45)\nwhere (a) follows from Theorem A.7 and Assumptions A.1-A.3, (b) uses Assumption A.4, (c) is from (4) and (7), and (d) is due to Assumption A.2.\nUsing this recursive relationship between \u2225\u03b8\u0304kt \u2212 \u02c7\u0304\u03b8 k t (\u03a8\u0303)\u22252 (line 2) and \u2225\u03b8\u0304 k\u22121 t \u2212 \u02c7\u0304\u03b8 k\u22121 t (\u03a8\u0303)\u22252 (the last line), together with the boundary condition \u2225\u03b8\u03040t \u2212 \u02c7\u0304\u03b8 0 t\u22252 = \u22250d\u22120d\u22252 = 0, we arrive at the solution\u2225\u2225\u03b8\u0304kt \u2212 \u02c7\u0304\u03b8kt (\u03a8\u0303)\u2225\u22252 \u2264 { 1\u2212Lk(1+\u03b1G2)k 1\u2212L(1+\u03b1G2) \u221a dA2G1 2C2 , if L(1 + \u03b1G2) \u0338= 1\nk \u221a dA2G1 2C2 , otherwise\n= O( \u221a d\nC2 ). (46)\nDividing by \u221a d and minimizing over \u03a8 on both side of \u2225\u2225\u03b8\u0304kt \u2212 \u02c7\u0304\u03b8kt (\u03a8)\u2225\u22252 \u2264 \u2225\u2225\u03b8\u0304kt \u2212 \u02c7\u0304\u03b8kt (\u03a8\u0303)\u2225\u22252 lead to min \u03a8 1\u221a d\n\u2225\u2225\u03b8\u0304kt \u2212 \u02c7\u0304\u03b8kt (\u03a8)\u2225\u22252 \u2264 1\u221ad\u2225\u2225\u03b8\u0304kt \u2212 \u02c7\u0304\u03b8kt (\u03a8\u0303)\u2225\u22252 = O( 1C2 ). (47) Plugging in (46) with k = K gives (9).\nIn addition, the tightness of (9) follows from the tightness of Theorem A.7."
        },
        {
            "heading": "B PROOF OF THEOREM 3.3",
            "text": "The proof of Theorem 3.3 relies on a more generic lemma which can be applied to non-smooth (but still Lipschitz) \u00afproxR\u2217,\u03b1.\nLemma B.1. Let f \u2208 C0(R) : R 7\u2192 R be an L-Lipschitz function. For \u2200\u03b61, \u03b62 \u2208 R and \u03b61 \u0338= \u03b62, define\nf\u0302(z) := (\u03b62 \u2212 z)f(\u03b61) + (z \u2212 \u03b61)f(\u03b62)\n\u03b62 \u2212 \u03b61 . (48)\nIt then holds for \u2200\u03b3 \u2208 [0, 1] that\u2223\u2223f((1\u2212 \u03b3)\u03b61 + \u03b3\u03b62)\u2212 f\u0302((1\u2212 \u03b3)\u03b61 + \u03b3\u03b62)\u2223\u2223 \u2264 L 2 |\u03b62 \u2212 \u03b61|. (49) Proof. Define g(\u03b3) := \u2223\u2223f((1\u2212 \u03b3)\u03b61 + \u03b3\u03b62)\u2212 f\u0302((1\u2212 \u03b3)\u03b61 + \u03b3\u03b62)\u2223\u2223. Following the same step (14) of Lemma A.6, it can be shown that there also exists at least one maximizer \u03b3\u2217 \u2208 (0, 1) of g(\u03b3). Thus we obtain g(\u03b3) \u2264 g(\u03b3\u2217) =\n\u2223\u2223f((1\u2212 \u03b3\u2217)\u03b61 + \u03b3\u2217\u03b62)\u2212 f\u0302((1\u2212 \u03b3\u2217)\u03b61 + \u03b3\u2217\u03b62)\u2223\u2223 (a) =\n\u2223\u2223f((1\u2212 \u03b3\u2217)\u03b61 + \u03b3\u2217\u03b62)\u2212 (1\u2212 \u03b3\u2217)f(\u03b61)\u2212 \u03b3\u2217f(\u03b62)\u2223\u2223 \u2264 (1\u2212 \u03b3\u2217)\n\u2223\u2223f((1\u2212 \u03b3\u2217)\u03b61 + \u03b3\u2217\u03b62)\u2212 f(\u03b61)\u2223\u2223+ \u03b3\u2217\u2223\u2223f((1\u2212 \u03b3\u2217)\u03b61 + \u03b3\u2217\u03b62)\u2212 f(\u03b62)\u2223\u2223 (b) \u2264 2\u03b3\u2217(1\u2212 \u03b3\u2217)L|\u03b62 \u2212 \u03b61| (c) \u2264 L 2 |\u03b62 \u2212 \u03b61|, (50)\nwhere (a) follows from the definition (48) of f\u0302 , (b) exploits the Lipschitzness of f , and (c) is due to that \u03b3\u2217(1\u2212 \u03b3\u2217) \u2264 1/4 for \u03b3\u2217 \u2208 (0, 1).\nWith Lemma B.1 at hand, Theorem 3.3 can be proved using similar techniques as Theorem 3.2.\nTheorem B.2 (Formal statement: finite-step PGD error for continuous proximal operators). Consider the notations defined in Theorem 3.2. With Assumptions 3.1 and A.2-A.4 in effect, it holds for t = 1, . . . , T that\nmin \u03a8 1\u221a d \u2225\u2225\u03b8\u0304Kt \u2212 \u02c7\u0304\u03b8Kt (\u03a8)\u2225\u22252 = O( 1C ). (51) This bound is tight with the additional constraints that \u03c8ki,0 = \u00afproxR\u2217i ,\u03b1(\u2212A) and \u03c8 k i,C = \u00afproxR\u2217i ,\u03b1(A), \u2200k, i.\nProof. Following the same steps of Theorem A.7, it can be shown that the per-step error bound for continuous \u00afproxR\u2217,\u03b1 is\nmin \u03c8ki max z\u2208[\u2212A,A] \u2223\u2223 \u00afproxR\u2217i ,\u03b1(z)\u2212 \u02c7\u0304proxki (z;\u03c8ki )\u2223\u2223 \u2264 ALC , \u2200i. (52) Also, this bound is tight provided with the additional constraints that \u03c8ki,0 = \u00afproxR\u2217i ,\u03b1(\u2212A) and \u03c8i,C = \u00afproxR\u2217i ,\u03b1(A).\nLikewise, we define \u03c8\u0303 k := argmin\u03c8k max\u2212A1d\u2aafz\u2aafA1d \u2225 \u00afproxR\u2217,\u03b1(z)\u2212 \u02c7\u0304proxk(z;\u03c8 k)\u22252 and \u03a8\u0303 := [\u03c8\u0303 1 , . . . , \u03c8\u0303 K ]. Then, it follows from the first two lines of (45) that\nmin \u03a8 \u2225\u2225\u03b8\u0304kt \u2212 \u02c7\u0304\u03b8kt (\u03a8)\u2225\u22252 \u2264 \u2225\u2225 \u00afproxR\u2217,\u03b1(z\u0304kt )\u2212 \u00afproxR\u2217,\u03b1(\u02c7\u0304zkt )\u2225\u22252 + \u2225\u2225 \u00afproxR\u2217,\u03b1(\u02c7\u0304zkt )\u2212 \u02c7\u0304proxk(\u02c7\u0304zkt ; \u03c8\u0303k)\u2225\u22252 (a)\n\u2264 L \u2225\u2225z\u0304kt \u2212 \u02c7\u0304zkt (\u03a8\u0303)\u2225\u22252 + \u221a dAL C (b)\n\u2264 L(1 + \u03b1G2) \u2225\u2225\u03b8\u0304k\u22121t \u2212 \u02c7\u0304\u03b8k\u22121t (\u03a8\u0303)\u2225\u22252 + \u221a dAL\nC , (53)\nwhere (a) is from Assumption A.4 and (52), and (b) utilizes (4), (7) and Assumption A.2.\nTo the end, this recursive relationship, combined with the condition \u2225\u03b8\u03040t \u2212 \u02c7\u0304\u03b8 0 t\u22252 = 0, results in\nmin \u03a8\n\u2225\u2225\u03b8\u0304Kt \u2212 \u02c7\u0304\u03b8Kt (\u03a8)\u2225\u22252 \u2264 { 1\u2212LK(1+\u03b1G2)K 1\u2212L(1+\u03b1G2) \u221a dAL C , if L(1 + \u03b1G2) \u0338= 1\nK \u221a dAL C , otherwise\n= O( \u221a d\nC ). (54)\nDividing both sides by \u221a d completes the proof."
        },
        {
            "heading": "C UPPER BOUND OF (10)",
            "text": "As discussed in Sec. 3.3, the optimal \u03a8\u2217 that minimizes (9) and (11) is typically unavailable. An feasible approximation is the sub-optimal \u03a8\u0302 obtained from the ERM (1), which brings about an extra error term 1\u221a\nd \u2225\u2225\u02c7\u0304\u03b8Kt (\u03a8\u0302)\u2212 \u02c7\u0304\u03b8Kt (\u03a8\u2217)\u2225\u22252; cf. (10). This section derives its upper bound, based on the following extra assumption. Assumption C.1. \u02c7\u0304proxk(z;\u03c8k) \u2208 C0([\u2212A,A]d) is L1- and L2-Lipschitz w.r.t. z and \u03c8k for \u2212A1d \u2aaf z \u2aaf A1d, respectively.\nDenoting by \u03c8\u0302 k and \u03c8\u2217k the k-th columns of \u03a8\u0302 and \u03a8\u2217 that parametrize \u02c7\u0304proxk, it holds for k = 1, . . . ,K that\u2225\u2225\u02c7\u0304\u03b8kt (\u03a8\u0302)\u2212 \u02c7\u0304\u03b8kt (\u03a8\u2217)\u2225\u22252 (a) = \u2225 \u02c7\u0304proxk(\u02c7\u0304zkt (\u03a8\u0302); \u03c8\u0302 k )\u2212 \u02c7\u0304proxk(\u02c7\u0304zkt (\u03a8 \u2217);\u03c8\u2217k)\u22252\n\u2264 \u2225 \u02c7\u0304proxk(\u02c7\u0304zkt (\u03a8\u0302); \u03c8\u0302 k )\u2212 \u02c7\u0304proxk(\u02c7\u0304zkt (\u03a8\u0302);\u03c8 \u2217k)\u22252 + \u2225 \u02c7\u0304proxk(\u02c7\u0304zkt (\u03a8\u0302);\u03c8 \u2217k)\u2212 \u02c7\u0304proxk(\u02c7\u0304zkt (\u03a8 \u2217);\u03c8\u2217k)\u22252 (b) \u2264 L2\u2225\u03c8\u0302 k \u2212\u03c8\u2217k\u22252 + L1\u2225\u02c7\u0304zkt (\u03a8\u0302)\u2212 \u02c7\u0304zkt (\u03a8\n\u2217)\u22252 (c) \u2264 L2\u2225\u03c8\u0302 k \u2212\u03c8\u2217k\u22252 + L1 ( \u2225\u02c7\u0304\u03b8k\u22121t (\u03a8\u0302)\u2212 \u02c7\u0304\u03b8 k\u22121 t (\u03a8 \u2217)\u22252+\n\u03b1\u2225\u2207\u02c7\u0304\u03b8k\u22121t L( \u02c7\u0304\u03b8k\u22121t (\u03a8\u0302);Dtrnt )\u2212\u2207\u02c7\u0304\u03b8k\u22121t L( \u02c7\u0304\u03b8k\u22121t (\u03a8 \u2217);Dtrnt )\u22252 ) \u2264 L2\u2225\u03c8\u0302 k \u2212\u03c8\u2217k\u22252 + L1(1 + \u03b1G2)\u2225\u02c7\u0304\u03b8k\u22121t (\u03a8\u0302)\u2212 \u02c7\u0304\u03b8 k\u22121 t (\u03a8\n\u2217)\u22252 (d) \u2264 L1(1 + \u03b1G2)\u2225\u02c7\u0304\u03b8k\u22121t (\u03a8\u0302)\u2212 \u02c7\u0304\u03b8 k\u22121 t (\u03a8\n\u2217)\u22252 + L2\u2225\u03a8\u0302\u2212\u03a8\u2217\u22251 (55) where (a) follows from (7b), (b) uses Assumption C.1, (c) is from (7a) and Assumption A.2, and (d) is due to that \u2225\u03c8\u0302 k \u2212\u03c8\u2217k\u22252 \u2264 maxKk=1 \u2225\u03c8\u0302 k \u2212\u03c8\u2217k\u22252 = \u2225\u03a8\u0302\u2212\u03a8\u2217\u22251.\nSolving the recursive relationship (55) using \u2225\u02c7\u0304\u03b8kt (\u03a8\u0302)\u2212 \u02c7\u0304\u03b8 k t (\u03a8 \u2217)\u22252 = 0 gives\u2225\u2225\u02c7\u0304\u03b8Kt (\u03a8\u0302)\u2212 \u02c7\u0304\u03b8Kt (\u03a8\u2217)\u2225\u22252 \u2264 { 1\u2212LK1 (1+\u03b1G2) K 1\u2212L1(1+\u03b1G2) L2\u2225\u03a8\u0302\u2212\u03a8 \u2217\u22251, if L1(1 + \u03b1G2) \u0338= 1\nKL2\u2225\u03a8\u0302\u2212\u03a8\u2217\u22251, otherwise , (56)\nwhich concludes that 1\u221a d \u2225\u2225\u02c7\u0304\u03b8Kt (\u03a8\u0302)\u2212 \u02c7\u0304\u03b8Kt (\u03a8\u2217)\u2225\u22252 = O( 1\u221ad\u2225\u03a8\u0302\u2212\u03a8\u2217\u22251 ) . (57)"
        },
        {
            "heading": "D ADDITIONAL REMARKS REGARDING THE THEORETICAL RESULTS",
            "text": "Next, three important remarks regarding the derived error bounds will be provided. Remark D.1 (Difference with convergence rate analysis). It is worth stressing these approximation error bounds are different from the convergence rate analysis. Essentially, it quantifies the impact of using a parametric \u02c7\u0304prox to approximate the optimal yet unknown \u00afproxR\u2217,\u03b1. Moreover, although the bounds increase with K, it is important to note that K is a sufficiently small constant (typically 1 \u2264 K \u2264 5) in the context of meta-learning (Finn et al., 2017), as the overall complexity for solving (2) scales linearly with K. Furthermore, the learning rate must satisfy \u03b1 \u2208 (0, 2/G2) to guarantee a gradient-related descent direction, with \u03b1 = 1/G2 being the optimal choice. A consequence of this choice is that (1 + \u03b1G2)K \u2208 [2, 32], which ensures that the constant in the upper bounds will not diverge. Remark D.2 (Factorability and scalability). Assumption 3.1 ensures that the prior dimension D scales with the task-specific parameter dimension d. As d in practice can be extremely large (e.g., \u2126(105)), a complete prior such as a full Gaussian pdf would incur prohibitively high complexity; that is, D = \u0398(d2) = \u2126(1010). A feasible simplification is to approximate the prior in Rd using the multiplication of d pdfs in R. This assumption essentially considers each dimension of \u03b8t to be mutually independent, leading to D = \u0398(d). Such an independence assumption is prevalent not only in meta-learning, but also in high-dimensional statistics when dealing with deep NNs.\nRemark D.3 (Validity of learned proximal operator). The learnable \u02c7prox in this paper remains a proximal operator, even when the corresponding regularizer is non-convex. Indeed, let \u02c7prox\u22121(x;\u03b8) := argminz{\u2225z\u2225 | \u02c7prox(z;\u03b8) = x} for x \u2208 { \u02c7prox(z;\u03b8)|A1d \u2aaf z \u2aaf A1d} := X , and x0 := \u02c7prox(z0;\u03b8) \u2208 X . The stationary point condition of the proximal operator indicates \u02c7prox\u22121(x0;\u03b8) \u2212 x0 \u2208 \u2202R(x0;\u03b8); thus, one of the regularizers satisfying this condition is R(\u03b8t;\u03b8) = \u222b x\u2208X :x\u227a\u03b8t( \u02c7prox\n\u22121(x;\u03b8) \u2212 x)dx. Compared to non-expansive operators, proximal operators induced by non-convex regularizers have gained popularity in recent years thanks to their enhanced expressiveness, and their convergence guarantees (Hurault et al., 2022). Our method fails precisely within this category. Additionally, the PLFs should be monotone to qualify as a valid proximal operator; see e.g., (Gribonval & Nikolova, 2020, Theorem 1). In practice, the sought monotonicity can be established by enforcing \u03c8ki,c \u2264 \u03c8ki,c\u2032 , \u2200c \u2264 c\u2032. This can be readily achieved upon defining \u03c8ki,c+1 := \u03c8 k i,c+exp(\u2206\u03c8 k i,c), \u2200i, c, k, and then learn the log-increment {\u2206\u03c8ki,c}i,c,k. Interestingly, we have observed that the learned PLFs are exactly monotone functions (see e.g., Fig. 4), even without an explicit constraint. This observation suggests an inherent preference for monotonic PLFs by the data."
        },
        {
            "heading": "E DETAILED SETUPS OF NUMERICAL TESTS",
            "text": "In this section, we introduce the dataset and elaborate the detailed setups of the numerical tests.\nThe miniImageNet dataset (Vinyals et al., 2016) consists of 60, 000 natural images sampled from the full ImageNet (ILSVRC-12) dataset. These images are categorized into 100 classes, each with 600 labeled samples. As suggested by (Ravi & Larochelle, 2017), all images are cropped and resized to size 84 \u00d7 84. The dataset is split into 3 disjoint groups containing 64, 16 and 20 classes, which can be respectively accessed during the training, validation, and testing phases of meta-learning. The experimental setups follow from the standard M -class N -shot few-shot learning protocol (Ravi & Larochelle, 2017; Finn et al., 2017). Specifically, Dtrnt per task t includes M classes randomly drawn from the dataset, each containing N labeled data. As a result, it is clear that |Dtrnt | = MN for each t.\nThe TieredImageNet (Ren et al., 2018) dataset is a larger subset of the ImageNet dataset, composed of 779, 165 images from 608 classes. Likewise, all the images are preprocessed to have size 84\u00d784. Instead of using a random split, classes are partitioned into 34 categories according to the hierarchy of ImageNet dataset. Each category contains 10 to 30 classes. These categories are further grouped into 3 different sets: 20 for training, 6 for validation, and 4 for testing.\nWe utilized the group of hyperparameters described in MAML (Finn et al., 2017) consistently throughout all the tests. To be specific, the maximum number K of PGD steps (7) is 5, and the total number R of mini-batch SGD iterations (8) is 60, 000. The number of convolutional channels is 64 for MetaProxNet+MAML, and 128 for MetaProxNet+MC. The learning rates for PGD and SGD are \u03b1 = 0.01 and \u03b2 = 0.001, with batch size B = 4. Adam optimizer is employed for tieredImageNet, while SGD with Nesterov momentum of 0.9 and weight decay of 10\u22124 is used for miniImageNet.\nThe interval [\u2212A,A] and number C of pieces are determined through a grid search leveraging the validation tasks. For both miniImageNet and TieredImgeNet datasets, A = 0.02 and C = 5. We found that C = 5 suffices to reach a satisfactory performance, while larger C only contributes marginally to MetaProxNet\u2019s empirical performance. This suggests the constants hidden inside the error bounds O(1/C) and O(1/C2) can be sufficiently small in practice."
        },
        {
            "heading": "F COMPLEXITY ANALYSIS OF METAPROXNET",
            "text": "It can be observed from (6) and (7b) that the per-step piecewise linear function (PLF) \u02c7\u0304proxk applies a dimension-wise affine transformation to its input. Although the per-dimension \u02c7\u0304proxki consists of C + 1 parameters, the affine transformation merely relies on a single piece [\u03b6ki,c\u22121, \u03b6 k i,c] of the PLF. Thus, each computation involves only two control points \u03c8ki,c\u22121, \u03c8 k i,c for every k = 1, . . . ,K and i = 1, . . . , d. As a result, the forward calculation and backward differentiation of PLFs both incur complexity O(Kd). While the K-step GD of (7a) also exhibits forward and backward complexities of O(Kd), its constant hidden within O can be much larger, as the convolutional operations in the\nCNN are more time-consuming than the affine ones. Consequently, PLFs contribute only marginally (< 5%) to the overall complexity compared to their backbone. This is also evidenced numerically in Tab. 3. Moreover, Assumption (3.1) indeed ensures that the prior dimension D scales with d, thereby mitigating any substantial complexity increase. Specifically, in practical scenarios where d is extremely large (e.g., d = 121, 093 or 463, 365 in our experiments), employing a complete prior, such as a full Gaussian pdf, would yield D = \u0398(d2). A feasible simplification is to approximate the prior in Rd using the multiplication of d pdfs in R, which leads to D = \u0398(d)."
        },
        {
            "heading": "G EXTRA ABLATION TESTS REGARDING WEIGHT UNTYING",
            "text": "The next test examines the effectiveness of the weight-untying technique; i.e., the per-step \u02c7\u0304prox. The experiment is conducted on the miniImageNet dataset with a 4-layer 32-channel CNN, and the corresponding results are summarized in Tab. 4. It is seen that the per-step proximal operator consistently outperforms the shared one in all four tests. In fact, the per-step \u02c7\u0304proxk inherently corresponds to an adaptive prior, which evolves with the optimization process. The same technique was originally provided by the renowned LISTA algorithm (Gregor & LeCun, 2010), which pioneered algorithm unrolling, and has since been widely adopted by the community on inverse problems."
        },
        {
            "heading": "H NUMERICAL VERIFICATION OF ERROR BOUNDS",
            "text": "Next, a toy numerical test is carried out to verify the derived PGD error bounds. For simplicity, we will exclusively focus on Theorem 3.2, while similar analysis can be readily applied to Theorem 3.3. Consider tasks defined by the linear relationship ynt = w \u2217\u22a4 t x n t + e n t , and a linear prediction model y\u0302nt = f(x n t ;\u03b8t) := \u03b8 \u22a4 t x n t with squared \u21132 loss L(\u03b8t;Dtrnt ) := 12 \u2211Ntrnt n=1 ||ynt \u2212 \u03b8 \u22a4 t x n t ||22, where the\nunknown oracle w\u2217t \u223c Uniform([\u22123, 3]d) and xnt \u223c N (0d, Id). In the test, we set K = 5, \u03b1 = 0.01, d = 64, |Dtrnt | = 8,\u03b8\ninit = 0d, A = 3 andC varying from 2 to 20. The target optimal proximal operator to be approximated is defined in (27) with G1 = 1. Additionally, the Lipschitz constant G2 of \u2207L(\u03b8t;Dtrnt ) is numerically computed from the randomly generated training data matrix Xtrnt . The plot comparing the numerical PGD error and its upper bound can be found in Fig. 5. It is observed that the numerical error aligns with the theoretical bound up to a small constant in the logscale. This discrepancy arises because the upper bound considers the worst-case scenario, where the largest error between proxR\u2217,\u03b1 and prox\nk is reached at each PGD step. Furthermore, this constant gap suggests that the numerical error is in the same order with the bound (notice that both axes are in log-scale); that is, O( 1C2 ). This empirically corroborates our theoretical proofs."
        },
        {
            "heading": "I CASE STUDY: FEW-SHOT REGRESSION",
            "text": "Next, a straightforward yet illuminating numerical case study is provided to show the claimed superior prior expressiveness. Consider few-shot regression tasks defined by the linear data model ynt = w \u2217\u22a4 t x n t +e n t , where the unknown per-task weights {w\u2217t }Tt=1 are i.i.d. samples from the oracle pdf p(w\u2217) := 12Uniform([\u221211,\u221210] d)+ 12Uniform([10, 11]\nd), and ent \u223c N (0, \u03c32e), \u2200t, n is the additive white Gaussian noise. Further, consider a linear prediction model y\u0302nt = f(x n t ;\u03b8t) := \u03b8 \u22a4 t x n t . Since p(w\u2217) is symmetric and isotropic, the optimal Gaussian prior of \u03b8t for this case must have a mean of 0d an isotropic covariance. In other words, if the prior pdf is chosen to be Gaussian p(\u03b8t;\u03b8) = N (\u00b5,\u03a3), \u03b8 := [\u00b5\u22a4, vec(\u03a3)\u22a4]\u22a4, its optimal parameter \u03b8\u2217 must consist of \u00b5\u2217 = 0d and \u03a3\u2217 = \u03bb\u2217\u22121Id for some \u03bb\u2217 \u2208 R. As a result, the corresponding regularizer for this prior is R(\u03b8t;\u03b8\u2217) = \u03bb \u2217 2 \u2225\u03b8t\u2225 2 2, which prevents \u03b8t deviating far from 0d. However, the ground-truth task model implies that the optimal \u03b8\u2217t should belong to the set S := [\u221211,\u221210]d\u222a[10, 11]d, and the regularizer is thus a barrier for optimizing \u03b8t. In contrast, if the prior pdf is allowed to be non-Gaussian, the optimal prior will be the ground-truth one, i.e., p(\u03b8t;\u03b8\u2217) = Uniform(S). The corresponding proximal operator in this case is the projection proxR\u2217,\u03b1(z) = PS(z), which is exactly a piecewise linear function driving \u03b8t to the oracle set. In summary, the Gaussian pdf fails to match the underlying prior due to its inherent unimodality, while the more expressive PLF-induced prior can perfectly align with the groundtruth prior to enhance the task-level learning.\nFor visualization purpose, a numerical test is carried out with d = 1. The remaining parameters are |Dtrnt | = 2, |Dvalt | = 5, \u03c3e = 1, K = 5, R = 10, 000, \u03b1 = 0.1, \u03b2 = 0.01, A = 15, C = 30, B = 4 and xnt \u223c N (0, 1). In Fig. 6, the linear function learned by MAML is inclined to have a slope close to 0, while the PLFs in MetaProxNet quickly refine \u03b8t into the set S = [\u221211,\u221210] \u222a [10, 11]. This empirical observation substantiates the advocated superior prior expressiveness.\nIn practical tasks such as drug discovery and robotic manipulations, the oracle model parameters can have similar multi-modal pdf defined on a bounded set. In drug discovery for instance, the efficacy of a drug might only manifest when one component accounts for a specific portion."
        },
        {
            "heading": "J A BRIEF INTRODUCTION TO ALGORITHM UNROLLING",
            "text": "Algorithm unrolling was first introduced in (Gregor & LeCun, 2010) to solve the inverse problem. In particular, it aims to recover a (transformed) signal x \u2208 Rn from its compressed measurements\ny = Ax+ e (58) where A \u2208 Rm\u00d7n is a given matrix with m\u226a n, and e is additive white Gaussian noise. Since the system (58) is under-determined, it has infinitely many solutions. To ensure the uniqueness of the solution, a prudent remedy is to rely on the prior p(x), which yields\nx\u0302 = argmin x\n\u2225y \u2212Ax\u222522 +R(x). (59)\nIn the above, \u2225y \u2212 Ax\u222522 and R(x) correspond to the nll \u2212 log p(y;x) and nlp \u2212 log p(x), respectively. As nature signals are inherently sparse in certain transform domains such as Fourier and wavelet ones, a popular choice is the sparse prior with R(x) = \u03bb\u2225x\u22251. With such a prior, the resultant optimization problem (59) can be efficiently solved by the well-documented iterative soft-thresholding algorithm (ISTA), which involves a two-step update rule\nzk = xk\u22121 \u2212 \u03b1A\u22a4(Axk\u22121 \u2212 y) = (In \u2212 \u03b1A\u22a4A)xk\u22121 + \u03b1A\u22a4y (60a)\nxk = argmin x\n1\n2\u03b1 \u2225zk \u2212 x\u222522 + \u2225x\u22251 = S\u03b1\u03bb(zk), k = 1, . . . ,K (60b)\nHere, S\u03b1\u03bb is the soft-thresholding operator shown in Fig. 2b.\nWhen given a dataset {(xn,yn)}Nn=1, it is possible to enhance the accuracy of x\u0302 by learning a more efficient optimization rule. In (Gregor & LeCun, 2010), the two steps (60) of ISTA for each k = 1, . . . ,K are replaces by two learnable NN blocks\nzk = Wkxx k\u22121 +Wkyy (61a)\nxk = S\u03b2k(zk) (61b)\nwith \u03b8 := {(Wkx,Wky, \u03b2k)}Kk=1 being the learnable weights of the NN. Denoting by f(y;\u03b8) this multi-block NN mapping, the NN-based update rule can be learned via\nmin \u03b8 N\u2211 n=1 \u2225xn \u2212 f(yn;\u03b8)\u222522. (62)\nThis method of unfolding and substituting the iterations of an optimization algorithm to form a multi-block NN is known as algorithm unrolling, and the resultant NN is termed an unrolled NN."
        },
        {
            "heading": "K RELATED WORK",
            "text": "NN-based meta-learning: Recurrent neural network (RNN) has been introduced in (Hochreiter et al., 2001) to learn the update rule of the task-specific model parameters \u03b8t, with prior encoded in the RNN\u2019s weights. Following this work, different RNN architechtures have been explored to enhance the learning of the update rules. On the one hand, gradient information has been leveraged in (Andrychowicz et al., 2016; Li & Malik, 2017; Ravi & Larochelle, 2017) to mimic the gradientbased optimization. On the other hand, temporal convolutions and soft attention have been utilized to aggregate and pinpoint information from past experiences (Mishra et al., 2018). More recently, this paradigm of NN-based optimization has been extended to Beyesian meta-learning, aiming to infer the posterior pdf p(\u03b8t|ytrnt ;Xtrnt ) (Gordon et al., 2019). Due to the blackbox nature of the RNNs however, it is hard to interpret the impact of learned prior from the NN-based update rules.\nOptimization-based meta-learning: To empower fast adaptation to a new task, MAML capitalized on learning a task-invariant initialization (Finn et al., 2017), with task-level learning defined by a cascade of a few GD steps on the task-specific parameter \u03b8t. Intuitively, by descending a small number of steps, \u03b8t should not deviate too far away from its initial value \u03b8. In fact, it has been pointed out in (Grant et al., 2018) that MAML\u2019s GD solver satisfies \u03b8\u2217t (\u03b8) \u2248 \u03b8\u0302t(\u03b8) = argmin\u03b8t L(\u03b8t;D trn t ) + 1 2\u2225\u03b8t \u2212 \u03b8\u2225 2 \u039bt\n, where \u039bt is determined by hyperparamters of GD. This observation indicates that MAML\u2019s optimization strategy approximates an implicit Gaussian prior p(\u03b8t;\u03b8) = N (\u03b8,\u039b\u22121t ), with initialization \u03b8 init = \u03b8 serving as the mean vector. Following MAML,\na spectrum of algorithms have been developed to encode different priors. For instance, MetaSGD (Li et al., 2017) augments MAML by meta-learning a dimension-wise step size, which essentially corresponds to a per-step diagonal Gaussian prior. Other examples of the induced priors include isotropic Gaussian (Rajeswaran et al., 2019; Abbas et al., 2022), diagonal Gaussian (Ravi & Beatson, 2019; Nguyen et al., 2020), per-step block-diagonal Gaussian (Park & Oliva, 2019; Flennerhag et al., 2020), and implicit Gaussian (Baik et al., 2020; 2021). Another line of research termed metric-based meta-learning (which can be either NN-based or optimization-based) splits the model into an embedding \u201cbody\u201d and a classifier/regressor \u201chead,\u201d and learn their priors independently (Vinyals et al., 2016; Snell et al., 2017; Sung et al., 2018; Li et al., 2020a). In particular, with \u03b8bodyt and \u03b8 head t denoting the corresponding partitions of \u03b8t, the prior is presumed factorable as p(\u03b8t;\u03b8) = p(\u03b8 body t ;\u03b8 body)p(\u03b8headt ;\u03b8 head), \u03b8 = [\u03b8body\u22a4,\u03b8head\u22a4]\u22a4. On the one hand, the head typically has a nontrivial prior such as the Gaussian one (Bertinetto et al., 2019; Lee et al., 2019). On the other hand, the body\u2019s prior is intentionally restricted to a degenerate pdf p(\u03b8bodyt ;\u03b8 body) := \u03b4(\u03b8bodyt \u2212 \u03b8 body), where \u03b4(\u00b7) is the Dirac delta function. Although freezing the body in task-level optimization remarkably reduces its complexity, it often leads to degraded performance compared to the full GD update (Raghu et al., 2020). In additional to degenerate priors, sparse priors have also been recently investigated to selectively update a subset of parameters (Lee & Choi, 2018; Tian et al., 2020a). Compared to these preset prior pdfs of fixed shapes, the focus of this work is to learn a data-driven prior pdf that can dynamically adjust itself to fit the given tasks.\nAlgorithm unrolling: The advocated MetaProxNet pertains to the algorithm unrolling category (Gregor & LeCun, 2010; Monga et al., 2021; Li et al., 2020b). Closely related to our work is the deep regularization approach introduced in (Li et al., 2020a). This method shares similar high-level idea of incorporating priors into PGD optimization iterations through algorithm unrolling. In (Li et al., 2020a), the hidden representations are transformed into a domain conducive to easy regularization by a predefined prior pdf (e.g., isotropic Gaussian). In contrast, our MetaProxNet approach involves the direct learning of the proximal operator within the parametric space."
        }
    ],
    "title": "META-LEARNING PRIORS USING UNROLLED PROXIMAL NETWORKS",
    "year": 2024
}