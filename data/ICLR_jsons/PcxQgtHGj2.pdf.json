{
    "abstractText": "Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with simple synthetic data for a small number of updates can also improve CQL, providing consistent performance improvement on D4RL Gym locomotion datasets. The results of this paper not only illustrate the importance of pre-training for offline DRL but also show that the pre-training data can be synthetic and generated with remarkably simple mechanisms.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zecheng Wang"
        },
        {
            "affiliations": [],
            "name": "Che Wang"
        },
        {
            "affiliations": [],
            "name": "Zixuan Dong"
        },
        {
            "affiliations": [],
            "name": "Keith Ross"
        }
    ],
    "id": "SP:d6d66ca67c64fa30a60edf92a6cb60e05c585381",
    "references": [
        {
            "authors": [
                "Connor Anderson",
                "Ryan Farrell"
            ],
            "title": "Improving fractal pre-training",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Lili Chen",
                "Kevin Lu",
                "Aravind Rajeswaran",
                "Kimin Lee",
                "Aditya Grover",
                "Misha Laskin",
                "Pieter Abbeel",
                "Aravind Srinivas",
                "Igor Mordatch"
            ],
            "title": "Decision transformer: Reinforcement learning via sequence modeling",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Xinyue Chen",
                "Zijian Zhou",
                "Zheng Wang",
                "Che Wang",
                "Yanqiu Wu",
                "Keith Ross"
            ],
            "title": "Bail: Best-action imitation learning for batch deep reinforcement learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Cheng-Han Chiang",
                "Hung-yi Lee"
            ],
            "title": "On the transferability of pre-trained language models: A study from artificial datasets",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Jeff Donahue",
                "Yangqing Jia",
                "Oriol Vinyals",
                "Judy Hoffman",
                "Ning Zhang",
                "Eric Tzeng",
                "Trevor Darrell"
            ],
            "title": "Decaf: A deep convolutional activation feature for generic visual recognition",
            "venue": "In International conference on machine learning,",
            "year": 2014
        },
        {
            "authors": [
                "Justin Fu",
                "Aviral Kumar",
                "Ofir Nachum",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "D4rl: Datasets for deep data-driven reinforcement learning",
            "venue": "arXiv preprint arXiv:2004.07219,",
            "year": 2020
        },
        {
            "authors": [
                "Scott Fujimoto",
                "David Meger",
                "Doina Precup"
            ],
            "title": "Off-policy deep reinforcement learning without exploration",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Hiroki Furuta",
                "Yutaka Matsuo",
                "Shixiang Shane Gu"
            ],
            "title": "Generalized decision transformer for offline hindsight information matching",
            "venue": "arXiv preprint arXiv:2111.10364,",
            "year": 2021
        },
        {
            "authors": [
                "Nicklas Hansen",
                "Zhecheng Yuan",
                "Yanjie Ze",
                "Tongzhou Mu",
                "Aravind Rajeswaran",
                "Hao Su",
                "Huazhe Xu",
                "Xiaolong Wang"
            ],
            "title": "On pre-training for visuo-motor control: Revisiting a learning-from-scratch baseline",
            "venue": "arXiv preprint arXiv:2212.05749,",
            "year": 2022
        },
        {
            "authors": [
                "Tairan He",
                "Yuge Zhang",
                "Kan Ren",
                "Minghuan Liu",
                "Che Wang",
                "Weinan Zhang",
                "Yuqing Yang",
                "Dongsheng Li"
            ],
            "title": "Reinforcement learning with automated auxiliary loss search",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Zexue He",
                "Graeme Blackwood",
                "Rameswar Panda",
                "Julian McAuley",
                "Rogerio Feris"
            ],
            "title": "Synthetic pre-training tasks for neural machine translation",
            "venue": "arXiv preprint arXiv:2212.09864,",
            "year": 2022
        },
        {
            "authors": [
                "Zexue He",
                "Graeme Blackwood",
                "Rameswar Panda",
                "Julian McAuley",
                "Rogerio Feris"
            ],
            "title": "Synthetic pre-training tasks for neural machine translation",
            "year": 2023
        },
        {
            "authors": [
                "Minyoung Huh",
                "Pulkit Agrawal",
                "Alexei A Efros"
            ],
            "title": "What makes imagenet good for transfer learning",
            "venue": "arXiv preprint arXiv:1608.08614,",
            "year": 2016
        },
        {
            "authors": [
                "Michael Janner",
                "Justin Fu",
                "Marvin Zhang",
                "Sergey Levine"
            ],
            "title": "When to trust your model: Model-based policy optimization",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Michael Janner",
                "Qiyang Li",
                "Sergey Levine"
            ],
            "title": "Offline reinforcement learning as one big sequence modeling problem",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Siddharth Karamcheti",
                "Suraj Nair",
                "Annie S Chen",
                "Thomas Kollar",
                "Chelsea Finn",
                "Dorsa Sadigh",
                "Percy Liang"
            ],
            "title": "Language-driven representation learning for robotics",
            "venue": "arXiv preprint arXiv:2302.12766,",
            "year": 2023
        },
        {
            "authors": [
                "Hirokatsu Kataoka",
                "Kazushige Okayasu",
                "Asato Matsumoto",
                "Eisuke Yamagata",
                "Ryosuke Yamada",
                "Nakamasa Inoue",
                "Akio Nakamura",
                "Yutaka Satoh"
            ],
            "title": "Pre-training without natural images",
            "venue": "In Proceedings of the Asian Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Simon Kornblith",
                "Jonathon Shlens",
                "Quoc V Le"
            ],
            "title": "Do better imagenet models transfer better",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Kostrikov",
                "Ashvin Nair",
                "Sergey Levine"
            ],
            "title": "Offline reinforcement learning with implicit q-learning",
            "venue": "arXiv preprint arXiv:2110.06169,",
            "year": 2021
        },
        {
            "authors": [
                "Kundan Krishna",
                "Jeffrey Bigham",
                "Zachary C Lipton"
            ],
            "title": "Does pretraining for summarization require knowledge transfer",
            "venue": "arXiv preprint arXiv:2109.04953,",
            "year": 2021
        },
        {
            "authors": [
                "Aviral Kumar",
                "Aurick Zhou",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "Conservative q-learning for offline reinforcement learning",
            "venue": "arXiv preprint arXiv:2006.04779,",
            "year": 2020
        },
        {
            "authors": [
                "Sergey Levine",
                "Aviral Kumar",
                "George Tucker",
                "Justin Fu"
            ],
            "title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems",
            "venue": "arXiv preprint arXiv:2005.01643,",
            "year": 2020
        },
        {
            "authors": [
                "Wenzhe Li",
                "Hao Luo",
                "Zichuan Lin",
                "Chongjie Zhang",
                "Zongqing Lu",
                "Deheng Ye"
            ],
            "title": "A survey on transformers in reinforcement learning",
            "venue": "arXiv preprint arXiv:2301.03044,",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Stephen Merity",
                "Caiming Xiong",
                "James Bradbury",
                "Richard Socher"
            ],
            "title": "Pointer sentinel mixture models, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Suraj Nair",
                "Aravind Rajeswaran",
                "Vikash Kumar",
                "Chelsea Finn",
                "Abhinav Gupta"
            ],
            "title": "R3m: A universal visual representation for robot manipulation",
            "venue": "arXiv preprint arXiv:2203.12601,",
            "year": 2022
        },
        {
            "authors": [
                "Isabel Papadimitriou",
                "Dan Jurafsky"
            ],
            "title": "Learning music helps you read: Using transfer to study linguistic structure in language models",
            "venue": "arXiv preprint arXiv:2004.14601,",
            "year": 2020
        },
        {
            "authors": [
                "Simone Parisi",
                "Aravind Rajeswaran",
                "Senthil Purushwalkam",
                "Abhinav Gupta"
            ],
            "title": "The unsurprising effectiveness of pre-trained vision models for control",
            "venue": "arXiv preprint arXiv:2203.03580,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "year": 2018
        },
        {
            "authors": [
                "Ilija Radosavovic",
                "Tete Xiao",
                "Stephen James",
                "Pieter Abbeel",
                "Jitendra Malik",
                "Trevor Darrell"
            ],
            "title": "Real-world robot learning with masked visual pre-training",
            "venue": "In Conference on Robot Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Machel Reid",
                "Yutaro Yamada",
                "Shixiang Shane Gu"
            ],
            "title": "Can wikipedia help offline reinforcement learning",
            "venue": "arXiv preprint arXiv:2201.12122,",
            "year": 2022
        },
        {
            "authors": [
                "Ryokan Ri",
                "Yoshimasa Tsuruoka"
            ],
            "title": "Pretraining with artificial language: Studying transferable knowledge in language models",
            "venue": "arXiv preprint arXiv:2203.10326,",
            "year": 2022
        },
        {
            "authors": [
                "Rutav Shah",
                "Vikash Kumar"
            ],
            "title": "Rrl: Resnet as representation for reinforcement learning. In Self-Supervision for Reinforcement Learning Workshop-ICLR",
            "year": 2021
        },
        {
            "authors": [
                "Shiro Takagi"
            ],
            "title": "On the effect of pre-training for transformer in different modality on offline reinforcement learning",
            "venue": "arXiv preprint arXiv:2211.09817,",
            "year": 2022
        },
        {
            "authors": [
                "Che Wang",
                "Xufang Luo",
                "Keith Ross",
                "Dongsheng Li"
            ],
            "title": "Vrl3: A data-driven framework for visual deep reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Wolf",
                "Julien Chaumond",
                "Lysandre Debut",
                "Victor Sanh",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Morgan Funtowicz",
                "Joe Davison",
                "Sam Shleifer"
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "In Conference on Empirical Methods in Natural Language Processing: System Demonstrations,",
            "year": 2020
        },
        {
            "authors": [
                "Yuhuai Wu",
                "Markus N Rabe",
                "Wenda Li",
                "Jimmy Ba",
                "Roger B Grosse",
                "Christian Szegedy"
            ],
            "title": "Lime: Learning inductive bias for primitives of mathematical reasoning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Yuhuai Wu",
                "Felix Li",
                "Percy S Liang"
            ],
            "title": "Insights into pre-training via simpler synthetic tasks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Zhihui Xie",
                "Zichuan Lin",
                "Deheng Ye",
                "Qiang Fu",
                "Yang Wei",
                "Shuai Li"
            ],
            "title": "Future-conditioned unsupervised pretraining for decision transformer",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Mengjiao Yang",
                "Ofir Nachum"
            ],
            "title": "Representation matters: Offline pretraining for sequential decision making",
            "venue": "arXiv preprint arXiv:2102.05815,",
            "year": 2021
        },
        {
            "authors": [
                "Albert Zhan",
                "Ruihan Zhao",
                "Lerrel Pinto",
                "Pieter Abbeel",
                "Michael Laskin"
            ],
            "title": "A framework for efficient robotic manipulation",
            "venue": "In Deep RL Workshop NeurIPS 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Reid"
            ],
            "title": "2022) for both pre-training and finetuning",
            "year": 2022
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "Qmax, where Qmax is the safe Q value predefined for each dataset. Due to the robustness of this method (Wang et al., 2022), we choose Qmax = 100 \u00d7 rmax given the discount factor of 0.99, where rmax is the maximum reward in the dataset. Note that we do not include a safe Q factor",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "It is well-known that pre-training can provide significant boosts in performance and robustness for downstream tasks, both for Natural Language Processing (NLP) and Computer Vision (CV). Recently, in the field of Deep Reinforcement Learning (DRL), research on pre-training is also becoming increasingly popular. An important step in the direction of pre-training DRL models is the recent paper by Reid et al. (2022), which showed that for Decision Transformer (DT) (Chen et al., 2021), pre-training with the Wikipedia corpus can significantly improve the performance of the downstream offline RL task. Reid et al. (2022) further showed that pre-training on predicting pixel sequences hurts performance. The authors state that their results indicate \u201ca foreseeable future where everyone should use a pre-trained language model for offline RL\u201d. In a more recent paper, Takagi (2022) explores more deeply why pre-training with a language corpus can improve DT. However, it remains unclear whether language data is special in providing such a benefit, or whether more naive pre-training approaches can achieve the same effect. Understanding this important question can help us develop better pre-training schemes for DRL algorithms that are more performant, robust and efficient.\nWe first explore pre-training Decision Transformer (DT) with synthetic data generated from a simple and seemingly naive approach. Specifically, we create a finite-state Markov Chain with a small number of states (100 states by default). The transition matrix of the Markov chain is obtained randomly and is not related to the environments or the offline datasets. Using the one-step MC, we generate a sequence of synthetic MC states. During pre-training, we treat each MC state in the sequence as a token, feed the sequence into the transformer, and employ autoregressive next-state (token) prediction, as is often done in transformer-based LLMs (Brown et al., 2020). We pre-train\n\u2217Equal contribution. \u2021Corresponding author, email: zw2374@nyu.edu \u2020This work was done prior to Che Wang joining Amazon.\nwith the synthetic data for a relatively small number of updates compared with that of language pre-training updates in (Reid et al., 2022). After pre-training with the synthetic data, we then finetune with a specific offline dataset using the DT offline-DRL algorithm. Surprisingly, this simple approach significantly outperforms standard DT (i.e., with no pre-training) and also outperforms pre-training with a large Wiki corpus. Additionally, we show that even pre-training with Independent and Identically Distributed (IID) data can still match the performance of Wiki pre-training.\nInspired by these results, we then consider pre-training Conservative Q-Learning (CQL) (Kumar et al., 2020) which employs a Multi-Layer Perceptron (MLP) backbone. Here, we randomly generate a policy and transition probabilities, from which we generate a sequence of Markov Decision Process (MDP) state-action pairs. We then feed the state-action pairs into the Q-network MLPs and pre-train them by predicting the subsequent state. After this, we fine-tune them with a specific offline dataset using CQL. Surprisingly, pre-training with IID and MDP data both can give a boost to CQL.\nOur experiments and extensive ablations show that pre-training offline DRL models with simple synthetic datasets can significantly improve performance compared with those with no pre-training, both for transformer- and MLP-based backbones, with a low computation overhead. The results also show that large language datasets are not necessary for obtaining performance boosts, which sheds light on what kind of pre-training strategies are critical to improving RL performance and argues for increased usage of pre-training with synthetic data for an easy and consistent performance boost."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Many practical applications of RL constrain agents to learn from an offline dataset that has already been gathered, without further interactions with the environment (Fujimoto et al., 2019; Levine et al., 2020). The early offline DRL papers often employ Multi-Layer Perceptron (MLP) architectures (Fujimoto et al., 2019; Chen et al., 2020; Kumar et al., 2020; Kostrikov et al., 2021). More recently, there has been significant interest in transformer-based architectures for offline DRL, including DT (Chen et al., 2021), Trajectory Transformer (Janner et al., 2021) and others (Furuta et al., 2021; Li et al., 2023). In this paper, we study both transformer-based and the more conventional Q-learningbased methods to understand how different pre-training schemes can affect their performance.\nIt is well-known that pre-training can provide significant improvements in performance and robustness for downstream tasks, both for Natural Language Processing (NLP) (Devlin et al., 2018; Radford et al., 2018; Brown et al., 2020) and Computer Vision (CV) (Donahue et al., 2014; Huh et al., 2016; Kornblith et al., 2019). In offline DRL, pre-training is becoming an increasingly popular research topic. An important step in the direction of pre-training offline DRL models is Reid et al. (2022), which shows that for DT, pre-training on Wikipedia can significantly improve the performance of the downstream RL task. Takagi (2022) further explores why such pre-training improves DT. Inspired by these recent findings, we aim for a more comprehensive understanding of pre-training in DRL.\nThere are also works that pretrain on generic image data or use offline DRL data itself to learn representations and then use them to learn offline or online DRL tasks (Yang & Nachum, 2021; Zhan et al., 2021; Wang et al., 2022; Shah & Kumar, 2021; Hansen et al., 2022; Nair et al., 2022; Parisi et al., 2022; Radosavovic et al., 2023; Karamcheti et al., 2023). Xie et al. (2023) shows future-conditioned unsupervised pretraining leads to superior performance in the offline-online setting. Different from these works, we focus on understanding whether language pre-training is special in providing a performance boost and investigate whether synthetic pre-training can help DRL.\nPre-training with synthetic data has been shown to benefit a wide range of downstream NLP tasks (Papadimitriou & Jurafsky, 2020; Krishna et al., 2021; Ri & Tsuruoka, 2022; Wu et al., 2022; Chiang & Lee, 2022), CV tasks (Kataoka et al., 2020; Anderson & Farrell, 2022), and mathematical reasoning tasks (Wu et al., 2021). There are also works that study the effect of different properties of synthetic NLP data Ri & Tsuruoka (2022); Chiang & Lee (2022); He et al. (2022b). In particular, we provide results that show the Identity and Case-Mapping synthetic data schemes from He et al. (2022b) can also improve offline RL performance in Appendix F. While these works focus on CV and NLP applications, we study the effect of pre-training from synthetic data with large domain gaps in DRL.\nTo the best of our knowledge, this is the first paper that shows pre-training on simple synthetic data can be a surprisingly effective approach to improve offline DRL performance for both transformer-based and Q-learning-based approaches."
        },
        {
            "heading": "3 PRE-TRAINING DECISION TRANSFORMER WITH SYNTHETIC DATA",
            "text": ""
        },
        {
            "heading": "3.1 OVERVIEW OF DECISION TRANSFORMER",
            "text": "Chen et al. (2021) introduced Decision Transformer (DT), a transformer-based algorithm for offline RL. An offline dataset consists of trajectories s1, a1, r1, . . . , sN , aN , where sn, an, and rn is the state, action, and reward at timestep n. DT models trajectories by representing them as\n\u03c3 = (R\u03021, s1, a1, . . . , R\u0302N , sN , aN ),\nwhere R\u0302n = \u03a3Nt=nrt is the return-to-go at timestep n. The sequence \u03c3 is modeled with a transformer in an autoregressive manner similar to autoregressive language modeling except that R\u0302n, sn, an at the same timestep n are first projected into separate embeddings while receiving the same positional embedding. In Chen et al. (2021), the model is optimized to predict each action an from (R\u03021, s1, a1, . . . , R\u0302n\u22121, sn\u22121, an\u22121, R\u0302n, sn). After the model is trained with the offline trajectories, at test time, the action at timestep t is selected by feeding into the trained transformer the test trajectory (R\u03021, s1, a1, . . . , R\u0302t, st), where R\u0302t is now an estimate of the optimal return-to-go.\nIn the original DT paper (Chen et al., 2021), there is no pre-training, i.e., training starts with random initial weights. Reid et al. (2022) consider first pre-training the transformer using the Wikipedia corpus, then fine-tuning with the DT algorithm to create a policy for the downstream offline RL task."
        },
        {
            "heading": "3.2 GENERATING SYNTHETIC MARKOV CHAIN DATA",
            "text": "We explore pre-training DT with synthetic data generated from a Markov Chain. For the synthetic data, we simply generate a sequence of states (tokens) using a finite-state Markov Chain with a small number of states. The transition probabilities of the Markov chain are obtained randomly (as described below) and are not related to the environment or the offline dataset. After creating the synthetic sequence data, during pre-training, we feed the sequence into the transformer and employ next state (token) prediction, as is often done in transformer-based LLMs (Brown et al., 2020). After pre-training with the synthetic data, we then fine-tune with the target offline dataset using DT.\nWe generate the MC transition probabilities as follows. Let S = 1, 2, . . . ,M denote the MC\u2019s finite state space, with M = 100 being the default value. For each state in S, we draw M independently and uniformly distributed values, and then create a distribution over the state space S by applying softmax to the vector of M values. In this manner, we generate M probability distributions, one for each state, where each distribution is over S. Using these fixed transition probabilities, we generate the pre-training sequence x0, x1, . . . , xT as follows: we randomly sample from S to get the initial state x0 in the sequence; after obtaining xt, we generate xt+1 using the MC transition probabilities.\nDuring pre-training, we train with autoregressive next-state prediction (Brown et al., 2020):\nL(x0, x1, . . . , xT ; \u03b8) = \u2212 logP\u03b8(x0, x1, . . . , xT ) = \u2212\u03a3Tt=1 logP\u03b8(xt|x0, x1, . . . , xt\u22121). As the states are discrete and analogous to the tokens in language modeling tasks, the embeddings for the states are learned during pre-training as is typically done in the NLP literature."
        },
        {
            "heading": "3.3 RESULTS FOR PRE-TRAINING DT WITH SYNTHETIC DATA",
            "text": "We first compare the performance of the DT baseline (DT), DT with Wikipedia pre-training (DT+Wiki), and DT with pre-training on synthetic data generated from a 1-step MC with 100 states (DT+Synthetic). We consider the same three MuJoCo environments and D4RL datasets (Fu et al., 2020) considered in Reid et al. (2022) plus the high-dimensional Ant environment, giving a total of 12 datasets. For a fair comparison, we use the authors\u2019 code from Reid et al. (2022) when running the downstream experiments for DT, and we keep the hyperparameters identical to those used in (Chen et al., 2021; Reid et al., 2022) whenever possible (Details in Appendix A.1). For each dataset, we fine-tune for 100,000 updates. For DT+Wiki, we perform 80K updates during pre-training following the authors. For DT+Synthetic, however, we found that we can achieve good performance with much fewer pre-training updates, namely, 20K updates. After every 5K updates during fine-tuning, we run 10 evaluation trajectories and record the normalized test performance1.\n1This evaluation metric follows D4RL (Fu et al., 2020). We provide a review in Appendix A.3\nWe report both final performance and, in Appendix B, best performance. For best performance, we use the best test performance seen over the entire fine-tuning period. Best performance is also employed in (Chen et al., 2021; Reid et al., 2022). In practice, to determine when the best performance occurs (and thus the best model parameters), interaction with the environment is required, which is inconsistent with the offline problem formulation. The final performance can be a better metric since it does not assume we can interact with the environment. For the final performance, we average over the last four sets of evaluations (after 85K, 90K, 95K, and 100K updates for DT). When comparing the algorithms, the two measures (final and best) lead to very similar qualitative conclusions. For all DT variants, we report the mean and standard deviation of performance over 20 random seeds.\nTable 1 shows the final performance for DT, DT pre-trained with Wiki, and DT pre-trained with synthetic MC data. We see that, for every dataset, synthetic pre-training does as well or better than the DT baseline, and provides an overall average improvement of nearly 10%. Moreover, synthetic pre-training also outperforms Wiki pre-training by 5% when averaged over all datasets, and this is done with significantly fewer pre-training updates. Compared to DT+Wiki, DT+Synthetic is much more computationally efficient, using only 3% of computation during pre-training and 67% during fine-tuning (Details in Appendix A). Figure 1 shows the normalized score and training loss for DT, DT with Wiki pre-training, and DT with MC pre-training. The curves are aggregated over all 12 datasets, each with 20 different seeds (per-environment curves in Appendix B.2). To account for the pre-training updates, we also offset the curve for DT+Synthetic to the right by 20K updates. Note that in practice, the pre-training only needs to be done once, but the offset here helps to show that even with this disadvantage, DT+Synthetic still quickly outperforms the other two variants.\nOur synthetic data uses a small state space (vocabulary) and carries no long-term contextual or semantic information. From Table 1 and Figure 1 we can conclude that the performance gains obtained by pre-training with the Wikipedia corpus are not due to special properties of language, such as the large vocabulary or the rich long-term contextual and semantic information in the dataset, as conjectured in Reid et al. (2022) and Takagi (2022). In the next subsection, we study how different properties of the synthetic data affect the downstream RL performance."
        },
        {
            "heading": "3.4 ABLATIONS FOR PRE-TRAINING DT WITH SYNTHETIC DATA",
            "text": "In the above results, we employed a one-step MC to generate the synthetic data. In natural language, token dependencies are not simply one-step dependencies. We now consider whether increasing the state dependencies beyond one step can improve downstream performance. Specifically, we consider using a multi-step Markov Chain for generating the synthetic data x0, x1, . . . , xT . In an n-step Markov chain, xt depends on xt\u22121, xt\u22122, . . . , xt\u2212n. For an n-step MC, we randomly construct the fixed n-step transition probabilities, from which we generate the synthetic data. Table 2 shows the final performance averaged over the final four evaluation periods for the DT baseline and for MC pre-training with different numbers of MC steps. We see that synthetic data with different step values all provide better performance than the DT baseline; however, increasing the amount of past dependence in the MC synthetic data does not improve performance over a one-step MC.\nWe now investigate whether increasing the size of the MC state space (analogous to increasing the vocabulary size in NLP) improves performance. Table 3 shows the final performance for DT baseline and DT pre-trained with MC data with different state space sizes. The results show that all state space sizes improve the performance over the baseline, with 100 and 1000 giving the best results.\nWe now consider how changing the temperature parameter in the softmax formula affects the results. (Default temperature is 1.0.) A lower temperature leads to more deterministic state transitions, while a higher temperature leads to more uniform state transitions. Table 4 shows the final performance for DT with MC pre-training with different temperature values. The results show that all temperatures provide a performance gain, with a temperature of 1 being the best. In this table, we also consider generating synthetic data with Independent and Identically Distributed (IID) states with uniform distributions over a state space of size 100. Surprisingly, even this scheme performs significantly better than both the baseline and the Wiki pre-training. This provides further evidence that the complex token dependencies in the Wiki corpus are not likely the cause of the performance boost.\nTable 5 shows the final performance for DT with MC pre-training with different numbers of pretraining updates. Our results show that with even just 1k updates, MC pre-training matches the\nperformance of DT+Wiki pre-training. Using as few as 20k updates (one-fourth of DT+Wiki), our method already obtains significantly better performance.\nThese ablation results show that synthetic pre-training is robust over different settings of the synthetic data, including the degree of past dependence, MC state-space size, the degree of randomness in the transitions, and the number of pre-training updates."
        },
        {
            "heading": "4 PRE-TRAINING CQL WITH SYNTHETIC DATA",
            "text": "Given that pre-training with synthetic data can significantly increase the performance of DT, we now study whether synthetic data can also help other MLP-based offline DRL algorithms. Specifically, we consider CQL, which is a popular offline DRL algorithm for the datasets considered in this paper. For the pre-training objective, we use forward dynamics prediction, as it has been shown to be useful in model-based methods (Janner et al., 2019) and auxiliary loss literature (He et al., 2022a). Since forward dynamics prediction will require both a state and an action as input, we generate a new type of synthetic data, which we call the synthetic Markov Decision Process (MDP) data. Different from synthetic MC, when generating synthetic MDP data, we also take actions into consideration."
        },
        {
            "heading": "4.1 GENERATING SYNTHETIC MDP DATA",
            "text": "To generate the synthetic MDP data, we first define a discrete state space S, a discrete action space A, a random policy distribution \u03c0, and a random transition distribution p. Similar to how we created an MC for the decision transformer, the policy and transition distributions are obtained by applying a softmax function on vectors of random values, and the shape of the distributions is controlled by a temperature term \u03c4 . For each trajectory in the generated data, we start by choosing a state from the state space, and then for each following step in the trajectory, we sample an action from the policy distribution and then sample a state from the transition distribution. Since CQL uses MLP networks and the state and action dimensions are different for each MuJoCo task, during pre-training we map each discrete MDP state and MDP action to a vector that has the same dimension as the MuJoCo\nRL task. For each state and action vector, entries are randomly chosen from a uniform distribution between \u22121 and 1, and then fixed. We pre-train the MLP with the forward dynamics objective, i.e., we predict the next state s\u0302\u2032 and minimize the MSE loss (s\u0302\u2032\u2212 s\u2032)2, where s\u2032 is the actual next state in the trajectory. After pre-training, we then fine-tune the MLP with a specific dataset using the CQL algorithm."
        },
        {
            "heading": "4.2 RESULTS FOR CQL WITH SYNTHETIC DATA PRE-TRAINING",
            "text": "In the experimental results presented here, for the CQL baseline, we train for 1 million updates. For CQL with synthetic MDP pre-training (CQL+MDP), we pre-train for only 100K updates and then train (i.e., fine-tune) for 1 million updates. By default, we set the state and action space sizes to 100 and use a temperature \u03c4 = 1 for both the policy and transition distributions. All results are over 20 random seeds. We do not tune any hyperparameters for CQL or CQL with synthetic pre-training but directly adopt the default ones in the codebase recommended by CQL authors2. More details on CQL experiments can be found in Appendix A.2.\nTable 6 compares the performance of CQL to CQL+MDP. The table includes a wide range of MDP state/action space sizes and shows that synthetic pre-training gives a significant and consistent performance boost. With 1,000 states/actions, synthetic pre-training provides a 10% average improvement over all datasets and up to 84% and 49% for two of the medium-expert datasets.\nTable 7 shows the final performance for CQL+MDP with different temperature values using the default state/action space size. The results show that either too small or too large of a temperature can reduce the performance boost, while the default temperature (\u03c4 = 1) gives good final performance averaged over all datasets. Table 7 also shows the results with uniformly distributed IID synthetic data, equivalent to using an infinitely large temperature. Surprisingly, the IID data performs almost as well as the MDP synthetic data, indicating the robustness of synthetic pre-training regardless of state dependencies. We provide a partial theoretical explanation of this behavior in the next subsection.\nTable 8 shows the final performance for CQL+MDP with different numbers of pre-training updates. Even with only 1K updates, synthetic MDP pre-training outperforms the baseline. The best performance boost is obtained with more pre-training updates of 100K and 500K.\nFigure 2 shows the normalized score and training loss (Q loss plus CQL conservative loss) averaged over all datasets during fine-tuning. Similar to Figure 1, our synthetic experiments (CQL+MDP and CQL+IID) have been offseted by 100k updates. Both pre-training schemes start to surpass the CQL baseline at around 400K updates and maintain a significant performance advantage onward. In addition, performing a pre-training update is quite fast since the forward dynamics objective only involves calculating the MSE loss of predicting the next state and backpropagation of the Q-network. 100K pre-train can be done in 5 minutes on a single GPU3.\n2https://github.com/young-geng/CQL 3Detailed computation time discussion can be found in Appendix A.2\nTo summarize, these results show that for a wide range of MDP data settings, pre-training with synthetic data provides a consistent performance improvement over the CQL baseline. Due to limited space, a number of additional experiments and analyses are presented in Appendix E, F, G, H, I, J, K."
        },
        {
            "heading": "4.3 ANALYSIS OF OPTIMIZATION OBJECTIVE",
            "text": "To gain some insight into why IID synthetic data does almost as well as MDP data, we now take a closer look at the pre-training loss function. Let f\u03b8(s, a) be an MLP that takes as input a state-action pair (s, a) and outputs a vector state s\u2032. Let \u03c3 = (s0, a0, s1, a1, . . . , sT\u22121, aT\u22121, sT ) denote the pre-training data, where the states and actions come from finite state and action spaces S and A. For the given pre-training data \u03c3, we optimize \u03b8 to minimize the forward-dynamics objective:\nJ(\u03b8) = T\u22121\u2211 t=0 ||f\u03b8(st, at)\u2212 st+1||2\nLet \u2206(s, a) be the set of states s\u2032 that directly follow (s, a) in the pre-training dataset \u03c3. If s\u2032 directly follows (s, a) multiple times, we list s\u2032 repeatedly in \u2206(s, a) for each occurrence. We can then rewrite J(\u03b8) as\nJ(\u03b8) = \u2211\n(s,a)\u2208S\u00d7A \u2211 s\u2032\u2208\u2206(s,a) ||f\u03b8(s, a)\u2212 s\u2032||2\nNow let\u2019s assume that the MLP is very expressive so that we can choose \u03b8 so that f\u03b8(s, a) can take on any desired vector x. For fixed (s, a), let\nx\u2217(s, a) = argmin x \u2211 s\u2032\u2208\u2206(s,a) ||x\u2212 s\u2032||2\nNote that x\u2217(s, a) is simply the centroid for the data in \u2206(s, a). Thus\nmin \u03b8\nJ(\u03b8) = \u2211\n(s,a)\u2208S\u00d7A \u2211 s\u2032\u2208\u2206(s,a) ||x\u2217(s, a)\u2212 s\u2032||2\nIn other words, the forward-dynamics objective is equivalent to finding the centroid of \u2206(s, a) for each (s, a) \u2208 S \u00d7A. For each (s, a) we want the MLP to predict the centroid in \u2206(s, a), that is, we want f\u03b8(s, a) = x\u2217(s, a). This observation is true no matter how the pre-training data \u03c3 is generated, for example, by an MDP or if each (s, a) pair is IID.\nNow let\u2019s compare the MDP and IID pre-training data approaches. For the two approaches, the centroid values will be different. In particular, for the IID case, the centroids will be near each other and collapse to a single point for an infinite-length sequence \u03c3. For the MDP case, the centroids will be farther apart and will be distinct for each (s, a) pair in the limiting case. From the results in Table 7, the performance after fine-tuning is largely insensitive to the distance among the various centroids."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we considered offline DRL and studied the effects of several pre-training schemes with synthetic data. The contributions of this paper are as follows:\n1. We propose a simple yet effective synthetic pre-training scheme for DT. Data generated from a one-step Markov Chain with a small state space provides better performance than pre-training with the Wiki corpus, whose vocabulary is much larger and contains much more complicated token dependencies. This novel finding challenges the previous view that language pre-training can provide unique benefits for DRL.\n2. We show that synthetic pre-training of CQL with an MLP backbone can also lead to significant performance improvement. This is the first paper that shows pre-training on simple synthetic data is a surprisingly effective approach to improve offline DRL performance for both transformer-based and Q-learning-based algorithms.\n3. We provide ablations showing the surprising robustness of synthetic pre-training over past dependence, state/action-space size, and the peakedness of the transition and policy distributions, giving a consistent performance gain across different data generation settings.\n4. Moreover, we show the proposed approach is efficient and easy to use. For DT, synthetic data pre-training achieves superior performance with 4\u00d7 less pre-train updates, taking only 3% computation time at pre-training and 67% at fine-tuning compared with DT+Wiki. For CQL, the generated data have consistent state and action dimensions with the downstream RL task, making it easy to use with MLPs, and the pre-training only takes 5 minutes.\n5. Finally, we provide theoretical insights into why IID data can still achieve a good performance. We show the forward dynamics objective is equivalent to finding the state centroids underlying the synthetic dataset, and CQL is largely insensitive to their distribution.\nThe novel findings in this paper bring up a number of exciting future research directions. One is to further understand why pre-training on data that is entirely unrelated to the RL task can improve performance. Here, it is unlikely the improvement comes from a positive transfer of features, so we suspect that such pre-training might have helped make the optimization process smoother during fine-tuning. Other interesting directions include exploring different synthetic data generation schemes and investigating the extent to which synthetic data can be helpful."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work is partially supported by Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning at NYU Shanghai."
        },
        {
            "heading": "A HYPERPARAMETERS & TRAINING DETAILS",
            "text": "A.1 DECISION TRANSFORMER\nImplementation & Experiment details Pre-trained models are trained with the HuggingFace Transformers library (Wolf et al., 2020). We used AdamW optimizer (Loshchilov & Hutter, 2017) for both pre-training and finetuning. Unless mentioned, we followed the default hyperparameter settings from Huggingface and PyTorch. Our model code is gpt2. Synthetic pre-training is done on synthetic datasets generated to be about the size of Wikitext-103 (Merity et al., 2016). Our hyperparameter choices follow those from Reid et al. (2022) for both pre-training and finetuning, which are shown in detail in table 9 and 10. In Reid et al. (2022), it is shown that the additional kmeans auxiliary loss and LM loss provide only marginal improvement (An average score of 0.3). Without using these losses, our synthetic pre-training results outperform DT+Wiki by an average score of 3.7, as shown in Table 1.\nDT Computation Time Discussion Table 11 shows the number of updates needed for each variant to reach 90% final performance of the DT baseline for individual datasets. Our synthetic models are about 27% faster compared to Wikipedia pre-training in reaching the goal returns averaging over all datasets.\nIn terms of pre-training computation time, we run both Wikipedia and synthetic pre-training on 2 rtx8000 GPUs. Synthetic pre-training takes about 2 hours and 11 minutes to train for 80k updates while Wikipedia pre-training takes about 16 hours and 45 minutes to train for the same number of\nupdates. The 87% reduction in training time is achieved largely due to the reduced number of token embeddings. Furthermore, Table 5 has shown that synthetic pre-training reaches ideal performance with as few as 20k pre-training updates (in about 33 minutes), which means that synthetic pre-training obtains superior results with only about 3% of the computation resources needed for Wikipedia pre-training.\nTable 12 shows the computation time comparison of downstream RL tasks over the medium-expert datasets only. Without using auxiliary losses in Reid et al. (2022), our pre-trained model runs at about the same speed as DT baseline which is much faster than DT+Wiki (we only use 67% of the time during fine-tuning).\nA.2 CQL EXPERIMENTS DETAILS\nWe develop our code based on the implementation recommended by CQL authors4. Most of the hyperparameters used in the training process or the dataset follow the default setting, and we list them in detail in Table 15 and Table 16. Also, we provide additional implementation and experiment details below.\nCQL Computation Time Table 13 first shows the number of updates required for each default algorithm variant to reach 90% of the final performance of the CQL baseline for individual datasets. Compared with the CQL baseline, CQL-MDP takes about 34% and CQL-IID takes about 32% fewer fine-tuning updates in reaching the same target test returns when averaged over all datasets. In terms of the real wall-clock computation time, Table 14 shows the time consumed on one single rtx8000 GPU to pre-train 100K updates with synthetic MDP data, and to fine-tune 1M updates with CQL algorithm for each downstream medium-expert dataset. Surprisingly, a few minutes of synthetic pre-training is enough to efficiently improve downstream performance.\nGenerate Synthetic MDP Data When generating the synthetic MDP data, we make use of numpy.random.seed() to construct policy/transition distributions instead of storing those probabilities in a huge table. For example, every time we retrieve a transition distribution specified by (conditioned on) an integer pair (s, a), we first set numpy.random.seed(s\u00d7 888 + a\u00d7 777), then generate a uniformly distributed (between 0 and 1) vector with the same length as the state space size, and finally input this vector to the softmax function with temperature \u03c4 to get a probability distribution. The next state transitioned from (s, a) can be sampled from this particular distribution.\n4https://github.com/young-geng/CQL\nSynthetic Pre-training Following the common framework of pre-training and then fine-tuning LLMs, we always pre-train our MLP by using only one seed of 42, while fine-tuning the MLP on multiple seeds due to the algorithmic sensitivity to hyperparameter settings of CQL (Kostrikov et al., 2021).\nCQL Fine-tuning We adopt the Safe Q Target technique (Wang et al., 2022) to alleviate the potential Q loss divergence due to RL training instability and distribution shift which has been proven to exist through our early experiments. When computing the target Q value ytarget in each update of the SAC algorithm, we simply set ytarget \u2190 Qmax if ytarget > Qmax, where Qmax is the safe Q value predefined for each dataset. Due to the robustness of this method (Wang et al., 2022), we choose Qmax = 100 \u00d7 rmax given the discount factor of 0.99, where rmax is the maximum reward in the dataset. Note that we do not include a safe Q factor as proposed in the original work. For more details, please refer to Wang et al. (2022).\nA.3 EVALUATION METRIC\nFor each experiment setting, we record the Normalized Test Score which is computed as AVG_TEST_RETURN\u2212MIN_SCORE\nMAX_SCORE\u2212MIN_SCORE \u00d7 100, where AVG_TEST_RETURN is the test return averaged over 10 undiscounted evaluation trajectories; MIN_SCORE and MAX_SCORE are environment-specific constants predefined by the D4RL dataset (Fu et al., 2020). We summarize those constants of each environment in Table 17."
        },
        {
            "heading": "B ADDITIONAL DT RESULTS",
            "text": "B.1 BEST SCORE RESULTS\nTable 18 shows the best performance for DT, DT pre-trained with Wiki, and DT pre-trained with synthetic MC data. Similar to Table 1, synthetic pre-training does as well or better than the DT baseline for every dataset. Synthetic pre-training also outperforms Wiki pre-training with significantly fewer pre-training updates.\nTable 19 shows best score comparison for DT and DT + MC pre-training with different MC steps. 1-step MC gives the best performance, while all settings provide a performance gain over the baseline.\nTable 20 shows the best score comparison of DT baseline and DT + MC pre-training with different state space sizes. All MC settings provide a performance boost, while state space sizes of 100 and 1000 give the best performance.\nTable 21 shows the best score comparison of DT baseline and DT + MC pre-training with different temperature values. All temperature settings provide some performance boost over the baseline.\nTable 22 shows the best score comparison for DT and DT + MC pre-training for different gradient steps. In this case, our synthetic pre-training experiments show a similar best score performance boost over the baseline.\nB.2 DT TRAINING CURVES\nFigure 3 and 4 shows the normalized score and training loss averaged over all datasets during finetuning for DT, DT with Wiki pre-training, and DT with synthetic pre-training. Each curve is averaged over 20 different seeds."
        },
        {
            "heading": "C ADDITIONAL CQL RESULTS",
            "text": "C.1 BEST SCORE RESULTS\nIn this section, we present the best score obtained along the fine-tuning process for individual datasets and different algorithm variants. Table 23 shows the best performance of CQL baseline and CQL+Synthetic with a range of different state space sizes, while setting the temperature to 1 and pre-training for 100K updates. Table 24 shows the best performance of CQL baseline, CQL+Synthetic with a range of temperatures, and CQL+IID, while keeping state and action space sizes to 100 and pretraining for 100K updates. Table 8 shows the best performance of CQL baseline and CQL+Synthetic pre-trained with a different number of updates while setting state and action space size to 100 and the temperature to 1. Most of the synthetic pre-trained variants can outperform the CQL baseline in terms of the best score averaged over all datasets, while in general, the best score is not as sensitive to the hyperparameter settings of synthetic pre-training as the final score. In addition, it is worth noting that synthetic pre-training usually leads to a smaller standard deviation of the acquired best scores compared with the CQL baseline, which indicates a more consistent appearance of high performance during fine-tuning.\nC.2 CQL FINE-TUNING CURVES FOR INDIVIDUAL DATASET\nFigure 5 and Figure 6 show the normalized test performance and the combined loss respectively for each dataset during the fine-tuning process. Each curve is averaged over 20 seeds. We shift all CQL+MDP and CQL+IID curves to the right to offset the synthetic pre-training for 100K updates."
        },
        {
            "heading": "D ADDITIONAL FIGURES",
            "text": "Figure 7 illustrates how temperature values affect the transition distribution:"
        },
        {
            "heading": "E DT WITH MORE FINE-TUNING UPDATES",
            "text": "Table 26 shows what happens when the DT baseline is trained with more fine-tuning updates. The results show that more updates can indeed further improve the DT baseline. The DT baseline with 80K more updates (DT+80K more) is able to achieve a similar or even stronger performance than DT+Wiki. However, the DT baseline with 80K additional updates is still significantly weaker than DT+Synthetic. Note that DT+80K more updates uses a total of 180K updates, while our DT+Synthetic scheme uses a total of 120K updates (20K pre-train and 100K finetune), and DT+Synthetic is still better. These results further support our finding that (1) Wiki pretraining does not have a special benefit, and (2) synthetic pre-training can significantly outperform the baseline, even with fewer total updates.\nFigure 8 presents the learning curves where the x-axis indicates the total number of updates, pretraining included. Here we train all three variants longer, to a total of 180K updates. Note that for the two pre-training schemes, the curve is shifted to the right to account for the number of pre-training updates. The curves are shown for the different datasets separately in Figure 9. Figure 9 shows that when trained for more fine-tuning updates, all variants obtain slightly better performance, while DT+Synthetic achieves the best performance quite consistently for different values of the total number of updates."
        },
        {
            "heading": "F OTHER DT PRE-TRAINING STRATEGIES",
            "text": "In Table 27 we study two alternative synthetic data generation schemes inspired by He et al. (2023), namely, Identity Operation and Token Mapping. For Identity Operation, the model is simply trained to predict the current state. For Token Mapping, the model is trained to predict a fixed one-to-one mapping from each state (token) in the state space (vocabulary) to a state (token) in the target state space (vocabulary). We use a similar vocabulary size as in He et al. (2023). As before, the DT baseline is fine-tuned for 100K updates, and DT+Wiki is pre-trained for 80K updates and fine-tuned for 100K updates. DT+Synthetic, DT+Identity, and DT+Mapping are all pre-trained with 20K updates and fine-tuned for 100K updates. The results show that these alternative schemes also lead to improved performance over the DT baseline; however, our proposed synthetic scheme provides the strongest performance boost.\nG IMPACT OF MC PARAMETERS FOR DIFFERENT AMOUNTS OF FINE-TUNING UPDATES"
        },
        {
            "heading": "H ANALYSIS OF HOW PRE-TRAINING EFFECTS DT WEIGHTS AND FEATURES",
            "text": "In this section, we provide an empirical analysis to shed light on how the different pre-training schemes affect the weights and features in the Decision Transformer. We consider the weights and features at three stages: (1) the randomly initialized weights; (2) the weights after pre-training; and (3) the weights after fine-tuning.\nFigure 12(a) shows in blue the weight cosine-similarities between the pre-trained and fine-tuned weights (PT vs. FT), and shows in orange the weight cosine-similarities between the randomly initialized (before training) and fine-tuned weights (RI vs. FT). Figure 12(b) is analogous but for features instead of weights. To obtain the features, we pass a portion of each offline RL dataset through a frozen model to extract the feature vectors before the prediction head. The comparison is made across all datasets for the DT baseline, DT with Wiki pre-training, DT with default synthetic pre-training, DT with random IID data pre-training, DT with Identity Operation pre-training, and DT with Mapping pre-training. From Figure 12 we observe the following:\n1. From Figure 12a, we see that the cosine-similarity between the initial weights and the weights after fine-tuning (RI vs. FT) for all the pre-training schemes are lower compared to that of the DT baseline (fine-tuning without pre-training at all). This suggests that pretraining together with fine-tuning alters the angle of the weights more than when doing fine-tuning alone. This phenomenon suggests that pre-training is able to move the weight vector to a new region which is more beneficial for downstream RL tasks.\n2. From Figure 12a, the cosine-similarities of the weights before and after fine-tuning (PT vs. FT) for the pre-training schemes are all higher than that of the DT baseline. This suggests that during the fine-tuning stage of a pre-training scheme, the weights are changed less. However, we observe that the similarities in the pre-training schemes are inversely proportional to their performance. (DT+Synthetic has the best performance while being the least similar, while DT+Wiki has the worst performance while being the most similar). This suggests that during the fine-tuning stage, encouraging a bigger movement in the weights is beneficial, and that our synthetic pre-training scheme positions the weights to allow for such a movement.\n3. Similar to the weight comparison, from Figure 12b, we also find that the cosine-similarities between the features from randomly initialized models and those after fine-tuning (RI vs. FT) for all the pre-training schemes are much lower compared to that of the DT baseline (by three orders of magnitude), suggesting that there is a bigger change in the features from pre-trained and then fine-tuning than when doing fine-tuning alone. Such a movement of the feature vectors might indicate better learning of the feature representations.\n4. We see from Figure 12b that the features before and after fine-tuning (PT vs. FT) for the synthetic pre-training schemes are more similar than those of DT+Wiki (0.33). This suggests\nthat for the Wiki pre-training scheme, the features need to be altered more due to the domain gap between language and RL, potentially hindering its performance."
        },
        {
            "heading": "I CQL WITH MORE FINE-TUNING UPDATES",
            "text": "Recall that for our CQL pre-training experiments in the main body of the paper, we pre-trained for 100K updates. Table 28 shows the performance of the CQL baseline when it is trained with additional 100K fine-tuning updates, so that the total (pre-training plus fine-tuning) number of updates is the same across all schemes. We see that the additional 100K fine-tuning updates (CQL+100K more) does not improve the final performance of the CQL baseline. Consequently, CQL pre-trained with synthetic data still significantly outperforms the baseline.\nFigure 13 presents the learning curves where the x-axis indicates the total number of updates, pretraining included. All three variants are trained with 1.1M updates in total. Note that for the two pre-training schemes, the curve is shifted to the right to account for the number of pre-training updates. The curves are shown for the different datasets separately in Figure 14."
        },
        {
            "heading": "J OTHER CQL PRETRAINING STRATEGIES",
            "text": "In Table 29 we again study the two alternative synthetic data generation schemes inspired by He et al. (2023), similar to what we did in Appendix F for DT. For Identity Operation, the model is simply trained to predict the concatenated vector of the current state and current action. For Token Mapping, the model is trained to predict a fixed one-to-one mapping from each state-action pair in the source space to a state-action pair in the target space. As before, the CQL baseline is fine-tuned for 1M updates. CQL+MDP, CQL pre-trained with Identity Operation (CQL+Identity), and CQL pre-trained with Token Mapping (CQL+Mapping) are all pre-trained for 100K updates and fine-tuned for 1M updates. The results show that these alternative schemes also lead to improved performance over the CQL baseline; however, our proposed synthetic scheme provides a significantly higher performance boost.\nFigure 15 presents the learning curves where the x-axis indicates the total number of updates, pretraining included. All variants are trained with 1M updates in total (pre-training and fine-tuning) and for the three pre-training schemes, the curves are shifted to the right to account for the number of pre-training updates. With fewer fine-tuning updates, the result shows that CQL pre-trained with either of the two additional synthetic data does not surpass the performance of the CQL baseline, while our MDP synthetic pre-training scheme still significantly boosts the performance.\nK IMPACT OF MDP PARAMETERS FOR DIFFERENT AMOUNTS OF FINE-TUNING UPDATES"
        }
    ],
    "title": "PRE-TRAINING WITH SYNTHETIC DATA HELPS OFFLINE REINFORCEMENT LEARNING",
    "year": 2024
}