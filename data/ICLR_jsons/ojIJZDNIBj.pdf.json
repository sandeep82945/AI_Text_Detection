{
    "abstractText": "Accurate uncertainty measurement is a key step to building robust and reliable machine learning systems. Conformal prediction is a distribution-free uncertainty quantification algorithm popular for its ease of implementation, statistical coverage guarantees, and versatility for underlying forecasters. However, existing conformal prediction algorithms for time series are limited to single-step prediction without considering the temporal dependency. In this paper we propose the Copula Conformal Prediction algorithm for multivariate, multi-step Time Series forecasting, CopulaCPTS. We prove that CopulaCPTS has finite sample validity guarantee. On four synthetic and real-world multivariate time series datasets, we show that CopulaCPTS produces more calibrated and efficient confidence intervals for multi-step prediction tasks than existing techniques.",
    "authors": [],
    "id": "SP:053e61d754956203ed0686783159ddb256005e1d",
    "references": [
        {
            "authors": [
                "Ahmed Alaa",
                "Mihaela Van Der Schaar"
            ],
            "title": "Frequentist uncertainty in recurrent neural networks via blockwise influence functions",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Anastasios N Angelopoulos",
                "Stephen Bates"
            ],
            "title": "A gentle introduction to conformal prediction and distribution-free uncertainty quantification",
            "venue": "arXiv preprint arXiv:2107.07511,",
            "year": 2021
        },
        {
            "authors": [
                "Charles Blundell",
                "Julien Cornebise",
                "Koray Kavukcuoglu",
                "Daan Wierstra"
            ],
            "title": "Weight uncertainty in neural network",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Ming-Fang Chang",
                "John W Lambert",
                "Patsorn Sangkloy",
                "Jagjeet Singh",
                "Slawomir Bak",
                "Andrew Hartnett",
                "De Wang",
                "Peter Carr",
                "Simon Lucey",
                "Deva Ramanan",
                "James Hays"
            ],
            "title": "Argoverse: 3d tracking and forecasting with rich maps",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Tianqi Chen",
                "Emily Fox",
                "Carlos Guestrin"
            ],
            "title": "Stochastic gradient hamiltonian monte carlo",
            "venue": "In International conference on machine learning,",
            "year": 2014
        },
        {
            "authors": [
                "Alexandre Drouin",
                "\u00c9tienne Marcotte",
                "Nicolas Chapados"
            ],
            "title": "Tactis: Transformer-attentional copulas for time series",
            "venue": "arXiv preprint arXiv:2202.03528,",
            "year": 2022
        },
        {
            "authors": [
                "Martin Eklund",
                "Ulf Norinder",
                "Scott Boyer",
                "Lars Carlsson"
            ],
            "title": "The application of conformal prediction to the drug discovery process",
            "venue": "Annals of Mathematics and Artificial Intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Yarin Gal",
                "Zoubin Ghahramani"
            ],
            "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
            "venue": "In international conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "Yarin Gal",
                "Zoubin Ghahramani"
            ],
            "title": "A theoretically grounded application of dropout in recurrent neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Jan Gasthaus",
                "Konstantinos Benidis",
                "Yuyang Wang",
                "Syama Sundar Rangapuram",
                "David Salinas",
                "Valentin Flunkert",
                "Tim Januschowski"
            ],
            "title": "Probabilistic forecasting with spline quantile function RNNs",
            "venue": "In AISTATS 22,",
            "year": 2019
        },
        {
            "authors": [
                "Isaac Gibbs",
                "Emmanuel Candes"
            ],
            "title": "Adaptive conformal inference under distribution shift",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Graves"
            ],
            "title": "Practical variational inference for neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2011
        },
        {
            "authors": [
                "Chancellor Johnstone",
                "Bruce Cox"
            ],
            "title": "Conformal uncertainty sets for robust optimization",
            "venue": "In Conformal and Probabilistic Prediction and Applications,",
            "year": 2021
        },
        {
            "authors": [
                "Kelvin Kan",
                "Fran\u00e7ois-Xavier Aubet",
                "Tim Januschowski",
                "Youngsuk Park",
                "Konstantinos Benidis",
                "Lars Ruthotto",
                "Jan Gasthaus"
            ],
            "title": "Multivariate quantile function forecaster",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "Abbas Khosravi",
                "Saeid Nahavandi",
                "Doug Creighton",
                "Amir F Atiya"
            ],
            "title": "Comprehensive review of neural network-based prediction intervals and new advances",
            "venue": "IEEE Transactions on neural networks,",
            "year": 2011
        },
        {
            "authors": [
                "Byol Kim",
                "Chen Xu",
                "Rina Barber"
            ],
            "title": "Predictive inference is free with the jackknife+-after-bootstrap",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Durk P Kingma",
                "Tim Salimans",
                "Max Welling"
            ],
            "title": "Variational dropout and the local reparameterization trick",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Thomas Kipf",
                "Ethan Fetaya",
                "Kuan-Chieh Wang",
                "Max Welling",
                "Richard Zemel"
            ],
            "title": "Neural relational inference for interacting systems",
            "venue": "arXiv preprint arXiv:1802.04687,",
            "year": 2018
        },
        {
            "authors": [
                "Danijel Kivaranovic",
                "Kory D Johnson",
                "Hannes Leeb"
            ],
            "title": "Adaptive, distribution-free prediction intervals for deep networks",
            "venue": "In AISTATS,",
            "year": 2020
        },
        {
            "authors": [
                "Balaji Lakshminarayanan",
                "Alexander Pritzel",
                "Charles Blundell"
            ],
            "title": "Simple and scalable predictive uncertainty estimation using deep ensembles",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jing Lei",
                "Larry Wasserman"
            ],
            "title": "Distribution free prediction bands",
            "venue": "arXiv preprint arXiv:1203.5422,",
            "year": 2012
        },
        {
            "authors": [
                "Jing Lei",
                "Max G\u2019Sell",
                "Alessandro Rinaldo",
                "Ryan J Tibshirani",
                "Larry Wasserman"
            ],
            "title": "Distribution-free predictive inference for regression",
            "venue": "Journal of the American Statistical Association,",
            "year": 2018
        },
        {
            "authors": [
                "Ming Liang",
                "Bin Yang",
                "Rui Hu",
                "Yun Chen",
                "Renjie Liao",
                "Song Feng",
                "Raquel Urtasun"
            ],
            "title": "Learning lane graph representations for motion forecasting",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Christos Louizos",
                "Max Welling"
            ],
            "title": "Multiplicative normalizing flows for variational bayesian neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Rachel Luo",
                "Shengjia Zhao",
                "Jonathan Kuck",
                "Boris Ivanovic",
                "Silvio Savarese",
                "Edward Schmerling",
                "Marco Pavone"
            ],
            "title": "Sample-efficient safety assurances using conformal prediction",
            "venue": "arXiv preprint arXiv:2109.14082,",
            "year": 2021
        },
        {
            "authors": [
                "Soundouss Messoudi",
                "S\u00e9bastien Destercke",
                "Sylvain Rousseau"
            ],
            "title": "Copula-based conformal prediction for multi-target regression",
            "venue": "Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Soundouss Messoudi",
                "S\u00e9bastien Destercke",
                "Sylvain Rousseau"
            ],
            "title": "Ellipsoidal conformal inference for multi-target regression",
            "venue": "In Conformal and Probabilistic Prediction with Applications,",
            "year": 2022
        },
        {
            "authors": [
                "Radford M Neal"
            ],
            "title": "Bayesian learning for neural networks, volume 118",
            "venue": "Springer Science & Business Media,",
            "year": 2012
        },
        {
            "authors": [
                "Youngsuk Park",
                "Danielle Maddix",
                "Fran\u00e7ois-Xavier Aubet",
                "Kelvin Kan",
                "Jan Gasthaus",
                "Yuyang Wang"
            ],
            "title": "Learning quantile functions without quantile crossing for distribution-free time series forecasting",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "Yaniv Romano",
                "Evan Patterson",
                "Emmanuel Candes"
            ],
            "title": "Conformalized quantile regression",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Ludger Ruschendorf"
            ],
            "title": "Asymptotic distributions of multivariate rank order statistics",
            "venue": "The Annals of Statistics,",
            "year": 1976
        },
        {
            "authors": [
                "Atsushi Sakai",
                "Daniel Ingram",
                "Joseph Dinius",
                "Karan Chawla",
                "Antonin Raffin",
                "Alexis Paques"
            ],
            "title": "Pythonrobotics: a python code collection of robotics",
            "venue": "algorithms. CoRR,",
            "year": 2018
        },
        {
            "authors": [
                "David Salinas",
                "Michael Bohlke-Schneider",
                "Laurent Callot",
                "Roberto Medico",
                "Jan Gasthaus"
            ],
            "title": "Highdimensional multivariate forecasting with low-rank gaussian copula processes",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Martim Sousa",
                "Ana Maria Tom\u00e9",
                "Jos\u00e9 Moreira"
            ],
            "title": "A general framework for multi-step ahead adaptive conformal heteroscedastic time series forecasting",
            "venue": "arXiv preprint arXiv:2207.14219,",
            "year": 2022
        },
        {
            "authors": [
                "Kamil\u0117 Stankevi\u010di\u016bt\u0117",
                "Ahmed Alaa",
                "Mihaela van der Schaar"
            ],
            "title": "Conformal time-series forecasting",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Natasa Tagasovska",
                "David Lopez-Paz"
            ],
            "title": "Single-model uncertainties for deep learning",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Ichiro Takeuchi",
                "Quoc Le",
                "Timothy Sears",
                "Alexander Smola"
            ],
            "title": "Nonparametric quantile estimation",
            "year": 2006
        },
        {
            "authors": [
                "Vladimir Vovk",
                "Alexander Gammerman",
                "Glenn Shafer"
            ],
            "title": "Algorithmic learning in a random world",
            "venue": "Springer Science & Business Media,",
            "year": 2005
        },
        {
            "authors": [
                "Vladimir Vovk",
                "Jieli Shen",
                "Valery Manokhin",
                "Min-ge Xie"
            ],
            "title": "Nonparametric predictive distributions based on conformal prediction",
            "venue": "In Conformal and probabilistic prediction and applications,",
            "year": 2017
        },
        {
            "authors": [
                "Bin Wang",
                "Jie Lu",
                "Zheng Yan",
                "Huaishao Luo",
                "Tianrui Li",
                "Yu Zheng",
                "Guangquan Zhang"
            ],
            "title": "Deep uncertainty quantification: A machine learning approach for weather forecasting",
            "venue": "In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
            "year": 2019
        },
        {
            "authors": [
                "Max Welling",
                "Yee W Teh"
            ],
            "title": "Bayesian learning via stochastic gradient langevin dynamics",
            "venue": "In ICML, pp",
            "year": 2011
        },
        {
            "authors": [
                "Dongxia Wu",
                "Liyao Gao",
                "Matteo Chinazzi",
                "Xinyue Xiong",
                "Alessandro Vespignani",
                "Yi-An Ma",
                "Rose Yu"
            ],
            "title": "Quantifying uncertainty in deep spatiotemporal forecasting",
            "venue": "In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining,",
            "year": 2021
        },
        {
            "authors": [
                "Chen Xu",
                "Yao Xie"
            ],
            "title": "Conformal prediction interval for dynamic time-series",
            "venue": "In International Conference on Machine Learning. PMLR,",
            "year": 2021
        },
        {
            "authors": [
                "Bianca Zadrozny",
                "Charles Elkan"
            ],
            "title": "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers",
            "venue": "In Icml,",
            "year": 2001
        },
        {
            "authors": [
                "Margaux Zaffran",
                "Olivier F\u00e9ron",
                "Yannig Goude",
                "Julie Josse",
                "Aymeric Dieuleveut"
            ],
            "title": "Adaptive conformal predictions for time series",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep learning models are becoming widely used in high-risk settings such as healthcare and transportation. In these settings, it is important that a model produces calibrated uncertainty to reflect its own confidence. Confidence regions are a common approach to quantify prediction uncertainty (Khosravi et al., 2011). A (1 \u2212 \u03b1)-confidence region \u03931\u2212\u03b1 for a random variable y is valid if it contains y\u2019s true value with high probability: P[y \u2208 \u03931\u2212\u03b1] \u2265 1 \u2212 \u03b1. Note that one can make the confidence region infinitely large to satisfy validity. But for the confidence region to be useful, we also want to minimize its area while remaining valid; this is known as efficiency of the region.\nConformal prediction (CP) is a powerful method to produces confidence regions with finite-sample guarantees of validity (Vovk et al., 2005; Lei et al., 2018). Furthermore, it makes no assumptions about the prediction model or the underlying data distribution. CP\u2019s generality, simplicity, and statistical guarantees have made it popular for many real-world applications including time series prediction (Xu & Xie, 2021), drug discovery (Eklund et al., 2015) and safe robotics (Luo et al., 2021).\nThis paper considers the setting of multi-step time series prediction in the presence of a training dataset of similar time series. We will motivate with an example application of car trajectory prediction, illustrated in figure 1. The task is to predicting future trajectory y1, . . . ,yk given past trajectory x1, . . . ,xt. These time steps are temporally dependent. On the other hand, we also have a dataset of past trajectories D, where the trajectories are independent of each other.\nThere are many real-world task that presents the same challenges as our motivating example, such as EEG forecasting (each patient is independent), short-term weather forecasting (local meteorology history is independent), etc. They require predicting multiple time steps into the future, so it is desired to have a \u201ccone of uncertainty\u201d that covers the entire course of the forecasts. Existing CP methods for time series data either only provide coverage guarantee for individual time steps (Gibbs & Candes, 2021; Xu & Xie, 2021) or produce confidence regions often too inefficient to be useful, especially in long horizons or multivariate settings (Stankevic\u030ciu\u0304te\u0307 et al., 2021).\nIn this paper, we present a practical and effective conformal prediction algorithm for multi-step time series forecasting. We introduce CopulaCPTS, a Copula-based Conformal Prediction algorithm for multi-step Time Series forecasting. We improve efficiency by utilizing copulas to model the dependency between forecasted time steps. A copula is a multivariate cumulative distribution function that models the dependence between multiple random variables. By using copulas to model the uncertainty jointly over time, we can shrink the confidence regions significantly while maintaining validity. Copulas have been used for conformal prediction (Messoudi et al., 2021), but they focus on multiple target prediction in non-temporal settings and did not provide a validity proof.\nIn summary, our contributions are:\n\u2022 We introduce CopulaCPTS, a general uncertainty quantification algorithm that can be applied to any multivariate multi-step forecaster, with statistical guarantees of validity.\n\u2022 CopulaCPTS produces significantly sharper and more calibrated uncertainty estimates than state-of-the-art baselines on two synthetic and two real-world benchmark datasets.\n\u2022 CopulaCPTS can be extended to obtain valid confidence intervals for time series forecasts of varying lengths."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Uncertainty Quantification for Deep Time-Series Forecasting. The two major paradigms of Uncertainty Quantification (UQ) methods for deep neural networks are Bayesian and Frequentist. Bayesian approaches estimate a distribution over the model parameters given data, and then marginalize these parameters to form output distributions via Markov Chain Monte Carlo (MCMC) sampling (Welling & Teh, 2011; Neal, 2012; Chen et al., 2014) or variational inference (VI) (Graves, 2011; Kingma et al., 2015; Blundell et al., 2015; Louizos & Welling, 2017). Wang et al. (2019); Wu et al. (2021) propose Bayesian Neural Networks (BNN) for UQ of spatiotemporal forecasts. In practice, Bayesian UQ has two major drawbacks: (1) it requires significant modification to the training process of the models, and (2) BNNs can be computationally expensive and difficult to optimize, especially for larger networks (Lakshminarayanan et al., 2017; Zadrozny & Elkan, 2001). Therefore, UQ for deep neural network time series forecasts often adopts approximate Bayesian inference such as MC-dropout (Gal & Ghahramani, 2016b; Gal et al., 2017).\nFrequentist UQ methods emphasize robustness against variations in the data. These approaches either rely on resampling the data or learning an interval bound to encompass the dataset. For time series forecasting UQ, frequentist approaches include ensemble methods such as bootstrap (Efron & Hastie, 2016; Alaa & Van Der Schaar, 2020) and jackknife methods (Kim et al., 2020; Alaa & Van Der Schaar, 2020); interval prediction methods include interval regression through proper scoring rules (Kivaranovic et al., 2020; Wu et al., 2021), and quantile regression (Takeuchi et al., 2006), with many recent advances for time series UQ (Tagasovska & Lopez-Paz, 2019; Gasthaus et al., 2019; Park et al., 2022; Kan et al., 2022). Many of the frequentist methods produce asymptotically valid confidence regions and can be categorized as distribution-free UQ techniques as they are (1) agnostic to the underlying model and (2) agnostic to data distribution.\nConformal Prediction. Conformal prediction (CP) is an important member of distribution-free UQ methods; we refer readers to Angelopoulos & Bates (2021) for a comprehensive introduction and survey. CP has become popular because of its simplicity, theoretical soundness, and low computational cost. A key difference between CP and other UQ methods is that under the exchangeability assumption, conformal methods guarantee validity in finite samples (Vovk et al., 2005).\nMost relevant to our work is the recent endeavor in generalizing CP to time-series forecasting. According to Stankevic\u030ciu\u0304te\u0307 et al. (2021) there are two settings: data generated from (1) one single\ntime series or (2) multiple independent time series. For the first setting, ACI (Gibbs & Candes, 2021) and EnbPI (Xu & Xie, 2021) developed CP algorithms that relax the exchangeability assumption while maintaining asymptotic validity via online learning (former) and ensembling (later); Zaffran et al. (2022) further improves online adaptation. Sousa et al. (2022) combines EnbPI with conformal quantile regression (Romano et al., 2019) to model heteroscedastic time series. However, because these algorithms operate on one single time series, the validity guarantees are on average over the predicted time steps and are asymptotic, rather than covering the full horizon as in our setting.\nCopulaCPTS is developed for the setting where data is generated from many independent time series. Stankevic\u030ciu\u0304te\u0307 et al. (2021) shares the same setting but provides only a univariate solution. We show that their method of applying Bonferroni correction produces inefficient confidence regions, especially for multidimensional data or long prediction horizons. Messoudi et al. (2021) uses a copula function for multi-target CP for non-temporal data, creating box-like regions to account for the correlations between the labels. However, they do not provide a theoretical proof and empirical results have shown that are often invalid. This paper builds upon these works and presents a novel two-step algorithm with guaranteed multivariate multi-step coverage and efficient confidence regions."
        },
        {
            "heading": "3 BACKGROUND",
            "text": ""
        },
        {
            "heading": "3.1 INDUCTIVE CONFORMAL PREDICTION (ICP)",
            "text": "Let D = {zi = (xi, yi)}ni=1 be a dataset with input xi \u2208 X and output yi \u2208 Y such that each data point zi \u2208 Z := X \u00d7 Y is drawn i.i.d. from an unknown distribution Z . We will briefly present the algorithm and theoretical results for conformal prediction, and refer readers to Angelopoulos & Bates (2021) for a thorough introduction. The goal of conformal prediction is to produce a valid confidence region (Def. 3.1) for any underlying prediction model. Definition 3.1 (Validity). Given a new data point (x, y) and a desired confidence 1\u2212 \u03b1 \u2208 (0, 1), the confidence region \u03931\u2212\u03b1(x) is a subset of Y containing probable outputs y\u0303 \u2208 Y given x. The region \u03931\u2212\u03b1 is valid if P(y \u2208 \u03931\u2212\u03b1(x)) \u2265 1\u2212 \u03b1 (1) Conformal prediction splits the dataset into a proper training set Dtrain and a calibration set Dcal. A prediction model f\u0302 : X \u2192 Y is trained onDtrain. We use a nonconformity score A : Z |Dtrain|\u00d7Z \u2192 R to quantify how well a data sample from calibration conforms to the training dataset. Typically, we choose a metric of disagreement between the prediction and the true label as the non-conformity score, such as the Euclidean distance:\nA(Dtrain, (x, y)) e.g. = d(y, f\u0302(x)) e.g. = \u2225y \u2212 f\u0302(x)\u22252 (2)\nFor conciseness, we write A(Dtrain, (xi, yi)) as A(zi) in rest of the paper. Let S = {A(zi)}zi\u2208Dcal denote the set of nonconformity scores of all samples in the calibration set Dcal. We can define a quantile function for the nonconformity scores S as:\nQ(1\u2212 \u03b1,S) := inf{s\u2217 : ( 1 |S| \u2211 si\u2208S 1si\u2264s\u2217) \u2265 1\u2212 \u03b1}. (3)\nConformal prediction is guaranteed to produce valid confidence regions (Vovk et al., 2005), under the exchangeablility assumption defined as follows, Definition 3.2 (Exchangeability). In a dataset {zi}ni=1 of size n, any of its n! permutations are equally probable.\nThe procedure introduced above is known as inductive conformal prediction, as it splits the dataset into training and calibration sets to reduce computation load (Vovk et al., 2005; Lei & Wasserman, 2012). Our method is based on inductive CP, but can also be easily adapted for other CP variants."
        },
        {
            "heading": "3.2 COPULA AND ITS PROPERTIES",
            "text": "Copula is a concept from statistics that describes the dependency structure in a multivariate distribution. It has also been used in generative models for multivariate time series (Salinas et al., 2019; Drouin et al., 2022). We can use Copulas to capture the joint distribution for multiple future time steps. We briefly introduce its notations and concepts.\nDefinition 3.3 (Copula). Given a random vector (X1, \u00b7 \u00b7 \u00b7Xk), define the marginal cumulative density function (CDF) for each variable Xh, h \u2208 {1, . . . , k} as\nFh(x) = P[Xh \u2264 x]\nThe copula of (X1, \u00b7 \u00b7 \u00b7Xk) is the joint CDF of (F1(X1), \u00b7 \u00b7 \u00b7 , Fk(Xk)), written as\nC(u1, \u00b7 \u00b7 \u00b7 , uk) = P [F1(X1) \u2264 u1, \u00b7 \u00b7 \u00b7 , Ft(Xk) \u2264 uk]\nIn other words, the Copula function captures the dependency structure between the variable Xs; we can view an k dimensional copula C : [0, 1]k \u2192 [0, 1] as a CDF with uniform marginals, as illustrated in Figure 2. A fundamental result in the theory of copula is Sklar\u2019s theorem.\nTheorem 3.4 (Sklar\u2019s theorem). Given a joint CDF as F (X1, \u00b7 \u00b7 \u00b7 , Xk) and the marginals F1(x), . . . , Fk(x), there exists a copula such that\nF (x1, \u00b7 \u00b7 \u00b7 , xk) = C(F1(x1), \u00b7 \u00b7 \u00b7 , Fk(xk))\nfor all xj \u2208 (\u2212\u221e,\u221e), j \u2208 {1, . . . , k}.\nSklar\u2019s theorem states that for all multivariate distribution functions, there exists a copula function such that the distribution can be expressed using the copula and multiple univariate marginal distributions. When all the Xks are independent, the copula function is known as the product copula: C(u1, \u00b7 \u00b7 \u00b7 , uk) = \u03a0ki=1ui."
        },
        {
            "heading": "4 COPULA CONFORMAL PREDICTION FOR TIME SERIES (COPULACPTS)",
            "text": "UQ methods are evaluated on two properties: validity and efficiency. A model is valid when the predicted confidence is greater than or equal to the probability of events falling into the predicted range (Definition 3.1). The term calibration describes the case of equality in the validity condition. Efficiency, on the other hand, refers to the size of the confidence region. In practice, we want the measure of the confidence region (e.g. its area or length) to be as small as possible, given that the validity condition holds. CopulaCPTS improves the efficiency of confidence regions by modeling the dependency of the time steps using a copula function.\nDenote the time series dataset of size n as D = {zi = (xi1:t, yi1:k)}ni=1, where x1:t \u2208 Rt\u00d7d is t input steps, and y1:k \u2208 Rk\u00d7d is k prediction steps, both with dimension d at each step. Each data point zi is sampled i.i.d. from an unknown distribution Z . In the multi-step forecasting setting, given a confidence level 1\u2212\u03b1, the algorithm produces k confidence regions for a test input xn+11:t , denoted as [\u03931\u2212\u03b11 , . . . ,\u0393 1\u2212\u03b1 k ]. We say the confidence regions are valid if all time steps in the forecast is covered:\nP[ \u2200j \u2208 {1, . . . , k}, yj \u2208 \u03931\u2212\u03b1j ] \u2265 1\u2212 \u03b1, \u2200j \u2208 {1, . . . , k}. (4)\nIn the following sections, we introduce CopulaCPTS, a conformal prediction algorithm that is both valid and efficient for multivariate multi-step time series forecasts."
        },
        {
            "heading": "4.1 ALGORITHM DETAILS",
            "text": "The key insight of our algorithm is that we can model the joint probability of uncertainty for multiple predicted time steps with a copula, hence better capturing the confidence regions. We divide the calibration set Dcal into two subsets: Dcal\u22121, which we use to estimate a Cumulative Distribution Function (CDF) on the nonconformity score of each time step, and Dcal\u22122, to calibrate the copula. The two calibration sets allows us to prove validity for both components of our algorithm. At the cost of using a subset of the data to calibrate a copula, our approach produces provably more efficient confidence regions compared to worst-case corrections such as union bounding in Stankevic\u030ciu\u0304te\u0307 et al. (2021) which is a lower bound for copulas (Appendix B.1), and more valid regions than Messoudi et al. (2021) (table 1).\nNonconformity of Multivariate Forecasts. If the time series is multivariate, we have each target time step yj \u2208 Rd. Given z = (x, y) \u223c Z , let the nonconformity score be the L-2 distance sij = A(z i)j e.g. = \u2225yij \u2212 f\u0302(xi)j\u2225 for each timestep j = 1, . . . , k, where f\u0302(x) is a forecasting model trained on Dtrain. The confidence region \u03931\u2212\u03b1(x) therefore is a d-dimensional ball. We chose this metric for simplicity, but one can choose other metrics such as Mahalanobis (Johnstone & Cox, 2021) or L-1 (Messoudi et al., 2021) distance based on domain needs, and our algorithm will remain valid.\nFor brevity, we will use S1 = {si}zi\u2208Dcal\u22121 to denote the set of nonconformity scores of data in Dcal\u22121 and S2 = {si}zi\u2208Dcal\u22122 the set of nonconformity scores of data in Dcal\u22122. Subscript j will be used to to index the set of specific time steps of the scores: S1j = {sij}zi\u2208Dcal\u22121 , S2j = {sij}zi\u2208Dcal\u22122 .\nCalibrating CDF on Dcal\u22121. We use Dcal\u22121 to build conformal predictive distributions for each time step\u2019s anomaly scores, which provides desirable validity properties (Vovk et al., 2017). The conformal cumulative distribution function is constructed as follows. 1\nF\u0302j(sj) := 1\n|S1j |+ 1\n( \u03c4 + \u2211 si\u2208S1j 1sij<sj ) , where \u03c4 \u223c (0, 1), for j \u2208 {1, . . . , k} (5)\nCopula Calibration on Dcal\u22122. Next, for every data point in Dcal\u22122, we evaluate the cumulative probability of its anomaly scores with the estimated conformal predictive distributions:\nU = {ui}i\u2208Dcal\u22122 , ui = (ui1, . . . , uik) = ( F\u03021(s i 1), . . . , F\u0302k(s i k) )\n(6)\nWe adopt the empirical copula (Ruschendorf, 1976) for modeling and proof in this work. The empirical copula is a non-parametric method of estimating marginals directly from observation, and hence does not introduce any bias. For the joint distribution of k time steps, we construct Cempirical : [0, 1]\nk \u2192 [0, 1] as eqn 7. Here boldface \u221e is a k-dimensional vector with each \u221ej =\u221e for j = 1, . . . , k.\nCempirical(u) = 1 |Dcal\u22122|+ 1 \u2211\ni\u2208Dcal\u22122\u222a{\u221e}\nk\u220f j=1 1uij<uj (7)\nTo fulfill the full-horizon validity condition of Eqn 4, we only need to find appropriate u\u2217 such that Cempirical(u \u2217) \u2265 1\u2212 \u03b1.\nargmin u\u2217 k\u2211 j=1 u\u2217j s.t. Cempirical(u \u2217) \u2265 1\u2212 \u03b1 (8)\nNote that the u\u2217 is not and does not have to be unique; any solution that satisfy the constraint in Eq. 8 will guarantee multi-step validity (Appendix A). The minimization helps with efficiency, i.e. the sharpness of the confidence regions. We use a gradient descent algorithm for the optimization in\n1Because of the random component, equation 5 is a \u201cthick\u201d CDF of thickness 1|S1|+1 , which becomes inconsequential when the calibration set is large. See Vovk et al. (2017) for theoretical justifications. In implementation, we treat \u03c4 = 1 for simplicity and better coverage.\nimplementation (see Appendix B.2 for details, and Appendix C.6 for a study of its effectiveness). Lastly, We obtain (s\u22171, . . . , s \u2217 k) by F\u0302 \u22121 j (u \u2217 j ) and construct the confidence region for each time step j \u2208 {1, . . . , k} as the set of all yj \u2208 Rd such that the nonconformity score is less than s\u2217j . Algorithm 1 summarizes the CoupulaCPTS procedure.\nThe full proof of CopulaCPTS\u2019s validity (theorem 4.1) can be found in Appendix A. Intuitively, CopulaCPTS performs conformal prediction twice: first calibrating the nonconformity scores of each time step with Dcal\u22121, and then calibrating the copula with Dcal\u22122. Theorem 4.1 (Validity of CopulaCPTS). CopulaCPTS (algorithm 1) produces valid confidence regions for the entire forecast horizon. i.e.\nP[ \u2200j \u2208 {1, . . . , k}, yj \u2208 \u03931\u2212\u03b1j ] \u2265 1\u2212 \u03b1, \u2200j \u2208 {1, . . . , k}.\nAlgorithm 1: Copula Conformal Time Series Prediction Input: Dataset D, test inputs Dtest, target significant level 1\u2212 \u03b1. Output: \u03931\u2212\u03b11 , . . . ,\u0393 1\u2212\u03b1 k for each test input.\n1\n2 // Training 3 Randomly split dataset D into Dtrain and Dcal. 4 Train k-step forecasting model f\u0302 on training set Dtrain. 5 // Calibration 6 Randomly split Dcal into Dcal\u22121 and Dcal\u22122. 7 for (xi1:t, yi1:k) \u2208 Dcal\u22121 \u222a Dcal\u22122 do 8 y\u0302i1:k \u2190 f\u0302(xi1:t) 9 sij \u2190 \u2225yij \u2212 y\u0302ij\u2225 for j = 1, . . . , k\n10 end for 11 F\u03021, . . . , F\u0302k \u2190 Eq. (5) on Dcal\u22121 12 Cempirical(\u00b7)\u2190 Eq. (7) on Dcal\u22122 13 u\u2217 \u2190 Eq. (8) 14 s\u2217j = F\u0302 \u22121 j (u \u2217 j ) for j = 1, . . . , k 15 // Prediction 16 for xi1:t \u2208 Dtest do 17 y\u0302i1:k \u2190 f\u0302(xi1:t) 18 \u03931\u2212\u03b1j \u2190 {y : \u2225y \u2212 y\u0302ih\u2225 < s\u2217j} for j = 1, . . . , k 19 yield \u03931\u2212\u03b11 , . . . ,\u0393 1\u2212\u03b1 k 20 end for"
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we show that CopulaCPTS produces more calibrated and efficient confidence regions compared to existing methods on two synthetic datasets and two real-world datasets. We demonstrate that CopulaCPTS\u2019s advantage is more evident over longer prediction horizons in Section 5.2. We also show its effectiveness in the autoregressive prediction setting in Section 5.2.\nAll experiments in this paper split the calibration set in half into equal-sized Dcal\u22121 and Dcal\u22122. Although the split does not significantly impact the result when calibration data is ample, performance deteriorates when there are not enough data in either one of the subsets.\nBaselines. We compare our model with three representative works in different paradigms of uncertainty quantification for neural network time series prediction: the Bayesian-motivated Monte Carlo dropout RNN (MC-dropout) by Gal & Ghahramani (2016a), the frequentist blockwise jackknife RNN (BJRNN) by Alaa & Van Der Schaar (2020), a conformal forecasting RNN (CFRNN) by Stankevic\u030ciu\u0304te\u0307 et al. (2021), and the multi-target copula algorithm that does not have the two step calibration (Copula) by Messoudi et al. (2021). We use the same underlying prediction model for post-hoc uncertainty quantification methods BJRNN, CF-RNN, and CopulaCPTS. The\nMC-dropout RNN is of the same architecture but is trained separately, as it requires an extra dropout step during training and inference.\nMetrics. We evaluate calibration and efficiency for each method. For calibration, we report the empirical coverage on the test set. Coverage should be as close to the desired confidence level 1\u2212 \u03b1 as possible. Coverage is calculated as:\nCoverage1\u2212\u03b1 = Ex,y\u223cZP[y \u2208 \u03931\u2212\u03b1(x)] \u2248 1|Dtest| \u2211 xi,yi\u2208|Dtest| 1(y i \u2208 \u03931\u2212\u03b1(xi)).\nFor efficiency, we report the average area (2D) or volume (3D) of the confidence regions. The measure should be as small as possible while being valid (coverage maintains above-specified confidence level). The area or volume is calculated as:\nArea1\u2212\u03b1 = Ex\u223cX [\u2225\u03931\u2212\u03b1(x)\u2225] \u2248 1|Dtest| \u2211 xi\u2208|Dtest| \u2225\u0393 1\u2212\u03b1(xi)\u2225."
        },
        {
            "heading": "5.1 SYNTHETIC DATASETS",
            "text": "We first test the effectiveness of our models on two synthetic spatiotemporal datasets - interacting particle systems (Kipf et al., 2018), and drone trajectory following simulated with PythonRobotics (Sakai et al., 2018). For particle simulation, we predict yt+1:t+h where t = 35, h = 25 and yt \u2208 R2; for drone simulation t = 60, h = 10, and yt \u2208 R3. To add randomness to the tasks, we added Gaussian noise of \u03c3 = .01 and .05 to the dynamics of particle simulation and \u03c3 = .02 to drone dynamics. We generate 5000 samples for each dataset, and split the data by 45/45/10 for train, calibration, and test, respectively. For baselines that does not require calibration, the calibration split is used for training the model. Please see Appendix C.1 for forecaster model details.\nWe visualize the calibration and efficiency of the methods in Figure 3 for confidence levels 1\u2212\u03b1 = 0.5 to 0.95. We can see that Copula-RNN, the red lines, are more calibrated and efficient compared to other baseline methods, especially when the confidence level is high (90% and 95%). We can see that for harder tasks (particle \u03c3 = 0.05, and drone trajectory prediction), MC-Dropout is overconfident, whereas BJ-RNN and CF-RNN produce very large (hence inefficient) confidence regions. This behavior of CF-RNN is expected because they apply Bonferroni correction to account for joint prediction for multiple time steps, which is an upper bound of copula functions. Numerical results for confidence level 90% are presented in Table 1. A qualitative comparison of confidence regions for drone simulation can be found in Figure 9 in Appendix C.5."
        },
        {
            "heading": "5.2 REAL WORLD DATASETS",
            "text": "COVID-19. We replicate the experiment setting of Stankevic\u030ciu\u0304te\u0307 et al. (2021) and predict new daily cases of COVID-19 in regions of the UK. The models take 100 days of data as input and forecast 50 days into the future. We used 200 time series for training, 100 for calibration, and 80 for testing.\nVehicle trajectory prediction. The Argoverse autonomous vehicle motion forecasting dataset (Chang et al., 2019) is a widely used vehicle trajectory prediction benchmark. The task is to predict 3 second trajectories based on all vehicle motion in the past 2 seconds sampled at 10Hz. Because trajectory prediction is a challenging task, we utilize a state-of-the-art prediction algorithm LaneGCN (Liang et al., 2020) as the underlying model for both CF-RNN and Copula-RNN (details in Appendix C.1). Flexibility of underlying forecasting model is an advantage of post-hoc UQ methods such as conformal prediction. For model-dependent baselines MC-dropout and BJRNN, we have to train an RNN forecasting model from scratch for each method, which induces additional computational cost.\nCopulaCPTS is both more calibrated and efficient compared to baseline models for real-world datasets (Table 1). The Covid-19 dataset demonstrates a potential failure case for our model when calibration data are scarce. Because there are only 100 calibration data, CDF and copula estimations are more stochastic depending on the dataset split, resulting in 1 case of invalidity among 3 experiment trials. Even so, CopulaCPTS shows strong performance on average by remaining valid and reducing the confidence width by 33%. For the trajectory prediction task, learning the copula results in a 40% sharper confidence region while still remaining valid for the 90% confidence interval. We visualize two samples from each dataset in Figure 3.The importance of efficiency in these scenarios is clear - the confidence regions need to be narrow enough for them to be useful for decision making. Given the same underlying prediction model, we can see that CopulaCPTS produces a much more efficient region while still remaining valid.\nComparison of models at different horizon lengths CopulaCPTS is an algorithm designed to produce calibrated and efficient confidence regions for multi-step time series. When the prediction horizon is long, CopulaCPTS\u2019s advantage is more pronounced. Figure 5 shows the performance comparison over increasing time horizons on the particle dataset. See Table 3 of Appendix C for numerical results. CopulaCPTS achieves a 30% decrease in area at 20 time steps compared to CF-RNN, the best performing baseline; the decrease is above 50% at 25 time steps. This experiment shows significant improvement of using copula to model the joint distribution of future time steps.\nCopulaCPTS for Autoregressive prediction The autoregressive extension of CopulaCPTS is illustrated in detail in Appendix B.3. To provide preliminary evidence of effectiveness, we present test results on the COVID-19 dataset. We train an RNN model with k = 7 and use it to autoregressively forecast the next 14 steps. Table 2 compares the performance of re-estimating the copula for each 7-step forecasts versus using a fixed copula calibrated using the first 7 steps. We also compare the model to a 14-step joint forecaster using CopulaCPTS. It is evident that daily cases of the pandemic is a non-stationary time series, where re-estimating the copula is necessary for validity."
        },
        {
            "heading": "6 CONCLUSION AND DISCUSSION",
            "text": "In this paper, we present CopulaCPTS, a conformal prediction algorithm for multi-step time series prediction. CopulaCPTS significantly improves calibration and efficiency of multi-step conformal confidence intervals by incorporating copulas to model the joint distribution of the uncertainty at each time step. We prove that CopulaCPTS has a finite sample validity guarantee over the entire prediction horizon. Our experiments show that CopulaCPTS produces confidence regions that are (1) valid, and (2) more efficient than state-of-the-art UQ methods on all 4 benchmark datasets, and over varying prediction horizons. The improvement is especially pronounced when the data dimension is high or the prediction horizon is long, cases when other methods are prone to be highly inefficient. Hence, we argue that our method is a practical and effective way to produce useful uncertainty quantification for machine learning forecasting models.\nWe also discuss the limitation of our algorithm. As CopulaCPTS require two calibration steps, it is suitable only when there are abundant data for calibration. The validity proof relies on using the empirical copula, so it does not apply to other learnable copula classes. Future work includes (1) improving the autoregressive extension of CopulaCPTS, to achieve coverage over the whole horizon, and (2) developing online settings onf CopulaCPTS for decision making."
        },
        {
            "heading": "A PROOF OF THEOREM 4.1",
            "text": "Theorem A.1 (Validity of CopulaCPTS). The confidence region provided by CopulaCPTS (algorithm 1) is valid. i.e. P[ \u2200j \u2208 {1, . . . , k}, yt+j \u2208 \u03931\u2212\u03b1j ] \u2265 1\u2212 \u03b1.\nProof. Define notations to be the same as in Section 4. Let D = {zi = (xi, yi)}ni=1 be a dataset with input xi \u2208 Rt\u00d7d, a time series with length t, and output yi \u2208 Rk\u00d7d a time series with length k. Each data sample (of entire time series, not time step) zi = (xi, yi) is drawn i.i.d. from an unknown distribution Z . This means that any other sample drawn Z is exchangeable with D. from Dataset D is divided into training set Dtrain and two calibration sets Dcal\u22121 and Dcal\u22122.\nWe have nonconformity score function A with prediction model f\u0302 trained on Dtrain. For each data point zi = (xi, yi) \u2208 Dcal, we calculate the nonconformity score for each time step j, concatenating them into a vector si of dimension k.\nsij = A(z i)j e.g. = \u2225yij \u2212 f\u0302(xi)j\u2225, j = 1, . . . , k (9)\nLet S1 = {si}zi\u2208Dcal\u22121 be the set of nonconformity scores of data in Dcal\u22121 and S2 = {si}zi\u2208Dcal\u22122 the set of nonconformity scores of data in Dcal\u22122. Subscript j will be used to to index the set of specific time steps of the scores: S1j = {sij}zi\u2208Dcal\u22121 , S2j = {sij}zi\u2208Dcal\u22122 .\nCDF Estimation on Dcal\u22121. We use Dcal\u22121 to build conformal predictive distributions (CPD) (Vovk et al., 2017) for each time step\u2019s anomaly scores. The cumulative distribution function is constructed as:\nF\u0302j(sj) := 1 |S1j |+ 1 \u2211\nsi\u2208S1j\u222a{\u221e}\n1sij<sj , for j \u2208 {1, . . . , k} (10)\nLemma A.2 (Validity of CPD. Theorem 11 of Vovk et al. (2017) ). The distribution F\u0302j(\u00b7) is valid in the sense that for a nonconformity score s = A(z) of a data sample z \u223c Z , PZ [F\u0302j(sj) \u2264 1\u2212 \u03b1] = 1\u2212 \u03b1 for any 0 < \u03b1 < 1 .\nCopula Calibration on Dcal\u22122. Next, for every data point Dcal\u22122, we calculate U = {ui}i\u2208Dcal\u22122 , ui = (ui1, . . . , uik) = ( F\u03021(s i 1), . . . , F\u0302k(s i k) )\nEach ui can be seen as a multivariate nonconformity score for data sample zi. We will now illustrate that an empirical copula on U is a rank statistic, and hence we can apply the proof of conformal prediction to prove finite sample validity guarantee.\nDefinition A.3 (Vector partial order). Define a partial order for k-dimensional vectors \u2aaf.\nu \u2aaf v i.f.f. \u2200j \u2208 {1, . . . , k}, uj \u2264 vj (11)\ni.e. u \u2aaf v\u21d0\u21d2 k\u220f\nj=1\n1uj\u2264vj (12)\nNext, we define the empirical multivariate quantile function for U , a set of k-dimensional vectors.\nQ\u0302(1\u2212 \u03b1,U) = argmin u\u2217 k\u2211 j=1 u\u2217j s.t. ( 1 |U| \u2211 u\u2208U 1u\u2aafu\u2217 ) \u2265 1\u2212 \u03b1 (13)\nThe empirical copula formula in CopulaCPTS (eqn 7 in section 4.1) is the same as the expression inside the inf function of Q(1\u2212 \u03b1,U \u222a {\u221e}). Therefore, finding s\u22171, . . . , s\u2217k by Equation 8 implies:\nQ\u0302(1\u2212 \u03b1,U \u222a {\u221e}) = z\nThe rest of the proof follows that of Inductive Conformal Prediction (ICP) in Vovk et al. (2005).\nGiven a test data sample zn+1 = (xn+11:t , y n+1 1:k ) \u223c Z , we want to prove that the confidence regions \u03931\u2212\u03b11 , . . . ,\u0393 1\u2212\u03b1 k output by CopulaCPTS satisfies:\nP[yj \u2208 \u03931\u2212\u03b1j ] \u2265 1\u2212 \u03b1, \u2200j \u2208 {1, . . . , k}\nWe first calculate un+1j = F\u0302j(A(z n+1)j) for j \u2208 {1, . . . , k}\nLet u\u2217 = Q(1\u2212 \u03b1,U \u222a {\u221e}), u\u2217 \u2208 [0, 1]k. An important observation for the conformal prediction proof is that if u\u2217 \u2aaf un+1, then\nQ\u0302(1\u2212 \u03b1,U \u222a {\u221e}) = Q\u0302(1\u2212 \u03b1,U \u222a {un+1})\nthe quantile remains unchanged. This fact can be re-written as\nun+1 \u2aaf Q\u0302(1\u2212 \u03b1,U \u222a {\u221e})\u21d0\u21d2 un+1 \u2aaf Q\u0302(1\u2212 \u03b1,U \u222a {un+1})\nThe above describes the condition where un+1 is among the \u2308(1 \u2212 \u03b1)(n + 1)\u2309 smallest of U . By exchangability, the probability of un+1\u2019s rank among U is uniform. Therefore,\nP[un+1 \u2aaf Q\u0302(p,U \u222a {\u221e})] = \u2308(1\u2212 \u03b1)(|U|+ 1)\u2309 (|U|+ 1) \u2265 1\u2212 \u03b1\nHence we have\nP[un+1 \u2aaf Q\u0302(1\u2212 \u03b1,U \u222a {\u221e})] \u2265 1\u2212 \u03b1 (14)\nNote again that:\n\u2022 u\u2217 = Q\u0302(1\u2212 \u03b1,U \u222a {\u221e}) = (F\u03021(s\u22171), . . . , F\u0302t(s\u2217k))\n\u2022 un+1 = (F\u03021(sn+11 ), . . . , F\u0302t(s n+1 k ))\n\u2022 The uncertain regions are constructed as as (Algorithm 1, line 17):\n\u03931\u2212\u03b1j \u2190 {y : \u2225y \u2212 y\u0302 n+1 j \u2225 < s \u2217 j} (15)\nBy definition of \u2aaf, we have\nun+1 \u2aaf u\u2217 (16) (11)\u21d0\u21d2 \u2200j \u2208 {1, . . . , k}, un+1j \u2264 u \u2217 j (17)\nLemma A.2 =\u21d2 \u2200j \u2208 {1, . . . , k}, sn+1j \u2264 s \u2217 j (18)\n(15)\u21d0\u21d2 \u2200j \u2208 {1, . . . , k}, yj \u2208 \u03931\u2212\u03b1j (19)\nCombining eqn 14 and eqn 19, we have\nP[ \u2200j \u2208 {1, . . . , k}, yj \u2208 \u03931\u2212\u03b1j ] \u2265 P[u n+1 \u2aaf Q\u0302(1\u2212 \u03b1,U \u222a {\u221e})] \u2265 1\u2212 \u03b1 (20)"
        },
        {
            "heading": "B ADDITIONAL ALGORITHM DETAILS",
            "text": ""
        },
        {
            "heading": "B.1 UPPER AND LOWER BOUNDS FOR COPULAS",
            "text": "To provide a better understanding of the properties of Copulas, consider the Frechet-Hoeffding Bounds (Theorem B.1). In fact, the Frechet-Hoeffding upper- and lower- bounds are both copulas. The lower bound is percisely the Bonferroni correction used in Stankevic\u030ciu\u0304te\u0307 et al. (2021) - therefore by estimating the copula more precisely instead of using a lower bound, we have a guaranteed efficiency improvement for the confidence region. Theorem B.1 (The Frechet-Hoeffding Bounds). Consider a copula C(u1, . . . , uk). Then\nmax {1\u2212 k + k\u2211\ni=1\nui, 0} \u2264 C(u1, . . . , uk) \u2264 min {u1, . . . , uk}"
        },
        {
            "heading": "B.2 NUMERICAL OPTIMIZATION WITH SGD FOR SEARCH",
            "text": "We continue to use the notation defined in Appendix A. The inverse of the predictive distributions (Equation 10) can be written as follows, similar to the empirical quantile function (Equation 3).\nF\u0302\u22121j (p) := inf{sj : ( 1 |S1j |+ 1 \u2211\nsi\u2208S1j\u222a{\u221e}\n1sij<sj ) \u2265 p} (21)\nWe find the optimal s\u2217j in Equation 8 and Algorithm 1 by minimizing the following loss:\nL(s1, . . . , sk) = 1 |Dcal\u22122| \u2211\ni\u2208Dcal\u22122\nk\u220f j=1 1 [ uij < F\u0302 \u22121 j (sj) ] \u2212 (1\u2212 \u03b1)\nThe indicator function is implemented using a sigmoid function whose input is multiplied by a constant for differentiablility. A small amount of L2 regularization is added to the loss function to ensure the searched scores are as low as possible. We use the Adam optimizer and perform gradient descent for 500 steps to get the final result. The optimization process to find s\u2217 typically takes a few seconds on CPU. For each run of our experiments, the calibration and prediction steps of CopulaCPTS combined took less than 1 minute to run on an Apple M1 CPU. Please refer to the CP class in reference code for implementation details."
        },
        {
            "heading": "B.3 COPULACPTS IN AUTO-REGRESSIVE FORECASTING",
            "text": "Auto-regressive forecasting is a common framework in time series forecasting. So far, we have been looking at forecasts for a predetermined number of time steps k. One can use a fixed length model to forecast variable horizons k\u2032 autoregressively, taking the model output as part of the input. In the conformal prediction setting, we want not only to autoregressively use the point forecasts, but also to propagate the uncertainty measurement.\nIf the time series and uncertainty are stationary (for example: additive Gaussian noise), the copula remains the same for any sliding window of k steps, i.e. C(u1, . . . , uk) = C(u2, . . . , uk+1). Therefore, after finding (u\u22171, . . . , u \u2217 k) such that C(u \u2217 1, . . . , u \u2217 k) \u2265 1 \u2212 \u03b1, we can simply search for u\u2217k+1 such that C(u\u22172, . . . , u \u2217 k, u \u2217 k+1) \u2265 1 \u2212 \u03b1. The guarantee proven in Theorem 4.1 still holds for the new estimate. In this way, we can achieve the coverage guarantee over the entire autoregressive forecasting horizon.\nOn the other hand, if the time series is non-stationary, we need to fit copulas C1(u1, . . . , uk), C2(u2, . . . , uk+1), . . . , Ck\u2032\u2212k(uk\u2032\u2212k, . . . , uk\u2032), one for each autoregressive prediction, which requires a calibration set with \u2265 k\u2032 time steps. The k\u2032 step autoregressive problem is then reduced to k\u2032 \u2212 k multi-step forecasting problems that can be solved by CopulaCPTS. It follows that each of the autoregressive predictions is valid. Appendix B.4 provides an example scenario where re-estimating the copula is necessary for validity."
        },
        {
            "heading": "B.4 AUTOREGRESSIVE PREDICTION",
            "text": "In the context of this paper to forecast autoregressively is given input x1:t and a k step forecasting model f\u0302 , perform prediction\ny\u0302t+1:t+k = f\u0302(x1:t)\ny\u0302t+2:t+k+1 = f\u0302(x2:t, y\u0302t+1) \u00b7 \u00b7 \u00b7\nuntil all k\u2032 time steps are predicted.\nWe now provide a toy scenario to illustrate when re-estimating the copula is necessary and improves validity. Consider a time series of three time steps t0, t1, t2. The two scenarios are illustrated in Figure 6. In both scenarios the mean and variance of all timeseps are 0 and 1 respectively. In scenario (a), t0 = t1 and hence their covariance is 1. The copula estimated on t0 and t1 is C0:1(F (t0), F (t1)) = F (t0) = F (t1). This copula will significantly underestimate the confidence region of t2 where its covariance with t1 is \u22121. In fact the coverage of C0:1(F1(t1), F2(t2)) = 0.74. On the other hand, (b) illustrates a scenario where the copula for any 2 consecutive time series remain the same C0 = C1. In this case, applying C0 directly to forecast C1 achieves precisely 90% coverage."
        },
        {
            "heading": "C EXPERIMENT DETAILS AND ADDITIONAL RESULTS",
            "text": ""
        },
        {
            "heading": "C.1 UNDERLYING FORECASTING MODELS",
            "text": "Particle Dataset The underlying forecasting model for the particle experiments is an 1-layer LSTM network with embedding size = 24. The hidden state is then passed through a linear network to forecast the timesteps concurrently (output has dimension k \u00d7 dy). We train the model for 150 epochs with batch size 128. Hyperparameters of the network are selected through a model search by performance on a 5-fold cross validation split of the dataset. The architecture and hyperparameters are shared for all baselines and CopulaCPTS in Table 1.\nDrone For the drone trajectory forecasting task, we use he same LSTM forecasting network as the particle dataset, but with hidden size increased to 128. We train the model for 500 epochs with batch size 128. The same architecture and hyperparameters are shared for all baselines and CopulaCPTS reported in Table 1.\nCovid-19 The base forecasting model for Covid-19 dataset is the same as the synthetic datasets, with hidden size = 128 and were trained for 150 epochs with batch size 128. The same architecture and hyperparameters are shared for all baselines and CopulaCPTS reported in Table 1.\nArgoverse As highlighted in the main text, we utilize a state-of-the-art prediction algorithm LaneGCN (Liang et al., 2020) as the underlying forcaster model for CF-RNN and CopulaRNN. We refer the readers to their paper and code base for model details. The architecture of the RNN network used for MC-Dropout and BJRNN is an Encoder-Decoder network. Both the encode and decoder contains a LSTM layer with encoding size 8 and hidden size 16. We chose this architecture because the is part of the official Argoverse baselines (https://github.com/jagjeet-singh/argoverse-forecasting) and demonstrates competitive performance."
        },
        {
            "heading": "C.2 COVID-19 DATASET",
            "text": "The COVID-19 dataset is downloaded directly from the official UK government website https://coronavirus.data.gov.uk/details/download by selecting region for area type and newCasesByPublishDate for metric. There are in total 380 regions and over 500 days of data, depending when it is downloaded. We selected 150-day time series from the collection to construct our dataset."
        },
        {
            "heading": "C.3 CALIBRATION AND EFFICIENCY CHART FOR COVID-19",
            "text": "Figure 7 shows comparison of calibration and efficiency for the daily new COVID 19 cases forecasting.\nTo see if the daily fluctuation due to testing behaviour disrupts other method, we also ran the same experiment on weekly aggregated new cases forecast. We take 14 weeks of data as input and output forecasts for the next 6 weeks. The results are illustrated in Figure 8. The weekly forecasting scenario gives us similar insights as the daily forecasts."
        },
        {
            "heading": "C.4 ARGOVERSE",
            "text": "The Argoverse autonomous vehicle dataset contains 205,942 samples, consisting of diverse driving scenarios from Miami and Pittsburgh. The data can be downloaded from the official Argoverse dataset website. We split 90/10 into a training set and validation set of size 185,348 and 20,594 respectively. The official validation set of size 39,472 is used for testing and reporting performance. We preprocess the scenes to filter out incomplete trajectories and cap the number of vehicles modeled to 60. If there are less than 60 cars in the scenario, we insert dummy cars into them to achieve consistent car numbers. For map information, we only include center lanes with lane directions as features. Similar to vehicles, we introduce dummy lane nodes into each scene to make lane numbers consistently equal to 650."
        },
        {
            "heading": "C.5 ADDITIONAL EXPERIMENT RESULTS",
            "text": "We present in figure 8 and 9 some qualitative results for uncertainty estimation.\nTo test how the effects of copulaCPTS compare with baseline on other base forecasters, we also include an encoder-decoder architecture with the same embedding size as the RNN models introduced in appendix C.1 for each dataset. The results are presented in Table 3. We omit these results in the main text because we found that they do did not bring significant improvement to time series forecasting UQ.\nTable 4 compares model performance compared across different prediction horizons. We show that the advantage of our method is more pronounced for longer horizon forecasts.\nC.6 STUDY ON \u03b1j SEARCH\nFigure 11 shows the \u03b1j values for each 1\u2212 \u03b1j = F\u0302j(s\u2217j ) used in Copula CPTS as outlined in line 15 of Algorithm 1. We present \u03b1j values searched using two methods of searching, with dichotomy search for a constant \u03b1 value for the horizon as in Messoudi et al. (2021), and by stochastic gradient descent as outlined in section 4.2.\nThe \u03b1j values are an indicator of how interrelated the uncertainty between each time step are: Bonferroni Correction used in Stankevic\u030ciu\u0304te\u0307 et al. (2021) (grey dotted line in Figure 11) assumes that the time steps are independent, with CopulaCPTS we have lower 1\u2212 \u03b1j levels while having valid coverage (blue and orange lines in Figure 11). This shows that the uncertainty of the time steps are not independent, and we are able to utilize this dependency to shrink the confidence region and still maintaining the coverage guarantee.\nTable 5 shows that there are no significant difference between coverage and area performance for the two search methods with in the scope of datasets we study in this paper. However, we want to highlight that SGD search is O(n) complexity to optimization steps, regardless to the prediction horizon. SGD also allows for varying \u03b1j which might be useful in some settings, for example capturing uncertainty spikes for some time steps as seen in the COVID-19 dataset of Figure 11. Dichotomy search, on the other hand, is O(nlog(n)) complexity to the search space depends on granularity, and will be O(knlog(kn) if we want to search for varying \u03b1j ."
        },
        {
            "heading": "C.7 COMPARISON TO ADDITIONAL BASELINES",
            "text": "We include a comparison to two additional simple UQ baselines on the particle simulation dataset.\nL2-Conformal. L2-Conformal uses the same underlying RNN forecaster as CF-RNN and Copula RNN. We use that nonconformity score of the vector norm of all timesteps concatenated together \u2225y\u0302t+1:t+k\u2212yt+1:t+k\u2225 to perform ICP. As there are no analytic way to represent a k\u00d7dy-dimensional\nuncertainty region on 2-D space, we calculate the area and plot the region for L2 Conformal baseline with the maximum deviation at each timestep such that the vector norm still stays within range.\nDirect Gaussian. Direct Gaussian use the same model architecture and training hyperparameters, with the addition of a linear layer that outputs the variance for each timestep, and is optimized using negative log loss, a proper scoring rule for probabilistic forecasting. We obtain the area by analytically calculating the 90% confidence interval for each variable.\nResults in Table 6 shows that L2-conformal produces inefficient confidence area, and directly outputting variance under-covers test data. These results align with previous findings and motivates our method, which is both more calibrated and sharper compared to these baselines. We show a visualization in Figure 12 to illustrate the different properties of the methods qualitatively.\nEllipsoidal conformal inference for Multi-Target Regression We also compare CopulaCPTS to a newer work, Ellipsoidal CP (Messoudi et al., 2022). The result is presented in table 7. This method models the uncertainty region of multi-target outputs as a high-dimensional ellipsoid, by estimating a covariance matrix on calibration data. We apply EllipsoidalCP on our data by flattening the time and space dimensions, so the particle simulation, for example, is treated as a multi-target prediction of dim = 50 = 25 (time steps) \u00d7 2 (dims) . We see that the results are comparable in our experiment. When the correlation is more pronounced such as in covid experiment, EllipsoidalCP can capture the correlation better than CopulaCPTS resulting in improved efficiency. On the other hand, the flexibility of our method allows us to achieve better efficiency than that of EllipsoidalCP. A notable concern for using EllipsoidalCP is that for higher output dimensions, the determinant of the covariance matrix can be extremely large (up to 1050 in our experiments) and can result in numerical instabilities."
        }
    ],
    "year": 2023
}