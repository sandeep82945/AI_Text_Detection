{
    "abstractText": "Restoring facial details from low-quality (LQ) images has remained challenging due to the nature of the problem caused by various degradations in the wild. The codebook prior has been proposed to address the ill-posed problems by leveraging an autoencoder and learned codebook of high-quality (HQ) features, achieving remarkable quality. However, existing approaches in this paradigm frequently depend on a single encoder pre-trained on HQ data for restoring HQ images, disregarding the domain gap and distinct feature representations between LQ and HQ images. As a result, encoding LQ inputs with the same encoder could be insufficient, resulting in imprecise feature representation and leading to suboptimal performance. To tackle this problem, we propose a novel dual-branch framework named DAEFR. Our method introduces an auxiliary LQ branch that extracts domainspecific information from the LQ inputs. Additionally, we incorporate association training to promote effective synergy between the two branches, enhancing code prediction and restoration quality. We evaluate the effectiveness of DAEFR on both synthetic and real-world datasets, demonstrating its superior performance in restoring facial details. Project page: https://liagm.github.io/DAEFR/.",
    "authors": [
        {
            "affiliations": [],
            "name": "FACE RESTORATION"
        },
        {
            "affiliations": [],
            "name": "Yu-Ju Tsai"
        },
        {
            "affiliations": [],
            "name": "Yu-Lun Liu"
        },
        {
            "affiliations": [],
            "name": "Lu Qi"
        },
        {
            "affiliations": [],
            "name": "Kelvin C.K. Chan"
        },
        {
            "affiliations": [],
            "name": "Ming-Hsuan Yang"
        },
        {
            "affiliations": [],
            "name": "Yang Ming Chiao"
        }
    ],
    "id": "SP:bca93d05cc60c9300f791419a05eb4a51b8cfd31",
    "references": [
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Kelvin CK Chan",
                "Xintao Wang",
                "Xiangyu Xu",
                "Jinwei Gu",
                "Chen Change Loy"
            ],
            "title": "Glean: Generative latent bank for large-factor image super-resolution",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Kelvin CK Chan",
                "Xiangyu Xu",
                "Xintao Wang",
                "Jinwei Gu",
                "Chen Change Loy"
            ],
            "title": "Glean: Generative latent bank for image super-resolution and beyond",
            "year": 2022
        },
        {
            "authors": [
                "Chaofeng Chen",
                "Dihong Gong",
                "Hao Wang",
                "Zhifeng Li",
                "Kwan-Yee K Wong"
            ],
            "title": "Learning spatial attention for face super-resolution",
            "venue": "TIP, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Chaofeng Chen",
                "Xiaoming Li",
                "Lingbo Yang",
                "Xianhui Lin",
                "Lei Zhang",
                "Kwan-Yee K Wong"
            ],
            "title": "Progressive semantic-aware style transformation for blind face restoration",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Yu Chen",
                "Ying Tai",
                "Xiaoming Liu",
                "Chunhua Shen",
                "Jian Yang"
            ],
            "title": "Fsrnet: End-to-end learning face super-resolution with facial priors",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Bowen Cheng",
                "Ishan Misra",
                "Alexander G Schwing",
                "Alexander Kirillov",
                "Rohit Girdhar"
            ],
            "title": "Maskedattention mask transformer for universal image segmentation",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Nicholas Chimitt",
                "Stanley H Chan"
            ],
            "title": "Simulating anisoplanatic turbulence by sampling intermodal and spatially correlated zernike coefficients",
            "venue": "Optical Engineering,",
            "year": 2020
        },
        {
            "authors": [
                "David Cornett",
                "Joel Brogan",
                "Nell Barber",
                "Deniz Aykac",
                "Seth Baird",
                "Nicholas Burchfield",
                "Carl Dukes",
                "Andrew Duncan",
                "Regina Ferrell",
                "Jim Goddard"
            ],
            "title": "Expanding accurate person recognition to new altitudes and ranges: The BRIAR dataset",
            "venue": "In WACV,",
            "year": 2023
        },
        {
            "authors": [
                "Jiankang Deng",
                "Jia Guo",
                "Niannan Xue",
                "Stefanos Zafeiriou"
            ],
            "title": "Arcface: Additive angular margin loss for deep face recognition",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Berk Dogan",
                "Shuhang Gu",
                "Radu Timofte"
            ],
            "title": "Exemplar guided face image super-resolution without facial landmarks",
            "venue": "In CVPRW,",
            "year": 2019
        },
        {
            "authors": [
                "Patrick Esser",
                "Robin Rombach",
                "Bjorn Ommer"
            ],
            "title": "Taming transformers for high-resolution image synthesis",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Jinjin Gu",
                "Yujun Shen",
                "Bolei Zhou"
            ],
            "title": "Image processing using multi-code gan prior",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Yuchao Gu",
                "Xintao Wang",
                "Liangbin Xie",
                "Chao Dong",
                "Gen Li",
                "Ying Shan",
                "Ming-Ming Cheng"
            ],
            "title": "Vqfr: Blind face restoration with vector-quantized dictionary and parallel decoder",
            "venue": "In ECCV, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Jingwen He",
                "Wu Shi",
                "Kai Chen",
                "Lean Fu",
                "Chao Dong"
            ],
            "title": "Gcfsr: a generative and controllable face super resolution method without facial and gan priors",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaobin Hu",
                "Wenqi Ren",
                "John LaMaster",
                "Xiaochun Cao",
                "Xiaoming Li",
                "Zechao Li",
                "Bjoern Menze",
                "Wei Liu"
            ],
            "title": "Face super-resolution guided by 3d facial priors",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaobin Hu",
                "Wenqi Ren",
                "Jiaolong Yang",
                "Xiaochun Cao",
                "David Wipf",
                "Bjoern Menze",
                "Xin Tong",
                "Hongbin Zha"
            ],
            "title": "Face restoration via plug-and-play 3d facial priors",
            "year": 2021
        },
        {
            "authors": [
                "Gary B Huang",
                "Marwan Mattar",
                "Tamara Berg",
                "Eric Learned-Miller"
            ],
            "title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments",
            "venue": "In Workshop on Faces in \u2019Real-Life\u2019 Images: Detection, Alignment, and Recognition,",
            "year": 2008
        },
        {
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei A Efros"
            ],
            "title": "Image-to-image translation with conditional adversarial networks",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Justin Johnson",
                "Alexandre Alahi",
                "Li Fei-Fei"
            ],
            "title": "Perceptual losses for real-time style transfer and super-resolution",
            "venue": "In ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "Ratheesh Kalarot",
                "Tao Li",
                "Fatih Porikli"
            ],
            "title": "Component attention guided face super-resolution network: Cagface",
            "venue": "In WACV,",
            "year": 2020
        },
        {
            "authors": [
                "Tero Karras",
                "Timo Aila",
                "Samuli Laine",
                "Jaakko Lehtinen"
            ],
            "title": "Progressive growing of gans for improved quality, stability, and variation",
            "venue": "In ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Deokyun Kim",
                "Minseon Kim",
                "Gihyun Kwon",
                "Dae-Shik Kim"
            ],
            "title": "Progressive face super-resolution via attention to facial landmark",
            "venue": "arXiv preprint arXiv:1908.08239,",
            "year": 1908
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Xiaoming Li",
                "Ming Liu",
                "Yuting Ye",
                "Wangmeng Zuo",
                "Liang Lin",
                "Ruigang Yang"
            ],
            "title": "Learning warped guidance for blind face restoration",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Xiaoming Li",
                "Chaofeng Chen",
                "Shangchen Zhou",
                "Xianhui Lin",
                "Wangmeng Zuo",
                "Lei Zhang"
            ],
            "title": "Blind face restoration via deep multi-scale component dictionaries",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Cheng Ma",
                "Zhenyu Jiang",
                "Yongming Rao",
                "Jiwen Lu",
                "Jie Zhou"
            ],
            "title": "Deep face super-resolution with iterative collaboration between attentive recovery and landmark estimation",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Sachit Menon",
                "Alexandru Damian",
                "Shijia Hu",
                "Nikhil Ravi",
                "Cynthia Rudin"
            ],
            "title": "Pulse: Self-supervised photo upsampling via latent space exploration of generative models",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Anish Mittal",
                "Rajiv Soundararajan",
                "Alan C Bovik"
            ],
            "title": "Making a \u201ccompletely blind\u201d image quality analyzer",
            "venue": "IEEE Signal Processing Letters,",
            "year": 2012
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Ali Razavi",
                "Aaron Van den Oord",
                "Oriol Vinyals"
            ],
            "title": "Generating diverse high-fidelity images with vq-vae-2",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Wenqi Ren",
                "Jiaolong Yang",
                "Senyou Deng",
                "David Wipf",
                "Xiaochun Cao",
                "Xin Tong"
            ],
            "title": "Face video deblurring using 3d facial priors",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Elad Richardson",
                "Yuval Alaluf",
                "Or Patashnik",
                "Yotam Nitzan",
                "Yaniv Azar",
                "Stav Shapiro",
                "Daniel Cohen-Or"
            ],
            "title": "Encoding in style: a stylegan encoder for image-to-image translation",
            "year": 2021
        },
        {
            "authors": [
                "Ziyi Shen",
                "Wei-Sheng Lai",
                "Tingfa Xu",
                "Jan Kautz",
                "Ming-Hsuan Yang"
            ],
            "title": "Deep semantic face deblurring",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In ICML,",
            "year": 2015
        },
        {
            "authors": [
                "Aaron Van Den Oord",
                "Oriol Vinyals"
            ],
            "title": "Neural discrete representation learning",
            "venue": "In NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Xintao Wang",
                "Ke Yu",
                "Chao Dong",
                "Chen Change Loy"
            ],
            "title": "Recovering realistic texture in image super-resolution by deep spatial feature transform",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Xintao Wang",
                "Yu Li",
                "Honglun Zhang",
                "Ying Shan"
            ],
            "title": "Towards real-world blind face restoration with generative facial prior",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Xinyao Wang",
                "Liefeng Bo",
                "Li Fuxin"
            ],
            "title": "Adaptive wing loss for robust face alignment via heatmap regression",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Yinhuai Wang",
                "Yujie Hu",
                "Jian Zhang"
            ],
            "title": "Panini-net: Gan prior based degradation-aware feature interpolation for face restoration",
            "venue": "In AAAI,",
            "year": 2022
        },
        {
            "authors": [
                "Zhixin Wang",
                "Ziying Zhang",
                "Xiaoyun Zhang",
                "Huangjie Zheng",
                "Mingyuan Zhou",
                "Ya Zhang",
                "Yanfeng Wang"
            ],
            "title": "Dr2: Diffusion-based robust degradation remover for blind face restoration",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Zhouxia Wang",
                "Jiawei Zhang",
                "Runjian Chen",
                "Wenping Wang",
                "Ping Luo"
            ],
            "title": "Restoreformer: Highquality blind face restoration from undegraded key-value pairs",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Lingbo Yang",
                "Shanshe Wang",
                "Siwei Ma",
                "Wen Gao",
                "Chang Liu",
                "Pan Wang",
                "Peiran Ren"
            ],
            "title": "Hifacegan: Face renovation via collaborative suppression and replenishment",
            "venue": "In ACM MM,",
            "year": 2020
        },
        {
            "authors": [
                "Shuo Yang",
                "Ping Luo",
                "Chen-Change Loy",
                "Xiaoou Tang"
            ],
            "title": "Wider face: A face detection benchmark",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Tao Yang",
                "Peiran Ren",
                "Xuansong Xie",
                "Lei Zhang"
            ],
            "title": "Gan prior embedded network for blind face restoration in the wild",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Zongsheng Yue",
                "Chen Change Loy"
            ],
            "title": "Difface: Blind face restoration with diffused error contraction",
            "venue": "arXiv preprint arXiv:2212.06512,",
            "year": 2022
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Xi Zhang",
                "Xiaolin Wu"
            ],
            "title": "Multi-modality deep restoration of extremely compressed face videos. TPAMI, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Yang Zhao",
                "Yu-Chuan Su",
                "Chun-Te Chu",
                "Yandong Li",
                "Marius Renn",
                "Yukun Zhu",
                "Changyou Chen",
                "Xuhui Jia"
            ],
            "title": "Rethinking deep face restoration",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Shangchen Zhou",
                "Kelvin Chan",
                "Chongyi Li",
                "Chen Change Loy"
            ],
            "title": "Towards robust blind face restoration with codebook lookup transformer",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Feida Zhu",
                "Junwei Zhu",
                "Wenqing Chu",
                "Xinyi Zhang",
                "Xiaozhong Ji",
                "Chengjie Wang",
                "Ying Tai"
            ],
            "title": "Blind face restoration via integrating face shape and generative priors",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "transformer (Vaswani"
            ],
            "title": "2017) module consisting of nine self-attention blocks for the structure. Additionally, we enhance its expressiveness by incorporating sinusoidal positional embedding (Carion et al., 2020; Cheng et al., 2022) on query Q and keys K. This modification aids in capturing positional information effectively within the transformer. In the feature fusion stage, we employ a multi-head cross-attention module (MHCA) to effectively merge the HQ and LQ features",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Blind face restoration presents a formidable challenge as it entails restoring facial images that are degraded by complex and unknown sources of degradation. This degradation process often results in the loss of valuable information. It introduces a significant domain gap, making it arduous to restore the facial image to its original quality with high accuracy. The task itself is inherently illposed, and prior works rely on leveraging different priors to enhance the performance of restoration algorithms. Notably, the codebook prior emerges as a promising solution, showcasing its effectiveness in generating satisfactory results within such challenging scenarios. By incorporating a codebook prior, these approaches demonstrate improved performance in blind face restoration tasks.\nExisting codebook methods (Zhou et al., 2022; Gu et al., 2022; Wang et al., 2022b) address the inclusion of low-quality (LQ) images by adjusting the encoder, which is pre-trained on high-quality (HQ) data, as depicted in Fig. 1(a). However, this approach introduces domain bias due to a domain gap and overlooks the distinct feature representations between the encoder and LQ images. Consequently, employing the pre-trained encoder to encode LQ information may potentially result in imprecise feature representation and lead to suboptimal performance in the restoration process. Furthermore, these approaches neglect the LQ domain\u2019s inherent visual characteristics and statistical properties, which provide valuable information for enhancing the restoration process.\nTo overcome these limitations, we present a novel framework called DAEFR, which incorporates a dedicated auxiliary branch for LQ information encoding (Fig. 1(b)). This auxiliary branch is exclusively trained on LQ data, alleviating domain bias and acquiring a precise feature representation of the LQ domain. By integrating the auxiliary branch into our framework, we effectively harness the enhanced LQ representation of identity and content information from the original LQ images, which can supplement the lost information. DAEFR utilizes both HQ and auxiliary LQ encoders to capture domain-specific information, thereby enhancing the representation of image content.\nThe core idea of our method is to effectively fuse visual information from two branches. If we naively combine the HQ and LQ features, the existing domain gap causes misalignment in feature\n\u2217Corresponding author\nrepresentation, rendering them challenging to utilize effectively. Similar to CLIP (Radford et al., 2021), we incorporate an association stage after extracting features from both branches. This stage aligns the features to a shared domain, effectively bridging the gap between LQ and HQ features and facilitating a more comprehensive and integrated representation of feature information. To ensure the effective fusion of information, we employ a multi-head cross-attention module after acquiring the associated encoders that can adequately represent both the HQ and LQ domains. This module enables us to merge the features from these associated encoders and generate fused features. Through the fusion process, where the features from the associated encoders are combined with the LQ domain information, our approach effectively mitigates the challenges of domain gap and information loss and leverages the complementary aspects of the HQ and LQ domains, leading to improved restoration results. The main contributions of this work are:\n\u2022 We introduce an auxiliary LQ encoder to construct a more precise feature representation that adeptly captures the unique visual characteristics and statistical properties inherent to the LQ domain. \u2022 By incorporating information from a hybrid domain, our association and feature fusion methods effectively use the representation from the LQ domain and address the challenge of domain gap and information loss in image restoration, resulting in enhanced outcomes. \u2022 We propose a novel approach, DAEFR, to address the challenging face restoration problem under severe degradation. We evaluate our method with extensive experiments and ablation studies and demonstrate its effectiveness with superior quantitative and qualitative performances."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Face Restoration. Blind face restoration techniques commonly exploit the structured characteristics of facial features and incorporate geometric priors to achieve desirable outcomes. Numerous methods propose the use of facial landmarks (Chen et al., 2018; Kim et al., 2019; Ma et al., 2020; Zhang & Wu, 2022), face parsing maps (Chen et al., 2021; Shen et al., 2018; Yang et al., 2020), facial component heatmaps (Wang et al., 2019; Chen et al., 2021; 2020; Kalarot et al., 2020), or 3D shapes (Hu et al., 2020; Ren et al., 2019; Zhu et al., 2022; Hu et al., 2021). However, accurately acquiring prior information from degraded faces poses a significant challenge, and relying solely on geometric priors may not yield adequate details for HQ face restoration.\nVarious reference-based methods have been developed to address these limitations (Dogan et al., 2019; Li et al., 2020; 2018). These methods typically rely on having reference images that share the same identity as the degraded input face. However, obtaining such reference images is often impractical or not readily available. Other approaches, such as DFDNet (Li et al., 2020), construct HQ facial component features dictionaries. However, these component-specific dictionaries may lack the necessary information to restore certain facial regions, such as skin and hair.\nTo address this issue, approaches utilize generative facial priors from pre-trained generators, such as StyleGAN2 (Karras et al., 2019). These priors are employed through iterative latent optimization for GAN inversion (Gu et al., 2020; Menon et al., 2020) or direct latent encoding of degraded faces (Richardson et al., 2021). However, preserving high fidelity in the restored faces becomes challenging when projecting degraded faces into the continuous infinite latent space.\nOn the other hand, GLEAN (Chan et al., 2021; 2022), GPEN (Yang et al., 2021), GFPGAN (Wang et al., 2021), GCFSR (He et al., 2022), and Panini-Net (Wang et al., 2022a) incorporate generative priors into encoder-decoder models, utilizing additional structural information from input images as guidance. Although these methods improve fidelity, they heavily rely on the guidance of the inputs, which can introduce artifacts when the images are severely corrupted.\nMost recently, diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) have been developed for generating HQ content. Several approaches (Yue & Loy, 2022; Wang et al., 2023) exploit the effectiveness of the diffusion prior to restore LQ face images. However, these methods do not preserve the identity information present in the LQ images well. Vector Quantized Codebook Prior. A vector-quantized codebook is introduced in the VQ-VAE framework (Van Den Oord et al., 2017). Unlike continuous outputs, the encoder network in this approach generates discrete outputs, and the codebook prior is learned rather than static. Subsequent research proposes various enhancements to codebook learning. VQVAE2 (Razavi et al., 2019) introduces a multi-scale codebook to improve image generation capabilities. On the other hand, VQGAN (Esser et al., 2021) trains the codebook using an adversarial objective, enabling the codebook to achieve high perceptual quality.\nRecently, codebook-based methods (Wang et al., 2022b; Gu et al., 2022; Zhou et al., 2022; Zhao et al., 2022) explore the use of learned HQ dictionaries or codebooks that contain more generic and detailed information for face restoration. CodeFormer (Zhou et al., 2022) employs a transformer to establish the appropriate mapping between LQ features and code indices. Subsequently, it uses the code index to retrieve the corresponding feature in the codebook for image restoration. RestoreFormer (Wang et al., 2022b) and VQFR (Gu et al., 2022) attempt to directly incorporate LQ information with the codebook information based on the codebook prior. However, these methods may encounter severe degradation limitations, as the LQ information can negatively impact the HQ information derived from the codebook."
        },
        {
            "heading": "3 METHOD",
            "text": "Our primary objective is to mitigate the domain gap and information loss that emerge while restoring HQ images from LQ images. This challenge has a substantial impact on the accuracy and effectiveness of the restoration process. To address this issue, we propose an innovative framework with an auxiliary LQ encoder incorporating domain-specific information from the LQ domain. Furthermore, we utilize feature association techniques between the HQ and LQ encoders to enhance restoration.\nIn our framework, we first create discrete codebooks for both the HQ and LQ domains and utilize vector quantization (Esser et al., 2021; Van Den Oord et al., 2017) to train a quantized autoencoder via self-reconstruction (Sec. 3.1). Then, we introduce a feature association technique in a way similar to the CLIP model (Radford et al., 2021) to associate two encoders and aim to reduce the domain gap between the two domains (Sec. 3.2). In the next stage, a feature fusion module is trained using a multi-head cross-attention (MHCA) technique to combine the features extracted from the two associated encoders (EAH and E A L ). We employ the transformer to perform the code prediction process using the integrated information from both encoders, which predicts the relevant code elements in the HQ codebook. Subsequently, the decoder utilizes the restored code features to generate HQ images (Sec. 3.3). Our framework is illustrated in Fig. 2."
        },
        {
            "heading": "3.1 DISCRETE CODEBOOK LEARNING STAGE",
            "text": "Similar to VQGAN (Esser et al., 2021), our approach involves encoding domain-specific information through an autoencoder and codebook, enabling the capture of domain characteristics during the training phase. Both HQ and LQ paths are trained using the same settings to ensure consistency in feature representation. Here, we present the HQ reconstruction path as an illustrative example, noting that the LQ reconstruction path follows an identical procedure.\nThe process shown in Fig. 2(a) involves the encoding of the HQ face image Ih \u2208 RH\u00d7W\u00d73 into a compressed feature Zh \u2208 Rm\u00d7n\u00d7d by the encoder EH . This step is carried out by replacing each feature vector of Zh with the nearest code item in the learnable codebook Ch = ck \u2208 Rd N\u22121 k=0 . Consequently, we obtain the quantized feature Zch \u2208 Rm\u00d7n\u00d7d:\nZ c(i,j) h = arg minck\u2208Ch \u2225Z(i,j)h \u2212 ck\u22252, k \u2208 [0, ..., N \u2212 1], (1)\nwhere Zc(i,j)h and Z (i,j) h are the feature vectors on the position (i, j) of Z c h and Zh. \u2225 \u00b7 \u22252 is the L2-norm. After obtaining the quantized feature Zch, the decoder DH then proceeds to reconstruct the HQ face image I rech \u2208 RH\u00d7W\u00d73 using Zch. Training Objectives. Similar to the prior arts (Gu et al., 2022; Zhou et al., 2022; Wang et al., 2022b; Esser et al., 2021) setting, We incorporate four distinct losses for training, which includes three image-level reconstruction losses (i.e., L1 loss L1, perceptual loss Lper (Zhang et al., 2018; Johnson et al., 2016), and adversarial loss Ladv (Isola et al., 2017)) and one code-level loss Lcode (Esser et al., 2021; Van Den Oord et al., 2017):\nL1 = \u2225Ih \u2212 I rech \u22251, Lper = \u2225\u03a6(Ih)\u2212 \u03a6(I rech )\u222522, Ladv = [logD(Ih) + log(1\u2212D(I rech ))], Lcode = \u2225sg(Zh)\u2212 Zch\u222522 + \u03b2\u2225Zh \u2212 sg(Zch)\u222522, (2)\nwhere the feature extractor of VGG19 (Simonyan & Zisserman, 2014) is represented by \u03a6, D is a patch-based discriminator (Isola et al., 2017), and sg(\u00b7) denotes the stop-gradient operator. The value of \u03b2 is set at 0.25. The final loss Lcodebook is:\nLcodebook = L1 + \u03bbper \u00b7 Lper + \u03bbadv \u00b7 Ladv + Lcode, (3)\nwhere we set \u03bbper = 1.0 and \u03bbadv = 0.8 in our setting."
        },
        {
            "heading": "3.2 ASSOCIATION STAGE",
            "text": "In this work, we reduce the domain gap between the HQ and LQ domains, allowing the two encoders to encompass a greater range of information from both domains. Once we obtain the domain encoders from the codebook learning stage, we take inspiration from the CLIP model (Radford et al., 2021) and propose a feature patch association algorithm. This technique involves applying the feature patch association on the output from the two encoders. By utilizing this approach, we aim to further minimize the domain gap between the HQ and LQ domains.\nAs shown in Fig. 2(b), after obtaining the HQ and LQ domain encoder (EH and EL) from the previous stage, we proceed to flatten the output features (Zh and Zl \u2208 Rm\u00d7n\u00d7d) into corresponding patches (PHi and P L i , i \u2208 [1, ...,m\u00d7 n]). This flattening enables us to construct a similarity matrix (Massoc \u2208 RN\u00d7N , N = m \u00d7 n), which we use to quantify the similarity between different patch features. Specifically, we calculate the cosine similarity for each patch feature and constrain them to maximize the similarity along the diagonal of the matrix. By applying this constraint, the two encoders are prompted to connect patch features close in both spatial location and feature level, preserving their spatial relationship throughout the association process. Combining the patch features from both encoders results in two associated encoders, denoted as EAH and E A L , that integrate specific domain information, which will be utilized in the subsequent stage. Training Objectives. We perform joint training on the HQ and LQ reconstruction paths, incorporating the association part. To facilitate the feature association process, we adopt the cross-entropy loss (LHCE and LLCE) to effectively constrain the similarity matrix Massoc:\nLHCE = \u2212 1\nN N\u2211 i=1 C\u2211 j=1 yi,j log(p h i,j), LLCE = \u2212 1 N N\u2211 i=1 C\u2211 j=1 yi,j log(p l i,j), (4)\nwhere N denotes the patch size, and C represents the number of classes, which, in our specific case, is equal to N . The ground truth label is denoted as yi,j , whereas the cosine similarity score in the similarity matrix Massoc in the HQ axis is represented as phi,j . Conversely, the score in the LQ axis is defined as pli,j . The final objective of the feature association part is:\nLassoc = L1 + \u03bbper \u00b7 Lper + \u03bbadv \u00b7 Ladv + Lcode + (LHCE + LLCE)/2, (5) where we integrate the same losses and weights used in the codebook learning stage to maintain the representation of features."
        },
        {
            "heading": "3.3 FEATURE FUSION & CODE PREDICTION STAGE",
            "text": "After obtaining the two associated encoders EAH and E A L from the feature association stage, we use both encoders to encode the LQ image Il, as shown in Fig. 2(c). Specifically, we extract feature information ZAh \u2208 Rm\u00d7n\u00d7d and ZAl \u2208 Rm\u00d7n\u00d7d from the LQ image Il using each encoder EAH and EAL , separately. Similar to (Vaswani et al., 2017; Wang et al., 2022b), we use a multi-head cross-attention (MHCA) module to merge the feature information from both encoders and generate a fused feature ZAf \u2208 Rm\u00d7n\u00d7d that incorporates the LQ domain information. This fused feature ZAf is expected to contain useful information from both HQ and LQ domains: ZAf = MHCA(Z A h , Z A l ). The MHCA mechanism lets the module focus on different aspects of the feature space and better capture the relevant information from both encoders.\nOnce the fused feature ZAf is obtained using the feature fusion technique with MHCA, we utilize a transformer-based classification approach (Zhou et al., 2022), to predict the corresponding class as code index s. Initially, we flatten the fused feature ZAf \u2208 Rm\u00d7n\u00d7d as Z\u0302Af \u2208 R(m\u00b7n)\u00d7d and input the flattened fused feature Z\u0302Af into the transformer and obtain the predicted code index s \u2208 {0, \u00b7 \u00b7 \u00b7 , N \u2212 1}m\u00b7n. During this process, the HQ codebook Ch and HQ decoder DH from the codebook learning stage are frozen. We then use the predicted code index s to locate the corresponding feature in the HQ codebook Ch and feed the resulting feature Zcf to the decoder DH to generate HQ images Ires, as depicted in Fig. 2(c). This step efficiently enhances the image restoration by incorporating information from the hybrid domain. Training Objectives. We utilize two losses to train the MHCA and transformer module effectively to ensure proper feature fusion and code index prediction learning. The first loss is an L2 loss Lfeatcode,\nwhich encourages the fused feature ZAf to closely resemble the quantized feature Z c h from the HQ codebook Ch. This loss helps ensure that the features are properly combined and maintains the relevant information from both the HQ and LQ domains. The second loss is a cross-entropy loss Lindexcode for code index prediction, enabling the model to accurately predict the corresponding code index s in the HQ codebook Ch.\nLfeatcode = \u2225ZAf \u2212 sg(Zch)\u222522, Lindexcode = mn\u22121\u2211 i=0 \u2212s\u0302i log(si), (6)\nwhere we obtain the ground truth feature Zch and code index s\u0302 from the codebook learning stage, which we retrieve the quantized feature Zch from the HQ codebook Ch using the code index s\u0302. The final objective of the feature fusion and code prediction is:\nLpredict = \u03bbfeat \u00b7 Lfeatcode + Lindexcode , (7) where we set the L2 loss weight \u03bbfeat = 10 in our experiments."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUP",
            "text": "Implementation Details. In our implementation, the size of the input face image is 512 \u00d7 512 \u00d7 3, and the size of the quantized feature is 16 \u00d7 16 \u00d7 256. The codebooks contain N = 1,024 code items, and the channel of each item is 256. Throughout the entire training process, we employ the Adam optimizer (Kingma & Ba, 2014) with a batch size 32 and set the learning rate to 1.44 \u00d7 10\u22124. The HQ and LQ reconstruction codebook priors are trained for 700K and 400K iterations, respectively, and the feature association part is trained for 70K iterations. Finally, the feature fusion and code prediction stage is trained for 100K iterations. The proposed method is implemented in Pytorch and trained with eight NVIDIA Tesla A100 GPUs. Training Dataset. We train our model on the FFHQ dataset (Karras et al., 2019), which contains 70,000 high-quality face images. For training, we resize all images from 1024 \u00d7 1024 to 512 \u00d7 512. To generate the paired data, we synthesize the degraded images on the FFHQ dataset using the same procedure as the compared methods (Li et al., 2018; 2020; Wang et al., 2021; 2022b; Gu et al., 2022; Zhou et al., 2022). First, the HQ image Ihigh is blurred (convolution operator \u2297) by a Gaussian kernel k\u03c3. Subsequently, a downsampling operation \u2193 with a scaling factor r is performed to reduce the image\u2019s resolution. Next, additive Gaussian noise n\u03b4 is added to the downsampled image. JPEG compression with a quality factor q is applied to further degrade image quality. The resulting image is then upsampled \u2191 with a scaling factor r to a resolution of 512 \u00d7 512 to obtain the degraded Ilow image. In our experiment setting, we randomly sample \u03c3, r, \u03b4, and q from [0.1, 15], [0.8, 30], [0, 20], and [30, 100], respectively. The procedure can be formulated as follows:\nIlow = {[(Ihigh \u2297 k\u03c3) \u2193r + n\u03b4] JPEGq} \u2191r. (8) Testing Dataset. Our evaluation follows the settings in prior literature (Wang et al., 2022b; Gu et al., 2022; Zhou et al., 2022), and includes four datasets: the synthetic dataset CelebA-Test and three real-world datasets, namely, LFW-Test, WIDER-Test, and BRIAR-Test. CelebA-Test comprises 3,000 images selected from the CelebA-HQ testing partition (Karras et al., 2018). LFW-Test consists of 1,711 images representing the first image of each identity in the validation part of the LFW dataset (Huang et al., 2008). Zhou et al. (Zhou et al., 2022) collected the WIDER-Test from the WIDER Face dataset (Yang et al., 2016), which comprises 970 face images. Lastly, the BRIAR-Test contains 2,120 face images selected from the BRIAR dataset (Cornett et al., 2023). The BRIAR-Test dataset provides a wider range of challenging and diverse degradation levels that enable the evaluation of the generalization and robustness of face restoration methods (All subjects shown in the paper have consented to publication.). Metrics. In evaluating our method\u2019s performance on the CelebA-Test dataset with ground truth, we employ PSNR, SSIM, and LPIPS (Zhang et al., 2018) as evaluation metrics. We utilize the commonly used non-reference perceptual metrics, FID (Heusel et al., 2017) and NIQE (Mittal et al., 2012) to evaluate real-world datasets without ground truth. Similar to prior work (Gu et al., 2022; Wang et al., 2022b; Zhou et al., 2022), we measure the identity of the generated images by using the embedding angle of ArcFace (Deng et al., 2019), referred to as \u201cIDA\u201d. To better measure the fidelity of generated facial images with accurate facial positions and expressions, we additionally adopt landmark distance (LMD) as the fidelity metric.\n(b) The synthetic CelebA-Test dataset."
        },
        {
            "heading": "4.2 COMPARISONS WITH STATE-OF-THE-ART METHODS",
            "text": "We compare the proposed method, DAEFR, with state-of-the-art methods: PSFRGAN (Chen et al., 2021), GFP-GAN (Wang et al., 2021), GPEN (Yang et al., 2021), RestoreFormer (Wang et al., 2022b), CodeFormer (Zhou et al., 2022), VQFR (Gu et al., 2022), and DR2 (Wang et al., 2023).\nEvaluation on Real-world Datasets. As shown in Table 1(a), our proposed DAEFR outperforms other methods regarding perceptual quality metrics, evidenced by the lowest FID scores on the WIDER-Test and BRIAR-Test dataset and second-best scores on the LFW-Test dataset, indicating the statistical similarity between the distributions of real and generated images. Regarding NIQE scores, which assess the perceptual quality of images, our method achieves the highest score on the LFW-Test dataset and the second-highest on the WIDER-Test and BRIAR-Test datasets.\nVisual comparisons in Fig. 3 further demonstrate the robustness of our method against severe degradation, resulting in visually appealing outcomes. In contrast, RestoreFormer (Wang et al., 2022b) and VQFR (Gu et al., 2022) exhibit noticeable artifacts in their results, while CodeFormer (Zhou et al., 2022) and DR2 (Wang et al., 2023) tend to produce smoothed outcomes, losing intricate facial details. DAEFR, however, effectively preserves the identity of the restored images, producing natural results with fine details. This preservation of identity can be attributed to the design of our model, which emphasizes the retention of original facial features during the restoration process. These observations underscore the strong generalization ability of our method.\nEvaluation on the Synthetic Dataset. We compare DAEFR quantitatively with existing approaches on the synthetic CelebA-Test dataset in Table 1(b). Our method demonstrates competitive performance in image quality metrics, namely FID and LPIPS, achieving the second-best scores among the evaluated methods. These metrics represent the distribution and visual similarity between the restored and original images. Moreover, our approach effectively preserves identity information, as evidenced by comparable IDA and LMD scores. These metrics assess how well the model preserves the identity and facial structure of the original image.\nTo further illustrate these points, we provide a qualitative comparison in Fig. 4. Our method demonstrates the ability to generate high-quality restoration results with fine facial details, surpassing the performance of the other methods. This ability to restore fine facial details results from our model\u2019s capacity to extract and utilize high-frequency information from the degraded images, contributing to its overall performance. We provide more visual comparisons in the appendix and supplementary."
        },
        {
            "heading": "4.3 ABLATION STUDY",
            "text": "Number of Encoders. In our research, we conduct an initial investigation to examine the impact of the number of encoders on the overall performance. We present this analysis in Exp. (a) to (c) as detailed in Table 2. The results of our investigation indicate that utilizing two encoders as the input source yields better performance than using a single encoder. This conclusion is supported by the superior NIQE score achieved in the experiments. This ablation study confirms that including an additional LQ encoder can provide domain-specific information, aiding image restoration. To provide further insights, we show the visual results of Exp. (a) to (c) in Fig. 5. Association Stage. Furthermore, we evaluate the effectiveness of our association stage through Exp. (c) and (d), as depicted in Table 2. The results indicate that utilizing associated encoders as our input source leads to superior performance across all evaluation metrics compared to non-associated encoders. This empirical evidence validates that our association stage effectively enhances the encoder\u2019s capability to retrieve information from the alternative domain and effectively reduces the domain gap between HQ and LQ. To provide visual evidence of the improvements achieved by our approach, we present the visual results of Exp. (c) and (d) in Fig. 5. These visual comparisons further support the superiority of our method. Feature Fusion Module. Finally, we assess the effectiveness of our feature fusion module, comparing two different approaches: a baseline method employing linear projection with three fully connected layers and our proposed multi-head cross-attention module (MHCA). The experimental results are presented in Table 2, specifically under Exp. (d) and Exp. (e). We aim to determine which approach yields better performance regarding both LMD and NIQE scores. The results demonstrate a notable improvement when utilizing the MHCA compared to the straightforward linear projection method for feature fusion. This indicates that our MHCA can effectively fuse domain-specific information from both the HQ and LQ domains while preserving crucial identity information. To provide a visual representation of the results, we showcase the outcomes of Exp. (d) and Exp. (e) in Fig. 5. These visual representations further support our findings and demonstrate the superior performance of our proposed MHCA in enhancing the image restoration process. Effectiveness of Low-Quality Feature from Auxiliary Branch. To demonstrate the effectiveness of our auxiliary LQ branch, we conduct validated experiments of fusing LQ features with feature Zcf in the feature fusion and code prediction stage. These experiments involve extracting LQ features from the LQ codebook and adding a control module (Wang et al., 2018; Zhou et al., 2022) to fuse the LQ feature and feature Zcf before feeding to the HQ decoder.\nOur quantitative evaluation employs quality metrics, including LPIPS, PSNR, and SSIM. We conduct these experiments on the CelebA-Test dataset. Our quantitative results clearly prove the positive impact when we increase the scale of LQ feature scalar slq, as shown in Table. 3. This experiment underscores the practical advantages of our LQ branch and shows the LQ features effectively encode essential visual attributes and domain-specific characteristics, enhancing the image restoration process. We place the detailed experiment setting and visual comparison in the appendix and supplementary.\nValidation on Downstream Face Recognition Task. We conduct a downstream face recognition task using the proposed restoration method on the LFW (Huang et al., 2008) face recognition validation split set (with 12,000 images). We use the unseen atmospheric turbulence degradation to simulate diverse degradation levels, employing the methodology outlined in (Chimitt & Chan, 2020) for degradation generation. The degradation parameters ranging from 10,000 to 40,000 correspond to varying levels of degradation, spanning from slight to severe. We provide the dataset samples in the appendix and supplementary material.\nWe employ the official ArcFace (Deng et al., 2019) model with Verification performance (%) as our evaluation metric to evaluate the restored images. The experimental results, presented in Table 4, demonstrate the superior performance of our method across various degradation levels. Particularly noteworthy is the widening performance gap between CodeFormer and our method as the degradation severity escalates. These findings validate the efficacy of our additional LQ encoder, which captures valuable information from the LQ domain, thus significantly augmenting the restoration process."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we propose DAEFR to effectively tackle the challenge of blind face restoration, generating high-quality facial images from low-quality ones despite the domain gap and information loss between HQ and LQ image types. We introduce an auxiliary LQ encoder that captures the LQ domain\u2019s specific visual characteristics and statistical properties. We also employ feature association techniques between the HQ and LQ encoders to alleviate the domain gap. Furthermore, we leverage attention mechanisms to fuse the features extracted from the two associated encoders, enabling them to contribute to the code prediction process and produce high-quality results. Our experiments show that DAEFR produces promising results in both synthetic and real-world datasets with severe degradation, demonstrating its effectiveness in blind face restoration."
        },
        {
            "heading": "6 ACKNOWLEDGEMENT",
            "text": "This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&D Contract No. 2022-21102100001. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon."
        },
        {
            "heading": "A OVERVIEW",
            "text": "This supplementary material presents additional results to complement the main manuscript. First, we describe the detailed network architecture in Section B. Then, we conduct more ablation studies to demonstrate the effectiveness of the association stage and our auxiliary LQ branch in Section C. Finally, we show more visual comparisons with state-of-the-art methods in Section D."
        },
        {
            "heading": "B NETWORK ARCHITECTURE",
            "text": "The detailed structures of the encoder and decoder are shown in Table 5. Our restoration process employs an identical encoder and decoder structure for both the HQ and LQ paths. We use a transformer (Vaswani et al., 2017) module consisting of nine self-attention blocks for the structure. Additionally, we enhance its expressiveness by incorporating sinusoidal positional embedding (Carion et al., 2020; Cheng et al., 2022) on query Q and keys K. This modification aids in capturing positional information effectively within the transformer. In the feature fusion stage, we employ a multi-head cross-attention module (MHCA) to effectively merge the HQ and LQ features extracted from the encoders, which is inspired by (Wang et al., 2022b; Vaswani et al., 2017). The implementation of MHCA is:\nQ = ZAl Wq + bq, K = Z A h Wk + bk, V = Z A h Wv + bv, (9)\nwhere we utilize feature ZAl as queries Q and feature Z A h as keys K and values V. These features are multiplied by their respective learnable weights Wq/k/v \u2208 Rd\u00d7d and biased by bq/k/v \u2208 Rd.\nZAf = MHCA(Z A h , Z A l ) = FFN(LN(Z A h , Z A l )), (10)\nwhere LN is the layer normalization, and FFN is the feed-forward network composed of two convolution layers."
        },
        {
            "heading": "C MORE ABLATION STUDIES",
            "text": "In this section, we delve into further ablation studies concerning the association stage and our auxiliary LQ branch, focusing on architecture design and various loss designs. In our proposed approach,\nwe aim to demonstrate the association stage\u2019s effectiveness and our auxiliary LQ branch. Through these studies, we provide detailed analysis and evidence to support the impact and contribution of the association stage and our auxiliary LQ branch in improving the overall performance of the image restoration process.\nDomain Gap before Association Stage We thoroughly investigate the domain gap in three distinct pathways: HQ path, LQ path, and Hybrid path. All three pathways involve the utilization of LQ images as input. Specifically, the LQ image is inputted into the LQ encoder in the Hybrid path, and the resulting features are subsequently forwarded to the HQ codebook. The HQ codebook is employed to identify the closest feature, which is then utilized for reconstruction using the HQ decoder. As illustrated in Figure 6, both the HQ path and Hybrid path exhibit limitations in effectively reconstructing or restoring the LQ image due to the domain gap. Notably, the Hybrid path fails to generate any facial features.\nDifferent Association Architecture We explore various network architectures to enhance the capabilities of the LQ encoder, as depicted in Fig.7. However, our experimental results demonstrated that solely enabling the LQ encoder to learn from the HQ encoder did not effectively address the domain gap issue. Furthermore, the less constrained HQ feature representation did not align well with LQ features, leading to the generation of non-face images, as depicted in Fig.8(c). Therefore, we opt for a joint optimization approach, incorporating both the HQ and LQ paths, to preserve the feature representation. This approach aims to empower both encoders to capture more comprehensive information from both domains and reduce the domain gap.\nDifferent Loss Settings We explore various types of loss functions to address the similarity between HQ and LQ features. We investigate using the L1 and L2 loss to enforce an exact match between the HQ and LQ features. However, the experimental results demonstrate that this approach does not yield satisfactory outcomes, as shown in Fig. 9. We also present the quantitative results of our CelebA validation dataset in Table 6.\nDrawing inspiration from the CLIP model (Radford et al., 2021), we construct a similarity matrix and utilize the cross-entropy loss to constrain the feature similarity. As depicted in Fig. 9, we configure our experiments to use the LQ image as input and employ the LQ encoder to generate features. These features are then processed through the HQ codebook to identify the nearest matching feature and then decoded using the HQ decoder.\nBy incorporating the cross-entropy loss, we effectively establish associations between the HQ and LQ features, enhancing the overall performance of our approach.\nEffectiveness of Low-Quality Feature from Auxiliary Branch. To demonstrate the effectiveness of our auxiliary LQ branch, we conduct validated experiments of fusing LQ features with feature Zcf in the feature fusion and code prediction stage. These experiments involve extracting LQ features Zcl from the LQ codebook and adding a control module (Wang et al., 2018; Zhou et al., 2022). Given a feature control module and feature scalar slq, we can control the scale of the LQ feature Zcl to fuse with the feature Zcf before feeding to the HQ decoder. The network architecture is shown in Fig. 10.\nWe conduct these experiments on the CelebA-Test dataset. Our visual results clearly prove the positive impact when we increase the scale of LQ feature scalar slq, as shown in Fig. 11. This experiment underscores the practical advantages of our LQ branch and shows the LQ features effectively encode essential visual attributes and domain-specific characteristics, enhancing the image restoration process.\nValidation on Downstream Face Recognition Task We conduct a comprehensive downstream face recognition task with the following procedure. We utilize the LFW (Huang et al., 2008) Face\nrecognition dataset\u2019s validation split, comprising 12,000 images. We choose the unseen atmospheric turbulence degradation to simulate diverse degradation levels, employing the methodology outlined in (Chimitt & Chan, 2020) for degradation generation. The degradation parameters ranging from 10,000 to 40,000 correspond to varying levels of degradation, spanning from slight to severe. We provide the dataset samples in Fig. 12."
        },
        {
            "heading": "D MORE VISUAL COMPARISON",
            "text": "In this section, we present additional visual comparisons with state-of-the-art methods: PSFRGAN (Chen et al., 2021), GFP-GAN (Wang et al., 2021), GPEN (Yang et al., 2021), RestoreFormer (Wang et al., 2022b), CodeFormer (Zhou et al., 2022), VQFR (Gu et al., 2022), and DR2 (Wang et al., 2023).\nThe qualitative comparisons on the LFW-Test are shown in Fig. 13. The qualitative comparisons on the WIDER-Test are shown in Fig. 14, Fig. 15 and Fig. 16. The qualitative comparisons on the BRIAR-Test are shown in Fig. 17. The qualitative comparisons on the CelebA-Test are shown in Fig. 18.\nWe also compare the large occlusion and pose situation with state-of-the-art methods in Fig. 19.\nWhile our method demonstrates robustness in most severe degradation scenarios, we also observe instances where it may fail, particularly in cases with large face poses. This can be expected as the FFHQ dataset contains few samples with large face poses, leading to a scarcity of relevant codebook features to effectively address such situations, resulting in less satisfactory restoration and reconstruction outcomes. We show the failure cases in Fig. 20.\nThese comparisons demonstrate that our proposed DAEFR generates high-quality facial images and effectively preserves the identities, even under severe degradation of the input faces. Furthermore, compared to the alternative methods, DAEFR performs better in recovering finer details and producing more realistic facial outputs.\nInput PSFRGAN GFP-GAN\nGPEN RestoreFormer CodeFormer\nVQFR DAEFR (Ours)\nInput PSFRGAN GFP-GAN\nGPEN RestoreFormer CodeFormer\nVQFR DAEFR (Ours)\nDR2\nDR2\nFigure 14: Qualitative comparison on WIDER-Test datasets. Our DAEFR method exhibits robustness in restoring high-quality faces in detail part."
        }
    ],
    "year": 2024
}