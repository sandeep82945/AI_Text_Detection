{
    "abstractText": "Imitation learning (IL) aims at producing agents that can imitate any behavior given a few expert demonstrations. Yet existing approaches require many demonstrations and/or running (online or offline) reinforcement learning (RL) algorithms for each new imitation task. Here we show that recent RL foundation models based on successor measures can imitate any expert behavior almost instantly with just a few demonstrations and no need for RL or fine-tuning, while accommodating several IL principles (behavioral cloning, feature matching, reward-based, and goal-based reductions). In our experiments, imitation via RL foundation models matches, and often surpasses, the performance of SOTA offline IL algorithms, and produces imitation policies from new demonstrations within seconds instead of hours.",
    "authors": [],
    "id": "SP:bcc8ebf39f200a3a4a0e183bd7700dc3c58617e7",
    "references": [
        {
            "authors": [
                "Pieter Abbeel",
                "Andrew Y. Ng"
            ],
            "title": "Apprenticeship learning via inverse reinforcement learning",
            "venue": "In ICML, volume 69 of ACM International Conference Proceeding Series. ACM,",
            "year": 2004
        },
        {
            "authors": [
                "Rishabh Agarwal",
                "Max Schwarzer",
                "Pablo Samuel Castro",
                "Aaron C. Courville",
                "Marc G. Bellemare"
            ],
            "title": "Deep reinforcement learning at the edge of the statistical precipice",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Firas Al-Hafez",
                "Davide Tateo",
                "Oleg Arenz",
                "Guoping Zhao",
                "Jan Peters"
            ],
            "title": "LS-IQ: implicit reward regularization for inverse reinforcement learning",
            "venue": "In ICLR. OpenReview.net,",
            "year": 2023
        },
        {
            "authors": [
                "Marcin Andrychowicz",
                "Dwight Crow",
                "Alex Ray",
                "Jonas Schneider",
                "Rachel Fong",
                "Peter Welinder",
                "Bob McGrew",
                "Josh Tobin",
                "Pieter Abbeel",
                "Wojciech Zaremba"
            ],
            "title": "Hindsight experience replay",
            "venue": "In NIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Michael Bain",
                "Claude Sammut"
            ],
            "title": "A framework for behavioural cloning",
            "venue": "In Machine Intelligence",
            "year": 1995
        },
        {
            "authors": [
                "L\u00e9onard Blier",
                "Corentin Tallec",
                "Yann Ollivier"
            ],
            "title": "Learning successor states and goal-dependent values: A mathematical viewpoint",
            "venue": "arXiv preprint arXiv:2101.07123,",
            "year": 2021
        },
        {
            "authors": [
                "Diana Borsa",
                "Andr\u00e9 Barreto",
                "John Quan",
                "Daniel Mankowitz",
                "R\u00e9mi Munos",
                "Hado van Hasselt",
                "David Silver",
                "Tom Schaul"
            ],
            "title": "Universal successor features approximators",
            "venue": "arXiv preprint arXiv:1812.07626,",
            "year": 2018
        },
        {
            "authors": [
                "David Brandfonbrener",
                "Ofir Nachum",
                "Joan Bruna"
            ],
            "title": "Inverse dynamics pretraining learns good representations for multitask imitation, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Andreas B\u00fchler",
                "Adrien Gaidon",
                "Andrei Cramariuc",
                "Rares Ambrus",
                "Guy Rosman",
                "Wolfram Burgard"
            ],
            "title": "Driving through ghosts: Behavioral cloning with false positives",
            "venue": "pp. 5431\u20135437,",
            "year": 2020
        },
        {
            "authors": [
                "Yuri Burda",
                "Harrison Edwards",
                "Amos J. Storkey",
                "Oleg Klimov"
            ],
            "title": "Exploration by random network distillation",
            "venue": "In 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Micah Carroll",
                "Orr Paradise",
                "Jessy Lin",
                "Raluca Georgescu",
                "Mingfei Sun",
                "David Bignell",
                "Stephanie Milani",
                "Katja Hofmann",
                "Matthew J. Hausknecht",
                "Anca D. Dragan",
                "Sam Devlin"
            ],
            "title": "Uni[mask]: Unified inference in sequential decision problems",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Matthew Chang",
                "Saurabh Gupta"
            ],
            "title": "One-shot visual imitation via attributed waypoints and demonstration augmentation",
            "venue": "In ICRA,",
            "year": 2023
        },
        {
            "authors": [
                "Jongwook Choi",
                "Archit Sharma",
                "Honglak Lee",
                "Sergey Levine",
                "Shixiang Shane Gu"
            ],
            "title": "Variational empowerment as representation learning for goal-conditioned reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kamil Ciosek"
            ],
            "title": "Imitation learning by reinforcement learning",
            "venue": "In ICLR. OpenReview.net,",
            "year": 2022
        },
        {
            "authors": [
                "Christopher R. Dance",
                "Julien Perez",
                "Th\u00e9o Cachet"
            ],
            "title": "Demonstration-conditioned reinforcement learning for few-shot imitation",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Sudeep Dasari",
                "Abhinav Gupta"
            ],
            "title": "Transformers for one-shot visual imitation",
            "venue": "In CoRL, volume 155 of Proceedings of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Yiming Ding",
                "Carlos Florensa",
                "Pieter Abbeel",
                "Mariano Phielipp"
            ],
            "title": "Goal-conditioned imitation learning",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Yan Duan",
                "Marcin Andrychowicz",
                "Bradly C. Stadie",
                "Jonathan Ho",
                "Jonas Schneider",
                "Ilya Sutskever",
                "Pieter Abbeel",
                "Wojciech Zaremba"
            ],
            "title": "One-shot imitation learning",
            "venue": "In NIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Benjamin Eysenbach",
                "Abhishek Gupta",
                "Julian Ibarz",
                "Sergey Levine"
            ],
            "title": "Diversity is all you need: Learning skills without a reward function",
            "venue": "CoRR, abs/1802.06070,",
            "year": 2018
        },
        {
            "authors": [
                "Chelsea Finn",
                "Tianhe Yu",
                "Tianhao Zhang",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "One-shot visual imitation learning via meta-learning",
            "venue": "In CoRL,",
            "year": 2017
        },
        {
            "authors": [
                "Justin Fu",
                "Aviral Kumar",
                "Ofir Nachum",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "D4RL: datasets for deep data-driven reinforcement learning",
            "year": 2004
        },
        {
            "authors": [
                "Scott Fujimoto",
                "Herke van Hoof",
                "David Meger"
            ],
            "title": "Addressing function approximation error in actor-critic methods",
            "venue": "In ICML, volume 80 of Proceedings of Machine Learning Research,",
            "year": 2018
        },
        {
            "authors": [
                "Divyansh Garg",
                "Shuvam Chakraborty",
                "Chris Cundy",
                "Jiaming Song",
                "Stefano Ermon"
            ],
            "title": "Iq-learn: Inverse soft-q learning for imitation",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Laurent George",
                "Thibault Buhet",
                "\u00c9milie Wirbel",
                "Gaetan Le-Gall",
                "Xavier Perrotton"
            ],
            "title": "Imitation learning for end to end vehicle longitudinal control with forward camera",
            "venue": "CoRR, abs/1812.05841,",
            "year": 2018
        },
        {
            "authors": [
                "Siddhant Haldar",
                "Jyothish Pari",
                "Anant Rai",
                "Lerrel Pinto"
            ],
            "title": "Teach a robot to fish: Versatile imitation from one minute of demonstrations, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Steven Hansen",
                "Will Dabney",
                "Andre Barreto",
                "Tom Van de Wiele",
                "David Warde-Farley",
                "Volodymyr Mnih"
            ],
            "title": "Fast task inference with variational intrinsic successor",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Stefano Ermon"
            ],
            "title": "Generative adversarial imitation learning",
            "venue": "In NIPS,",
            "year": 2016
        },
        {
            "authors": [
                "Jonathan Ho",
                "Jayesh K. Gupta",
                "Stefano Ermon"
            ],
            "title": "Model-free imitation learning with policy optimization",
            "venue": "In ICML, volume 48 of JMLR Workshop and Conference Proceedings,",
            "year": 2016
        },
        {
            "authors": [
                "Ahmed Hussein",
                "Eyad Elyan",
                "Mohamed Medhat Gaber",
                "Chrisina Jayne"
            ],
            "title": "Deep imitation learning for 3d navigation tasks",
            "venue": "Neural Comput. Appl.,",
            "year": 2018
        },
        {
            "authors": [
                "Stephen James",
                "Michael Bloesch",
                "Andrew J. Davison"
            ],
            "title": "Task-embedded control networks for few-shot imitation learning",
            "venue": "In CoRL,",
            "year": 2018
        },
        {
            "authors": [
                "Jordan Juravsky",
                "Yunrong Guo",
                "Sanja Fidler",
                "Xue Bin Peng"
            ],
            "title": "PADL: language-directed physicsbased character control",
            "venue": "CoRR, abs/2301.13868,",
            "year": 2023
        },
        {
            "authors": [
                "Geon-Hyeong Kim",
                "Jongmin Lee",
                "Youngsoo Jang",
                "Hongseok Yang",
                "Kee-Eung Kim"
            ],
            "title": "Lobsdice: Offline learning from observation via stationary distribution correction estimation",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Geon-Hyeong Kim",
                "Seokin Seo",
                "Jongmin Lee",
                "Wonseok Jeon",
                "HyeongJoo Hwang",
                "Hongseok Yang",
                "Kee-Eung Kim"
            ],
            "title": "Demodice: Offline imitation learning with supplementary imperfect demonstrations",
            "venue": "In ICLR. OpenReview.net,",
            "year": 2022
        },
        {
            "authors": [
                "Kyoichiro Kobayashi",
                "Takato Horii",
                "Ryo Iwaki",
                "Yukie Nagai",
                "Minoru Asada"
            ],
            "title": "Situated GAIL: multitask imitation using task-conditioned adversarial inverse reinforcement learning",
            "year": 1911
        },
        {
            "authors": [
                "Ilya Kostrikov",
                "Ofir Nachum",
                "Jonathan Tompson"
            ],
            "title": "Imitation learning via off-policy distribution matching",
            "venue": "In ICLR. OpenReview.net,",
            "year": 2020
        },
        {
            "authors": [
                "Michael Laskin",
                "Hao Liu",
                "Xue Bin Peng",
                "Denis Yarats",
                "Aravind Rajeswaran",
                "Pieter Abbeel"
            ],
            "title": "Cic: Contrastive intrinsic control for unsupervised skill discovery",
            "venue": "arXiv preprint arXiv:2202.00161,",
            "year": 2022
        },
        {
            "authors": [
                "Youngwoon Lee",
                "Andrew Szot",
                "Shao-Hua Sun",
                "Joseph J. Lim"
            ],
            "title": "Generalizable imitation learning from observation via inferring goal proximity",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Rudolf Lioutikov",
                "Gerhard Neumann",
                "Guilherme Maeda",
                "Jan Peters"
            ],
            "title": "Learning movement primitive libraries through probabilistic segmentation",
            "venue": "The International Journal of Robotics Research,",
            "year": 2017
        },
        {
            "authors": [
                "Evan Zheran Liu",
                "Milad Hashemi",
                "Kevin Swersky",
                "Parthasarathy Ranganathan",
                "Junwhan Ahn"
            ],
            "title": "An imitation learning approach for cache replacement",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Fangchen Liu",
                "Hao Liu",
                "Aditya Grover",
                "Pieter Abbeel"
            ],
            "title": "Masked autoencoding for scalable and generalizable decision making",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Hao Liu",
                "Pieter Abbeel"
            ],
            "title": "Aps: Active pretraining with successor features",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Yicheng Luo",
                "Zhengyao Jiang",
                "Samuel Cohen",
                "Edward Grefenstette",
                "Marc Peter Deisenroth"
            ],
            "title": "Optimal transport for offline imitation learning",
            "venue": "In ICLR. OpenReview.net,",
            "year": 2023
        },
        {
            "authors": [
                "Yecheng Jason Ma",
                "Andrew Shen",
                "Dinesh Jayaraman",
                "Osbert Bastani"
            ],
            "title": "Versatile offline imitation from observations and examples via regularized state-occupancy matching",
            "venue": "In ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Zhao Mandi",
                "Homanga Bharadhwaj",
                "Vincent Moens",
                "Shuran Song",
                "Aravind Rajeswaran",
                "Vikash Kumar"
            ],
            "title": "Cacti: A framework for scalable multi-task multi-scene visual imitation",
            "year": 2022
        },
        {
            "authors": [
                "Russell Mendonca",
                "Oleh Rybkin",
                "Kostas Daniilidis",
                "Danijar Hafner",
                "Deepak Pathak"
            ],
            "title": "Discovering and achieving goals via world models",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Ashvin Nair",
                "Dian Chen",
                "Pulkit Agrawal",
                "Phillip Isola",
                "Pieter Abbeel",
                "Jitendra Malik",
                "Sergey Levine"
            ],
            "title": "Combining self-supervised learning and imitation for vision-based rope manipulation",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2017
        },
        {
            "authors": [
                "Xue Peng",
                "Erwin Coumans",
                "Tingnan Zhang",
                "Tsang-Wei Lee",
                "Jie Tan",
                "Sergey Levine"
            ],
            "title": "Learning agile robotic locomotion skills by imitating animals",
            "year": 2020
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Pieter Abbeel",
                "Sergey Levine",
                "Michiel van de Panne"
            ],
            "title": "Deepmimic: Exampleguided deep reinforcement learning of physics-based character skills",
            "venue": "ACM Trans. Graph.,",
            "year": 2018
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Yunrong Guo",
                "Lina Halper",
                "Sergey Levine",
                "Sanja Fidler"
            ],
            "title": "ASE: large-scale reusable adversarial skill embeddings for physically simulated characters",
            "venue": "ACM Trans. Graph.,",
            "year": 2022
        },
        {
            "authors": [
                "Karl Pertsch",
                "Ruta Desai",
                "Vikash Kumar",
                "Franziska Meier",
                "Joseph J. Lim",
                "Dhruv Batra",
                "Akshara Rai"
            ],
            "title": "Cross-domain transfer via semantic skill imitation, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Bilal Piot",
                "Matthieu Geist",
                "Olivier Pietquin"
            ],
            "title": "Boosted and reward-regularized classification for apprenticeship learning",
            "venue": "In AAMAS,",
            "year": 2014
        },
        {
            "authors": [
                "Dean Pomerleau"
            ],
            "title": "ALVINN: an autonomous land vehicle in a neural network",
            "venue": "In NIPS,",
            "year": 1988
        },
        {
            "authors": [
                "Siddharth Reddy",
                "Anca D. Dragan",
                "Sergey Levine"
            ],
            "title": "SQIL: imitation learning via reinforcement learning with sparse rewards",
            "venue": "In 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Moritz Reuss",
                "Maximilian Li",
                "Xiaogang Jia",
                "Rudolf Lioutikov"
            ],
            "title": "Goal-conditioned imitation learning using score-based diffusion policies",
            "venue": "In Robotics: Science and Systems,",
            "year": 2023
        },
        {
            "authors": [
                "St\u00e9phane Ross",
                "Geoffrey J. Gordon",
                "Drew Bagnell"
            ],
            "title": "A reduction of imitation learning and structured prediction to no-regret online learning",
            "venue": "In AISTATS,",
            "year": 2011
        },
        {
            "authors": [
                "Stefan Schaal"
            ],
            "title": "Learning from demonstration",
            "venue": "In NIPS,",
            "year": 1996
        },
        {
            "authors": [
                "Lior Shani",
                "Tom Zahavy",
                "Shie Mannor"
            ],
            "title": "Online apprenticeship learning",
            "venue": "In AAAI,",
            "year": 2022
        },
        {
            "authors": [
                "Lucy Xiaoyang Shi",
                "Archit Sharma",
                "Tony Z. Zhao",
                "Chelsea Finn"
            ],
            "title": "Waypoint-based imitation learning for robotic manipulation",
            "venue": "CoRR, abs/2307.14326,",
            "year": 2023
        },
        {
            "authors": [
                "Zhenyu Shou",
                "Xuan Di",
                "Jieping Ye",
                "Hongtu Zhu",
                "Hua Zhang",
                "Robert Hampshire"
            ],
            "title": "Optimal passenger-seeking policies on e-hailing platforms using markov decision process and imitation learning",
            "venue": "Transportation Research Part C: Emerging Technologies,",
            "year": 2020
        },
        {
            "authors": [
                "Harshit Sikchi",
                "Qinqing Zheng",
                "Amy Zhang",
                "Scott Niekum"
            ],
            "title": "Dual rl: Unification and new methods for reinforcement and imitation learning, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Richard S. Sutton",
                "Andrew G. Barto"
            ],
            "title": "Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018",
            "venue": "URL http://incompleteideas.net/book/the-book-2nd. html",
            "year": 2018
        },
        {
            "authors": [
                "Umar Syed",
                "Robert E. Schapire"
            ],
            "title": "A game-theoretic approach to apprenticeship learning",
            "venue": "In NIPS,",
            "year": 2007
        },
        {
            "authors": [
                "Umar Syed",
                "Michael H. Bowling",
                "Robert E. Schapire"
            ],
            "title": "Apprenticeship learning using linear programming",
            "venue": "In ICML, volume 307 of ACM International Conference Proceeding Series,",
            "year": 2008
        },
        {
            "authors": [
                "Faraz Torabi",
                "Garrett Warnell",
                "Peter Stone"
            ],
            "title": "Behavioral cloning from observation",
            "venue": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Ahmed Touati",
                "Yann Ollivier"
            ],
            "title": "Learning one representation to optimize all rewards",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Ahmed Touati",
                "J\u00e9r\u00e9my Rapin",
                "Yann Ollivier"
            ],
            "title": "Does zero-shot reinforcement learning exist? In ICLR",
            "venue": "OpenReview.net,",
            "year": 2023
        },
        {
            "authors": [
                "Luca Viano",
                "Angeliki Kamoutsi",
                "Gergely Neu",
                "Igor Krawczuk",
                "Volkan Cevher"
            ],
            "title": "Proximal point imitation learning",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Nolan Wagener",
                "Andrey Kolobov",
                "Felipe Vieira Frujeri",
                "Ricky Loynd",
                "Ching-An Cheng",
                "Matthew Hausknecht"
            ],
            "title": "Mocapact: A multi-task dataset for simulated humanoid control, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Alexander W. Winkler",
                "Jungdam Won",
                "Yuting Ye"
            ],
            "title": "Questsim: Human motion tracking from sparse sensors with simulated avatars",
            "venue": "In SIGGRAPH Asia,",
            "year": 2022
        },
        {
            "authors": [
                "Philipp Wu",
                "Arjun Majumdar",
                "Kevin Stone",
                "Yixin Lin",
                "Igor Mordatch",
                "Pieter Abbeel",
                "Aravind Rajeswaran"
            ],
            "title": "Masked trajectory models for prediction, representation, and control, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Haoran Xu",
                "Xianyuan Zhan",
                "Honglei Yin",
                "Huiling Qin"
            ],
            "title": "Discriminator-weighted offline imitation learning from suboptimal demonstrations",
            "venue": "In ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Sherry Yang",
                "Ofir Nachum",
                "Yilun Du",
                "Jason Wei",
                "Pieter Abbeel",
                "Dale Schuurmans"
            ],
            "title": "Foundation models for decision making: Problems, methods, and opportunities",
            "venue": "arXiv preprint arXiv:2303.04129,",
            "year": 2023
        },
        {
            "authors": [
                "Denis Yarats",
                "David Brandfonbrener",
                "Hao Liu",
                "Michael Laskin",
                "Pieter Abbeel",
                "Alessandro Lazaric",
                "Lerrel Pinto"
            ],
            "title": "Don\u2019t change the algorithm, change the data: Exploratory data for offline reinforcement learning",
            "venue": "arXiv preprint arXiv:2201.13425,",
            "year": 2022
        },
        {
            "authors": [
                "Lantao Yu",
                "Tianhe Yu",
                "Jiaming Song",
                "Willie Neiswanger",
                "Stefano Ermon"
            ],
            "title": "Offline imitation learning with suboptimal demonstrations via relaxed distribution matching",
            "venue": "In AAAI,",
            "year": 2023
        },
        {
            "authors": [
                "Tianhe Yu",
                "Chelsea Finn",
                "Sudeep Dasari",
                "Annie Xie",
                "Tianhao Zhang",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "One-shot imitation from observing humans via domain-adaptive meta-learning",
            "venue": "In Robotics: Science and Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Haotian Zhang",
                "Ye Yuan",
                "Viktor Makoviychuk",
                "Yunrong Guo",
                "Sanja Fidler",
                "Xue Bin Peng",
                "Kayvon Fatahalian"
            ],
            "title": "Learning physically simulated tennis skills from broadcast videos",
            "venue": "ACM Trans. Graph.,",
            "year": 2023
        },
        {
            "authors": [
                "Tianhao Zhang",
                "Zoe McCarthy",
                "Owen Jow",
                "Dennis Lee",
                "Xi Chen",
                "Ken Goldberg",
                "Pieter Abbeel"
            ],
            "title": "Deep imitation learning for complex manipulation tasks from virtual reality teleoperation",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2018
        },
        {
            "authors": [
                "Mandi Zhao",
                "Fangchen Liu",
                "Kimin Lee",
                "Pieter Abbeel"
            ],
            "title": "Towards more generalizable one-shot visual imitation learning",
            "venue": "In ICRA,",
            "year": 2022
        },
        {
            "authors": [
                "Yang Zhou",
                "Rui Fu",
                "Chang Wang",
                "Ruibin Zhang"
            ],
            "title": "Modeling car-following behaviors and driving styles with generative adversarial imitation learning",
            "venue": "Sensors (Basel, Switzerland),",
            "year": 2020
        },
        {
            "authors": [
                "Zhuangdi Zhu",
                "Kaixiang Lin",
                "Bo Dai",
                "Jiayu Zhou"
            ],
            "title": "Off-policy imitation learning from observations",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Brian D. Ziebart",
                "Andrew L. Maas",
                "J. Andrew Bagnell",
                "Anind K. Dey"
            ],
            "title": "Maximum entropy inverse reinforcement learning",
            "venue": "In AAAI,",
            "year": 2008
        },
        {
            "authors": [
                "Konrad Zolna",
                "Alexander Novikov",
                "Ksenia Konyushkova",
                "\u00c7aglar G\u00fcl\u00e7ehre",
                "Ziyu Wang",
                "Yusuf Aytar",
                "Misha Denil",
                "Nando de Freitas",
                "Scott E. Reed"
            ],
            "title": "Offline learning from demonstrations and unlabeled experience",
            "venue": "URL https://arxiv.org/abs/2011",
            "year": 2011
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The objective of imitation learning (Schaal, 1996, IL) is to develop agents that can imitate any behavior from few demonstrations. For instance, a cooking robot may learn how to prepare a new recipe from a single demonstration provided by an expert chef. A virtual character may learn to play different sports in a virtual environment from just a few videos of athletes performing the real sports. Imitation learning algorithms achieved impressing results in challenging domains such as autonomous car driving (B\u00fchler et al., 2020; Zhou et al., 2020; George et al., 2018), complex robotic tasks (Nair et al., 2017; Lioutikov et al., 2017; Zhang et al., 2018; Peng et al., 2020; Mandi et al., 2022; Pertsch et al., 2022; Haldar et al., 2023), navigation tasks (Hussein et al., 2018; Shou et al., 2020), cache management (Liu et al., 2020), and virtual character animation (Zhang et al., 2023; Peng et al., 2018; Wagener et al., 2023). Despite these achievements, existing approaches (see Sec. 2 for a detailed review) suffer from several limitations: for any new behavior to imitate, they often require several demonstrations, extensive interaction with the environment, running complex reinforcement learning routines, or knowing in advance the family of behaviors to be imitated.\nIn this paper, we tackle these limitations by leveraging behavior foundation models (BFMs)1 to accurately solve imitation learning tasks from few demonstrations. To achieve this objective, we want our BFM to have the following properties: 1) When pre-training the BFM, no prior knowledge or demonstrations of the behaviors to be imitated are available, and only a dataset of unsupervised transitions/trajectories is provided; 2) The BFM should accurately solve any imitation task without any additional samples on top of the demonstrations, and without solving any complex reinforcement learning (RL) problem. This means that the computation needed to return the imitation policy (i.e., the inference time) should be minimal; 3) Since many different ways to formalize the imitation learning problem have been proposed (e.g., behavior cloning, apprenticeship learning, waypoint imitation), we also want a BFM that is compatible with different imitation learning settings.\nOur main contributions can be summarized as follows.\n\u2022 We leverage recent advances in BFMs based on successor measures, notably the forward-backward (FB) framework (Touati et al., 2023; Touati & Ollivier, 2021), to build BFMs that can be used to solve any imitation task, and satisfy the three properties above. We focus on FB for its demonstrated performance at zero-shot reinforcement learning compared to other approaches (Touati et al., 2023). We refer to the set of resulting algorithms as FB-IL.\n1The term \u201cBehavior\u201d emphasizes that the model aims at controlling an agent in a dynamical environment. This avoids confusion with widely used foundation models for images, videos, motions, and language. See Yang et al. (2023) for an extensive review of the latter for decision making.\n\u2022 We test FB-IL algorithms across environments from the DeepMind Control Suite (Tassa et al., 2018a) with multiple imitation tasks, using different IL principles and settings. We show that not only do FB-IL algorithms perform on-par or better than the corresponding state-of-the-art offline imitation learning baselines (Fig. 1), they also solve imitation tasks within few seconds, which is three orders of magnitude faster than offline IL methods that need to run full RL routines to compute an imitation policy (Fig. 2). Furthermore, FB-IL methods perform better than other BFM methods, while being able to implement a much wider range of imitation principles.\n2 RELATED WORK\nOffline IL Time BC 3h14m TD3-IL 7h3m Demodice 12h59m\nFB-IL Time BCFB 1m ERFB < 5s BBELLFB 4m\nFigure 2: Time for computing an imitation policy from a single demonstration for a subset of offline IL baselines and FBIL methods, averaged over all environments and tasks.\nWhile a thorough literature review and classification is out of the scope of this work, we recall some of the most popular formulations of IL, each of which will be implemented via BFMs in Sect. 4.\nBehavioral Cloning (Pomerleau, 1988; Bain & Sammut, 1995, BC) aims at directly reproducing the expert policy by maximizing the likelihood of the expert actions under the trained imitation policy. While this is the simplest approach to IL, it needs access to expert actions, it may suffer from compounding errors caused by covariate shift (Ross et al., 2011), and it often requires many demonstrations to learn an accurate imitation policy. Variants of the formulation include regularized BC (e.g., Piot et al., 2014; Xu et al., 2022) and BC from observations only (e.g., Torabi et al., 2018).\nAnother, simple but effective, IL principle is to design (e.g., Ciosek, 2022; Reddy et al., 2020) or infer a reward (e.g., Zolna et al., 2020; Luo et al., 2023; Kostrikov et al., 2020) from the demonstrations, then use it to train an RL agent. For instance, SQIL (Reddy et al.,\n2020) assigns a reward of 1 to expert samples and 0 to non-expert samples obtained either from an offline dataset or online from the environment. Other methods (e.g., Ho & Ermon, 2016; Zolna et al., 2020; Kostrikov et al., 2020; Kim et al., 2022b;a; Ma et al., 2022) learn a discriminator to infer a reward separating expert from non-expert samples. OTR (Luo et al., 2023) uses optimal transport to compute a distance between expert and non-expert transitions that is used as a reward. Finally, other approaches frame IL as a goal-conditioned task (e.g., Ding et al., 2019; Lee et al., 2021), and leverage advances in goal-oriented RL by using goals extracted from expert trajectories.\nMany imitation learning algorithms can be formally derived through the lens of either Apprenticeship Learning (AL) (e.g., Abbeel & Ng, 2004; Syed & Schapire, 2007; Ziebart et al., 2008; Syed et al., 2008; Ho et al., 2016; Ho & Ermon, 2016; Garg et al., 2021; Shani et al., 2022; Viano et al., 2022; Al-Hafez et al., 2023; Sikchi et al., 2023) or Distribution Matching (DM) (e.g., Kostrikov et al., 2020; Kim et al., 2022a; Zhu et al., 2020; Kim et al., 2022b; Ma et al., 2022; Yu et al., 2023). AL looks for a policy that matches or outperforms the expert for any possible reward function in a known class. If the reward is linearly representable w.r.t. a set of features, a sufficient condition is to find a policy whose successor features match that of the expert. DM approaches directly aim to minimize some f -divergence between the stationary distribution of the learned policy and the one of expert.\nThe main limitation of these approaches is that they need to solve a new (online or offline) RL problem for each imitation task from scratch. This often makes their sample and computational complexity prohibitive. Brandfonbrener et al. (2023) pre-train inverse dynamics representations from multitask demonstrations, that can be efficiently fine-tuned with BC to solve some IL tasks with reduced sample complexity. Masked trajectory models (Carroll et al., 2022; Liu et al., 2022; Wu et al., 2023) pre-train transformer-based architectures using random masking of trajectories and can perform waypoint-conditioned imitation if provided with sufficiently curated expert datasets at pre-training. In a similar setting, Reuss et al. (2023) use pre-trained goal-conditioned policies based on diffusion models for waypoint-conditioned imitation. Wagener et al. (2023) pre-train autoregressive architectures with multiple experts, but focuses on trajectory completion rather than \u201cfull\u201d imitation learning. One-shot IL (e.g., Duan et al., 2017; Finn et al., 2017; Yu et al., 2018; Zhao et al., 2022; Chang & Gupta, 2023) uses meta-learning to provide fast adaptation to a new demonstration at train time. This requires carefully curated datasets at train time with access to several expert demonstrations. Task-conditioned approaches (e.g., James et al., 2018; Kobayashi et al., 2019; Dasari & Gupta, 2020; Dance et al., 2021) can solve IL by (meta-)learning a model conditioned on reward, task or expert embeddings by accessing privileged information at train time.2"
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "Markov decision processes. Let M = (S,A, P, ) be a reward-free Markov decision process (MDP), where S is the state space, A is the state space, P (ds0|s, a) is the probability measure on s0 2 S defining the stochastic transition to the next state obtained by taking action a in state s, and 0 < < 1 is a discount factor (Sutton & Barto, 2018). Given (s0, a0) 2 S \u21e5 A and a policy \u21e1 : S ! Prob(A), we denote Pr(\u00b7|s0, a0,\u21e1) and E[\u00b7|s0, a0,\u21e1] the probabilities and expectations under state-action sequences (st, at)t 0 starting at (s0, a0) and following policy \u21e1 in the environment, defined by sampling st \u21e0 P (dst|st 1, at 1) and at \u21e0 \u21e1(dat|st). We define P \u21e1(ds0|s) := R P (ds0|s, a)\u21e1(da|s), the state transition probabilities induced by \u21e1. Given a reward function r : S ! R, the Q-function of \u21e1 for r is Q\u21e1r (s0, a0) := P t 0 t E[r(st)|s0, a0,\u21e1]. The optimal Q-function is Q?r(s, a) := sup\u21e1 Q\u21e1(s, a). (For simplicity, we assume the reward only depends on st+1 instead on the full triplet (st, at, st+1), but this is not essential.)\nFor each policy \u21e1 and each s0 2 S, a0 2 A, the successor measure M\u21e1(s0, a0, \u00b7) over S describes the cumulated discounted time spent at each state st+1 if starting at (s0, a0) and following \u21e1, namely,\nM \u21e1(s0, a0, X) := P t 0 t Pr(st+1 2 X|s0, a0,\u21e1) 8X \u21e2 S. (1)\nThe forward-backward (FB) framework. The FB framework (Touati & Ollivier, 2021) learns a tractable representation of successor measures that provides approximate optimal policies for any reward. Let Rd be a representation space, and let \u21e2 be an arbitrary distribution over states, typically the distribution of states in the training set. FB learns two maps F : S \u21e5 A \u21e5 Rd ! Rd and B : S ! Rd, and a set of parametrized policies (\u21e1z)z2Rd , such that \u21e2 M \u21e1z (s0, a0, X) \u21e1 R X F (s0, a0, z) > B(s) \u21e2(ds), 8s0 2 S, a0 2 A,X \u21e2 S, z 2 Rd\n\u21e1z(s) \u21e1 argmaxa F (s, a, z)>z, 8(s, a) 2 S \u21e5 A, z 2 Rd. (2)\nWe recall some properties of FB that will be leveraged to derive FB-based imitation methods. In the following, we use the short forms CovB := Es\u21e0\u21e2[B(s)B(s)>] and\nM \u21e1(s) := Ea\u21e0\u21e1(s) M\u21e1(s, a), F (s, z) := Ea\u21e0\u21e1z(s) F (s, a, z). (3)\n2A few papers (e.g., Peng et al., 2022; Juravsky et al., 2023) have used expert trajectories to speed up the learning of task-conditioned policies. Their objective is not IL but task generalization and/or compositionality.\nProposition 1 (Touati & Ollivier (2021)). Assume (2) holds exactly. Then the following holds. First, for any reward function r : S ! R, let\nzr = Es\u21e0\u21e2[r(s)B(s)]. (4) Then \u21e1zr is optimal for r, i.e., \u21e1zr 2 argmax\u21e1 Q\u21e1r (s, a). Moreover, Q?r(s, a) = F (s, a, zr)>zr. Finally, for each policy \u21e1z and each (s0, a0) 2 S \u21e5 A, F (s0, a0, z) 2 Rd are the successor features associated to the state embedding '(s) = (CovB) 1B(s), i.e.,\nF (s0, a0, z) = E hP t 0 t '(st+1)|s0, a0,\u21e1z i . (5)\nIn practice, the properties in Prop. 1 only hold approximately, as F>B is a rank-d model of the successor measures, \u21e1z may not be the exact greedy policy, and all of them are learned from samples. Moreover, (4) expresses zr as an expectation over states from the training distribution \u21e2. If sampling from a different distribution \u21e20 at test time, an approximate formula is (Touati & Ollivier, 2021, \u00a7B.5):\nzr = (Es\u21e0\u21e2 B(s)B(s)>)(Es\u21e0\u21e20 B(s)B(s)>) 1 Es\u21e0\u21e20 [r(s)B(s)]. (6)\nPre-training an FB model can be done from a non-curated, offline dataset of trajectories or transitions, thus fulfilling property 1) above. Training is done via the measure-valued Bellman equation satisfied by successor measures. We refer to (Touati et al., 2023) for a full description of FB training.\nFB belongs to a wider class of methods based on successor features (e.g., Borsa et al. (2018)). Many of our imitation algorithms still make sense with other methods in this class, see App. A.6. We focus on FB as it has demonstrated better performance for zero-shot reinforcement learning within this family (Touati et al., 2023)."
        },
        {
            "heading": "4 FORWARD-BACKWARD METHODS FOR IMITATION LEARNING",
            "text": "We consider the standard imitation learning problem, where we have access to a few expert trajectories \u2327 = (s0, s1, . . . , s`(\u2327)), each of length `(\u2327), generated by some unknown expert policy \u21e1e, and no reward function is available. In general, we do not need access to the expert actions, except for behavioral cloning. We denote by E\u2327 the empirical average over the expert trajectories \u2327 and by \u21e2e the empirical distribution of states visited by the expert trajectories.3\nWe now describe several IL methods based on a pre-trained FB model. These run only from demonstration data, without solving any complex RL problem at test time (property 2)). Some methods just require a near-instantaneous forward pass through B at test time, while others require a gradient descent over the small-dimensional parameter z. The latter is still much faster than solving a full RL problem, as shown in Fig. 2. At imitation time, we assume access to the functions F , B, the matrix CovB, and the policies \u21e1z , but we do not reuse the unsupervised dataset used for FB training. To illustrate how FB can accommodate different IL principles, we present the methods in loose groups by the underlying IL principle."
        },
        {
            "heading": "4.1 BEHAVIORAL CLONING",
            "text": "In case actions are available in the expert trajectories, we can directly implement the behavioral cloning principle using the policies (\u21e1z)z returned by the FB model. Each policy \u21e1z defines a probability distribution on state-action sequences given the initial state s0, namely Pr(a0, s1, a1, . . . |s0,\u21e1z) = Q t 0 \u21e1z(at|st)P (st+1|st, at). We look for the \u21e1z for which the expert trajectories are most likely, by minimizing the loss\nLBC(z) := E\u2327 ln Pr((a0, s1, a1, . . . |s0,\u21e1z) = E\u2327 X\nt\nln\u21e1z(at|st) + cst, (7)\nwhere the constant absorbs the environment transition probabilities P (dst+1|st, at), which do not depend on z. Since we have access to \u21e1z(a|s), this can be optimized over z given the expert trajectories, leading to the behavior cloning-FB (BCFB) approach.\n3We give each expert trajectory the same weight in \u21e2e independently of its length, so \u21e2e corresponds to first sampling a trajectory, then sampling a state in that trajectory.\nSince the FB policies (\u21e1z)z are trained to be approximately optimal for some reward, we expect FB (and BFMs in general) to provide a convenient \u201cbias\u201d to identify policies, instead of performing BC among the set of all (optimal or not) policies."
        },
        {
            "heading": "4.2 REWARD-BASED IMITATION LEARNING",
            "text": "Existing reward-based IL methods require running RL algorithms to optimize an imitation policy based on a reward function specifically built to mimic the expert\u2019s behavior. Leveraging FB models, we can avoid solving an RL problem at test time, and directly obtain the imitation policy via a simple forward pass of the B model. Indeed, as mentioned in Sec. 3, FB models can recover a (near-optimal) policy for any reward function r by setting z = Es\u21e0\u21e2[r(s)B(s)]. Depending on the specific reward function, we obtain the following algorithms to estimate a z, after which we just use \u21e1z .\nFirst, consider the case of r(\u00b7) = \u21e2e(\u00b7)/\u21e2(\u00b7) in (Kim et al., 2022b;a; Ma et al., 2022). This yields z = Es\u21e0\u21e2[r(s)B(s)] = E\u21e2e [B] = E\u2327 h 1\n`(\u2327) P t 0 B(st+1) i (8)\nwhich amounts to using the FB formula (4) for z by just putting a reward at every state visited by the expert. We refer to this as empirical reward via FB (ERFB).\nSimilarly, the reward r(\u00b7) = \u21e2e(\u00b7)/(\u21e2(\u00b7) + \u21e2e(\u00b7)) used in (Reddy et al., 2020; Zolna et al., 2020) leads to regularized empirical reward via FB (RERFB), derived from (6) in App. A.1:\nz = Cov(B) \u21e3 Cov(B) + Es\u21e0\u21e2e [B(s)B(s)>] \u2318 1 E\u21e2e [B]. (9)\nEven though these reward functions are defined via the distribution \u21e2 of the unsupervised dataset, this can be instantiated using only the pre-trained FB model, with no access to the unsupervised dataset, and no need to train a discriminator.\nNote that (8) and (9) are independent of the order of states in the expert trajectory. This was not a problem in our setup, because the states themselves carry dynamical information (speed variables). If this proves limiting in some environment, this can easily be circumvented by training successor measures over visited transitions (st, st+1) rather than just states st+1, namely, training the FB model with B(st, st+1). A similar trick is applied, e.g., in (Zhu et al., 2020; Kim et al., 2022a)."
        },
        {
            "heading": "4.3 DISTRIBUTION MATCHING AND FEATURE MATCHING",
            "text": "Apprenticeship learning and distribution matching are popular ways to provide a formal definition of IL as the problem of imitating the expert\u2019s visited states. We take a unified perspective on these two categories and derive several FB-IL methods starting from the saddle-point formulation of IL common to many AL and DM methods. Let \u21e20 be an arbitrary initial distribution over S. For any reward r and policy \u21e1, the expected discounted cumulated return of \u21e1 is equal to Es0\u21e0\u21e20hM\u21e1(s0), ri by definition of M\u21e1 . Consequently, the AL criterion of minimizing the worst-case performance gap between \u21e1 and the expert can be seen as a measure of divergence between successor measures: inf \u21e1 sup r2R Es0\u21e0\u21e20 [hM\u21e1e(s0), ri hM\u21e1(s0), ri] = inf\u21e1 kEs0\u21e0\u21e20 M \u21e1e(s0) Es0\u21e0\u21e20 M\u21e1(s0)kR? (10)\nwhere R is any class of reward functions, and k\u00b7kR? the resulting dual seminorm. Since FB directly models M\u21e1 , it can directly tackle (10) as finding the policy \u21e1z that minimizes the loss\nL\u0304R?(z) := kEs0\u21e0\u21e20 M\u21e1z (s0) Es0\u21e0\u21e20 M\u21e1e(s0)k 2 R? . (11)\nIn practice, instead of (11), we consider the loss LR?(z) := Es0\u21e0\u21e2e kM\u21e1z (s0) M\u21e1e(s0)k 2 R? . (12) This is a stricter criterion than (11) as it requires the successor measure of the imitation policy and the expert policy to be similar for any s observed along expert trajectories. This avoids undesirable effects from averaging successor measures over \u21e20, which may \u201cerase\u201d too much information about the policy (e.g., take S = {s1, s2} where one policy swaps s1 and s2 and the other policy does nothing: on average over the starting point, the two policies have the same occupation measure). This increases robustness in our experiments (see App. E.6).\nWe can derive a wide range of algorithms depending on the choice of R, how we estimate M\u21e1e from expert demonstrations, and how we leverage FB models to estimate M\u21e1z . For instance, our algorithms can be extended to the KL divergence between the distributions (App. A.5).\nSuccessor feature matching. A popular choice for R is to consider rewards linear in a given feature basis (Abbeel & Ng, 2004). Here we can leverage the FB property of estimating optimal policies for rewards in the linear span of B (Touati et al., 2023). Taking RB := {r = w>B, w 2 Rd, kwk2  1} in (10) yields the seminorm kMkB\u21e4 := supr2RB R r(s)M(ds) = R B(s)M(ds) 2 and the loss\nLB?(z) := Es0\u21e0\u21e2e R B(s)M\u21e1z (s0, ds) R B(s)M\u21e1e(s0, ds) 2 2\n(13)\nnamely, the averaged features B of states visited under \u21e1z and \u21e1e should match. This can be computed by using the FB model for M\u21e1z and the expert trajectories for M\u21e1e , as follows. Theorem 2. Assume that the FB successor feature property (5) holds. Then the loss (13) satisfies\nLB?(z) = Est\u21e0\u21e2e E  (CovB)F (st, z) P k 0 k B(st+k+1) 2\n2 | st,\u21e1e + cst. (14)\nThis can be estimated by sampling a segment (st, st+1, . . .) starting at a random time t on an expert trajectory. Then we can perform gradient descent over z. We refer to this method as FMFB.\nDistribution matching. If R is restricted to the span of some features, we only get a seminorm on successor measures (any information not in the features is lost). Instead, one can take R = L2(\u21e2), which provides a full norm kM\u21e1e M\u21e1kL2(\u21e2)? on visited state distributions: this matches state distributions instead of features. This can be instantiated with FB (App. A.3), but the final loss is very similar to (67), as FB neglects features outside of B anyway. We refer to this method as DMFB.\nBellman residual minimization for distribution matching. An alternative approach is to identify the best imitation policy or its stationary distribution via the Bellman equations they satisfy. This is to distribution matching what TD is to direct Monte Carlo estimation of Q-functions.\nThe successor measure of a policy \u21e1 satisfies the measure-valued Bellman equation M\u21e1(st, ds0) = P\n\u21e1(st, ds0)+ R st+1 P \u21e1(st, dst+1)M\u21e1(st+1, ds0), or more compactly M\u21e1 = P\u21e1+ P\u21e1M\u21e1 (Blier et al., 2021). So the successor measure of the expert policy satisfies M\u21e1e = P\u21e1e + P\u21e1eM\u21e1e . Therefore, if we want to find a policy \u21e1z that behaves like \u21e1e, M\u21e1z should approximately satisfy the Bellman equation for P\u21e1e , namely, M\u21e1z \u21e1 P\u21e1e + P\u21e1eM\u21e1z . Thus, we can look for a policy \u21e1z whose Bellman gaps for \u21e1e are small. This leads to the loss\nLR?Bell(z) := M\u21e1z P\u21e1e P\u21e1eM\u0304\u21e1z 2 R? (15)\nwhere the bar above M on the right denotes a stop-grad operator, as usual for deep Q-learning.\nThe method we call BBELLFB uses the seminorm k\u00b7kB? in (15). This amounts to minimizing Bellman gaps of Q-functions for all rewards linearly spanned by B. With the FB model, the loss (15) with this norm takes a tractable form allowing for gradient descent over z (App., Thm. 4):\nLB?Bell(z) = Est\u21e0\u21e2e,st+1\u21e0P\u21e1e (\u00b7|st) \u21e5 2F (st, z)>(CovB)B(st+1)\n+ (F (st, z) F\u0304 (st+1, z))>(CovB)2(F (st, z) F\u0304 (st+1, z)) \u21e4 + cst. (16)\nThe norm from R = L2(\u21e2) in (15) yields a loss similar to the one used during FB training (indeed, FB is trained via a similar Bellman equation with \u21e1z instead of \u21e1e). The final loss only differs from (16) by CovB factors, so we report it in App. A.4 (Thm. 5). We call this method FBLOSSFB.\nRelationship between IL principles: loss bounds. Any method that provides a policy close to \u21e1e will provide state distributions close to that of \u21e1e as a result, so we expect a relationship between the losses from different approaches. Indeed, the Bellman gap loss bounds the distribution matching loss (12), and the BC loss bounds the KL version of (12). This is formalized in Thms. 7 and 8 (App. A.7)."
        },
        {
            "heading": "4.4 IMITATING NON-STATIONARY BEHAVIORS: GOAL-BASED IMITATION",
            "text": "While most IL methods are designed to imitate stationary behaviors, we can leverage FB models to imitate non-stationary behaviors. Consider the case where only a single expert demonstration \u2327 is available. At each time step t, we can use the FB method to reach a state st+k slightly ahead of\nst in the expert trajectory, where k 0 is a small, fixed integer. Namely, we place a single reward at st+k, use the FB formula (4) to obtain the zt corresponding to this reward, zt := B(st+k), and use the policy \u21e1zt . We call this method GOALFB. This is related to settings such as tracking (e.g., Wagener et al., 2023; Winkler et al., 2022), waypoint imitation (e.g., Carroll et al., 2022; Chang & Gupta, 2023; Shi et al., 2023), or goal-based IL (e.g., Liu et al., 2022; Reuss et al., 2023).\nGOALFB leverages the possibility to change the reward in real time with FB. A clear advantage is its ability to reproduce behaviors that do not correspond to optimizing a (Markovian) reward function, such as cycling over some states, or non-stationary behaviors. GOALFB may have advantages even in the stationary case, as it may mitigate approximation errors from the policies or the representation of M\u21e1: by selecting a time-varying z, the policy can adapt over time and avoid deviations in the execution of long behaviors through a stationary policy. However, goal-based IL is limited to copying one single expert trajectory, by reproducing the same state sequence. The behavior cannot necessarily be extended past the end of the expert trajectory, and no reusable policy is extracted."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we evaluate FB-IL against the objectives stated in the introduction: Property 2. We verify if an FB model pre-trained on one specific environment is able to imitate a wide range of tasks with only access to few demonstrations and without solving any RL problem. Property 3. We assess the generality of FB-IL by considering a variety of imitation learning principles and settings.\nProtocol and baselines. We evaluate IL methods on 21 tasks in 4 domains (Maze, Walker, Cheetah, Quadruped) from (Touati et al., 2023). We use the standard reward-based evaluation protocol for IL. For each task, we train expert policies using TD3 (Fujimoto et al., 2018) on a task-specific reward function (Tassa et al., 2018a). We use the expert policies to generate 200 trajectories for each task to be used for IL. In our first series of experiments, the IL algorithms are provided with a single expert demonstration (see App. E.4 for the effect of additional demonstrations). Each experiment (i.e., pair algorithm and task) was repeated with 20 random seeds. We report the cumulated reward achieved by the IL policy, computed using the ground-truth task-specific reward and averaged over 1000 episodes starting from the same initial distribution used to collect the expert demonstrations.\nFor each environment, we train an FB model using only unsupervised samples generated using RND (Burda et al., 2019). We repeat the FB pre-training 10 times, and report performance averaged over the resulting models (variance is reported in App. E.3). For FB-IL methods that require a gradient descent over z (BCFB, BBELLFB, and FMFB), we use warm-start with z0 = ERFB({\u2327e}) (8), which can be computed with forward passes on B only. GOALFB is run with a lookahead window k = 10.\nFirst (Section 5.1), we compare FB-IL to standard offline IL algorithms trained on each specific imitation task, using the same unsupervised and expert samples as FB-IL. For behavioral cloning approaches, we use vanilla BC. For reward-based IL, we include SQIL (Reddy et al., 2020) (which is originally online but can easily be adapted offline; SQIL balances sampling in the update step and runs SAC); TD3-IL (where we merge all samples in the replay buffer and use TD3 instead of SAC); ORIL (Zolna et al., 2020) from state-action demonstrations and only state; and OTR (Luo et al., 2023) using TD3 as the offline RL subroutine. For AL and DM IL, we use DEMODICE (Kim et al., 2022b), and IQLEARN (Garg et al., 2021). See App. B for details.\nNext (Section 5.2), we also include alternative behavior foundation models beyond FB, pre-trained for each environment on the same unsupervised samples as FB. GOAL-TD3 pre-trains goalconditioned policies \u21e1(a|s, g) on the unsupervised dataset using TD3 with Hindsight Experience Replay (Andrychowicz et al., 2017). At test time, it can implement goal-based IL, i.e., at each time step t it selects the policy \u21e1(at|st, set+k) where the goal set+k corresponds to a state k steps ahead in the expert trajectory. (Despite the simplicity, we did not find this algorithm proposed in the literature.) Next, GOAL-GPT (Liu et al., 2022) pre-trains a goal-conditioned, transformer-based auto-regressive policy \u21e1(at|(st, g), (st 1, g), . . . (st h+1, g); g = st+k) to predict the next action based on last h states and the state k steps in the future as the goal of the policy. MASKDP (Liu et al., 2022) uses a bidirectional transformer to reconstruct trajectories with randomly masked states and actions. Both models can be used to perform goal-based IL. We adapt DIAYN (Eysenbach et al., 2018) to pre-train a set of policies (\u21e1z) with z 2 Rd and a skill decoder ' : S ! Rd predicting which policy is more likely to reach a specific state. (This requires online interaction during pre-training.) It can be used to\nimplement behavioral cloning as in (7), a method similar to ERFB, and goal-based IL by selecting zt = '(st+k). See App. C for extra details.\nAdditional results. App. E.1 contains detailed results with additional baselines and FB-IL variants. App. E.2 ablates over our warm-start strategy for optimization-based FB-IL methods. App. E.4 studies the influence of the number of expert trajectories, with FB methods being the least sensitive, and BC methods the most. App E.5 tests the methods under a shift between the distribution of the initial states for imitation at test time and the one of expert trajectories: the overall picture is largely unchanged from Fig. 1, although the slight lead of goal-based methods disappears. App. E.3 shows that performance is not very sensitive to variations of the pretrained FB foundation model (estimated across 10 random seeds for FB training), thus confirming robustness of the overall approach."
        },
        {
            "heading": "5.1 COMPARISON TO OFFLINE IL BASELINES",
            "text": "Fig. 3 compares the performance of FB-IL methods and offline baselines grouped by IL principle. For ease of presentation, we report the performance averaged over tasks of each environment. Overall, FBIL methods perform on-par or better than each of the baselines implementing the same IL principle, consistently across domains and IL principle. In addition, FB-IL is able to recover the imitation policy in few seconds, almost three orders of magnitude faster than the baselines, that need to be re-trained for each expert demonstration (Tab. 2). This confirms that FB models are effective BFMs for solving a wide range of imitation learning tasks with few demonstrations and minimal compute.\nAs expected, BC baselines perform poorly with only one expert trajectory. BCFB has a much stronger performance, confirming that the set (\u21e1z) contains good imitation policies for a large majority of tasks and that they can be recovered by behavioral cloning from even a single demonstration.\nReward-based FB-IL methods \u2013ERFB (8), RERFB (9)\u2013 achieve consistent performance across all environments and perform on par or even better than the baselines sharing the same implicit reward function. This shows that FB models are effective at recovering near-optimal policies from rewards. On the other hand, reward-based IL offline baselines display a significant variance in their performance across environment (e.g., ORIL for state-action completely fails in maze tasks). The baselines derived from distribution matching and apprenticeship learning perform poorly in almost all the domains and tasks. This may be because they implement conservative offline RL algorithms that\nare strongly biased towards the unsupervised data and fail at imitating the expert demonstrations.4 On the other hand, FB-IL variants achieve good results (about 4 times the performance of DICE) in all domains except maze where they lag behind other FB-IL methods. In general, this shows that FB models are effective in implementing different IL principles even when offline baselines struggle."
        },
        {
            "heading": "5.2 COMPARISON TO OTHER BFM METHODS",
            "text": "The BFM methods reported in Fig. 3 display a trade-off between generality and performance. DIAYN pre-trained policies and discriminator can be used to implement a wide range of imitation learning principles (except for distribution matching), but its performance does not match the corresponding top offline baselines and it is worse than FB-IL across all domains. Methods based on maskedtrajectory models can implement a goal-based reduction of IL and work better than DIAYN. Finally, GOAL-TD3 is performing best among the other BFM methods and it is close second w.r.t. GOALFB. Nonetheless, as discussed in Sect. 4.4, all goal-based reduction methods are more limited in their applicability, since they can only use a single expert demonstration, cannot generalize beyond the expert trajectory, and do not produce a policy reusable over the whole space."
        },
        {
            "heading": "5.3 WAYPOINT IMITATION LEARNING",
            "text": "We consider non-realizable and non-stationary experts by generating demonstrations as the concatenation of \u201cyoga poses\u201d from (Mendonca et al., 2021), implicitly assuming that the expert policy can instantaneously switch between any two poses. We keep each pose fixed for 100 steps and generate trajectories of 1000 steps. In this case, no imitation policy can perfectly reproduce the sequence of poses and only goal-based IL algorithms can be applied, since all other IL methods assume stationary expert policies. We evalute the same pre-trained models used in the previous section.\nFig. 4 shows that GOALFB matches the performance of GOAL-TD3 and outperforms GOAL-GPT. This confirms that even in this specific case, FB-IL is competitive with other BFM models that are specialized (and limited) to goal-reaching tasks, whereas the same pre-trained FB model can be used to implement a wide range of imitation learning principles. GOAL-GPT\u2019s poor performance may be because the algorithm tries to reproduce trajectories in the training dataset rather than learning the optimal way to reach goals. We refer to App. F for a qualitative evaluation of the imitating behaviors."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "Behavior foundation models offer a new alternative for imitation learning, reducing by orders of magnitude the time needed to produce an imitation policy from new task demonstrations. This comes at the cost of pretraining an environment-specific (but task-agnostic) foundation model. BFMs can be used concurrently with a number of imitation learning design principles, and reach state-of-the-art performance when evaluated for the ground-truth task reward. One theoretical limitation is that, due to imperfections in the underlying BFM, one may not recover optimal performance even with infinite expert demonstrations. This can be mitigated by increasing the BFM capacity, by improving the training data, or by fine-tuning the BFM at test-time, which we leave to future work.\n4In App. G we confirm this intuition by showing that the baselines in this category achieve much better performance when the unsupervised dataset contains expert samples (e.g., D4RL data). Unfortunately, this requires curating the dataset for each expert and it would not allow solving multiple tasks in the same environment."
        }
    ],
    "year": 2023
}