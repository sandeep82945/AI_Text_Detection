{
    "abstractText": "This paper presents a new approach and algorithm for solving a class of constrained Bi-Level Optimization (BLO) problems in which the lower-level problem involves constraints coupling both upper-level and lower-level variables. Such problems have recently gained significant attention due to their broad applicability in machine learning. However, conventional gradient-based methods unavoidably rely on computationally intensive calculations related to the Hessian matrix. To address this challenge, we devise a smooth proximal Lagrangian value function to handle the constrained lower-level problem. Utilizing this construct, we introduce a single-level reformulation for constrained BLOs that transforms the original BLO problem into an equivalent optimization problem with smooth constraints. Enabled by this reformulation, we develop a Hessian-free gradient-based algorithm\u2014termed proximal Lagrangian Value function-based Hessian-free Bilevel Algorithm (LV-HBA)\u2014that is straightforward to implement in a single loop manner. Consequently, LV-HBA is especially well-suited for machine learning applications. Furthermore, we offer non-asymptotic convergence analysis for LVHBA, eliminating the need for traditional strong convexity assumptions for the lower-level problem while also being capable of accommodating non-singleton scenarios. Empirical results substantiate the algorithm\u2019s superior practical performance.",
    "authors": [
        {
            "affiliations": [],
            "name": "HESSIAN-FREE ALGORITHM"
        },
        {
            "affiliations": [],
            "name": "Wei Yao"
        },
        {
            "affiliations": [],
            "name": "Chengming YU"
        },
        {
            "affiliations": [],
            "name": "Shangzhi Zeng"
        },
        {
            "affiliations": [],
            "name": "Jin Zhang"
        }
    ],
    "id": "SP:f43e8175244b2677ec87a5e116733917a6462375",
    "references": [
        {
            "authors": [
                "Nazanin Abolfazli",
                "Ruichen Jiang",
                "Aryan Mokhtari",
                "Erfan Yazdandoost Hamedani"
            ],
            "title": "An inexact conditional gradient method for constrained bilevel optimization",
            "venue": "arXiv preprint arXiv:2306.02429,",
            "year": 2023
        },
        {
            "authors": [
                "Roberto Andreani",
                "Jos\u00e9 Mario Mart\u0131\u0301nez",
                "Benar Fux Svaiter"
            ],
            "title": "A new sequential optimality condition for constrained optimization and algorithmic consequences",
            "venue": "SIAM Journal on Optimization,",
            "year": 2010
        },
        {
            "authors": [
                "Michael Arbel",
                "Julien Mairal"
            ],
            "title": "Amortized implicit differentiation for stochastic bilevel optimization",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Michael Arbel",
                "Julien Mairal"
            ],
            "title": "Non-convex bilevel games with critical point selection maps",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "H\u00e9dy Attouch",
                "Roger J-B Wets"
            ],
            "title": "A convergence theory for saddle functions",
            "venue": "Transactions of the American Mathematical Society,",
            "year": 1983
        },
        {
            "authors": [
                "Heinz H. Bauschke",
                "Patrick L. Combettes"
            ],
            "title": "Convex analysis and monotone operator theory in Hilbert spaces",
            "venue": "CMS Books in Mathematics,",
            "year": 2011
        },
        {
            "authors": [
                "Amir Beck"
            ],
            "title": "First-order methods in optimization",
            "year": 2017
        },
        {
            "authors": [
                "J Fr\u00e9d\u00e9ric Bonnans",
                "Alexander Shapiro"
            ],
            "title": "Perturbation analysis of optimization problems",
            "venue": "Springer Science & Business Media,",
            "year": 2013
        },
        {
            "authors": [
                "Lesi Chen",
                "Yaohua Ma",
                "Jingzhao Zhang"
            ],
            "title": "Near-optimal fully first-order algorithms for finding stationary points in bilevel optimization",
            "venue": "arXiv preprint arXiv:2306.14853,",
            "year": 2023
        },
        {
            "authors": [
                "Tianyi Chen",
                "Yuejiao Sun",
                "Wotao Yin"
            ],
            "title": "Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Tianyi Chen",
                "Yuejiao Sun",
                "Quan Xiao",
                "Wotao Yin"
            ],
            "title": "A single-timescale method for stochastic bilevel optimization",
            "venue": "In AISTATS,",
            "year": 2022
        },
        {
            "authors": [
                "Xuxing Chen",
                "Krishnakumar Balasubramanian",
                "Saeed Ghadimi"
            ],
            "title": "Stochastic nested compositional bi-level optimization for robust feature learning",
            "venue": "arXiv preprint arXiv:2307.05384,",
            "year": 2023
        },
        {
            "authors": [
                "Ziyi Chen",
                "Bhavya Kailkhura",
                "Yi Zhou"
            ],
            "title": "A fast and convergent proximal algorithm for regularized nonconvex and nonsmooth bi-level optimization",
            "venue": "arXiv preprint arXiv:2203.16615,",
            "year": 2022
        },
        {
            "authors": [
                "Mathieu Dagr\u00e9ou",
                "Pierre Ablin",
                "Samuel Vaiter",
                "Thomas Moreau"
            ],
            "title": "A framework for bilevel optimization that enables stochastic and global variance reduction algorithms",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Stephan Dempe",
                "Alain B Zemkoho"
            ],
            "title": "The bilevel programming problem: reformulations, constraint qualifications and optimality conditions",
            "venue": "Mathematical Programming,",
            "year": 2013
        },
        {
            "authors": [
                "Thomas Elsken",
                "Benedikt Staffler",
                "Jan Hendrik Metzen",
                "Frank Hutter"
            ],
            "title": "Meta-learning of neural architectures for few-shot learning",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Alireza Fallah",
                "Aryan Mokhtari",
                "Asuman Ozdaglar"
            ],
            "title": "Personalized federated learning: A metalearning approach",
            "venue": "arXiv preprint arXiv:2002.07948,",
            "year": 2020
        },
        {
            "authors": [
                "Luca Franceschi",
                "Michele Donini",
                "Paolo Frasconi",
                "Massimiliano Pontil"
            ],
            "title": "Forward and reverse gradient-based hyperparameter optimization",
            "venue": "In ICML,",
            "year": 2017
        },
        {
            "authors": [
                "Luca Franceschi",
                "Paolo Frasconi",
                "Saverio Salzo",
                "Riccardo Grazzi",
                "Massimiliano Pontil"
            ],
            "title": "Bilevel programming for hyperparameter optimization and meta-learning",
            "venue": "In ICML,",
            "year": 2018
        },
        {
            "authors": [
                "Lucy L Gao",
                "Jane J. Ye",
                "Haian Yin",
                "Shangzhi Zeng",
                "Jin Zhang"
            ],
            "title": "Value function based differenceof-convex algorithm for bilevel hyperparameter selection problems",
            "year": 2022
        },
        {
            "authors": [
                "Lucy L Gao",
                "Jane J. Ye",
                "Haian Yin",
                "Shangzhi Zeng",
                "Jin Zhang"
            ],
            "title": "Moreau envelope based difference-of-weakly-convex reformulation and algorithm for bilevel programs",
            "venue": "arXiv preprint arXiv:2306.16761,",
            "year": 2023
        },
        {
            "authors": [
                "Saeed Ghadimi",
                "Mengdi Wang"
            ],
            "title": "Approximation methods for bilevel programming",
            "venue": "arXiv preprint arXiv:1802.02246,",
            "year": 2018
        },
        {
            "authors": [
                "Riccardo Grazzi",
                "Luca Franceschi",
                "Massimiliano Pontil",
                "Saverio Salzo"
            ],
            "title": "On the iteration complexity of hypergradient computation",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Isabelle Guyon",
                "Steve Gunn",
                "Asa Ben-Hur",
                "Gideon Dror"
            ],
            "title": "Result analysis of the NIPS 2003 feature selection challenge",
            "venue": "In NeurIPS,",
            "year": 2004
        },
        {
            "authors": [
                "Elias S Helou",
                "Sandra A Santos",
                "Lucas EA Sim\u00f5es"
            ],
            "title": "A primal nonsmooth reformulation for bilevel optimization problems",
            "venue": "Mathematical Programming,",
            "year": 2023
        },
        {
            "authors": [
                "Tin Kam Ho",
                "Eugene M. Kleinberg"
            ],
            "title": "Building projectable classifiers of arbitrary complexity",
            "year": 1996
        },
        {
            "authors": [
                "Mingyi Hong",
                "Hoi-To Wai",
                "Zhaoran Wang",
                "Zhuoran Yang"
            ],
            "title": "A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic",
            "venue": "SIAM Journal on Optimization,",
            "year": 2023
        },
        {
            "authors": [
                "Feihu Huang"
            ],
            "title": "On momentum-based gradient methods for bilevel optimization with nonconvex lower-level",
            "venue": "arXiv preprint arXiv:2303.03944,",
            "year": 2023
        },
        {
            "authors": [
                "Feihu Huang",
                "Junyi Li",
                "Shangqian Gao",
                "Heng Huang"
            ],
            "title": "Enhanced bilevel optimization via bregman distance",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiyi Ji",
                "Jason D. Lee",
                "Yingbin Liang",
                "H. Vincent Poor"
            ],
            "title": "Convergence of meta-learning with task-specific adaptation over partial parameters",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiyi Ji",
                "Junjie Yang",
                "Yingbin Liang"
            ],
            "title": "Bilevel optimization: Convergence analysis and enhanced design",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiyi Ji",
                "Mingrui Liu",
                "Yingbin Liang",
                "Lei Ying"
            ],
            "title": "Will bilevel optimizers benefit from loops",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Prashant Khanduri",
                "Ioannis Tsaknakis",
                "Yihua Zhang",
                "Jia Liu",
                "Sijia Liu",
                "Jiawei Zhang",
                "Mingyi Hong"
            ],
            "title": "Linearly constrained bilevel optimization: A smoothed implicit gradient approach",
            "venue": "In ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Youngdae Kim",
                "Sven Leyffer",
                "Todd Munson"
            ],
            "title": "MPEC methods for bilevel optimization problems",
            "venue": "In Bilevel Optimization,",
            "year": 2020
        },
        {
            "authors": [
                "Ganesh Ramachandra Kini",
                "Orestis Paraskevas",
                "Samet Oymak",
                "Christos Thrampoulidis"
            ],
            "title": "Labelimbalanced and group-sensitive classification under overparameterization",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Jeongyeol Kwon",
                "Dohyun Kwon",
                "Stephen Wright",
                "Robert D Nowak"
            ],
            "title": "A fully first-order method for stochastic bilevel optimization",
            "venue": "In ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Jeongyeol Kwon",
                "Dohyun Kwon",
                "Steve Wright",
                "Robert Nowak"
            ],
            "title": "On penalty methods for nonconvex bilevel optimization and first-order stochastic approximation",
            "venue": "arXiv preprint arXiv:2309.01753,",
            "year": 2023
        },
        {
            "authors": [
                "Junyi Li",
                "Bin Gu",
                "Heng Huang"
            ],
            "title": "Improved bilevel model: Fast and optimal algorithm with theoretical guarantee",
            "venue": "arXiv preprint arXiv:2009.00690,",
            "year": 2020
        },
        {
            "authors": [
                "Mingchen Li",
                "Xuechen Zhang",
                "Christos Thrampoulidis",
                "Jiasi Chen",
                "Samet Oymak"
            ],
            "title": "Autobalance: Optimized loss functions for imbalanced data",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Yu-Wei Li",
                "Gui-Hua Lin",
                "Xide Zhu"
            ],
            "title": "Solving bilevel programs based on lower-level Mond-Weir duality",
            "venue": "INFORMS Journal on Computing,",
            "year": 2024
        },
        {
            "authors": [
                "Yuwei Li",
                "Gui-Hua Lin",
                "Jin Zhang",
                "Xide Zhu"
            ],
            "title": "A novel approach for bilevel programs based on Wolfe duality",
            "venue": "arXiv preprint arXiv:2302.06838,",
            "year": 2023
        },
        {
            "authors": [
                "Hanwen Liang",
                "Shifeng Zhang",
                "Jiacheng Sun",
                "Xingqiu He",
                "Weiran Huang",
                "Kechen Zhuang",
                "Zhenguo Li"
            ],
            "title": "DARTS+: Improved differentiable architecture search with early stopping",
            "year": 1909
        },
        {
            "authors": [
                "Gui-Hua Lin",
                "Mengwei Xu",
                "Jane J Ye"
            ],
            "title": "On solving simple bilevel programs with a nonconvex lower level program",
            "venue": "Mathematical Programming,",
            "year": 2014
        },
        {
            "authors": [
                "Hanxiao Liu",
                "Karen Simonyan",
                "Yiming Yang"
            ],
            "title": "DARTS: Differentiable architecture search",
            "venue": "In ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Risheng Liu",
                "Pan Mu",
                "Xiaoming Yuan",
                "Shangzhi Zeng",
                "Jin Zhang"
            ],
            "title": "A generic first-order algorithmic framework for bi-level programming beyond lower-level singleton",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Risheng Liu",
                "Jiaxin Gao",
                "Jin Zhang",
                "Deyu Meng",
                "Zhouchen Lin"
            ],
            "title": "Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Risheng Liu",
                "Xuan Liu",
                "Xiaoming Yuan",
                "Shangzhi Zeng",
                "Jin Zhang"
            ],
            "title": "A value-function-based interior-point method for non-convex bi-level optimization",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Risheng Liu",
                "Yaohua Liu",
                "Shangzhi Zeng",
                "Jin Zhang"
            ],
            "title": "Towards gradient-based bilevel optimization with non-convex followers and beyond",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Risheng Liu",
                "Pan Mu",
                "Xiaoming Yuan",
                "Shangzhi Zeng",
                "Jin Zhang"
            ],
            "title": "A general descent aggregation framework for gradient-based bi-level optimization",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Risheng Liu",
                "Xuan Liu",
                "Shangzhi Zeng",
                "Jin Zhang",
                "Yixuan Zhang"
            ],
            "title": "Value-function-based sequential minimization for bi-level optimization",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Risheng Liu",
                "Yaohua Liu",
                "Wei Yao",
                "Shangzhi Zeng",
                "Jin Zhang"
            ],
            "title": "Averaged method of multipliers for bi-level optimization without lower-level strong convexity",
            "venue": "In ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Lorraine",
                "Paul Vicol",
                "David Duvenaud"
            ],
            "title": "Optimizing millions of hyperparameters by implicit differentiation",
            "venue": "In AISTATS,",
            "year": 2020
        },
        {
            "authors": [
                "Songtao Lu"
            ],
            "title": "SLM: A smoothed first-order lagrangian method for structured constrained nonconvex optimization",
            "venue": "In NeurIPS,",
            "year": 2024
        },
        {
            "authors": [
                "Zhaosong Lu",
                "Sanyou Mei"
            ],
            "title": "First-order penalty methods for bilevel optimization",
            "venue": "arXiv preprint arXiv:2301.01716,",
            "year": 2023
        },
        {
            "authors": [
                "Zhi-Quan Luo",
                "Jong-Shi Pang",
                "Daniel Ralph"
            ],
            "title": "Mathematical programs with equilibrium constraints",
            "year": 1996
        },
        {
            "authors": [
                "Matthew Mackay",
                "Paul Vicol",
                "Jonathan Lorraine",
                "David Duvenaud",
                "Roger Grosse"
            ],
            "title": "Self-tuning networks: Bilevel optimization of hyperparameters using structured best-response functions",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Dougal Maclaurin",
                "David Duvenaud",
                "Ryan Adams"
            ],
            "title": "Gradient-based hyperparameter optimization through reversible learning",
            "venue": "In ICML,",
            "year": 2015
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "In ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Aur\u00e9lien Ouattara",
                "Anil Aswani"
            ],
            "title": "Duality approach to bilevel programs with a convex lower level",
            "venue": "Annual American Control Conference (ACC),",
            "year": 2018
        },
        {
            "authors": [
                "Ji\u0159\u0131\u0301 V Outrata"
            ],
            "title": "On the numerical solution of a class of Stackelberg problems",
            "venue": "Zeitschrift fu\u0308r Operations Research,",
            "year": 1990
        },
        {
            "authors": [
                "Fabian Pedregosa"
            ],
            "title": "Hyperparameter optimization with approximate gradient",
            "venue": "In ICML,",
            "year": 2016
        },
        {
            "authors": [
                "Aravind Rajeswaran",
                "Chelsea Finn",
                "Sham M Kakade",
                "Sergey Levine"
            ],
            "title": "Meta-learning with implicit gradients",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "R Tyrrell Rockafellar"
            ],
            "title": "Conjugate duality and optimization",
            "year": 1974
        },
        {
            "authors": [
                "R Tyrrell Rockafellar",
                "Roger J-B Wets"
            ],
            "title": "Variational analysis, volume 317",
            "venue": "Springer Science & Business Media,",
            "year": 2009
        },
        {
            "authors": [
                "Amirreza Shaban",
                "Ching-An Cheng",
                "Nathan Hatch",
                "Byron Boots"
            ],
            "title": "Truncated back-propagation for bilevel optimization",
            "venue": "In AISTATS,",
            "year": 2019
        },
        {
            "authors": [
                "Han Shen",
                "Tianyi Chen"
            ],
            "title": "On penalty-based bilevel gradient descent method",
            "venue": "In ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Daouda Sow",
                "Kaiyi Ji",
                "Ziwei Guan",
                "Yingbin Liang"
            ],
            "title": "A primal-dual approach to bilevel optimization with multiple inner minima",
            "venue": "arXiv preprint arXiv:2203.01123,",
            "year": 2022
        },
        {
            "authors": [
                "Davoud Ataee Tarzanagh",
                "Mingchen Li",
                "Christos Thrampoulidis",
                "Samet Oymak"
            ],
            "title": "Fednest: Federated bilevel, minimax, and compositional optimization",
            "venue": "In ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Ioannis Tsaknakis",
                "Prashant Khanduri",
                "Mingyi Hong"
            ],
            "title": "An implicit gradient-type method for linearly constrained bilevel problems",
            "venue": "In ICASSP,",
            "year": 2022
        },
        {
            "authors": [
                "Ioannis Tsaknakis",
                "Prashant Khanduri",
                "Mingyi Hong"
            ],
            "title": "An implicit gradient method for constrained bilevel problems using barrier approximation",
            "venue": "In ICASSP,",
            "year": 2023
        },
        {
            "authors": [
                "Eric Wong",
                "Leslie Rice",
                "J Zico Kolter"
            ],
            "title": "Fast is better than free: Revisiting adversarial training",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Quan Xiao",
                "Songtao Lu",
                "Tianyi Chen"
            ],
            "title": "An alternating optimization method for bilevel problems under the Polyak-\u0141ojasiewicz",
            "venue": "condition. NeurIPS,",
            "year": 2023
        },
        {
            "authors": [
                "Quan Xiao",
                "Han Shen",
                "Wotao Yin",
                "Tianyi Chen"
            ],
            "title": "Alternating projected sgd for equalityconstrained bilevel optimization",
            "venue": "In ICAIS,",
            "year": 2023
        },
        {
            "authors": [
                "Siyuan Xu",
                "Minghui Zhu"
            ],
            "title": "Efficient gradient approximation method for constrained bilevel optimization",
            "venue": "In AAAI,",
            "year": 2023
        },
        {
            "authors": [
                "Haikuo Yang",
                "Luo Luo",
                "Chris Junchi Li",
                "Michael Jordan",
                "Maryam Fazel"
            ],
            "title": "Accelerating inexact hypergradient descent for bilevel optimization",
            "year": 2023
        },
        {
            "authors": [
                "Yifan Yang",
                "Peiyao Xiao",
                "Kaiyi Ji"
            ],
            "title": "SimFBO: Towards simple, flexible and communicationefficient federated bilevel learning",
            "venue": "NeurIPS,",
            "year": 2023
        },
        {
            "authors": [
                "Jane J. Ye",
                "Daoli Zhu"
            ],
            "title": "Optimality conditions for bilevel programming",
            "venue": "problems. Optimization,",
            "year": 1995
        },
        {
            "authors": [
                "Jane J. Ye",
                "Xiaoming Yuan",
                "Shangzhi Zeng",
                "Jin Zhang"
            ],
            "title": "Difference of convex algorithms for bilevel programs with applications in hyperparameter selection",
            "venue": "Mathematical Programming,",
            "year": 2023
        },
        {
            "authors": [
                "Mao Ye",
                "Bo Liu",
                "Stephen Wright",
                "Peter Stone",
                "Qiang Liu"
            ],
            "title": "Bome! bilevel optimization made easy: A simple first-order approach",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Yihua Zhang",
                "Guanhua Zhang",
                "Prashant Khanduri",
                "Mingyi Hong",
                "Shiyu Chang",
                "Sijia Liu"
            ],
            "title": "Revisiting and advancing fast adversarial training through the lens of bi-level optimization",
            "year": 2022
        },
        {
            "authors": [
                "Yihua Zhang",
                "Prashant Khanduri",
                "Ioannis Tsaknakis",
                "Yuguang Yao",
                "Mingyi Hong",
                "Sijia Liu"
            ],
            "title": "An introduction to bi-level optimization: Foundations and applications in signal processing and machine learning",
            "venue": "arXiv preprint arXiv:2308.00788,",
            "year": 2023
        },
        {
            "authors": [
                "Daniel Z\u00fcgner",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Adversarial attacks on graph neural networks via meta learning",
            "venue": "In ICLR,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In this work, we consider the constrained Bi-Level Optimization (BLO) problems with possibly coupled lower-level (LL) constraints, which is in form of,\nmin x\u2208X,y\u2208Y F (x, y) s.t. y \u2208 S(x), (1)\nwhere S(x) denotes the set of optimal solutions for the constrained LL problem,\nmin y\u2208Y f(x, y) s.t. g(x, y) \u2264 0. (2)\nHere both X \u2286 Rn and Y \u2286 Rm are closed convex sets. The upper-level (UL) objective F : X \u00d7 Y \u2192 R, the LL objective f : X \u00d7 Y \u2192 R, and the LL constraint mapping g : X \u00d7 Y \u2192 Rl are continuously differentiable functions. It is noteworthy that both LL objective f and constraint mapping g are functions of UL variable x LL variable y.\nBLO has recently emerged as a powerful tool for tackling various modern machine learning problems characterized by inherent hierarchical structures, such as hyperparameter optimization Pedregosa (2016); Franceschi et al. (2018); Mackay et al. (2019), meta learning Franceschi et al. (2018); Zu\u0308gner & Gu\u0308nnemann (2019); Rajeswaran et al. (2019); Ji et al. (2020), neural architecture search Liu et al. (2018); Liang et al. (2019); Elsken et al. (2020), to name a few. Among them, the constrained BLOs capture several important applications, including adversarial learning\n*Correspondence to Jin Zhang (zhangj9@sustech.edu.cn)\nMadry et al. (2018); Wong et al. (2019); Zhang et al. (2022), federated learning Fallah et al. (2020); Tarzanagh et al. (2022); Yang et al. (2023b), see the recent survey papers Liu et al. (2021a); Zhang et al. (2023) for more applications in machine learning and signal processing.\nOwing to their effectiveness and scalability, gradient-based algorithms have become mainstream techniques for BLO in learning and vision fields Liu et al. (2021a). While gradient-based algorithms for unconstrained BLO problems have been extensively explored in the literature Ghadimi & Wang (2018); Shaban et al. (2019); Liu et al. (2020; 2021b); Huang et al. (2022); Ji et al. (2021; 2022); Hong et al. (2023); Dagre\u0301ou et al. (2022); Ye et al. (2022); Liu et al. (2023b); Kwon et al. (2023a), research focusing on efficient methods for constrained BLO problems is quite limited. This gap is especially evident in scenarios where LL constraints couple both UL and LL variables.\nIndeed, the majority of existing works in this direction focus on particular types of constrained LL problems. For instance, recent works Tsaknakis et al. (2022); Khanduri et al. (2023) address constrained BLOs where LL problem pertains to minimizing a strongly convex objective subject to linear inequality constraints. The study Xiao et al. (2023a) considers the stochastic BLO problems with equality constraints at both upper and lower levels, while Xu & Zhu (2023) studies BLOs wherein LL problem is convex with equality and inequality constraints and presumes that LL objective is strongly convex and the constraints satisfy Linear Independence Constraint Qualification (LICQ). Notably, the methods presented in these works all employ implicit gradient-based techniques, relying on implicit gradient computation of LL solution mapping. This dependency requires both the uniqueness and smoothness of LL solution mapping, thereby limiting its applicability. Critically, implicit gradient techniques necessitate computationally intensive calculations related to LL Hessian matrix. In this context, a natural yet important question is: Can we devise Hessian-free algorithms for constrained BLOs?\nA recent affirmation to this question is provided in Liu et al. (2023a), leveraging the value function approach Ye & Zhu (1995). For constrained BLOs, value function-based methods encounter issues tied to non-differentiable constraints emerging from the value function-based reformulation. To circumvent this non-smoothness issue, Liu et al. (2023a) introduces a sequential approximation minimization strategy. Herein, quadratic regularization alongside penalty/barrier functions of LL inequality constraints are applied to smooth LL value function. Nonetheless, this approach necessitates solving a series of subproblems and lacks a non-asymptotic analysis. This leads to a practical question: Can we devise a Hessian-free algorithm in a single-loop manner for constrained BLOs?"
        },
        {
            "heading": "1.1 MAIN CONTRIBUTIONS",
            "text": "In this study, we provide an affirmative answer to the previously raised question. We first propose a single-level reformulation for constrained BLO problems by defining a proximal Lagrangian value function associated with the constrained LL problem. This function is defined as the value\nfunction of a strongly-convex-strongly-concave proximal min-max problem and exhibits continuous differentiability. As a result, our approach recasts constrained BLO problems into single-level optimization problems with smooth constraints. Drawing from this reformulation, we devise a Hessianfree gradient-based algorithm for constrained BLOs, and provide the non-asymptotic convergence analysis. Conducting such an analysis, especially given LL constraints, is non-trivial. By utilizing the strongly-convex-strongly-concave structure within the min-max problem of the proximal Lagrangian value function, we can approximate its gradient using only the first-order data from LL problem. The error in this gradient approximation remains controllable, without the need for LL objective\u2019s strong convexity. This facilitates our establishment of the non-asymptotic convergence analysis of the proposed algorithm for constrained LL problem with a merely convex LL objective.\nOur primary contributions are outlined below.\n\u2022 We introduce a novel proximal Lagrangian value function to handle constrained LL problem. By leveraging this function, we present a new single-level reformulation for constrained BLOs, converting them into equivalent single-level optimization problems with smooth constraints.\n\u2022 Drawing from our reformulation, we propose the proximal Lagrangian Value functionbased Hessian-free Bi-level Algorithm (LV-HBA) tailored for constrained BLO problems with LL constraints coupling both UL and LL variables. To our knowledge, this work is the first to develop a provably Hessian-free gradient-based algorithm for constrained BLOs in a single-loop manner. A brief summary of the comparison of LV-HBA with closely related works is provided in Table 1.\n\u2022 We rigorously establish the non-asymptotic convergence analysis of LV-HBA. Employing the proximal Lagrangian value function, we eliminate the necessity for LL objective\u2019s strong convexity, thereby accommodating merely convex LL scenarios.\n\u2022 We evaluate the efficiency of LV-HBA through numerical experiments on synthetic problems, hyperparameter optimization for SVM and federated bilevel learning. Empirical results validate the superior practical performance of LV-HBA."
        },
        {
            "heading": "1.2 RELATED WORK",
            "text": "In the section we give a brief review of some recent works that are directly related to ours. An expanded review of recent studies on BLOs is provided in Section A.2.\nReformulations for BLOs. One of the most commonly approaches for BLOs is to reformulate them as single-level problems. This can often be done in two ways Dempe & Zemkoho (2013). One is known as KKT reformulation Kim et al. (2020), which replaces LL problem with its Karush-KuhnTucker (KKT) conditions. Consequently, it unavoidably relies on first-order gradient information. As a result, gradient-based algorithms based on it also necessitate second-order gradient information. In contrast, value function reformulation does not rely on any gradient information. Benefiting from this, the majority of existing Hessian-free gradient-based algorithms for both unconstrained and constrained BLOs, are developed based on value function reformulation, see, e.g., Liu et al. (2021b; 2023a); Ye et al. (2022); Sow et al. (2022); Shen & Chen (2023); Kwon et al. (2023a); Lu & Mei (2023); Lu (2024). Recently, Gao et al. (2023) proposes a new reformulation of BLOs, using Moreau envelope of LL problem, to weaken the underlying assumption from LL full convexity in Gao et al. (2022) to weak convexity. However, Moreau envelope-based reformulation still encounters challenges related to non-differentiable constraints, arising from the reformulation itself.\nAlgorithms for Constrained BLOs. Other than the previously mentioned works that focus on LL constraints, there is a line of research dedicated to addressing the constrained UL setting, including: implicit approximation methods in Ghadimi & Wang (2018); two-timescale framework in Hong et al. (2023); single-timescale method in Chen et al. (2022a); initialization auxiliary method in Liu et al. (2021c); Bregman distance-based method in Huang et al. (2022); proximal gradient-type algorithm in Chen et al. (2022b); inexact conditional gradient method in Abolfazli et al. (2023)."
        },
        {
            "heading": "2 PROXIMAL LAGRANGIAN VALUE FUNCTION APPROACH",
            "text": "In this section, we introduce a novel single-level reformulation for constrained BLO problems, foundational to our proposed methodology. Furthermore, we describe the proposed algorithm LV-HBA. To simplify our notation, throughout this paper, for LL constraint mapping g : X \u00d7 Y \u2192 Rl and any vectors \u03bb \u2208 Rl+, z \u2208 Rl , we represent \u2211l i=1 \u03bbigi and \u2211l i=1 zigi as \u03bbg and zg, respectively.\nSimilarly, \u2211l i=1 \u03bbi\u2207ygi and \u2211l i=1 zi\u2207ygi are denoted by \u03bb\u2207yg and z\u2207yg, respectively."
        },
        {
            "heading": "2.1 REFORMULATION VIA PROXIMAL LAGRANGIAN VALUE FUNCTION",
            "text": "We start by introducing the proximal Lagrangian value function v\u03b3(x, y, z) for LL problem, drawing inspiration from Moreau envelope value function discussed in Gao et al. (2023). It is defined as:\nv\u03b3(x, y, z) := min \u03b8\u2208Y max \u03bb\u2208Rl+\n{ f(x, \u03b8) + \u03bbg(x, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u22252\n} , (3)\nwhere z \u2208 Rl, Rl+ := {\u03bb \u2208 Rl|\u03bbi \u2265 0 \u2200i}, \u03b3 := (\u03b31, \u03b32) \u2265 0 is the proximal parameter. Note that f(x, y) + \u03bbg(x, y) is the Lagrangian function of LL problem. Employing this function, we present a smooth reformulation for constrained BLO problem (1):\nmin (x,y)\u2208C,z\u22650 F (x, y) s.t. f(x, y)\u2212 v\u03b3(x, y, z) \u2264 0, (4)\nwhere C := {(x, y) \u2208 X \u00d7 Y | g(x, y) \u2264 0}. Under the convexity of LL problem and the existence of multipliers for LL problem, the equivalence between reformulation (4) and constrained BLO problem (1) is established. Notably, f(x, y) \u2212 v\u03b3(x, y, z) \u2265 0 for any (x, y, z) \u2208 C \u00d7 Rl+. Comprehensive proofs can be found in Theorem A.1 in Appendix A.3.\nTo guarantee the theoretical convergence of the proposed method, instead of directly solving reformulation (4), we consider its variant using a truncated proximal Lagrangian value function,\nv\u03b3,r(x, y, z) := min \u03b8\u2208Y max \u03bb\u2208Z\n{ f(x, \u03b8) + \u03bbg(x, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u22252\n} , (5)\nwhere Z := [0, r]l \u2286 Rl+ and r > 0. Compared with v\u03b3(x, y, z), the truncated version v\u03b3,r(x, y, z) is defined by maximizing \u03bb over a bounded set Z instead of over Rl+. And the truncated proximal Lagrangian value function gives us the following variant to reformulation (4),\nmin (x,y)\u2208C,z\u2208Z F (x, y) s.t. f(x, y)\u2212 v\u03b3,r(x, y, z) \u2264 0. (6)\nNote that f(x, y)\u2212v\u03b3,r(x, y, z) \u2265 0 for any (x, y, z) \u2208 C\u00d7Z. If r is sufficiently large, the solution of reformulation (4) can be obtained by solving variant (6). A comprehensive proof is presented in Theorem A.2 within Appendix A.3."
        },
        {
            "heading": "2.2 GRADIENT OF PROXIMAL LAGRANGIAN VALUE FUNCTION",
            "text": "An important property of v\u03b3(x, y, z) and its truncated counterpart v\u03b3,r(x, y, z) is their continuous differentiability under the setting of this study, as elucidated in Assumptions 3.2 and 3.3. Specifically, if f(x, \u00b7) and g(x, \u00b7) are convex on Y , the proximal min-max problems in (3) and (5) are both strongly-convex-strongly-concave. By invoking saddle point theorem, these problems possess unique saddle points. Moreover, with continuous differentiability for both f and g, the gradient \u2207v\u03b3,r(x, y, z) can be derived with the explicit expression as\n\u2207v\u03b3,r(x, y, z) = ( \u2207xf(x, \u03b8\u2217) + \u03bb\u2217\u2207xg(x, \u03b8\u2217),\n(y \u2212 \u03b8\u2217) \u03b31 , (\u03bb\u2217 \u2212 z) \u03b32\n) , (7)\nwhere (\u03b8\u2217, \u03bb\u2217) := (\u03b8\u2217(x, y, z), \u03bb\u2217(x, y, z)) denotes the unique saddle point for the min-max problem in (5). Similarly, for v\u03b3(x, y, z), the gradient \u2207v\u03b3(x, y, z) shares the same form as in (7), but with (\u03b8\u2217, \u03bb\u2217) corresponding to the unique saddle point of the min-max problem in (3). A detailed proof can be found in Lemma A.1 within the Appendix."
        },
        {
            "heading": "2.3 THE PROPOSED ALGORITHM",
            "text": "We introduce LV-HBA, a Hessian-free gradient-based algorithm designed for constrained BLOs.\nAt each iteration, given the current values of (xk, yk, zk, \u03b8k, \u03bbk), we initiate by executing a single gradient descent ascent (GDA) step for the proximal min-max problem described in (5), updating the variables (\u03b8, \u03bb) as follows\n(\u03b8k+1, \u03bbk+1) = ProjY\u00d7Z ( (\u03b8k, \u03bbk)\u2212 \u03b7k(dk\u03b8 , dk\u03bb) ) , (8)\nwhere ProjZ represents the Euclidean projection onto the bounded box Z, and\n(dk\u03b8 , d k \u03bb) := ( \u2207yf(xk, \u03b8k) + \u03bbk\u2207yg(xk, \u03b8k) + 1\n\u03b31 (\u03b8k \u2212 yk),\u2212g(xk, \u03b8k) + 1 \u03b32 (\u03bbk \u2212 zk)\n) . (9)\nSubsequently, we update the variables (x, y, z) as follows (xk+1, yk+1) = ProjC ( (xk, yk)\u2212 \u03b1k(dkx, dky) ) ,\nzk+1 = ProjZ ( zk \u2212 \u03b2kdkz ) ,\n(10)\nwhere the directions are defined as:\ndkx := 1\nck \u2207xF (xk, yk) +\u2207xf(xk, yk)\u2212\u2207xf(xk, \u03b8k+1)\u2212 \u03bbk+1\u2207xg(xk, \u03b8k+1),\ndky := 1\nck \u2207yF (xk, yk) +\u2207yf(xk, yk)\u2212\n1\n\u03b31 (yk \u2212 \u03b8k+1),\ndkz := \u2212 1\n\u03b32 (\u03bbk+1 \u2212 zk).\n(11)\nA comprehensive description of LV-HBA is provided in Algorithm 1.\nAlgorithm 1 proximal Lagrangian Value function-based Hessian-free Bi-level Algorithm (LV-HBA) Initialize: (x0, y0) \u2208 X\u00d7Y , z0 \u2208 Z, (\u03b80, \u03bb0) \u2208 Y \u00d7Rl+, stepsizes \u03b1k, \u03b2k, \u03b7k, proximal parameter \u03b3, penalty parameter ck;\n1: for k = 0, 1, . . . ,K \u2212 1 do 2: calculate (dk\u03b8 , d k \u03bb) as in equation (9); 3: update (\u03b8k+1, \u03bbk+1) = ProjY\u00d7Z ( (\u03b8k, \u03bbk)\u2212 \u03b7k(dk\u03b8 , dk\u03bb) ) ; 4: calculate dkx, d k y , d k z as in equation (11); 5: update (xk+1, yk+1) = ProjC ( (xk, yk)\u2212 \u03b1k(dkx, dky) ) ,\nzk+1 = ProjZ ( zk \u2212 \u03b2kdkz ) .\n6: end for\nWe provide insight into the construction of LV-HBA. The update of variables (x, y, z) in (10) can be interpreted as an inexact alternating proximal gradient step from (xk, yk, zk) concerning the following minimization problem:\nmin (x,y)\u2208C,z\u2208Z\n1\nck F (x, y) + f(x, y)\u2212 v\u03b3,r(x, y, z).\nDrawing from the gradient formula of v\u03b3,r(x, y, z) given in (7), (\u2207xf(xk, \u03b8k+1) + \u03bbk+1\u2207xg(xk, \u03b8k+1), (yk \u2212 \u03b8k+1)/\u03b31, (\u03bbk+1 \u2212 zk)/\u03b32) in (11) can be considered as approximations to \u2207v\u03b3,r(x, y, z), using (\u03b8k+1, \u03bbk+1) as a proxy to (\u03b8\u2217, \u03bb\u2217). Particularly, when the feasible sets Y and C exhibit \u201cprojection-friendly\u201d characteristics, meaning that the Euclidean projection onto them is computationally efficient, LV-HBA can be characterized as a single-loop Hessian-free gradient-based algorithm for constrained BLO problems."
        },
        {
            "heading": "3 NON-ASYMPTOTIC CONVERGENCE ANALYSIS",
            "text": "In this section, we conduct a non-asymptotic analysis for LV-HBA. We begin by outlining the basic assumptions adopted throughout this work."
        },
        {
            "heading": "3.1 GENERAL ASSUMPTIONS",
            "text": "The following assumptions formalize the smoothness property of UL objective F , and smoothness and convexity properties of LL objective f and LL constraints g. Assumption 3.1 (Upper-Level Objective). The UL objective F is LF -smooth* on X \u00d7 Y . Additionally, F is bounded below on X \u00d7 Y , i.e., F := inf(x,y)\u2208X\u00d7Y F (x, y) > \u2212\u221e. Assumption 3.2 (Lower-Level Objective). Assume that the following conditions hold:\n(i) f is convex w.r.t. LL variable y on Y for any x \u2208 X .\n(ii) f is continuously differentiable on an open set containing X\u00d7Y and is Lf -smooth on X\u00d7Y .\nGiven that f is Lf -smooth on X\u00d7Y , by leveraging the descent lemma (Beck, 2017, Lemma 5.7), it can be deduced that f is also Lf -weakly convex, i.e, f(x, y) +Lf\u2225(x, y)\u22252/2 is convex on X \u00d7 Y . Consequently, under Assumption 3.2, f is \u03c1f -weakly convex on X \u00d7 Y , with \u03c1f \u2265 0 potentially being smaller than Lf . To precisely determine the range for the step sizes of LV-HBA , we will employ the weak convexity constant of f , \u03c1f , in subsequent results. Assumption 3.3 (Lower-Level Constraints). Assume that the following conditions hold:\n(i) g(x, y) is convex and be Lg-Lipschitz continuous on X \u00d7 Y .\n(ii) g(x, y) is continuously differentiable on an open set containing X \u00d7 Y , \u2207xg(x, y) and \u2207yg(x, y) are Lg1 and Lg2 -Lipschitz continuous on X \u00d7 Y , respectively.\nOur assumptions are based solely on the first-order differentiability of the problem data. The setting of this study substantially relaxes the existing requirement for second-order differentiability in constrained BLO literature. Notably, we do not impose strong convexity on the LL objective f . As a result, our analysis encompasses LL problem non-singleton scenarios, as detailed in Table 1."
        },
        {
            "heading": "3.2 CONVERGENCE RESULTS",
            "text": "To derive the non-asymptotic convergence results of LV-HBA, we first demonstrate the decreasing property of a merit function introduced below,\nVk := \u03d5ck(x k, yk, zk) + C\u03b8\u03bb \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217r(xk, yk, zk), \u03bb\u2217r(xk, yk, zk))\u2225\u22252 , (12) where C\u03b8\u03bb := max{(Lf + CZLg1)2 + 1/(2\u03b321) + L2g, 1/\u03b322}, CZ := maxz\u2208Z \u2225z\u2225, and\n\u03d5ck(x, y, z) := 1\nck\n( F (x, y)\u2212 F ) + f(x, y)\u2212 v\u03b3,r(x, y, z). (13)\nLemma 3.1. Under Assumptions 3.1, 3.2 and 3.3, let \u03b31 \u2208 (0, 1/\u03c1f ), \u03b32 > 0, ck+1 \u2265 ck and \u03b7k \u2208 (\u03b7, \u03c1T /L2B) with \u03b7 > 0, \u03c1T := min{1/\u03b31 \u2212 \u03c1f , 1/\u03b32} and LB := max{Lf +Lg +CZLg2 + 1/\u03b31, Lg + 1/\u03b32}, then there exist constants c\u03b1, c\u03b2 > 0 such that when \u03b1k \u2208 (0, c\u03b1] and \u03b2k \u2208 (0, c\u03b2 ], the sequence of (xk, yk, zk) generated by LV-HBA satisfies\nVk+1 \u2212 Vk \u2264 \u2212 1 4\u03b1k \u2225(xk+1, yk+1)\u2212 (xk, yk)\u22252 \u2212 1 4\u03b2k \u2225zk+1 \u2212 zk\u22252\n\u2212 \u03b7\u03c1TC\u03b8\u03bb \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217r(xk, yk, zk), \u03bb\u2217r(xk, yk, zk))\u2225\u22252 . (14)\nThe step sizes are carefully chosen to guarantee the sufficient descent property of Vk. This is essential for the non-asymptotic convergence analysis. Comprehensive proofs for Lemma 3.1 and accompanying auxiliary lemmas can be found in Sections A.4 , A.5 in Appendix.\nGiven the decreasing property of Vk, we proceed to establish the non-asymptotic convergence analysis. Owing to the constraint f(x, y)\u2212 v\u03b3,r(x, y, z) \u2264 0 in (6), by employing an argument analogous to Ye & Zhu (1995), it can be deduced that conventional constraint qualifications are not satisfied\n*Recall that a function h is said to be L-smooth on \u2126 if h is continuously differentiable and its gradient \u2207h is L-Lipschitz continuous on \u2126.\nat any feasible point of constrained problem (6). As a result, the standard KKT conditions are inappropriate as necessary optimality conditions for problem (6). Motivated by the approximate KKT condition presented in Andreani et al. (2010), which is characterized as an optimality condition for nonlinear program, regardless of constraint qualifications\u2019 fulfillment, we consider the following residual function Rk := Rk(x, y, z) as a stationarity measure,\nRk := dist (0, (\u2207F (x, y), 0) + ck ((\u2207f(x, y), 0)\u2212\u2207v\u03b3,r(x, y, z)) +NC\u00d7Z(x, y, z)) , (15) where N\u2126(s) denotes the normal cone to \u2126 at s. This residual function Rk(x, y, z) also serves as a stationarity measure for the penalized problem of (6), with ck serving as the penalty parameter,\nmin (x,y)\u2208C,z\u2208Z \u03c8ck(x, y, z) := F (x, y) + ck (f(x, y)\u2212 v\u03b3,r(x, y, z)) . (16)\nEvidently, Rk(x, y, z) = 0 if and only if (x, y, z) is a stationary point for problem (16), meaning 0 \u2208 \u2207\u03c8ck(x, y, z)+NC\u00d7Z(x, y, z). The following theorem offers the non-asymptotic convergence for LV-HBA, and the proof is detailed in Section A.6 of the Appendix. Theorem 3.1. Under Assumptions of Lemma 3.1, let \u03b31 \u2208 (0, 1/\u03c1f ), \u03b32 > 0, ck = c(k + 1)p with p \u2208 (0, 1/2), c > 0 and \u03b7k \u2208 (\u03b7, \u03c1T /L2B), then there exist c\u03b1, c\u03b2 > 0 such that when \u03b1k \u2208 (\u03b1, c\u03b1) and \u03b2k \u2208 (\u03b2, c\u03b2) with \u03b1, \u03b2 > 0, the sequence of (xk, yk, zk, \u03b8k, \u03bbk) generated by LV-HBA satisfies\nmin 0\u2264k\u2264K \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217r(xk, yk, zk), \u03bb\u2217r(xk, yk, zk))\u2225\u2225 = O( 1K1/2 ) ,\nand\nmin 0\u2264k\u2264K\nRk(x k+1, yk+1, zk+1) = O\n( 1\nK(1\u22122p)/2\n) .\nFurthermore, if there exists M > 0 such that \u03c8ck(x k, yk, zk) \u2264 M for any k, the sequence of (xk, yk, zk) satisfies\n0 \u2264 f(xK , yK)\u2212 v\u03b3(xK , yK , zK) \u2264 f(xK , yK)\u2212 v\u03b3,r(xK , yK , zK) = O ( 1\nKp\n) ."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we evaluate the empirical performance of LV-HBA using numerical experiments on synthetic problems, hyperparameter optimization for SVM, and federated bilevel learning problem. We compare LV-HBA against AiPOD, E-AiPOD (Xiao et al., 2023b), and GAM (Xu & Zhu, 2023). The results underscore the effectiveness of LV-HBA in practical scenarios. Detailed experimental settings and parameter configurations can be found in Appendix A.1.The code is available at https://github.com/SUSTech-Optimization/LV-HBA."
        },
        {
            "heading": "4.1 SYNTHETIC EXPERIMENTS.",
            "text": "We test LV-HBA in comparison to AiPOD and E-AiPOD, on two synthetic coupling equalityconstrained BLOs from two distinct scenarios: merely convex and strongly convex LL objectives.\nLL merely convex. We consider BLO with coupling equality constraints given by\nmin x\u2208Rn,y:=(y1,y2)\u2208R2n\n1 2 \u2225x\u2212 y2\u22252 + 1 2 \u2225y1 \u2212 1\u22252 s.t. y \u2208 argmin\ny\u2032\u2208Y(x)\n{ 1\n2 \u2225y\u20321\u2225\n2 \u2212 xT y\u20321 + 1T y\u20322 } ,\nwhere 1 \u2208 Rn represents a vector with all elements equal to 1 and Y(x) = {y \u2208 R2n | 1Tx + 1T y1 + 1 T y2 = 0}. Its optimal solution can be analytically expressed as x\u2217 = \u2212 3101, y \u2217 1 = 7 101, y\u22172 = \u2212 4101. For the problem where n = 100, we test algorithms from two distinct initial points: 10 \u00b7 1 \u2208 R300 and 100 \u00b7 1 \u2208 R300. The convergence curves relative to time are presented in Figure 1. Notably, LV-HBA provides a more precise approximation to the optimal solution and demonstrates faster convergence compared to AiPOD and E-AiPOD. The inadequate performance of AiPOD and E-AiPOD may stem from the fact that while our synthetic problem has a merely convex LL objective, both AiPOD and E-AiPOD require a strongly convex LL objective for convergence. Moreover, we examine the sensitivity of parameter ck in LV-HBA, and present its convergence curve in Figure 2. We further test the synthetic problem in a high-dimensional setting to demonstrate the computational efficiency of LV-HBA by increasing the dimension n. We record the time when \u2225xk \u2212 x\u2217\u2225/\u2225x\u2217\u2225 \u2264 10\u22122 is met by the iterates generated by LV-HBA. Results in Figure 2 highlight the computational efficiency of LV-HBA.\nLL strongly convex. We consider the strongly convex instance as presented in Xiao et al. (2023b):\nmin x\u2208X\nsin ( c\u22a4x+ d\u22a4y\u2217(x) ) + ln ( \u2225x+ y\u2217(x)\u22252 + 1 ) s.t. y\u2217(x) = argmin\ny\u2208Y(x)\n1 2 \u2225x\u2212 y\u22252,\nwhere X = {x | Bx = 0} \u2282 R100,Y(x) = {y | Ay+ Hx = 0} \u2282 R100, and A,B,H, c, d are non-zero matrices or vectors imported from the code of Xiao et al. (2023b) available at https: //github.com/hanshen95/AiPOD.. Contrary to the experiment in Xiao et al. (2023b), we do not add Gaussian noise in this simulation. We test algorithms from two different initial points: 5 \u00b7 (1,1) and 10 \u00b7 (1,1) in R200. We use the norm of \u2225yk \u2212 y\u2217(x)\u2225 as one stationarity measure, another one is the value of hyper-objective F (xk, y\u2217(xk)). We depict the respective convergence curves over time in Figure 3. Empirical results highlight the superior speed of convergence of our LV-HBA compared to both AiPOD and E-AiPOD."
        },
        {
            "heading": "4.2 HYPERPARAMETER OPTIMIZATION",
            "text": "We test the performance of our algorithm LV-HBA in comparison to GAM (Xu & Zhu, 2023). Both algorithms are applied to the hyperparameter optimization problem of SVM and the data hypercleaning task, as described in Xu & Zhu (2023). A comprehensive discussion of the problem formulation and the specific implementation settings can be found in Appendix A.1.\nHyperparameter Optimization of SVM We center our attention on the linear SVM model and conduct experiments on the dataset diabetes from Dua et al. (2017) and the dataset fourclass from Ho & Kleinberg (1996). The results are presented in Table 2. Moreover, Figure 4 illustrates the curve between test accuracy and time for the result on the dataset diabetes. Notably, our LV-HBA outperforms GAM by achieving superior accuracy within a significantly reduced time.\nData Hyper-Cleaning We compare our LV-HBA against GAM on the data hyper-cleaning task (Franceschi et al. (2017); Shaban et al. (2019)), utilizing dataset gisette (Guyon et al., 2004). Results are tabulated in Table 2. A performance curve for test accuracy against time is depicted in Figure 4. Remarkably, LV-HBA surpasses GAM, delivering enhanced test accuracy in a shorter time."
        },
        {
            "heading": "4.3 FEDERATED LOSS FUNCTION TUNING",
            "text": "In this part, we test our LV-HBA compared to E-AiPOD (Xiao et al., 2023b) and FedNest (Tarzanagh et al., 2022) using the federated loss function tuning problem, as explored in Xiao et al. (2023b). Detailed descriptions of the problem formulation and the specific implementation settings are available in Appendix A.1. This federated learning with imbalanced data task aims to develop a model ensuring fairness and generalization across datasets dominated by under-represented classes Li et al. (2021). Results, detailed in Table 3 and Figure 4, indicate that LV-HBA surpasses E-AiPOD and FedNest in both communication complexity and computational efficiency."
        },
        {
            "heading": "5 CONCLUSIONS",
            "text": "This work proposes a new approach and algorithm for solving a class of constrained BLO problems in which LL problem involves constraints coupling both UL and LL variables. The key enabling technique is to introduce a smooth proximal Lagrangian value function to handle the constrained LL problem. This allows us to smoothly reformulate the original BLO, and develop a Hessian-free gradient-based algorithm. In the future we would be interested in studying stochastic algorithms for constrained BLOs, leveraging the simplicity of our approach and incorporating techniques such as extrapolation, variance reduction, momentum, and others."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "Authors listed in alphabetical order. This work is supported by National Key R & D Program of China (2023YFA1011400), National Natural Science Foundation of China (12222106, 12326605, 62331014, 12371305), Guangdong Basic and Applied Basic Research Foundation (No. 2022B1515020082) and Shenzhen Science and Technology Program (No. RCYX20200714114700072)."
        },
        {
            "heading": "A APPENDIX",
            "text": "The appendix is organized as follows:\n\u2022 The experimental details is provided in Section A.1.\n\u2022 Expanded related work is provided in Section A.2.\n\u2022 The equivalent results of the reformulated problem 4 are provided in Section A.3.\n\u2022 Some useful auxiliary lemmas are provided in Section A.4.\n\u2022 The proof of Lemma 3.1 is given in Section A.5.\n\u2022 The proof of Proposition 3.1 is provided in Section A.6."
        },
        {
            "heading": "A.1 EXPERIMENTAL DETAILS",
            "text": "In this section, we outline the specific experimental settings. All experiments were conducted using Python 3.8 on a computer with an Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz CPU and an NVIDIA A100 GPU with 40GB memory GPU."
        },
        {
            "heading": "A.1.1 SYNTHETIC EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "LL merely convex:",
            "text": "Hyper-parameter settings for algorithms.\nLV-HBA: In Figure 1, the step sizes are chosen as \u03b1 = 0.005, \u03b2 = 0.002, \u03b7 = 0.03, \u03b31 = \u03b32 = 10, r = 1 with parameter ck = (k + 1)0.3. In Figure 2, the step sizes are chosen as \u03b1 = 0.002, \u03b2 = 0.002, \u03b7 = 0.03, \u03b31 = \u03b32 = 0.1, r = 1, ck = (k + 1)p with various p.\nE-AiPOD: Projection probability is p = 0.3, total iterations are K = 300/p, UL iterations are T = 2, and LL iterations are S = 5. In Figure 1, the step sizes are set as \u03b1 = 0.0001, \u03b2 = 0.001. In both AiPOD and E-AiPOD, the parameter T is set to 2, as this choice has been demonstrated to yield best performance, as shown in (Xiao et al., 2023b, Figure 1)."
        },
        {
            "heading": "LL strongly convex Case:",
            "text": "The strongly convex instance is adapted from Xiao et al. (2023b) by omitting the Gaussian noise.\nHyper-parameter settings for algorithms. For LV-HBA, the step sizes are chosen as \u03b1 = 0.02, \u03b2 = 0.001, \u03b7 = 0.1, \u03b31 = \u03b32 = 1, r = 1000 with parameter ck = (k + 1)0.3. For both AiPOD and E-AiPOD, the hyper-parameters are set consistent with the code in Xiao et al. (2023b). Specifically, the projection probability p = 0.3, the number of UL iterations in is T = 2, the number of LL iterations is S = 5, and the step sizes are set as \u03b1 = 0.001, \u03b2 = 0.02."
        },
        {
            "heading": "Sensitivity of parameters:",
            "text": "Additionally, to assess the sensitivity of the remaining parameters, further numerical experiments were conducted on LL merely convex synthetic model. We record the time when \u2225xk\u2212x\u2217\u2225/\u2225x\u2217\u2225 \u2264 10\u22122 is met by the iterates generated by LV-HBA. The results are as follows:\nSensitivity Parameters\n\u03b1 \u03b2 \u03b7 \u03b3 = \u03b31 = \u03b32 c p r Time(s)\n\u03b1 0.001 0.02 0.01 10 0.025 0.3 1000 0.23 0.005 0.02 0.01 10 0.025 0.3 1000 0.064 0.01 0.02 0.01 10 0.025 0.3 1000 0.052\n\u03b2 0.005 0.005 0.01 10 0.025 0.3 1000 0.068 0.005 0.02 0.01 10 0.025 0.3 1000 0.064 0.005 0.1 0.01 10 0.025 0.3 1000 0.063\n\u03b7 0.005 0.02 0.005 10 0.025 0.3 1000 0.098 0.005 0.02 0.01 10 0.025 0.3 1000 0.064 0.005 0.02 0.05 10 0.025 0.3 1000 0.025\n\u03b3 = \u03b31 = \u03b32\n0.005 0.02 0.01 5 0.025 0.3 1000 0.069 0.005 0.02 0.01 10 0.025 0.3 1000 0.064 0.005 0.02 0.01 500 0.005 0.3 1000 0.064\nc 0.005 0.02 0.01 10 0.005 0.3 1000 0.026 0.005 0.02 0.01 10 0.05 0.3 1000 0.064 0.005 0.02 0.01 10 0.025 0.3 1000 0.086\nr 0.005 0.02 0.01 10 0.025 0.3 200 0.066 0.005 0.02 0.01 10 0.025 0.3 1000 0.064 0.005 0.02 0.01 10 0.025 0.3 2000 0.065"
        },
        {
            "heading": "A.1.2 HYPERPAMETER OPTIMIZATION",
            "text": "The hyperparameter optimization of SVM and the data hyper-cleaning experiments were performed using qpth version 0.0.11 and cvxpy version 1.2.0.\nHyperparameter Optimization of SVM We test the performance of our algorithm LV-HBA in comparison to GAM proposed in Xu & Zhu (2023) on the same hyperparameter optimization problem of SVM as considered in Xu & Zhu (2023). Experiments are conducted using datasets diabetes from Dua et al. (2017). and fourclass from Ho & Kleinberg (1996). For dataset diabetes, we randomly partition it into training, validation, and testing subsets containing 500, 150, and 118 examples, respectively. Similarly, dataset fourclass is partitioned into training, validation, and testing subsets with 500, 150, and 212 examples, respectively. We conduct experiments on each dataset with 40 repetitions.\nThe hyperparameter optimization of SVM can be expressed as:\nmin c\n\u03a6(c) = LDval (w\u2217, b\u2217) ,\nwhere the hyperparameter to be optimized is c = [c1, . . . , cN ] and w\u2217, b\u2217 are solution to the SVM optimization problem given by\n(w\u2217, b\u2217, \u03be\u2217) = arg min w,b,\u03be\n1 2 \u2225w\u22252 + 1 2 N\u2211 i=1 eci\u03be2i\ns.t. li ( w\u22a4\u03d5 (zi) + b ) \u2265 1\u2212 \u03bei, i = 1, 2, . . . N.\nHere, Dval represents the validation data, and Dtr the training data. For all 1 \u2264 i \u2264 N , zi denotes the data point, li is the label, and (zi, li) \u2208 Dtr . The upper-level objective function is:\nLDval (w\u2217, b\u2217) = 1 |Dval | \u2211\n(z,l)\u2208Dval\nL (w\u2217, b\u2217; z, l) ,\nwhere L (w\u2217, b\u2217;Dval ) is given by\nL (w\u2217, b\u2217; z, l) = \u03c3\n( \u2212l ( z\u22a4w\u2217 + b ) \u2225w\u2217\u2225 ) ,\nwith \u03c3(x) = 1\u2212e \u2212x 1+e\u2212x . The term l(z\u22a4w\u2217+b)\n\u2225w\u2217\u2225 signifies the signed distance between point z and the decision plane z\u22a4w\u2217 + b = 0. It is positive when predictions are accurate, and negative otherwise. Thus, LDval (w\u2217, b\u2217) serves as a differentiable surrogate for validation accuracy. Hyper-parameter settings for algorithms. For LV-HBA, the step sizes are chosen as \u03b1 = 0.01, \u03b2 = 0.1, \u03b7 = 0.01, \u03b31 = \u03b32 = 10, r = 100 with parameter ck = (k + 1)0.3. For GAM, hyperparameters are set in alignment with the code in Xu & Zhu (2023): \u03b3 = 0.3, \u03f50 = 0.3, \u03b2 = 0.5. In GAM\u2019s implementation, to uphold the LL strong convexity assumption, the LL problem\u2019s objective function is set as 12\u2225w\u2225 2 + 12 \u2211N i=1 e ci\u03be2i + 1 2\u00b5b 2 where \u00b5 is a small positive number.\nData Hyper-Cleaning We adopt the data hyper-cleaning formulation as the hyperparameter optimization of SVM, as presented in Xu & Zhu (2023). In this model, post-optimization of hyperparameter c, the penalty term corresponding to the corrupted data (zi, yi) approaches 0. Consequently, the corrupted data (zi, yi) is identified and has a negligible impact on the training and prediction of the classifier model. Experiments are conducted using the datasets gisette from Guyon et al. (2004). For dataset gisette, we segment it into training, validation, and testing subsets, comprising 400, 180, and 5420 examples, respectively. We conduct experiments on each dataset with 40 repetitions.\nHyper-parameter settings for algorithms. For LV-HBA, we select step sizes with values \u03b1 = 0.01, \u03b2 = 0.1, and \u03b7 = 0.01, accompanied by parameter ck = (k + 1)0.3. For GAM, hyperparameters are consistent with specifications in Xu & Zhu (2023), with \u03b3 = 0.3, \u03f50 = 0.3, and \u03b2 = 0.5."
        },
        {
            "heading": "A.1.3 FEDERATED LOSS FUNCTION TUNING",
            "text": "In this part, we test our LV-HBA compared to E-AiPOD Xiao et al. (2023b) and FedNest Tarzanagh et al. (2022) on the same federated loss function tuning problem, as explored in Xiao et al. (2023b). The experiments were executed with opencv-python version 4.6.0.66. In the federated loss function tuning problem, the UL optimizes loss-tuning parameters to enhance both generalization and fairness. Meanwhile, the LL focuses on training model parameters on potentially imbalanced datasets. The formal problem statement is:\nmin x\u2208X\n1\nM M\u2211 m=1 fupvs (y \u2217 m(x);Dmval) ,\ns.t. y\u2217(x) = argmin y\u2208Y\n1\nM M\u2211 m=1 f lowvs (x, ym;Dmtr ) ,\nwhere M = 50 representing the number of clients, x is the loss-tuning parameter and y indicates the neural network parameters. Dmtr and Dmval are the training and validation sets of client m. The consensus sets X and Y are given by X := {x | x1 = \u00b7 \u00b7 \u00b7 = xM} and Y := {y | y1 = \u00b7 \u00b7 \u00b7 = yM}, respectively. Training datasets {Dmtr } M m=1 have class imbalances. As introduced by Kini et al. (2021), the vector-scaling loss f lowvs is\nf lowvs (x, y;D) := \u2212 1 |D| \u2211 dn\u2208D \u03c9ln log exp (\u03b4lnhln (y; dn) + \u03c4ln)\u2211C c=1 exp (\u03b4chc (y; dn) + \u03c4c) ,\nwhere N signifies dataset size, C denotes the class count, and dn is the n-th data item with label ln in dataset D. The logit output of the neural network for parameters y and input dn is h (y; dn) = [h1 (y; dn) , . . . , hC (y; dn)]\n\u22a4 \u2208 RC , x is defined as (\u03c9, \u03b4, \u03c4) with \u03c9 := [\u03c91, . . . , \u03c9C ]\u22a4 \u2208 RC , and \u03b4, \u03c4 defined in a similar manner. The upper-level loss f upvS is a variant of f low vs where \u03b4 = 1, \u03c4 = 0, and \u03c9 is a static class weight for the validation dataset.\nHyper-parameter settings for algorithms. For LV-HBA, we select step sizes with values \u03b1 = 0.01, \u03b2 = 0.01, and \u03b7 = 0.01, \u03b31 = \u03b32 = 10, r = 100, parameter ck = (k+1)0.3 and bath size as 256. For E-AiPOD and FedNest, hyperparameters align with specifications in Xiao et al. (2023b): E-AiPOD has a communication probability p = 0.3, S = 20, \u03b1 = 0.01, \u03b2 = 0.04, N \u2032 = 3, and batch size of 256. FedNest utilizes LL iteration number \u03c4 = 3, episode T = 3, resulting in a communication frequency of 0.3 per LL iteration."
        },
        {
            "heading": "A.2 EXPANDED RELATED WORK",
            "text": "In this section, we provide an extensive review of recent studies closely related to our work.\nApproaches for BLO. One of the most commonly employed approaches for tackling BLO problems is to reformulate them as single-level problems. This can often be accomplished in two ways Dempe & Zemkoho (2013). One of these approaches, known as the KKT (or MPEC) reformulation, replaces the LL problem with its Karush-Kuhn-Tucker (KKT) conditions and minimizes over the original variables as well as multipliers if the LL constraints exist. The resulting problem is the so-called mathematical program with complementarity/equilibrium constraints (MPCC/MPEC) Luo et al. (1996), which itself poses a significant challenge when treated as a nonlinear programming problem Kim et al. (2020). Remarkably, the KKT reformulation employs the KKT conditions, thereby unavoidably relying on first-order gradient information. Consequently, gradient-based algorithms based on KKT reformulation also depend on second-order gradient information.\nAnother often used approach is the value function approach, originally proposed in Outrata (1990) and Ye & Zhu (1995). It is obtained by replacing the LL problem by its description via the (optimal) value function. Unlike the KKT reformulation, the value function reformation does not use any gradient information of the objective and constraint functions in the LL problem. To the best of our knowledge, the majority of existing Hessian-free (also referred to as fully first-order) gradient-based algorithms for both unconstrained and constrained BLOs, are developed based on the value function reformulation, see, e.g., Liu et al. (2021b; 2023a); Ye et al. (2022); Sow et al. (2022); Shen & Chen (2023); Kwon et al. (2023a); Lu & Mei (2023). It should be noted, however, that the value function is typically nonsmooth, even when the functions involved are linear and affine. Hence, the value function reformulation often leads to a nonsmooth problem. To alleviate the nonsmooth issue, the recent works Ye et al. (2023); Gao et al. (2022) develop difference of convex algorithms for solving BLO problems in which the UL objective is a difference of convex function and the LL problem is fully convex.\nRecently, to weaken the underlying assumption from lower level full convexity to weak convexity, Gao et al. (2023) proposes a new reformulation of BLOs, using Moreau envelope of the LL problem. They also demonstrate the equivalence between the reformulated and the original BLO problems in the convex setting. Other approaches for BLOs include implicit methods Franceschi et al. (2017); Ghadimi & Wang (2018); Shaban et al. (2019), penalty methods Lin et al. (2014), duality-based solution approach Ouattara & Aswani (2018); Li et al. (2023; 2024).\nUnconstrained BLO. The LL strong convexity in unconstrained BLO significantly contributes to the development of efficient BLO algorithms, see, e.g., Maclaurin et al. (2015); Franceschi et al. (2017); Shaban et al. (2019); Mackay et al. (2019); Grazzi et al. (2020); Ji et al. (2021; 2022) for the iterative differentiation (ITD) based approach; Pedregosa (2016); Ghadimi & Wang (2018); Rajeswaran et al. (2019); Lorraine et al. (2020); Hong et al. (2023); Chen et al. (2021); Arbel & Mairal (2022a); Dagre\u0301ou et al. (2022); Ye et al. (2022); Yang et al. (2023a) for the approximate implicit differentiation (AID) based approach. Recently, based on the value function-based reformulation, Kwon et al. (2023a) developed stochastic and deterministic fully first-order BLO algorithms and established their non-asymptotic convergence guarantees, while an improved convergence analysis is provided in the recent work Chen et al. (2023a).\nConvex LL problems introduce additional challenges, such as the presence of multiple LL solutions (Non-Singleton), which can impede the utilization of implicit-based approaches developed for nonconvex-strongly-convex BLO. To tackle Non-Singleton, recent advances include: aggregation methods (or called sequential averaging methods) in Liu et al. (2020); Li et al. (2020); Liu et al. (2022) with asymptotic convergence guarantees; in Liu et al. (2023b) with convergence rate analysis; value function-based difference-of-convex algorithm in Ye et al. (2023); Gao et al. (2022); primal-dual algorithms in Sow et al. (2022); min-max optimization reformulation-based first-order penalty methods in Lu & Mei (2023).\nEfficient methods for nonconvex-nonconvex BLO remain under-explored, recent advances include: initialization auxiliary and pessimistic trajectory truncation method in Liu et al. (2021c); value function-based interior-point method in Liu et al. (2021b); possibly degenerate implicit differentiation-based unrolled optimization algorithms in Arbel & Mairal (2022b); momentumbased algorithm in Huang (2023); generalized alternating method in Xiao et al. (2023a); fully first-\norder value function-based algorithm in Ye et al. (2022); penalty-based fully first-order algorithm in Shen & Chen (2023); a smoothed first-order Lagrangian method in Lu (2024).\nConstrained BLO. While gradient-based algorithms for unconstrained BLO problems have been extensively explored, the investigation of efficient methods for constrained BLO problems is relatively limited, especially when addressing LL constraints coupling both UL and LL variables.\nRecently, driven by applications in machine learning, the recent works such as Tsaknakis et al. (2022); Khanduri et al. (2023) study the constrained BLOs, where the LL problem involves the minimization of a strongly convex objective over a set of linear inequality constraints; Xiao et al. (2023a) investigates the stochastic BLO problems with possibly coupled equality constraints in both upper and lower levels; Xu & Zhu (2023) considers BLOs in which the LL problem is convex with general equality and inequality constraints, while assuming that the LL objective is strongly convex and the constraints satisfy strict Linear Independence Constraint Qualification (LICQ); Tsaknakis et al. (2023) develop a novel barrier-based gradient approximation algorithm that transforms the general constrained BLO problem to a problem with only linear equality constraints. Observe that all of these works employ implicit gradient-based methods, relying on the computation of the implicit gradient of the unique LL solution mapping. Among them, Khanduri et al. (2023) develops a linear perturbation-based smoothing framework for the linearly constrained LL problem that ensures the existence of the implicit gradient in an almost sure sense. Notably, these implicit gradient-based methods for constrained BLOs unavoidably rely on computationally intensive calculations related to the Hessian matrix.\nThe value function-based methods can avoid recurrent calculations related to the Hessian matrix, see, e.g., Liu et al. (2023a); Lu & Mei (2023). Both papers have considered BLOs with general constraints in the LL problem. For constrained BLOs, value function-based methods face challenges related to non-differentiable constraints, stemming from the value function-based reformulation. To address the nonsmooth issue, Liu et al. (2023a) proposes a sequential minimization algorithmic framework, by adding a quadratic regularization and penalty/barrier functions of the LL inequality constraints to the LL objective; Lu & Mei (2023) develops first-order penalty methods by solving a sequence of minimax problems or a single minimax problem. Recent advances include primal-dual algorithms in Sow et al. (2022); primal nonsmooth reformulation-based algorithm in Helou et al. (2023); penalty-based first-order algorithms in Kwon et al. (2023b).\nThere is also a line of works devoted to tackle the constrained UL setting including: implicit approximation methods in Ghadimi & Wang (2018); a two-timescale framework in Hong et al. (2023); a single-timescale method in Chen et al. (2022a); initialization auxiliary method in Liu et al. (2021c); Bregman distance-based method in Huang et al. (2022); value function-based Differenceof-Convex algorithm in Gao et al. (2022); proximal gradient-type algorithm in Chen et al. (2022b); penalty-based method in Shen & Chen (2023); Moreau Envelope-based Difference-of-weaklyConvex method in Gao et al. (2023); inexact conditional gradient method in Abolfazli et al. (2023); nested compositional BLO in Chen et al. (2023b)."
        },
        {
            "heading": "A.3 EQUIVALENT RESULTS OF REFORMULATED PROBLEM",
            "text": "In the following result, we demonstrate that the reformulation problem (4) is equivalent to the BLO problem (1).\nRecall that for the sake of notational simplicity, throughout this paper, for LL constraint mapping g : X \u00d7 Y \u2192 Rl and any vectors \u03bb, z \u2208 Rl, we represent \u2211l i=1 \u03bbigi and \u2211l i=1 zigi as \u03bbg and zg,\nrespectively. Similarly, \u2211l i=1 \u03bbi\u2207ygi and \u2211l i=1 zi\u2207ygi are denoted by \u03bb\u2207yg and z\u2207yg, respectively.\nFor the reader\u2019s convenience, we restate the reformulation problem (4) as follows:\nmin (x,y)\u2208C,z\u22650 F (x, y) s.t. f(x, y)\u2212 v\u03b3(x, y, z) \u2264 0, (4)\nwhere C := {(x, y) \u2208 X \u00d7 Y | g(x, y) \u2264 0}, and v\u03b3(x, y, z) is the proximal Lagrangian value function, restated below,\nv\u03b3(x, y, z) := min \u03b8\u2208Y max \u03bb\u2208Rl+\n{ f(x, \u03b8) + \u03bbg(x, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u22252\n} . (3)\nThe proximal Lagrangian value function v\u03b3(x, y, z) defined in equation (3) can be regarded as the (upper) Yosida approximate of the Lagrangian function L(x, y, \u03bb) := f(x, y) + \u03bbg(x, y) of the LL problem (Attouch & Wets, 1983, Section 5).\nTheorem A.1. Assume that f(x, \u00b7) and g(x, \u00b7) are both convex on Y . Suppose \u03b31, \u03b32 > 0, the reformulated problem (4) is equivalent to the BLO problem (1), if multiplier of the lower-level problem( 2) exists for any feasible point (x, y) of the BLO problem (1).\nProof. First, let (x, y, z) be any feasible point of problem (4), then we have (x, y) \u2208 X \u00d7Y , z \u2265 0, and g(x, y) \u2264 0. Furthermore, the following inequalities hold,\nf(x, y) \u2264 v\u03b3(x, y, z) := min \u03b8\u2208Y max \u03bb\u22650\n{ f(x, \u03b8) + \u03bbg(x, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u22252 } \u2264 min\n\u03b8\u2208Y max \u03bb\u22650\n{ f(x, \u03b8) + \u03bbg(x, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u22252 } \u2264 min\n\u03b8\u2208Y\n{ f(x, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u22252 \u2223\u2223 g(x, \u03b8) \u2264 0} \u2264 f(x, y).\n(17)\nSince the first and the last terms in the above inequalities are the same, we must have equalities throughout. Specially, we have\ny \u2208 argmin \u03b8\u2208Y\n{ f(x, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u22252 \u2223\u2223 g(x, \u03b8) \u2264 0} . Because f(x, \u00b7) and g(x, \u00b7) are both convex functions on Y , considering g(x, \u00b7) \u2264 0 as an abstract convex set constraint and using the first-order optimality conditions, we obtain that\ny \u2208 argmin\u03b8\u2208Y {f(x, \u03b8) | g(x, \u03b8) \u2264 0} ,\nand thus y \u2208 S(x). Then the point (x, y) is feasible to the BLO problem (1). Conversely, suppose that (x, y) is an feasible point of the BLO problem (1), then we have (x, y) \u2208 X \u00d7 Y and y \u2208 S(x). On one hand, according to the assumption that multiplier z \u2265 0 of the LL problem (2) exists at (x, y), since the LL problem is convex, by the first-order optimality conditions, we get y \u2208 argmin\u03b8\u2208Y {f(x, \u03b8) + zg(x, \u03b8)}. Once more, due to the convexity of the LL problem, this implies that\ny \u2208 argmin\u03b8\u2208Y { f(x, \u03b8) + zg(x, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u22252\n} .\nOn the other hand, since g(x, y) \u2264 0, by the complementarity conditions, i.e., zg(x, y) = 0, z \u2208 argmax\u03bb\u22650 { f(x, y) + \u03bbg(x, y)\u2212 1\n2\u03b32 \u2225\u03bb\u2212 z\u22252\n} .\nHence, the point (y, z) is a saddle point of the following strong convex strong concave function L : Y \u00d7 Rl+ \u2192 R:\nL(\u03b8, \u03bb) := f(x, \u03b8) + \u03bbg(x, \u03b8) + 1 2\u03b31 \u2225\u03b8 \u2212 y\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u22252.\nThus it follows from saddle point theorem that\nmin \u03b8\u2208Y max \u03bb\u22650\n{ f(x, \u03b8) + \u03bbg(x, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u22252 } =f(x, y) + zg(x, y) = f(x, y),\nwhere the last inequality uses the complementarity conditions zg(x, y) = 0. Therefore, v\u03b3(x, y, z) = f(x, y) and then (x, y, z) is feasible to the reformulation problem (4).\nRemark A.1. Indeed, as per the estimate provided in (17), the proximal Lagrangian value function v\u03b3(x, y, z) establishes a lower bound for f(x, y), that is, v\u03b3(x, y, z) \u2264 f(x, y) for any (x, y, z) \u2208 C \u00d7 Rl+. Specifically, we have\nv\u03b3(x, y, z) \u2264 min \u03b8\u2208Y\n{ f(x, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u22252 \u2223\u2223 g(x, \u03b8) \u2264 0} =: v\u03b31(x, y), (18) where v\u03b31(x, y) coincides with the so-called Moreau envelope function, initially introduced by Gao et al. (2023) for BLO problems. Additionally, the latter also acts as a lower bound for the function f(x, y).\nSubsequently, we demonstrate that for a sufficiently large r, the solution to the reformulation (4) can be obtained by solving variant (6). Note that ProjZ in Algorithm 1 is a simple Euclidean projection on a box Z := [0, r]l with lower bounds 0 and upper bounds r \u2265 0. This projection is pivotal in guaranteeing the boundedness of the auxiliary variable zk+1. For clarity, we restate the variant (6) of the reformulation (4) as follows:\nmin (x,y)\u2208X\u00d7Y,z\u2208Z F (x, y) s.t. f(x, y)\u2212 v\u03b3,r(x, y, z) \u2264 0, g(x, y) \u2264 0, (6)\nwhere v\u03b3,r(x, y, z) is a truncated proximal Lagrangian value function, defined in equation (5),\nv\u03b3,r(x, y, z) := min \u03b8\u2208Y max \u03bb\u2208Z\n{ f(x, \u03b8) + \u03bbg(x, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u22252\n} . (5)\nTheorem A.2. Suppose \u03b31, \u03b32 > 0, let an optimal solution (x\u2217, y\u2217, z\u2217) of reformulation (4) exist such that z\u2217 is within the set Z. Then (x\u2217, y\u2217, z\u2217) is also an optimal solution for the reformulation (6). Consequently, the optimal values for both reformulations, (4) and (6), are identical. Moreover, any optimal solution of (6) is optimal to reformulation (4).\nProof. Firstly, according to the definitions of v\u03b3 and v\u03b3,r, we have that v\u03b3(x, y, z) \u2265 v\u03b3,r(x, y, z) for any (x, y, z) \u2208 X\u00d7Y \u00d7Z. Therefore, any feasible point (x, y, z) of problem (6) is also feasible to problem (4) and thus the optimal value of problem (6) is larger or equal to that of problem (4).\nConversely, let (x\u2217, y\u2217, z\u2217) be an optimal solution of the reformulation problem (4) with z\u2217 belonging to the set Z, since (x\u2217, y\u2217, z\u2217) is feasible to problem (4). As shown in the proof of Theorem A.1, we obtain y\u2217 \u2208 S(x\u2217). Next we show that z\u2217 is a multiplier of the LL problem (2) at (x\u2217, y\u2217). To this end, it suffices to prove that\n(y\u2217, z\u2217) = argmin \u03b8\u2208Y argmax \u03bb\u22650\n{ f(x\u2217, \u03b8) + \u03bbg(x\u2217, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u2217\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u2217\u22252\n} . (19)\nBy estimates in (17), we get\ny\u2217 \u2208argmin \u03b8\u2208Y max \u03bb\u22650\n{ f(x\u2217, \u03b8) + \u03bbg(x\u2217, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u2217\u22252\n} ,\ny\u2217 =argmin \u03b8\u2208Y max \u03bb\u22650\n{ f(x\u2217, \u03b8) + \u03bbg(x\u2217, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u2217\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u2217\u22252\n} .\nFurthermore, the optimal values of the above optimization problems are equal. This implies that z\u2217g(x\u2217, y\u2217) = 0, and then\nz\u2217 \u2208 argmax\u03bb\u22650 { f(x\u2217, y\u2217) + \u03bbg(x\u2217, y\u2217)\u2212 1\n2\u03b32 \u2225\u03bb\u2212 z\u2217\u22252\n} .\nNow since z\u2217 \u2208 Z, we have\n(y\u2217, z\u2217) = argmin \u03b8\u2208Y argmax \u03bb\u2208Z\n{ f(x\u2217, \u03b8) + \u03bbg(x\u2217, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u2217\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u2217\u22252\n} ,\nleading to v\u03b3(x\u2217, y\u2217, z\u2217) = v\u03b3,r(x\u2217, y\u2217, z\u2217) and thus (x\u2217, y\u2217, z\u2217) is also feasible to problem (6). Therefore, the optimal value of problem (6) is equal to that of problem (4). Then, because any feasible point (x, y, z) of problem (6) is feasible to problem (4), we get the conclusion."
        },
        {
            "heading": "A.4 AUXILIARY LEMMAS",
            "text": "The following lemma provides a characterization of the gradient of the Lagrangian based proximal value function v\u03b3,r(x, y, z).\nGiven that f is Lf -smooth on X\u00d7Y , by leveraging the descent lemma (Beck, 2017, Lemma 5.7), it can be deduced that f is also Lf -weakly convex, i.e, f(x, y) +Lf\u2225(x, y)\u22252/2 is convex on X \u00d7 Y . Consequently, under Assumption 3.2, f is \u03c1f -weakly convex on X \u00d7 Y , with \u03c1f \u2265 0 potentially being smaller than Lf . To precisely determine the range for the step sizes of LV-HBA , we will employ the weak convexity constant of f , \u03c1f , in subsequent results. Lemma A.1. Under Assumptions 3.2 and 3.3, let \u03b31 \u2208 (0, 1/\u03c1f ) and \u03b32 > 0. Then v\u03b3,r(x, y, z) is continuously differentiable on X \u00d7 Y \u00d7 Rl, and for any (x, y, z) \u2208 X \u00d7 Y \u00d7 Rl,\n\u2207v\u03b3,r(x, y, z) = ( \u2207xf(x, \u03b8\u2217) + \u03bb\u2217\u2207xg(x, \u03b8\u2217),\n(y \u2212 \u03b8\u2217) \u03b31 , (\u03bb\u2217 \u2212 z) \u03b32\n) , (20)\nwhere \u03b8\u2217 := \u03b8\u2217(x, y, z) and \u03bb\u2217 := \u03bb\u2217(x, y, z) is the unique saddle point of the min-max problem:\nmin \u03b8\u2208Y max \u03bb\u2208Z\n{ f(x, \u03b8) + \u03bbg(x, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u22252\n} . (21)\nFurthermore, for any \u03c1v \u2265 \u03c1f/(1\u2212\u03b31\u03c1f ), v\u03b3,r(x, y, z) is \u03c1v-weakly convex with respect to variables (x, y) on X \u00d7 Y for any fixed z \u2208 Rl.\nProof. Firstly, we define an auxiliary function,\n\u03c6\u03b3(x, \u03b8, z) := max \u03bb\u2208Z\n{ f(x, \u03b8) + \u03bbg(x, \u03b8)\u2212 1\n2\u03b32 \u2225\u03bb\u2212 z\u22252\n} .\nNoticed that \u03c6\u03b3(x, \u03b8, z) can be rewritten as\n\u03c6\u03b3(x, \u03b8, z) = \u2212 inf \u03bb\u2208Z\n{ \u2212f(x, \u03b8)\u2212 \u03bbg(x, \u03b8) + 1\n2\u03b32 \u2225\u03bb\u2212 z\u22252\n} .\nBy Assumptions 3.2 and 3.3, f and g are both continuous differentiable on an open set containing X\u00d7Y , it can be easily shown that \u2212f(x, \u03b8)\u2212\u03bbg(x, \u03b8)+ 12\u03b32 \u2225\u03bb\u2212z\u2225\n2 satisfies the inf-compactness condition in (Bonnans & Shapiro, 2013, Theorem 4.13) on any point (x\u0304, \u03b8\u0304, z\u0304) \u2208 X \u00d7 Y \u00d7 Rl, that is, for any (x\u0304, \u03b8\u0304, z\u0304) \u2208 X \u00d7 Y \u00d7 Rl, there exist c \u2208 R, compact set D and neighborhood W of (x\u0304, \u03b8\u0304, z\u0304) such that the level set {\u03bb \u2208 Z | \u2212 f(x, \u03b8)\u2212 \u03bbg(x, \u03b8) + 1\u03b32 \u2225\u03bb\u2212 z\u2225 2 \u2264 c} is nonempty and\ncontained in D for any (x, \u03b8, z) \u2208 W . Because argmax\u03bb\u2208Z { f(x, \u03b8) + \u03bbg(x, \u03b8)\u2212 12\u03b32 \u2225\u03bb\u2212 z\u2225 2 } is unique for any (x, \u03b8, z) \u2208 X\u00d7Y \u00d7Rl, we denote it by \u03bb\u0302\u2217(x, \u03b8, z). Then, by Assumptions 3.2 and 3.3, we can derive from (Bonnans & Shapiro, 2013, Theorem 4.13, Remark 4.14) that \u03c6\u03b3(x, \u03b8, z) is differentiable at any point on X \u00d7 Y \u00d7 Rl, and for any (x, \u03b8, z) \u2208 X \u00d7 Y \u00d7 Rl,\n\u2207\u03c6\u03b3(x, \u03b8, z) = ( \u2207f(x, \u03b8) + \u03bb\u0302\u2217(x, \u03b8, z)\u2207g(x, \u03b8),\u2212(z \u2212 \u03bb\u0302\u2217(x, \u03b8, z))/\u03b32 ) . (22)\nBy simple calculation, we can obtain that\n\u03bb\u0302\u2217(x, \u03b8, z) := argmax \u03bb\u2208Z\n{ f(x, \u03b8) + \u03bbg(x, \u03b8)\u2212 1\n2\u03b32 \u2225\u03bb\u2212 z\u22252\n} = ProjZ (z + \u03b32g(x, \u03b8)) .\nSince by Assumptions 3.2 and 3.3 that f and g are both continuous differentiable on an open set containing X \u00d7 Y , and ProjZ is continuous, hence \u2207\u03c6\u03b3(x, \u03b8, z) is continuous on X \u00d7 Y \u00d7 Rl. Secondly, with the introduced auxiliary function \u03c6\u03b3 , we can rewrite v\u03b3,r as\nv\u03b3,r(x, y, z) = min \u03b8\u2208Y\n{ \u03c6\u03b3(x, \u03b8, z) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u22252\n} . (23)\nNext we will show that \u03c6\u03b3(x, y, z) is \u03c1f -weakly convex with respect to variables (x, y) on X \u00d7 Y for any fixed z \u2208 Rl. By Assumptions 3.2 and 3.3, we have that for any \u03bb \u2208 Rl+,\nf(x, y) + \u03bbg(x, y)\u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u22252 + \u03c1f 2 \u2225(x, y)\u22252,\nis convex with respect to variables (x, y) on X \u00d7 Y . Then by (Rockafellar, 1974, Theorem 1), we obtain that\n\u03c6\u03b3(x, y, z) + \u03c1f 2 \u2225(x, y)\u22252 = max \u03bb\u2208Z\n{ f(x, y) + \u03bbg(x, y)\u2212 1\n2\u03b32 \u2225\u03bb\u2212 z\u22252 + \u03c1f 2 \u2225(x, y)\u22252 } is convex with respect to variables (x, y) on X \u00d7 Y and thus \u03c6\u03b3(x, y, z) is \u03c1f -weakly convex with respect to variables (x, y) on X \u00d7 Y for any fixed z \u2208 Rl. Then, by Assumptions 3.2 and 3.3, it can be easily shown that when \u03b31 \u2208 (0, 1/\u03c1f ), \u03c6\u03b3(x, \u03b8, z) + 12\u03b31 \u2225\u03b8 \u2212 y\u2225\n2 satisfies the infcompactness condition on any point (x\u0304, y\u0304, z\u0304) \u2208 X \u00d7 Y \u00d7 Rl. Next, because when \u03b31 \u2208 (0, 1/\u03c1f ), \u03c6\u03b3(x, \u03b8, z) +\n1 2\u03b31 \u2225\u03b8\u2212 y\u22252 is strongly convex with respect to \u03b8, argmin\u03b8\u2208Y {\u03c6\u03b3(x, \u03b8, z) + 12\u03b31 \u2225\u03b8\u2212 y\u22252} is unique and it is equal to \u03b8\u2217(x, y, \u03bb). By using (Bonnans & Shapiro, 2013, Theorem 4.13, Remark 4.14), the continuous differentiablility of \u03c6\u03b3 established above and equation (22), we can obtain that v\u03b3,r(x, y, z) is differentiable at any point on X \u00d7 Y \u00d7 Rl, and for any (x, y, z) \u2208 X \u00d7 Y \u00d7 Rl,\n\u2207v\u03b3,r(x, y, z) = ( \u2207xf(x, \u03b8\u2217) + \u03bb\u0302\u2217(x, \u03b8\u2217, z)\u2207xg(x, \u03b8\u2217), (y \u2212 \u03b8\u2217)/\u03b31, (\u03bb\u0302\u2217(x, \u03b8\u2217, z)\u2212 z)/\u03b32 ) ,\nwhere \u03b8\u2217 denotes \u03b8\u2217(x, y, z).\nFinally, noticed that under Assumptions 3.2 and 3.3, when \u03b31 \u2208 (0, 1/\u03c1f ) and \u03b32 > 0, the function\nf(x, \u03b8) + \u03bbg(x, \u03b8) + 1 2\u03b31 \u2225\u03b8 \u2212 y\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u22252,\nis strongly convex and strongly concave with respect to \u03b8 and \u03bb, respectively. Therefore, it follows from saddle point theorem,\nmin \u03b8\u2208Y max \u03bb\u2208Z\n{ f(x, \u03b8) + \u03bbg(x, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u22252 } = max\n\u03bb\u2208Z min \u03b8\u2208Y\n{ f(x, \u03b8) + \u03bbg(x, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u22252\n} ,\nleading to \u03bb\u0302\u2217(x, \u03b8\u2217, z) = \u03bb\u2217(x, y, z),\nand thus the conclusion follows.\nRemark A.2. Using the a similar argument as above, the following result holds for the Lagrangian based proximal value function v\u03b3(x, y, z) when \u03b31 \u2208 (0, 1/\u03c1f ) and \u03b32 > 0. That is,\n(1) The function v\u03b3(x, y, z) is continuously differentiable on X \u00d7 Y \u00d7 Rl;\n(2) The gradient of v\u03b3(x, y, z) has closed-form given by \u2207v\u03b3,r(x, y, z) = ( \u2207xf(x, \u03b8\u2217) + \u03bb\u2217\u2207xg(x, \u03b8\u2217),\n(y \u2212 \u03b8\u2217) \u03b31 , (\u03bb\u2217 \u2212 z) \u03b32\n) , (24)\nwhere \u03b8\u2217 := \u03b8\u2217(x, y, z) and \u03bb\u2217 := \u03bb\u2217(x, y, z) is the unique saddle point of the following minmax problem:\nmin \u03b8\u2208Y max \u03bb\u22650\n{ f(x, \u03b8) + \u03bbg(x, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u22252\n} . (25)\n(3) Furthermore, for any \u03c1v \u2265 \u03c1f/(1 \u2212 \u03b31\u03c1f ), v\u03b3(x, y, z) is \u03c1v-weakly convex with respect to variables (x, y) on X \u00d7 Y for any fixed z \u2208 Rl.\nLemma A.2. Under the assumption of Lemma A.1, let \u03b31 \u2208 (0, 1/\u03c1f ), \u03b32 > 0, and (x\u0304, y\u0304, z\u0304) \u2208 X\u00d7Y \u00d7Rl. Then for any \u03c1v \u2265 \u03c1f/(1\u2212\u03b31\u03c1f ) and (x, y) in X\u00d7Y , the following inequality holds:\n\u2212v\u03b3,r(x, y, z\u0304) \u2264 \u2212v\u03b3,r(x\u0304, y\u0304, z\u0304)\u2212 \u27e8\u2207xyv\u03b3,r(x\u0304, y\u0304, z\u0304), (x, y)\u2212 (x\u0304, y\u0304)\u27e9+ \u03c1v 2 \u2225(x, y)\u2212 (x\u0304, y\u0304)\u22252.\nProof. The conclusion follows directly from Lemma A.1 that v\u03b3,r(x, y, z) is \u03c1v-weakly convex with respect to variables (x, y) on X \u00d7 Y for any fixed z \u2208 Rl.\nLemma A.3. Under Assumptions 3.2 and 3.3, let \u03b31 \u2208 (0, 1/\u03c1f ), \u03b32 > 0. Then, for any (x, y, z), (x\u2032, y\u2032, z\u2032) \u2208 X \u00d7 Rm \u00d7 Rl, the following Lipschitz property holds:\n\u2225(\u03b8\u2217(x, y, z), \u03bb\u2217(x, y, z))\u2212 (\u03b8\u2217(x\u2032, y\u2032, z\u2032), \u03bb\u2217(x\u2032, y\u2032, z\u2032))\u2225\n\u2264 Lf + CZLg2 + Lg \u03c1T \u2225x\u2212 x\u2032\u2225+ 1 \u03b31\u03c1T \u2225y \u2212 y\u2032\u2225+ 1 \u03b32\u03c1T \u2225z \u2212 z\u2032\u2225 \u2264L\u03b8\u03bb\u2225(x, y, z)\u2212 (x\u2032, y\u2032, z\u2032)\u2225,\n(26)\nwhere \u03c1T := min{1/\u03b31 \u2212 \u03c1f , 1/\u03b32}, CZ := maxz\u2208Z \u2225z\u2225 and L\u03b8\u03bb := \u221a 3max{Lf + CZLg2 + Lg, 1/\u03b31, 1/\u03b32}/\u03c1T .\nProof. For succinctness, we denote (x, y, z) and (x\u2032, y\u2032, z\u2032) by w and w\u2032, respectively. Given that (\u03b8\u2217(w), \u03bb\u2217(w)) is the saddle point for min-max problem\nmin \u03b8\u2208Y max \u03bb\u2208Z\n{ f(x, \u03b8) + \u03bbg(x, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u22252\n} ,\nit follows from the stationary condition that\n0 \u2208 \u2207yf(x, \u03b8\u2217(w)) + \u03bb\u2217(w)\u2207yg(x, \u03b8\u2217(w)) + (\u03b8\u2217(w)\u2212 y)/\u03b31 +NY (\u03b8\u2217(w)), 0 \u2208 \u2212g(x, \u03b8\u2217(w))) + (\u03bb\u2217(w)\u2212 z)/\u03b32 +NZ(\u03bb\u2217(w)).\n(27)\nUnder Assumptions 3.2 and 3.3, and that \u03b31 \u2208 (0, 1/\u03c1f ) and \u03b32 > 0, we know that the function\nf(x, \u03b8) + \u03bbg(x, \u03b8) + 1 2\u03b31 \u2225\u03b8 \u2212 y\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u22252\nis (1/\u03b31 \u2212 \u03c1f )-strongly convex and 1/\u03b32-strongly concave with respect to \u03b8 and \u03bb, respectively. Then, it follows from (Rockafellar & Wets, 2009, Theorem 12.17 and Exercise 12.59) that the operator\nTw(\u03b8, \u03bb) := (\u2207yf(x, \u03b8) + \u03bb\u2207yg(x, \u03b8) + (\u03b8 \u2212 y)/\u03b31 +NY (\u03b8),\u2212g(x, \u03b8) + (\u03bb\u2212 z)/\u03b32 +NZ(\u03bb)) is \u03c1T := min{1/\u03b31 \u2212 \u03c1f , 1/\u03b32}-strongly monotone. Using Tw(\u03b8, \u03bb), the inclusion (27) can be rewritten as 0 \u2208 Tw(\u03b8\u2217(w), \u03bb\u2217(w)). Similarly, since (\u03b8\u2217(w\u2032), \u03bb\u2217(w\u2032)) is a saddle point for min-max problem\nmin \u03b8\u2208Y max \u03bb\u2208Z\n{ f(x\u2032, \u03b8) + \u03bbg(x\u2032, \u03b8) + 1\n2\u03b31 \u2225\u03b8 \u2212 y\u2032\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u2032\u22252\n} ,\nwe have 0 \u2208 Tw\u2032(\u03b8\u2217(w\u2032), \u03bb\u2217(w\u2032)).\nNext, by the definition of Tw(\u03b8, \u03bb), we have\n(e1, e2) \u2208 Tw(\u03b8\u2217(w\u2032), \u03bb\u2217(w\u2032)), with\ne1 := \u2207yf(x, \u03b8\u2217(w\u2032))\u2212\u2207yf(x\u2032, \u03b8\u2217(w\u2032)) + \u03bb\u2217(w\u2032) (\u2207yg(x, \u03b8\u2217(w\u2032))\u2212\u2207yg(x\u2032, \u03b8\u2217(w\u2032))) + (y\u2032 \u2212 y)/\u03b31,\ne2 := \u2212g(x, \u03b8\u2217(w\u2032))) + g(x\u2032, \u03b8\u2217(w\u2032))) + (z\u2032 \u2212 z)/\u03b32. Given that Tw(\u03b8, \u03bb) is \u03c1T -strongly monotone, we have\n\u27e8\u2212(e1, e2), (\u03b8\u2217(w), \u03bb\u2217(w))\u2212 (\u03b8\u2217(w\u2032), \u03bb\u2217(w\u2032))\u27e9 \u2265\u03c1T \u2225(\u03b8\u2217(w), \u03bb\u2217(w))\u2212 (\u03b8\u2217(w\u2032), \u03bb\u2217(w\u2032))\u22252.\n(28)\nFor \u03bb\u2217(w), we can obtain that\n\u03bb\u2217(w) = argmax \u03bb\u2208Z\n{ f(x, \u03b8\u2217(w)) + \u03bbg(x, \u03b8\u2217(w)) + 1\n2\u03b31 \u2225\u03b8\u2217(w)\u2212 y\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 z\u22252 } = ProjZ (z + \u03b32g(x, \u03b8 \u2217(w))) .\nSince Z is a bounded set, it follows that \u03bb\u2217(w) \u2264 CZ , for any x \u2208 X, y \u2208 Y, z \u2208 Z. According to Assumptions 3.2 and 3.3 and the fact that \u03b8\u2217(w) \u2208 Y , we can derive that\n\u2225e1\u2225 \u2264 Lf\u2225x\u2212 x\u2032\u2225+ CZLg2\u2225x\u2212 x\u2032\u2225+ 1\n\u03b31 \u2225y \u2212 y\u2032\u2225,\nand \u2225e2\u2225 \u2264 Lg\u2225x\u2212 x\u2032\u2225+ 1\n\u03b32 \u2225z \u2212 z\u2032\u2225.\nThus, it follows from inequality (28) that\n\u2225(\u03b8\u2217(w), \u03bb\u2217(w))\u2212 (\u03b8\u2217(w\u2032), \u03bb\u2217(w\u2032))\u2225\n\u2264 1 \u03c1T \u2225(e1, e2)\u2225 \u2264 1 \u03c1T (\u2225e1\u2225+ \u2225e2\u2225)\n\u2264 Lf + CZLg2 + Lg \u03c1T \u2225x\u2212 x\u2032\u2225+ 1 \u03b31\u03c1T \u2225y \u2212 y\u2032\u2225+ 1 \u03b32\u03c1T \u2225z \u2212 z\u2032\u2225,\nwhich implies the desired result.\nLemma A.4. Suppose Assumptions of Lemma A.3 holds, and let \u03b31 \u2208 (0, 1/\u03c1f ), \u03b32 > 0 and (x\u0304, y\u0304, z\u0304) \u2208 X \u00d7 Rm \u00d7 Rl. Then for any z \u2208 Rl, we have\n\u2212v\u03b3,r(x\u0304, y\u0304, z) \u2264 \u2212v\u03b3,r(x\u0304, y\u0304, z\u0304)\u2212 \u27e8\u2207zv\u03b3,r(x\u0304, y\u0304, z\u0304), z \u2212 z\u0304\u27e9+ Lvz 2 \u2225z \u2212 z\u0304\u22252,\nwith Lvz := (\u03b32\u03c1T + 1)/(\u03b3 2 2\u03c1T ).\nProof. By using Lemma A.3, with fixed (x\u0304, y\u0304, z\u0304) \u2208 X \u00d7 Rm \u00d7 Rl, for any z \u2208 Rl, we have\n\u2225(\u03b8\u2217(x\u0304, y\u0304, z), \u03bb\u2217(x\u0304, y\u0304, z))\u2212 (\u03b8\u2217(x\u0304, y\u0304, z\u0304), \u03bb\u2217(x\u0304, y\u0304, z\u0304))\u2225 \u2264 1 \u03b32\u03c1T \u2225z \u2212 z\u0304\u2225,\nand thus it follows from Lemma A.1 that\n\u2225\u2207zv\u03b3,r(x\u0304, y\u0304, z)\u2212\u2207zv\u03b3,r(x\u0304, y\u0304, z\u0304)\u2225 = 1\n\u03b32 \u2225\u03bb\u2217(x\u0304, y\u0304, z)\u2212 z \u2212 \u03bb\u2217(x\u0304, y\u0304, z\u0304) + z\u0304\u2225\n\u2264 \u03b32\u03c1T + 1 \u03b322\u03c1T \u2225z \u2212 z\u0304\u2225.\nThen the conclusion follows from (Beck, 2017, Lemma 5.7).\nLemma A.5. Under Assumption of Lemma A.3, let \u03b31 \u2208 (0, 1/\u03c1f ), \u03b32 > 0 and \u03b7k \u2208 (0, \u03c1T /L2B) with \u03c1T := min{1/\u03b31 \u2212 \u03c1f , 1/\u03b32} and LB := max{Lf +Lg +CZLg2 + 1/\u03b31, Lg + 1/\u03b32}, then, the sequence of (xk, yk, zk, \u03b8k, \u03bbk) generated by Algorithm 1 satisfies\u2225\u2225(\u03b8k+1, \u03bbk+1)\u2212 (\u03b8\u2217(xk, yk, zk), \u03bb\u2217(xk, yk, zk))\u2225\u2225\n\u2264 (1\u2212 \u03b7k\u03c1T ) \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217(xk, yk, zk), \u03bb\u2217(xk, yk, zk))\u2225\u2225 . (29)\nProof. With given (xk, yk, zk) \u2208 X \u00d7 Y \u00d7 Z, we denote \u03b8\u2217(xk, yk, zk) and \u03bb\u2217(xk, yk, zk) by \u03b8\u2217 and \u03bb\u2217, respectively, for conciseness. By Assumptions 3.2 and 3.3, and that \u03b31 \u2208 (0, 1/\u03c1f ), \u03b32 > 0, we know that\nf(xk, \u03b8) + \u03bbg(xk, \u03b8) + 1 2\u03b31 \u2225\u03b8 \u2212 yk\u22252 \u2212 1 2\u03b32 \u2225\u03bb\u2212 zk\u22252\nis (1/\u03b31 \u2212 \u03c1f )-strongly convex and 1/\u03b32-strongly concave with respect to \u03b8 and \u03bb, respectively. Then, the proximal min-max problem in equation (3) is equivalent to finding (\u03b8, \u03bb) satisfying\n0 \u2208 (A+B)(\u03b8, \u03bb),\nwith A(\u03b8, \u03bb) := NY (\u03b8)\u00d7NZ(\u03bb),\nand B(\u03b8, \u03bb) := ( \u2207yf(xk, \u03b8) + \u03bb\u2207yg(xk, \u03b8) + (\u03b8 \u2212 yk)/\u03b31,\u2212g(xk, \u03b8) + (\u03bb\u2212 zk)/\u03b32 ) .\nAnd, therefore, 0 \u2208 (A+B)(\u03b8\u2217, \u03bb\u2217). Because f(xk, \u03b8)+\u03bbg(xk, \u03b8)+ 12\u03b31 \u2225\u03b8\u2212y k\u22252\u2212 12\u03b32 \u2225\u03bb\u2212z k\u22252 is (1/\u03b31 \u2212 \u03c1f )-strongly convex and 1/\u03b32-strongly concave with respect to \u03b8 and \u03bb, respectively, it follows from (Rockafellar & Wets, 2009, Theorem 12.17 and Exercise 12.59) that the operator B is \u03c1T := min{1/\u03b31 \u2212 \u03c1f , 1/\u03b32}-strongly monotone on Y \u00d7 Z. And by Assumptions 3.2 and 3.3, we have that for any (\u03b8, \u03bb), (\u03b8\u2032, \u03bb\u2032) \u2208 Y \u00d7 Z,\n\u2225B(\u03b8, \u03bb)\u2212B(\u03b8\u2032, \u03bb\u2032)\u2225 \u2264\u2225\u2207yf(xk, \u03b8)\u2212\u2207yf(xk, \u03b8\u2032)\u2225+ \u2225\u03bb\u2207yg(xk, \u03b8)\u2212 \u03bb\u2032\u2207yg(xk, \u03b8\u2032)\u2225\n+ 1 \u03b31 \u2225\u03b8 \u2212 \u03b8\u2032\u2225+ \u2225g(xk, \u03b8)\u2212 g(xk, \u03b8\u2032)\u2225+ 1 \u03b32 \u2225\u03bb\u2212 \u03bb\u2032\u2225\n\u2264 ( Lf + Lg + 1\n\u03b31\n) \u2225\u03b8 \u2212 \u03b8\u2032\u2225+ 1\n\u03b32 \u2225\u03bb\u2212 \u03bb\u2032\u2225\n+ \u2225\u03bb\u2207yg(xk, \u03b8)\u2212 \u03bb\u2032\u2207yg(xk, \u03b8)\u2225+ \u2225\u03bb\u2032\u2207yg(xk, \u03b8)\u2212 \u03bb\u2032\u2207yg(xk, \u03b8\u2032)\u2225 \u2264 ( Lf + Lg + CZLg2 + 1\n\u03b31\n) \u2225\u03b8 \u2212 \u03b8\u2032\u2225+ ( Lg + 1\n\u03b32\n) \u2225\u03bb\u2212 \u03bb\u2032\u2225,\nwhere the last inequality follows from the fact that CZ := maxz\u2208Z \u2225z\u2225 and maxx\u2208X,\u03b8\u2208Y \u2225\u2207yg(x, \u03b8)\u2225 \u2264 Lg . Therefore, we obtain that the operator B is LB := max{Lf + Lg + CZLg2 + 1/\u03b31, Lg + 1/\u03b32}-Lipschitz continuous. Then, we can have from (Bauschke & Combettes, 2011, Proposition 26.1(iv)) that (\u03b8\u2217, \u03bb\u2217) = (Id+\u03b7kA)\u22121((\u03b8\u2217, \u03bb\u2217)\u2212\u03b7kB(\u03b8\u2217, \u03bb\u2217)), with Id denoting the identity operator. Since (\u03b8k+1, \u03bbk+1) = (Id + \u03b7kA)\u22121((\u03b8k, \u03bbk) \u2212 \u03b7kB(\u03b8k, \u03bbk)), as shown in the proof of (Bauschke & Combettes, 2011, Proposition 26.9) that when \u03b7k \u2208 (0, 2\u03c1T /L2B),\n\u2225(\u03b8k+1, \u03bbk+1)\u2212 (\u03b8\u2217, \u03bb\u2217)\u2225 \u2264 ( 1\u2212 \u03b7k(2\u03c1T \u2212 \u03b7kL2B) ) \u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217, \u03bb\u2217)\u2225.\nWhen \u03b7k \u2208 (0, \u03c1T /L2B), we can obtain from the above inequality that\n\u2225(\u03b8k+1, \u03bbk+1)\u2212 (\u03b8\u2217, \u03bb\u2217)\u2225 \u2264 (1\u2212 \u03b7k\u03c1T ) \u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217, \u03bb\u2217)\u2225.\nAs previously stated that the update of variables (x, y, z) in equation (10) can be interpreted as inexact alternating proximal gradient from (xk, yk, zk) on min(x,y)\u2208C,z\u2208Z \u03d5ck(x, y, z), in which \u03d5ck is defined in equation (13) as\n\u03d5ck(x, y, z) := 1\nck\n( F (x, y)\u2212 F ) + f(x, y)\u2212 v\u03b3,r(x, y, z).\nSince v\u03b3,r(x, y, z) \u2264 v\u03b3(x, y, z) \u2264 f(x, y) for all (x, y) \u2208 C, by Assumption 3.1, we have \u03d5ck(x, y, z) \u2265 0 for all (x, y, z) \u2208 C \u00d7 Rl. In the following lemma, we demonstrate that the function \u03d5ck(x, y, z) exhibits a decreasing property with errors at each iteration.\nLemma A.6. Under Assumptions 3.1, 3.2 and 3.3, let \u03b31 \u2208 (0, 1/\u03c1f ) and \u03b32 > 0. Then the sequence of (xk, yk, \u03b8k) generated by Algorithm 1 satisfies\n\u03d5ck(x k+1, yk+1, zk+1) \u2264\u03d5ck(xk, yk, zk) \u2212 ( 1\n2\u03b1k \u2212 L\u03d5k 2 \u2212 \u03b2kL\n2 \u03b8\u03bb\n\u03b322\n) \u2225(xk+1, yk+1)\u2212 (xk, yk)\u22252\n\u2212 ( 1\n2\u03b2k \u2212 Lvz 2\n) \u2225zk+1 \u2212 zk\u22252\n+ ( \u03b1kL 2 g +\n\u03b2k \u03b322 )\u2225\u2225\u03bbk+1 \u2212 \u03bb\u2217(xk, yk, zk)\u2225\u22252 + \u03b1k 2 ( 2(Lf + CZLg1) 2 + 1\n\u03b321\n)\u2225\u2225\u03b8k+1 \u2212 \u03b8\u2217(xk, yk, zk)\u2225\u22252 ,\n(30)\nwhere L\u03d5k := LF /ck + Lf + \u03c1v .\nProof. Given Assumptions 3.1 and 3.2 that \u2207F and \u2207f are LF - and Lf -Lipschitz continuous on X \u00d7 Y , respectively, and applying (Beck, 2017, Lemma 5.7) and Lemma A.2, we obtain\n\u03d5ck(x k+1, yk+1, zk) \u2264\u03d5ck(xk, yk, zk) + \u27e8\u2207xy\u03d5ck(xk, yk, zk), (xk+1, yk+1)\u2212 (xk, yk)\u27e9\n+ L\u03d5k 2\n\u2225(xk+1, yk+1)\u2212 (xk, yk)\u22252, (31)\nwith L\u03d5k := LF /ck + Lf + \u03c1v . Based on the update rule of variable (x, y) in equation (10), the convexity of C := {(x, y) \u2208 X \u00d7 Y | g(x, y) \u2264 0} and the property of the projection operator ProjC , we have\u2329\n(xk, yk)\u2212 \u03b1k(dkx, dky)\u2212 (xk+1, yk+1), (xk, yk)\u2212 (xk+1, yk+1) \u232a \u2264 0,\nleading to \u2329 (dkx, d k y), (x k+1, yk+1)\u2212 (xk, yk) \u232a \u2264 \u2212 1\n\u03b1k \u2225(xk+1, yk+1)\u2212 (xk, yk)\u22252.\nCombining this with inequality (31), we infer that\n\u03d5ck(x k+1, yk+1, zk) \u2264\u03d5ck(xk, yk, zk)\u2212\n( 1\n\u03b1k \u2212 L\u03d5k 2\n) \u2225(xk+1, yk+1)\u2212 (xk, yk)\u22252\n+ \u2329 \u2207xy\u03d5ck(xk, yk, zk)\u2212 (dkx, dky), (xk+1, yk+1)\u2212 (xk, yk) \u232a .\n(32)\nIt should be noticed that since (xk, yk, zk) \u2208 X \u00d7 Y \u00d7 Z and (\u03b8\u2217(xk, yk, zk), \u03bb\u2217(xk, yk, zk)), (\u03b8k, \u03bbk) \u2208 Y \u00d7 Z for all k, it holds that \u03bbk \u2208 Z and \u2225\u2207xg(xk, \u03b8\u2217(xk, yk, zk)\u2225 \u2264 maxx\u2208X,\u03b8\u2208Y \u2225\u2207xg(x, \u03b8)\u2225 \u2264 Lg for all k. Considering the formula of \u2207xyv\u03b3,r(x, y, z) derived in Lemma A.2 and the definitions of dkx and d\nk y provided in equation (11), we can obtain that\u2225\u2225\u2207xy\u03d5ck(xk, yk, zk)\u2212 (dkx, dky)\u2225\u22252\n= \u2225\u2225\u2207xf(xk, \u03b8\u2217(xk, yk, zk)) + \u03bb\u2217(xk, yk, zk)\u2207xg(xk, \u03b8\u2217(xk, yk, zk))\n\u2212\u2207xf(xk, \u03b8k+1)\u2212 \u03bbk+1\u2207xg(xk, \u03b8k+1) \u2225\u22252 + 1\n\u03b321 \u2225\u03b8\u2217(xk, yk, zk)\u2212 \u03b8k+1\u22252\n\u2264 2 \u2225\u2225\u2207xf(xk, \u03b8\u2217(xk, yk, zk)) + \u03bbk+1\u2207xg(xk, \u03b8\u2217(xk, yk, zk)) \u2212\u2207xf(xk, \u03b8k+1)\u2212 \u03bbk+1\u2207xg(xk, \u03b8k+1)\n\u2225\u22252 + 2\u2225\u03bb\u2217(xk, yk, zk)\u2207xg(xk, \u03b8\u2217(xk, yk, zk))\u2212 \u03bbk+1\u2207xg(xk, \u03b8\u2217(xk, yk, zk))\u22252\n+ 1\n\u03b321 \u2225\u03b8\u2217(xk, yk, zk)\u2212 \u03b8k+1\u22252\n\u2264 ( 2(Lf + CZLg1) 2 + 1\n\u03b321\n)\u2225\u2225\u03b8k+1 \u2212 \u03b8\u2217(xk, yk, zk)\u2225\u22252 + 2L2g \u2225\u2225\u03bbk+1 \u2212 \u03bb\u2217(xk, yk, zk)\u2225\u22252 ,\n(33)\nwhere the last inequality follows from Assumptions 3.2 and 3.3, \u2225\u03bbk+1\u2225 \u2264 CZ and \u2225\u2207xg(xk, \u03b8\u2217(xk, yk, zk))\u2225 \u2264 Lg . This yields\u2329\n\u2207xy\u03d5ck(xk, yk, zk)\u2212 (dkx, dky), (xk+1, yk+1)\u2212 (xk, yk) \u232a\n\u2264 \u03b1k 2\n( 2(Lf + CZLg1) 2 + 1/\u03b321 ) \u2225\u2225\u03b8k+1 \u2212 \u03b8\u2217(xk, yk, zk)\u2225\u22252 + \u03b1kL2g \u2225\u2225\u03bbk+1 \u2212 \u03bb\u2217(xk, yk, zk)\u2225\u22252\n+ 1\n2\u03b1k \u2225(xk+1, yk+1)\u2212 (xk, yk)\u22252,\nwhich combing with inequality (32) leads to\n\u03d5ck(x k+1, yk+1, zk) \u2264\u03d5ck(xk, yk, zk)\u2212 ( 1\n2\u03b1k \u2212 L\u03d5k 2\n) \u2225(xk+1, yk+1)\u2212 (xk, yk)\u22252\n+ \u03b1k 2\n( 2(Lf + CZLg1) 2 + 1\n\u03b321 )\u2225\u2225\u03b8k+1 \u2212 \u03b8\u2217(xk, yk, zk)\u2225\u22252 + \u03b1kL 2 g \u2225\u2225\u03bbk+1 \u2212 \u03bb\u2217(xk, yk, zk)\u2225\u22252 . (34)\nAccording to the update rule of variable z in equation (10) and the property of the projection operator ProjZ , we have \u2329\ndkz , z k+1 \u2212 zk\n\u232a \u2264 \u2212 1\n\u03b2k \u2225zk+1 \u2212 zk\u22252. (35)\nUsing Lemma A.4, we obtain\n\u03d5ck(x k+1, yk+1, zk+1)\n\u2264\u03d5ck(xk+1, yk+1, zk) + \u27e8\u2207z\u03d5ck(xk+1, yk+1, zk), zk+1 \u2212 zk\u27e9+ Lvz 2\n\u2225zk+1 \u2212 zk\u22252. (36)\nCombining this with inequality (35), we can derive\n\u03d5ck(x k+1, yk+1, zk+1) \u2264\u03d5ck(xk+1, yk+1, zk)\u2212 ( 1\n\u03b2k \u2212 Lvz 2\n) \u2225zk+1 \u2212 zk\u22252\n+ \u2329 \u2207z\u03d5ck(xk+1, yk+1, zk)\u2212 dkz , zk+1 \u2212 zk \u232a .\n(37)\nUsing the definition of \u03d5ck and the formula of \u2207zv\u03b3,r derived in Lemma A.1 and the definition of dkz provided in equation (11), we have\u2225\u2225\u2207z\u03d5ck(xk+1, yk+1, zk)\u2212 dkz\u2225\u22252 = \u2225\u2225\u2212\u2207zv\u03b3,r(xk+1, yk+1, zk)\u2212 dkz\u2225\u22252\n= \u2225\u2225(zk \u2212 \u03bb\u2217(xk+1, yk+1, zk))/\u03b32 \u2212 (zk \u2212 \u03bbk+1)/\u03b32\u2225\u22252\n= 1\n\u03b322 \u2225\u2225\u03bbk+1 \u2212 \u03bb\u2217(xk+1, yk+1, zk)\u2225\u22252 , and thus\u2329\n\u2207z\u03d5ck(xk+1, yk+1, zk)\u2212 dkz , zk+1 \u2212 zk \u232a \u2264 \u03b2k 2\u03b322 \u2225\u2225\u03bbk+1 \u2212 \u03bb\u2217(xk+1, yk+1, zk)\u2225\u22252 + 1\n2\u03b2k \u2225zk+1 \u2212 zk\u22252.\nThen, we have from inequality (37) that\n\u03d5ck(x k+1, yk+1, zk+1) \u2264\u03d5ck(xk+1, yk+1, zk)\u2212 ( 1\n2\u03b2k \u2212 Lvz 2\n) \u2225zk+1 \u2212 zk\u22252\n+ \u03b2k 2\u03b322 \u2225\u2225\u03bbk+1 \u2212 \u03bb\u2217(xk+1, yk+1, zk)\u2225\u22252 \u2264\u03d5ck(xk+1, yk+1, zk)\u2212 ( 1\n2\u03b2k \u2212 Lvz 2\n) \u2225zk+1 \u2212 zk\u22252 + \u03b2k\n\u03b322 \u2225\u2225\u03bbk+1 \u2212 \u03bb\u2217(xk, yk, zk)\u2225\u22252 + \u03b2kL 2 \u03b8\u03bb\n\u03b322 \u2225(xk+1, yk+1)\u2212 (xk, yk)\u22252.\n(38)\nwhere the last inequality follows from Lemma A.3. The conclusion follows by combining estimates (34) and (38)."
        },
        {
            "heading": "A.5 PROOF OF LEMMA 3.1",
            "text": "By utilizing the auxiliary lemmas established in the previous section, we will demonstrate the decreasing property of\nVk := \u03d5ck(x k, yk, zk) + C\u03b8\u03bb \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217(xk, yk, zk), \u03bb\u2217(xk, yk, zk))\u2225\u22252 , (39) where C\u03b8\u03bb := max{(Lf + CZLg1)2 + 1/(2\u03b321) + L2g, 1/\u03b322}, and\n\u03d5ck(x, y, z) := 1\nck\n( F (x, y)\u2212 F ) + f(x, y)\u2212 v\u03b3,r(x, y, z). (13)\nLemma A.7. Under Assumptions 3.1, 3.2 and 3.3, let \u03b31 \u2208 (0, 1/\u03c1f ), \u03b32 > 0, ck+1 \u2265 ck and \u03b7k \u2208 (\u03b7, \u03c1T /L2B) with \u03b7 > 0, \u03c1T := min{1/\u03b31 \u2212 \u03c1f , 1/\u03b32} and LB := max{Lf +Lg +CZLg2 + 1/\u03b31, Lg +1/\u03b32}, then there exist constants c\u03b1, c\u03b2 > 0 such that when 0 < \u03b1k \u2264 c\u03b1 and 0 < \u03b2k \u2264 c\u03b2 , the sequence of (xk, yk, zk) generated by Algorithm 1 satisfies\nVk+1 \u2212 Vk \u2264 \u2212 1 4\u03b1k \u2225(xk+1, yk+1)\u2212 (xk, yk)\u22252 \u2212 1 4\u03b2k \u2225zk+1 \u2212 zk\u22252\n\u2212 \u03b7\u03c1TC\u03b8\u03bb \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217(xk, yk, zk), \u03bb\u2217(xk, yk, zk))\u2225\u22252 . (40)\nProof. For succinctness, we denote (xk, yk, zk) bywk. Let us first recall estimate (30) from Lemma A.6, which states that\n\u03d5ck(w k+1) \u2264\u03d5ck(wk)\u2212\n( 1\n2\u03b1k \u2212 L\u03d5k 2 \u2212 \u03b2kL\n2 \u03b8\u03bb\n\u03b322\n) \u2225(xk+1, yk+1)\u2212 (xk, yk)\u22252\n\u2212 ( 1\n2\u03b2k \u2212 Lvz 2\n) \u2225zk+1 \u2212 zk\u22252 + ( \u03b1kL 2 g +\n\u03b2k \u03b322 )\u2225\u2225\u03bbk+1 \u2212 \u03bb\u2217(wk)\u2225\u22252 + \u03b1k 2 ( 2(Lf + CZLg1) 2 + 1\n\u03b321\n)\u2225\u2225\u03b8k+1 \u2212 \u03b8\u2217(wk)\u2225\u22252 . (41)\nSince ck+1 \u2265 ck, we can infer that (F (xk+1, yk+1) \u2212 F )/ck+1 \u2264 (F (xk+1, yk+1) \u2212 F )/ck. Combining with inequality (41) leads to Vk+1 \u2212 Vk =\u03d5ck+1(wk+1)\u2212 \u03d5ck(wk)\n+ C\u03b8\u03bb \u2225\u2225(\u03b8k+1, \u03bbk+1)\u2212 (\u03b8\u2217(wk+1), \u03bb\u2217(wk+1))\u2225\u22252 \u2212 C\u03b8\u03bb \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u2225\u22252\n\u2264\u03d5ck(xk+1, yk+1, zk+1)\u2212 \u03d5ck(xk, yk, zk) + C\u03b8\u03bb \u2225\u2225(\u03b8k+1, \u03bbk+1)\u2212 (\u03b8\u2217(wk+1), \u03bb\u2217(wk+1))\u2225\u22252 \u2212 C\u03b8\u03bb \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u2225\u22252\n\u2264 \u2212 ( 1\n2\u03b1k \u2212 L\u03d5k 2 \u2212 \u03b2kL\n2 \u03b8\u03bb\n\u03b322\n) \u2225(xk+1, yk+1)\u2212 (xk, yk)\u22252\n\u2212 ( 1\n2\u03b2k \u2212 Lvz 2\n) \u2225zk+1 \u2212 zk\u22252 + C\u03b8\u03bb \u2225\u2225(\u03b8k+1, \u03bbk+1)\u2212 (\u03b8\u2217(wk+1), \u03bb\u2217(wk+1))\u2225\u22252 \u2212 C\u03b8\u03bb \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u2225\u22252 + (\u03b1kL2g + \u03b2k\u03b322 )\u2225\u2225\u03bbk+1 \u2212 \u03bb\u2217(wk)\u2225\u22252\n+ \u03b1k 2\n( 2(Lf + CZLg1) 2 + 1\n\u03b321 )\u2225\u2225\u03b8k+1 \u2212 \u03b8\u2217(wk)\u2225\u22252 \u2264 \u2212 ( 1\n2\u03b1k \u2212 L\u03d5k 2 \u2212 \u03b2kL\n2 \u03b8\u03bb\n\u03b322\n) \u2225(xk+1, yk+1)\u2212 (xk, yk)\u22252 \u2212 ( 1\n2\u03b2k \u2212 Lvz 2\n) \u2225zk+1 \u2212 zk\u22252\n+ C\u03b8\u03bb \u2225\u2225(\u03b8k+1, \u03bbk+1)\u2212 (\u03b8\u2217(wk+1), \u03bb\u2217(wk+1))\u2225\u22252 \u2212 C\u03b8\u03bb \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u2225\u22252\n+ 2max{\u03b1k, \u03b2k}C\u03b8\u03bb \u2225\u2225(\u03b8k+1, \u03bbk+1)\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u2225\u22252 ,\n(42) where the last inequality follows from the fact that\nC\u03b8\u03bb := max { (Lf + CZLg1) 2 + 1/(2\u03b321) + L 2 g, 1/\u03b3 2 2 } .\nWe can demonstrate that\u2225\u2225(\u03b8k+1, \u03bbk+1)\u2212 (\u03b8\u2217(wk+1), \u03bb\u2217(wk+1))\u2225\u22252 \u2212 \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u2225\u22252 + 2\u03b1k\n\u2225\u2225(\u03b8k+1, \u03bbk+1)\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u2225\u22252 \u2264 (1 + \u03f5k + 2\u03b1k)\n\u2225\u2225(\u03b8k+1, \u03bbk+1)\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u2225\u22252 \u2212 \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u2225\u22252 + (1 + 1\n\u03f5k )\u2225(\u03b8\u2217(wk+1), \u03bb\u2217(wk+1))\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u22252\n\u2264 (1 + \u03f5k + 2\u03b1k)(1\u2212 \u03b7k\u03c1T )2\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u22252 \u2212 \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u2225\u22252\n+ (1 + 1\n\u03f5k )L2\u03b8\u03bb\n\u2225\u2225wk+1 \u2212 wk\u2225\u22252 ,\nfor any \u03f5k > 0, where the second inequality is a consequence of Lemmas A.3 and A.5. By setting \u03f5k = \u03b7k\u03c1T /2 in the above inequality, we obtain that when \u03b1k \u2264 \u03b7k\u03c1T /4, it holds that (1 + \u03f5k + 2\u03b1k)(1\u2212 \u03b7k\u03c1T ) \u2264 1 and thus\u2225\u2225(\u03b8k+1, \u03bbk+1)\u2212 (\u03b8\u2217(wk+1), \u03bb\u2217(wk+1))\u2225\u22252 \u2212 \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u2225\u22252\n+ 2\u03b1k \u2225\u2225(\u03b8k+1, \u03bbk+1)\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u2225\u22252\n\u2264 \u2212 \u03b7k\u03c1T \u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u22252 + ( 1 + 2\n\u03b7k\u03c1T\n) L2\u03b8\u03bb \u2225\u2225wk+1 \u2212 wk\u2225\u22252 , (43) Similarly, we can show that when \u03b2k \u2264 \u03b7k\u03c1T /4, it holds that\u2225\u2225(\u03b8k+1, \u03bbk+1)\u2212 (\u03b8\u2217(wk+1), \u03bb\u2217(wk+1))\u2225\u22252 \u2212 \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u2225\u22252\n+ 2\u03b2k \u2225\u2225(\u03b8k+1, \u03bbk+1)\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u2225\u22252\n\u2264 \u2212 \u03b7k\u03c1T \u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u22252 + ( 1 + 2\n\u03b7k\u03c1T\n) L2\u03b8\u03bb \u2225\u2225wk+1 \u2212 wk\u2225\u22252 . (44) Combining estimates (42), (43) and (44), we have\nVk+1 \u2212 Vk \u2264 \u2212 ( 1\n2\u03b1k \u2212 L\u03d5k 2 \u2212 \u03b2kL\n2 \u03b8\u03bb \u03b322 \u2212 ( 1 + 2 \u03b7k\u03c1T ) L2\u03b8\u03bbC\u03b8\u03bb )\u2225\u2225(xk+1, yk+1)\u2212 (xk, yk)\u2225\u22252 \u2212 ( 1\n2\u03b2k \u2212 Lvz 2 \u2212 ( 1 + 2 \u03b7k\u03c1T ) L2\u03b8\u03bbC\u03b8\u03bb ) \u2225zk+1 \u2212 zk\u22252\n\u2212 \u03b7k\u03c1TC\u03b8\u03bb\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217(wk), \u03bb\u2217(wk))\u22252. (45)\nWhen ck+1 \u2265 ck, \u03b7k \u2265 \u03b7 > 0, \u03b1k \u2264 \u03b7\u03c1T /4 and \u03b2k \u2264 \u03b7\u03c1T /4, it holds that for any k, \u03b1k \u2264 \u03b7k\u03c1T /4, \u03b2k \u2264 \u03b7k\u03c1T /4,\nL\u03d5k 2 + \u03b2kL\n2 \u03b8\u03bb\n\u03b322 +\n( 1 + 2\n\u03b7k\u03c1T\n) L2\u03b8\u03bbC\u03b8\u03bb \u2264\nL\u03d50 2 + + \u03b7\u03c1TL\n2 \u03b8\u03bb\n4\u03b322 +\n( 1 + 2\n\u03b7\u03c1T\n) L2\u03b8\u03bbC\u03b8\u03bb =: C\u03b1,\n(46) and\nLvz 2 +\n( 1 + 2\n\u03b7k\u03c1T\n) L2\u03b8\u03bbC\u03b8\u03bb \u2264\nLvz 2 +\n( 1 + 2\n\u03b7\u03c1T\n) L2\u03b8\u03bbC\u03b8\u03bb =: C\u03b2 , (47)\nConsequently, if c\u03b1, c\u03b2 > 0 satisfies c\u03b1 \u2264 min { 1\n4 \u03b7\u03c1T ,\n1\n4C\u03b1\n} , c\u03b2 \u2264 min { 1\n4 \u03b7\u03c1T ,\n1\n4C\u03b2\n} , (48)\nthen, when 0 < \u03b1k \u2264 c\u03b1 and 0 < \u03b2k \u2264 c\u03b2 , it holds that\n1 2\u03b1k \u2212 L\u03d5k 2 \u2212 \u03b2kL\n2 \u03b8\u03bb \u03b322 \u2212 ( 1 + 2 \u03b7k\u03c1T ) L2\u03b8\u03bbC\u03b8\u03bb \u2265 1 4\u03b1k ,\nand 1\n2\u03b2k \u2212 Lvz 2 \u2212 ( 1 + 2 \u03b7k\u03c1T ) L2\u03b8\u03bbC\u03b8\u03bb \u2265 1 4\u03b2k .\nThen, the conclusion follows from estimate (45)."
        },
        {
            "heading": "A.6 PROOF OF THEOREM 3.1",
            "text": "We establish the non-asymptotic convergence of LV-HBA in the following theorem, as measured by the residual function defined in equation (15),\nRk := dist (0, (\u2207F (x, y), 0) + ck ((\u2207f(x, y), 0)\u2212\u2207v\u03b3,r(x, y, z)) +NC\u00d7Z(x, y, z)) . (15)\nTheorem A.3. Under Assumptions of Lemma A.7, let \u03b31 \u2208 (0, 1/\u03c1f ), \u03b32 > 0, ck = c(k + 1)p with p \u2208 (0, 1/2), c > 0, and \u03b7k \u2208 (\u03b7, \u03c1T /L2B) with \u03b7 > 0, then there exists c\u03b1, c\u03b2 > 0 such that when \u03b1k \u2208 (\u03b1, c\u03b1) and \u03b2k \u2208 (\u03b2, c\u03b2) with \u03b1, \u03b2 > 0, the sequence of (xk, yk, zk, \u03b8k, \u03bbk) generated by LV-HBA in Algorithm 1 satisfies\nmin 0\u2264k\u2264K \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217r(xk, yk, zk), \u03bb\u2217r(xk, yk, zk))\u2225\u2225 = O( 1K1/2 ) , (49)\nand\nmin 0\u2264k\u2264K\nRk(x k+1, yk+1, zk+1) = O\n( 1\nK(1\u22122p)/2\n) . (50)\nFurthermore, if there exists M > 0 such that \u03c8ck(x k, yk, zk) \u2264 M for any k, the sequence of (xk, yk, zk) also satisfies\n0 \u2264 f(xk, yk)\u2212 v\u03b3(xk, yk, zk) \u2264 f(xk, yk)\u2212 v\u03b3,r(xk, yk, zk) = O ( 1\nKp\n) . (51)\nProof. Firstly, given c\u03b1, c\u03b2 > 0 in equation (48), Lemma 3.1 guarantees that the inequality (14) holds when \u03b1k \u2264 c\u03b1, \u03b2k \u2264 c\u03b2 . By telescoping the inequality (14) for k = 0, 1, . . . ,K \u2212 1, we get\nK\u22121\u2211 k=0\n( 1\n4\u03b1k \u2225(xk+1, yk+1)\u2212 (xk, yk)\u22252 + 1 4\u03b2k \u2225zk+1 \u2212 zk\u22252\n+ \u03b7\u03c1TC\u03b8\u03bb \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217r(xk, yk, zk), \u03bb\u2217r(xk, yk, zk))\u2225\u22252 ) \u2264V0 \u2212 VK \u2264 V0, (52)\nwhere the last inequality is valid since VK is nonnegative. The latter is implies by the fact that v\u03b3,r(x, y, z) \u2264 v\u03b3(x, y, z) \u2264 f(x, y) for all (x, y) \u2208 C. Thus, we have\n\u221e\u2211 k=0 \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217r(xk, yk, zk), \u03bb\u2217r(xk, yk, zk))\u2225\u22252 <\u221e. (53) This implies that the estimate (49) holds, that is,\nmin 0\u2264k\u2264K \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217r(xk, yk, zk), \u03bb\u2217r(xk, yk, zk))\u2225\u2225 = O( 1K1/2 ) . (54)\nSecondly, According to the update rule of variables (x, y, z) in equation (10), we have that\n0 \u2208 ck(dkx, dky) +NC(xk+1, yk+1) + ck \u03b1k\n( (xk+1, yk+1)\u2212 (xk, yk) ) ,\n0 \u2208 ckdkz +NZ(zk+1) + ck \u03b2k\n( zk+1 \u2212 zk ) .\n(55)\nBy the definitions of dkx, d k y and d k z given in equation (11), we obtain\n(ekxy, e k z) \u2208\n( \u2207F (xk+1, yk+1), 0 ) + ck (( \u2207f(xk+1, yk+1), 0 ) \u2212\u2207v\u03b3,r(xk+1, yk+1, zk+1) ) +NC\u00d7Z(xk+1, yk+1, zk+1),\nwith\nekxy := \u2207xy\u03c8ck(xk+1, yk+1, zk+1)\u2212 ck(dkx, dky)\u2212 ck \u03b1k\n( (xk+1, yk+1)\u2212 (xk, yk) ) ,\nekz := \u2207z\u03c8ck(xk+1, yk+1, zk+1)\u2212 ckdkz \u2212 ck \u03b2k\n( zk+1 \u2212 zk ) .\n(56)\nNext, we estimate \u2225ekxy\u2225. We have\n\u2225ekxy\u2225 \u2264\u2225\u2207xy\u03c8ck(xk+1, yk+1, zk+1)\u2212\u2207xy\u03c8ck(xk, yk, zk)\u2225+ \u2225\u2207xy\u03c8ck(xk, yk, zk)\u2212 ck(dkx, dky)\u2225\n+ ck \u03b1k\n\u2225\u2225(xk+1, yk+1)\u2212 (xk, yk)\u2225\u2225 .\nFor the first term in the right hand side of the above inequality, by using Assumptions 3.2, 3.2 and 3.3, Lemmas A.1 and A.3, we can obtain the existence of L\u03c81 > 0 such that\n\u2225\u2207xy\u03c8ck(xk+1, yk+1, zk+1)\u2212\u2207xy\u03c8ck(xk, yk, zk)\u2225 \u2264 ckL\u03c81\u2225(xk+1, yk+1, zk+1)\u2212(xk, yk, zk)\u2225. Using the inequality (33) and Lemma A.5, we have\n\u2225\u2207xy\u03c8ck(xk, yk, zk)\u2212 ck(dkx, dky)\u2225 = ck \u2225\u2225\u2207xy\u03d5ck(xk, yk, zk)\u2212 (dkx, dky)\u2225\u2225\n\u2264 ckC\u03c81 \u2225\u2225(\u03b8k+1, \u03bbk+1)\u2212 (\u03b8\u2217r(xk, yk, zk), \u03bb\u2217r(xk, yk, zk))\u2225\u2225\n\u2264 ckC\u03c81 \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217r(xk, yk, zk), \u03bb\u2217r(xk, yk, zk))\u2225\u2225 ,\n(57) with C\u03c81 := \u221a max{2(Lf + CZLg1)2 + 1/\u03b321 , 2L2g}. Hence, we have\n\u2225ekxy\u2225 \u2264 ckL\u03c81\u2225(xk+1, yk+1, zk+1)\u2212 (xk, yk, zk)\u2225+ ck \u03b1k \u2225\u2225(xk+1, yk+1)\u2212 (xk, yk)\u2225\u2225 + ckC\u03c81\n\u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217r(xk, yk, zk), \u03bb\u2217r(xk, yk, zk))\u2225\u2225 . For \u2225ekz\u2225, we have\n\u2225ekz\u2225 \u2264\u2225\u2207z\u03c8ck(xk+1, yk+1, zk+1)\u2212 ckdkz\u2225+ ck \u03b2k \u2225\u2225zk+1 \u2212 zk\u2225\u2225 . Using Lemmas A.1 and A.5, we have\n\u2225\u2207z\u03c8ck(xk+1, yk+1, zk+1)\u2212 ckdkz\u2225 = ck \u2225\u2225\u2212\u2207zv\u03b3,r(xk+1, yk+1, zk+1)\u2212 dkz\u2225\u2225\n\u2264 ck \u03b32 (\u2225\u2225\u03bbk+1 \u2212 \u03bb\u2217r(xk+1, yk+1, zk+1)\u2225\u2225+ \u2225\u2225zk+1 \u2212 zk\u2225\u2225) \u2264 ck \u03b32\n(\u2225\u2225\u03bbk \u2212 \u03bb\u2217r(xk+1, yk+1, zk+1)\u2225\u2225+ \u2225\u2225zk+1 \u2212 zk\u2225\u2225) . Therefore, we have\n\u2225ekz\u2225 \u2264 ck \u03b2k \u2225\u2225zk+1 \u2212 zk\u2225\u2225+ ck \u03b32 (\u2225\u2225\u03bbk \u2212 \u03bb\u2217r(xk+1, yk+1, zk+1)\u2225\u2225+ \u2225\u2225zk+1 \u2212 zk\u2225\u2225) . With the estimations of \u2225ekxy\u2225 and \u2225ekz\u2225, we obtain the existence of L\u03c8 > 0 such that\nRk(x k+1, yk+1, zk+1) \u2264 ckL\u03c8\u2225(xk+1, yk+1, zk+1)\u2212 (xk, yk, zk)\u2225\n+ ck \u03b1k \u2225\u2225(xk+1, yk+1)\u2212 (xk, yk)\u2225\u2225+ ck \u03b2k \u2225\u2225zk+1 \u2212 zk\u2225\u2225 + ck ( C\u03c81 + 1\n\u03b32 )\u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217r(xk, yk, zk), \u03bb\u2217r(xk, yk, zk))\u2225\u2225 . Utilizing this inequality, let \u03b1k \u2265 \u03b1 and \u03b2k \u2265 \u03b2 for some positive constants \u03b1, \u03b2, we can show that there exists CR > 0 such that\n1\nc2k Rk(x\nk+1, yk+1, zk+1)2\n\u2264CR\n( 1\n4\u03b1k \u2225(xk+1, yk+1)\u2212 (xk, yk)\u22252 + 1 4\u03b2k \u2225zk+1 \u2212 zk\u22252 + \u03b7\u03c1TC\u03b8\u03bb \u2225\u2225(\u03b8k, \u03bbk)\u2212 (\u03b8\u2217r(xk, yk, zk), \u03bb\u2217r(xk, yk, zk))\u2225\u22252 ) .\n(58)\nCombining this with the inequality (52) implies that \u221e\u2211 k=0 1 c2k Rk(x k+1, yk+1, zk+1)2 <\u221e. (59)\nBecause 2p < 1, it holds that K\u2211 k=0 1 c2k = 1 c2 K\u2211 k=0 ( 1 k + 1 )2p \u2265 1 c2 \u222b K+2 1 1 t2p dt \u2265 (K + 2) 1\u22122p \u2212 1 (1\u2212 2p)c2 ,\nand we can conclude from the inequality (59) that\nmin 0\u2264k\u2264K\nRk(x k+1, yk+1, zk+1) = O\n( 1\nK(1\u22122p)/2\n) .\nThus we complete the proof of the estimate (50).\nFinally, since \u03c8ck(x k, yk, zk) \u2264M and F (xk, yk) \u2265 F for any k, we have\nck ( f(xk, yk)\u2212 v\u03b3,r(xk, yk, zk) ) \u2264M \u2212 F , \u2200k,\nand we can obtain from ck = c(k + 1)p that f(xk, yk)\u2212 v\u03b3,r(xk, yk, zk) = O ( 1\nKp\n) .\nSince v\u03b3,r(x, y, z) \u2264 v\u03b3(x, y, z) \u2264 f(x, y) for all (x, y) \u2208 C, we get 0 \u2264 f(xk, yk)\u2212 v\u03b3(xk, yk, zk) \u2264 f(xk, yk)\u2212 v\u03b3,r(xk, yk, zk) = O ( 1\nKp\n) .\nThis establishes the desired estimate (51), completing the proof."
        }
    ],
    "year": 2024
}