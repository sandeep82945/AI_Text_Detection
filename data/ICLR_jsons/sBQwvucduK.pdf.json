{
    "abstractText": "Recent advancements in diffusion models have significantly enhanced the data synthesis with 2D control. Yet, precise 3D control in street view generation, crucial for 3D perception tasks, remains elusive. Specifically, utilizing Bird\u2019s-Eye View (BEV) as the primary condition often leads to challenges in geometry control (e.g., height), affecting the representation of object shapes, occlusion patterns, and road surface elevations, all of which are essential to perception data synthesis, especially for 3D object detection tasks. In this paper, we introduce MAGICDRIVE, a novel street view generation framework, offering diverse 3D geometry controls including camera poses, road maps, and 3D bounding boxes, together with textual descriptions, achieved through tailored encoding strategies. Besides, our design incorporates a cross-view attention module, ensuring consistency across multiple camera views. With MAGICDRIVE, we achieve high-fidelity street-view image & video synthesis that captures nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV segmentation and 3D object detection. Front Front Left Front Right ... ... R ai n y",
    "authors": [
        {
            "affiliations": [],
            "name": "Ruiyuan Gao"
        },
        {
            "affiliations": [],
            "name": "Kai Chen"
        },
        {
            "affiliations": [],
            "name": "Enze Xie"
        },
        {
            "affiliations": [],
            "name": "Lanqing Hong"
        },
        {
            "affiliations": [],
            "name": "Zhenguo Li"
        },
        {
            "affiliations": [],
            "name": "Qiang Xu"
        }
    ],
    "id": "SP:038ff20c2f350c1ab651d8c93add7475d010a647",
    "references": [
        {
            "authors": [
                "Tim Brooks",
                "Aleksander Holynski",
                "Alexei"
            ],
            "title": "A Efros. Instructpix2pix: Learning to follow image editing instructions",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Holger Caesar",
                "Varun Bankiti",
                "Alex H Lang",
                "Sourabh Vora",
                "Venice Erin Liong",
                "Qiang Xu",
                "Anush Krishnan",
                "Yu Pan",
                "Giancarlo Baldan",
                "Oscar Beijbom"
            ],
            "title": "nuscenes: A multimodal dataset for autonomous driving",
            "year": 2020
        },
        {
            "authors": [
                "Kai Chen",
                "Lanqing Hong",
                "Hang Xu",
                "Zhenguo Li",
                "Dit-Yan Yeung"
            ],
            "title": "Multisiam: Self-supervised multi-instance siamese representation learning for autonomous driving",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Kai Chen",
                "Zhili Liu",
                "Lanqing Hong",
                "Hang Xu",
                "Zhenguo Li",
                "Dit-Yan Yeung"
            ],
            "title": "Mixed autoencoder for self-supervised visual representation learning",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Kai Chen",
                "Chunwei Wang",
                "Kuo Yang",
                "Jianhua Han",
                "Lanqing Hong",
                "Fei Mi",
                "Hang Xu",
                "Zhengying Liu",
                "Wenyong Huang",
                "Zhenguo Li",
                "Dit-Yan Yeung",
                "Lifeng Shang",
                "Xin Jiang",
                "Qun Liu"
            ],
            "title": "Gaining wisdom from setbacks: Aligning large language models via mistake analysis",
            "venue": "arXiv preprint arXiv:2310.10477,",
            "year": 2023
        },
        {
            "authors": [
                "Kai Chen",
                "Enze Xie",
                "Zhe Chen",
                "Lanqing Hong",
                "Zhenguo Li",
                "Dit-Yan Yeung"
            ],
            "title": "Integrating geometric control into text-to-image diffusion models for high-quality detection data generation via text prompt",
            "venue": "arXiv preprint arXiv:2306.04607,",
            "year": 2023
        },
        {
            "authors": [
                "Yaran Chen",
                "Haoran Li",
                "Ruiyuan Gao",
                "Dongbin Zhao"
            ],
            "title": "Boost 3-d object detection via point clouds segmentation and fused 3-d giou-l1 loss",
            "venue": "IEEE TNNLS,",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Esser",
                "Robin Rombach",
                "Bjorn Ommer"
            ],
            "title": "Taming transformers for high-resolution image synthesis",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Ruiyuan Gao",
                "Chenchen Zhao",
                "Lanqing Hong",
                "Qiang Xu"
            ],
            "title": "DiffGuard: Semantic mismatchguided out-of-distribution detection using pre-trained diffusion models",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Chongjian Ge",
                "Junsong Chen",
                "Enze Xie",
                "Zhongdao Wang",
                "Lanqing Hong",
                "Huchuan Lu",
                "Zhenguo Li",
                "Ping Luo"
            ],
            "title": "MetaBEV: Solving sensor failures for bev detection and map segmentation",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Yunhao Gou",
                "Zhili Liu",
                "Kai Chen",
                "Lanqing Hong",
                "Hang Xu",
                "Aoxue Li",
                "Dit-Yan Yeung",
                "James T Kwok",
                "Yu Zhang"
            ],
            "title": "Mixture of cluster-conditional lora experts for vision-language instruction tuning",
            "venue": "arXiv preprint arXiv:2312.12379,",
            "year": 2023
        },
        {
            "authors": [
                "Nicholas Guttenberg"
            ],
            "title": "Diffusion with offset noise",
            "venue": "https://www.crosslabs.org/blog/ diffusion-with-offset-noise,",
            "year": 2023
        },
        {
            "authors": [
                "Jianhua Han",
                "Xiwen Liang",
                "Hang Xu",
                "Kai Chen",
                "Lanqing Hong",
                "Chaoqiang Ye",
                "Wei Zhang",
                "Zhenguo Li",
                "Xiaodan Liang",
                "Chunjing Xu"
            ],
            "title": "Soda10m: Towards large-scale object detection benchmark for autonomous driving",
            "venue": "arXiv preprint arXiv:2106.11118,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Junjie Huang",
                "Guan Huang",
                "Zheng Zhu",
                "Ye Yun",
                "Dalong Du"
            ],
            "title": "Bevdet: High-performance multicamera 3d object detection in bird-eye-view",
            "venue": "arXiv preprint arXiv:2112.11790,",
            "year": 2021
        },
        {
            "authors": [
                "Yuanfeng Ji",
                "Zhe Chen",
                "Enze Xie Xie",
                "Lanqing Hong",
                "Xihui Liu",
                "Zhaoqiang Liu",
                "Tong Lu",
                "Zhenguo Li",
                "Ping Luo"
            ],
            "title": "DDP: Diffusion model for dense visual prediction",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Kaican Li",
                "Kai Chen",
                "Haoyu Wang",
                "Lanqing Hong",
                "Chaoqiang Ye",
                "Jianhua Han",
                "Yukuai Chen",
                "Wei Zhang",
                "Chunjing Xu",
                "Dit-Yan Yeung"
            ],
            "title": "Coda: A real-world road corner case dataset for object detection in autonomous driving",
            "venue": "arXiv preprint arXiv:2203.07724,",
            "year": 2022
        },
        {
            "authors": [
                "Pengxiang Li",
                "Zhili Liu",
                "Kai Chen",
                "Lanqing Hong",
                "Yunzhi Zhuge",
                "Dit-Yan Yeung",
                "Huchuan Lu",
                "Xu Jia"
            ],
            "title": "Trackdiffusion: Multi-object tracking data generation via diffusion models",
            "venue": "arXiv preprint arXiv:2312.00651,",
            "year": 2023
        },
        {
            "authors": [
                "Yuheng Li",
                "Haotian Liu",
                "Qingyang Wu",
                "Fangzhou Mu",
                "Jianwei Yang",
                "Jianfeng Gao",
                "Chunyuan Li",
                "Yong Jae Lee"
            ],
            "title": "Gligen: Open-set grounded text-to-image generation",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "Nan Liu",
                "Shuang Li",
                "Yilun Du",
                "Antonio Torralba",
                "Joshua B Tenenbaum"
            ],
            "title": "Compositional visual generation with composable diffusion models",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Zhijian Liu",
                "Haotian Tang",
                "Alexander Amini",
                "Xingyu Yang",
                "Huizi Mao",
                "Daniela Rus",
                "Song Han"
            ],
            "title": "Bevfusion: Multi-task multi-sensor fusion with unified bird\u2019s-eye view representation",
            "venue": "In ICRA,",
            "year": 2023
        },
        {
            "authors": [
                "Zhili Liu",
                "Jianhua Han",
                "Kai Chen",
                "Lanqing Hong",
                "Hang Xu",
                "Chunjing Xu",
                "Zhenguo Li"
            ],
            "title": "Taskcustomized self-supervised pre-training with scalable dynamic routing",
            "venue": "In AAAI,",
            "year": 2022
        },
        {
            "authors": [
                "Zhili Liu",
                "Kai Chen",
                "Yifan Zhang",
                "Jianhua Han",
                "Lanqing Hong",
                "Hang Xu",
                "Zhenguo Li",
                "Dit-Yan Yeung",
                "James Kwok"
            ],
            "title": "Geom-erasing: Geometry-driven removal of implicit concept in diffusion models",
            "venue": "arXiv preprint arXiv:2310.05873,",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P Srinivasan",
                "Matthew Tancik",
                "Jonathan T Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob Mcgrew",
                "Ilya Sutskever",
                "Mark Chen"
            ],
            "title": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Alexander Swerdlow",
                "Runsheng Xu",
                "Bolei Zhou"
            ],
            "title": "Street-view image generation from a bird\u2019seye view layout",
            "venue": "arXiv preprint arXiv:2301.04634,",
            "year": 2023
        },
        {
            "authors": [
                "Shitao Tang",
                "Fuyang Zhang",
                "Jiacheng Chen",
                "Peng Wang",
                "Yasutaka Furukawa"
            ],
            "title": "Mvdiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion",
            "venue": "arXiv preprint arXiv:2307.01097,",
            "year": 2023
        },
        {
            "authors": [
                "Hung-Yu Tseng",
                "Qinbo Li",
                "Changil Kim",
                "Suhib Alsisan",
                "Jia-Bin Huang",
                "Johannes Kopf"
            ],
            "title": "Consistent view synthesis with pose-guided diffusion models",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Su Wang",
                "Chitwan Saharia",
                "Ceslee Montgomery",
                "Jordi Pont-Tuset",
                "Shai Noy",
                "Stefano Pellegrini",
                "Yasumasa Onoe",
                "Sarah Laszlo",
                "David J Fleet",
                "Radu Soricut"
            ],
            "title": "Imagen editor and editbench: Advancing and evaluating text-guided image inpainting",
            "year": 2023
        },
        {
            "authors": [
                "Weilun Wang",
                "Jianmin Bao",
                "Wengang Zhou",
                "Dongdong Chen",
                "Dong Chen",
                "Lu Yuan",
                "Houqiang Li"
            ],
            "title": "Semantic image synthesis via diffusion models",
            "venue": "arXiv preprint arXiv:2207.00050,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaofeng Wang",
                "Zheng Zhu",
                "Guan Huang",
                "Xinze Chen",
                "Jiagang Zhu",
                "Jiwen Lu"
            ],
            "title": "Drivedreamer: Towards real-world-driven world models for autonomous driving",
            "venue": "arXiv preprint arXiv:2309.09777,",
            "year": 2023
        },
        {
            "authors": [
                "Xiaofeng Wang",
                "Zheng Zhu",
                "Yunpeng Zhang",
                "Guan Huang",
                "Yun Ye",
                "Wenbo Xu",
                "Ziwei Chen",
                "Xingang Wang"
            ],
            "title": "Are we ready for vision-centric driving streaming perception? the asap benchmark",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Jay Zhangjie Wu",
                "Yixiao Ge",
                "Xintao Wang",
                "Stan Weixian Lei",
                "Yuchao Gu",
                "Yufei Shi",
                "Wynne Hsu",
                "Ying Shan",
                "Xiaohu Qie",
                "Mike Zheng Shou"
            ],
            "title": "Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Weijia Wu",
                "Yuzhong Zhao",
                "Hao Chen",
                "Yuchao Gu",
                "Rui Zhao",
                "Yefei He",
                "Hong Zhou",
                "Mike Zheng Shou",
                "Chunhua Shen"
            ],
            "title": "Datasetdm: Synthesizing data with perception annotations using diffusion models",
            "venue": "arXiv preprint arXiv:2308.06160,",
            "year": 2023
        },
        {
            "authors": [
                "Kairui Yang",
                "Enhui Ma",
                "Jibin Peng",
                "Qing Guo",
                "Di Lin",
                "Kaicheng Yu"
            ],
            "title": "Bevcontrol: Accurately controlling street-view elements with multi-perspective consistency via bev sketch layout",
            "venue": "arXiv preprint arXiv:2308.01661,",
            "year": 2023
        },
        {
            "authors": [
                "Yijun Yang",
                "Ruiyuan Gao",
                "Xiaosen Wang",
                "Nan Xu",
                "Qiang Xu"
            ],
            "title": "Mma-diffusion: Multimodal attack on diffusion models",
            "venue": "arXiv preprint arXiv:2311.17516,",
            "year": 2023
        },
        {
            "authors": [
                "Lvmin Zhang",
                "Anyi Rao",
                "Maneesh Agrawala"
            ],
            "title": "Adding conditional control to text-to-image diffusion models",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Shu Zhang",
                "Xinyi Yang",
                "Yihao Feng",
                "Can Qin",
                "Chia-Chih Chen",
                "Ning Yu",
                "Zeyuan Chen",
                "Huan Wang",
                "Silvio Savarese",
                "Stefano Ermon"
            ],
            "title": "Hive: Harnessing human feedback for instructional visual editing",
            "venue": "arXiv preprint arXiv:2303.09618,",
            "year": 2023
        },
        {
            "authors": [
                "Wenliang Zhao",
                "Lujia Bai",
                "Yongming Rao",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "title": "Unipc: A unified predictorcorrector framework for fast sampling of diffusion models",
            "venue": "arXiv preprint arXiv:2302.04867,",
            "year": 2023
        },
        {
            "authors": [
                "Ziyang Zheng",
                "Ruiyuan Gao",
                "Qiang Xu"
            ],
            "title": "Non-cross diffusion for semantic consistency",
            "venue": "arXiv preprint arXiv:2312.00820,",
            "year": 2023
        },
        {
            "authors": [
                "LIU Zhili",
                "Kai Chen",
                "Jianhua Han",
                "HONG Lanqing",
                "Hang Xu",
                "Zhenguo Li",
                "James Kwok"
            ],
            "title": "Taskcustomized masked autoencoder via mixture of cluster-conditional experts",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Bolei Zhou",
                "Hang Zhao",
                "Xavier Puig",
                "Tete Xiao",
                "Sanja Fidler",
                "Adela Barriuso",
                "Antonio Torralba"
            ],
            "title": "Semantic understanding of scenes through the ade20k dataset",
            "year": 2019
        },
        {
            "authors": [
                "Brady Zhou",
                "Philipp Kr\u00e4henb\u00fchl"
            ],
            "title": "Cross-view transformers for real-time map-view semantic segmentation",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Gen (Swerdlow"
            ],
            "title": "2023), particularly excelling in objects. Such enhancement can be attributed to MAGICDRIVE\u2019s utilization of the diffusion model and the adoption of a customized condition injection strategy",
            "venue": "BEVGen",
            "year": 2023
        },
        {
            "authors": [
                "Chen et al",
                "Liu"
            ],
            "title": "2022b) and the large language models (LLMs) (Chen et al., 2023b; Gou et al., 2023), is an appealing future research direction. It is also interesting to utilize the geometric controls in different circumstances beyond 3D scenarios (e.g., multi-object tracking (Li et al., 2023a) and concept removal",
            "venue": "(Liu et al., 2023b)). H DETAILED ANALYSIS ON 3D OBJECT DETECTION WITH SYNTHETIC DATA",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The high costs associated with data collection and annotation often impede the effective training of deep learning models. Fortunately, cutting-edge generative models have illustrated that synthetic data can notably boost performance across various tasks, such as object detection (Chen et al., 2023c) and semantic segmentation (Wu et al., 2023b). Yet, the prevailing methodologies are largely tailored to 2D contexts, primarily relying on 2D bounding boxes (Lin et al., 2014; Han et al., 2021) or segmentation maps (Zhou et al., 2019) as layout conditions (Chen et al., 2023c; Li et al., 2023b).\nIn autonomous driving applications, a thorough grasp of the 3D environment is essential. This demands reliable techniques for tasks like Bird\u2019s-Eye View (BEV) map segmentation (Zhou & Kra\u0308henbu\u0308hl, 2022; Ji et al., 2023) and 3D object detection (Chen et al., 2020; Huang et al., 2021; Liu et al., 2023a; Ge et al., 2023). A genuine 3D geometry representation is crucial for capturing intricate details from 3D annotations, such as road elevations, object heights, and their occlusion pat-\n\u2217Equal contribution. \u2020Corresponding authors. Project Page: https://flymin.github.io/magicdrive.\nterns, as shown in Figure 2. Consequently, generating multi-camera street-view images according to 3D annotations becomes vital to boost downstream perception tasks.\nFor street-view data synthesis, two pivotal criteria are realism and controllability. Realism requires that the quality of the synthetic data should align with that of real data; and in a given scene, views from varying camera perspectives should remain consistent with one another (Mildenhall et al., 2020). On the other hand, controllability emphasizes the precision in generating street-view images that adhere to provided conditions: the BEV map, 3D object bounding boxes, and camera poses for views. Beyond these core requirements, effective data augmentation should also grant the flexibility to tweak finer scenario attributes, such as prevailing weather conditions or the time of day. Existing solutions like BEVGen (Swerdlow et al., 2023) approach street view generation by encapsulating all semantics within BEV. Conversely, BEVControl (Yang et al., 2023a) starts by projecting 3D coordinates to image views, subsequently using 2D geometric guidance. However, both methods compromise certain geometric dimensions\u2014height is lost in BEVGen and depth in BEVControl.\nThe rise of diffusion models has significantly pushed the boundaries of controllable image generation quality. Specifically, ControlNet (Zhang et al., 2023a) proposes a flexible framework to incorporate 2D spatial controls based on pre-trained Text-to-Image (T2I) diffusion models (Rombach et al., 2022). However, 3D conditions are distinct from pixel-level conditions or text. The challenge of seamlessly integrating them with multi-camera view consistency in street view synthesis remains.\nIn this paper, we introduce MAGICDRIVE, a novel framework dedicated to street-view synthesis with diverse 3D geometry controls1. For realism, we harness the power of pre-trained stable diffusion (Rombach et al., 2022), further fine-tuning it for street view generation. One distinctive component of our framework is the cross-view attention module. This simple yet effective component provides multi-view consistency through interactions between adjacent views. In contrast to previous methods, MAGICDRIVE proposes a separate design for objects and road map encoding to improve controllability with 3D data. More specifically, given the sequence-like, variable-length nature of 3D bounding boxes, we employ cross-attention akin to text embeddings for their encoding. Besides, we propose that an addictive encoder branch like ControlNet (Zhang et al., 2023a) can encode maps in BEV and is capable of view transformation. Therefore, our design achieves geometric controls without resorting to any explicit geometric transformations or imposing geometric constraints on multi-camera consistency. Finally, MAGICDRIVE factors in textual descriptions, offering attribute control such as weather conditions and time of day.\nOur MAGICDRIVE framework, despite its simplicity, excels in generating strikingly realistic images & videos that align with road maps, 3D bounding boxes, and varied camera perspectives. Besides, the images produced can enhance the training for both 3D object detection and BEV segmentation tasks. Furthermore, MAGICDRIVE offers comprehensive geometric controls at the scene, background, and foreground levels. This flexibility makes it possible to craft previously unseen street views suitable for simulation purposes. We summarize the main contributions of this work as:\n\u2022 The introduction of MAGICDRIVE, an innovative framework that generates multi-perspective camera views & videos conditioned on BEV and 3D data tailored for autonomous driving.\n\u2022 The development of simple yet potent strategies to manage 3D geometric data, effectively addressing the challenges of multi-camera view consistency in street view generation.\n\u2022 Through rigorous experiments, we demonstrate that MAGICDRIVE outperforms prior street view generation techniques, notably for the multi-dimensional controllability. Additionally, our results reveal that synthetic data delivers considerable improvements in 3D perception tasks.\n1In this paper, our 3D geometry controls contain control from road maps, 3D object boxes, and camera poses. We do not consider others like the exact shape of objects or background contents."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Diffusion Models for Conditional Generation. Diffusion models (Ho et al., 2020; Song et al., 2020; Zheng et al., 2023) generate images by learning a progressive denoising process from the Gaussian noise distribution to the image distribution. These models have proven exceptional across diverse tasks, such as text-to-image synthesis (Rombach et al., 2022; Nichol et al., 2022; Yang et al., 2023b), inpainting (Wang et al., 2023a), and instructional image editing (Zhang et al., 2023b; Brooks et al., 2023), due to their adaptability and competence in managing various form of controls (Zhang et al., 2023a; Li et al., 2023b) and multiple conditions (Liu et al., 2022a; Gao et al., 2023). Besides, data synthesized from geometric annotations can aid downstream tasks such as 2D object detection (Chen et al., 2023c; Wu et al., 2023b). Thus, this paper explores the potential of T2I diffusion models in generating street-view images and benefiting downstream 3D perception models.\nStreet View Generation. Numerous street view generation models condition on 2D layouts, such as 2D bounding boxes (Li et al., 2023b) and semantic segmentation (Wang et al., 2022). These methods leverage 2D layout information corresponding directly to image scale, whereas the 3D information does not possess this property, thereby rendering such methods unsuitable for leveraging 3D information for generation. For street view synthesis with 3D geometry, BEVGen (Swerdlow et al., 2023) is the first to explore. It utilizes a BEV map as a condition for both roads and vehicles. However, the omission of height information limits its application in 3D object detection. BEVControl (Yang et al., 2023a) amends the loss of object\u2019s height by the height-lifting process. Similarly, Wang et al. (2023b) also projects 3D boxes to camera views to guide generation. However, the projection from 3D to 2D results in the loss of essential 3D geometric information, like depth and occlusion. In this paper, we propose to encode bounding boxes and road maps separately for more nuanced control and integrate scene descriptions, offering enhanced control over the generation of street views.\nMulti-camera Image Generation of a 3D scene fundamentally requires viewpoint consistency. Several studies have addressed this issue within the context of indoor scenes. For instance, MVDiffusion (Tang et al., 2023) employs panoramic images and a cross-view attention module to maintain global consistency, while Tseng et al. (2023) leverage epipolar geometry as a constraining prior. These approaches, however, primarily rely on the continuity of image views, a condition not always met in street views due to limited camera overlap and different camera configurations (e.g., exposure, intrinsic). Our MAGICDRIVE introduces extra cross-view attention modules to UNet, which significantly enhances consistency across multi-camera views."
        },
        {
            "heading": "3 PRELIMINARY",
            "text": "Problem Formulation. In this paper, we consider the coordinate of the LiDAR system as the ego car\u2019s coordinate, and parameterize all geometric information according to it. Let S = {M,B,L} be the description of a driving scene around the ego vehicle, where M \u2208 {0, 1}w\u00d7h\u00d7c is the binary map representing a w \u00d7 h meter road area in BEV with c semantic classes, B = {(ci, bi)}Ni=1 represents the 3D bounding box position (bi = {(xj , yj , zj)}8j=1 \u2208 R8\u00d73) and class (ci \u2208 C) for each object in the scene, and L is the text describing additional information about the scene (e.g., weather and time of day). Given a camera pose P = [K,R,T] (i.e., intrinsics, rotation, and translation), the goal of street-view image generation is to learn a generator G(\u00b7) which synthesizes realistic images I \u2208 RH\u00d7W\u00d73 corresponding to the scene S and camera pose P as, I = G(S,P, z), where z \u223c N (0, 1) is a random noise from Gaussian distribution. Conditional Diffusion Models. Diffusion models (Ho et al., 2020; Song et al., 2020) generate data (x0) by iteratively denoising a random Gaussian noise (xT ) for T steps. Typically, to learn the denoising process, the network is trained to predict the noise by minimizing the mean-square error:\n\u2113simple = Ex0,c,\u03f5,t [ ||\u03f5\u2212 \u03f5\u03b8( \u221a \u03b1\u0304tx0 + \u221a 1\u2212 \u03b1\u0304t\u03f5, t, c)||2 ] , (1)\nwhere \u03f5\u03b8 is the network to train, with parameters \u03b8, c is optional conditions, which is used for the conditional generation, t \u2208 [0, T ] is the time-step, \u03f5 \u2208 N (0, I) is the additive Gaussian noise, and \u03b1\u0304t is a scalar parameter. Latent diffusion models (LDM) (Rombach et al., 2022) is a special kind of diffusion model, where they utilize a pre-trained Vector Quantized Variational AutoEncoder (VQVAE) (Esser et al., 2021) and perform diffusion process in the latent space. Given the VQ-VAE encoder as z = E(x), one can rewrite \u03f5\u03b8(\u00b7) in Equation 1 as \u03f5\u03b8( \u221a \u03b1\u0304tE(x0) + \u221a 1\u2212 \u03b1\u0304t\u03f5, t, c) for LDM. Besides, LDM considers text describing the image as condition c."
        },
        {
            "heading": "4 STREET VIEW GENERATION WITH 3D INFORMATION",
            "text": "The overview of MAGICDRIVE is depicted in Figure 3. Operating on the LDM pipeline, MAGICDRIVE generates street-view images conditioned on both scene annotations (S) and the camera pose (P) for each view. Given the 3D geometric information in scene annotations, projecting all to a BEV map, akin to BEVGen (Swerdlow et al., 2023) or BEVControl (Yang et al., 2023a), doesn\u2019t ensure precise guidance for street view generation, as exemplified in Figure 2. Consequently, MAGICDRIVE categorizes conditions into three levels: scene (text and camera pose), foreground (3D bounding boxes), and background (road map); and integrates them separately via cross-attention and an additive encoder branch, detailed in Section 4.1. Additionally, maintaining consistency across different cameras is crucial for synthesizing street views. Thus, we introduce a simple yet effective cross-view attention module in Section 4.2. Lastly, we elucidate our training strategies in Section 4.3, emphasizing Classifier-Free Guidance (CFG) in integrating various conditions."
        },
        {
            "heading": "4.1 GEOMETRIC CONDITIONS ENCODING",
            "text": "As illustrated in Figure 3, two strategies are employed for information injection into the UNet of diffusion models: cross-attention and additive encoder branch. Given that the attention mechanism (Vaswani et al., 2017) is tailored for sequential data, cross-attention is apt for managing variable length inputs like text tokens and bounding boxes. Conversely, for grid-like data, such as road maps, the additive encoder branch is effective in information injection (Zhang et al., 2023a). Therefore, MAGICDRIVE employs distinct encoding modules for various conditions.\nScene-level Encoding includes camera pose P = {K \u2208 R3\u00d73,R \u2208 R3\u00d73,T \u2208 R3\u00d71}, and text sequence L. For text, we construct the prompt with a template as \u201cA driving scene image at {location}. {description} \u201d, and leverage a pre-trained CLIP text encoder (Etext) as LDM (Rombach et al., 2022), as shown by Equation 2, where L is the token length of L. As for camera pose, we first concat each parameter by their column, resulting in P\u0304 = [K,R,T]T \u2208 R7\u00d73. Since P\u0304 contains values from sin/cos functions and also 3D offsets, to have the model effectively interpret these high-frequency variations, we apply Fourier embedding (Mildenhall et al., 2020) to each 3-dim vector before leveraging a Multi-Layer Perception (MLP, Ecam) to embed the camera pose parameters, as in Equation 3. To maintain consistency, we set the dimension of hc the same as that of hti. Through the CLIP text encoder, each text embedding h t i already contains positional information (Radford et al., 2021). Therefore, we prepend the camera pose embedding hc to text embeddings, resulting in scene-level embedding hs = [hc,ht].\nht = [ht1 . . . h t L] = Etext(L), (2)\nhc = Ecam(Fourier(P\u0304)) = Ecam(Fourier([K,R,T] T )). (3)\n3D Bounding Box Encoding. Since each driving scene has a variable length of bounding boxes, we inject them through the cross-attention mechanism similar to scene-level information. Specifically, we encode each box into a hidden vector hb, which has the same dimensions as that of ht. Each 3D bounding box (ci, bi) contains two types of information: class label ci and box position bi. For\nclass labels, we utilize the method similar to Li et al. (2023b), where the pooled embeddings of class names (Lci ) are considered as label embeddings. For box positions bi \u2208 R8\u00d73, represented by the coordinates of its 8 corner points, we utilize Fourier embedding to each point and pass through an MLP for encoding, as in Equation 4. Then, we use an MLP to compress both class and position embedding into one hidden vector, as in Equation 5. The final hidden states for all bounding boxes of each scene are represented as hb = [hb1 . . . h b N ], where N is the number of boxes.\nebc(i) = AvgPool(Etext(Lci)), e b p(i) = MLPp(Fourier(bi)), (4)\nhbi = Ebox(ci, bi) = MLPb(e b c(i), e b p(i)). (5)\nIdeally, the model learns the geometric relationship between bounding boxes and camera pose through training. However, the distribution of the number of visible boxes to different views is long-tailed. Thus, we bootstrap learning by filtering visible objects to each view (vi), i.e., fviz in Equation 6. Besides, we also add invisible boxes for augmentation (more details in Section 4.3).\nhbvi = {h b i \u2208 hb|fviz(bi,Rvi ,Tvi) > 0}. (6)\nRoad Map Encoding. The road map has a 2D-grid format. While Zhang et al. (2023a) shows the addictive encoder can incorporate this kind of data for 2D guidance, the inherent perspective differences between the road map\u2019s BEV and the camera\u2019s First-Person View (FPV) create discrepancies. BEVControl (Yang et al., 2023a) employs a back-projection to transform from BEV to FPV but complicates the situation with an ill-posed problem. In MAGICDRIVE, we propose that explicit view transformation is unnecessary, as sufficient 3D cues (e.g., height from object boxes and camera pose) allow the addictive encoder to accomplish view transformation. Specifically, we integrate scene-level and 3D bounding box embeddings into the map encoder (see Figure 3). Scene-level embeddings provide camera poses, and box embeddings offer road elevation cues. Additionally, incorporating text descriptions facilitates the generation of roads under varying conditions (e.g., weather and time of day). Thus, the map encoder can synergize with other conditions for generation."
        },
        {
            "heading": "4.2 CROSS-VIEW ATTENTION MODULE",
            "text": "In multi-camera view generation, it is crucial that image synthesis remains consistent across different perspectives. To maintain consistency, we introduce a cross-view attention module (Figure 4). Given the sparse arrangement of cameras in driving contexts, each cross-view attention allows the target view to access information from its immediate left and right views, as in Equation 7; here, t, l, and r are the target, left, and right view respectively. Then, the target view aggregates such information with skip connection, as in Equation 8, where hv indicates the hidden state of the target view.\nAttentionicv(Qt,Ki, Vi) = softmax( QtK\nT i\u221a\nd ) \u00b7 Vi, i \u2208 {l, r}, (7)\nhvout = h v in +Attention l cv +Attention r cv . (8)\nWe inject cross-view attention after the cross-attention module in the UNet and apply zeroinitialization (Zhang et al., 2023a) to bootstrap the optimization. The efficacy of the cross-view attention module is demonstrated in Figure 4 right, Figure 5, and Figure 6. The multilayered structure of UNet enables aggregating information from long-range views after several stacked blocks. Therefore, using cross-view attention on adjacent views is enough for multi-view consistency, further evidenced by the ablation study in Appendix C."
        },
        {
            "heading": "4.3 MODEL TRAINING",
            "text": "Classifier-free Guidance reinforces the impact of conditional guidance (Ho & Salimans, 2021; Rombach et al., 2022). For effective CFG, models need to discard conditions during training occasionally. Given the unique nature of each condition, applying a drop strategy is complex for multiple conditions. Therefore, our MAGICDRIVE simplifies this for four conditions by concurrently dropping scene-level conditions (camera pose and text embeddings) at a rate of \u03b3s. For boxes and maps, which have semantic representations for null (i.e., padding token in boxes and 0 in maps) in their encoding, we maintain them throughout training. At inference, we utilize null for all conditions, enabling meaningful amplification to guide generation.\nTraining Objective and Augmentation. With all the conditions injected as inputs, we adapt the training objective described in Section 3 to the multi-condition scenario, as in Equation 9.\n\u2113 = Ex0,\u03f5,t,{S,P } [ ||\u03f5\u2212 \u03f5\u03b8( \u221a \u03b1\u0304tE(x0) + \u221a 1\u2212 \u03b1\u0304t\u03f5, t, {S,P})|| ] . (9)\nBesides, we emphasize two essential strategies when training our MAGICDRIVE. First, to counteract our filtering of visible boxes, we randomly add 10% invisible boxes as an augmentation, enhancing the model\u2019s geometric transformation capabilities. Second, to leverage cross-view attention, which facilitates information sharing across multiple views, we apply unique noises to different views in each training step, preventing trivial solutions to Equation 9 (e.g., outputting the shared component across different views). Identical random noise is reserved exclusively for inference."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "5.1 EXPERIMENTAL SETUPS",
            "text": "Dataset and Baselines. We employ the nuScenes dataset (Caesar et al., 2020), a prevalent dataset in BEV segmentation and detection for driving, as the testing ground for MAGICDRIVE. We adhere to the official configuration, utilizing 700 street-view scenes for training and 150 for validation. Our baselines are BEVGen (Swerdlow et al., 2023) and BEVControl (Yang et al., 2023a), both recent propositions for street view generation. Our method considers 10 object classes and 8 road classes, surpassing the baseline models in diversity. Appendix B holds additional details.\nEvaluation Metrics. We evaluate both realism and controllability for street view generation. Realism is mainly measured using Fre\u0301chet Inception Distance (FID), reflecting image synthesis quality. For controllability, MAGICDRIVE is evaluated through two perception tasks: BEV segmentation and 3D object detection, with CVT (Zhou & Kra\u0308henbu\u0308hl, 2022) and BEVFusion (Liu et al., 2023a) as perception models, respectively. Both of them are renowned for their performance in each task. Firstly, we generate images aligned with the validation set annotations and use perception models pre-trained with real data to assess image quality and control accuracy. Then, data is generated based on the training set to examine the support for training perception models as data augmentation.\nModel Setup. Our MAGICDRIVE utilizes pre-trained weights from Stable Diffusion v1.5, training only newly added parameters. Per Zhang et al. (2023a), a trainable UNet encoder is created for Emap. New parameters, except for the zero-init module and the class token, are randomly initialized. We adopt two resolutions to reconcile discrepancies in perception tasks and baselines: 224\u00d7400 (0.25\u00d7 down-sample) following BEVGen and for CVT model support, and a higher 272\u00d7736 (0.5\u00d7 down-sample) for BEVFusion support. Unless stated otherwise, images are sampled using the UniPC (Zhao et al., 2023) scheduler for 20 steps with CFG at 2.0."
        },
        {
            "heading": "5.2 MAIN RESULTS",
            "text": "Realism and Controllability Validation. We assess MAGICDRIVE\u2019s capability to create realistic street-view images with the annotations from the nuScenes validation set. As shown by Table 1, MAGICDRIVE outperforms others in image quality, yielding notably lower FID scores. Regarding controllability, assessed via BEV segmentation tasks, MAGICDRIVE equals or exceeds baseline results at 224\u00d7400 resolution due to the distinct encoding design that enhances vehicle generation precision. At 272\u00d7736 resolution, our encoding strategy advancements enhance vehicle mIoU performance. Cropping large areas negatively impacts road mIoU on CVT. However, our bounding box encoding efficacy is backed by BEVFusion\u2019s results in 3D object detection.\nTraining Support for BEV Segmentation and 3D Object Detection. MAGICDRIVE can produce augmented data with accurate annotation controls, enhancing the training for perception tasks. For BEV segmentation, we augment an equal number of images as in the original dataset, ensuring consistent training iterations and batch sizes for fair comparisons to the baseline. As shown in Table 3, MAGICDRIVE significantly enhances CVT in both settings, outperforming BEVGen, which only marginally improves vehicle segmentation. For 3D object detection, we train BEVFusion models with MAGICDRIVE \u2019s synthetic data as augmentation. To optimize data augmentation, we randomly exclude 50% of bounding boxes in each generated scene. Table 2 shows the advantageous impact of MAGICDRIVE \u2019s data in both CAM-only (C) and CAM+LiDAR (C+L) settings. It\u2019s crucial to note that in CAM+LiDAR settings, BEVFusion utilizes both modalities for object detection, requiring more precise image generation due to LiDAR data incorporation. Nevertheless, MAGICDRIVE\u2019s synthetic data integrates seamlessly with LiDAR inputs, highlighting the data\u2019s high fidelity."
        },
        {
            "heading": "5.3 QUALITATIVE EVALUATION",
            "text": "Comparison with Baselines. We assessed MAGICDRIVE against two baselines, BEVGen and BEVControl, synthesizing multi-camera views for the same validation scenes (the comparison with BEVGen is in the Appendix D). Figure 5 illustrates that MAGICDRIVE generates images markedly superior in quality to BEVControl, particularly excelling in accurate object positioning and maintaining consistency in street views for backgrounds and objects. Such performance primarily stems from MAGICDRIVE \u2019s bounding box encoder and its cross-view attention module.\nMulti-level Controls. The design of MAGICDRIVE introduces multi-level controls to street-view generation through separation encoding. This section demonstrates the capabilities of MAGICDRIVE by exploring three control signal levels: scene level (time of day and weather), background level (BEV map alterations and conditional views), and foreground level (object orientation and deletion). As illustrated in Figure 1, Figure 6, and Appendix E, MAGICDRIVE adeptly accommodates alterations at each level, maintaining multi-camera consistency and high realism in generation.\nTable 2: Comparison about support for 3D object detection model (i.e., BEVFusion). MAGICDRIVE generates 272\u00d7736 images for augmentation. Results are reported on the nuScenes validation set.\nTable 3: Comparison about support for BEV segmentation model (i.e., CVT). Results are reported by testing on the nuScenes validation set."
        },
        {
            "heading": "5.4 EXTENSION TO VIDEO GENERATION",
            "text": "We demonstrate the extensibility of MAGICDRIVE to video generation by fine-tuning it on nuScenes videos. This involves modifying self-attention to ST-Attn (Wu et al., 2023a), adding a temporal attention module to each transformer block (Figure 7 left), and tuning the model on 7-frame clips with only the first and the last frames having bounding boxes. We sample initial noise independently for each frame using the UniPC (Zhao et al., 2023) sampler for 20 steps and illustrate an example in Figure 7 right.\nFurthermore, by utilizing the interpolated annotations from ASAP (Wang et al., 2023c) like DriveDreamer (Wang et al., 2023b), MagicDrive can be extended to 16-frame video generation at 12Hz trained on Nvidia V100 GPUs. More results (e.g., video visualization) can be found on our website."
        },
        {
            "heading": "6 ABLATION STUDY",
            "text": "Bounding Box Encoding. MAGICDRIVE utilizes separate encoders for bounding boxes and road maps. To demonstrate the efficacy, we train a ControlNet (Zhang et al., 2023a) that takes the BEV map with both road and object semantics as a condition (like BEVGen), denoted as \u201cw/o Ebox\u201d in Table 4. Objects in BEV maps are relatively small, which require separate Ebox for accurate vehicle annotations, as shown by the vehicle mIoU performance gap. Applying visible object filter fviz significantly improves both road and vehicle mIoU by reducing the optimization burden. A MAGICDRIVE variant incorporating Ebox with BEV of road and object semantics didn\u2019t enhance performance, emphasizing the importance of integrating diverse information through different strategies.\nEffect of Classifier-free Guidance. We focus on the two most crucial conditions, i.e. object boxes and road maps, and analyze how CFG affects the performance of generation. We change CFG from 1.5 to 4.0 and plot the change of validation results from CVT in Figure 8. Firstly, by increasing\nTable 4: Ablation of the separate box encoder. Evaluation results are from CVT on the synthetic nuScenes validation set, without M = {0} in CFG scale = 2. MAGICDRIVE has better controllability and keeps image quality.\nMethod FID \u2193 RoadmIoU \u2191 Vehicle mIoU \u2191\nw/o Ebox 18.06 58.31 5.50 w/o fviz 14.67 56.46 24.73 w/ Ebox & mapobj 14.70 56.04 26.20 Ours 14.46 59.31 27.13 Figure 8: Effect of CFG on different conditions to each metrics.\nCFG scale, FID degrades due to notable changes in contrast and sharpness, as seen in previous studies (Chen et al., 2023c). Secondly, retaining the same map for both conditional and unconditional inference eliminates CFG\u2019s effect on the map condition. As shown by blue lines of Figure 8, increasing CFG scale results in the highest vehicle mIoU at CFG=2.5, but the road mIoU keeps decreasing. Thirdly, with M = {0} for unconditional inference in CFG, road mIoU significantly increases. However, it slightly degrades the guidance on vehicle generation. As mentioned in Section 4.3, CFG complexity increases with more conditions. Despite simplifying training, various CFG choices exist during inference. We leave the in-depth investigation for this case as future work."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "This paper presents MAGICDRIVE, a novel framework to encode multiple geometric controls for high-quality multi-camera street view generation. With the separation encoding design, MAGICDRIVE fully utilizes geometric information from 3D annotations and realizes accurate semantic control for street views. Besides, the proposed cross-view attention module is simple yet effective in guaranteeing consistency across multi-camera views. As evidenced by experiments, the generations from MAGICDRIVE show high realism and fidelity to 3D annotations. Multiple controls equipped MAGICDRIVE with improved generalizability for the generation of novel street views. Meanwhile, MAGICDRIVE can be used for data augmentation, facilitating the training for perception models on both BEV segmentation and 3D object detection tasks.\nLimitation and Future Work. We show failure cases from MAGICDRIVE in Figure 9. Although MAGICDRIVE can generate night views, they are not as dark as real images (as in Figure 9a). This may be due to that diffusion models are hard to generate too dark images (Guttenberg, 2023). Figure 9b shows that MAGICDRIVE cannot generate unseen weathers for nuScenes. Future work may\nfocus on how to improve the cross-domain generalization ability of street view generation.\nAcknowledgement. This work is supported in part by the General Research Fund (GRF) of Hong Kong Research Grants Council (RGC) under Grant No. 14203521, in part by the CUHK SSFCRS funding No. 3136023, and in part by the Research Matching Grant Scheme under Grant No. 7106937, 8601130, and 8601440. We gratefully acknowledge the support of MindSpore, CANN (Compute Architecture for Neural Networks) and Ascend AI Processor used for this research. This research has been made possible by funding support from the Research Grants Council of Hong Kong through the Research Impact Fund project R6003-21."
        },
        {
            "heading": "A OBJECT FILTERING",
            "text": "In Equation 6, we employ fviz for object filtering to facilitate bootstrap learning. We show more details of fviz here. Refer to Figure 10 for illustration. For the sake of simplicity, each camera\u2019s Field Of View (FOV) is not considered. Objects are defined as visible if any corner of their bounding boxes is located in front of the camera (i.e., zvi > 0) within each camera\u2019s coordinate system. The application of fviz significantly lightens the workload of the bounding box encoder, evidence for which can be found in Section 6."
        },
        {
            "heading": "B MORE EXPERIMENTAL DETAILS",
            "text": "Semantic Classes for Generation. To support most perception models on nuScenes, we try to include semantics commonly used in most settings (Huang et al., 2021; Zhou & Kra\u0308henbu\u0308hl, 2022; Liu et al., 2023a; Ge et al., 2023). Specifically, for objects, ten categories include car, bus, truck, trailer, motorcycle, bicycle, construction vehicle, pedestrian, barrier, and traffic cone. For the road map, eight categories include drivable area, pedestrian crossing, walkway, stop line, car parking area, road divider, lane divider, and roadblock.\nOptimization. We train all newly added parameters using AdamW (Loshchilov & Hutter, 2019) optimizer and a constant learning rate at 8e\u22125 and batch size 24 (total 144 images for 6 views) with a linear warm-up of 3000 iterations, and set \u03b3s = 0.2."
        },
        {
            "heading": "C ABLATION ON NUMBER OF ATTENDING VIEWS",
            "text": "In Table 5, we demonstrate the impact of varying the number of attended views on evaluation results. Attending to a single view yields superior FID results; the reduced influx of information from neighboring views simplifies optimization for that view. However, this approach compromises mIoU, also reflecting less consistent generation, as depicted in Figure 11. Conversely, incorporating all views deteriorates performance across all metrics, potentially due to excessive information causing interference in cross-attention. Since each view has an intersection with both left and right views, attending to one view cannot guarantee consistency, especially for foreground objects, while attending to more views requires more computation. Thus, we opt for 2 attended views in our main paper, striking a balance between consistency and computational efficiency."
        },
        {
            "heading": "D QUALITATIVE COMPARISON WITH BEVGEN",
            "text": "Figure 12 illustrates that MAGICDRIVE generates images with higher quality compared to BEVGen (Swerdlow et al., 2023), particularly excelling in objects. Such enhancement can be attributed to MAGICDRIVE\u2019s utilization of the diffusion model and the adoption of a customized condition injection strategy."
        },
        {
            "heading": "E MORE RESULTS WITH CONTROL FROM DIFFERENT CONDITIONS",
            "text": "Figure 13 shows scene level control (time of day) and background level control (BEV map alterations). MAGICDRIVE can effectively reflect these changes in control conditions through the generated camera views."
        },
        {
            "heading": "F MORE EXPERIMENTS WITH 3D OBJECT DETECTION",
            "text": "In Table 6, we show additional experimental results on training 3D object detection models using synthetic data produced by MAGICDRIVE. Given that BEVFusion utilizes a lightweight backbone (i.e., Swin-T (Liu et al., 2021)), model performance appears to plateau with training through 1- 2\u00d7 epochs (2\u00d7: 20 for CAM-Only and 6 for CAM+LiDAR). Reducing epochs can mitigate this saturation, allowing more varied data to enhance the model\u2019s perceptual capacity in both settings. This improvement is evident even when epochs for 3D object detection are further reduced to 0.5\u00d7. Our MAGICDRIVE accurately augments street-view images with the annotations. Future works may focus on annotation sampling and construction strategies for synthetic data augmentation."
        },
        {
            "heading": "G MORE DISCUSSION",
            "text": "More future work. Note that MAGICDRIVE-generated street views can currently only perform as augmented samples to train with real data, and it is exciting to train detectors solely with generated data, which will be explored in the future. More flexible usage of the generated street views beyond data augmentation, especially incorporation with generative pre-training (Chen et al., 2023a; Zhili et al., 2023), contrastive learning (Chen et al., 2021; Liu et al., 2022b) and the large language models (LLMs) (Chen et al., 2023b; Gou et al., 2023), is an appealing future research direction. It is also interesting to utilize the geometric controls in different circumstances beyond 3D scenarios (e.g., multi-object tracking (Li et al., 2023a) and concept removal (Liu et al., 2023b))."
        },
        {
            "heading": "H DETAILED ANALYSIS ON 3D OBJECT DETECTION WITH SYNTHETIC DATA",
            "text": "We provide per-class AP for 3D object detection from the nuScenes validation set using BEVFusion in Table 7. From the results, we observe that, firstly, the improvements for large objects are significant, for example, buses, trailers, and construction vehicles. Secondly, objects with less diverse appearances, such as traffic cones and barriers, show more improvement, especially compared to trucks. Thirdly, we note that the improvement is marginal for cars, while significant for pedestrians, motorcycles, and bicycles. This may be because the baseline already performs well for cars. For pedestrians, motorcycles, and bicycles, even though distant objects from the ego car are generated less faithfully, MAGICDRIVE can synthesize high-quality objects near the ego car, as shown in Figure 17-18. Therefore, more accurate detection of objects near the ego car contributes to improvements for these classes. Overall, mAP improvement comes with promotion in all classes\u2019 AP, indicating MAGICDRIVE can indeed help the training of perception models."
        },
        {
            "heading": "I MORE RESULTS FOR BEV SEGMENTATION",
            "text": "BEVFusion is also capable of BEV segmentation and considers most of the classes we used in the BEV map condition. Due to the lack of baselines, we present the results in Table 8 to facilitate comparison for future works. As can be seen, the 272\u00d7736 resolution does not outperform the 224\u00d7400 resolution. This is consistent with the results from CVT in Table 1 on the Road segment. Such results confirm that better map controls rely on maintaining the original aspect ratio for generation training (i.e., avoiding cropping on each side)."
        },
        {
            "heading": "J GENERALIZATION OF CAMERA PARAMETERS",
            "text": "To improve generalization ability, MAGICDRIVE encodes raw camera intrinsic and extrinsic parameters for different perspectives. However, the generalization ability is somewhat limited due to nuScenes fixing camera poses for different scenes. Nevertheless, we attempt to exchange the intrinsic and extrinsic parameters between the three front cameras and three back cameras. The comparison is shown in Figure 14. Since the positions of the nuScenes cameras are not symmetrical from front to back, and the back camera has a 120\u25e6 FOV compared to the 70\u25e6 FOV of the other cameras, clear differences between front and back views can be observed for the same 3D coordinates."
        },
        {
            "heading": "K MORE GENERATION RESULTS",
            "text": "We show some corner-case (Li et al., 2022) generations in Figure 15, and more generations in Figure 16-Figure 18."
        }
    ],
    "title": "MAGICDRIVE: STREET VIEW GENERATION WITH DIVERSE 3D GEOMETRY CONTROL",
    "year": 2024
}