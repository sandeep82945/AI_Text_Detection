{
    "abstractText": "Conventional causal discovery methods rely on centralized data, which is inconsistent with the decentralized nature of data in many real-world situations. This discrepancy has motivated the development of federated causal discovery (FCD) approaches. However, existing FCD methods may be limited by their potentially restrictive assumptions of identifiable functional causal models or homogeneous data distributions, narrowing their applicability in diverse scenarios. In this paper, we propose a novel FCD method attempting to accommodate arbitrary causal models and heterogeneous data. We first utilize a surrogate variable corresponding to the client index to account for the data heterogeneity across different clients. We then develop a federated conditional independence test (FCIT) for causal skeleton discovery and establish a federated independent change principle (FICP) to determine causal directions. These approaches involve constructing summary statistics as a proxy of the raw data to protect data privacy. Owing to the nonparametric properties, FCIT and FICP make no assumption about particular functional forms, thereby facilitating the handling of arbitrary causal models. We conduct extensive experiments on synthetic and real datasets to show the efficacy of our method. The code is available at https://github.com/lokali/FedCDH.git.",
    "authors": [
        {
            "affiliations": [],
            "name": "Loka Li"
        },
        {
            "affiliations": [],
            "name": "Ignavier Ng"
        },
        {
            "affiliations": [],
            "name": "Gongxu Luo"
        },
        {
            "affiliations": [],
            "name": "Biwei Huang"
        },
        {
            "affiliations": [],
            "name": "Guangyi Chen"
        },
        {
            "affiliations": [],
            "name": "Tongliang Liu"
        },
        {
            "affiliations": [],
            "name": "Bin Gu"
        },
        {
            "affiliations": [],
            "name": "Kun Zhang"
        },
        {
            "affiliations": [],
            "name": "Mohamed bin Zayed"
        }
    ],
    "id": "SP:ea8a6345c0a34766b591da8fdcdde504a4b93970",
    "references": [
        {
            "authors": [
                "Amin Abyaneh",
                "Nino Scherrer",
                "Patrick Schwab",
                "Stefan Bauer",
                "Bernhard Sch\u00f6lkopf",
                "Arash"
            ],
            "title": "Mehrjou. Fed-cd: Federated causal discovery from interventional and observational data",
            "venue": "arXiv preprint arXiv:2211.03846,",
            "year": 2022
        },
        {
            "authors": [
                "Abbas Acar",
                "Hidayet Aksu",
                "A Selcuk Uluagac",
                "Mauro Conti"
            ],
            "title": "A survey on homomorphic encryption schemes: Theory and implementation",
            "venue": "ACM Computing Surveys (Csur),",
            "year": 2018
        },
        {
            "authors": [
                "Muhammad Arshad",
                "Marius C Silaghi"
            ],
            "title": "Distributed simulated annealing",
            "venue": "Distributed Constraint Problem Solving and Reasoning in Multi-Agent Systems,",
            "year": 2004
        },
        {
            "authors": [
                "Marcus Bendtsen"
            ],
            "title": "Regime aware learning",
            "venue": "In Conference on Probabilistic Graphical Models,",
            "year": 2016
        },
        {
            "authors": [
                "Stephen Boyd",
                "Neal Parikh",
                "Eric Chu",
                "Borja Peleato",
                "Jonathan Eckstein"
            ],
            "title": "Distributed optimization and statistical learning via the alternating direction method of multipliers",
            "venue": "Foundations and Trends\u00ae in Machine learning,",
            "year": 2011
        },
        {
            "authors": [
                "Vince D Calhoun",
                "Robyn Miller",
                "Godfrey Pearlson",
                "Tulay Adal\u0131"
            ],
            "title": "The chronnectome: timevarying connectivity networks as the next frontier in fmri data",
            "venue": "discovery. Neuron,",
            "year": 2014
        },
        {
            "authors": [
                "Chandler Squires"
            ],
            "title": "causaldag: creation, manipulation, and learning of causal models",
            "venue": "https://github.com/uhlerlab/causaldag,",
            "year": 2018
        },
        {
            "authors": [
                "Rong Chen",
                "Krishnamoorthy Sivakumar",
                "Hillol Kargupta"
            ],
            "title": "Learning bayesian network structure from distributed data",
            "venue": "In Proceedings of the 2003 SIAM International Conference on Data Mining,",
            "year": 2003
        },
        {
            "authors": [
                "David Maxwell Chickering"
            ],
            "title": "Optimal structure identification with greedy search",
            "venue": "Journal of machine learning research,",
            "year": 2002
        },
        {
            "authors": [
                "Ronald Cramer",
                "Ivan Bjerre Damg\u00e5rd"
            ],
            "title": "Secure multiparty computation",
            "year": 2015
        },
        {
            "authors": [
                "JJ Daudin"
            ],
            "title": "Partial association measures and an application to qualitative regression",
            "year": 1980
        },
        {
            "authors": [
                "Dorit Dor",
                "Michael Tarsi"
            ],
            "title": "A simple algorithm to construct a consistent extension of a partially oriented graph. Technicial Report R-185",
            "venue": "Cognitive Systems Laboratory,",
            "year": 1992
        },
        {
            "authors": [
                "Paul Erd\u0151s",
                "Alfr\u00e9d R\u00e9nyi"
            ],
            "title": "On the evolution of random graphs",
            "venue": "Publ. Math. Inst. Hung. Acad. Sci,",
            "year": 1960
        },
        {
            "authors": [
                "Kenji Fukumizu",
                "Arthur Gretton",
                "Xiaohai Sun",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Kernel measures of conditional dependence",
            "venue": "Advances in neural information processing systems,",
            "year": 2007
        },
        {
            "authors": [
                "Erdun Gao",
                "Junjia Chen",
                "Li Shen",
                "Tongliang Liu",
                "Mingming Gong",
                "Howard Bondell"
            ],
            "title": "Feddag: Federated dag structure learning",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Slawomir Goryczka",
                "Li Xiong"
            ],
            "title": "A comprehensive comparison of multiparty secure additions with differential privacy",
            "venue": "IEEE transactions on dependable and secure computing,",
            "year": 2015
        },
        {
            "authors": [
                "Kui Xiang Gou",
                "Gong Xiu Jun",
                "Zheng Zhao"
            ],
            "title": "Learning bayesian network structure from distributed homogeneous data. In Eighth acis international conference on software engineering, artificial intelligence, networking, and parallel/distributed computing",
            "venue": "(snpd 2007),",
            "year": 2007
        },
        {
            "authors": [
                "Arthur Gretton",
                "Kenji Fukumizu",
                "Choon Teo",
                "Le Song",
                "Bernhard Sch\u00f6lkopf",
                "Alex Smola"
            ],
            "title": "A kernel statistical test of independence",
            "venue": "Advances in neural information processing systems,",
            "year": 2007
        },
        {
            "authors": [
                "Patrik Hoyer",
                "Dominik Janzing",
                "Joris M Mooij",
                "Jonas Peters",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Nonlinear causal discovery with additive noise models",
            "venue": "Advances in neural information processing systems,",
            "year": 2008
        },
        {
            "authors": [
                "Biwei Huang",
                "Kun Zhang",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Identification of time-dependent causal model: A gaussian process treatment",
            "venue": "In Twenty-Fourth international joint conference on artificial intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Biwei Huang",
                "Kun Zhang",
                "Jiji Zhang",
                "Joseph Ramsey",
                "Ruben Sanchez-Romero",
                "Clark Glymour",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Causal discovery from heterogeneous/nonstationary data",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Jianli Huang",
                "Kui Yu",
                "Xianjie Guo",
                "Fuyuan Cao",
                "Jiye Liang"
            ],
            "title": "Towards privacy-aware causal structure learning in federated setting",
            "venue": "arXiv preprint arXiv:2211.06919,",
            "year": 2022
        },
        {
            "authors": [
                "Brian Kidd",
                "Kunbo Wang",
                "Yanxun Xu",
                "Yang Ni"
            ],
            "title": "Federated learning for sparse bayesian models with applications to electronic health records and genomics",
            "venue": "In PACIFIC SYMPOSIUM ON BIOCOMPUTING 2023: Kohala Coast, Hawaii,",
            "year": 2023
        },
        {
            "authors": [
                "Phillip Lippe",
                "Taco Cohen",
                "Efstratios Gavves"
            ],
            "title": "Efficient neural causal discovery without acyclicity constraints",
            "venue": "International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Lars Lorch",
                "Scott Sussex",
                "Jonas Rothfuss",
                "Andreas Krause",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Amortized inference for causal structure learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In Artificial intelligence and statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Osman Mian",
                "David Kaltenpoth",
                "Michael Kamp",
                "Jilles Vreeken"
            ],
            "title": "Nothing but regrets\u2014privacypreserving federated causal discovery",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Yongchan Na",
                "Jihoon Yang"
            ],
            "title": "Distributed bayesian network structure learning",
            "venue": "IEEE International Symposium on Industrial Electronics,",
            "year": 2010
        },
        {
            "authors": [
                "Ignavier Ng",
                "Kun Zhang"
            ],
            "title": "Towards federated bayesian network structure learning with continuous optimization",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "Ignavier Ng",
                "S\u00e9bastien Lachapelle",
                "Nan Rosemary Ke",
                "Simon Lacoste-Julien",
                "Kun Zhang"
            ],
            "title": "On the convergence of continuous constrained optimization for structure learning",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "Ignavier Ng",
                "Biwei Huang",
                "Kun Zhang"
            ],
            "title": "Structure learning with continuous optimization: A sober look and beyond",
            "venue": "arXiv preprint arXiv:2304.02146,",
            "year": 2023
        },
        {
            "authors": [
                "Ana Rita Nogueira",
                "Jo\u00e3o Gama",
                "Carlos Abreu Ferreira"
            ],
            "title": "Causal discovery in machine learning: Theories and applications",
            "venue": "Journal of Dynamics & Games,",
            "year": 2021
        },
        {
            "authors": [
                "Jonas Peters",
                "Peter B\u00fchlmann"
            ],
            "title": "Identifiability of gaussian structural equation models with equal error variances",
            "year": 2014
        },
        {
            "authors": [
                "Russell A Poldrack",
                "Timothy O Laumann",
                "Oluwasanmi Koyejo",
                "Brenda Gregory",
                "Ashleigh Hover",
                "Mei-Yen Chen",
                "Krzysztof J Gorgolewski",
                "Jeffrey Luci",
                "Sung Jun Joo",
                "Ryan L Boyd"
            ],
            "title": "Longterm neural and physiological phenotyping of a single human",
            "venue": "Nature communications,",
            "year": 2015
        },
        {
            "authors": [
                "Ali Rahimi",
                "Benjamin Recht"
            ],
            "title": "Random features for large-scale kernel machines",
            "venue": "Advances in neural information processing systems,",
            "year": 2007
        },
        {
            "authors": [
                "Alexander Reisach",
                "Christof Seiler",
                "Sebastian Weichwald"
            ],
            "title": "Beware of the simulated DAG! causal discovery benchmarks may be easy to game",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Basil Saeed",
                "Snigdha Panigrahi",
                "Caroline Uhler"
            ],
            "title": "Causal structure discovery from distributions arising from mixtures of dags",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Saeed Samet",
                "Ali Miri"
            ],
            "title": "Privacy-preserving bayesian network for horizontally partitioned data",
            "venue": "In 2009 International Conference on Computational Science and Engineering,",
            "year": 2009
        },
        {
            "authors": [
                "Xinpeng Shen",
                "Sisi Ma",
                "Prashanthi Vemuri",
                "Gyorgy Simon"
            ],
            "title": "Challenges and opportunities with causal discovery algorithms: application to alzheimer\u2019s pathophysiology",
            "venue": "Scientific reports,",
            "year": 2020
        },
        {
            "authors": [
                "Shohei Shimizu",
                "Patrik O Hoyer",
                "Aapo Hyv\u00e4rinen",
                "Antti Kerminen",
                "Michael Jordan"
            ],
            "title": "A linear non-gaussian acyclic model for causal discovery",
            "venue": "Journal of Machine Learning Research,",
            "year": 2006
        },
        {
            "authors": [
                "Peter Spirtes"
            ],
            "title": "An anytime algorithm for causal inference",
            "venue": "In International Workshop on Artificial Intelligence and Statistics,",
            "year": 2001
        },
        {
            "authors": [
                "Peter Spirtes",
                "Kun Zhang"
            ],
            "title": "Causal discovery and inference: concepts and recent methodological advances",
            "venue": "In Applied informatics,",
            "year": 2016
        },
        {
            "authors": [
                "Peter Spirtes",
                "Clark N Glymour",
                "Richard Scheines",
                "David Heckerman"
            ],
            "title": "Causation, prediction, and search",
            "venue": "MIT press,",
            "year": 2000
        },
        {
            "authors": [
                "Eric V Strobl",
                "Kun Zhang",
                "Shyam Visweswaran"
            ],
            "title": "Approximate kernel-based conditional independence tests for fast non-parametric causal discovery",
            "venue": "Journal of Causal Inference,",
            "year": 2019
        },
        {
            "authors": [
                "Danica J Sutherland",
                "Jeff Schneider"
            ],
            "title": "On the error of random fourier features",
            "venue": "arXiv preprint arXiv:1506.02785,",
            "year": 2015
        },
        {
            "authors": [
                "Ruibo Tu",
                "Kun Zhang",
                "Bo Bertilson",
                "Hedvig Kjellstrom",
                "Cheng Zhang"
            ],
            "title": "Neuropathic pain diagnosis simulator for causal discovery algorithm evaluation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Matej Vukovi\u0107",
                "Stefan Thalmann"
            ],
            "title": "Causal discovery in manufacturing: A structured literature review",
            "venue": "Journal of Manufacturing and Materials Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Zhaoyu Wang",
                "Pingchuan Ma",
                "Shuai Wang"
            ],
            "title": "Towards practical federated causal structure learning",
            "venue": "arXiv preprint arXiv:2306.09433,",
            "year": 2023
        },
        {
            "authors": [
                "Dennis Wei",
                "Tian Gao",
                "Yue Yu"
            ],
            "title": "DAGs with no fears: A closer look at continuous optimization for learning Bayesian networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Robert F Woolson"
            ],
            "title": "Wilcoxon signed-rank test",
            "venue": "Wiley encyclopedia of clinical trials, pp",
            "year": 2007
        },
        {
            "authors": [
                "Eric P Xing",
                "Wenjie Fu",
                "Le Song"
            ],
            "title": "A state-space mixed membership blockmodel for dynamic network tomography",
            "venue": "The Annals of Applied Statistics,",
            "year": 2010
        },
        {
            "authors": [
                "Qiaoling Ye",
                "Arash A Amini",
                "Qing Zhou"
            ],
            "title": "Distributed learning of generalized linear causal networks",
            "venue": "arXiv preprint arXiv:2201.09194,",
            "year": 2022
        },
        {
            "authors": [
                "Yue Yu",
                "Jie Chen",
                "Tian Gao",
                "Mo Yu"
            ],
            "title": "Dag-gnn: Dag structure learning with graph neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Yue Yu",
                "Tian Gao",
                "Naiyu Yin",
                "Qiang Ji"
            ],
            "title": "Dags with no curl: An efficient dag structure learning approach",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kun Zhang",
                "Lai-Wan Chan"
            ],
            "title": "Extensions of ica for causality discovery in the hong kong stock market",
            "venue": "In International Conference on Neural Information Processing,",
            "year": 2006
        },
        {
            "authors": [
                "Kun Zhang",
                "Aapo Hyvarinen"
            ],
            "title": "On the identifiability of the post-nonlinear causal model",
            "venue": "arXiv preprint arXiv:1205.2599,",
            "year": 2012
        },
        {
            "authors": [
                "Kun Zhang",
                "Jonas Peters",
                "Dominik Janzing",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Kernel-based conditional independence test and application in causal discovery",
            "venue": "arXiv preprint arXiv:1202.3775,",
            "year": 2012
        },
        {
            "authors": [
                "Kun Zhang",
                "Biwei Huang",
                "Jiji Zhang",
                "Bernhard Sch\u00f6lkopf",
                "Clark Glymour"
            ],
            "title": "Discovery and visualization of nonstationary causal models",
            "venue": "arXiv preprint arXiv:1509.08056,",
            "year": 2015
        },
        {
            "authors": [
                "Xun Zheng",
                "Bryon Aragam",
                "Pradeep K Ravikumar",
                "Eric P Xing"
            ],
            "title": "Dags with no tears: Continuous optimization for structure learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Fangting Zhou",
                "Kejun He",
                "Yang Ni"
            ],
            "title": "Causal discovery with heterogeneous observational data",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Causal discovery aims to learn the causal structure from observational data, attracting significant attention from fields such as machine learning and artificial intelligence (Nogueira et al., 2021), healthcare (Shen et al., 2020), economics (Zhang & Chan, 2006), manufacturing (Vukovic\u0301 & Thalmann, 2022) and neuroscience (Tu et al., 2019). Recently, it has been facing new opportunities and challenges from the rapid growth of data volume. One of the key challenges is data decentralization. Traditionally, causal discovery is conducted at a centralized site where all data is gathered in one location. However, in real-world scenarios, data is often distributed across multiple parties, such as the healthcare data across various hospitals (Kidd et al., 2022). Consequently, there has been increasing interest in federated causal discovery (FCD), which aims to uncover the underlying causal structure of decentralized data with privacy and security concerns.\nExisting FCD methods from observational data can be generally classified as continuousoptimization-based, constraint-based, and score-based methods. Some continuous-optimizationbased methods extend NOTEARS (Zheng et al., 2018) with federated strategies, such as NOTEARSADMM (Ng & Zhang, 2022) that relies on the ADMM (Boyd et al., 2011) optimization method, FedDAG (Gao et al., 2022) that employs FedAvg (McMahan et al., 2017) technique, and FED-CD (Abyaneh et al., 2022) that utilizes belief aggregation. These methods might suffer from various technical issues, including convergence (Wei et al., 2020; Ng et al., 2022), nonconvexity (Ng et al., 2023), and sensitivity to data standardization (Reisach et al., 2021). As for score-based methods, DARLIS (Ye et al., 2022) utilizes the distributed annealing (Arshad & Silaghi, 2004) strategy to search for the optimal graph, while PERI (Mian et al., 2023) aggregates the results of the local greedy equivalent search (GES) (Chickering, 2002) and chooses the worst-case regret for each iteration. One constraint-based method, FEDC2SL (Wang et al., 2023), extendes \u03c72 test to the federated\nversion, however, this method is restrictive on discrete variables and therefore not applicable for any continuous variables. Other constraint-based methods, such as FedPC (Huang et al., 2022), aggregate the skeletons and directions of the Peter-Clark (PC) algorithm (Spirtes et al., 2000) by each client via a voting mechanism. However, as shown in Table 1, most of these methods heavily rely on either identifiable functional causal models or homogeneous data distributions. These assumptions may be overly restrictive and difficult to be satisfied in real-world scenarios, limiting their diverse applicability. For instance, distribution shifts may often occur in the real world across different clients owing to different interventions, collection conditions, or domains, resulting in the presence of heterogeneous data. Please refer to Appendix A2 for further discussion of related works, including those of causal discovery, heterogeneous data and FCD.\nIn this paper, we propose FedCDH, a novel constraint-based approach for Federated Causal Discovery from Heterogeneous data. The primary innovation of FedCDH lies in using summary statistics as a proxy for raw data during skeleton discovery and direction determination in a federated fashion. Specifically, to address heterogeneous data, we first introduce a surrogate variable corresponding to the client or domain index, allowing our method to model distribution changes. Unlike existing FCD methods that only leverage the data from different clients to increase the total sample size, we demonstrate how such data heterogeneity across different clients benefits the identification of causal directions and how to exploit it. Furthermore, we propose a federated conditional independence test (FCIT) for causal skeleton discovery, incorporating random features (Rahimi & Recht, 2007) to approximate the kernel matrix which facilitates the construction of the covariance matrix. Additionally, we develop a federated independent change principle (FICP) to determine causal directions, exploiting the causal asymmetry. FICP also employs random features to approximate the embeddings of heterogeneous conditional distributions for representing changing causal models. It is important to note that FCIT and FICP are non-parametric, making no assumption about specific functional forms, thus facilitating the handling of arbitrary causal models. To evaluate our method, we conduct extensive experiments on synthetic datasets including linear Gaussian models and general functional models, and real-world dataset including fMRI Hippocampus (Poldrack et al., 2015) and HK Stock Market datasets (Huang et al., 2020). The significant performance improvements over other FCD methods demonstrate the superiority of our approach."
        },
        {
            "heading": "2 REVISITING CAUSAL DISCOVERY FROM HETEROGENEOUS DATA",
            "text": "In this section, we will firstly provide an overview of causal discovery and some common assumptions, then we will introduce the characterizations of conditional independence and independent change. This paper aims at extending those techniques from the centralized to the federated setting."
        },
        {
            "heading": "1) Causal Discovery with Changing Causal Models.",
            "text": "Consider d observable random variables denoted by V =(V1, . . . , Vd) and K clients, and one client corresponds to one unique domain. In this paper, we focus on horizontally-partitioned data (Samet & Miri, 2009), where each client holds a different subset of the total data samples while all the clients share the same set of features. Let k be the client index, and \u2127 be the domain index, where k,\u2127\u2208{1, . . . ,K}. Each client has nk samples, in total there are n samples, denoted by\n1Linear Gaussian model with equal noise variance (EV) (Peters & Bu\u0308hlmann, 2014) and nonlinear additive noise model (Hoyer et al., 2008) are identifiable, while linear Gaussian model with non-equal noise variance (NV) is not identifiable. Generalized linear model and arbitrary functional model are certainly not identifiable.\nn= \u2211K\nk=1 nk. The task of federated causal discovery is to recover the causal graph G given the decentralized data matrix V \u2208Rn\u00d7d. When the data is homogeneous, the causal process for each variable Vi can be represented by the following structural causal model (SCM): Vi=fi(PAi, \u03f5i), where fi is the causal function, PAi is the parents of Vi, \u03f5i is a noise term with non-zero variance, and we assume the \u03f5i\u2019s are mutually independent. When the data is heterogeneous, there must be some causal models changing across different domains. The changes may be caused by the variation of causal strengths or noise variances. Therefore, we formulate the causal process for heterogeneous data as: Vi=fi(PAi, \u03f5i, \u03b8i(\u2127), \u03c8\u0303(\u2127)), where \u2127 is regarded as an observed random variable referred as the domain index, the function fi or the distribution of the noise \u03f5i is different or changing across different domains, both \u03c8\u0303(\u2127) and \u03b8i(\u2127) are unobserved domain-changing factors represented as the functions of variable \u2127, \u03c8\u0303(\u2127) is the set of \u201dpseudo confounders\u201d that influence the whole set of variables and we assume there are L such confounders (\u03c8\u0303(\u2127)={\u03c8l(\u2127)}Ll=1, the minimum value for L can be 0 meaning that there is no such latent confounder in the graph, while the maximum value can be C2d = d(d+1) 2 , meaning that each pair of observed variables has a hidden confounder), \u03b8i(\u2127) denotes the effective parameters of Vi in the model, and we assume that \u03b8i(\u2127) is specific to Vi and is independent of \u03b8j(\u2127) for any i\u0338=j. \u03c8\u0303(\u2127) and \u03b8i(\u2127) input \u2127 which is a positive integer and output a real number. Let Gobs be the underlying causal graph over V , and Gaug be the augmented graph over V \u222a\u03c8\u0303(\u2127)\u222a{\u03b8i(\u2127)}di=1. For causal discovery with changing causal models, we follow previous work such as CD-NOD (Huang et al., 2020) and make the following assumptions.\nAssumption 1 (Pseudo Causal Sufficiency). There is no confounder in the dataset of one domain, but we allow the changes of different causal modules across different domains to be dependent.\nAssumption 2 (Markov and Faithfulness). The joint distribution over V \u222a \u03c8\u0303(\u2127) \u222a {\u03b8i(\u2127)}di=1 is Markov and faithful to Gaug .\nTo remove the potential influence from confounders and recover causal relations across different domains, causal discovery could be conducted on the augmented graph Gaug instead of Gobs. While \u03c8\u0303(\u2127)\u222a{\u03b8i(\u2127)}di=1 are unobserved variables, the domain index \u2127 is observed variable. Therefore, \u2127 is introduced as the surrogate variable (Huang et al., 2020) for causal discovery from heterogeneous data. An illustration is given in Figure 1, where the augmented graph with the unobserved domain-changing variables \u03c8\u0303(\u2127) and \u03b8i(\u2127) could be simplified by an augmented graph with just a surrogate variable \u2127. If there is an edge between surrogate variable \u2127 and observed variable Vi on Gaug , then it means that the causal model related to Vi is changing across different domains, in other words, the data distribution of Vi is heterogeneous across domains.\n2) Characterization of Conditional Independence. Let X,Y, Z be random variables or sets of random variables, with the domains X ,Y,Z , respectively. Define a measurable and positive definite kernel kX , and denote the corresponding reproducing kernel Hilbert space (RKHS) HX . Similarly, we define kY ,HY , kZ andHZ . One of the most used characterizations of conditional independence (CI) is: X \u22a5\u22a5 Y |Z if and only if PXY |Z = PX|ZPY |Z , or equivalently PX|Y,Z = PX|Z . Another characterization of CI is given in terms of the partial cross-covariance operator on RKHS.\nLemma 1 (Characterization of CI with Partial Cross-covariance (Fukumizu et al., 2007)). Let X\u0308 \u225c (X,Z), kX\u0308 \u225c kXkZ , and HX\u0308 be the RKHS corresponding to kX\u0308 . Assume that HX \u2282 L2X ,HY \u2282 L2Y ,HZ \u2282 L2Z . Further assume that kX\u0308kY is a characteristic kernel on (X \u00d7 Z) \u00d7 Y , and that HZ + R (the direct sum of two RHKSs) is dense in L2(PZ). Let \u03a3X\u0308Y |Z be the partial crosscovariance operator, then\n\u03a3X\u0308Y |Z = 0 \u21d0\u21d2 X \u22a5\u22a5 Y |Z. (1)\nBased on the above lemma, we further consider a different characterization of CI which enforces the uncorrelatedness of functions in suitable spaces, which may be intuitively more appealing. More details about the interpretation of \u03a3X\u0308Y |Z , the definition of characteristic kernel, and the uncorrelatedness-based characterization of CI, are put in Appendix A3.1.\n3) Characterization of Independent Change. The Hilbert-Schmidt Independence Criterion (HSIC) (Gretton et al., 2007) is a statistical measure used to assess the independence between two random variables in the RKHS. We use the normalized HSIC to evaluate the independence of two changing causal models. The value of the normalized HSIC ranges from 0 to 1, and a smaller value indicates that the two changing causal models are more independent. Let \u25b3\u0302X\u2192Y be the normalized HSIC between P(X) and P(Y |X), and \u25b3\u0302Y\u2192X be the normalized HSIC between P(Y ) and P(X|Y ). Then, we can determine the causal direction between X and Y with the following lemma. Lemma 2 (Independent Change Principle (Huang et al., 2020)). Let X and Y be two random observed variables. Assume that both X and Y have changing causal models (both of them are adjacent to \u2127 in Gaug). Then the causal direction between X and Y can be determined according to the following rules\ni) If \u25b3\u0302X\u2192Y < \u25b3\u0302Y\u2192X , output the direction X\u2192Y ,\nii) If \u25b3\u0302X\u2192Y > \u25b3\u0302Y\u2192X , output the direction Y\u2192X .\nMore details about the definition and formulation of \u25b3\u0302X\u2192Y and \u25b3\u0302X\u2192Y are in Appendix A3.2. It is important to note that: once the Gaussian kernel is utilized, the kernel-based conditional independence test (Zhang et al., 2012) and the kernel-based independent change principal (Huang et al., 2020) assume smoothness for the relationship of continuous variables."
        },
        {
            "heading": "3 FEDERATED CAUSAL DISCOVERY FROM HETEROGENEOUS DATA",
            "text": "In this section, we will explain our proposed FedCDH method in details. An overall framework of FedCDH is given in Figure 2. Two key submodules of our method are federated conditional independent test (FCIT; Theorem 4 and Theorem 5) and federated independent change principle (FICP; Theorem 6), which are presented in Section 3.1 and Section 3.2, respectively. We then illustrate how to construct the summary statistics and how to implement FCIT and FICP with summary statistics (Theorem 8) in Section 3.3. Last but not least, we discuss the communication costs and secure computations in Section 3.4. For the proofs of theorems and lemmas, please refer to Appendix A4."
        },
        {
            "heading": "3.1 FEDERATED CONDITIONAL INDEPENDENT TEST (FCIT)",
            "text": "1) Null Hypothesis. Consider the null and alternative hypotheses\nH0 : X \u22a5\u22a5 Y |Z, H1 : X \u22a5\u0338\u22a5 Y |Z. (2)\nAccording to Eq. 1, we can measure conditional independence based on the RKHSs. Therefore, we equivalently rewrite the above hypothesis more explicitly as\nH0 : \u2225\u03a3X\u0308Y |Z\u2225 2 HS = 0, H1 : \u2225\u03a3X\u0308Y |Z\u2225 2 HS > 0. (3)\nNote that the computation forms of Hilbert-Schmidt norm and Frobenius norm are the same, and the difference is that the Hilbert-Schmidt norm is defined in infinite Hilbert space while the Frobenius norm is defined in finite Euclidean space. We here consider the squared Frobenius norm of the empirical partial cross-covariance matrix as an approximation for the hypotheses, given as\nH0 : \u2225CX\u0308Y |Z\u2225 2 F = 0, H1 : \u2225CX\u0308Y |Z\u2225 2 F > 0, (4)\nwhere CX\u0308Y |Z= 1 n \u2211n i=1[(A\u0308i\u2212E(A\u0308|Z))T (Bi\u2212E(B|Z))] corresponds to the partial cross-covariance matrix with n samples, CX\u0308Y |Z\u2208Rh\u00d7h, A\u0308=f(X\u0308)\u2208Rn\u00d7h, B=g(Y )\u2208Rn\u00d7h, {f j(X\u0308)|hj=1}\u2208FX\u0308 , {gj(Y )|hj=1}\u2208FY . n and h denote the number of total samples of all clients and the number"
        },
        {
            "heading": "Skeleton Discovery",
            "text": "of hidden features or mapping functions, respectively. Since X\u0308\u225c(X,Z), then for each function f j :X\u0308 7\u2192FX\u0308 , the input is X\u0308\u2208Rn\u00d72 and the output f j(X\u0308)\u2208Rn. For each function gk:Y 7\u2192FY , the input is Y \u2208Rn and the output gk(Y )\u2208Rn. Notice that FX\u0308 and FY are function spaces, which are set to be the support of the process \u221a 2 cos(w \u00b7+b), w follows standard Gaussian distribution, and b follows uniform distribution from [0, 2\u03c0]. We choose these specific spaces because in this paper we use random features to approximate the kernels. E(A\u0308|Z) and E(B|Z) could be non-linear functions of Z which are difficult to estimate. Therefore, we would like to approximate them with linear functions. Let q(Z)\u2208Rn\u00d7h, {qj(Z)|hj=1}\u2208FZ , FZ shares a similar function space with FY . We could estimate E(f j |Z) with the linear ridge regression solution uTj q(Z) and estimate E(gj |Z) with vTj q(Z) under mild conditions (Sutherland & Schneider, 2015). Now we give the following lemma.\nLemma 3 (Characterization of Conditional Independence). Let f j and gj be the functions defined for the variables X\u0308 and Y , respectively. ThenX \u22a5\u22a5 Y |Z is approximated by the following condition\nE(f\u0303 g\u0303) = 0, \u2200f\u0303 \u2208 FX\u0308|Z and \u2200g\u0303 \u2208 FY |Z , (5)\nwhere FX\u0308|Z={f\u0303 | f\u0303 j=f j\u2212uTj q(Z), f j\u2208FX\u0308} and FY |Z={g\u0303 | g\u0303j=gj\u2212vTj q(Z), gj\u2208FY }.\nLet \u03b3 be a small ridge parameter. According to Eq. 1 and Eq. 5, by ridge regression, we obtain\nCX\u0308Y |Z = CX\u0308Y \u2212 CX\u0308Z(CZZ + \u03b3I) \u22121CZY . (6)\n2) Test Statistic and Null Distribution. In order to ensure the convergence to a non-degenerate distribution, we multiply the empirical estimate of the Frobenius norm by n, and set it as the test statistic TCI=n\u2225CX\u0308Y |Z\u22252F . Let K\u0303X\u0308|Z be the centralized kernel matrix, given by K\u0303X\u0308|Z\u225cHRX\u0308|ZRTX\u0308|ZH , where H=I\u2212 1n11\nT and RX\u0308|Z\u225cf\u0303(X\u0308)=f(X\u0308)\u2212uT q(Z) which can be seen as the residual after kernel ridge regression. Here, I refers to the n\u00d7n identity matrix and 1 denotes the vector of n ones. We now define K\u0303Y |Z similarly. Let \u03bbX\u0308|Z and \u03bbY |Z be the eigenvalues of K\u0303X\u0308|Z and K\u0303Y |Z , respectively. Let {\u03b11, . . . , \u03b1L} denote i.i.d. standard Gaussian variables, and thus {\u03b121, . . . , \u03b12L} denote i.i.d. \u03c721 variables. Considering n i.i.d. samples from the joint distribution PX\u0308Y Z , we have Theorem 4 (Federated Conditional Independent Test). Under the null hypothesisH0 (X and Y are conditionally independent given Z), the test statistic\nTCI \u225c n\u2225CX\u0308Y |Z\u2225 2 F , (7)\nhas the asymptotic distribution\nT\u0302CI \u225c 1\nn2 L\u2211 i,j=1 \u03bbX\u0308|Z,i\u03bbY |Z,j\u03b1 2 ij .\nAlthough the defined test statistics are equivalent to that of kernel-based conditional independence test (KCI) (Zhang et al., 2012), the asymptotic distributions are in different forms. Please note that the large sample properties are needed when deriving the asymptotic distribution T\u0302CI above, and the proof is shown in Appendix A4.2.\nGiven that X \u22a5\u22a5 Y |Z, we could introduce the independence between RX\u0308|Z and RY |Z , which leads to the separation between \u03bbX\u0308|Z,i and \u03bbY |Z,j . We show that this separated form could help to approximate the null distribution in terms of a decomposable statistic, such as the covariance matrix.\nWe approximate the null distribution with a two-parameter Gamma distribution, which is related to the mean and variance. Under the hypothesis H0 and given the sample D, the distribution of T\u0302CI can be approximated by the \u0393(k\u0302, \u03b8\u0302) distribution: P(t) = (tk\u0302\u22121 \u00b7 e\u2212t/\u03b8\u0302)/(\u03b8k\u0302 \u00b7 \u0393(k\u0302)), where k\u0302 = E2(T\u0302CI |D)/Var(T\u0302CI |D), and \u03b8\u0302 = Var(T\u0302CI |D)/E(T\u0302CI |D). We propose to approximate the null distribution with the mean and variance in the following theorem. Theorem 5 (Null Distribution Approximation). Under the null hypothesisH0 (X and Y are conditionally independent given Z), we have\nE(T\u0302CI |D) = tr(CX\u0308|Z) \u00b7 tr(CY |Z),\nVar(T\u0302CI |D) = 2\u2225CX\u0308|Z\u2225 2 F \u00b7 \u2225CY |Z\u22252F ,\n(8)\nwhere CX\u0308|Z= 1 nR T X\u0308|ZHHRX\u0308|Z , CY |Z= 1 nR T Y |ZHHRY |Z , and tr(\u00b7) means the trace operator.\nFor testing the conditional independence X \u22a5\u22a5 Y |Z, in this paper, we only deal with the scenarios where X and Y each contain a single variable while Z could contain a single variable, multiple variables, or be empty. When Z is empty, the test becomes the federated unconditional independent test (FUIT), as a special case. We provide more details about FUIT in Appendix A5."
        },
        {
            "heading": "3.2 FEDERATED INDEPENDENT CHANGE PRINCIPLE (FICP)",
            "text": "As described in Lemma 2, we can use independent change principle (ICP) to evaluate the dependence between two changing causal models. However, existing ICP (Huang et al., 2020) heavily relies on the kernel matrix to calculate the normalized HSIC. It may be challenging for decentralized data because the off-diagonal entries of kernel matrix require the raw data from different clients, which violates the data privacy in federated learning. Motivated by that, we propose to estimate the normalized HSIC with the following theorem. Theorem 6 (Federated Independent Change Principle). In order to check whether two causal models change independently across different domains, we can estimate the dependence by\n\u25b3\u0302X\u2192Y = \u2225C\u2217 X,Y\u0303 \u22252F\ntr(C\u2217X) \u00b7 tr(C\u2217Y\u0303 ) , \u25b3\u0302Y\u2192X =\n\u2225C\u2217 Y,X\u0303 \u22252F\ntr(C\u2217Y ) \u00b7 tr(C\u2217X\u0303) , (9)\nwhere Y\u0303\u225c(Y |X), X\u0303\u225c(X|Y ), C\u2217 X,Y\u0303 and C\u2217 Y,X\u0303 are specially-designed covariance matrices, and C\u2217X , C\u2217Y , C\u2217X\u0303 and C \u2217 Y\u0303 are specially-designed variance matrices."
        },
        {
            "heading": "3.3 IMPLEMENTING FCIT AND FICP WITH SUMMARY STATISTICS",
            "text": "More details are given about how to implement FCIT and FICP with summary statistics. The procedures at the clients and the server are shown in Algorithm 1. Each client needs to calculate its local sample size and covariance tensor, which are aggregated into summary statistics at the server.\nThe summary statistics contain two parts: total sample size n and covariance tensor CT . With the summary statistics as a proxy, we can substitute the raw data at each client for FCD. The global\nAlgorithm 1 FedCDH: Federated Causal Discovery from Heterogeneous Data Input: data matrix Dk \u2208 Rnk\u00d7d at each client, k,\u2127 \u2208 {1, . . . ,K} Output: a causal graph G\nClient executes: 1: (Summary Statistics Calculation) For each client k, use the local data Dk to get the sample size nk and calculate the covariance tensor CTk , and send them to the server. Server executes: 2: (Summary Statistics Construction) Construct the summary statistics by summing up the local sample sizes and the local covariance tensors: n = \u2211K k=1 nk, CT = \u2211K k=1 CTk . 3: (Augmented Graph Initialization) Build a completely undirected graph G0 on the extended variable set V \u222a {\u2127}, where V denotes the observed variables and \u2127 is surrogate variable. 4: (Federated Conditional Independence Test) Conduct the federated conditional independence test based on the summary statistics, for skeleton discovery on augmented graph and direction determination with one changing causal module. In the end, get an intermediate graph G1. 5: (Federated Independent Change Principle) Conduct the federated independent change principle based on the summary statistics, for direction determination with two changing causal modules. Ultimately, output the causal graph G.\nstatistics are decomposable because they could be obtained by simply summing up the local ones, such as n= \u2211K k=1 nk and CT = \u2211K k=1 CTk . Specifically, we incorporate the random Fourier features (Rahimi & Recht, 2007), because they have shown competitive performances to approximate the continuous shift-invariant kernels. According to the following Lemma, we could derive a decomposable covariance matrix from an indecomposable kernel matrix via random features. Lemma 7 (Estimating Covariance Matrix from Kernel Matrix). Assuming there are n i.i.d. samples for the centralized kernel matrices K\u0303x, K\u0303y, K\u0303x,y and the covariance matrix Cx,y , we have\ntr(K\u0303x,y) \u2248 tr(\u03d5\u0303w(x)\u03d5\u0303w(y)T ) = tr(\u03d5\u0303w(y)T \u03d5\u0303w(x)) = n tr(Cx,y), tr(K\u0303xK\u0303y) \u2248 tr(\u03d5\u0303w(x)\u03d5\u0303w(x)T \u03d5\u0303w(y)\u03d5\u0303w(y)T ) = \u2225\u03d5\u0303w(x)T \u03d5\u0303w(y)\u22252 = n2\u2225Cx,y\u22252,\n(10)\nwhere x,y\u2208Rn, K\u0303x, K\u0303y, K\u0303x,y\u2208Rn\u00d7n, Cx,y\u2208Rh\u00d7h, \u03d5\u0303w(x)\u2208Rn\u00d7h is the centralized random feature, \u03d5\u0303w(x)=H\u03d5w(x), \u03d5w(x)\u225c \u221a 2 h [cos(w1x+b1), . . . , cos(whx+bh)]\nT and \u03d5w(x)\u2208Rn\u00d7h, and similarly for \u03d5\u0303w(y) and \u03d5w(y). w is drawn from P(w) and b is drawn uniformly from [0, 2\u03c0].\nIn this paper, we use random features to approximate the Gaussian kernel for continuous variables and the delta kernel for discrete variables such as the surrogate variable \u2127. It is important to note that this surrogate variable \u2127 is essentially a discrete variable (more specifically, a categorical variable, with no numerical order among different values), and a common approach to deal with such discrete variables is to use delta kernel. Notice that Cx,y denotes the covariance matrix for variable sets x and y, which is sample-wise decomposable because Cx,y= \u2211K k=1 Cxk,yk , where Cxk,yk corresponds to the local covariance matrix of variable sets xk and yk at the k-th client. Here, we have xk,yk\u2208Rnk , Cxk,yk\u2208Rh\u00d7h. In the augmented graph, there are d\u2032=d+1 variables (d observed variables and one surrogate variable), thus we could construct a global covariance tensor CT \u2208Rd \u2032\u00d7d\u2032\u00d7h\u00d7h by summing up the local ones CTk\u2208Rd \u2032\u00d7d\u2032\u00d7h\u00d7h. Theorem 8 (Sufficiency of Summary Statistics). The summary statistics, consisting of total sample size n and covariance tensor CT , are sufficient to represent all the statistics for FCD, including TCI in Eq. 7, E(T\u0302CI |D) and Var(T\u0302CI |D) in Eq. 8, and \u25b3\u0302X\u2192Y and \u25b3\u0302Y\u2192X in Eq. 9.\nAccording to the above theorem, with the total sample size n and the global covariance tensor CT at the server, it is sufficient to conduct FCIT and FICP in the FCD procedures. More details about skeleton discovery and direction determination rules will be given in Appendix A6."
        },
        {
            "heading": "3.4 COMMUNICATION COSTS AND SECURE COMPUTATIONS",
            "text": "We propose to construct summary statistics without directly sharing the raw data, which has already preserved the data privacy to some extent. The original sample size of raw data is in Rn\u00d7d\u2032 , where\nwe assume n \u226b d\u2032, h. The constructed covariance tensor is in dimension Rd\u2032\u00d7d\u2032\u00d7h\u00d7h, which could significantly reduce the communication costs when the sample size n is large enough and the hidden dimension h is small enough. Furthermore, if each client is required to not directly share the local summary statistics, one can incorporate some standard secure computation techniques, such as secure multiparty computation (Cramer et al., 2015), which allows different clients to collectively compute a function over their inputs while keeping them private, or homomorphic encryption (Acar et al., 2018), which enables complex mathematical operations to process encrypted data without compromising the encryption. Please refer to Goryczka & Xiong (2015) for more about secure computation. It is worth noting that some secure computation techniques can introduce significant computation overhead. To further enhance privacy protection and computational efficiency, it would be beneficial to further improve our proposed method and we leave it for future explorations."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "To evaluate the efficacy of our proposed method, we conduct extensive experiments on both synthetic and real-world datasets. For the synthetic datasets, we consider the linear Gaussian model and general functional model to show that our method can handle arbitrary functional causal models. We ensure that all synthetic datasets have some changing causal models, meaning that they are heterogeneous data. To show the wide applicability of our method, we run two real-world datasets, fMRI Hippocampus (Poldrack et al., 2015) and HK Stock Market datasets (Huang et al., 2020).\nSynthetic Datasets. The true DAGs are simulated by Erdo\u0308s-Re\u0301nyi model (Erdo\u030bs et al., 1960) with the number of edges equal to the number of variables. We randomly select 2 variables out of d variables to be changing across clients. For the changing causal model, we generate according to the SCM: Vi= \u2211 Vj\u2208PAi \u03c3\u0302 k ijfi(Vj)+\u03b3\u0302\nk\u03f5i, where Vj\u2208PAi is the direct cause of Vi. The causal strength \u03c3\u0302kij and the parameter \u03b3\u0302\nk change across different client with index k, which are uniformly sampled from U(0.5, 2.5) and U(1, 3), respectively. We separately generate the data for each domain with different causal models and then combine them together. For the fixed causal model, we generate according to Vi= \u2211 Vj\u2208PAi \u03c3\u0302ijfi(Vj)+\u03f5i. We consider the linear Gaussian model with non-equal\nnoise variances and the general functional model. For linear Gaussian model, fi(Vj)=Vj and \u03f5i are sampled from Gaussian distribution with a non-equal variance which is uniformly sampled from U(1, 2). For general functional model, fi is randomly chosen from linear, square, sinc, and tanh functions, and \u03f5i follows uniform distribution U(\u22120.5, 0.5) or Gaussian distribution N (0, 1). We compare our FedCDH method with other FCD baselines, such as NOTEARS-ADMM (for linear case) (Ng & Zhang, 2022), NOTEARS-MLP-ADMM (for non-linear case) (Ng & Zhang, 2022), FedDAG (Gao et al., 2022) and FedPC (Huang et al., 2022). We consider these baselines mainly because of their publicly available implementations. We evaluate both the undirected skeleton and the directed graph, denoted by \u201cSkeleton\u201d and \u201cDirection\u201d in the Figures. We use the structural Hamming distance (SHD), F1 score, precision, and recall as evaluation criteria. We evaluate variable d \u2208 {6, 12, 18, 24, 30} while fixing other variables such as K=10 and nk=100. We set client K\u2208{2, 4, 8, 16, 32} while fixing others such as d=6 and nk=100. We let the sample size in one client nk\u2208{25, 50, 100, 200, 400} while fixing other variables such as d=6 and K=10. Following the setting of previous works such as (Ng & Zhang, 2022), we set the sample size of each client to be equal, although our method can handle both equal and unequal sample size per client. For each setting, we run 10 instances with different random seeds and report the means and standard deviations. The results of F1 score and SHD are given in Figure 3 and Figure A3 for two models, where our FedCDH method generally outperforms the other methods. Although we need large sample properties in the proof of Theorem 4, in practice we only have finite samples. According to the experiment of varying samples, we can see that with more samples the performance of our method is getting better. More analysis including the implementation details, the results of the precision and recall, the analysis of computational time, and the hyperparameter study, the statistical significance test, and the evaluation on graph density are provided in Appendix A7.\nReal-world Datasets. We evaluate our method and the baselines on two real-world dataset, fMRI Hippocampus (Poldrack et al., 2015) and HK Stock Market datasets (Huang et al., 2020). (i) fMRI Hippocampus dataset contains signals from d=6 separate brain regions: perirhinal cortex (PRC), parahippocampal cortex (PHC), entorhinal cortex (ERC), subiculum (Sub), CA1, and CA3/Dentate Gyrus (DG) in the resting states on the same person in 84 successive days. The records for each day can be regarded as one domain, and there are 518 samples for each domain. We select nk=100 samples for each day and select K\u2208{4, 8, 16, 32, 64} days for evaluating varying number of clients. We selectK=10 days and select nk\u2208{25, 50, 100, 200, 400} samples for evaluating varying number of samples. (ii) HK Stock Market dataset contains d=10 major stocks in Hong Kong stock market, which records the daily closing prices from 10/09/2006 to 08/09/2010. Here one day can be also seen as one domain. We set the number of clients to be K\u2208{2, 4, 6, 8, 10} while randomly select nk=100 samples for each client. All other settings are following previous one by default. More dataset information, implementation details, results and analysis are provided in Appendix A8."
        },
        {
            "heading": "5 DISCUSSION AND CONCLUSION",
            "text": "Discussion. (i) Strengths: First of all, by formulating our summary statistics, the requirement for communication between the server and clients is restricted to only one singular instance, thereby substantially reducing the communication times. This is a marked improvement over other baseline methods that necessitate iterative communications. Additionally, the utilization of a surrogate variable enhances our capability to handle heterogeneous data. Furthermore, leveraging the nonparametric characteristics of our proposed FCIT and FICP, our FedCDH method can adeptly manage arbitrary functional causal models. (ii) Limitations: Firstly, the efficiency of our summary statistics in reducing communication costs may not be considerable when the sample size n is small or the hidden dimension h is large. Secondly, our method is designed specifically for horizontally-partitioned federated data, hence it cannot be directly applied to vertically-partitioned federated data.\nConclusion. This paper has put forth a novel constraint-based federated causal discovery method called FedCDH, demonstrating broad applicability across arbitrary functional causal models and heterogeneous data. We construct the summary statistics as a stand-in for raw data, ensuring the protection of data privacy. We further propose FCIT and FICP for skeleton discovery and direction determination. The extensive experiments, conducted on both synthetic and real-world datasets, underscore the superior performance of our method over other baseline methods. For future research, we will enhance our method to address more complex scenarios, such as vertically-partitioned data."
        },
        {
            "heading": "ACKNOWLEDGEMENT",
            "text": "This material is based upon work supported by the AI Research Institutes Program funded by the National Science Foundation under AI Institute for Societal Decision Making (AI-SDM), Award No. 2229881. The project is also partially supported by the National Institutes of Health (NIH) under Contract R01HL159805, and grants from Apple Inc., KDDI Research Inc., Quris AI, and Infinite Brain Technology."
        },
        {
            "heading": "Appendix for",
            "text": "\u201cFederated Causal Discovery from Heterogeneous Data\u201d\nAppendix organization:"
        },
        {
            "heading": "A1 Summary of Symbols 15",
            "text": ""
        },
        {
            "heading": "A2 Related Works 15",
            "text": ""
        },
        {
            "heading": "A3 Details about the Characterization 16",
            "text": "A3.1 Characterization of Conditional Independence . . . . . . . . . . . . . . . . . . . . 16\nA3.2 Characterization of Independent Change . . . . . . . . . . . . . . . . . . . . . . . 18"
        },
        {
            "heading": "A4 Proofs 18",
            "text": "A4.1 Proof of Lemma 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nA4.2 Proof of Theorem 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nA4.3 Proof of Theorem 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nA4.4 Proof of Theorem 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nA4.5 Proof of Lemma 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nA4.6 Proof of Theorem 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24"
        },
        {
            "heading": "A5 Details about Federated Unconditional Independence Test 24",
            "text": "A5.1 Null Hypothesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\nA5.2 Null Distribution Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 25"
        },
        {
            "heading": "A6 Details about Skeleton Discovery and Direction Determination 25",
            "text": "A6.1 Skeleton Discovery. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\nA6.2 Direction Determination. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26"
        },
        {
            "heading": "A7 Details about the Experiments on Synthetic Datasets 28",
            "text": "A7.1 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\nA7.2 Analysis of F1 and SHD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\nA7.3 Results of Precision and Recall . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nA7.4 Results of Computational Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nA7.5 Hyperparameter Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\nA7.6 Statistical Significance Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nA7.7 Evaluation on Dense Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nA7.8 Evaluation on the power of conditional independence test . . . . . . . . . . . . . . 32\nA7.9 Evaluation on the order of domain indices . . . . . . . . . . . . . . . . . . . . . . 32\nA8 Details about the Experiments on Real-world Dataset 33\nA8.1 Details about fMRI Hippocampus Dataset . . . . . . . . . . . . . . . . . . . . . . 33\nA8.2 Details about HK Stock Market Dataset . . . . . . . . . . . . . . . . . . . . . . . 34"
        },
        {
            "heading": "A1 SUMMARY OF SYMBOLS",
            "text": "In order to improve the readability of our paper, we summarize the most important symbols and their meanings throughout the paper, as shown in Table A1.\nTable A1: Summary of symbols Symbol Meaning Symbol Meaning\nd the number of observed variables. n the total sample size of all clients. K the number of clients. V the data matrix, V \u2208 Rn\u00d7d. k the client index, k \u2208 {1, ...,K}. nk the sample size of k-th client. \u2127 domain index, \u2127 \u2208 {1, ...,K}. Vi the i-th variable, i \u2208 {1, ..., d}. fi(\u00b7) the causal function of variable Vi. PAi the parents of Vi. \u03f5i the noise term of Vi. \u03c8\u0303 a set of \u201cpseudo confounders\u201d. \u03b8i the effective parameter of Vi. L the number of \u201cpseudo confounders\u201d.\nG, Gobs the causal graph with d variables. Gaug the augmented graph with d+1 variables. X,Y, Z a set of random variables. kX , kY , kZ the positive definite kernels. X ,Y,Z the domains for the variables. HX ,HY ,HZ the reproducing kernel Hilbert spaces. X\u0308 X\u0308 \u225c (X,Z). \u25b3\u0302 the normalized HSIC. \u03a3 the cross-covariance operator in infinite dimension. C the cross-covariance matrix in finite dimension. h the number of hidden features/mapping functions. f(\u00b7), g(\u00b7), q(\u00b7) the mapping functions. F the function spaces. u, v the regression coefficients. A\u0308 A\u0308 = f(X\u0308) \u2208 Rn\u00d7h. B B = g(Y ) \u2208 Rn\u00d7h. \u03b3 the ridge parameter. I the identity matrix.\nK\u0303 the centralized kernel matrix. H the matrix for centralization,H = I \u2212 1n11 T .\nTCI the test statistic for conditional independence. R the residual for ridge regression. \u03b1 the standard Gaussian variable. \u03b12 the \u03c721 variable. \u03bb the nonzero eigenvalues. T\u0302CI the asymptotic statistic.\nk\u0302, \u03b8\u0302 the parameters for Gamma distribution \u0393(k\u0302, \u03b8\u0302). C\u2217 the specially-designed covariance matrix. tr(\u00b7) the trace operator. d\u2032 d\u2032 = d+ 1 (plus one surrogate variable). CT global covariance tensor, CT \u2208 Rd \u2032\u00d7d\u2032\u00d7h\u00d7h. CTk local covariance tensor, CTk \u2208 Rd \u2032\u00d7d\u2032\u00d7h\u00d7h.\nw the coefficients for random features. b the intercepts for random features."
        },
        {
            "heading": "A2 RELATED WORKS",
            "text": "Causal Discovery. In general, there are mainly three categories of methods for causal discovery (CD) from observed data (Spirtes & Zhang, 2016): constraint-based methods, score-based methods and function-based methods. Constraint-based methods utilize the conditional independence test (CIT) to learn a skeleton of the directed acyclic graph (DAG), and then orient the edges upon the skeleton. Such methods contain Peter-Clark (PC) algorithm (Spirtes & Zhang, 2016) and Fast Causal Inference (FCI) algorithm (Spirtes, 2001). Some typical CIT methods include kernel-based independent conditional test (Zhang et al., 2012) and approximate kernel-based conditional independent test (Strobl et al., 2019). Score-based methods use a score function and a greedy search method to learn a DAG with the highest score by searching all possible DAGs from the data, such as Greedy Equivalent Search (GES) (Chickering, 2002). Within the score-based category, there is a continuous optimization-base subcategory attracting increasing attention. NOTEARS (Zheng et al., 2018) firstly reformulates the DAG learning process as a continuous optimization problem and solves it using gradient-based method. NOTEARS is designed under the assumption of the linear relations between variables. Subsequent works have extended NOTEARS to handle nonlinear cases via deep neural networks, such as DAG-GNN (Yu et al., 2019) and DAG-NoCurl (Yu et al., 2021). ENCO (Lippe et al., 2022) presents an efficient DAG discovery method for directed acyclic causal graphs utilizing both observational and interventional data. AVCI (Lorch et al., 2022) infers causal structure by performing amortized variational inference over an arbitrary data-generating distribution. These continuous-optimization-based methods might suffer from various technical issues, including convergence (Wei et al., 2020; Ng et al., 2022), nonconvexity (Ng et al., 2023), and sensitivity to\ndata standardization (Reisach et al., 2021). Function-based methods rely on the causal asymmetry property, including the linear non-Gaussion model (LiNGAM) (Shimizu et al., 2006), the additive noise model (Hoyer et al., 2008), and the post-nonlinear causal model (Zhang & Hyvarinen, 2012).\nCausal Discovery from Heterogeneous Data. Most of the causal discovery methods mentioned above usually assume that the data is independently and identically distributed (i.i.d.). However, in practical scenarios, distribution shift is possibly occurring across datasets, which can be changing across different domains or over time, as featured by heterogeneous or non-stationary data (Huang et al., 2020). To tackle the issue of changing causal models, one may try to find causal models on sliding windows for non-stationary data (Calhoun et al., 2014), and then compare them. Improved versions include the regime aware learning algorithm to learn a sequence of Bayesian networks that model a system with regime changes (Bendtsen, 2016). Such methods may suffer from high estimation variance due to sample scarcity, large type II errors, and a large number of statistical tests. Some methods aim to estimate the time-varying causal model by making use of certain types of smoothness of the change (Huang et al., 2015), but they do not explicitly locate the changing causal modules. Several methods aim to model time-varying time-delayed causal relations (Xing et al., 2010), which can be reduced to online parameter learning because the direction of the causal relations is given (i.e., the past influences the future). Moreover, most of these methods assume linear causal models, limiting their applicability to complex problems with nonlinear causal relations. In particular, a nonparametric constraint-based method to tackle this causal discovery problem from non-stationary or heterogeneous data, called CD-NOD (Huang et al., 2020), was recently proposed, where the surrogate variable was introduced, written as smooth functions of time or domain index. The first model-based method was proposed for heterogeneous data in the presence of cyclic causality and confounders, named CHOD (Zhou et al., 2022). Saeed et al. (Saeed et al., 2020) provided a graphical representation via the mixture DAG of distributions that arise as mixtures of causal DAGs.\nFederated Causal Discovery. A two-step procedure was adopted (Gou et al., 2007) to learn a DAG from horizontally partitioned data, which firstly estimated the structures independently using each client\u2019s local dataset, and secondly applied further conditional independence test. Instead of using statistical test in the second step, a voting scheme was used to pick those edges identified by more than half of the clients (Na & Yang, 2010). These methods leverage only the final graphs independently estimated from each local dataset, which may lead to suboptimal performance as the information exchange may be rather limited. Furthermore, (Samet & Miri, 2009) developed a privacy-preserving method based on secure multiparty computation, but was limited to the discrete case. For vertically partitioned data, (Yang et al., 2019) constructed an approximation to the score function in the discrete case and adopted secure multiparty computation. (Chen et al., 2003) developed a four-step procedure that involves transmitting a subset of samples from each client to a central site, which may lead to privacy concern. NOTEARS-ADMM (Ng & Zhang, 2022) and Fed-DAG (Gao et al., 2022) were proposed for the federated causal discovery (FCD) based on continuous optimization methods. Fed-PC (Huang et al., 2022) was developed as a federated version of classical PC algorithm, however, it was developed for homogeneous data, which may lead to poor performance on heterogeneous data. DARLIS (Ye et al., 2022) utilizes the distributed annealing (Arshad & Silaghi, 2004) strategy to search for the optimal graph, while PERI (Mian et al., 2023) aggregates the results of the local greedy equivalent search (GES) (Chickering, 2002) and chooses the worst-case regret for each iteration. Fed-CD (Abyaneh et al., 2022) was proposed for both observational and interventional data based on continuous optimization. FEDC2SL (Wang et al., 2023) extended \u03c72 test to the federated version, however, this method is restrictive on discrete variables and therefore not applicable for any continuous variables. Notice that most of these above-mentioned methods heavily rely on either identifiable functional causal models or homogeneous data distributions. These assumptions may be overly restrictive and difficult to be satisfied in real-world scenarios, limiting their diverse applicability."
        },
        {
            "heading": "A3 DETAILS ABOUT THE CHARACTERIZATION",
            "text": ""
        },
        {
            "heading": "A3.1 CHARACTERIZATION OF CONDITIONAL INDEPENDENCE",
            "text": "In this section, we will provide more details about the interpretation of \u03a3X\u0308Y |Z as formulated in Eq. 13, the definition of characteristic kernel as shown in Lemma 9, which is helpful to understand the\nLemma 1 in the main paper. We then provide the uncorrelatedness-based characterization of CI in Lemma 10.\nFirst of all, for the random vector (X,Y ) on X \u00d7 Y , the cross-covariance operator fromHY toHX is defined by the relation\n\u27e8f,\u03a3XY g\u27e9HX = EXY [f(X)g(Y )]\u2212 EX [f(X)]EY [g(Y )], (11)\nfor all f \u2208 HX and g \u2208 HY . Furthermore, we define the partial cross-covariance operator as\n\u03a3XY |Z = \u03a3XY \u2212 \u03a3XZ\u03a3\u22121ZZ\u03a3ZY . (12)\nIf \u03a3ZZ is not invertible, use the right inverse instead of the inverse. We can intuitively interpret the operator \u03a3XY |Z as the partial cross-covariance between {f(X),\u2200f\u2208HX } and {g(Y ),\u2200g\u2208HY} given {q(Z),\u2200q\u2208HZ}. Lemma 9 (Characteristic Kernel (Fukumizu et al., 2007)). A kernel KX is characteristic, if the condition EX\u223cPX [f(X)]=EX\u223cQX [f(X)] (\u2200f\u2208HX ) implies PX=QX , where PX and QX are two probability distributions of X . Gaussian kernel and Laplacian kernel are characteristic kernels.\nAs shown in Lemma 1, if we use characteristic kernel and define X\u0308 \u225c (X,Z), the characterization of CI could be related to the partial cross-covariance as \u03a3X\u0308Y |Z = 0 \u21d0\u21d2 X \u22a5\u22a5 Y |Z, where\n\u03a3X\u0308Y |Z = \u03a3X\u0308Y \u2212 \u03a3X\u0308Z\u03a3 \u22121 ZZ\u03a3ZY . (13)\nSimilarly, we can intuitively interpret the operator \u03a3X\u0308Y |Z as the partial cross-covariance between {f(X\u0308),\u2200f\u2208HX\u0308 } and {g(Y ),\u2200g\u2208HY} given {q(Z),\u2200q\u2208HZ}. Based on Lemma 1, we further consider a different characterization of CI which enforces the uncorrelatedness of functions in suitable spaces, which may be intuitively more appealing. Denote the probability distribution of X as PX and the joint distribution of (X,Y ) as PXY . Let L2X be the space of square integrable functions of X and L 2 XY be that of (X,Y ). Specifically, L2X = {f(X)|E(f2) <\u221e}, and likewise for L2XY . Particularly, consider the following constrained L2 spaces:\nSX\u0308 \u225c {f \u2208 L 2 X\u0308 | E(f |Z) = 0},\nSY\u0308 \u225c {g \u2208 L 2 Y\u0308 | E(g|Z) = 0},\nS \u2032Y |Z \u225c {g \u2032 | g\u2032 = g(Y )\u2212 E(g|Z), g \u2208 L2Y }.\n(14)\nThey can be constructed from the corresponding L2 spaces via nonlinear regression. From example, for any function f \u2208 L2XZ , the corresponding function f \u2032 is given by:\nf \u2032(X\u0308) = f(X\u0308)\u2212 E(f |Z) = f(X\u0308)\u2212 \u03b2\u2217f (Z), (15)\nwhere \u03b2\u2217f (Z) \u2208 L2Z is the regression function of f(X\u0308) on Z. Then, we can then relate the different characterization of CI from Lemma 1 to the uncorrelatedness in the following lemma. Lemma 10 (Characterization of CI based on Partial Association (Daudin, 1980)). Each of the following conditions are equivalent to X \u22a5\u22a5 Y |Z\n(i.) E(fg) = 0,\u2200f \u2208 SX\u0308 and \u2200g \u2208 SY\u0308 , (ii.) E(fg\u2032) = 0,\u2200f \u2208 SX\u0308 and \u2200g \u2032 \u2208 S \u2032Y |Z ,\n(iii.) E(fg\u0303) = 0,\u2200f \u2208 SX\u0308 and \u2200g\u0303 \u2208 L 2 Y\u0308 , (iv.) E(fg\u0303\u2032) = 0,\u2200f \u2208 SX\u0308 and \u2200g\u0303 \u2032 \u2208 L2Y .\n(16)\nWhen (X,Y, Z) are jointly Gaussian, the independence is equivalent to the uncorrelatedness, in other words, X \u22a5\u22a5 Y |Z is equivalent to the vanishing of the partial correlation coefficient \u03c1XY |Z . We can regard the Lemma 10 as as a generalization of the partial correlation based characterization of CI.\nFor example, condition (i) means that any \u201dresidual\u201d function of (X,Z) given Z is uncorrelated with that of (Y,Z) given Z. Here we can observe the similarity between Lemma 1 and Lemma 10, except the only difference that Lemma 10 considers all functions in L2 spaces, while Lemma 1 exploits the spaces corresponding to some characteristic kernels. If we restrict the function f and g\u2032 in condition (ii) to the spacesHX\u0308 andHY , respectively, Lemma 10 is then reduced to Lemma 1. Based on the two lemmas mentioned above plus the Lemma 1, we could further derive Lemma 3 in our main paper."
        },
        {
            "heading": "A3.2 CHARACTERIZATION OF INDEPENDENT CHANGE",
            "text": "In Lemma 2 of the main paper, we provide the independent change principle (ICP) to evaluate the dependence between two changing causal models. Here, we give more details about the definition and the assigned value of normalized HSIC. A smaller value means being more independent. Definition 1 (Normalized HSIC (Fukumizu et al., 2007)). Given variables U and V , HSIC provides a measure for testing their statistical independence. An estimator of normalized HSIC is given as\nHSICNUV = tr(M\u0303UM\u0303V )\ntr(M\u0303U ) tr(M\u0303V ) , (17)\nwhere M\u0303U and M\u0303V are the centralized Gram matrices, M\u0303U \u225c HMUH , M\u0303V \u225c HMVH , H = I \u2212 1n11\nT , I is n \u00d7 n identity matrix and 1 is vector of n ones. How to construct MU andMV will be explained in the corresponding cases below. To check whether two causal modules change independently across different domains, the dependence between P(X) and P(Y |X) and the dependence between P(Y ) and P(X|Y ) on the given data can be given by\n\u25b3X\u2192Y = tr(M\u0303XM\u0303Y |X)\ntr(M\u0303X) tr(M\u0303Y |X) , \u25b3Y\u2192X =\ntr(M\u0303Y M\u0303X|Y )\ntr(M\u0303Y ) tr(M\u0303X|Y ) . (18)\nAccording to CD-NOD (Huang et al., 2020), instead of working with conditional distribution P(X|Y ) and P(Y |X), we could use the \u201djoint distribution\u201d P(X,Y ), which is simpler, for estimation. Here we use Y instead of Y to emphasize that in this constructed distribution X and Y are not symmetric. Then, the dependence values listed in Eq. 18 could be estimated by\n\u25b3\u0302X\u2192Y = tr(M\u0303XM\u0303Y X)\ntr(M\u0303X) tr(M\u0303Y X) , \u25b3\u0302Y\u2192X =\ntr(M\u0303Y M\u0303XY )\ntr(M\u0303Y ) tr(M\u0303XY ) , (19)\nwhere M\u0303X \u225cHMXH ,MX \u225c \u00b5\u0302X|\u2127\u00b7\u00b5\u0302TX|\u2127. Similarly, we define M\u0303Y ,MY and \u00b5\u0302Y |\u2127. According to (Huang et al., 2020), we have\n\u00b5\u0302X|\u2127 \u225c \u03d5(\u2127)(C\u2127\u2127 + \u03b3I)\u22121C\u2127X , (20)\nwhere \u00b5\u0302X|\u2127 \u225c \u03d5(\u2127)(C\u2127\u2127 + \u03b3I)\u22121C\u2127X , \u00b5\u0302X|\u2127, \u03d5(\u2127) \u2208 Rn\u00d7h, \u03b3 is a small ridge parameter, \u03d5 represents the feature map, and \u2127 is the surrogate variable indicating different domains or clients. Similarly, we define M\u0303Y ,MY and \u00b5\u0302Y |\u2127.\n\u00b5\u0302Y |\u2127 \u225c \u03d5(\u2127)(C\u2127\u2127 + \u03b3I)\u22121C\u2127Y . (21)\nMoreover, M\u0303Y X \u225cHMY XH ,MY X \u225c \u00b5\u0302Y X|\u2127 \u00b7 \u00b5\u0302TY X|\u2127. Similarly, we define M\u0303XY ,MXY and \u00b5\u0302XY .\n\u00b5\u0302Y X|\u2127 \u225c \u03d5(\u2127)(C\u2127\u2127 + \u03b3I)\u22121C\u2127,(Y,X) \u00b5\u0302XY |\u2127 \u225c \u03d5(\u2127)(C\u2127\u2127 + \u03b3I)\u22121C\u2127,(X,Y ),\n(22)\nEq. 19 as formulated above is helpful to further derive Theorem 5 in our main paper."
        },
        {
            "heading": "A4 PROOFS",
            "text": "Here, we provide the proofs of the theorems and lemmas, including Lemma 3, Theorem 4, Theorem 5, Theorem 6, Lemma 7, and Theorem 8 in our main paper."
        },
        {
            "heading": "A4.1 PROOF OF LEMMA 3",
            "text": "Proof: We define the covariance matrix in the null hypothesis as CX\u0308Y |Z = 1 n \u2211n i=1[(A\u0308i \u2212 E(A\u0308|Z))T (Bi\u2212E(B|Z))] which corresponds to the partial cross-covariance matrix with n samples, CX\u0308Y |Z\u2208Rh\u00d7h, A\u0308=f(X\u0308)\u2208Rn\u00d7h, B=g(Y )\u2208Rn\u00d7h, {f j(X\u0308)|hj=1}\u2208FX\u0308 , {gj(Y )|hj=1}\u2208FY . Notice that FX\u0308 and FY are function spaces. n and h denote the number of total samples of all clients and the number of hidden features or mapping functions, respectively.\nNotice that E(A\u0308|Z) and E(B|Z) could be non-linear functions of Z which may be difficult to estimate. therefore, we would like to approximate them with linear functions. Let q(Z)\u2208Rn\u00d7h, {qj(Z)|hj=1}\u2208FZ . We could estimate E(f j |Z) with the ridge regression output uTj q(Z) under the mild conditions given below. Lemma 11. (Sutherland & Schneider, 2015) Consider performing ridge regression of f j on Z. Assume that (i) \u2211n i=1 f j i = 0, f\nj is defined on the domain of X\u0308; (ii) the empirical kernel matrix of Z, denoted by KZ , only has finite entries (i.e., \u2225KZ\u2225\u221e < \u221e); (iii) the range of Z is compact, Z \u2282 RdZ . Then we have\nP [ |E\u0302(f j |Z)\u2212 uTj q(Z)| \u2265 \u03f5 ] \u2264 c0 \u03f52 e\u2212h\u03f5 2c1 , (23)\nwhere E\u0302(f j |Z) is the estimate of E(f j |Z) by ridge regression, c0 and c1 are both constants that do not depend on the sample size n or the number of hidden dimensions or mapping functions h.\nThe exponential rate with respect to h in the above lemma suggests we can approximate the output of ridge regression with a small number of hidden features. Moreover, we could similarly estimate E(gj |Z) with vTj q(Z), because we could guarantee that P [ |E\u0302(gj |Z)\u2212 vTj q(Z)| \u2265 \u03f5 ] \u2192 0 for any\nfixed \u03f5 > 0 at an exponential rate with respect to h.\nSimilar to the L2 spaces in condition (ii) of Lemma 10, we can consider the following condition to approximate conditional independence:\nE(f\u0303 g\u0303) = 0,\u2200f\u0303 \u2208 F\u0303X\u0308|Z and \u2200g\u0303 \u2208 F\u0303Y |Z , where\nF\u0303X\u0308|Z = {f\u0303 | f\u0303 j = f j \u2212 E(f j |Z), f j \u2208 FX\u0308},\nF\u0303Y |Z = {g\u0303 | g\u0303j = gj \u2212 E(gj |Z), gj \u2208 FY }.\n(24)\nAccording to Eq. 23, we could estimate E(f j |Z) and E(gj |Z) by uTj q(Z) and vTj q(Z), respectively. Thus, we can reformulate the function spaces as\nF\u0303X\u0308|Z = {f\u0303 | f\u0303 j = f j \u2212 uTj q(Z), f j \u2208 FX\u0308},\nF\u0303Y |Z = {g\u0303 | g\u0303j = gj \u2212 vTj q(Z), gj \u2208 FY }. (25)\nProof ends.\nA4.2 PROOF OF THEOREM 4\n#(%)\n'()\u0308)\n!!(\u2127)\n\"! \"\"\n!\"(\u2127) $\u2113(\u2127)\n*(+)\n(a)\n(b)\n,- ,-! \"$\u0308|\" |\nFigure A1: Given that X \u22a5\u22a5 Y |Z, we could introduce the independence between RX\u0308|Z and RY |Z .\nProof: Assume that there are n i.i.d. samples for X,Y, Z. Let K\u0303X\u0308|Z be the centralized kernel matrix, given by K\u0303X\u0308|Z\u225cR\u0303X\u0308|ZR\u0303 T X\u0308|Z=HRX\u0308|ZR T X\u0308|ZH , where RX\u0308|Z\u225cf\u0303(X\u0308)=f(X\u0308)\u2212uT q(Z) which can be seen as the residual after ridge regression. Similarly, We could define K\u0303Y |Z\u225cR\u0303Y |ZR\u0303TY |Z=HRY |ZR T Y |ZH and RY |Z\u225cg\u0303(Y )=g(Y )\u2212vT q(Z). Accordingly, we let K\u0303X\u0308Y |Z\u225cR\u0303X\u0308|ZR\u0303 T Y |Z=HRX\u0308|ZR T Y |ZH . We set the test statistic as TCI=n\u2225CX\u0308Y |Z\u22252F , where CX\u0308Y |Z\u225cR\u0303TX\u0308|ZR\u0303Y |Z= 1 nR T X\u0308|ZHHRY |Z .\nLet \u03bbX\u0308|Z and \u03bbY |Z be the eigenvalues of K\u0303X\u0308|Z and K\u0303Y |Z , respectively. Furthermore, we define the EVD decomposition K\u0303X\u0308|Z = VX\u0308|Z\u039bX\u0308|ZV T X\u0308|Z , where \u039bX\u0308|Z is the diagonal matrix containing non-negative eigenvalues \u03bbX\u0308|Z,i. Similarly, we define K\u0303Y |Z = VY |Z\u039bY |ZV T Y |Z with eigenvalues \u03bbY |Z,i. Let \u03c8X\u0308|Z = [\u03c8X\u0308|Z,1, \u03c8X\u0308|Z,2, . . . , \u03c8X\u0308|Z,n] \u225c VY |Z\u039b 1/2 Y |Z and \u03d5Y |Z = [\u03d5Y |Z,1, \u03d5Y |Z,2, . . . , \u03d5Y |Z,n] \u225c VY |Z\u039b 1/2 Y |Z .\nOn the other hand, consider eigenvalues \u03bb\u2217 X\u0308|Z,i and eigenfunctions uX\u0308|Z,i of the kernel kX\u0308|Z w.r.t. the probablity measure with the density P(x\u0308), i.e., \u03bb\u2217 X\u0308|Z,i and uX\u0308|Z,i satisfy \u222b kX\u0308|Z(x\u0308, x\u0308\n\u2032) \u00b7 uX\u0308|Z,i(x\u0308) \u00b7 P(x\u0308)dx\u0308 = \u03bb\u2217X\u0308|Z,i \u00b7 uX\u0308|Z,i(x\u0308 \u2032), where we assume that uX\u0308|Z,i have unit variance, i.e., E[u2 X\u0308|Z,i(X\u0308)] = 1. Similarly, we define kY |Z , \u03bb \u2217 Y |Z,i, and u \u2217 Y |Z,i. Let {\u03b11, . . . , \u03b1n2} denote i.i.d. standard Gaussian variables, and thus {\u03b121, . . . , \u03b12n2} denote i.i.d. \u03c721 variables. Lemma 12 (Kernel-based Conditional Independence Test (Zhang et al., 2012)). Under the null hypothesis that X and Y are conditional independent given Z, we have that the test statistic TCI \u225c 1 n tr(K\u0303X\u0308|ZK\u0303Y |Z) have the same asymptotic distribution as T\u0302CI \u225c 1 n \u2211n2 k=1 \u03bb\u0303k \u00b7 \u03b12k, where \u03bb\u0303k are eigenvalues of wwT , w = [w1, . . . ,wn], with the vector wt obtained by stacking Mt = [\u03c8X\u0308|Z,1(X\u0308t), \u03c8X\u0308|Z,2(X\u0308t), . . . , \u03c8X\u0308|Z,n(X\u0308t)] T \u00b7 [\u03d5Y |Z,1(Yt), \u03d5Y |Z,2(Yt), . . . , \u03d5Y |Z,n(Yt)].\nIn the above lemma, their test statistic is equivalent to ours, due to the fact that 1\nn tr(K\u0303X\u0308|ZK\u0303Y |Z) =\n1 n tr(R\u0303X\u0308|Z(R\u0303 T X\u0308|ZR\u0303Y |ZR\u0303 T Y |Z))\n= 1\nn tr((R\u0303T X\u0308|ZR\u0303Y |ZR\u0303 T Y |Z)R\u0303X\u0308|Z)\n= 1\nn \u2225R\u0303T X\u0308|ZR\u0303Y |Z\u2225 2 F\n= 1\nn \u2225nCX\u0308Y |Z\u2225 2 F\n= n\u2225CX\u0308Y |Z\u2225 2 F .\n(26)\nHowever, their asymptotic distribution is different from ours. Based on their asymptotic distribution, we could go further. The first two rows of Eq. 26 hold true because of the commutative property of trace, namely, tr(AB) = BA, refer to Lemma 6 for more details. According to the formulation of R\u0303X\u0308|Z and R\u0303Y |Z , we have {\nf(X\u0308) = uT q(Z) +RX\u0308|Z g(Y ) = vT q(Z) +RY |Z .\n(27)\nBased on the above formulations, we could easily draw the causal graph as shown in Fig. A1. In particular, considering that X and Y are conditionally independent given Z, we could further determine that RX\u0308|Z and RY |Z are independent, namely, we have\nX \u22a5\u22a5 Y |Z \u21d0\u21d2 RX\u0308|Z \u22a5\u22a5 RY |Z . (28)\nAs f(X\u0308) and g(Y ) are uncorrelated, then E(wt) = 0. Furthermore, the covariance is \u03a3 = C ov(wt) = E(wtwTt ), where w is defined in the same way as in Lemma 12. If RX\u0308|Z \u22a5\u22a5 RY |Z , for k \u0338= i or l \u0338= j, we denote the non-diagonal (ND) entries of \u03a3 as eND, where\neND = E[ \u221a \u03bb\u2217 X\u0308|Z,i\u03bb \u2217 Y |Z,j\u03bb \u2217 X\u0308|Z,k\u03bb \u2217 Y |Z,luX\u0308|Z,iuY |Z,juX\u0308|Z,kuY |Z,l]\n= \u221a \u03bb\u2217 X\u0308|Z,i\u03bb \u2217 Y |Z,j\u03bb \u2217 X\u0308|Z,k\u03bb \u2217 Y |Z,lE[uX\u0308|Z,iuX\u0308|Z,k]E[uY |Z,juY |Z,l]\n= 0.\n(29)\nWe then denote the diagonal entries of \u03a3 as eD, where eD = \u03bb \u2217 X\u0308|Z,i\u03bb \u2217 Y |Z,jE[u 2 X\u0308|Z,i]E[u 2 Y |Z,j ]\n= \u03bb\u2217 X\u0308|Z,i\u03bb \u2217 Y |Z,j ,\n(30)\nwhich are eigenvalues of \u03a3. According to (Zhang et al., 2012), 1n\u03bbX\u0308|Z,i converge in probability \u03bb\u2217 X\u0308|Z . Substituting all the results into the asymptotic distribution in Lemma 12, we can get the updated asymptotic distribution\nT\u0302CI \u225c 1\nn2 \u03b2\u2211 i,j=1 \u03bbX\u0308|Z,i\u03bbY |Z,j\u03b1 2 ij as \u03b2 = n\u2192\u221e. (31)\nwhere \u03b2 is the number of nonzero eigenvalues \u03bbX\u0308|Z of the kernel matrices K\u0303X\u0308|Z .\nConsequently, TCI and T\u0302CI have the same asymptotic distribution. Proof ends."
        },
        {
            "heading": "A4.3 PROOF OF THEOREM 5",
            "text": "Proof: First of all, since \u03b12ij follow the \u03c72 distribution with one degree of freedom, thus we have E(\u03b12ij) = 1 and Var(\u03b12ij) = 2. According to the asymptotic distribution in Theorem 4 and the derivation of Lemma 7, we have\nE(T\u0302CI |D) = 1\nn2 \u2211 i,j \u03bbX\u0308|Z,i\u03bbY |Z,j\n= 1\nn2 \u2211 i \u03bbX\u0308|Z,i \u2211 j \u03bbY |Z,j\n= 1\nn2 tr(K\u0303X\u0308|Z) tr(K\u0303Y |Z)\n= 1\nn2 tr(R\u0303X\u0308|ZR\u0303\nT X\u0308|Z) tr(R\u0303Y |ZR\u0303 T Y |Z)\n= 1\nn2 tr(n \u00b7 CX\u0308|Z) tr(n \u00b7 CY |Z)\n= tr(CX\u0308|Z) tr(CY |Z),\n(32)\nwhere R\u0303X\u0308|Z and R\u0303Y |Z are defined in the proof of Theorem 3 above. Therefore, E(T\u0302CI |D) = tr(CX\u0308|Z) tr(CY |Z).\nFurthermore, \u03b12ij are independent variables across i and j, and notice that tr(K\u0303 2 X\u0308|Z) = \u2211 i \u03bb 2 X\u0308|Z,i,\nand similarly tr(K\u03032Y |Z) = \u2211 i \u03bb 2 Y |Z,i. Based on the asymptotic distribution in Theorem 4, we have\nVar(T\u0302CI |D) = 1\nn4 \u2211 i,j \u03bb2 X\u0308|Z,i\u03bb 2 Y |Z,jVar(\u03b1 2 ij)\n= 2\nn4 \u2211 i \u03bb2 X\u0308|Z,i \u2211 j \u03bb2Y |Z,j\n= 2\nn4 tr(K\u03032 X\u0308|Z) tr(K\u0303 2 Y |Z).\n(33)\nAdditionally, according to the similar rule as in Eq. 26, we have\ntr(K\u03032 X\u0308|Z) = tr(R\u0303X\u0308|ZR\u0303 T X\u0308|ZR\u0303X\u0308|ZR\u0303 T X\u0308|Z)\n= tr(R\u0303T X\u0308|ZR\u0303X\u0308|ZR\u0303 T X\u0308|ZR\u0303X\u0308|Z)\n= \u2225R\u0303T X\u0308|ZR\u0303X\u0308|Z\u2225 2 F\n= \u2225n \u00b7 CX\u0308|Z\u2225 2 F\n= n2\u2225CX\u0308|Z\u2225 2 F .\n(34)\nSimilarly, we have tr(K\u03032Y |Z) = n 2\u2225CY |Z\u22252F . Substituting the results into the above formulation about variance, we have 2n4 tr(K\u0303 2 X\u0308|Z) tr(K\u0303 2 Y |Z) = 2 n4 \u00b7 n\n2\u2225CX\u0308|Z\u22252F \u00b7 n2\u2225CY |Z\u22252F . Thus, Var(T\u0302CI |D) = 2 \u00b7 \u2225CX\u0308|Z\u22252F \u00b7 \u2225CY |Z\u22252F . Proof ends."
        },
        {
            "heading": "A4.4 PROOF OF THEOREM 6",
            "text": "Proof: According to the above-mentioned formulations, we have M\u0303X \u225c HMXH = \u02dc\u0302\u00b5X|\u2127 \u00b7 \u02dc\u0302\u00b5TX|\u2127,\n\u02dc\u0302\u00b5X|\u2127 \u225c H \u00b7 \u00b5\u0302X|\u2127. Based on the rules of estimating covariance matrix from kernel matrix in Lemma 6, we have\ntr(M\u0303X) = tr(\u02dc\u0302\u00b5X|\u2127 \u00b7 \u02dc\u0302\u00b5TX|\u2127)\n= tr(\u02dc\u0302\u00b5TX|\u2127 \u00b7 \u02dc\u0302\u00b5X|\u2127) (35)\n= tr((H\u03d5(\u2127)(C\u2127\u2127 + \u03b3I)\u22121C\u2127X)T (H\u03d5(\u2127)(C\u2127\u2127 + \u03b3I)\u22121C\u2127X)) (36) = tr(CX\u2127(C\u2127\u2127 + \u03b3I)\u22121\u03d5(\u2127)TH \u00b7H\u03d5(\u2127)(C\u2127\u2127 + \u03b3I)\u22121C\u2127X))\n= 1\nn tr(CX\u2127(C\u2127\u2127 + \u03b3I)\u22121C\u2127\u2127(C\u2127\u2127 + \u03b3I)\u22121C\u2127X) (37)\n= 1\nn tr(C\u2217X). (38)\nEq. 35 is obtained due to the trace property of the product of the matrices, as shown in Lemma 6. Eq. 36 is substituting from Eq. 20. Here we use Eq. 38 for simple notation. We can see that it can be represented with some combinations of different covariance matrices. Similarly, we have\ntr(M\u0303Y ) = 1\nn tr(CY\u2127(C\u2127\u2127 + \u03b3I)\u22121C\u2127\u2127(C\u2127\u2127 + \u03b3I)\u22121C\u2127Y ) =\n1 n tr(C\u2217Y ). (39)\nRegarding the centralized Gram matrices for joint distribution, similarly we have\ntr(M\u0303Y X) = 1\nn tr(C(Y,X),\u2127(C\u2127\u2127 + \u03b3I)\u22121C\u2127\u2127(C\u2127\u2127 + \u03b3I)\u22121C\u2127,(Y,X)) =\n1 n tr(C\u2217 Y\u0303 ),\ntr(M\u0303XY ) = 1\nn tr(C(X,Y ),\u2127(C\u2127\u2127 + \u03b3I)\u22121C\u2127\u2127(C\u2127\u2127 + \u03b3I)\u22121C\u2127,(X,Y )) =\n1 n tr(C\u2217 X\u0303 ),\n(40)\nwhere tr(M\u0303Y X) = tr(M\u0303XY ). Furthermore, based on Lemma 6 and Eq. 22, we have\ntr(M\u0303XM\u0303Y X) = tr(\u02dc\u0302\u00b5X|\u2127 \u02dc\u0302\u00b5 T X|\u2127 \u00b7 \u02dc\u0302\u00b5Y X|\u2127 \u02dc\u0302\u00b5 T Y X|\u2127)\n= tr(\u02dc\u0302\u00b5TX|\u2127 \u02dc\u0302\u00b5Y X|\u2127 \u02dc\u0302\u00b5 T Y X|\u2127 \u00b7 \u02dc\u0302\u00b5X|\u2127) (41)\n= \u2225 \u02dc\u0302\u00b5TX|\u2127 \u02dc\u0302\u00b5Y X|\u2127\u2225 2 F\n= \u2225(H\u03d5(\u2127)(C\u2127\u2127 + \u03b3I)\u22121C\u2127X)T (H\u03d5(\u2127)(C\u2127\u2127 + \u03b3I)\u22121C\u2127,(Y,X))\u22252F (42) = \u2225CX\u2127(C\u2127\u2127 + \u03b3I)\u22121\u03d5(\u2127)TH \u00b7H\u03d5(\u2127)(C\u2127\u2127 + \u03b3I)\u22121C\u2127,(Y,X)\u22252F\n= \u2225 1 n CX\u2127(C\u2127\u2127 + \u03b3I)\u22121C\u2127\u2127(C\u2127\u2127 + \u03b3I)\u22121C\u2127,(Y,X)\u22252F (43) = 1\nn2 \u2225C\u2217 X,Y\u0303 \u22252F . (44)\nEq. 41 is obtained due to the trace property of the product of the matrices, as shown in Lemma 6. Eq. 41 is substituting from Eq. 20 and Eq. 22. Here we use Eq. 44 for simple notation. We can see that it can be represented with some combinations of different covariance matrices. Similarly, we have\ntr(M\u0303Y M\u0303XY ) = \u2225 1\nn CY\u2127(C\u2127\u2127 + \u03b3I)\u22121C\u2127\u2127(C\u2127\u2127 + \u03b3I)\u22121C\u2127,(X,Y )\u22252F\n= 1\nn2 \u2225C\u2217 Y,X\u0303 \u22252F .\n(45)\nSubstituting the equations above into Eq. 19, we have\n\u25b3\u0302X\u2192Y = \u2225C\u2217 X,Y\u0303 \u22252F\ntr(C\u2217X) \u00b7 tr(C\u2217Y\u0303 ) , \u25b3\u0302Y\u2192X =\n\u2225C\u2217 Y,X\u0303 \u22252F\ntr(C\u2217Y ) \u00b7 tr(C\u2217X\u0303) . (46)\nProof ends."
        },
        {
            "heading": "A4.5 PROOF OF LEMMA 7",
            "text": "Proof: First of all, we incorporate random Fourier features to approximate the kernels, because they have shown competitive performances to approximate the continuous shift-invariant kernels. Lemma 13 (Random Features (Rahimi & Recht, 2007)). For a continuous shift-invariant kernel K(x, y) on R, we have:\nK(x, y) = \u222b R p(w)ejw(x\u2212y)dw = Ew[\u03b6w(x)\u03b6w(y)], (47)\nwhere \u03b6w(x)\u03b6w(y) is an unbiased estimate of K(x, y) when w is drawn from p(w).\nSince both the probability distribution p(w) and the kernel entry K(x, y) are real, the integral in Eq. 47 converges when the complex exponentials are replaced with cosines. Therefore, we may get a real-values mapping by:\nK(x, y) \u2248 \u03d5w(x)T\u03d5w(y),\n\u03d5w(x) \u225c\n\u221a 2\nh [cos(w1x+ b1), ..., cos(whx+ bh)]\nT ,\n\u03d5w(y) \u225c\n\u221a 2\nh [cos(w1y + b1), ..., cos(why + bh)]\nT ,\n(48)\nwhere w is drawn from p(w) and b is drawn uniformly from [0, 2\u03c0]. x, y, w, b \u2208 R, and the randomized feature map \u03d5w : R \u2192 Rh. The precise form of p(w) relies on the type of the shift-invariant kernel we would like to approximate. Here in this paper, we choose to approximate Gaussian kernel as one of the characteristic kernels, and thus set the probability distribution p(w) to the Gaussian one. Based on Eq. 48, we have\ntr(K\u0303x,y) \u2248 tr(\u03d5\u0303w(x)\u03d5\u0303w(y)T ), (49)\nwhere x,y\u2208Rn, K\u0303x,y\u2208Rn\u00d7n, \u03d5\u0303w(x)\u2208Rn\u00d7h is the centralized random feature, \u03d5\u0303w(x)=H\u03d5w(x). Furthermore, benefiting from the commutative property of the trace of the product of two matrices, we have\ntr(\u03d5\u0303w(x)\u03d5\u0303w(y) T ) = tr(\u03d5\u0303w(y) T \u03d5\u0303w(x)), (50)\nSince each random feature is centralized, meaning the zero mean for each feature, therefore, we have:\ntr(\u03d5\u0303w(y) T \u03d5\u0303w(x)) = tr(\n1 n Cx,y) = 1 n tr(Cx,y), (51)\nwhere Cx,y is the covariance matrix for variable x and y, Cx,y \u2208 Rh\u00d7h, h is the number of hidden features.\nFor the second formulation, we have\ntr(K\u0303xK\u0303y) = tr[\u03d5\u0303w(x)\u03d5\u0303w(x) T \u03d5\u0303w(y)\u03d5\u0303w(y) T ]\n= tr[\u03d5\u0303w(x)(\u03d5\u0303w(x) T \u03d5\u0303w(y)\u03d5\u0303w(y) T )]\n= tr[(\u03d5\u0303w(x) T \u03d5\u0303w(y)\u03d5\u0303w(y) T )\u03d5\u0303w(x)]\n= tr[\u03d5\u0303w(x) T \u03d5\u0303w(y)\u03d5\u0303w(y) T \u03d5\u0303w(x)]\n= \u2225\u03d5\u0303w(x)T \u03d5\u0303w(y)\u22252F = \u2225nCx,y\u22252F = n2\u2225Cx,y\u22252F .\n(52)\nTogether with Eq. 49, Eq. 50, Eq. 51 and Eq. 52 formulated above, we could prove the Lemma 7 in the main paper. Proof ends."
        },
        {
            "heading": "A4.6 PROOF OF THEOREM 8",
            "text": "Proof. The summary statistics contain two parts: total sample size n and covariance tensor CT \u2208 Rd\u2032\u00d7d\u2032\u00d7h\u00d7h. Let CijT \u2208 Rh\u00d7h be the (i, j)-th entry of the covariance tensor, which denotes the covariance matrix of the i-th and the j-th variable.\nWith the summary statistics as a proxy, we can substitute the raw data at each client. During the procedures of causal discovery, the needed statistics include TCI in Theorem 4, E(T\u0302CI |D) and Var(T\u0302CI |D) in Theorem 5, and \u25b3\u0302X\u2192Y and \u25b3\u0302Y\u2192X in Theorem 6. 1) Based on the Eq. (7) in the main paper, we have\nCX\u0308Y |Z = CX\u0308Y \u2212 CX\u0308Z(CZZ + \u03b3I) \u22121CZY\n= C(X,Z),Y \u2212 C(X,Z),Z(CZZ + \u03b3I)\u22121CZY (53) = (CXY + CZY )\u2212 (CXZ + CZZ)(CZZ + \u03b3I)\u22121CZY\nIn this paper, we consider the scenarios where X and Y are single variables, and Z may be a single variable, a set of variables, or empty. Assuming that Z contains L variables. We have\nCZY = L\u2211\ni=1\nCZiY , CXZ = L\u2211\ni=1\nCXZi , CZZ = L\u2211\ni=1 L\u2211 j=1 CZiZj , (54)\nwhere CXY , CZiY , CXZi , and CZiZj are the entries of the covariance tensor CT . According to Theorem 3, TCI \u225c n\u2225CX\u0308Y |Z\u22252F . Therefore, the summary statistics are sufficient to represent TCI .\n2) Similar to Eq. 53, we have\nCX\u0308|Z = (CXX + 2CXZ + CZZ)(CXZ + CZZ)(CZZ + \u03b3I) \u22121(CXZ + CZZ) (55)\nCY |Z = CY Y \u2212 CY Z(CZZ + \u03b3I)\u22121CZY . (56)\nSubstituting Eq. 54 into Eq. 55 and Eq. 56, we can also conclude that the covariance tensor is sufficient to represent CX\u0308|Z and CY |Z . In other words, the summary statistics are sufficient to represent E(T\u0302CI |D) and Var(T\u0302CI |D). 3) As shown in section A4.3, we have\n\u25b3\u0302X\u2192Y = \u2225C\u2217 X,Y\u0303 \u22252F\ntr(C\u2217X) \u00b7 tr(C\u2217Y\u0303 ) , \u25b3\u0302Y\u2192X =\n\u2225C\u2217 Y,X\u0303 \u22252F\ntr(C\u2217Y ) \u00b7 tr(C\u2217X\u0303) , (57)\nwhere each components can be represented as some combinations of covariance matrices, as shown in Eq. 37, Eq. 39, Eq. 40, Eq. 43, and Eq. 45. Therefore, the summary statistics are sufficient to represent \u25b3\u0302X\u2192Y and \u25b3\u0302Y\u2192X . 4) To sum up, we could conclude that: The summary statistics, consisting of total sample size n and covariance tensor CT , are sufficient to represent all the statistics needed for federated causal discovery.\nProof ends."
        },
        {
            "heading": "A5 DETAILS ABOUT FEDERATED UNCONDITIONAL INDEPENDENCE TEST",
            "text": "Here, we provide more details about the federated unconditional independence test (FUIT), where the conditioning set Z is empty. Generally, this method follows similar theorems for federated conditional independent test (FCIT)."
        },
        {
            "heading": "A5.1 NULL HYPOTHESIS",
            "text": "Consider the null and alternative hypothesis\nH0 : X \u22a5\u22a5 Y, H1 : X \u22a5\u0338\u22a5 Y. (58)\nSimilar to FCIT, we consider the squared Frobenius norm of the empirical covariance matrix as an approximation, given as\nH0 : \u2225CX\u0308Y \u2225 2 F = 0, H1 : \u2225CX\u0308Y \u2225 2 F > 0. (59)\nIn this unconditional case, we set the test statistics as TUI \u225c n\u2225CX\u0308Y \u22252F , and give the following theorem.\nTheorem 14 (Federated Unconditional Independent Test). Under the null hypothesisH0 (X and Y are independent), the test statistic\nTUI \u225c n\u2225CXY \u22252F , (60) has the asymptotic distribution\nT\u0302UI \u225c 1\nn2 L\u2211 i,j=1 \u03bbX,i\u03bbY,j\u03b1 2 ij ,\nwhere \u03bbX and \u03bbY are the eigenvalues of K\u0303X and K\u0303Y , respectively. Here, the proof is similar to the proof of Theorem 3, thus we refer the readers to section A4.2 for more details."
        },
        {
            "heading": "A5.2 NULL DISTRIBUTION APPROXIMATION",
            "text": "We also approximate the null distribution with a two-parameter Gamma distribution, which is related to the mean and variance. Under the hypothesisH0 and given the sample D, the distribution of T\u0302CI can be approximated by the \u0393(\u03ba, \u03b8) distribution. Here we provide the theorem for null distribution approximation.\nTheorem 15 (Null Distribution Approximation). Under the null hypothesisH0 (X and Y are independent), we have\nE(T\u0302UI |D) = tr(CX) \u00b7 tr(CY ), Var(T\u0302UI |D) = 2\u2225CX\u22252F \u00b7 \u2225CY \u22252F ,\n(61)\nHere, the proof is similar to the proof of Theorem 4, thus we refer the readers to section A4.3 for more details."
        },
        {
            "heading": "A6 DETAILS ABOUT SKELETON DISCOVERY AND DIRECTION DETERMINATION",
            "text": "In this section, we will introduce how we do the skeleton discovery and direction determination during the process of federated causal discovery. All those steps are conducted on the server side. Our steps are similar to the previous method, such as CD-NOD (Huang et al., 2020), the core difference are that we develop and utilize our proposed federated conditional independent test (FCIT) and federated independent change principle (FICP)."
        },
        {
            "heading": "A6.1 SKELETON DISCOVERY.",
            "text": "We first conduct skeleton discovery on the augmented graph. The extra surrogate variable is introduced in order to deal with the data heterogeneity across different clients.\nLemma 16. Given the Assumptions 1, 2 and 3 in the main paper, for each Vi \u2208 V , Vi and \u2127 are not adjacent in the graph if and only if they are independent conditional on some subset of {Vj |j \u0338= i}.\nProof. If Vi\u2019s causal module is invariant, which means that P(Vi|PAi) remains the same for every value of \u2127, then Vi \u22a5\u22a5 \u2127|PAi. Thus, if Vi and \u2127 are not independent conditional on any subset of other variables, Vi\u2019s module changes with \u2127, which is represented by an edge between Vi and \u2127. Conversely, we assume that if Vi\u2019s module changes, which entails that Vi and \u2127 are not independent given PAi, then Vi and \u2127 are not independent given any other subset of V \\{Vi}. Proof ends.\nLemma 17. Given the Assumptions 1, 2 and 3 in the main paper, for every Vi, Vj \u2208 V , Vi and Vj are not adjacent if and only if they are independent conditional on some subset of {Vl|l \u0338= i, l \u0338= j} \u222a {\u2127}.\nProof. The \u201dif\u201d direction shown based on the faithfulness assumption on Gaug and the fact that {\u03c8l(\u2127)}Ll=1 \u222a {\u03b8i(\u2127)}di=1 is a deterministic function of \u2127. The \u201donly if\u201d direction is proven by making use of the weak union property of conditional independence repeatedly, the fact that all {\u03c8l(\u2127)}Ll=1 and {\u03b8i(\u2127)}di=1 are deterministic function of \u2127, the above three assumptions, and the properties of mutual information. Please refer to (Zhang et al., 2015) for more complete proof.\nWith the given three assumptions in the main paper, we can do skeleton discovery.\ni) Augmented graph initialization. First of all, build a completely undirected graph on the extended variable set V \u222a{\u2127}, where V denotes the observed variables and \u2127 is surrogate variable.\nii) Changing module detection. For each edge \u2127 \u2212 Vi, conduct the federated conditional independence test or federated unconditional independent test. If they are conditionally independent or independent, remove the edge between them. Otherwise, keep the edge and orient \u2127\u2192 Vi.\niii) Skeleton discovery. Moreover, for each edge Vi \u2212 Vj , also conduct the federated independence test or federated unconditional independent test. If they are conditionally independent or independent, remove the edge between them.\nIn the procedures, how observed variables depend on surrogate variable \u2127 is unknown and usually nonlinear, thus it is crucial to use a general and non-parametric conditional independent test method, which should also satisfy the federated learning constraints. Here, we utilize our proposed FCIT."
        },
        {
            "heading": "A6.2 DIRECTION DETERMINATION.",
            "text": "After obtaining the skeleton, we can go on with the causal direction determination. By introducing the surrogate variable \u2127, it does not only allow us to infer the skeleton, but also facilitate the direction determinations. For each variable Vi whose causal module is changing (i.e., \u2127\u2212 Vi), in some ways we might determine the directions of every edge incident to Vi. Assume another variable Vj which is adjacent to Vi, then we can determine the directions via the following rules.\ni) Direction determination with one changing module. When Vj\u2019s causal module is not changing, we can see \u2127 \u2212 Vi \u2212 Vj forms an unshielded triple. For practice purposes, we can take the direction between \u2127 and Vi as \u2127\u2192 Vi, since we let \u2127 be the surrogate variable to indicate whether this causal module is changing or not. Then we can use the standard orientation rules (Spirtes et al., 2000) for unshielded triples to orient the edge between Vi and Vj . (1) If \u2127 and Vi are independent conditional on some subset of {Vl|l \u0338= j} which is excluding Vj , then the triple forms a V-structure, thus we have \u2127\u2192 Vi \u2190 Vj . (2) If \u2127 and Vi are independent conditional on some subset of {Vl|l \u0338= i} \u222a {Vj} which is including Vj , then we have \u2127\u2192 Vi \u2192 Vj . In the procedure, we apply our proposed FCIT.\nii) Direction determination with two changing modules. When Vj\u2019s causal module is changing, we can see there is a special confounder \u2127 between Vi \u2212 Vj . First of all, as mentioned above, we can still orient \u2127 \u2192 Vi and \u2127 \u2192 Vj . Then, inspired by that P (cause) and P (effect|cause) change independently, we can identify the direction between Vi and Vj according to Lemma 1, and we apply our proposed FICP.\n(a) Precision and recall on linear Gaussian model.\n(b) Precision and recall on general functional model.\nFigure A2: Results of the synthetic dataset on (a) linear Gaussian model and (b) general functional model. By rows in each subfigure, we evaluate varying number of variables d, varying number of clients K, and varying number of samples nk. By columns in each subfigure, we evaluate Skeleton Precision (\u2191), Skeleton Recall (\u2191), Direction Precision (\u2191) and Direction Recall (\u2191)."
        },
        {
            "heading": "A7 DETAILS ABOUT THE EXPERIMENTS ON SYNTHETIC DATASETS",
            "text": "More details about the synthetic datasets are explained in this section, including the implementation details in section A7.1, the results analysis of F1 and SHD in section A7.2, the complete results of precision and recall in section A7.3, the computational time analysis in section A7.4, the hyperparameter study on the number of hidden features h in section A7.5, the statistical significance test for the results in section A7.6, and the evaluation on dense graph in section A7.7.\nA7.1 IMPLEMENTATION DETAILS\nWe provide the implementation details of our method and other baseline methods.\n\u2022 FedDAG (Gao et al., 2022): Codes are available at the author\u2019s Github repository https: //github.com/ErdunGAO/FedDAG. The hyperparameters are set by default.\n\u2022 NOTEARS-ADMM and NOTEARS-MLP-ADMM (Ng & Zhang, 2022): Codes are available at the author\u2019s Github repository https://github.com/ignavierng/ notears-admm. The hyperparameters are set by default, e.g., we set the threshold level to 0.1 for post-processing.\n\u2022 FedPC (Huang et al., 2022): Although there is no public implementation provided by the author, considering that it is the only constraint-based method among all the existing works for federated causal discovery, we still compared with it. We reproduced it based on the Causal-learn package https://github.com/py-why/causal-learn. Importantly, we follow the paper, set the voting rate as 30% and set the significance level to 0.05.\n\u2022 FedCDH (Ours): Our method is developed based on the CD-NOD (Huang et al., 2020) and KCI (Zhang et al., 2012) which are publicly available in the Causal-learn package https://github.com/py-why/causal-learn. We set the hyperparameter h to 5, and set the significance level for FCIT to 0.05. Our source code has been appended in the Supplementary Materials.\nFor NOTEARS-ADMM, NOTEARS-MLP-ADMM, and FedDAG, the output is a directed acyclic graph (DAG), while FedPC and our FedCDH may output a completed partially directed acyclic graph (CPDAG). To ease comparisons, we use the simple orientation rules (Dor & Tarsi, 1992) implemented by Causal-DAG (Chandler Squires, 2018) to convert a CPDAG into a DAG. We evaluate both the undirected skeleton and the directed graph, denoted by \u201cSkeleton\u201d and \u201cDirection\u201d as shown in the Figures."
        },
        {
            "heading": "A7.2 ANALYSIS OF F1 AND SHD",
            "text": "We have provided the results of F1 and SHD in the main paper as shown in Figure 3 and Figure A3, here we provide further discussions and analysis.\nThe results of linear Gaussian model are given in Figure 3 and those of general functional model are provided in Figure A3. According to the results, we observe that our FedCDH method generally outperforms all other baselines across different criteria and settings. According to the results of our method on both of the two models, when d increases, the F1 score decreases and the SHD increases for skeletons and directions, indicating that FCD with more variables might be more challenging. On the contrary, when K and nk increase, the F1 score grows and the SHD reduces, suggesting that more joint clients or samples could contribute to better performances for FCD.\nIn linear Gaussian model, NOTEARS-ADMM and FedPC generally outperform FedDAG. The reason may be that the front two methods were proposed for linear model while the latter one was specially proposed for nonlinear model. In general functional model, FedPC obtained the worst performance compared to other methods in direction F1 score, possibly due to its strong assumptions on linear model and homogeneous data. FedDAG and NOTEARS-MLP-ADMM revealed poor results regarding SHD, the reasons may be two-fold: they assume nonlinear identifiable model, which may not well handle the general functional model; and both of them are continuous-optimization-based methods, which might suffer from various issues such as convergence and nonconvexity.\nFigure A3: Results of synthetic dataset on general functional model. By rows, we evaluate varying number of variables d, varying number of clients K, and varying number of samples nk. By columns, we evaluate Skeleton F1 (\u2191), Skeleton SHD (\u2193), Direction F1 (\u2191) and Direction SHD (\u2193)."
        },
        {
            "heading": "A7.3 RESULTS OF PRECISION AND RECALL",
            "text": "In the main paper, we have only provided the results of F1 score and SHD, due to the space limit. Here, we provide more results and analysis of the precision and the recall. The results of average and standard deviation are exhibited in Figure A2. According to the results, we could observe that our FedCDH method generally outperformed all other baseline methods, regarding the precision of both skeleton and direction.\nMoreover, in the linear Gaussian model, NOTEARS-ADMM generally achieved the best performance regarding the recall although it performed poorly in precision, the reason might be that NOTEARS-ADMM assumed homogeneous data distribution, which might face challenges in the scenarios with heterogeneous data. In the general functional model, when evaluating varying numbers of clients K and samples nk, FedDAG performed the best with respect to the recall, however, neither FedDAG nor NOTEARS-MLP-ADMM obtained satisfactory results in the precision, the reason might be that both of them are continuous-optimization-based methods, which might potentially suffer from various issues such as convergence and nonconvexity."
        },
        {
            "heading": "A7.4 RESULTS OF COMPUTATIONAL TIME",
            "text": "Existing works about federated causal discovery rarely evaluate the computational time when conducting experiments. Actually, it is usually difficult to measure the exact computational time in real life, because of some facts, such as the paralleled computation for clients, the communication time costs between the clients and the server, and so on. However, the computational time is a significant factor to measure the effectiveness of a federated causal discovery method to be utilized in practical scenarios. Therefore, in this section, for making fair comparisons, we evaluate the computational time for each method, assuming that there is no paralleled computation (meaning that we record the computational time at each client and server and then simply add them up) and no extra communication cost (indicating zero time cost for communication).\nWe evaluate different settings as mentioned above, including varying number of variables d, varying number of clients K, and varying number of samples nk. We generate data according to linear\nTable A2: Results of computational time for varying number of variables d, varying number of clients K, and varying number of samples nk. We report the average and standard deviation over 10 runs. This is the synthetic dataset based on linear Gaussian model.\nData Sizes Methods d K nk FedPC NOTEARS-ADMM FedDAG FedCDH (Ours)\n6\n10 100 3.87 \u00b1 1.97s 14.10 \u00b1 1.89s 136.92 \u00b1 21.50s 8.14 \u00b1 2.47s 12 32.01 \u00b1 3.54s 28.33 \u00b1 2.46s 321.84 \u00b1 65.94s 62.69 \u00b1 7.77s 18 39.58 \u00b1 4.75s 35.13 \u00b1 2.89s 398.27 \u00b1 149.51s 98.57 \u00b1 9.23s 24 84.05 \u00b1 7.64s 40.01 \u00b1 2.94s 715.80 \u00b1 268.93s 172.11 \u00b1 18.18s 30 94.03 \u00b1 9.48s 56.35 \u00b1 3.91s 1441.13 \u00b1 519.04s 232.35 \u00b1 26.67s\n6\n2\n100 0.72 \u00b1 0.24s 7.04 \u00b1 0.64s 50.38 \u00b1 11.29s 3.88 \u00b1 1.49s 4 2.07 \u00b1 0.73s 9.07 \u00b1 0.77s 85.08 \u00b1 15.68s 5.24 \u00b1 1.74s 8 3.64 \u00b1 1.54s 10.80 \u00b1 0.78s 114.81 \u00b1 29.67s 8.01 \u00b1 2.32s\n16 5.79 \u00b1 2.59s 19.40 \u00b1 2.51s 342.34 \u00b1 62.28s 12.60 \u00b1 2.98s 32 14.08 \u00b1 4.44s 30.56 \u00b1 2.88s 714.06 \u00b1 137.31s 20.30 \u00b1 4.37s\n6 10 25 0.48 \u00b1 0.10s 13.06 \u00b1 1.91s 125.77 \u00b1 20.64s 3.75 \u00b1 1.29s 50 1.47 \u00b1 0.64s 13.75 \u00b1 2.51s 127.25 \u00b1 20.38s 5.74 \u00b1 1.61s 100 3.87 \u00b1 1.97s 14.10 \u00b1 1.89s 136.92 \u00b1 21.50s 8.14 \u00b1 2.47s 200 16.52 \u00b1 3.63s 14.68 \u00b1 2.23s 138.67 \u00b1 31.91s 13.78 \u00b1 3.75s 400 51.10 \u00b1 6.87s 15.90 \u00b1 2.54s 140.37 \u00b1 34.42s 22.86 \u00b1 4.55s\nGaussian model. For each setting, we run 10 instances, report the average and the standard deviation of the computational time. The results are exhibited in Table A2.\nAccording to the results, we could observe that among the four FCD methods, FedDAG is the least efficient method with the largest time cost, because it uses a two-level structure to handle the heterogeneous data: the first level learns the edges and directions of the graph and communicates with the server to get the model information from other clients, while the second level approximates the mechanism among variables and personally updates on its own data to accommodate the data heterogeneity. Meanwhile, FedPC, NOTEARS-ADMM and our FedCDH are comparable. In the setting of varying variables, our method exhibited unsatisfactory performance among the three methods, because the other two methods, FedPC and NOTEARS-ADMM, are mainly for homogeneous data. However, in the case of varying variables, NOTEARS-ADMM is the most ineffective method, because with the increasing of clients, more parameters (one client corresponds to one sub adjacency matrix which needs to be updated) should get involved in the optimization process, therefore, the total processing time can also increase by a large margin. In the scenario of varying samples, FedPC is the slowest one among the three methods."
        },
        {
            "heading": "A7.5 HYPERPARAMETER STUDY",
            "text": "We conduct experiments on the hyperparameter, such as the number of mapping functions or hidden features h. Regarding the experiments in the main paper, we set h to 5 by default. Here in this section, we set h \u2208 {5, 10, 15, 20, 25, 30}, d = 6, K = 10, nk = 100 and evaluate the performances. We generate data according to linear Gaussian model. We use the F1 score, the precision, the recall and the SHD for both skeleton and direction. We also report the runtime. We run 10 instances and report the average values. The experimental results are given in Table A3.\nAccording to the results, we could observe that with the number of hidden features h increasing, the performance of the direction is obviously getting better, while the performance of the skeleton may fluctuate a little bit.\nTheoretically, the more hidden features or a larger h we consider, the better performance of how closely the random features approximate the kernels should be. When the number of hidden features approaches infinity, the performance of random features and that of kernels should be almost the same. And the empirical results seem to be consistent with the theory, where a large h can lead to a higher F1 score and precision for the directed graph.\nTable A3: Hyperparameter study on the number of hidden features h. We evaluate the F1 score, precision, recall, and SHD of both skeleton and direction. We report the average over 10 runs. This is the synthetic dataset based on linear Gaussian model.\nh Metrics Skeleton Direction Time\u2193\nF1 \u2191 Precision\u2191 Recall\u2191 SHD\u2193 F1 \u2191 Precision\u2191 Recall\u2191 SHD\u2193 5 0.916 0.980 0.867 0.9 0.721 0.765 0.683 2.0 8.14s 10 0.916 0.980 0.867 0.9 0.747 0.810 0.700 2.0 8.87s 15 0.907 0.980 0.850 1.0 0.762 0.818 0.717 1.8 10.57s 20 0.889 0.980 0.833 1.2 0.767 0.833 0.717 1.8 12.72s 25 0.896 0.980 0.833 1.1 0.789 0.838 0.750 1.6 20.93s 30 0.896 0.980 0.833 1.1 0.825 0.873 0.783 1.4 37.60s\nTable A4: Test result of statistical significance of our FedCDH method compared with other baseline methods. We report the p values via Wilcoxon signed-rank test (Woolson, 2007). This is the synthetic dataset based on linear Gaussian model.\nParameters [FedCDH vs. FedPC] [FedCDH vs. NOTEARS-ADMM] [FedCDH vs. FedDAG]\nd k n S-F1 S-SHD D-F1 D-SHD S-F1 S-SHD D-F1 D-SHD S-F1 S-SHD D-F1 D-SHD"
        },
        {
            "heading": "6 10 100 0.00 0.05 0.01 0.12 0.00 0.01 0.11 0.10 0.00 0.01 0.01 0.01",
            "text": "12 10 100 0.00 0.01 0.01 0.01 0.00 0.00 0.15 0.00 0.00 0.00 0.11 0.00 18 10 100 0.00 0.01 0.00 0.01 0.00 0.00 0.03 0.00 0.00 0.00 0.02 0.00 24 10 100 0.01 0.01 0.00 0.01 0.00 0.00 0.01 0.00 0.01 0.01 0.01 0.00 30 10 100 0.00 0.01 0.00 0.01 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 6 2 100 0.00 0.00 0.01 0.01 0.01 0.00 0.21 0.01 0.00 0.00 0.03 0.00 6 4 100 0.00 0.01 0.00 0.01 0.01 0.01 0.01 0.00 0.00 0.01 0.01 0.00 6 8 100 0.00 0.00 0.01 0.02 0.02 0.01 0.03 0.02 0.00 0.00 0.09 0.00 6 16 100 0.00 0.01 0.01 0.02 0.00 0.00 0.10 0.03 0.00 0.00 0.07 0.00 6 32 100 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.01 0.00 0.00 0.03 0.00 6 10 25 0.00 0.00 0.01 0.01 0.01 0.01 0.26 0.02 0.00 0.00 0.03 0.00 6 10 50 0.00 0.01 0.01 0.00 0.01 0.00 0.99 0.03 0.00 0.00 0.02 0.00 6 10 200 0.00 0.01 0.01 0.02 0.00 0.00 0.03 0.02 0.00 0.00 0.11 0.01 6 10 400 0.00 0.01 0.01 0.01 0.01 0.00 0.03 0.01 0.00 0.01 0.01 0.00\nMoreover, the computational time is also increasing. When h is smaller than 20, the runtime increases steadily. When h is greater than 20, the runtime goes up rapidly. Importantly, we could see that even when h is small, such as h = 5, the general performance of our method is still robust and competitive."
        },
        {
            "heading": "A7.6 STATISTICAL SIGNIFICANCE TEST",
            "text": "In order to show the statistical significance of our method compared with other baseline methods on the synthetic linear Gaussian model, we report the p values via Wilcoxon signed-rank test (Woolson, 2007), as shown in Table A4. For each baseline method, we evaluate four criteria: Skeleton F1 (SF1), Skeleton SHD (S-SHD), Direction F1 (D-F1), and Direction SHD (D-SHD).\nWe set the significance level to 0.05. Those p values higher than 0.05 are underlined. From the results, we can see that the improvements of our method are statistically significant at 5% significance level in general."
        },
        {
            "heading": "A7.7 EVALUATION ON DENSE GRAPH",
            "text": "As shown in Figure 3 in the main paper, the true DAGs are simulated using the Erdo\u0308s\u2013Re\u0301nyi model (Erdo\u030bs et al., 1960) with the number of edges equal to the number of variables. Here we consider a more dense graph with the number of edges are two times the number of variables.\nwe evaluate on synthetic linear Gaussian model and general functional model, and record the F1 score and SHD for both skeleton and directed graphs. All other settings are following the previous ones by default.\nFigure A4: We evaluate on synthetic linear Gaussian model (Top Row) and general functional model (Bottom Row) when the number of edges are two times the number of variables. By columns, we evaluate Skeleton F1 (\u2191), Skeleton SHD (\u2193), Direction F1 (\u2191) and Direction SHD (\u2193).\nAccording to the results as shown in Figure A4, we can see that our methods still outperformed other baselines in varying number of variables. Interestingly, when the generated graph is more dense, the performance of FedPC will obviously go down for various number of variables."
        },
        {
            "heading": "A7.8 EVALUATION ON THE POWER OF CONDITIONAL INDEPENDENCE TEST",
            "text": "Here we added a new set of experiments to compare the power of our proposed federated conditional independence test and the centralized conditional independence test (i.e., kernel-based conditional independence test (Zhang et al., 2012)).\nWe followed the previous paper (Zhang et al., 2012) and used the post-nonlinear model (Zhang & Hyvarinen, 2012) to generate data. Assume there are four variables W,X, Y , and Z. X = g\u0302(f\u0302(W ) + \u03f5X), Y = g\u0302(f\u0302(W ) + \u03f5Y ), and Z is independent from both X and Y . f\u0302 and g\u0302 are functions randomly chosen from linear, square, sin and tan functions. \u03f5X , \u03f5Y ,W and Z are sampled from either uniform distribution U(\u22120.5, 0.5) or Gaussian distribution N (0, 1). \u03f5X and \u03f5Y are random noises. In this case, X and Y are dependent due to the shared component of W . Since Z is independent from both X and Y , therefore, we have X \u22a5\u0338\u22a5 Y |Z. Here we set the significance level to 0.05, and the total sample size varies from 200, 400, 600, 800 to 1000. For federated CIT, we set the number of clients to 10, therefore, each client has 20, 40, 60, 80, or 100 samples. We run 1000 simulations and record the power of the two tests. From the result in Figure A5, we can see that the power of our federated CIT is almost similar to that of centralized CIT. Particularly, when the sample size reaches 1000, both of the two tests achieve power with more than 95%."
        },
        {
            "heading": "A7.9 EVALUATION ON THE ORDER OF DOMAIN INDICES",
            "text": "In this section, we aim to find out whether the order of domain indices will impact the results. Theoretically, there should be no impact on the results when it takes different values because this domain index \u2127 is essentially a discrete variable (more specifically, a categorical variable, with no numerical order among different values), a common approach to deal with such discrete variable is to use delta kernel (based on Kronecker delta function), and therefore it is reasonable to use random features to approximate the delta kernel for discrete variables.\nEmpirically, we have added one new set of experiments to evaluate whether the order of domain indices will impact the results. We have one set of domain indices and run our FedCDH on the synthetic linear Gaussian model with varying number variables d \u2208 {6, 12, 18, 24, 30} while keeping K = 10 and nk = 100, other settings are the same as those in our main paper. Then, we randomly shuffle the indices for different domains, denoted by \u201cFedCDH+Shuffle\u201d.\n200 300 400 500 600 700 800 900 1000 Number of samples n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPo we\nr\nFederated CIT Centralized CIT\nFigure A5: Comparison regarding the power of test between federate conditional independence test and the centralized conditional independence test.\n10 20 30 Number of variables d\n0.00\n0.25\n0.50\n0.75\n1.00\nSk el\net on\nF 1\n10 20 30 Number of variables d\n0\n10\n20\n30\nSk el\net on\nS H\nD\n10 20 30 Number of variables d\n0.00\n0.25\n0.50\n0.75\n1.00\nD ire\nct io\nn F 1\n10 20 30 Number of variables d\n0\n10\n20\n30\nD ire\nct io\nn SH\nD\nFedCDH+Shuffle FedCDH\nFigure A6: Evaluation on the order of domain indices on linear Gaussian model. We evaluate varying number of variables d. By columns, we evaluate Skeleton F1 (\u2191), Skeleton SHD (\u2193), Direction F1 (\u2191) and Direction SHD (\u2193).\nAs shown in Figure A6, the results turned out that: the performances between the two sets of different domain indices are quite similar, and we may conclude that it has no obvious impact on the results when the domain indices take different values."
        },
        {
            "heading": "A8 DETAILS ABOUT THE EXPERIMENTS ON REAL-WORLD DATASET",
            "text": ""
        },
        {
            "heading": "A8.1 DETAILS ABOUT FMRI HIPPOCAMPUS DATASET",
            "text": "We evaluate our method and the baselines on fMRI Hippocampus (Poldrack et al., 2015). The directions of anatomical ground truth are: PHC\u2192 ERC, PRC\u2192 ERC, ERC\u2192 DG, DG\u2192 CA1, CA1 \u2192 Sub, Sub\u2192 ERC and ERC\u2192 CA1. Generally, we follow a similar setting as the experiments on synthetic datasets. For each of them, we use the structural Hamming distance (SHD), the F1 score as evaluation criteria. We measure both the undirected skeleton and the directed graph. Here, we consider varying number of clients K and varying number of samples in each client nk.\nThe results of F1 score and SHD is given in Figure A7. According to the results, we could observe that our FedCDH method generally outperformed all other baseline methods, across all the criteria listed. The reason could be that our method is specifically designed for heterogeneous data while some baseline methods assume homogeneity like FedPC and NOTEARS-MLP-ADMM, furthermore, our method can handle arbitrary functional causal models, different from some baseline methods that assume linearity such as FedPC. Compared with our method, FedDAG performed much worse, the reason might be its nature of the continuous optimization, which might suffer from various issues such as convergence and nonconvexity.\nPublished as a conference paper at ICLR 2024Real data\nFigure A7: Results of real-world dataset fMRI Hippocampus (Poldrack et al., 2015). By rows, we evaluate varying number of clients K and varying number of samples nk. By columns, we evaluate Skeleton F1 (\u2191), Skeleton SHD (\u2193), Direction F1 (\u2191) and Direction SHD (\u2193).\n2.5 5.0 7.5 10.0 Number of clients K\n0.4\n0.6\nSk el\net on\nF 1\n2.5 5.0 7.5 10.0 Number of clients K\n0\n5\n10\n15\nSk el\net on\nS H\nD\n2.5 5.0 7.5 10.0 Number of clients K\n0.4\n0.6\nD ire\nct io\nn F 1\n2.5 5.0 7.5 10.0 Number of clients K\n0\n5\n10\n15\nD ire\nct io\nn SH\nD\nFedPC NOTEARS-MLP-ADMM FedDAG FedCDH\nFigure A8: Results of real-world dataset HK Stock Market (Huang et al., 2020). We evaluate varying number of clients K, and we evaluate Skeleton F1 (\u2191), Skeleton SHD (\u2193), Direction F1 (\u2191) and Direction SHD (\u2193)."
        },
        {
            "heading": "A8.2 DETAILS ABOUT HK STOCK MARKET DATASET",
            "text": "We also evaluate on HK stock market dataset (Huang et al., 2020) (See Page 41 for more details about the dataset). The HK stock dataset contains 10 major stocks, which are daily closing prices from 10/09/2006 to 08/09/2010. The 10 stocks are Cheung Kong Holdings (1), Wharf (Holdings) Limited (2), HSBC Holdings plc (3), Hong Kong Electric Holdings Limited (4), Hang Seng Bank Ltd (5), Henderson Land Development Co. Limited (6), Sun Hung Kai Properties Limited (7), Swire Group (8), Cathay Pacific Airways Ltd (9), and Bank of China Hong Kong (Holdings) Ltd (10). Among these stocks, 3, 5, and 10 belong to Hang Seng Finance Sub-index (HSF), 1, 8, and 9 belong to Hang Seng Commerce and Industry Sub-index (HSC), 2, 6, and 7 belong to Hang Seng Properties Sub-index (HSP), and 4 belongs to Hang Seng Utilities Sub-index (HSU).\nHere one day can be also seen as one domain. We set the number of clients to be K\u2208{2, 4, 6, 8, 10} while randomly select nk=100 samples for each client. All other settings are following previous ones by default. The results are provided in Figure A8. According to the results, we can infer that our FedCDH method also outperformed the other baseline methods, across the different criteria. Similar to the analysis above, our method is tailored for heterogeneous data, in contrast to baseline methods like FedPC and NOTEARS-MLP-ADMM, which assume homogeneity. Additionally, our approach is capable of handling arbitrary functional causal models, setting it apart from baseline methods like FedPC that assume linearity. When compared to our method, FedDAG exhibited significantly poorer performance. This could be attributed to its reliance on continuous optimization, which may encounter challenges such as convergence and nonconvexity."
        }
    ],
    "year": 2024
}