{
    "abstractText": "Although transformers demonstrate impressive capabilities in a variety of tasks, the fairness issue remains a significant concern when deploying these models. Existing works to address fairness issues in transformers require sensitive labels (such as age, gender, etc.), which can raise privacy concerns or violate legal regulations. An alternative way is through fairness without demographics. However, existing works that improve Rawlsian Max-Min fairness may impose overly restrictive constraints. Other methods that use auxiliary networks could be parameter inefficient. In this paper, we present a new approach to debiasing transformers by leveraging their inherent structure. By reconsidering the roles of important components (queries, keys, and values) in the attention mechanism, we introduce a simple yet effective debiasing strategy from two perspectives: 1) Grounded in theoretical analysis, we normalize and apply absolute value operations to queries and keys to minimize the bias in attention weight allocation; 2) We reduce the bias within values through local alignment via contrastive learning. Throughout the entire process, our approach does not require any sensitive labels. Furthermore, to enhance memory efficiency in the training phase, we propose a strategy that debias only the last encoder to improve fairness in pre-trained models. We conduct experiments in computer vision and natural language processing tasks and show that our method is comparable and even outperforms the state-of-the-art method with substantially lower energy consumption.",
    "authors": [],
    "id": "SP:ea25e708cda935f99844c449f2b636d1803dda9b",
    "references": [
        {
            "authors": [
                "Ioana Baldini",
                "Dennis Wei",
                "Karthikeyan Natesan Ramamurthy",
                "Mikhail Yurochkin",
                "Moninder Singh"
            ],
            "title": "Your fairness may vary: Pretrained language model fairness in toxic text classification",
            "venue": "arXiv preprint arXiv:2108.01250,",
            "year": 2021
        },
        {
            "authors": [
                "Mislav Balunovi\u0107",
                "Anian Ruoss",
                "Martin Vechev"
            ],
            "title": "Fair normalizing flows",
            "venue": "arXiv preprint arXiv:2106.05937,",
            "year": 2021
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Junyi Chai",
                "Taeuk Jang",
                "Xiaoqian Wang"
            ],
            "title": "Fairness without demographics through knowledge distillation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Kevin Clark",
                "Urvashi Khandelwal",
                "Omer Levy",
                "Christopher D Manning"
            ],
            "title": "What does bert look at? an analysis of bert\u2019s attention",
            "year": 1906
        },
        {
            "authors": [
                "Elliot Creager",
                "J\u00f6rn-Henrik Jacobsen",
                "Richard Zemel"
            ],
            "title": "Environment inference for invariant learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Sixue Gong",
                "Xiaoming Liu",
                "Anil K Jain"
            ],
            "title": "Mitigating face recognition bias via group adaptive classifier",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Tatsunori Hashimoto",
                "Megha Srivastava",
                "Hongseok Namkoong",
                "Percy Liang"
            ],
            "title": "Fairness without demographics in repeated loss minimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Youngkyu Hong",
                "Eunho Yang"
            ],
            "title": "Unbiased classification through bias-contrastive and biasbalanced learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Prannay Khosla",
                "Piotr Teterwak",
                "Chen Wang",
                "Aaron Sarna",
                "Yonglong Tian",
                "Phillip Isola",
                "Aaron Maschinot",
                "Ce Liu",
                "Dilip Krishnan"
            ],
            "title": "Supervised contrastive learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Joon Sik Kim",
                "Jiahao Chen",
                "Ameet Talwalkar"
            ],
            "title": "Fact: A diagnostic for group fairness trade-offs",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Polina Kirichenko",
                "Pavel Izmailov",
                "Andrew Gordon Wilson"
            ],
            "title": "Last layer re-training is sufficient for robustness to spurious correlations",
            "venue": "arXiv preprint arXiv:2204.02937,",
            "year": 2022
        },
        {
            "authors": [
                "Preethi Lahoti",
                "Alex Beutel",
                "Jilin Chen",
                "Kang Lee",
                "Flavien Prost",
                "Nithum Thain",
                "Xuezhi Wang",
                "Ed Chi"
            ],
            "title": "Fairness without demographics through adversarially reweighted learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Evan Z Liu",
                "Behzad Haghgoo",
                "Annie S Chen",
                "Aditi Raghunathan",
                "Pang Wei Koh",
                "Shiori Sagawa",
                "Percy Liang",
                "Chelsea Finn"
            ],
            "title": "Just train twice: Improving group robustness without training group information",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ziwei Liu",
                "Ping Luo",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "In Proceedings of International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "Binny Mathew",
                "Punyajoy Saha",
                "Seid Muhie Yimam",
                "Chris Biemann",
                "Pawan Goyal",
                "Animesh Mukherjee"
            ],
            "title": "Hatexplain: A benchmark dataset for explainable hate speech detection",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Junhyun Nam",
                "Hyuntak Cha",
                "Sungsoo Ahn",
                "Jaeho Lee",
                "Jinwoo Shin"
            ],
            "title": "Learning from failure: De-biasing classifier from biased classifier",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Sungho Park",
                "Jewook Lee",
                "Pilhyeon Lee",
                "Sunhee Hwang",
                "Dohyung Kim",
                "Hyeran Byun"
            ],
            "title": "Fair contrastive learning for facial attribute classification",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Felix Petersen",
                "Debarghya Mukherjee",
                "Yuekai Sun",
                "Mikhail Yurochkin"
            ],
            "title": "Post-processing for individual fairness",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yao Qiang",
                "Chengyin Li",
                "Prashant Khanduri",
                "Dongxiao Zhu"
            ],
            "title": "Fairness-aware vision transformer via debiased self-attention",
            "venue": "arXiv preprint arXiv:2301.13803,",
            "year": 2023
        },
        {
            "authors": [
                "Shiori Sagawa",
                "Pang Wei Koh",
                "Tatsunori B Hashimoto",
                "Percy Liang"
            ],
            "title": "Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization",
            "year": 1911
        },
        {
            "authors": [
                "Sruthi Sudhakar",
                "Viraj Prabhu",
                "Arvindkumar Krishnakumar",
                "Judy Hoffman"
            ],
            "title": "Mitigating bias in visual transformers via targeted alignment",
            "venue": "arXiv preprint arXiv:2302.04358,",
            "year": 2023
        },
        {
            "authors": [
                "Yonglong Tian",
                "Dilip Krishnan",
                "Phillip Isola"
            ],
            "title": "Contrastive multiview coding",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Sid Wang",
                "John Nguyen",
                "Ke Li",
                "Carole-Jean Wu"
            ],
            "title": "Read: Recurrent adaptation of large transformers",
            "venue": "arXiv preprint arXiv:2305.15348,",
            "year": 2023
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel R Bowman"
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "arXiv preprint arXiv:1704.05426,",
            "year": 2017
        },
        {
            "authors": [
                "Jingjing Xu",
                "Wangchunshu Zhou",
                "Zhiyi Fu",
                "Hao Zhou",
                "Lei Li"
            ],
            "title": "A survey on green deep learning",
            "venue": "arXiv preprint arXiv:2111.05193,",
            "year": 2021
        },
        {
            "authors": [
                "Song Yang Zhang",
                "Zhifei",
                "Hairong Qi"
            ],
            "title": "Age progression/regression by conditional adversarial autoencoder",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Transformer-based models demonstrate immense power and achieve remarkable success in both computer vision and natural language processing fields. The transformer architecture is based on the concept of self-attention, which allows the model to weigh the importance of different tokens or patches of the input sequence when making predictions.\nFairness is a crucial factor to consider in machine learning models. For example, Gong et al. (2021) highlight the potential risk of applying biased face recognition systems in law enforcement. Although Transformer models exhibit outstanding performance, Sudhakar et al. (2023), Qiang et al. (2023), and Baldini et al. (2021) observe that transformers make biased predictions in vision and NLP domains. Addressing fairness issues in transformers is a significant but challenging task.\nExisting works attempt to address fairness issues in transformers in two ways: Targeted alignment for debiasing transformers (TADeT) and debiasing self-attention (DSA). Sudhakar et al. (2023) proposed to debias transformer by aligning \u201cQuery\u201d in different sensitive groups within the same task. DSA (Qiang et al., 2023) generates adversarial examples by attacking spurious features, and aligns attention weight between training sample and adversarial examples. However, these methods necessitate sensitive attributes (such as gender and race) to improve fair transformers. Collecting such information is not only costly but may also raise privacy concerns.\nSeveral studies (Hashimoto et al., 2018; Lahoti et al., 2020; Liu et al., 2021; Creager et al., 2021; Chai et al., 2022) propose to resolve general fairness issues without sensitive information. Hashimoto et al. (2018) propose to optimize the worst group to achieve equal utility in different sensitive groups. However, this method is likely to be affected by outliers, leading to a significant drop in performance. Lahoti et al. (2020) share the same idea to solve fairness issues but use a reweighting strategy. However, their method incorporates adversary training, which introduces instability. Moreover, improving the worst-performing group imposes a strong constraint on the objective func-\ntion. Liu et al. (2021) suggest a method where a network is trained twice, emphasizing samples misclassified by the initial model. However, this double-training approach lacks efficiency and runs counter to green deep learning initiatives (Xu et al., 2021). Creager et al. (2021) propose a method that is also a two-stage method, where the first step is to perform environment inference, followed by leveraging a robust optimization method to train the network. The limitation here is that their environment inference relies on the Bernoulli distribution, making it applicable only to binary sensitive labels. A recent study (Chai et al., 2022) tackles fairness without demographics via knowledge distillation. However, this approach involves a teacher network that is significantly larger than the student network for making predictions, thus is time-consuming and parameter-inefficient.\nAdditionally, large-scale pre-trained models also face fairness issues. Many existing methods require tuning all the parameters to achieve a balance between fairness and accuracy, but updating a large number of parameters can be prohibitively expensive (Petersen et al., 2021).\nIn summary, existing works of fair transformers require sensitive attributes. Existing methods for fairness without demographics either need a large auxiliary network or lay a strong constraint. For addressing fairness concerns in pre-trained models, tuning all parameters lacks efficiency and necessitates high-demand computational resources.\nTo resolve the above challenges, we propose a novel approach to tailoring the fairness considerations to the transformer encoder. Instead of debiasing the entire representation of the transformer encoder which places significant constraints on the representation, we propose to debias each component (Query, Key, and Value) in the attention formulation. This approach, derived from theoretical foundations, presents a simple yet effective strategy to address the fairness concern in transformers without the necessity of auxiliary networks or iterative training.\nWe debias from two perspectives within the attention mechanism.\n\u2022 Attention allocation: Drawing inspiration from theoretical analyses, we find that normalizing and applying the absolute value to q and k reduces discrepancies in attention weight between different sensitive groups.\n\u2022 Local alignment on value: We mitigate bias in v through local alignment. This is accomplished with a supervised contrastive learning approach, which encourages the core segments from different sensitive groups to be similar.\nWe conduct extensive experiments on real-world datasets, encompassing various classification tasks in computer vision and natural language processing (NLP) fields. Furthermore, we provide a GPU memory-efficient solution to address fairness in pre-trained models. We append a fairness-aware encoder on these models and only train that encoder. We demonstrate its efficacy in enhancing fairness within pre-trained models.\nWe summarize our contributions as follows: \u2022 To the best of our knowledge, we are the first to enhance fairness in transformers without\ndemographics. \u2022 Our algorithm achieves effective results in both computer vision and NLP domains. \u2022 Our method could plug in a pre-trained model to improve fairness without the need for\nre-training, resulting in improved memory efficiency."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 FAIRNESS-AWARE TRANSFORMERS",
            "text": "Recently, Sudhakar et al. (2023) and Qiang et al. (2023) aim to solve fairness issues in vision transformers. Sudhakar et al. (2023) proposed to use an adversarial training technique to hide the sensitive information in the classification token. Plus, they apply L2 Loss to penalize query vectors with large discrepancies among different sensitive groups with the same target label. However, debiasing on query may not be sufficient for resolving fairness issues in transformers, and the introduced adversary training may incur instability issues. Qiang et al. (2023) propose a two-stage method to mitigate bias in vision transformer (ViT). They first identify sensitive related patches and use adversarial attacks to perturb those patches. Next, they train a ViT using both the original training set and the attacked dataset to perform attention weight alignment. However, the hypothesis of this work is\nthat the sensitive attribute must be identifiable and has no overlap with target related patch. This laid a too strong constraint and may have a negative effect on the model performance. Moreover, both existing methods require sensitive attributes during the training phase."
        },
        {
            "heading": "2.2 FAIRNESS WITHOUT DEMOGRAPHICS",
            "text": "Distributionally robust optimization (DRO) (Hashimoto et al., 2018) achieves Rawlsian Max-Min fairness by optimizing the risk of the worst-case samples. Adversarially reweighted learning (ARL) (Lahoti et al., 2020) aims to optimize the performance of computationally identifiable samples by reweighting them, thus improving the worst-case performance. Knowledge distillation (Chai et al., 2022) leverages the soft labels generated by a teacher network and combines them with hard labels to debias a student model. Just Train Twice (JTT) (Liu et al., 2021) follows a reweighting paradigm, emphasizing error samples in the training set by giving them elevated weights. The training phase is split into two stages: an Empirical Risk Minimization (ERM) stage, succeeded by a reweighted ERM stage. Environment Inference for Invariant Learning (EIIL) (Creager et al., 2021) is also a two-stage method. The initial step employs a trained network to infer group labels, and the second step utilizes a robust optimization strategy to re-train the network. Learning from Failure (LfF) Nam et al. (2020) involves training two neural networks concurrently: one with amplified bias and another focusing on samples that contradict this bias. Through this process, the second network is conditioned to correct the errors made by the first."
        },
        {
            "heading": "2.3 CONTRASTIVE LOSS",
            "text": "The contrastive loss has been proven to be more robust and has a better generalization performance than cross-entropy loss. Supervised contrastive loss (Supcon) (Khosla et al., 2020) utilizes labels information to pull together normalized embeddings of the same class and push apart normalized embeddings from different classes. Differing from self-supervised representation learning (Chen et al., 2020; Tian et al., 2020), SupCon selects positive samples from the same class within a minibatch, leading to many positive and negative samples for each anchor, which enhances performance."
        },
        {
            "heading": "3 METHOD",
            "text": "Problem setup We denote a dataset D : (X,Y,A) \u2208 X \u00d7 Y \u00d7 A, with X as features, Y being target labels, and A being sensitive attributes. Our goal is to learn a classifier f\u03b8 : X 7\u2192 Y\u0302 , where Y\u0302 is a prediction labels that satisfies fairness criteria. For example, Demographic parity (DP) requires Y\u0302 \u22a5 A, Equal opportunity (EOp) requires Y\u0302 \u22a5 A|Y = 1 , and Equalized odds (EOd) requires Y\u0302 \u22a5 A|Y = y, y \u2208 {0, 1}. We focus on the setting where we do not have access to sensitive attributes in the training set. Background and notation The informative capability of the transformer model can be attributed to its attention mechanism. In a transformer, an input image (or a sequence in NLP) is divided into patches (tokens in NLP). These patches or tokens are first processed through an embedding layer. Subsequently, they undergo a transformation in the encoder through three linear modules, which map them to query, key, and value Vaswani et al. (2017); Dosovitskiy et al. (2020). In this work, we denote Q \u2208 RN\u00d7dk as query, K \u2208 RN\u00d7dk as key, and V \u2208 RN\u00d7dv as value, where N is the number of tokens. Additionally, we denote q \u2208 Rdk as a query vector corresponding to a patch, extracted from Q. Similarly, we denote k \u2208 Rdk as a key vector, v \u2208 Rdv as a value vector. We use the subscript \u201ccls\u201d as a special symbol to denote a patch or token used for a downstream task. This notation aligns with the conventions established in Devlin et al. (2018). Motivation A recent study Sudhakar et al. (2023) reveals disparities in average query across different sensitive groups when performing the same task, highlighting discriminatory behavior in the attention mechanism. In addition, Sudhakar et al. (2023); Qiang et al. (2023) underscore the importance of the attention mechanism in mitigating bias issues. They empirically validate that addressing fairness concerns within the attention mechanism is effective and yields good performance compared to other in-processing methods. Based on these findings, our work focuses on debiasing within the attention mechanism. To this end, we revisit the attention mechanism (Vaswani et al., 2017):\nAttention(Q,K,V) = softmax( QK\u22a4\u221a\ndk )V, (1)\nThe attention mechanism in equation 1 can be viewed as a two-stage operation: 1) Attention allocation performed by the inner product of Q and K, and 2) Weighted sum over V. We attribute the\ndiscrimination in a transformer-based model to 1) a misallocation of attention weight and 2) biased representation of V. Based on the observations, we present a two-stage approach that does not require sensitive attributes to mitigate discrimination in a transformer encoder."
        },
        {
            "heading": "3.1 FAIRNESS-AWARE ATTENTION WEIGHT RELOCATION",
            "text": "Drawing inspiration from the principle of minimizing the statistical distance between representations of various groups to enhance fairness (Balunovic\u0301 et al., 2021), we introduce a method that aligns the expected attention weights across diverse demographic groups without accessing sensitive attributes. To be specific, we denote w = q\n\u22a4k\u221a dk\nas attention weight of two tokens (patches). Given the fairness consideration, our optimization problem can be written as:\nmin |\u03b4| s.t. \u03b4 = E[w|a = 0]\u2212 E[w|a = 1] (2)\nwhich is to minimize the disparity in attention weights w between different sensitive groups. Directly solving equation 2 can be challenging due to the inaccessibility of the sensitive attribute. We present Theorem 1, which shows that a series operation can effectively reduce the attention allocation disparity among different sensitive groups.\nAssumption 1. Denote P (q|a = i) = N (\u00b5q,i,\u03a3q,i), P (k|a = i) = N (\u00b5k,i,\u03a3k,i), i \u2208 {0, 1}. For qj \u2208 q and kj \u2208 k, qj and kj are independent1, j \u2208 {1, ..., dk}. The base rate for P (a = 0) = \u03bb0 and P (a = 1) = \u03bb1. The distribution of query q and the key k of a transformer can be written as:\nq \u223c\u03bb0N (\u00b5q,0,\u03a3q,0) + \u03bb1N (\u00b5q,1,\u03a3q,1) (3) k \u223c\u03bb0N (\u00b5k,0,\u03a3k,0) + \u03bb1N (\u00b5k,1,\u03a3k,1) (4)\nFor a transformer, the expected difference in attention weight is\n|\u03b4| = |E[q \u22a4k\u221a dk |a = 0]\u2212 E[q \u22a4k\u221a dk |a = 1]| = 1\u221a dk | dk\u2211 j=1 (E[qjkj |a = 0]\u2212 E[qjkj |a = 1])|\n= 1\u221a dk | dk\u2211 j=1 (\u00b5q,0,j\u00b5k,0,j \u2212 \u00b5q,1,j\u00b5k,1,j)|\n(5)\n\u03b4 in a vanilla transformer is unbounded. To resolve this issue, we reference Theorem 1.\nTheorem 1. For a vector x = [x1, ..xd] \u2208 Rd, define n(x) = [x1\u2212E[x1]\u221aVar[x1] , ..., xd\u2212E[xd]\u221a Var[xd] ], and m(x) = [|x1|, ..., |xd|]. Given query q \u2208 Rdk and key k \u2208 Rdk , denote the debiased query and key as qde = m \u25e6 n(q) \u2208 Rdk , kde = m \u25e6 n(k) \u2208 Rdk . Under Assumption 1, the disparity in debiased attention weight is upper bounded by\n|\u03b4de| = \u2223\u2223\u2223\u2223\u2223E[qde \u22a4 kde\u221a dk |a = 0]\u2212 E[q de\u22a4kde\u221a dk |a = 1] \u2223\u2223\u2223\u2223\u2223 \u2264\u221adk[( \u221a 2 \u03c0\u03bb0 + \u221a \u03bb1 \u03bb0 )2 + (\u221a 2 \u03c0\u03bb1 + \u221a \u03bb0 \u03bb1 )2] For the attention mechanism in a transformer encoder, as shown in equation 5, the disparity of the attention weights across different demographic groups has the potential to diverge toward infinity. Theorem 1 indicates that upon normalization of vectors in Q and K, and taking their absolute values, the disparity in attention allocation could be bounded at a constant, wherein the constant incorporates the base rate of the training set and the embedding dimension. Note that when the sensitive attribution is balanced (e.g. The numbers of male and female samples are the same in CelebA), the disparity is minimal. We leave the details of the proof for Theorem 1 in the appendix.\nIn practice, we compute attention weight based on qde and kde, where qde = m \u25e6 n(q) \u2208 Rdk , kde = m \u25e6 n(k) \u2208 Rdk , as stated in Theorem 1. During the inference period, we use the mean and standard deviation estimated from the training set to normalize q and k for the test samples. To estimate these statistics, we utilize the running mean (e) and running standard deviation (s) as\n1The independence of qj and kj follows the same assumption presented in (Vaswani et al., 2017).\nsuggested by (Ioffe & Szegedy, 2015). The running mean is initialized with zero tensors, and the running standard deviation is initialized with one tensors. We update the running mean and running standard deviation using a momentum-based approach:\ne\u2190 (1\u2212 p)e+ penorm, s\u2190 (1\u2212 p)s+ psnorm, (6) where p is the momentum. we set p = 0.1 by following existing protocols (Ba et al., 2016; Ioffe & Szegedy, 2015). enorm and snorm are sample mean and sample standard deviation in a batch."
        },
        {
            "heading": "3.2 LOCAL ALIGNMENT ON VALUE",
            "text": "In transformer structures, the value of a token (a patch) can be considered as a local representation. Tokens (patches) with high attention weights encapsulate core objects, ensuring target-specific information encoding. For considerations of fairness, it is crucial that the encoding within these tokens (patches) remains consistent. Conversely, tokens (patches) with low attention weights, typically representative of background elements, enhance the richness of the final representation.\nIn representation learning, Chen et al. (2020) demonstrates that using a nonlinear projection head improves the representation quality. We follow this finding and define a small nonlinear head g : v 7\u2192 z to map value v to a latent representation z. To mitigate bias in the representation while preserving the discriminative power, our goal is to ensure similar representations within the same target label, while dissimilar representations from different target labels, regardless of the sensitive attribute. A natural choice for achieving this objective is through supervised contrastive loss (Khosla et al., 2020):\nLalignment = \u2211 i \u22121 |P (i)| \u2211 p\u2208P (i) log exp(zi \u00b7 zp/\u03c4)\u2211 k\u2208B(i) exp(zi \u00b7 zk/\u03c4) , (7)\nwhere i \u2208 I = {1, ...,M} is the index of anchor, M is the number of samples in a mini-batch, B(i) = I\\{i}, P (i) = {p \u2208 B(i) : yp = yi} is the set of indices of all positive samples in a mini-batch. \u03c4 is a temperature hyperparameter, we set \u03c4 = 0.07. We selected the Supervised Contrastive Loss in accordance with our objective of achieving consistent representations. Though the recently proposed Fair Supervised Contrastive Loss (FSCL) integrates fairness by specifying negative samples within an identical sensitive group (Park et al., 2022). FSCL might compromise the efficacy of contrastive learning and necessitates the inclusion of both target and sensitive labels.\nWe argue that directly employing supervised contrastive learning on the representation of the entire image or sentence does not necessarily contribute to fairness. Park et al. (2022) demonstrate that with the use of Supervised Contrastive Learning (SupCon) to pre-train a ResNet model, unfairness issue still exists. We attribute this phenomenon to the correlation between the target and sensitive labels. Since the network contrasts the entire representation, it can still leverage sensitive-related information. From this observation, we propose an attention-weight guided local value alignment technique to effectively tackle fairness concerns in the representation. The learning framework contains the following steps:\n\u2022 We define wi = qdecls\u00b7k de i\u221a\ndk as the attention weight of the i-th token (patch), W = {w1, ..., wl},\nwhere l is the number of patches (in vision) or the number of tokens (in NLP). We select vs\u2217 with top t attention weights: s\u2217 = argmaxs \u2211 i\u2208s wi, s.t. wi \u2208W, |s\u2217| = t\n\u2022 We concatenate all vs\u2217 and apply a nonlinear projection head g(\u00b7) to map from the value vector to representations: z = g(vs\u2217). We then apply supervised contrastive loss equation 7 on z to perform the local alignment of the value vector.\nWhy does local alignment help fairness: Our method debias on the attention mechanism using two steps. The first step of debiasing the attention weight (in Subsection 3.1) is crucial for the second step (in Subsection 3.2) when selecting patches with high attention weights. Utilizing these debiased weights from the first step, we select the top t patches based on target label relevance. Incorporating supervised contrastive learning encourages patches within a class to share similar representations, focusing on encoding information related to the target label, disregarding sensitive information. This approach reduces the sensitive information encoded in the representations, thereby enhancing fairness. It\u2019s important to note that without debiasing the attention weights in the first step, the technique of local alignment on value in the second step does not contribute to fairness. This is evidenced by our ablation study in Appendix F, where using local alignment alone barely impacts fairness. In contrast, combining debiased attention with local alignment achieves optimal results in terms of fairness."
        },
        {
            "heading": "3.3 LEARNING FRAMEWORK",
            "text": "We present our learning framework in Figure 1. Our learning framework contains two steps: First, we normalize each q and k and take their absolute values in the last encoder layer to compute attention weights. This approach ensures that attention allocation is less affected by sensitive attributes. Next, we select value v associated with top t attention weights to perform local value alignment. Throughout this process, we do not leverage sensitive attributes. We summarized our method in an algorithm, detailed in Appendix G."
        },
        {
            "heading": "3.4 GPU MEMORY EFFICIENT COMPUTATION",
            "text": "Last encoder training Even if the pre-trained model is powerful, fairness issues still exist. However, mitigating bias in large pre-trained models is challenging. Fine-tuning all the parameters is not only time-consuming but also heavily relies on high-performance computational devices.\nThe bias is spread in all the layers of encoders. However, as demonstrated by (Kirichenko et al., 2022), the objective of empirical risk minimization (ERM) training is sufficient for learning the core features on which a debiasing procedure can be deployed at the very last layer of the network.\nBased on this observation, we append our proposed encoder layer to the top of the large pre-trained model. The pre-trained model is capable of capturing the useful features within an image (Caron et al., 2021) or sentence (Clark et al., 2019). We employ the pre-trained model as a feature extractor and only train our fairness-aware one-layer encoder. This method does not require a GPU with large memory capacity, because parameters in pre-trained models are not updated. We show it has maintained utility and improved fairness in section 4.3."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We evaluate our method across various settings. We first explore our method in CV by conducting experiments on two real-world datasets. We next extend the application of our method to the NLP field. Additionally, we test the capability of our method to address fairness issues in pre-trained models with limited resources. Finally, we explore the trade-off between fairness and accuracy."
        },
        {
            "heading": "4.1 VISION EXPERIMENTS",
            "text": "Experimental setup We test all methods on two real-world datasets: CelebA (Liu et al., 2015), and UTK (Zhang & Qi, 2017). For CelebA dataset, we follow the fairness study\u2019s protocol and take y = Blond Hair, a = Male (Sagawa et al., 2019). UTK dataset is a widely used facial dataset in\nfairness research. In this study, we follow the task outlined in (Hong & Yang, 2021). The task is to predict y = gender, the sensitive attributes are a \u2208{White, not White}. We compare our method with ERM and other state-of-the-art methods without demographics: Distributionally robust optimization (DRO) (Hashimoto et al., 2018), Adversarially reweighted learning (ARL) (Lahoti et al., 2020), Fairness without demographics through knowledge distillation (KD) (Chai et al., 2022), Just train twice (JTT), and Learning from failure (LfF). Please refer to the Appendix for detailed implementation information. In our implementation (Ours), the backbone is the same as the ERM model. We normalize and apply the absolute value to Q and K in the last encoder layer. We select V with the 2 highest attention weights to perform the local alignment on value procedure. For vision tasks, all methods are trained from scratch. We evaluate each method w.r.t accuracy and fairness. For the fairness metric, we use demographic parity (DP), Equal Opportunity (EOp), and equalized odd (EOd), we leave details on the computation of fair metrics in the appendix.\nWe train all methods on a single NVIDIA RTX-3090 GPU with 24576 MiB memory. Each method is independently trained three times, and we report the mean and standard deviation of the results. We follow a protocol for efficiency assessment in machine learning and report the overall energy consumption, which can be computed by (Wang et al., 2023):\nE = \u222b H 0 u(t)p0dt (Wh), (8)\nwhere u(t) \u2208 [0, 1] (in percent) is the GPU utilization level, H represents the duration of a training process lasting H hours on a GPU. p0 is the power requirement for the GPU. For an NVIDIA RTX3090 GPU, p0 = 350 W. In practice, we sample u(t) per minute.\nVision experimental results Results from experiments on the UTK and CelebA datasets can be found in Tables 1 and 2. We mark the best results in dark blue, and second-best in light blue. In the UTK dataset, our method achieves enhanced fairness performance with a slight decrease in accuracy. In the CelebA dataset, we observe that LfF attains optimal fairness. However, this comes at the expense of a significant reduction in accuracy, where the base rate for y = 0 is 84.67%, and the accuracy of LfF is merely 87.97%. JTT demonstrates a balanced performance between fairness and utility. However, in a smaller-scale dataset, the performance of JTT approaches that of ERM training. We hypothesize that this is due to JTT\u2019s tendency to overfit the up-weighted training set. Our proposed method not only yields outcomes that are comparable to those of JTT, but also effectively mitigates bias existing in smaller datasets. Remarkably, our method requires significantly lower energy consumption compared to JTT. We provide a visualization of attention weight in the last encoder layer, please refer to the appendix."
        },
        {
            "heading": "4.2 NLP EXPERIMENTS",
            "text": "Expermental setup We evaluate our method within the domain of natural language processing (NLP) utilizing both the HateXplain (Mathew et al., 2021) and MultiNLI (Williams et al., 2017) datasets. For the HateXplain dataset, the target label, denoted by y, is equal to 0 for normal sentences and 1 for those that are considered hateful or offensive. In line with (Baldini et al., 2021) for choosing sensitive attributes, we assess whether a given corpus contains information that pertains to the attribute of gender (a). For the MultiNLI dataset, we follow (Liu et al., 2021), the task is\nto predict the entailment of the two sentences, y={entailed, neutral, contradictory}. The sensitive labels are a={no negation, negation}. Pre-trained models have significantly advanced the field of NLP. To ensure an equitable comparison, we leverage a pre-trained model and augment it with an additional encoder. Specifically, we select \u201cBERT Large\u201d (Devlin et al., 2018) and \u201cBERT Base\u201d (Devlin et al., 2018) to represent large-scale (340M parameters) and medium-scale (110M parameters) models, respectively. For NLP tasks, we exclude ARL, KD, and LfF from the comparison because these methods necessitate an auxiliary network, making it challenging to align with the specifications of a pre-trained model.\nWe apply AdamW to optimize the parameters. The learning rates of 10\u22125 are used for both \u201cBERT Large\u201dand \u201cBERT Base\u201d. All methods share the same batch size and optimizer configuration. In the baseline model, the add-on layer is the same as one of the encoder layers of the backbone model.\nNLP experimental results Table 3 presents results for the HateXplain dataset. It can be observed that our model not only achieves optimal fairness but also retains utility without compromising performance. Remarkably, when the backbone is the BERT base model, our method outperforms other methods in fairness and utility. Given the relatively small size of the HateXplain dataset, JTT tends to overfit, leading to suboptimal performance. This observation aligns with the results observed in the CV task.\nTable 4 presents the results for the MultiNLI dataset. Our method outperforms both the ERM and DRO approaches. However, it falls short in comparison to JTT for fairness. This can be attributed to the base rates of the two groups in MultiNLI, which are \u03bb0 = 92.9% and \u03bb1 = 7.1%. According to Theorem 1, the upper bound on the disparity in the expected attention allocation is proportional to 1\u03bb0 + 1 \u03bb1 . Consequently, a highly skewed base rate results in an elevated upper bound."
        },
        {
            "heading": "4.3 EXPERIMENT ON THE EFFICIENCY OF GPU MEMORY TRAINING",
            "text": "We evaluate the effectiveness of our method for mitigating bias in pre-trained models. We employ the CelebA and HateXplain datasets, with tasks as delineated in sections 4.1 and 4.2.\nFor the CelebA dataset, we append a fairness-aware encoder on a DINO pre-trained ViT-16 model (Caron et al., 2021). For the HateXplain dataset, we incorporate a fairness-aware encoder into pretrained models, specifically \u201cBERT Large\u201d and \u201cBERT Base\u201d. Additionally, for a fair comparison, we append a standard ViT/ BERT encoder to the ERM model. We evaluate two approaches: 1) fine-tuning all network parameters, denoted as \u201c-FT\u201d, and 2) training only the last encoder, denoted as \u201c-LE\u201d. We evaluate all methods on accuracy, fairness metrics, and GPU memory consumption. The results are shown in Table 5, 6.\nWe observed that the ERM model maintains comparable utility even when the parameters in the backbone model are frozen. This indicates that pre-trained backbones can effectively extract meaningful features. Additionally, only training the last encoder layer demands considerably less GPU memory during training. Furthermore, training only the last encoder layer resulted in enhanced fairness outcomes. This improvement is due to the fewer parameters updated, potentially mitigating overfitting issues. Notably, Ours-LE approach achieves superior fairness results while preserving a comparable utility."
        },
        {
            "heading": "4.4 FAIRNESS-ACCURACY TRADE-OFF",
            "text": "We explore the effect of hyperparameters. We tune the following hyperparameters: Ours: We change t, where t is the number of value vectors to perform the alignment. We choose t \u2208 {2, 4, 6}; KD: We change \u03bb in the loss function that governs how much soft-label is being used. We choose \u03bb \u2208 [0.3, 0.7, 1]; DRO: We change \u03b7 in the loss function that controls the range of the worstcase groups. We choose \u03b7 \u2208 {0.1, 0.3, 0.5}; ARL: We change l the number of encoders stacked for the adversary network. We choose l \u2208 {2, 4, 6}; JTT: We change the up-weight coefficient \u03bbup \u2208 {20, 50, 100}. Inspired by (Kim et al., 2020), we depict a Pareto frontier to analyze the trade-off between fairness and accuracy. We use CelebA dataset with a =male and y =Blond hair. All other hyperparameters remain unchanged with Table 2. All methods share the same seed. The results can be found in Fig 2.\nObservations indicate that JTT outperforms in fairness performance. However, its sensitivity to hyperparameters suggests reduced robustness. Both our method and KD show comparable utility, but our approach outperforms in terms of fairness. DRO demonstrates less sensitivity to hyperparameter variations. Our method is close to the theoretical tradeoff line between fairness and accuracy, achieving an optimal balance between fairness and accuracy."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we introduce a novel method to tackle fairness issues in transformers without demographics. To mitigate bias in attention weight allocation, we normalize q and k, then take their absolute values to compute the attention weight. We address bias in value vectors through local alignment. We conduct rigorous theoretical analysis to validate our method. We address fairness issues in pre-trained models and propose a method of using an additional fairness-aware encoder to mitigate these concerns. To validate our approach, we conduct experiments on various datasets in computer vision and NLP domains. Our results demonstrate the efficacy of our method in addressing fairness issues. Furthermore, we verify that our approach is useful for improving fairness in pre-trained models even with limited GPU memory resources."
        },
        {
            "heading": "A EMPIRICAL VALIDATION ASSUMPTION 1",
            "text": "We use ERM to train a ViT on the CelebA dataset. We save the model that achieves the highest validation accuracy. From the last encoder, we extract qCLS and K10, where \u201cCLS\u201d refers to a special token and K10 represents the key vector of the 10-th token. We randomly select one dimension from qCLS and K10 to generate two conditional distribution plots based on a."
        },
        {
            "heading": "B PROOF FOR THEROREM 1",
            "text": "We first consider a 1-D case and next generalize the derivation to dk dimensions.\nFor the 1-D derivation, now we focus only on q, then\nq \u223c \u03bb0N (\u00b50, \u03c320) + \u03bb1N (\u00b51, \u03c321) (9)\nwhere \u03bb0 + \u03bb1 = 1. Then the expectation of q is\n\u00b5 := E[q] = \u03bb0\u00b50 + \u03bb1\u00b51 (10)\nand the variance can be written as\n\u03c32 := Var[q] =E[q2]\u2212 (E[q])2, (11) =\u03bb0EN (\u00b50,\u03c320)[q 2] + \u03bb1EN (\u00b51,\u03c321)[q 2]\u2212 (\u03bb0\u00b50 + \u03bb1\u00b51)2 (12)\n=\u03bb0(\u00b5 2 0 + \u03c3 2 0) + \u03bb1(\u00b5 2 1 + \u03c3 2 1)\u2212 (\u03bb0\u00b50 + \u03bb1\u00b51)2 (13)\n=(\u03bb0\u03c3 2 0 + \u03bb1\u03c3 2 1) + \u03bb0\u03bb1(\u00b50 \u2212 \u00b51)2 (14)\n=(\u03bb0\u03c3 2 0 + \u03bb1\u03c3 2 1) + \u03bb0\u03bb1\u2206 2 WLOG, let \u2206 = \u00b50 \u2212 \u00b51 > 0 (15)\nAfter normalization with qnorm = q\u2212E[q]\u221a Var[q] , the conditioned variable qnorm|a = 0 and qnorm|a = 1 are still Gaussian, and the conditioned expectations can be written as\n\u00b50,norm := E[qnorm|a = 0] = \u00b50 \u2212 \u00b5\n\u03c3 = \u03bb1\u2206\u221a (\u03bb0\u03c320 + \u03bb1\u03c3 2 1) + \u03bb0\u03bb1\u2206 2 (16)\n\u00b51,norm := E[qnorm|a = 1] = \u00b51 \u2212 \u00b5 \u03c3 = \u2212 \u03bb0\u2206\u221a\n(\u03bb0\u03c320 + \u03bb1\u03c3 2 1) + \u03bb0\u03bb1\u2206\n2 (17)\nand the corresponding variances are\n\u03c3i,norm = \u03c3i \u03c3 , i = 0, 1 (18)\nAfter taking the absolute values, that is qde = |qnorm|, the conditioned expectations can be written as\nE[qde|a = i] =\u2212 \u222b 0 \u2212\u221e tp(t)dt+ \u222b \u221e 0 tp(t)dt, p(t) = N (\u00b5i,norm, \u03c32i,norm), i = 0, 1 (19)\n=\u03c3i,norm\n\u221a 2\n\u03c0 exp(\u2212 \u00b52i,norm 2\u03c32i,norm ) + \u00b5i,normerf( \u00b5i,norm\u221a 2\u03c3i,norm ) (20)\n(21)\nRecall that \u00b5i,norm = \u00b5i\u2212\u00b5\u03c3 and \u03c3i,norm = \u03c3i/\u03c3, we have\nE[qde|a = i] =\u03c3i \u03c3\n\u221a 2\n\u03c0 exp(\u2212\n(\u00b5i\u2212\u00b5\u03c3 ) 2\n2(\u03c3i\u03c3 ) 2 ) +\n\u00b5i \u2212 \u00b5 \u03c3 erf( \u00b5i\u2212\u00b5\n\u03c3\u221a 2\u03c3i\u03c3 ) (22)\n= 1\n\u03c3\n[ \u03c3i \u221a 2\n\u03c0 exp(\u2212(\u00b5i \u2212 \u00b5\u221a 2\u03c3i )2) + (\u00b5i \u2212 \u00b5)erf( \u00b5i \u2212 \u00b5\u221a 2\u03c3i\n) ]\n(23)\n= 1\n\u03c3\n[ \u03c3i \u221a 2\n\u03c0 exp(\u2212(\u03bb1\u2212i\u2206\u221a 2\u03c3i )2) + (\u03bb1\u2212i\u2206)erf( \u03bb1\u2212i\u2206\u221a 2\u03c3i\n) ]\n(24)\nwhich is because \u00b51\u2212\u00b5 = \u00b51\u2212\u03bb1\u00b51\u2212\u03bb0\u00b50 = \u03bb0(\u00b51\u2212\u00b50) = \u2212\u03bb0\u2206 and \u00b50\u2212\u00b5 = \u00b50\u2212\u03bb1\u00b51\u2212 \u03bb0\u00b50 = \u03bb1(\u00b50 \u2212 \u00b51) = \u03bb1\u2206. And (\u2212x)erf(\u2212x) = xerf(x).\nNote that \u03bb1\u2212i\u2206\u221a 2\u03c3i > 0, we have exp(\u2212(\u03bb1\u2212i\u2206\u221a 2\u03c3i )2) < 1 and erf(\u03bb1\u2212i\u2206\u221a 2\u03c3i ) < 1. As a result, the expectation can be bounded by\nE[qde|a = i] \u2264 1 \u03c3\n[ \u03c3i \u221a 2\n\u03c0 + \u03bb1\u2212i\u2206\n] (25)\n= \u03c3i \u221a 2 \u03c0\u221a\n(\u03bb0\u03c320 + \u03bb1\u03c3 2 1) + \u03bb0\u03bb1\u2206\n2 + \u03bb1\u2212i\u2206\u221a (\u03bb0\u03c320 + \u03bb1\u03c3 2 1) + \u03bb0\u03bb1\u2206 2 (26)\n\u2264 lim \u03c3i\u2192\u221e\n\u03c3i \u221a 2 \u03c0\u221a\n(\u03bb0\u03c320 + \u03bb1\u03c3 2 1) + \u03bb0\u03bb1\u2206\n2 + lim \u2206\u2192\u221e \u03bb1\u2212i\u2206\u221a (\u03bb0\u03c320 + \u03bb1\u03c3 2 1) + \u03bb0\u03bb1\u2206 2 (27)\n=\n\u221a 2\n\u03c0\u03bbi + \u221a \u03bb1\u2212i \u03bbi\n(28)\nSimilar results hold for k, too. Now the original statement can be re-written as\n|\u00b5q0,de\u00b5k0,de \u2212 \u00b5q1,de\u00b5k1,de| (29) = \u2223\u2223E[qde|a = 0]E[kde|a = 0]\u2212 E[qde|a = 1]E[kde|a = 1]\u2223\u2223 (30) \u2264E[qde|a = 0]E[kde|a = 0] + E[qde|a = 1]E[kde|a = 1] (31)\n\u2264 (\u221a 2\n\u03c0\u03bb0 + \u221a \u03bb1 \u03bb0 )2 + (\u221a 2 \u03c0\u03bb1 + \u221a \u03bb0 \u03bb1 )2 (32)\nNow consider the high-dimensional scenario, we have\n|\u03b4de| = \u2223\u2223\u2223\u2223\u2223E[qde \u22a4 kde\u221a dk |a = 0]\u2212 E[q de\u22a4kde\u221a dk |a = 1] \u2223\u2223\u2223\u2223\u2223 (33) =\n1\u221a dk \u2223\u2223E[ dk\u2211 j=1 (qdej k de j )|a = 0]\u2212 E[ dk\u2211 j=1 (qdej k de j )|a = 1] \u2223\u2223 (34) =\n1\u221a dk \u2223\u2223\u2223 dk\u2211 j=1 E[qdej kdej |a = 0]\u2212 dk\u2211 j=1 E[qdej kdej |a = 1] \u2223\u2223\u2223 (35)\n= 1\u221a dk \u2223\u2223\u2223 dk\u2211 j=1 (E[qdej kdej |a = 0]\u2212 E[qdej kdej |a = 1]) \u2223\u2223\u2223 (36)\n\u2264 1\u221a dk dk\u2211 j=1 \u2223\u2223\u2223E[qdej kdej |a = 0]\u2212 E[qdej kdej |a = 1]\u2223\u2223\u2223 (37) =\n1\u221a dk dk\u2211 j=1 \u2223\u2223\u2223E[qdej |a = 0]E[kdej |a = 0]\u2212 E[qdej |a = 1]E[kdej |a = 1]\u2223\u2223\u2223 (38) \u2264 \u221a dk [(\u221a 2 \u03c0\u03bb0 + \u221a \u03bb1 \u03bb0 )2 + (\u221a 2 \u03c0\u03bb1 + \u221a \u03bb0 \u03bb1 )2] .\u25a1 (39)\nC IMPLEMENTATION DETAILS\nWe summarize the implementation details as follows:\n\u2022 ERM: We take the transformer with 8 stack encoders as the baseline model. Each encoder has 8 attention heads. We implement the ERM model using the Huggingface library without pre-trained weights. We resize the image from CelebA and UTK datasets to 64 \u00d7 64, and divide it into 16 patches. For CelebA and UTK, we take the AdamW as the optimizer with a learning rate of 10\u22124, and no scheduler is applied for the fair comparison. For NLP tasks, we take the AdamW as the optimizer with a learning rate of 10\u22125. We share all methods with the same configuration as the ERM model.\n\u2022 Distributionally robust optimization (DRO): We adapt the backbone to the same as the ERM model. We tune the hyper-parameter \u03b7 at the validation set to achieve the highest accuracy. For CelebA and UTK experiments, we set \u03b7 = 0.15 and \u03b7 = 0.10 respectively. For HateXplain and MultiNLI, we set \u03b7 = 0.25.\n\u2022 Adversarially reweighted learning (ARL): We adapt the learner network to the same as the ERM model. For the adversary network, we apply a 6-layer stack encoder in the transformer. This is a smaller configuration compared to the learner network, as recommended by the authors.\n\u2022 Fairness without demographics through knowledge distillation (KD): We adapt the student model that is the same as the ERM model. For the teacher network, we follow the suggestion of the authors that use a larger network, hence we adopt the vision transformer with 12 stacked encoder layers. The student network is trained by the output of the teacher model with softmax activation.\n\u2022 Just train twice (JTT): We adapt the backbone network is the same as the ERM model. For CelebA, we follow the suggestion in the paper choose the number of epochs of training the identification model T = 1. During the second training, we choose the upsampling factor \u03bbup = 50. For UTK, we set T = 10, \u03bbup = 50. For HateXplain, we set T = 20, \u03bbup = 50. For MultiNLI, we set T = 2, \u03bbup = 20.\n\u2022 Learning from failure (LfF): We take the networks for a biased model and a debiased model are the same as the ERM model. We set the amplification coefficient in generalized cross entropy loss q = 0.7 as suggested in the original paper.\n\u2022 Ours: For vision tasks, our method employs a backbone identical to the ERM model, which comprises an 8-stacked encoder vision transformer. In the final encoder, we normalize and apply the absolute value to q and k for each head. From this encoder layer, we select the v vectors associated with the two highest attention weights and execute a local alignment. For NLP tasks, we extend a pre-trained model with an additional encoder layer, applying our method specifically to this added layer. Similarly, in the NLP tasks, we choose the v vectors with the two highest attention weights for alignment.\nFor all methods, we maintain a consistent batch size. Specifically, for vision tasks, the batch size is set to 256. For BERT Large, it\u2019s 32, and for BERT Base, we use a batch size of 64."
        },
        {
            "heading": "D EVALUATION METRICS",
            "text": "The group fairness metrics are measurements of the performance of different sensitive groups. We focus on three specific metrics: Demographic Parity, Equal Opportunity, and Equalized Odds. Demographic Parity (DP): DP focuses on the equality of the outcomes across different demographic groups, regardless of their abilities. It ensures that each group receives positive outcomes at the same rate. Equal Opportunity (EOp): EOp ensures that samples who should receive a positive outcome have an equal chance of being correctly identified, regardless of their group. Equalized Odds (EOd): EOd requires both that samples that should receive a positive outcome have an equal chance of being correctly identified (like EOp), and also that samples that should receive a negative outcome have an equal chance of being correctly identified, across all sensitive groups. The computations for the fairness metrics are as follows:\nDP = |PPi \u2212 PPj |, EOp = |TPRi \u2212 TPRj |,\nEOd = 1\n2 (|TPRi \u2212 TPRj |+ |FPRi \u2212 FPRj |), i, j \u2208 A\n(40)\nwhere PP, TPR, and FPR are the positive prediction rate, the true positive rate, and the false positive rate."
        },
        {
            "heading": "E ATTENTION WEIGHT VISUALIZATION",
            "text": "We present a visualization of the average attention weight in the last encoder layer for both the ERM and our proposed models in Fig 4. We opted for models that demonstrated the highest validation accuracy. The ERM model achieved an accuracy of 94.27%, our model achieved an accuracy of 94.08%. We observe inconsistencies in attention allocation using the ERM training objective function. Despite its high accuracy, the ERM model predominantly focuses on facial features, failing to distribute attention adequately. In contrast, our model provides a more uniform attention allocation, effectively reducing the focus on facial features or other irrelevant features."
        },
        {
            "heading": "F ABLATION STUDY",
            "text": "We perform an ablation experiment from three perspectives to understand their importance. w/o local alignment: We train our model without incorporating the local value alignment technique. The training process is solely guided by the cross-entropy loss. w/o debias attention: While training our model, we exclude the normalization and exclude the absolute values in vectors q and k when calculating the attention weight. w/o absolute value: We train our model using normalized vectors q and k to compute the attention weight, but we exclude applying the absolute value on these vectors. We use CelebA dataset with a =male and y =Blond hair. All the methods share the same seed with the Baseline model.\nTable 7 demonstrates the efficacy of our design modules. Notably, without the debias attention, the outcomes are close to those of the Baseline model. This highlights the significance of debias attention as an essential component for debiasing attention. Concurrently, the local value alignment technique further improves fairness, with a large impact on EOd. When the network integrates both techniques, it achieves optimal fairness with a slight drop in accuracy."
        },
        {
            "heading": "G ALGORITHM",
            "text": "Algorithm 1 Debias Attention mechanism Input: Input tokens X \u2208 RN\u00d7dmodel , label y. Weight matrices WQ \u2208 Rdmodel\u00d7dk ,WK \u2208 Rdmodel\u00d7dk ,WV \u2208 Rdmodel\u00d7dv for Query, Key, Value. Output: The loss of local alignment on value\n1: Project X to Query, Key, and Value matrices: Q\u2190 XWQ, K\u2190 XWK, V\u2190 XWV 2: For each q \u2208 Q, k \u2208 K:\nqde = m \u25e6 n(q), kde = m \u25e6 n(k) m and n are defined in Theorem 1.\n3: Compute attention weights wi:\nwi = qdecls \u00b7 kdei\u221a\ndk\n4: Let W = {w1, w2, ..., wl}where l is the number of tokens. Select value vectors corresponding to the top t attention weights:\ns\u2217 = argmax s \u2211 i\u2208s wi s.t. wi \u2208W, |s| = t\n5: Extract and concatenate the top t value vectors:\nv\u2190 concat(Vs\u2217)\n6: Mapping to the contrastive space: z = g(v)\n7:\nLalignment = \u2211 i \u22121 |P (i)| \u2211 p\u2208P (i) log exp(zi \u00b7 zp/\u03c4)\u2211 k\u2208B(i) exp(zi \u00b7 zk/\u03c4) ,\nwhere i \u2208 I = {1, ..., n}, B(i) = I\\{i}, P (i) = {p \u2208 B(i) : yp = yi}, \u03c4 = 0.07. return Lalignment"
        }
    ],
    "year": 2023
}