{
    "abstractText": "Contrastive Learning (CL) has attracted enormous attention due to its remarkable capability in unsupervised representation learning. However, recent works have revealed the vulnerability of CL to backdoor attacks: the feature extractor could be misled to embed backdoored data close to an attack target class, thus fooling the downstream predictor to misclassify it as the target. Existing attacks usually adopt a fixed trigger pattern and poison the training set with trigger-injected data, hoping for the feature extractor to learn the association between trigger and target class. However, we find that such fixed trigger design fails to effectively associate trigger-injected data with target class in the embedding space due to special CL mechanisms, leading to a limited attack success rate (ASR). This phenomenon motivates us to find a better backdoor trigger design tailored for CL framework. In this paper, we propose a bi-level optimization approach to achieve this goal, where the inner optimization simulates the CL dynamics of a surrogate victim, and the outer optimization enforces the backdoor trigger to stay close to the target throughout the surrogate CL procedure. Extensive experiments show that our attack can achieve a higher attack success rate (e.g., 99% ASR on ImageNet-100) with a very low poisoning rate (1%). Besides, our attack can effectively evade existing state-of-the-art defenses. Code is available at: https://github.com/SWY666/SSL-backdoor-BLTO.",
    "authors": [
        {
            "affiliations": [],
            "name": "BI-LEVEL TRIG"
        },
        {
            "affiliations": [],
            "name": "GER OPTIMIZATION"
        },
        {
            "affiliations": [],
            "name": "Weiyu Sun"
        },
        {
            "affiliations": [],
            "name": "Xinyu Zhang"
        },
        {
            "affiliations": [],
            "name": "Hao Lu"
        },
        {
            "affiliations": [],
            "name": "Yingcong Chen"
        },
        {
            "affiliations": [],
            "name": "Ting Wang"
        },
        {
            "affiliations": [],
            "name": "Jinghui Chen"
        },
        {
            "affiliations": [],
            "name": "Lin Lu"
        }
    ],
    "id": "SP:4223346e9af7636999de74d4334d636fbaf8b8de",
    "references": [
        {
            "authors": [
                "REFERENCES Randall Balestriero",
                "Mark Ibrahim",
                "Vlad Sobal",
                "Ari Morcos",
                "Shashank Shekhar",
                "Tom Goldstein",
                "Florian Bordes",
                "Adrien Bardes",
                "Gregoire Mialon",
                "Yuandong Tian"
            ],
            "title": "A cookbook of self-supervised learning",
            "venue": "arXiv preprint arXiv:2304.12210,",
            "year": 2023
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Andreas Terzis"
            ],
            "title": "Poisoning and backdooring contrastive learning, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Kaiming He"
            ],
            "title": "Exploring simple siamese representation learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Xinyun Chen",
                "Chang Liu",
                "Bo Li",
                "Kimberly Lu",
                "Dawn Song"
            ],
            "title": "Targeted backdoor attacks on deep learning systems using data poisoning",
            "venue": "arXiv preprint arXiv:1712.05526,",
            "year": 2017
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Linus Ericsson",
                "Henry Gouk",
                "Chen Change Loy",
                "Timothy M. Hospedales"
            ],
            "title": "Self-supervised representation learning: Introduction, advances, and challenges",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2022
        },
        {
            "authors": [
                "Shiwei Feng",
                "Guanhong Tao",
                "Siyuan Cheng",
                "Guangyu Shen",
                "Xiangzhe Xu",
                "Yingqi Liu",
                "Kaiyuan Zhang",
                "Shiqing Ma",
                "Xiangyu Zhang"
            ],
            "title": "Detecting backdoors in pre-trained encoders",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Guo",
                "Mohammad Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tianyu Gu",
                "Brendan Dolan-Gavitt",
                "Siddharth Garg"
            ],
            "title": "Badnets: Identifying vulnerabilities in the machine learning model supply chain",
            "venue": "arXiv preprint arXiv:1708.06733,",
            "year": 2017
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Forrest N Iandola",
                "Song Han",
                "Matthew W Moskewicz",
                "Khalid Ashraf",
                "William J Dally",
                "Kurt Keutzer"
            ],
            "title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and\u00a1 0.5 mb model size",
            "venue": "arXiv preprint arXiv:1602.07360,",
            "year": 2016
        },
        {
            "authors": [
                "Jinyuan Jia",
                "Yupei Liu",
                "Neil Zhenqiang Gong"
            ],
            "title": "Badencoder: Backdoor attacks to pre-trained encoders in self-supervised learning",
            "venue": "IEEE Symposium on Security and Privacy (SP),",
            "year": 2022
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Changjiang Li",
                "Ren Pang",
                "Zhaohan Xi",
                "Tianyu Du",
                "Shouling Ji",
                "Yuan Yao",
                "Ting Wang"
            ],
            "title": "An embarrassingly simple backdoor attack on self-supervised learning",
            "venue": "arXiv preprint arXiv:2210.07346,",
            "year": 2023
        },
        {
            "authors": [
                "Yiming Li",
                "Yong Jiang",
                "Zhifeng Li",
                "Shu-Tao Xia"
            ],
            "title": "Backdoor learning: A survey",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yuezun Li",
                "Yiming Li",
                "Baoyuan Wu",
                "Longkang Li",
                "Ran He",
                "Siwei Lyu"
            ],
            "title": "Invisible backdoor attack with sample-specific triggers",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Hongbin Liu",
                "Jinyuan Jia",
                "Neil Zhenqiang Gong"
            ],
            "title": "PoisonedEncoder: Poisoning the unlabeled pretraining data in contrastive learning",
            "venue": "In 31st USENIX Security Symposium (USENIX Security",
            "year": 2022
        },
        {
            "authors": [
                "Ningning Ma",
                "Xiangyu Zhang",
                "Hai-Tao Zheng",
                "Jian Sun"
            ],
            "title": "ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design",
            "year": 2018
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "Minzhou Pan",
                "Yi Zeng",
                "Lingjuan Lyu",
                "Xue Lin",
                "Ruoxi Jia"
            ],
            "title": "Asset: Robust backdoor data detection across a multiplicity of deep learning paradigms",
            "venue": "In Proceedings of the 32nd USENIX Conference on Security Symposium,",
            "year": 2023
        },
        {
            "authors": [
                "Sinno Jialin Pan",
                "Qiang Yang"
            ],
            "title": "A survey on transfer learning",
            "venue": "IEEE Transactions on knowledge and data engineering,",
            "year": 2009
        },
        {
            "authors": [
                "Aniruddha Saha",
                "Akshayvarun Subramanya",
                "Hamed Pirsiavash"
            ],
            "title": "Hidden trigger backdoor",
            "venue": "attacks. CoRR,",
            "year": 2019
        },
        {
            "authors": [
                "Aniruddha Saha",
                "Ajinkya Tejankar",
                "Soroush Abbasi Koohpayegani",
                "Hamed Pirsiavash"
            ],
            "title": "Backdoor attacks on self-supervised learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Mark Sandler",
                "Andrew Howard",
                "Menglong Zhu",
                "Andrey Zhmoginov",
                "Liang-Chieh Chen"
            ],
            "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern",
            "year": 2018
        },
        {
            "authors": [
                "Hossein Souri",
                "Liam Fowl",
                "Rama Chellappa",
                "Micah Goldblum",
                "Tom Goldstein"
            ],
            "title": "Sleeper agent: Scalable hidden trigger backdoors for neural networks trained from scratch",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Turner",
                "Dimitris Tsipras",
                "Aleksander Madry"
            ],
            "title": "Label-consistent backdoor attacks. Cornell University - arXiv,Cornell University - arXiv, Dec 2019",
            "year": 2019
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton"
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research,",
            "year": 2008
        },
        {
            "authors": [
                "Bolun Wang",
                "Yuanshun Yao",
                "Shawn Shan",
                "Huiying Li",
                "Bimal Viswanath",
                "Haitao Zheng",
                "Ben Y Zhao"
            ],
            "title": "Neural cleanse: Identifying and mitigating backdoor attacks in neural networks",
            "venue": "IEEE Symposium on Security and Privacy (SP),",
            "year": 2019
        },
        {
            "authors": [
                "Feng Wang",
                "Huaping Liu"
            ],
            "title": "Understanding the behaviour of contrastive loss",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "Tongzhou Wang",
                "Phillip Isola"
            ],
            "title": "Understanding contrastive representation learning through alignment and uniformity on the hypersphere",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Zhirong Wu",
                "Yuanjun Xiong",
                "Stella X. Yu",
                "Dahua Lin"
            ],
            "title": "Unsupervised feature learning via non-parametric instance discrimination",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, Jun 2018. doi: 10.1109/cvpr.2018.00393. URL http://dx.doi.org/10.1109/cvpr.2018.00393",
            "year": 2018
        },
        {
            "authors": [
                "Jing Xu",
                "Yu Pan",
                "Xinglin Pan",
                "Steven Hoi",
                "Zhang Yi",
                "Zenglin Xu"
            ],
            "title": "Regnet: Self-regulated network for image classification",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Chang Yue",
                "Peizhuo Lv",
                "Ruigang Liang",
                "Kai Chen"
            ],
            "title": "Invisible backdoor attacks using data poisoning in the frequency domain",
            "venue": "arXiv preprint arXiv:2207.04209,",
            "year": 2022
        },
        {
            "authors": [
                "Yi Zeng",
                "Si Chen",
                "Won Park",
                "Zhuoqing Mao",
                "Ming Jin",
                "Ruoxi Jia"
            ],
            "title": "Adversarial unlearning of backdoors via implicit hypergradient",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Yi Zeng",
                "Minzhou Pan",
                "Hoang Anh Just",
                "Lingjuan Lyu",
                "Meikang Qiu",
                "Ruoxi Jia"
            ],
            "title": "Narcissus: A practical clean-label backdoor attack with limited information, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Jinghuai Zhang",
                "Hongbin Liu",
                "Jinyuan Jia",
                "Neil Zhenqiang Gong"
            ],
            "title": "Corruptencoder: Data poisoning based backdoor attacks to contrastive learning",
            "venue": "arXiv preprint arXiv:2211.08229,",
            "year": 2022
        },
        {
            "authors": [
                "Mengxin Zheng",
                "Jiaqi Xue",
                "Xun Chen",
                "Lei Jiang",
                "Qian Lou"
            ],
            "title": "Ssl-cleanse: Trojan detection and mitigation in self-supervised learning",
            "venue": "CoRR, abs/2303.09079,",
            "year": 2023
        },
        {
            "authors": [
                "Runkai Zheng",
                "Rongjun Tang",
                "Jianze Li",
                "Li Liu"
            ],
            "title": "Data-free backdoor removal based on channel lipschitzness",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Zihao Zhu",
                "Mingda Zhang",
                "Shaokui Wei",
                "Li Shen",
                "Yanbo Fan",
                "Baoyuan Wu"
            ],
            "title": "Boosting backdoor attack with a learnable poisoning sample selection",
            "year": 2023
        },
        {
            "authors": [
                "non-backdoor",
                "CTRL Li"
            ],
            "title": "The experimental results are show in Table 14, which indicates that our BLTO attack can also outperform other attacks on MoCO. Victim\u2019s downstream task. In practice, the victim may use different downstream dataset to finetune their resulting model with the pre-trained one. Though parts of these situation",
            "year": 2022
        },
        {
            "authors": [
                "re-implemented CTRL Li"
            ],
            "title": "Table 1 is a bit different from the original paper",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In recent years, contrastive learning (CL) (Chen et al., 2020; He et al., 2020; Grill et al., 2020; Chen & He, 2021) emerges as an important self-supervised learning technique (Balestriero et al., 2023), even outperforming supervised learning baselines in certain scenarios (e.g., transfer learning (Pan & Yang, 2009)). The superior performance of CL makes it a popular choice in modern machine learning designs (Ericsson et al., 2022), where a feature extractor is pre-trained on large-scale unlabeled data, based on which different predictors can be customized to serve a variety of downstream tasks.\nDespite the popularity of CL in various applications, Saha et al. (2022) have revealed the backdoor threats in CL: one can backdoor the feature extractor by poisoning the unlabeled training data, such that the derived downstream predictor would misclassify any trigger-injected sample into the target class. To backdoor the feature extractor during CL, existing attackers usually add specific backdoor triggers to a small amount of target-class samples. Then, during the training procedure, the feature extractor will be misled to encode the trigger and the target close in the embedding space when maximizing the similarity between augmented views1 (Wang & Isola, 2020). Consequently, due to their similar representations, the downstream predictor will confuse the trigger-injected data with the target class. Following this paradigm, existing attacks (Saha et al., 2022; Zhang et al., 2022; Li et al., 2023) have explored various types of triggers to backdoor CL, e.g., Patch (Gu et al., 2017) and frequency-domain perturbation (Yue et al., 2022).\n1After data augmentation, the situation may arise where two augmented views contain the trigger pattern and the physical features of the target class respectively.\nThough seems intuitive, in practice, these attacks with non-optimized trigger designs usually cannot effectively fool the feature extractor to associate trigger pattern with the target class, leading to a limited attack success rate (ASR). To verify this, we conduct a preliminary experiment on CIFAR-10: we poison the training data via CTRL (Li et al., 2023), SSL backdoor (Saha et al., 2022) or our proposed attack; then we perform SimCLR (Chen et al., 2020) on the poisoned data; for each CL epoch, we monitor the normalized similarity between trigger-injected data and target-class data2, as well as ASR on the downstream task. Experiment details can be found in Appendix A.1. As shown in Figure 1 Left, throughout the training procedure of the victim SimCLR,\nnone of existing attacks can achieve a high normalized similarity. Such a phenomenon suggests that they cannot effectively fool the feature extractor to tightly cluster the triggered samples with the target class in the embedding space, which corresponds to their low ASR in the downstream task, as shown in Figure 1 Right.\nThe failure of these existing attacks prompts us to find a more dedicated backdoor trigger design that fits better to the CL paradigm. In this paper, we propose a Bi-Level Trigger Optimization (BLTO) method to keep the triggered data staying close to the target-class data throughout the CL procedure. Specifically, in our bi-level optimization objective, the inner optimization aims to simulate the victim contrastive learning procedure when the feature extractor is trained on a backdoored dataset, and the outer optimization updates a backdoor generator which perturbs the input to approach the target cluster in the embedding space. By alternatively performing the inner and outer update, we guide our attack to identify a resilient trigger design that could keep a high similarity between the triggered samples and the target-class samples during the CL procedure. In summary, our key contributions are highlighted as follows:\n1. We identify the weakness of existing backdoor attacks on CL, that existing trigger designs fails to mislead feature extractor to embed trigger-injected data close to target-class data, limiting their attack success rate;\n2. We propose a novel bi-level trigger optimization approach to identify a resilient trigger design that could maintain a high similarity between the triggered and target-class data in the embedding space of CL;\n3. Extensive experiments are conducted under various practical scenarios to verify the effectiveness and tranferability of our proposed backdoor attack. The results suggest that our backdoor attack can achieve a high attack success rate with a low poisoning rate (1%). Furthermore, we provide thorough analyses to justify our attack\u2019s ability to survive special CL mechanisms, e.g., data augmentation and uniformity."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Backdoor Attack in Supervised Learning (SL) The backdoor attack (Gu et al., 2017) aims to utilize the trigger embedded in the dataset to control the behavior of the victim model, such as changing the model prediction. Backdoor attacks have well developed in the SL scenario (Li et al., 2022; Souri et al., 2022; Zhu et al., 2023). Most SL attacks (Gu et al., 2017; Chen et al., 2017) work via label polluting (i.e., changing the backdoor data\u2019s label as the target label in the victim\u2019s dataset), which induces the victim\u2019s model to associate the trigger with the target class. Such a simple paradigm gives birth to multiple efficient attacks, such as the invisible backdoor attack (Li et al., 2021). Other SL attacks combine the trigger with the target\u2019s semantic information (known as label consistent attack (Turner et al., 2019; Saha et al., 2019)) to avoid the label polluting, such as Narcissus (Zeng et al., 2022), which renders the backdoor dataset more natural.\n2Their normalized similarity is calculated as the cosine similarity between the centroid of the trigger-injected data and the target-class data in the embedding space, normalized by the averaged cosine similarity among all classes.\nBackdoor Attack in Contrastive Learning (CL) Vision-language CL is shown vulnerable to backdoor attacks (Carlini & Terzis, 2022) which works by inserting trigger in image and adjusting text to a downstream target. However, without the participation of supervision from training labels or other modalities (i.e., text), attacking CL is generally more difficult (Wu et al., 2018) than in SL. Nevertheless, (Saha et al., 2022) reveals the potential backdoor threats in CL: by embedding triggered data close to the target class, the victim\u2019s feature extractor can be backdoored to further mislead predictors in downstream tasks. One line of CL backdoor attacks directly tamper the victim\u2019s feature extractor during CL procedure (e.g., BadEncoder (Jia et al., 2022; Yue et al., 2022)). While achieving good attack performance, these attacks need to access/control how the victim performs CL, thus is less practical. On the contrary, we consider a different and more realistic attack setting, where we are only allowed to poison a small set of CL training data and has no control on how the victim performs CL. Existing works (Saha et al., 2022; Li et al., 2023) in this line design their attacks to associate the trigger with the attack target. Another line of work (Liu et al., 2022; Zhang et al., 2022) introduces additional image synthesis to achieve better attack performance. PoisonedEncoder (Liu et al., 2022) creates poisoning input by combining a target input and a reference input to associate the trigger with the attack, but it was designed for data poisoning attack in CL. CorruptEncoder (Zhang et al., 2022) combines reference objects and background images together with the trigger to exploit the random cropping mechanism in CL for backdoor attack.\nBackdoor Defense Most backdoor defense solutions are post-processing to either detect abnormality (e.g., Neural Cleanse (Wang et al., 2019), SSL-Cleanse (Zheng et al., 2023), DECREE (Feng et al., 2023) and ASSET (Pan et al., 2023)), or mitigate backdoor threat (e.g., I-BAU (Zeng et al., 2021), CLP (Zheng et al., 2022)) in the trained model. However, as reported in prior works (Jia et al., 2022; Zheng et al., 2023), these backdoor defense solutions originally designed for SL fail to deal with CL backdoor threats. To this end, several CL-targeted defenses have emerged, such as knowledge distillation (Saha et al., 2022) and trigger inversion based on embedding space. These solutions can work directly on the trained feature extractor without the involvement of specific downstream tasks, thus suits better for the CL framework."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "In this paper, we focus on poisoning unlabeled training data, such that after performing contrastive learning (CL) on this data, the resulting feature extractor (and downstream predictor) are backdoored to misclassify triggered data into a target class. We now briefly introduce CL preliminaries and our thread model.\nContrastive Learning (CL) Classic CL generally adopts a siamese network structure, such as SimCLR (Chen et al., 2020), BYOL (Grill et al., 2020), and SimSiam (Chen & He, 2021). Though detailed designs and optimization objectives could differ across different CL frameworks, their underlying working mechanisms are similar (Wang & Isola, 2020; Chen & He, 2021): maximizing the similarity between augmented views of each instance, subject to certain conditions for avoiding collapse. According to Wang & Isola (2020), the mechanism to maximize the similarity corresponds to the alignment property of CL, which aims to align views of the same instance; the collapse avoiding mechanism presents the uniformity property, which can spread views of different instances uniformly in the embedding space. Take SimSiam (Chen & He, 2021) as an example. Formally, given a batch of inputs x \u2208 X , SimSiam first applies data augmentations to produce two augmented views: t1(x) and t2(x), where t1, t2 \u2208 T and T is a set of augmentation operations. Then, SimSiam optimizes a feature extractor f\u03b8(\u00b7) and a projector head p\u03d5(\u00b7) by maximizing the similarity between t1(x) and t2(x) in the embedding space, as the following objective shown:\nmin \u03b8,\u03d5 LCL(x;\u03b8) = \u2212Et1,t2\u2208T\n[ S(f\u03b8(t1(x)), z2) + S(f\u03b8(t2(x)), z1) ] , (1)\nwhere S(\u00b7, \u00b7) is a similarity measurement, and zi (i \u2208 {1, 2}) is the projected output defined as zi = stopgrad(p\u03d5(f\u03b8(ti(x))) where gradient is blocked. By minimizing Eq. (1), the similarity between t1(x) and t2(x) is promoted (i.e., the alignment mechanism); by blocking the gradient via stopgrad(\u00b7), different instances are better separated in the embedding space (i.e., the uniformity mechanism) (Wang & Isola, 2020).\nIssues in Existing CL Backdoor via Data Poisoning In this type of attack (Saha et al., 2022; Li et al., 2023), the attacker collects a small set of target-class data (i.e., reference data), and directly injects a specific trigger to poison the data. When performing CL on this data to align views of the same instance (i.e., target-class data with trigger injected), the feature extractor could be misled to associate the trigger with the target class. This attack effect can be achieved by the alignment mechanism in the victim CL (i.e., solving Eq. (1)). However, this attack paradigm overlooks the influence of the uniformity mechanism in the victim CL: uniformity encourages dissimilarity among distinct instances (i.e., target-class data with trigger injected), which could impair the association between trigger and target class, thus constraining backdoor performance. This issue can be verified by our observation in Figure 1 and experiment in Figure 4. Beyond the uniformity issue, the data augmentations adopted in the victim CL could damage trigger patterns and impede the backdoor injection, as shown in (Li et al., 2023; Yue et al., 2022) and our experiment in Table 6. In summary, directly applying the existing trigger design from SL to CL scenarios (e.g., patch) ignores these special mechanisms in CL (e.g., uniformity and data augmentation), resulting in limited attack effectiveness. This paper aims to find a tailored trigger design that can alleviate these issues and accommodate to CL mechanisms."
        },
        {
            "heading": "4 METHODOLOGY",
            "text": "Threat Model We first give an overview of the threat model, considering its goal and capability. In a standard contrastive learning (CL) setting, a feature extractor is first pretrained on an unlabeled data to provide feature embeddings, then a predictor is concatenated and trained for a specific downstream task. The attacker\u2019s goal is to poison the victim\u2019s training dataset with a small amount of trigger-injected data, such that the resulting feature extractor presents the following behaviors: 1) when the input is triggered, the downstream predictor derived from the backdoored feature extractor will output a target class defined by the attacker; 2) when the input is intact (i.e., without trigger), the feature extractor and the derived predictor will behave like a normal clean model. The attacker capability is limited to access and control a small portion of the victim\u2019s training data for CL. Besides, for reference, the attacker can collect some unlabeled data from public source that belong to the target class, and we call it reference data. We consider a practical setting, where the attacker cannot directly access and tamper the victim\u2019s CL procedure (i.e., no information about the victim\u2019s CL strategies, encoder architectures, (hyper-)parameters, the training data distribution, etc).\nDesired Trigger Design As discussed in Section 3, existing poisoning attacks on CL adopting non-optimized triggers (e.g., fixed patch (Saha et al., 2022)) ignores the influence of special designs in CL (e.g., data augmentation and uniformity mechanism), thus can not well adapt to victim\u2019s CL behaviors, causing unsatisfactory backdoor performance. This observation motivates us to answer this question: what makes an effective trigger design in backdooring CL? We argue that a successful backdoor trigger should be able to survive these CL mechanisms, that is: after performing the CL procedure (with data augmentation and uniformity promoting), the resulting feature extractor will still regard the backdoored data and target-class data to be similar. With this goal in mind, we propose a Bi-Level Trigger Optimization (BLTO) method for tailored poisoning attack on contrastive learning.\nOur Proposed Bi-Level Trigger Optimization (BLTO) Consider that we cannot access the victim\u2019s CL information beforehand, to make the trigger survive possible CL mechanisms, our motivation is to simulate the victim\u2019s CL behavior via a surrogate CL pipeline, and the trigger is optimized to maximize the similarity between backdoored data and target-class data throughout the surrogate CL procedure. This can be naturally formulated as the following bi-level optimization problem:\nmax \u03c8\nEx\u223cD,t1,t2\u223cT [ S(f\u03b8(t1(g\u03c8(x))), f\u03b8(t2(xr))) ] , s.t. \u03b8 = argmin\n\u03b8 Ex\u223cDb LCL(x;\u03b8). (2)\nwhere in the outer optimization, D is a clean dataset, g\u03c8(\u00b7) : X \u2192 X is a backdoor generator that aims to inject trigger to an input x and generate its backdoored version g\u03c8(x), and xr is the reference data sampled from the target class. In the inner optimization, Db is the backdoored dataset defined as D \u222a g\u03c8(xr), and recall that LCL(x;\u03b8) is a general CL objective as in Eq. (1).\nIntuitively as illustrated in Figure 2, the inner optimization aims to obtain a feature extractor trained by a surrogate victim on the backdoored data Db, whose backdoor effect could be influenced by aforementioned CL mechanisms; the outer optimization aims to find an effective backdoor generator that can still mislead the feature extractor to maximize the similarity between backdoored and target-class data. As a result, if the optimized backdoor generator can fool the surrogate feature extractor, it is likely to adapt to unseen victims.\nBi-level Optimization Algorithm To solve this bi-level optimization problem, we perform an interleaving update for the feature extractor (i.e., inner optimization step) and the backdoor generator (i.e., outer optimization). Detailed steps are summarized in Algorithm 1. Besides, to escape local optimum, in practice, we regularly re-initialize the surrogate feature extractor in the interleaving optimization procedure.\nAlgorithm 1 Bi-Level Trigger Optimization (BLTO) 1: Input: clean dataset D, reference data xr, data augmentation set T 2: Initialize parameters of backdoor generator \u03c80 and surrogate feature extractor \u03b80 3: repeat N iterations: 4: Generate the backdoored dataset Db \u2190 D \u222a g\u03c8(xr) 5: for k in {0, 1, ...,K \u2212 1} do \u25b7 Inner optimization 6: Sample backdoored data batch x \u223c Db; sample augmentations t1, t2 \u223c T 7: \u03b8k+1 \u2190 \u03b8k \u2212 \u03b7i\u2207\u03b8 LCL(x;\u03b8k) 8: for j in {0, 1, ..., J \u2212 1} do \u25b7 Outer optimization 9: Sample clean data batch x \u223c D; sample augmentations t1, t2 \u223c T 10: \u03c8j+1 \u2190 \u03c8j \u2212 \u03b7o\u2207\u03c8 S(f\u03b8K (t1(g\u03c8j (x))), f\u03b8K (t2(xr))) 11: Output: backdoor generator \u03c8"
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "5.1 EVALUATION SETUP",
            "text": "Following prior works (Saha et al., 2022; Li et al., 2023), we verify our backdoor attack on three benchmark datasets: CIFAR-10/-100 (Krizhevsky, 2009), and ImageNet-100. Among them, ImageNet-100 is a randomly selected 100-class subset of the ImageNet ILSVRC-2012 dataset (Deng et al., 2009).\nAttacker\u2019s Setting When solving the bi-level optimization problem to poison unlabeled data, we adopt SimSiam (Chen & He, 2021) as the surrogate CL framework (performance of other CL frameworks refer to Appendix C.6), where we use ResNet-18 (He et al., 2016) as the backbone encoder for CIFAR-10/-100 and\nResNet-34 for ImageNet-100. The backdoor generator g\u03c8(\u00b7) is implemented as a DNN detailed in Appendix B. To control the trigger strength, we project the backdoored data g\u03c8(x) in an \u2113\u221e ball of the input x with a radius \u03f5 = 8/255. The poisoning rate is set as P = 1% by default, meaning the attacker can poison 1% training data. We assume the attack target class as \u201ctruck\u201d, \u201capple\u201d and \u201cnautilus\u201d when attacking CIFAR-10, CIFAR-100 and ImageNet-100 respectively.\nVictim\u2019s Setting The victim will pretrain a feature extractor on the backdoored data via CL, and train a predictor for a downstream classification task. In default experiment setting, the victim uses SimSiam and ResNet (same as the attacker\u2019s surrogate setting). However in practice, the victim could use any CL strategies (e.g., BYOL (Grill et al., 2020), SimCLR (Chen et al., 2020) and MoCo He et al. (2020)) and backbone encoders (e.g., RegNet (Xu et al., 2023)). We evaluate the effectiveness of our trigger design in such challenging scenario, where the victim adopts distinct CL strategies or encoders, and uses training set whose distribution is unseen by the attack. Studies on more victim\u2019s settings can be found in Appendix C.6.\nMetrics The attack performance is evaluated by the resulting backdoored model. We use Backdoor Accuracy (BA) to measure the attack stealthiness, calculated as the model accuracy on intact inputs (higher BA implies better stealthiness). We use Attack Success Rate (ASR) to measure the attack effectiveness, defined as the accuracy in classifying backdoored inputs as the target class (higher ASR implies better effectiveness)."
        },
        {
            "heading": "5.2 PERFORMANCE UNDER DIFFERENT ATTACKING SCENARIOS",
            "text": "Transferability across CL Strategies In Table 1, we compare the performance of our attack against baselines, i.e., SSL backdoor (Saha et al., 2022) and CTRL (Li et al., 2023), when the victim uses different CL strategies: SimCLR (Chen et al., 2020), BYOL (Grill et al., 2020), and SimSiam (Chen & He, 2021). Besides, we provide the non-backdoor case as a reference. The results demonstrate that our attack achieves a remarkably high ASR, while maintaining a comparable BA to the non-backdoor case. Moreover, the backdoor trigger generator optimized via surrogate SimSiam is shown to also generalize well on other CL strategies (BYOL and SimCLR) adopted by the victim, indicating a superior transferable ability of our attack across CL methods.\nTransferability across Backbones During trigger optimization, we adopt ResNet-18 (He et al., 2016) as the backbone encoder on CIFAR-10. However, in practice, the victim could use a different backbone, demanding the attack to transfer across different backbones. Therefore, we evaluate our attack performance when the victim uses other backbone architectures: ResNet-34 (He et al., 2016), ResNet-50, MobileNet-v2(Sandler et al., 2018), ShuffleNet-V2 (Ma et al., 2018), SqueezeNet (Iandola et al., 2016)), RegNet (Xu et al., 2023) and ViT (Dosovitskiy et al., 2021). In this evaluation, the victim uses SimCLR as the CL method. Results on CIFAR-10 are shown in Table 2, where ACC is the benign model\u2019s accuracy when there is no backdoor.\nWe observe a stable attack performance even when the victim uses a different backbone encoder, which suggests an excellent transferability of our attack across backbones. Besides, we further use ViT to evaluate the transferability on ImageNet-100 in Appendix C.4.\nTransferability on Unseen Data Since in practice the attacker cannot access victim\u2019s data information, the injected poisoned data could follow a distribution distinct from to the victims\u2019s actual training data. We now verify the performance of our attack under such case. Assume the attack poisons CIFAR-10, while the victim performs CL on a mixed set of CIFAR-10 and CIFAR-100. We\nreport our attack performance in Table 3, where the ratio of CIFAR-10 in the mixed set varies to indicate the degree of distribution shift. Note that our attack only poison CIFAR-10 and the poison rate keeps 1% (i.e., 1% of the whole training set is poisoned). Results show that our attack maintains a high ASR even when most victim\u2019s data are from a different distribution, implying a good transferability of our attack on unseen data. In summary (Table. 1, 2 and 3), our proposed backdoor attack can work effectively in challenging attacking scenarios, where the actual victim conducts different CL procedures from the surrogate victim."
        },
        {
            "heading": "5.3 EVALUATION AGAINST BACKDOOR DEFENSE",
            "text": "We now evaluate the attack performance against seven typical backdoor defenses via backdoor detection or mitigation on CIFAR-10. More details about this experiment setup can be found in Appendix A.5.\nBackdoor Detection Backdoor detection aims to check abnormalities of victim\u2019s models or train datasets. Prior works (Jia et al., 2022; Zheng et al., 2023) have shown that existing SL-targeted detection solutions (e.g., Neural Cleanse (Wang et al., 2019)) are less effective in detecting CL backdoor threats, compared with CL-targeted solutions (e.g., SSL-Cleanse (Zheng et al., 2023) , DECREE (Feng et al., 2023) and ASSET (Pan et al., 2023)). According to the experiment results (The backdoor threat can be\ndetected if the anomaly index exceeds 2 (Wang et al., 2019).) in Table. 4 and Appendix C.5 (covers our performance against DECREE and ASSET), our attack not only breaks the SL-targeted detection solution (i.e., Neural Cleanse), but also successfully survives the more advanced CL-targeted detection solutions.\nBackdoor Mitigation Backdoor mitigation aims to neutralize the potential backdoor threats in the model directly. Backdoor mitigation strategies include pruning sensitive neurons (e.g., CLP (Zheng et al., 2022)), adversarial training (e.g., I-BAU (Zeng et al., 2021)), and knowledge distillation (Saha et al., 2022) on clean data. We report the attack performance when these defenses are adopted to verify the resistance of our backdoor attack. As shown in Table. 5, among these methods, at a cost of model accuracy (about \u221210% on BA), the adversarial training (I-BAU) provides the best\nmitigation performance (about \u221230% on ASR), but our attack outperforms baselines without defenses as in Table 1. Besides, our attack still remains effective under knowledge distillation, which was previously shown to successfully neutralize SSL backdoor (Saha et al., 2022). The effectiveness of our attack under existing common defenses calls for more dedicated defense designs for robust CL pipelines."
        },
        {
            "heading": "5.4 ANALYSIS AND DISCUSSION",
            "text": "This analysis is to justify our understanding about how CL mechanisms influence attack effectiveness. Besides, influence of attack strength (i.e., perturbation budget \u03f5 and poisoning ratio P ) is studied in Appendix C.2, the ablation study of the bi-level optimization can be found in Appendix C.3, and the transferability study across different hyper-parameters (e.g., victim\u2019s batch size and BLTO\u2019s surrogate model) is shown in Appendix C.6.\nOur attack can confuse victim to miscluster backdoored data with the target-class data. We first visualize how the victim encodes data in the embedding space to intuitively show if our initial motivation is realized: if the victim\u2019s feature extractor is successfully backdoored, it should embed the backdoored data (injected with backdoor trigger) to be close to the target-class data. Suppose the victim trains a ResNet18 backbone on CIFAR-10 via SimCLR, and the attack target is \u201ctruck\u201d class. Figure 3 shows the t-SNE visualization (Van der Maaten & Hinton, 2008) of data embedded by the victim\u2019s feature extractor, where black dots are the trigger-injected data and light blue dots (with id=9) are the target-class data. We can clearly observe that in our attack, the cluster formed by backdoored data overlaps with the target-class cluster, which suggests that our attack is more effective in misleading the victim to associate backdoored data with the target class. As a consequence, the predictive head (which takes these embeddings as features in the downstream task) will be fooled to misclassify backdoored data into target class, leading to a higher ASR. This result is also consistent with our findings in Figure 1. These observations verify the reason why our attack is so effective: we can successfully confuse the victim\u2019s feature extractor between backdoored data and target-class data.\nOur attack can survive different types of data augmentations. Prior works (Chen et al., 2020; He et al., 2020) show that properly-designed augmentations can benefit CL. However from the attacker\u2019s perspective, augmentations adopted by the victim could destroy the trigger pattern thus diminishing the backdoor effect. A successful backdoor attack should be able to survive all possible augmentations. To verify this, we consider a default augmentation set Tvictim = {RandomResizedCrop, RandomHorizontalFlip, RandomColorJitter, RandomGrayscale} that could be used by the victim, following CTRL (Li et al., 2023).\nA special augmentation is GaussianBlur, whose effect will be separately discussed. Our attack considers all these augmentations in the bi-level optimization. We compare our attack with CTRL (Li et al., 2023) on ImageNet-100, with the attack target class as \u201cnautilus\u201d. As shown in Table 6, our attack in general keeps a high ASR no matter what data augmentation operations the victim is using. Particularly, even when the victim includes the GaussianBlur augmentation, our backdoor attack can still achieve a remarkably high ASR (i.e., 99.13%), while CTRL suffers a large collapse on ASR\nfrom 35.62% to 1%. The observation indicates that existing attacks could be fragile to the victim\u2019s augmentation strategy, and our stable performance demonstrates the necessity of bi-level trigger optimization for surviving possible CL augmentation mechanisms.\n0 100 200 300 400 500 600 700 800 epoch\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nA lig\nnm en\nt l os\ns\nSSL backdoor CTRL Ours\n(a) Alignment Loss\n0 100 200 300 400 500 600 700 800 epoch\n3.25\n3.00\n2.75\n2.50\n2.25\n2.00\n1.75\n1.50\nU ni\nfo rm\nity lo\nss\n(b) Uniformity Loss\nFigure 4: Alignment and uniformity during backdoor training on SimCLR.\nOur attack is less impacted by the uniformity effect of CL. Prior works (Wang & Isola, 2020) have shown that the CL objective (e.g., InfoNCE (Oord et al., 2018) in SimCLR) can be interpreted as minimizing an alignment loss (to align views of the same instance) and a uniformity loss (to spread views of different instances uniformly in the embedding space to prevent collapse). These two mechanisms significantly influence the CL backdoor effectiveness: the attack can take the advantage of the alignment mechanism to correlate the trigger and the target class (Saha et al., 2022); on the\ncontrast, the uniformity mechanism may impair their correlation by imposing dissimilarity across backdoored instances, thus nullifying the attack. Therefore, we provide a new understanding of backdooring CL: a successful attack should minimize the alignment loss on backdoored data to correlate trigger and target, while encouraging a large uniformity loss to retain their correlation across instances. To verify this, we monitor the alignment loss and uniformity loss on backdoored images throughout the victim\u2019s CL procedure (using SimCLR to train ResNet-18 on poisoned CIFAR-10). Figure 4 compares alignment and uniformity loss when the training data is poisoned by SSL backdoor, CTRL or our BLTO attack. We have following observations: 1) our alignment loss is better minimized, which implies a stronger alignment between the trigger and the target; 2) our uniformity loss is prominently higher, which indicates that our attack is more resistant to the uniformity mechanism of CL. Our findings again highlight the importance of taking into account the special mechanisms in CL to design tailored backdoor attacks.\nOur generated triggers capture global semantics, which explains why our attack can mislead victim feature extractors, survive CL data augmentation and uniformity mechanism. We visualize the actual trigger pattern by calculating the difference between the original image and its backdoored version, i.e., ||g\u03c8(x)\u2212 x||. Figure 5 illustrates an example image of backdooring ImageNet-100 with the target as \u201cnautilus\u201d. We observe that the trigger generated by our attack presents a global pattern carrying similar semantic meaning as in the original image (e.g., the tentacle-like\npatterns in Figure 5c). Such property is beneficial in three aspects: 1) a global trigger that is semantically aligned with the original image may not be easily cropped or blurred, thus bringing resilience to various data augmentations (e.g., RandomSizeCrop, GaussianBlur), which explains Table 6; 2) during the outer optimization when trigger is added to target class data, since the trigger captures similar semantic as the target, the similarity between the backdoored image and the attack target is intensified, thus strengthening the cluster formed among backdoored and target-class data, which explains Figure 3 and Figure 1; 3) since the semantic of trigger can adapt with the input image, the trigger pattern could differ across instances, thus is less penalized by the uniformity effect in CL, which explains Figure 4."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we proposed a novel bi-level trigger optimization method designed for backdooring contrastive learning frameworks. The method was motivated by our observation that existing backdoor attacks on CL (via data poisoning) fails to maintain the similarity between the triggered data and the target class in the embedding space due to special CL mechanisms: data augmentation may impair trigger pattern, and the uniformity effect may destroy the association between trigger and target class. Our proposed method can mitigate such issues by simulating possible CL dynamic, and optimizing the trigger generator to survive it. Extensive experiments have verified that our method is transferable to varying victim CL settings and is resilient against common backdoor defenses. Moreover, comprehensive analyses were conducted to justify how CL mechanisms could influence the attack effectiveness and reason the success of our attack."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We would like to thank all anonymous reviewers for spending time and efforts and bringing in constructive questions and suggestions, which help us greatly to improve the quality of the paper. We would like to also thank the Program Chairs and Area Chairs for handling this paper and providing the valuable and comprehensive comments."
        },
        {
            "heading": "A.2 REFERENCE DATA",
            "text": "As introduced in Section 5, our attack goals are: \u201ctruck\u201d for CIFAR-10, \u201capple\u201d for CIFAR-100 and \u201cnautilus\u201d for ImageNet-100. When preparing the backdoored dataset, we require a clean dataset D and a batch of reference data xr (of the target category). Under the default poisoning rate (1%), ratio of data number in D and xr is 99 : 1. Specifically, we enunciate the components of D and xr when preparing the backdoored dataset for inner optimization in Algorithm 1 (or for the victim\u2019s backdoor training).\n(1) CIFAR-10: xr is a set of 500 images (belong to category \u201dtruck\u201d) randomly sampled from the training set of CIFAR-10, and D is the set of the remaining 49500 images. (2) CIFAR-100: xr is a set of 500 images (belong to category \u201dapple\u201d) sampled from the training set of CIFAR-100, and D is the set of the remaining 49500 images. (3) ImageNet-100: xr is a set of 1300 images (belong to category \u201dnautilus\u201d) sampled from the training set of ImageNet-100, and D is the set of the remaining 128700 images."
        },
        {
            "heading": "A.3 DATA AUGMENTATIONS",
            "text": "Without specific notations, the augmentation T used to optimize the trigger is [RandomResizedCrop, RandomHorizontalFlip, RandomColorJitter, RandomGrayscale, GaussianBlur], whose detailed implementation is shown in Algorithm 3:\nAlgorithm 2 Default augmentation T for trigger optimization (Pytorch version).\nTransform = T.Compose([ T.RandomResizedCrop(image_size, scale=(0.2, 1.0), antialias=True), T.RandomHorizontalFlip(), T.RandomApply([T.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8), T.RandomGrayscale(p=0.2), T.RandomApply([T.GaussianBlur(kernel_size=image_size//20*2+1, sigma=(0.1, 2.0))], p\n=0.5), T.ToTensor(), T.Normalize(*mean_std) # The mean_std depends on the involved dataset.\n])\nOn the other side, following previous works (Li et al., 2023), the default data augmentation used by the victim is defined as [RandomResizedCrop, RandomHorizontalFlip, RandomColorJitter, RandomGrayscale], whose detailed implementation is shown in Algorithm 3. In Section 5, we have specifically discussed the scenario where the victim uses data augmentation of Algorithm 2 (i.e., considering T.GaussianBlur) to train CL models.\nAlgorithm 3 The default victim\u2019s data augmentation (Pytorch version).\nTransform = T.Compose([ T.RandomResizedCrop(image_size, scale=(0.2, 1.0), antialias=True), T.RandomHorizontalFlip(), T.RandomApply([T.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8), T.RandomGrayscale(p=0.2), T.ToTensor(), T.Normalize(*mean_std) # The mean_std depends on the involved dataset.\n])\nA.4 IMPLEMENTATION ON CL TRAINING\nOur experiments in Section 5 involve three CL strategies: SimSiam, SimCLR, and BYOL. The implementation of SimSiam follows the implementation of https://github.com/PatrickHua/SimSiam on all three data benchmarks (CIFAR-10/-100, ImageNet-100). Besides, the implementations of BYOL and SimCLR refer to https://github.com/meet-cjli/CTRL.\nFollowing previous works (Li et al., 2023), when training a predictor for evaluating the backdoor performance, we use the clean training set in CIFAR-10/-100 and ImageNet-100 as the downstream training set. We use the testing set in CIFAR-10/-100 and the validation set of ImageNet-100 for performance evaluation. The predictor is a knn monitor by default.\nA.5 IMPLEMENTATION ON BACKDOOR DEFENSE\nWe strictly follow the official implementations of the involved backdoor defense solutions (if they are open-sourced). We first list the official urls (U-BAU, CLP, and Neural Cleanse) as follows:\n(1) I-BAU: https://github.com/YiZeng623/I-BAU. We use the clean CIFAR-10 training set to perform the unlearning step of I-BAU. We report the ASR and BA of our attack after running I-BAU for 50\nepochs. Noteworthy, as the I-BAU is a supervised-learning targeted defense solution, we use the MLP with three linear layers (the same implementation as the MLP in https://github.com/jinyuan-jia/ BadEncoder) to replace the knn monitor as the downstream predictor before performing I-BAU. Besides, we perform the same operation when defending via CLP and Neural Cleanse.\n(2) CLP: https://github.com/rkteddy/channel-Lipschitzness-based-pruning. Noteworthy, CLP (Zheng et al., 2022) has an important hyperparameter u. Under a lower u, CLP lowers the threshold to judge and prune the suspicious neurons of the backdoored DNN model, which is more likely to mitigate the backdoor threat. However, as more neurons are pruned, backdoor accuracy (BA) performance will drop simultaneously. In Table 5, we reported the backdoor performance of our backdoor attack with the u set as 4.0 in CLP.\nIn practice, the defender may attempt different u to perform CLP. To this end, in Figure. 6, we provide our backdoor performance or our attack under CLP with the values of u ranging from 0 to 5.5. As observed, under most implementations of u (when the BA after defense is acceptable (e.g., BA> 80%)), our backdoor attack can still retain a high attack success rate.\n(3) Neural Cleanse: https://github.com/bolunwang/backdoor.\n(4) DECREE: https://github.com/GiantSeaweed/DECREE/tree/master.\n(5) ASSET: https://github.com/ruoxi-jia-group/ASSET/tree/main.\nThen, we discuss our implementations on the remaining backdoor defense solutions that are not open-sourced.\n(1) SSL-Cleanse: The implementations on the trigger reversion follow the original paper (Zheng et al., 2023), and the outlier detection (on those abnormal reversed triggers) methods refer to that of the Neural Cleanse.\n(2) Data Distillation: We follow the official implementation reported in (Saha et al., 2022), and the dataset used for data distillation is the whole training set (clean) of CIFAR-10."
        },
        {
            "heading": "B BACKDOOR IMAGE GENERATOR",
            "text": "In this paper, we select the DNN generator g(\u00b7) : X \u2192 X to produce backdoor data (i.e., g(x)). The specific architecture of g(\u00b7) is shown in Figure. 7. The specific implementation (Pytorch) for the g(\u00b7) : X \u2192 X can refer to https://github.com/ Muzammal-Naseer/TTP."
        },
        {
            "heading": "C ADDITIONAL ATTACK ANALYSIS",
            "text": ""
        },
        {
            "heading": "C.1 ALIGNMENT AND UNIFORMITY LOSS ANALYSIS",
            "text": "Additional Alignment and Uniformity Loss on BYOL and SimSiam. In the main text, we exhibit the variation of the alignment and uniformity loss (we monitor them on the backdoored data exclusive) throughout the victim\u2019s training (via SimCLR) in Figure 4. When conducting the experiment, we assume the victim trains a ResNet-18 feature extractor over CIFAR-10, the attack target is \u201ctruck\u201d, and the poisoning rate P is 1%. In Figure 8, we further exhibit the variation of alignment and uniformity loss when attacking BYOL and SimSiam, where we can observe the similar phenomenon with that in SimCLR (Figure 4).\nC.2 IMPACT OF ATTACK STRENGTH\nImpact on \u03f5 and Poisoning Ratio P . Fig. 9 exhibits the ablation study on the clamping threshold (i.e., \u03f5 in the \u2113\u221e ball) and the poisoning rate (P ). We set \u03f5 as 8/255 and P as 1% when performing our bi-level optimization by default. When performing ablation study on \u03f5, we fix P as 1%. When performing the ablation study on P , we fix \u03f5 = 8/255. We assume the victim uses BYOL to train a ResNet18 backbone on the CIFAR-10. According to Fig. 9, our ASR positively correlates with the trigger intensity and poisoning rate, which is consistent with prior works (Saha et al., 2022; Li et al., 2023). These results indicate that we can conveniently control the\nbackdoor performance by adjusting these hyperparameters, rendering it suitable for different attacking scenarios.\nC.3 IMPACT OF INNER AND OUTER OPTIMIZATION\nBoth inner and outer optimization are necessary for a good CL attack. In this section, we discuss the function of the inner and outer optimization in our bi-level optimization. We first exhibit an ablation study on these two components. Specifically, we assume the victim trains a ResNet-18 feature extractor over CIFAR-10 using SimCLR, BYOL, and SimSiam. Then, we evaluate the attack success rate of three scenarios: (1) Our attack is optimized via both inner and outer optimization. (2) Our attack is optimized via outer\noptimization only. (3) Our attack is not optimized (i.e., performing inner optimization only). Based on the results in Fig. 7, we observe that the attack optimized exclusively via outer optimization presents a volatile backdoor performance over different CL strategies (e.g., < 20% ASR in SimSiam). Such a phenomenon highlights the significance of the constraint condition (i.e., \u03b8 = argmin\u03b8 Ex\u223cDb LCL(x;\u03b8)) in eq (2), without which may render the optimized g\u03c8(\u00b7) deviate from the solution of eq (2). If the g\u03c8(\u00b7) fails to meet eq (2) during the surrogate CL training, the g\u03c8(\u00b7) is less likely to perform robustly in practical attacking scenarios."
        },
        {
            "heading": "C.4 ATTACK TRANSFERABILITY ON MODERN BACKBONES",
            "text": "This section supplements the performance of our BLTO attack on non-CNN architecture, ViT (Dosovitskiy et al., 2021). Specifically, the attacker uses ResNet34 as the surrogate model to train the trigger generator over ImageNet-100 (target is \u201dnautilus\u201d), then attack the victim\u2019s ViT-small/16 encoder in practice with the poisoning rate as 1%. The attack performance is shown as follows (along with attacking ResNet34 presented in Table 1):"
        },
        {
            "heading": "C.5 PERFORMANCE ON MORE CL DEFENSE METHODS",
            "text": "DECREE DECREE (Feng et al., 2023) is a trigger inversion backdoor defense method tailored for encoders trained in a SSL manner. We re-implement DECREE on our backdoored ResNet-18 encoder trained via SimCLR and BYOL on CIFAR-10. The reversed trigger and corresponding L1-norms of the trigger mask m (along with P1-norms) are demonstrated in Figure 10. According to their works, when the encoder\u2019s P1-norm is lower than 0.1, it will be regarded as backdoored. Based on the result in Figure 10, we find our attack can effectively avoid the detection of DECREE.\nASSET ASSET (Pan et al., 2023) aims to separate backdoored samples from clean ones by inducing different model behaviors between the backdoor and clean samples to promote their separation. We re-implement their defense solutions on our backdoored CIFAR-10 dataset. To be specific, we utilize our synthesized trigger on CIFAR-10 (target is 9) to generate a poisoned CIFAR-10 (poisoning rate is 1%, our default setting). The real poisoned index is the first 501 indexes of data points (whose target id is 9) in CIFAR-10, and the poisoned feature extractor \u03b8\u2217poi is the ResNet18 backbone trained on the poisoned CIFAR-10. The loss function utilized in the outer optimization loop (both in min step and max step) is Lvar (i.e., Eqn. 3 in ASSET (Pan et al., 2023)).\nTPR depicts how well a specific backdoor detection method filters out the backdoored samples. A higher True Positive Rate (TPR) (closer to 100%) denotes a stronger filtering ability. False Positive Rate (FPR)\ndepicts how precise the filtering is: when a specific method achieves TPR that is high enough, FPR helps us to understand the trade-off, i.e., how many clean samples are wasted and wrongly flagged as backdoored during the detection. A lower FPR shows that fewer clean samples are wasted, and more clean data shall be kept and available for downstream usage. According to the metrics in ASSET, TPR can be calculated as follows: TPR = 5.39%, and FPR = 32.17%. It indicates that our poisoned data can effectively evade the detect of ASSET.\nC.6 IMPACT OF OTHER HYPER-PARAMETERS\nAttacker\u2019s surrogate model. In Table 1, we assume the attacker uses Simsiam Chen & He (2021) as the surrogate method in our BLTO attack. This section talks about the attack performance under another two surrogate CL methods (BYOL (Grill et al., 2020) and SimCLR (Chen et al., 2020)), which is shown in the following table:\nIt indicates that our BLTO attacks can suit different CL methods.\nVictim\u2019s batch size and learning rate. In practice, the victim may use different hyperparameters (e.g., batch size, learning rate, or temperature) to train their encoders. This section provides our attack performance under these practical scenarios. The ablation study is conducted on CIFAR-10, the victim\u2019s CL method is SimCLR and backbone is ResNet18. The attacker\u2019s surrogate CL method is SimSiam (the same setting with that in Table 1), the attacker uses batch size as 512, and a learning rate scheduler (base lr 0.03, final lr 0). The ablation study on batch size is shown in Table 11, learning rate is shown in Table 12 and temperature of SimCLR in Table 13.\n256 512 (default) 1024 BA 86.05% 90.10% 90.85%\nASR 90.88% 91.27% 89.39%\nTable 11: Batch size ablation.\n\u00d70.5 \u00d71 (default) \u00d72 BA 88.05% 90.10% 88.76%\nASR 91.80% 91.27% 92.54%\nTable 12: Learning rate ablation.\nVictim\u2019s CL model. In Table 1, we conducted the attack evaluation when the victim uses SimCLR, BYOL and SimSiam as the CL method. This section we conduct a supplementary evaluation on MoCo He et al. (2020). We assume the attacker is going to attack \u201dtruck\u201d in CIFAR-10 (use SimSiam as the surrogate CL method), and the victim uses ResNet18 as the backbone. The victim\u2019s queue length is set as 4096, batch size is 512, temperature \u03c4 is set as 0.1. Besides, we additional evaluate other three attacking scenarios: non-backdoor, CTRL Li et al. (2023) and SSL-backdoor Saha et al. (2022). The experimental results are show in Table 14, which indicates that our BLTO attack can also outperform other attacks on MoCO.\nVictim\u2019s downstream task. In practice, the victim may use different downstream dataset to finetune their resulting model with the pre-trained one. Though parts of these situation have been discussed in Table 3, this section discuss a more practical and challenging situation: the downstream dataset is totally different with the pre-trained model (no overlapping). We assume the victim trains a ResNet18 encoder on our backdoored CIFAR-10, and the downstream dataset is STL-10 (train part). The results of our attack performance under this situation is shown as Table 15. It indicates that our BLTO attacks doesn\u2019t demand the overlapping between the pre-trained dataset and downstream dataset to claim an remarkable attack performance."
        },
        {
            "heading": "C.7 SUPPLEMENTARY IMPLEMENTATION DETAILS OF INVOLVED CL ATTACKS.",
            "text": "Note that the performance of our re-implemented CTRL Li et al. (2023) in Table 1 is a bit different from the original paper Li et al. (2023). In fact, the ASR difference is due to the contrastive learning (CL) implementation adopted by the victim, which can be seen from the model accuracy on clean data: using our CL implementation, the victim encoder can achieve almost 90% BA (i.e., ACC) on CIFAR-10; while with the original CTRL\u2019s CL implementation, the encoder achieves 80.2% ACC on CIFAR-10 (i.e., Table 1 in CTRL (Li et al., 2023)). Note that in our threat model, the attacker cannot control how the victim trains the actual CL model. In practice, victims would prefer adopting CL that produces a more accurate encoder with higher"
        },
        {
            "heading": "ACC ASR ACC ASR ACC ASR",
            "text": "ACC. In fact, our CL implementation follows standard practices with matching ACC: SimCLR (Table B.5 in [4]) reports 90.6%."
        }
    ],
    "year": 2024
}