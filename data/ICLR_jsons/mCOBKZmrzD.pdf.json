{
    "abstractText": "Equivariant Transformers such as Equiformer have demonstrated the efficacy of applying Transformers to the domain of 3D atomistic systems. However, they are limited to small degrees of equivariant representations due to their computational complexity. In this paper, we investigate whether these architectures can scale well to higher degrees. Starting from Equiformer, we first replace SOp3q convolutions with eSCN convolutions to efficiently incorporate higher-degree tensors. Then, to better leverage the power of higher degrees, we propose three architectural improvements \u2013 attention re-normalization, separable S activation and separable layer normalization. Putting this all together, we propose EquiformerV2, which outperforms previous state-of-the-art methods on large-scale OC20 dataset by up to 9% on forces, 4% on energies, offers better speed-accuracy trade-offs, and 2\u02c6 reduction in DFT calculations needed for computing adsorption energies. Additionally, EquiformerV2 trained on only OC22 dataset outperforms GemNet-OC trained on both OC20 and OC22 datasets, achieving much better data efficiency. Finally, we compare EquiformerV2 with Equiformer on QM9 and OC20 S2EF-2M datasets to better understand the performance gain brought by higher degrees.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yi-Lun Liao"
        },
        {
            "affiliations": [],
            "name": "Brandon Wood"
        },
        {
            "affiliations": [],
            "name": "Abhishek Das"
        },
        {
            "affiliations": [],
            "name": "Tess Smidt"
        }
    ],
    "id": "SP:501172ffa88e4f21e72d5a9b5c71e7366d1630d1",
    "references": [
        {
            "authors": [
                "Ilyes Batatia",
                "David Peter Kovacs",
                "Gregor N.C. Simm",
                "Christoph Ortner",
                "Gabor Csanyi"
            ],
            "title": "MACE: Higher order equivariant message passing neural networks for fast and accurate force fields",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Simon Batzner",
                "Albert Musaelian",
                "Lixin Sun",
                "Mario Geiger",
                "Jonathan P. Mailoa",
                "Mordechai Kornbluth",
                "Nicola Molinari",
                "Tess E. Smidt",
                "Boris"
            ],
            "title": "Kozinsky. E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials",
            "venue": "Nature Communications,",
            "year": 2022
        },
        {
            "authors": [
                "Johannes Brandstetter",
                "Rob Hesselink",
                "Elise van der Pol",
                "Erik J Bekkers",
                "Max Welling"
            ],
            "title": "Geometric and physical quantities improve e(3) equivariant message passing",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Lowik Chanussot",
                "Abhishek Das",
                "Siddharth Goyal",
                "Thibaut Lavril",
                "Muhammed Shuaibi",
                "Morgane Riviere",
                "Kevin Tran",
                "Javier Heras-Domingo",
                "Caleb Ho",
                "Weihua Hu",
                "Aini Palizhati",
                "Anuroop Sriram",
                "Brandon Wood",
                "Junwoong Yoon",
                "Devi Parikh",
                "C. Lawrence Zitnick",
                "Zachary Ulissi"
            ],
            "title": "Open catalyst 2020 (oc20) dataset and community challenges",
            "venue": "ACS Catalysis,",
            "year": 2021
        },
        {
            "authors": [
                "Taco S. Cohen",
                "Mario Geiger",
                "Jonas K\u00f6hler",
                "Max Welling"
            ],
            "title": "Spherical CNNs",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Alexey Gritsenko",
                "Vighnesh Birodkar",
                "Cristina Vasconcelos",
                "Yi Tay",
                "Thomas Mensink",
                "Alexander Kolesnikov",
                "Filip Paveti\u0107",
                "Dustin Tran",
                "Thomas Kipf",
                "Mario Lu\u010di\u0107",
                "Xiaohua Zhai",
                "Daniel Keysers",
                "Jeremiah Harmsen",
                "Neil Houlsby"
            ],
            "title": "Scaling vision transformers to 22 billion parameters",
            "venue": "arXiv preprint arXiv:2302.05442,",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arxiv preprint arxiv:1810.04805,",
            "year": 2019
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Vijay Prakash Dwivedi",
                "Xavier Bresson"
            ],
            "title": "A generalization of transformer networks to graphs. arxiv preprint arxiv:2012.09699, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Stefan Elfwing",
                "Eiji Uchibe",
                "Kenji Doya"
            ],
            "title": "Sigmoid-weighted linear units for neural network function approximation in reinforcement learning",
            "venue": "arXiv preprint arXiv:1702.03118,",
            "year": 2017
        },
        {
            "authors": [
                "Nathan Frey",
                "Ryan Soklaski",
                "Simon Axelrod",
                "Siddharth Samsi",
                "Rafael Gomez-Bombarelli",
                "Connor Coley",
                "Vijay Gadepally"
            ],
            "title": "Neural scaling of deep chemical models. ChemRxiv, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Fabian Fuchs",
                "Daniel E. Worrall",
                "Volker Fischer",
                "Max Welling"
            ],
            "title": "Se(3)-transformers: 3d rototranslation equivariant attention networks",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Johannes Gasteiger",
                "Shankari Giri",
                "Johannes T. Margraf",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Fast and uncertainty-aware directional message passing for non-equilibrium molecules. In Machine Learning for Molecules Workshop, NeurIPS, 2020a",
            "year": 2020
        },
        {
            "authors": [
                "Johannes Gasteiger",
                "Janek Gro\u00df",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Directional message passing for molecular graphs",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Johannes Gasteiger",
                "Muhammed Shuaibi",
                "Anuroop Sriram",
                "Stephan G\u00fcnnemann",
                "Zachary Ulissi",
                "C Lawrence Zitnick",
                "Abhishek Das"
            ],
            "title": "GemNet-OC: Developing Graph Neural Networks for Large and Diverse Molecular Simulation Datasets",
            "venue": "Transactions on Machine Learning Research (TMLR),",
            "year": 2022
        },
        {
            "authors": [
                "Justin Gilmer",
                "Samuel S. Schoenholz",
                "Patrick F. Riley",
                "Oriol Vinyals",
                "George E. Dahl"
            ],
            "title": "Neural message passing for quantum chemistry",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Godwin",
                "Michael Schaarschmidt",
                "Alexander L Gaunt",
                "Alvaro Sanchez-Gonzalez",
                "Yulia Rubanova",
                "Petar Veli\u010dkovi\u0107",
                "James Kirkpatrick",
                "Peter Battaglia"
            ],
            "title": "Simple GNN regularisation for 3d molecular property prediction and beyond",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "B. Hammer",
                "L.B. Hansen",
                "J.K. N\u00f8rskov"
            ],
            "title": "Improved adsorption energetics within densityfunctional theory using revised perdew-burke-ernzerhof functionals",
            "venue": "Phys. Rev. B,",
            "year": 1999
        },
        {
            "authors": [
                "Gao Huang",
                "Yu Sun",
                "Zhuang Liu",
                "Daniel Sedra",
                "Kilian Q. Weinberger"
            ],
            "title": "Deep networks with stochastic depth",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2016
        },
        {
            "authors": [
                "Weile Jia",
                "Han Wang",
                "Mohan Chen",
                "Denghui Lu",
                "Lin Lin",
                "Roberto Car",
                "Weinan E",
                "Linfeng Zhang"
            ],
            "title": "Pushing the limit of molecular dynamics with ab initio accuracy to 100 million atoms with machine learning. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC \u201920",
            "year": 2020
        },
        {
            "authors": [
                "Bowen Jing",
                "Stephan Eismann",
                "Patricia Suriana",
                "Raphael John Lamarre Townshend",
                "Ron Dror"
            ],
            "title": "Learning from protein structure with geometric vector perceptrons",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Johannes Klicpera",
                "Florian Becker",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Gemnet: Universal directional graph neural networks for molecules",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Risi Kondor",
                "Zhen Lin",
                "Shubhendu Trivedi"
            ],
            "title": "Clebsch\u2013gordan nets: a fully fourier space spherical convolutional neural network",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Devin Kreuzer",
                "Dominique Beaini",
                "William L. Hamilton",
                "Vincent L\u00e9tourneau",
                "Prudencio Tossou"
            ],
            "title": "Rethinking graph transformers with spectral attention",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Janice Lan",
                "Aini Palizhati",
                "Muhammed Shuaibi",
                "Brandon M Wood",
                "Brook Wander",
                "Abhishek Das",
                "Matt Uyttendaele",
                "C Lawrence Zitnick",
                "Zachary W"
            ],
            "title": "Ulissi. AdsorbML: Accelerating adsorption energy calculations with machine learning",
            "venue": "arXiv preprint arXiv:2211.16486,",
            "year": 2022
        },
        {
            "authors": [
                "Tuan Le",
                "Frank No\u00e9",
                "Djork-Arn\u00e9 Clevert"
            ],
            "title": "Equivariant graph attention networks for molecular property prediction",
            "venue": "arXiv preprint arXiv:2202.09891,",
            "year": 2022
        },
        {
            "authors": [
                "Jae Hyeon Lee",
                "Payman Yadollahpour",
                "Andrew Watkins",
                "Nathan C. Frey",
                "Andrew Leaver-Fay",
                "Stephen Ra",
                "Kyunghyun Cho",
                "Vladimir Gligorijevic",
                "Aviv Regev",
                "Richard Bonneau"
            ],
            "title": "Equifold: Protein structure prediction with a novel coarse-grained structure representation",
            "venue": "bioRxiv,",
            "year": 2022
        },
        {
            "authors": [
                "Yi-Lun Liao",
                "Tess Smidt"
            ],
            "title": "Equiformer: Equivariant graph attention transformer for 3d atomistic graphs",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Yi Liu",
                "Limei Wang",
                "Meng Liu",
                "Yuchao Lin",
                "Xuan Zhang",
                "Bora Oztekin",
                "Shuiwang Ji"
            ],
            "title": "Spherical message passing for 3d molecular graphs",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Denghui Lu",
                "Han Wang",
                "Mohan Chen",
                "Lin Lin",
                "Roberto Car",
                "Weinan E",
                "Weile Jia",
                "Linfeng Zhang"
            ],
            "title": "pflops deep potential molecular dynamics simulation of 100 million atoms with ab initio accuracy",
            "venue": "Computer Physics Communications,",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Kurt Miller",
                "Mario Geiger",
                "Tess E. Smidt",
                "Frank No\u00e9"
            ],
            "title": "Relevance of rotationally equivariant convolutions for predicting molecular properties",
            "venue": "arxiv preprint arxiv:2008.08461,",
            "year": 2020
        },
        {
            "authors": [
                "Albert Musaelian",
                "Simon Batzner",
                "Anders Johansson",
                "Lixin Sun",
                "Cameron J. Owen",
                "Mordechai Kornbluth",
                "Boris Kozinsky"
            ],
            "title": "Learning local equivariant representations for large-scale atomistic dynamics. arxiv preprint arxiv:2204.05249, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Maho Nakata",
                "Tomomi Shimazaki"
            ],
            "title": "Pubchemqc project: A large-scale first-principles electronic structure database for data-driven chemistry",
            "venue": "Journal of chemical information and modeling,",
            "year": 2017
        },
        {
            "authors": [
                "Saro Passaro",
                "C Lawrence Zitnick"
            ],
            "title": "Reducing SO(3) Convolutions to SO(2) for Efficient Equivariant GNNs",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Zhuoran Qiao",
                "Matthew Welborn",
                "Animashree Anandkumar",
                "Frederick R. Manby",
                "Thomas F. Miller"
            ],
            "title": "OrbNet: Deep learning for quantum chemistry using symmetry-adapted atomic-orbital features",
            "venue": "The Journal of Chemical Physics,",
            "year": 2020
        },
        {
            "authors": [
                "Joshua A Rackers",
                "Lucas Tecot",
                "Mario Geiger",
                "Tess E Smidt"
            ],
            "title": "A recipe for cracking the quantum scaling limit with machine learned electron densities",
            "venue": "Machine Learning: Science and Technology,",
            "year": 2023
        },
        {
            "authors": [
                "Prajit Ramachandran",
                "Barret Zoph",
                "Quoc V. Le"
            ],
            "title": "Searching for activation functions",
            "venue": "arXiv preprint arXiv:1710.05941,",
            "year": 2017
        },
        {
            "authors": [
                "Raghunathan Ramakrishnan",
                "Pavlo O Dral",
                "Matthias Rupp",
                "O Anatole von Lilienfeld"
            ],
            "title": "Quantum chemistry structures and properties of 134 kilo molecules",
            "venue": "Scientific Data,",
            "year": 2014
        },
        {
            "authors": [
                "Albert Reuther",
                "Jeremy Kepner",
                "Chansup Byun",
                "Siddharth Samsi",
                "William Arcand",
                "David Bestor",
                "Bill Bergeron",
                "Vijay Gadepally",
                "Michael Houle",
                "Matthew Hubbell",
                "Michael Jones",
                "Anna Klein",
                "Lauren Milechin",
                "Julia Mullen",
                "Andrew Prout",
                "Antonio Rosa",
                "Charles Yee",
                "Peter Michaleas"
            ],
            "title": "Interactive supercomputing on 40,000 cores for machine learning and data analysis",
            "venue": "IEEE High Performance extreme Computing Conference (HPEC),",
            "year": 2018
        },
        {
            "authors": [
                "Lars Ruddigkeit",
                "Ruud van Deursen",
                "Lorenz C. Blum",
                "Jean-Louis Reymond"
            ],
            "title": "Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17",
            "venue": "Journal of Chemical Information and Modeling,",
            "year": 2012
        },
        {
            "authors": [
                "Alvaro Sanchez-Gonzalez",
                "Jonathan Godwin",
                "Tobias Pfaff",
                "Rex Ying",
                "Jure Leskovec",
                "Peter W. Battaglia"
            ],
            "title": "Learning to simulate complex physics with graph networks",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "V\u00edctor Garcia Satorras",
                "Emiel Hoogeboom",
                "Max Welling"
            ],
            "title": "E(n) equivariant graph neural networks",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "K.T. Sch\u00fctt",
                "P.-J. Kindermans",
                "H.E. Sauceda",
                "S. Chmiela",
                "A. Tkatchenko",
                "K.-R. M\u00fcller"
            ],
            "title": "Schnet: A continuous-filter convolutional neural network for modeling quantum interactions",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2017
        },
        {
            "authors": [
                "Kristof T. Sch\u00fctt",
                "Oliver T. Unke",
                "Michael Gastegger"
            ],
            "title": "Equivariant message passing for the prediction of tensorial properties and molecular spectra",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "Yu Shi",
                "Shuxin Zheng",
                "Guolin Ke",
                "Yifei Shen",
                "Jiacheng You",
                "Jiyan He",
                "Shengjie Luo",
                "Chang Liu",
                "Di He",
                "Tie-Yan Liu"
            ],
            "title": "Benchmarking graphormer on large-scale molecular modeling datasets. arxiv preprint arxiv:2203.04810, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Muhammed Shuaibi",
                "Adeesh Kolluru",
                "Abhishek Das",
                "Aditya Grover",
                "Anuroop Sriram",
                "Zachary Ulissi",
                "C. Lawrence Zitnick"
            ],
            "title": "Rotation invariant graph neural networks using spin convolutions",
            "venue": "arxiv preprint arxiv:2106.09575,",
            "year": 2021
        },
        {
            "authors": [
                "Anuroop Sriram",
                "Abhishek Das",
                "Brandon M Wood",
                "C. Lawrence Zitnick"
            ],
            "title": "Towards training billion parameter graph neural networks for atomic simulations",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "title": "Dropout: A simple way to prevent neural networks from overfitting",
            "venue": "Journal of Machine Learning Research,",
            "year": 2014
        },
        {
            "authors": [
                "Philipp Th\u00f6lke",
                "Gianni De Fabritiis"
            ],
            "title": "Equivariant transformers for neural network based molecular potentials",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Nathaniel Thomas",
                "Tess E. Smidt",
                "Steven Kearnes",
                "Lusann Yang",
                "Li Li",
                "Kai Kohlhoff",
                "Patrick Riley"
            ],
            "title": "Tensor field networks: Rotation- and translation-equivariant neural networks for 3d point clouds",
            "venue": "arxiv preprint arXiv:1802.08219,",
            "year": 2018
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Matthijs Douze",
                "Francisco Massa",
                "Alexandre Sablayrolles",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "arXiv preprint arXiv:2012.12877,",
            "year": 2020
        },
        {
            "authors": [
                "Raphael J.L. Townshend",
                "Brent Townshend",
                "Stephan Eismann",
                "Ron O. Dror"
            ],
            "title": "Geometric prediction: Moving beyond scalars",
            "venue": "arXiv preprint arXiv:2006.14163,",
            "year": 2020
        },
        {
            "authors": [
                "Richard Tran",
                "Janice Lan",
                "Muhammed Shuaibi",
                "Brandon Wood",
                "Siddharth Goyal",
                "Abhishek Das",
                "Javier Heras-Domingo",
                "Adeesh Kolluru",
                "Ammar Rizvi",
                "Nima Shoghi",
                "Anuroop Sriram",
                "Zachary Ulissi",
                "C. Lawrence Zitnick"
            ],
            "title": "The open catalyst 2022 (oc22) dataset and challenges for oxide electrocatalysis",
            "venue": "arXiv preprint arXiv:2206.08917,",
            "year": 2022
        },
        {
            "authors": [
                "Oliver T. Unke",
                "Markus Meuwly"
            ],
            "title": "PhysNet: A neural network for predicting energies, forces, dipole moments, and partial charges",
            "venue": "Journal of Chemical Theory and Computation,",
            "year": 2019
        },
        {
            "authors": [
                "Oliver Thorsten Unke",
                "Mihail Bogojeski",
                "Michael Gastegger",
                "Mario Geiger",
                "Tess Smidt",
                "Klaus Robert Muller"
            ],
            "title": "SE(3)-equivariant prediction of molecular wavefunctions and electronic densities",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2017
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio"
            ],
            "title": "Graph attention networks",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Maurice Weiler",
                "Mario Geiger",
                "Max Welling",
                "Wouter Boomsma",
                "Taco Cohen"
            ],
            "title": "3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Daniel E. Worrall",
                "Stephan J. Garbin",
                "Daniyar Turmukhambetov",
                "Gabriel J. Brostow"
            ],
            "title": "Harmonic networks: Deep translation and rotation equivariance",
            "venue": "arxiv preprint arxiv:1612.04642,",
            "year": 2016
        },
        {
            "authors": [
                "Tian Xie",
                "Jeffrey C. Grossman"
            ],
            "title": "Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties",
            "venue": "Physical Review Letters,",
            "year": 2018
        },
        {
            "authors": [
                "Chengxuan Ying",
                "Tianle Cai",
                "Shengjie Luo",
                "Shuxin Zheng",
                "Guolin Ke",
                "Di He",
                "Yanming Shen",
                "Tie-Yan Liu"
            ],
            "title": "Do transformers really perform badly for graph representation",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Sheheryar Zaidi",
                "Michael Schaarschmidt",
                "James Martens",
                "Hyunjik Kim",
                "Yee Whye Teh",
                "Alvaro Sanchez-Gonzalez",
                "Peter Battaglia",
                "Razvan Pascanu",
                "Jonathan Godwin"
            ],
            "title": "Pre-training via denoising for molecular property prediction",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "A. Zee"
            ],
            "title": "Group Theory in a Nutshell for Physicists",
            "year": 2016
        },
        {
            "authors": [
                "Linfeng Zhang",
                "Jiequn Han",
                "Han Wang",
                "Roberto Car",
                "Weinan E"
            ],
            "title": "Deep potential molecular dynamics: A scalable model with the accuracy of quantum mechanics",
            "venue": "Phys. Rev. Lett.,",
            "year": 2018
        },
        {
            "authors": [
                "Larry Zitnick",
                "Abhishek Das",
                "Adeesh Kolluru",
                "Janice Lan",
                "Muhammed Shuaibi",
                "Anuroop Sriram",
                "Zachary Ulissi",
                "Brandon Wood"
            ],
            "title": "Spherical channels for modeling atomic interactions",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In recent years, machine learning (ML) models have shown promising results in accelerating and scaling high-accuracy but compute-intensive quantum mechanical calculations by effectively accounting for key features of atomic systems, such as the discrete nature of atoms, and Euclidean and permutation symmetries (Gilmer et al., 2017; Zhang et al., 2018; Jia et al., 2020; Gasteiger et al., 2020a; Batzner et al., 2022; Lu et al., 2021; Unke et al., 2021; Sriram et al., 2022; Rackers et al., 2023; Lan et al., 2022). By bringing down computational costs from hours or days to fractions of seconds, these methods enable new insights in many applications such as molecular simulations, material design and drug discovery. A promising class of ML models that have enabled this progress is equivariant graph neural networks (GNNs) (Thomas et al., 2018; Weiler et al., 2018; Kondor et al., 2018; Fuchs et al., 2020; Batzner et al., 2022; Brandstetter et al., 2022; Musaelian et al., 2022; Liao & Smidt, 2023; Passaro & Zitnick, 2023).\nEquivariant GNNs treat 3D atomistic systems as graphs, and incorporate inductive biases such that their internal representations and predictions are equivariant to 3D translations, rotations and optionally inversions. Specifically, they build up equivariant features of each node as vector spaces of irreducible representations (or irreps) and have interactions or message passing between nodes based on equivariant operations such as tensor products. Recent works on equivariant Transformers, specifically Equiformer (Liao & Smidt, 2023), have shown the efficacy of applying Transformers (Vaswani et al., 2017), which have previously enjoyed widespread success in computer vision (Carion et al., 2020; Dosovitskiy et al., 2021; Touvron et al., 2020), language (Devlin et al., 2019; Brown et al., 2020), and graphs (Dwivedi & Bresson, 2020; Kreuzer et al., 2021; Ying et al., 2021; Shi et al., 2022), to this domain of 3D atomistic systems.\nA bottleneck in scaling Equiformer as well as other equivariant GNNs is the computational complexity of tensor products, especially when we increase the maximum degree of irreps Lmax. This limits these models to use small values of Lmax (e.g., Lmax \u010f 3), which consequently limits their performance. Higher degrees can better capture angular resolution and directional information, which is critical to accurate prediction of atomic energies and forces (Batzner et al., 2022; Zitnick et al., 2022; Passaro & Zitnick, 2023). To this end, eSCN (Passaro & Zitnick, 2023) recently proposes efficient convolutions\n\u201cDTP\u201d denotes depth-wise tensor products used in Equiformer. Gray cells indicate intermediate irreps features.\nto reduce SOp3q tensor products to SOp2q linear operations, bringing down the computational cost from OpL6maxq to OpL3maxq and enabling scaling to larger values of Lmax (e.g., Lmax up to 8). However, except using efficient convolutions for higher Lmax, eSCN still follows SEGNN-like message passing network (Brandstetter et al., 2022) design, and Equiformer has been shown to improve upon SEGNN. More importantly, this ability to use higher Lmax challenges whether the previous design of equivariant Transformers can scale well to higher-degree representations.\nIn this paper, we are interested in adapting eSCN convolutions for higher-degree representations to equivariant Transformers. We start with Equiformer (Liao & Smidt, 2023) and replace SOp3q convolutions with eSCN convolutions. We find that naively incorporating eSCN convolutions does not result in better performance than the original eSCN model. Therefore, to better leverage the power of higher degrees, we propose three architectural improvements \u2013 attention re-normalization, separable S2 activation and separable layer normalization. Putting these all together, we propose EquiformerV2, which is developed on large and diverse OC20 dataset (Chanussot* et al., 2021). Experiments on OC20 show that EquiformerV2 outperforms previous state-of-the-art methods with improvements of up to 9% on forces and 4% on energies, and offers better speed-accuracy trade-offs compared to existing invariant and equivariant GNNs. Additionally, when used in the AdsorbML algorithm (Lan et al., 2022) for performing adsorption energy calculations, EquiformerV2 achieves the highest success rate and 2\u02c6 reduction in DFT calculations to achieve comparable adsorption energy accuracies as previous methods. Furthermore, EquiformerV2 trained on only OC22 (Tran* et al., 2022) dataset outperforms GemNet-OC (Gasteiger et al., 2022) trained on both OC20 and OC22 datasets, achieving much better data efficiency. Finally, we compare EquiformerV2 with Equiformer on QM9 dataset (Ramakrishnan et al., 2014; Ruddigkeit et al., 2012) and OC20 S2EF-2M dataset to better understand the performance gain of higher degrees and the improved architecture."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "We present background relevant to this work here and discuss related works in Sec. B.\n2.1 SEp3q{Ep3q-EQUIVARIANT NEURAL NETWORKS We discuss the relevant background of SEp3q{Ep3q-equivariant neural networks here. Please refer to Sec. A in appendix for more details of equivariance and group theory.\nIncluding equivariance in neural networks can serve as a strong prior knowledge, which can therefore improve data efficiency and generalization. Equivariant neural networks use equivariant irreps features built from vector spaces of irreducible representations (irreps) to achieve equivariance to 3D rotation. Specifically, the vector spaces are p2L` 1q-dimensional, where degree L is a non-negative integer. L can be intuitively interpreted as the angular frequency of the vectors, i.e., how fast the vectors rotate with respect to a rotation of the coordinate system. Higher L is critical to tasks sensitive to angular\ninformation like predicting forces (Batzner et al., 2022; Zitnick et al., 2022; Passaro & Zitnick, 2023). Vectors of degree L are referred to as type-L vectors, and they are rotated with Wigner-D matrices DpLq when rotating coordinate systems. Euclidean vectors r\u20d7 in R3 can be projected into type-L vectors by using spherical harmonics Y pLqp r\u20d7||r\u20d7|| q. We use order m to index the elements of type-L vectors, where \u00b4L \u010f m \u010f L. We concatenate multiple type-L vectors to form an equivariant irreps feature f . Concretely, f has CL type-L vectors, where 0 \u010f L \u010f Lmax and CL is the number of channels for type-L vectors. In this work, we mainly consider CL \u201c C, and the size of f is pLmax ` 1q2 \u02c6 C. We index f by channel i, degree L, and order m and denote as f pLqm,i . Equivariant GNNs update irreps features by passing messages of transformed irreps features between nodes. To interact different type-L vectors during message passing, we use tensor products, which generalize multiplication to equivariant irreps features. Denoted as bL3L1,L2 , the tensor product uses Clebsch-Gordan coefficients to combine type-L1 vector f pL1q and type-L2 vector gpL2q and produces type-L3 vector hpL3q:\nhpL3qm3 \u201c pf pL1q bL3L1,L2 g pL2qqm3 \u201c L1 \u00ff\nm1\u201c\u00b4L1\nL2 \u00ff\nm2\u201c\u00b4L2\nC pL3,m3q pL1,m1qpL2,m2qf pL1q m1 g pL2q m2 (1)\nwhere m1 denotes order and refers to the m1-th element of f pL1q. Clebsch-Gordan coefficients C\npL3,m3q pL1,m1qpL2,m2q are non-zero only when |L1 \u00b4 L2| \u010f L3 \u010f |L1 ` L2| and thus restrict output vectors to be of certain degrees. We typically discard vectors with L \u0105 Lmax, where Lmax is a hyper-parameter, to prevent vectors of increasingly higher dimensions. In many works, message passing is implemented as equivariant convolutions, which perform tensor products between input irreps features xpL1q and spherical harmonics of relative position vectors Y pL2qp r\u20d7||r\u20d7|| q."
        },
        {
            "heading": "2.2 EQUIFORMER",
            "text": "Equiformer (Liao & Smidt, 2023) is an SEp3q/Ep3q-equivariant GNN that combines the inductive biases of equivariance with the strength of Transformers. First, Equiformer replaces scalar node features with equivariant irreps features to incorporate equivariance. Next, it performs equivariant operations on these irreps features and equivariant graph attention for message passing. These operations include tensor products and equivariant linear operations, equivariant layer normalization (Ba et al., 2016) and gate activation (Weiler et al., 2018). For stronger expressivity in the attention compared to typical Transformers, Equiformer uses non-linear functions for both attention weights and message passing. Additionally, Equiformer incorporates regularization techniques commonly used by Transformers, e.g., dropout (Srivastava et al., 2014) to attention weights (Velic\u030ckovic\u0301 et al., 2018) and stochastic depth (Huang et al., 2016) to the outputs of equivariant graph attention and feed forward networks."
        },
        {
            "heading": "2.3 ESCN CONVOLUTION",
            "text": "eSCN convolutions (Passaro & Zitnick, 2023) are proposed to use SOp2q linear operations for efficient tensor products. We provide an outline and intuition for their method here, please refer to Sec. A.3 and their work (Passaro & Zitnick, 2023) for mathematical details.\nA traditional SOp3q convolution interacts input irreps features xpLiqmi and spherical harmonic projections of relative positions Y pLf qmf pr\u20d7ijq with an SOp3q tensor product with Clebsch-Gordan coefficients C\npLo,moq pLi,miq,pLf ,mf q. The projection Y pLf q mf pr\u20d7ijq becomes sparse if we rotate the relative position vector r\u20d7ij with a rotation matrix Dij to align with the direction of L \u201c 0 and m \u201c 0, which corresponds to the z axis traditionally but the y axis in the conventions of e3nn (Geiger et al., 2022). Concretely, given Dij r\u20d7ij aligned with the y axis, Y pLf q mf pDij r\u20d7ijq \u2030 0 only for mf \u201c 0. If we consider only mf \u201c 0, CpLo,moqpLi,miq,pLf ,mf q can be simplified, and C pLo,moq pLi,miq,pLf ,0q \u2030 0 only when mi \u201c \u02d8mo. Therefore, the original expression depending on mi, mf , and mo is now reduced to only depend on mo. This means we are no longer mixing all integer values of mi and mf , and outputs of order mo are linear combinations of inputs of order \u02d8mo. eSCN convolutions go one step further and replace the remaining non-trivial paths of the SOp3q tensor product with an SOp2q linear operation to allow for additional parameters of interaction between \u02d8mo without breaking equivariance."
        },
        {
            "heading": "3 EQUIFORMERV2",
            "text": "Starting from Equiformer (Liao & Smidt, 2023), we first use eSCN convolutions to scale to higherdegree representations (Sec. 3.1). Then, we propose three architectural improvements, which yield further performance gain when using higher degrees: attention re-normalization (Sec. 3.2), separable S2 activation (Sec. 3.3) and separable layer normalization (Sec. 3.4). Figure 1 illustrates the overall architecture of EquiformerV2 and the differences from Equiformer."
        },
        {
            "heading": "3.1 INCORPORATING ESCN CONVOLUTIONS FOR HIGHER DEGREES",
            "text": "The computational complexity of SOp3q tensor products used in traditional SOp3q convolutions scale unfavorably with Lmax. Because of this, it is impractical for Equiformer to use Lmax \u0105 2 for large-scale datasets like OC20. Since higher Lmax can better capture angular information and are correlated with model expressivity (Batzner et al., 2022), low values of Lmax can lead to limited performance on certain tasks such as predicting forces. Therefore, we replace original tensor products with eSCN convolutions for efficient tensor products, enabling Equiformer to scale up Lmax to 6 or 8 on OC20 dataset. Equiformer uses equivariant graph attention for message passing. The attention consists of depth-wise tensor products, which mix information across different degrees, and linear layers, which mix information between channels of the same degree. Since eSCN convolutions mix information across both degrees and channels, we replace the SOp3q convolution, which involves one depth-wise tensor product layer and one linear layer, with a single eSCN convolutional layer, which consists of a rotation matrix Dij and an SOp2q linear layer as shown in Figure 1b."
        },
        {
            "heading": "3.2 ATTENTION RE-NORMALIZATION",
            "text": "Equivariant graph attention in Equiformer uses tensor products to project node embeddings xi and xj , which contain vectors of degrees from 0 to Lmax, to scalar features f p0q ij and applies non-linear functions to f p0qij for attention weights aij . We propose attention re-normalization and introduce one additional layer normalization (LN) (Ba et al., 2016) before non-linear functions. Specifically, given f p0qij , we first apply LN and then use one leaky ReLU layer and one linear layer to calculate zij \u201c wJa LeakyReLUpLNpf p0q ij qq and aij \u201c softmaxjpzijq \u201c exppzijq \u0159\nkPNpiq exppzikq , where wa is\na learnable vector of the same dimension as f p0qij . The motivation is similar to ViT-22B (Dehghani et al., 2023), where they find that they need an additional layer normalization to stabilize training when increasing widths. When we scale up Lmax, we effectively increase the number of input channels for calculating f p0qij . By normalizing f p0q ij , the additional LN can make sure the inputs to subsequent non-linear functions and softmax operations still lie within the same range as lower Lmax is used. This empirically improves the performance as shown in Table 1a."
        },
        {
            "heading": "3.3 SEPARABLE S2 ACTIVATION",
            "text": "The gate activation (Weiler et al., 2018) used by Equiformer applies sigmoid activation to scalar features to obtain non-linear weights and then multiply irreps features of degree \u0105 0 with nonlinear weights to add non-linearity to equivariant features. The activation only accounts for the interaction from vectors of degree 0 to those of degree \u0105 0 and can be sub-optimal when we scale up Lmax. To better mix the information across degrees, SCN (Zitnick et al., 2022) and eSCN adopt S2 activation (Cohen et al., 2018). The activation first converts vectors of all degrees to point samples on a sphere for each channel, applies unconstrained functions F to those samples, and finally convert them back to vectors. Specifically, given an input irreps feature x P RpLmax`1q2\u02c6C , the output is y \u201c G\u00b41pF pGpxqqq, where G denotes the conversion from vectors to point samples on a sphere, F\ncan be typical SiLU activation (Elfwing et al., 2017; Ramachandran et al., 2017) or typical MLPs, and G\u00b41 is the inverse of G. We provide more details of S2 activation in Sec. A.4.\nHowever, we find that directly replacing the gate activation with S2 activation in Equiformer results in large gradients and training instability (Index 3 in Table 1a). To address the issue, we propose separable S2 activation, which separates activation for vectors of degree 0 and those of degree \u0105 0. Similar to gate activation, we have more channels for vectors of degree 0. As shown in Figure 2c, we apply a SiLU activation to the first part of vectors of degree 0, and the second part of vectors of degree 0 are used for S2 activation along with vectors of higher degrees. After S2 activation, we concatenate the first part of vectors of degree 0 with vectors of degrees \u0105 0 as the final output and ignore the second part of vectors of degree 0. Separating the activation for vectors of degree 0 and those of degree \u0105 0 prevents large gradients, enabling using more expressive S2 activation for better performance. Additionally, we use separable S2 activation in feed forward networks (FFNs). Figure 2 illustrates the differences between gate activation, S2 activation and separable S2 activation."
        },
        {
            "heading": "3.4 SEPARABLE LAYER NORMALIZATION",
            "text": "Equivariant layer normalization used by Equiformer normalizes vectors of different degrees independently. However, it potentially ignores the relative importance of different degrees since the relative magnitudes between different degrees become the same after the normalization. Therefore, instead of performing normalization to each degree independently, motivated by the separable S2 activation mentioned above, we propose separable layer normalization (SLN), which separates normalization for vectors of degree 0 and those of degrees \u0105 0. Mathematically, let x P RpLmax`1q2\u02c6C denote an input irreps feature of maximum degree Lmax and C channels, and x pLq m,i denote the L-th degree, m-th order and i-th channel of x. SLN calculates the output y as follows. For L \u201c 0, yp0q \u201c \u03b3p0q \u02dd \u00b4\nxp0q\u00b4\u00b5p0q \u03c3p0q\n\u00af\n` \u03b2p0q, where \u00b5p0q \u201c 1C \u0159C i\u201c1 x p0q 0,i and \u03c3\np0q \u201c b\n1 C \u0159C i\u201c1px p0q 0,i \u00b4 \u00b5p0qq2.\nFor L \u0105 0, ypLq \u201c \u03b3pLq \u02dd \u00b4 xpLq\n\u03c3pL\u01050q\n\u00af , where \u03c3pL\u01050q \u201c b\n1 Lmax \u0159Lmax L\u201c1 ` \u03c3pLq \u02d82 and \u03c3pLq \u201c c\n1 C \u0159C i\u201c1 1 2L`1 \u0159L m\u201c\u00b4L\n\u00b4\nx pLq m,i\n\u00af2\n. \u03b3p0q, \u03b3pLq, \u03b2p0q P RC are learnable parameters, \u00b5p0q and \u03c3p0q\nare mean and standard deviation of vectors of degree 0, \u03c3pLq and \u03c3pL\u01050q are root mean square values (RMS), and \u02dd denotes element-wise product. Figure 3 compares how \u00b5p0q, \u03c3p0q, \u03c3pLq and \u03c3pL\u01050q are calculated in equivariant layer normalization and SLN. Preserving the relative magnitudes between degrees \u0105 0 improves performance as shown in Table 1a."
        },
        {
            "heading": "3.5 OVERALL ARCHITECTURE",
            "text": "Equivariant Graph Attention. Figure 1b illustrates equivariant graph attention after the above modifications. Given node embeddings xi and xj , we first concatenate them along the channel dimension and then rotate them with rotation matrices Dij based on their relative positions or edge directions r\u20d7ij . We replace depth-wise tensor products and linear layers between xi, xj and fij with a single SOp2q linear layer. To consider the information of relative distances ||r\u20d7ij ||, we transform ||r\u20d7ij || with a radial function to obtain edge distance embeddings and then multiply the edge distance embeddings with concatenated node embeddings before the first SOp2q linear layer. We split the outputs fij of the first SOp2q linear layer into two parts. The first part is scalar features f p0qij , which only contains vectors of degree 0, and the second part is irreps features f pLqij and includes vectors of all degrees up to Lmax. As mentioned in Sec. 3.2, we first apply an additional LN to f p0q ij and then follow the design of Equiformer by applying one leaky ReLU layer, one linear layer and a final softmax layer to obtain attention weights aij . As for value vij , we replace the gate activation with separable S2 activation with F being a single SiLU activation and then apply the second SOp2q linear layer. While in Equiformer, the message mij sent from node j to node i is mij \u201c aij \u02c6 vij , here we need to rotate aij \u02c6 vij back to original coordinate frames and the message mij becomes D\u00b41ij paij \u02c6 vijq. Finally, we can perform h parallel equivariant graph attention functions given fij . The h different outputs are concatenated and projected with a linear layer to become the final output yi. Parallelizing attention functions and concatenating can be implemented with \u201cReshape\u201d.\nFeed Forward Network. As shown in Figure 1d, we replace the gate activation with separable S2 activation. The function F consists of a two-layer MLP, with each linear layer followed by SiLU, and a final linear layer.\nare averaged over the four validation sub-splits. The base model setting is marked in gray .\nEmbedding. This module consists of atom embedding and edge-degree embedding. The former is the same as that in Equiformer. For the latter, as depicted in the right branch in Figure 1c, we replace original linear layers and depth-wise tensor products with a single SOp2q linear layer followed by a rotation matrix D\u00b41ij . Similar to equivariant graph attention, we consider the information of relative distances by multiplying the outputs of the SOp2q linear layer with edge distance embeddings. Radial Basis and Radial Function. We represent relative distances ||r\u20d7ij || with a finite radial basis like Gaussian radial basis functions (Sch\u00fctt et al., 2017) to capture their subtle changes. We transform radial basis with a learnable radial function to generate edge distance embeddings. The function consists of a two-layer MLP, with each linear layer followed by LN and SiLU, and a final linear layer. Output Head. To predict scalar quantities like energy, we use a feed forward network to transform irreps features on each node into a scalar and then sum over all nodes. For predicting atom-wise forces, we use a block of equivariant graph attention and treat the output of degree 1 as our predictions."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 OC20 DATASET",
            "text": "Our main experiments focus on the large and diverse OC20 dataset (Chanussot* et al., 2021). Please refer to Sec. D.1 for details on the dataset. We first conduct ablation studies on EquiformerV2 trained on the 2M split of OC20 S2EF dataset (Sec. 4.1.1). Then, we report the results of training All and All+MD splits (Sec. 4.1.2). Additionally, we investigate the performance of EquiformerV2 when used in the AdsorbML algorithm (Lan et al., 2022) (Sec. 4.1.3). Please refer to Sec. C and D for details of models and training."
        },
        {
            "heading": "4.1.1 ABLATION STUDIES",
            "text": "Architectural Improvements. In Table 1a, we start with incorporating eSCN convolutions into Equiformer for higher-degree representations (Index 1) and then ablate the three proposed architectural improvements. First, with attention re-normalization, energy mean absolute errors (MAE) improve by 2.4%, while force MAE are about the same (Index 1 and 2). Second, we replace the gate activation with S2 activation, but that does not converge (Index 3). With the proposed S2 activation, we stabilize training and successfully leverage more expressive activation to improve force MAE (Index 4). Third, replacing equivariant layer normalization with separable layer normalization further improves force MAE (Index 5). We note that simply incorporating eSCN convolutions into Equiformer and using higher degrees (Index 1) do not result in better performance than the original eSCN baseline (Index 8), and that the proposed architectural changes are necessary. We additionally compare the performance gain of architectural improvements with that of training for longer. Attention re-normalization (Index 1 and 2) improves energy MAE by the same amount as increasing training epochs from 12 to 20\n(Index 5 and 7). The improvement of SLN (Index 4 and 5) in force MAE is about 40% of that of increasing training epochs from 12 to 20 (Index 5 and 7), and SLN is about 6% faster in training. We also conduct the ablation studies on the QM9 dataset and summarize the results in Sec. F.3.\nScaling of Parameters. In Tables 1c, 1d, 1e, we vary the maximum degree Lmax, the maximum order Mmax, and the number of Transformer blocks and compare with equivalent eSCN variants. Across all experiments, EquiformerV2 performs better than its eSCN counterparts. Besides, while one might intuitively expect higher resolutions and larger models to perform better, this is only true for EquiformerV2, not eSCN. For example, increasing Lmax from 6 to 8 or Mmax from 3 to 4 degrades the performance of eSCN on energy predictions but helps that of EquiformerV2. In Table 1b, we show that increasing the training epochs from 12 to 30 epochs can consistently improve performance.\nSpeed-Accuracy Trade-offs. We compare trade-offs between inference speed or training time and force MAE among prior works and EquiformerV2 and summarize the results in Figure 4a and Figure 4b. EquifomerrV2 achieves the lowest force MAE across a wide range of inference speed and training cost."
        },
        {
            "heading": "4.1.2 MAIN RESULTS",
            "text": "Table 2 reports results on the test splits for all the three tasks of OC20 averaged over all four sub-splits. Models are trained on either OC20 S2EF-All or S2EF-All+MD splits. All test results are computed via the EvalAI evaluation server1. We train EquiformerV2 of two sizes, one with 153M parameters and the other with 31M parameters. When trained on the S2EF-All+MD split, EquiformerV2 (\u03bbE \u201c 4, 153M) improves previous state-of-the-art S2EF energy MAE by 4%, S2EF force MAE by 9%, IS2RS Average Forces below Threshold (AFbT) by absolute 4% and IS2RE energy MAE by 4%. In particular, the improvement in force predictions is significant. Going from SCN to eSCN, S2EF test force MAE improves from 17.2 meV/\u00c5 to 15.6 meV/\u00c5 , largely due to replacing approximate equivariance in SCN with strict equivariance in eSCN during message passing. Similarly, by scaling up the degrees of representations in Equiformer, EquiformerV2 (\u03bbE \u201c 4, 153M) further improves force MAE to 14.2 meV/\u00c5, which is similar to the gain of going from SCN to eSCN. Additionally, the smaller EquiformerV2 (\u03bbE \u201c 4, 31M) improves upon previously best results for all metrics except IS2RS AFbT and achieves comparable training throughput to the fastest GemNet-OC-L-E. Although the training time of EquiformerV2 is higher here, we note that this is because training EquiformerV2\n1eval.ai/web/challenges/challenge-page/712\nfor longer keeps improving performance and that we already demonstrate EquiformerV2 achieves better trade-offs between force MAE and speed."
        },
        {
            "heading": "4.1.3 ADSORBML RESULTS",
            "text": "Lan et al. (2022) recently proposes the AdsorbML algorithm, wherein they show that recent state-ofthe-art GNNs can achieve more than 1000\u02c6 speedup over DFT relaxations at computing adsorption energies within a 0.1eV margin of DFT results with an 87% success rate. This is done by using OC20-trained models to perform structure relaxations for an average 90 configurations of an adsorbate placed on a catalyst surface, followed by DFT single-point calculations for the top-k structures with lowest predicted relaxed energies, as a proxy for calculating the global energy minimum or adsorption energy. We refer readers to Sec. D.4 and the work (Lan et al., 2022) for more details. We benchmark AdsorbML with EquiformerV2, and Table 3 shows that EquiformerV2 (\u03bbE \u201c 4, 153M) improves over SCN by a significant margin, with 8% and 5% absolute improvements at k \u201c 1 and k \u201c 2, respectively. Moreover, EquiformerV2 (\u03bbE \u201c 4, 153M) at k \u201c 2 is more accurate at adsorption energy calculations than all the other models even at k \u201c 5, thus requiring at least 2\u02c6 fewer DFT calculations. Since the speedup is with respect to using DFT for structure relaxations and that ML models are much faster than DFT, the speedup is dominated by the final DFT single-point calculations and ML models with the same value of k have roughly the same speedup. To better understand the speed-accuracy trade-offs of different models, we also report average GPU-seconds of running one structure relaxation in Table 3. Particularly, EquiformerV2 (\u03bbE \u201c 4, 31M) improves upon previous methods while being 3.7\u02c6 to 9.8\u02c6 faster than GemNet-OC-MD-Large and SCN, respectively."
        },
        {
            "heading": "4.2 OC22 DATASET",
            "text": "Dataset. The OC22 dataset (Tran* et al., 2022) focuses on oxide electrocatalysis. One crucial difference between OC22 and OC20 is that the energies in OC22 are DFT total energies. DFT total energies are harder to predict but are the most general and closest to a DFT surrogate, offering the flexibility to study property prediction beyond adsorption energies. Similar to OC20, the tasks in OC22 are S2EF-Total and IS2RE-Total. We train models on the OC22 S2EF-Total dataset and evaluate them on energy and force MAE on the S2EF-Total validation and test splits. We use the trained models to perform structural relaxations and predict relaxed energy. Relaxed energy predictions are evaluated on the IS2RE-Total test split.\nTraining Details. Please refer to Section E.1 for details on architectures, hyper-parameters and training time.\nResults. We train two EquiformerV2 models with different energy coefficients \u03bbE and force coefficients \u03bbF . We follow the practice of OC22 models and use linear reference (Tran* et al., 2022). The results are summarized in Table 4. EquiformerV2 improves upon previous models on all the tasks. EquiformerV2 (\u03bbE \u201c 4, \u03bbF \u201c 100) trained on only OC22 achieves better results on all the tasks than GemNet-OC trained on both OC20 and OC22. We note that OC22 contains about 8.4M structures and OC20 contains about 130M structures, and therefore EquiformerV2 demonstrates significatly better data efficiency. Additionally, the performance gap between eSCN and EquiformerV2 is larger than that on OC20, suggesting that more complicated structures can benefit more from the proposed architecture. When trained on OC20 S2EF-All+MD, EquiformerV2 (\u03bbE \u201c 4, 153M) improves upon eSCN by 4% on energy MAE and 9% on force MAE. For OC22, EquiformerV2 (\u03bbE \u201c 4, \u03bbF \u201c 100) improves upon eSCN by 18.9% on average energy MAE and 8.9% on average force MAE."
        },
        {
            "heading": "4.3 COMPARISON WITH EQUIFORMER",
            "text": "QM9 Dataset. We follow the setting of Equiformer and train EquiformerV2 on the QM9 dataset (Ramakrishnan et al., 2014; Ruddigkeit et al., 2012) and summarize the results in Table 5. The performance gain of using higher degrees and the improved architecture is not as significant as that on OC20 and OC22 datasets. This, however, is not surprising. The training set of QM9 contains only 110k examples, which is much less than OC20 S2EF-2M with 2M examples and OC22 with 8.4M examples. Moreover, QM9 has much less numbers of atoms in each example and much less diverse atom types, and each example has less angular variations. Nevertheless, EquiformerV2 achieves better results than Equiformer on 9 out of the 12 tasks and is therefore the overall best performing model. We additionally train EquiformerV2 with Noisy Nodes (Godwin et al., 2022) to better understand the gain of higher degrees in Sec. F.1.\nOC20 S2EF-2M Dataset. We use similar configurations (e.g., numbers of blocks and numbers of channels) and train Equiformer on OC20 S2EF-2M dataset for the same number of epochs as training EquiformerV2. We vary Lmax for both Equiformer and EquiformerV2 and compare the results in Table 6. For Lmax \u201c 2, EquiformerV2 is 2.3\u02c6 faster than Equiformer since EquiformerV2 uses eSCN convolutions for efficient SOp3q convolutions. Additionally, EquiformerV2 achieves better force MAE and similar energy MAE, demonstrating the effectiveness of the proposed improved architecture. For Lmax \u0105 2, we encounter out-of-memory errors when training Equiformer even after we reduce the number of blocks and use the batch size \u201c 1. In contrast, We can easily train EquiformerV2 with Lmax up to 6. When increasing Lmax from 2 to 4, EquiformerV2 achieves lower energy MAE and significantly lower force MAE than Equiformer and requires 1.4\u02c6 less training time. The comparison suggests that complicated datasets have more performance to gain from using more expressive models, enabling better performance and lower computational cost.\nDiscussion. One limitation of EquiformerV2 is that the performance gains brought by scaling to higher degrees and the proposed architectural improvements can depend on datasets and tasks. For small datasets like QM9, the performance gain is not significant. We additionally compare Equiformer and EquiformerV2 on OC20 IS2RE dataset in Sec. D.5. For different tasks, the improvements are also different, and force predictions benefit more from better expressivity than energy predictions. We note that the first issue can be mitigated by first pre-training on large datasets like OC20 and PCQM4Mv2 (Nakata & Shimazaki, 2017) optionally via denoising (Godwin et al., 2022; Zaidi et al., 2023) and then fine-tuning on smaller datasets. The second issue might be mitigated by combining DFT with ML models. For example, AdsorbML uses ML forces for structural relaxations and a single-point DFT for calculating the final relaxed energies."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we investigate how equivariant Transformers can be scaled up to higher degrees of equivariant representations. We start by replacing SOp3q convolutions in Equiformer with eSCN convolutions, and then we propose three architectural improvements to better leverage the power of higher degrees \u2013 attention re-normalization, separable S2 activation and separable layer normalization. With these modifications, we propose EquiformerV2, which outperforms state-of-the-art methods on all the tasks on the OC20 and OC22 datasets, improves speed-accuracy trade-offs, and achieves the best success rate when used in AdsorbML. We also compare EquiformerV2 with Equiformer to better understand the performance gain brought by higher degrees and the improved architecture."
        },
        {
            "heading": "6 ETHICS STATEMENT",
            "text": "EquiformerV2 achieves more accurate approximation of quantum mechanical calculations and demonstrates one further step toward being able to replace DFT compute force fields with machine learned ones for practical applications in chemistry and material science. We hope these promising results will encourage the community to make further progress in applications like material design and drug discovery, rather than use these methods for adversarial purposes. We note that these methods only facilitate the identification of molecules or materials with specific properties; there remain substantial hurdles to synthesize and deploy such molecules or materials at scale. Finally, we note that the proposed method is general and can be applied to different problems like protein structure prediction (Lee et al., 2022) as long as inputs can be modeled as 3D graphs."
        },
        {
            "heading": "7 REPRODUCIBILITY STATEMENT",
            "text": "We include details on architectures, hyper-parameters and training time in Sec. D.2 (OC20), Sec. E.1 (OC22) and Sec. F.2 (QM9).\nThe code for reproducing the results of EquiformerV2 trained on OC20 S2EF-2M and QM9 datasets is available at https://github.com/atomicarchitects/equiformer_v2."
        },
        {
            "heading": "ACKNOWLEDGEMENT",
            "text": "We thank Larry Zitnick and Saro Passaro for helpful discussions. We also thank Muhammed Shuaibi for helping with the DFT evaluations for AdsorbML (Lan et al., 2022). We acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center (Reuther et al., 2018) for providing high performance computing and consultation resources that have contributed to the research results reported within this paper.\nYi-Lun Liao and Tess Smidt were supported by DOE ICDI grant DE-SC0022215."
        },
        {
            "heading": "APPENDIX",
            "text": "A Additional background A.1 Group theory A.2 Equivariance A.3 eSCN convolution A.4 S2 activation\nB Related works B.1 SE(3)/E(3)-equivariant GNNs B.2 Invariant GNNs\nC Details of architecture D Details of experiments on OC20\nD.1 Detailed description of OC20 dataset D.2 Training details D.3 Details of running relaxations D.4 Details of AdsorbML D.5 Additional comparison with Equiformer on OC20 IS2RE\nE Details of experiments on OC22 E.1 Training details\nF Details of experiments on QM9 F.1 Additional results of training with Noisy Nodes F.2 Training details F.3 Ablation study on architectural improvements"
        },
        {
            "heading": "A ADDITIONAL BACKGROUND",
            "text": "We first provide relevant mathematical background on group theory and equivariance. We note that most of the content is adapted from Equiformer (Liao & Smidt, 2023) and that these works (Zee, 2016; Dresselhaus et al., 2007) have more in-depth and pedagogical discussions. Then, we provide mathematical details of eSCN convolutions."
        },
        {
            "heading": "A.1 GROUP THEORY",
            "text": "Definition of Groups. A group is an algebraic structure that consists of a set G and a binary operator \u02dd : G \u02c6 G \u00d1 G. Typically denoted as G, groups satisfy the following four axioms:\n1. Closure: g \u02dd h P G for all g, h P G. 2. Identity: There exists an identity element e P G such that g \u02dd e \u201c e \u02dd g \u201c g for all g P G. 3. Inverse: For each g P G, there exists an inverse element g\u00b41 P G such that g \u02dd g\u00b41 \u201c\ng\u00b41 \u02dd g \u201c e. 4. Associativity: g \u02dd h \u02dd i \u201c pg \u02dd hq \u02dd i \u201c g \u02dd ph \u02dd iq for all g, h, i P G.\nIn this work, we consider 3D Euclidean symmetry, and relevant groups are:\n1. The Euclidean group in three dimensions Ep3q: 3D rotation, translation and inversion. 2. The special Euclidean group in three dimensions SEp3q: 3D rotation and translation. 3. The orthogonal group in three dimensions Op3q: 3D rotation and inversion. 4. The special orthogonal group in three dimensions SOp3q: 3D rotation.\nSince eSCN (Passaro & Zitnick, 2023) and this work only consider equivariance to 3D rotation and invariance to 3D translation but not inversion, we mainly discuss SEp3q-equivariance in the main text and in appendix and note that more details of Ep3q-equivariance can be found in the work of Equiformer (Liao & Smidt, 2023).\nGroup Representations. Given a vector space X , the way a group G acts on X is given by the group representation DX . DX is parameterized by g P G, with DXpgq : X \u00d1 X . Group representations DX are invertible matrices, and group transformations, or group actions, take the form of matrix multiplications. This definition of group representations satisfies the requirements of groups, including associativity, DpgqDphq \u201c Dpg \u02dd hq for all g, h P G. We say that the two group representations Dpgq and D1pgq are equivalent if there exists a change-of-basis N \u02c6 N matrix P such that P\u00b41DpgqP \u201c D1pgq for all g P G. Dpgq is reducible if D1pgq is block diagonal for all g P G, meaning that D1pgq acts on multiple independent subspaces of the vector space. Otherwise, the representation Dpgq is said to be irreducible. Irreducible representations, or irreps, are a class of representations that are convenient for composing different group representations. Specifically, for the case of SOp3q, Wigner-D matrices are irreducible representations, and we can express any group representation of SOp3q as a direct sum (concatentation) of Wigner-D matrices (Zee, 2016; Dresselhaus et al., 2007; Geiger et al., 2022):\nDpgq \u201c P\u00b41 \u02dc \u00e0\ni\nDpLiqpgq \u00b8 P \u201c P\u00b41\n\u00a8\n\u02da\n\u02dd\nDpL0qpgq DpL1qpgq\n. . .\n\u02db\n\u2039\n\u201a\nP (2)\nwhere DpLiqpgq are Wigner-D matrices of degree Li."
        },
        {
            "heading": "A.2 EQUIVARIANCE",
            "text": "A function f mapping between vector spaces X and Y is equivariant to a group of transformations G if for any input x P X , output y P Y and group element g P G, we have fpDXpgqxq \u201c DY pgqfpxq \u201c DY pgqy, where DXpgq and DY pgq are transformation matrices or group representations parametrized by g in X and Y . Additionally, f is invariant when DY pgq is an identity matrix for any g P G. As neural networks comprise many composable operations, equivariant neural networks comprise many equivariant operations to maintain the equivariance of input, intermediate, and output features. Incorporating equivariance as a strong prior knowledge can help improve data efficiency and generalization of neural networks (Batzner et al., 2022; Rackers et al., 2023; Frey et al., 2022). In this work, we achieve equivariance to 3D rotation by operating on vector spaces of SOp3q irreps, incorporate invariance to 3D translation by acting on relative positions, but do not consider inversion."
        },
        {
            "heading": "A.3 ESCN CONVOLUTION",
            "text": "Message passing is used to update equivariant irreps features and is typically implemented as SOp3q convolutions. A traditional SOp3q convolution interacts input irrep features xpLiqmi and spherical harmonic projections of relative positions Y pLf qmf pr\u20d7tsq with an SOp3q tensor product with ClebschGordan coefficients CpLo,moqpLi,miq,pLf ,mf q. Since tensor products are compute-intensive, eSCN convolutions (Passaro & Zitnick, 2023) are proposed to reduce the complexity of tensor products when they are used in SOp3q convolutions. Rotating the input irreps features xpLiqmi based on the relative position vectors r\u20d7ts simplifies the tensor products and enables reducing SOp3q convolutions to SOp2q linear operations. Below we provide the mathematical details of SOp3q convolutions built from tensor products and how rotation can reduce their computational complexity.\nTensor products interact type-Li vector xpLiq and type-Lf vector f pLf q to produce type-Lo vector ypLoq with Clebsch-Gordan coefficients CpLo,moqpLi,miq,pLf ,mf q. Clebsch-Gordan coefficients C pLo,moq pLi,miq,pLf ,mf q are non-zero only when |Li \u00b4 Lo| \u010f Lf \u010f |Li ` Lo|. Each non-trivial combination of Li b Lf \u00d1 Lo is called a path, and each path is independently equivariant and can be assigned a learnable weight wLi,Lf ,Lo .\nWe consider the message mts sent from source node s to target node t in an SOp3q convolution. The Lo-th degree of mts can be expressed as:\nm pLoq ts \u201c \u00ff\nLi,Lf\nwLi,Lf ,Lo\n\u00b4\nxpLiqs b Lo Li,Lf Y pLf qpr\u0302tsq \u00af\n(3)\nwhere xs is the irreps feature at source node s, x pLiq s denotes the Li-th degree of xs, and r\u0302ts \u201c r\u20d7ts|r\u20d7ts| . The spherical harmonic projection of relative positions Y pLf qpr\u0302tsq becomes sparse if we rotate r\u0302ts with a rotation matrix Rts to align with the direction of L \u201c 0 and m \u201c 0, which corresponds to the z axis traditionally but the y axis in the conventions of e3nn (Geiger et al., 2022). Concretely, given Rtsr\u0302ts aligned with the y axis, Y pLf q mf pRtsr\u0302tsq \u2030 0 only for mf \u201c 0. Without loss of equivariance, we re-scale Y pLf q0 pRtsr\u20d7tsq to be one. Besides, we denote DpLiqpRtsq \u201c DpLiq and DpLoqpRtsq \u201c DpLoq as Wigner-D matrices of degrees Li and Lo based on rotation matrix Rts, respectively, and we define DpLiqxpLiqs \u201c x\u0303pLiqs . Therefore, by rotating xpLiqs and Y pLf q based on r\u0302ts, we can simplify Eq. 3 as follows:\nm pLoq ts \u201c\n\u00b4 DpLoqpRtsq \u00af\u00b41 \u00ff\nLi,Lf\nwLi,Lf ,Lo\n\u00b4\nDpLiqpRtsqxpLiqs b Lo Li,Lf Y pLf qpRtsr\u0302tsq \u00af\n\u201c \u00b4 DpLoq \u00af\u00b41 \u00ff\nLi,Lf\nwLi,Lf ,Lo \u00e0\nmo\n\u02dc\n\u00ff\nmi,mf\n\u00b4\nDpLiqxpLiqs\n\u00af\nmi C\npLo,moq pLi,miq,pLf ,mf q\n\u00b4 Y pLf qpRtsr\u0302tsq \u00af\nmf\n\u00b8\n\u201c \u00b4 DpLoq \u00af\u00b41 \u00ff\nLi,Lf\nwLi,Lf ,Lo \u00e0\nmo\n\u02dc\n\u00ff\nmi\n\u00b4\nDpLiqxpLiqs\n\u00af\nmi C\npLo,moq pLi,miq,pLf ,0q\n\u00b8\n\u201c \u00b4 DpLoq \u00af\u00b41 \u00ff\nLi,Lf\nwLi,Lf ,Lo \u00e0\nmo\n\u02dc\n\u00ff\nmi\n\u00b4\nx\u0303pLiqs\n\u00af\nmi C\npLo,moq pLi,miq,pLf ,0q\n\u00b8\n(4) where \u00c0\ndenotes concatenation. Additionally, given mf \u201c 0, Clebsch-Gordan coefficients C\npLo,moq pLi,miq,pLf ,0q are sparse and are non-zero only when mi \u201c \u02d8mo, which further simplifies Eq. 4:\nm pLoq ts \u201c\n\u00b4 DpLoq \u00af\u00b41 \u00ff\nLi,Lf\nwLi,Lf ,Lo \u00e0\nmo\n\u02c6\n\u00b4\nx\u0303pLiqs\n\u00af\nmo C\npLo,moq pLi,moq,pLf ,0q `\n\u00b4\nx\u0303pLiqs\n\u00af\n\u00b4mo C\npLo,moq pLi,\u00b4moq,pLf ,0q\n\u02d9\n(5) By re-ordering the summations and concatenation in Eq. 5, we have:\n\u00b4 DpLoq \u00af\u00b41 \u00ff\nLi\n\u00e0\nmo\n\u00a8\n\u02dd\n\u00b4\nx\u0303pLiqs\n\u00af\nmo\n\u00ff\nLf\n\u00b4\nwLi,Lf ,LoC pLo,moq pLi,moq,pLf ,0q\n\u00af ` \u00b4\nx\u0303pLiqs\n\u00af\n\u00b4mo\n\u00ff\nLf\n\u00b4\nwLi,Lf ,LoC pLo,moq pLi,\u00b4moq,pLf ,0q\n\u00af\n\u02db\n\u201a\n(6) Instead of using learnable parameters for wLi,Lf ,Lo , eSCN proposes to parametrize w\u0303 pLi,Loq mo and w\u0303 pLi,Loq \u00b4mo as below:\nw\u0303pLi,Loqmo \u201c \u00ff\nLf\nwLi,Lf ,LoC pLo,moq pLi,moq,pLf ,0q \u201c \u00ff\nLf\nwLi,Lf ,LoC pLo,\u00b4moq pLi,\u00b4moq,pLf ,0q for m \u0105\u201c 0\nw\u0303 pLi,Loq \u00b4mo \u201c \u00ff\nLf\nwLi,Lf ,LoC pLo,\u00b4moq pLi,moq,pLf ,0q \u201c \u00b4 \u00ff\nLf\nwLi,Lf ,LoC pLo,moq pLi,\u00b4moq,pLf ,0q for m \u0105 0\n(7)\nThe parametrization of w\u0303pLi,Loqmo and w\u0303 pLi,Loq \u00b4mo enables removing the summation over Lf and further simplifies the computation. By combining Eq. 6 and Eq. 7, we have:\nm pLoq ts \u201c\n\u00b4 DpLoq \u00af\u00b41 \u00ff\nLi\n\u00e0\nmo\n\u00b4\ny pLi,Loq ts\n\u00af\nmo\n\u00b4\ny pLi,Loq ts\n\u00af\nmo \u201c w\u0303pLi,Loqmo\n\u00b4\nx\u0303pLiqs\n\u00af\nmo \u00b4 w\u0303pLi,Loq\u00b4mo\n\u00b4\nx\u0303pLiqs\n\u00af\n\u00b4mo for mo \u0105 0\n\u00b4\ny pLi,Loq ts\n\u00af\n\u00b4mo \u201c w\u0303pLi,Loq\u00b4mo\n\u00b4\nx\u0303pLiqs\n\u00af\nmo ` w\u0303pLi,Loqmo\n\u00b4\nx\u0303pLiqs\n\u00af\n\u00b4mo for mo \u0105 0\n\u00b4\ny pLi,Loq ts\n\u00af\nmo \u201c w\u0303pLi,Loqmo\n\u00b4\nx\u0303pLiqs\n\u00af\nmo for mo \u201c 0\n(8)\nThe formulation of ypLi,Loqts coincides with performing SOp2q linear operations (Worrall et al., 2016; Passaro & Zitnick, 2023). Additionally, eSCN convolutions can further simplify the computation by considering only a subset of mo components in Eq. 8, i.e., |mo| \u010f Mmax.\nIn summary, efficient SOp3q convolutions can be achieved by first rotating irreps features xpLiqs based on relative position vectors r\u20d7ts and then performing SOp2q linear operations on rotated features. The key idea is that rotation simplifies the computation as in Eq. 4, 5, 7, and 8. Please refer to their work (Passaro & Zitnick, 2023) for more details. We note that eSCN convolutions consider only simplifying the case of taking tensor products between input irreps features and spherical harmonic projections of relative position vectors. eSCN convolutions do not simplify general cases such as taking tensor products between input irreps features and themselves (Batatia et al., 2022) since the relative position vectors used to rotate irreps features are not clearly defined."
        },
        {
            "heading": "A.4 S2 ACTIVATION",
            "text": "S2 activation was first proposed in Spherical CNNs (Cohen et al., 2018). Our implementation of S2 activation is the same as that in e3nn (Geiger et al., 2022), SCN (Zitnick et al., 2022) and eSCN (Passaro & Zitnick, 2023). Basically, we uniformly sample a fixed set of points on a unit sphere along the dimensions of longitude, parametrized by \u03b1 P r0, 2\u03c0q, and latitude, parametrized by \u03b2 P r0, \u03c0q. We set the resolutions R of \u03b1 and \u03b2 to be 18 when Lmax \u201c 6, meaning that we will have 324 p\u201c 18 \u02c6 18q points. Once the points are sampled, they are kept the same during training and inference, and therefore there is no randomness. For each point on the unit sphere, we compute the spherical harmonics projection of degrees up to Lmax. We consider an equivariant feature of C channels and each channel contains vectors of all degrees from 0 to Lmax. When performing S2 activation, for each channel and for each sampled point, we first compute the inner product between the vectors of all degrees contained in one channel of the equivariant feature and the spherical harmonics projections of a sampled point. This results in R \u02c6 R \u02c6 C values, where the first two dimensions, R \u02c6 R, correspond to grid resolutions and the last dimension corresponds to channels. They can be viewed as 2D grid feature maps and treated as scalars, and we can apply any standard or typical activation functions like SiLU or use standard linear layers performing feature aggregation along the channel dimension. After applying these functions, we project back to vectors of all degrees by multiplying those values with their corresponding spherical harmonics projections of sampled points. The process is the same as performing a Fourier transform, applying some functions and then performing an inverse Fourier transform.\nMoreover, since the inner products between one channel of vectors of all degrees and the spherical harmonics projections of sampled points sum over all degrees, the conversion to 2D grid feature maps implicitly considers the information of all degrees. Therefore, S2 activation, which converts equivariant features into 2D grid feature maps, uses the information of all degrees to determine the non-linearity. In contrast, gate activation only uses vectors of degree 0 to determine the non-linearity of vectors of higher degrees. For tasks such as force predictions, where the information of degrees is critical, S2 activation can be better than gate activation since S2 activation uses all degrees to determine non-linearity.\nAlthough there is sampling on a sphere, the works (Cohen et al., 2018; Passaro & Zitnick, 2023) mention that as long as the number of samples, or resolution R, is high enough, the equivariance error can be close to zero. Furthermore, eSCN (Passaro & Zitnick, 2023) empirically computes such errors in Figure 9 in their latest manuscript and shows that the errors of using Lmax \u201c 6 and R \u201c 18 are close to 0.2%, which is similar to the equivariance errors of tensor products in e3nn (Geiger et al., 2022). We note that the equivariance errors in e3nn are due to numerical precision."
        },
        {
            "heading": "B RELATED WORKS",
            "text": "B.1 SE(3)/E(3)-EQUIVARIANT GNNS\nEquivariant neural networks (Thomas et al., 2018; Kondor et al., 2018; Weiler et al., 2018; Fuchs et al., 2020; Miller et al., 2020; Townshend et al., 2020; Batzner et al., 2022; Jing et al., 2021; Sch\u00fctt et al., 2021; Satorras et al., 2021; Unke et al., 2021; Brandstetter et al., 2022; Th\u00f6lke & Fabritiis, 2022; Le et al., 2022; Musaelian et al., 2022; Batatia et al., 2022; Liao & Smidt, 2023; Passaro & Zitnick, 2023)\nuse equivariant irreps features built from vector spaces of irreducible representations (irreps) to achieve equivariance to 3D rotation (Thomas et al., 2018; Weiler et al., 2018; Kondor et al., 2018). They operate on irreps features with equivariant operations like tensor products. Previous works differ in equivariant operations used in their networks and how they combine those operations. TFN (Thomas et al., 2018) and NequIP (Batzner et al., 2022) use equivariant graph convolution with linear messages built from tensor products, with the latter utilizing extra equivariant gate activation (Weiler et al., 2018). SEGNN (Brandstetter et al., 2022) introduces non-linearity to messages passing (Gilmer et al., 2017; Sanchez-Gonzalez et al., 2020) with equivariant gate activation, and the non-linear messages improve upon linear messages. SE(3)-Transformer (Fuchs et al., 2020) adopts equivariant dot product attention (Vaswani et al., 2017) with linear messages. Equiformer (Liao & Smidt, 2023) improves upon previously mentioned equivariant GNNs by combining MLP attention and non-linear messages. Equiformer additionally introduces equivariant layer normalization and regularizations like dropout (Srivastava et al., 2014) and stochastic depth (Huang et al., 2016). However, the networks mentioned above rely on compute-intensive SOp3q tensor products to mix the information of vectors of different degrees during message passing, and therefore they are limited to small values for maximum degrees Lmax of equivariant representations. SCN (Zitnick et al., 2022) proposes rotating irreps features based on relative position vectors and identifies a subset of spherical harmonics coefficients, on which they can apply unconstrained functions. They further propose relaxing the requirement for strict equivariance and apply typical functions to rotated features during message passing, which trades strict equivariance for computational efficiency and enables using higher values of Lmax. eSCN (Passaro & Zitnick, 2023) further improves upon SCN by replacing typical functions with SOp2q linear layers for rotated features and imposing strict equivariance during message passing. However, except using more efficient operations for higher Lmax, SCN and eSCN mainly adopt the same network design as SEGNN, which is less performant than Equiformer. In this work, we propose EquiformerV2, which includes all the benefits of the above networks by incorporating eSCN convolutions into Equiformer and adopts three additional architectural improvements.\nB.2 INVARIANT GNNS\nPrior works (Sch\u00fctt et al., 2017; Xie & Grossman, 2018; Unke & Meuwly, 2019; Gasteiger et al., 2020b;a; Qiao et al., 2020; Liu et al., 2022; Shuaibi et al., 2021; Klicpera et al., 2021; Sriram et al., 2022; Gasteiger et al., 2022) extract invariant information from 3D atomistic graphs and operate on the resulting graphs augmented with invariant features. Their differences lie in leveraging different geometric features such as distances, bond angles (3 atom features) or dihedral angles (4 atom features). SchNet (Sch\u00fctt et al., 2017) models interaction between atoms with only relative distances. DimeNet series (Gasteiger et al., 2020b;a) use triplet representations of atoms to incorporate bond angles. SphereNet (Liu et al., 2022) and GemNet (Klicpera et al., 2021; Gasteiger et al., 2022) further include dihedral angles by considering quadruplet representations. However, the memory complexity of triplet and quadruplet representations of atoms do not scale well with the number of atoms, and this requires additional modifications like interaction hierarchy used by GemNet-OC (Gasteiger et al., 2022) for large datasets like OC20 (Chanussot* et al., 2021). Additionally, for the task of predicting DFT calculations of energies and forces on the large-scale OC20 dataset, invariant GNNs have been surpassed by equivariant GNNs recently."
        },
        {
            "heading": "C DETAILS OF ARCHITECTURE",
            "text": "In this section, we define architectural hyper-parameters like maximum degrees and numbers of channels in certain layers in EquiformerV2, which are used to specify the detailed architectures in Sec. D.2, Sec. D.5, Sec. E.1 and Sec. F.2. Besides, we note that eSCN (Passaro & Zitnick, 2023) and this work mainly consider SEp3q-equivariance. We denote embedding dimensions as dembed, which defines the dimensions of most irreps features. Specifically, the output irreps features of all modules except the output head in Figure 1a have dimension dembed. For separable S2 activation as illustrated in Figure 2c, we denote the resolution of point samples on a sphere as R, which can depend on maximum degree Lmax, and denote the unconstrained functions after projecting to point samples as F .\nFor equivariant graph attention in Figure 1b, the input irreps features xi and xj have dimension dembed. The dimension of the irreps feature f pLq ij is denoted as dattn_hidden. Equivariant graph\nattention can have h parallel attention functions. For each attention function, we denote the dimension of the scalar feature f p0qij as dattn_alpha and denote the dimension of the value vector, which is in the form of irreps features, as dattn_value. For the separable S2 activation used in equivariant graph attention, the resolution of point samples is R, and we use a single SiLU activation for F . We share the layer normalization in attention re-normalization across all h attention functions but have different h linear layers after that. The last linear layer projects the dimension back to dembed. The two intermediate SOp2q linear layers operate with maximum degree Lmax and maximum order Mmax. For feed forward networks (FFNs) in Figure 1d, we denote the dimension of the output irreps features of the first linear layer as dffn. For the separable S2 activation used in FFNs, the resolution of point samples is R, and F consists of a two-layer MLP, with each linear layer followed by SiLU, and a final linear layer. The linear layers have the same number of channels as dffn.\nFor radial functions, we denote the dimension of hidden scalar features as dedge. For experiments on OC20, same as eSCN (Passaro & Zitnick, 2023), we use Gaussian radial basis to represent relative distances and additionally embed the atomic numbers at source nodes and target nodes with two scalar features of dimension dedge. The radial basis and the two embeddings of atomic numbers are fed to the radial function to generate edge distance embeddings.\nThe maximum degree of irreps features is denoted as Lmax. All irreps features have degrees from 0 to Lmax and have C channels for each degree. We denote the dimension as pLmax, Cq. For example, irreps feature xirreps of dimension p6, 128q has maximum degree 6 and 128 channels for each degree. The dimension of scalar feature xscalar can be expressed as p0, Cscalarq. Following Equiformer (Liao & Smidt, 2023), we apply dropout (Srivastava et al., 2014) to attention weights and stochastic depth (Huang et al., 2016) to outputs of equivariant graph attention and feed forward networks. However, we do not apply dropout or stochastic depth to the output head."
        },
        {
            "heading": "D DETAILS OF EXPERIMENTS ON OC20",
            "text": ""
        },
        {
            "heading": "D.1 DETAILED DESCRIPTION OF OC20 DATASET",
            "text": "The large and diverse OC20 dataset (Chanussot* et al., 2021) (Creative Commons Attribution 4.0 License) consists of 1.2M DFT relaxations for training and evaluation, computed with the revised Perdew-Burke-Ernzerhof (RPBE) functional (Hammer et al., 1999). Each structure in OC20 has an adsorbate molecule placed on a catalyst surface, and the core task is Structure to Energy Forces (S2EF), which is to predict the energy of the structure and per-atom forces. Models trained for the S2EF task are evaluated on energy and force mean absolute error (MAE). These models can in turn be used for performing structure relaxations by using the model\u2019s force predictions to iteratively update the atomic positions until a relaxed structure corresponding to a local energy minimum is found. These relaxed structure and energy predictions are evaluated on the Initial Structure to Relaxed Structure (IS2RS) and Initial Structure to Relaxed Energy (IS2RE) tasks. The \u201cAll\u201d split of OC20 contains 134M training structures spanning 56 elements, the \u201cMD\u201d split consists of 38M structures, and the \u201c2M\u201d split has 2M structures. For validation and test splits, there are four sub-splits containing in-distribution adsorbates and catalysts (ID), out-of-distribution adsorbates (OOD Ads), out-of-distribution catalysts (OOD Cat), and out-of-distribution adsorbates and catalysts (OOD Both)."
        },
        {
            "heading": "D.2 TRAINING DETAILS",
            "text": "Hyper-Parameters. We summarize the hyper-parameters for the base model setting on OC20 S2EF-2M dataset and the main results on OC20 S2EF-All and S2EF-All+MD datasets in Table 7. For the ablation studies on OC20 S2EF-2M dataset, when trained for 20 or 30 epochs as in Table 1b, we increase the learning rate from 2 \u02c6 10\u00b44 to 4 \u02c6 10\u00b44. When using Lmax \u201c 8 as in Table 1c, we increase the resolution of point samples R from 18 to 20. We vary Lmax and the widths for speed-accuracy trade-offs in Figure 4. Specifically, we first decrease Lmax from 6 to 4. Then, we multiply h and the number of channels of pdembed, dattn_hidden, dffnq by 0.75 and 0.5. We train all models for 30 epochs. The same strategy to scale down eSCN models is adopted for fair comparisons.\nTraining Time, Inference Speed and Numbers of Parameters. Table 8 summarizes the training time, inference speed and numbers of parameters of models in Tables 1a (Index 1, 2, 3, 4, 5), 1c, 1d and 2. V100 GPUs with 32GB are used to train all models. We use 16 GPUs to train each individual model on S2EF-2M dataset, 64 GPUs for S2EF-All, 64 GPUs for EquiformerV2 (31M) on S2EFAll+MD, and 128 GPUs for EquiformerV2 (153M) on S2EF-All+MD."
        },
        {
            "heading": "D.3 DETAILS OF RUNNING RELAXATIONS",
            "text": "A structural relaxation is a local optimization where atom positions are iteratively updated based on forces to minimize the energy of the structure. We perform ML relaxations using the LBFGS optimizer (quasi-Newton) implemented in the Open Catalyst Github repository (Chanussot* et al., 2021). The structural relaxations for OC20 IS2RE and IS2RS tasks are allowed to run for 200 steps or until the maximum predicted force per atom Fmax \u010f 0.02 eV/\u00c5 , and the relaxations for AdsorbML are allowed to run for 300 steps or until Fmax \u010f 0.02 eV/\u00c5 . These settings are chosen to be consistent with prior works. We run relaxations on V100 GPUs with 32GB. The computational cost of running relaxations with EquiformerV2 (153M) for OC20 IS2RE and IS2RS tasks is 1011 GPU-hours, and that of running ML relaxations for AdsorbML is 1075 GPU-hours. The time for\nrunning relaxations with EquiformerV2 (31M) is 240 GPU-hours for OC20 IS2RE and IS2RS and 298 GPU-hours for AdsorbML."
        },
        {
            "heading": "D.4 DETAILS OF ADSORBML",
            "text": "We run the AdsorbML algorithm on the OC20-Dense dataset in accordance with the procedure laid out in the paper (Lan et al., 2022), which is summarized here:\n1. Run ML relaxations on all initial structures in the OC20-Dense dataset. There are around 1000 different adsorbate-surface combinations with about 90 adsorbate placements per combination, and therefore we have roughly 90k structures in total.\n2. Remove invalid ML relaxed structures based on physical constraints and rank the other ML relaxed structures in order of lowest to highest ML predicted energy.\n3. Take the top k ML relaxed structures with the lowest ML predicted energies for each adsorbate-surface combination and run DFT single-point calculations. The single-point calculations are performed on the ML relaxed structures to improve the energy predictions without running a full DFT relaxation and are run with VASP using the same setting as the original AdsorbML experiments. As shown in Table 3, we vary k from 1 to 5.\n4. Compute success and speedup metrics based on our lowest DFT single-point energy per adsorbate-surface combination and the DFT labels provided in the OC20-Dense dataset.\nTo better understand the speed-accuracy trade-offs of different models, we compare the AdsorbML success rate averaged over k from 1 to 5 and average GPU-seconds of running one structure relaxation in Figure 5. We visualize some examples of relaxed structures from eSCN (Passaro & Zitnick, 2023), EquiformerV2 and DFT in Figure 6."
        },
        {
            "heading": "D.5 ADDITIONAL COMPARISON WITH EQUIFORMER ON OC20 IS2RE",
            "text": "Training Details. We follow the same setting as Equiformer (Liao & Smidt, 2023) and train two EquiformerV2 models on OC20 IS2RE dataset without and with IS2RS auxiliary task. We use the same radial basis function as Equiformer. When IS2RS auxiliary task is adopted, we use a linearly decayed weight for loss associated with IS2RS, which starts at 15 and decays to 1 and adopt Noisy Nodes data augmentation (Godwin et al., 2022). The hyper-parameters are summarized in Table 9. We train EquiformerV2 on OC20 IS2RE with 16 V100 GPUs with 32GB and train on OC20 IS2RE with IS2RS auxiliary task and Noisy Nodes data augmentation with 32 V100 GPUs. The training costs are 574 and 2075 GPU-hours, and the numbers of parameters are 36.03M and 95.24M.\nResults. The comparison is shown in Table 10. Without IS2RS auxiliary task, EquiformerV2 overfits the training set due to higher degrees and achieves worse results than Equiformer. However, with IS2RS auxiliary task and Noisy Nodes data augmentation, EquiformerV2 achieves better energy MAE. The different rankings of models under different settings is also found in Noisy Nodes (Godwin\net al., 2022), where they mention a node-level auxiliary task can prevent overfitting and enable more expressive models to perform better."
        },
        {
            "heading": "E DETAILS OF EXPERIMENTS ON OC22",
            "text": ""
        },
        {
            "heading": "E.1 TRAINING DETAILS",
            "text": "The hyper-parameters for OC22 dataset is summarized in Table 11. We use 32 V100 GPUs with 32GB and train two EquiformerV2 models with different energy coefficients \u03bbE and force coefficeints \u03bbF . The number of parameters is 121.53M, and the training cost of each model is 4552 GPU-hours. The time for running relaxations for OC22 IS2RE is 38 GPU-hours."
        },
        {
            "heading": "F DETAILS OF EXPERIMENTS ON QM9",
            "text": ""
        },
        {
            "heading": "F.1 ADDITIONAL RESULTS OF TRAINING WITH NOISY NODES",
            "text": "Similar to Sec. D.5, we train EquiformV2 on QM9 with Noisy Nodes (Godwin et al., 2022) to show that the performance gain brought by using higher degrees can be larger when trained with a node-level auxiliary task and data augmentation. In Table 12, we summarize the results and compare with previous works using Noisy Nodes and pre-training via denoising (Zaidi et al., 2023). When trained with Noisy Nodes, EquiformerV2 performs better than Equiformer on more tasks. Specifically, without Noisy Nodes, EquiformerV2 is better than Equiformer on 9 out of the 12 tasks, similar on 1 task, and worse on the other 2 tasks. With Noisy Nodes, EquiformerV2 achieves better MAE on 10 tasks, similar on 1 task, and worse on 1 task. Additionally, we note that EquiformerV2 with Noisy Nodes is overall better than GNS-TAT with both pre-training and Noisy Nodes (Zaidi et al., 2023) on 9 out of the 12 tasks even though EquiformerV2 is not pre-trained on PCQM4Mv2 dataset,\nwhich is more than 30\u02c6 larger than QM9. This shows that a more expressive model can match the performance with significantly less data."
        },
        {
            "heading": "F.2 TRAINING DETAILS",
            "text": "We follow the data partition of Equiformer. For the tasks of \u00b5, \u03b1, \u03b5HOMO, \u03b5LUMO, \u2206\u03b5, and C\u03bd , we use batch size \u201c 64, the number of epochs \u201c 300, learning rate \u201c 5 \u02c6 10\u00b44, Gaussian radial basis functions with the number of bases \u201c 128, the number of Transformer blocks \u201c 6, weight decay \u201c 5 \u02c6 10\u00b43, and dropout rate \u201c 0.2 and use mixed precision for training. For the task of R2, we use batch size \u201c 48, the number of epochs \u201c 300, learning rate \u201c 1.5 \u02c6 10\u00b44, Gaussian radial basis functions with the number of bases \u201c 128, the number of Transformer blocks \u201c 5, weight decay \u201c 5 \u02c6 10\u00b43, and dropout rate \u201c 0.1 and use single precision for training. For the task of ZPVE, we use batch size \u201c 48, the number of epochs \u201c 300, learning rate \u201c 1.5 \u02c6 10\u00b44, Gaussian radial basis functions with the number of bases \u201c 128, the number of Transformer blocks \u201c 5, weight decay \u201c 5 \u02c6 10\u00b43, and dropout rate \u201c 0.2 and use single precision for training. For the task of G, H , U , and U0, we use batch size \u201c 48, the number of epochs \u201c 300, learning rate \u201c 1.5 \u02c6 10\u00b44, Gaussian radial basis functions with the number of bases \u201c 128, the number of Transformer blocks \u201c 5, weight decay \u201c 0.0, and dropout rate \u201c 0.0 and use single precision for training. Other hyper-parameters are the same across all the tasks, and we summarize them in Table 13. We use a single A6000 GPU and train different models for different tasks. The training costs are 72 GPU-hours for mixed precision training and 137 GPU-hours for single precision training. The number of parameters are 11.20M for 6 blocks and 9.35M for 5 blocks.\nAs for training with Noisy Nodes as mentioned in Sec. F.1, we add noise to atomic coordinates and incorporate a node-level auxiliary task of denoising atomic coordinates. We thus introduce\nfour additional hyper-parameters, which are noise standard deviation \u03c3denoise, denoising coefficient \u03bbdenoise, denoising probability pdenoise and corrupt ratio rdenoise. The noise standard deviation \u03c3denoise denotes the standard deviation of Gaussian noise added to each xyz component of atomic coordinates. The denoising coefficient \u03bbdenoise controls the relative importance of the auxiliary task compared to the original task. The denoising probability pdenoise denotes the probability of adding noise to atomic coordinates and optimizing for both the auxiliary task and the original task. Using pdenoise \u0103 1 enables taking original atomistic structures without any noise as inputs and optimizing for only the original task for some training iterations. The corrupt ratio rdenoise denotes the ratio of the number of atoms, which we add noise to and denoise, to the total number of atoms. Using rdenoise \u0103 1 allows only adding noise to and denoising a subset of atoms within a structure. For the task of R2, we use \u03c3denoise \u201c 0.02, \u03bbdenoise \u201c 0.1, pdenoise \u201c 0.5 and rdenoise \u201c 0.125. For other tasks, we use \u03c3denoise \u201c 0.02, \u03bbdenoise \u201c 0.1, pdenoise \u201c 0.5 and rdenoise \u201c 0.25. We share the above hyper-parameters for training EquiformerV2 and Equiformer, and we add one additional block of equivariant graph attention for the auxiliary task. We slightly tune other hyper-parameters when trained with Noisy Nodes. For Equiformer, we additionally use stochastic depth \u201c 0.05 for the tasks of \u03b1,\u2206\u03b5, \u03b5HOMO, \u03b5LUMO, and C\u03bd . As for EquiformerV2, we additionally use stochastic depth \u201c 0.05 for the tasks of \u00b5,\u2206\u03b5, \u03b5HOMO, \u03b5LUMO, and C\u03bd . We increase the number of blocks from 5 to 6 and increase the batch size from 48 to 64 for the tasks of G,H,U, and U0. We increase the learning rate from 1.5 \u02c6 10\u00b44 to 5 \u02c6 10\u00b44 and increase the number of blocks from 5 to 6 for the task of R2."
        },
        {
            "heading": "F.3 ABLATION STUDY ON ARCHITECTURAL IMPROVEMENTS",
            "text": "We conduct ablation studies on the proposed architectural improvements using the task of \u2206\u03b5 on QM9 and compare with Equiformer baseline (Liao & Smidt, 2023). The reults are summarized in Table 14. The comparison between Index 0 and Index 1 shows that directly increasing Lmax from 2 to 4 and using eSCN convolutions degrade the performance. This is due to overfitting since the QM9 dataset is smaller, and each structure in QM9 has fewer atoms, less diverse atom types and much less angular variations than OC20 and OC22. Comparing Index 1 and Index 2, attention re-normalization clearly improves the MAE result. Although using S2 activation is stable here (Index 3) unlike OC20, it results in higher error than using gate activation (Index 2) and the Equiformer baseline (Index 0). When using the proposed separable S2 activation (Index 4), we achieve lower error than using gate activation (Index 2). We can further reduce the error by using the proposed separable layer normalization (Index 5). Comparing Index 0 and Index 5, we note that the proposed architectural improvements are necessary to achieve better results than the baseline when using higher degrees on QM9. Overall, these ablation results follow the same trends as OC20."
        }
    ],
    "title": "EQUIFORMERV2: IMPROVED EQUIVARIANT TRANSFORMER",
    "year": 2024
}