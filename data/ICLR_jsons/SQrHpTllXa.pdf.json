{
    "abstractText": "Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) \u2013 a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question-answering LLM (QA LLM). To further aid the relevance scorer, CABINET employs a weakly supervised module that generates a parsing statement describing the criteria of rows and columns relevant to the question and highlights the content of corresponding table cells. CABINET significantly outperforms various tabular LLM baselines, as well as GPT3-based in-context learning methods, is more robust to noise, maintains outperformance on tables of varying sizes, and establishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We release our code and datasets here.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sumit Bhatia"
        },
        {
            "affiliations": [],
            "name": "Yaman Kumar Singla"
        },
        {
            "affiliations": [],
            "name": "Balaji Krishnamurthy"
        }
    ],
    "id": "SP:798ebcd290ad09c0bab5db085b73b2bebda48a9e",
    "references": [
        {
            "authors": [
                "Rami Aly",
                "Zhijiang Guo",
                "Michael Schlichtkrull",
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Oana Cocarascu",
                "Arpit Mittal"
            ],
            "title": "Feverous: Fact extraction and verification over unstructured and structured information",
            "venue": "Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks,",
            "year": 2021
        },
        {
            "authors": [
                "Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jiawei Chen",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun"
            ],
            "title": "Benchmarking large language models in retrieval-augmented generation, 2023a",
            "year": 2023
        },
        {
            "authors": [
                "Nuo Chen",
                "Linjun Shou",
                "Ming Gong",
                "Jian Pei",
                "Chenyu You",
                "Jianhui Chang",
                "Daxin Jiang",
                "Jia Li"
            ],
            "title": "Bridge the gap between language models and tabular understanding, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Wenhu Chen"
            ],
            "title": "Large language models are few(1)-shot table reasoners. In Findings of the Association for Computational Linguistics: EACL 2023",
            "venue": "Association for Computational Linguistics. URL https://aclanthology.org/2023. findings-eacl.83",
            "year": 2023
        },
        {
            "authors": [
                "Wenhu Chen",
                "Hongmin Wang",
                "Jianshu Chen",
                "Yunkai Zhang",
                "Hong Wang",
                "Shiyang Li",
                "Xiyou Zhou",
                "William Yang Wang"
            ],
            "title": "Tabfact: A large-scale dataset for table-based fact verification",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Zhiyu Chen",
                "Wenhu Chen",
                "Hanwen Zha",
                "Xiyou Zhou",
                "Yunkai Zhang",
                "Sairam Sundaresan",
                "William Yang Wang"
            ],
            "title": "Logic2Text: High-fidelity natural language generation from logical forms",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Zhiyu Chen",
                "Shiyang Li",
                "Charese Smiley",
                "Zhiqiang Ma",
                "Sameena Shah",
                "William Yang Wang"
            ],
            "title": "ConvFinQA: Exploring the chain of numerical reasoning in conversational finance question answering",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Zhoujun Cheng",
                "Haoyu Dong",
                "Zhiruo Wang",
                "Ran Jia",
                "Jiaqi Guo",
                "Yan Gao",
                "Shi Han",
                "Jian-Guang Lou",
                "Dongmei Zhang"
            ],
            "title": "HiTab: A hierarchical table dataset for question answering and natural language generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "year": 2022
        },
        {
            "authors": [
                "Zhoujun Cheng",
                "Tianbao Xie",
                "Peng Shi",
                "Chengzu Li",
                "Rahul Nadkarni",
                "Yushi Hu",
                "Caiming Xiong",
                "Dragomir Radev",
                "Mari Ostendorf",
                "Luke Zettlemoyer",
                "Noah A. Smith",
                "Tao Yu"
            ],
            "title": "Binding language models in symbolic languages",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models",
            "venue": "arXiv preprint arXiv:2210.11416,",
            "year": 2022
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models",
            "venue": "arXiv preprint arXiv:2210.11416,",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Deng",
                "Huan Sun",
                "Alyssa Lees",
                "You Wu",
                "Cong Yu"
            ],
            "title": "Turl: table understanding through representation learning",
            "venue": "Proceedings of the VLDB Endowment,",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "year": 2019
        },
        {
            "authors": [
                "Julian Eisenschlos",
                "Syrine Krichene",
                "Thomas M\u00fcller"
            ],
            "title": "Understanding tables with intermediate pre-training",
            "venue": "pp. 281\u2013296,",
            "year": 2020
        },
        {
            "authors": [
                "Julian Eisenschlos",
                "Maharshi Gor",
                "Thomas M\u00fcller",
                "William Cohen"
            ],
            "title": "MATE: Multi-view attention for table transformer efficiency",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "November"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "doi: 10.18653/v1/2021.emnlp-main. 600. URL https://aclanthology.org/2021.emnlp-main.600.",
            "year": 2021
        },
        {
            "authors": [
                "Siddhant Garg",
                "Thuy Vu",
                "Alessandro Moschitti"
            ],
            "title": "Tanda: Transfer and adapt pre-trained transformer models for answer sentence selection",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Zihui Gu",
                "Ju Fan",
                "Nan Tang",
                "Preslav Nakov",
                "Xiaoman Zhao",
                "Xiaoyong Du"
            ],
            "title": "PASTA: Tableoperations aware fact verification via sentence-table cloze pre-training",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Hongwei Han",
                "Jialiang Xu",
                "Mengyu Zhou",
                "Yijia Shao",
                "Shi Han",
                "Dongmei Zhang"
            ],
            "title": "Luna: Language understanding with number augmentations on transformers via number plugins and pretraining",
            "venue": "arXiv preprint arXiv:2212.02691,",
            "year": 2022
        },
        {
            "authors": [
                "Pengcheng He",
                "Jianfeng Gao",
                "Weizhu Chen"
            ],
            "title": "DeBERTav3: Improving deBERTa using ELECTRA-style pre-training with gradient-disentangled embedding sharing",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Herzig",
                "Pawel Krzysztof Nowak",
                "Thomas M\u00fcller",
                "Francesco Piccinno",
                "Julian Eisenschlos"
            ],
            "title": "TaPas: Weakly supervised table parsing via pre-training",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4320\u20134333,",
            "year": 2020
        },
        {
            "authors": [
                "Roshni Iyer",
                "Thuy Vu",
                "Alessandro Moschitti",
                "Yizhou Sun"
            ],
            "title": "Question-answer sentence graph for joint modeling answer selection",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics,",
            "year": 2023
        },
        {
            "authors": [
                "Jinhao Jiang",
                "Kun Zhou",
                "Zican Dong",
                "Keming Ye",
                "Wayne Xin Zhao",
                "Ji-Rong Wen"
            ],
            "title": "Structgpt: A general framework for large language model to reason over structured data, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Yi Mao",
                "Pengcheng He",
                "Graham Neubig",
                "Weizhu Chen"
            ],
            "title": "OmniTab: Pretraining with natural and synthetic data for few-shot table-based question answering",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Syrine Krichene",
                "Thomas M\u00fcller",
                "Julian Eisenschlos"
            ],
            "title": "DoT: An efficient double transformer for NLP tasks with tables. In Findings of the Association for Computational Linguistics: ACLIJCNLP",
            "venue": "Online,",
            "year": 2021
        },
        {
            "authors": [
                "Vishwajeet Kumar",
                "Yash Gupta",
                "Saneem Chemmengath",
                "Jaydeep Sen",
                "Soumen Chakrabarti",
                "Samarth Bharadwaj",
                "Feifei Pan"
            ],
            "title": "Multi-row, multi-span distant supervision for Table+Text question answering",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Qian Liu",
                "Bei Chen",
                "Jiaqi Guo",
                "Morteza Ziyadi",
                "Zeqi Lin",
                "Weizhu Chen",
                "Jian-Guang Lou"
            ],
            "title": "TAPEX: Table pre-training via learning a neural SQL executor",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "SGDR: Stochastic gradient descent with warm restarts",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Kai Nakamura",
                "Sharon Levy",
                "Yi-Lin Tuan",
                "Wenhu Chen",
                "William Yang Wang"
            ],
            "title": "HybriDialogue: An information-seeking dialogue dataset grounded on tabular and textual data",
            "venue": "In Findings of the Association for Computational Linguistics: ACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Linyong Nan",
                "Dragomir Radev",
                "Rui Zhang",
                "Amrit Rau",
                "Abhinand Sivaprasad",
                "Chiachun Hsieh",
                "Xiangru Tang",
                "Aadit Vyas",
                "Neha Verma",
                "Pranav Krishna",
                "Yangxiaokang Liu",
                "Nadia Irwanto",
                "Jessica Pan",
                "Faiaz Rahman",
                "Ahmad Zaidi",
                "Mutethia Mutuma",
                "Yasin Tarabar",
                "Ankit Gupta",
                "Tao Yu",
                "Yi Chern Tan",
                "Xi Victoria Lin",
                "Caiming Xiong",
                "Richard Socher",
                "Nazneen Fatema Rajani"
            ],
            "title": "DART: Open-domain structured data record to text generation",
            "year": 2021
        },
        {
            "authors": [
                "Linyong Nan",
                "Chiachun Hsieh",
                "Ziming Mao",
                "Xi Victoria Lin",
                "Neha Verma",
                "Rui Zhang",
                "Wojciech Kry\u015bci\u0144ski",
                "Hailey Schoelkopf",
                "Riley Kong",
                "Xiangru Tang",
                "Mutethia Mutuma",
                "Ben Rosand",
                "Isabel Trindade",
                "Renusree Bandaru",
                "Jacob Cunningham",
                "Caiming Xiong",
                "Dragomir Radev"
            ],
            "title": "FeTaQA: Free-form table question answering",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "J. Neeraja",
                "Vivek Gupta",
                "Vivek Srikumar"
            ],
            "title": "Incorporating external knowledge to enhance tabular reasoning",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2799\u20132809,",
            "year": 2021
        },
        {
            "authors": [
                "Ansong Ni",
                "Srini Iyer",
                "Dragomir Radev",
                "Ves Stoyanov",
                "Wen-tau Yih",
                "Sida I Wang",
                "Xi Victoria Lin"
            ],
            "title": "Lever: Learning to verify language-to-code generation with execution",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning",
            "year": 2023
        },
        {
            "authors": [
                "Vaishali Pal",
                "Evangelos Kanoulas",
                "Maarten Rijke"
            ],
            "title": "Parameter-efficient abstractive question answering over tables or text",
            "venue": "In Proceedings of the Second DialDoc Workshop on Documentgrounded Dialogue and Conversational Question Answering,",
            "year": 2022
        },
        {
            "authors": [
                "Ankur Parikh",
                "Xuezhi Wang",
                "Sebastian Gehrmann",
                "Manaal Faruqui",
                "Bhuwan Dhingra",
                "Diyi Yang",
                "Dipanjan Das"
            ],
            "title": "ToTTo: A controlled table-to-text generation dataset",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Panupong Pasupat",
                "Percy Liang"
            ],
            "title": "Compositional semantic parsing on semi-structured tables",
            "venue": "CoRR, abs/1508.00305,",
            "year": 2015
        },
        {
            "authors": [
                "Xinyu Pi",
                "Bing Wang",
                "Yan Gao",
                "Jiaqi Guo",
                "Zhoujun Li",
                "Jian-Guang Lou"
            ],
            "title": "Towards robustness of text-to-SQL models against natural and realistic adversarial table perturbation",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Matt Post"
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "In Proceedings of the Third Conference on Machine Translation: Research Papers,",
            "year": 2018
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-totext transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Akash Srivastava",
                "Charles Sutton"
            ],
            "title": "Autoencoding variational inference for topic models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Lya Hulliyyatus Suadaa",
                "Hidetaka Kamigaito",
                "Kotaro Funakoshi",
                "Manabu Okumura",
                "Hiroya Takamura"
            ],
            "title": "Towards table-to-text generation with numerical reasoning",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Laurens van der Maaten",
                "Geoffrey Hinton"
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of Machine Learning Research,",
            "year": 2008
        },
        {
            "authors": [
                "Nancy X.R. Wang",
                "Diwakar Mahajan",
                "Marina Danilevsky",
                "Sara Rosenthal"
            ],
            "title": "SemEval-2021 task 9: Fact verification and evidence finding for tabular data in scientific documents (SEM-TABFACTS)",
            "venue": "In Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval2021), pp. 317\u2013326,",
            "year": 2021
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V Le",
                "Ed H. Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou"
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Yingyao Wang",
                "Junwei Bao",
                "Chaoqun Duan",
                "Youzheng Wu",
                "Xiaodong He",
                "Tiejun Zhao"
            ],
            "title": "MuGER2: Multi-granularity evidence retrieval and reasoning for hybrid question answering",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Jingfeng Yang",
                "Aditya Gupta",
                "Shyam Upadhyay",
                "Luheng He",
                "Rahul Goel",
                "Shachi Paul"
            ],
            "title": "TableFormer: Robust transformer modeling for table-text encoding",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Yunhu Ye",
                "Binyuan Hui",
                "Min Yang",
                "Binhua Li",
                "Fei Huang",
                "Yongbin Li"
            ],
            "title": "Large language models are versatile decomposers: Decomposing evidence and questions for table-based reasoning",
            "venue": "In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2023
        },
        {
            "authors": [
                "Pengcheng Yin",
                "Graham Neubig",
                "Wen-tau Yih",
                "Sebastian Riedel"
            ],
            "title": "TaBERT: Pretraining for joint understanding of textual and tabular data",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8413\u20138426,",
            "year": 2020
        },
        {
            "authors": [
                "Tao Yu",
                "Chien-Sheng Wu",
                "Xi Victoria Lin",
                "bailin wang",
                "Yi Chern Tan",
                "Xinyi Yang",
                "Dragomir Radev",
                "richard socher",
                "Caiming Xiong"
            ],
            "title": "Gra{pp}a: Grammar-augmented pre-training for table semantic parsing",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Zeyu Zhang",
                "Thuy Vu",
                "Alessandro Moschitti"
            ],
            "title": "Joint models for answer verification in question answering systems",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Yilun Zhao",
                "Linyong Nan",
                "Zhenting Qi",
                "Rui Zhang",
                "Dragomir Radev"
            ],
            "title": "ReasTAP: Injecting table reasoning skills during pre-training via synthetic reasoning examples",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Yilun Zhao",
                "Chen Zhao",
                "Linyong Nan",
                "Zhenting Qi",
                "Wenlin Zhang",
                "Xiangru Tang",
                "Boyu Mi",
                "Dragomir Radev"
            ],
            "title": "Robut: A systematic study of table qa robustness against human-annotated adversarial perturbations",
            "venue": "arXiv preprint arXiv:2306.14321,",
            "year": 2023
        },
        {
            "authors": [
                "Victor Zhong",
                "Caiming Xiong",
                "Richard Socher"
            ],
            "title": "Seq2sql: Generating structured queries from natural language using reinforcement learning",
            "year": 2017
        },
        {
            "authors": [
                "Zhang"
            ],
            "title": "2021) built over TANDA by considering remaining answer candidates as evidence while deciding the appropriateness of a particular answer candidate. This was further extended by Iyer et al. (2023) who modeled the relation between other similar question samples and corresponding answers in the dataset with the given question and answer",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Understanding tabular data through ML models has been extensively explored through various tasks such as question-answering (QA) (Chen et al., 2022; Cheng et al., 2022; Nan et al., 2022), factverification (Chen et al., 2020b; Wang et al., 2021; Aly et al., 2021), table-to-description generation (Chen et al., 2020a; Parikh et al., 2020; Chen et al., 2020c; Suadaa et al., 2021; Nan et al., 2021) and table grounded dialogue (Nakamura et al., 2022). Table QA has been studied with a specific focus as it allows to conveniently query the table in natural language to extract desired information. Large Language Models (LLMs), which have shown remarkable generalization on various Natural Language Processing (NLP) tasks, have also been used to reason over tables achieving impressive performance (Yu et al., 2021; Neeraja et al., 2021; Gu et al., 2022; Chen, 2023).\nTables contain information organized in rows and columns, and typical transformer-based LLMs such as BERT (Devlin et al., 2019), T5 (Raffel et al., 2020), and GPT (Brown et al., 2020) trained over unstructured natural language text using standard language modeling objectives do not account for the table structure and underlying compositionality of data (Yu et al., 2021). Many works on table understanding therefore, adapt LLMs for tables through joint learning over tabular and text content (Yin et al., 2020), pre-training on table semantic parsing (Liu et al., 2022; Jiang et al., 2022) and synthesizing template-based questions to improve reasoning skills over tables (Gu et al., 2022). Typically, only a small number of cells contain the information required to derive the answer for a question. The irrelevant tabular data acts as distracting information or noise, resulting in suboptimal performance since LLMs are susceptible to noise in the input (Kumar et al., 2023; Chen et al., 2023a). Performance degradation is further amplified in large tables due to presence of even more data as illustrated in Figure 4 in Section 4.3.\n*equal contribution; +work done during internship at Adobe Media and Data Science Research Lab\nSignificant efforts have been made to mitigate the issue of noise by pruning tabular data, albeit at cost of accuracy (Krichene et al., 2021), and by retrieving content from table for QA (Wang et al., 2022; Lei et al., 2023; Kumar et al., 2023). DATER (Ye et al., 2023), one of the state-of-the-art methods for table QA, proposed decomposing a table into simpler sub-tables containing information needed to answer the question by providing in-context examples to GPT-3 based Codex (Chen et al., 2021).\nSuch a question-conditioned hard decomposition of table is sub-optimal as the subsequent QA model cannot correct the error made during decomposition if relevant information is not selected (as shown in Figure 1). To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) \u2013 a framework for table QA that weighs different table parts based on their relevance to the question without explicitly removing any content. CABINET comprises a relevance scorer (\u00a7 3.1), which takes question and table as input to provide a relevance score to table content. The score is used to weigh corresponding content passed to the QA LLM, allowing it to focus more on the relevant content. The relevance scorer is unsupervised and trained with QA LLM differentiably due to lack of annotations denoting relevant table information. Although answer generation loss enables learning of relevance scorer, it acts as an indirect training signal.\nHence, to aid relevance scorer, inspired by how humans process tables, CABINET employs a parsing statement generator (\u00a7 3.2) that describes which rows and columns are relevant to the question. For instance, consider the example in Figure 1, CABINET generates \u201cconsider rows with result as \u2019loss\u2019, and note the higher value in the \u2019score\u2019 column\u201d. The parsing statement is then used to identify corresponding cells, and their content is given more weight during relevance scoring. CABINET establishes new SoTA on three challenging table QA datasets (WikiTQ, FeTaQA and WikiSQL) significantly outperforming various strong baselines (\u00a7 4.1). We show that CABINET is more robust to noise in tables and structural biases i.e. row and column ordering (\u00a7 4.2). Further, the performance gains achieved by CABINET are even more pronounced for larger tables (\u00a7 4.3), indicating that it successfully mitigates noisy table information irrelevant to a given question."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Table Specific Architecture: Tables contain information in a structured format, organized in rows and columns. Hence, many works have focused on developing table-specific models to utilize the semantics of table structure through its description. TabBERT (Yin et al., 2020) pre-trains BERT (Devlin et al., 2019) on paired table-text samples through masked language modeling (MLM). Deng et al. (2020) modified the bidirectional attention in BERT to incorporate table structure while performing MLM. TAPAS (Herzig et al., 2020) utilizes positional embeddings for rows and columns to explicitly capture cell location. Yang et al. (2022) noted that methods using positional embeddings are vulnerable to column and row permutations. To address this, they introduce TableFormer, a table-text encoding architecture that incorporates tabular structure through learnable attention biases. We show that LLMs become less susceptible to such permutations by learning to focus on relevant table parts through CABINET (\u00a7 4.2).\nTable QA Specific Pre-training: Eisenschlos et al. (2020) argued that the MLM objective to just fill in the blanks of table cells and descriptions is insufficient to capture relations between cells and associated text needed to perform table QA. They introduced additional pre-training tasks that require explicit question-table reasoning and complex table operations (such as aggregation). Other improvements include handling of numeric tokens (Han et al., 2022), temporal relations (Zhao et al., 2022), and selectively masking tokens that require table based reasoning (Gu et al., 2022). Methods like TAPEX (Liu et al., 2022), OmniTab (Jiang et al., 2022) etc. typically involve joint training over natural language-SQL pairs so that the underlying model learns to map the information implied in the question to the required table operations. However, as discussed in experiments (\u00a7 4.2 and 4.3), these methods suffer significant performance drop when dealing with large and noisy tables owing to their limited capability at identifying information relevant to question.\nFew/Zero-Shot Learning with Large Language Models: Given the remarkable performance of LLMs on various tasks without any task-specific training, their use for table understanding has also been explored extensively. Chen (2023) have shown that LLMs perform strongly on various table QA tasks using Chain of Thought (CoT) (Wei et al., 2022; Wang et al., 2023) prompting. Since typical LLMs are trained over unstructured text data, models specifically designed to handle structured data, such as StructGPT (Jiang et al., 2023) have also been used for table QA. LEVER (Ni et al., 2023) and BINDER (Cheng et al., 2023) utilized code-optimized GPT-Codex (Chen et al., 2021) to generate SQL statements that can be executed to answer questions over tabular data. DATER (Ye et al., 2023) uses Codex to break table into sub-tables conditioned on a given question through incontext learning. Such methods have no way to recover relevant table part to generate the correct answer in case it is omitted while generating sub-tables (as discussed in Figure 1)."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "We summarize the architecture of CABINET in Figure 2. It comprises two components: 1) Unsupervised Relevance Scorer, an unsupervised module comprising a transformer encoder that takes question and table as input and tokenizes them (steps 1 and 2 in Fig. 2) followed by assigning a relevance score to each table token (step 3 in Fig. 2). The relevance score is then multiplied with the corresponding token embedding at the time of giving it as input to QA LLM encoder (step 7 in Fig. 2). This ensures that noisy content with lower relevance score get suppressed and the QA LLM can focus on relevant tokens. The unsupervised relevance scorer is connected to QA LLM in a differentiable manner enabling it to be trained through answer generation loss (step 8 in Fig. 2).\nEven though answer generation loss enables learning of unsupervised relevance scorer, it acts as an indirect training signal. To aid relevance scoring, we propose a weakly supervised module: 2) Relevant Cell Predictor through Table Parsing that parses table conditioned on question to highlight cells containing relevant information (steps 4 and 5 in Fig. 2). It comprises two substeps where we first train a Parsing Statement Generator that describes criteria in natural language about which rows and columns should be used to derive the answer (step 4 in Fig. 2). Table cells corresponding to the criteria described in the parsing statement (step 5 in Fig. 2) are highlighted such that score for content tokens in highlighted cells is boosted by combining it with unsupervised relevance score through a linear combination (step 6 in Fig. 2). We conduct extensive ablations to establish efficacy for different modules (\u00a7 4.4). We now discuss the details of each component."
        },
        {
            "heading": "3.1 UNSUPERVISED RELEVANCE SCORER",
            "text": "The unsupervised relevance scorer is used to assign a score to table content tokens. Since annotating cells of a table relevant to a given question is tedious, the relevance scorer is unsupervised and gets trained along with QA LLM through answer generation loss. Formally, consider a pair of table T and a question Q about T . Qtokens = {q1, q2, ..., q|Q|} represents the question tokens, T = {cij |1 \u2264 i \u2264 Nrow, 1 \u2264 j \u2264 Ncol}, where Nrow and Ncol indicate number of rows and columns in T respectively, and cij represents string in cell in the ith row and jth column. To make T suitable to be fed as input to a transformer-based LLM, we follow the commonly used linearising scheme (Liu et al., 2022) where table is flattened as (step 1 in Fig. 2):\nTflattened = [HEAD] : c11 | c12 | \u00b7 \u00b7 \u00b7 | c1Ncol | [ROW ]1 : c21 | \u00b7 \u00b7 \u00b7 | c2Ncol | [ROW ]2 : \u00b7 \u00b7 \u00b7 (1)\n[HEAD] and [ROW ]k indicate start of column header row and kth data row respectively. We separate special tokens and cell content using pipe symbol \u2018|\u2019. The string in Equation 1 is tokenized"
        },
        {
            "heading": "Relevant Cell Predictor through Table Parsing",
            "text": "using the tokenizer of underlying QA LLM to obtain table tokens Ttokens = {t1, t2, ..., t|Ttokens|}. Ttokens is concatenated to Qtokens to obtain Itokens = (Qtokens; Ttokens) which is given as input (steps 2 and 3 in Fig. 2) to Unsupervised Relevance Scorer (URS) comprising a transformer encoder TEURS . The contextualized representation hp \u2208 Rd of the pth token is estimated as:\neURS1 , e URS 2 , \u00b7 \u00b7 \u00b7 ,eURS|Itokens| = EmbeddingURS(Itokens) (2)\nh1, \u00b7 \u00b7 \u00b7 , hp, \u00b7 \u00b7 \u00b7 , h|Itokens| = TEURS(e URS 1 , e URS 2 , \u00b7 \u00b7 \u00b7 , eURS|Itokens|) (3)\nWe aim to predict relevance score for each table token, however, since annotations for relevant table parts are unavailable, token relevance is not explicitly observable and we consider it as a latent variable. Further, we hypothesize that the representation space of table tokens can be structured better for modeling relevance by clustering their encodings into two categories - relevant and nonrelevant. Variational Inference (VI) has been commonly used to estimate latent variable probability and group data points on the basis of latent topics (Srivastava & Sutton, 2017). Hence, we estimate relevance \u03b7unsp of table token tp (|Qtokens|+ 1 \u2264 p \u2264 |Qtokens|+ |Ttokens|) as (step 3 in Fig. 2):\n\u00b5p = \u03d5\u00b5(hp); \u03c3p = \u03d5\u03c3(hp) (4) \u03b7unsp = sigmoid(zp); zp = \u00b5p + s \u2217 \u03c3p (5)\ns is sampled from standard normal distribution, \u03d5\u00b5 and \u03d5\u03c3 are FC layers with weights W\u00b5 \u2208 Rd\u00d71 and W\u03c3 \u2208 Rd\u00d71, sigmoid is applied to normalize the relevance score in the range 0 to 1. To enable the relevance scorer to assign appropriate scores, we structure the latent space of TEURS by clustering table tokens into relevant and non-relevant. We use the method of van der Maaten & Hinton (2008) (details in appendix A.1) which performs clustering in a trainable manner using clustering loss Lclu . We apply Lclu over latent representation hp of tokens which enables us to tune TEURS for clustering. During experiments, we observed that unit vectors for cluster centroids \u00b5clurelevant and \u00b5 clu irrelevant are not well separated. To mitigate this, we enforce a separation loss Lsep that increases the distance between unit vectors representing cluster centroids:\nLsep = 2\u2212 \u2223\u2223\u2223\u2223\u2223\u2223\u00b5clurelevant \u2212 \u00b5cluirrelevant\u2223\u2223\u2223\u2223\u2223\u22232 (6)\nFurther, it is desirable that relevance scores for tokens in one cluster (corresponding to irrelevant tokens) are low. To achieve this, we apply a sparsification loss Lsparse where the score logit zp is exponentiated with a negative coefficient to push logit values for relevant and irrelevant clusters to \u221e and \u2212\u221e respectively that enables final score (after applying sigmoid) to be close to 1 and 0:\nLsparse = 1 |Ttokens| \u2211 p e\u2212z 2 p ; |Qtokens|+ 1 \u2264 p \u2264 |Qtokens|+ |Ttokens| (7)\nAt the time of providing question and table as input to transformer encoder TEQA of the QA LLM, embedding (e \u2032\np) corresponding to question tokens is used as is while embedding of each table token\nis multiplied by its corresponding relevance score (steps 7 and 8 in Fig. 2): e1, e2, \u00b7 \u00b7 \u00b7 , e|Itokens| = EmbeddingQA(Itokens) (8)\ne \u2032\np = \u03b7p \u2299 ep; |Qtokens|+1 \u2264 p \u2264 |Qtokens|+ |Ttokens| (9)\nh \u2032 1, \u00b7 \u00b7 \u00b7 , h \u2032 |Itokens| = TEQA(e \u2032 1, e \u2032 2, \u00b7 \u00b7 \u00b7 , e \u2032 |Itokens|) (10)\na1, a2, \u00b7 \u00b7 \u00b7 , aN = TDQA(h \u2032 1, \u00b7 \u00b7 \u00b7 , h \u2032\n|Itokens|) (11) \u2018\u2299\u2019 indicates scalar multiplication with vector operation, TDQA represents the transformer decoder of the QA LLM that generates the answer tokens an sequentially. TEURS , TEQA and TDQA are trained in an end-to-end manner through cross-entropy loss LCE between the generated and ground-truth answer tokens. Thus, the total loss L becomes:\nL = LCE + \u03bbclu \u2217 Lclu + \u03bbsep \u2217 Lsep + \u03bbsparse \u2217 Lsparse (12) The answer generation loss acts as an indirect training signal for relevance scorer. To aid unsupervised scorer, we propose a weakly-supervised module (trained separately from URS and QA LLM) that highlights relevant cells (discussed in next subsection). Table tokens for highlighted cells are assigned cell-based score \u03b7cellp that is combined with unsupervised relevance score \u03b7 uns p through linear combination (step 6 in Fig. 2). Thus, final relevance score \u03b7p used in Eq. 9 is: \u03b7p = \u03bbuns \u2217 \u03b7unsp + \u03bbcell \u2217 \u03b7cellp (13)"
        },
        {
            "heading": "3.2 RELEVANT CELL PREDICTOR THROUGH TABLE PARSING",
            "text": "As discussed above, we train a separate module to highlight table cells relevant to a given question in a weakly-supervised manner. Since there is no Table QA dataset that contains annotations for table cells useful to answer a given question, we adopt a two-stage approach where we first train a Parsing Statement Generator (step 4 in Fig. 2) to generate a natural language text describing criteria for rows and columns relevant to the given question. Subsequently, we train another model that takes the parsing statement and table as input to identify the cells matching the criteria (step 5 in Fig. 2).\nParsing Statement Generator (PSG) comprises a pre-trained LLM - Flan T5-xl (Chung et al., 2022a) that is fine-tuned to take the question and table as input (Itokens) to generate a parsing statement textparse (step 4 in Fig. 2). The statement describes criteria stating which rows and columns are useful to derive the answer. To bootstrap training of PSG, we manually annotate very few (\u223c300) question-table pairs with parsing statement. For instance, for table and question shown in Figure 2, we annotate parsing statement as \u2018To derive answer, note the values of higher score in rows with result as loss\u2019. To circumvent annotating samples for each table QA dataset, we choose WikiTableQuestions (WikiTQ) dataset (Pasupat & Liang, 2015) to select the samples for annotation since it is the most complex QA dataset containing a variety of samples. We sample diverse set of questions for annotation (please refer appendix A.2 for details and examples). The sampled question along with its table are manually annotated with parsing statement which is used to fine-tune PSG. The trained PSG model is then used to generate parsing statement for any question-table pair from datasets studied for experiments. We release the dataset of manually annotated parsing statements.\nCell Highlighting based on Parsing Statement: To identify table cells for the criteria described in the parsing statement textparse, we need a way to map the statement to corresponding cells. To this end, we use ToTTo dataset (Parikh et al., 2020) that contains samples of (table, list of highlighted cell coordinates) pairs. Each pair is accompanied by a text description summarising the content of the corresponding list of cells. We fine-tune a cell highlighting model Cell HighlighterLLM comprising of Flan T5-xl on ToTTo dataset where the table and summarising text are given as input to generate the content of corresponding highlighted cells. Once Cell HighlighterLLM is trained, we provide the table and textparse as input to identify and generate content of corresponding cells. For instance, consider example in Figure 2, given the parsing statement shown in figure as input, the cell predictor generates \u203238\u2212 12 \u2225 loss \u2225 45\u2212 0 \u2225 loss \u2225...\u2032 (step 5). More formally,\nchighlighted1 || \u00b7 \u00b7 \u00b7 || c highlighted M = Cell HighlighterLLM (T , textparse) (14)\nchighlightedr represents the string of r th highlighted cell predicted based on parsing statement. M is a variable number, \u2018||\u2019 is a delimiter to separate cell content. For 1 \u2264 r \u2264 M , if chighlightedr exactly matches with the content of some cell in T , then the tokens tp of matching cell is assigned a cell relevance score (\u03b7cellp ) of 1. \u03b7 cell p is set to 0 for table tokens belonging to cells in T whose content does not match with any chighlightedr . \u03b7 cell p is then combined with unsupervised relevance score \u03b7 uns p as in Eq. 13. We now discuss experiments performed to validate the efficacy of our approach."
        },
        {
            "heading": "4 EXPERIMENTS AND EVALUATION",
            "text": "Implementation Details: For the encoder (TEQA) and decoder (TDQA) of the QA LLM, we employ the OmniTab (Jiang et al., 2022) backbone (pre-trained for table understanding) comprising of BART-Large (Lewis et al., 2020). The embeddings of unsupervised relevance scorer (URS), EmbeddingURS , and QA model, EmbeddingQA, are shared. URS encoder (TEURS) is initialized with the architecture and weights of QA LLM encoder (TEQA), though they do not share weights during QA training. Consequently, the hidden dimension d of TEURS is 1024. We train CABINET and baselines (wherever needed) for 30 epochs on an effective batch size (BS) of 128 using 8 80GB A100 GPUs (BS of 8/GPU with gradient accumulation 2) using a learning rate of 1e\u22125 with cosine annealing (Loshchilov & Hutter, 2017) through AdamW optimizer (Loshchilov & Hutter, 2019).\nDatasets and Evaluation Metrics: We evaluate CABINET on three commonly used datasets \u2013 (i) WikiTableQuestion (WikiTQ) (Pasupat & Liang, 2015) which is one of the most commonly used and highly complex datasets consisting of about 2100 HTML tables from Wikipedia and about 22, 033 questions that require complex operations such as comparison, aggregation, and arithmetic operations to arrive at the answer; (ii) FeTaQA (Nan et al., 2022), a challenging dataset consisting of about 10, 000 questions that have a long-form natural language answer (18 words on average) such that it requires fetching multiple entities from the table, aggregating and reasoning over these entities, and structuring the inferred information to produce a coherent answer; and (iii) WikiSQL (Zhong et al., 2017) that comprises roughly 80, 654 questions over 24, 241 Wikipedia tables. It also provides the equivalent SQL query for each question to obtain the correct answer. While we do not generate SQL (or other implicit logical forms) and only use the natural language questions and answers from this dataset, it serves as a useful benchmark to compare CABINET with table understanding methods that generate explicit logical forms to extract relevant answers from table. The ground-truth answers in both WikiTQ and WikiSQL datasets are short (1-2 words). Hence, we use exact-match accuracy (Acc.) to compare various methods. For FetaQA dataset, ground-truth answers being long-form (\u2248 18 words on average), we employ commonly used overlap-based metric Sacre-BLEU (S-BLEU) (Post, 2018). We report performance on test split for all datasets."
        },
        {
            "heading": "4.1 PERFORMANCE OF CABINET ON TABLE QA",
            "text": "We present a detailed comparative analysis of results achieved by CABINET with a variety of baselines. We consider three different categories of methods \u2013 (i) LLMs specifically pre-trained for table understanding and fine-tuned for QA, such as TAPEX (Liu et al., 2022), ReasTAP (Zhao et al., 2022) and OmniTab (Jiang et al., 2022); (ii) fine-tuning LLMs (pretrained on text only) such as T5-3b (Raffel et al., 2020) and Flan T5-xl (Chung et al., 2022b); and (iii) few or zero shot prompting of LLMs like StructGPT (Jiang et al., 2023) and approaches that employ such LLMs for in-context learning like LEVER (Ni et al., 2023), BINDER (Cheng et al., 2023) and DATER (Ye et al., 2023).\nTable 1 presents the performance of various methods on the WikiTQ dataset, and we can observe CABINET with an accuracy of 69.1% outperforms the best-performing baselines in each of the three categories and establishes new state-of-the-art. Specifically, CABINET outperforms OmniTab, DATER, and fine-tuned Flan T5-xl by 6.4%, 3.2% and 4.7%, in absolute terms, respectively. Also, note that simple prompting of ChatGPT does not work well for Table QA. We want to highlight that CABINET performs much better than GPT-3 and Codex-based SoTA in-context learning methods despite containing orders of magnitude fewer parameters."
        },
        {
            "heading": "Method S-BLEU # params Fine-tuning Table-specific LLMs",
            "text": ""
        },
        {
            "heading": "Fine-tuning text-based LLMs",
            "text": ""
        },
        {
            "heading": "Few/zero shot Prompting of LLMs",
            "text": "Table 3: Comparison with different categories of baselines on WikiSQL. CABINET achieves better Accuracy (Acc.).\nMethod Acc. # params\nFine-tuning Table-specific LLMs TAPAS (Herzig et al., 2020) 86.4 345 M GraPPa (Yu et al., 2021) 84.7 355 M DoT (Krichene et al., 2021) 85.5 299 M TAPEX (Liu et al., 2022) 86.4 406 M OmniTab (Jiang et al., 2022) 87.9 406 M UTP (Chen et al., 2023b) 88.1 345 M ReasTAP (Zhao et al., 2022) 88.8 406 M\nFine-tuning text-based LLMs T5-3b (Xie et al., 2022) 85.9 2.9 B FlanT5-xl 87.8 2.9 B\nFew/zero shot Prompting of LLMs ChatGPT (Jiang et al., 2023) 51.6 175 B StructGPT (Jiang et al., 2023) 54.4 175 B\nCABINET (Ours) 89.5 560 M\nSimilar observations hold for FeTaQA (Table 2) and WikiSQL (Table 3) datasets where CABINET achieves new SoTA performance. For generating long descriptive answers for questions in FeTaQA, CABINET achieves SoTA S-BLEU of 40.5 outperforming OmniTab, fine-tuned Flan T5-xl and DATER by a margin of 5.6, 4.3 and 9.6 absolute percentage points, respectively. We report performance only for baselines that have explored the dataset in their work (except for T5 and Flan T5). We use their code/API for evaluation if available or else specify performance as reported in their paper. Similarly, for WikiSQL dataset, CABINET pushes the SoTA by 0.7% on already high performance of ReasTAP (current SoTA). Since best performance on WikiSQL is already high, the absolute performance gains of 0.7% should be interpreted as a proportion of scope of further improvement possible, i.e., 0.7/(100\u2212 88.8), which is \u2248 6%."
        },
        {
            "heading": "4.2 HOW ROBUST IS CABINET TO NOISE AND IRRELEVANT INFORMATION?",
            "text": "Despite the remarkable success of transformer-based models on table understanding, they are sensitive to noise and perturbations to the tabular data (Pi et al., 2022; Yang et al., 2022; Zhao et al., 2023). We examine the robustness and sensitivity of CABINET towards noise while performing Table QA. We introduce noise by perturbing tables in test split and report the relative percentage drop in performance. We perform four types of perturbations: 1) Row Addition (RA): insert noise into a table by adding rows from another table that contains same number of columns; 2) Row Permutation (RP): randomly permute ordering of rows (Pi et al., 2022); 3) Column Permutation (CP): randomly permute column ordering; and (4) Cell Replacement (CR): replace content of certain cells with content from some other table. We perform each perturbation separately to obtain four perturbed test splits for each dataset. Please see appendix A.5 for further details about the procedure.\nFigure 3 summarizes the relative drop in performance of CABINET and the dataset-specific best baseline for the three datasets. Note that for all the perturbation categories, CABINET leads to significantly less drop in performance when compared with the corresponding baseline, highlighting the robustness and ability of CABINET to identify the relevant portions of the underlying table. Specifically, CABINET is significantly less sensitive to row and column permutations (RP and CP), indicating that relevance scoring of tokens helps the QA LLM to focus more on relevant information and reduces the potential ordering biases commonly observed in models pre-trained on tabular data (Yang et al., 2022). For the cell replacement (CR) and row addition (RA) perturbations, where extraneous information is explicitly added to the table, the drop in performance suffered by CABINET is significantly less compared to the baselines owing to the superior ability of CABINET to identify relevant information. For instance, in the case of WikiTQ, the relative drop in performance for RA is \u2248 19% for OmniTab, almost 40% higher than CABINET (\u2248 11.5%). This consistent trend holds for FeTaQA and WikiSQL datasets as well.\nOmniTab CABINET\n-18.89\nAc cu\nra cy 20 30 40 50 60 70 80\n0-1 00 100 -20 0 20 0-3 00 300 -40 0 400 -50 0 500 +\nOmniTab\nCABINET\n-18.89-11.46 -7.85-4.42-4.20-1.99-10.32-4.83-6.5-2.15-1.1 -1.07ReasTAP\nCABINET\n7275 67 73\n63 70\n53 57\n40 47\n28 38\nOmniTAB CABINET enter your text here enter your text here enter your text here enter your text here\n(a) WikiTQ\nSBL\nEU\n10\n20\n30\n40\n50\n0-4 0 40- 80 80- 120 120 -16 0 160 -20 0 200 +\nOmniTab\nCABINET\n-18.89 -11.46-7.85\n-4.42-4.20-1.99-10.32-4.83-6.5 -2.15-1.14 -1.07ReasTAP\nCABINET\n72 75 67 73\n63 70\n57\nOmniTAB\nCABINET\nenter your text here\nenter your text here\nenter your text here enter your text here\n37 41 37 41 38 42\n23\n34\n15\n36 36\n18\nOmniTab CABINET enter your text here enter your text here enter your text here enter your text here\n(b) FeT QA\nAc cu ra cy\n60\n70\n80\n90\n0-1 00 100 -20 0 200 -30 0 300 -40 0 400 -50 0 500 +\nOmniTab CABINET -18.89-11.46-7.854. 2-4.201.99-10.324.86.57-2.151. 4- .07ReasTAP I ET\n89 91\n8988\n63\n90\n57\nReasTAP CABINETenter your text hereenter your text here\n374137413842233415\n3636\n18\nOmniTab\nCABINET\nenter your text here enter your text hereenter your text here enter your text here\n88 85\n86\n76 82\n70\n79\nenter your text here enter your text here\n(c) WikiSQL\nFigure 4: Variation in performance with table size (# cells). We compare CABINET (green) with OmniTab (red) on WikiTQ (left) and FeTaQA (middle), and against ReasTAP (red) for WikiSQL (right). It can be seen that CABINET performs much better than the baselines on larger tables."
        },
        {
            "heading": "4.3 IMPACT OF TABLE SIZE ON PERFORMANCE",
            "text": "We now study how CABINET performs with tables of different sizes. Tables typically comprise a large amount of data, so the entire information is usually not required to answer a given question and acts as distracting information (Neeraja et al., 2021). This noise or irrelevant data poses a severe challenge for table understanding models and leads to poor generalization for larger tables (Kumar et al., 2023; Chen, 2023). We consider the number of cells in the table as a proxy for its size and bin all the questions in the three datasets into six categories based on the number of cells (Figure 4) and compare the performance of CABINET with dataset-specific best-performing baseline. We note that for all the datasets, while model performance drops with increasing table size, CABINET consistently and significantly outperforms the baseline methods across all table size categories. Moreover, the differences become starker for larger tables. For instance, for the largest tables in FeTaQA, CABINET achieves double the S-BLEU scores compared to OmniTab (36 vs. 18). Similarly, for the other two datasets, CABINET achieves significantly high performance for the largest tables (> 500 cells) compared to the baselines \u2013 accuracy of 38 vs. OmniTab\u2019s 28 for WikiTQ and 79 vs. ReasTAP\u2019s 70 for WikiSQL. These empirical observations provide further evidence for CABINET\u2019s ability to identify relevant content, making the QA LLM relatively robust to table size."
        },
        {
            "heading": "4.4 DISCUSSION ON THE IMPACT OF DIFFERENT DESIGN CHOICES FOR CABINET",
            "text": "Effect of Clustering Table Tokens: We study the impact of clustering the table tokens using their latent representations (discussed in Section 3.1). To do so, we toggle the clustering loss (Lclu), cluster centroids separation loss (Lsep), and score sparsification loss (Lsparse) by setting their weight (\u03bbclu, \u03bbsep, \u03bbsparse) to 0 or 1. For this study, we only use unsupervised relevance scorer by turning off weakly supervised cell predictor to eliminate other influencing factors. Results are summarized in Table 4 where we can observe that applying all three losses yields the best performance (row 6). Specifically, for WikiSQL, clustering improves performance when score sparsification loss is applied (row 4 vs. row 3) which is due to the fact that sparsification enables categorizing scores into low and high. For WikiTQ and FeTaQA, adding the cluster centroids separation loss further increases the efficacy of clustering and sparsification yielding the best results.\nCombining Unsupervised Relevance Scorer with Cell Predictor: We vary the relative importance given to relevance score predicted by unsupervised relevance scorer and weakly-supervised cell predictor by varying \u03bbuns and \u03bbcell in Eq. 13. Table 5 shows that combining the two modules yields much better accuracy for WikiTQ and FeTaQA compared to just using unsupervised relevance scorer (row 1 vs. row 2). This highlights that the weakly-supervised cell predictor complements unsuper-\nTable 4: Effect of applying clustering (Lclu), centroid separation (Lsep) and relevance score sparsification loss (Lsparse). Clustering table tokens by enforcing sparsity in relevance scores and distance between cluster centroids improves performance.\nTable 5: Impact of combining unsupervised relevance score (weight \u03bbuns) and weaklysupervised cell-based relevance score (weight \u03bbcell). Fusing the relevance from both components gives optimal performance.\nvised scorer by identifying further relevant table content (Figure 5 depicts qualitative visualisation for the same). For WikiSQL, same performance is observed with and without the cell predictor. Further it is observed that using only the cell predictor (last row) achieves significantly low performance due to the fact that the number of cells highlighted by the cell predictor is much lesser resulting in assigning a score of zero to most table content in cases where it misses to identify important cells.\nWe show CABINET can be used with TAPEX backbone (instead of OmniTab) to improve it\u2019s performance showing generality of our framework (Appendix A.7). We show that giving parsing statement as input to QA LLM, replacing URS with BERT based similarity metric for relevance scoring, and using question directly instead of parsing statement to generate highlighted cells gives sub-optimal performance compared to CABINET, justifying our design choice (Appendix A.8). Appendix A.10 shows case study depicting how clustering losses interact to yield improvements. Appendix A.11 shows that CABINET can be used to improve other NLP tasks like reading comprehension."
        },
        {
            "heading": "5 CONCLUSIONS",
            "text": "We studied the problem of question-answering over tables and focused on identifying the relevant portions of the table to derive the answer. Generally, only a small subset of the tabular data is required to answer the question, and owing to the vulnerability of LLMs to noise, the extraneous information leads to sub-optimal performance. This problem is further exacerbated in the case of large tables. Our proposed framework, CABINET addresses this issue by weighing the table content based on its relevance to the question, identifying the relevant rows and columns, and highlighting the content of the relevant cells. CABINET establishes new SoTA on three commonly used challenging benchmarks, outperforming table-specific models, as well as methods that employ incontext learning with much larger GPT-3 scale models. We show empirically that CABINET is more robust to noise and generalizes well for larger tables, indicating its efficacy in mitigating noise and overcoming table structural biases typically learned during training."
        },
        {
            "heading": "6 ETHICS AND REPRODUCIBILITY STATEMENT",
            "text": "We use publicly available datasets and LLMs (which are commonly used) to conduct the study in our work. The only data that we annotate is \u223c 300 samples of table-question pairs with parsing statement describing rows and columns relevant to question. The parsing statement were written keeping in mind the safety and ethics guidelines without any potential concerns. To encourage reproducibility, we release our code and datasets (including manually written parsing statements) at this link. We describe the details of the datasets in \u00a7 4 (under \u2018Datasets and Evaluation Metrics\u2019) and the LLMs used in \u00a7 4 (under \u2018Implementation Details\u2019) and \u00a7 3.2. Further, we provide the implementation details of our method in \u00a7 4 (under \u2018Implementation Details\u2019) and discuss baselines used for comparison in \u00a7 4.1. Finally, we elaborate further details of our method in Appendix - Trainable clustering over latent representation of table tokens (A.1), Details of parsing statement annotation procedure (A.2) and Further details on table perturbation procedure (A.5)."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 CLUSTERING LATENT VECTORS",
            "text": "As discussed in Section 3.1, the table tokens are clustered in a trainable manner (van der Maaten & Hinton, 2008) using their latent representations encoded through Unsupervised Relevance Scorer (URS). We discuss the details of the trainable clustering algorithm.\nFormally, the probability of latent vector hp corresponding to the pth token belonging to jth cluster is given by Equation 15\nqpj = (1 + ||hp \u2212 \u00b5cluj ||2/\u03b1)\u2212 \u03b1+1 2\u2211\nj\u2032 (1 + ||hp \u2212 \u00b5cluj\u2032 || 2/\u03b1)\u2212 \u03b1+1 2\n(15)\nHere, hp is the contextualised latent vector of the pth token obtained using TEURS , \u03b1 is the degrees of freedom of the Student\u2019s t distribution, \u00b5cluj is the centroid of the j\nth cluster and j \u2208 {0, 1}. Here, \u00b5clu0 = \u00b5 clu relevant and \u00b5 clu 1 = \u00b5 clu irrelevant. Moreover, the cluster centroids are learnable.\nThe clustering process is iteratively refined by enforcing KL divergence minimization between the probability distribution for each token and a pseudo distribution generated using qpj . Mathematically, the clustering loss is\nLclu = 1\nB \u2211 b KL(Z||Q) = 1 B \u2211 b \u2211 p \u2211 j zpj log zpj qpj\n(16)\nwhere b \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , B}, B is the batch size, and Z is the target distribution. A naive approach to model Z would be setting each zp to a delta distribution (to the nearest centroid) for representations above a confidence threshold and ignoring the rest. However, because qp are soft assignments, it is more natural and flexible to use softer probabilistic targets. So, we model zpj with Equation 17\nzpj = q2pj/fpj\u2211 j\u2032 q 2 pj\u2032/fpj\u2032 , where fpj = \u2211 p qpj is the soft cluster frequency (17)"
        },
        {
            "heading": "A.2 DETAILS OF ANNOTATING TABLE PARSING STATEMENT",
            "text": "As discussed in Section 3.2, we train a parsing statement generator that generates a criteria describing which rows and columns contain information relevant to the table. The generated parsing statement is used to identify corresponding cells which are then combined with the relevant table part detected by the unsupervised relevance scorer through relevance scoring. To bootstrap the training of the parsing statement generator, we manually annotate \u223c300 question-table pairs from the WikiTQ dataset with parsing statement. To select samples to be annotated, we sample questions from WikiTQ as it consists of a diverse and complex set of reasoning questions over tables, thereby sampling questions from this dataset allows us to select a set that is representative of questions in other datasets for Table QA.\nTo select a diverse set of questions from WikiTQ for annotation, we group questions into 5 clusters by clustering their representations such that we randomly sample an equal number of questions from each cluster. Specifically, we use an encoder-only model DeBERTa-V2 to encode the questions. Since the questions pertain to tables, we embed questions in train split of WikiTQ through an LLM possessing table understanding - DeBERTaV2 (He et al., 2023) initialized with weights tuned through table understanding task proposed by PASTA (Gu et al., 2022).\nOnce the questions are clustered, we sample a small subset (2.5%) from each cluster to manually annotate with the parsing statement. Some examples of questions, the parsing statement and the corresponding answer from each cluster can be seen in Table 6."
        },
        {
            "heading": "A.3 ADDITIONAL RELATED WORK",
            "text": "Graphs have been used to capture table structure and similarity between table-question samples across the dataset (Iyer et al., 2023). TANDA (Garg et al., 2020) performs the task of answer sentence selection (AS2) for a given question by fine-tuning the transformer-based model to select the right candidate from answer candidates. They first fine-tune an LLM on the AS2 task followed by adapting it to a specific domain. Zhang et al. (2021) built over TANDA by considering remaining answer candidates as evidence while deciding the appropriateness of a particular answer candidate. This was further extended by Iyer et al. (2023) who modeled the relation between other similar question samples and corresponding answers in the dataset with the given question and answer candidates using a graph. In our current work, we mainly focussed on identifying relevant parts of the table useful to derive the answer to the question, however, a similar method can also be explored as future work that utilizes a graph to capture the table structure (positioning of different cells in the table) for a given sample and also model the similarity between multiple question-table pairs across the dataset. Another direction focuses on applying semantic parsing over the input text (question) and table to generate a logical form (such as SQL) which when executed fetches relevant information (Yu et al., 2021)."
        },
        {
            "heading": "A.4 PERFORMANCE ON NUMERIC VS NON-NUMERIC & RETRIEVAL VS AGGREGATION-BASED ANSWERS",
            "text": "We analyse the performance of CABINET on generating answers of distinct types by categorizing them into four categories: numeric, non-numeric, retrieval, and non-retrieval (aggregation). This meticulous categorization allows us to gain a nuanced understanding of how CABINET performs against baselines for table-question pair that require different types of answers to be generated.\nAc cu ra cy\n50\n60\n70\n80\nRetrieval Aggregation Numeric Non-numeric\nOmniTab CABINET -18.891.46-7.854. 2- 2099- 0.324.6 572 11 .07ReasTAP I\nReasTAP CABINET enter your text here enter your text here\n374137413842233415363618 OmniTab CABINET\nenter your text here\nenter your text here\n76 82\n70 79\nenter your text here enter your text here\nOmniTab CABINETenter your text hereenter your text here enter your text here enter your text here\n74.7 71.3\n48.2\n59.7\n51.1\n59.8\n70.2 75.1\n(a) WikiTQ\nAc cu ra cy\n80\n85\n90\n95\nRetrieval Aggregation Numeric Non-numeric\nOmniTab CABINET -18.891.467.85-4. 2.201 99- 0.32. 36 572 1. 0ReasTAP I\nReasTAP CABINET\n37413741342233415363618 OmniTab CABINET\nenter your text here enter your text here\n76 82 70 79\nReasTAP CABINETenter your text hereenter your text here enter your text here enter your text here\n91.8 91.6\n84.4 85.7\n81.1\n84.3\n89.7\n90.4\n(b) WikiSQL\nFigure 6: Performance comparison between CABINET with OmniTab on WikiTQ and with ReasTAP on WikiSQL when test samples are categorised based on the answer type - retrieval, nonretrieval (aggregation), numeric and non-numeric. We see CABINET provides significantly better performance on all answer type categories. It\u2019s noteworthy to mention that this analysis excludes FeTaQA, as it comprises of free-form long answers, hence categorization is not possible.\nFigure 6 summarises the results. For both WikiTQ and WikiSQL, our method consistently demonstrates better performance for all four categories. We see an improvement of around 8 \u2212 10% for WikiTQ and around 3\u2212 5% for WikiSQL for numeric and non-retrieval type answers, thereby highlighting the improved aggregation capabilities using CABINET."
        },
        {
            "heading": "A.5 DETAILS OF TABLE PERTURBATION",
            "text": "In Section 4.2, we discussed the impact of perturbing the tables on performance. We discuss the detailed steps that we followed to inject the 4 types of perturbation separately in the tables. For Row Addition, we create sets of table with same number of columns. Further, based on the number of cells m in a table, we insert n rows from another table with the same number of columns at a random position. The exact scheme followed is, 1) n = 1 if m \u2264 150, 2) n = 2 if m > 150 & m \u2264 300, 3) n = 5 if m > 300 & m \u2264 450 and 4) n = 8 if m > 450. For row permutation and column permnutation, we randomly permute the rows and columns respectively without any specific conditioning over the number of rows and columns present in the table. For\n18\ncell replacement, we divide the dataset again into the 4 buckets same as that of row addition and randomly replace 0.02%, 0.05%,0.1% and 0.12% of the total number of cells in the table."
        },
        {
            "heading": "A.6 ADDITIONAL IMPLEMENTATION DETAILS",
            "text": "As stated in section 4 (under implementation details) of the main paper, we employ the OmniTab backbone built over BART-Large architecture for the encoder (TEQA) and decoder (TDQA) of the QA LLM. Further, URS encoder (TEURS) is initialized with the architecture and weights of QA LLM encoder (TEQA), though, they do not share weights during training. However, the embedding layers EmbeddingURS and EmbeddingQA share weights. The hidden dimension d of TEURS is 1024, similar to that of both TEQA and TDQA. We train CABINET and other baselines for 30 epochs on an effective batch size (BS) of 128 using 8 80 GB A100 GPUs (BS of 8/GPU with gradient accumulation 2) using learning rate of 1e\u22125 with Cosine Annealing through AdamW optimizer. We carry out hyper-parameter tuning based on the validation set to come up with the optimal values of learning rate (1e\u22125), scheduler (Cosine Annealing), batch size (8), gradient accumulation steps (2) and the optimizer (AdamW). We leverage text pre-trained Flan T5-xl as the backbone for Parsing Statement Generator (PSG) and the Cell Highlighter LLM. The Parsing Statement Generator is trained for 50 epochs on the 300 manually annotated question-table-parsing statement triplets with hyper-parameters setting same as that of CABINET, however the only difference being the effective batch size of 16 (BS of 2/GPU with 8 GPUs without gradient accumulation). On a similar note, the Cell Highlighter LLM is trained for 3 epochs on the ToTTo dataset to generate highlighted cells given the table and a natural language statement describing a subpart of the table as the input. During the inference phase of generating answer, parsing statements and highlighted cells, we use beam search decoding with a beam size of 3."
        },
        {
            "heading": "A.7 CAN CABINET BE USED WITH OTHER QA LLM BACKBONES TO IMPROVE PERFORMANCE?",
            "text": "To verify the generality of CABINET as a framework that can be used with any encoder-decoder style Table QA method, we use TAPEX (Liu et al., 2022) as the backbone to initialise the unsupervised contextual relevance scorer (instead of OmniTab). TAPEX is also used as the underlying QA LLM. Table 7 summarises the results where it can be seen that the performance improves on all the three datasets. This highlights the generality of CABINET in improving performance."
        },
        {
            "heading": "A.8 ABLATIONS ON ALTERNATE DESIGN CHOICES FOR CABINET",
            "text": "We discuss several explorations on alternate ways of utlising the different components of CABINET framework. The results pertaining to those can be seen in table 8.\n1. Instead of using the generated parsing statement to highlight cells for our cell-based scoring mechanism, we feed the parsing statement in the input to the QA LLM along with the question and table in order to verify if parsing statement alone can improve the QA performance when directly used as an instruction to the model. We use the unsupervised relevance scorer along with the QA LLM. Table 8 summarises the results. It can be seen that using the parsing statement as input to the QA model gives significantly sub-optimal performance (row 2 vs row 7 in Table 8) compared to highlighting cells to obtain the corresponding scores that can be combined with the relevance scores predicted by the unsupervised relevance scorer.\nMethod WikiTQ FeTaQA WikiSQL OmniTab 63.1 35.9 85.8\nCABINET w parsing statement as input to QA model instead of highlighting corresponding cells 66.2 34.9 85.9\nCABINET with BERT based relevance scoring (as discussed above) without cell highlighter 61.8 34.9 83.7\nCABINET with BERT based relevance scoring (as discussed above) with cell highlighter 64.5 36.7 85.1\nCABINET with question as input to cell highlighter 63.7 34.4 85.7\nCABINET with URS only and without cell highlighter 65.6 35.8 89.3\nCABINET 69.1 40.5 89.5"
        },
        {
            "heading": "A.9 DATASET STATISTICS",
            "text": "In this section, we tabulate the number of samples in the training, validation and test set of all the three datasets in table 9."
        },
        {
            "heading": "A.10 CASE STUDY ON HOW CLUSTERING LOSSES INTERACT TO YIELD IMPROVEMENTS",
            "text": "We now discuss a case study depicting how the loss functions interact to yield improvements:\nConsider the example in figure 2 in the paper pdf - we plot the histogram of relevance score assigned to table tokens during inference for this example for 4 differently trained variants of CABINET - a) URS trained w/o any clustering losses, b) URS trained with clustering loss, c) URS trained with clustering and cluster mean separation loss, and d) URS trained with clustering, cluster mean separation and relevance score sparsification loss. The histogram plots depicting count of table tokens against relevance score assigned during inference for the 4 variants are shown in figure 7, we summarise our observations below:\n1. From the sub-plot figure 7(a), it can be seen that when URS is trained without any of the three losses, the relevance score for most of the tokens is in the range 0.7 - 0.9 which is undesirable since many tokens which are irrelevant are also assigned a decently high relevance score which is roughly equivalent to passing the table to the QA LLM as it is.\n2. When URS is trained with clustering loss (sub-plot figure 7(b)), the frequency distribution of relevance scores becomes bi-modal with the majority of tokens corresponding to the first mode assigned a relevance score in the range 0.6-0.7 and for the second mode around 0.8-0.9. This shows that clustering loss trains the URS in structuring the latent space representation into two categories, however, still there are many tokens with a relevance score around 0.7 between the two modes which means that many irrelevant tokens are still assigned decently high relevance.\n3. When URS is trained with clustering and cluster mean separation loss (figure 7(c)), the first mode corresponding to a relatively lower relevance score is observed around 0.55-0.65 while the second mode corresponding to a higher relevance score is observed around 0.8- 0.9. This shows that URS trained with cluster mean separation loss amplifies the effect of clustering loss by pushing more tokens into lower relevance category resulting in more tokens assigned lower relevance around the first mode. Also note that the number of tokens in the range 0.7-0.8 also reduces owing to the increased gap between token representations belonging to two categories.\n4. Finally when URS is additionally trained with relevance score sparsification loss (figure 7(d)), we observe that magnitude of relevance score assigned to tokens in low relevance score cluster decreases further which is useful since irrelevant tokens gets suppressed further and the downstream QA LLM can focus better on relevant parts. Further, more irrelevant tokens are assigned a low relevance score. This can be seen in figure 7(d) where more tokens are assigned relevance score in the range 0.45-0.6.\nFurther, since the URS is trained end-to-end with QA LLM differentiably, tokens in higher score category are likely to be relevant in order to enable the QA LLM to be able to generate the correct answer. Additionally, in figure 8, we visualise the t-SNE plots (corresponding to the same example in figure 2 as discussed above) of the latent representation of table tokens encoded during inference by URS corresponding to the 4 variants trained differently - a) URS trained w/o any clustering losses (figure 8(a)), b) URS trained with clustering loss (figure 8(b)), c) URS trained with clustering and cluster mean separation loss (figure 8(c)), and d) URS trained with clustering, cluster mean separation and relevance score sparsification losses (figure 8(d)). The table tokens having relevance score greater than the average relevance score (assigned to table tokens in this example) are depicted as relevant tokens (in red) while those tokens having score less than the average relevance are depicted as non-relevant (in blue). It can be seen that URS model trained with clustering loss, cluster means separation loss and relevance score sparsification loss is better able to segregate the table tokens into two categories and assign lower relevance to tokens in one category while assigning higher relevance to tokens in the second category. Further, since the URS is trained end-to-end with QA LLM differentiably, tokens in higher score category are likely to be relevant in order to enable the QA LLM to be able to generate the correct answer."
        },
        {
            "heading": "A.11 CAN CABINET BE USED FOR IMPROVING OTHER NLP TASKS LIKE READING COMPREHENSION?",
            "text": "We employ CABINET on the reading comprehension task, where given a paragraph and a corresponding question, LLMs need to answer the question based on the paragraph. Drawing an analogy from the Table QA task, certain tokens in the paragraph are more relevant for answering the given question. To achieve this, we employ the URS component of CABINET on top of pre-trained BART-Large. We experiment with a commonly used benchmark SQuAD-v2 and report accuracy on the test set (Table 10), hence validating the task-level generality of CABINET. It can be seen that using CABINET helps in improving the performance of vanilla BART-large LLM for the reading comprehension task."
        }
    ],
    "year": 2024
}