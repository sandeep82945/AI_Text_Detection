{
    "abstractText": "Due to its empirical success in few-shot classification and reinforcement learning, meta-learning has recently received significant interest. Meta-learning methods leverage data from previous tasks to learn a new task in a sample-efficient manner. In particular, model-agnostic methods look for initialisation points from which gradient descent quickly adapts to any new task. Although it has been empirically suggested that such methods perform well by learning shared representations during pretraining, there is limited theoretical evidence of such behaviour. More importantly, it has not been shown that these methods still learn a shared structure, despite architectural misspecifications. In this direction, this work shows, in the limit of an infinite number of tasks, that first-order ANIL with a linear two-layer network architecture successfully learns linear shared representations. This result even holds with overparametrisation; having a width larger than the dimension of the shared representations results in an asymptotically low-rank solution. The learnt solution then yields a good adaptation performance on any new task after a single gradient step. Overall, this illustrates how well model-agnostic methods such as first-order ANIL can learn shared representations.",
    "authors": [],
    "id": "SP:f859bf614cbfd36ae2c434236ed6cc98bd282af0",
    "references": [
        {
            "authors": [
                "Rie Kubota Ando",
                "Tong Zhang",
                "Peter Bartlett"
            ],
            "title": "A framework for learning predictive structures from multiple tasks and unlabeled data",
            "venue": "Journal of Machine Learning Research,",
            "year": 2005
        },
        {
            "authors": [
                "Antreas Antoniou",
                "Harri Edwards",
                "Amos Storkey"
            ],
            "title": "How to train your maml",
            "venue": "In Seventh International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Nadav Cohen",
                "Wei Hu",
                "Yuping Luo"
            ],
            "title": "Implicit regularization in deep matrix factorization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Etienne Boursier",
                "Mikhail Konobeev",
                "Nicolas Flammarion"
            ],
            "title": "Trace norm regularization for multitask learning with scarce data",
            "venue": "In Conference on Learning Theory,",
            "year": 2022
        },
        {
            "authors": [
                "Bin Cheng",
                "Guangcan Liu",
                "Jingdong Wang",
                "Zhongyang Huang",
                "Shuicheng Yan"
            ],
            "title": "Multi-task low-rank affinity pursuit for image segmentation",
            "venue": "In 2011 International Conference on Computer Vision,",
            "year": 2011
        },
        {
            "authors": [
                "Kurtland Chua",
                "Qi Lei",
                "Jason D Lee"
            ],
            "title": "How fine-tuning allows for effective meta-learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Liam Collins",
                "Aryan Mokhtari",
                "Sewoong Oh",
                "Sanjay Shakkottai"
            ],
            "title": "Maml and anil provably learn representations",
            "venue": "arXiv preprint arXiv:2202.03483,",
            "year": 2022
        },
        {
            "authors": [
                "Simon Du",
                "Wei Hu",
                "Sham Kakade",
                "Jason Lee",
                "Qi Lei"
            ],
            "title": "Few-shot learning via learning the representation, provably",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Alireza Fallah",
                "Aryan Mokhtari",
                "Asuman Ozdaglar"
            ],
            "title": "On the convergence theory of gradientbased model-agnostic meta-learning algorithms",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Alireza Fallah",
                "Aryan Mokhtari",
                "Asuman Ozdaglar"
            ],
            "title": "Generalization of model-agnostic metalearning algorithms: Recurring and unseen tasks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Suriya Gunasekar",
                "Blake E Woodworth",
                "Srinadh Bhojanapalli",
                "Behnam Neyshabur",
                "Nati Srebro"
            ],
            "title": "Implicit regularization in matrix factorization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Timothy Hospedales",
                "Antreas Antoniou",
                "Paul Micaelli",
                "Amos Storkey"
            ],
            "title": "Meta-learning in neural networks: A survey",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Hsu",
                "Sham M Kakade",
                "Tong Zhang"
            ],
            "title": "Random design analysis of ridge regression",
            "venue": "In Conference on learning theory,",
            "year": 2012
        },
        {
            "authors": [
                "Kaiyi Ji",
                "Junjie Yang",
                "Yingbin Liang"
            ],
            "title": "Theoretical convergence of multi-step model-agnostic meta-learning",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Haotian Ju",
                "Dongyue Li",
                "Hongyang R Zhang"
            ],
            "title": "Robust fine-tuning of deep neural networks with hessian-based generalization guarantees",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Zhiyuan Li",
                "Yuping Luo",
                "Kaifeng Lyu"
            ],
            "title": "Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning",
            "venue": "arXiv preprint arXiv:2012.09839,",
            "year": 2020
        },
        {
            "authors": [
                "Gergely Neu",
                "Lorenzo Rosasco"
            ],
            "title": "Iterate averaging as regularization for stochastic gradient descent",
            "venue": "In Proceedings of the 31st Conference On Learning Theory,",
            "year": 2018
        },
        {
            "authors": [
                "Alex Nichol",
                "John Schulman"
            ],
            "title": "Reptile: a scalable metalearning algorithm",
            "venue": "arXiv preprint arXiv:1803.02999,",
            "year": 2018
        },
        {
            "authors": [
                "Aniruddh Raghu",
                "Maithra Raghu",
                "Samy Bengio",
                "Oriol Vinyals"
            ],
            "title": "Rapid learning or feature reuse? towards understanding the effectiveness of maml",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Noam Razin",
                "Nadav Cohen"
            ],
            "title": "Implicit regularization in deep learning may not be explainable by norms",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yi Ren",
                "Shangmin Guo",
                "Wonho Bae",
                "Danica J Sutherland"
            ],
            "title": "How to prepare your task head for finetuning",
            "venue": "arXiv preprint arXiv:2302.05779,",
            "year": 2023
        },
        {
            "authors": [
                "P Rigollet",
                "JC H\u00fctter"
            ],
            "title": "Sub-gaussian random variables",
            "venue": "High Dimensional Statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Angelika Rohde",
                "Alexandre B Tsybakov"
            ],
            "title": "Estimation of high-dimensional low-rank matrices",
            "venue": "The Annals of Statistics,",
            "year": 2011
        },
        {
            "authors": [
                "Nikunj Saunshi",
                "Yi Zhang",
                "Mikhail Khodak",
                "Sanjeev Arora"
            ],
            "title": "A sample complexity separation between non-convex and convex meta-learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Nikunj Saunshi",
                "Arushi Gupta",
                "Wei Hu"
            ],
            "title": "A representation learning perspective on the importance of train-validation splitting in meta-learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kiran K Thekumparampil",
                "Prateek Jain",
                "Praneeth Netrapalli",
                "Sewoong Oh"
            ],
            "title": "Statistically and computationally efficient linear meta-representation learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Nilesh Tripuraneni",
                "Michael Jordan",
                "Chi Jin"
            ],
            "title": "On the theory of transfer learning: The importance of task diversity",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Nilesh Tripuraneni",
                "Chi Jin",
                "Michael Jordan"
            ],
            "title": "Provable meta-learning of linear representations",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Roman Vershynin"
            ],
            "title": "Introduction to the non-asymptotic analysis of random matrices",
            "venue": "arXiv preprint arXiv:1011.3027,",
            "year": 2010
        },
        {
            "authors": [
                "Weihang Xu",
                "Simon S Du"
            ],
            "title": "Over-parameterization exponentially slows down gradient descent for learning a single neuron",
            "venue": "arXiv preprint arXiv:2302.10034,",
            "year": 2023
        },
        {
            "authors": [
                "Yuan Yao",
                "Lorenzo Rosasco",
                "Andrea Caponnetto"
            ],
            "title": "On early stopping in gradient descent learning",
            "venue": "Constr. Approx.,",
            "year": 2007
        },
        {
            "authors": [
                "initialisations. Collins"
            ],
            "title": "2022) instead require that the smallest eigenvalue of B\u22a4",
            "year": 2022
        },
        {
            "authors": [
                "Collins"
            ],
            "title": "2022) (exponential vs. polynomial). A similar slow down due to overparametrisation has been recently shown when learning a single ReLU neuron (Xu & Du, 2023). In our setting, rates are more difficult to obtain for the second and third limits, as the decay of quantities of interest depends on other terms in complex ways",
            "year": 2023
        },
        {
            "authors": [
                "Collins"
            ],
            "title": "In the infinite samples limit, a rate for the third limit can yet be derived when k = k\u2032. Relaxation to a finite number of tasks. In the limit of infinite tasks, Theorem 1 proves that the asymptotic solution is low-rank and the complement B\u22c6,\u22a5 is unlearned. Figure 3 shows this behaviour with a relatively big initialisation. For pretraining with a finite number of tasks, the decay",
            "year": 2022
        },
        {
            "authors": [
                "Collins"
            ],
            "title": "With this assumption, there is no dynamics in the kernel space of C0. More precisely, we show that for all time t, ker(C0) \u2286 ker(Ct) \u2229 ker(\u2206t)",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Supervised learning usually requires a large amount of data. To overcome the limited number of available training samples for a single task, multi-task learning estimates a model across multiple tasks (Ando et al., 2005; Cheng et al., 2011). The global performance can then be improved for individual tasks once structural similarities between these tasks are correctly learnt and leveraged. Closely related, meta-learning aims to quickly adapt to any new task, by leveraging the knowledge gained from previous tasks, e.g., by learning a shared representation that enables fast adaptation.\nMeta-learning has been mostly popularised by the success of the Model-Agnostic Meta-Learning (MAML) algorithm for few-shot image classification and reinforcement learning (Finn et al., 2017). MAML searches for an initialisation point such that only a few task-specific gradient descent iterations yield good performance on any new task. It is model-agnostic in the sense that the objective is readily applicable to any architecture that is trained with a gradient descent procedure, without any modifications. Subsequently, many model-agnostic methods have been proposed (Nichol & Schulman, 2018; Antoniou et al., 2019; Raghu et al., 2020; Hospedales et al., 2021). Raghu et al. (2020) empirically support that MAML implicitly learns a shared representation across the tasks, since its intermediate layers do not significantly change during task-specific finetuning. Consequently, they propose the Almost-No-Inner-Loop (ANIL) algorithm, which only updates the last layer during task-specific updates and performs similarly to MAML. However, to avoid heavy computations for second-order derivatives, practitioners generally use first-order approximations such as FO-MAML or FO-ANIL that achieve comparable performances at a cheaper cost (Nichol & Schulman, 2018).\nDespite the empirical success of model-agnostic methods, little is known about their behaviours in theory. To this end, our work considers the following question on the pretraining of FO-ANIL."
        },
        {
            "heading": "Do model-agnostic methods learn shared representations in few-shot settings?",
            "text": "Proving positive optimisation results on the pretraining of meta-learning models is out of reach in general, complex settings that may be encountered in practice. Indeed, research beyond linear models has mostly been confined to the finetuning phase (Ju et al., 2022; Chua et al., 2021). Hence, to allow a tractable analysis, we study FO-ANIL in the canonical multi-task model of a linear shared\nrepresentation; and consider a linear two-layer network, which is the minimal architecture achieving non-trivial performance. Traditional multi-task learning methods such as Burer-Monteiro factorisation (or matrix factorisation) (Tripuraneni et al., 2021; Du et al., 2021; Thekumparampil et al., 2021) and nuclear norm regularisation (Rohde & Tsybakov, 2011; Boursier et al., 2022) are known for correctly learning the shared representation. Besides being specific to this linear model, they rely on prior knowledge of the hidden dimension of the common structure that is unknown in practice.\nFor meta-learning in this canonical multi-task model, Saunshi et al. (2020) has shown the first result under overparametrisation by considering a unidimensional shared representation, infinite samples per task, and an idealised algorithm. More recently, (Collins et al., 2022) has provided a multidimensional analysis for MAML and ANIL in which the hidden layer recovers the ground-truth low-dimensional subspace at an exponential rate. Similar to multi-task methods, the latter result relies on well-specification of the network width, i.e., it has to coincide with the hidden dimension of the shared structure. Moreover, it requires a weak alignment between the hidden layer and the ground truth at initialisation, which is not satisfied in high-dimensional settings.\nThe power of MAML and ANIL, however, comes from their good performance despite mismatches between the architecture and the problem; and in few-shot settings, where the number of samples per task is limited but the number of tasks is not. In this direction, we prove a learning result under a framework that reflects the meta-learning regime. Specifically, we show that FO-ANIL successfully learns multidimensional linear shared structures with an overparametrised network width and without initial weak alignment. Our setting of finite samples and infinite tasks is better suited for practical scenarios and admits novel behaviours unobserved in previous works. In particular, FOANIL not only learns the low-dimensional subspace, but it also unlearns its orthogonal complement. This unlearning does not happen with infinite samples and is crucial during task-specific finetuning. In addition, we reveal a slowdown due to overparametrisation, which has been also observed in supervised learning (Xu & Du, 2023). Overall, our result provides the first learning guarantee under misspecifications, and shows the benefits of model-agnostic meta-learning over multi-task learning.\nContributions. We study FO-ANIL in a linear shared representation model introduced in Section 2. In order to allow a tractable yet non-trivial analysis, we consider infinite tasks idealisation, which is more representative of meta-learning than the infinite samples idealisation considered in previous works. Section 3 presents our main result, stating that FO-ANIL asymptotically learns an accurate representation of the hidden problem structure despite a misspecification in the network width. When adapting this representation to a new task, FO-ANIL quickly achieves a test loss comparable to linear regression on the hidden low-dimensional subspace. Section 4 then discusses in detail these results, their limitations, and compares them with the literature. Finally, Section 5 empirically illustrates the success of model-agnostic methods in learnt representation and at test time."
        },
        {
            "heading": "2 PROBLEM SETTING",
            "text": ""
        },
        {
            "heading": "2.1 DATA DISTRIBUTION",
            "text": "In the following, tasks are indexed by i \u2208 N. Each task corresponds to a d-dimensional linear regression task with parameter \u03b8\u22c6,i \u2208 Rd and m observation samples. Mathematically, we have for each task i observations (Xi, yi) \u2208 Rm\u00d7d \u00d7 Rm such that\nyi = Xi\u03b8\u22c6,i + zi where zi \u2208 Rm is some random noise. Some shared structure is required between the tasks to meta-learn, i.e., to be able to speed up the learning of a new task. Similarly to the multi-task linear representation learning setting, we assume that the regression parameters \u03b8\u22c6,i all lie in the same small k-dimensional linear subspace, with k < d. Equivalently, there is an orthogonal matrix B\u22c6 \u2208 Rd\u00d7k and representation parameters w\u22c6,i \u2208 Rk such that \u03b8\u22c6,i = B\u22c6w\u22c6,i for any task i. To derive a proper analysis of this setting, we assume a random design of the different quantities of interest, summarised in Assumption 1. This assumption and how it could be relaxed is discussed in Section 4.\nAssumption 1 (random design). Each row of Xi is drawn i.i.d. according to N (0, Id) and the coordinates of zi are i.i.d., centered random variables of variance \u03c32. Moreover, the task parameters w\u22c6,i are drawn i.i.d with E[w\u22c6,i] = 0 and covariance matrix \u03a3\u22c6 := E[w\u22c6,iw\u22a4\u22c6,i] = c Ik with c > 0."
        },
        {
            "heading": "2.2 FO-ANIL ALGORITHM",
            "text": "This section introduces FO-ANIL in the setting described above for N \u2208 N tasks, as well as in the idealised setting of infinite tasks (N = \u221e). The goal of model-agnostic methods is to learn parameters, for a given neural network architecture, that quickly adapt to a new task. This work focuses on a linear two-layer network architecture, parametrised by \u03b8 := (B,w) \u2208 Rd\u00d7k\u2032 \u00d7 Rk\u2032 with k \u2264 k\u2032 \u2264 d. The estimated function is then given by f\u03b8 : x 7\u2192 x\u22a4Bw. The ANIL algorithm aims at minimising the test loss on a new task, after a small number of gradient steps on the last layer of the neural network. For the sake of simplicity, we here consider a single gradient step. ANIL then aims at minimising over \u03b8 the quantity\nLANIL(\u03b8) := Ew\u22c6,i,Xi,yi [ Li ( \u03b8 \u2212 \u03b1\u2207wL\u0302i(\u03b8;Xi, yi) )] , (1)\nwhere Li is the (expected) test loss on the task i, which depends on w\u22c6,i; L\u0302i(\u03b8;Xi, yi) is the empirical loss on the observations (Xi, yi); and \u03b1 is the gradient step size. When the whole parameter is updated at test time, i.e.,\u2207w is replaced by \u2207\u03b8, this instead corresponds to the MAML algorithm. For model-agnostic methods, it is important to split the data in two for inner and outer loops during training. Otherwise, the model would indeed overfit the training set and would learn a poor, full rank representation of the task parameters (Saunshi et al., 2021). For min +mout = m with min < m, we split the observations of each task as (X ini , y in i ) \u2208 Rmin\u00d7d\u00d7Rmin the min first rows of (Xi, yi); and (Xouti , y out i ) \u2208 Rmout\u00d7d \u00d7 Rmout the mout last rows of (Xi, yi).\nWhile training, ANIL alternates at each step t \u2208 N between an inner and an outer loop to update the parameter \u03b8t. In the inner loop, the last layer of the network is adapted to each task i following\nwt,i \u2190 wt \u2212 \u03b1\u2207wL\u0302i(\u03b8t;X ini , yini ). (2) Again, updating the whole parameter \u03b8t,i with \u2207\u03b8 would correspond to MAML algorithm. In the outer loop, ANIL then takes a gradient step (with learning rate \u03b2) on the validation loss obtained for the observations (Xouti , y out i ) after this inner loop. With \u03b8t,i := (Bt, wt,i), it updates\n\u03b8t+1 \u2190 \u03b8t \u2212 \u03b2N \u2211N\ni=1 H\u0302t,i(\u03b8t)\u2207\u03b8L\u0302i(\u03b8t,i;Xouti , youti ), (3) where the matrix H\u0302t,i accounts for the derivative of the function \u03b8t 7\u2192 \u03b8t,i. Computing the secondorder derivatives appearing in H\u0302t,i is often very costly. Practitioners instead prefer to use first-order approximations, since they are cheaper in computation and yield similar performances (Nichol & Schulman, 2018). FO-ANIL then replaces H\u0302t,i by the identity matrix in Equation (3)."
        },
        {
            "heading": "2.2.1 DETAILED ITERATIONS",
            "text": "In our regression setting, the empirical squared error is given by L\u0302i((B,w);Xi, yi) = 12m\u2225yi \u2212 XiBw\u222522. In that case, the FO-ANIL inner loop of Equation (2) gives in the setting of Section 2.1:\nwt,i = wt \u2212 \u03b1 min B\u22a4t (X in i ) \u22a4X ini (Btwt \u2212B\u22c6w\u22c6,i) + \u03b1 min B\u22a4t (X in i ) \u22a4zini . (4)\nThe multi-task learning literature often considers a large number of tasks (Thekumparampil et al., 2021; Boursier et al., 2022) to allow a tractable analysis. Similarly, we study FO-ANIL in the limit of an infinite number of tasks N =\u221e to simplify the outer loop updates. In this limit, iterations are given by the exact gradient of the ANIL loss defined in Equation (1), when ignoring the second-order derivatives. The first-order outer loop updates of Equation (3) then simplify with Assumption 1 to\nwt+1 = wt \u2212 \u03b2(Ik\u2032 \u2212 \u03b1B\u22a4t Bt)B\u22a4t Btwt, (5) Bt+1 = Bt \u2212 \u03b2BtE[wt,iw\u22a4t,i] + \u03b1\u03b2B\u22c6\u03a3\u22c6B\u22a4\u22c6 Bt, (6)\nwhere wt,i is still given by Equation (4). Moreover, Lemma 15 in the Appendix allows with Assumption 1 to compute an exact expression of E[wt,iw\u22a4t,i] as\nE[wt,iw\u22a4t,i] = (Ik\u2032 \u2212 \u03b1B\u22a4t Bt)wtw\u22a4t (Ik\u2032 \u2212 \u03b1B\u22a4t Bt) + \u03b12B\u22a4t B\u22c6\u03a3\u22c6B\u22a4\u22c6 Bt\n+ \u03b12\nmin B\u22a4t\n( Btwtw \u22a4 t B \u22a4 t +B\u22c6\u03a3\u22c6B \u22a4 \u22c6 + ( \u2225Btwt\u22252 +Tr(\u03a3\u22c6) + \u03c32 ) Id ) Bt.\n(7)\nThe first line is the covariance obtained for an infinite number of samples. The second line comes from errors due to the finite number of samples and the label noise. As a comparison, MAML also updates matrices Bt,i in the inner loop, which then intervene in the updates of wt and Bt. Because of this entanglement, the iterates of first-order MAML (and hence its analysis) are very cumbersome."
        },
        {
            "heading": "3 LEARNING A GOOD REPRESENTATION",
            "text": "Given the complexity of its iterates, FO-ANIL is very intricate to analyse even in the simplified setting of infinite tasks. The objective function is non-convex in its arguments and the iterations involve high-order terms in both wt and Bt, as seen in Equations (5) to (7). Theorem 1 yet characterizes convergence towards some fixed point (of the iterates) satisfying a number of conditions.\nTheorem 1. Let B0 and w0 be initialized such that B\u22a4\u22c6 B0 is full rank,\n\u2225B0\u222522 = O ( \u03b1\u22121 min ( 1 min , min \u03c32 )) , \u2225w0\u222522 = O (\u03b1\u03bbmin(\u03a3\u22c6)) ,\nwhere \u03bbmin(\u03a3\u22c6) is the smallest eigenvalue of \u03a3\u22c6, \u03c32 := Tr(\u03a3\u22c6) + \u03c32 and the O notation hides universal constants. Let also the step sizes satisfy \u03b1 \u2265 \u03b2 and \u03b1 = O (1/\u03c3). Then under Assumption 1, FO-ANIL (given by Equations (5) and (6)) with initial parameters B0, w0, asymptotically satisfies the following\nlim t\u2192\u221e B\u22a4\u22c6,\u22a5Bt = 0, lim t\u2192\u221e Btwt = 0,\nlim t\u2192\u221e\nB\u22a4\u22c6 BtB \u22a4 t B\u22c6 = \u039b\u22c6 :=\n1\n\u03b1 min min + 1\n( Ik \u2212 (min + 1 \u03c32 \u03a3\u22c6 + Ik )\u22121) ,\n(8)\nwhere B\u22c6,\u22a5 \u2208 Rd\u00d7(d\u2212k) is an orthogonal matrix spanning the orthogonal of col(B\u22c6), i.e., B\u22a4\u22c6,\u22a5B\u22c6,\u22a5 = Id\u2212k, and B \u22a4 \u22c6 B\u22c6,\u22a5 = 0.\nAn extended version of Theorem 1 and its proof are postponed to Appendix C. We conjecture that Theorem 1 holds with arbitrary task covariances \u03a3\u22c6 beyond the identity covariance in Assumption 1. Before discussing the implications of Theorem 1, we provide details on the proof strategy.\nThe proof is based on the monotonic decay of \u2225B\u22a4\u22c6,\u22a5Bt\u22252, \u2225Btwt\u22252 and the monotonic increase of B\u22a4\u22c6 BtB \u22a4 t B\u22c6 in the Loewner order sense. As these three quantities are interrelated, simultaneously controlling them is challenging. The initialisation given in Theorem 1 achieves this by conditioning the dynamics to be bounded and well-behaved. The choice of \u03b1 is also crucial as it guarantees the decay of \u2225Btwt\u22252 after \u2225B\u22a4\u22c6,\u22a5Bt\u22252 decays. While these two quantities decay, \u039bt := B\u22a4\u22c6 BtB\u22a4t B\u22c6 follows a recursion where B\u22a4\u22c6,\u22a5Bt and Btwt act as noise terms. The proof utilizes two associated recursions to respectively upper and lower bound \u039bt (in Loewner order) and then show their monotonic convergence to \u039b\u22c6. A more detailed sketch of the proof could be found in Appendix B.\nTheorem 1 states that under mild assumptions on initialisation and step sizes, the parameters learnt by FO-ANIL verify three key properties after convergence:\n1. B\u221e is rank-deficient, i.e., FO-ANIL learns to ignore the entire d\u2212k dimensional orthogonal subspace given by B\u22c6,\u22a5, as expressed by the first limit in Equation (8).\n2. The learnt initialisation yields the zero function, as given by the second limit in Equation (8). Note that wt does not necessarily converge to 0; however, it converges to the null space of B\u22c6, thanks to the third property. Although intuitive, showing that Btwt converges to the mean task parameter (assumed 0 here) is very challenging when starting away from it, as discussed in Section 4. This property is crucial for fast adaptation on a new task.\n3. B\u22a4\u22c6 B\u221eB \u22a4 \u221eB\u22c6 is proportional to identity. Along with the first property, this fact implies\nthat the learnt matrix B\u221e exactly spans col(B\u22c6). Moreover, its squared singular values scale as \u03b1\u22121, allowing to perform rapid learning with a single gradient step of size \u03b1.\nThese three properties allow to obtain a good performance on a new task after a single gradient descent step, as intended by the training objective of ANIL. The generalisation error at test time is precisely quantified by Proposition 1 in Section 3.1. In addition, the limit points characterised by Theorem 1 are shown to be global minima of the ANIL objective in Equation (1) in Appendix F.\nInterestingly, Theorem 1 holds for quite large step sizes \u03b1, \u03b2 and the limit points only depend on these parameters by the \u03b1\u22121 scaling of \u039b\u22c6. Also note that \u039b\u22c6 \u2192 1\u03b1Ik when min \u2192 \u221e. Yet, there is some shrinkage of \u039b\u22c6 for finite number of samples, that is significant when min is of order of the inverse eigenvalues of 1\u03c32\u03a3\u22c6. This shrinkage mitigates the variance of the estimator returned after a single gradient step, while this estimator is unbiased with no shrinkage (min =\u221e). Although the limiting behaviour of FO-ANIL holds for any finite min, the convergence rate can be arbitrarily slow for large min. In particular, FO-ANIL becomes very slow to unlearn the orthogonal complement of col(B\u22c6) when min is large, as highlighted by Equation (14) in Appendix B. At the limit of infinite samples min =\u221e, FO-ANIL thus does not unlearn the orthogonal complement and the first limit of Equation (8) in Theorem 1 does not hold anymore. This unlearning is yet crucial at test time, since it reduces the dependency of the excess risk from k\u2032 to k (see Proposition 1)."
        },
        {
            "heading": "3.1 FAST ADAPTATION TO A NEW TASK",
            "text": "Thanks to Theorem 1, FO-ANIL learns the shared representation during pretraining. It is yet unclear how this result enhances the learning of new tasks, often referred as finetuning in the literature. Consider having learnt parameters (B\u0302, w\u0302) \u2208 Rd\u00d7k\u2032 \u00d7 Rk\u2032 following Theorem 1,\nB\u22a4\u22c6,\u22a5B\u0302 = 0; B\u0302w\u0302 = 0; B \u22a4 \u22c6 B\u0302B\u0302 \u22a4B\u22c6 = \u039b\u22c6. (9)\nWe then observe a new regression task with mtest observations (X, y) \u2208 Rmtest\u00d7d \u00d7 Rmtest and parameter w\u22c6 \u2208 Rk such that\ny = XB\u22c6w\u22c6 + z, (10)\nwhere the entries of z are i.i.d. centered \u03c3 sub-Gaussian random variables and the entries of X are i.i.d. standard Gaussian variables following Assumption 1. The learner then estimates the regression parameter of the new task doing one step of gradient descent:\nwtest = w\u0302 \u2212 \u03b1\u2207wL\u0302((B\u0302, w\u0302);X, y) = w\u0302 + \u03b1B\u0302\u22a4\u03a3testB\u22c6w\u22c6 + \u03b1 mtest B\u0302\u22a4X\u22a4z, (11)\nwith \u03a3test := 1mtestX \u22a4X . As in the inner loop of ANIL, a single gradient step is processed here. Note that it is unclear whether a single or more gradient steps should be run at test time. Notably, (B\u0302, w\u0302) has not exactly converged in practice, since we consider a finite training time: B\u0302 is thus full rank. The least squares estimator of the linear regression with data (XB\u0302, y) might then lead to overfitting. Running just a few gradient steps can be helpful in that case, since it prevents overfitting by implicitly regularising the norm of the estimated parameters (Yao et al., 2007; Neu & Rosasco, 2018). The best strategy (e.g. the number of gradient steps) to run while finetuning is an intricate problem, independently studied in the literature (see e.g. Chua et al., 2021; Ren et al., 2023) and is out of the scope of this work. Additional details are provided in Appendix I.\nWhen estimating the regression parameter with B\u0302wtest, the excess risk on this task is exactly \u2225B\u0302wtest \u2212B\u22c6w\u22c6\u222522. Proposition 1 below allows to bound the risk on any new observed task.\nProposition 1. Let B\u0302, wtest satisfy Equations (9) and (11) for a new task defined by Equation (10). If mtest \u2265 k, then with probability at least 1\u2212 4e\u2212 k 2 ,\n\u2225B\u0302wtest \u2212B\u22c6w\u22c6\u22252 = O (1 + \u03c32/\u03bbmin(\u03a3\u22c6)\nmin \u2225w\u22c6\u22252 + \u2225w\u22c6\u22252\n\u221a k\nmtest + \u03c3\n\u221a k\nmtest\n) ,\nwhere we recall \u03c32 = Tr(\u03a3\u22c6) + \u03c32.\nA more general version of Proposition 1 and its proof are postponed to Appendix D. The proof relies on the exact expression of wtest after a single gradient update. The idea is to decompose the difference B\u0302wtest \u2212B\u22c6w\u22c6 in three terms, which are then bounded using concentration inequalities. The first two terms come from the error due to proceeding a single gradient step, instead of converging towards the ERM weights: the first one is the bias of this error, while the second one is due to its variance. The last term is the typical error of linear regression on a k dimensional space. Note this bound does not depend on the feature dimension d (nor k\u2032), but only on the hidden dimension k.\nWhen learning a new task without prior knowledge, e.g., with a simple linear regression on the ddimensional space of the features, the error instead scales as \u03c3 \u221a\nd mtest (Hsu et al., 2012). FO-ANIL thus leads to improved estimations on new tasks, when it beforehand learnt the shared representation. Such a learning is guaranteed thanks to Theorem 1. Surprisingly, FO-ANIL might only need a single gradient step to outperform linear regression on the d-dimensional feature space, as empirically confirmed in Section 5. As explained, this quick adaptation is made possible by the \u03b1\u22121 scaling of B\u0302, which leads to considerable updates in the model parameter after a single gradient step."
        },
        {
            "heading": "4 DISCUSSION",
            "text": "No prior structure knowledge. Previous works on model-agnostic methods and matrix factorisation consider a well-specified learning architecture, i.e., k\u2032 = k (Tripuraneni et al., 2021; Thekumparampil et al., 2021; Collins et al., 2022). In practical settings, the true dimension k is hidden, and estimating it is part of learning the representation. Theorem 1 instead states that FO-ANIL recovers this hidden true dimension k asymptotically when misspecified (k\u2032 > k) and still learns good shared representation despite overparametrisation (e.g., k\u2032 = d). Theorem 1 thus illustrates the adaptivity of model-agnostic methods, which we believe contributes to their empirical success.\nProving good convergence of FO-ANIL despite misspecification in network width is the main technical challenge of this work. When correctly specified, it is sufficient to prove that FO-ANIL learns the subspace spanned by B\u22c6, which is simply measured by the principal angle distance by Collins et al. (2022). When largely misspecified (k\u2032 = d), this measure is always 1 and poorly reflects how good is the learnt representation. Instead of a single measure, two phenomena are quantified here. FO-ANIL indeed not only learns the low-dimensional subspace, but it also unlearns its orthogonal complement.1 More precisely, misspecification sets additional difficulties in controlling simultaneously the variables wt and Bt through iterations. When k\u2032 = k, this control is possible by lower bounding the singular values of Bt. A similar argument is however not possible when k\u2032 > k, as the matrix Bt is now rank deficient (at least asymptotically). To overcome this challenge, we use a different initialisation regime and analysis techniques with respect to Saunshi et al. (2020); Collins et al. (2022). These advanced techniques allow to prove convergence of FO-ANIL with different assumptions on both the model and the initialisation regime, as explained below.\nSuperiority of agnostic methods. When correctly specified (k\u2032 = k), model-agnostic methods do not outperform traditional multi-task learning methods. For example, the Burer-Monteiro factorisation minimises the non-convex problem\nmin B\u2208Rd\u00d7k\u2032\nW\u2208Rk\u2032\u00d7N\n1 2N \u2211N i=1 L\u0302i(BW (i);Xi, yi), (12)\nwhere W (i) stands for the i-th column of the matrix W . Tripuraneni et al. (2021) show that any local minimum of Equation (12) correctly learns the shared representation when k\u2032 = k. However\n1Although Saunshi et al. (2020) consider a misspecified setting, the orthogonal complement is not unlearnt in their case, since they assume an infinite number of samples per task (see Infinite tasks model paragraph).\nwhen misspecified (e.g., taking k\u2032 = d), there is no such guarantee. In that case, the optimal B need to be full rank (e.g., B = Id) to perfectly fit the training data of all tasks, when there is label noise. This setting then resembles running independent d-dimensional linear regressions for each task and directly leads to a suboptimal performance of Burer-Monteiro factorisations, as illustrated in Section 5. This is another argument in favor of model-agnostic methods in practice: while they provably work despite overparametrisation, traditional multi-task methods a priori do not.\nAlthough Burer-Monteiro performs worse than FO-ANIL in the experiments of Section 5, it still largely outperforms the single-task baseline. We believe this good performance despite overparametrisation might be due to the implicit bias of matrix factorisation towards low-rank solutions. This phenomenon remains largely misunderstood in theory, even after being extensively studied (Gunasekar et al., 2017; Arora et al., 2019; Razin & Cohen, 2020; Li et al., 2020). Explaining the surprisingly good performance of Burer-Monteiro thus remains a major open problem.\nInfinite tasks model. A main assumption in Theorem 1 is the infinite tasks model, where updates are given by the exact (first-order) gradient of the objective function in Equation (1). Theoretical works often assume a large number of tasks to allow a tractable analysis (Thekumparampil et al., 2021; Boursier et al., 2022). The infinite tasks model idealises this type of assumption and leads to simplified parameters\u2019 updates. Note these updates, given by Equations (5) and (6), remain intricate to analyse. Saunshi et al. (2020); Collins et al. (2022) instead consider an infinite number of samples per task, i.e., min =\u221e. This assumption leads to even simpler updates, and their analysis extends to the misspecified setting with some extra work, as explained in Appendix G. Collins et al. (2022) also extend their result to a finite number of samples in finite-time horizon, using concentration bounds on the updates to their infinite samples counterparts when sufficiently many samples are available.\nMore importantly, the infinite samples idealisation is not representative of few-shot settings and some phenomena are not observed in this setting. First, the superiority of model-agnostic methods is not apparent with an infinite number of samples per task. In that case, matrices B only spanning col(B\u22c6) also minimise the problem of Equation (12), potentially making Burer-Monteiro optimal despite misspecification. Second, a finite number of samples is required to unlearn the orthogonal of col(B\u22c6). When min =\u221e, FO-ANIL does not unlearn this subspace, which hurts the performance at test time for large k\u2032, as observed in Section 5. Indeed, there is no risk of overfitting (and hence no need to unlearn the orthogonal space) with an infinite number of samples. On the contrary with a finite number of samples, FO-ANIL tends to overfit during its inner loop. This overfitting is yet penalised by the outer loss and leads to unlearning the orthogonal space.\nExtending Theorem 1 to a finite number of tasks is left open for future work. Section 5 empirically supports that a similar result holds. A finite tasks and sample analysis similar to Collins et al. (2022) is not desirable, as mimicking the infinite samples case through concentration would omit the unlearning part, as explained above. With misspecification, we believe that extending Theorem 1 to a finite number of tasks is directly linked to relaxing Assumption 1. Indeed, the empirical task mean and covariance are not exactly 0 and the identity matrix in that case. Obtaining a convergence result with general task mean and covariance would then help in understanding the finite tasks case.\nLimitations. Assumption 1 assumes zero mean task parameters, \u00b5\u22c6 := E[w\u22c6,i] = 0. Considering non-zero task mean adds two difficulties to the existing analysis. First, controlling the dynamics of wt is much harder, as there is an extra term \u00b5\u22c6 in its update, but also because Btwt should not converge to 0 anymore but B\u22c6\u00b5\u22c6 instead. Moreover, updates of Bt have an extra asymmetric rank 1 term depending on \u00b5\u22c6. Experiments in Appendix I yet support that both FO-ANIL and FO-MAML succeed when \u00b5\u22c6 is non zero.\nIn addition, we assume that the task covariance \u03a3\u22c6 is identity. The condition number of \u03a3\u22c6 is related to the task diversity and the problem hardness (Tripuraneni et al., 2020; Thekumparampil et al., 2021; Collins et al., 2022). Under Assumption 1, the task diversity is perfect (i.e., the condition number is 1), which simplifies the problem. The main challenge in dealing with general task covariances is that the updates involve non-commutative terms. Consequently, the main update rule of B\u22a4\u22c6 BtB \u22a4 t B\u22c6 no longer preserves the monotonicity used to derive upper and lower bounds on its iterates. However, experimental results in Section 5 suggest that Theorem 1 still holds with any diagonal covariance. Hence, we believe our analysis can be extended to any diagonal task covariance. The matrix \u03a3\u22c6 being diagonal is not restrictive, as it is always the case for a properly chosen B\u22c6.\nLastly, the features Xi follow a standard Gaussian distribution here. It is needed to derive an exact expression of E[wt,iw\u22a4t,i] with Lemma 15, which can be easily extended to any spherically symmetric distribution. Whether Theorem 1 holds for general feature distributions yet remains open.\nAdditional technical discussion. For space reasons, we leave the technical details on Theorem 1 to Appendix A. In particular, we remark that our initialisation only requires full-rank initialisation without any initial alignment and describe how to derive a rate for the first limit in Theorem 1 which shows a slowdown due to overparametrisation similar to the previous work by Xu & Du (2023)."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "This section empirically studies the behaviour of model-agnostic methods on a toy example. We consider a setup with a large but finite number of tasks N = 5000, feature dimension d = 50, a limited number of samples per task m = 30, small hidden dimension k = 5 and Gaussian label noise with variance \u03c32 = 4. We study a largely misspecified problem where k\u2032 = d. To demonstrate that Theorem 1 holds more generally, we consider a non-identity covariance \u03a3\u22c6 proportional to diag(1,\u00b7 \u00b7 \u00b7, k). Further experimental details, along with additional experiments involving two-layer and three-layer ReLU networks, can be found in Appendix I.\nTo observe the differences between the idealised models and the true algorithm, FO-ANIL with finite samples and tasks is compared with both its infinite tasks and infinite samples versions. It is also compared with FO-MAML and Burer-Monteiro factorisation.\nFigure 2 first illustrates how the different methods learn the ground truth subspace given by B\u22c6. More precisely, it shows the evolution of the largest and smallest squared singular value of B\u22a4\u22c6 Bt. On the other hand, Figure 3 illustrates how different methods unlearn the orthogonal complement of col(B\u22c6), by showing the evolution of the largest and averaged squared singular value of B\u22a4\u22c6,\u22a5Bt.\nFinally, Table 1 compares the excess risks achieved by these methods on a new task with both 20 and 30 samples. The parameter is estimated by a ridge regression on (XB\u0302, y), where B\u0302 is the representation learnt while training. Additionally, we report the loss obtained for model-agnostic methods after a single gradient descent update. These methods are also compared with the single-task baseline that performs ridge regression on the d-dimensional feature space, and the oracle baseline that\ndirectly performs ridge regression on the ground truth k-dimensional parameter space. Ridge regression is used for all methods, since regularising the objective largely improves the test loss here. For each method, the regularisation parameter is tuned using a grid-search over multiple values.\nAs predicted by Theorem 1, FO-ANIL with infinite tasks exactly converges to \u039b\u22c6. More precisely, it quickly learns the ground truth subspace and unlearns its orthogonal complement as the singular values of B\u22a4\u22c6,\u22a5Bt decrease to 0, at the slow rate given in Appendix H. FO-ANIL and FO-MAML with a finite number of tasks, almost coincide. Although very close to infinite tasks FO-ANIL, they seem to unlearn the orthogonal space of col(B\u22c6) even more slowly. In particular, there are a few directions (given by the maximal singular value) that are unlearnt either very slowly or up to a small error. However on average, the unlearning happens at a comparable rate, and the effect of the few extreme directions is negligible. These methods thus learn a good representation and reach an excess risk approaching the oracle baseline when doing either ridge regression or just a single gradient step.\nOn the other hand, as predicted in Section 4, FO-ANIL with an infinite number of samples quickly learns col(B\u22c6), but it does not unlearn the orthogonal complement. The singular values along the orthogonal complement stay constant. A similar behaviour is observed for Burer-Monteiro factorisation: the ground truth subspace is quickly learnt, but the orthogonal complement is not unlearnt. Actually, the singular values along the orthogonal complement even increase during the first steps of training. For both methods, the inability of unlearning the orthogonal complement significantly hurts the performance at test time. Note however that they still outperform the single-task baseline. The singular values along col(B\u22c6) are indeed larger than along its orthogonal complement. More weight is then put on the ground truth subspace when estimating a new task.\nThese experiments confirm the phenomena described in Sections 3 and 4. Model-agnostic methods not only learn the good subspace, but also unlearn its orthogonal complement. This unlearning yet happens slowly and many iterations are required to completely ignore the orthogonal space."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "This work studies first-order ANIL in the shared linear representation model with a linear twolayer architecture. Under infinite tasks idealisation, FO-ANIL successfully learns the shared, low-dimensional representation despite overparametrisation in the hidden layer. More crucially for performance during task-specific finetuning, the iterates of FO-ANIL not only learn the lowdimensional subspace but also forget its orthogonal complement. Consequently, a single-step gradient descent initialised on the learnt parameters achieves a small excess risk on any given new task. Numerical experiments confirm these results and suggest they hold in more general setups, e.g., with uncentered, anisotropic task parameters and a finite number of tasks. As a consequence, our work suggests that model-agnostic methods are also model-agnostic in the sense that they successfully learn the shared representation, although their architecture is not adapted to the problem parameters. Extending our theoretical results to these more general settings or more intricate methods, such as MAML, remains open for future work. Lastly, our work presents a provable shared representation learning result for the pretraining of meta-learning algorithms. Thus, it connects to the literature on representation learning with pretraining; in particular, it demonstrates a slowdown due to overparametrisation that has been recently demonstrated in supervised learning."
        },
        {
            "heading": "A ADDITIONAL DISCUSSION",
            "text": "Initialisation regime. Theorem 1 requires a bounded initialisation to ensure the dynamics of FOANIL stay bounded. Roughly, we need the squared norm of B\u22a4\u22c6,\u22a5B0 to be O ( (\u03b1min)\n\u22121) to guarantee \u2225Bt\u22252 \u2264 \u03b1\u22121 for any t. We believe the min dependency is an artifact of the analysis and it is empirically not needed. Additionally, we bound w0 to control the scale of E[wt,iw\u22a4t,i] that appears in the update of Bt. A similar inductive condition is used by Collins et al. (2022).\nMore importantly, our analysis only needs a full rank B\u22a4\u22c6 B0, which holds almost surely for usual initialisations. Collins et al. (2022) instead require that the smallest eigenvalue of B\u22a4\u22c6 B0 is bounded strictly away from 0, which does not hold when d \u226b k\u2032. This indicates that their analysis covers only the tail end of training and not the initial alignment phase.\nRate of convergence. In contrast with the convergence result of Collins et al. (2022), Theorem 1 does not provide any convergence rate for FO-ANIL but only states asymptotic results. Appendix H provides an analogous rate for the first limit of Theorem 1: \u2225B\u22a4\u22c6,\u22a5Bt\u222522 = O ( min\n\u03b12\u03b2\u03c32t\n) . Due to mis-\nspecification, this rate is slower than the one by Collins et al. (2022) (exponential vs. polynomial). A similar slow down due to overparametrisation has been recently shown when learning a single ReLU neuron (Xu & Du, 2023). In our setting, rates are more difficult to obtain for the second and third limits, as the decay of quantities of interest depends on other terms in complex ways. Remark that rates for these two limits are not studied by Collins et al. (2022). In the infinite samples limit, a rate for the third limit can yet be derived when k = k\u2032.\nRelaxation to a finite number of tasks. In the limit of infinite tasks, Theorem 1 proves that the asymptotic solution is low-rank and the complement B\u22c6,\u22a5 is unlearned. Figure 3 shows this behaviour with a relatively big initialisation. For pretraining with a finite number of tasks, the decay is not towards 0 but to a small residual value. The scale of this residual depends on the number of tasks (decreasing to 0 when the number of tasks goes to infinity) and possibly on other problem parameters such as k and \u03b1. Most notably, when a very small initialisation is chosen, the singular values in the complement space can increase until this small scale.\nThis indicates that Theorem 1 relaxed to a finite number of tasks will include a non-zero but small component in the orthogonal space. In our experiments, these residuals do not hurt the finetuning performance. Note that these residuals are also present for simulations of FO-ANIL with infinite tasks due to the finite time horizon.\nTheoretical analyses of model-agnostic meta-learning. Other theoretical works on modelagnostic meta-learning have focused on convergence guarentees (Fallah et al., 2020; Ji et al., 2022) or generalisation (Fallah et al., 2021). In contrast, this work focuses on pretraining of model-agnostic meta-learning and learning of shared representations under the canonical model of multi-task learning."
        },
        {
            "heading": "B SKETCH OF PROOF",
            "text": "The challenging part of Theorem 1 is that Bt \u2208 Rd\u00d7k \u2032 involves two separate components with different dynamics: Bt = B\u22c6B \u22a4 \u22c6 Bt +B\u22c6,\u22a5B \u22a4 \u22c6,\u22a5Bt.\nThe first term B\u22a4\u22c6 Bt eventually scales in \u03b1 \u22121/2 whereas the second term B\u22a4\u22c6,\u22a5Bt converges to 0, resulting in a nearly rank-deficient Bt. The dynamics of these two terms and wt are interdependent, which makes it challenging to bound any of them.\nRegularity conditions. The first part of the proof consists in bounding all the quantities of interest. Precisely, we show by induction that the three following properties hold for any t,\n1. \u2225B\u22a4\u22c6 Bt\u222522 \u2264 \u2225\u039b\u22c6\u22252, 2. \u2225wt\u22252 \u2264 \u2225w0\u22252, 3. \u2225B\u22a4\u22c6,\u22a5Bt\u22252 \u2264 \u2225B\u22a4\u22c6,\u22a5B0\u22252. (13)\nImportantly, the first and third conditions, along with the initialisation conditions, imply \u2225Bt\u222522 \u2264 \u03b1\u22121. The monotonicity of the function fU described below leads to \u2225B\u22a4\u22c6 Bt+1\u222522 \u2264 \u2225\u039b\u22c6\u22252. Also, using the inductive assumptions with the update equations for B\u22a4\u22c6,\u22a5Bt and wt allows us to show that both the second and third properties hold at time t+ 1.\nNow that the three different quantities of interest have been properly bounded, we can show the three limiting results of Theorem 1.\nUnlearning the orthogonal complement. We first show that limt\u2192\u221e B\u22a4\u22c6,\u22a5Bt = 0. Equation (6) directly yields B\u22a4\u22c6,\u22a5Bt+1 = B \u22a4 \u22c6,\u22a5Bt ( Ik\u2032 \u2212 \u03b2E[wt,iw\u22a4t,i] ) . The previous bounding conditions guarantee for a well chosen \u03b2 that \u2225E[wt,iw\u22a4t,i]\u22252 \u2264 \u03b2\u22121. Moreover thanks to Equation (7), E[wt,iw\u22a4t,i] \u2ab0 \u03b12 \u03c3 2 min B\u22a4\u22c6,\u22a5BtB \u22a4 t B\u22c6,\u22a5, which finally yields\n\u2225B\u22a4\u22c6,\u22a5Bt+1\u222522 \u2264 ( 1\u2212 \u03b12\u03b2 \u03c3 2\nmin \u2225B\u22a4\u22c6,\u22a5Bt\u222522\n) \u2225B\u22a4\u22c6,\u22a5Bt\u222522. (14)\nLearning the task mean. We can now proceed to the second limit in Theorem 1. Btwt can be decomposed into two parts, giving \u2225Btwt\u22252 \u2264 \u2225B\u22a4\u22c6 Btwt\u22252+\u2225B\u22a4\u22c6,\u22a5Btwt\u22252. As \u2225wt\u22252 is bounded and \u2225B\u22a4\u22c6,\u22a5Bt\u22252 converges to 0, the second term vanishes. A detailed analysis on the updates of B\u22a4\u22c6 Btwt gives\n\u2225B\u22a4\u22c6 Bt+1wt+1\u22252 \u2264 ( 1\u2212 \u03b2\n4\u03b1 + \u03b1\u03b2\u2225\u03a3\u22c6\u22252\n) \u2225B\u22a4\u22c6 Btwt\u22252 +O ( \u2225B\u22a4\u22c6,\u22a5Bt\u222522\u2225wt\u22252 ) ,\nwhich implies that limt\u2192\u221e Btwt = 0 for properly chosen \u03b1, \u03b2.\nFeature learning. We now focus on the limit of the matrix \u039bt := B\u22a4\u22c6 BtB\u22a4t B\u22c6 \u2208 Rk\u00d7k. The recursion on \u039bt induced by Equations (5) and (6) is as follows,\n\u039bt+1 = (Ik + \u03b1\u03b2Rt(\u039bt)) \u039bt (Ik + \u03b1\u03b2Rt(\u039bt))\n\u2212 2\u03b2Sym ( (Ik + \u03b1\u03b2Rt(\u039bt))B \u22a4 \u22c6 BtUtB \u22a4 t B\u22c6 ) + \u03b22B\u22a4\u22c6 BtU 2 t B \u22a4 t B\u22c6.\n(15)\nwhere Sym(A) := 12 ( A+A\u22a4 ) , Rt(\u039bt) := ( Ik \u2212 \u03b1(min+1)min \u039bt ) \u03a3\u22c6 \u2212 \u03b1min ( \u03c32 + \u2225Btwt\u222522 ) and Ut is some noise term defined in Appendix C. From there, we can define functions fLt and f U approximating the updates given in Equation (15) such that\nfLt (\u039bt) \u2aaf \u039bt+1 \u2aaf fU (\u039bt). Moreover, these functions preserve the Loewner matrix order for commuting matrices of interest. Thanks to that, we can construct bounding sequences of matrices (\u039bLt ), (\u039b U t ) such that\n1. \u039bLt+1 = f L t (\u039b L t ), 2. \u039b U t+1 = f U (\u039bUt ), 3. \u039b L t \u2aaf \u039bt \u2aaf \u039bUt .\nUsing the first two points, we can then show that both sequences \u039bLt , \u039b U t are non-decreasing and converge to \u039b\u22c6 under the conditions of Theorem 1. The third point then concludes the proof."
        },
        {
            "heading": "C PROOF OF THEOREM 1",
            "text": "The full version of Theorem 1 is given by Theorem 2. In particular, it gives more precise conditions on the required initialisation and step sizes.\nTheorem 2. Assume that c1 < 1, c2 are small enough positive constants verifying\nc2 min + 1\nmin +\nc1 ( c2 + \u03c3 2 )\n2min(min + 1) < \u03bbmin(\u03a3\u22c6),\nand \u03b1, \u03b2 are selected such that the following conditions hold:\n1. \u03b2 \u2264 \u03b1,\n2. 1\n\u03b12 \u2265 4\u2225\u03a3\u22c6\u22252,\n3. 1 \u03b1\u03b2 \u2265 ( c2 min + 2 min + c1c2 2min(min + 1) + 2\u03c32 min + min + 1 min \u2225\u03a3\u22c6\u22252 + 4 3 min (min + 1)2 ) ,\n4. 1\n\u03b1\u03b2 \u2265 6\n( \u2225\u03a3\u22c6\u22252 + c2 + \u03c3 2\nmin + 1\n) .\nFurthermore, suppose that parameters B0 and w0 are initialized such that the following three conditions hold:\n1. B\u22a4\u22c6 B0 is full rank,\n2. \u2225B0\u222522 \u2264 1\n\u03b1 c1 min + 1 ,\n3. \u2225w0\u222522 \u2264 \u03b1c2. Then, FO-ANIL (given by Equations (5) and (6)) with initial parameters B0, w0, inner step size \u03b1, outer step size \u03b2, asymptotically satisfies the following\nlim t\u2192\u221e\nB\u22a4\u22c6,\u22a5Bt = 0, (16)\nlim t\u2192\u221e Btwt = 0, (17)\nlim t\u2192\u221e\nB\u22a4\u22c6 BtB \u22a4 t B\u22c6 = \u039b\u22c6 =\n1\n\u03b1 min min + 1\n( Ik \u2212 ( min + 1\n\u03c32 \u03a3\u22c6 + Ik)\n)\u22121) . (18)\nThe main tools for the proof are presented and discussed in the following subsections. Section C.1 proves monotonic decay in noise terms provided that Bt is bounded by above. Section C.2 provides bounds for iterates and describes the monotonicity between updates. Section C.3 constructs sequences that bound the iterates from above and below. Section C.4 presents the full proof using the tools developed in previous sections. In the following, common recursions on relevant objects are derived.\nThe recursion on Bt defined in Equation (6) leads to the following recursions on Ct := B\u22a4\u22c6 Bt \u2208 Rk\u00d7k\u2032 and Dt := B\u22a4\u22c6,\u22a5Bt \u2208 R(d\u2212k)\u00d7k \u2032 ,\nCt+1 = ( Ik + \u03b1\u03b2 ( Ik \u2212 \u03b1(min + 1)\nmin CtC\n\u22a4 t ) \u03a3\u22c6 \u2212 \u03b12\u03b2\nmin\n( \u2225Btwt\u22252 +Tr(\u03a3\u22c6) + \u03c32 ) CtC \u22a4 t ) Ct\n\u2212 \u03b2Ct [ ( Ik\u2032 \u2212 \u03b1B\u22a4t Bt ) wtw \u22a4 t ( Ik\u2032 \u2212 \u03b1B\u22a4t Bt ) + \u03b12\nmin B\u22a4t Btwtw \u22a4 t B \u22a4 t Bt\n+ \u03b12\nmin\n( \u2225Btwt\u22252 +Tr(\u03a3\u22c6) + \u03c32 ) D\u22a4t Dt ] , (19)\nDt+1 = Dt [ Ik\u2032 \u2212 \u03b2 ( Ik\u2032 \u2212 \u03b1B\u22a4t Bt ) wtw \u22a4 t ( Ik\u2032 \u2212 \u03b1B\u22a4t Bt ) \u2212 \u03b1 2\u03b2\nmin B\u22a4t Btwtw \u22a4 t B \u22a4 t Bt\n\u2212 \u03b1 2\u03b2(min + 1)\nmin C\u22a4t \u03a3\u22c6Ct \u2212\n\u03b12\u03b2\nmin\n( \u2225Btwt\u22252 +Tr(\u03a3\u22c6) + \u03c32 ) B\u22a4t Bt ] . (20)\nFor ease of notation, let \u03c32 := Tr(\u03a3\u22c6) + \u03c32, \u03b4t := \u2225Btwt\u222522 + \u03c32 and define the following objects,\nR(\u039b, \u03c4) := ( Ik \u2212 \u03b1(min + 1)\nmin \u039b\n) \u03a3\u22c6 \u2212 \u03b1\nmin\n( \u03c32 + \u03c4 ) \u039b, Rt(\u039b) := R(\u039b, \u2225Btwt\u222522),\nWt := ( Ik\u2032 \u2212 \u03b1B\u22a4t Bt ) wtw \u22a4 t ( Ik\u2032 \u2212 \u03b1B\u22a4t Bt ) + \u03b12\nmin B\u22a4t Btwtw \u22a4 t B \u22a4 t Bt,\nUt := Wt + \u03b12\nmin \u03b4tD\n\u22a4 t Dt,\nVt := Wt + \u03b12(min + 1)\nmin C\u22a4t \u03a3\u22c6Ct +\n\u03b12\nmin \u03b4tB\n\u22a4 t Bt. (21)\nThen, the recursion for \u039bt := CtC\u22a4t is\n\u039bt+1 = (Ik + \u03b1\u03b2Rt(\u039bt)) \u039bt (Ik + \u03b1\u03b2Rt(\u039bt)) \u22a4 + \u03b22CtU 2 t C \u22a4 t\n\u2212 \u03b2 (Ik + \u03b1\u03b2Rt(\u039bt))CtUtC\u22a4t \u2212 \u03b2CtUtC\u22a4t (Ik + \u03b1\u03b2Rt(\u039bt))\u22a4 . (22)"
        },
        {
            "heading": "C.1 REGULARITY CONDITIONS",
            "text": "Lemmas 1 and 2 control \u2225wt\u22252 and \u2225Dt\u22252 across iterations, respectively. Lemma 3 shows that \u2225Ctwt\u22252 is decaying with a noise term that vanishes as \u2225Dt\u22252 gets small. Corollary 1 combines all three results and yields the first two claims of Theorem 1,\nlim t\u2192\u221e B\u22c6,\u22a5Bt = 0, lim t\u2192\u221e Btwt = 0,\nunder the assumption that conditions of Lemmas 2 and 3 are satisfied for all t. Lemmas 4 and 5 bound \u2225Ut\u22252 and \u2225Wt\u22252, ensuring that the recursions of \u039bt are well-behaved in later sections. Lemma 1. Assume that\nc0Ik\u2032 \u2aaf B\u22a4t Bt \u2aaf 1\n\u03b1 min + c1 min + 1 Ik\u2032 ,\nfor constants 0 \u2264 c0, 0 < c1 < 1 such that \u03b2c0(1\u2212 c1) \u2264 min + 1. Then,\n\u2225wt+1\u22252 \u2264 ( 1\u2212 \u03b2 c0(1\u2212 c1)\nmin + 1\n) \u2225wt\u22252.\nProof. From the assumption, 1\u2212 c1 min + 1 Ik\u2032 \u2aaf Ik\u2032 \u2212 \u03b1B\u22a4t Bt \u2aaf (1\u2212 \u03b1c0)Ik\u2032 ,\nand\nB\u22a4t Bt(Ik\u2032 \u2212 \u03b1B\u22a4t Bt) \u2ab0 c0(1\u2212 c1) min + 1 Ik\u2032 .\nRecalling the recursion for wt defined in Equation (5),\n\u2225wt+1\u22252 \u2264 ( 1\u2212 \u03b2 c0(1\u2212 c1)\nmin + 1\n) \u2225wt\u22252.\nLemma 2. Assume that\n\u2225Bt\u222522 \u2264 1\n\u03b1 , \u2225wt\u222522 \u2264 c\u03b1,\nfor a constant c \u2265 0 and \u03b1, \u03b2 satisfy 1\n\u03b1\u03b2 \u2265 min + 2 min c+ 1 min\n( (min + 1)\u2225\u03a3\u22c6\u22252 + \u03c32 ) , (23)\n1 \u03b1\u03b2 \u2265 2\u03c3\n2\nmin . (24)\nThen,\n\u2225Dt+1D\u22a4t+1\u22252 \u2264 ( 1\u2212 \u03b1 2\u03b2\nmin \u03c32\u2225DtD\u22a4t \u22252\n) \u2225DtD\u22a4t \u22252.\nProof. The recursion on DtD\u22a4t is given by\nDt+1D \u22a4 t+1 = Dt(Ik\u2032 \u2212 \u03b2Vt)2D\u22a4t ,\nwhere we recall Vt is defined in Equation (21). First step is to show Ik\u2032 \u2212 \u03b2Vt \u2ab0 0 by proving \u2225Vt\u22252 \u2264 1\u03b2 . By the definition of Vt,\n\u2225Vt\u22252 \u2264 \u2225Wt\u22252 \ufe38 \ufe37\ufe37 \ufe38\n(A)\n+ \u03b12(min + 1)\nmin \u2225C\u22a4t \u03a3\u22c6Ct\u22252 \ufe38 \ufe37\ufe37 \ufe38 (B)\n+ \u03b12\nmin \u03b4t\u2225B\u22a4t Bt\u22252 \ufe38 \ufe37\ufe37 \ufe38 (C) .\nTerm (A) is bounded by Lemma 5. For the term (B), using \u2225Ct\u22252 = \u2225B\u22a4\u22c6 Bt\u22252 \u2264 \u2225Bt\u22252,\n\u2225C\u22a4t \u03a3\u22c6Ct\u22252 \u2264 1\n\u03b1 \u2225\u03a3\u22c6\u22252.\nTerm (C) is bounded as\n\u03b4t = \u2225Btwt\u222522 + \u03c32 \u2264 1 \u03b1 \u2225wt\u222522 + \u03c32 \u2264 c+ \u03c32, \u2225B\u22a4t Bt\u22252 \u2264 \u2225Bt\u222522 \u2264 1 \u03b1 .\nCombining three bounds and using the condition in Equation (23),\n\u2225Vt\u22252 \u2264 min + 1\nmin \u03b1c+\n\u03b1\nmin\n( (min + 1)\u2225\u03a3\u22c6\u22252 + \u03c32 ) + \u03b1c \u2264 1\n\u03b2 .\nTherefore, it is possible to upper bound Dt+1D\u22a4t+1 as follows,\nDt+1D \u22a4 t+1 = Dt(Ik\u2032 \u2212 \u03b2Vt)2D\u22a4t \u2aaf Dt(Ik\u2032 \u2212 \u03b2Vt)D\u22a4t\n\u2aaf Dt [ Ik\u2032 \u2212 \u03b12\u03b2\nmin \u03c32D\u22a4t Dt\n] D\u22a4t\n= [ Ik\u2032 \u2212 \u03b12\u03b2\nmin \u03c32DtD \u22a4 t\n] DtD \u22a4 t .\nLet DtD\u22a4t = \u2126tSt\u2126 \u22a4 t be the SVD decomposition of DtD \u22a4 t in this proof. Then,\nDt+1D \u22a4 t+1 \u2aaf \u2126t ( St \u2212 \u03b12\u03b2\nmin \u03c32S2t\n) \u2126\u22a4t .\nNote that 1\u03b1 < min 2\u03b12\u03b2\u03c32 by Equation (24) and for any s1 \u2264 s2 < 1 \u03b1 < min 2\u03b12\u03b2\u03c32 ,\ns2(1\u2212 \u03b12\u03b2\nmin \u03c32s2) \u2265 s1(1\u2212\n\u03b12\u03b2 min \u03c32s1),\nby monotonicity of x 7\u2192 x(1\u2212 \u03b12\u03b2min \u03c3 2x). Hence, if s is the largest eigenvalue of St, s(1\u2212 \u03b1 2\u03b2 min \u03c32s) is the largest eigenvalue of (St \u2212 \u03b1 2\u03b2\nmin \u03c32S2t ) and\n\u2225Dt+1D\u22a4t+1\u22252 \u2264 ( 1\u2212 \u03b1 2\u03b2\nmin \u03c32\u2225DtD\u22a4t \u22252\n) \u2225DtD\u22a4t \u22252.\nLemma 3. Suppose that \u03b2 \u2264 \u03b1 and the following conditions hold,\n\u2225Bt\u222522 \u2264 1\n\u03b1 , \u2225\u039bt\u222522 \u2264\n1\n\u03b1 min min + 1 , \u2225wt\u222522 \u2264 \u03b1c,\nwhere c \u2265 0 is a constant such that ( 1\u2212 \u03b24\u03b1 )\n\u03b1\u03b2 \u2265 \u03c3\n2\nmin + 1 + c\nmin + 2 min + 1 + min (min + 1)2 .\nThen,\n\u2225Ct+1wt+1\u22252 \u2264 ( 1\u2212 \u03b2\n4\u03b1 + \u03b1\u03b2\u2225\u03a3\u22c6\u22252\n) \u2225Ctwt\u22252 +M\u2225Dt\u222522\u2225wt\u22252,\nfor a constant M depending only on \u03b1.\nProof. Let \u2126t := C\u22a4t Ct. Expanding the recursion for wt+1,\nCt+1wt+1 = Ct+1 ( Ik\u2032 \u2212 \u03b2\u2126t + \u03b1\u03b2\u21262t ) wt\ufe38 \ufe37\ufe37 \ufe38\n(A)\n+ \u03b1\u03b2Ct+1 ( D\u22a4t Dt \u2212 2\u03b1D\u22a4t Dt \u2212 \u03b12D\u22a4t Dt\u2126t \u2212 \u03b12\u2126tD\u22a4t Dt \u2212 \u03b12D\u22a4t DtD\u22a4t Dt ) wt\ufe38 \ufe37\ufe37 \ufe38\n(B)\n.\nSince \u2225Bt\u222522 \u2264 1\u03b1 , there is some constant MB depending only on \u03b1 such that\nMB\u2225Dt\u222522\u2225wt\u22252 \u2265 \u2225(B)\u22252. Expanding term (A),\nCt+1 ( Ik\u2032 \u2212 \u03b2\u2126t + \u03b1\u03b2\u21262t ) wt = \u03b1\u03b2 ( I \u2212 \u03b1min + 1\nmin \u039bt\n) \u03a3\u22c6Ct ( Ik\u2032 \u2212 \u03b2\u2126t + \u03b1\u03b2\u21262t ) wt\n\ufe38 \ufe37\ufe37 \ufe38 (C)\n\u2212 Ct ( Ik\u2032 \u2212 \u03b12\u03b2\nmin \u03b4t\u2126t \u2212 \u03b2 (Ik\u2032 \u2212 \u03b1\u2126t)wtw\u22a4t (Ik\u2032 \u2212 \u03b1\u2126t)\u2212\n\u03b12\u03b2 min \u2126twtw \u22a4 t \u2126t\n)( Ik\u2032 \u2212 \u03b2\u2126t + \u03b1\u03b2\u21262t ) wt\n\ufe38 \ufe37\ufe37 \ufe38 (D)\n+ \u03b1\u03b2Ct ( D\u22a4t Dtwtw \u22a4 t (Ik\u2032 \u2212 \u03b1\u2126t) + (Ik\u2032 \u2212 \u03b1\u2126t)wtw\u22a4t D\u22a4t Dt \u2212 \u03b1D\u22a4t Dtwtw\u22a4t D\u22a4t Dt ) \ufe38 \ufe37\ufe37 \ufe38\n(E)\n,\nwhere \u03b1 := \u03b1min+1min . Similarly to term (B), there is a constant ME depending only on \u03b1 such that\nME\u2225Dt\u222522\u2225wt\u222522 \u2265 \u2225(E)\u22252. Bounding term (C),\n\u2225(C)\u22252 = \u2225\u2225\u2225\u2225\u2225 ( I \u2212 \u03b1min + 1 min \u039bt ) \u03a3\u22c6 ( Ik \u2212 \u03b2\u039bt + \u03b1\u03b2\u039b2t ) Ctwt \u2225\u2225\u2225\u2225\u2225 2\n\u2264 \u2225I \u2212 \u03b1min + 1 min \u039bt\u22252\u2225\u03a3\u22c6\u22252\u2225Ik \u2212 \u03b2\u039bt + \u03b1\u03b2\u039b2t\u22252\u2225Ctwt\u22252 = \u2225\u03a3\u22c6\u22252 ( 1\u2212 \u03b1min + 1\nmin \u03bbk(\u039bt)\n)( 1\u2212 \u03b2\u03bbk(\u039bt) + \u03b1\u03b2\u03bbk(\u039bt)2 ) \u2225Ctwt\u22252.\nRe-writing term (D),\n(D) = (( Ik \u2212 \u03b12\u03b2\nmin \u03b4t\u039bt\n)( Ik \u2212 \u03b2\u039bt + \u03b1\u03b2\u039b2t ) \u2212 \u03b2d1 (Ik\u2032 \u2212 \u03b1\u039bt)\u2212 \u03b12\u03b2\nmin d2\u039bt\n)\n\ufe38 \ufe37\ufe37 \ufe38 (F)\nCtwt\nwhere d1 and d2 are defined as\nd1 := \u2329 (Ik\u2032 \u2212 \u03b1\u2126t)wt, ( Ik\u2032 \u2212 \u03b2\u2126t + \u03b1\u03b2\u21262t ) wt \u232a , d2 := \u2329 \u2126twt, ( Ik \u2212 \u03b2\u2126t + \u03b1\u03b2\u21262t ) wt \u232a .\nAs all eigenvalues of \u2126t are in [ 0, 1\u03b1 min min+1 ] ,\n( 1\u2212 \u03b2\n4\u03b1\n) Ik\u2032 \u2aaf Ik\u2032 \u2212 \u03b2\u2126t + \u03b1\u03b2\u21262t \u2aaf Ik\u2032 ,\nand\n1\nmin + 1\n( 1\u2212 \u03b2\n4\u03b1\n) \u2aaf (Ik\u2032 \u2212 \u03b1\u2126t) ( Ik\u2032 \u2212 \u03b2\u2126t + \u03b1\u03b2\u21262t ) \u2aaf Ik\u2032 ,\n0 \u2aaf \u2126t ( Ik \u2212 \u03b2\u2126t + \u03b1\u03b2\u21262t ) \u2aaf 1\n\u03b1 min min + 1 .\nTherefore, d1 and d2 are non-negative and bounded from above as follows,\n\u03b1c ( 1\u2212 \u03b24\u03b1 )\nmin + 1 \u2264 d1 \u2264 \u03b1c, 0 \u2264 d2 \u2264 cmin min + 1 .\nBy assumptions,\n\u03b12\u03b2 min \u03b4t \u2225\u2225\u039bt ( Ik \u2212 \u03b2\u039bt + \u03b1\u03b2\u039b2t ) \u2225\u2225 2 \u2264 \u03b1\u03b2 min + 1 \u03b4t \u2264\n\u03b1\u03b2 ( c+ \u03c32 )\nmin + 1 ,\nand combining all the negative terms in (F),\n\u03b12\u03b2 min \u03b4t\u039bt ( Ik \u2212 \u03b2\u039bt + \u03b1\u03b2\u039b2t ) +\u03b2d1 (Ik\u2032 \u2212 \u03b1\u039bt)+ \u03b12\u03b2 min d2\u039bt \u2aaf \u03b1\u03b2\n( c+ \u03c32\nmin + 1 + c+ min (min + 1)2\n) Ik\u2032 .\nHence, (F) is bounded by below and above,\n0 \u2aaf (F ) \u2aaf ( Ik \u2212 \u03b2\u039bt + \u03b1\u03b2\u039b2t ) .\nThus, the norm of (F) is bounded by above,\n\u2225(F )\u22252 \u2264 ( 1\u2212 \u03b2\n4\u03b1\n) .\nCombining all the bounds,\n\u2225Ct+1wt+1\u22252 \u2264 ( 1\u2212 \u03b2\n4\u03b1 + \u03b1\u03b2\u2225\u03a3\u22c6\u22252\n) \u2225Ctwt\u22252 +M\u2225Dt\u222522\u2225wt\u22252,\nwhere M is a constant depending only on \u03b1.\nCorollary 1. Assume that conditions of Lemma 2 are satisfied for a fixed c > 0 for all times t. Then, Lemma 2 directly implies that\nlim t\u2192\u221e B\u22a4\u22c6,\u22a5Bt = lim t\u2192\u221e Dt = 0.\nFurther, assume that conditions of Lemma 3 is satisfied for all times t and\n1\n\u03b12 \u2265 4\u2225\u03a3\u22c6\u22252. (25)\nThen, Lemmas 2 and 3 together imply that\nlim t\u2192\u221e\n\u2225Btwt\u22252 = 0.\nProof. The first result directly follows as by Lemma 2,\nlim t\u2192\u221e\n\u2225Dt\u22252 = 0.\nHence, for any \u03f5 > 0, there exist a t\u03f5 such that\n\u2200t > t\u03f5, \u2225Dt\u22252 < \u03f5\u221a c\u03b1 .\nObserve that for any t,\n\u2225Bt+1wt+1\u22252 \u2264 \u2225B\u22c6Ct+1wt+1\u22252 + \u2225B\u22c6,\u22a5Dt+1wt+1\u22252 = \u2225Ct+1wt+1\u22252 + \u2225Dt+1wt+1\u22252. Therefore, by Lemma 3, for any t > t\u03f5,\n\u2225Bt+1wt+1\u22252 \u2264 ( 1\u2212 \u03b2\n4\u03b1 + \u03b1\u03b2\u2225\u03a3\u22c6\u22252\n) \u2225Ctwt\u22252 + \u03f52\nM\u221a c\u03b1 + \u03f5\n\u2264 ( 1\u2212 \u03b2\n4\u03b1 + \u03b1\u03b2\u2225\u03a3\u22c6\u22252\n) \u2225Btwt\u22252 + \u03f52\nM\u221a c\u03b1 + \u03f5.\nBy Equation (25), ( 1\u2212 \u03b2\n4\u03b1 + \u03b1\u03b2\u2225\u03a3\u22c6\u22252\n) < 1,\nand \u2225Btwt\u22252 is decaying for t > t\u03f5 as long as\n\u2225Btwt\u22252 \u2265 \u03f5 ( 1 + \u03f5 M\u221a c\u03b1 )\n\u03b1\u03b2 (\n1 4\u03b12 \u2212 \u2225\u03a3\u22c6\u22252\n) .\nHence, for any \u03f5\u2032 > 0, it is possible to find t\u03f5\u2032 > t\u03f5 such that for all t > t\u03f5\u2032 ,\n\u2225Btwt\u22252 \u2264 \u03f5 ( 1 + \u03f5 M\u221a c\u03b1 )\n\u03b1\u03b2 (\n1 4\u03b12 \u2212 \u2225\u03a3\u22c6\u22252\n) + \u03f5\u2032.\nAs \u03f5 and \u03f5\u2032 are arbitrary, lim t\u2192\u221e \u2225Btwt\u22252 = 0.\nLemma 4. Assume that \u2225Dt\u222522 \u2264 1\u03b1 c12(min+1) , \u2225Bt\u2225 2 2 \u2264 1\u03b1 , \u2225wt\u222522 \u2264 \u03b1c2 for constants c1, c2 \u2208 R+. Then,\n\u2225Ut\u22252 \u2264 \u03b1 ( c2 min + 1\nmin +\nc1 ( c2 + \u03c3 2 )\n2min(min + 1)\n) .\nProof. By definition of Ut,\n\u2225Ut\u22252 \u2264 \u2225Wt\u22252 \ufe38 \ufe37\ufe37 \ufe38\n(A)\n+ \u03b12\nmin \u03b4t\u2225D\u22a4t Dt\u22252 \ufe38 \ufe37\ufe37 \ufe38 (B) .\nTerm (A) is bounded by Lemma 5. For the term (B), bounding \u03b4t by conditions on Bt and wt,\n\u03b4t = \u2225Btwt\u222522 + \u03c32 \u2264 c2 + \u03c32, one has the following bound\n\u03b12\nmin \u03b4t\u2225D\u22a4t Dt\u22252 \u2264\n\u03b1c1 ( c2 + \u03c3 2 )\n2min (min + 1) .\nCombining the two bounds yields the result,\n\u2225Ut\u22252 \u2264 \u03b1 ( c2 min + 1\nmin +\nc1 ( c2 + \u03c3 2 )\n2min(min + 1)\n) .\nLemma 5. Assume that \u2225Bt\u222522 \u2264 1\u03b1 and \u2225wt\u222522 \u2264 \u03b1c for a constant c \u2208 R+. Then,\n\u2225Wt\u22252 \u2264 \u03b1c min + 1\nmin .\nProof. By using 0 \u2aaf B\u22a4t Bt \u2aaf 1\u03b1Ik\u2032 ,\n\u2225(Ik\u2032 \u2212 \u03b1B\u22a4t Bt)wtw\u22a4t (Ik\u2032 \u2212 \u03b1B\u22a4t Bt)\u22252 = \u2225(Ik\u2032 \u2212 \u03b1B\u22a4t Bt)wt\u222522 \u2264 \u2225wt\u222522 \u2264 \u03b1c,\n\u2225B\u22a4t Btwtw\u22a4t B\u22a4t Bt\u22252 = \u2225B\u22a4t Btwt\u222522 \u2264 1\n\u03b12 \u2225wt\u222522 \u2264\nc \u03b1 ,\nand the result follows by\n\u2225Wt\u22252 \u2264 \u2225(Ik\u2032 \u2212 \u03b1B\u22a4t Bt)wtw\u22a4t (Ik\u2032 \u2212 \u03b1B\u22a4t Bt)\u22252 + \u03b12\nmin \u2225B\u22a4t Btwtw\u22a4t B\u22a4t Bt\u22252 \u2264 \u03b1c\nmin + 1\nmin ."
        },
        {
            "heading": "C.2 BOUNDS ON ITERATES AND MONOTONICITY",
            "text": "The recursion for \u039bt given in Equation (22) has the following main term:\n(Ik + \u03b1\u03b2Rt(\u039bt))\u039bt(Ik + \u03b1\u03b2Rt(\u039bt)) \u22a4.\nLemma 6 bounds \u039bt+1 from above by this term, i.e., terms involving Ut are negative. On the other hand, Lemma 7 bounds \u039bt+1 from below with the expression\n(Ik + \u03b1\u03b2Rt(\u039bt)\u2212 \u03b1\u03b2\u03b3tIk)\u039bt(Ik + \u03b1\u03b2Rt(\u039bt)\u2212 \u03b1\u03b2\u03b3tIk)\u22a4, (26) where \u03b3t \u2208 R+ is a scalar such that \u2225Ut\u22252 \u2264 \u03b1\u03b3t. Lastly, Lemma 9 shows that updates of the form of Equation (26) enjoy a monotonicity property which allows the control of \u039bt over time from above and below by constructing sequences of matrices, as described in Appendix C.3.\nLemma 6. Suppose that \u2225Ut\u22252 \u2264 1\u03b2 . Then,\n\u039bt+1 \u2aaf (Ik + \u03b1\u03b2Rt(\u039bt)) \u039bt (Ik + \u03b1\u03b2Rt(\u039bt))\u22a4 .\nProof. As \u2225Ut\u22252 \u2264 1\u03b2 ,\nCtUtC \u22a4 t \u2212 \u03b2CtU2t C\u22a4t = Ct(Ut \u2212 \u03b2U2t )C\u22a4t \u2ab0 0.\nUsing Appendix C.2,\n\u039bt+1 = (Ik + \u03b1\u03b2Rt(\u039bt)) ( \u039bt \u2212 \u03b2CtUtC\u22a4t ) (Ik + \u03b1\u03b2Rt(\u039bt))\n\u22a4 \u2212 \u03b2CtUtC\u22a4t + \u03b22CtU2t C\u22a4t \u2aaf (Ik + \u03b1\u03b2Rt(\u039bt)) ( \u039bt \u2212 \u03b2CtUtC\u22a4t ) (Ik + \u03b1\u03b2Rt(\u039bt)) \u22a4 \u2aaf (Ik + \u03b1\u03b2Rt(\u039bt)) \u039bt (Ik + \u03b1\u03b2Rt(\u039bt))\u22a4 .\nLemma 7. Let \u03b3t be a scalar such that \u2225Ut\u22252 \u2264 \u03b1\u03b3t \u2264 12\u03b2 . Then,\n(Ik + \u03b1\u03b2Rt(\u039bt)\u2212 \u03b1\u03b2\u03b3tIk)\u039bt(Ik + \u03b1\u03b2Rt(\u039bt)\u2212 \u03b1\u03b2\u03b3tIk)\u22a4 \u2aaf \u039bt+1.\nProof. By using \u2225Ut\u22252 \u2264 \u03b1\u03b3t, \u03b1\u03b3t\u039bt \u2212 CtUtC\u22a4t = Ct(\u03b1\u03b3tIk \u2212 Ut)C\u22a4t \u2ab0 0.\nMoreover, as x 7\u2192 x\u2212 \u03b2x2,\nis an increasing function in [0, 12\u03b2 ], the maximal eigenvalue of\nUt \u2212 \u03b2U2t is s\u2212 \u03b2s2 \u2264 \u03b1\u03b3t \u2212 \u03b12\u03b2\u03b32t where s is the maximal eigenvalue Ut. Hence,\n( \u03b1\u03b3t \u2212 \u03b12\u03b2\u03b32t ) Ik\u2032 \u2212 ( Ut \u2212 \u03b2U2t ) \u2ab0 0.\nTherefore, the following expression is positive semi-definite,\n2Sym ( (Ik + \u03b1\u03b2Rt(\u039bt)) ( \u03b1\u03b3t\u039bt \u2212 CtUtC\u22a4t )) \u2212 \u03b2(\u03b12\u03b32t\u039bt \u2212 CtU2t C\u22a4t )\n= (Ik + \u03b1\u03b2Rt(\u039bt)) ( \u03b1\u03b3t\u039bt \u2212 CtUtC\u22a4t ) (Ik + \u03b1\u03b2Rt(\u039bt)) \u22a4\n+ (( \u03b1\u03b3t\u039bt \u2212 CtUtC\u22a4t ) \u2212 \u03b2 ( \u03b12\u03b32t\u039bt \u2212 CtU2t C\u22a4t ))\n\u2ab0 Ct [( \u03b1\u03b3t \u2212 \u03b12\u03b2\u03b32t ) Ik\u2032 \u2212 ( Ut \u2212 \u03b2U2t )] C\u22a4t .\nThe result follows by\n\u039bt+1 = (Ik + \u03b1\u03b2Rt(\u039bt)) \u039bt (Ik + \u03b1\u03b2Rt(\u039bt)) \u22a4 \u2212 2\u03b2Sym ( (Ik + \u03b1\u03b2Rt(\u039bt))CtUtC \u22a4 t ) \u2212 \u03b22CtU2t C\u22a4t\n\u2ab0 (Ik + \u03b1\u03b2Rt(\u039bt)) \u039bt (Ik + \u03b1\u03b2Rt(\u039bt))\u22a4 \u2212 2\u03b1\u03b2\u03b3tSym ((Ik + \u03b1\u03b2Rt(\u039bt)) \u039bt)\u2212 \u03b12\u03b22\u03b32t\u039bt = (Ik + \u03b1\u03b2Rt(\u039bt)\u2212 \u03b1\u03b2\u03b3tIk)\u039bt(Ik + \u03b1\u03b2Rt(\u039bt)\u2212 \u03b1\u03b2\u03b3tIk)\u22a4.\nLemma 8. Let Ct = \u03a8tSt\u0393\u22a4t be the (thin) SVD decomposition of Ct and let \u03b3t be a scalar such that \u2225\u0393\u22a4t Ut\u0393t\u22252 \u2264 \u03b1\u03b3t \u2264 12\u03b2 . Then,\n(Ik + \u03b1\u03b2Rt(\u039bt)\u2212 \u03b1\u03b2\u03b3tIk)\u039bt(Ik + \u03b1\u03b2Rt(\u039bt)\u2212 \u03b1\u03b2\u03b3tIk)\u22a4 \u2aaf \u039bt+1.\nProof. It is sufficient to observe that \u03b1\u03b3t\u039bt \u2212 CtUtC\u22a4t = \u03a8tSt ( \u03b1\u03b3tIk\u2032 \u2212 \u0393\u22a4t Ut\u0393t ) St\u03a8 \u22a4 t \u2ab0 0,\nand use the same argument as in the proof of Lemma 7.\nLemma 9. For non-negative scalars \u03c4, \u03b3, let f(\u00b7; \u03c4, \u03b3) : Symk(R) \u2192 Symk(R) be defined as follows,\nf(\u039b; \u03c4, \u03b3) := (Ik + \u03b1\u03b2R(\u039b, \u03c4)\u2212 \u03b1\u03b2\u03b3Ik)\u039b(Ik + \u03b1\u03b2R(\u039b, \u03c4)\u2212 \u03b1\u03b2\u03b3Ik)\u22a4. Then, f(\u00b7; \u03c4, \u03b3) preserves the partial order between any \u039b,\u039b\u2032 that commutes with each other and \u03a3\u22c6, i.e.,\n1\n\u03b1 min min + 1 Ik \u2ab0 \u039b \u2ab0 \u039b\u2032 \u2ab0 0 =\u21d2 f(\u039b; \u03c4, \u03b3) \u2ab0 f(\u039b\u2032; \u03c4, \u03b3), when the following condition holds,\n1\u2212 \u03b1\u03b2\u03b3 \u2265 5\u03b1\u03b2(\u2225\u03a3\u22c6\u22252 + \u03c32 + \u03c4\nmin + 1 ).\nProof. The result follows if and only if (1\u2212 \u03b1\u03b2\u03b3)2(\u039b\u2212 \u039b\u2032) \u2ab0 \u03b1\u03b2(1\u2212 \u03b1\u03b2\u03b3)[R(\u039b\u2032, \u03c4)\u039b\u2032 \u2212R(\u039b, \u03c4)\u039b]\ufe38 \ufe37\ufe37 \ufe38\n(A)\n+ \u03b1\u03b2(1\u2212 \u03b1\u03b2\u03b3)[\u039b\u2032R(\u039b\u2032, \u03c4)\u2212 \u039bR(\u039b, \u03c4)]\ufe38 \ufe37\ufe37 \ufe38 (B) + \u03b12\u03b22[R(\u039b\u2032, \u03c4)\u039b\u2032R(\u039b\u2032, \u03c4)\u2212R(\u039b, \u03c4)\u039bR(\u039b, \u03c4)]\ufe38 \ufe37\ufe37 \ufe38 (C) . (27)\nBy Lemma 16,\n\u039b2 \u2212 \u039b\u20322 = 1 2 (\u039b\u2212 \u039b\u2032)(\u039b + \u039b\u2032) + 1 2 (\u039b + \u039b\u2032)(\u039b\u2212 \u039b\u2032)\n\u2aaf \u2225\u039b + \u039b\u2032\u22252(\u039b\u2212 \u039b\u2032) \u2aaf 2\u2225\u039b\u22c6\u22252(\u039b\u2212 \u039b\u2032). Bounding term (A) by using commutativity of \u039b,\u039b\u2032 with \u03a3\u22c6 and \u039b,\u039b\u2032 \u2aaf 1\u03b1 minmin+1 ,\nR(\u039b\u2032, \u03c4)\u039b\u2032 \u2212R(\u039b, \u03c4)\u039b = \u03b1(min + 1) min \u039b\n[ \u03a3\u22c6 + \u03c32 + \u03c4\nmin + 1 Ik\n] \u039b\n\u2212 \u03b1(min + 1) min\n\u039b\u2032 [ \u03a3\u22c6 + \u03c32 + \u03c4\nmin + 1 Ik\n] \u039b\u2032 \u2212 \u03a3\u22c6(\u039b\u2212 \u039b\u2032)\n\u2aaf \u03b1(min + 1) min\n[ \u2225\u03a3\u22c6\u22252 + \u03c32 + \u03c4\nmin + 1\n] (\u039b2 \u2212 \u039b\u20322).\nThe term (B) is equal to the term (A) and thus bounded by the same expression. By Lemma 17,\n\u039b3 \u2212 \u039b\u20323 \u2ab0 0. Bounding term (C), using the commutativity of \u039b,\u039b\u2032 with \u03a3\u22c6 and \u039b,\u039b\u2032 \u2aaf 1\u03b1 minmin+1 ,\nR(\u039b\u2032, \u03c4)\u039b\u2032R(\u039b\u2032, \u03c4)\u2212R(\u039b, \u03c4)\u039bR(\u039b, \u03c4) = 2\u03b1(min + 1) min\n[ \u03a32\u22c6 + \u03c32 + \u03c4\nmin + 1 \u03a3\u22c6\n] (\u039b2 \u2212 \u039b\u20322)\n\u2212 \u03a3\u22c6(\u039b\u2212 \u039b\u2032)\u03a3\u22c6 \u2212 ( \u03b1(min + 1)\nmin\n)2 [ \u03a3\u22c6 + \u03c32 + \u03c4\nmin + 1 Ik\n]2 (\u039b3 \u2212 \u039b\u20323)\n\u2aaf 4\u2225\u03a3\u22c6\u22252 [ \u2225\u03a3\u22c6\u22252 + \u03c32 + \u03c4\nmin + 1\n] (\u039b\u2212 \u039b\u2032).\nTherefore, Equation (27) is satisfied if\n(1\u2212 \u03b1\u03b2\u03b3)2 \u2265 4\u03b1\u03b2(1\u2212 \u03b1\u03b2\u03b3) [ \u2225\u03a3\u22c6\u22252 + \u03c32 + \u03c4\nmin + 1\n] + 4\u03b12\u03b22\u2225\u03a3\u22c6\u22252 [ \u2225\u03a3\u22c6\u22252 + \u03c32 + \u03c4\nmin + 1\n] ,\nwhich holds by the given condition.\nRemark 1. Let \u03c4, \u03b3 be scalars such that 0 < \u03c4 and 0 < \u03b3 < \u03bbmin(\u03a3\u22c6). Define \u039b\u22c6(\u03c4, \u03b3) as follows,\n\u039b\u22c6 (\u03c4, \u03b3) := 1\n\u03b1 min min + 1\n( Ik \u2212 ( \u03c32 + \u03c4\nmin + 1 + \u03b3\n)( \u03a3\u22c6 + \u03c32 + \u03c4\nmin + 1 Ik\n)\u22121) . (28)\n(\u039b\u22c6, \u03c4, \u03b3) is a fixed point of the function f as\nR (\u039b\u22c6(\u03c4, \u03b3)) = \u03b3Ik.\nCorollary 2. Let \u039b be a symmetric p.s.d. matrix which commutes with \u03a3\u22c6 and satisfy\n\u039b \u2aaf \u039b\u22c6(\u03c4, \u03b3), for some scalars 0 < \u03c4 and 0 < \u03b3 < \u03bbmin(\u03a3\u22c6). Then, assuming that conditions of Lemma 9 are satisfied,\n\u039b \u2aaf f(\u039b; \u03c4, \u03b3) \u2aaf \u039b\u22c6(\u03c4, \u03b3).\nProof. For the left-hand side, note that\nR(\u039b, \u03c4, \u03b3) \u2ab0 \u03b3Ik \u21d0\u21d2 \u039b \u2aaf \u039b\u22c6(\u03c4, \u03b3). Hence, by the given assumption and commutativity,\n\u039b \u2aaf (Ik + \u03b1\u03b2R(\u039b, \u03c4)\u2212 \u03b1\u03b2\u03b3Ik) \u039b (Ik + \u03b1\u03b2R(\u039b, \u03c4)\u2212 \u03b1\u03b2\u03b3Ik)\u22a4 = f(\u039b; \u03c4, \u03b3). For the right-hand side, note that by Lemma 9\nf(\u039b; \u03c4, \u03b3) \u2aaf f(\u039b\u22c6(\u03c4, \u03b3); \u03c4, \u03b3) = \u039b\u22c6(\u03c4, \u03b3).\nLemma 10. Let \u03c4t and \u03b3t be non-negative, non-increasing scalar sequences such that \u03b30 < \u03bbmin(\u03a3\u22c6), and \u039b be a symmetric p.s.d. matrix that commutes with \u03a3\u22c6 such that\n\u039b \u2aaf \u039b\u22c6(\u03c40, \u03b30), where \u039b\u22c6(\u03c4, \u03b3) is defined in Equation (28). Furthermore, suppose that \u03b1 and \u03b2 satisfy\n1 \u03b1\u03b2 \u2265 ( s\u22c6 + \u03c32 + \u03c40 min + 1 ) .\nThen, the sequence of matrices that are defined recursively as\n\u039b(0) := \u039b, \u039b(t+1) := f(\u039b(t); \u03c4t, \u03b3t),\nsatisfy lim t\u2192\u221e \u039b(t) = \u039b\u22c6( lim t\u2192\u221e \u03c4t, lim t\u2192\u221e \u03b3t).\nProof. By the monotone convergence theorem, \u03c4t and \u03b3t are convergent. Let \u03c4\u221e and \u03b3\u221e denote the limits, i.e.,\n\u03c4\u221e := lim t\u2192\u221e \u03c4t, \u03b3\u221e := lim inf t\u2192\u221e \u03b3t.\nAs \u039b(0) and \u03a3\u22c6 are commuting normal matrices, they are simultaneously diagonalisable, i.e., there exists an orthogonal matrix Q \u2208 Rk\u00d7k and diagonal matrices with positive entries D(0), D\u22c6 such that\n\u039b(0) = QD(0)Q\u22a4, \u03a3\u22c6 = QD\u22c6Q \u22a4.\nThen, applying f to any matrix of from \u039b = QDQ\u22a4, where D is a diagonal matrix with positive entries, yields\nf(\u039b; \u03c4, \u03b3) = Q ( Ik + \u03b1\u03b2D\u22c6 \u2212 \u03b12\u03b2 min + 1\nmin D\n( D\u22c6 + \u03c32 + \u03c4\nmin + 1 Ik\n) \u2212 \u03b1\u03b2\u03b3 )2 DQ\u22a4.\nObserve that f operates entry-wise on diagonal elements of D, i.e., for any diagonal element s of D, the output in the corresponding entry of f is given by the following map g(\u00b7, s\u22c6, \u03c4, \u03b3) : R\u2192 R,\ng(s; s\u22c6, \u03c4, \u03b3) := ( 1 + \u03b1\u03b2s\u22c6 \u2212 \u03b12\u03b2 min + 1\nmin s(s\u22c6 +\n\u03c32 + \u03c4\nmin + 1 )\u2212 \u03b1\u03b2\u03b3\n)2 s,\nwhere s\u22c6 is the corresponding diagonal entry of D\u22c6. Hence, Lemma 10 holds if\nlim t\u2192\u221e st = s\u221e(\u03c4\u221e, \u03b3\u221e),\nwhere st is defined recursively from an initial value s0 for any t \u2265 1 as follows, st+1 := g(st; s\u22c6, \u03c4t, \u03b3t),\nand s\u221e(\u03c4, \u03b3) is defined as\ns\u221e(\u03c4, \u03b3) := 1\n\u03b1 min min + 1\n( 1\u2212 ( \u03b3 + \u03c32 + \u03c4\nmin + 1\n)( s\u22c6 + \u03c32 + \u03c4\nmin + 1\n)\u22121) .\nObserve that\ns\u221e(\u03c4, \u03b3) ( s\u22c6 + \u03c32 + \u03c4\nmin + 1\n) = 1\n\u03b1 min min + 1 (s\u22c6 \u2212 \u03b3) ,\nand\ng(st; s\u22c6, \u03c4t, \u03b3t) = ( 1 + \u03b1\u03b2 (s\u221e(\u03c4, \u03b3)\u2212 st) ( s\u22c6 + \u03c32 + \u03c4\nmin + 1\n)\u22121) st.\nHence,\ns\u221e(\u03c4t, \u03b3t)\u2212 st+1 = (s\u221e(\u03c4t, \u03b3t)\u2212 st) ( 1\u2212 \u03b1\u03b2 ( s\u22c6 + \u03c32 + \u03c4\nmin + 1\n)\u22121) ,\nand in each iteration st takes a step towards s\u221e(\u03c4t, \u03b3t). By assumptions s0 \u2264 s\u221e(\u03c40, \u03b30) and as 1\n\u03b1\u03b2 \u2265 ( s\u22c6 + \u03c32 + \u03c4t min + 1 ) ,\nfor all t, st+1 never overshoots s\u221e(\u03c4t, \u03b3t), i.e.,\nst \u2264 st+1 \u2264 s\u221e(\u03c4t, \u03b3t) \u2264 s\u221e(\u03c4t+1, \u03b3t+1). Therefore, st is an increasing sequence bounded above by s\u221e(\u03c4\u221e, \u03b3\u221e) and by invoking the monotone convergence theorem, st is convergent. Assume that st convergences to a s\u2032\u221e < s\u221e(\u03c4\u221e, \u03b3\u221e). Then, there exist a t\u03f5 such that s\u221e(\u03c4t\u03f5 , \u03b3t\u03f5) > s \u2032 \u221e + \u03f5. By analysing the sequence,\ns\u2032t\u03f5 = st\u03f5 , s \u2032 t\u03f5+s = g(s \u2032 t\u03f5+s\u22121, s\u22c6, \u03c4t\u03f5 , \u03b3t\u03f5),\nit is easy to show that\nst\u03f5+s \u2265 s\u2032t\u03f5+s, and lims\u2192\u221e s \u2032 t\u03f5+s = s\u221e(\u03c4t\u03f5 , \u03b3t\u03f5) > s \u2032 \u221e,\nwhich leads to a contradiction. Hence, limt\u2192\u221e st = s\u221e(\u03c4\u221e, \u03b3\u221e).\nRemark 2. Assume the setup of Lemma 10 and that the sequences \u03c4t and \u03b3t converge to 0. Then, as t\u2192\u221e, \u039bt convergences to \u039b\u22c6,\nlim t\u2192\u221e \u039bt = \u039b\u22c6."
        },
        {
            "heading": "C.3 SEQUENCE OF BOUNDS",
            "text": "Lemma 11 constructs a sequence of matrices \u039bUt that upper bounds iterates of \u039bt. The idea is to use the monotonicity property described in Lemma 9, together with the upper bound in Lemma 6, to control \u039bt from above. Lemma 10 with Remark 2 then allow to conclude limt\u2192\u221e \u039bUt = \u039b\u22c6. For this purpose, Lemma 11 assume a sufficiently small initialisation that leads to a dynamics where \u2225Bt\u22252 \u2264 \u03b1\u22121/2 and \u2225wt\u22252, \u2225Dt\u22252 are monotonically decreasing. In a similar spirit, Lemma 12 construct a sequence of lower bound matrices \u039bLt given that it is possible to select two scalar sequences \u03c4t and \u03b3t. At each step, the lower bounds \u039bLt takes a step towards \u039b\u22c6(\u03c4t, \u03b3t) described by Remark 1. For ensuring that \u039bt does not decay, the sequences \u03c4t and \u03b3t are chosen to be non-increasing, which results in increasing \u039b\u22c6(\u03c4t, \u03b3t) and \u039bLt . In the limit t \u2192 \u221e, \u039bLt convergences to the fixed-point \u039b\u22c6(limt\u2192\u221e \u03c4t, limt\u2192\u221e \u03b3t), which serves as the asymptotic lower bound. Finally, Corollary 3 shows that it is possible to construct these sequences with the limit 0 under some conditions.\nLemma 11. Assume that B0 and w0 are initialized such that\n\u2225B0\u222522 \u2aaf c1 \u03b1\n1\nmin + 1 , \u2225w0\u222522 \u2264 \u03b1c2,\nfor constants 0 < c1 < 1, 0 < c2 and \u03b1, \u03b2 satisfy the following conditions:\n1. 1\n\u03b1\u03b2 \u2265 max\n( c2 min + 2\nmin +\n1\nmin\n( (min + 1)\u2225\u03a3\u22c6\u22252 + \u03c32 ) , 2\u03c32\nmin\n) ,\n2. 1\n\u03b1\u03b2 \u2265 2\n( c2 min + 1\nmin +\nc1 ( c2 + \u03c3 2 )\n2min(min + 1)\n) ,\n3. 1\n\u03b1\u03b2 \u2265 5(\u2225\u03a3\u22c6\u22252 +\n\u03c32\nmin + 1 ),\n4. \u03b2 \u2264 \u03b1."
        },
        {
            "heading": "The series \u039bUt defined recursively as",
            "text": "\u039bU0 := \u2225\u039b0\u22252Ik, \u039bUt+1 := \u2225(Ik + \u03b1\u03b2R(\u039bUt ))\u039bUt (Ik + \u03b1\u03b2R(\u039bUt ))\u22252Ik,\nupper bounds the iterates \u039bt, i.e., for all t, \u039bUt \u2ab0 \u039bt. Moreover, \u039b\u22c6 \u2ab0 \u039bUt for all t. Proof. The result follows by induction. It is easy to check that the given assumptions satisfy the conditions of Lemmas 2 and 9 for all time steps. Assume that for time t, the following assumptions hold.\n1. \u2225DsD\u22a4s \u22252 is a non-increasing sequence for s \u2264 t. 2. \u2225ws\u22252 is a non-increasing sequence for s \u2264 t. 3. \u039bs \u2aaf \u039bUs \u2aaf \u039b\u22c6 for all s \u2264 t.\nThen, for time t+ 1, the following conditions holds:\n1. By using \u039bt \u2aaf \u039b\u22c6 \u2aaf 1\u03b1 min(min+1) and DtD \u22a4 t \u2aaf B0B\u22a40 \u2aaf c1\u03b1 1min+1 ,\nBtB \u22a4 t \u2aaf\n1\n\u03b1 min + c1 min + 1 , \u2225Bt\u22252 \u2264 1 \u03b1 .\nTherefore, by Lemma 1, and Lemma 2,\n\u2225wt+1\u22252 \u2264 \u2225wt\u22252, \u2225Dt+1D\u22a4t+1\u22252 \u2264 \u2225DtD\u22a4t \u22252.\n2. By applying Lemma 4,\n\u2225Ut\u22252 \u2264 \u03b1 ( c2 min + 1\nmin +\nc1 ( c2 + \u03c3 2 )\n2min(min + 1)\n) \u2264 1\n2\u03b2 .\nTherefore, by Lemma 6,\n\u039bt+1 \u2aaf (Ik + \u03b1\u03b2Rt)\u039bt(Ik + \u03b1\u03b2Rt)\u22a4.\n3. By applying Lemma 9 with \u039b := \u039bUt and \u039b \u2032 := \u039bt,\n(Ik + \u03b1\u03b2Rt)\u039bt(Ik + \u03b1\u03b2Rt) \u22a4 = f(\u039bt; 0, 0) \u2aaf f(\u039bUt ; 0, 0) \u2aaf \u039bUt+1.\n4. By applying Lemma 9 with \u039b := \u039b\u22c6 and \u039b\u2032 := \u039bUt ,\nf(\u039bUt ; 0, 0) \u2aaf f(\u039b\u22c6; 0, 0) = \u039b\u22c6. Therefore, \u039bUt+1 \u2aaf \u2225\u039b\u22c6\u22252Ik = \u039b\u22c6.\n5. Combining all the results,\n\u039bt+1 \u2aaf \u039bUt+1 \u2aaf \u039b\u22c6.\nLemma 12. Let \u03c4t and \u03b3t be non-increasing scalar sequences such that\n\u2225Btwt\u222522 \u2264 \u03c4t, \u2225Ut\u22252 \u2264 \u03b1\u03b3t \u2264 1\n2\u03b2 ,\nand \u03c40 \u2264 c2, \u03b30 < \u03bbmin(\u03a3\u22c6). Assume that all the assumptions of Lemma 11 hold with constants c1 and c2. and \u03b1, \u03b2 satisfy the following extra conditions\n1\n\u03b1\u03b2 \u2265 5(\u2225\u03a3\u22c6\u22252 +\nc2 + \u03c3 2 min + 1 ) + \u03bbmin(\u03a3\u22c6)."
        },
        {
            "heading": "Then, the series \u039bLt defined as follows",
            "text": "\u039bL0 = min (\u03bbmin (\u039b0) , \u03bbmin (\u039b\u22c6 (\u03c40, \u03b30))) Ik,\n\u039bLt+1 = \u03bbmin ( (Ik + \u03b1\u03b2R(\u039b L t , \u03c4t)\u2212 \u03b1\u03b2\u03b3tIk)\u039bLt (Ik + \u03b1\u03b2R(\u039bLt , \u03c4t)\u2212 \u03b1\u03b2\u03b3tIk)\u22a4 ) Ik,\nlower bounds the iterates \u039bt, i.e., for all t, \u039bLt \u2aaf \u039bt. Moreover, \u039bLt \u2aaf \u039bLt+1 for all t. Proof. The result follows by induction. It is easy to check that given assumptions satisfy the conditions of Lemmas 2 and 9 for all time steps. Suppose that for all time s \u2264 t,\n\u039bLs \u2aaf \u039bs \u2aaf \u039b\u22c6, \u039bLs \u2aaf \u039b\u22c6(\u03c4t, \u03b3t). Then, for time t+ 1, the following conditions hold:\n1. By Lemma 7,\n\u039bt+1 \u2ab0 (Ik + \u03b1\u03b2Rt(\u039bt)\u2212 \u03b1\u03b2\u03b3tIk)\u039bt(Ik + \u03b1\u03b2Rt(\u039bt)\u2212 \u03b1\u03b2\u03b3tIk)\u22a4.\n2. By Lemma 9,\n(Ik + \u03b1\u03b2Rt(\u039bt)\u2212 \u03b1\u03b2\u03b3tIk)\u039bt(Ik + \u03b1\u03b2Rt(\u039bt)\u2212 \u03b1\u03b2\u03b3tIk)\u22a4\n\u2ab0 (Ik + \u03b1\u03b2Rt(\u039bLt )\u2212 \u03b1\u03b2\u03b3tIk)\u039bLt (Ik + \u03b1\u03b2Rt(\u039bLt )\u2212 \u03b1\u03b2\u03b3tIk)\u22a4.\n3. Using commutativity of \u03a3\u22c6 and \u039bLt ,\n(Ik + \u03b1\u03b2Rt(\u039b L t )\u2212 \u03b1\u03b2\u03b3tIk)\u039bLt (Ik + \u03b1\u03b2Rt(\u039bLt )\u2212 \u03b1\u03b2\u03b3tIk)\u22a4\n\u2ab0 (Ik + \u03b1\u03b2R(\u039bLt , \u03c4t)\u2212 \u03b1\u03b2\u03b3tIk)\u039bLt (Ik + \u03b1\u03b2R(\u039bLt , \u03c4t)\u2212 \u03b1\u03b2\u03b3tIk)\u22a4\n\u2ab0 \u039bLt+1.\n4. By Corollary 2, \u039bLt \u2aaf \u039bLt+1 \u2aaf \u039b\u22c6(\u03c4t, \u03b3t).\nAs \u03c4t+1 \u2264 \u03c4t and \u03b3t+1 \u2264 \u03b3t, \u039bLt+1 \u2aaf \u039b\u22c6(\u03c4t, \u03b3t) \u2aaf \u039b\u22c6(\u03c4t+1, \u03b3t+1).\n5. Combining all the results,\n\u039bLt+1 \u2aaf \u039bt+1, \u039bLt+1 \u2aaf \u039b\u22c6(\u03c4t, \u03b3t) \u2aaf \u039b\u22c6(\u03c4t+1, \u03b3t+1).\nRemark 3. The condition on \u03b3t in Lemma 12 can be relaxed by the condition used in Lemma 8.\nCorollary 3. Assume that Lemma 12 holds with constants c1, c2, and constant sequences\n\u03c4 := c2, \u03b3 := ( c2 min + 1\nmin +\nc1 ( c2 + \u03c3 2 )\n2min(min + 1)\n) < \u03bbmin(\u03a3\u22c6).\nFurthermore, suppose \u03b1, \u03b2 satisfy the following extra properties, 1\n\u03b12 \u2265 4\u2225\u03a3\u22c6\u22252,\n( 1\u2212 \u03b24\u03b1 )\n\u03b1\u03b2 \u2265 \u03c3\n2\nmin + 1 + c2\nmin + 2 min + 1 + min (min + 1)2 .\nLet Ct = \u03a8tSt\u0393\u22a4t be the (thin) SVD decomposition of Ct. Then, there exist non-increasing scalar sequences \u03c4t and \u03b3t such that\n\u2225Btwt\u222522 \u2264 \u03c4t \u2264 c2, \u2225\u0393tUt\u0393\u22a4t \u22252 \u2264 \u03b1\u03b3t \u2264 1\n2\u03b2 ,\nwith the limit lim t\u2192\u221e \u03c4t = 0, lim t\u2192\u221e \u03b3t = 0.\nProof. All the assumptions of Corollary 1 are satisfied with constant c := c2. Hence, lim t\u2192\u221e \u2225Dt\u22252 = 0, lim t\u2192\u221e \u2225Btwt\u22252 = 0. (29)\nMoreover, the sequence \u2225Btwt\u22252 is upper bounded above, \u2225Btwt\u22252 \u2264 \u2225Bt\u22252\u2225wt\u22252 \u2264 c2.\nTake any sequence 0 \u2264 \u03c4 \u2032t \u2264 c2 that monotonically decays to 0. Set \u03c40 = \u03c4 \u20320 and st = 0. Recursively define \u03c4t as follows: for each t > 0, find the smallest st such that \u2225Bsws\u22252 \u2264 \u03c4 \u2032t , for all s \u2265 st. Then, set \u03c4st = \u03c4 \u2032t and for all st\u22121 \u2264 s < st, set \u03c4s = \u03c4 \u2032t\u22121. It is easy to check that this procedure yields a non-increasing scalar sequence \u03c4t with the desired limit.\nBy Lemma 12 with \u03b3t := \u03b3, \u039bt is non-decaying, and its lowest eigenvalue is bounded from below. Using the limits in Equation (29),\nlim t\u2192\u221e\nCtUtC \u22a4 t = 0,\nwhich implies that limt\u2192\u221e \u2225\u0393tUt\u0393\u22a4t \u22252 = 0. A similar argument yields a non-increasing scalar sequence \u03b3t with the desired limit."
        },
        {
            "heading": "C.4 PROOF OF THEOREM 2",
            "text": "By Lemma 11, \u039bt \u2aaf \u039bUt \u2aaf \u039b\u22c6 and \u2225Dt+1\u22252 \u2264 \u2225Dt\u22252 for all t. Using the initialisation condition,\n\u2225Bt\u222522 = \u2225Ct\u222522 + \u2225Dt\u222522 \u2264 \u2225\u039bt\u22252 + \u2225D0\u222522 \u2264 \u2225\u039b\u22c6\u22252 + \u2225B0\u222522 \u2264 1\n\u03b1 .\nNow, the conditions of Corollary 1 are satisfied with c := c2. By Corollary 1,\nlim t\u2192\u221e Dt = 0, lim t\u2192\u221e Btwt = 0.\nMoreover, by Corollary 3, there exist non-increasing sequences \u03c4t and \u03b3t that are decaying. By Lemma 12 with these sequences yield \u039bLt \u2aaf \u039bt, for all t. Finally, by Lemma 10,\nlim t\u2192\u221e \u039bLt \u2192 \u039b\u22c6 and lim t\u2192\u221e \u039bUt \u2192 \u039b\u22c6, which concludes Theorem 2."
        },
        {
            "heading": "D PROOF OF PROPOSITION 1",
            "text": "Proposition 2 below gives a more complete version of Proposition 1, stating an upper bound holding with probability at least 1\u2212 \u03b4 for any \u03b4 > 0. Proposition 2. Let B\u0302, wtest satisfy Equations (9) and (11) for a new task defined by Equation (10). For any \u03b4 > 0 with probability at least 1\u2212 \u03b4,\n\u2225B\u0302wtest \u2212B\u22c6w\u22c6\u22252 = O ( 1 + \u03c3 2 /\u03bbmin(\u03a3\u22c6)\nmin \u2225w\u22c6\u2225+max\n  \u221a k + \u221a log( 4\u03b4 )\u221a\nmtest , k + log( 4\u03b4 ) mtest\n  \u2225w\u22c6\u2225\n+ \u03c3\n\u221a k\nmtest\n 1 + \u221a log( 4\u03b4 )\nk\n   1 + \u221a log( 4\u03b4 )\nmtest\n  ) ,\nwhere we recall \u03c32 = Tr(\u03a3\u22c6) + \u03c32.\nUsing Equation (11), it comes\nB\u0302wtest \u2212B\u22c6w\u22c6 = ( \u03b1B\u0302B\u0302\u22a4\u03a3testB\u22c6 \u2212B\u22c6 ) w\u22c6 + \u03b1\nmtest B\u0302B\u0302\u22a4X\u22a4z\n= B\u22c6 (\u03b1\u039b\u22c6 \u2212 Ik)w\u22c6 \ufe38 \ufe37\ufe37 \ufe38\n(A)\n+\u03b1B\u22c6\u039b\u22c6 ( B\u22a4\u22c6 \u03a3testB\u22c6 \u2212 Ik ) w\u22c6\n\ufe38 \ufe37\ufe37 \ufe38 (B)\n+ \u03b1\nmtest B\u22c6\u039b\u22c6B\n\u22a4 \u22c6 X \u22a4z \ufe38 \ufe37\ufe37 \ufe38\n(C)\n.\nThe rest of the proof aims at individually bounding the norms of the terms (A), (B) and (C). First note that by definition of \u039b\u22c6,\n\u03b1\u039b\u22c6 \u2212 Ik = \u2212 1\nmin + 1 Ik \u2212\nmin\u03c3 2\n(min + 1)2\n[ \u03a3\u22c6 + \u03c32\nmin + 1 Ik\n]\u22121 .\nThis directly implies that\n\u2225\u03b1\u039b\u22c6 \u2212 Ik\u22252 = 1\nmin + 1 +\nmin\u03c3 2\n(min + 1)2 \u00b7 1 \u03bbmin(\u03a3\u22c6) +\n\u03c32 min+1\n\u2264 1 + \u03c3\n2\n\u03bbmin(\u03a3\u22c6)+\u03c32/min min + 1 . (30)\nMoreover, the concentration inequalities of Lemmas 13 and 14 claim that with probability at least 1\u2212 \u03b4:\n\u2225B\u22a4\u22c6 \u03a3testB\u22c6 \u2212 Ik\u22252 \u2264 3max   \u221a k + \u221a 2 log(4\u03b4 )\u221a\nmtest ,\n(\u221a k + \u221a 2 log(4\u03b4 ) )2\nmtest\n  ,\n\u2225B\u22a4\u22c6 X\u22a4z\u22252 \u2264 16\u03c3 \u221a mtestk (1 +\n\u221a log( 4\u03b4 )\n2k )(1 +\n\u221a log( 4\u03b4 )\n2mtest ).\nThese two bounds along with Equation (30) then allow to bound the terms (A), (B) and (C) as follows\n\u2225(A)\u22252 \u2264 1 + \u03c3\n2\n\u03bbmin(\u03a3\u22c6)+\u03c32/min min + 1 \u2225w\u22c6\u2225\n\u2225(B)\u22252 \u2264 3  1\u2212 1 + \u03c3 2\n\u2225\u03a3\u22c6\u22252+\u03c32/min min + 1\n max   \u221a k + \u221a 2 log(4\u03b4 )\u221a\nmtest ,\n(\u221a k + \u221a 2 log(4\u03b4 ) )2\nmtest\n  \u2225w\u22c6\u2225\n\u2225(C)\u22252 \u2264 16  1\u2212 1 + \u03c3 2\n\u2225\u03a3\u22c6\u22252+\u03c32/min min + 1\n \u03c3 \u221a k\nmtest (1 +\n\u221a log( 4\u03b4 )\n2k )(1 +\n\u221a log( 4\u03b4 )\n2mtest ),\nwhere we used in the two last bounds that \u03b1\u2225\u039b\u22c6\u22252 \u2264 1 \u2212 1+ \u03c32 \u2225\u03a3\u22c6\u22252+\u03c32/min min+1 . Summing these three bounds finally yields Proposition 2, and Proposition 1 with the particular choice \u03b4 = 4e\u2212 k 2 .\nLemma 13. For any \u03b4 > 0, with probability at least 1\u2212 \u03b42 ,\n\u2225B\u22a4\u22c6 \u03a3testB\u22c6 \u2212 Ik\u22252 \u2264 3max   \u221a k + \u221a 2 log(4\u03b4 )\u221a\nmtest ,\n(\u221a k + \u221a 2 log(4\u03b4 ) )2\nmtest\n \nand \u2225B\u22a4\u22c6 X\u22a4\u22252 \u2264 \u221a mtest ( 1 + \u221a k+ \u221a 2 log( 14\u03b4 )\u221a mtest ) .\nProof. Note that B\u22a4\u22c6 X\u22a4 is a matrix in Rk\u00d7mtest whose entries are independent standard Gaussian variables. From there, applying Corollary 5.35 and Lemma 5.36 from Vershynin (2010) with t =\u221a 2 log( 14\u03b4 ) directly leads to Lemma 13.\nLemma 14.\nP  \u2225B\u22a4\u22c6 X\u22a4z\u22252 \u2265 16\u03c3 \u221a mtestk (1 + \u221a log( 4\u03b4 )\n2k )(1 +\n\u221a log( 4\u03b4 )\n2mtest )\n  \u2264 \u03b4\n2 .\nProof. Let A = B\u22a4\u22c6 X\u22a4 in this proof. Recall that A has independent entries following a standard normal distribution. A and z are independent, which implies that A z\u2225z\u2225 \u223c N (0, Ik). Typical bounds on Gaussian variables then give (see e.g. Rigollet & Hu\u0308tter, 2017, Remark 2.2.2)\nP  \u2225Az\u2225 \u2225z\u2225 \u2265 4 \u221a k (1 + \u221a log( 4\u03b4 ) 2k )   \u2264 \u03b4 4 .\nA similar bound holds on the \u03c3 sub-Gaussian vector z, which is of dimension mtest:\nP  \u2225z\u2225 \u2265 4\u221amtest (1 + \u221a log( 4\u03b4 )\n2mtest )\n  \u2264 e\u2212mtest2 .\nCombining these two bounds then yields Lemma 14."
        },
        {
            "heading": "E TECHNICAL LEMMAS",
            "text": "Lemma 15. Let \u03a3 = 1nX\n\u22a4X where X \u2208 Rn\u00d7d is such that each row is composed of i.i.d. samples x \u223c N(0, Id). For any unit vector v,\nE[\u03a3vv\u22a4\u03a3] = 1\nn Id +\nn+ 1\nn vv\u22a4.\nProof. Let x, x\u2032 \u223c N(0, Id). By expanding covariance \u03a3 and i.i.d. assumption,\nE[\u03a3vv\u22a4\u03a3] = 1 n E [ \u27e8x, v\u27e92xx\u22a4 ] \ufe38 \ufe37\ufe37 \ufe38\n(A)\n+ n\u2212 1 n E [ \u27e8x, v\u27e9\u27e8x\u2032, v\u27e9xx\u2032\u22a4 ] \ufe38 \ufe37\ufe37 \ufe38\n(B)\n.\nFor the term (A),\nE [ \u27e8x, v\u27e92xx\u22a4 ] jk = E\n[ ( d\u2211\ni=1\nxivi) 2xjxk ] .\nAny term with an odd-order power cancels out as the data is symmetric around the origin, and\nE [ \u27e8x, v\u27e92xx\u22a4 ] = 2vv\u22a4 + Id,\nby the following computations,\nE [ \u27e8x, v\u27e92xx\u22a4 ] jj = v2jE[x4j ] + \u2211\ni \u0338=j v2i E[x2ix2j ] = 3v2j +\n\u2211 i \u0338=j v2i = 2v 2 j + 1,\nE [ \u27e8x, v\u27e92xx\u22a4 ] jk = 2vjvkE[x2jx2k] = 2vjvk.\nFor the term (B), by i.i.d. assumption,\nE [ \u27e8x, v\u27e9\u27e8x\u2032, v\u27e9xx\u2032\u22a4 ] = E[\u27e8x, v\u27e9x]E[\u27e8x, v\u27e9x]\u22a4.\nWith a similar argument, it is easy to see\nE[\u27e8x, v\u27e9x]i = E[x2i vi] = vi, and E[\u27e8x, v\u27e9x] = v. Combining the two terms yields Lemma 15.\nLemma 16. Let A and B be positive semi-definite symmetric matrices of shape k \u00d7 k and AB = BA. Then,\nAB \u2aaf \u2225A\u22252B.\nProof. As A and B are normal matrices that commute, there exist an orthogonal Q such that A = Q\u039bAQ \u22a4 and B = Q\u039bBQ\u22a4 where \u039bA and \u039bB are diagonal. Then,\nAB = Q\u039bA\u039bBQ \u22a4 \u2aaf \u2225A\u2225Q\u039bBQ\u22a4,\nas for any vector v \u2208 Rk,\nv\u22a4ABv = k\u2211\ni=1\n(\u039bA)ii(\u039bB)ii(Qvi) 2 \u2264 \u2225A\u22252\nk\u2211\ni=1\n(\u039bB)ii(Qvi) 2 = \u2225A\u22252B.\nLemma 17. Let A and B be positive semi-definite symmetric matrices of shape k \u00d7 k such that AB = BA and A \u2aaf B. Then, for any k \u2208 N,\nAk \u2aaf Bk. (31)\nProof. As A and B are normal matrices that commute, there exist an orthogonal Q such that A = Q\u039bAQ \u22a4 and B = Q\u039bBQ\u22a4 where \u039bA and \u039bB are diagonal. Then,\nBk \u2212Ak = Q(\u039bkB \u2212 \u039bkA)Q\u22a4 \u2ab0 0, as B \u2ab0 A implies \u039bB \u2ab0 \u039bA."
        },
        {
            "heading": "F FIXED POINTS CHARACTERIZED BY THEOREM 1 ARE GLOBAL MINIMA",
            "text": "The ANIL loss with m samples in the inner loop reads,\nLANIL(B,w;m) = 1\n2 Ew\u22c6,i,Xi,yi\n[ \u2225Bw\u0303(w;Xi, yi)\u2212B\u22c6w\u22c6,i\u22252 ] , (32)\nwhere is the updated head after a step of gradient descent, i.e.,\nw\u0303(w;Xi, yi) := ( w \u2212 \u03b1\nm B\u22a4X\u22a4i (XiBw \u2212 yi)\n) . (33)\nWhenever the context is clear, we will write w\u0303 or w\u0303(w) instead of w\u0303(w;Xi, yi) for brevity. Theorem 1 proves that minimising objective in Equation (32) with FO-ANIL algorithm asymptotically convergences to a set of fixed points, under some conditions. In Proposition 3, we show that these points are global minima of the Equation (32).\nProposition 3. Fix any (B\u0302, w\u0302) that satisfy the three limiting conditions of Theorem 1,\nB\u22a4\u22c6,\u22a5B\u0302 = 0,\nB\u0302w\u0302 = 0,\nB\u22a4\u22c6 B\u0302B\u0302 \u22a4B\u22c6 = \u039b\u22c6.\nThen, (B\u0302, w\u0302) is the minimiser of the Equation (32), i.e.,\n(B\u0302, w\u0302) \u2208 argmin B,w LANIL(B,w;min).\nProof. The strategy of proof is to iteratively show that modifying points to satisfy these three limits reduce the ANIL loss. Lemmas 18 to 20 demonstrates how to modify each point such that the resulting point obeys a particular limit and has better generalisation.\nFor any (B,w), define the following points,\n(B1, w1) = ( B \u2212B\u22a4\u22c6,\u22a5B\u22a4\u22c6,\u22a5B,w ) , (B2, w2) = ( B1, w1 \u2212B\u22a41 ( B1B \u22a4 1 )\u22121 B1w1 ) .\nThen, Lemmas 18 to 20 show that\nLANIL(B,w;min) \u2265 LANIL(B1, w1;min) \u2265 LANIL(B2, w2;min) \u2265 LANIL(B\u0302, w\u0302;min). Since (B,w) is arbitrary,\n(B\u0302, w\u0302) \u2208 argmin B,w LANIL(B,w;min).\nLemma 18. Consider any parameters (B,w) \u2208 Rd\u00d7k\u2032 \u00d7 Rk\u2032 . Let B\u2032 = B \u2212 B\u22c6,\u22a5B\u22a4\u22c6,\u22a5B. Then, for any m > 0, we have\nLANIL(B,w;m) \u2265 LANIL(B\u2032, w;m).\nProof. Decomposing the loss into two orthogonal terms yields the desired result,\nLANIL(B,w;m) = 1\n2 Ew\u22c6,i,Xi,yi\n[\u2225\u2225B\u22a4\u22c6 Bw\u0303 \u2212 w\u22c6,i \u2225\u22252 ] + 1\n2 EXi,yi\n[\u2225\u2225B\u22a4\u22c6,\u22a5Bw\u0303 \u2225\u22252 ]\n\u2264 1 2 Ew\u22c6,i,Xi,yi\n[\u2225\u2225B\u22a4\u22c6 Bw\u0303 \u2212 w\u22c6,i \u2225\u22252 ]\n= LANIL(B\u2032, w;m).\nLemma 19. Consider any parameters (B,w) \u2208 Rd\u00d7k\u2032 \u00d7 Rk\u2032 such that B\u22a4\u22c6,\u22a5B = 0. Let w\u2032 = w \u2212B\u22a4 ( BB\u22a4 )\u22121 Bw. Then, for any m > 0, we have\nLANIL(B,w;m) \u2265 LANIL(B,w\u2032;m),\nProof. Expanding the square,\nLANIL(B,w;m)\u2212 LANIL(B,w\u2032;m) = 1\n2 Ew\u22c6,i,Xi,yi\n[\u2225\u2225B\u22a4\u22c6 Bw\u0303(w)\u2212 w\u22c6,i \u2225\u22252 \u2212 \u2225\u2225B\u22a4\u22c6 Bw\u0303(w\u2032)\u2212 w\u22c6,i \u2225\u22252 ]\n= 1\n2 Ew\u22c6,i,Xi,yi\n[ \u2225Bw\u0303(w)\u22252 \u2212 \u2225Bw\u0303(w\u2032)\u22252 ]\n\ufe38 \ufe37\ufe37 \ufe38 (A)\n\u2212 Ew\u22c6,i,Xi,yi [\u27e8B\u22c6w\u22c6,i, Bw\u0303(w)\u2212Bw\u0303(w\u2032)\u27e9] \ufe38 \ufe37\ufe37 \ufe38\n(B)\n.\nFirst, expanding w\u0303(w) and w\u0303(w\u2032) by Equation (33),\nBw\u0303(w) = ( Id \u2212 \u03b1\nm BB\u22a4X\u22a4i Xi\n) Bw + \u03b1\nm BB\u22a4X\u22a4i yi, Bw\u0303(w \u2032) = \u03b1 m BB\u22a4X\u22a4i yi.\nFor the first term, (A) = EXi [\u2225\u2225\u2225 ( Id \u2212 \u03b1\nm BB\u22a4X\u22a4i Xi\n) Bw \u2225\u2225\u2225 2 ] + 2\u03b1\nm Ew\u22c6,i,Xi\n[\u2329( Id \u2212 \u03b1\nm BB\u22a4X\u22a4i Xi\n) Bw,BB\u22a4X\u22a4i XiB\u22c6w\u22c6 \u232a]\n= EXi [\u2225\u2225\u2225 ( Id \u2212 \u03b1\nm BB\u22a4X\u22a4i Xi\n) Bw \u2225\u2225\u2225 2 ] \u2265 0,\nwhere we have used that the tasks and the noise are centered around 0. For the second term,\n(B) = \u2329 Ew\u22c6,i [B\u22c6w\u22c6,i] ,EXi [( Id \u2212 \u03b1\nmin BB\u22a4X\u22a4i Xi\n) Bw ]\u232a = 0,\nwhere we have again used that the tasks are centered around 0. Putting two results together yields Lemma 19.\nLemma 20. Consider any parameters (B,w) \u2208 Rd\u00d7k\u2032 \u00d7 Rk\u2032 such that B\u22a4\u22c6,\u22a5B = 0, Bw = 0. Let (B\u2032, w\u2032) \u2208 Rd\u00d7k\u2032 \u00d7 Rk\u2032 such that B\u22a4\u22c6,\u22a5B\u2032 = 0, B\u2032w\u2032 = 0 and B\u22a4\u22c6 B\u2032B\u2032\u22a4B\u22c6 = \u039b\u22c6. Then, we have\nLANIL(B,w;min) \u2265 LANIL(B\u2032, w\u2032;min).\nProof. Let \u039b := B\u22a4\u22c6 BB\u22a4B\u22c6 in this proof. Using B\u22a4\u22c6,\u22a5B = 0, we have\nLANIL(B,w;min) = 1\n2 Ew\u22c6,i,Xi,yi\n[\u2225\u2225B\u22a4\u22c6 Bw\u0303 \u2212 w\u22c6,i \u2225\u22252 ] .\nPlugging in the definition of w\u0303,\nLANIL(B,w;min) = \u03b12\n2\n1\nm2in Ew\u22c6,i,Xi,yi\n[\u2225\u2225B\u22a4\u22c6 BB\u22a4X\u22a4i yi \u2225\u22252 ]\n\ufe38 \ufe37\ufe37 \ufe38 (A)\n\u2212 \u03b1 1 min\nEw\u22c6,i,Xi,yi [ \u27e8w\u22c6,i, B\u22a4\u22c6 BB\u22a4X\u22a4i yi\u27e9 ] \ufe38 \ufe37\ufe37 \ufe38 (B) + 1 2 tr (\u03a3\u22c6) .\nUsing that the label noise is centered,\n(A) = Ew\u22c6,i,Xi [\u2225\u2225B\u22a4\u22c6 BB\u22a4\u03a3iB\u22c6w\u22c6 \u2225\u22252 ]\n\ufe38 \ufe37\ufe37 \ufe38 (C)\n+ EXi,zi [\u2225\u2225B\u22a4\u22c6 BB\u22a4X\u22a4i zi \u2225\u22252 ]\n\ufe38 \ufe37\ufe37 \ufe38 (D)\n,\nwhere \u03a3i := 1minX \u22a4 i Xi. By the independence of w\u22c6,i, Xi and Lemma 15,\n(C) = tr ( B\u22a4\u22c6 BB \u22a4Ew\u22c6,i,Xi [ \u03a3iB\u22c6w\u22c6w \u22a4 \u22c6 B \u22a4 \u22c6 \u03a3i ] BB\u22a4B\u22c6 )\n= tr ( B\u22a4\u22c6 BB \u22a4EXi [ \u03a3iB\u22c6\u03a3\u22c6B \u22a4 \u22c6 \u03a3i ] BB\u22a4B\u22c6 )\n= min + 1 min tr ( B\u22a4\u22c6 BB \u22a4B\u22c6\u03a3\u22c6B \u22a4 \u22c6 BB \u22a4B\u22c6 ) + 1 min tr ( B\u22a4\u22c6 BB \u22a4BB\u22a4B\u22c6 ) = min + 1\nmin tr (\u039b\u03a3\u22c6\u039b) +\n1\nmin tr (\u03a3\u22c6) tr\n( \u039b2 ) .\nFor the term (D), we have\n(D) = 1 min tr ( B\u22a4\u22c6 BB \u22a4EXi,zi [ X\u22a4i ziz \u22a4 i Xi ] BB\u22a4B\u22c6 )\n= \u03c32tr ( B\u22a4\u22c6 BB \u22a4EXi [\u03a3i]BB\u22a4B\u22c6 ) = \u03c32tr ( B\u22a4\u22c6 BB \u22a4BB\u22a4B\u22c6 ) = \u03c32tr ( \u039b2 ) .\nLastly, for the term (B), we have\n(B) = 1\nmin Ew\u22c6,i,Xi\n[ \u27e8w\u22c6,i, B\u22a4\u22c6 BB\u22a4X\u22a4i XiB\u22c6w\u22c6,i\u27e9 ]\n= Ew\u22c6,i [ \u27e8w\u22c6,i, B\u22a4\u22c6 BB\u22a4B\u22c6w\u22c6,i\u27e9 ]\n= tr (\u039b\u03a3\u22c6) .\nPutting everything together using \u03a3\u22c6 is scaled identity,\nLANIL(B,w;min) = \u03b12\n2min\n( (min + 1) tr (\u039b\u03a3\u22c6\u039b) + tr (\u03a3\u22c6) tr ( \u039b2 )) \u2212 \u03b1tr (\u039b\u03a3\u22c6) + 1\n2 tr (\u03a3\u22c6)\n= \u03b12\n2min\n( (min + 1) \u2225\u03a3\u22c6\u22252 + \u03c32 ) tr ( \u039b2 ) \u2212 \u03b1\u2225\u03a3\u22c6\u22252tr(\u039b) + 1\n2 tr (\u03a3\u22c6) .\nHence, the loss depends on B only through \u039b := B\u22a4\u22c6 BB \u22a4B\u22c6 for all (B,w) such that B\u22a4\u22c6,\u22a5B = 0, Bw = 0. Taking the derivative w.r.t. \u039b yields that \u039b is a minimiser if and only if \u03b1\nmin\n( (min + 1) \u2225\u03a3\u22c6\u22252 + \u03c32 ) \u039b\u2212 \u03bbmax (\u03a3\u22c6) I = 0.\nThis quantity is minimised for \u039b\u22c6 as\n\u03b1 min + 1\nmin \u039b\u22c6\n( \u03a3\u22c6 + \u03c32\nmin + 1 \u039b\u22c6\n) = \u03a3\u22c6."
        },
        {
            "heading": "G EXTENDING COLLINS ET AL. (2022) ANALYSIS TO THE MISSPECIFIED SETTING",
            "text": "We show that the dynamics for infinite samples in the misspecified setting k < k\u2032 \u2264 d is reducible to a well-specified case studied in Collins et al. (2022). The idea is to show that the dynamics is restricted to a k-dimensional subspace via a time-independent bijection between misspecified and well-specified iterates.\nIn the infinite samples limit, min =\u221e,mout =\u221e, the outer loop updates of Equation (3) simplify with Assumption 1 to\nwt+1 = wt \u2212 \u03b2\u2206tB\u22a4t (Btwt \u2212B\u22c6\u00b5\u22c6) , Bt+1 = Bt \u2212 \u03b2Bt\u2206twt ( \u2206twt + \u03b1B \u22a4 t B\u22c6\u00b5\u22c6 )\u22a4\n+ \u03b2 ( Id \u2212 \u03b1BtB\u22a4t ) B\u22c6 ( \u00b5\u22c6 (\u2206twt) \u22a4 + \u03b1\u03a3\u22c6B \u22a4 \u22c6 Bt ) .\n(34)\nwhere \u00b5\u22c6 and \u03a3\u22c6 respectively are the empirical task mean and covariance, and \u2206t := Ik\u2032\u2212\u03b1B\u22a4t Bt. This leads to following updates on Ct := B\u22a4\u22c6 Bt,\nCt+1 = ( Ik + \u03b1\u03b2 ( Ik \u2212 CtC\u22a4t ) \u03a3\u22c6 ) Ct \u2212 \u03b2Ct\u2206twt ( \u2206twt + \u03b1C \u22a4 t \u00b5\u22c6 )\u22a4\n+ \u03b2 ( Ik \u2212 \u03b1CtC\u22a4t ) \u00b5\u22c6 (\u2206twt) \u22a4 .\nA key observation of this recursion is that all the terms end with Ct or \u2206t. This observation is sufficient to deduce that Ct is fixed in its row space.\nAssume that B0 is initialised such that ker(C0) \u2286 ker(\u22060). This condition is always satisfiable by a choice of B0 that guarantees B\u22a40 B0 = \u03b1Ik\u2032 , similarly to Collins et al. (2022). With this assumption, there is no dynamics in the kernel space of C0. More precisely, we show that for all time t, ker(C0) \u2286 ker(Ct) \u2229 ker(\u2206t). Then, it is easy to conclude that Bt has simplified rank-deficient dynamics.\nAssume the following inductive hypothesis at time t, ker(C0) \u2286 ker(Ct) \u2229 ker(\u2206t).\nFor time step t + 1, we have for all v \u2208 ker(Ct) \u2229 ker(\u2206t), Ct+1v = 0. As a result, the next step contains the kernel space of the previous step, i.e., ker(C0) \u2286 ker(Ct) \u2229 ker(\u2206t) \u2286 ker(Ct+1). Similarly, inspecting the expression for \u2206t+1, we have for all v \u2208 ker(Ct) \u2229 ker(\u2206t), \u2206t+1v = 0 and ker(C0) \u2286 ker(Ct) \u2229 ker(\u2206t) \u2286 ker(\u2206t+1). Therefore, the induction hypothesis at time step t+ 1 holds.\nNow, using that ker(Ct) = col(C\u22a4t ) \u22a5, row spaces of Ct are confined in the same k-dimensional subspace, col(C\u22a4t ) \u2287 col(C\u22a40 ). Let R \u2208 Rk\u00d7k \u2032 and R\u22a5 \u2208 R(k \u2032\u2212k)\u00d7k\u2032 be two orthogonal matrices that span col(C\u22a40 ) and col(C \u22a4 0 )\n\u22a5, respectively. That is, R and R\u22a5 satisfy RR\u22a4 = Ik, col(R) = col(C\u22a40 ) and R\u22a5R \u22a4 \u22a5 = Ik\u2032\u2212k, col(R\u22a5) = col(C \u22a4 0 )\n\u22a5. It is easy to show that updates to Bt and wt are orthogonal to col(R\u22a5), i.e.,\nBtR \u22a4 \u22a5 = B0R \u22a4 \u22a5, and R\u22a5wt = R\u22a5w0.\nWith this result, we can prove that there is a k-dimensional parametrisation of the misspecified dynamics. Let w\u03020 \u2208 Rk, B\u03020 \u2208 Rd\u00d7k defined as\nB\u03020 := B0R \u22a4, w\u03020 := Rw0.\nRunning FO-ANIL in the infinite samples limit, initialized with B\u03020 and w\u03020, mirrors the dynamics of the original misspecified iterations, i.e., B\u0302t and w\u0302t satisfy,\nB\u0302t = BtR \u22a4, w\u0302t = Rwt, B\u0302tw\u0302t = Btwt \u2212B0R\u22a4\u22a5R\u22a5w0.\nThis given bijection proves that iterates are fixed throughout training on the k\u2032 \u2212 k-dimensional subspace col(R\u22a5). Hence, as argued in Section 4, the infinite samples dynamics do not capture unlearning behaviour observed in Section 5. In contrast, the infinite tasks idealisation exhibits both learning and unlearning dynamics."
        },
        {
            "heading": "H CONVERGENCE RATE FOR UNLEARNING",
            "text": "In Proposition 4, we derive the rate \u2225B\u22a4\u22c6,\u22a5Bt\u22252 = O ( min\n\u03b12\u03b2\u03c32t\n) .\nProposition 4. Under the conditions of Theorem 2, \u2225\u2225B\u22a4\u22c6,\u22a5Bt \u2225\u22252 2 \u2264 1\n\u03b12\u03b2 \u03c3 2 min t+ 1\u2225B\u22a4\u22c6,\u22a5B0\u222522\n, (35)\nfor any time t \u2265 0. Proof. Recall that Lemma 2 holds for all time steps by Theorem 2. That is, for all t > 0,\n\u2225\u2225B\u22a4\u22c6,\u22a5Bt+1 \u2225\u22252 2 \u2264 ( 1\u2212 \u03ba \u2225\u2225B\u22a4\u22c6,\u22a5Bt \u2225\u22252 2 )\u2225\u2225B\u22a4\u22c6,\u22a5Bt \u2225\u22252 2 , (36)\nwhere \u03ba := \u03b1 2\u03b2\nmin \u03c32 for brevity. Now, assume the inductive hypothesis in Equation (35) holds for\ntime t. Observe that the function x 7\u2192 (1\u2212 \u03bax)x is increasing on [0, 12\u03ba ] and\n\u2225B\u22a4\u22c6,\u22a5Bt\u222522 \u2264 \u2225B\u22a4\u22c6,\u22a5B0\u222522 \u2264 1\n\u03b1\n1 min + 1 \u2264 1 2\u03ba ,\nby the assumptions of Theorem 2. Then, by Equation (36) and monotonicity of x 7\u2192 (1\u2212 \u03bax)x,\n\u2225B\u22a4\u22c6,\u22a5Bt+1\u222522 \u2264 ( 1\u2212 \u03ba\n\u03bat+ 1\u2225B\u22c6,\u22a5B0\u222522\n) 1\n\u03bat+ 1\u2225B\u22c6,\u22a5B0\u222522 = \u03ba (t\u2212 1) + 1\u2225B\u22a4\u22c6,\u22a5B0\u222522( \u03bat+ 1\u2225B\u22c6,\u22a5B0\u222522 )2 .\nUsing the inequality of arithmetic and geometric means,\n\u2225B\u22a4\u22c6,\u22a5Bt+1\u222522 \u2264 \u03ba (t\u2212 1) + 1\u2225B\u22a4\u22c6,\u22a5B0\u222522(\n\u03bat+ 1\u2225B\u22c6,\u22a5B0\u222522\n)2 \u00b7 \u03ba (t+ 1) + 1\u2225B\u22c6,\u22a5B0\u222522 \u03ba (t+ 1) + 1\u2225B\u22c6,\u22a5B0\u222522\n\u2264 1 \u03ba (t+ 1) + 1\u2225B\u22c6,\u22a5B0\u222522 .\nHence, the induction hypothesis at time step t+ 1 holds."
        },
        {
            "heading": "I ADDITIONAL MATERIAL ON EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "I.1 EXPERIMENTAL DETAILS",
            "text": "In the experiments considered in Section 5, samples are split into two subsets with min = 20 and mout = 10 for model-agnostic methods. The task parameters w\u22c6,i are drawn i.i.d. from N (0,\u03a3\u22c6), where \u03a3\u22c6 = cdiag(1, . . . , k) and c is a constant chosen so that \u2225\u03a3\u22c6\u2225F = \u221a k. Moreover, the features are drawn i.i.d. following a standard Gaussian distribution. All the curves are averaged over 10 training runs.\nModel-agnostic methods are all trained using step sizes \u03b1 = \u03b2 = 0.025. For the infinite tasks model, the iterates are computed using the close form formulas given by Equations (5) and (6) for min = 20. For the infinite samples model, it is computed using the closed form formula of Collins et al. (2022, Equation (3)) with N = 5000 tasks. The matrix B0 is initialised randomly as an orthogonal matrix such that B\u22a40 B0 = 1 4\u03b1Ik\u2032 . The vector w0 is initialised uniformly at random on the k\n\u2032-dimensional sphere with squared radius 0.01k\u2032\u03b1.\nFor training Burer-Monteiro method, we initialise B0 is initialised randomly as an orthogonal matrix such that B\u22a40 B0 = 1 100Ik\u2032 and each column of W is initialised uniformly at random on the k\n\u2032- dimensional sphere with squared radius 0.01k\u2032\u03b1. 2 Also, similarly to Tripuraneni et al. (2021), we add a 18\u2225B\u22a4t Bt \u2212WtW\u22a4t \u22252F regularising term to the training loss to ensure training stability. The matrices Bt and Wt are simultaneously trained with LBFGS using the default parameters of scipy.\nFor Table 1, we consider ridge regression for each learnt representation. For example, if we learnt the representation given by the matrix B\u0302 \u2208 Rd\u00d7k\u2032 , the Ridge estimator is given by\nargmin w\u2208Rk\u2032\nL\u0302test(B\u0302w;X, y) + \u03bb\u2225w\u222522.\nThe regularisation parameter \u03bb is tuned for each method using a grid search over multiple values."
        },
        {
            "heading": "I.2 GENERAL TASK DISTRIBUTIONS",
            "text": "In this section, we run similar experiments to Section 5, but with a more difficult task distribution and 3 training runs per method. In particular the task parameters are now generated as w\u22c6,i \u223c N (\u00b5\u22c6,\u03a3\u22c6), where \u00b5\u22c6 is chosen uniformly at random on the k-sphere of radius \u221a k. Also, \u03a3\u22c6 is\nchosen proportional to diag(e1, ..., ek), so that its Frobenius-norm is 2 \u221a k and its condition number is ek\u22121.\nSimilarly to Section 5, Figures 4 and 5 show the evolution of the squared singular values on the good subspace and its orthogonal component during the training. Similarly to the well-behaved case of Section 5, model-agnostic methods seem to correctly learn the good subspace and unlearn its orthogonal complement, still at a very slow rate. The main difference is that the matrix towards which B\u22a4\u22c6 BtB \u22a4 t B\u22c6 converges does not exactly correspond to the \u039b\u22c6 matrix defined in Theorem 1. We believe this is due to an additional term that should appear in the presence of a non-zero task mean. We yet do not fully understand what this term should be.\n2We choose a small initialisation regime for Burer-Monteiro to be in the good implicit bias regime. Note that Burer-Monteiro yields worse performance when using a larger initialisation scale.\nFigure 6 on the other hand shows the evolution of \u2225Btwt\u2212B\u22c6\u00b5\u22c6\u2225while training. This value quickly decreases to 0. This decay implies that model-agnostic methods learn not only the low-dimensional space on which the task parameters lie, but also their mean value. It then chooses this mean value as the initial point, and consequentially, the task adaptation happens quickly at test time. Overall, the experiments in this section suggest that model-agnostic methods still learn a good representation when facing more general task distributions."
        },
        {
            "heading": "I.3 NUMBER OF GRADIENT STEPS AT TEST TIME",
            "text": "This section studies what should be done at test time for the different methods. Figure 7 illustrates how the excess risk evolves when running gradient descent over the head parameters w, for the methods trained in Section 5. For all results, gradient descent is run with step size 0.01, which is actually smaller than the \u03b1 used while training FO-ANIL.\nKeeping the step size equal to \u03b1 leads to optimisation complications when running gradient descent: the objective loss diverges, since the step size is chosen too large. This divergence is due to the fact that FO-ANIL chooses a large scale Bt while training: this ensures a quick adaptation after a single gradient step but also leads to divergence of gradient descent after many steps.\nThe excess risk first decreases for all the methods while running gradient descent. However, after some critical threshold, it increases again for all methods except the Oracle. It is due to the fact that at some point in the task adaptation, the methods start overfitting the noise using components along the orthogonal complement of the ground-truth space. Even though the representation learnt by FO-ANIL is nearly rank-deficient, it is still full rank. As can be seen in the difference between FO-ANIL and Oracle, this tiny difference between rank-deficient and full rank actually leads to a huge performance gap when running gradient descent until convergence.\nAdditionally, Figure 7 nicely illustrates how early stopping plays some regularising role here. Overall, this suggests it is far from obvious how the methods should adapt at test time, despite having learnt a good representation.\nI.4 IMPACT OF NOISE AND NUMBER OF SAMPLES IN INNER UPDATES In this section, we run additional experiments to illustrate the impact of label noise and the number of samples on the decay of the orthogonal complement of the ground-truth subspace. The experimental setup is the same as Section 5 for FO-ANIL with finite tasks, except for the changes in the number of samples per task and the variance of label noise.\nFigure 8 illustrates the decay of squared singular value of B\u22a4\u22c6,\u22a5Bt during training. As predicted by Appendix H, the unlearning is fastest when min = 10 and slowest when min = 30. Figure 9 plots the decay with respect to different noise levels. The rate derived for the infinite tasks model suggests that the decay is faster for larger noise. However, experimental evidence with a finite number of tasks is more nuanced. The decay is indeed fastest for \u03c32 = 4 and slowest for \u03c32 = 0 on average. However, the decay of the largest singular value slows down for \u03c32 = 4 in a second time, while the decay still goes on with \u03c32 = 0, and the largest singular value eventually becomes smaller than in the \u03c32 = 4 case. This observation might indicate the intricate dynamics of FO-ANIL with finite tasks."
        },
        {
            "heading": "I.5 SCALING LAWS IN PROPOSITION 1",
            "text": "In this subsection, we study the scaling laws predicted by the upper bound in Proposition 1. We compute excess risk and estimation errors and compare them with the predictions from Proposition 1. All errors are computed by sampling 1000 test-time tasks with 1000 test samples each.\nIn order to show that there is no dependency on d after pretraining with FO-ANIL, we run experiments with varying d = k\u2032 and k, in the same experimental setup as described in Appendix I.1. To mimic few-shot and high-sample regimes, we select mtest = 20 and mtest = 1000. Our results are shown in Figure 10. The excess risk does not scale with the ambient dimension d but with the hidden low-rank dimension k.\nNext, we run a series of experiments to evaluate scaling laws predicted by Proposition 3. Similarly, we follow the experimental setting detailed in Appendix I.1 with an identity \u03a3\u22c6. In order to provide a clean comparison, \u03a3\u22c6 is scaled in test time such that E[\u2225w\u22c6\u2225] = 1 for all k values. This allows us to isolate the impact of k,min and mtest on the generalization error after adaptation. We also set d = k\u2032 = 25 and N = 25000 for the rest of this subsection.\nFigure 11 shows the scaling of the loss with respect to k and different choices of mtest for min = 20 and min = 40, together with predictions made from Proposition 1. Black horizontal lines are bounds for the first term that does not scale with mtest, i.e., , bound to the generalisation error when mtest = \u221e. We observe that the bound used in Proposition 1 is tight. The dependency on k is through the term \u03c32/\u03c3min(\u03a3\u22c6) which is equal to tr(\u03a3)/\u03c3min(\u03a3\u22c6) = k when \u03c32 = 0.\nIn order to evaluate the other two terms in Proposition 1, we subtract the dashed black lines from \u2225Bwtest\u2212B\u22c6w\u22c6\u22252 and plot it with respect to \u221a k/mtest. Figure 12 shows that this excess estimation\nerror is linear in \u221a k/mtest. Black horizontal lines are ( 1 + \u03c32 )\u221a k/mtest that serves as upper bound to the two last terms in Proposition 1. Overall, our results indicate the scaling given by Proposition 1 is tight.\nI.6 IMPACT OF NONLINEARITY AND MULTIPLE LAYERS We train two-layer and three-layer ReLU networks and study the scaling of test loss with the dimension d and hidden dimension k. All the hidden layers have d units. The experimental setting is the same as in Appendix I.1 except that tasks are processed in batches of size 100 out of a pool of 25000 for a faster training. In the case of two-layer ReLU networks, we set \u03b1 = \u03b2 = 0.025, while for three-layer ReLU networks, we adjust the values to \u03b1 = \u03b2 = 0.01.\nFigure 13 shows that the excess risk does not scale with the ambient data dimensionality d but with the hidden problem dimension k. This is evidence that suggests the adaptability of model-agnostic meta-learning pretraining extends to more general networks.\nNext, we study representation learning in ReLU networks. Let f (\u00b7) : Rd \u2192 Rd represent the network function that sends data to intermediate representations before the last layer. Then, we compute the best linear approximation of f as follows: Let X \u2208 RN\u00d7d be a matrix with each row is sampled from the d-dimensional isotropic Gaussian distribution. Solve the following minimization problem over all B \u2208 Rd\u00d7d:\nargmin B\u2208Rd\u00d7d\n\u2225XB \u2212 f(X)\u222522,\nwhere f(X) \u2208 RN is the output of the network applied to each row separately. Applying this to each time step t with N = 1000, we obtain Bt that approximates the ReLU network throughout its trajectory. Finally, we repeat the experiments on singular values to check learning in the good feature space and unlearning in the complement space with the sequence Bt.\nThe feature learning behavior in two-layer and three-layer ReLU networks, as illustrated in Figures 14 and 16, closely mirrors that of the linear case. Notably, both ANIL, MAML, and their first-order counterparts exhibit increasing singular values in the good feature space. We observe a swifter learning with second-order methods. Moreover, there is a difference in the scale of singular values between first-order and second-order approaches in two-layer networks. In the context of three-layer networks, ANIL and MAML exhibit distinct scales.\nIn Figures 15 and 17, the dynamics in the complement feature space for two-layer and three-layer ReLU networks are depicted. While the average singular value exhibits a decaying trend, contrary to our experiments with two-layer linear networks, the maximal singular value does not show a similar decay. Note, however, that the scale of the initialisation is smaller than in the linear case and singular values in all complement directions remain small when compared to good feature directions in the pretraining phase. This smaller initialisation is due to nonlinearities in ReLU networks; the linear approximation yield smaller singular values than the weight matrices of the ReLU network which is initialised at the same scale as experiments with linear networks (for three-layer networks, their product is of the same scale). As elaborated further in Appendix A, this behavior in the complement space is also observable in two-layer linear networks under small initialisations and is influenced by the finite number of tasks as opposed to the infinite tasks considered in our Theorem 1.\nOverall, experiments with two-layer and three-layer ReLU networks show that they learn the kdimensional shared structure with a higher magnitude than the rest of the complement directions. This implies the learning of a shared task structure and good generalisation under adaptation with\nfew samples, and might indicate that Theorem 1 and Proposition 1 could be extended to nonlinear networks. The regularisation effect of model-agnostic meta-learning on complement directions could be better seen with initialisations that result in a linear map with high singular values in every direction. We leave detailed exploration of the unlearning process to future work."
        }
    ],
    "year": 2023
}