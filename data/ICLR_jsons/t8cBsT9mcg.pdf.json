{
    "abstractText": "Machine learning models are often used to automate routine tasks. In settings where mistakes are costly, we can trade off accuracy for coverage by abstaining from making a prediction on instances for which the model is uncertain. In this work, we present a new approach to selective classification in deep learning with concepts. Our approach constructs a concept bottleneck model where the front-end model can make predictions given soft concepts and leverage concept confirmation to improve coverage and performance under abstention. We develop techniques to propagate uncertainty and identify concepts for confirmation. We evaluate our approach on real-world and synthetic datasets, showing that it can improve performance and coverage across a range of tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "CONCEPTUAL SAFEGUARDS"
        }
    ],
    "id": "SP:ee994a0c699d580c9f0369fe102f2b130e27fee6",
    "references": [
        {
            "authors": [
                "Siddhartha Bhattacharyya",
                "Sanjeev Jha",
                "Kurian Tharakunnel",
                "J Christopher Westland"
            ],
            "title": "Data mining for credit card fraud: A comparative study",
            "venue": "Decision support systems,",
            "year": 2011
        },
        {
            "authors": [
                "Nontawat Charoenphakdee",
                "Zhenghang Cui",
                "Yivan Zhang",
                "Masashi Sugiyama"
            ],
            "title": "Classification with rejection based on cost-sensitive classification",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "C Chow"
            ],
            "title": "On optimum recognition error and reject tradeoff",
            "venue": "IEEE Transactions on information theory,",
            "year": 1970
        },
        {
            "authors": [
                "Chi-Keung Chow"
            ],
            "title": "An optimum character recognition system using decision functions",
            "venue": "IRE Transactions on Electronic Computers,",
            "year": 1957
        },
        {
            "authors": [
                "Katherine Maeve Collins",
                "Matthew Barker",
                "Mateo Espinosa Zarlenga",
                "Naveen Raman",
                "Umang Bhatt",
                "Mateja Jamnik",
                "Ilia Sucholutsky",
                "Adrian Weller",
                "Krishnamurthy Dvijotham"
            ],
            "title": "Human uncertainty in concept-based ai systems",
            "venue": "In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society,",
            "year": 2023
        },
        {
            "authors": [
                "Ran El-Yaniv"
            ],
            "title": "On the foundations of noise-free selective classification",
            "venue": "Journal of Machine Learning Research,",
            "year": 2010
        },
        {
            "authors": [
                "Andre Esteva",
                "Brett Kuprel",
                "Roberto A Novoa",
                "Justin Ko",
                "Susan M Swetter",
                "Helen M Blau",
                "Sebastian Thrun"
            ],
            "title": "Dermatologist-level classification of skin cancer",
            "year": 2017
        },
        {
            "authors": [
                "Jean Feng",
                "Arjun Sondhi",
                "Jessica Perry",
                "Noah Simon"
            ],
            "title": "Selective prediction-set models with coverage rate",
            "venue": "guarantees. Biometrics,",
            "year": 2023
        },
        {
            "authors": [
                "Adam Fisch",
                "Tommi Jaakkola",
                "Regina Barzilay"
            ],
            "title": "Calibrated selective classification",
            "venue": "arXiv preprint arXiv:2208.12084,",
            "year": 2022
        },
        {
            "authors": [
                "Vaclav Voracek Vojtech Franc",
                "Daniel Prusa",
                "Vaclav Voracek"
            ],
            "title": "Optimal strategies for reject option classifiers",
            "venue": "Journal of Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Giorgio Fumera",
                "Fabio Roli"
            ],
            "title": "Reject option with multiple thresholds",
            "venue": "Pattern recognition,",
            "year": 2000
        },
        {
            "authors": [
                "Yonatan Geifman",
                "Ran El-Yaniv"
            ],
            "title": "Selective classification for deep neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Seung Seog Han",
                "Myoung Shin Kim",
                "Woohyung Lim",
                "Gyeong Hun Park",
                "Ilwoo Park",
                "Sung Eun Chang"
            ],
            "title": "Classification of the clinical images for benign and malignant cutaneous tumors using a deep learning algorithm",
            "venue": "Journal of Investigative Dermatology,",
            "year": 2018
        },
        {
            "authors": [
                "Marton Havasi",
                "Sonali Parbhoo",
                "Finale Doshi-Velez"
            ],
            "title": "Addressing leakage in concept bottleneck models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Erik Jones",
                "Shiori Sagawa",
                "Pang Wei Koh",
                "Ananya Kumar",
                "Percy Liang"
            ],
            "title": "Selective classification can magnify disparities across groups",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Jeremy Kawahara",
                "Sara Daneshvar",
                "Giuseppe Argenziano",
                "Ghassan Hamarneh"
            ],
            "title": "Seven-point checklist and skin lesion classification using multitask multimodal neural nets",
            "venue": "IEEE journal of biomedical and health informatics,",
            "year": 2018
        },
        {
            "authors": [
                "Pang Wei Koh",
                "Thao Nguyen",
                "Yew Siang Tang",
                "Stephen Mussmann",
                "Emma Pierson",
                "Been Kim",
                "Percy Liang"
            ],
            "title": "Concept bottleneck models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Anita Mahinpei",
                "Justin Clark",
                "Isaac Lage",
                "Finale Doshi-Velez",
                "Weiwei Pan"
            ],
            "title": "Promises and pitfalls of black-box concept learning models",
            "venue": "arXiv preprint arXiv:2106.13314,",
            "year": 2021
        },
        {
            "authors": [
                "Andrei Margeloiu",
                "Matthew Ashman",
                "Umang Bhatt",
                "Yanzhi Chen",
                "Mateja Jamnik",
                "Adrian Weller"
            ],
            "title": "Do concept bottleneck models learn as intended?, 2021a. URL https://arxiv.org/abs/2105.04289",
            "year": 2021
        },
        {
            "authors": [
                "Andrei Margeloiu",
                "Matthew Ashman",
                "Umang Bhatt",
                "Yanzhi Chen",
                "Mateja Jamnik",
                "Adrian Weller"
            ],
            "title": "Do concept bottleneck models learn as intended",
            "venue": "arXiv preprint arXiv:2105.04289,",
            "year": 2021
        },
        {
            "authors": [
                "Mohammad Sadegh Norouzzadeh",
                "Anh Nguyen",
                "Margaret Kosmala",
                "Alexandra Swanson",
                "Meredith S Palmer",
                "Craig Packer",
                "Jeff Clune"
            ],
            "title": "Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2018
        },
        {
            "authors": [
                "Tuomas Oikarinen",
                "Subhro Das",
                "Lam M Nguyen",
                "Tsui-Wei Weng"
            ],
            "title": "Label-free concept bottleneck models",
            "venue": "arXiv preprint arXiv:2304.06129,",
            "year": 2023
        },
        {
            "authors": [
                "John Platt"
            ],
            "title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods",
            "venue": "Advances in large margin classifiers,",
            "year": 1999
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jon Shlens",
                "Zbigniew Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Michael A Tabak",
                "Mohammad S Norouzzadeh",
                "David W Wolfson",
                "Steven J Sweeney",
                "Kurt C VerCauteren",
                "Nathan P Snow",
                "Joseph M Halseth",
                "Paul A Di Salvo",
                "Jesse S Lewis",
                "Michael D White"
            ],
            "title": "Machine learning to classify animal species in camera trap images: Applications in ecology",
            "venue": "Methods in Ecology and Evolution,",
            "year": 2019
        },
        {
            "authors": [
                "Catherine Wah",
                "Steve Branson",
                "Peter Welinder",
                "Pietro Perona",
                "Serge Belongie"
            ],
            "title": "The caltech-ucsd birds-2002011 dataset",
            "venue": "Technical report, California Institute of Technology,",
            "year": 2011
        },
        {
            "authors": [
                "Chih-Kuan Yeh",
                "Been Kim",
                "Sercan Arik",
                "Chun-Liang Li",
                "Tomas Pfister",
                "Pradeep Ravikumar"
            ],
            "title": "On completeness-aware concept-based explanations in deep neural networks. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Mert Yuksekgonul",
                "Maggie Wang",
                "James Zou"
            ],
            "title": "Post-hoc concept bottleneck models",
            "venue": "arXiv preprint arXiv:2205.15480,",
            "year": 2022
        },
        {
            "authors": [
                "Bianca Zadrozny",
                "Charles Elkan"
            ],
            "title": "Transforming classifier scores into accurate multiclass probability estimates",
            "venue": "In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2002
        },
        {
            "authors": [
                "melanoma Kawahara"
            ],
            "title": "2018) lesion is melanoma",
            "year": 2018
        },
        {
            "authors": [
                "skincancer Kawahara"
            ],
            "title": "2018) lesion is cancerous",
            "year": 2018
        },
        {
            "authors": [
                "cubcommon Wah"
            ],
            "title": "bird in 10 most prevalent classes 5,994 112 2,907 cubrare Wah et al. (2011) bird in 60 least prevalent classes 5,994",
            "venue": "warbler Wah et al",
            "year": 2011
        },
        {
            "authors": [
                "flycatcher Wah"
            ],
            "title": "bird is type of flycatcher",
            "year": 2011
        },
        {
            "authors": [
                "Koh"
            ],
            "title": "cubcommon, cubrare, warbler, flycatcher, cubspecies & cubtypes These datasets are derived from the CUB 2011 dataset",
            "venue": "Wah et al",
            "year": 2011
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "One of the most promising applications of modern machine learning is the ability to automate routine tasks. These opportunities include tasks such as diagnosing lesions from dermatology images Han et al. (2018), detecting suspicious activity in credit card transactions Bhattacharyya et al. (2011), or species identification using wildlife footage Tabak et al. (2019). Despite rapid progress in automated systems\u2019 predictive capabilities, deploying these systems in high-stakes settings remains challenging. Two requirements slow adoption:\n1. Performance: Models are often used to automate tasks a human can perform correctly. In these cases, maintaining high performance is expected given that a non-automated system should have near-perfect performance and because even small performance gaps can have a disproportionate impact. This is because automation scales, meaning that systems can compromise safety by rapidly generating a high volume of incorrect predictions. For instance, a small decrease in accuracy in skin cancer detection could result in delayed treatment, posing significant risks to human health and lives.\n2. Interpretability: One of the barriers to adopting systems that can automate routine tasks is a lack of interpretability. Put simply, stakeholders would like to understand how a system makes its prediction, check that it is working correctly once it is deployed, and maintain the ability to override the prediction in an informed manner.\nOne way to build interpretability into models for decision-making is to use deep learning with concept labels, or concept bottleneck models. However, concept bottleneck models will often perform poorly on datasets where we lack sufficiently rich concepts or when given uncertain concept predictions since they must be trained on a dataset of hard concept labels Yeh et al. (2020); Yuksekgonul et al. (2022). Overcoming this challenge requires building concept bottleneck models that can handle uncertain predictions, perform well under selective classification, and are amenable to concept confirmation. To build models that can support selective classification, we must quantify the uncertainty y in concept predictions and propagate this uncertainty end-to-end, including capturing uncertainty related to concept prediction.\nTo enable concept confirmation, we must build the system in such a way that imputing the true concept value reliably improves performance. In this work, we present a new approach to building deep learning systems that address these challenges by drawing on ideas in selective classification and concept bottleneck models to build a versatile system for automation. Our approach seeks to build a classification model with conceptual safeguards (see Fig. 1): i.e., a model that can assign predictions on the basis of human-verifiable concepts, and that can abstain from prediction when these concepts\nare not detected with adequate certainty. Conceptual safeguards use selective classification to restrict automation to instances where the model is sufficiently confident \u2013 i.e., by allowing models to abstain from predicting when they are insufficiently confident. In tandem, conceptual safeguards address interpretability through a concept bottleneck model \u2013 which provides an architecture that allows end-users to understand how the system makes its predictions, check the prediction logic and reduce the number of abstentions by verifying uncertain concepts at prediction time. In effect, conceptual safeguards maintain interpretability while maintaining performance and maximizing coverage, or the proportion of samples on which the system can safely predict.\nThe main contributions of this work include:\n1. We introduce conceptual safeguards, a versatile approach to automating classification tasks that promotes safety and interpretability on deep learning tasks with concept labels.\n2. We develop a technique to propagate the uncertainty from predicted concepts to predicted labels. The technique improves overall model performance by accounting for uncertainty and supports the development of reliable selective classification in concept bottleneck models.\n3. We propose a method for flagging predicted concepts for human confirmation. In our proposed systems, concept confirmation can reliably improve model coverage while maintaining performance.\n4. We conduct an empirical investigation of our proposed approach compared with baseline concept bottleneck models equipped with selective classification. Our results show that the proposed approach can improve coverage and performance without confirmation as well as lead to dramatic gains in coverage under concept confirmation.\nRELATED WORK\nSelective Classification Our work builds on work in selective classification or classification with a reject option. Selective classification is a general-purpose family of methods to build models that abstain from prediction on low-confidence samples (Chow, 1957; 1970; Fumera & Roli, 2000; Franc et al., 2023). The goal of selective classification is to improve performance while keeping prediction coverage as high as possible. Allowing for abstention also enables partial automation, in which prediction is automated only on samples with high certainty (Feng et al., 2023). This is useful in applications with strict performance criteria due to the cost of mistakes, such as in cancer diagnosis Esteva et al. (2017), and in applications where it is difficult to train a model with\nconsistently high performance, such as in species identification Norouzzadeh et al. (2018). Recent work extends these benefits to deep learning settings Geifman & El-Yaniv (2017); El-Yaniv et al. (2010). Applying selective classification in deep learning is challenging because most approaches require accurate uncertainty estimates to guide abstention decisions (Charoenphakdee et al., 2021; Fisch et al., 2022). We seek to overcome this challenge in concept bottleneck models by propagating uncertainty from the concept detectors to the front-end outcome predictor.\nDeep Learning with Concepts Labels Our work is related to work on using concept labels in deep learning. In particular, concept bottleneck models seek to improve model interpretability by leveraging dataset annotations to learn concepts and outcome predictions sequentially Koh et al. (2020). Methods for learning with concepts derived from raw inputs are motivated by their ability to provide an unprecedented degree of interpretability, such as allowing observation of counterfactuals or correcting concepts to improve performance. One challenge for learning with concepts includes a need for annotations in training data, which limits the performance of these types of models compared to alternative approaches that do not require concept labels Yuksekgonul et al. (2022); Oikarinen et al. (2023); Yeh et al. (2020).\nPerformance is further hampered by the requirement that the front-end model be trained on and accept hard concept labels, or else risk label leakage and preclude concept confirmation Havasi et al. (2022); Margeloiu et al. (2021b). While concept bottleneck models can be trained on soft labels, we are specifically interested in concept models that allow confirmation to improve performance and coverage. This challenge limits the overall performance of these types of models Koh et al. (2020) and makes it difficult to estimate model uncertainty. We seek to meet the requirements for confirmation and avoid label leakage while leveraging uncertainty to improve overall model performance."
        },
        {
            "heading": "2 PROBLEM STATEMENT",
            "text": "We consider a classification task with the goal of predicting a label using a set of complex features and simple concepts. We start with a dataset of n i.i.d. training instances {(xi, ci, yi)}ni=1. Each example consists of a vector of d features xi \u2208 X \u2286 Rd (e.g., pixels in an x-ray image), a label yi \u2208 Y = {0, 1} (e.g., yi = 1 if patient i has arthritis), and m concepts ci \u2208 C = {0, 1}m (e.g., ci,k = 1 if image i contains a bone spur).\nWe use the dataset to build a concept bottleneck model to predict the label y. A concept bottleneck model consists of two components:\n\u2022 A concept detector g : X \u2192 [0, 1]m, which maps features to a vector of probability predictions qi := g(xi) \u2208 [0, 1]m where qi,k is the predicted probability that ci,k = 1.\n\u2022 A front-end model f : C \u2192 Y , which maps a vector of hard concepts ci to a predicted probability of the outcome yi := f(ci), where yi is the predicted probability that yi = 1.\nWe train the concept detector and front-end model using supervised learning with the datasets {(xi, ci)}ni=1 and {(ci, yi)}ni=1, respectively. Training the concept detectors separately ensures that we can train each concept detector using as much data as possible. In contrast, training a single model that predicts all concepts at once requires that we only consider a subset of training instances without missing concept labels. We train the concept detectors g1, . . . , gm to output probability predictions (e.g., ERM with the cross-entropy loss). We calibrate the models using a post-hoc calibration technique (e.g., isotonic temperature scaling (Zadrozny & Elkan, 2002) or Platt Scaling (Platt et al., 1999)). We train the front-end model f independently from the concept detectors \u2013 i.e., using the true concepts c \u2208 {0, 1} rather than the predicted concepts q. Independent training is a key requirement for any concept bottleneck model where humans intervene on concepts. A front-end model that is trained using the predicted concepts may require incorrect concept predictions to assign accurate label predictions (Margeloiu et al., 2021a; Mahinpei et al., 2021; Havasi et al., 2022). Concept bottleneck models that do use uncertain concepts trade this mechanism for intervenability, or the ability to intervene on concepts to improve predictions.\nModel Pipeline We view the concept detector and front-end model as an end-to-end predictor h : X \u2192 {0, 1,\u22a5} and denote its output y\u0302 := h(x). Here, the end-to-end predictor h either outputs a\nhard label prediction y\u0302 \u2208 0, 1 or abstains from prediction y\u0302 =\u22a5. The requirements for our end-to-end predictor h are twofold:\n1. We control selective accuracy, i.e., the accuracy of h over instances on which it outputs a prediction: Accuracy(h) := Pr (y = y\u0302 | y\u0302 6=\u22a5)\n2. We maximize coverage, i.e., the proportion of instances instances on which h outputs a prediction: Coverage(h) := Pr (y\u0302 6=\u22a5).\nOur goal is to maximize coverage subject to the constraint that accuracy exceeds 1 \u2212 \u03b1, for a user-specified error tolerance \u03b1 \u2208 [0, 1]. We control accuracy and coverage through abstention and concept confirmation, respectively (see Fig. 2).\nAbstention To avoid making predictions on instances where confidence is low and errors are likely, we incorporate the option to abstain. Given a confidence threshold \u03c4 \u2208 [0, 1], an abstention function \u03d5\u03c4 : [0, 1]\u2192 Y \u222a {\u22a5} takes as input a probabilistic prediction yi \u2208 [0, 1], and returns a prediction:\ny\u0302i = \u03d5\u03c4 (yi) =  1 if yi > 1\u2212 \u03c4 0 if yi < \u03c4 \u22a5 otherwise\nIntuitively, the abstention function rounds the probabilistic prediction to a hard prediction if there is sufficient confidence and otherwise abstains.\nConfirmation We let users confirm the predicted concepts for any given instance. For example, in the x-ray screening task, we could ask a human to confirm that concept k is present in x-ray i. We assume that all concepts are human-verifiable (c.f., concepts where a human may be uncertain as in Collins et al., 2023). Given any concept that we choose to confirm. we replace the soft prediction qi,k \u2208 [0, 1] with its ground-truth value ci,k \u2208 {0, 1}. We use the term \u201cconfirmation\u201d as it reflects a specific kind of intervention.1\nWe write the confirmation process as a function \u03c8S : [0, 1]m \u2192 [0, 1]m where S \u2286 [m] denotes a subset of concepts to confirm. The function takes as input the vector of concept predictions and outputs pi = [pi,1, . . . , pi,m] \u2208 [0, 1]m where:\npi,k := { ci,k if k \u2208 S qi,k if k /\u2208 S\n(1)\nIn what follows, we will consider systems in which we the concept predictions determine the subset of concepts that we flag for confirmation, in which case we write S(qi) instead of S.\nDesign Objectives Our task is to design these components so that they collectively define a model h : X \u2192 Y \u222a {\u22a5} to maximize coverage subject to a target level of system accuracy. Taken together, our end-to-end predictor has the following structure: given features, we infer concepts with g, confirm a subset of the concepts with \u03c8, predict the label with f , and decide to abstain with \u03d5:\nh : x g7\u2192 q \u03c87\u2192 p f7\u2192 y \u03d57\u2192 y\u0302\n1In general, the term \u201cintervention\u201d can refer to a broad family of procedures where a human alters the output of a concept detector. For example, it may refer to \u201ccorrection\u201d in which a human replaces a hard concept prediction with its correct value.\nWe treat the concept detectors and front-end model as given. Thus, our main components for building conceptual safeguards consist of developing techniques for uncertainty propagation and confirmation.\n\u2022 Confirmation: We wish to design methods that can select samples to confirm so that we maximize coverage, maintain accuracy, and adhere to a userspecific confirmation budget that is set to reflect the desired level of human intervention in a system.\n\u2022 Propagating Concept Uncertainty: After concept detection and confirmation, we are left with concept predictions that represent probabilities pi,k \u2208 [0, 1]. However, the front-end model requires hard concepts in {0, 1} as inputs. We require a method for retrofitting the system with a mechanism to relay concept uncertainties through the front-end model f ."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "3.1 UNCERTAINTY PROPAGATION",
            "text": "We use uncertainty propagation to allow the system to accept probabilities, rather than the hard labels required by the front-end model f . We make two key assumptions about the underlying data distribution to motivate our approach. Assumption 1. The label y and features x are conditionally independent given the concepts c Assumption 2. The concepts {c1, . . . , cm} are conditionally independent given the features x.\nWe note that these assumptions might not be met in practice, and include experiments in settings that violate these assumptions in Section 4. Given these assumptions, we can write the conditional distribution p(y | x) in terms of quantities that we natively estimate using the concept detectors and front-end model:\np(y | x) = \u2211\nc\u2208{0,1}m p(y | c,x) p(c | x)\n= \u2211\nc\u2208{0,1}m p(y | c) p(c | x) (Assumption 1)\n= \u2211\nc\u2208{0,1}m p(y | c) \u220f k\u2208[m] p(ck | x) (Assumption 2)\nWe can estimate p(y | x) under these assumptions by plugging in our estimates of p(y | c) and p(ck | x). Given the vector of probability predictions from the concept detectors pi = (pi,1, . . . , pi,k) , we can compute the estimate of p(y | x) as:\nf(pi) := \u2211\nc\u2208{0,1}m f(c) \u220f k\u2208[m] pcki,k(1\u2212 pi,k) 1\u2212ck (2)\nComputing the estimate in (2) requires 2m calls to the front-end model. In practice, this is negligible as the vast majority of concept models are trained with a limited number of concepts. In settings where m is large or computation is prohibitively expensive, however, we can construct a Monte Carlo estimate using a sample of concept vectors. Definition 3. Given a label y \u2208 {0, 1} and probabilistic prediction p \u2208 [0, 1], we say that p is a calibrated prediction for y if Pr (y = 1 | p = t) = t for all thresholds t \u2208 [0, 1]. Proposition 4. Suppose that p is a calibrated prediction for y. Then a selective classification procedure \u03d5\u03c4 (p) that abstains when p has confidence below 1\u2212 \u03c4 achieves accuracy at least 1\u2212 \u03c4 .\nProposition 4 assumes that the front-end model will output calibrated probabilty predictions. In the event that the model is not calibrated then Proposition 4 will hold only given the degree of calibration. We can address this case by setting \u03c4 using a more sophisticated method that works with \u201cconfidence scores\u201d that may or not be calibrated Geifman & El-Yaniv (2017)."
        },
        {
            "heading": "3.2 CONFIRMATION",
            "text": "Since our goal is to reduce the need for human oversight where possible, we consider a method that greedily selects concepts to confirm based on the expectation of the gain in certainty. Our methods associate the confirmation of each concept k with a cost \u03b3k \u2265 0 and seek to select concepts to confirm so that the total cost of confirmation C(S) := \u2211 k\u2208S \u03b3k meets a fixed confirmation budget B \u2265 0. 2\nAlgorithm 1 Greedy Concept Selection\nInput: C\u0302, set of concept predictions that lead to abstention, h(c\u0302) = 0 Input: B \u2265 0, confirmation budget Input: \u03b31, . . . , \u03b3m, costs to confirm each concept\n1: S1, . . . , Sn \u2190 {} concepts to confirm for each instance 2: repeat 3: i\u2217, k\u2217 \u2190 argmaxi,k Gain(c\u0302i, k) s.t. k 6\u2208 Si select best remaining concept 4: Si \u2190 Si \u222a {k} 5: B \u2190 B \u2212 \u03b3k 6: until B < 0 or Si = [m] for all i\nOutput: S1, . . . , Sn, confirmations for each sample\nOne way to select concepts to confirm is to test the sensitivity of model output with respect to a concept. If confirming concept k leads to a large variance in the prediction, then confirming the concept will likely significantly impact the prediction. In our experiments, we use as our gain measure the variance of the result of confirmation, where c\u0304k is c\u0302 after confirming concept k:\nGain(c\u0302, k) := Var[f(c\u0304k)] = E((f(c\u0304k))2)\u2212 (E(f(c\u0304k)))2 (3) Since we do not know f(c\u0304S+{k}) before confirmation, we view f(c\u0304S+{k}) as a random variable that takes on values f(c\u0304S [c\u0304k = 1]) with probability c\u0302k and f(c\u0304S [c\u0304k = 0] with probability (1\u2212 c\u0302k). Here c\u0304S [c\u0304k = 0] means we have confirmed c\u0304S at index k with 0."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We evaluate our method on synthetic and real-world datasets. Our goal is to benchmark the performance and coverage of our approach across prediction tasks with ablations to separate the impacts of confirmation and uncertainty propagation. We provide additional details on and results in Appendix A, and the code to reproduce these results in an anonymized repository."
        },
        {
            "heading": "4.1 SETUP",
            "text": "We consider six classification datasets with concept labels. The melanoma and skincancer datasets are image classification tasks to diagnose melanoma and skin cancer that are derived from the Derm7pt dataset Kawahara et al. (2018). The warbler and flycatcher datasets are image classification bird identification tasks derived from the CalTech-UCSD Birds dataset Wah et al. (2011). We process each dataset to binarize categorical concepts (e.g., WingColor or VascularStructures) and (WingColorRed or IrregularVascularStructures). We construct four systems for each dataset. Each system is an end-to-end selective classification model that allows for concept-based interventions. We build each system using the same front-end model and concept detectors as described in Section 2. We train a front-end model f and concept detectors g1, . . . , gm for each dataset using logistic regression. We perform uncertainty propagation using Monte Carlo estimation with 10,000 samples and train concept detectors for each dataset \u2013 other than synthetic \u2013 using\n2We can set B \u2208 [0,m] to control the average number of concepts to confirm per example. Alternatively, we can set B \u2208 [0, 1], restricts the total number of instances to B.\nembeddings from a pre-trained model (i.e., InceptionV3 Szegedy et al., 2016). The four systems include:\n\u2022 Baseline: An independent concept bottleneck model build using concept detectors g1, . . . , gm and a front-end model f trained on the true concepts. This is a traditional concept bottleneck model without conceptual safeguards.\n\u2022 CS + ImpactConf: Our proposed system uses uncertainty propagation and the targetted confirmation strategy as described in Section 3.\n\u2022 Baseline + RandomConf & CS + RandomConf: The Baseline and CS systems paired with a baseline confirmation strategy that randomly selects concepts for confirmation. We report results for these systems as a baseline that we can use to assess the value added of our proposed confirmation strategy.\nEvaluation We split each dataset into a training set to train models and their components (80%) and a test set (20%) to evaluate their performance. We construct an accuracy coverage curve for each method on each dataset by plotting the coverage and selective accuracy of the system for abstention thresholds of \u03c4 \u2208 [0.5, . . . , 0.95]. (see Section 2). We evaluate these values when we confirm concepts on a subset of instances on which a model abstains (using the ImpactConf or the RandomConf strategies). We limit the number of confirmations to match a confirmation budgets. We then construct accuracy-coverage curves for confirmation budgets of 0%/10%/20%/50% which reflect the returns to no/low/medium/high human degrees of human intervention."
        },
        {
            "heading": "4.2 RESULTS",
            "text": "Our results show that in general, our proposed methods CS + RandomConf and CS + ImpactConf outperform baselines Baseline and Baseline + RandomConf with regards to performance and coverage under selective classification. These gains vary across datasets. Given the same performance threshold, we achieve lower coverage on datasets Melanoma and SkinCancer than on datasets CUBCommon and CUBRare, likely as a result of the differences in the difficulty and quality of the concepts. In all cases, leveraging uncertainty across concepts improves this trade-off. These gains vary across datasets, where the difficulty of the task and the performance of the concept detectors impact the benefit of conceptual safeguards. Across all tasks, confirming concepts improves performance, with the greatest gains achieved by selecting the concepts to confirm based on their impact on the final prediction (CS + ImpactConf).\nOn the Gains of Accounting for Uncertainty The results summarized in Table Table 4 emphasize the advantages of incorporating uncertainty into the model, where the extent of these advantages varies based on the specific task and confirmation budget. For instance, when no confirmation is allowed (Confirmation Budget at 0%), the dataset flycatcher shows little to no benefit from accounting for uncertainty. In contrast, the skincancer dataset reveals a substantial performance gap between a model that includes uncertainty (CS + RandomConf, blue) and one that doesn\u2019t (Baseline + RandomConf, green). The impact of incorporating uncertainty also changes when a confirmation budget is introduced. Even though uncertainty made little difference in the flycatcher dataset at a 0% confirmation budget, allowing a 20% confirmation budget led to a significant increase in coverage, from 46.2% to 63.5%, when the threshold is set at \u03c4 = 0.05 (see Table 2). This is noteworthy because it allows for the automation of predictions on more than an additional 17% of instances without requiring human intervention.\nOn Learning from Noisy Concepts We evaluate the effects of noisy concepts on model performance, we construct a synthetic dataset with varying levels of noise in the underlying concepts that generate the labels (see Table 6). When the probability of noise is 0%, the concept is fully deterministic and calculated via a parity function of its relevant features. As the probability of noise increases (e.g., 25% or 75%), the concept becomes increasingly stochastic, thus simulating real-world scenarios where underlying features may be subject to noise or uncertainty. Results from the synthetic datasets demonstrate that the benefits of conceptual safeguards increase with increasingly noisy concepts. With low noise p(noise) = 25%, conceptual safeguards provide a real but small benefit to coverage and performance. With high noise p(noise) = 25%, conceptual safeguards provide a significant benefit. See Appendix A.1 for details on synthetic data construction and experiments.\nOn Confirmation Confirming concepts improves performance across all datasets. On dataset flycatcher, confirmation with a budget of 20% improves coverage from 26.92% (Baseline) to 46.15% (Baseline + RandomConf) at threshold \u03c4 = 0.5, leading to nearly twice as many samples that can be automated. These gains compound with conceptual safeguards, where coverage reaches 63.46% with uncertainty propagation alone, without sacrificing performance or requiring additional human intervention (CS + RandConf). For conceptual safeguards overall, this coverage improves to reach 84.62% (CS + ImpactConf). This additional gain is a result of ImpactConf. Instead of intervening randomly on concepts up to the confirmation budget, ImpactConf considers the variance\nof the confirmation with respect to the outcome prediction and intervenes first on the highest impact concepts. This weighting allows humans to confirm the concepts with the greatest benefit first, leading to considerable gains in coverage without sacrificing safety. Confirming concepts can also lower automation tasks in applications where the concepts require a lower level of expertise than the label. For example, non-expert users may be able to confirm concepts such as WingColorRed, while confirming the final species prediction to RedFacedCormorant may require greater expertise. The gains of confirmation depend on the underlying task and dataset. For example, the gains may be smaller when the concept detectors perform well enough without confirmation to achieve high accuracy (e.g., for the warbler and noisyconcepts25 datasets).\nOn Multiclass Prediction Tasks While we limit our theoretical exposition to binary prediction tasks, conceptual safeguards can also be applied in multiclass settings. The main adaption required is in the uncertainty propagation component of the conceptual safeguards. Specifically, rather than propagating uncertainty about the prediction in general, we extend the prediction vector to include probabilities for each of the classes using the possible concepts c \u2208 {0, 1}m. For the selective classification component \u03d5\u03c4 , we estimate uncertainty based on the likelihood of the most probable class and threshold prediction accordingly. We include results on multiclass datasets in Appendix A.3."
        },
        {
            "heading": "5 CONCLUDING REMARKS",
            "text": "In this work, we introduce conceptual safeguards, a versatile and interpretable approach for automating classification tasks. Empirical investigation substantiates the effectiveness of our approach in various applications, establishing it as a promising avenue for safe and interpretable automation. One limitation of our approach is its potential to exacerbate performance disparities over subpopulations Jones et al. (2021). Another limitation is the model\u2019s reliance on well-defined concepts to capture all relevant information about the label from the input. If the chosen concepts do not encapsulate all necessary information, the model\u2019s performance will necessarily be compromised. These limitations highlight important areas for future research, centered on improving equitable outcomes and refining the utility of concept bottleneck models within our conceptual safeguards framework."
        },
        {
            "heading": "A SUPPORTING EXPERIMENTAL INFORMATION",
            "text": "A.1 DATASETS\nmelanoma & skincancer These datasets are derived from the Derm7pt dataset Kawahara et al. (2018), which is de-identified and publicly available without patient information. We preprocess the dataset by splitting the original seven categorical image annotations (pigment_network, streaks, pigmentation, regression_structures, dots_and_globules, blue_whitish_veil, vascular_structures) into seventeen binary concepts. We consider two tasks: predicting melanoma and predicting skincancer (melanoma or basal cell carcinoma). We split the validation indices in the original dataset into a validation set and a hold-out test set for evaluation. We then balance the resulting classes by downsampling the majority class. To train the concept models, we augment the original training images with random color enhancements and random flipping to obtain 10x total training images.\ncubcommon, cubrare, warbler, flycatcher, cubspecies & cubtypes These datasets are derived from the CUB 2011 dataset Wah et al. (2011). We follow the same preprocessing described in Koh et al. (2020). cubcommon predicts if a bird is \u201ccommon\u201d or in the top 10 largest groups. cubrare predicts if a bird is \u201crare\u201d or in the bottom 60 groups. warbler classifies birds of type warbler and flycatcher classifies birds of type flycatcher. cubspecies and cubtypes are multiclass datasets for predicting bird species and bird types, respectively. To train the concept models, we augment the original training images with random color enhancements and random flipping to obtain 10x total training images.\nnoisy The noisy datasets are synthetic datasets that we primarily use to evaluate how their performance changes with respect to the quality of concept detectors. We sample these the examples in these datasets (xi, ci, yi) from the following distribution:\nx1, . . . , x5 = Bernouilli(0.7) \u03be1, \u03be2, \u03be3 = Bernouilli(p\u03be)\nc1 = parity(x1, x2, x4)\u2295 \u03be1 c2 = parity(x1, x2, x3)\u2295 \u03be2 c3 = parity(x1, x2, x5)\u2295 \u03be3 p = logistic(1.0c1 + 2.0c2 + 3.0c3 \u2212 2.0) y \u223c Bernoulli(p)\n(4)\nThe distribution shown in (4) includes an explicit noise parameter p\u03be \u2208 [0, 1] that we can set to control the noise in concept labels. When p\u03be = 0, the values of c1, c2, c3 operate as parity functions, which can only be learned through a sufficiently complex model. When p\u03be > 0, we inject noise into the concept labels by randomly flipping the values of c1, c2, c3 with probability p\u03be. Thus, the noise parameter sets an upper bound on the accuracy of concept labels \u2013 and larger values of p\u03be lead to less accurate concept models. In contrast to the real-world datasets, we train the concept detectors for the noisy datasets directly (i.e., without an embedding layer) by fitting multi-layer perceptron with a single hidden layer.\nA.2 ADDITIONAL RESULTS\nA.3 MULTICLASS EXPERIMENTS"
        }
    ],
    "year": 2023
}