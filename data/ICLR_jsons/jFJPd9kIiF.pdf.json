{
    "abstractText": "This paper introduces Least Volume\u2014a simple yet effective regularization inspired by geometric intuition\u2014that can reduce the necessary number of latent dimensions needed by an autoencoder without requiring any prior knowledge of the intrinsic dimensionality of the dataset. We show that the Lipschitz continuity of the decoder is the key to making it work, provide a proof that PCA is just a linear special case of it, and reveal that it has a similar PCA-like importance ordering effect when applied to nonlinear models. We demonstrate the intuition behind the regularization on some pedagogical toy problems, and its effectiveness on several benchmark problems, including MNIST, CIFAR-10 and CelebA.",
    "authors": [
        {
            "affiliations": [],
            "name": "LEAST VOLUME"
        },
        {
            "affiliations": [],
            "name": "Qiuyi Chen"
        }
    ],
    "id": "SP:4bf7feb366ca90eb4f9585c1668a959835afc634",
    "references": [
        {
            "authors": [
                "Yoshua Bengio",
                "Aaron Courville",
                "Pascal Vincent"
            ],
            "title": "Representation learning: A review and new perspectives",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2013
        },
        {
            "authors": [
                "Christopher M Bishop",
                "Nasser M Nasrabadi"
            ],
            "title": "Pattern recognition and machine learning, volume",
            "year": 2006
        },
        {
            "authors": [
                "Herv\u00e9 Bourlard",
                "Yves Kamp"
            ],
            "title": "Auto-association by multilayer perceptrons and singular value decomposition",
            "venue": "Biological cybernetics,",
            "year": 1988
        },
        {
            "authors": [
                "E Glen"
            ],
            "title": "Bredon. Topology and geometry, volume 139",
            "venue": "Springer Science & Business Media,",
            "year": 2013
        },
        {
            "authors": [
                "Nutan Chen",
                "Alexej Klushyn",
                "Francesco Ferroni",
                "Justin Bayer",
                "Patrick Van Der Smagt"
            ],
            "title": "Learning flat latent manifolds with vaes",
            "venue": "arXiv preprint arXiv:2002.04881,",
            "year": 2020
        },
        {
            "authors": [
                "Li Deng"
            ],
            "title": "The mnist database of handwritten digit images for machine learning research",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2012
        },
        {
            "authors": [
                "Carl Eckart",
                "Gale Young"
            ],
            "title": "The approximation of one matrix by another of lower rank",
            "year": 1936
        },
        {
            "authors": [
                "Charles Fefferman",
                "Sanjoy Mitter",
                "Hariharan Narayanan"
            ],
            "title": "Testing the manifold hypothesis",
            "venue": "Journal of the American Mathematical Society,",
            "year": 2016
        },
        {
            "authors": [
                "Partha Ghosh",
                "Mehdi SM Sajjadi",
                "Antonio Vergari",
                "Michael Black",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "From variational to deterministic autoencoders",
            "venue": "arXiv preprint arXiv:1903.12436,",
            "year": 2019
        },
        {
            "authors": [
                "Henry Gouk",
                "Eibe Frank",
                "Bernhard Pfahringer",
                "Michael J Cree"
            ],
            "title": "Regularisation of neural networks by enforcing lipschitz continuity",
            "venue": "Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Li Jing",
                "Jure Zbontar"
            ],
            "title": "Implicit rank-minimizing autoencoder",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Mark A Kramer"
            ],
            "title": "Nonlinear principal component analysis using autoassociative neural networks",
            "venue": "AIChE journal,",
            "year": 1991
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Vinod Nair",
                "Geoffrey Hinton"
            ],
            "title": "The cifar-10 dataset",
            "venue": "online: http://www. cs. toronto. edu/kriz/cifar. html,",
            "year": 2014
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Communications of the ACM,",
            "year": 2017
        },
        {
            "authors": [
                "Quoc V Le",
                "Jiquan Ngiam",
                "Adam Coates",
                "Abhik Lahiri",
                "Bobby Prochnow",
                "Andrew Y Ng"
            ],
            "title": "On optimization methods for deep learning",
            "venue": "In Proceedings of the 28th International Conference on International Conference on Machine Learning,",
            "year": 2011
        },
        {
            "authors": [
                "Honglak Lee",
                "Alexis Battle",
                "Rajat Raina",
                "Andrew Ng"
            ],
            "title": "Efficient sparse coding algorithms",
            "venue": "Advances in neural information processing systems,",
            "year": 2006
        },
        {
            "authors": [
                "John Lee"
            ],
            "title": "Introduction to topological manifolds, volume 202",
            "venue": "Springer Science & Business Media,",
            "year": 2010
        },
        {
            "authors": [
                "Ziwei Liu",
                "Ping Luo",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Takeru Miyato",
                "Toshiki Kataoka",
                "Masanori Koyama",
                "Yuichi Yoshida"
            ],
            "title": "Spectral normalization for generative adversarial networks",
            "venue": "arXiv preprint arXiv:1802.05957,",
            "year": 2018
        },
        {
            "authors": [
                "Michael Moor",
                "Max Horn",
                "Bastian Rieck",
                "Karsten Borgwardt"
            ],
            "title": "Topological autoencoders",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Philipp Nazari",
                "Sebastian Damrich",
                "Fred A Hamprecht"
            ],
            "title": "Geometric autoencoders-what you see is what you decode",
            "year": 2023
        },
        {
            "authors": [
                "Andrew Ng"
            ],
            "title": "Sparse autoencoder",
            "venue": "CS294A Lecture notes,",
            "year": 2011
        },
        {
            "authors": [
                "Andrew Y Ng"
            ],
            "title": "Feature selection, l 1 vs. l 2 regularization, and rotational invariance",
            "venue": "In Proceedings of the twenty-first international conference on Machine learning,",
            "year": 2004
        },
        {
            "authors": [
                "Bruno A Olshausen",
                "David J Field"
            ],
            "title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images",
            "year": 1996
        },
        {
            "authors": [
                "Karl Pearson"
            ],
            "title": "Liii. on lines and planes of closest fit to systems of points in space",
            "venue": "The London, Edinburgh, and Dublin philosophical magazine and journal of science,",
            "year": 1901
        },
        {
            "authors": [
                "Chi-Hieu Pham",
                "Sa\u00efd Ladjal",
                "Alasdair Newson"
            ],
            "title": "Pca-ae: Principal component analysis autoencoder for organising the latent space of generative networks",
            "venue": "Journal of Mathematical Imaging and Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Elad Plaut"
            ],
            "title": "From principal subspaces to principal components with linear autoencoders",
            "venue": "arXiv preprint arXiv:1804.10253,",
            "year": 2018
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Marc\u2019Aurelio Ranzato",
                "Y-Lan Boureau",
                "Yann Cun"
            ],
            "title": "Sparse feature learning for deep belief networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2007
        },
        {
            "authors": [
                "Oren Rippel",
                "Michael Gelbart",
                "Ryan Adams"
            ],
            "title": "Learning ordered representations with nested dropout",
            "venue": "In International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "Robert Tibshirani"
            ],
            "title": "Regression shrinkage and selection via the lasso",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological),",
            "year": 1996
        },
        {
            "authors": [
                "Ilya Trofimov",
                "Daniil Cherniavskii",
                "Eduard Tulchinskii",
                "Nikita Balabin",
                "Evgeny Burnaev",
                "Serguei Barannikov"
            ],
            "title": "Learning topology-preserving data representations",
            "venue": "arXiv preprint arXiv:2302.00136,",
            "year": 2023
        },
        {
            "authors": [
                "Aaron Van Den Oord",
                "Oriol Vinyals"
            ],
            "title": "Neural discrete representation learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "LEE Yonghyeon",
                "Sangwoong Yoon",
                "MinJun Son",
                "Frank C Park"
            ],
            "title": "Regularized autoencoders for isometric representation learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Learning data representation is crucial to machine learning (Bengio et al., 2013). On one hand, a good representation can distill the primary features from the data samples, thus enhancing downstream tasks such as classification (Krizhevsky et al., 2017; Simonyan & Zisserman, 2014; He et al., 2016). On the other hand, when the data representation lies in a low dimensional latent space Z and can be mapped backward to the data samples via some decoder g, we can considerably facilitate generative tasks by training generative models in Z (Ramesh et al., 2021; Rombach et al., 2022).\nBut what makes a data representation\u2014i.e., Z = e(X ) \u2282 Z of a dataset X\u2014good? Often, a low dimensional Z is preferred. It is frequently hypothesized that a real world dataset X in high dimensional data space X only resides on a low dimensional manifold (Fefferman et al., 2016) (or at least most part of X is locally Euclidean of low dimension), hence due to the rank theorem (Lee, 2012), X \u2019s low dimensionality will be inherited by its latent set Z through an at least piecewise smooth and constant rank encoder e (i.e., Z is low dimensional even if Z is high dimensional). Therefore, for a Z \u2282 Z retaining sufficient information about X , a low dimensional Z can provide several advantages. First, it can improve the efficacy of downstream tasks by aligning its latent dimensions more with the informative dimensions of Z and alleviating the curse of dimensionality. In addition, it can increase the robustness of tasks such as data generation. Specifically, if a subset U \u2286 Z constitutes an n-D manifold that can be embedded in an n-D Euclidean space, then due to the invariance of domain (Bredon, 2013), U is open in Z as long as Z is n-D. Thanks to the basis criterion (Lee, 2010), this openness means for a conditional generative model trained in such Z, if its one prediction z\u0302 is not far away from its target z \u2208 U , then z\u0302 will also fall inside U \u2286 Z , thus still be mapped back into X by g rather than falling outside it. Moreover, in the case where Z is a manifold that cannot embed in similar dimensional Euclidean space, or where Z is not even a manifold, retrieving the least dimensional Z needed to embed Z may pave the way for studying the complexity and topology of Z and X . Additionally, people also hope the latent representation can indicate the amount of information each dimension has (Rippel et al., 2014; Pham et al., 2022), so that the data variations along the principal dimensions can be easily studied, and trade-offs can be easily made when it is necessary to strip off less informant latent dimensions due to computational cost. This is why PCA (Pearson, 1901), despite being a century old, is still widely applied to different areas, even if it is a simple linear model.\nTo automatically learn a both low dimensional and ordered nonlinear latent representation, in this work we introduce the Least Volume regularization, which is based on the intuition that packing a flat paper into a box consumes much less space than a curved one. This paper\u2019s contributions are:\n1. We introduce Least Volume (LV) regularization for autoencoders (AEs) that can compress the latent set into a low dimensional latent subspace spanned by the latent space\u2019s standard basis. We show that upper bounding the decoder\u2019s Lipschitz constant is the key to making it work, and verify its necessity with an ablation study.\n2. We prove that PCA is exactly a special case of least volume\u2019s application on a linear autoencoder. In addition, we show that just like in PCA, there is a close-to-1 positive correlation between the latent dimension\u2019s degree of importance and standard deviation.\n3. We apply least volume to several benchmark problems including a synthetic dataset with known dimensionality, MNIST and CIFAR-10, and show that the volume penalty is more effective than the traditional regularizer Lasso in terms of compressing the latent set. We make the code public on GitHub1 to ensure reproducibility."
        },
        {
            "heading": "2 METHODOLOGY AND INTUITION",
            "text": "As proved later in Theorem 2, if a continuous autoencoder can reconstruct the dataset perfectly, then its latent set is a homeomorphic copy of it. Concretely speaking, if through the lens of the manifold hypothesis we conceive the dataset as an elastic curved surface in the high dimensional data space, then the latent set can be regarded as an intact \u201cflattened\u201d version of it tucked into the low dimensional latent space by the encoder. Therefore, the task of finding the least dimensional latent space can be imagined as the continuation of this flattening process, in which we keep compressing this elastic latent surface onto latent hyperplanes of even lower dimensionality until it cannot be flattened anymore. Thereafter, we can extract the final hyperplane as the desired least dimensional latent space. Ideally, we prefer a hyperplane that is either perpendicular or parallel to each latent coordinate axis, such that we can extract it with ease."
        },
        {
            "heading": "2.1 VOLUME PENALTY",
            "text": "To flatten the latent set and align it with the latent coordinate axes, we want the latent code\u2019s standard deviation (STD) vector \u03c3 to be as sparse as possible, which bespeaks the compression of the dataset onto a latent subspace of the least dimension. The common penalty for promoting sparsity is the L1 norm. However, \u2225\u03c3\u22251 does not necessarily lead to flattening, as we will discuss later in \u00a7A.1.1. An alternative regularizer is \u220f i \u03c3i\u2014the product of all elements of the latent code\u2019s STD vector \u03c3 . We call this the volume penalty. It is based on the intuition that a curved surface can only be enclosed by a cuboid of much larger volume than a cuboid that encloses its flattened counterpart. The cuboid has its sides parallel to the latent coordinate axes (as shown in Fig. 1) so that when its volume is minimized, the flattened latent set inside is also aligned with these axes. To evaluate the cuboid\u2019s volume, we can regard the STD of each latent dimension as the length of each side of this cuboid. The cuboid\u2019s volume reaches zero only when one of its sides has zero length (i.e., \u03c3i = 0), indicating that the latent surface is compressed into a linear subspace. Conceptually, we can then extract this subspace as the new latent space and continue performing this flattening process recursively until the latent set cannot be flattened any more in the final latent space.\nIn practice, though, this recursive process is troublesome to implement, whereas directly minimizing the volume may induce vanishing gradients when several \u03c3i are marginal, hindering flattening. Therefore, we supplement each \u03c3i with an amount \u03b7 \u2265 0 to make the volume penalty become\u220f\ni(\u03c3i + \u03b7). To avoid extremely large gradients when the latent dimension m is large, we implement instead equivalently minimizing the geometric mean m \u221a\u220f i(\u03c3i + \u03b7), which we evaluate using the ExpMeanLog trick to avoid numerical issues."
        },
        {
            "heading": "2.2 LIPSCHITZ CONTINUITY REGULARIZATION ON DECODER",
            "text": "Mechanically encouraging such latent sparsity, however, can lead to a trivial solution where we drive all latent STDs close to zero without learning anything useful. This can occur when the elastic latent surface shrinks itself isotropically to na\u00efvely shorten all of the enclosing cuboid\u2019s sides, without\n1https://github.com/IDEALLab/Least_Volume_ICLR2024\nfurther flattening the latent surface. To forestall this arbitrary shrinking that causes the trivial solution, we need to properly regularize the autoencoder\u2019s latent set\u2019s elasticity.\nThe appropriate regularization turns out to be the Lipschitz continuity of the decoder. The intuition is as follows. In the trivial solution, the encoder would na\u00efvely compresses all latent STDs to almost zero. The decoder would then fight the encoder\u2019s shrinking by scaling up its network weights such that the now small perturbations in the latent input still induce large enough output variations in data space to achieve low reconstruction error. To do this, however, the decoder must possess a large Lipschitz constant (K) by definition. If instead, the decoder had a small bounded Lipschitz constant, then any latent dimension with STD close to zero must also have close-to-zero variation in the data space due to the small K and thus correspond to a dimension perpendicular to the data surface. Meanwhile, a principal data dimension must retain a large variation in the latent space, as otherwise, the decoder would need to violate its bounded Lipschitz constant to achieve low reconstruction error. This bounded Lipschitz constant on the decoder thereby prevents the encoder from arbitrarily collapsing the latent codes to the trivial solution. Figure 1 illustrates this intuition. Ghosh et al. (2019) imposed Lipschitz constraint on the decoder under a similar motivation.\nTo achieve this regularization, Lipschitz continuity can be conveniently hard-constrained via spectral normalization (Miyato et al., 2018), namely normalizing the spectral norm (the maximum singular value) of all linear layers to 1. With 1-Lipschitz functions such as LeakyReLU as activation functions, the Lipschitz constant of the decoder is guaranteed to be not greater than 1. For accurate regularization of any convolutional layers, we employ the power method in (Gouk et al., 2021)."
        },
        {
            "heading": "2.3 LEAST VOLUME FORMULATION",
            "text": "In summary, for an unsupervised representation learning problem in which the latent set Z = e\u03b8(X ) is required to preserve enough information of a dataset X\u2014as per the criterion that it minimizes a reconstruction loss function J(g\u03b8(Z),X ), conceptually the Least Volume (LV) problem is:\nargmin \u03b8\nLvol(Z) = Lvol(e\u03b8(X )) (1)\ns.t. Z \u2208 Z\u22c6 := {Z = e\u03b8(X ) | \u03b8 minimizes J(g\u03b8 \u25e6 e\u03b8(X ),X )} (2) \u2225g\u03b8(z1)\u2212 g\u03b8(z2)\u2225 \u2264 K\u2225z1 \u2212 z2\u2225, \u2200{z1, z2} \u2286 Z (3)\nObserve that for any homeomorphism h\u2014a bijective continuous function whose inverse is also continuous\u2014the new latent set h(Z) = (h\u25e6e\u03b8)(X ), which is homeomorphic to Z , is equivalent to Z in the sense that g\u03b8 \u25e6 e\u03b8(x) = (g\u03b8 \u25e6 h\u22121) \u25e6 (h \u25e6 e\u03b8)(x). Therefore, as long as g\u03b8 and e\u03b8 have enough complexity to respectively represent g\u03b8 \u25e6 h\u22121 and h \u25e6 e\u03b8 for a given set H of h, each homeomorphic latent set h(Z) with h \u2208 H must also be an optimal solution to J , thus residing in Z\u22c6. Hence, the more complexity g\u03b8 and e\u03b8 have, the larger the set H is, thus a more complicated homeomorphism h\nwe can obtain to flatten Z more. In reality, though, due to the complexity of this optimization, we need to resort to the weighted sum objective\nL = J + \u03bb \u00b7 Lvol (4) to derive an approximate solution, where \u03bb needs to be fine-tuned."
        },
        {
            "heading": "3 THEORETICAL ANALYSIS",
            "text": "Given the least volume formulation, in this section we formalize its surface-flattening intuition with theoretical analysis, inspect the introduced variables\u2019 properties and their relationships, and demonstrate its equivalency to PCA in the linear special case, before moving onto our empirical results in \u00a75. For space reasons, many of the proof details are placed in \u00a7A of the appendix."
        },
        {
            "heading": "3.1 WHAT EXACTLY DO WE MEAN BY VOLUME?",
            "text": "The volume \u220f\ni \u03c3i of a latent set is the square root of the diagonal product of the covariance matrix S of the latent codes. Apart from this trivial fact, below we show that its square is the tight upper bound of the latent determinant detS, which is referred to as Generalized Variance (GV) in some literature. Theorem 1. The square of volume\u2014i.e., \u220f i \u03c3 2 i \u2014is a tight upper bound of the latent determinant\ndetS. If detS is lower bounded by a positive value, then \u220f\ni \u03c3 2 i is minimized down to detS if and\nonly if S is diagonal.\nTheorem 1 can lead to some interesting results that we will see in \u00a73.4. Its proof in given in \u00a7A.1. Here the positive lower bound of detS is an unknown inherent constant determined by the dataset, and it originates from the intuition that when the latent space reduces into the least dimensional one in which Z cannot be flattened anymore, then detS cannot be zero, as otherwise it suggests Z resides in a linear latent subspace and creates contradiction. Nor should detS be arbitrarily close to 0, as the K-Lipschitz decoder prevents degenerate shrinkage. As mentioned in \u00a72.1, in practice we instead minimize the supplemented geometric mean m \u221a\u220f i(\u03c3i + \u03b7) with \u03b7 \u2265 0 to avoid the recursive subspace extraction process in conception. Minimizing this variable not only equivalently minimizes an upper bound of the volume, but also naturally interpolates between m \u221a\u220f i \u03c3i and 1 m\u2225\u03c3\u22251 gradient-wise, given that as \u03b7 \u2192 \u221e:\n\u2207\u03b8 m \u221a\u220f\ni\n(\u03c3i + \u03b7) = 1\nm \u2211 i\nm \u221a\u220f\ni(\u03c3i + \u03b7)\n\u03c3i + \u03b7 \u00b7 \u2207\u03b8\u03c3i \u2212\u2192\n1\nm \u2211 i \u2207\u03b8\u03c3i = \u2207\u03b8 1 m \u2225\u03c3\u22251 (5)\nSo we can seamlessly shift from volume penalty to L1 penalty by increasing \u03b7. We can see that the lower the \u03b7, the more the regularizer\u2019s gradient prioritizes minimizing smaller \u03c3i, which intuitively should make \u03c3 sparser than the L1 penalty does that scales every \u2207\u03b8\u03c3i equally. We shall see in Section 5 that m \u221a\u220f i(\u03c3i + \u03b7) is indeed more efficient. For a further simple pedagogical example where minimizing the volume produces sparser and more compact latent sets than the L1 norm, we refer readers to \u00a7A.1.1 in the appendix. More relevant discussions can be found in \u00a7C."
        },
        {
            "heading": "3.2 AN ERRORLESS CONTINUOUS AUTOENCODER LEARNS A TOPOLOGICAL EMBEDDING",
            "text": "It is pointless to only minimize the volume, given that we can always reduce it to 0 by trivially encoding every data sample to the same latent point. To make the \u201cflattening\u201d process in \u00a72 meaningful, we need to ensure the autoencoder has good reconstruction performance over the dataset, such that the latent set is a low dimensional replica of the dataset that preserves useful topological information. This is justified by the following propositions. Their proofs are given in \u00a7A.2. Lemma 1. If \u2200x \u2208 X , g \u25e6 e(x) = x, then both e and g are bijective between X and Z = e(X ).\nAlthough some regard an errorless autoencoder as \u201ctrivial\u201d since it only learns an one-to-one representation, it is actually not as trivial as they think, thanks to the continuity of the autoencoder. Theorem 2. A continuous autoencoder between the data space X = Rn and the latent space Z = Rm with the norm-based reconstruction error \u2225g \u25e6 e(x)\u2212 x\u2225 = 0 everywhere on the dataset X \u2286 X learns a topological embedding of the dataset. In other words, the latent set Z = e(X ) \u2286 Z is a homeomorphic copy of the dataset.\nBecause Z is a homeomorphic copy of X , X \u2019s important topological properties like connectedness and compactness\u2014which are invariant under homeomorphism\u2014are preserved on Z . This means if some of these invariant properties are the only information we rely on when analyzing X , then analyzing Z is equivalent to directly analyzing X . For instance, if we want to know how many disconnected components X has, or evaluate its local dimensionality, then theoretically we can derive the same results from Z . Of course, Z becomes more efficient to analyze if it resides in a low dimensional linear subspace easy to extract. Corollary 2.1. If the above errorless autoencoder\u2019s latent set has the form Z = Z \u2032 \u00d7 c where c \u2208 Rp is constant and Z \u2032 \u2286 Z \u2032 = Rm\u2212p, then \u03c0 \u25e6 e|X is also a topological embedding of X , where \u03c0 : Z \u2032 \u00d7 c \u2192 Z \u2032 is the projection map.\nCorollary 2.1 suggests that if we can force the errorless autoencoder\u2019s latent set to assume such a form while increasing the vector c\u2019s dimension p as much as possible, we can obtain a latent space that is the lowest dimensional Euclidean space that can still correctly embed the dataset. If achieved, the resulting low dimensional latent space is not only more efficient to analyze, but also provides useful information about the topology of X . For instance, not every smooth m dimensional manifold can be embedded in Rm (e.g., sphere and Klein bottle), but the Whitney Embedding Theorem (Lee, 2012) states that it can be embedded in R2m. Thus if X is a smooth manifold, then obtaining the least dimensional Z through a smooth autoencoder can provide a lower bound of X \u2019s dimensionality. In practice, the autoencoder\u2019s ideal everywhere-errorless-reconstruction is enforced by minimizing the mean reconstruction error \u03f5 = E\u2225g \u25e6 e(x)\u2212 x\u2225, yet due to inevitable numerical errors and data noise that should be ignored by the autoencoder reconstruction, \u03f5 cannot strictly reach zero after optimization. We can only empirically set a pragmatic tolerance \u03b4 for it, and regard the autoencoder as having approximately learned the topological embedding when \u03f5 < \u03b4. Likewise, numerically the latent set cannot strictly take the form in Corollary 2.1, but rather only at best has marginal STDs in several latent dimensions. Intuitively, we may consider the latent set as flattened into a thin plane and discard these nearly-constant latent dimensions to obtain a latent set of lower dimensionality with \u03c0. But how do we properly trim off these dimensions? And more importantly, is it safe?"
        },
        {
            "heading": "3.3 IS IT SAFE TO PRUNE TRIVIAL LATENT DIMENSIONS?",
            "text": "After training an autoencoder with least volume, we can expect several latent dimensions learned by the encoder e to have STDs close to zero. Although such latent set Z is not exactly in the form of Z \u2032 \u00d7 c in Corollary 2.1 for us to apply \u03c0, we can still regarded its dimensions of marginal STDs as trivial latent dimensions, in the sense that pruning them\u2014i.e., fixing their value at their mean\u2014will not induce much difference to the decoder g\u2019s reconstruction, provided that g\u2019s Lipschitz constant is not large. This is justified by the following theorem, whose proof is presented in \u00a7A.3. Theorem 3. Suppose the STD of latent dimension i is \u03c3i. If we fix zi at its mean z\u0304i for each i \u2208 P where P is the set of indices of the dimensions we decide to prune, then the L2 reconstruction error\nof the autoencoder with a K-Lipschitz decoder g increases by at most K \u221a\u2211\ni\u2208P \u03c3 2 i .\nThe pruned Z\u0303 is then of the very form Z\u0303 = Z \u2032 \u00d7 c and can be fed to \u03c0 to extract Z \u2032. The inverse \u03c0\u22121 helps map Z \u2032 back into the data space through g \u25e6 \u03c0\u22121 without inducing large reconstruction error. This theorem also supports our intuition about why having a Lipschitz continuous decoder is necessary for learning a compressed latent subspace\u2014for small K, a latent dimension of near-zero STD cannot correspond to a principal dimension of the data manifold. In contrast, in the absence of this constraint, the decoder may learn to scale up K to align near-zero variance latent dimensions with some of the principal dimensions."
        },
        {
            "heading": "3.4 RELATIONSHIP TO PCA AND THE ORDERING EFFECT",
            "text": "Surprisingly, as shown in \u00a7A.4, one can prove that PCA is a linear special case of least volume. This implies there could be more to volume minimization than just flattening the surface. Proposition 1. An autoencoder recovers the principal components of a dataset X after minimizing the volume, if we:\n1. Make the encoder e(x) = Ax+ a and the decoder g(z) = Bz + b linear maps,\n2. Force the reconstruction error \u2225g \u25e6 e(X)\u2212X\u2225F to be strictly minimized,\n3. Set the Lipschitz constant to 1 for the decoder\u2019s regularization,\n4. Assume the rank of the data covariance Cov(X) is not less than the latent space dimension m.\nSpecifically, B\u2019s columns consist of the principal components of X , while A acts exactly the same as B\u22a4 on X . When X has full row rank, A = B\u22a4.\nThe proof of Proposition 1 in \u00a7A.4 suggests that the ordering effect of PCA\u2014i.e., the magnitude of its each latent dimension\u2019s variance indicates each latent dimension\u2019s degree of importance\u2014is at least partially a result of reducing the latent determinant until the singular values of B reach their upper bound 1, such that g becomes isometric and preserves distance. In other words, this means e is not allowed to scale up the dataset X along any direction in the latent space if it is unnecessary, as it will increase the latent determinant, and thus the volume. Therefore, likewise, although Theorem 3 does not prevent a latent dimension of large STD from being aligned to a trivial data dimension, we may still expect minimizing the volume or any other sparsity penalties to hinder this unnecessary scaling by the encoder e, given that it increases the penalty\u2019s value. This suggests that the latent dimension\u2019s importance in the data space should in general scale with its latent STD.\nTo investigate if least volume indeed induces a similar importance ordering effect for nonlinear autoencoders, we need to first quantify the importance of each latent dimension. This can be done by generalizing the Explained Variance used for PCA, after noticing that it is essentially Explained Reconstruction: Proposition 2. Let \u03bbi be the eigenvalues of the data covariance matrix. The explained variance R({i}) = \u03bbi\u2211n\nj \u03bbj of a given latent dimension i of a linear PCA model is the ratio between\nE\u2225\u00b7\u222522({i})\u2014the MSE reconstruction error induced by pruning this dimension (i.e., setting its value to its mean 0) and E\u2225\u00b7\u222522(\u2126)\u2014the one induced by pruning all latent dimensions, where \u2126 denotes the set of all latent dimension indices.\nSo for PCA, the explained variance actually measures the contribution of each latent dimension in minimizing the MSE reconstruction error E\u2225x\u2212 x\u0302\u222522 as a percentage. Since for a nonlinear model the identity R({i}) +R({j}) = R({i, j}) generally does not hold, there is no good reason to stick with the MSE. It is then natural to extrapolate R and E to our nonlinear case by generalizing E\u2225\u00b7\u222522(P ) to ED(P ), i.e., the induced reconstruction error w.r.t. metric D after pruning dimensions in P :\nED(P ) = E[D(x\u0303P , x)]\u2212 \u03f5 = E[D(x\u0303P , x)]\u2212 E[D(x\u0302, x)] (6) Then RD(P )\u2014the explained reconstruction of latent dimensions in P w.r.t. metric D\u2014can be defined as RD(P ) = ED(P )/ED(\u2126). More details about this extrapolation can be found in Remark 2.1. For our experiments, we choose L2 distance as D, i.e., L2(x\u0303P , x) = \u2225x\u0303P \u2212 x\u22252, and measure the Pearson correlation coefficient (PCC) between the latent STD and RL2({i}) to see if there is any similar ordering effect. If this coefficient is close to 1, then the latent STD can empirically serve as an indicator of the importance of each latent dimension, just like in PCA."
        },
        {
            "heading": "4 RELATED WORKS AND LIMITATIONS",
            "text": "The usual way of encouraging the latent set to be low dimensional is sparse coding (Bengio et al., 2013), which applies sparsity penalties like Lasso (Tibshirani, 1996; Ng, 2004; Lee et al., 2006), Student\u2019s t-distribution (Olshausen & Field, 1996; Ranzato et al., 2007), KL divergence (Le et al., 2011; Ng et al., 2011) etc., over the latent code vector to induce as many zero-features as possible. However, as discussed in \u00a72.3, a latent representation transformed by a homeomorphism h is equivalent to the original one in the sense that g\u03b8 \u25e6 e\u03b8(x) = (g\u03b8 \u25e6 h\u22121) \u25e6 (h \u25e6 e)(x), so translating the sparse-coding latent set arbitrarily\u2014which makes the zero-features no longer zero\u2014provides us an equally good representation. This equivalent latent set is then one that has zero-STDs along many latent dimensions, which is what this work tries to construct. Yet h is not restricted to translation. For instance, rotation is also homeomorphic, so rotating that flat latent set also gives an equivalently good representation. IRMAE (Jing et al., 2020) can be regarded as a case of this, in which the latent\nset is implicitly compressed into a linear subspace not necessarily aligned with the latent coordinate axes. There are also stochastic methods like K-sparse AE (Makhzani & Frey, 2013) that compress the latent set by randomly dropping out inactive latent dimensions during training.\nPeople also care about the information content each latent dimension contains, and hope the latent dimensions can be ordered by their degrees of importance. Nested dropout (Rippel et al., 2014) is a probabilistic way that makes the information content in each latent dimension decrease as the latent dimension index increases. PCA-AE (Pham et al., 2022) achieves a similar effect by gradually expanding the latent space while reducing the covariance loss. Our work differs given that least volume is deterministic, requiring no multistage training, and we only require the information content to decrease with the STD of the latent dimension instead of the latent dimension\u2019s index number. Some researchers have also investigated the relationship between PCA and linear autoencoders (Rippel et al., 2014; Plaut, 2018; Kramer, 1991). In recent years more researchers started to look into preserving additional information on top of topological properties in the latent space (Moor et al., 2020; Trofimov et al., 2023; Gropp et al., 2020; Chen et al., 2020; Yonghyeon et al., 2021; Nazari et al., 2023), such as geometric information. A detailed methodological comparison between least volume and some aforementioned methods is given in Table 1 to illustrate its distinction.\nIt is worth noting that this work is discussed under the scope of continuous autoencoders. Recently VQ-VAE (Van Den Oord et al., 2017) has attracted a lot of attention for its efficacy. Although it is modelled by continuous neural networks, its encoding operation is not continuous, because mapping the encoder\u2019s output to the nearest embedding vector is discontinuous. Hence this work cannot be readily applied to it. More discussions about LV\u2019s other technical limitations are included in \u00a7C.1."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we demonstrate the primary experiment results that reflect the properties and performance of Least Volume. We examine LV\u2019s dimension reduction and ordering effect on benchmark image datasets, and conduct ablation study on the volume penalty and the decoder\u2019s Lipschitz regularization. Due to the space limit, we instead present in \u00a7B of the appendix more details about these experiments, additional examinations, applications to downstream tasks and some toy problems for illustrating the effect of LV."
        },
        {
            "heading": "5.1 IMAGE DATASETS",
            "text": "In this experiment, we compare the performance of different latent regularizers that have the potential of producing a compressed latent subspace. Specifically, these four regularizers are: L1 norm of the latent code (denoted by \u201classo\u201d), L1 norm of the latent STD vector (denoted by \u201cl1\u201d), volume penalty Lvol with \u03b7 = 1 (denoted by \u201cvol\u201d or \u201cvol_e1.0\u201d) and the one in (Ranzato et al., 2007) based on student\u2019s t-distribution (denoted by \u201cst\u201d). We activate the spectral normalization on the decoder and apply these four regularizers to a 5D synthetic image dataset (detailed in \u00a7B.2.2 of the appendix), the MNIST dataset (Deng, 2012) and the CIFAR-10 dataset (Krizhevsky et al., 2014), then investigate how good they are at reducing the latent set\u2019s dimensionality without sacrificing reconstruction performance, and whether there is any high correlation between the latent STD and the degree of importance (We also additionally perform these experiments on the CelebA dataset (Liu et al., 2015), which is included in the appendix \u00a7B.2.1 for space reasons). For these image datasets,\nwe use latent spaces of 50, 128, and 2048 dimensions respectively. For each case, we then conduct the experiment over a series of \u03bb ranging from 0.03 to 0.0001 with three cross validations and use the result\u2019s STD as the error bars. We normalize all pixels values between 0 and 1 and use binary cross entropy as the reconstruction loss J for all experiments, not only because in practice it has faster convergence rate than the usual MSE loss, but also because minimizng it in the [0, 1] range is equivalent to minimizing the other norm-based reconstruction loss. All experiments are performed on NVIDIA A100 SXM GPU 80GB. More tips regarding hyperparameter tuning, model training and usage can be found in \u00a7C of the appendix.\nComparing these regularizers\u2019 dimension reduction performance is not a straightforward task. This is because each latent regularizer R, when paired with the reconstruction loss J via L = J + \u03bb \u00b7 R, creates its own loss landscape, such that the same \u03bb leads to different J for different R after optimizing L. This means under the same \u03bb, some regularizers may choose to compress the latent set more simply by sacrificing the reconstruction quality more, so we cannot compare these regularizers under the same \u03bb. However, because adjusting \u03bb is essentially trading off between reconstruction and dimension reduction, we can plot the dimension reduction metric against the reconstruction metric for comparison. An efficient R should compress the latent set more for the same reconstruction quality. Figure 2 plots the latent set dimension against the autoencoder\u2019s L2 reconstruction error, where the latent set dimension is the number of dimensions left after cumulatively pruning the latent dimensions with the smallest STDs until their joint explained reconstruction exceeds 1%. We can see that for all datasets, the volume penalty Lvol always achieves the highest compression when the L2 reconstruction error is reasonable (empirically that means < 2. See \u00a7B.3.1 in the appendix).\nFor each dataset, we select three autoencoding cases with comparable reconstruction quality respectively for the four regularizers (marked by large dots in Fig. 2). Then in Fig. 3, for each case we sort the latent dimensions in descending order of STD value, and plot their individual explained reconstructions against their ordered indices. For all regularizers, we see that the explained L2 reconstruction generally increases with the latent STD, although it is not perfectly monotonic. Nevertheless, we can conclude that the explained reconstruction is highly correlated with latent STD, since the PCCs are all close to 1 (except for \u201cst\u201d based on student\u2019s t-distribution, which has a sudden drop as the dimension index reach 0), as shown in Fig. 3."
        },
        {
            "heading": "5.2 ABLATION STUDY",
            "text": "We first investigate the necessity of the Lipschtiz regularization on the decoder. For each experiment we keep everything the same except switching off the spectral normalization. Figure 4 shows that without Lipschitz regularization, the latent set\u2019s dimensionality cannot be effectively reduced. This is indeed due to the isotropic shrinkage as mentioned in \u00a72.2, as the latent dimensions of the autoencoders without Lipschitz regularization have orders of magnitude lower STD values (see \u00a7B.3.4). In addition, Fig. 5 shows that apart from the Lipschitz regularization, the regularization on latent set is also necessary for reducing the latent set\u2019s dimension. The Lipschitz regularization alone does not induce any dimension reduction effect."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "This work introduces Least Volume regularization for continuous autoencoders. It can compress the latent representation of a dataset into a low dimensional latent subspace for easy extraction. We show that minimizing the volume is equivalent to minimizing an upper bound of the latent code\u2019s generalized variance, and that using a K-Lipschitz decoder prevents the latent set\u2019s isotropic shrinkage. This approach achieves better dimension reduction results than several L1 distance-based counterparts on multiple benchmark image datasets. We further prove that PCA is a linear special case of the least volume formulation, and when least volume is applied to nonlinear autoencoders, it demonstrates a similar ordering effect, such that the STD of each latent dimension is highly correlated with the latent dimension\u2019s importance."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We acknowledge the support from the National Science Foundation through award #1943699 as well as ARPA-E award DE-AR0001216."
        },
        {
            "heading": "A PROOFS AND REMARKS FOR \u00a73",
            "text": ""
        },
        {
            "heading": "A.1 PROOFS FOR \u00a73.1",
            "text": "Theorem 1. The square of volume\u2014i.e., \u220f\ni \u03c3 2 i \u2014is a tight upper bound of the latent determinant detS. If detS is lower bounded by a positive value, then \u220f\ni \u03c3 2 i is minimized down to detS if and\nonly if S is diagonal.\nProof. The diagonal entries of positive semi-definite matrices are always non-negative, and Cholesky decomposition states that any real positive definite matrix S can be decomposed into S = LL\u22a4 where L is a real lower triangular matrix with positive diagonal entries. So detS = (detL)2 =\u220f\ni[L] 2 ii \u2264 \u220f i \u2225li\u222522 = \u220f i[S]ii = \u220f i \u03c3 2 i , where li is the ith row of L.\nHence for a positive definite S, the inequality can only be reduced to equality when L is diagonal, which in turn makes S diagonal."
        },
        {
            "heading": "A.1.1 EXAMPLE: PITFALL OF L1",
            "text": "We can use a simple example to show that the volume is better than L1-based regularizers in terms of determining the sparsity of \u03c3 , i.e., how flat the latent set is. Figure A.1 shows a straight orange line Zl of length 32\u03c0 and a blue arc Za of the same length. Clearly there exist an isometry h : Zl \u2192 Za. Suppose the arc is a latent set created through Za = e(X ), then Zl = h\u22121 \u25e6 e(X ) is an equivalently good one, provided that the encoder e has enough complexity to learn h\u22121 \u25e6 e. Moreover, because the isometry h is 1-Lipschitz, then a K-Lipschitz decoder g with enough complexity can also learn g \u25e6 h since this is also K-Lipschitz, so both Zl and Za are latent sets that can be produced by e and g with enough complexity. Now we introduce uniform distribution over these two sets and evaluate \u2225\u03c3\u22251 and \u220f i \u03c3i on them. Figure A.1 shows that only the volume \u220f i \u03c3i correctly tells the line Zl is flatter."
        },
        {
            "heading": "A.2 PROOFS FOR \u00a73.2",
            "text": "Lemma 1. If \u2200x \u2208 X , g \u25e6 e(x) = x, then both e and g are bijective between X and Z = e(X ).\nProof. e|X is injective because for any pair {x, x\u2032} \u2286 X that satisfies e(x) = e(x\u2032), we have x = g \u25e6 e(x) = g \u25e6 e(x\u2032) = x\u2032. It is then bijective because e|X is surjective onto Z by definition. g|Z is surjective onto X because g(Z) = g \u25e6 e(X ) = X . It is then injective thus bijective because \u2200z = e(x) \u2208 Z , e \u25e6 g(z) = e \u25e6 g \u25e6 e(x) = e(x) = z.\nTheorem 2. A continuous autoencoder between the data space X = Rn and the latent space Z = Rm with the norm-based reconstruction error \u2225g \u25e6 e(x)\u2212 x\u2225 = 0 everywhere on the dataset X \u2286 X learns a topological embedding of the dataset. In other words, the latent set Z = e(X ) \u2286 Z is a homeomorphic copy of the dataset.\nProof. Due to the positive definiteness of norm and Lemma 1, both the encoder restriction e|X and the decoder restriction g|Z are bijective functions between X and Z . Because both the encoder e : X \u2192 Z and the decoder g : Z \u2192 X are continuous, their restrictions are also continuous (in terms of the subspace topology). Since g|Z is then the continuous inverse function of e|X , by definition e|X is a topological embedding (Lee, 2010).\nCorollary 2.1. If the above errorless autoencoder\u2019s latent set has the form Z = Z \u2032 \u00d7 c where c \u2208 Rp is constant and Z \u2032 \u2286 Z \u2032 = Rm\u2212p, then \u03c0 \u25e6 e|X is also a topological embedding, where \u03c0 : Z \u2032 \u00d7 c \u2192 Z \u2032 is the projection map.\nProof. \u03c0 is homeomorphic because it is bijective, while continuous and open in terms of the product topology. Thus \u03c0 \u25e6 e|X is also homeomorphic onto its image Z \u2032.\nRemark. Because Z \u2032 is homeomorphic to X , the subspace dimension dimZ \u2032 = m \u2212 p cannot be lower than dimX , as otherwise it violates the topological invariance of dimension (Lee, 2012). So we need not to worry about extravagant scenarios like \u201cZ \u2032 collapsing into a line while X is a surface\u201d. This is not guaranteed for a discontinuous autoencoder since it does not learn a topological embedding."
        },
        {
            "heading": "A.3 PROOFS FOR \u00a73.3",
            "text": "Theorem 3. Suppose the STD of latent dimension i is \u03c3i. If we fix zi at its mean z\u0304i for each i \u2208 P where P is the set of indices of the dimensions we decide to prune, then the L2 reconstruction error of the autoencoder with a K-Lipschitz decoder g increases by at most K \u221a\u2211\ni\u2208P \u03c3 2 i .\nProof. Suppose the reconstruction error \u03f5 is measured by L2 distance in the data space as \u03f5 = E\u2225x\u2212 x\u0302\u2225 = E\u2225x\u2212g \u25e6e(x)\u2225 = E\u2225x\u2212g(z)\u2225. Let the new reconstruction error after latent pruning be \u03f5\u0303 = E\u2225x\u2212 x\u0303P \u2225 = E\u2225x\u2212 g \u25e6 pP \u25e6 e(x)\u2225 = E\u2225x\u2212 g(z\u0303P )\u2225, where pP is the pruning operator defined\nby [z\u0303P ]i = [pP (z)]i = { zi i \u0338\u2208 P z\u0304i i \u2208 P . Then due to the subadditivity of norm and the K-Lipschitz continuity of g we have\n\u03f5\u0303\u2212 \u03f5 \u2264 E\u2225x\u0302\u2212 x\u0303P \u2225 = E\u2225g(z\u0303P )\u2212 g(z)\u2225 \u2264 K \u00b7 E\u2225z\u0303P \u2212 z\u2225 = K \u221a E [\u2225z\u0303P \u2212 z\u22252]\u2212Var [\u2225z\u0303P \u2212 z\u2225] (7)\n\u2264 K \u221a E [\u2225z\u0303P \u2212 z\u22252] = K \u221a\u2211 i\u2208P E [(zi \u2212 z\u0304i)2] = K \u221a\u2211 i\u2208P \u03c32i (8)\nOf course, (7) is an upper bound tighter than (8), but the minimalistic (8) also conveys our idea clearly."
        },
        {
            "heading": "A.4 PROOFS FOR \u00a73.4",
            "text": "Lemma 2. Suppose for a symmetric matrix S we have A\u22a4SA = \u03a3 and A\u22a4A = I , where \u03a3 is a diagonal matrix whose diagonal elements are the largest eigenvalues of S. Then A\u2019s columns consist of the orthonormal eigenvectors corresponding to the largest eigenvalues of S.\nProof. Denote the eigenvalues of S by \u03c31 \u2265 \u03c32 \u2265 \u00b7 \u00b7 \u00b7 , the corresponding orthonormal eigenvectors by u1, u2, \u00b7 \u00b7 \u00b7 , and the ith column vector of A by ai. Without any loss of generality, we also assume that the diagonal elements of \u03a3 are sorted in descending order so that \u03a3ii = \u03c3i, as the following proof is essentially independent of the specific order.\nWe can prove this lemma by induction. To begin with, suppose for a1 we have a\u22a41 Sa1 = \u03c31, then a1 must fall inside the eigenspace E\u03c31 spanned by the eigenvector(s) of \u03c31. Assume this is not true, then a1 = v + v\u22a5 where v \u2208 E\u03c31 , v\u22a5 \u22a5 E\u03c31 , \u2225v\u22a5\u2225 \u0338= 0 and \u2225a1\u22252 = \u2225v\u22252 + \u2225v\u22a5\u22252 = 1. It follows that a\u22a41 Sa1 = v\n\u22a4Sv + v\u22a4\u22a5Sv\u22a5 < \u03c31\u2225v\u22252 + \u03c31\u2225v\u22a5\u22252 = \u03c31, which violates the assumption. The inequality holds because we have the decomposition v\u22a5 = \u2211 i\u2208I \u03b1iui, I = {i | ui \u22a5 E\u03c31} where\nui\u2019s corresponding \u03c3i is smaller than \u03c31, so v\u22a4\u22a5Sv\u22a5 = \u2211 i\u2208I \u03c3i\u03b1 2 i < \u03c31 \u2211 i\u2208I \u03b1 2 i = \u03c31\u2225v\u22a5\u22252.\nNow assume that for ak, its predecessors a1 . . . ak\u22121 respectively all fall inside the corresponding eigenspaces E\u03c31 . . . E\u03c3k\u22121 . Since ak is orthogonal to a1 . . . ak\u22121, we can only find ak in \u2211 i\u2265k E\u03c3i . Then following the same rationale as in the a1 case, ak must be inside E\u03c3k .\nProposition 1. An autoencoder recovers the principal components of a dataset X after minimizing the volume, if we:\n1. Make the encoder e(x) = Ax+ a and the decoder g(z) = Bz + b linear maps,\n2. Force the reconstruction error \u2225g \u25e6 e(X)\u2212X\u2225F to be strictly minimized,\n3. Set the Lipschitz constant to 1 for the decoder\u2019s regularization,\n4. Assume the rank of the data covariance Cov(X) is not less than the latent space dimension m.\nSpecifically, B\u2019s columns consist of the principal components of X , while A acts exactly the same as B\u22a4 on X . When X has full row rank, A = B\u22a4.\nProof. According to (Bourlard & Kamp, 1988), minimizing the reconstruction error cancels out the biases a and b together, and simplifies the reconstruction loss into \u2225BAX \u2032\u2212X \u2032\u2225F where X \u2032 = X\u2212X\u0304 . Therefore, for simplicity and without any loss of generality, hereafter we assume X\u0304 = 0, e(x) = Ax and g(z) = Bz. Given a dataset X \u2208 Rd\u00d7n of n samples, condition #1 means the corresponding latent codes are Z = AX , whose covariance matrix is S = 1n\u22121ZZ \u22a4 = 1n\u22121AXX \u22a4A\u22a4 = ASXA \u22a4 where SX is the data covariance.\nCondition #2 necessitates that both the encoder A and the decoder B possess full-rank. This comes from the Eckart\u2013Young\u2013Mirsky theorem (Eckart & Young, 1936) and the inequality rank(AB) \u2264 min(rank(A), rank(B)). Moreover, for a full-rank decoder B = V \u03a3\u22121U\u22a4 (here for simplicity we use the SVD formulation where \u03a3 is diagonal), the encoder A must satisfy AX = B\u2020X = U\u03a3V \u22a4X when we minimize the reconstruction loss \u2225BAX \u2212X\u2225F , so the reconstruction loss reduces into \u2225V V \u22a4X \u2212X\u2225F . From the minimum-error formulation of PCA (Bishop & Nasrabadi, 2006), we know V should be of the form V = PQ, where Q\u22a4Q = QQ\u22a4 = I and P \u2019s column vectors are the orthonormal eigenvectors corresponding to the m largest eigenvalues of the data covariance SX , such that P\u22a4SXP = \u039b where \u039b is a diagonal matrix of the m largest eigenvalues. Therefore we can only minimize the volume by changing U , \u03a3 and Q.\nIt follows that S = ASXA\u22a4 = U\u03a3V \u22a4SXV \u03a3\u22a4U\u22a4 = U\u03a3Q\u22a4\u039bQ\u03a3\u22a4U\u22a4. Because according to Theorem 1, minimizing the volume \u220fm i \u221a [S]ii minimizes the determinant detS = (det\u03a3)2 det\u039b, then under condition #3 and #4, this is minimized only when [\u03a3]ii = 1 for all i, given that the decoder is required to be 1-Lipschitz so [\u03a3]ii \u2265 1 for all i, and det\u039b > 0. Hence \u03a3 = I , B\u2020 = B\u22a4 and B has orthonormal columns because B\u22a4B = U\u03a3\u22122U\u22a4 = I .\nTherefore S\u22c6\u2014the volume-minimized S\u2014satisfies S\u22c6 = UQ\u22a4\u039bQU\u22a4 and thus is orthogonally similar to \u039b. Since S\u22c6 is diagonal (Theorem 1), it must have the same set of diagonal entries as those of \u039b, though not necessarily in the same order. Thus when the volume is minimized, we have ASXA \u22a4 = B\u2020SX(B \u2020)\u22a4 = B\u22a4SXB = S\n\u22c6. Because B\u2019s columns are orthonormal, according to Lemma 2, this identity only holds when B\u2019s column vectors are the eigenvectors corresponding to the m largest eigenvalues of SX , so the linear decoder B consists of the principal components of X .\nRemark 1.1. Although A is not necessarily equal to B\u22a4, it projects X into the latent space the same way as B\u22a4. It can also be verified that it must have the form A = B\u22a4 +W where each row of W lies in kerX\u22a4. This means the identity A = B\u22a4 holds when X has full row rank, in which case A also recovers the principal components of X .\nRemark 1.2. It is easy to check that setting the decoder\u2019s Lipschitz constant to any value K will just scale down all the eigenvalues by K2, without hurting any of the principal dimension aligning of PCA or distorting the ratio between dimensions.\nRemark 1.3. This proof indicates that minimizing the volume is disentangled from reducing the reconstruction loss, at least for the linear case. Specifically, the reconstruction loss controls only P , while the least volume regularization controls the rotation and scaling in the latent space respectively through U , Q and \u03a3. The product U\u03a3Q\u22a4 then models the family H of linear homeomorphisms that the linear autoencoder (without bias vectors) can additionally learn beyond preserving information through P\u22a4, as discussed in \u00a72.3. Indeed, the linear encoder is of the form A = (U\u03a3Q\u22a4) \u25e6 P\u22a4 while the linear decoder is of the form B = P \u25e6Q\u03a3\u22121U\u22a4 = P \u25e6 (U\u03a3Q\u22a4)\u22121. Due to the minimization of volume, \u03a3 ends up being an identity matrix, making both g and e isometric over the dataset X . This means e does not scale up or down any data dimension in the latent space. We argue that this is the primary reason why each PCA latent dimension\u2019s degree of importance\nscales with its variance, because one can easily check that as long as \u03a3 = I , Proposition 2 below still holds for an \u201cimperfect\u201d PCA where A = B\u22a4 = UQ\u22a4P\u22a4. We may expect a similar ordering effect for nonlinear autoencoders.\nThe rotations U and Q, on the other hand, help cram as much data information into the most primary latent dimensions as possible, such that when we remove any number of the least informant latent dimensions, the remaining dimensions still store the most amount of information they can extract from X in terms of minimizing the reconstruction loss. For nonlinear autoencoders, we may expect a cramming effect that is similar yet not identical, because after stripping off some least informant latent dimensions, the nonlinear autoencoder might further reduce the reconstruction loss by curling up the decoder\u2019s image\u2014now of lower dimension\u2014in the dataset to traverse more data region.\nProposition 2. Let \u03bbi be the eigenvalues of the data covariance matrix. The explained variance \u03bbi\u2211n j \u03bbj\nof a given latent dimension i of a linear PCA model is the ratio between E\u2225\u00b7\u222522({i})\u2014the MSE reconstruction error induced by pruning this dimension (i.e., setting its value to its mean 0) and E\u2225\u00b7\u222522(\u2126)\u2014the one induced by pruning all latent dimensions, where \u2126 denotes the set of all latent dimension indices.\nProof. First we show that the ith eigenvalue \u03bbi of the data covariance matrix equals the MSE introduced by pruning principal dimension i. Let vi denote the eigenvector corresponding to the ith principal dimension. Then pruning this dimension leads to an incomplete reconstruction x\u0303{i} that satisfies:\nE\u2225\u00b7\u222522({i}) := E[\u2225x\u0303{i} \u2212 x\u2225 2 2] = E[\u2225\u27e8x, vi\u27e9 \u00b7 vi\u222522] = E[\u27e8x, vi\u27e92] = \u03bbi (9)\nNext we show that the E of linear PCA has additivity, i.e., the joint MSE induced by pruning several dimensions in P altogether equals the sum of their individual induced MSE:\nE\u2225\u00b7\u222522(P ) := E[\u2225x\u0303P \u2212 x\u2225 2 2] = E[\u2225 \u2211 i\u2208P \u27e8x, vi\u27e9 \u00b7 vi\u222522]\n= E[\u27e8 \u2211 i\u2208P \u27e8x, vi\u27e9 \u00b7 vi, \u2211\ni\u2208P \u27e8x, vi\u27e9 \u00b7 vi\u27e9] = E[ \u2211 i\u2208P \u27e8x, vi\u27e92] = \u2211 i\u2208P E[\u27e8x, vi\u27e92]\n= \u2211 i\u2208P \u03bbi = \u2211 i\u2208P E\u2225\u00b7\u222522({i})\n(10)\nHence the claim is proved.\nRemark 2.1. The subtraction in Eqn. 6 is a generalization of the case where the PCA model\u2019s number of components is set smaller than the data covariance\u2019s rank r. In this case, the PCA model\u2014as a linear AE\u2014will inevitably have a reconstruction error \u03f5 even if no latent dimension is pruned. Yet we may regard this inherent error \u03f5 = E[\u2225x\u0303I \u2212 x\u222522] as the result of implicitly pruning a fixed collection I of additional r\u2212n latent dimensions required for perfect reconstruction. Then \u03bbi in (9) can instead be explicitly expressed as \u03bbi = E[\u2225x\u0303{i}\u222aI \u2212x\u222522]\u2212\u03f5 to accommodate this implicit pruning. Therefore the induced reconstruction error E\u2225\u00b7\u222522({i}) is actually the change in reconstruction error before and after pruning i (with the inherent reconstruction error \u03f5 taken into account). The one in Eqn. 6 is thus a generalization of this, using the change in reconstruction error to indicate each dimension\u2019s importance."
        },
        {
            "heading": "B TECHNICAL DETAILS OF EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "B.1 TOY PROBLEMS",
            "text": "We apply least volume to low-dimensional toy datasets to pedagogically illustrate its effect. In each case, we make the latent space dimension equal to the data space dimension. Figure B.2 shows that the autoencoders regularized by least volume successfully recover the low dimensionality of the 1D and 2D data manifold respectively in the latent spaces by compressing the latent sets into low dimensional latent subspaces, without sacrificing the reconstruction quality. Not only that, with a large enough weight \u03bb for the volume penalty, in the noisy 2D problem, the autoencoder also manages to remove the data noise perpendicular to the 2D manifold (Fig. B.2b.ii)."
        },
        {
            "heading": "B.1.1 DATASET",
            "text": "The 1D manifold dataset {(x, y)} consists of 50 samples, where x are uniformly sampled from [\u22120.5, 0.5] and y are created via y = 10x(x\u2212 0.4)(x+ 0.35). The 2D manifold dataset {(x, y, z)} consists of 100 samples, where x and y are uniformly sampled from [\u22120.5, 0.5]\u00d7 [\u22120.5, 0.5], and z are created via z = 10x(x\u2212 0.3)(x+ 0.3). After this, we add Gaussian noise N (\u00b5 = 0, \u03c3 = 0.1) to the z dimension"
        },
        {
            "heading": "B.1.2 ARCHITECTURE",
            "text": "In Table B.1 we list the architectures of the autoencoders for both toy datasets. Here Linearn denotes a fully connected layer of n output features, while SN-Linearn denotes one regularized by spectral normalization. LeakyReLU\u03b1 denotes a LeakyReLU activation function with negative slope \u03b1."
        },
        {
            "heading": "B.1.3 HYPERPARAMETERS",
            "text": "The hyperparameters are listed in Table B.2. The reconstruction loss is MSE, the volume penalty is m \u221a\u220f\ni \u03c3i, and the optimizer is Adam.\nTable B.2: Hyperparameters\nDataset Batch Size \u03bb \u03b7 Learning Rate Epochs\n1D 50 0.001 0 0.001 10000\n2D 100 0.01 0 0.0001 10000\nB.2 IMAGE DATASETS"
        },
        {
            "heading": "B.2.1 CELEBA RESULT",
            "text": "We additionally perform the dimension reduction experiment in \u00a75.1 on the CelebA dataset.\n(a) Latent Set Dimensionality vs L2 Reconstruction Error\n(b) Explained Reconstruction vs Latent Dimension #\n(c) L2 Reconstruction vs \u03bb (more in \u00a7B.3.1)\nFigure B.3: Dimension Reduction Result of Sparse Regularizers on CelebA"
        },
        {
            "heading": "B.2.2 DATASETS",
            "text": "The 5D synthetic dataset is produced by feeding an image of a red circle to Torchvision\u2019s RandomAffine and ColorJitter layers to randomly generate images of transformed circles of different colors, then resizing them to 32 \u00d7 32 \u00d7 3. Specifically, we set their parameters to translate=(0.2, 0.2), scale=(0.2, 0.5), brightness=(0.3, 1), hue=(0, 0.5), corresponding to 3 spatial dimensions (x-translation, y-translation, circle size) and 2 color dimensions (hue and brightness). The other information of the three image sets are listed in Table B.3. All pixel values are normalized to between 0 and 1.\nImage Dimension 32\u00d7 32\u00d7 3 32\u00d7 32\u00d7 1 32\u00d7 32\u00d7 3 32\u00d7 32\u00d7 3"
        },
        {
            "heading": "B.2.3 ARCHITECTURE",
            "text": "In Table B.4 we list the architectures of the autoencoders for these datasets. Here Convn denotes convolutional layer of n output channels, while SN-Deconvn denotes spectral-normalized deconvolutional layer of n output channels. For all such layers we set kernel size to 4, stride to 2 and padding to 1."
        },
        {
            "heading": "B.2.4 HYPERPARAMETERS",
            "text": "The hyperparameters are listed in Table B.5. The reconstruction loss is binary cross entropy and the optimizer is Adam. Only the volume penalty needs \u03b7. By default \u03b7 = 1 for all experiments."
        },
        {
            "heading": "B.3 ADDITIONAL EXPERIMENT RESULTS OF IMAGE DATASETS",
            "text": ""
        },
        {
            "heading": "B.3.1 L2 RECONSTRUCTION LOSS",
            "text": "\u03bb controls the trade-off between latent set compression and reconstruction. We can see in Fig. B.4 that as \u03bb goes up, the autoencoder\u2019s L2 reconstruction loss increases (without pruning, marked by suffix \u201c_non\u201d), and eventually it converges to the reconstruction loss that is achieved after pruning all latent dimensions (marked by suffix \u201c_all\u201d). In this extreme case, the latent set approximately collapses into a single point and stores no useful information about the dataset. In terms of the four\nregularizers\u2019 influence on the reconstruction, the ST regularizer seems to be the least sensitive to \u03bb compared with the other two that act directly on the latent STD vector \u03c3 .\nTo better read Fig. 2, we need to determine which segments of the curves correspond to low quality reconstruction, so that we can ignore them. For this, it should be helpful to plot the autoencoders\u2019 reconstructions corresponding to different L2 loss, so that we can grasp at what level of L2 loss do the autoencoders\u2019 outputs have acceptable visual quality. In Fig. B.5 we can see that the image reconstructions have good quality when the L2 reconstruction loss is under 2.\nB.3.2 EFFECT OF \u03b7 As mentioned in \u00a73.1, the supplemented volume penalty m \u221a\u220f i \u03c3i + \u03b7 interpolates between m \u221a\u220f\ni \u03c3i and the L1 penalty 1m\u2225\u03c3\u22251 gradient-wise. We investigate this by performing experiments at different \u03b7 in {0, 1, 3, 10, 30} and plot their curves of Latent Set Dimensions vs L2 Reconstruction Loss in Fig B.6. The hyperparameter configurations are listed in Table B.6. We can perceive this interpolation rather clearly in Fig. B.6b and B.6c that as \u03b7 increases, the dimension-reconstruction curve of the volume penalty converges to that of the L1 penalty. The convergence to L1 is not very apparent in Fig. B.6a probably because \u03b7 is much smaller than most STDs (See Table B.7).\nHowever, \u03b7 = 0 seems to be a special case whose curves differ greatly from the other \u03b7, and in Fig. B.6a and Fig. B.6c it is not as effective as the other \u03b7 in terms of dimension reduction. This is\nprobably due to the gradient explosion induced by the scaling factor m \u221a\u220f\ni \u03c3i \u03c3i for small \u03c3i."
        },
        {
            "heading": "B.3.3 ABLATION STUDY OF ORDERING EFFECT",
            "text": "It seems that even without Lipschitz regularization or latent regularization, the latent STD still in general correlates well with the explained reconstruction (or the degree of importance), as shown in Fig. B.7 and B.8. Although there is no clear answer to this, we hypothesize that this is because an unregularized decoder is still a Lipschitz function such that Theorem 3 still works. When minimizing the latent regularizers during training, although an unregularized decoder\u2019s Lipschitz constant is not upper bounded by spectral normalization, due to the gradient-based optimization it is not likely to increase abruptly, so practically we may regard the decoder\u2019s Lipschitz constant as being \u201csoftly\u201d\nupper bounded such that it still collaborates to some degree with the latent regularization. This hypothesis might explain the ordering effect in Fig. B.7.\nHowever, in the case without latent regularization (Fig. B.8), both the autoencoders with and without Lipschitz regularization possess much more \u201crugged\u201d explained reconstruction curves compared to Fig. 3 and B.7, which signifies that the latent dimensions with large STDs are more likely to correspond to trivial data dimensions when there is no latent regularization (Fig. B.8c is a good example of this ruggedness, especially when compared to Fig. B.7c and 3c). This agrees with our discussion in \u00a73.4 about the latent regularization\u2019s role in producing the ordering effect."
        },
        {
            "heading": "B.3.4 ABLATION STUDY OF ISOTROPIC SHRINKAGE",
            "text": "We list in Table B.7 the range of the principal latent dimensions\u2019 STDs for the cases with and without Lipschitz regularization on the decoder. Here \u201cprincipal latent dimensions\u201d refers to the latent dimensions left after cumulatively pruning those with the smallest STDs until their joint explained reconstruction exceeds 1%. We can see that the STDs in the cases without Lipschitz regularization\nare orders of magnitude smaller than those with Lipschtiz regularization, and also smaller than their counterparts without latent regularization. This together with Fig. 4 shows that isotropic shrinkage happens when there is no Lipschitz regularization."
        },
        {
            "heading": "B.3.5 DOWNSTREAM KNN CLASSIFICATION TASKS",
            "text": "We perform KNN classification on MNIST and CIFAR-10 using the latent representation learned by different sparse AEs of similar sparsity levels (See Fig.B.9). In short, all methods are comparable on MNIST (Table B.8), but LV has much better performance on CIFAR-10 (Table B.9)."
        },
        {
            "heading": "C TIPS FOR HYPERPARAMETER TUNING, MODEL TRAINING AND USAGE",
            "text": ""
        },
        {
            "heading": "C.1 HYPERPARAMETER TUNING",
            "text": "There are three hyperparameters\u2014K, \u03bb, \u03b7\u2014to tune for least volume. However, usually we only need to take care of \u03bb. Below we discuss them in detail.\n1. K is always first settled when constructing the decoder network and fixed afterwards, because the activation functions are fixed and the spectral normalization makes the linear layers 1-Lipschitz. Normally K = 1 works well, just like in all the experiments we did. However, if the data space is very high dimensional such that the data manifold has great length scale in terms of its geodesic distance, then this will in turn make the latent set large-scale after the flattening (check those large \u03c3i in Table B.7 when the decoder is 1- Lipschitz). This might decelerate the training of the autoencoder, as the encoder then needs to dramatically increase its weights to scale up the latent code to reduce the reconstruction loss, which can be time-consuming. In this situation, increasing K could reduce the length scale of the latent set and accelerate training. In case the length scale of the dataset really becomes a problem, to set K and integrate it into the autoencoders:\n\u2022 As to the value of K, a practical choice is the dimension of the data space X or any value of similar order of magnitude, provided the dataset is normalized. This value does not need to be exact because LV works for any K.\n\u2022 We can attach a scaling layer f(x) = Kx to the input of g to make it K-Lipschitz (i.e., dnew = d \u25e6 f ). Alternatively, we can either attach f to the output of the encoder e (i.e., enew = f \u25e6 e) or just increase the initial weights of e to simply scale up the latent set. This keeps the decoder g 1-Lipschitz.\n2. Empirically, it is fine to leave \u03b7 at 1 by default for K = 1. In general, it had better be a value more than one or two orders of magnitude smaller than the largest latent STDs to make it not too L1-like. However, it should be emphasized that in this work, this \u03b7 is introduced more like a concession, an unsatisfying alternative to the ideal but uneasy-to-implement recursive subspace extraction process introduced in \u00a72.1. Without that recursive extraction and compression, the optimizer will keep compressing those trivialized latent dimensions unnecessarily even if they cannot be compressed any more. This could limit the complexity of the autoencoder thus prevent further flattening of the latent set, especially when the latent space is higher dimensional (given that for the same dataset, there are more trivial latent dimensions to compress in the latent space). This is the main drawback of this work in our opinion. A reliable realization of the recursive subspace extraction process is the missing piece that may unleash the full potential of Least Volume, thus is worth researching in the future.\n3. \u03bb controls the trade-off between reconstruction and dimension reduction. This needs the user to set a reasonable threshold \u03b4 for the reconstruction error \u03f5, and tune \u03bb to make \u03f5 < \u03b4. It is hard to tell what a proper \u03b4 is, given that different people have different tolerance to \u03f5 for different problems. The best practice for tackling this dilemma is probably transparency and openness: always report \u03b4 in the LV result. We can infer that when the autoencoder\u2019s latent dimension m needs to be changed, \u03bb should be adjusted in proportion to m to negate the scaling variation caused by the factor 1/m in Eqn. 5, so that the gradients induced by the volume penalty on the model\u2019s parameters have consistent magnitudes across different m."
        },
        {
            "heading": "C.2 MODEL TRAINING",
            "text": "At the start of each training, if \u03bb is too large, it is likely that the volume penalty will prevent the reduction of reconstruction error, trapping the model parameters at the local minima. In case this happens, it is practical to set a schedule for \u03bb that gradually increases it to the target value as the training proceeds. If \u03bb is set correctly, normally at the start of each training the reconstruction loss will immediately decrease, whereas the volume penalty will increase for a short period of time before\nit starts to drop, as shown in Fig. C.10. This is because when K is not large, the latent set has to expand itself first to reduce the reconstruction loss, according to Theorem 3.\nAs to determining the number of epochs for training, a useful strategy is to inspect the history curve of the volume penalty, as usually the reconstruction loss converges faster than the volume penalty (Fig. C.10). It is ideal to stop training when the volume penalty value converges. Monitoring the histogram of latent STDs throughout training is another useful and informative way to assess the dimension reduction progress."
        },
        {
            "heading": "C.3 MODEL USAGE",
            "text": "To quickly determine the compressed latent set\u2019s dimension after training LV models, rather than tediously evaluating the explained reconstructions one dimension by one dimension, a much more convenient way is to illustrate all latent STDs in descending order in a bar plot and put the y-axis on log scale. In every LV application we have done so far (including those outside this work), there is always a noticeable plummet at a given index in the log-scale bar plot (a real example is shown in Fig. C.11). All latent dimensions before this plummet can be regarded as corresponding to the principal dimensions of the dataset, considering their significantly larger STDs and empirically also much larger explained reconstructions."
        }
    ],
    "year": 2024
}