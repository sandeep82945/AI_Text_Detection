{
    "abstractText": "Programming-by-example is the task of synthesizing a program that is consistent with a set of user-provided input-output examples. As examples are often an under-specification of one\u2019s intent, a good synthesizer must choose the intended program from the many that are consistent with the given set of examples. Prior work frames program synthesis as a cooperative game between a listener (that synthesizes programs) and a speaker (a user choosing examples), and shows that models of computational pragmatic inference are effective in choosing the user intended programs. However, these models require counterfactual reasoning over a large set of programs and examples, which is infeasible in realistic program spaces. In this paper, we propose a novel way to amortize this search with neural networks. We sample pairs of programs and examples via self-play between listener and speaker models, and use pragmatic inference to choose informative training examples from this sample. We then use the informative dataset to train models to improve the synthesizer\u2019s ability to disambiguate user-provided examples without human supervision. We validate our method on the challenging task of synthesizing regular expressions from example strings, and find that our method (1) outperforms models trained without choosing pragmatic examples by 23% (a 51% relative increase) (2) matches the performance of supervised learning on a dataset of pragmatic examples provided by humans, despite using no human data in training.",
    "authors": [],
    "id": "SP:6381b0ac34fe4e2595986b29a34c98985994ab99",
    "references": [
        {
            "authors": [
                "Jacob Andreas",
                "Dan Klein"
            ],
            "title": "Reasoning about pragmatics with neural listeners and speakers",
            "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2016
        },
        {
            "authors": [
                "Matej Balog",
                "Alexander L. Gaunt",
                "Marc Brockschmidt",
                "Sebastian Nowozin",
                "Daniel Tarlow"
            ],
            "title": "Deepcoder: Learning to write programs",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Xinyun Chen",
                "Petros Maniatis",
                "Rishabh Singh",
                "Charles Sutton",
                "Hanjun Dai",
                "Max Lin",
                "Denny Zhou"
            ],
            "title": "Spreadsheetcoder: Formula prediction from semi-structured context",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Reuben Cohn-Gordon",
                "Noah Goodman"
            ],
            "title": "Lost in machine translation: A method to reduce meaning loss",
            "venue": "In Proceedings of the",
            "year": 2019
        },
        {
            "authors": [
                "Reuben Cohn-Gordon",
                "Noah Goodman",
                "Christopher Potts"
            ],
            "title": "Pragmatically informative image captioning with character-level inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Jonathan Uesato",
                "Surya Bhupatiraju",
                "Rishabh Singh",
                "Abdel rahman Mohamed",
                "Pushmeet Kohli"
            ],
            "title": "RobustFill: Neural program learning under noisy I/O",
            "venue": "Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Kevin Ellis",
                "Catherine Wong",
                "Maxwell Nye",
                "Mathias Sabl\u00e9-Meyer",
                "Lucas Morales",
                "Luke Hewitt",
                "Luc Cary",
                "Armando Solar-Lezama",
                "Joshua B. Tenenbaum"
            ],
            "title": "Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning",
            "venue": "In Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation,",
            "year": 2021
        },
        {
            "authors": [
                "Yu Feng",
                "Ruben Martins",
                "Osbert Bastani",
                "Isil Dillig"
            ],
            "title": "Program synthesis using conflict-driven learning",
            "venue": "ACM SIGPLAN Notices,",
            "year": 2018
        },
        {
            "authors": [
                "Margarida Ferreira",
                "Miguel Terra-Neves",
                "Miguel Ventura",
                "In\u00eas Lynce",
                "Ruben Martins"
            ],
            "title": "Forest: An interactive multi-tree synthesizer for regular expressions",
            "venue": "Tools and Algorithms for the Construction and Analysis of Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Michael C. Frank",
                "Noah D. Goodman"
            ],
            "title": "Predicting pragmatic reasoning in language",
            "venue": "games. Science,",
            "year": 2012
        },
        {
            "authors": [
                "Daniel Fried",
                "Jacob Andreas",
                "Dan Klein"
            ],
            "title": "Unified pragmatic models for generating and following instructions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
            "year": 2018
        },
        {
            "authors": [
                "Daniel Fried",
                "Ronghang Hu",
                "Volkan Cirik",
                "Anna Rohrbach",
                "Jacob Andreas",
                "Louis-Philippe Morency",
                "Taylor Berg-Kirkpatrick",
                "Kate Saenko",
                "Dan Klein",
                "Trevor Darrell"
            ],
            "title": "Speaker-follower models for vision-and-language navigation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Sumit Gulwani"
            ],
            "title": "Automating string processing in spreadsheets using input-output examples",
            "venue": "Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages,",
            "year": 2011
        },
        {
            "authors": [
                "Luke Hewitt",
                "Tuan Anh Le",
                "Joshua Tenenbaum"
            ],
            "title": "Learning to learn generative programs with memoised wake-sleep",
            "venue": "Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI),",
            "year": 2020
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi"
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Angeliki Lazaridou",
                "Anna Potapenko",
                "Olivier Tieleman"
            ],
            "title": "Multi-agent communication meets natural language: Synergies between functional and structural language learning",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7663\u20137674,",
            "year": 2020
        },
        {
            "authors": [
                "Bill McDowell",
                "Noah Goodman"
            ],
            "title": "Learning from omission",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Will Monroe",
                "Christopher Potts"
            ],
            "title": "Learning in the rational speech acts model",
            "venue": "In 20 th Amsterdam Colloquium,",
            "year": 2015
        },
        {
            "authors": [
                "Yewen Pu",
                "Kevin Ellis",
                "Marta Kryven",
                "Josh Tenenbaum",
                "Armando Solar-Lezama"
            ],
            "title": "Program synthesis with pragmatic communication",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yewen Pu",
                "Saujas Vaduguru",
                "Priyan Vaithilingam",
                "Elena Glassman",
                "Daniel Fried"
            ],
            "title": "Amortizing pragmatic program synthesis with rankings, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Priyan Vaithilingam",
                "Yewen Pu",
                "Elena L. Glassman"
            ],
            "title": "The usability of pragmatic communication in regular expression synthesis, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Julia White",
                "Jesse Mu",
                "Noah D Goodman"
            ],
            "title": "Learning to refer informatively by amortizing pragmatic reasoning",
            "venue": "In CogSci,",
            "year": 2020
        },
        {
            "authors": [
                "Xi Ye",
                "Qiaochu Chen",
                "Isil Dillig",
                "Greg Durrett"
            ],
            "title": "1162/tacl a 00461",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In program synthesis\u2014specifically programming by example (PBE)\u2014an user describes a target program using input-output examples (i.e. test cases) and the synthesizer finds a program that is consistent with these input-output examples. In PBE, the users directly express the semantics of the intended program (what it should do) without having to understand the syntax of the program (what it should look like). Such systems have found real-world use in a variety of scenarios such as spreadsheet formulas (Chen et al., 2021; Gulwani, 2011) and data wrangling (Feng et al., 2018).\nAn important aspect of inferring programs from examples is dealing with ambiguity. Given a set of examples, there can be many spurious programs consistent with the set, and picking out the right one the user has in mind is a long-standing challenge. For example, when describing the regular expression a+b*,1 an informative user might provide the example (ab,\u2713) indicating that the string ab matches the target regular expression. However, to the program synthesizer, both a+b* and a*b+c? 2 would be among many acceptable answers based on this example.\nPu et al. (2020) resolves this ambiguity by framing program synthesis as a coorporative communicative game: the user chooses an informative set of examples to convey the program to the synthesizer, and the synthesizer chooses a program under the assumption that these examples were chosen informatively. Models of pragmatic inference, specifically, the Rational Speech Acts (RSA) framework (Frank & Goodman, 2012) can then be used to build a program synthesizer that can resolve ambiguity via recursive Bayesian inference. The RSA framework allows the synthesizer to reason about what program a user intended, given that they chose that particular set of examples rather than a\n11 or more a s followed by 0 or more b s 2c? means optionally having a c at the end\ndifferent one. For example, the synthesizer could reason that a user that wanted to describe a*b+c? would have chosen an example containing the character c. However, this approach requires the synthesizer to perform expensive counterfactual inference over the space of all programs and examples to resolve ambiguity, making it difficult to scale to realistic programming domains.\nTo scale to realistic program spaces, modern approaches of PBE systems have relied on training neural networks to efficiently search through large program spaces for consistent programs given examples (Balog et al., 2017; Devlin et al., 2017). In this paper, we explore whether we can use simulated reasoning in communication games using the RSA framework as a way to generate training data consisting of pragmatic examples. The generated data is then used to train neural networks to enable scaleable pragmatic program synthesis. We hypothesize that since the RSA framework computationally models how a human chooses examples to communicate a program, end users would succeed more often when communicating with a neural synthesizer trained on pragmatic data (our work) as compared to a neural synthesizer trained on non-pragmatic data (Balog et al., 2017; Devlin et al., 2017).\nAn overview of our approach is shown in Figure 1. We start with a neural literal listener \u2014 a synthesizer trained in the style of Devlin et al. (2017) \u2014 and a neural literal speaker that generates examples consistent with a given program. We generate a sequence of pragmatic examples incrementally to obtain a training pair (program,examples). This pair is then added to an aggregate training set, which is used to finetune both the speaker model \u2014 making it more likely to generate pragmatic examples \u2014 and the listener model \u2014 making it more likely to recover the intended program given pragmatic examples.\nWe validate the effectiveness of our methods on the well-studied PBE task of inferring regular expressions from a set of examples. Each example is a pair (string,bool), indicating whether a particular string matches the regex. To compare our training algorithm to standard supervised learning from human-annotated data, we collect a novel dataset of human annotations, consisting of (program,examples) where the examples were given by a person for a total of 440 regular expressions. We find that with only a small number (40) of human annotations \u2013 used only for model selection \u2013 our method is able to outperform a system that is fine-tuned using 400 annotated regexes from this dataset. We conduct human evaluation of our approach by giving a user a target regex to communicate interactively to the synthesizer using examples, and find that the informative ex-\namples generated by our procedure substantially improve the performance of a regular expression synthesizer, with improvements of 22.8% absolute (51.4% relative) in accuracy (11 participants, 340 regexes total). Our approach, despite not using human-provided data during training, matches the performance of of a model fine-tuned on a large dataset of human-written pragmatic examples."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "Programming-by-Example In this paper, we tackle the task of finding a program \u2208 P , where P is a space of possible programs. As a specification of intent, a user provides a sequence of inputoutput examples \u2208 E+,3 where E = X \u00d7 Y is the space of all possible input-output pairs that programs in P operate over. For example, P may be a space of regular expression programs, X the space of all strings in the alphabet that the regular expressions are defined over, and Y \u2208 {\u2713,\u2717} where output \u2713 indicates whether the input string matches the regular expression. For simplicity of explanation, in this section we consider cases where the specification consists of a single example, deferring the cases of multiple examples to the next section.\nThe semantics of programs are captured by the consistency relation M between P and E: M = {(program,example) | example = (x, y) \u2208 X \u00d7 Y, program(x) = y}\nA program is consistent with an example iff executing the program on the input produces the intended output. We can view M as a consistency matrix where each row corresponds to an example, each column corresponds to a program, and an element is 1 if the program is consistent with the example and 0 otherwise (Figures 1 and 2, left).\nLiteral Model of Program Synthesis A minimal requirement of a program synthesizer is that it finds any program that is consistent with the given specification. We refer to such a synthesizer as the literal listener L0, which naively assigns equal probability to any consistent program.\nL0(program|example) \u221dM(example, program)P (program) (1) This literal listener distribution is given by normalizing the rows of the consistency matrix to produce uniform probability distributions (L0 in Figure 2). However, this literal listener cannot resolve ambiguity when interacting with users, as it places equal probability on all consistent programs. A literal speaker \u2013 one that generates any consistent examples for a given program \u2013 is defined analogously as S0(example|program) \u221dM(example, program)P (example).\n3Here the notation X+ indicates a sequence of 1 or more elements belonging to X\nPragmatic Model of Program Synthesis When interacting with a synthesizer, users choose examples that are informative \u2013 those that distinguish the program they desire from others. For example, given the specification [(ab,\u2713)], a user is likely wants the regular expression a+b+ than a+b+c?, since they probably would have included the character c if they wanted the latter expression.\nTo leverage the informativity of examples, Pu et al. (2020) use the Rational Speech Acts (RSA; Frank & Goodman 2012) framework to derive a pragmatic program synthesizer that resolves ambiguity by modeling how people choose examples informatively. First, they construct a pragmatic speaker S1 that chooses an example in proportion to the likelihood that it would make the literal listener infer the intended program:\nS1(example|program) \u221d L0(program|example)P (example) (2) Using a uniform prior P (example), the S1 distribution is given by normalizing the columns of the L0 matrix (S1 in Figure 2). As we can see, given a program S1 selects an example in proportion to the likelihood of L0 recovering the program given the example, choosing examples that are informative to the listener.4\nFinally, a pragmatic listener (program synthesizer) L1 is built on top of S1: L1(program|example) \u221d S1(example|program)P (program) (3)\nusing a prior P (program) over programs. This listener resolves ambiguity by choosing that program that an informative speaker (modeled by S1) would have described using the chosen example.\nPu et al. (2020) demonstrated that building a pragmatic synthesizer L1 in this way allows for users to communicate the target program to the synthesizer using fewer examples without training a model on human-produced examples or explicitly defining a prior over the space of programs. However, in realistic domains, enumerating large numbers of programs and examples is intractable, preventing the application of this framework to a broader range of tasks."
        },
        {
            "heading": "3 METHOD",
            "text": "In this section, we describe the iterative process by which we bootstrap a pragmatic neural program synthesizer by generating informative specifications, without human supervision (Figure 1). The full algorithm is detailed in Appendix C."
        },
        {
            "heading": "3.1 SPEAKER AND LISTENER MODELS",
            "text": "We build on past work that uses neural models as specification-conditioned proposal distributions over programs (Balog et al., 2017; Devlin et al., 2017). Our listener (synthesizer) models represent distributions over programs L\u03b8(program|examples). Our speaker (specification generation) models generate the sequence of examples in a specification autoregressively: S\u03d5(examplei|program,examples1:i\u22121).5 While all our listener and speaker models share the same architecture and initialization, we vary their training data, as described below."
        },
        {
            "heading": "3.2 TRAINING BASE MODELS",
            "text": "As a foundation for our approach, we train base listener and speaker models to approximate literal speakers and listeners S0 and L0 (Equation (1)). Since we cannot enumerate the consistency matrix completely and normalize rows, we obtain these approximate models by training on data obtained by randomly sampling an input from the space of inputs X , and executing the program on the input to obtain the output (e.g., by checking if an sampled example string is matched by a sampled regular expression). This lets us generate as many samples from M as we can, which we can use to train a base listener to approximate L0 (Equation (1)), and a base listener to approximate an analogous S0, using standard maximum likelihood training. This is essentially the method proposed by Devlin et al. (2017). We denote the resulting base listener model as L\u03b80 and the base speaker model as S\u03d50 , and use these as the initial models in our iterative model bootstrapping procedure.\n4For a sequence of examples, Pu et al. (2020) propose factoring the pragmatic speaker distribution autoregressively as S1(examples|program) = \u220fNexamples i=1 S1(examplei|program, examples:i)\n5We train the speaker to predict the input-output pair to encourage the model to capture aspects of the domain semantics"
        },
        {
            "heading": "3.3 GENERATING INFORMATIVE EXAMPLES",
            "text": "The crux of our algorithm is using the existing S\u03d5 and L\u03b8 to approximate S1, which can then be used to generate training data to improve S\u03d5 and L\u03b8 over rounds of training. At each round r of our approach, we use the current speaker and listener models, together with the RSA procedure, to create a dataset of informative examples specifying programs (\u2780 of Figure 1). We incrementally generate examples to create a specification. Given a partial specification of i examples examples1:i, we sample a set of additional candidate examples: S\u03d5r (examplei+1|program,examples1:i). Similarly, we sample a set of alternative programs: L\u03b8r (program|examples1:i) from the partial specification.6\nWe can then compute the sampled consistency matrix over the generated examples and programs, and use RSA inference as shown in Figure 2 to choose the highest scoring example from the approximate S1 distribution.7 This example is added to the partial specification, and we repeat until a maximum number of examples are reached.8 The completed program-specification pair is then added to a dataset Dr of examples from that round of training. This process amounts to choosing an example proposed by the current speaker model that minimizes ambiguity among programs that the current listener infers to be likely. The full algorithm for incrementally generating a sequence of examples is presented in Algorithm 2 (Appendix C)."
        },
        {
            "heading": "3.4 MODEL UPDATES",
            "text": "We use the dataset Dr to update both the speaker and listener models as sketched in part \u2781 of Figure 1 using standard maximum likelihood training. In each round r we further fine-tune the speaker and listener models on the generated data to obtain the updated parameters \u03b8r+1 and \u03d5r+1. The full algorithm to iteratively generate examples and update the model is presented in Algorithm 1 (Appendix C). To select the model that works best with human-provided examples, we choose the model that maximizes a model selection metric computed over a small set of programs paired with humanprovided examples.9 Note that this validation set is never used to update the model parameters, and only is used to choose a model."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 REGULAR EXPRESSIONS",
            "text": "We validate the training algorithm we propose on the task of synthesizing regular expressions (\u2018regexes\u2019) as formally defined in Section 2. We use the regular expression domain-specific language presented by Ye et al. (2020). In addition to defining a regular expression specification language, they also define a sampling distribution over the space of regular expressions that we use to sample programs for training and evaluating our model. This distribution uses templates that generalize types of regular expressions that people ask about on fora such as StackOverflow. Further details are provided in Appendix A."
        },
        {
            "heading": "4.2 MODELS FOR COMPARISON",
            "text": "Base models We use ByT5-small models (Xue et al., 2022) as the backbone for all speaker and listener models. To obtain the base speaker S\u03b80 and listener L\u03d50 models that approximate the literal speaker and listener respectively, we use a set of 300,000 randomly-generated program\u2013specification pairs (with varying numbers of examples in each specification) and finetune the pretrained ByT5\n6We follow prior work (Pu et al., 2020) and impose a uniform, rather than learned, prior over this sample. 7Ideally, one could perform exact inference to draw samples from S1 directly. As stated earlier, this is intractable. Therefore, we first sample a subset of the rows and columns in the consistency matrix, then perform the RSA inference over this much smaller and denser sampled matrix. However, the consistency matrix M is sparse (mostly 0s) \u2013 most programs are inconsistent with any non-trivial set of examples \u2013 allowing for reasoning about a sample of the matrix.\n8Since the models are used only to generate the programs and utterances that are used to create the lexicon, we can draw examples from other sources, including models other than S\u03d5r (examplei|program, examples:i) and L\u03b8r (program|examples:i).\n9This amounts to performing early stopping on the validation metric.\ncheckpoint. Full details of training are provided in Appendix B. The L\u03b80 model acts as the LITERAL model in our experiments.\nPragmatic model We start with the base models S\u03b80 and L\u03d50 and use the iterative data generation and finetuning algorithm to obtain a sequence of synthesis models L\u03d5r for rounds r = 0, . . . , Rmax. We use the TOP-1 metric evaluated on a small validation set to choose the best model which we refer to as the PRAGMATIC model.\nFinetuning on human-provided specifications We obtain an HFT model by fine-tuning L\u03d50 on a curated high-quality human-provided specifications (Section 4.5). This model allows us to compare how well our approach of using model generated informative examples compares with sourcing more expensive human annotations.\nGPT-3.5 We evaluate GPT-3.5 by using the program-specification pairs we obtain as users interact with the other three models, and revealing each specification to the GPT-3.5 model in the order the user provided them, one example at a time. We stop when the model guesses the correct regular expression, or when all the examples are presented.We can think of this as a form of interaction where the human doesn\u2019t observe the outputs of this model while giving examples. Further details of how the model is prompted are in Appendix F.\nInference Crucial to our approach is the ability to generate programs from examples, and vice versa. To generate programs from a specification (sequence of examples from either self-play or human), we present it to the listener, and sample 500 programs using top-p sampling (Holtzman et al., 2020) with p = 0.9. We then deduplicate the set of sampled programs, and filter out programs inconsistent with the given specification. We can then sort the remaining programs by their score under the model to obtain a ranked list of consistent programs. Similarly, to generate specifications from a program, we sample 500 examples using top-p sampling with p = 1 and check that the examples are consistent with the program."
        },
        {
            "heading": "4.3 PROCEDURE",
            "text": "We evaluate the model on the basis of successful communications on 11 human participants. A sampled regex p is given to a human participant, whom describes it using a sequence of examples, providing one example in each turn for upto a maximum of 10 turns. The synthesizer takes the examples provided and generates a ranked list of inferred programs, of which the top-1 regex p\u2032 is shown to the participant. The communication is successful when p = p\u2032, at which point the interaction ends.10 A total three synthesizers were considered \u2013 the LITERAL model, the human fine-tuned HFT model, and the model trained using the method we propose, which we refer to as the PRAGMATIC model. The identity of the models is referred to the users only as differently colored robots. The study yielded communication history over a total of 340 regexes (109 to the LITERAL model, 113 to the HFT model, and 118 to the PRAGMATIC model)11. Further details about how the user study is conducted are provided in Appendix E."
        },
        {
            "heading": "4.4 MEASUREMENT",
            "text": "We consider the following metrics. TOP-1@t measures whether the model\u2019s top-1 matches intended regular expression at any point at turn t of the interaction. We can also consider the average value of TOP-1 by aggregating across the turns t \u2208 {1, . . . , 10}. Averaging across turns rewards models that pass success criteria given fewer turns \u2014 a model that can infer the target regex at turn 4 on average is better than a model that can only infer the target at turn 10. We use TOP-1 over a validation set as the model selection criterion for our proposed method. TOP-10@t and TOP-10 are similarly defined. EDIT DISTANCE \u2264 1@t measures whether the model\u2019s highest scoring prediction in any of the turns up to t is at most a 1 token edit from the intended program, and EDIT DISTANCE \u2264 1 is the average of this value over t \u2208 {1, . . . , 10}.\n10We used the greenery Python library to identify regex matches in terms of semantic similarity, and not just surface form match.\n11a small bug caused us to collect few extra interactions for some of the models, which does not change the measurements on the models\u2019 relative performances"
        },
        {
            "heading": "4.5 HUMAN-PROVIDED SPECIFICATIONS",
            "text": "An alternative to generating informative examples using the method we propose is to have human annotators provide examples. We collect a new dataset of high-quality program-specification pairs.\nProcedure We present a participant with a sampled regular expression, and instruct them to provide examples that they might use to illustrate the regular expression to another person. Participants are asked to provide at least 5-7 examples. We verify whether the examples are informative by checking whether a different annotator is able to identify the program which the given set of examples describes. Further details about the data collection process are presented in Appendix D.\nUsage of Data We collect a total of 440 program-specification pairs. We sample a small subset of 40 pairs that received 2 \u201ccorrect\u201d verifications as a validation set for model selection. We use the other 400 pairs as a training set to finetune the L\u03b80 models on human-provided informative examples, obtaining HFT (see Appendix B)."
        },
        {
            "heading": "4.6 RESULTS",
            "text": "Table 1 shows the rate of success for different models at the end of 10 turns of interaction. We see that training on informatively examples results in large gains in performance. Both the PRAGMATIC and HFT models significantly outperform the literal model for all three criteria of success. Looking at the aggregate success rate across turns in Table 2 reveals that it is not just that the PRAGMATIC synthesizer eventually catches up to the HFT model, but also performs on par with it over the course of the interaction. Figure 3 shows the progression of each metric over the course of interaction. In contrast, we see that GPT-3.5 performs worse than the literal model. One reason for this could be that the distribution of regular expressions that the GPT-3.5 encounters in its training data could be quite different, leading to worse performance. In conclusion, the experiments validate our hypothesis that humans communicate more effectively with the model trained on informative examples (the PRAGMATIC and HFT models) than with a model trained on randomly chosen examples (the LITERAL model).\nExamples of synthesized programs Figure 4 shows examples of guesses by the LITERAL and PRAGMATIC models given the same sequence of examples. In the first case, we see that the PRAGMATIC model is able to infer that if a user wanted a regular expression that accepted 4A, they would have specified that, and instead correctly guesses that the user wanted at least two A s in the string. The second example also shows how the LITERAL model synthesizes a regular expression that is correct, but is too specific, while the PRAGMATIC model recovers the correct generalization."
        },
        {
            "heading": "5 ANALYSIS OF TRAINING",
            "text": "Figure 5 shows the progression of the TOP-1 metric over the course of different rounds of training. We see that as we train the model for more rounds, the performance of the model generally increases, and then tapers off. This shows that as the model is trained for more rounds, it gets increasingly pragmatic. Since the model we choose for the user study is trained for 5 rounds, on 5\u00d71024 = 5120 programs, we also compare to training the model for only a single round on the same number of programs. In a replay study (similar to how we evaluated GPT-3.5; Figure 5), we find that iteratively generating data and updating the model performs better. We also see that finetuning the base model on 400 examples (to match the HFT setting) from a later round of training also results in a strong model, suggesting that as the speaker is trained more, it generates examples that are useful to finetune a listener model."
        },
        {
            "heading": "6 RELATED WORK",
            "text": "Neural network models of pragmatic reasoning Prior work has applied the the RSA pragmatic reasoning framework to improve neural models at inference time for tasks including image captioning (Andreas & Klein, 2016; Cohn-Gordon et al., 2018), instruction generation (Fried et al., 2018a), vision-and-language navigation (Fried et al., 2018b), and machine translation (Cohn-Gordon & Goodman, 2019). These approaches differ from ours because (1) they require human data to train the base models, while our approach does not and (2) they typically require multiple calls to the speaker and listener models at inference time. Our approach amortizes these calls into training time by generating data, allowing for much faster inference.\nOther approaches have used pragmatically-motivated training procedures. The closest works to ours are White et al. (2020) and Lazaridou et al. (2020), who use reinforcement learning approaches to fine-tune a speaker model using reward from a fixed listener model. Monroe & Potts (2015) and McDowell & Goodman (2019) backpropagate through the RSA procedure at training time to\nreason counterfactually. Again, our approach is unique in that we do not require human datas during training, but instead, rely on the consistency matrix M defined by the synthesis task alone. Andreas & Klein (2016) also consider the problem of amortizing (compiling) pragmatic reasoning into a single model, and find that this does not perform as well as inference-time pragmatic reasoning in the image captioning domain. We show that for programming-by-example, we are able to compile pragmatic reasoning into a single model and improve the performance of the model.\nPragmatic reasoning for program synthesis Similar to our work, Vaithilingam et al. (2023) conduct a study of how users interact with an exact RSA pragmatic regular expression synthesizer over a toy domain of \u223c1000 regexes total over strings of only 0s and 1s. Pu et al. (2023) propose a way to make pragmatic PBE more efficient by inferring a global ranking function, but still relies on the expensive exact RSA during training. Our approach is different in that by using neural models for speakers and listeners at training time, we are able to scale to a realistic regex domain. Pertseva et al. also present an approach to version space algebra-based approach to regular expression synthesis from examples by explicitly modeling the probability of examples describing programs (as in our speaker models), but they work with only positive examples (a subset of our example space with examples that have the output \u2713, excluding those with the output \u2717). Ferreira et al. (2021) present an SMT-based method that reasons about distinguishing inputs to synthesize regular expressions. We discuss connections to iterated bootstrapped training for program synthesis in Appendix H."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "In this paper, we present a novel approach to train program synthesis models that can resolve ambiguity in program specification examples by modeling pragmatics: how users choose examples informatively. Our method bootstraps pragmatic synthesizers by iteratively generating data using speaker (example generator) and listener (program synthesizer) models, filtering the generated data with an example selection method drawn from work on computational pragmatics, and further training the speaker and listener models on the generated data. We show that our approach produces pragmatic program synthesizers with minimal supervision: in a challenging regular expression domain, matching the performance of synthesizers trained on human-produced examples, despite not using any human-produced data to train our approach.\nFuture work might explore scaling pragmatic program synthesis further, to domains such as openended Python code generation, which have an even larger space of possible programs and input/output specifications, and where it is even more challenging to collect supervised pairs of programs and specifications. It also opens up the possibility of reasoning about informativity in multimodal specifications \u2014 e.g. with natural language and examples (Ye et al., 2020).\nETHICS STATEMENT\nOur dataset collection process and user study constitute human subjects research. Our studies were deemed exempt from full IRB review by our institution. All participation was voluntary. Participants signed an online consent form, and were compensated fairly for their time."
        },
        {
            "heading": "A PROGRAM SAMPLING",
            "text": "We sample programs from the CONCATENATION and SEPARATION templates from Ye et al. (2020) since the INTERSECTION template is not supported by some popular regular expression libraries like Python\u2019s standard re module. We sample programs with a maximum concatentation depth of 3, and sample programs from the CONCATENATION and SEPARATION templates in a 5:1 ratio using the sampling tools provided by Ye et al. (2020).\nDespite controlling the complexity of programs during sampling, many sampled regular expressions are quite long and difficult for a human to reason about easily. To make the task easier for annotators and user study participants, we use the number of tokens in the regular expression program as a proxy for complexity, and cut off programs that are shown to humans at a maximum length of 30 tokens."
        },
        {
            "heading": "B TRAINING BASE MODELS",
            "text": "We use ByT5 pretrained models (Xue et al., 2022) as the backbone to build all our speaker and listener models. We sample programs as described in Appendix A and generate uninformatively chosen examples for the programs. We then train the models on a sequence-to-sequence task. The input to the listener is a linearized sequence of examples, and the output is the program. The input to the speaker is a program followed by a linearized sequence of prior examples, and the output is the next example in the sequence.\nWe generate random examples by first choosing whether the example is a positive or negative examples with equal probability. If the example is positive, we use the rstr Python package to sample strings that match the target regular expression. If the example is negative, we use the same package to sample a string from the regular expression .* and check that it doesn\u2019t match the target program.\nWe train the base models on 100,000 programs. For each program, we randomly sample 3 specifications. The length of each specification is chosen to be an integer between 0 and 15, uniformly at random. The models are trained for 1 epoch, using the AdamW optimizer with a learning rate of 5 \u00d7 10\u22125, with the learning rate warmed up over the first 10% of training steps, and then decayed linearly. The batch size is set to 32.\nWe also compare to training the base model from a randomly-initalized checkpoint Figure 6, and find that training from the pretrained checkpoint is far more sample-efficient."
        },
        {
            "heading": "C PRAGMATIC MODEL TRAINING",
            "text": "Finally, we finetune the the base models we train by iteratively using the speaker and listener models to generate data, and finetuning on the generated data using the algorithm shown above.\nTo generate the candidates at a particular round r, we use both the base models and the models from round r. Sampling from multiple models with different biases can help us draw a more diverse sample of programs and examples, and get a better approximation of the consistency matrix to perform RSA reasoning over. We draw 250 samples from each of the models we are sampling \u2013 the base listener, the listener from round r, the base speaker, and the speaker from round r.\nAs we note in the algorithm, we update the models by training from the initial parameters (of the base speaker or listener model) on all the datasets generated up to and including that round. We train\nAlgorithm 1 Outer training loop 1: procedure TRAINING(L\u03b80 , S\u03d50 ,H,V, METRIC) 2: Input: Base listener model L\u03b80 3: Input: Base speaker model S\u03d50 4: Input: Set of hypothesesH 5: Input: Validation pairs V 6: Input: Validation metric METRIC 7: Output: Trained listener model L\u03b8 8: for r = 0 to Rmax do 9: Sample set of k hypotheses H \u223c H 10: Dr \u2190 {(h, GENERATEPRAGMATICSPECIFICATION(h)) | h \u2208 H} 11: L\u03b8r+1 \u2190 TRAINLISTENER(L\u03b80 , \u22c3r j=0Dj)\n12: S\u03d5r+1 \u2190 TRAINSPEAKER(S\u03d50 , \u22c3r\nj=0Dj) 13: H \u2190 H \\H 14: end for 15: return argmaxr METRIC(L\u03b8r ,V) 16: end procedure\nAlgorithm 2 Approximate RSA inference 1: procedure GENERATEPRAGMATICSPECIFICATION(L\u03b8, S\u03d5, h) 2: Input: Listener model L\u03b8 3: Input: Speaker model S\u03d5 4: Input: Target program h 5: Output: Specification s 6: s\u2190 [ ] 7: for i = 1 to Nexamples do 8: \u25b7 Sample examples from S\u03d5 conditioned on s and filter for consistency with h 9: E \u2190 GETEXAMPLECANDIDATES(S\u03d5, h, s) 10: \u25b7 Sample programs from L\u03b8 conditioned on s and filter for consistency with s 11: D \u2190 GETDISTRACTORS(L\u03b8, s) 12: for p in D \u222a {h} do 13: for e in E do 14: \u25b7 Populate consistency matrix with whether p is consistent with e 15: M [e, p]\u2190 1[p \u22a2 e] 16: end for 17: end for 18: \u25b7 Compute the literal listener distribution over the sample M of the consistency matrix 19: for p in D \u222a {h} do 20: for e in E do 21: Lliteral[e, p]\u2190 M [e,p]\u2211\np\u2032 M [e,p \u2032]\n22: end for 23: end for 24: \u25b7 Populate consistency matrix with whether p is consistent with e 25: for p in D \u222a {h} do 26: for e in E do 27: SRSA[e, p]\u2190 Lliteral[e,p]\u2211\ne\u2032 Lliteral[e \u2032,p]\n28: end for 29: end for 30: e\u2217 \u2190 argmaxe SRSA[e, h] 31: s\u2190 s\u2295 [e\u2217] 32: end for 33: return s 34: end procedure\nfor for Rmax = 20 rounds with k = 1024 programs per round, generating specifications with up to Nutterances = 10 examples. We choose the model trained for 4 rounds based on the TOP-1 metric on the validation set of 40 examples. At each round we update the models using the AdamW optimizer for 1 epoch, using a learning rate of 5\u00d7 10\u22125. The batch size is set to 32."
        },
        {
            "heading": "D DATASET COLLECTION",
            "text": "We recruited 18 computer science graduate students. The data collection task proceeded in two phases \u2013 specification creation and specification verification. Some participants did the both tasks, while some only did the verification task. Participants who did both tasks spent approximately 3 hours on the task and were compensated with $45, while those who did the verification task spent approximately 2 hours and were compensated with $30. Participants were allowed to perform the task online at a location of their choice, and were allowed to perform the task across multiple sittings.\nEach participant was given a short tutorial on the syntax and semantics of regular expressions, and small quiz where they had to enter three positive and three negative examples for a given regular expression to proceed to the rest of the study. Participants were then told about a communication game, and to provide examples that they would use if they were asking another person to write the regular expression for them \u2013 as they might on an online forum like StackOverflow.\nParticipants then went to the annotation screen for the specification creation task (if they took part, if not they proceeded directly to the next stage), where they were shown a regular expression and asked to enter examples consistent with it, in the interface shown in Figure 9. Consistency was automatically checked, so participants could not enter inconsistent examples. A minimum number of examples between 5 and 7 was randomly picked for each instance (to allow the participants from simply providing the same minimum number for each program), but no limit on the number of examples was placed. Participants created specifications for 40 programs before proceeding to the verification task.\nFor the verification task, participants are shown a specification and 5 regular expressions, of which the target regular expression (that the specification was written for) is one, as shown in Figure 10. The others are distractors. To ensure the distractors are not too easy to distinguish from the target, we sample distractors using the LITERAL model. Of the 4 distractors, we attempt to find 2 which are consistent a subset of the specification, and 2 which are inconsistent with the specification. Since providing the entire specification might lead to too few consistent programs remaining, or distractors that are too hard to tell apart from the target, we use only the first example of the specification as\ninput to the LITERAL model to generate distractors. Depending on whether they did the specification creation task, participants verify 40-90 specifications.\nThe dataset contains 440 examples in total. Of these, 423 have 2 verifications and 17 have one verification. Of the 440, 352 (80%) have two verifications that recover the target. 406 (92%) of the 440 have at least one verification recovering the target. We use 40 of the programs with two verifications recovering the target as the validation set."
        },
        {
            "heading": "E USER STUDY",
            "text": "To conduct the user study, we recruited 11 computer science graduate students. Participants spent approximately 1.5 hours on the task and were compensated with $25. Participants were allowed to perform the task online at a location of their choice, and were allowed to perform the task across multiple sittings. Figure 11 shows the UI.\nEach participant was given a short tutorial on the syntax and semantics of regular expressions, and small quiz where they had to enter three positive and three negative examples for a given regular expression to proceed to the rest of the study. Participants were then told about a communication game, and informed that they would be playing the role of the speaker in the game and describing a target program. Each participant interacted with the three models in order \u2013 PRAGMATIC, LITERAL, and HFT \u2013 and cycled through the list in order, but first model they encountered was chosen randomly. Each model was assigned a color, and the participants were told the color of the model they were interacting with, but not given any other details about it. Participants provided examples one at a time, and observed the synthesizer\u2019s highest ranked guess at every step. If the guess matched the target program, the interaction ended. Participants were given the option to move to the next task after providing 10 examples. Each participant completed 31 instances they provided examples for 31 different programs, with one of the interactions from one of the particpicipants being excluded due to an error."
        },
        {
            "heading": "F PROMPTING AN LLM",
            "text": "We prompt the gpt-3.5-turbo-instruct variant of GPT-3.5. We provide examples of 5 programspecification pairs in the context of a code snippet, followed by a set of examples. We sample 10 generations with temperature t = 0.7 and p = 0.9. We sample for a maximum of 64 tokens, or stop tokens that indicate that the regular expression is complete are sampled.\nWe rank the generated completions using the sum of token log probabilities, and choose the highest scoring program consistent with the given answer as the top-1 guess, and other consistent programs as the top-10 guesses to calculate the TOP-1 and TOP-10 metrics."
        },
        {
            "heading": "G SPEAKER QUALITY",
            "text": "One measure of the speaker\u2019s quality is the loss evaluated in examples provided by humans. Evaluating loss allows us to see if human provided examples are more likely under a model\u2019s distribution, without penalizing the model for the exact example not appearing in a set of examples decoded from the model (since there are many possible examples that are consistent with a program, and multiple possible informative examples too). We see in Figure 12 that even without being trained on any human-provided examples, the speaker loss on human-provided validation examples reduces."
        },
        {
            "heading": "H CONNECTIONS TO ITERATED BOOTSTRAPPED TRAINING",
            "text": "Methods such as DreamCoder (Ellis et al., 2021) and Memoized Wake Sleep (Hewitt et al., 2020) use iterated bootstrap training to build a hierarchy of abstractions to solve tasks more effectively\nover iterations of training. At each iteration of training, they create tasks by sampling a program from a changing library of abstractions, and executing the program on a fixed distribution of inputs to train a synthesizer to work with updated abstractions.\nOur method on the other hand is aimed at allowing a synthesizer to reason about how examples are chosen to demonstrate programs from a fixed library. We change the distribution of examples being drawn from a trained speaker model over the course of iterated bootstrapped training towards more informative examples."
        }
    ],
    "title": "TRAIN NEURAL PROGRAM SYNTHESIZERS",
    "year": 2023
}