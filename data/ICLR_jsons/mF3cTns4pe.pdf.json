{
    "abstractText": "Daily internet communication relies heavily on tree-structured graphs, embodied by popular data formats such as XML and JSON. However, many recent generative (probabilistic) models utilize neural networks to learn a probability distribution over undirected cyclic graphs. This assumption of a generic graph structure brings various computational challenges, and, more importantly, the presence of non-linearities in neural networks does not permit tractable probabilistic inference. We address these problems by proposing sum-product-set networks, an extension of probabilistic circuits from unstructured tensor data to tree-structured graph data. To this end, we use random finite sets to reflect a variable number of nodes and edges in the graph and to allow for exact and efficient inference. We demonstrate that our tractable model performs comparably to various intractable models based on neural networks.",
    "authors": [],
    "id": "SP:4692924ac06b6573a4db7b51e64886d188b3ec13",
    "references": [
        {
            "authors": [
                "REFERENCES David J Aldous"
            ],
            "title": "Representations for partially exchangeable arrays of random variables",
            "venue": "Journal of Multivariate Analysis,",
            "year": 1981
        },
        {
            "authors": [
                "Ole Barndorff-Nielsen"
            ],
            "title": "Information and exponential families: in statistical theory",
            "year": 1978
        },
        {
            "authors": [
                "P. Billingsley"
            ],
            "title": "Probability and Measure",
            "venue": "Wiley Series in Probability and Statistics. Wiley,",
            "year": 1995
        },
        {
            "authors": [
                "Hei Chan",
                "Adnan Darwiche"
            ],
            "title": "On the robustness of most probable explanations",
            "venue": "In Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence,",
            "year": 2006
        },
        {
            "authors": [
                "Y Choi",
                "Antonio Vergari",
                "Guy Van den Broeck"
            ],
            "title": "Probabilistic circuits: A unifying framework for tractable probabilistic models",
            "venue": "UCLA. URL: http://starai. cs. ucla. edu/papers/ProbCirc20",
            "year": 2020
        },
        {
            "authors": [
                "Daryl J Daley",
                "David Vere-Jones"
            ],
            "title": "An introduction to the theory of point processes: volume I: elementary theory and methods",
            "year": 2003
        },
        {
            "authors": [
                "Adnan Darwiche"
            ],
            "title": "A differential approach to inference in bayesian networks",
            "venue": "Journal of the ACM (JACM),",
            "year": 2003
        },
        {
            "authors": [
                "Bruno de Finetti"
            ],
            "title": "Funzione caratteristica di un fenomeno aleatorio",
            "venue": "In Atti del Congresso Internazionale dei Matematici: Bologna del 3 al 10 de settembre di 1928,",
            "year": 1928
        },
        {
            "authors": [
                "Bruno de Finetti"
            ],
            "title": "Foresight: Its logical laws, its subjective sources",
            "venue": "In Breakthroughs in Statistics: Foundations and Basic Theory,",
            "year": 1937
        },
        {
            "authors": [
                "Persi Diaconis"
            ],
            "title": "Finite forms of de Finetti\u2019s theorem on",
            "venue": "exchangeability. Synthese,",
            "year": 1977
        },
        {
            "authors": [
                "Persi Diaconis"
            ],
            "title": "Recent progress on de finetti\u2019s notions of exchangeability",
            "venue": "Bayesian statistics,",
            "year": 1988
        },
        {
            "authors": [
                "Persi Diaconis"
            ],
            "title": "Sufficiency as statistical symmetry",
            "venue": "In Proceedings of the AMS Centennial Symposium,",
            "year": 1988
        },
        {
            "authors": [
                "Persi Diaconis",
                "David Freedman"
            ],
            "title": "Finite exchangeable sequences",
            "venue": "The Annals of Probability, pp",
            "year": 1980
        },
        {
            "authors": [
                "Persi Diaconis",
                "David Freedman"
            ],
            "title": "Partial exchangeability and sufficiency",
            "venue": "Statistics: applications and new directions,",
            "year": 1984
        },
        {
            "authors": [
                "Charles J Geyer"
            ],
            "title": "Likelihood inference for spatial point processes: Likelihood and computation. In Stochastic Geometry: Likelihood and Computation, pp. 141\u2013172",
            "venue": "Chapman and Hall/CRC,",
            "year": 1999
        },
        {
            "authors": [
                "IR Goodman",
                "RP Mahler",
                "Hung T Nguyen"
            ],
            "title": "Mathematics of Data Fusion, volume 37",
            "venue": "Springer Science & Business Media,",
            "year": 1997
        },
        {
            "authors": [
                "Geoffrey Grimmett",
                "David Stirzaker"
            ],
            "title": "Probability and random processes",
            "venue": "Oxford university press,",
            "year": 2001
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Stefan L\u00fcdtke",
                "Christian Bartelt",
                "Heiner Stuckenschmidt"
            ],
            "title": "Exchangeability-aware sum-product networks",
            "venue": "In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Ronald PS Mahler"
            ],
            "title": "Statistical multisource-multitarget information fusion, volume 685",
            "venue": "Artech House Norwood, MA, USA,",
            "year": 2007
        },
        {
            "authors": [
                "G. Math\u00e9ron"
            ],
            "title": "Random Sets and Integral Geometry",
            "venue": "Wiley Series in Probability and Mathematical Statistics. Wiley,",
            "year": 1974
        },
        {
            "authors": [
                "Jan Motl",
                "Oliver Schulte"
            ],
            "title": "The CTU Prague relational learning repository",
            "venue": "arXiv preprint arXiv:1511.03086,",
            "year": 2015
        },
        {
            "authors": [
                "Aniruddh Nath",
                "Pedro Domingos"
            ],
            "title": "Learning relational sum-product networks",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Hung T Nguyen"
            ],
            "title": "An introduction to random sets",
            "venue": "Chapman and Hall/CRC,",
            "year": 2006
        },
        {
            "authors": [
                "Mathias Niepert",
                "Pedro Domingos"
            ],
            "title": "Exchangeable variable models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Mathias Niepert",
                "Guy Van den Broeck"
            ],
            "title": "Tractability through exchangeability: A new perspective on efficient probabilistic inference",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2014
        },
        {
            "authors": [
                "Peter Orbanz",
                "Daniel M Roy"
            ],
            "title": "Bayesian models of graphs, arrays and other exchangeable random structures",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2014
        },
        {
            "authors": [
                "Robert Peharz",
                "Sebastian Tschiatschek",
                "Franz Pernkopf",
                "Pedro Domingos"
            ],
            "title": "On theoretical properties of sum-product networks",
            "venue": "In 18th International Conference on Artificial Intelligence and Statistics,",
            "year": 2015
        },
        {
            "authors": [
                "Robert Peharz",
                "Steven Lang",
                "Antonio Vergari",
                "Karl Stelzner",
                "Alejandro Molina",
                "Martin Trapp",
                "Guy Van den Broeck",
                "Kristian Kersting",
                "Zoubin Ghahramani"
            ],
            "title": "Einsum networks: Fast and scalable learning of tractable probabilistic circuits",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Hoifung Poon",
                "Pedro Domingos"
            ],
            "title": "Sum-product networks: A new deep architecture",
            "venue": "IEEE International Conference on Computer Vision Workshops (ICCV Workshops),",
            "year": 2011
        },
        {
            "authors": [
                "Yujia Shen",
                "Arthur Choi",
                "Adnan Darwiche"
            ],
            "title": "Tractable operations for arithmetic circuits of probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Martin Trapp",
                "Robert Peharz",
                "Hong Ge",
                "Franz Pernkopf",
                "Zoubin Ghahramani"
            ],
            "title": "Bayesian learning of sum-product networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "MNM Van Lieshout"
            ],
            "title": "Markov point processes and their applications",
            "venue": "World Scientific,",
            "year": 2000
        },
        {
            "authors": [
                "Antonio Vergari",
                "Nicola Di Mauro",
                "Floriana Esposito"
            ],
            "title": "Visualizing and understanding sumproduct networks",
            "venue": "Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Antonio Vergari",
                "YooJung Choi",
                "Anji Liu",
                "Stefano Teso",
                "Guy Van den Broeck"
            ],
            "title": "A compositional atlas of tractable circuit operations: From simple transformations to complex informationtheoretic queries",
            "venue": "arXiv preprint arXiv:2102.06137,",
            "year": 2021
        },
        {
            "authors": [
                "Ba-Ngu Vo",
                "Nhan Dam",
                "Dinh Phung",
                "Quang N Tran",
                "Ba-Tuong Vo"
            ],
            "title": "Model-based learning for point pattern data",
            "venue": "Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Alan J Weir"
            ],
            "title": "Lebesgue Integration and Measure, volume 1",
            "year": 1973
        },
        {
            "authors": [
                "Sandy L Zabell"
            ],
            "title": "Symmetry and its discontents: essays on the history of inductive probability",
            "year": 2005
        },
        {
            "authors": [
                "Han Zhao",
                "Pascal Poupart",
                "Geoff Gordon"
            ],
            "title": "A unified approach for learning the parameters of sum-product networks",
            "venue": "In Proceedings of the 30th Advances in Neural Information Processing Systems,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "One of the essential paradigm shifts in artificial intelligence and machine learning over the last years has been the transition from probabilistic models over fixed-size unstructured data (tensors) to probabilistic models over variable-size structured data (graphs) (Bronstein et al., 2017; Wu et al., 2021). Tree-structured data are a specific type of generic graph-structured data that describe real or abstract objects (vertices) and their hierarchical relations (edges). These data structures appear in many scientific domains, including cheminformatics (Bianucci et al., 2000), physics (Kahn et al., 2022), and natural language processing (Ma et al., 2018). They are used by humans in data-collecting mechanisms to organize knowledge into various machine-generated formats, such as JSON (Pezoa et al., 2016), XML (Tekli et al., 2016) and YAML (Ben-Kiki et al., 2009) to mention a few. Designing a probabilistic model for these tree-structured file formats is one of the key motivations of this paper.\nThe development of models for tree-structured data has been thoroughly, but almost exclusively, pursued in the NLP domain (Tai et al., 2015; Zhou et al., 2016; Cheng et al., 2018; Ma et al., 2018). Unfortunately, these models rely solely on variants of neural networks (NNs), are non-generative, and lack clear probabilistic interpretation. There has also recently been growing interest in designing generative models for general graph-structured data (Simonovsky & Komodakis, 2018; De Cao & Kipf, 2018; You et al., 2018; Jo et al., 2022; Luo et al., 2021). However, the underlying principle of these generic models is to perform the message passing over a neighborhood of each node in the graph, meaning that they visit many of the nodes several times. Directly applying them to the trees, and not adapting them to respect the parent-child ancestry of the trees, would incur unnecessary computational costs. Moreover, these models assume that the features are assigned to each node and all have the same dimension. More importantly, they preclude tractable probabilistic inference, necessitating approximate techniques to answer even basic queries.\nIn sensitive applications (e.g., healthcare, finance, and cybersecurity), there is an increasing legal concern about providing non-approximate and fast decision-making. Probabilistic circuits (PCs) (Vergari et al., 2020) are tractable probabilistic (generative) models that guarantee to answer a large family of complex probabilistic queries (Vergari et al., 2021) exactly and efficiently. For instance, the marginal queries form the fundamental part of many more advanced queries. Their practicality lies in that they allow us to consistently deal with missing data. In this paper, we are interested in a specific sub-type of PCs\u2014sum-product networks (SPNs)\u2014which represent a probability density over fixed-size, unstructured, data Poon & Domingos (2011).\nTo our knowledge, no SPN is designed to represent a probability distribution over a variable-size, tree-structured, graph. Here, it is essential to note that an SPN is also a graph. To distinguish between the two graphs, we refer to the former as the data graph and the latter as the computational graph. Similarly, to distinguish between the vertices of these two graphs, we refer to vertices of the data graph and computational graph as data nodes and computational units, respectively. We propose a new SPN (i.e., a new type of PCs) by seeing the data graph as a recursive hierarchy of sets, where each parent data node is a set of its child data nodes. As oposed to the models for the generic graphs, our model respects the parent-child direction when passing the data graph. We use the theory of random finite sets (Nguyen, 2006) to induce a probability distribution over repeating data subgraphs, i.e., sets with identical properties. This allows us to extend the computational graph with an original computational unit, a set unit. Our model provides an efficient sampling of new data graphs and exact marginal inference over selected data nodes, which allows us to deal with missing parts of the JSON files. It also permits the data nodes to have heterogeneous features, where each node can represent data of different dimensions and modalities.\nIn summary, this paper offers the following contributions:\n\u2022 We propose sum-product-set networks (SPSNs), extending the predominant focus of PCs from unstructured tensor data to tree-structured graph data (Section 3).\n\u2022 We show that SPSNs respecting the standard structural constraints of PCs are tractable under mild assumptions on the set unit (Section 3.1).\n\u2022 We investigate the exchangeability of SPSNs, concluding that SPSNs are permutation invariant under the reordering of the arguments in the set units and input units (if the input units admit some form of exchangeability) and that the invariance propagates through SPSNs fundamentally based on the structural constraints (Section 3.2).\n\u2022 We show that SPSNs deliver competitive performance to intractable models relying on much more densely connected and highly nonlinear NNs, which are unequipped to provide exact answers to probabilistic queries (Section 5)."
        },
        {
            "heading": "2 TREE-STRUCTURED DATA",
            "text": "A single instance of tree-structured, heterogeneous data is given by an attributed data graph, T . In contrast to a fixed-size, unstructured, random variable, x = (x1, . . . , xn) \u2208 X \u2286 Rn, this graph forms a hierarchy of random-size sets.\nDefinition 1. (Data graph). T := (V,E,X) is an attributed, tree-structured graph, where V is a set of vertices, E is a set of edges, and X is a set of attributes (features). V := (L,H,O) splits into three subsets of data nodes: leaf nodes, L, heterogeneous nodes, H , and homogeneous nodes, O. Let ch(v) and pa(v) denote the set of child and parent nodes of v \u2208 V , respectively. All elements of ch(v) are of an identical type if v \u2208 O, and some or all elements of ch(v) are of a different type if v \u2208 H . We assume that only the leaf nodes are attributed by xv \u2208 Xv \u2286 Rnv , with possibly different space and its dimension for each v \u2208 L.\nDefinition 2. (Schema). Let T be an instance of tree-structured, heterogeneous data. Then, a subtree, S, which results from T by following all children of each v \u2208 H and only one child of each v \u2208 O\u2014such that it allows us to reach the deepest level of T\u2014is referred to as the schema1.\nWe model each heterogeneous node, Tv := (Tw1 , . . . , Twm) \u2208 Tv , v \u2208 H , as a finite set. It is an ordered set of features or other sets, Tw \u2208 Tw, where the elements are random but their number,m \u2208 N0, is fixed. Tv is the Cartesian product space composed of the spaces corresponding to the elements of Tv . Importantly, we propose to model each homogeneous node, Tv := {Tw1 , . . . , Twm} \u2208 Tv , v \u2208 O, as a finite random set (RFS), i.e., a simple, finite point process (Van Lieshout, 2000; Daley et al., 2003; Nguyen, 2006; Mahler, 2007). This is an unordered set of distinct features or other sets, such that not only the individual elements, Tw \u2208 Tw, are random, but also their number, m \u2208 N0, is random. Tv := F(Tw) is the hyperspace of all finite subsets of some underlying space Tw (which is common to all elements). We refer to Section B for more details on RFSs. The leaf node, Tv := xv = (x1 . . . , xm), v \u2208 L, contains a feature vector (i.e., it is also a finite set). Tv is usually a subspace of Rmv . We show an example of a single instance of tree-structured data in Figure 1, where T = (x, x, x, x, {(x, {(x, x, x, x), (x, x, x, x)}, x, x), . . . , (x, {(x, x, x, x)}, x, x)}). The tree T is defined on a hybrid space T , i.e., a product space composed of continuous spaces, discrete spaces, but also hyperspaces of other RFSs. Each instance of T is thus a hierarchy of several RFSs. Indeed, such constructions are possible and are used to define \u201crandom cluster processes\u201d (Mahler, 2001; Mahler & MN, 2002).\nNote from Definition 1 that, for heterogeneous nodes, v \u2208 H , each child subtree, Tw, has a different schema for all w \u2208 ch(v); whereas for homogeneous nodes, v \u2208 O, each child, Tw, has the same schema for all w \u2208 ch(v). This implies that T is defined recursively by a subtree, Tv := [Tw1 . . . , Twm ], rooted at v \u2208 V , where the parentheses [\u00b7] instantiate into {\u00b7} for homogeneous nodes, and into (\u00b7) for heterogeneous nodes, in order to distinguish if the number of elements is random or not. Note also that the cardinality of all homogeneous nodes, v \u2208 O, differs for each instance of T , while it remains the same for all heterogeneous nodes, v \u2208 H . Problem definition. Our objective is to learn a probability density over tree-structured graphs (Definition 1), p(T ), given a collection of observed graphs {T1, . . . , Tm}, where each Ti contains a different number of vertices and edges but follows the same schema."
        },
        {
            "heading": "3 SUM-PRODUCT-SET NETWORKS",
            "text": "A sum-product-set network (SPSN) is a probability density over a tree-structured graph, p(T ). This differs from the conventional SPN (Poon & Domingos, 2011), which is a probability density over the\n1Recall that one the key motivations of this paper is to design a probabilistic model for the tree-structured file formats (Section 1). The intuition behind Definition 2 is motivated by the schema of the JSON files (Pezoa et al., 2016). We do not consider the schema used in the relational databases.\nunstructured vector data, p(x). For a short introduction to SPNs, see Section A in the supplementary material. We define an SPSN by a parameterized computational graph, G, and a scope function, \u03c8. Definition 3. (Computational graph). G := (V, E , \u03b8) is a parameterized, directed, acyclic graph, where V is a set of vertices, E is set of edges, and \u03b8 \u2208 \u0398 are parameters. V := (S,P,B, I) contains four subsets of computational units: sum units, S, product units, P, set units, B, and input units, I. The sum units and product units have multiple children; however, as detailed later, the set unit has only two children, ch(u) := (a, b), u \u2208 B. \u03b8 contains parameters of sum units, i.e., non-negative and normalized weights, (wu,c)c\u2208ch(u), wu,c \u2265 0, \u2211 c\u2208ch(u) wu,c = 1, u \u2208 S, and parameters of input units which are specific to possibly different densities. Definition 4. (Scope function). The mapping \u03c8 : V \u2192 F(T )\u2014from the set of units to the power set of T \u2208 T\u2014outputs a subset of T for each u \u2208 V and is referred to as the scope function. If u is the root unit, then \u03c8u = T . If u is a sum unit, product unit, or set unit, then \u03c8u = \u22c3 c\u2208ch(u) \u03c8c.\nEach unit of the computational graph (Definition 3), u \u2208 V , induces a probability density over a given (subset of) node(s) of the data graph (Definition 1), v \u2208 V . The functionality of this density, pu(Tv), depends on the type of the computational unit.\nThe sum unit computes the mixture density, pu(Tv) = \u2211\nc\u2208ch(u) wu,cpc(Tv), u \u2208 S, v \u2208 (L,H), where wu,c is the weight connecting the sum unit with a child unit. The product unit computes the factored density, pu(Tv) = \u220f c\u2208ch(u) pc(\u03c8c), u \u2208 P, u \u2208 (L,H). It introduces conditional independence among the scopes of its children, \u03c8c, establishing unweighted connections between this unit and its child units. The input unit computes a user-defined probability density, pu(xv), u \u2208 I, v \u2208 L. It is defined over a subset xv of Tv := xv given by the scope, \u03c8u, which can be univariate or multivariate (Peharz et al., 2015).\nThe newly introduced set unit computes a probability density of an RFS,\npu(Tv) = p(m)c mm!p(Tw1 , . . . , Twm), (1)\nu \u2208 B, v \u2208 O, where p(m) is the cardinality distribution, and p(Tw1 , . . . , Twm) is the feature density (conditioned on m). These are the two children of the set unit (Definition 3), spanning computational subgraphs on their own (Figure 2). The proportionality factor, m! = \u220fm i=1 i, comes from the symmetry of p(Tw1 , . . . , Twm). It reflects the fact that p(Tw1 , . . . , Twm) is permutation invariant, i.e., it gives the same value to all m! possible permutations of {Tw1 , . . . , Twm}. Recall from Section 2 that Tv \u2208 F(Tw), where Tw is the underlying space of the RFS. c is a constant with units of hyper-volume of Tw, which ensures that (1) is unit-less by canceling out the units of p(Tw1 , . . . , Twm) with the units of c\nm. This becomes important when computing integrals with (1). Section B provides more details on the subject of integrating functions of RFSs. Note that, contrary to the other units, the set unit is defined only for the homogeneous node. Assumption 1. (Requirements on the set unit). The requirements for a probability density of the set unit to be correctly defined are as follows: (a) each element of Tv := {Tw1 , . . . , Twm} resides in the same space, i.e., Tw \u2208 Tw, for all w \u2208 ch(v); (b) the elements {Tw1 , . . . , Twm} are independent and identically distributed; (c) the realizations {Tw1 , . . . , Twm} are distinct; and (d) the cardinality distribution p(m) = 0 for a sufficiently large m.\nGiven that Assumption 1 is satisfied, (1) contains the product of m densities, p(Tw1 , . . . , Twm) =\u220fm i=i p(Twi) over the identical scope (i.e., not the disjoint scope and, therefore, it is not the product unit). The feature density treats {Twi}mi=i as instances, aggregating them by the product of densities. It is a single density indexed by the same set of parameters for each {Twi}mi=i. For example, if p(m) is the Poisson distribution, then (1) is the Poisson point process (Grimmett & Stirzaker, 2001).\nConstructing SPSNs. We illustrate the design of SPSNs in Figure 2. The construction begins with extracting the schema of the data graph (Figure 2(a) which corresponds to the example of T in Figure 1). Then, starting from the top of the schema, we create an SPSN block for each of the heterogeneous nodes (Figure 2(b)). The SPSN block (Figure 2(c)) alters many layers of sum units and product units (green). Every time there is a product layer, the heterogeneous node, Tv := (Tw1 , . . . , Twm), is split into two (or multiple depending on the number of children of the product units) parts, Tv1 and Tv2 . This process is repeated recursively until Tv1 and Tv2 are either singletons or subsets that result from a user-defined limit on the maximum number of product layers in the block. Each of these subsets or singletons is then modeled by the input unit (blue). Importantly,\nthis reduction always has to separate all homogeneous nodes from Tv as single elements modeled by the set unit (orange). Note that the sum units create duplicate parts of the computational graph (children have an identical scope), which we leave out in our illustration for simplicity (the dashed line). The consequence is that there will be multiple set units. We gather all edges leading from the feature density of the set unit and connect them to the block (Figure 2(b)) modeling the subsequent heterogeneous node in the schema (Figure 2(a)). We provide a detailed algorithm to construct SPSNs in Section E, along with a simple example without the block structures (Figure 4).\nHyper-parameters. The key hyper-parameter is the number of layers of the SPSN block, nl. We consider that a single SPSN layer comprises one sum layer and one product layer. The other hyperparameters are the number of children of all sum units, ns, and product units, np, which are common across all layers (nl = 2, ns = 2, and np = 2 in Figure 2(c))."
        },
        {
            "heading": "3.1 TRACTABILITY",
            "text": "Tractability is the cornerstone of PCs. A PC is tractable if it performs probabilistic inference exactly and efficiently (Choi et al., 2020; Vergari et al., 2021). In other words, the probabilistic queries are answered without approximation (or heuristics) and in time, which is polynomial in the number of edges of the computational graph. Various standard probabilistic queries (e.g., moments, marginal queries) can be defined in terms of the following integral:\n\u03bd(f) = \u222b f(T )p(T )\u03bd(dT ), (2)\nwhere f : T \u2192 R is a function that helps us to formulate probabilistic queries, and \u03bd is a reference measure on T (Section 2). The composite nature of T imposes a rather complex structure on \u03bd. It contains measures specifically tailored for RFSs, which makes the integration different compared to the standard Lebesgue measure (see Section B and Section D.2 for details).\nStructural constraints. If (2) admits an algebraically closed-form solution, then SPSNs are tractable. This is what we demonstrate in this section. To this end, both f and p have to satisfy certain restrictions on their structure, which we present in Definition 5 and Definition 6.\nDefinition 5. (Structural constrains on f .) Let f(T ) := \u220f\nu\u2208L fu(\u03c8u) be a factorization of f , where (\u03c8u)u\u2208L are pairwise disjoint scopes that are unique among all the input units.\nNote that the collection (\u03c8u)u\u2208I is a multiset since there are usually repeating scopes among all u \u2208 I. These repetitions result from the presence of sum units in the computational graph, as they have children with identical scopes. The collection (\u03c8u)u\u2208L, on the other hand, contains no repeating elements, i.e., L \u2286 I, are the input units with a unique scope. We can obtain this set if we follow only a single child of each sum unit when traversing the computational graph from the root to the inputs (the induced tree (Zhao et al., 2016; Trapp et al., 2019)).\nDefinition 6. (Structural constraints on p). We consider the following constraints on G. Smoothness: children of any sum unit have the same scopes, i.e., each u \u2208 S satisfies \u2200a, b \u2208 ch(u) : \u03c8a = \u03c8b. Decomposability: children of any product unit have pairwise disjoint scopes, i.e., each u \u2208 P satisfies \u2200a, b \u2208 ch(u) : \u03c8a \u2229 \u03c8b = \u2205.\nThe SPSNs are amenable to the standard structural constraints used in PCs (Shen et al., 2016; Vergari et al., 2021). The set unit does not violate these constraints since the cardinality distribution and the feature density are computational subgraphs given by the SPSN units (Definition 3).\nDefinition 5 allows us to target an arbitrary part of the data graph, T , which is spanned from a given data node, v \u2208 V , (Definition 1). That is, we can define T := T\u2212v \u222a Tv , where Tv is composed of the subsets of T that are reachable from v, and T\u2212v is the complement. Consider f(T ) := 1A(T ), where the indicator function 1A(T ) := 1 if T \u2208 A, for a measurable subset A \u2286 T , or 1A(T ) := 0 otherwise. Now, let A := E\u2212v \u00d7 Av , where E\u2212v \u2208 T\u2212v is an evidence assignment (a specific realization) corresponding to T\u2212v , and Av \u2286 Tv is a measurable subset corresponding to Tv . Then, the integral (2) yields the marginal query P (E\u2212v, Av). This query is useful if there is a (subset of) node(s) with missing values, e.g., a leaf node, v \u2208 L, which we demonstrate in Section 5. Proposition 1. (Tractability of SPSNs). Let p(T ) be an SPSN satisfying Assumption 1 and Definition 6, and let f(T ) be a function satisfying Definition 5. Then, the integral (2) is tractable and can\nbe computed recursively as follows:\nIu =  \u2211\u221e k=0 p(k) \u220fk i=1 Ii, for u \u2208 B,\u2211 c\u2208ch(u) wu,cIc, for u \u2208 S,\u220f c\u2208ch(u) Ic, for u \u2208 P,\u222b fu(\u03c8u)pu(\u03c8u)\u03bdu(d\u03c8u), for u \u2208 I,\nwhere the measure \u03bdu(d\u03c8u) instantiates itself either as the Lebesgue measure or the counting measure, depending on the specific form of the scope \u03c8u.\nProof. See Section D.2 in the supplementary material.\nProposition 1 starts the integration by finding a closed-form solution for the integrals w.r.t. the input units, u \u2208 I. The results, Iu, are then recursively propagated in the feed-forward pass through the computational graph and are simply aggregated based on the rules characteristic to the sum, product, and set unit. Specifically, the integration passes through the set unit similarly to the sum and product units. It computes an algebraically closed-form solution consisting of a weighted sum of products of integrals passed from the feature density (i.e., a tractable sub-SPSN). Note that the integration reduces to the one used in the conventional SPNs if there are no set units, see Proposition 5.\nThe infinite sum in the aggregation rule of the set unit (u \u2208 B) in Proposition 1 might give an impression that SPSNs are intractable. However, recall from Assumption 1 that p(k) = 0 for a sufficiently large k (consider, e.g., the Poisson distribution), and the infinite sum therefore becomes a finite one (Remark 1). This is commonly the case in practice since p(k) is learned from collections of graphs with a finite number of edges."
        },
        {
            "heading": "3.2 EXCHANGEBILITY",
            "text": "The study of probabilistic symmetries has attracted increased attention in the neural network literature (Bloem-Reddy & Teh, 2020). On the other hand, the exchangeability of PCs has been investigated marginally. The relational SPNs (Nath & Domingos, 2015) and the exchangeability-aware SPNs (Lu\u0308dtke et al., 2022) are, to the best of our knowledge, the only examples of PCs introducing exchangeable computational units. However, none of them answers the fundamental question about exchangeability: Under what constraints is it possible to permute the arguments of a PC?\nTo define the notion of finite full exchangeability of a probability density (see Section C for details), we use the finite symmetric group of a set of n elements, Sn. This is a set of all n! permutations of [n] := (1, . . . , n), and, any of its members, \u03c0 \u2208 Sn, exchanges the elements of an n-dimensional vector, x := (x1, . . . , xn), in the following way: \u03c0 \u00b7 x = (x\u03c0(1), . . . , x\u03c0(n)). Definition 7. (Full exchangeability). The probability density p is fully exchangeable iff p(x) = p(\u03c0 \u00b7 x) for all \u03c0 \u2208 Sn . We say that x is fully exchangeable if p(x) is.\nThe full exchangeability (complete probabilistic symmetry) is sometimes unrealistic in practical applications. The relaxed notion of finite partial exchangeability (de Finetti, 1937; Aldous, 1981; Diaconis & Freedman, 1984; Diaconis, 1988a;b) admits the existence of several different and related groups where full exchangeability applies within each group but not across the groups.\nTo describe the partial exchangeability, we rely on the product ofm finite symmetric groups, Snm := Sn1\u00d7\u00b7 \u00b7 \u00b7\u00d7Snm , where nm := (n1, . . . , nm). Any member, \u03c0 \u2208 Snm , permutes each ofm elements in the collection, X := (x1, . . . ,xm), individually as follows: \u03c0 \u00b7X = (\u03c01 \u00b7 x1, . . . , \u03c0m \u00b7 xm). Definition 8. (Partial exchangeability). The probability density p is partially exchangeable iff p(X) = p(\u03c0 \u00b7X) for all \u03c0 \u2208 Snm . We say that X is partially exchangeable if p(X) is.\nDefinition 8 allows us to study the exchangeability of probabilistic models in situations where they follow structural limitations that prevent the direct use of full exchangeability. SPSNs respecting Assumption 1 and Definition 6 have a constrained computational graph, which imposes limitations on their input-output behavior. Therefore, it does not apply that exchanging two given nodes in the data graph, T , has no impact on the value of p(T ). We present Proposition 3 to describe under what restrictions the data nodes can be exchanged, how the permutations propagate through the computational graph, and in what sense the exchangeability affects the different types of computational units.\nProposition 2. (Structurally constrained permutations.) Consider a PC satisfying Definition 6. Then, qa \u2208 Q is a permutation operator which targets a specific computational unit, a \u2208 V , and propagates through the computational graph G\u2014from the root to the inputs\u2014in the following way:\npu(qa \u00b7 \u03c8u) =  \u2211 c\u2208ch(u) wu,cpc(qa \u00b7 \u03c8u), for a \u0338= u and u \u2208 S,\u220f v\u2208cha(u) pv(qv \u00b7 \u03c8v) \u220f c\u2208cha(u) pc(\u03c8c), for a \u0338= u and u \u2208 P,\npu(\u03c0 \u00b7 \u03c8u), for a = u and u /\u2208 {S,P},\nwhere cha(u) and cha(u) are children of u that are and are not the ancestors of a, respectively. Consequently, Q is a group that results from the structural restrictions on the computational graph G.\nProof. See Section C.1.\nProposition 2 shows that qa propagates through each sum unit to all its children, passes through each product unit only to those children that are the ancestors of a, and instantiates itself to the permutation, \u03c0, when reaching the targeted unit, a. This recursive mechanism allows us to formulate the exchangeability of SPSNs in Proposition 3. Proposition 3. (Exchangeability of SPSNs). Let p(T ) be an SPSN satisfying Assumption 1 and Definition 6. Let IE \u2208 I be a subset of input units that are exchangeable in the sense of Definition 7 or Definition 8. Then, the SPSN is partially exchangeable, p(qa \u00b7 T ) = p(T ), for each a \u2208 {IE,B}.\nProof. See Section C.2.\nProposition 3 states that changing the order of the arguments corresponding to exchangeable input units and set units does not influence the resulting value of p(T ), i.e., the SPSNs are invariant under the reordering of the elements in the scopes of these units. There must be at least one input unit that is multivariate and exchangeable in the sense of Definition 7 or Definition 8 to satisfy exchangebility w.r.t. IE; otherwise, SPSNs are exchangeable only w.r.t. B. This implies that T can always be exchanged w.r.t. the homogeneous nodes and only the leaf nodes admitting exchangeability. The heterogeneous nodes are not exchangeable. Full exchangeability is possible only when there are no product units, and the input units are fully exchangeable. The presence of product units thus always imposes partial exchangeability. The structural constraints of SPSNs impose a specific type of probabilistic symmetry. In other words, an SPSN can be seen as a probabilistic symmetry structure invariant under the action of a group, Q, resulting from the connections in the computational graph."
        },
        {
            "heading": "4 RELATED WORK",
            "text": "Non-probabilistic models (NPMs). Graph neural networks (GNNs) have become a powerful approach for non-probabilistic representation learning on graphs. Variants of GNNs range from their original formulation (Gori et al., 2005; Scarselli et al., 2008) to GCN (Kipf & Welling, 2017), MPNN (Gilmer et al., 2017), GAT (Velic\u030ckovic\u0301 et al., 2018) and GraphSAGE (Hamilton et al., 2017), among others. They encode undirected cyclic graphs into a low-dimensional representation by aggregating and sharing features from neighboring nodes. However, they repeatedly and unnecessarily visit many of the nodes when applied to graphs with structural constraints, which is a waste of computational resources. This led to the design of GNNs for directed acyclic graphs (Thost & Chen, 2021). GNNs for trees traverse the graph bottom-up (or up-bottom) and update a given node based only on its children. Examples of this approach include RNN (Socher et al., 2011; Shuai et al., 2016), Tree-LSTM (Tai et al., 2015) and TreeNet (Cheng et al., 2018).\nIntractable probabilistic models (IPMs). Extending deep generative models from unstructured to graph-structured data has recently gained significant attention. Variational autoencoders learn a probability distribution over graphs, p(G), by training an encoder and a decoder to map between space of graphs and continuous latent space (Kipf & Welling, 2016; Simonovsky & Komodakis, 2018; Grover et al., 2019). Generative adversarial networks learn p(G) by training (i) a generator to map from latent space to space of graphs and (ii) a discriminator to distinguish whether the graphs are synthetic or real (De Cao & Kipf, 2018; Bojchevski et al., 2018). Flow models use the change of variables formula to transform a base distribution on latent space to a distribution on space of\ngraphs, p(G), via an invertible mapping (Liu et al., 2019; Luo et al., 2021). Autoregressive models learn p(G) by using the chain rule of probability to decompose the graph, G, into a sequence of subgraphs, constructing G node by node (You et al., 2018; Liao et al., 2019). Diffusion models learn p(G) by noising and denoising trajectories of graphs based on forward and backward diffusion processes, respectively (Jo et al., 2022; Huang et al., 2022; Vignac et al., 2022). NMPs are used in all these generative models, so computing their marginal probability density is intractable.\nTractable probabilistic models (TPMs). There has yet to be a substantial interest in probabilistic models facilitating tractable inference for graph-structured data. Graph-structured SPNs (Zheng et al., 2018) decompose cyclic graphs into subgraphs that are isomorphic to a pre-specified set of possibly cyclic templates, designing the conventional SPN for each of them. The sum unit and a layer of the product units aggregate the roots of these SPNs. Graph-induced SPNs (Errica & Niepert, 2023) also decompose cyclic graphs, constructing a collection of trees based on a userspecified neighborhood. The SPNs are not designed for the trees but only for the feature vectors in the nodes. The aggregation is performed by conditioning the sum units at upper levels of the tree by the posterior probabilities at the lower levels. Relational SPNs (RSPNs) (Nath & Domingos, 2015) are TPMs for relational data (a particular form of cyclic graphs). Our set unit is similar to the exchangeable distribution template of the RSPNs. The key difference is that SPSNs model cardinality. The RSPNs do not provide this feature, making them unable to generate new graphs. The mixture densities over finite random sets are most related to SPSNs (Phung & Vo, 2014; Tran et al., 2016; Vo et al., 2018). They can be seen as the sum unit with children given by the set units. These shallow models are designed only for sets. SPSNs generalize them to deep models for hierarchies of sets, achieving higher expressivity by stacking the computational units. SPNs are also used to introduce correlations into graph variational autoencoders (Xia et al., 2023), which are intractable models. Logical circuits can be used to induce probability distributions over discrete objects via knowledge compilation (Chavira & Darwiche, 2008; Ahmed et al., 2022). However, they assume fixed-size inputs, making them applicable only to fixed-size graphs, such as grids."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We illustrate the performance and properties of the algebraically tractable SPSN models compared to various intractable, NN-based models. In this context, we would like to investigate their performance in the discriminative learning regime and their robustness to missing values. We provide the implementation of SPSNs at https://drive.google.com/file/d/ 1-EY1uDNgPFL3JV8czyXl6BSclks-XQVy/view?usp=sharing.\nModels. To establish the baseline with the intractable models, we choose variants of recurrent NNs (RNNs) for tree-structured data. Though these models are typically used in the NLP domain (Section 1), they are, too, applicable to the tree-structured data in Definition 1. These tree-RNNs differ in the type of cell. We consider the simple multi-layer perceptron (MLP) cell, the gated recurrent unit (GRU) cell (Zhou et al., 2016), and the long-short term memory (LSTM) cell (Tai et al., 2015). The key assumption of these models is that they consider each leaf node to have the same dimension. This requirement does not hold in Definition 1. Therefore, for all these NN-based models, we add a single dense layer with the linear activation function in front of each leaf node, v \u2208 L, to make the\ninput dimension the same. As another competitor, we use the hierarchical multiple-instance learning (HMIL) network (Pevny\u0301 & Somol, 2016), which is also tailored for the tree-structured data.\nSettings. We convert eight publicly available datasets from the CTU relational repository (Motl & Schulte, 2015) into the JSON format (Pezoa et al., 2016). The dictionary nodes, list nodes, and atomic nodes of the JSON format directly correspond to the heterogeneous nodes, homogeneous nodes, and leaf nodes of the tree-structured data, respectively (Definition 1, Figure 2). We present the rest of the settings in Section F, including the schemata of the datasets. All models and experiments are implemented in Julia, using JSONGrinder.jl and Mill.jl (Mandl\u0131\u0301k et al., 2022).\nGraph classification. Table 1 shows the test accuracy of classifying the tree-structured graphs. The HMIL and GRU networks deliver the best performance, while the SPSN falls slightly behind. If we look closely at the individual lines, we can see that the SPSN is often very similar to (or the same as) the HMIL and GRU networks. We consider these results unexpectedly good, given that the (NNbased) MLP, GRU, LSTM, and HMIL architectures are denser than the sparse SPSN architecture.\nMissing values. We consider an experiment where we select the best model in the grid search based on the validation data (as in Table 1) and evaluate its accuracy on the test data containing a fraction of randomly-placed missing values. Figure 3 demonstrates that the SPSN either outperforms or is similar to the NNs. Most notably, for cora and webkp, the SPSN network keeps its classification performance longer compared to the NN models, showing increased robustness to missing values. This experiment applies Proposition 1 to perform marginal inference on the leaf nodes, v \u2208 L, that contain missing values. The marginalization is efficient, taking only one pass through the network. Note that the randomness in placing the missing values can lead to situations where all children of the heterogeneous node are missing, allowing us to marginalize the whole heterogeneous node."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We have leveraged the theory of finite random sets to develop a new class of deep learning models\u2014 sum-product-set networks (SPSNs)\u2014that represent a probability density over tree-structured graphs. The key advantage of SPSNs is their tractability, which enables exact and efficient inference over specific parts of the data graph. To achieve tractability, SPSNs have to adhere to the structural constraints that are commonly found in other PCs. Consequently, the computational graph of SPSNs has much less connections compared to the computational graph of highly interconnected and nonlinear NNs. Notwithstanding this, SPSNs perform comparably to the NNs in the graph classification task, sacrificing only a small amount of performance to retain their tractable properties. Our findings reveal that the tractable and simple inference of SPSNs has also enabled us to achieve results that are comparable to the NNs regarding the robustness to missing values. In future work, we plan to enhance the connectivity within the SPSN block by vectorizing the computational units. We anticipate that this modification will close the small performance gap to the NN models."
        },
        {
            "heading": "A PROBABILISTIC CIRCUITS",
            "text": "A probabilistic circuit (PC) is a deep learning model representing a joint probability density, p(x), over a fixed-size, unstructured, random variable, x = (x1, . . . , xn) \u2208 X \u2282 Rn. The key feature of a PC is that\u2014under certain regularity assumptions\u2014it permits exact and efficient inference scenarios. We define a PC by a parameterized computational graph, G, and a scope function, \u03c8. Definition 9. (Computational graph). G := (V, E , \u03b8) is a parameterized, directed, acyclic graph, where V is a set of vertices, E is set of edges, and \u03b8 \u2208 \u0398 are parameters. V := (S,P, I) contains three different subsets of computational units: sum units, S, product units, P, and input units, I. Let ch(u) and pa(u) denote the set of child and parent units of u \u2208 V , respectively. If pa(u) = \u2205, then u is the root unit. If ch(u) = \u2205, then u is a input unit. We consider that V contains only a single root unit, and each product unit has only a single parent. The parameters \u03b8 := (\u03b8s, \u03b8l) are divided into (i) parameters of all sum units, \u03b8u = (wu,c)c\u2208ch(u), which contain non-negative and locally normalized weights (Peharz et al., 2015), wu,c \u2265 0, \u2211 c\u2208ch(u) wu,c = 1; and (ii) parameters of all input units, \u03b8l, which are specific to a given family of densities, with possibly a different density for each u \u2208 I. Definition 10. (Scope function). The mapping \u03c8 : V \u2192 F(x)\u2014from the set of units to the power set of x\u2014outputs a subset of x \u2208 X for each u \u2208 V and is referred to as the scope function. If u is the root unit, then \u03c8u = x. If u is a sum unit or a product unit, then \u03c8u = \u22c3 c\u2208ch(u) \u03c8c.\nPCs are an instance of neural networks (Vergari et al., 2019; Peharz et al., 2020), where each computational unit is a probability density characterized by certain functionality. Input units are the input of a PC. For each u \u2208 I, they compute a (user-specified) probability density, pu(\u00b7), over a subset of x given by the scope, \u03c8u, which can be univariate or multivariate (Peharz et al., 2015). Sum units are mixture densities that compute the weighted sum over its children, pu(\u00b7) = \u2211 c\u2208ch(u) wu,cpc(\u00b7), where wu,c (Definition 9) weights the connection between the sum unit and a child unit. Product units are factored densities that compute the product of its children, pu(\u03c8u) = \u220f c\u2208ch(u) pc(\u03c8c), establishing an unweighted connection between u and c and introducing the conditional independence among the scopes of its children, \u03c8c. It is commonly the case that (layers of) sum units interleave (layers of) product units. The computations then proceed recursively through G until reaching the root unit\u2014the output of a PC.\nPCs are generally intractable. They instantiate themselves into specific circuits\u2014and thus permit tractability of specific inference scenarios\u2014by imposing various constraints on G, examples include smoothness, decomposability, structured decomposability, determinism, consistency (Chan & Darwiche, 2006; Poon & Domingos, 2011; Shen et al., 2016). In this work, we use only the first two of these constraints, as summarized in Definition 6.\nA PC satisfying Definition 6 can be seen as a polynomial composed of input units (Darwiche, 2003). This construction guarantees that any single-dimensional integral interchanges with a sum unit and impacts only a single child of a product unit (Peharz et al., 2015). The integration is then propagated down to the input units, where it can be computed under a closed-form solution (for a tractable form of u \u2208 I). The key practical consequence lies in that various inference tasks\u2014such as integrals of p(x) over (xa, . . . , xb) \u2282 x\u2014are tractable and can be computed in time which is linear in the circuit size (i.e., the cardinality of V). p(x) is guaranteed to be normalized if Definition 6 holds and input units are from the exponential family (Barndorff-Nielsen, 1978). PCs that fulfill Definition 6 are commonly referred to as sum-product networks."
        },
        {
            "heading": "B RANDOM FINITE SETS",
            "text": "Random Finite Sets. A random finite set (RFS), X := {x1, . . . ,xm}, is a random variable taking values in F(X ), the hyperspace of all finite (closed) subsets of some underlying space, X . The randomness of this mathematical object originates from all the elements in the set, xi \u2208 X , but also from the cardinality of this set, m := |X|, i.e., the number of the elements is itself random. This is the key difference to the standard fixed-size vector, x = (x1, . . . , xm), where xi\u2019s are stochastic but m is deterministic. The example realizations of an RFS are X = \u2205 (empty set), X = {x1} (singleton), X = {x1,x2} (tuple), etc. They are points in the hyperspace, X \u2208 F(X ), and each of them is a finite subset of X . The elements of an RFS, x1, . . . ,xm, are assumed distinct (non-\nrepeated) and unordered. An RFS is then equivalent to a simple point process (Van Lieshout, 2000; Daley et al., 2003; Nguyen, 2006; Mahler, 2007).\nA more formal definition of an RFS is as follows. Let \u2126 be a sample space, \u03c3(\u2126) be a sigma algebra of events on \u2126, and P be a probability measure on the measurable space (\u2126, \u03c3(\u2126)), i.e., P(\u2126) = 1. Let X be a locally compact, Hausdorff, separable metric space (e.g., X \u2286 Rn), F(X ) be the hyperspace of all finite subsets of X , \u03c3(F) be a sigma algebra of events on F(X ), and \u00b5 be a dominating (reference) measure on the measurable space (F(X ), \u03c3(F)), which we specify later on. Now consider the probability space and the measure space (\u2126, \u03c3(\u2126),P) and (F(X ), \u03c3(F), \u00b5), respectively. Then, an RFS is a measurable mapping2\nX : \u2126\u2192 F(X ). (3)\nTo have the ability to build probabilistic models of the RFS (3), we need tools that characterize its statistical behavior. These tools include a probability distribution, a probability density, and a suitable reference measure to perform the integration. The hyperspace F(X ) does not inherit the standard Euclidean topology, but the Mathe\u0301ron \u201chit-or-miss\u201d topology (Mathe\u0301ron, 1974), which implies that some of these tools are built differently compared to those designed purely for X . However, as demonstrated in this section, we can work with them in a way consistent with the conventional probabilistic calculus.\nProbability distribution. The probability law of the RFS (3) is characterized by its probability distribution, P (A) := P(X\u22121(A)) = P({\u03c9 \u2208 \u2126|X(\u03c9) \u2208 A}), (4) for any Borel-measurable subset A \u2208 \u03c3(F). Reference measure. A measure \u03bb on the measurable space (X , \u03c3(X )) is a countably-additive function \u03bb : \u03c3(X ) \u2192 [0,\u221e]. It generalizes the notions of length, area, and volume to the subsets of X , which typically involves physical dimensions expressed in the units of X . However, not all measures have units, e.g., the probability measure is unitless. When working with an RFS, one cannot simply use a conventional measure on (X , \u03c3(X )), but it is necessary to extend it to (F(X ), \u03c3(F)). We aim to show how to extend the Lebesgue measure on (X , \u03c3(X )) to a measure on (F(X ), \u03c3(F)). Let \u03bb(S) be the Lebesgue measure on (X , \u03c3(X )), for any Borel-measurable subset S \u2208 \u03c3(X ), and let \u03bbi(S\u2032) be the extension of the Lebesgue measure to the Cartesian-product, measurable space (X i, \u03c3(X i)), for any subset S\u2032 \u2208 \u03c3(X i). Furthermore, consider a mapping \u03c7 : \u228ei\u22650X i \u2192 F(X ) from vectors of i elements to sets of i elements given by \u03c7(x1, . . . ,xi) = {x1, . . . ,xi}, where \u228e denotes disjoint union. The mapping \u03c7 is measurable (Goodman et al., 1997; Van Lieshout, 2000), and, therefore, \u03c7\u22121(A) is a measurable subset of \u228ei\u22650X i for any subset A \u2208 \u03c3(F). Consequently, the reference measure on the measurable space (F(X ), \u03c3(F))\u2014which is commonly adopted in the theory of finite point processes\u2014is defined as follows:\n\u00b5(A) = \u221e\u2211 i=0 \u03bbi(\u03c7\u22121(A) \u2229 X i) cii! , (5)\nwhere \u03c7\u22121(A) \u2229 X i \u2208 \u03c3(X i) restricts \u03c7\u22121(A) into ith Cartesian product of X , respecting the convention X 0 := \u2205. Consider that the unit of measurement in X is \u03b9, then the unit of measurement of \u03bbi is \u03b9i. This is why (5) contains the constant c whose unit of measurement is \u03b9. Without this constant, each term in (5) would have different units of measurement, and the infinite sum would be undefined. The measure (5) is therefore unitless.\nSay that the units of p(x1) are cm\u22121, and the units of p(x1,x2) are cm\u22122, then it always holds that p(x1) > p(x1,x2), see (Vo et al., 2018) for an illustrative example. c thus prevents the incompatibility between the probabilities of two sets with different cardinalities.\nIntegral. The integral of a unitless function f : F(X )\u2192 R over a subset A \u2208 \u03c3(F) with respect to the measure \u00b5 is (Geyer, 1999; Mahler, 2007)\u222b\nA\nf(X)\u00b5(dX) = \u221e\u2211 i=0 1 cii! \u222b \u03c7\u22121(A)\u2229X i f({x1, . . . ,xi})\u03bbi(dx1, . . . , dxi). (6)\n2Note the difference to the standard random variable X : \u2126 \u2192 X defined directly on the measure space (X , \u03c3(X ), \u00b5), where X is typically equipped with the standard Euclidean topology.\nRemark 1. (Tractable integration.) The analytical tractability of (6) depends on whether the integral of f({x1, . . . ,xi}) w.r.t. \u03bbi allows us to find a closed-form solution and whether the infinite sum becomes a finite one. These requirements are satisfied by designing f based on a suitable family of functions and ensuring that f({x1, . . . ,xi}) = 0 for a sufficiently large i (Goodman et al., 1997).\nProbability density. The probability density function is the central tool in probabilistic modeling. It is obtained from the Radon-Nikody\u0301m theorem (Billingsley, 1995). Its definition states that for two \u03c3-finite measures \u00b51 and \u00b52 on the same measurable space (F(X ), \u03c3(F)) there exists an almost everywhere unique function f : F(X ) \u2192 [0,\u221e) such that \u00b52(A) = \u222b A g(X)\u00b51(dX) if and only if \u00b52 << \u00b51, i.e., \u00b52 is absolutely continuous w.r.t. \u00b51, or, in other words, \u00b51(A) = 0 implies \u00b52(A) = 0 for any subset A \u2208 \u03c3(F). The function f = d\u00b52d\u00b51 is then referred to as the density function or the Radon-Nikody\u0301m derivative of \u00b52 w.r.t. \u00b51. This allows us to define the probability density function of an RFS as the Radon-Nikody\u0301m derivative of the probability measure (4) w.r.t. the reference measure (5),\np(X) = dP\nd\u00b5 (X), (7) establishing the relation between the two measures as follows: P (A) = \u222b A p(X)\u00b5(dX). The probability density function (7) has no units of measurement since the probability distribution (4) is unitless and the reference measure (5) is also unitless. This contrasts the standard probability density function defined on X , which gives probabilities per unit of X . Exchangeability of RFSs. In point process theory (Daley et al., 2003), the probability density of an RFS (finite point process) is often constructed based on an mth-order, non-probabilistic measure, defined on the measurable space (Xm, \u03c3(Xm)), as follows:\nJm(A1, . . . , Am) = p(m) \u2211 perm Pm(Ai1 , . . . , Aim), (8)\nfor any Ai \u2208 \u03c3(X ). Here, p(m) is the cardinality distribution, which determines the total number of elements in the RFS; Pm is the joint distribution on (Xm, \u03c3(Xm)), describing the positions of the elements in the RFS conditionally on m; \u2211 perm denotes the summation over all m! possible permutations of i1, . . . , im. The measure (8) is exchangeable (permutation invariant), i.e., it gives the same value to all permutations of A1, . . . , Am. Following this prescription in its full generality would be computationally very expensive, as it requires m! evaluations of Pm. Fortunately, we assume that the elements of the RFS follow no specific order, and that we can make a symmetric version of Pm as follows: P sym m (\u00b7) = 1m! \u2211 perm Pm(\u00b7), which is simply an equally weighted mixture\nover all possible permutations. Consequently, after substituting for \u2211\nperm Pm in (8), we obtain a fully exchangeable\u2014yet computationally more convenient\u2014Janossy measure,\nJm(A1, . . . , Am) = p(m)m!P sym m (A1, . . . , Am). (9)\nIf (9) is absolutely continuous w.r.t. the reference measure \u03bbm, then there exists the Janossy density,\njm(x1, . . . ,xm) = p(m)m!p sym m (x1, . . . ,xm). (10)\nNote that (9) and (10) are not a probability measure and a probability density, respectively. Indeed, it holds that Jm(Xm) = \u222b jm(x1, . . . ,xm)\u03bb\nm(dx1, . . . , dxm) \u0338= 1. However, they are favored for their reduced combinatorial nature and easy interpretability, i.e., 1 m!jm(x1, . . . ,xm)\u03bb\nm(dx1, . . . , dxm) is the probability of finding exactly one element in each of the m distinct infinitesimal regions. To ensure that (10) is the probability density (7) of an RFS, X\u2014which is taken w.r.t. the reference measure (5)\u2014it has to hold that\np({x1, . . . ,xm}) = cmjm(x1, . . . ,xm). (11)\nIndependent and identically distributed clusters. The feature (joint) density, psymm , in (10) allows us to model the dependencies among the elements of the RFS, X . In certain applications, it is more suitable (or simplifying) to assume that the elements, x1, . . . ,xm, are independent and identically distributed (i.i.d.). The feature density then reads psymm (x1, . . . ,xm) := \u220fm i=1 p(xi), where p is a probability density on X indexed by the same parameters for all i \u2208 {1, . . . ,m}. Note that the assumption of independent elements, but, more importantly, the assumption of identically distributed\nelements (the same parameters for all i \u2208 {1, . . . ,m}), ensures the symmetry of the feature density under all permutations of the elements. The density (11) then becomes\np({x1, . . . ,xm}) = cmp(m)m! n\u220f\ni=1\np(xi), (12)\nwhich is commonly referred to as the i.i.d. cluster model. In the special case, where the cardinality distribution p(m) is the Poisson distribution, (12) represents the Poisson point process (Grimmett & Stirzaker, 2001)."
        },
        {
            "heading": "C EXCHANGEABILITY",
            "text": "Probabilistic symmetries. Probabilistic symmetry\u2014the most fundamental one of which is exchangeability\u2014is a long-standing subject in the probability literature (Zabell, 2005). The notion of probabilistic symmetry is useful for constructing probabilistic models of exchangeable data structures, including graphs, partitions, and arrays (Orbanz & Roy, 2014). Infinite exchangeability is related to the conditionally i.i.d. sequences of random variables via the de Finetti\u2019s theorem (de Finetti, 1929; 1937). It states that an infinite sequence of random variables x1, x2, . . . is exchangeable if and only if (iff) there exists a measure \u03bb on \u0398, such that p(x1, . . . , xn) = \u222b \u220fn i=1 p\u03b8(xi)\u03bb(d\u03b8). Consequently, conditionally i.i.d. sequences of random variables are exchangeable. The converse of this assertion is true in the infinite case. Finite exchangeability does not satisfy the converse assertion. It defines that for an extendable finite sequence, x1, . . . , xn3, the de Finetti\u2019s representation holds only approximately, i.e., there is a bounded error between the finite and infinite representations (Diaconis, 1977; Diaconis & Freedman, 1980). We are not interested in finite exchangeability from the perspective of its asymptotic properties. However, we use it to investigate whether a probabilistic model is structurally invariant under the action of a compact group operating on its input, which is considered non-extendable.\nExchangeability of PCs. As discussed in Section 3.2, the study (and application) of exchangeability in (to) PCs has attracted limited attention. The exchangeability-aware SPNs (Lu\u0308dtke et al., 2022) use the mixtures of exchangeable variable models (Niepert & Domingos, 2014; Niepert & Van den Broeck, 2014) as the input units, proposing a structure-learning algorithm that learns the structure by statistically testing the exchangeability within the groups of random variables. The relational SPNs (Nath & Domingos, 2015) introduce the exchangeable distribution templates, which are similar in certain aspects to SPSNs. However, though these approaches adopt exchangeable components, none of them investigates how the exchangeability propagates through a PC. Proposition 4. (Exchangeability of PCs). Let p(x) be a PC satisfying Definition 6 and let IE \u2208 I be a subset of input units that are exchangeable in the sense of Definition 7 or Definition 8. Then, the PC is partially exchangeable, p(qa \u00b7 x) = p(x), for each a \u2208 IE.\nProof. The result follows from the recursive application of Proposition 2.\nProposition 4 says that PCs satisfying Definition 6 preserve the exchangeability of their input units. It holds only when there is at least one input unit that is multivariate and exchangeable in the sense of Definition 7 or Definition 8. Note that the ordering of the scopes (blocks) in the product units remains fixed in the computational graph, i.e., the scopes representing the children of the product units are not exchangeable (only the variables in them).\nAn alternative way to prove Proposition 4 would be to convert a PC to its mixture representation (Zhao et al., 2016; Trapp et al., 2019). This converted model is a mixture of products of the input units, for which the partial exchangeability can be proven in a way similar to the mixtures of exchangeable variable models (Niepert & Domingos, 2014; Niepert & Van den Broeck, 2014).\nC.1 PROOF OF PROPOSITION 2\nSum units. The exchangeability of the sum unit follows from the smoothness assumption (Definition 6). The fact that the scope of all children of any sum unit is identical ensures that any permuta-\n3This means that the sequence x1, . . . , xn is a part of the longer sequence, x1, . . . , xm, m > n, with the same statistical properties.\ntion (Definition 8), \u03c0 \u2208 Snm , propagates through the sum unit, pu(\u03c0 \u00b7 \u03c8u) = \u2211\nc\u2208ch(u) wu,cpc(\u03c0 \u00b7 \u03c8u), u \u2208 S. In other words, the probability density of the sum unit is partially (or fully) exchangeable, pu(\u03c0 \u00b7\u03c8u) = pu(\u03c8u), if and only if the probability densities of all its children are partially (or fully) exchangeable, pc(\u03c0 \u00b7 \u03c8u) = pc(\u03c8u), for all c \u2208 ch(u). If we replace \u03c0 by qa, the operator targeting a specific computational unit, we come to the same conclusion.\nProduct units. The exchangeability of the product unit is based on the decomposability assumption (Definition 6). The consequence of that the scopes of all children of any product unit are pairwise disjoint is that no matter the type of exchangeability of the child units, the product unit is always only partially exchangeable under the partition of the scopes of its children, pu(\u03c0 \u00b7 \u03c8u) = pu(\u03c01 \u00b7 \u03c8c1 , . . . , \u03c0m \u00b7\u03c8cm) = \u220f c\u2208ch(u) pc(\u03c0c \u00b7\u03c8c), for all \u03c0 \u2208 Snm and u \u2208 P. Therefore, we can say that the product unit, pu(\u03c0 \u00b7 \u03c8u) = pu(\u03c8u), preserves the exchangeability of its children. The product group Snm can be designed such that some of its elements can be an identity group, Sni := Ini . In this case, there exists an identity operator, ei, which does not permute the entries of x := (x1, . . . , xni), i.e., we have ei \u00b7 x = (x1, . . . , xni). Consequently, \u03c0 \u2208 Snm permutes only some of m elements in the collection, X := (x1, . . . ,xm), e.g., as follows: \u03c0 \u00b7X = (\u03c01 \u00b7 x1, e2 \u00b7 x2, . . . , \u03c0m \u00b7 xm). This allows us to target the permutations only to certain children of the product unit pu(\u03c0 \u00b7 \u03c8u) = \u220f v\u2208ch(u) pv(\u03c0v \u00b7 \u03c8v) \u220f c\u2208ch(u) pc(\u03c8c), where ch(u) are the children targeted\nwith permutations, and ch(u) are the children that are supposed stay intact. If we consider replacing \u03c0 by qa, then this principle reveals how to propagate qa only through certain children of the product unit. For example, we have: qa \u00b7 \u03c8u = (qv1 \u00b7 \u03c8v1 , ev2 \u00b7 \u03c8v2 , . . . , qvm \u00b7 \u03c8vm). Input units. The input units are user-specified probability densities, pu(\u03c8u), for each u \u2208 I. The exchangeability of any input unit thus depends on the choice of its density, which can be fully exchangeable (Definition 7), pu(\u03c8u) = pu(\u03c0 \u00b7 \u03c8u), or partially exchangeable (Definition 8), pu(\u03c8u) = pu(\u03c0 \u00b7 \u03c8u). This also implies that qa = \u03c0 if a = u. The leaf units terminate the propagation through the computational graph.\nC.2 PROOF OF PROPOSITION 3\nThe result follows from the recursive application of Proposition 2 and the fact that the set unit is fully exchangeable by design (Section B). That is, for any homogeneous node, Tv := {Tw1 , . . . , Twm}, it holds that pu(\u03c0 \u00b7 Tv) = pu(Tv) where v \u2208 O, u \u2208 B, and \u03c0 \u2208 Sm."
        },
        {
            "heading": "D TRACTABILITY",
            "text": "The primary purpose of training (learning the parameters of) probabilistic models is to prepare them to answer intricate information-theoretic queries (questions) about events influenced by uncertainty (e.g., computing the probability of some quantities of interest, expectation, entropy). This procedure\u2014referred to as probabilistic inference\u2014often requires calculating integrals of, or w.r.t., the joint probability density representing the model. Many recent probabilistic models deployed in machine learning and artificial intelligence rely on neural networks. The integrals in these models do not admit a closed-form solution, and the inference procedure is, therefore, intractable. To answer even the basic queries with these intractable probabilistic models, we are forced to resort to numerical approximations. The inference procedure is then computationally less efficient, more complex, and brings more uncertainty into the answers. Tractable probabilistic models, on the other hand, provide a closed-form solution to the integrals involved in the inference procedure and thus answer our queries faithfully to the joint probability density without relying on approximations or heuristics. The inference procedure is then less complicated and computationally more efficient.\nTractability of PCs. PCs have become a canonical part of tractable probabilistic modeling. They can answer a range of probabilistic queries exactly, i.e., without involving any approximation, and efficiently, i.e., in time which is polynomial in the number of edges of their computational graph. The range of admissible probabilistic queries varies depending on the types of structural constraints satisfied by the computational graph (Choi et al., 2020).\nWe recall only some standard probabilistic queries that are feasible under the usual structural constraints of Definition 6 and can collectively be expressed in terms of the following integral:\n\u03bb(f) = \u222b f(x)p(x)\u03bb(dx). (13)\nWe refer the reader to (Choi et al., 2020; Vergari et al., 2021) for more complex and compositional probabilistic queries.\nEven when a PC, p(x), satisfies Definition 6, it does not directly mean that (13) admits a closed-form solution. For this to be the case, the function f(x) has to satisfy certain properties. Definition 11. (Tractable function for PCs.) Let f : X \u2192 R be a measurable function which factorizes as f(x) := \u220f u\u2208L fu(\u03c8u), where L \u2286 I is the subset of input units with unique and\npresumably multivariate scopes such that x = \u22c3\nu\u2208L \u03c8u. Under this factorization, it follows from the properties of the scope function (Definition 10) that fu(\u03c8u) := \u220f c\u2208ch(u) fc(\u03c8c) for each u \u2208 P.\nTo show how to define various probabilistic queries in terms of the integral (13), we provide examples of f . If f(x) := 1A(x), where 1A is the indicator function, and A := A1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Am, then (13) yields P (A), the probability of A. Given A := e1 \u00d7 e2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Am\u22121 \u00d7 Am, where ei are evidence assignments, Ai are measurable subsets of X , and m = |L|, we obtain the marginal query P (e1, e2, . . . , Am\u22121, Am), which can easily be used to build a conditional query of interest. The full evidence query is obtained for A := e1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 em. To compute the first-order moment of any u \u2208 L, we define fu(\u03c8u) := \u03c8u and fa(\u03c8a) := 1 for a \u2208 L/(n). Proposition 5. (Tractability of PCs). Let p(x) be a PC satisfying Definition 6 and let f(x) be a function satisfying Definition 11. Then, the integral (13) is tractable and can be computed recursively as follows:\nIu =  \u2211 c\u2208ch(u) wu,cIc, for u \u2208 S,\u220f c\u2208ch(u) Ic, for u \u2208 P,\u222b fu(\u03c8u)pu(\u03c8u)\u03bbu(d\u03c8u), for u \u2208 I,\nwhere the measure \u03bbu(d\u03c8u) is defined on the space \u03a8u, which corresponds to the scope \u03c8u, and instantiates itself for all u \u2208 V into either the Lebesgue measure or the counting measure.\nProof. See Section D.1.\nProposition 5 states that, to compute the integral (13), we have to first compute the resulting values, Iu, of the integrals for each input unit, u \u2208 I. Then, Iu is recursively propagated in the feed-forward manner (from the inputs to the root) throughout the computational graph and updated by the sum and product units.\nD.1 PROOF OF PROPOSITION 5\nWe aim to demonstrate that the integral (13) is tractable. We show this in a recursive manner, considering how the integral propagates through each computational unit of p(x).\nBefore we start, let us remind Definition 11, which shows that f(x) := \u220f\ni\u2208L fi(\u03c8i). Considering Definition 6 is satisfied; then, based on Definition 10, the partial factorization fu(\u03c8u) :=\u220f\ni\u2208Lu fi(\u03c8i) can be extracted from f(x) for any u \u2208 (S,P). Here, Lu \u2286 L contains only the input units that are reachable from u and have a unique scope. The partial factorization, fu(\u03c8u), leads to the definition of the following intermediate integral:\n\u03bbu(fu) = \u222b fu(\u03c8u)pu(\u03c8u)\u03bbu(d\u03c8u), (14)\nwhich acts on a given computational unit u. In (14), pu(\u03c8u) is the probability density of u, and \u03bbu is the reference measure on the measurable space (\u03a8u, \u03c3(\u03a8u)). We will see that the proof consists of seeking an algebraic closure (recursion) for the functional form of the integral (14).\nInput units. The existence of a closed-form solution of the integral (14) for u \u2208 I is ensured if pu(\u03c8u) is selected from a family of tractable probability densities (e.g., the exponential family (BarndorffNielsen, 1978)) and fu(\u03c8u) is an algebraically simple function that does not prevent the solution to\nbe found. The solution, therefore, depends purely on our choice. We use Iu to denote a concrete value of the integral.\nSum units. Consider that the smoothness assumption (Definition 6) is satisfied. Then, after substituting the probability density of the sum unit for pu(\u03c8u) into (14), and exchanging the integration and summation utilizing the Fubini\u2019s theorem (Weir, 1973), we obtain\u222b\nfu(\u03c8u)pu(\u03c8u)\u03bbu(d\u03c8u) = \u2211\nc\u2208ch(u)\nwu,c \u222b fu(\u03c8u)pc(\u03c8u)\u03bbu(d\u03c8u),\nwhere u \u2208 S. The smoothness assumption (Definition 6) states that the children of the sum unit have an identical scope. It implies that the integration affects each child of the sum unit in the same way. Consequently, the integral w.r.t. pu can be solved if the integrals w.r.t. pc can be solved for all c \u2208 ch(u). In other words, the sum unit is tractable if all its children (i.e., other sub-PCs) are tractable. The tractability of the sum unit thus propagates from its children. We can see that the functional form of the l.h.s. integral is the same as the functional form of the r.h.s. integrals representing the children of the sum unit. Therefore, we can replace these functional prescriptions with concrete realizations Iu (l.h.s.) and Ic (r.h.s.). The sum unit then propagates already realized values of these integrals and multiplies them by the weights.\nProduct units. Assume that the decomposability assumption (Definition 6) holds. Then, after substituting the probability density of the product unit for pu(\u03c8u) into (14), and factorizing the function fu(\u03c8u) in accordance with the pairwise disjoint scopes of the product unit, i.e., fu(\u03c8u) :=\n\u220f c\u2208ch(u) fc(\u03c8c), we have\u222b\nfu(\u03c8u)pu(\u03c8u)\u03bbu(d\u03c8u) = \u222b \u220f c\u2208ch(u) fc(\u03c8c) \u220f c\u2208ch(u) pc(\u03c8c)\u03bbc(d\u03c8c)\n= \u220f\nc\u2208ch(u)\n\u222b fc(\u03c8c)pc(\u03c8c)\u03bbc(d\u03c8c),\nwhere u \u2208 P. The decomposability assumption (Definition 6) says that children of the product unit have independent scopes. The consequence is that the integral reduces to the product of simpler integrals. The integral w.r.t. pu is tractable if the integrals of all its children pc are tractable for all c \u2208 ch(n), i.e., the tractability of the product unit propagates from its children. Similarly as before, the functional forms of the l.h.s. integral of the product unit and the r.h.s. integrals of its children are the same, which allows us to replace them with concrete realizations Iu (l.h.s.) and Ic (r.h.s.).\nIf the tractability holds for each unit u \u2208 {S,P, I} in the computational graph, then a PC is tractable and (13) admits a closed-form solution.\nD.2 PROOF OF PROPOSITION 1\nOur goal is to show that the integral (2) can be computed recursively under a closed-form solution. To this end, we proceed analogously as in the proof of Proposition 5. Recall from Definition 5 that f(T ) := \u220f i\u2208L fi(\u03c8i), and if Definition 6 and Assumption 1 hold, then it follows from the properties\nof Definition 4 that the partial factorization fu(\u03c8u) := \u220f\ni\u2208Lu fi(\u03c8i) can be extracted from f(T ) for any u \u2208 {S,P,B}, where Lu \u2286 L is the set of the input units that can be reached from u and have the unique scope. Similarly as before, the existence of fu(\u03c8u) allows us to define the intermediate integral, which acts on a given computational unit u, as follows:\n\u03bdu(fu) = \u222b fu(\u03c8u)pu(\u03c8u)\u03bdu(d\u03c8u), (15)\nwhere pu(\u03c8u) is the probability density of u, and \u03bdu is the reference measure on the measurable space (\u03a8u, \u03c3(\u03a8u)). \u03a8u can take various forms depending on whether the scope \u03c8u is the homogeneous node, (a part of) the heterogeneous node or the leaf node. If \u03c8u is the homogeneous node, then \u03a8u is the hyperspace of all finite subsets of some underlying space, \u03a8, which characterizes the feature density of the set unit. If \u03c8u is the heterogeneous node, then \u03a8u is the Cartesian product space composed of, e.g., continuous spaces, discrete spaces, but also hyperspaces defining other RFSs. If \u03c8u is the leaf node, then \u03a8u can be the Cartesian product of continuous and (or) discrete spaces. For\nthis reason, the reference measure \u03bdu instantiates itself depending on a given computational unit, u, and can take various forms based on the space, \u03a8u.\nIn the case \u03c8u is a (subset of) leaf node(s), the proof is carried out in the same way as for Proposition 5. This is also true when \u03c8u is a (subset of) heterogeneous node(s). The difference is that \u03bbu in (14) is replaced by the more general measure \u03bdu. This leaves us to prove only the last case where \u03c8u is a homogeneous node.\nSet units. Consider the scope of the set unit, \u03c8u, u \u2208 B, is an RFS taking values in \u03a8u := F(\u03a8w), the hyperspace of all finite subsets of some underlying space, \u03a8w. That is, we have \u03c8u = {\u03c8w1 . . . , \u03c8wk}, where \u03c8wi \u2208 \u03a8w are k distinct instances of the identical scope of the feature density of the set unit u. Furthermore, let \u03bdu be the unitless reference measure (5) on (\u03a8u, \u03c3(\u03a8u), associated to the set unit, pu(\u03c8u), and let \u03bdw be the reference measure on (\u03a8w, \u03c3(\u03a8w)), corresponding to the feature density, p(\u03c8wi). Now, from the properties of f , we have fu(\u03c8u) = \u220fk i=1 fi(\u03c8wi), and, after substituting this function, along with the density of the set unit (1), into (15), we obtain\u222b fu(\u03c8u)pu(\u03c8u)\u03bdu(d\u03c8u)\n= \u221e\u2211 k=0 1 ckk! \u222b \u03a8kw fu({\u03c8w1 , . . . , \u03c8wk})pu({\u03c8w1 , . . . , \u03c8wk})\u03bdkw(d\u03c8w1 , . . . , d\u03c8wk)\n= \u221e\u2211 k=0 1 ckk! \u222b \u03a8kw k\u220f i=1 fi(\u03c8wi)c kk!p(k) k\u220f i=1 p(\u03c8wi)\u03bdw(d\u03c8wi)\n= \u221e\u2211 k=0 p(k) k\u220f i=1 \u222b \u03a8w fi(\u03c8wi)p(\u03c8wi)\u03bd(d\u03c8wi)\n= \u221e\u2211 k=0 p(k) k\u220f i=1 \u222b fi(\u03c8wi)p(\u03c8wi)\u03bd(d\u03c8wi),\nwhere \u03a8kw := \u03a8w \u00d7 \u00b7 \u00b7 \u00b7 \u00d7\u03a8w. Once again, we can see that the functional form of the l.h.s. integral is the same as that of the r.h.s. integrals, allowing us to replace them with Iu and Ii.\nE IMPLEMENTATION\nThere are multiple ways to implement SPSNs. However, the construction of the computational graph always has to follow from the properties of computational units (Definition 3) and respect the structural constraints (Definition 6). We provide more details on the layer-wise implementation introduced in Section 3. However, before that, we first present an intuitive description of constructing SPSNs in a node-wise manner.\nE.1 NODE-WISE APPROACH\nLet us consider the example in Figure 4. The left part (Figure 4(a)) shows the data graph (Definition 1) and its schema (Definition 2) highlighted by the dashed line. We can see that the schema is in fact a simplified graph that excludes the structurally identical children of the homogeneous nodes and keeps only the child that allows us to reach the deepest level of the tree. The construction of an SPSN follows from the schema. We start at the root heterogeneous node. Any heterogeneous node can be modelled by possibly many alterations of sum units and product unis. It is possible to recursively split the heterogeneous node as long as it still contains enough elements since, every time we apply the product unit, we split the heterogeneous node into two parts (or more depending on np). The right part (Figure 4(b)) displays that the root heterogeneous node is modeled by only a single sum unit whose children are two product units (the right one is indicated by the dashed arrow, which we hide for simplicity). This is due to the fact that the root heterogeneous node contains only two children and the product unit separated the children into two singletons: homogeneous node and heterogeneous node depicted in the left and right child of the root heterogeneous node of the data graph. The homogeneous node can be modeled only by the set units; therefore, we add a set unit into the first child of the aforementioned product unit. The heterogeneous node has again only\ntwo children, which means that we will model it in the same way as the root one, i.e., by using a single sum unit with a product unit in each of its children. Now, if we go back to the homogeneous node, then we can see that, in the schema, its child is another heterogeneous node with two children. Therefore, we repeat the same process as before. Since one of the children of this heterogeneous node is the leaf node, we place an input unit into the computational graph. This continues until we traverse all parts of the data graph, extending the computational graph in the process.\nE.2 LAYER-WISE APPROACH\nThe node-wise approach is a simple mechanism to construct SPSNs. Nonetheless, it is often computationally inefficient for the implementation with modern automatic differentiation tools. The reason for this lies in that these tools have to produce their own computational diagram (graph) from the computational graph of the SPSN. Therefore, we provide a layer-wise approach to construct SPSNs, simplifying the underlying differentiation mechanisms.\nAlgorithm 1 contains the procedure spsn network(\u03c8, nc, nl, ns, np) which constructs an SPSN based on the following inputs: \u03c8 is the scope of the root unit (which we set to the schema when applying the procedure), nc is the number of root units of the network (we set nc > 1 when using the network for classification), and nl, ns, and np are the number of layers, children in the sum units, and children of the product units, respectively, which are common to all blocks in the network. This imposes a regular structure on the network and makes it suitable to the layer-wise ordering of the computational units. The resulting computational graph is consequently more efficient for the implementation with the automatic differentiation tools and also more convenient for parallelization on the contemporary computational hardware.\nThe key procedure is spsn block, which recursively constructs the computational graph of an SPSN in the block-by-block manner as depicted in Figure 2. We first create an empty block (line 1). Then, we continue by creating layers of scope functions in scope layers (line 2). We provide more details on this procedure below. For each layer of these scopes, we add (via \u2190) two layers of computational units into the block. (i) The layer of sum units, slayer, where the first argument is the number of children of each sum unit and the second argument is the number of sum units. (ii) The layer of product units, player, where the meaning of the arguments is the same as with slayer. We repeat this until nl = 1. Then, we add the layer of input units, ilayer. This layer assigns an input unit to each scope in l based on its type. That is, if the scope represents the leaf nodes, then it checks the type of data (floats, integers, strings) and creates appropriate probability density. If the scope is the homogeneous node, then it creates the set unit. The feature densities of all set units in this input layer are not connected to any other part of the computational graph at this moment. The procedure then proceeds by gathering all these unconnected scopes via scope set units. All these steps are now repeated by calling spsn block again (line 14). Every iteration, spsn block\nreturns the network that was created to this moment, N , and the scopes of the unplugged set units that are provided to the next iteration. The network that has been generated so far, N , is connected to the unplugged scopes of the set units in the current block B by using connect.\nAlgorithm 2 presents the scope layers procedure. It starts by assigning the input set of scopes \u03c8 into L, a structure holding all layers of scopes, creating the first layer of scopes. The next layer is made by the scope slayer and scope player procedures. scope slayer uses repeat to make ns copies of each element in \u03c8, to reflect the fact that the children of the sum unit have the identical scope. Similarly, scope player splits each element in \u03c8 into np parts, to reflect the fact that the children of the product unit have disjoint scopes. This process is repeated until we either (i) reach maximum allowable number of layers or (ii) there is a scope in \u03c8 represented by a singleton. The latter is realized by minimum length, which first evaluates the number of elements in each scope of \u03c8 and then finds their minimum.\nThe block size. The regular structure of the SPSN block allows us to find a closed-form solution for its size in terms of the number of computational units containing parameters. The number of sum units is given by Ks = |\u03c8| \u2211nl\u22121 l=0 (nsnp)\nl, where |\u03c8| is the cardinality of the input set of scopes in spsn block. The number of input units is Ki = |\u03c8|(nsnp)l. While Ks can directly be used to compute the number of parameters in all sum units, Ki serves only to complete an intuition about the size of each block. To obtain a concrete number of parameters in the input layer, we need to count the parameters in each of its units due to the differences in the data types.\nAlgorithm 1 Construct the SPSN network procedure spsn network(\u03c8, nc, nl, ns, np)\n1: \u03c8 = repeat(\u03c8, nc) 2: N, = spsn block(\u03c8, nl, ns, np) 3: return N\nprocedure spsn block(\u03c8, nl, ns, np) 1: B = \u2205 2: L = scope layers(\u03c8, nl, ns, np) 3: for all l \u2208 L do 4: k = length(l) 5: if nl > 1 then 6: B \u2190 slayer(ns, k) 7: B \u2190 player(np, k \u2217 ns) 8: else 9: B \u2190 ilayer(l)\n10: \u03c8 = scope set units(l) 11: end if 12: nl = nl \u2212 1 13: end for 14: N,\u03c8 = spsn block(\u03c8, nl, ns, np) 15: N = connect(N,B) 16: return N,\u03c8\nAlgorithm 2 Procedures procedure scope layers(\u03c8, nl, ns, np)\n1: L = (\u03c8) 2: while true do 3: \u03c8 = scope slayer(\u03c8, ns) 4: \u03c8 = scope player(\u03c8, np) 5: L\u2190 \u03c8 6: if minimum lenght(\u03c8) is 1 or nl is 1 then 7: break 8: else 9: nl = nl \u2212 1\n10: end if 11: end while 12: return L procedure scope slayer(\u03c8, ns)\n1: \u03c8\u0304 = \u2205 2: for all s \u2208 \u03c8 do 3: \u03c8\u0304 \u2190 repeat(s, ns) 4: end for 5: return \u03c8\u0304\nprocedure scope player(\u03c8, np) 1: \u03c8\u0304 = \u2205 2: for all s \u2208 \u03c8 do 3: \u03c8\u0304 \u2190 split(s, np) 4: end for 5: return \u03c8"
        },
        {
            "heading": "F EXPERIMENTAL SETTINGS",
            "text": "The leaf nodes, v \u2208 L, contain different data types, including reals, integers, and strings. We use the default feature extractor from JSONGrinder.jl (v2.3.2) to pre-process these data. We perform the grid search over the hyper-parameters of the models mentioned in Section 5. For the MLP, GRU, and LSTM networks, we set the dimension of the hidden state(s) and the output in {10, 20, 30, 40}. For the HMIL network, we use the default settings of the model builder from Mill.jl (v2.8.1),\nonly changing the number of hidden units of all the inner layers in {10, 20, 30, 40}. We add a single dense layer with the linear activation function to adapt the outputs of these networks to the number of classes in the datasets. For the SPSN networks, we choose the Poisson distribution as the cardinality distribution and the following hyper-parameters: nl \u2208 {1, 2, 3}, ns \u2208 {2, 3, . . . , 10}, and np := 2. We use the ADAM optimizer (Kingma & Ba, 2014) with fixing 10 samples in the minibatch and varying the step-size in {0.1, 0.01, 0.001}. The datasets are randomly split into 64%, 16%, and 20% for training, validation, and testing, respectively.\nWe performed the experiments on a computational cluster equipped with 116 CPUs (Intel Xeon Scalable Gold 6146). The jobs to perform the grid search over the admissible range of hyperparameters (Sections 5 and F in the updated manuscript) were scheduled by SLURM 23.02.2. We limited each job to a single core and 8GB of memory. The computational time was restricted to one day, but all jobs were finished under that limit (ranging approximately between 2-18 hours per dataset)."
        },
        {
            "heading": "G DATASETS",
            "text": "The CTU Prague relational learning repository (Motl & Schulte, 2015) is a rich source of structured data. These data form a directed graph where the nodes are tables and edges are the foreign keys. Some of these datasets are already in the form of threes; however, there are also graphs containing cycles. As a part of the preprocessing, we decompose these cyclic graphs to tree graphs by selecting a node and then reaching to the neighborhood nodes in the one-hop distance.\nTable 2 shows the two main attributes of the datasets under study: the number of instances (i.e., the number of tree-structured graphs) and the number of classes of these instances. A detailed description of these datasets, additional attributes, and accompanying references are accessible at https://relational.fit.cvut.cz/.\nFigure 1 shows a single instance of the tree-structured graph data in the JSON format (Pezoa et al., 2016), and Figure 5 illustrates the corresponding schema (Definition 2 of the main paper). As can be seen (and as also mentioned in the main paper), the leaf nodes contain different data types: integers, floats, and strings. In Figures 6-12, we provide the schemata of the remaining datasets in Table 2."
        }
    ],
    "year": 2023
}