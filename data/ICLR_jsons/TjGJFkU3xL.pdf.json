{
    "abstractText": "Proximal causal learning is a powerful framework for identifying the causal effect under the existence of unmeasured confounders. Within this framework, the doubly robust (DR) estimator was derived and has shown its effectiveness in estimation, especially when the model assumption is violated. However, the current form of the DR estimator is restricted to binary treatments, while the treatments can be continuous in many real-world applications. The primary obstacle to continuous treatments resides in the delta function present in the original DR estimator, making it infeasible in causal effect estimation and introducing a heavy computational burden in nuisance function estimation. To address these challenges, we propose a kernel-based DR estimator that can well handle continuous treatments for proximal causal learning. Equipped with its smoothness, we show that its oracle form is a consistent approximation of the influence function. Further, we propose a new approach to efficiently solve the nuisance functions. We then provide a comprehensive convergence analysis in terms of the mean square error. We demonstrate the utility of our estimator on synthetic datasets and real-world applications1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yong Wu"
        },
        {
            "affiliations": [],
            "name": "Yanwei Fu"
        },
        {
            "affiliations": [],
            "name": "Shouyan Wang"
        },
        {
            "affiliations": [],
            "name": "Xinwei Sun"
        }
    ],
    "id": "SP:0bb3f771e3ba47c0ed7bc5f6989059a7c4389ab0",
    "references": [
        {
            "authors": [
                "Luca Ambrogioni",
                "Umut G\u00fc\u00e7l\u00fc",
                "Marcel AJ van Gerven",
                "Eric Maris"
            ],
            "title": "The kernel mixture network: A nonparametric method for conditional density estimation of continuous random variables",
            "venue": "arXiv preprint arXiv:1705.07111,",
            "year": 2017
        },
        {
            "authors": [
                "Heejung Bang",
                "James M Robins"
            ],
            "title": "Doubly robust estimation in missing data and causal inference models",
            "year": 2005
        },
        {
            "authors": [
                "Christopher M Bishop"
            ],
            "title": "Mixture density networks",
            "year": 1994
        },
        {
            "authors": [
                "Marco Carone",
                "Alexander R Luedtke",
                "Mark J van der Laan"
            ],
            "title": "Toward computerized efficient estimation in infinite-dimensional models",
            "venue": "Journal of the American Statistical Association,",
            "year": 2018
        },
        {
            "authors": [
                "Marine Carrasco",
                "Jean-Pierre Florens",
                "Eric Renault"
            ],
            "title": "Linear inverse problems in structural econometrics estimation based on spectral decomposition and regularization",
            "venue": "Handbook of econometrics,",
            "year": 2007
        },
        {
            "authors": [
                "Xiaohong Chen",
                "Demian Pouzo"
            ],
            "title": "Estimation of nonparametric conditional moment models with possibly nonsmooth generalized residuals",
            "year": 2012
        },
        {
            "authors": [
                "Yen-Chi Chen"
            ],
            "title": "A tutorial on kernel density estimation and recent advances",
            "venue": "Biostatistics & Epidemiology,",
            "year": 2017
        },
        {
            "authors": [
                "Victor Chernozhukov",
                "Whitney K Newey",
                "Rahul Singh"
            ],
            "title": "Automatic debiased machine learning of causal and structural effects",
            "year": 2022
        },
        {
            "authors": [
                "Kyle Colangelo",
                "Ying-Ying Lee"
            ],
            "title": "Double debiased machine learning nonparametric inference with continuous treatments",
            "venue": "arXiv preprint arXiv:2004.03036,",
            "year": 2020
        },
        {
            "authors": [
                "Yifan Cui",
                "Hongming Pu",
                "Xu Shi",
                "Wang Miao",
                "Eric Tchetgen Tchetgen"
            ],
            "title": "Semiparametric proximal causal inference",
            "venue": "Journal of the American Statistical Association,",
            "year": 2023
        },
        {
            "authors": [
                "Nishanth Dikkala",
                "Greg Lewis",
                "Lester Mackey",
                "Vasilis Syrgkanis"
            ],
            "title": "Minimax estimation of conditional moment models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Laurent Dinh",
                "Jascha Sohl-Dickstein",
                "Samy Bengio"
            ],
            "title": "Density estimation using real nvp",
            "venue": "arXiv preprint arXiv:1605.08803,",
            "year": 2016
        },
        {
            "authors": [
                "John J Donohue III",
                "Steven D Levitt"
            ],
            "title": "The impact of legalized abortion on crime",
            "venue": "The Quarterly Journal of Economics,",
            "year": 2001
        },
        {
            "authors": [
                "Arnaud Doucet",
                "Adam M Johansen"
            ],
            "title": "A tutorial on particle filtering and smoothing",
            "venue": "Fifteen years later. Handbook of nonlinear filtering,",
            "year": 2009
        },
        {
            "authors": [
                "Conor Durkan",
                "Artur Bekasov",
                "Iain Murray",
                "George Papamakarios"
            ],
            "title": "Neural spline flows",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Dylan J Foster",
                "Vasilis Syrgkanis"
            ],
            "title": "Orthogonal statistical learning",
            "venue": "arXiv preprint arXiv:1901.09036,",
            "year": 2019
        },
        {
            "authors": [
                "AmirEmad Ghassami",
                "Andrew Ying",
                "Ilya Shpitser",
                "Eric Tchetgen Tchetgen"
            ],
            "title": "Minimax kernel machine learning for a class of doubly robust functionals with application to proximal causal inference",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Jens Hainmueller"
            ],
            "title": "Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies",
            "venue": "Political analysis,",
            "year": 2012
        },
        {
            "authors": [
                "Peter Hedstr\u00f6m",
                "Petri Ylikoski"
            ],
            "title": "Causal mechanisms in the social sciences",
            "venue": "Annual review of sociology,",
            "year": 2010
        },
        {
            "authors": [
                "Jennifer L Hill"
            ],
            "title": "Bayesian nonparametric modeling for causal inference",
            "venue": "Journal of Computational and Graphical Statistics,",
            "year": 2011
        },
        {
            "authors": [
                "Keisuke Hirano",
                "Guido W Imbens"
            ],
            "title": "The propensity score with continuous treatments",
            "venue": "Applied Bayesian modeling and causal inference from incomplete-data perspectives,",
            "year": 2004
        },
        {
            "authors": [
                "Jie Kate Hu",
                "Dafne Zorzetto",
                "Francesca Dominici"
            ],
            "title": "A bayesian nonparametric method to adjust for unmeasured confounding with negative controls",
            "venue": "arXiv preprint arXiv:2309.02631,",
            "year": 2023
        },
        {
            "authors": [
                "Hidehiko Ichimura",
                "Whitney K Newey"
            ],
            "title": "The influence function of semiparametric estimators",
            "venue": "Quantitative Economics,",
            "year": 2022
        },
        {
            "authors": [
                "Kosuke Imai",
                "Marc Ratkovic"
            ],
            "title": "Covariate balancing propensity score",
            "venue": "Journal of the Royal Statistical Society Series B: Statistical Methodology,",
            "year": 2014
        },
        {
            "authors": [
                "Kosuke Imai",
                "David A Van Dyk"
            ],
            "title": "Causal inference with general treatment regimes: Generalizing the propensity score",
            "venue": "Journal of the American Statistical Association,",
            "year": 2004
        },
        {
            "authors": [
                "W Guido"
            ],
            "title": "Imbens. The role of the propensity score in estimating dose-response functions",
            "year": 2000
        },
        {
            "authors": [
                "Guido W Imbens"
            ],
            "title": "Nonparametric estimation of average treatment effects under exogeneity: A review",
            "venue": "Review of Economics and statistics,",
            "year": 2004
        },
        {
            "authors": [
                "Kuanhao Jiang",
                "Rajarshi Mukherjee",
                "Subhabrata Sen",
                "Pragya Sur"
            ],
            "title": "A new central limit theorem for the augmented ipw estimator: Variance inflation, cross-fit covariance and beyond",
            "venue": "arXiv preprint arXiv:2205.10198,",
            "year": 2022
        },
        {
            "authors": [
                "Nathan Kallus",
                "Masatoshi Uehara"
            ],
            "title": "Doubly robust off-policy value and gradient estimation for deterministic policies",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Nathan Kallus",
                "Angela Zhou"
            ],
            "title": "Policy evaluation and optimization with continuous treatments",
            "venue": "In International conference on artificial intelligence and statistics,",
            "year": 2018
        },
        {
            "authors": [
                "Nathan Kallus",
                "Xiaojie Mao",
                "Masatoshi Uehara"
            ],
            "title": "Causal inference under unmeasured confounding with negative controls: A minimax learning approach",
            "venue": "arXiv preprint arXiv:2103.14029,",
            "year": 2021
        },
        {
            "authors": [
                "Joseph DY Kang",
                "Joseph L Schafer"
            ],
            "title": "Demystifying double robustness: A comparison of alternative strategies for estimating a population mean from incomplete data",
            "year": 2007
        },
        {
            "authors": [
                "Edward H Kennedy",
                "Zongming Ma",
                "Matthew D McHugh",
                "Dylan S Small"
            ],
            "title": "Non-parametric methods for doubly robust estimation of continuous treatment effects",
            "venue": "Journal of the Royal Statistical Society Series B: Statistical Methodology,",
            "year": 2017
        },
        {
            "authors": [
                "Sylvia Klosin"
            ],
            "title": "Automatic double machine learning for continuous treatment effects",
            "venue": "arXiv preprint arXiv:2104.10334,",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Kompa",
                "David Bellamy",
                "Tom Kolokotrones",
                "Andrew Beam"
            ],
            "title": "Deep learning methods for proximal inference via maximum moment restriction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Manabu Kuroki",
                "Judea Pearl"
            ],
            "title": "Measurement bias and effect restoration",
            "venue": "in causal inference. Biometrika,",
            "year": 2014
        },
        {
            "authors": [
                "Afsaneh Mastouri",
                "Yuchen Zhu",
                "Limor Gultchin",
                "Anna Korba",
                "Ricardo Silva",
                "Matt Kusner",
                "Arthur Gretton",
                "Krikamol Muandet"
            ],
            "title": "Proximal causal learning with kernels: Two-stage estimation and moment restriction",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Wang Miao",
                "Zhi Geng",
                "Eric J Tchetgen Tchetgen"
            ],
            "title": "Identifying causal effects with proxy variables of an unmeasured confounder",
            "year": 2018
        },
        {
            "authors": [
                "Wang Miao",
                "Xu Shi",
                "Eric Tchetgen Tchetgen"
            ],
            "title": "A confounding bridge approach for double negative control inference on causal effects",
            "venue": "arXiv preprint arXiv:1808.04945,",
            "year": 2018
        },
        {
            "authors": [
                "Krikamol Muandet",
                "Wittawat Jitkrittum",
                "Jonas K\u00fcbler"
            ],
            "title": "Kernel conditional moment test via maximum moment restriction",
            "venue": "In Conference on Uncertainty in Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Krikamol Muandet",
                "Arash Mehrjou",
                "Si Kai Lee",
                "Anant Raj"
            ],
            "title": "Dual instrumental variable regression",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jonas Peters",
                "Dominik Janzing",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Elements of causal inference: foundations and learning algorithms",
            "year": 2017
        },
        {
            "authors": [
                "Zhengling Qi",
                "Rui Miao",
                "Xiaoke Zhang"
            ],
            "title": "Proximal learning for individualized treatment regimes under unmeasured confounding",
            "venue": "Journal of the American Statistical Association,",
            "year": 2023
        },
        {
            "authors": [
                "Danilo Rezende",
                "Shakir Mohamed"
            ],
            "title": "Variational inference with normalizing flows",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "James Robins",
                "Mariela Sued",
                "Quanhong Lei-Gomez",
                "Andrea Rotnitzky"
            ],
            "title": "Comment: Performance of double-robust estimators when\u201d inverse probability\u201d weights are highly variable",
            "venue": "Statistical Science,",
            "year": 2007
        },
        {
            "authors": [
                "Bernhard Sch\u00f6lkopf",
                "Ralf Herbrich",
                "Alex J Smola"
            ],
            "title": "A generalized representer theorem",
            "venue": "In International conference on computational learning theory,",
            "year": 2001
        },
        {
            "authors": [
                "Xu Shi",
                "Wang Miao",
                "Jennifer C Nelson",
                "Eric J Tchetgen Tchetgen"
            ],
            "title": "Multiply robust causal inference with double-negative control adjustment for categorical unmeasured confounding",
            "venue": "Journal of the Royal Statistical Society Series B: Statistical Methodology,",
            "year": 2020
        },
        {
            "authors": [
                "Rahul Singh"
            ],
            "title": "Kernel methods for unobserved confounding: Negative controls, proxies, and instruments",
            "venue": "arXiv preprint arXiv:2012.10315,",
            "year": 2020
        },
        {
            "authors": [
                "Kihyuk Sohn",
                "Honglak Lee",
                "Xinchen Yan"
            ],
            "title": "Learning structured output representation using deep conditional generative models",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Eric J Tchetgen Tchetgen",
                "Andrew Ying",
                "Yifan Cui",
                "Xu Shi",
                "Wang Miao"
            ],
            "title": "An introduction to proximal causal learning",
            "venue": "arXiv preprint arXiv:2009.10982,",
            "year": 2020
        },
        {
            "authors": [
                "Stefan T\u00fcbbicke"
            ],
            "title": "Entropy balancing for continuous treatments",
            "venue": "Journal of Econometric Methods,",
            "year": 2021
        },
        {
            "authors": [
                "Hal R Varian"
            ],
            "title": "Causal inference in economics and marketing",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2016
        },
        {
            "authors": [
                "Martin J Wainwright"
            ],
            "title": "High-dimensional statistics: A non-asymptotic viewpoint, volume 48",
            "year": 2019
        },
        {
            "authors": [
                "Spencer Woody",
                "Carlos M Carvalho",
                "P Richard Hahn",
                "Jared S Murray"
            ],
            "title": "Estimating heterogeneous effects of continuous exposures using bayesian tree ensembles: revisiting the impact of abortion rates on crime",
            "venue": "arXiv preprint arXiv:2007.09845,",
            "year": 2020
        },
        {
            "authors": [
                "Liyuan Xu",
                "Heishiro Kanagawa",
                "Arthur Gretton"
            ],
            "title": "Deep proxy causal learning and its application to confounded bandit policy evaluation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Afsaneh Yazdani",
                "Eric Boerwinkle"
            ],
            "title": "Causal inference in the age of decision medicine",
            "venue": "Journal of data mining in genomics & proteomics,",
            "year": 2015
        },
        {
            "authors": [
                "Andrew Ying",
                "Wang Miao",
                "Xu Shi",
                "Eric J Tchetgen Tchetgen"
            ],
            "title": "Proximal causal inference for complex longitudinal studies",
            "venue": "Journal of the Royal Statistical Society Series B: Statistical Methodology,",
            "year": 2023
        },
        {
            "authors": [
                "Rui Zhang",
                "Masaaki Imaizumi",
                "Bernhard Sch\u00f6lkopf",
                "Krikamol Muandet"
            ],
            "title": "Maximum moment restriction for instrumental variable regression",
            "venue": "arXiv preprint arXiv:2010.07684,",
            "year": 2010
        },
        {
            "authors": [
                "\u03b4\u0302n. Moreover",
                "Dikkala"
            ],
            "title": "2020) suggests using the covering number to obtain an upper bound on the empirical Rademacher complexity and thus the critical radius. We denote withN(\u03b5,G, \u2225\u00b7\u2225) as the size of the smallest empiricalcover of G. The empirical metric entropy of G is defined as H(\u03b5,G",
            "year": 2020
        },
        {
            "authors": [
                "els. Then Miao"
            ],
            "title": "2018b) and its extensions (Shi et al., 2020",
            "venue": "Tchetgen et al.,",
            "year": 2020
        },
        {
            "authors": [
                "Ying"
            ],
            "title": "Building upon this foundation, Cui et al",
            "year": 2023
        },
        {
            "authors": [
                "Mastouri"
            ],
            "title": "2021) propose to use a two-stage kernel estimator for the outcome bridge function",
            "year": 2021
        },
        {
            "authors": [
                "KPV",
                "Xu"
            ],
            "title": "2021) further improved upon this with an adaptive features derived from neural networks (DFPV). Besides, an alternative approach based on maximum moment restriction",
            "year": 2021
        },
        {
            "authors": [
                "Mastouri"
            ],
            "title": "2021) extends the MMR framework to the proximal setting through the use",
            "year": 2021
        },
        {
            "authors": [
                "Hu"
            ],
            "title": "2023), as well as the times-series setting introduced in Miao et al. (2018b) that satisfies the proximal causality framework",
            "venue": "Similar to Tab. 1,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The causal effect estimation is a significant issue in many fields such as social sciences (Hedstro\u0308m & Ylikoski, 2010), economics (Varian, 2016), and medicine (Yazdani & Boerwinkle, 2015). A critical challenge in causal inference is non-compliance to randomness due to the presence of unobserved confounders, which can induce biases in the estimation.\nOne approach to address this challenge is the proximal causal learning (PCL) framework (Miao et al., 2018a; Tchetgen et al., 2020; Cui et al., 2023), which offers an opportunity to learn about causal effects where ignorability condition fails. This framework employs two proxies - a treatmentinducing proxy and an outcome-inducing proxy - to identify the causal effect by estimating the bridge/nuisance functions. Particularly, Cui et al. (2023) derived the doubly robust estimator within the PCL framework, which combines the estimator obtained from the treatment bridge function and the estimator obtained from the outcome bridge function. The doubly robust estimator has been widely used in causal effect estimation (Bang & Robins, 2005), as it is able to tolerate violations of model assumptions of bridge functions.\nHowever, current doubly robust estimators (Cui et al., 2023) within the proximal causal framework mainly focus on binary treatments, whereas the treatments can be continuous in many real-world scenarios, including social science, biology, and economics. For example, in therapy studies, we are not only interested in estimating the effect of receiving the drug but also the effectiveness of the drug dose. Another example comes from the data (Donohue III & Levitt, 2001) that focused on policy-making, where one wishes to estimate the effect of legalized abortion on the crime rate.\n\u2217Corresponding author 1 Code is available at https://github.com/yezichu/PCL_Continuous_Treatment.\nPrevious work on causal effect for continuous treatments has focused primarily on the unconfoundedness assumption (Kallus & Zhou, 2018; Colangelo & Lee, 2020). However, extending them within the proximal causal farmework encounters several key challenges. Firstly, the Proximal Inverse Probability Weighting (PIPW) part in the original doubly robust (DR) estimator relies on a delta function centered around the treatment value being analyzed, rendering it impractical for empirically estimating causal effects with continuous treatments. Secondly, deriving the influence function will involve dealing with the Gateaux derivative of bridge functions, which is particularly intricate due to its implicit nature. Lastly, the existing estimation process of bridge functions requires running an optimization for each new treatment, rendering it computationally inefficient for practical applications. In light of these formidable challenges, our contribution lies in addressing the open question of deriving the DR estimator for continuous treatments within the proximal causal framework.\nTo address these challenges, we propose a kernel-based method that can well handle continuous treatments for PCL. Specifically, we incorporate the kernel function into the PIPW estimator, as a smooth approximation to causal effect. We then derive the DR estimator and show its consistency for a broad family of kernel functions. Equipped with smoothness, we show that such a DR estimator coincides with the influence function. To overcome the computational issue in nuisance function estimation, we propose to estimate the propensity score and incorporate it into a min-max optimization problem, which is sufficient to estimate the nuisance functions for all treatments. We show that our estimator enjoys the O(n\u22124/5) convergence rate in mean squared error (MSE). We demonstrate the utility and efficiency on synthetic data and the policy-making (Donohue III & Levitt, 2001).\nContributions. To summarize, our contributions are:\n1. We propose a kernel-based DR estimator that is provable to be consistent for continuous treatments effect within the proximal causal framework.\n2. We efficiently solve bridge functions for all treatments with only a single optimization.\n3. We present the convergence analysis of our estimator in terms of MSE.\n4. We demonstrated the utility of our estimator on two synthetic data and real data."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "Proximal Causal Learning. The proximal causal learning (PCL) can be dated back to Kuroki & Pearl (2014), which established the identification of causal effects in the presence of unobserved confounders under linear models. Then Miao et al. (2018a;b) and its extensions (Shi et al., 2020; Tchetgen et al., 2020) proposed to leverage two proxy variables for causal identification by estimating the outcome bridge function. Building upon this foundation, Cui et al. (2023) introduced a treatment bridge function and incorporated it into the Proximal Inverse Probability Weighting (PIPW) estimator. Besides, under binary treatments, they derived the Proximal Doubly Robust (PDR) estimator via influence functions. However, continuous treatments pose a challenge as the treatment effect is not pathwise differentiable with respect to them, preventing the derivation of a DR estimator. In this paper, we employ the kernel method that is provable to be consistent in treatment effect estimation. We further show that the kernel-based DR estimator can be derived from influence functions.\nCausal inference for Continuous Treatments. The most common approaches for estimating continuous treatment effects are regression-based models (Imbens, 2004; Hill, 2011), generalized propensity score-based models (Imbens, 2000; Hirano & Imbens, 2004; Imai & Van Dyk, 2004), and entropy balance-based methods (Hainmueller, 2012; Imai & Ratkovic, 2014; Tu\u0308bbicke, 2021). Furthermore, Kennedy et al. (2017); Kallus & Zhou (2018) and Colangelo & Lee (2020) extended the DR estimation to continuous treatments by combining regression-based models and the generalized propensity score-based models. However, it remains open to derive the DR estimator for continuous treatments within the proximal causal framework. In this paper, we fill in this blank with a new kernel-based DR estimator that is provable to derive from influence function.\nNuisance Parameters Estimation. In proximal causal learning, one should estimate nuisance parameters to obtain the causal effect. Many methods have been proposed for this goal (Tchetgen et al., 2020; Singh, 2020; Xu et al., 2021; Kompa et al., 2022), but they primarily focus on the estimation of the outcome bridge function. Recently, Kallus et al. (2021); Ghassami et al. (2022) have provided non-parametric estimates of treatment bridge function, but they are restricted to binary treatments. When it comes to continuous treatments, existing methods can be computationally inefficient since\nit has to resolve an optimization problem for each treatment. In this paper, we propose a new method that can efficiently solve bridge functions for all treatments with only a single optimization."
        },
        {
            "heading": "3 PROXIMAL CAUSAL INFERENCE",
            "text": "Problem setup. We consider estimating the Average Causal Effect (ACE) of a continuous treatment A on an outcome Y : E[Y (a)], where Y (a) for any a \u2208 supp(A) denotes the potential outcome when the treatment A = a is received. We respectively denote X and U as observed covariates and unobserved confounders. To estimate E[Y (a)], we assume the following consistency assumptions that are widely adopted in causal inference (Peters et al., 2017): Assumption 3.1 (Consistency and Positivity). We assume (i) Y (A) = Y almost surely (a.s.); and (ii) 0 < p(A = a|U = u,X = x) < 1 a.s. Assumption 3.2 (Latent ignorability). We assume Y (a) \u22a5 A|U,X .\nAssump. 3.2 means that the strong ignorability condition may fail due to the presence of unobserved confounder U . To account for such a confounding bias, the proximal causal learning incorporates a treatment-inducing proxy Z and an outcome-inducing proxy W . As illustrated in Fig. 1, these proxies should satisfy the following conditional independence: Assumption 3.3 (Conditional Independence of Proxies). The treatment-inducing proxy Z and the outcome-inducing proxy W satisfy the following conditional independence: (i) Y \u22a5 Z | A,U,X; and (ii) W \u22a5 (A,Z) | U,X .\nEquipped with such conditional independence, previous work by Miao et al. (2018a); Cui et al. (2023) demonstrated that we can express the causal effect, denoted as \u03b2(a), as follows:\nE [Y (a)] = E [h0(a,W,X)] = E [I(A = a)q0(a, Z,X)Y ] , (1) where h0 and q0 are two nuisance/bridge functions such that the following equations hold:\nRh(h0; y) := E[Y \u2212 h0(A,W,X)|A,Z,X] = 0, (2) Rq(q0; p) := E [q0(A,Z,X)\u2212 1/p(A|W,X)|A,W,X] = 0. (3)\nTo ensure the existence and uniqueness of solutions to the above equations, we additionally assume that (Miao et al., 2018a; Tchetgen et al., 2020; Cui et al., 2023): Assumption 3.4. Let \u03bd denote any square-integrable function. For any (a, x), we have\n1. (Completeness for outcome bridge functions). We assume that E[\u03bd(U)|W,a, x] = 0 and E[\u03bd(Z)|W,a, x] = 0 iff \u03bd(U) = 0 almost surely. 2. (Completeness for treatment bridge functions). We assume that E[\u03bd(U)|Z, a, x] = 0 and E[\u03bd(W )|Z, a, x] = 0 iff \u03bd(U) = 0 almost surely.\nUnder assump. 3.4, we can solve h0 and q0 via several optimization approaches derived from conditional moment equations, including two-stage penalized regression (Singh, 2020; Mastouri et al., 2021; Xu et al., 2021), maximum moment restriction (Zhang et al., 2020; Muandet et al., 2020a), and minimax optimization (Dikkala et al., 2020; Muandet et al., 2020b; Kallus et al., 2021). With solved h0, q0, we can estimate E[Y (a)] via:\nEn[Y (a)] = 1\nn n\u2211 i=1 h0(ai, wi, xi), or En[Y (a)] = 1 n n\u2211 i=1 I(ai = a)q0(a, zi, xi)yi.\nFurthermore, Cui et al. (2023) proposes a doubly robust estimator to improve robustness against misspecification of bridge functions.\nE [Y (a)] = E[I(A = a)q0(a, Z,X) (Y \u2212 h0(a,W,X)) + h0(a,W,X)], (4)\n\u2248 1 n n\u2211 i=1 (I(A = a)q0(a, zi, xi)(yi \u2212 h0(a,wi, xi)) + h0(a,wi, xi)) . (5)\nAlthough this proximal learning method can efficiently estimate E[Y (a)] for binary treatments, it suffers from several problems when it comes to continuous treatments. First, for any a \u2208 supp(A), it almost surely holds that there does not exist any sample i that satisfies ai = a for i = 1, ..., n, making Eq. 5 infeasible. Besides, it is challenging to derive the influence function for continuous treatments as it involves the derivative computation for implicit functions h0 and q0. Lastly, to estimate q0, previous methods suffered from a large computational cost since they had to re-run the optimization algorithm for each new treatment, making it inapplicable in real-world applications.\nTo resolve these problems for continuous treatment, we first introduce a kernel-based method in Sec. 4, which can estimate E[Y (a)] in a feasible way. Then in Sec. 5, we introduce a new optimization algorithm that can estimate h0, q0 for all treatments with a single optimization algorithm. Finally, we present the theoretical results in Sec. 6."
        },
        {
            "heading": "4 PROXIMAL CONTINUOUS ESTIMATION",
            "text": "In this section, we introduce a kernel-based doubly robust estimator for \u03b2(a) := E[Y (a)] with continuous treatments. We first present the estimator form in Sec. 4.1, followed by Sec. 4.2 to show that such an estimator can well approximate the influence function for \u03b2(a)."
        },
        {
            "heading": "4.1 KERNEL-BASED PROXIMAL ESTIMATION",
            "text": "As mentioned above, the main challenge for continuous treatments lies in the estimation infeasibility caused by the indicator function in the proximal inverse probability weighted estimator (PIPW) with q0: \u03b2\u0302(a) = 1n \u2211n i=1 I(ai = a)q0(a, zi, xi)yi. To resolve this problem, we note that the indicator function can be viewed as a Dirac delta function \u03b4a(ai). The average of this Dirac delta function over n samples 1n \u2211n i=1 \u03b4a(ai) approximates to the marginal probability P(a) (Doucet et al., 2009), which equals to 0 when A is continuous.\nTo address this problem, we integrate the kernel function K(A \u2212 a) that can alleviate the unit concentration of the Dirac delta function. We can then rewrite the PIPW estimator as follows, dubbed as Proximal Kernel Inverse Probability Weighted (PKIPW) estimator:\n\u03b2\u0302(a) = 1\nn n\u2211 i=1 Khbw(ai \u2212 a)q0(a, zi, xi)yi, (6)\nwhere hbw > 0 is the bandwidth such that Khbw(ai \u2212 a) = 1hbwK ( ai\u2212a hbw ) . The kernel function Khbw(A\u2212 a) that has been widely adopted in density estimation, assigns a non-zero weight to each sample, thus making it feasible to estimate \u03b2(a). To demonstrate its validity, we next show that it can approximate \u03b2(a) well. This result requires that the kernel functionK is bounded differentiable, as formally stated in the following. Assumption 4.1. The second-order symmetric kernel function K (\u00b7) is bounded differentiable, i.e.,\u222b k(u)du = 1, \u222b uk(u)du = 0, \u03ba2(K) = \u222b u2k(u)du <\u221e. We define \u2126(i)2 (K) = \u222b (k(i)(u))2du.\nAssump. 4.1 adheres to the conventional norms within the domain of nonparametric kernel estimation and maintains its validity across widely adopted kernel functions, including but not limited to the Epanechnikov and Gaussian kernels. Under assump. 4.1, we have the following theorem: Theorem 4.2. Under assump. 4.1, suppose \u03b2(a) = E[I(A = a)q0(a, Z,X)Y ] is continuous and bounded uniformly respect to a, then we have\nE[Y (a)] = E[I(A = a)q0(a, Z,X)Y ] = lim hbw\u21920 E [Khbw(A\u2212 a)q0(a, Z,X)Y ] ,\nRemark 4.3. The kernel function has been widely used in machine learning applications (Kallus & Zhou, 2018; Kallus & Uehara, 2020; Colangelo & Lee, 2020; Klosin, 2021). Different from these works, we are the first to integrate them into the proximal estimation to handle continuous treatments. Remark 4.4. The choice of bandwidth hbw is a trade-off between bias and variance. When hbw is small, the kernel estimator has less bias as shown in Thm. 4.2, however, will increase the variance. In Sec. 6, we show that the optimal rate for hbw is O(n\u22121/5), which leads to the MSE converges at a rate of O(n\u22124/5) for our kernel-based doubly robust estimator.\nSimilar to Eq. 6, we can therefore derive the Proximal Kernel Doubly Robust (PKDR) estimator as:\n\u03b2\u0302(a) = 1\nnhbw n\u2211 i=1 K ( ai \u2212 a hbw ) (yi \u2212 h0(a,wi, xi)) q0(a, zi, xi) + h0(a,wi, xi). (7)\nSimilar to Thm. 4.2, we can also show that this estimator is unbiased as hbw \u2192 0. In the subsequent section, we show that this estimator in Eq. 7 can also be derived from the smooth approximation of the influence function of \u03b2(a)."
        },
        {
            "heading": "4.2 INFLUENCE FUNCTION UNDER CONTINUOUS TREATMENTS",
            "text": "In this section, we employ the method of Gateaux derivative (Carone et al., 2018; Ichimura & Newey, 2022) to derive the influence function of \u03b2(a). (For our non-regular parameters, we borrow the terminology \u201cinfluence function\u201d in estimating a regular parameter. See Hampel Ichimura & Newey (2022), for example.) Specifically, we denote PX as the distribution function for any variableX , and rewrite \u03b2(a) as \u03b2(a;P0O) where P0O denotes the true distribution forO := (A,Z,W,X, Y ). Besides, we consider the special submodel P\u03b5hbwO = (1 \u2212 \u03b5)P0O + \u03b5P hbw O , where P hbw O (\u00b7) maps a point o to a distribution of O, i.e., PhbwO (o) for a fixed o denotes the distribution of O that approximates to a point mass at o. Different types of PhbwO (o) lead to different forms of Gateaux derivative. In our paper, we choose the distribution PhbwO (o) whose corresponding probability density function (pdf) phbwO (o) = Khbw(O\u2212 o)I(p0O(o) > hbw), which has limhbw\u21920 p hbw O (o) = limhbw\u21920Khbw(O\u2212 o).\nWe can then calculate the limit of the Gateaux derivative (Ichimura & Newey, 2022) of the functional \u03b2(a;P\u03b5hbwO ) with respect to a deviation P hbw O \u2212 P0O. The following theorem shows that our kernelbased doubly robust estimator corresponds to the influence function: Theorem 4.5. Under a nonparametric model, the limit of the Gateaux derivative is\nlim hbw\u21920\n\u2202\n\u2202\u03b5 \u03b2(a;P\u03b5hbw ) \u2223\u2223\u2223\u2223 \u03b5=0 = (Y \u2212 h0(a,W,X)) q0(a, Z,X) lim hbw\u21920 Khbw (A\u2212 a) + h0 (a,W,X)\u2212 \u03b2(a)\nRemark 4.6. For binary treatments, the DR estimator with the indicator function in Eq. 4 corresponds to the efficient influence function, as derived within the non-parametric framework (Cui et al., 2023). Different from previous works Colangelo & Lee (2020), deriving the influence function within the proximal causal framework is much more challenging as it involves the Gateau derivatives for nuisance functions h0, q0 that have implicit functional forms. By employing our estimator, even when the unconfoundedness assumption from Colangelo & Lee (2020) is not satisfied, we can still effectively obtain causal effects."
        },
        {
            "heading": "5 NUISANCE FUNCTION ESTIMATION",
            "text": "In this section, we propose to solve h0, q0 from integral equations Eq. 2, 3 for continuous treatments. We first introduce the estimation of q0. Previous methods (Kallus et al., 2021; Ghassami et al., 2022) solved q0(a, Z,X) by running an optimization algorithm for each a = 0, 1. However, it is computationally infeasible for continuous treatments. Please see Appx. D.2 for detailed comparison. Instead of running an optimization for each a, we would like to estimate q0(A,Z,X) with a single optimization algorithm. To achieve this goal, we propose a two-stage estimation algorithm. We first estimate the policy function p(A|w, x) and plug into Eq. 3. To efficiently solve q0, we note that it is equivalent to minimize the residual mean squared error denoted as Lq(q; p) = E[(Rq (q, p))2]. According to the lemma shown below, such a mean squared error can be reformulated into a maximization-style optimization, thereby converting into a min-max optimization problem. Lemma 5.1. Denote \u2225f(X)\u22252L2 := E[f 2(X)]. For any parameter \u03bbm > 0, we have\nLq(q; p) = sup m\u2208M E [m(A,W,X) (q0(A,Z,X)\u2212 1/p(A|W,X))]\u2212 \u03bbm\u2225m(A,W,X)\u22252L2 ,\nwhere M is the space of continuous functions over (A,W,X).\nWe leave the proof in Appx. D. Motivated by Lemma. 5.1, we can solve q0 via the following minmax optimization:\nmin q\u2208Q max m\u2208M\n\u03a6n,\u03bbmq (q,m; p) := 1\nn \u2211 i ( q(ai, zi, xi)\u2212\n1\np(ai|wi, xi)\n) m(ai, wi, xi)\u2212 \u03bbm\u2225m\u222522,n,\n(8)\nwhere \u03bbm\u2225m\u222522,n is called stabilizer with \u2225m\u222522,n := 1n \u2211 im 2(ai, wi, xi). We can parameterize q and m as reproducing kernel Hilbert space (RKHS) with kernel function to solve the min-max problem. We derive their closed solutions in the Appendix F. Besides, we can also use Generative Adversarial Networks (Goodfellow et al., 2014) to solve this problem.\nEstimating the policy function p(A = a|w, x). To optimize Eq. 8, we should first estimate p(a|w, x). Several methods can be used for this estimation, such as the kernel density estimation and normalizing flows (Chen, 2017; Bishop, 1994; Ambrogioni et al., 2017; Sohn et al., 2015; Rezende & Mohamed, 2015; Dinh et al., 2016). In this paper, we employ the kernel density function (Chen, 2017) that has been shown to be effective in low-dimension scenarios. When the dimension of (W,X) is high, we employ the conditional normalizing flows (CNFs), which have been shown to be universal density approximator (Durkan et al., 2019) and thus can be applied to complex scenarios.\nNuisance function h0. Since the estimation of h0 does not involve indicator functions, we can apply many off-the-shelf optimization approaches derived from conditional moment equations, such as two-stage penalized regression (Singh, 2020; Mastouri et al., 2021; Xu et al., 2021), maximum moment restriction (Zhang et al., 2020; Muandet et al., 2020a), and minimax optimization (Dikkala et al., 2020; Muandet et al., 2020b). To align well with q0, here we choose to estimate h0 via the following min-max optimization problem that has been derived in Kallus et al. (2021):\nmin h\u2208H max g\u2208G\n\u03a6 n,\u03bbg h (h, g) :=\n1\nn \u2211 i g(ai, zi, xi) (yi \u2212 h(ai, wi, xi))\u2212 \u03bbg\u2225g\u222522,n, (9)\nwhere H and G respectively denote the bridge functional class and the critic functional class."
        },
        {
            "heading": "6 THEORETICAL RESULTS",
            "text": "In this section, we provide convergence analysis of Eq. 8, 9 for nuisance functions h0, q0, as well as for the causal effect \u03b2(a) with the PKDR estimator in Eq. 7.\nWe first provide convergence analysis for q0, while the result for h0 is similar and left to the Appx. E. Different from previous works Dikkala et al. (2020); Ghassami et al. (2022), our analysis encounters a significant challenge arising from the estimation error inherent in the propensity score function. By addressing this challenge, our result can effectively account for this estimation error.\nFormally speaking, we consider the projected residual mean squared error (RMSE) E[projq(q\u0302 \u2212 q0)\n2], where projq(\u00b7) := E [\u00b7|A,W,X]. Before presenting our results, we first introduce the assumption regarding the critic functional class in M, which has been similarly made in Dikkala et al. (2020); Ghassami et al. (2022); Qi et al. (2023).\nAssumption 6.1. (1) (Boundness) \u2225Q\u2225\u221e <\u221e and p\u0302 is uniformly bounded; (2) (Symmetric) M is a symmetric class, i.e, if m \u2208 M, then \u2212m \u2208 M; (3) (Star-shaped) M is star-shaped class, that is for each function m in the class, \u03b1m for any \u03b1 \u2208 [0, 1] also belongs to the class; (4) (Realizability) q0 \u2208 Q; (5) (Closedness) 12\u03bbm projq(q \u2212 q0) \u2208 M.\nUnder assumption 6.1, we have the following convergence result in terms of \u2225 projq(q\u0302 \u2212 q0)\u2225L2 . Theorem 6.2. Let \u03b4qn respectively be the upper bound on the Rademacher complexity of M. For any \u03b7 \u2208 (0, 1), define \u03b4q := \u03b4qn + c q 0 \u221a log(cq1/\u03b7) n for some constants c q 0, c q 1; then under assump. 6.1, we have with probability 1\u2212 \u03b7 that\u2225\u2225projq(q\u0302 \u2212 q0)\u2225\u22252 = O(\u03b4q\u221a\u03bb2m + \u03bbm + 1 + \u2225\u2225\u2225\u22251p \u2212 1p\u0302 \u2225\u2225\u2225\u2225 2 ) , p stands for p(a|w, x).\nRemark 6.3. Inspired by Chen & Pouzo (2012); Dikkala et al. (2020); Kallus et al. (2021), we can obtain the same upper bound for the RMSE \u2225q\u0302 \u2212 q0\u22252, up to a measure of ill-posedness denoted as \u03c4q := supq\u2208Q \u2225q \u2212 q0\u22252/\u2225projq(q \u2212 q0)\u22252 <\u221e.\nThe bound mentioned above comprises two components. The first part pertains to the estimation of q, while the second part concerns the estimation of 1/p. The first part is mainly occupied by the Rademacher complexity \u03b4qn, which can attain O(n\n\u22121/4) if we parameterize M as bounded metric entropy such as Holder balls, Sobolev balls, and RKHSs. For the second part, we can also\nachieve O(n\u22121/4) for \u22251/p \u2212 1/p\u0302\u22252 under some conditions (Chernozhukov et al., 2022; Klosin, 2021; Colangelo & Lee, 2020).\nNow we are ready to present the convergence result for \u03b2(a) within the proximal causal framework.\nTheorem 6.4. Under assump. 3.1-3.4 and 4.1, suppose \u2225h\u0302 \u2212 h\u22252 = o(1), \u2225q\u0302 \u2212 q\u22252 = o(1) and \u2225h\u0302 \u2212 h\u22252\u2225q\u0302 \u2212 q\u22252 = o((nhbw)\u22121/2), nh5bw = O(1), nhbw \u2192 \u221e, h0(a,w, x), p(a, z|w, x) and p(a,w|z, x) are twice continuously differentiable wrt a as well as h0, q0, h\u0302, q\u0302 are uniformly bounded. Then for any a, we have the following for the bias and variance of the PKDR estimator given Eq. 7:\nBias(\u03b2\u0302(a)) := E[\u03b2\u0302(a)]\u2212\u03b2(a) = h 2 bw\n2 \u03ba2(K)B+o((nhbw)\n\u22121/2),Var[\u03b2\u0302(a)] = \u21262(K)\nnhbw (V +o(1)),\nwhere B = E[q0(a, Z,X)[2 \u2202\u2202Ah0(a,W,X) \u2202 \u2202Ap(a,W | Z,X) +\n\u22022\n\u2202A2h0(a,W,X)]], V = E[I(A = a)q0(a, Z,X)\n2(Y \u2212 h0(a,W,X))2]. Remark 6.5. The smoothness condition can hold for a broad family of distributions and be thus similarly made for kernel-based methods (Kallus & Zhou, 2018; Kallus & Uehara, 2020). According to Thm. 6.2, we have \u2225h\u0302 \u2212 h0\u22252 = O(n\u22121/4) and \u2225q\u0302 \u2212 q0\u22252 = O(n\u22121/4), thus can satisfy the consistency condition required as long as hbw = o(1). Besides, we show in Thm. E.9 in Appx. E.5 that this estimator is n2/5-consistent.\nFrom Thm. 6.4, we know that the optimal bandwidth is hbw = O(n\u22121/5) in terms of MES that converges at the rate of O(n\u22124/5). Note that this rate is slower than the optimal rate O(n\u22121), which is a reasonable sacrifice to handle continuous treatment within the proximal causal framework and agrees with existing studies (Kennedy et al., 2017; Colangelo & Lee, 2020)."
        },
        {
            "heading": "7 EXPERIMENTS",
            "text": "In this section, we evaluate the effectiveness of our method using two sets of synthetic data \u2014 one in a low-dimensional context and the other in a high-dimensional context \u2014 as well as the legalized abortion and crime dataset (Donohue III & Levitt, 2001). In Appx. G, we conduct experiments on more benchmark datasets, including time-series forecasting.\nCompared baselines. We compare our method with the following baselines that use only h0 for estimation, i.e., \u03b2\u0302(a) = 1n \u2211n i=1 h0(ai, wi, xi): (i) Proximal Outcome Regression (POR) that solved Eq. 9 for estimation; ii) PMMR (Mastouri et al., 2021) that employed the Maximal Moment Restriction (MMR) framework to estimate the bridge function via kernel learning; iii) KPV (Mastouri et al., 2021) that used two-stage kernel regression; iv) DFPV (Xu et al., 2021) that used deep neural networks to model high-dimensional nonlinear relationships between proxies and outcomes; v) MINMAX (Dikkala et al., 2020) that used Generate adversarial networks to solve Eq. 9; vi) NMMR (Kompa et al., 2022) that introduced data-adaptive kernel functions derived from neural networks.\nFor our method, we implement the Inverse probability weighting (IPW) estimator PKIPW that uses q0 for estimation via Eq. 6, and the doubly robust estimator PKDR that used both the nuisance function h0 and q0 to estimate causal effects through Eq. 7. For simplicity, we only present the result of PKDR that uses POR to estimate h0.\nImplementation Details. In the PKIPW and PKDR estimators, we choose the second-order Epanechnikov kernel, with bandwidth hbw = c\u03c3\u0302An\u22121/5 with estimated std \u03c3\u0302A and the hyperparameter c > 0. In our paper, we vary c over the range {0.5, 1, 1.5, \u00b7 \u00b7 \u00b7 , 4.0} and report the optimal c in terms of cMSE. To estimate nuisance functions, we parameterize Q and M (resp., H and G) via RKHS for q0 (resp., h0), where we use Gaussian kernels with the bandwidth parameters being initialized using the median distance heuristic. For policy estimation, we employ the KDE in the low-dimensional synthetic dataset and the real-world data, while opting for CNFs in the highdimensional synthetic dataset. We leave more details about hyperparameters in the Appx. H.\nEvaluation metrics. We report the causal Mean Squared Error (cMSE) across 100 equally spaced points in the range of supp(A): cMSE := 1100 \u2211100 i=1(E[Y ai ] \u2212 E\u0302[Y ai ])2. Here, we respectively take supp(A) := [\u22121, 2], [0, 1], [0, 2] in low-dimensional synthetic data, high-dimensional synthetic data, and real-world data. The truth E[Y a] is derived through Monte Carlo simulations comprising 10,000 replicates of data generation for each a."
        },
        {
            "heading": "7.1 SYNTHETIC STUDY",
            "text": "We consider two distinct scenarios. The first scenario demonstrates the effectiveness of the kernel method in the context of the doubly robust estimator under model misspecification, while the second scenario evaluates the utility in high-dimensional settings. For both scenarios, we report the mean cMSE of each method across 20 times."
        },
        {
            "heading": "7.1.1 DOUBLY ROBUSTNESS STUDY",
            "text": "Data generation. We follow the generative process in Mastouri et al. (2021) and leave details in the Appx. H. Similar to Kang & Schafer (2007); Cui et al. (2023), we consider four scenarios where either or both confounding bridge functions are misspecified by considering a model using a transformation of observed variables:\n\u2022 Scenario 1. We follow Mastouri et al. (2021) to generate data; \u2022 Scenario 2. The outcome confounding bridge function is misspecified with W \u2217 = |W |1/2 + 1; \u2022 Scenario 3. The treatment confounding bridge function is misspecified with Z\u2217 = |Z|1/2 + 1; \u2022 Scenario 4. Both confounding bridge functions are mis-specified.\nResults. We present the mean and the standard deviation (std) of cMSE over 20 times across four scenarios, as depicted in Fig. 2 and Tab. 1. For each scenario, we consider two sample sizes, 500 and 1,000. In the first scenario, our PKDR is comparable and even better than the estimator based on h. For scenarios with misspecification, the PKIPW estimator and the baselines with only h0 respectively perform well in scenario 2 and scenario 3. Notably, the PKDR can constantly perform well in these scenarios, due to its doubly robustness against model mis-specifications. In scenario 4 where both models of h0 and q0 are misspecified, all methods suffer from a large error. Besides, we can see that the PKIPW method has a large variance in scenario 4, where both estimations of the policy function and q0 can be inaccurate due to mis-specifications (Robins et al., 2007; Jiang et al., 2022). It is worth mentioning that compared to others, DFPV exhibits minimal errors in scenario 4. This could be attributed to their approach of individually fitting each variable\u2019s kernel function using different neural networks, thereby enhancing flexibility in their models.\nSensitivity Analysis. According to Thm. 6.4, hbw is the trade-off between bias and variance. To show this, we report the cMSE as c in hbw := c\u03c3\u0302An\u22121/5 varies in {0.5, 1.0, 1.5, ..., 4.0}. As c (i.e.,\nhbw) increases, the cMSE first decreases, then rises, and reaches its optimum at c = 1.5, which is consistent with the optimal value derived in Kallus et al. (2021)."
        },
        {
            "heading": "7.1.2 HIGH DIMENSIONAL STUDY",
            "text": "Data generation. We follow Colangelo & Lee (2020); Singh (2020) to generate data, in which we set dim(X) = 100, dim(Z) = 10, and dim(W ) = 10. Specifically, we set X \u223c N(0,\u03a3) with \u03a3 \u2208 R100\u00d7100 has \u03a3ii = 1 for i \u2208 [dim(X)] and \u03a3ij = 12 \u00b7 I|i\u2212 j| = 1 for i \u0338= j. The outcome Y is generated from Y = A2 + 1.2A+ 1.2(X\u22a4\u03b2x +W\u22a4\u03b2w) +AX1 + 0.25U , where \u03b2x, \u03b2w exhibit quadratic decay, i.e., [\u03b2x]j = j\u22122. More details can be found in the Appx. H.\nResults. We report the mean and std of cMSE over 20 times with sample sizes set to 1,000 and 2,000, as depicted in Fig. 2 and Tab. 1. As shown, we find that the ATE curve fitted by PKDR estimator is closest to the real curve, and its cMSE is also the lowest. This result suggests the robustness of our methods against high-dimensional covariates."
        },
        {
            "heading": "7.2 LEGALIZED ABORTION AND CRIME",
            "text": "We obtain the data from Donohue III & Levitt (2001); Mastouri et al. (2021) that explores the relationship between legalized abortion and crime. In this study, we take the treatment as the effective abortion rate, the outcome variable Y as the murder rate, the treatment-inducing proxy Z as the generosity towards families with dependent children, and the outcome-inducing proxies W as beer consumption per capita, log-prisoner population per capita, and concealed weapons laws. We follow the protocol Woody et al. (2020) to preprocess data. We take the remaining variables as the unobserved confounding variables U . Following Mastouri et al. (2021), the ground-truth value of \u03b2(a) is taken from the generative model fitted to the data.\nResults. The results are presented in Fig. 2 and Tab. 1. It is evident that all three methods effectively estimate \u03b2(a), which suggests the utility of our method in real-world scenarios. However, when a falls within the range of [1.5, 2], deviations become apparent in the fitted curve. We attribute these deviations to an inadequate sample size as Fig. 2. It\u2019s worth noting that the DFPV method employing Neural Networks (NN) exhibits higher variances. This suggests potential numerical instability in certain experiments, a phenomenon in line with observations made in Kompa et al. (2022)."
        },
        {
            "heading": "8 CONCLUSION",
            "text": "In this paper, we propose a kernel-based doubly robust estimator for continuous treatments within the proximal causal framework, where we replace the conventional indicator function with a kernel function. Additionally, we propose a more efficient approach to estimating the nuisance function q0 by estimating the policy function and incorporating it into a min-max optimization. Our analysis reveals that the MSE converges at a rate of O(n\u22124/5) when we select the optimal bandwidth to balance bias and variance. We demonstrate the utility of our PKDR estimator in synthetic as well as the legalized abortion and crime dataset.\nLimitation and future works. Our estimator is required to estimate the policy function, which may lead to a large variance especially when the policy function is mis-specified. Potential solutions include the variance reduction method including the stabilized IPW estimator, whose estimation forms and theoretical analysis will be explored in the future."
        },
        {
            "heading": "9 ACKNOWLEDGMENTS",
            "text": "This work was supported by the National Key Research and Development Program of China (No. 2022YFC2405100); STI 2030\u2014Major Projects (No. 2022ZD0205300); Shanghai Municipal Science and Technology Major Project (No.2021SHZDZX0103); the State Key Program of National Natural Science Foundation of China under Grant No. 12331009. The computations in this research were performed using the CFFF platform of Fudan University."
        },
        {
            "heading": "APPENDIX",
            "text": ""
        },
        {
            "heading": "A Preliminaries 15",
            "text": "A.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\nA.2 Critical Radius . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16"
        },
        {
            "heading": "B Related Works 17",
            "text": ""
        },
        {
            "heading": "C Regularity condition 18",
            "text": ""
        },
        {
            "heading": "D Estimating nuisance function 19",
            "text": "D.1 Proof of Lemma 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nD.2 Comparison of existing methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 20"
        },
        {
            "heading": "E Proofs and Derivation 22",
            "text": "E.1 Proof of Theorem 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nE.2 Proof of Theorem 4.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nE.3 Proof of Theorem 6.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\nE.4 Proof of Theorem 6.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nE.5 Consistency of the Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35"
        },
        {
            "heading": "F Computation Details 38",
            "text": ""
        },
        {
            "heading": "G Additional Experiments 40",
            "text": "G.1 Experiments with Different Data Generating Process . . . . . . . . . . . . . . . . 40\nG.2 Experiments for Time Series Data . . . . . . . . . . . . . . . . . . . . . . . . . . 41\nG.3 Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41"
        },
        {
            "heading": "H Experiments 43",
            "text": "H.1 Data Generating Process in the Low-Dimensional Synthetic Experiment . . . . . . 43\nH.2 Data Generating Process in the High-Dimensional Synthetic Experiment . . . . . . 43\nH.3 Legalized Abortion and Crime . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\nH.4 Hyperparameters Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44"
        },
        {
            "heading": "A PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "A.1 NOTATION",
            "text": "In this section, we will define some notations used throughout the proof in the appendix. Moreover, we will introduce other notations in the corresponding subsection."
        },
        {
            "heading": "A.2 CRITICAL RADIUS",
            "text": "The bound we provide is based on the critical radii of the function classes involved, as described in Sec. 6. These critical radius are defined in terms of the empirical localized Rademacher critical radius, which characterizes the critical radius of a function class up to a constant factor. Specifically, the empirical Rademacher complexity and population Rademacher complexity of a function class G : V \u2192 [\u22121, 1] is defined as follows:\nR\u0302n(\u03b4;G) = E{\u03f5i}ni=1\n[ sup\ng\u2208G,\u2225g\u22252\u2264\u03b4\n\u2223\u2223\u2223\u2223\u2223 1n n\u2211\ni=1\n\u03f5ig (vi) \u2223\u2223\u2223\u2223\u2223 ]\nRn(\u03b4;G) = E{\u03f5i}ni=1,{vi}ni=1\n[ sup\ng\u2208G,\u2225g\u22252\u2264\u03b4\n\u2223\u2223\u2223\u2223\u2223 1n n\u2211\ni=1\n\u03f5ig (vi) \u2223\u2223\u2223\u2223\u2223 ]\nwhere {vi}ni=1 are i.i.d. samples from some distribution D on V and {\u03f5i}ni=1 are i.i.d. Rademacher random variables taking values equiprobably in {\u22121, 1}. The empirical critical radius is defined as any solution \u03b4\u0302n to R\u0302n(\u03b4;G) \u2264 \u03b42 and the critical radius of a function class G is the smallest solution \u03b4\u2217n to the inequality Rn(\u03b4;G) \u2264 \u03b42. Proposition 14.1 of Wainwright (2019) shows that w.p. 1\u2212 \u03b6,\n\u03b4n = O ( \u03b4\u0302n + \u221a log(1/\u03b6)\nn\n) .\nThus we can choose \u03b4n based on the empirical critical radius \u03b4\u0302n. Moreover, Dikkala et al. (2020) suggests using the covering number to obtain an upper bound on the empirical Rademacher complexity and thus the critical radius. We denote withN(\u03f5,G, \u2225\u00b7\u2225) as the size of the smallest empiricalcover of G. The empirical metric entropy of G is defined as H(\u03f5,G, \u2225 \u00b7 \u2225) = log(N(\u03f5,G, \u2225 \u00b7 \u2225)). An empirical \u03b4-slice of G is defined as G\u03b4 = {g \u2208 G : \u2225g\u22252,n \u2264 \u03b4}. Then the empirical critical radius of G is upper bounded by any solution to the inequality:\u222b \u03b4\n\u03b42/8\n\u221a H(\u03f5,G\u03b4, \u2225 \u00b7 \u2225)\nn d\u03f5 \u2264 \u03b4\n2\n20 (10)"
        },
        {
            "heading": "B RELATED WORKS",
            "text": "Regarding the proximal causal inference framework, there is a growing literature on using proxy variables for causal inference from observational data, due to its ability to account for unmeasured U through two confounding proxy variables: a treatment-inducing proxy Z and an outcome-inducing proxy W , which are respectively independent to the outcome and the treatment.\nThe proximal causal learning (PCL) can be dated back to Kuroki & Pearl (2014), which established the identification of causal effects in the presence of unobserved confounders under linear models. Then Miao et al. (2018b) and its extensions (Shi et al., 2020; Tchetgen et al., 2020) proposed to leverage two proxy variables (Z,W ) for causal identification by estimating the outcome bridge function (Eq. 2). Building upon this foundation, Cui et al. (2023); Ying et al. (2023) introduced a treatment bridge function (Eq. 3) and incorporated it into the Proximal Inverse Probability Weighting (PIPW) estimator. Besides, under binary treatments, they derived the Proximal Doubly Robust (PDR) estimator via influence functions. However, their methods cannot handle continuous treatments, whereas the treatments can be continuous in many real-world scenarios, including social science, biology, and economics. The main challenge for continuous treatments lies in the estimation infeasibility caused by the indicator function in two estimators. To the best of our knowledge, we are the first to generalize the PIPW estimator and PDR estimator to the continuous case by replacing the conventional indicator function with a kernel function. The advantage of using the kernel function is that we do not need to discretize the continuous data, but use the kernel function to incorporate local information about the similar treatments. Besides,we derive the corresponding influence function, a process that involves handling the Gateaux derivative of bridge functions.\nMost existing work focuses on how to estimate the outcome bridge function. Singh (2020) and Mastouri et al. (2021) propose to use a two-stage kernel estimator for the outcome bridge function (KPV), and Xu et al. (2021) further improved upon this with an adaptive features derived from neural networks (DFPV). Besides, an alternative approach based on maximum moment restriction (MMR) uses single-stage estimators of the bridge function. The masterpiece in this regard is that Mastouri et al. (2021) extends the MMR framework to the proximal setting through the use of kernel functions (PMMR). Kompa et al. (2022) introducex data-adaptive kernel functions derived from neural networks. Another traditional method is to transform the conditional moment equation into an unconditional moment equation, and then solve a minimax optimization problem (Dikkala et al., 2020; Ghassami et al., 2022; Qi et al., 2023) (MINIMAX, POR). However, these methods that estimate only the outcome bridge function, rather than also estimating the treatment bridge function, which would permit us to construct a doubly robust estimator. For the treatment bridge function, (Kallus et al., 2021; Ghassami et al., 2022) solved q0(a, Z,X) by running an optimization algorithm for each a = 0, 1. However, it is computationally infeasible for continuous treatments. Instead of running an optimization for each a, we would like to estimate q0(A,Z,X) with a single optimization algorithm. To achieve this goal, we propose a two-stage estimation algorithm. We first estimate the policy function and then convert the conditional moments into equivalent forms, which can lead to a min-max optimization problem."
        },
        {
            "heading": "C REGULARITY CONDITION",
            "text": "One conventional approach to studying their solutions is through singular value decomposition, as discussed by Carrasco et al. (2007). We first introduce the singular value decomposition of the operator:\nGiven Hilbert spaces H1 and H2, a compact operator K : H1 7\u2212\u2192 H2 and its adjoint operator K \u2032 : H2 7\u2212\u2192 H1, there exists a singular system (\u03bbn, \u03c6n, \u03c8n)+\u221en=1 of K with nonzero singular values {\u03bbn} and orthogonal sequences {\u03c6n \u2208 H1} and {\u03c8n \u2208 H2} such that\nK\u03c6n = \u03bbn\u03c8n,K \u2032\u03c8n = \u03bbn\u03c6n.\nBy means of singular value decomposition, Picard\u2019s theorem characterizes the conditions for the existence of solutions of the corresponding Fredholm integral equations of the first type. We apply Picard\u2019s theorem to the setting of proxy variables.\nLet L2{P(x)} denote the space of all square integrable functions of x with respect to a cumulative distribution function P(x), which is a Hilbert space with inner product \u27e8g1, g2\u27e9 =\u222b g1(x)g2(x)dP(x). For brevity, we replace Wij and Zij with W and Z below. Let Ta,x denote the operator: L2{P(w|a, x)} \u2192 L2{P(z|a, x)}, Ta,xh = E[h(W )|z, a, x] and let (\u03bba,x,n, \u03c6a,x,n, \u03d5a,x,n)\u221en=1 denote a singular value decomposition of Ta,x. Also let T \u2032 a,x denote the operator: L2{P(z|a, x)} \u2192 L2{P(w|a, x)}, T \u2032a,xq = E[q(Z)|w, a, x] and let (\u03bb\u2032a,x,n, \u03c6 \u2032 a,x,n, \u03d5 \u2032 a,x,n) \u221e n=1 denote a singular value decomposition of T \u2032 a,x. We assume the following regularity conditions: Assumption C.1. (Regularity conditions)\n1. (Existence of compact operator Ta,x and T \u2032a,x)\u222b \u222b p(w|z, a, x)p(z|w, a, x)dwdz <\u221e;\n2. (Existence of solutions) \u222b E2[Y |z, a, x]p(z|a, x)dz <\u221e;\n3. (Eigenvalue structure of compact operator Ta,x) \u221e\u2211\nn=1\n\u03bb\u22122a,x,n|\u27e8E[Y |z, x, x], \u03d5a,x,n\u27e9|2 <\u221e;\n4. (Existence of solutions) \u222b p\u22122(a|w, x)p(w|a, x)dw <\u221e;\n5. (Eigenvalue structure of compact operator T \u2032a,x)\n\u221e\u2211 n=1 \u03bb \u2032\u22122 a,x,n|\u27e8p\u22121(a|w, x), \u03d5\u2032a,x,n\u27e9|2 <\u221e.\nTheorem C.2 (Cui et al. (2023)). Under Assumption C.1(1,2,3) and Assumption 3.4(1), there exist functions h(w, a, x) such that\nE[Y |Z,A, x] = \u222b h(w,A, x)dP(w|Z,A, x),\nalmost surely.\nTheorem C.3 (Cui et al. (2023)). Under Assumption C.1(1,4,5) and Assumption 3.4(2), there exist functions q(z, a, x) such that\nE[q(Z, a, x)|W,A = a, x] = 1 p(A = a|W,x)\nalmost surely."
        },
        {
            "heading": "D ESTIMATING NUISANCE FUNCTION",
            "text": "We next return to how to solve the nuisance function q. For simplicity, define\nRq(q; p) = E [(\n1\np (A |W,X) \u2212 q (A,Z,X)\n) | A,W,X ]\nNotice that Rq(q, p) being zero almost surely is actually a conditional moment equation, which is equivalent to finding q such that the following residual mean squared error (RMSE) is minimized for all q:\nmin q\u2208Q\nLq(q; p) := E [ (Rq (q, p))2 ] (11)"
        },
        {
            "heading": "D.1 PROOF OF LEMMA 5.1",
            "text": "In the following steps, we will utilize the technique of minimax estimation. To begin with, we will introduce Interchangeability:\nDefinition D.1 (Fenchel duality). Let \u2113 : R \u00d7 R \u2192 R+ be a proper, convex, and lower semicontinuous loss function for any value in its first argument and \u2113\u22c6y := \u2113\n\u22c6(y, \u00b7) a convex conjugate of \u2113y := \u2113(y, \u00b7) which is also proper, convex, and lower semi-continuous w.r.t. the second argument. Then, \u2113y(v) = maxu{uv \u2212 \u2113\u22c6y(u)}. The maximum is achieved at v \u2208 \u2202\u2113\u22c6(u), or equivalently u \u2208 \u2202\u2113(v).\nTheorem D.2 (Interchangeability). Let \u03c9 be a random variable on \u2126 and, for any \u03c9 \u2208 \u2126, the function f(\u00b7, \u03c9) : R \u2192 (\u2212\u221e,\u221e) is proper and upper semi-continuous concave function. Then,\nE\u03c9 [ max u\u2208R f(u, \u03c9) ] = max u(\u00b7)\u2208U(\u2126) E\u03c9[f(u(\u03c9), \u03c9)],\nwhere U(\u2126) := {u(\u00b7) : \u2126 \u2192 R} is the entire space of functions defined on the support \u2126.\nLemma 5.1. Denote \u2225f(X)\u22252L2 := E[f 2(X)]. For any parameter \u03bbm > 0, we have\nLq(q; p) = sup m\u2208M E [m(A,W,X) (q0(A,Z,X)\u2212 1/p(A|W,X))]\u2212 \u03bbm\u2225m(A,W,X)\u22252L2 ,\nwhere M is the space of continuous functions over (A,W,X).\nProof. Notice that the squared loss function \u2113y(v) = 14\u03bb (y\u2212 v) 2, we have \u2113\u22c6y(u) = uy+\u03bbu 2. Then by Fenchel duality, we have\n\u2113y(v) = 1\n4\u03bb (y \u2212 v)2 = max u\u2208R\n{ vu\u2212 \u2113\u22c6y(u) } = max\nu\u2208R\n{ vu\u2212 uy \u2212 \u03bbu2 } = max\nu\u2208R\n{ (v \u2212 y)u\u2212 \u03bbu2 } Let y = E [ 1 p(A|W,X) | A,W,X ] , v = E [q (A,Z,X) | A,W,X] , u = m, we have\n1\n4\u03bbm\n( E [\n1\np (A |W,X) | A,W,X\n] \u2212 E [q (A,Z,X) | A,W,X] )2 = max\nm\u2208R\n{( E [q (A,Z,X) | A,W,X]\u2212 E [ 1\np (A |W,X) | A,W,X\n]) m\u2212 \u03bbmm2 }\nTherefore, applying the interchangeability, we have\nLq(q; p) = 4\u03bbmE\n[ 1\n4\u03bbm\n( E [(\n1\np (A |W,X) \u2212 q (A,Z,X)\n) | A,W,X ])2]\n=4\u03bbmE [ max m\u2208R {( E [q (A,Z,X) | A,W,X]\u2212 E [ 1 p (A |W,X) | A,W,X ]) m\u2212 \u03bbmm2 }] =4\u03bbmE [ max m\u2208R { EZ [ q (A,Z,X)\u2212 1 p (A |W,X) | A,W,X ] m\u2212 \u03bbmm2\n}] =4\u03bbm max m\u2208M E [ E [ q (A,Z,X)\u2212 1 p (A |W,X) | A,W,X ] m (A,W,X)\u2212 \u03bbmm2 (A,W,X)\n] =4\u03bbm max m\u2208M E [( q (A,Z,X)\u2212 1 p (A |W,X) ) m (A,W,X)\u2212 \u03bbmm2 (A,W,X)\n] We define\n\u03a6\u03bbmq (q,m; p) = E [( q (A,Z,X)\u2212 1\np (A |W,X)\n) m (A,W,X) ] \u2212 \u03bbm \u2225m\u222522 . (12)\nAs long as the dual function class M is expressive enough such that 12\u03bbmRq (q, p) \u2208 M, we have\nLq(q; p) = max m\u2208M \u03a6n,\u03bbmq (q,m; p)\nwe can express Eq. 11 in an alternative form:\nmin q\u2208Q max m\u2208M\n\u03a6\u03bbmq (q,m; p). (13)\nWe denote the empirical version\n\u03a6n,\u03bbmq (q,m; p) = 1\nn \u2211 i ( q (zi, ai, xi)\u2212\n1\np (ai | wi, xi)\n) m (ai, wi, xi)\u2212 \u03bbm \u2225m\u222522,n .\nFurthermore, for simplicity, we define \u03a6q(q,m; p) as the non regularized version of \u03a6\u03bbmq . \u03a6q(q,m; p) = E [( q (A,Z,X)\u2212 1\np (A |W,X)\n) m (A,W,X) ] (14)\nIn fact, due to the fact that we need to estimate the density function p, we consider the following min-max optimization problem:\nmin q\u2208Q max m\u2208M\n\u03a6n,\u03bbmq (q,m; p\u0302) (15)\nSimilarly, for the nuisance function h, we consider the following min-max optimization problem:\nmin h\u2208H max g\u2208G\n\u03a6 n,\u03bbg h (h, g) =\n1\nn \u2211 i (yi \u2212 h (wi, ai, xi)) g (ai, zi, xi)\u2212 \u03bbg \u2225g\u222522,n . (16)"
        },
        {
            "heading": "D.2 COMPARISON OF EXISTING METHODS",
            "text": "Closely related to us is the estimation of the general treatments proposed by Kallus et al. (2021). They hope to estimate the generalized average causal effect (GACE):\nJ = E [\u222b Y (a)\u03c0(a | X)d\u00b5(a) ] .\nwhere \u03c0(a | X) is contrast function. Based on the idea, they propose the method for solve the nuisance function q\nmin q\u2208Q max m\u2208M\nEn[\u03c0(A | X)q(A,Z,X)m(A,W,X)\u2212 (Tm)(W,X)]\u2212 \u03bbm \u2225m\u222522,n\nwhere (Tm)(w, x) = \u222b m(a,w, x)\u03c0(a|x)d\u00b5(a).\nHowever, consider the continuous treatments a \u2208 supp(A), then in this case, \u03c0(a | X) = I(A = a). Correspondingly, the conditional moment equation for the action bridge function q is equivalent to\nmin q\u2208Q max m\u2208M\nEn[I (A = a) q(A,Z,X)m(A,W,X)\u2212 (Tm)(W,X)]\u2212 \u03bbm \u2225m\u222522,n\nAs mentioned above, the main challenge for continuous treatments lies in the estimation infeasibility caused by the indicator function. Therefore we cannot estimate the nuisance function q when the treatment is continuous.\nThe second paper to solve q is from Ghassami et al. (2022). They estimate the causal effects of binary treatments and illustrate how the double robustness property of these influence functions can be used to formulate estimating equations for the nuisance functions. Specifically,\nmin q\u2208Q max m\u2208M\nEn [ {\u2212I(A = a)q(Z,X) + 1}m (W,X)\u2212m2 (W,X) ] Then functions q\u0302(a, z, x) = I(a = 0)q\u03020(z, x)+I(a = 1)q\u03021(z, x). Note that since the indicator function appears in their optimization equation, we still cannot solve the case of continuous treatments.\nInstead of running an optimization for each a, we would like to estimate q0(A,Z,X) with a single optimization algorithm. To achieve this goal, we propose a two-stage estimation algorithm. We first estimate the policy function and then convert the conditional moments into equivalent forms, which can lead to a min-max optimization problem.\nmin q\u2208Q max m\u2208M\n1\nn \u2211 i ( q(ai, zi, xi)\u2212\n1\np(ai|wi, xi)\n) m(ai, wi, xi)\u2212 \u03bbm\u2225m\u222522,n.\nSince the treatment is continuous, we have to estimate the policy function before solving the moment equation. However, when the treatment is binary, we can transform the moment equation so that the policy function disappears. This is the cost of estimating the causal effects of continuous treatments."
        },
        {
            "heading": "E PROOFS AND DERIVATION",
            "text": ""
        },
        {
            "heading": "E.1 PROOF OF THEOREM 4.2",
            "text": "Theorem 4.2. Under assump. 4.1, suppose \u03b2(a) = E[I(A = a)q0(a, Z,X)Y ] is continuous and bounded uniformly respect to a, then we have\nE[Y (a)] = E[I(A = a)q0(a, Z,X)Y ] = lim hbw\u21920 E [Khbw(A\u2212 a)q0(a, Z,X)Y ] ,\nProof. By definition we have E[I(A = a0)q0(a0, Z,X)Y ] = \u222b O\\A yq(a0, z, x)dP (a0, z, x, y)\n= \u222b O \u03b4a0 (a) yq0(a, z, x)dP (a, z, x, y)\n= \u222b A \u03b4a0 (a) \u222b O\\A yq0(a, z, x)p (a, z, x, y) d\u00b5(z, x, y)da\n= \u222b A \u03b4a0 (a)\u03b2 (a) da = \u27e8\u03b4a0 , \u03b2\u27e9 = \u03b2 (a0)\nwhere the last equation uses the properties of the Dirac function. Similarly for the kernel function, we also have \u27e8Khbw , \u03b2\u27e9 = \u222b A Khbw(a\u2212 a0)\u03b2 (a) da a=hbws+a0========= \u222b S K(s)\u03b2 (hbws+ a0) ds As \u03b2 is continous and bounded uniformly, this integral is dominated by CK(s). Moreover, because \u03b2 is continuous, the integral converges point-wise to K(s)\u03b2(a0). Applying the dominated convergence theorem and Assumption 4.1(The kernel function integral is 1.) yields\nlim hbw\u21920 \u27e8Khbw , \u03b2\u27e9 = \u222b S lim hbw\u21920 K(s)\u03b2 (hbws+ a0) ds = \u222b S K(s)\u03b2 (a0) ds = \u03b2 (a0)\nwhich finally shows E[Y (a)] = lim\nhbw\u21920 E [Khbw(A\u2212 a)q0(A,Z,X)Y ]"
        },
        {
            "heading": "E.2 PROOF OF THEOREM 4.5",
            "text": "We denote PX as the distribution function for any variable X , and rewrite \u03b2(a) as \u03b2(a;P0O) where P0O denotes the true distribution for O := (A,Z,W,X, Y ). Besides, we consider the special submodel P\u03b5hbwO = (1\u2212\u03b5)P0O+\u03b5P hbw O , where P hbw O (\u00b7) maps a point o to a distribution ofO, i.e., P hbw O (o) for any o denotes the distribution of O that approach a point mass at o as hbw \u2192 0. Theorem 4.5. Under a nonparametric model, the limit of the Gateaux derivative is\nlim hbw\u21920\n\u2202\n\u2202\u03b5 \u03b2(a;P\u03b5hbw ) \u2223\u2223\u2223\u2223 \u03b5=0 = (Y \u2212 h0(a,W,X)) q0(a, Z,X) lim hbw\u21920 Khbw (A\u2212 a) + h0 (a,W,X)\u2212 \u03b2(a)\nProof. Similar to \u03b2(a) rewritten as \u03b2(a;P0O), we can rewrite h0 (a,w, x) as h0 ( a,w, x;P0AWX ) . For simplicity, we omit the subscript of the distribution. Please identify according to context.\n\u2202\n\u2202\u03b5 \u03b2(a;P\u03b5hbw) \u2223\u2223\u2223\u2223 \u03b5=0 = \u2202 \u2202\u03b5 \u222b h0 ( a,w, x;P\u03b5hbw ) dP\u03b5hbw (w, x)\n=\n\u222b \u2202\n\u2202\u03b5 h0 ( a,w, x;P\u03b5hbw )\u2223\u2223 \u03b5=0\ndP0 (w, x)\ufe38 \ufe37\ufe37 \ufe38 (I)\n+ \u222b h0 (a,w, x) \u2202\n\u2202\u03b5 p\u03b5hbw (w, x) \u2223\u2223 \u03b5=0\nd\u00b5 (w, x)\ufe38 \ufe37\ufe37 \ufe38 (II)\nSince P\u03b5hbwO = (1\u2212 \u03b5)P0O + \u03b5P hbw O , we have\n\u2202\n\u2202\u03b5 P\u03b5hbwO \u2223\u2223\u2223 \u03b5=0 = PhbwO \u2212 P 0 O.\nFor term (II)\n(II) = \u222b h0 (a,w, x) \u2202\n\u2202\u03b5 p\u03b5hbw (w, x) \u2223\u2223 \u03b5=0 d\u00b5 (w, x)\n= \u222b h0 (a,w, x) ( phbw (w, x)\u2212 p0 (w, x) ) d\u00b5 (w, x)\nhbw\u21920====== h0 (a,W,X)\u2212 \u03b2 (a)\nFor term (I)\n(I) =\n\u222b \u2202\n\u2202\u03b5 h0 ( a,w, x;P\u03b5hbw )\u2223\u2223 \u03b5=0 dP0 (w, x)\n=\n\u222b \u2202\n\u2202\u03b5 h0 ( a,w, x;P\u03b5hbw )\u2223\u2223 \u03b5=0 p0 (a,w, x) p0 (a | w, x) d\u00b5 (w, x)\n=\n\u222b \u2202\n\u2202\u03b5 h0 ( a,w, x;P\u03b5hbw )\u2223\u2223 \u03b5=0 p0 (a,w, x) \u222b q0 (a, z, x) p 0 (z | a,w, x) d\u00b5zd\u00b5 (w, x)\n= \u222b q0 (a, z, x) \u2202 \u2202\u03b5 h0 ( a,w, x;P\u03b5hbw )\u2223\u2223 \u03b5=0 p0 (w, y | a, z, x) p0 (a, z, x) d\u00b5 (w, x, z, y)\nAnd by Eq. 2, we have\n\u2202\n\u2202\u03b5\n\u222b ( y \u2212 h0 ( a,w, x;P\u03b5hbw )) p\u03b5hbw (y, w | a, z, x) d\u00b5 (y, w) \u2223\u2223\u2223\u2223 \u03b5=0 = 0\nThen\n\u222b \u2202\n\u2202\u03b5 h0 ( a,w, x;P\u03b5hbw )\u2223\u2223 \u03b5=0 p0 (w, y | a, z, x)d\u00b5 (w, y)\n= \u222b [y \u2212 h0 (a,w, x)] \u2202 \u2202\u03b5p \u03b5hbw (w, y, a, z, x) \u2223\u2223 \u03b5=0\np0 (a, z, x) d\u00b5 (w, y)\n\u2212 \u222b [y \u2212 h0 (a,w, x)] p0 (w, y, a, z, x) \u2202\u2202\u03b5p \u03b5hbw (a, z, x) \u2223\u2223 \u03b5=0\n(p0)2 (a, z, x) d\u00b5 (w, y)\nwhere we use the equation\n\u2202\n\u2202\u03b5 p\u03b5hbw (w, y | a, z, x) \u2223\u2223\u2223\u2223 \u03b5=0 = \u2202 \u2202\u03b5p \u03b5hbw (w, y, a, z, x) \u2223\u2223 \u03b5=0 p0 (a, z, x) \u2212 p0 (w, y, a, z, x) \u2202\u2202\u03b5p \u03b5hbw (a, z, x) \u2223\u2223 \u03b5=0 (p0)2 (a, z, x)\nSubstituting (I), we obtain\n(I) = \u222b q0 (a, z, x) (y \u2212 h0 (a,w, x)) p0 (a, z, x) \u2202 \u2202\u03b5p \u03b5hbw (w, y, a, z, x) \u2223\u2223 \u03b5=0\np0 (a, z, x) d\u00b5 (w, x, z, y)\n\u2212 \u222b q0 (a, z, x) (y \u2212 h0 (a,w, x)) p0 (a, z, x) p0 (o) \u2202\u2202\u03b5p \u03b5hbw (a, z, x) \u2223\u2223 \u03b5=0\n(p0) 2 (a, z, x)\nd\u00b5 (w, x, z, y)\n= \u222b q0 (a, z, x) (y \u2212 h0 (a,w, x)) p0 (a, z, x)\nphbw (o)\u2212 p0 (o) p0 (a, z, x) d\u00b5 (w, x, z, y)\n\u2212 \u222b q0 (a, z, x) (y \u2212 h0 (a,w, x)) p0 (a, z, x) p (o) ( phbw (a, z, x)\u2212 p0 (a, z, x) ) (p0) 2 (a, z, x) d\u00b5 (w, x, z, y)\nhbw\u21920====== q0 (a, Z,X) (Y \u2212 h0 (a,w, x)) lim hbw\u21920 phbwA (a)\n+ \u222b q0 (a, z, x) (y \u2212 h0 (a,w, x)) p0 (w, y | a, z, x) phbw (a, z, x)d\u00b5 (w, x, z, y)\nhbw\u21920====== q0 (a, Z,X) (Y \u2212 h0 (a,W,X)) lim hbw\u21920 phbwA (a)\nwhere the last line is because of Eq. 2.\nThe corresponding probability density function (pdf) phbwO (o) = Khbw(O \u2212 o)I(p0O(o) > hbw) is our kernel density, and we thus have limhbw\u21920 p hbw O (o) = limhbw\u21920Khbw(O \u2212 o). Combining the two terms, we get\nlim hbw\u21920\n\u2202\n\u2202\u03b5 \u03b2(a;P\u03b5hbw ) \u2223\u2223\u2223\u2223 \u03b5=0 = (Y \u2212 h0(a,W,X)) q0(a, Z,X) lim hbw\u21920 Khbw (A\u2212 a) + h0 (a,W,X)\u2212 \u03b2(a)"
        },
        {
            "heading": "E.3 PROOF OF THEOREM 6.2",
            "text": "To prove Theorem 6.2, we first give some Lemma. Lemma E.1 (Theorem 14.1 in Wainwright (2019)). Given a star-shaped, b-uniformly bounded function class F , let \u03b4n be any positive solution of the inequality Rn(\u03b4;G) \u2264 \u03b42/b. Then for any t \u2265 \u03b4n, we have \u2223\u2223\u2225f\u222522,n \u2212 \u2225f\u222522\u2223\u2223 \u2264 12\u2225f\u222522 + t22 , \u2200f \u2208 F , with probability at least 1 \u2212 c1e\u2212c2nt\n2/b2 . If in addition n\u03b42n \u2265 2 log (4 log (1/\u03b4n)) /c2, then we have that \u2223\u2223\u2225f\u222522,n \u2212 \u2225f\u222522\u2223\u2223 \u2264 c0\u03b4n, \u2200f \u2208 F , with probability at least 1\u2212 c\u20321 exp(\u2212c\u20322n\u03b42n/b2). Lemma E.2 (Lemma 11 in Foster & Syrgkanis (2019)). Consider a function class F , with supf\u2208F \u2225f\u2225\u221e \u2264 b, and pick any f\u22c6 \u2208 F . Also, consider a loss function \u2113 : R \u00d7 Y 7\u2192 R which is L-Lipschitz in its first argument with respect to the l2 norm. Let \u03b42n \u2265 4d log(41 log(2c2n)) c2n\nbe any solution to the inequalities:\nRn (\u03b4; star (F \u2212 f)) \u2264 \u03b42\n\u2225F\u2225\u221e ,\nwhere star (F \u2212 f) = \u03b1 (f \u2212 f\u2217) for \u2200f \u2208 F , \u03b1 \u2208 [0, 1]. Then for any t \u2265 \u03b4n and some universal constants c1, c2 > 0, with probability 1\u2212 c1e\u2212c2nt 2/b2 , it holds that\n|(En [\u2113 (f(x), y)]\u2212 En [\u2113 (f\u22c6(x), y)])\u2212 (E [\u2113 (f(x), y)]\u2212 E [\u2113 (f\u22c6(x), y)])| \u2264 18Ldt {\u2225f \u2212 f\u22c6\u22252 + t} , for any f \u2208 F . If furthermore, the loss function \u2113 is linear in f , i.e., \u2113((f+f \u2032)(x), y) = \u2113(f(x), y)+ \u2113(f \u2032(x), y) and \u2113(\u03b1f(x), y) = \u03b1\u2113(f(x), y), then the lower bound on \u03b42n is not required. If the outcome f\u0302 of constrained ERM satisfies that with the same probability,\nE [ \u2113 ( f\u0302(x), y )] \u2212 E [\u2113 (f\u22c6(x), y)] \u2264 18Ldt {\u2225\u2225\u2225f\u0302 \u2212 f\u22c6\u2225\u2225\u2225 2 + t } .\nLemma E.3. Let some a, b, d \u2265 0 be given, and suppose that\naX2 \u2264 bX + d,\nfor some X \u2265 0. Then, we have\nX \u2264 b+ \u221a ad\na\nProof. Since a, b and d are both positive, the quadratic aX2 \u2212 bX \u2212 d must have a positive and a negative root. Therefore, this quadratic is negative if and only if X is less than the positive root; that is, we have\naX2 \u2212 bX \u2212 d \u2264 0\n\u21d0\u21d2 X \u2264 b+ \u221a b2 + 4ad\n2a\n=\u21d2 X \u2264 b+ \u221a ad\na .\nProposition E.4. Let \u03b4qn respectively be the upper bound on the Rademacher complexity of M. For any \u03b7 \u2208 (0, 1), define \u03b4q := \u03b4qn + c q 0\n\u221a log(cq1/\u03b7)\nn for some constants c q 0, c q 1; then under assump. 6.1,\nwe have with probability 1\u2212 \u03b7 that\nsup m\u2208M\n\u03a6n,\u03bbmq (q0,m; p\u0302) \u2264 1\n\u03bbm \u2225\u2225\u2225\u22251p \u2212 1p\u0302 \u2225\u2225\u2225\u22252 2 + { \u03bbm 2 + c2C21 \u03bbm + cC1 } (\u03b4q)2\nProof. To relate \u2225m\u222522,n and \u2225m\u222522, by Lemma E.1, it holds with probability at least 1\u2212 \u03b7 that\u2223\u2223\u2223\u2225m\u222522,n \u2212 \u2225m\u222522\u2223\u2223\u2223 \u2264 12 \u2225m\u222522 + 12 (\u03b4q)2 , \u2200m \u2208 M, (17) as long as we choose t equal to \u03b4q = \u03b4qn + c0 \u221a log(c1/\u03b7) n , where \u03b4 q n is an upper bound on the critical radius of M. On the other hand, to relate \u03a6n,\u03bbmq (q\u0302,m; p\u0302) = \u03a6nq (q\u0302,m; p\u0302) \u2212 \u03bbm \u2225m\u2225 2 2 to its empirical version, we apply Lemma E.2 to \u2113(a1, a2) := a1a2, a1 = m(A,W,X), a2 = q0(A,Z,X)\u2212 1p\u0302(A|W,X) that is C1-Lipschitz with respect to a1 by noting q0(A,Z,X)\u2212 1 p\u0302(A|W,X) is in [\u2212C1, C1] with some constants C1 = 1/\u2225p\u0302\u2225\u221e + \u2225Q\u2225\u221e:\n|\u2113 (a1, a2)\u2212 \u2113 (a\u20321, a2)| \u2264 C1 |a1 \u2212 a\u20321| .\nTherefore, we have that there exists a positive constant c such that, with probability at least 1\u2212 \u03b7, \u2223\u2223\u03a6nq (q0,m; p\u0302)\u2212 \u03a6q (q0,m; p\u0302)\u2223\u2223 \u2264 cC1 {\u03b4q \u2225m\u22252 + (\u03b4q)2} . \u2200m \u2208 M (18) Thus, we can further deduce that, for some absolute constants c > 0, with probability at least 1\u2212 \u03b7,\nsup m\u2208M \u03a6n,\u03bbmq (q0,m; p\u0302) = sup m\u2208M\n{ \u03a6nq (q0,m; p\u0302)\u2212 \u03bbm \u2225m\u2225 2 2,n } (1) \u2264 sup m\u2208M { \u03a6q (q0,m; p\u0302) + cC1{\u03b4q \u2225m\u22252 + (\u03b4 q) 2} \u2212 \u03bbm \u2225m\u222522,n\n} (2) \u2264 sup m\u2208M { \u03a6q (q0,m; p\u0302) + cC1{\u03b4q \u2225m\u22252 + (\u03b4 q) 2} \u2212 \u03bbm 2 \u2225m\u222522 + \u03bbm 2 (\u03b4q) 2 } ,\nwhere (1) is derived from Eq. 18, and (2) is derived from Eq. 17. We cam further bound the righthand side of the above inequality as\nsup m\u2208M \u03a6n,\u03bbmq (q0,m; p\u0302) \u2264 sup m\u2208M\n{ \u03a6q (q0,m; p\u0302)\u2212\n\u03bbm 4 \u2225m\u222522 } + sup\nm\u2208M\n{ cC1{\u03b4q \u2225m\u22252 + (\u03b4\nq)2} \u2212 \u03bbm 4 \u2225m\u222522 + \u03bbm 2\n(\u03b4q)2 }\n(1) \u2264 sup m\u2208M \u03a6q (q0,m; p) + { \u03bbm 2 + c2C21 \u03bbm + cC1 } (\u03b4q)2\n+ sup m\u2208M\n{ \u03a6q (q0,m; p\u0302)\u2212 \u03a6q (q0,m; p)\u2212\n\u03bbm 4 \u2225m\u222522 } (2) \u2264 sup m\u2208M { E [( 1 p \u2212 1 p\u0302 ) m ] \u2212 \u03bbm 4 \u2225m\u222522 } + { \u03bbm 2 + c2C21 \u03bbm + cC1 } (\u03b4q)2\n(3) \u2264 sup m\u2208M {\u2225\u2225\u2225\u22251p \u2212 1p\u0302 \u2225\u2225\u2225\u2225 2 \u2225m\u22252 \u2212 \u03bbm 4 \u2225m\u222522 } + { \u03bbm 2 + c2C21 \u03bbm + cC1 } (\u03b4q)2\n(1) \u2264 1 \u03bbm \u2225\u2225\u2225\u22251p \u2212 1p\u0302 \u2225\u2225\u2225\u22252 2 + { \u03bbm 2 + c2C21 \u03bbm + cC1 } (\u03b4q)2\nwhere (1) from the fact sup\u2225m\u22252{a\u2225m\u22252 \u2212 b\u2225m\u2225 2 2} \u2264 a2/4b for any b > 0, (2) holds from the fact that \u03a6q (q0,m; p) = 0 and (3) holds from Cauchy\u2019s inequality.\nTheorem 6.2. Let \u03b4qn respectively be the upper bound on the Rademacher complexity of M. For any \u03b7 \u2208 (0, 1), define \u03b4q := \u03b4qn + c q 0\n\u221a log(cq1/\u03b7)\nn for some constants c q 0, c q 1; then under assump. 6.1,\nwe have with probability 1\u2212 \u03b7 that\u2225\u2225projq(q\u0302 \u2212 q0)\u2225\u22252 = O(\u03b4q\u221a\u03bb2m + \u03bbm + 1 + \u2225\u2225\u2225\u22251p \u2212 1p\u0302 \u2225\u2225\u2225\u2225 2 ) , p stands for p(a|w, x).\nProof. First we note that,\nsup m\u2208M \u03a6n,\u03bbmq (q\u0302,m; p\u0302) = sup m\u2208M\n{ \u03a6nq (q\u0302,m; p\u0302)\u2212 \u03a6nq (q0,m; p\u0302) + \u03a6nq (q0,m; p\u0302)\u2212 \u03bbm \u2225m\u2225 2 2,n } \u2265 sup\nm\u2208M\n{ \u03a6nq (q\u0302,m; p\u0302)\u2212 \u03a6nq (q0,m; p\u0302)\u2212 2\u03bbm \u2225m\u2225 2 2,n } \ufe38 \ufe37\ufe37 \ufe38\n(\u22c6)\n+ inf m\u2208M\n{ \u03a6nq (q0,m; p\u0302) + \u03bbm \u2225m\u2225 2 2,n } By the symmetry of M, we have\ninf m\u2208M\n{ \u03a6nq (q0,m; p\u0302) + \u03bbm \u2225m\u2225 2 2,n } = inf\n\u2212m\u2208M\n{ \u03a6nq (q0,\u2212m; p\u0302) + \u03bbm \u2225m\u2225 2 2,n } = inf\n\u2212m\u2208M\n{ \u2212\u03a6nq (q0,m; p\u0302) + \u03bbm \u2225m\u2225 2 2,n } = \u2212 sup\n\u2212m\u2208M\n{ \u03a6nq (q0,m; p\u0302)\u2212 \u03bbm \u2225m\u2225 2 2,n } = \u2212 sup\n\u2212m\u2208M \u03a6n,\u03bbmq (q0,m; p\u0302)\n= \u2212 sup m\u2208M \u03a6n,\u03bbmq (q0,m; p\u0302)\nIn the sequel, we upper and lower bound term (\u22c6) respectively.\n(i). Upper bound of term (\u22c6).\nBy definition of the estimator q\u0302 and the assumption q0 \u2208 Q, we have\nsup m\u2208M \u03a6n,\u03bbmq (q\u0302,m; p\u0302) \u2264 sup m\u2208M \u03a6n,\u03bbmq (q0,m; p\u0302) .\nWe have\n(\u22c6) \u2264 sup m\u2208M \u03a6n,\u03bbmq (q\u0302,m; p\u0302) + sup m\u2208M \u03a6n,\u03bbmq (q0,m; p\u0302)\n\u2264 2 sup m\u2208M \u03a6n,\u03bbmq (q0,m; p\u0302) \u2264 2\n( 1\n\u03bbm \u2225\u2225\u2225\u22251p \u2212 1p\u0302 \u2225\u2225\u2225\u22252 2 + { \u03bbm 2 + c2C21 \u03bbm + cC1 } (\u03b4q)2 )\n(ii). Lower bound of term (\u22c6).\nWe now invoke Lemma E.2 with \u2113(a1, a2), a1 = m and a2 = q\u2212q0 that isC2-Lipschitz with respect to a1 by noting q(A,Z,X)\u2212 q0(A,Z,X) is in [\u2212C2, C2] with some constants C2 = 2\u2225Q\u2225\u221e:\n|\u2113(a1, a2)\u2212 \u2113(a\u20321, a2)| \u2264 C2|a1 \u2212 a\u20321|.\nTherefore, we have that there exists a positive constant c such that, with probability at least 1\u2212 \u03b7,\u2223\u2223{\u03a6nq (q,m; p\u0302)\u2212 \u03a6nq (q0,m; p\u0302)}\u2212 {\u03a6q (q,m; p\u0302)\u2212 \u03a6q (q0,m; p\u0302)}\u2223\u2223 \u2264 cC2{\u03b4q \u2225m\u22252 + (\u03b4q)2} (19) Now we are ready to prove the lower bound on term (\u22c6). Since mq := 12\u03bbm projq (q \u2212 q0) \u2208 M, and Star-shaped, we have mq2 \u2208 M\n(\u22c6) = sup m\u2208M\n{ \u03a6nq (q\u0302,m; p\u0302)\u2212 \u03a6nq (q0,m; p\u0302)\u2212 2\u03bbm \u2225m\u2225 2 2,n } \u2265 \u03a6nq ( q\u0302, mq 2 ; p\u0302 ) \u2212 \u03a6nq ( q0, mq 2 ; p\u0302 ) \u2212 \u03bbm 2 \u2225mq\u222522,n\n\u2265\u03a6q ( q\u0302, mq 2 ; p\u0302 ) \u2212 \u03a6q ( q0, mq 2 ; p\u0302 )\n\ufe38 \ufe37\ufe37 \ufe38 (\u22c4)\n\u2212cC2 { \u03b4q \u2225\u2225\u2225mq\n2 \u2225\u2225\u2225 2 + (\u03b4q) 2 } \u2212 \u03bbm 2 ( 3 2 \u2225mq\u222522 + (\u03b4q) 2 2 )\nwhere the last line holds from the Eq. 17 and 19. For the term (\u22c4), we have\n(\u22c4) = \u03a6q ( q\u0302, mq 2 ; p ) +\u03a6q ( q\u0302, mq 2 ; p\u0302 ) \u2212 \u03a6q ( q\u0302, mq 2 ; p ) \u2212 \u03a6q ( q0, mq 2 ; p )\n+\u03a6q ( q0,\nmq 2\n; p ) \u2212 \u03a6q ( q0,\nmq 2\n; p\u0302 )\n= \u03a6q ( q\u0302, mq 2 ; p ) + E [( 1 p \u2212 1 p\u0302 ) mq 2 ] + E [( 1 p\u0302 \u2212 1 p ) mq 2 ] = \u03a6q ( q\u0302, mq 2 ; p )\nwhere we used \u03a6q ( q0, mq 2 ; p ) = 0. Moreover, since we have\nE [ q\u0302 (A,Z,X)\u2212 1\np (A |W,X) | A,W,X\n] = E [q\u0302 (A,Z,X)\u2212 q0 (A,Z,X) | A,W,X]\n= projq (q\u0302 \u2212 q0)\nand mq = 12\u03bbm projq (q\u0302 \u2212 q0), we have\n\u03a6q ( q\u0302, mq 2 ; p ) = 1 2 E [( q (Z,A,X)\u2212 1 p (A |W,X) ) mq (A,W,X) ] = 1 2 E [ mq (A,W,X)E [ q (A,Z,X)\u2212 1 p (A |W,X) | A,W,X\n]] = 1\n4\u03bbm E [( projq (q\u0302 \u2212 q0) )2] = \u03bbm \u2225mq\u222522\nTherefore, we obtain the lower bound of (\u22c6):\n(\u22c6) \u2265 \u03bbm \u2225mq\u222522 \u2212 cC2 { \u03b4q \u2225\u2225\u2225mq\n2 \u2225\u2225\u2225 2 + (\u03b4q) 2 } \u2212 \u03bbm 2 ( 3 2 \u2225mq\u222522 + (\u03b4q) 2 2 )\n(iii). Combining upper bound and lower bound of term (\u22c6).\nNow we are ready to combine the upper bound and lower bound of (\u22c6).\n\u03bbm \u2225mq\u222522 \u2212 cC2 { \u03b4q \u2225\u2225\u2225mq\n2 \u2225\u2225\u2225 2 + (\u03b4q) 2 } \u2212 \u03bbm 2 ( 3 2 \u2225mq\u222522 + (\u03b4q) 2 2 )\n\u2264 2\n( 1\n\u03bbm \u2225\u2225\u2225\u22251p \u2212 1p\u0302 \u2225\u2225\u2225\u22252 2 + { \u03bbm 2 + c2C21 \u03bbm + cC1 } (\u03b4q)2 )\nThis give a quadratic inequality on \u2225mq\u22252, i.e.\n\u03bbm \u2225mq\u222522 \u2212 2cC2\u03b4 q\ufe38 \ufe37\ufe37 \ufe38\n(B)\n\u2225mq\u22252 \u2212\n( 8\n\u03bbm \u2225\u2225\u2225\u22251p \u2212 1p\u0302 \u2225\u2225\u2225\u22252 2 + { 5\u03bbm + 8c2C21 \u03bbm + 8cC1 + 4cC2 } (\u03b4q)2 ) \ufe38 \ufe37\ufe37 \ufe38\n(C)\n\u2264 0\nBy Lemma E.3, we have that\n\u2225mq\u22252 \u2264 B +\n\u221a B2 + 4\u03bbmC\n2\u03bbm \u2264 1 \u03bbm\n( B + \u221a \u03bbmC ) Applying the definition of A and B, we conclude that, with probability at least 1\u2212 \u03b7,\n\u2225mq\u22252 \u2264 2cC2\u03b4\nq\n\u03bbm +\n1\n\u03bbm \u221a\u221a\u221a\u221a\u03bbm( 8 \u03bbm \u2225\u2225\u2225\u22251p \u2212 1p\u0302 \u2225\u2225\u2225\u22252 2 + { 5\u03bbm + 8c2C21 \u03bbm + 8cC1 + 4cC2 } (\u03b4q)2 )\n\u2264 ( 2cC2 \u03bbm + \u221a 5 + 8cC1 + 4cC2 \u03bbm + 8c2C21 \u03bb2m ) \u03b4q + 2 \u221a 2 \u03bbm \u2225\u2225\u2225\u22251p \u2212 1p\u0302 \u2225\u2225\u2225\u2225 2\n\u2264 \u221a 2 ( 5 +\n8cC1 + 4cC2 \u03bbm + 4c2 (2C21 + C 2 2 ) \u03bb2m\n) \u03b4q + 2 \u221a 2\n\u03bbm \u2225\u2225\u2225\u22251p \u2212 1p\u0302 \u2225\u2225\u2225\u2225 2\n\u2272 \u221a 1 + 1\n\u03bbm +\n1\n\u03bb2m \u03b4q +\n1\n\u03bbm \u2225\u2225\u2225\u22251p \u2212 1p\u0302 \u2225\u2225\u2225\u2225 2\nwhere the third line holds from the \u221a a+ \u221a b \u2264 \u221a 2(a+ b). According Eq. 11, we have\u221a\nLq (q; p) = \u221a E [( projq (q\u0302 \u2212 q0) )2] = \u221a 4\u03bb2mE [ (mq) 2 ] = 2\u03bbm \u2225mq\u22252 ,\nwe can bound the Projected RMSE by\u2225\u2225projq(q\u0302 \u2212 q0)\u2225\u22252 =\u221aLq (q; p) = 2\u03bbm \u2225mq\u22252 \u2272 \u03b4q\u221a\u03bb2m + \u03bbm + 1 + \u2225\u2225\u2225\u22251p \u2212 1p\u0302 \u2225\u2225\u2225\u2225 2\nFor bridge function h, we also the similar theorem. We first give some assumption. Assumption E.5. (1) (Boundness) \u2225G\u2225\u221e <\u221e and y is uniformly bounded; (2) (Symmetric) G is a symmetric class, i.e, if g \u2208 G, then \u2212g \u2208 G; (3) (Star-shaped) G is star-shaped class, that is for each function g in the class, \u03b1g for any \u03b1 \u2208 [0, 1] also belongs to the class; (4) (Realizability) h0 \u2208 H; (5) (Closedness) 12\u03bbg projh(h\u2212 h0) \u2208 H.\nTheorem E.6. Let \u03b4hn respectively be the upper bound on the Rademacher complexity of G. For any \u03b7 \u2208 (0, 1), define \u03b4h := \u03b4hn + ch0 \u221a log(ch1 /\u03b7) n for some constants c h 0 , c h 1 ; then under assump. E.5, we have with probability 1\u2212 \u03b7 that\u2225\u2225\u2225projh(h\u0302\u2212 h0)\u2225\u2225\u2225 2 = O ( \u03b4h \u221a \u03bb2g + \u03bbg + 1\n) The proof of the Thm. E.6 is detailed in Kallus et al. (2021)."
        },
        {
            "heading": "E.4 PROOF OF THEOREM 6.4",
            "text": "We prove the bias and variance of the PKDR estimator given Eq. 7 respectively. The proof method comes from Colangelo & Lee (2020); Kallus & Uehara (2020).\nTheorem E.7. Under assump. 3.1-3.4 and 4.1, suppose \u2225h\u0302 \u2212 h\u22252 = o(1), \u2225q\u0302 \u2212 q\u22252 = o(1) and \u2225h\u0302 \u2212 h\u22252\u2225q\u0302 \u2212 q\u22252 = o((nhbw)\u22121/2), nh5bw = O(1), nhbw \u2192 \u221e, h0(a,w, x), p(a, z|w, x) and p(a,w|z, x) are twice continuously differentiable wrt a, as well as h0, q0, h\u0302, q\u0302 are uniformly bounded. Then for any a, we have the following for the bias of the PKDR estimator given Eq. 7:\nE [ \u03b2\u0302 (a) ] \u2212 \u03b2(a) = h 2 bw\n2 \u03ba2(K)B + o((nhbw)\n\u22121/2),\nwhere B = E[q0(a, Z,X)[2 \u2202\u2202Ah0(a,W,X) \u2202 \u2202Ap(a,W | Z,X) +\n\u22022\n\u2202A2h0(a,W,X)]].\nProof. We calculate the expectation of the estimator for a single data point. For simplicity, we treat this data point as a random variable. We have\nE [ \u03b2\u0302 (a)\u2212 \u03b2(a) ] = E [ \u03b2\u0302 (a) ] \u2212 \u03b2(a)\n= E [ Khbw (A\u2212 a) {( Y \u2212 h\u0302 (a,W,X) ) q\u0302 (a, Z,X) }] \ufe38 \ufe37\ufe37 \ufe38 (I) \u2212 E [Khbw (A\u2212 a) {(Y \u2212 h0 (a,W,X)) q0 (a, Z,X)}]\ufe38 \ufe37\ufe37 \ufe38\n(II) + E [ h\u0302 (a,W,X)\u2212 h0 (a,W,X) ] \ufe38 \ufe37\ufe37 \ufe38 (III) + E [Khbw (A\u2212 a) (Y \u2212 h0 (a,W,X)) q0 (a, Z,X)]\ufe38 \ufe37\ufe37 \ufe38\n(IV)\nFor the (IV) term, we first have\nE [Khbw (A\u2212 a) q0 (a, Z,X) (Y \u2212 h0 (a,W,X))] =E [Khbw (A\u2212 a) q0 (a, Z,X)E [(Y \u2212 h0 (a,W,X)) | A,Z,X]] =E [Khbw (A\u2212 a) q0 (a, Z,X)E [(h0 (A,W,X)\u2212 h0 (a,W,X)) | A,Z,X]] =E [q0 (a, Z,X)E [Khbw (A\u2212 a) (h0 (A,W,X)\u2212 h0 (a,W,X)) | Z,X]]\n=E [ q0 (a, Z,X) \u222b Khbw (a \u2032 \u2212 a) (h0 (a\u2032, w, x)\u2212 h0 (a,w, x)) p (a\u2032, w | z, x) d\u00b5 (a\u2032, w) ]\n=E [ q0 (a, Z,X) \u222b K (u) (h0 (a+ hbwu,w, x)\u2212 h0 (a,w, x)) p (a+ hbwu,w | z, x) d\u00b5 (u,w) ]\nwhere the last line holds from a\u2032 = hbwu + a. Consider Taylor expansion of h0 (a,w, x) and p (a,w | z, x) around A = a:\np (hbwu+ a,w | z, x)\u2212 p (a,w | z, x) = hbwu \u2202\n\u2202A p (a,w | z, x) +O\n( h2bw ) h0 (a+ hbwu,w, x)\u2212 h0 (a,w, x) = hbwu ( \u2202\n\u2202A h0 (a,w, x) ) + (hbwu) 2\n2\n( \u22022\n\u2202A2 h0 (a,w, x)\n) +O ( h3bw ) Then, we can compute the conditional expectation by integrating the approximation to the density term by term. Here, \u03baj(K) represents the jth kernel moment, defined as \u03baj(K) = \u222b ujK(u)du. It\u2019s important to note that for a symmetric kernel, the odd-order moments integrate to 0. Therefore, we have E [Khbw (A\u2212 a) q0 (a, Z,X) (Y \u2212 h0 (a,W,X))]\n=E [ q0 (a, Z,X) \u222b K (u) (h0 (a+ hbwu,w, x)\u2212 h0 (a,w, x)) p (a+ hbwu,w | z, x) d\u00b5 (u,w) ] =h2bw\u03ba2(K)E [ q0 (a, Z,X) [ \u2202\n\u2202A h0 (a,W,X)\n\u2202 \u2202A p (a,W | Z,X) + 1 2\n( \u22022\n\u2202A2 h0 (a,W,X) )]] + o ( h2bw\n) For the (I)-(III) term, we have\n(I) \u2212 (II) + (III) =E [ Khbw (A\u2212 a) (q\u0302 (a, Z,X)\u2212 q0 (a, Z,X)) ( h0 (a,W,X)\u2212 h\u0302 (a,W,X) )] (20)\n+E [Khbw (A\u2212 a) (q\u0302 (a, Z,X)\u2212 q0 (a, Z,X)) (Y \u2212 h0 (a,W,X))] (21) +E [ Khbw (A\u2212 a) q0 (a, Z,X) ( h0 (a,W,X)\u2212 h\u0302 (a,W,X) ) \u2212 ( h0 (a,W,X)\u2212 h\u0302 (a,W,X) )] .\n(22)\nWe will explain in turn that the above three convergence rates are o((nhbw)\u22121/2), o(1) \u00d7 O(h2bw) and o(1)\u00d7O(h2bw) respectively. From now on, we prove Eq. 21 is o(1)\u00d7O(h2bw).\nE [Khbw (A\u2212 a) (q\u0302 (a, Z,X)\u2212 q0 (a, Z,X)) (Y \u2212 h0 (a,W,X))] =E [Khbw (A\u2212 a) (q\u0302 (a, Z,X)\u2212 q0 (a, Z,X))E [(Y \u2212 h0 (a,W,X)) | A,Z,X]] (1) =E [Khbw (A\u2212 a) (q\u0302 (a, Z,X)\u2212 q0 (a, Z,X))E [(h0 (A,W,X)\u2212 h0 (a,W,X)) | A,Z,X]] =E [Khbw (A\u2212 a) (q\u0302 (a, Z,X)\u2212 q0 (a, Z,X)) (h0 (A,W,X)\u2212 h0 (a,W,X))] =E [(q\u0302 (a, Z,X)\u2212 q0 (a, Z,X))E [Khbw (A\u2212 a) (h0 (A,W,X)\u2212 h0 (a,W,X)) | Z,X]] (2) =E [ (q\u0302 (a, Z,X)\u2212 q0 (a, Z,X)) { O(h2bw)\n}] (3) =o(1)\u00d7O(h2bw),\nwhere (1) is derived from Eq. 2, (3) is derived from assumption \u2225q\u0302 \u2212 q\u22252 = o(1) and (2) is because\nE [Khbw (A\u2212 a) (h0 (A,W,X)\u2212 h0 (a,W,X)) | Z,X]\n= \u222b Khbw (a \u2032 \u2212 a) (h0 (a\u2032, w, x)\u2212 h0 (a,w, x)) p (a\u2032, w | z, x) d\u00b5 (a\u2032, w)\n= \u222b K (u) (h0 (a+ hbwu,w, x)\u2212 h0 (a,w, x)) p (a+ hbwu,w | z, x) d\u00b5 (u,w)\n= \u222b K (u) ( hbwu \u2202\n\u2202A h0 (a,w, x) +O\n( h2bw )) (p (a,w | z, x) +O (hbwu)) d\u00b5 (u,w)\n=O ( h2bw ) .\nNext, we prove Eq. 22 is o(1)\u00d7O(h2bw). E[Khbw (A\u2212 a) q0 (a, Z,X) (h0 (a,W,X)\u2212 h\u0302 (a,W,X))\u2212 (h0 (a,W,X)\u2212 h\u0302 (a,W,X))] = E[(h0 (a,W,X)\u2212 h\u0302 (a,W,X))E [Khbw (A\u2212 a) q0 (a, Z,X)\u2212 1 |W,X]].\nWe consider E [Khbw (A\u2212 a) q0 (a, Z,X)\u2212 1 |W,X]\n= \u222b (Khbw (a \u2032 \u2212 a) q0 (a, z, x)\u2212 1) p (a\u2032, z | w, x) d\u00b5 (a\u2032, z)\n= \u222b Khbw (a \u2032 \u2212 a) q0 (a, z, x) p (a\u2032, z | w, x) d\u00b5 (a\u2032, z)\u2212 1\n= \u222b K (u) q0 (a, z, x) p (a+ hbwu, z | w, x) d\u00b5 (u, z)\u2212 1\n= \u222b K (u) q0 (a, z, x) ( p (a, z | w, x) + hbwu \u2202\n\u2202A h0 (a,w, x) +O\n( h2bw )) d\u00b5 (u, z)\u2212 1\n= \u222b q0 (a, z, x) p (a, z | w, x) d\u00b5 (z)\u2212 1 +O ( h2bw ) ,\nwhere we use the first-order Taylor expansion of p (a, z | w, x):\np (hbwu+ a, z | w, x) = p (a, z | w, x) + hbwu \u2202\n\u2202A h0 (a,w, x) +O\n( h2bw ) .\nTherefore, we have E [( h0 (a,W,X)\u2212 h\u0302 (a,W,X) )(\u222b q0 (a, z, x) p (a, z | w, x) d\u00b5 (z)\u2212 1 +O ( h2bw ))] (1) =E [( h0 (a,W,X)\u2212 h\u0302 (a,W,X) )(\u222b q0 (a, z, x) p (a, z | w, x) d\u00b5 (z)\u2212 1\n)] + o (1)\u00d7O ( h2bw\n) where (1) is derived from assumption \u2225h\u0302\u2212 h\u22252 = o(1). We assert that the first expression is 0:\nE [( h0 (a,W,X)\u2212 h\u0302 (a,W,X) )(\u222b q0 (a, z, x) p (a, z | w, x) d\u00b5 (z)\u2212 1 )] = \u222b ( h0 (a,w, x)\u2212 h\u0302 (a,w, x) ) q0 (a, z, x) p (a, z, w, x) d\u00b5 (z, w, x)\n\u2212 \u222b ( h0 (a,w, x)\u2212 h\u0302 (a,w, x) ) p (w, x) d\u00b5 (w, x)\n(1) = \u222b h\u0302 (a,w, x) p (w, x) d\u00b5 (w, x)\u2212 \u222b h\u0302 (a,w, x) q0 (a, z, x) p (a, z, w, x) d\u00b5 (z, w, x)\n= \u222b h\u0302 (a,w, x) p (w, x) d\u00b5 (w, x)\u2212 \u222b h\u0302 (a,w, x) p (a,w, x)\np (a | w, x) d\u00b5 (w, x) = 0\nwhere we used the following property for (1)\n\u03b2 (a) = \u222b h0 (a,w, x) p (w, x) d\u00b5 (w, x)\n= \u222b h0 (a,w, x) q0 (a, z, x) p (a, z, w, x) d\u00b5 (z, w, x).\nBy assumption \u2225h\u0302\u2212h\u22252\u2225q\u0302\u2212 q\u22252 = o((nhbw)\u22121/2), we have Eq. 20 is o((nhbw)\u22121/2). Combining these terms we get\n(I) \u2212 (II) + (III) = o((nhbw)\u22121/2) + o(1)\u00d7O(h2bw) + o(1)\u00d7O(h2bw) = o((nhbw)\u22121/2) where we use nh5bw = O(1). Therefore,\nE [ \u03b2\u0302 (a) ] \u2212 \u03b2(a) = h 2 bw\n2 \u03ba2(K)B + o((nhbw)\n\u22121/2),\nwhere B = E[q0(a, Z,X)[2 \u2202\u2202Ah0(a,W,X) \u2202 \u2202Ap(a,W | Z,X) +\n\u22022\n\u2202A2h0(a,W,X)]].\nTheorem E.8. Under assump. 3.1-3.4 and 4.1, suppose \u2225h\u0302 \u2212 h\u22252 = o(1), \u2225q\u0302 \u2212 q\u22252 = o(1) and \u2225h\u0302 \u2212 h\u22252\u2225q\u0302 \u2212 q\u22252 = o((nhbw)\u22121/2), nh5bw = O(1), nhbw \u2192 \u221e, h0(a,w, x), p(a, z|w, x) and p(a,w|z, x) are twice continuously differentiable wrt a as well as h0, q0, h\u0302, q\u0302 are uniformly bounded. Then for any a, we have the following for the variance of the PKDR estimator given Eq. 7:\nVar[\u03b2\u0302(a)] = \u21262(K)\nnhbw (V + o(1)),\nwhere V = E[I(A = a)q0(a, Z,X)2(Y \u2212 h0(a,W,X))2].\nProof. For convenience, we let\nm (o;h, q) = Khbw (A\u2212 a) (Y \u2212 h (a,W,X)) q (a, Z,X) (23) \u03d5 (o;h, q) = m (o;h, q) + h (a,W,X) (24)\nWe first use cross-fitting which allows us to exchange the order of summation and variance. More specifically, we split the data randomly into two halves O1 and O2. Then we have\nE [ En [ \u03d51 ( o; h\u0302, q\u0302 )]2] = E [ E [ En [ \u03d51 ( o; h\u0302, q\u0302 )]2 |O2 ]]\n= n\u22121E [ E [( \u03d51 ( o; h\u0302, q\u0302 )2) |O2 ]]\nWe will omit this step later, please identify it according to the context. According to the definition of variance, we have\nVar ( En [ \u03d5 ( o; h\u0302, q\u0302 )] \u2212 \u03b2 (a) ) \u2264Var (En [\u03d5 (o;h0, q0)]\u2212 \u03b2 (a))\ufe38 \ufe37\ufe37 \ufe38\n(I)\n+Var ( En [ \u03d5 (o;h0, q0)\u2212 \u03d5 ( o; h\u0302, q\u0302 )]) \ufe38 \ufe37\ufe37 \ufe38\n(II)\n+2 \u221a Var (En [\u03d5 (o;h0, q0)]\u2212 \u03b2 (a))Var ( En [ \u03d5 (o;h0, q0)\u2212 \u03d5 ( o; h\u0302, q\u0302 )]) \ufe38 \ufe37\ufe37 \ufe38\n(III)\nWe use cross-fitting and by Eq. 24:\n(I) = 1\nn Var (m (o;h0, q0) + h0 (a,W,X)\u2212 \u03b2 (a)) (25)\nWe first consider Var (m (o;h0, q0)). Since\nVar (m (o;h0, q0)) = 1\nn\n( E [ m (o;h0, q0) 2 ] \u2212 (E [m (o;h0, q0)])2 ) \u2264 1 n E [ m (o;h0, q0) 2 ] ,\nwe only consider the second moment of a term in the estimator\nE [ m (o;h0, q0) 2 ]\n=E [ (Khbw (A\u2212 a) (Y \u2212 h0 (a,W,X)) q0 (a, Z,X)) 2 ]\n=E [ q0 (a, Z,X) 2 E [ Khbw (A\u2212 a) 2 (Y \u2212 h0 (a,W,X))2 | Z,X ]] =E [ q0 (a, Z,X) 2 \u222b 1\nh2bw K ( a\u2032 \u2212 a hbw )2 (y \u2212 h0 (a,w, x))2p (a\u2032, y, w | z, x) d\u00b5 (a\u2032, y, w) ]\n=E [ q0 (a, Z,X) 2 \u222b 1\nhbw K (u)\n2 (y \u2212 h0 (a,w, x))2 p (a+ uhbw, y, w | z, x) d\u00b5 (u, y, w) ] =E [ q0 (a, Z,X) 2 \u222b 1\nhbw K (u)\n2 (y \u2212 h0 (a,w, x))2 (p (a, y, w | z, x) + o (hbw)) d\u00b5 (u, y, w) ] = 1\nhbw {\u21262(K)V + o (hbw)}\n(26) where V = E [ I (A = a) q0 (a, Z,X)2 (Y \u2212 h0 (a,W,X))2 ] Because h0 is bound, we have Var (h0 (a,W,X)\u2212 \u03b2 (a)) is bound. Therefore, substituting the above equation to Eq. 25, we have\n(I) = 1\nn Var (m (o;h0, q0) + h0 (a,W,X)\u2212 \u03b2 (a))\n\u2264 1 n Var (m (o;h0, q0)) + 1 n Var (h0 (a,W,X)\u2212 \u03b2 (a)) + 2\nn\n\u221a Var (m (o;h0, q0))Var (h0 (a,W,X)\u2212 \u03b2 (a))\n\u2264 1 nhbw {\u21262(K)V + o (hbw)}+ C n + 2 n\n{ 1\nnhbw {V + o (hbw)}\n}1/2( C\nn )1/2 \u2248 1 nhbw {\u21262(K)V + o (hbw)}\nFor the term of (II), we have\n(II) =Var ( En [ \u03d5 (o;h0, q0)\u2212 \u03d5 ( o; h\u0302, q\u0302 )]) = 1\nn\n( E [( \u03d5 (o;h0, q0)\u2212 \u03d5 ( o; h\u0302, q\u0302 ))2] \u2212 ( E [ \u03d5 (o;h0, q0)\u2212 \u03d5 ( o; h\u0302, q\u0302 )])2) \u2264 1 n E [( \u03d5 (o;h0, q0)\u2212 \u03d5 ( o; h\u0302, q\u0302 ))2] .\n(27)\nSimilar to Kallus & Uehara (2020), according to the definition of Eq. 24 and decomposition of Eq. 24 (Eq. 20-22), we have\nE [( \u03d5 (o;h0, q0)\u2212 \u03d5 ( o; h\u0302, q\u0302 ))2] (1) =E [ Khbw (A\u2212 a) 2 (q\u0302 (a, Z,X)\u2212 q0 (a, Z,X))2 ( h0 (a,W,X)\u2212 h\u0302 (a,W,X)\n)2] + E [ Khbw (A\u2212 a) 2 (q\u0302 (a, Z,X)\u2212 q0 (a, Z,X))2 (Y \u2212 h0 (a,W,X))2\n] + E [ Khbw (A\u2212 a) 2 q0 (a, Z,X) 2 ( h0 (a,W,X)\u2212 h\u0302 (a,W,X)\n)2] + E [( h\u0302 (a,W,X)\u2212 h0 (a,W,X) )2] +\u2206\n(2) \u2272E [ Khbw (A\u2212 a) 2 (q\u0302 (a, Z,X)\u2212 q0 (a, Z,X))2 ( h0 (a,W,X)\u2212 h\u0302 (a,W,X) )2] + E [ Khbw (A\u2212 a) 2 (q\u0302 (a, Z,X)\u2212 q0 (a, Z,X))2 (Y \u2212 h0 (a,W,X))2\n] + E [ Khbw (A\u2212 a) 2 q0 (a, Z,X) 2 ( h0 (a,W,X)\u2212 h\u0302 (a,W,X)\n)2] + E [( h\u0302 (a,W,X)\u2212 h0 (a,W,X) )2] +O(1)\n(3) \u2272h\u22121bw max { E [\u2225\u2225\u2225h0 (a,W,X)\u2212 h\u0302 (a,W,X)\u2225\u2225\u22252\n2\n] ,E [ \u2225q\u0302 (a, Z,X)\u2212 q0 (a, Z,X)\u222522 ]} +O(1)\n=o(h\u22121bw)\nwhere (1) is the square expansion of Eq. 20-22 and \u2206 is sum of cross terms, (2) is derived from h\u0302, q\u0302, h, q is uniformly bounded, (3) uses the same approach as Eq. 26.\nTherefore, substituting the above equation to Eq. 27, we have (II) = Var ( En [ \u03d5 (o;h0, q0)\u2212 \u03d5 ( o; h\u0302, q\u0302 )]) \u2264 o ( (nhbw) \u22121 )\n(28)\nFor the term of (III), we only need to substitute (I) and (II) to (III)\n(III) = \u221a Var (En [\u03d5 (o;h0, q0)]\u2212 \u03b2 (a))Var ( En [ \u03d5 (o;h0, q0)\u2212 \u03d5 ( o; h\u0302, q\u0302 )]) = { 1\nnhbw {V + o (hbw)}\n}1/2 o ( n\u22121/2h\n\u22121/2 bw ) Therefore, combining the three terms (I), (II) and (III), we get\nVar ( En [ \u03d5 ( o; h\u0302, q\u0302 )] \u2212 \u03b2 (a) ) = 1\nnhbw {\u21262 (k)V + o (hbw)}+ 2\n{ 1\nnhbw {V + o (hbw)}\n}1/2 o ( n\u22121/2h\n\u22121/2 bw\n) + o ( n\u22121h\u22121bw ) = 1\nnhbw {\u21262(k)V + o (1)}\nTheorem 6.4. Under assump. 3.1-3.4 and 4.1, suppose \u2225h\u0302 \u2212 h\u22252 = o(1), \u2225q\u0302 \u2212 q\u22252 = o(1) and \u2225h\u0302 \u2212 h\u22252\u2225q\u0302 \u2212 q\u22252 = o((nhbw)\u22121/2), nh5bw = O(1), nhbw \u2192 \u221e, h0(a,w, x), p(a, z|w, x) and\np(a,w|z, x) are twice continuously differentiable wrt a as well as h0, q0, h\u0302, q\u0302 are uniformly bounded. Then for any a, we have the following for the bias and variance of the PKDR estimator given Eq. 7:\nBias(\u03b2\u0302(a)) := E[\u03b2\u0302(a)]\u2212\u03b2(a) = h 2 bw\n2 \u03ba2(K)B+o((nhbw)\n\u22121/2),Var[\u03b2\u0302(a)] = \u21262(K)\nnhbw (V +o(1)),\nwhere B = E[q0(a, Z,X)[2 \u2202\u2202Ah0(a,W,X) \u2202 \u2202Ap(a,W | Z,X) +\n\u22022\n\u2202A2h0(a,W,X)]], V = E[I(A = a)q0(a, Z,X) 2(Y \u2212 h0(a,W,X))2].\nProof. By Theorem. E.7 and E.8, we completed the proof. If we want to optimize the bias-variance tradeoff of the asymptotic mean squared error, we choose the optimal bandwidth hbw such that neither term dominates the other.\nMSE ( \u03b2\u0302 (a)\u2212 \u03b2 (a) ) = Bias2 +Variance\n= h4bw 4 (\u03ba (K)B) 2 + 1 nhbw \u2126 (K)V + o\n( 1\nnhbw ) Optimizing the leading terms of the asymptotic MSE with respect to the bandwidth hbw:\n\u2202\n\u2202hbw MSE = (\u03ba (K)B)\n2 h3bw \u2212\n\u2126 (K)V\nnh2bw = 0\nTherefore, we can select the optimal bandwidth is hbw = O(n\u22121/5) in terms of the mean squared error (MSE) that converges at the rate of O(n\u22124/5)."
        },
        {
            "heading": "E.5 CONSISTENCY OF THE ESTIMATOR",
            "text": "Theorem E.9. Under assump. 3.1-3.4 and 4.1, suppose h0(a,w, x) and p(a,w|z, x) are twice continuously differentiable wrt a as well as h0, q0, h\u0302, q\u0302 are uniformly bounded. Then, for some universal constants c1 and c2, with probability 1\u2212 \u03b7, the PKDR estimator given Eq. 7 error is bounded by:\u2223\u2223\u2223\u03b2(a)\u2212 \u03b2\u0302(a)\u2223\u2223\u2223 \u2264 \u2225\u2225\u2225I (A = a)(h0 \u2212 h\u0302)\u2225\u2225\u2225\n2 \u2225proj (q\u0302 \u2212 q0)\u22252+c1\n\u221a log (c2/\u03b7)\nn + h2bw 2 \u03ba2(K)R+o\n( h2bw ) where R = E [ q\u0302 (a, Z,X) [ 2 \u2202\u2202Ah0 (a,W,X) \u2202 \u2202Ap (a,W | Z,X) + ( \u22022 \u2202A2h0 (a,W,X) )]] .\nProof. From the relationship between causal effect and nuisance function, we have\u2223\u2223\u2223\u03b2 (a)\u2212 \u03b2\u0302 (a)\u2223\u2223\u2223 = \u2223\u2223\u2223E [I(A = a)q0 (a, Z,X) (Y \u2212 h0 (a,W,X)) + h0 (a,W,X)]\u2212 \u03b2\u0302(a)\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223E [I (A = a) q0 (Y \u2212 h0) + h0]\u2212 E [I (A = a) q\u0302 (Y \u2212 h\u0302)+ h\u0302 ]\u2223\u2223\u2223\ufe38 \ufe37\ufe37 \ufe38\n(I) + \u2223\u2223\u2223E [(I (A = a)\u2212Khbw (A\u2212 a)) q\u0302 (Y \u2212 h\u0302)]\u2223\u2223\u2223\ufe38 \ufe37\ufe37 \ufe38\n(II) + \u2223\u2223\u2223E [Khbw (A\u2212 a) q\u0302 (Y \u2212 h\u0302)+ h\u0302]\u2212 \u03b2\u0302(a)\u2223\u2223\u2223\ufe38 \ufe37\ufe37 \ufe38\n(III)\nwhere h0 = h0 (a,W,X) , q0 = q0 (a, Z,X) , h\u0302 = h\u0302 (a,W,X) and q\u0302 = q\u0302 (a, Z,X).\nWe can bound each term as follows. For the first term, we have (I) = E [ I (A = a) { q0 (Y \u2212 h0)\u2212 q\u0302 ( Y \u2212 h\u0302 )}] \u2212 E [ h0 \u2212 h\u0302 ] = E [I (A = a)Y (q0 \u2212 q\u0302)] + E [ I(A = a) { q\u0302h\u0302\u2212 q0h0 }] \u2212 E [ h0 \u2212 h\u0302\n] (1) = E [I (A = a)h0 (q0 \u2212 q\u0302)] + E [ I(A = a) { q\u0302h\u0302\u2212 q0h0 }] \u2212 E [ I(A = a)q0 ( h0 \u2212 h\u0302\n)] = E [ I (A = a) (q0 \u2212 q\u0302) ( h0 \u2212 h\u0302 )] (2) \u2264 \u2225\u2225\u2225I (A = a)(h0 \u2212 h\u0302)\u2225\u2225\u2225\n2 \u2225proj (q\u0302 \u2212 q0)\u22252\nwhere (2) is derived from Cauchy\u2019s inequality and (1) is derived from\nE [I (A = a)Y (q0 \u2212 q\u0302)] = E [I (A = a) (q0 \u2212 q\u0302)E [Y |A,Z,X]] = E [I (A = a) (q0 \u2212 q\u0302)E [h0 (A,W,X) |A,Z,X]] = E [I (A = a) (q0 \u2212 q\u0302)h0]\nE [ h0 \u2212 h\u0302 ] = \u222b ( h0 \u2212 h\u0302 ) p (a,w, x) p (a|w, x) d\u00b5(w, x)\n= \u222b ( h0 \u2212 h\u0302 ) p (a,w, x)E [q0 (a, Z,X) |A,W,X] d\u00b5(w, x)\n= E [ I (A = a) ( h0 \u2212 h\u0302 ) q0 ] For the second term, we have\nE [ Khbw (A\u2212 a) q\u0302 (a, Z,X) ( Y \u2212 h\u0302 (a,W,X) )] =E [ Khbw (A\u2212 a) q\u0302 (a, Z,X)E [( Y \u2212 h\u0302 (a,W,X) ) |A,Z,X\n]] =E [ Khbw (A\u2212 a) q\u0302 (a, Z,X) ( h0 (A,W,X)\u2212 h\u0302 (a,W,X)\n)] =E [ q\u0302 (a, Z,X)E [ Khbw (A\u2212 a) ( h0 (A,W,X)\u2212 h\u0302 (a,W,X) ) |Z,X\n]] =E [ q\u0302 (a, Z,X) \u222b K (a\u2032 \u2212 a) ( h0 (a \u2032, w, x)\u2212 h\u0302 (a,w, x) ) p (w, a\u2032|z, x) d\u00b5 (w, a\u2032) ]\n=E q\u0302 (a, Z,X)\u222b K (u)(h0 (a+ hbwu,w, x)\u2212 h\u0302 (a,w, x)) p (w, a+ hbwu|z, x)\ufe38 \ufe37\ufe37 \ufe38 (\u22c6) d\u00b5 (w, u)  where the last line holds from a\u2032 = hbwu + a. Consider Taylor expansion of h0 (a,w, x) and p (a,w | z, x) around A = a:\np (hbwu+ a,w | z, x)\u2212 p (a,w | z, x) = hbwu \u2202\n\u2202A p (a,w | z, x) +O\n( h2bw ) h0 (a+ hbwu,w, x)\u2212 h0 (a,w, x) = hbwu ( \u2202\n\u2202A h0 (a,w, x) ) + (hbwu) 2\n2\n( \u22022\n\u2202A2 h0 (a,w, x)\n) +O ( h3bw ) Therefore, we have\n(\u22c6) = ( h0 (a,w, x)\u2212 h\u0302 (a,w, x) ) p (a,w | z, x)\n+ ( h0 (a,w, x)\u2212 h\u0302 (a,w, x) ) hbwu \u2202\n\u2202A p (a,w | z, x)\n+ hbwu\n( \u2202\n\u2202A h0 (a,w, x)\n) p (a,w | z, x)\n+ (hbwu)\n2\n2\n( \u22022\n\u2202A2 h0 (a,w, x)\n) p (a,w | z, x)\n+ hbwu\n( \u2202\n\u2202A h0 (a,w, x)\n) hbwu \u2202\n\u2202A p (a,w | z, x)\n+ (hbwu)\n2\n2\n( \u22022\n\u2202A2 h0 (a,w, x)\n) hbwu \u2202\n\u2202A p (a,w | z, x) +O\n( h3bw ) Then, we can compute the conditional expectation by integrating the approximation to the density term by term. Here, \u03baj(K) represents the jth kernel moment, defined as \u03baj(K) = \u222b ujK(u)du. It\u2019s\nimportant to note that for a symmetric kernel, the odd-order moments integrate to 0. Therefore, we have\nE [ Khbw (A\u2212 a) q\u0302 (a, Z,X) ( Y \u2212 h\u0302 (a,W,X) )] =E [ q\u0302 (a, Z,X) \u222b K (u) ( h0 (a+ hbwu,w, x)\u2212 h\u0302 (a,w, x) ) p (a+ hbwu,w | z, x) d\u00b5 (u,w)\n] =h2bw\u03ba2(K)E [ q\u0302 (a, Z,X) [ \u2202\n\u2202A h0 (a,W,X)\n\u2202 \u2202A p (a,W | Z,X) + 1 2\n( \u22022\n\u2202A2 h0 (a,W,X) )]] + E [ I (A = a) q\u0302 (a, Z,X) ( Y \u2212 h\u0302 (a,W,X) )] + o ( h2bw\n) Therefore, we obtain\n(II) = \u2223\u2223\u2223E [(I (A = a)\u2212Khbw (A\u2212 a)) q\u0302 (Y \u2212 h\u0302)]\u2223\u2223\u2223\n= h2bw 2 \u03ba2(K)R+ o\n( h2bw ) where R = E [ q\u0302 (a, Z,X) [ 2 \u2202\u2202Ah0 (a,W,X) \u2202 \u2202Ap (a,W | Z,X) + ( \u22022 \u2202A2h0 (a,W,X) )]] .\nThe third terms are upper-bounded by Bernstein inequality. This concludes\u2223\u2223\u2223\u03b2(a)\u2212 \u03b2\u0302(a)\u2223\u2223\u2223 \u2264 \u2225\u2225\u2225I (A = a)(h0 \u2212 h\u0302)\u2225\u2225\u2225 2 \u2225proj (q\u0302 \u2212 q0)\u22252+c1 \u221a log (c2/\u03b7) n + h2bw 2 \u03ba2(K)R+o ( h2bw )\nRemark E.10. According to Thm. 6.2 and E.6, we have \u2225h\u0302 \u2212 h0\u22252 = O(n\u22121/4) and \u2225q\u0302 \u2212 q0\u22252 = O(n\u22121/4). Therefore the order of the estimation error is controlled by hbw. From Thm. 6.4, we know that the optimal bandwidth is hbw = O(n\u22121/5) in terms of estimator error that converges at the rate of O(n\u22122/5). Note that this rate is slower than the optimal rate O(n\u22121/2), which is a reasonable sacrifice to handle continuous treatment within the proximal causal framework and agrees with existing studies (Kennedy et al., 2017; Colangelo & Lee, 2020)."
        },
        {
            "heading": "F COMPUTATION DETAILS",
            "text": "We can consider the nuisance/bridge function class Q or H and the dual/critic functional class M or G are the RKHS class. The inner maximization in Eq. 8 and 9 may no longer have closedform solutions with the RKHS norm constraints. Similar to Dikkala et al. (2020), we consider the following optimization problem\nmin q\u2208Q max m\u2208M\n1\nn \u2211 i ( q(ai, zi, xi)\u2212\n1\np(ai|wi, xi)\n) m(ai, wi, xi)\u2212 \u03bbm \u2225m\u222522,n \u2212 \u03b3m \u2225m\u2225 2 M + \u03b3q \u2225q\u2225 2 Q\nmin h\u2208H max g\u2208G\n1\nn \u2211 i (yi \u2212 h (wi, ai, xi)) g (ai, zi, xi)\u2212 \u03bbg \u2225g\u222522,n \u2212 \u03b3g \u2225g\u2225 2 M + \u03b3h \u2225h\u2225 2 Q\nProposition F.1. Suppose M and G are RKHS spaces with kernel KM and KG equipped with the canonical RKHS norm, then for any q, h, we have\nmax m\u2208M\n\u03a6nq \u2212 \u03bbm \u2225m\u2225 2 2,n \u2212 \u03b3m \u2225m\u2225 2 M =\n1\n4\u03b3m \u03c8\u22a4q,nKM,n ( \u03bbm \u03b3m 1 n KM,n + \u03b3mI )\u22121 \u03c8q,n\n= 1\n4\u03b3m \u03c8\u22a4nK 1/2 M,n ( \u03bbm \u03b3m 1 n KM,n + \u03b3mI )\u22121 K 1/2 M,n\u03c8n\nmax g\u2208G\n\u03a6nh \u2212 \u03bbg \u2225g\u2225 2 2,n \u2212 \u03b3g \u2225g\u2225 2 G =\n1\n4\u03b3g \u03c8\u22a4h,nKG,n ( \u03bbg \u03b3g 1 n KG,n + \u03b3gI )\u22121 \u03c8h,n\n= 1\n4\u03b3g \u03c8\u22a4h,nK 1/2 G,n ( \u03bbg \u03b3g 1 n KG,n + \u03b3gI )\u22121 K 1/2 G,n\u03c8 \u22a4 h,n\nwhere KM,n = (KM(ai, wi, xi, aj , wj , xj))ni,j=1,KG,n = (KG(ai, zi, xi, aj , zj , xj)) n i,j=1 the empirical kernel matrix and \u03c8q,n = ( 1n (q(ai, zi, xi) \u2212 1 p(ai|wi,xi) )) n i=1, \u03c8h,n =(\n1 n (yi \u2212 h (ai, wi, xi)) )n i=1 .\nProof. By the generalized representer theorem of Scho\u0308lkopf et al. (2001), implies that an optimal solution of the constrained problem takes the form\nm(a,w, x) = n\u2211 i=1 \u03b1iKM(ai, wi, xi, a, w, x).\nWe denote KM,n = (KM(ai, wi, xi, aj , wj , xj))ni,j=1 the empirical kernel matrix. And we have \u2225m\u22252M = \u03b1\u22a4KM,n\u03b1, f(zi) = e\u22a4i KM,n\u03b1 and \u2225m\u2225 2 2,n = 1 n\u03b1\n\u22a4K2M,n\u03b1. Thus the penalized problem is equivalent to the finite dimensional maximization problem:\nmax \u03b1\u2208Rn \u03c8\u22a4q,nKM,n\u03b1\u2212 \u03b1\u22a4 ( \u03bbm n KM,n + \u03b3mI ) KM,n\u03b1,\nwhere \u03c8q,n = ( 1 n ( q(ai, zi, xi)\u2212 1p(ai|wi,xi) ))n i=1\n. By taking the first order condition, the latter has a closed form optimizer of:\n\u03b1 = 1\n2\u03b3m ( \u03bbm \u03b3m 1 n KM,n + I )\u22121 \u03c8q,n\nand optimal value of:\n1\n4\u03b3m \u03c8\u22a4nKM,n ( \u03bbm \u03b3m 1 n KM,n + I )\u22121 \u03c8q,n = 1 4\u03b3m \u03c8\u22a4nK 1/2 M,n ( \u03bbm \u03b3m 1 n KM,n + I )\u22121 K 1/2 M,n\u03c8q,n\nSimilarly, we have\nmax g\u2208g\n\u03a6nh \u2212 \u03bbg \u2225g\u2225 2 2,n \u2212 \u03b3g \u2225g\u2225 2 G =\n1\n4\u03b3g \u03c8\u22a4h,nKG,n ( \u03bbg \u03b3g 1 n KG,n + I )\u22121 \u03c8h,n\n= 1\n4\u03b3g \u03c8\u22a4h,nK 1/2 G,n ( \u03bbg \u03b3g 1 n KG,n + I )\u22121 K 1/2 G,n\u03c8 \u22a4 h,n\nwhere \u03c8h,n = ( 1 n (yi \u2212 h (ai, wi, xi)) )n i=1 .\nIf further Q and H are RKHS, we can obtain the closed form solution about the outer maximization, by solving\nq\u0302 = argmin q\u2208Q\n\u03c8\u22a4q,nKM,n ( \u03bbm \u03b3m 1 n KM,n + I )\u22121 \u03c8q,n + 4\u03b3m\u03b3q \u2225q\u22252Q\nh\u0302 = argmin h\u2208H\n\u03c8\u22a4h,nKG,n ( \u03bbg \u03b3g KG,n + I )\u22121 \u03c8h,n + 4\u03b3g\u03b3h \u2225h\u22252H\nAgain by the representation theorem, we have h\u0302 (\u00b7) = \u2211 i \u03b1\u0302ik ((ai, wi, xi) , \u00b7) , q\u0302 (\u00b7) = \u2211 i \u03b2\u0302ik ((ai, zi, xi) , \u00b7)\nwhere \u03b1\u0302 = (KH,nGhKH,n + 4\u03b3h\u03b3gKH,n) \u22121 KH,nGhyi\n\u03b2\u0302 = (KQ,nMqKQ,n + 4\u03b3q\u03b3mKQ,n) \u22121 KQ,nMq\n1\np(ai|wi, xi)\nfor Gh = K 1/2 G,n ( \u03bbg \u03b3g 1 nKG,n + I )\u22121 K 1/2 G,n and Mq = K 1/2 M,n ( \u03bbm \u03b3m 1 nKM,n + I )\u22121 K 1/2 M,n.\nThere are several tuning parameters in the estimation of h0 and q0. We accept the tricks and recommendation defaults by Dikkala et al. (2020). The following parameters will be used to determine.\n\u03bbg \u03b3g (n) = 5 n0.4 (29)\n\u03b3h\u03b3g(s, n) = s\n2 ( \u03bbg \u03b3g (n) )4 (30)\nFor \u03bbm\u03b3m and \u03b3q\u03b3m, we also choose parameters like this."
        },
        {
            "heading": "G ADDITIONAL EXPERIMENTS",
            "text": "In this section, we consider three more synthetic settings introduced in Hu et al. (2023), as well as the times-series setting introduced in Miao et al. (2018b) that satisfies the proximal causality framework. Similar to Tab. 1, our methods are comparable or better than others.\nImplementation Details. In the PKIPW and PKDR estimators, we choose the second-order Epanechnikov kernel, with bandwidth hbw = c\u03c3\u0302An\u22121/5 with estimated std \u03c3\u0302A and the hyperparameter c = 1.5. For policy estimation, we employ the KDE in the two datasets. The rest of the implementation details are consistent with the experiments in the text.\nEvaluation metrics. We report the causal Mean Squared Error (cMSE) across 100 equally spaced points in the range of supp(A): cMSE := 1100 \u2211100 i=1(E[Y ai ] \u2212 E\u0302[Y ai ])2. Here, we respectively take supp(A) := [5.5, 7], [4, 5.5], [5.5, 7], [\u22122, 2] in three synthetic data and the times series data. The truth E[Y a] is derived through Monte Carlo simulations comprising 10,000 replicates of data generation for each a."
        },
        {
            "heading": "G.1 EXPERIMENTS WITH DIFFERENT DATA GENERATING PROCESS",
            "text": "Data generation. We consider three different data-generating mechanisms (Hu et al., 2023). Under each of scenarios, we simulate the continuous unmeasured confounder U following a normal distribution with mean 1 and variance 0.2, denoted by U \u223c N(1, 0.2). Similarly, we simulate two type of proxy variables W |U \u223c N(1\u2212 2 \u00b7U, 0.2) and Z|U \u223c N(\u22121 + 1.5 \u00b7U, 0.2). The three scenarios vary according to the data generation process based on the models for the outcome Y |A,U , and for the treatment A|U .\n\u2022 Scenario 1. We assume that the true distribution of the outcome Y is a parabola, i.e., the outcome is a second-order regression function of the treatment:\nY |A,U \u223c N(\u221210 + 2.2 \u00b7 (A\u2212 6)2 + 4 \u00b7 Ui, 0.2)\nand A|U \u223c N(2.5 + 4 \u00b7 U, 0.2). \u2022 Scenario 2. We assume that the true distribution of the outcome Y has a sigmoidal shape:\nY |A,U \u223c N(1.5 + sign(A\u2212 5) \u00b7 \u221a |A\u2212 5|+ 1.7 \u00b7 U, 0.05),\nwhere sign is the sign function such that it is equal to 1 when a \u2265 0 and -1 otherwise. We assume A|U \u223c N(1 + 4 \u00b7 U, 0.2). \u2022 Scenario 3. We assume that the true distribution of the outcome Y is monotonically increasing with a non-linear relationship with both variables A and U :\nY |A,U \u223c N(\u22122 \u00b7 e\u22121.4\u00b7(A\u22126) + 0.8 \u00b7 eU , 0.2).\nWe assume A | U \u223c N(2.5 + 4 \u00b7 U, 0.2)\nResults. We report the mean and the standard deviation (std) of cMSE over 20 times across four scenarios, as depicted in Fig. 4 and Tab. 3. For each scenario, we take n = 1, 000. We can see that the PKIPW method suffers from large errors in scenarios 1 and 2 while performing well in scenario 3, where the treatment-inducing proxy Z is misspecified. However, the PKDR method still performs well due to its doubly robust. In addition, we find that the MINIMAX method does not perform well because it requires more samples to fit the neural network."
        },
        {
            "heading": "G.2 EXPERIMENTS FOR TIME SERIES DATA",
            "text": "Data generation. We follow Miao et al. (2018b) to generate data.\nUi = \u03beUi\u22121 + (1\u2212 \u03be2)1/2\u03b51i, Vi = 0.6Ui + \u03b52i, Ai = 0.4 + 1.5Vi + \u03b7Ui + \u03b53i, Yi = 0.5 + 0.7Ai + 1.5Vi + 0.9Ui + \u03b54i, \u03b51i, \u03b52i, \u03b53i, \u03b54i \u223c N(0, 1),\nwhere Ui is a stationary autoregressive process with autocorrelation coefficient \u03be, and \u03b7 controls the magnitude of confounding. Here, we let \u03be = 0.8, \u03b7 = 0.5. For our proximal causal approach, we use Wi = Yi\u22121 and Zi = Ai+1 as two types of proxy variables and do not need auxiliary data.\nResults. We report the mean and the standard deviation (std) of cMSE over 20 times across four scenarios, as depicted in Fig. 4 and Tab. 3. For each scenario, we consider two sample sizes, n = 500 and n = 1, 000. As shown in Fig. 4, our PKIPW and PKDR accurately estimate the causal effect across all treatment values, making its overall cMSE comparable or better than other baselines. This result suggests the effectiveness of our methods for different scenarios."
        },
        {
            "heading": "G.3 RATE",
            "text": "Due to the error introduced in kernel approximation, this is a reasonable sacrifice to handle continuous treatment within the proximal causal framework. Besides, according to Ichimura & Newey (2022), since the estimand is non-regular, therefore it may not enjoy the properties of \u221a n-consistent and asymptotically normality. Such flexible kernel function approximation will make a nonnegligible contribution to the limiting behavior of the estimator, preventing asymptotic normality and root-n consistency.\nWe conducted empirical numerical verification using Scenario 1 from the initial synthesis experiment outlined in Appendix G. As per the synthesis mechanism, we can easily obtain the density function\nf(A|W ) = 1\u221a 0.4\u03c0 e\u2212 1 0.4 (A\u22122.5+4W ) 2 .\nWe compute the empirical estimator error with sample sizes {200, 400, 600, 800, 1, 000} and compare the estimator error in Figure 5. As we speculated before, the convergence rate is difficult to reach n\u22121/2."
        },
        {
            "heading": "H EXPERIMENTS",
            "text": "In this section, we present the data generation process of experiments and the detailed settings of hyper-parameters."
        },
        {
            "heading": "H.1 DATA GENERATING PROCESS IN THE LOW-DIMENSIONAL SYNTHETIC EXPERIMENT",
            "text": "We describe the data generating mechanism in the Synthetic-Data Experiment. The generative process from Mastouri et al. (2021). Since the original data generates Y \u2208 [0, 1] and the overall trend is flat, we modify the structural equation of Y to make it easier to distinguish.\nU := [U1, U2] , U2 \u223c Uniform [\u22121, 2] U1 \u223c Uniform [0, 1]\u2212 I [0 \u2264 U2 \u2264 1] W := [W1,W2] = [U1 +Uniform [\u22121, 1] , U2 + \u03b51] Z := [Z1, Z2] = [U1 + \u03b52, U2 +Uniform[\u2212 1, 1]] A := U2 + \u03b53\nY := 3 cos (2 (0.3U1 + 0.3U2 + 0.2) + 1.5A) + \u03b54\nwhere {\u03f5i}4i=1 \u223c N(0, 1)."
        },
        {
            "heading": "H.2 DATA GENERATING PROCESS IN THE HIGH-DIMENSIONAL SYNTHETIC EXPERIMENT",
            "text": "For X \u2208 Rdim(X), Z \u2208 Rdim(Z), W \u2208 Rdim(W ) and (A,D) \u2208 R, we first generate the unobserved noise:\n{\u03f5i}i\u2208[3] i.i.d\u223c N(0, 1), \u03bdz \u223c Uniform[\u22121, 1]dim(Z), \u03bdw \u223c Uniform[\u22121, 1]dim(W )\nNext, we generate the following data structure\n\u2022 For unobserved confounders U , we have\nUz = \u03f51 + \u03f53, Uw = \u03f52 + \u03f53\n\u2022 For two types of proxies Z and W , we have\nZ = \u03bdz + 0.25 \u00b7 Uz \u00b7 1dim(Z), W = \u03bdw + 0.25 \u00b7 Uw \u00b7 1dim(W ) where 1p \u2208 Rp is the vector of ones of length p.\n\u2022 For covariates X , we have\nX \u223c N(0,\u03a3), where \u03a3 \u2208 Rdim(X)\u00d7dim(X),\u03a3ii = 1 and \u03a3ij = 1\n2 \u00b7 I{|i\u2212 j| = 1} for i \u0338= j.\n\u2022 For treatment A, we have\nA = \u039b(3X\u22a4\u03b2x + 3Z \u22a4\u03b2z) + 0.25 \u00b7 Uw,\nwhere \u03b2x \u2208 Rdim(X) and \u03b2z \u2208 Rdim(Z) are quadratically decaying coefficients, e.g. [\u03b2x]j = j\u22122. \u039b is the truncated logistic link function \u039b(t) = (0.9\u2212 0.1) exp(t)1+exp(t) + 0.1.\n\u2022 For outcome Y , we have\nY = \u03b8ATE0 (A) + 1.2(X \u22a4\u03b2x +W \u22a4\u03b2w) +AX1 + 0.25 \u00b7 Uz,\nwhere \u03b2w \u2208 Rdim(W ) are quadratically decaying coefficients, e.g. [\u03b2w]j = j\u22122.\nFollow Colangelo & Lee (2020), we use the quadratic design, \u03b8ATE0 (a) = a 2 + 1.2a."
        },
        {
            "heading": "H.3 LEGALIZED ABORTION AND CRIME",
            "text": "In the Abortion and Criminality dataset, as described in the reference Woody et al. (2020), the key variables are as follows:\n\u2022 Treatment Variable A: Effective abortion rate;\n\u2022 Outcome variable Y : Murder rate; \u2022 Treatment-inducing proxy Z: Generosity towards families with dependent children; \u2022 Outcome-inducing proxy W : Beer consumption per capita, log-prisoner population per capita,\nand concealed weapons laws.\nWe take the remaining variables as the unobserved confounding variables U . Following Mastouri et al. (2021), the ground-truth value of \u03b2(a) is taken from the generative model fitted to the data.\nThe dataset is available at https://github.com/yuchen-zhu/kernel_proxies/ tree/main/data/sim_1d_no_x."
        },
        {
            "heading": "H.4 HYPERPARAMETERS SELECTION",
            "text": "In all our numerical studies, RKHSs G,H,M,Q are equipped with Gaussian kernels\nK(x1, x2) = exp{\u03b3\u2225x1 \u2212 x2\u222522}.\nThe median heuristic bandwidth parameter \u03b3\u22121 = median{\u2225xi \u2212 xj\u222522}i<j\u2208I for indices subset I \u2282 {1, . . . , n}. For the regularization coefficient, we automatically select it according to Eq. 29 and 30.\nFor KDE, we also choose the Gaussian kernel. For bandwidth, we employ three fold crossvalidation, where the bandwidth is chosen as 20 values uniformly distributed in logarithmic space between 10\u22120.1 and 10 raised to 100.\nFor CNFs, we recommend using the package probaforms, where the prior distributions is multivariate normal distribution.\nWe stack four CNFs-block and finally solve the density function.\nFor the KPV method, we used the Gaussian kernel where the bandwidth is determined by the median trick. We select the regularizers \u03bb1 = \u03bb2 = 0.005.\nFor the PMMR method, we used the Gaussian kernel where the bandwidth is determined by the median trick. We select the regularizers \u03bb1 = \u03bb2 = 0.1.\nFor the DFPV, we optimize the model using Adam with learning rate = 0.001, \u03b21 = 0.9, \u03b22 = 0.999 and \u03b5 = 10\u22128. Regularizers \u03bb1 = \u03bb2 are both set to 0.1 as a result of the tuning procedure.\nFor the MINIMAX, we used learner and adversary networks where learner l2= 1e-4,learner lr=1e-4, adversary l2 = 5e-3 and adversary lr = 5e-4.\nFor the NMMR, we optimize the model using Adam with learning rate =3e-3, decay=3e-6 and epoch = 10000."
        }
    ],
    "title": "DOUBLY ROBUST PROXIMAL CAUSAL LEARNING FOR CONTINUOUS TREATMENTS",
    "year": 2024
}