{
    "abstractText": "AI agents are commonly trained with large datasets of demonstrations of human 1 behavior. However, not all behaviors are equally safe or desirable. Desired char2 acteristics for an AI agent can be expressed by assigning desirability scores, which 3 we assume are assigned to collective trajectories, but not to individual behaviors. 4 For example, in a dataset of vehicle interactions, these scores might relate to the 5 number of incidents that occurred. We first assess the effect of each individual 6 agent\u2019s behavior on the collective desirability score, e.g., assessing how likely an 7 agent is to cause incidents. This allows us to afterward only imitate agents with 8 desired behavior, e.g., only imitating agents that are unlikely to cause incidents. 9 To enable this, we propose the concept of an agent\u2019s Exchange Value, which quan10 tifies an individual agent\u2019s contribution to the collective desirability score. This is 11 expressed as the expected change in desirability score when substituting the agent 12 for a randomly selected agent. We propose additional methods for estimating Ex13 change Values from real-world datasets, enabling us to learn aligned imitation 14 policies that outperform relevant baselines. 15",
    "authors": [],
    "id": "SP:95b9f1a6a2036869149ac9c7c553d2530c74af74",
    "references": [
        {
            "authors": [
                "Learning Proceedings"
            ],
            "title": "1994",
            "venue": "doi: 10.1016/b978-1-55860-335-6.50027-1.",
            "year": 1994
        },
        {
            "authors": [
                "Lloyd Shapley"
            ],
            "title": "A value for n-person games",
            "venue": "Contributions to the Theory of Games, pp",
            "year": 1953
        },
        {
            "authors": [
                "Andy Shih",
                "Stefano Ermon",
                "Dorsa Sadigh"
            ],
            "title": "Conditional imitation learning for multi-agent games",
            "year": 2022
        },
        {
            "authors": [
                "548 2see",
                "e.g",
                "I. Covert",
                "S.I. Lee"
            ],
            "title": "Improving kernelshap: Practical shapley value estimation via linear regression",
            "venue": "arXiv preprint arXiv:2012.01536",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "AI agents are commonly trained with large datasets of demonstrations of human1 behavior. However, not all behaviors are equally safe or desirable. Desired char-2 acteristics for an AI agent can be expressed by assigning desirability scores, which3 we assume are assigned to collective trajectories, but not to individual behaviors.4 For example, in a dataset of vehicle interactions, these scores might relate to the5 number of incidents that occurred. We first assess the effect of each individual6 agent\u2019s behavior on the collective desirability score, e.g., assessing how likely an7 agent is to cause incidents. This allows us to afterward only imitate agents with8 desired behavior, e.g., only imitating agents that are unlikely to cause incidents.9 To enable this, we propose the concept of an agent\u2019s Exchange Value, which quan-10 tifies an individual agent\u2019s contribution to the collective desirability score. This is11 expressed as the expected change in desirability score when substituting the agent12 for a randomly selected agent. We propose additional methods for estimating Ex-13 change Values from real-world datasets, enabling us to learn aligned imitation14 policies that outperform relevant baselines.15\n1 INTRODUCTION16\nImitating human behaviors from large datasets is a promising technique for achieving human-AI and17 AI-AI interactions in complex environments (Carroll et al., 2019; , FAIR; He et al., 2023; Shih et al.,18 2022). However, such large datasets can contain undesirable human behaviors, making direct imita-19 tion problematic. Rather than imitating all behaviors, it may be preferable to ensure that AI agents20 imitate behaviors that align with predefined desirable characteristics. In this work, we assume that21 desirable characteristics are quantified as desirability scores given for each trajectory in the dataset.22 This is commonly the case when the evaluation of the desirability of individual actions is impracti-23 cal or too expensive (Stiennon et al., 2020). For complex datasets that involve multiple interacting24 agents, assigning desirability scores to collective trajectories \u2013 but not to individual behavior \u2013 may25 be the only viable option. For instance, in a football match, while the final score directly gauges26 team performance, determining individual player contributions is more difficult.27\nWe develop an imitation learning method for multi-agent datasets that ensures alignment with de-28 sirable characteristics \u2013 expressed through a Desired Value Function (DVF1) that assigns a score to29 each collective trajectory. This scenario is applicable to several areas that involve learning behavior30 from data of human groups. One example is a dataset of vehicle interactions and desirability scores31 which indicate the number of occurred incidents in a collective trajectory and the aim to imitate only32 behavior that is unlikely to result in incidents (e.g. aiming to imitate driving with foresight). Another33 example is a dataset of a multi-player online game and desirability scores reflecting players\u2019 average34 enjoyment in each round and the goal to imitate only behavior that creates a positive experience.35\nAssessing the desirability of an individual agent\u2019s behavior involves gauging its impact on the col-36 lective desirability score. For instance, it requires evaluating whether an agent\u2019s behavior increases37 the likelihood of accidents while driving or decreases the enjoyment of other players in a game. This38 is termed the credit assignment problem (Shapley, 1953), akin to fairly dividing the value produced39\n1The DVF itself is not sufficient to describe desired behavior completely, as it possibly only covers a subset of behavior, e.g., safety-relevant aspects. It is complementary to the more complex and nuanced behaviors that are obtained by imitating human demonstrations, providing guardrails or additional guidance.\nby a group of players among the players themselves. The credit assignment problem proves com-40 plex in real-world scenarios due to three main factors (see Figure 2 for details): First, many scenarios41 only permit specific group sizes. This makes Shapley Values (Shapley, 1953) \u2013 a concept commonly42 used in Economics for credit assignment \u2013 inapplicable, as it relies on the comparisons of groups of43 different sizes. Second, real-world datasets for large groups are in practice always incomplete, i.e.44 do not contain trajectories for all (combinatorially many) possible groups of agents. Third, datasets45 of human interactions may be fully anonymized by assigning one-time-use IDs. In this case, if an46 agent is present in two trajectories, it will appear in the dataset as if it is two different agents, making47 the credit assignment problem degenerate. This requires incorporating behavior information.48\nTo address these challenges we propose Exchange Values (EVs), akin to Shapley Values, which49 quantify an agent\u2019s contribution as the expected change in desirability when substituting the agent50 randomly. EVs are applicable to scenarios with fixed group sizes, making them more versatile. We51 introduce EV-Clustering that estimates EVs from incomplete datasets by maximizing inter-cluster52 variance. We show a theoretical connection to clustering by unobserved individual contributions53 and adapt this method to fully-anonymized datasets, by considering low-level behavioral cues.54\nBy incorporating agents\u2019 estimated EVs, we introduce Exchange Value based Behavior Cloning55 (EV2BC), which imitates large datasets by only imitating the behavior of agents with EVs higher56 than a tuneable threshold (see Figure 1). This approach allows learning from interactions with agents57 with all behaviors, without necessarily imitating them, which is not possible when simply excluding58 all trajectories with a low collective desirability score. Our work makes the following contributions:59\n\u2022 We introduce Exchange Values (Def. 4.1) to compute an agent\u2019s individual contribution to a col-60 lective value function and show their relation to Shapley Values.61\n\u2022 We propose EV-Clustering (Def. 4.4) to estimate contributions from incomplete datasets and show62 a theoretical connection to clustering agents by their unobserved individual contributions.63\n\u2022 We empirically demonstrate how EVs can be estimated from fully-anonymized data and employ64 EV2BC (Def. 4.5) to learn policies aligned with the DVF, outperforming relevant baselines.65\n2 RELATED WORK66\nMost previous work on aligning AI agents\u2019 policies with desired value functions either relies on67 simple hand-crafted rules (Xu et al., 2020; , FAIR), which do not scale to complex environments, or68 performs postprocessing of imitation policies with fine-tuning (Stiennon et al., 2020; Ouyang et al.,69 2022; Glaese et al., 2022; Bai et al., 2022), which requires access to the environment or a simulator.70 In language modeling, Korbak et al. (2023) showed that accounting for the alignment of behavior71 with the DVF already during imitation learning yields results superior to fine-tuning after-the-fact,72 however, their approach considers an agent-specific value function. In contrast, we consider learn-73 ing a policy aligned with a collective value function, and from offline data alone. Credit assignment74 in multi-agent systems was initially studied in Economics (Shapley, 1953). Subsequently, Shapley75 Values (Shapley, 1953) and related concepts have been applied in multi-agent reinforcement learn-76\ning, to distribute rewards among individual agents during the learning process (Chang et al., 2003;77 Foerster et al., 2018; Nguyen et al., 2018; Wang et al., 2020; Li et al., 2021; Wang et al., 2022).78 Outside of policy learning, Heuillet et al. (2022) used Shapley Values to analyze agent contribu-79 tions in multi-agent environments, however this requires privileged access to a simulator, in order80 to replace agents with randomly-acting agents. In contrast to Shapley Values, the applicability of81 EVs to all group sizes allows us to omit the need to simulate infeasible coalitions by summing82 over multiple outcomes or with random-action policies. In contrast to this work, existing work in83 multi-agent imitation learning typically assumes observations to be generated by optimal agents, as84 well as simulator access (Le et al., 2017; Song et al., 2018; Yu et al., 2019). Similar to our frame-85 work, offline multi-agent reinforcement learning (Jiang & Lu, 2021; Tseng et al., 2022; Tian et al.,86 2022) involves policy learning from multi-agent demonstrations using offline data alone, however,87 it assumes a dense reward signal to be given, while the DVF assigns a single score per collective tra-88 jectory. In single-agent settings, a large body of work investigates estimating demonstrator expertise89 to enhance imitation learning (Chen et al., 2021; Zhang et al., 2021; Cao & Sadigh, 2021; Sasaki &90 Yamashina, 2021; Beliaev et al., 2022; Yang et al., 2021). However, these methods do not translate91 to the multi-agent setting due to the challenge of credit assignment. To the best of our knowledge,92 no prior work has considered the problem of imitating multi-agent datasets containing unaligned93 agents, while ensuring alignment with a collective value function.94\n3 BACKGROUND AND NOTATION95\nMarkov Game. We consider Markov Games (Littman, 1994), which generalize Markov Decision96 Processes (MDPs) to multi-agent scenarios. In a Markov Game, agents interact in a common envi-97 ronment. At time step t, each agent (the ith of a total of m agents) takes the action ati and the environ-98 ment transitions from state st to st+1. A reduced Markov game (without rewards) is then defined by99 a state space S (st \u2208 S), a distribution of initial states \u03b7, the action space Ai (ati \u2208 Ai) of each agent100 i, an environment state transition probability P (st+1|st, a1, . . . , am) and the episode length T . We101 denote this Markov Game as M = (S,A, P, T ), with collective trajectories \u03c4 = (s0,a0, . . . , sT ).102\nSet of multi-agent demonstrations generated by many agents. We consider a Markov game M103 of m agents and a set of demonstrator agents N = {1, ..., n} where n \u2265 m. The Markov Game104 further is assumed to be symmetric (we can change the ordering of players without changing the105 game). The demonstration set D captures interactions among various groups of agents in M. Every106 entry Di = (si, \u03c4si) contains a trajectory \u03c4si for a group of agents si \u2286 N . Notably, \u03c4si contains107 the collective trajectory of all agents in the group si.108\nShapley Values. We now define the concept of the Shapley Value of an agent Shapley (1953),109 which is commonly used to evaluate contributions of individual agents to a collective value function110\nin a characteristic function game. Definition 3.2 below is somewhat unconventional, but can be111 easily seen to be equivalent to the standard definition.112 Definition 3.1 (Characteristic function game). A characteristic function game G is given by a pair113 (N, v), where N = {1, . . . , n} is a finite, non-empty set of agents and v : 2N \u2192 R is a characteristic114 function, which maps each group (sometimes also referred to as coalition) C \u2286 N to a real number115 v(C); it is assumed that v(\u2205) = 0. The number v(C) is referred to as the value of the group C.116\nGiven a characteristic function game G = (N, v), let \u03a0N\\{i} denote the set of all permutations117 of N\\{i}, i.e., one-to-one mappings from N\\{i} to itself. For each permutation \u03c0 \u2208 \u03a0N\\{i}, we118 denote by S\u03c0(m) the slice of \u03c0 up until and including position m; we think of S\u03c0(m) as the set of119 all agents that appear in the first m positions in \u03c0 (note that S\u03c0(0) = \u2205). The marginal contribution120 of an agent i with respect to a permutation \u03c0 and a slice m in a game G = (N, v) is given by121 \u2206Gm,\u03c0(i) = v(S\u03c0(m) \u222a {i})\u2212 v(S\u03c0(m)).122 This quantity measures the increase in the value of the group when agent i joins them. We can now123 define the Shapley Value of an agent i: it is simply the agent\u2019s average marginal contribution, where124 the average is taken over all permutations of N\\{i} and all slices.125 Definition 3.2 (Shapley Value). Given a characteristic function game G = (N, v) with |N | = n,126 the Shapley Value of an agent i \u2208 N is denoted by SVi(G) and is given by127\nSVi(G) = 1/n! \u00b7 \u2211n\u22121\nm=0 \u2211 \u03c0\u2208\u03a0N\\{i}\u2206 G m,\u03c0(i). (1)\nDef. 3.2 is important in the context of credit assignment, as a possible solution for distributing128 collective value to individual agents. It also has several consistency properties (Shapley, 1953).129\n4 METHODS130\nProblem setting. Given a dataset D of trajectories generated by groups of interacting agents and131 a Desired Value Function (DVF), the goal of our paper is to learn an imitation policy for a single132 agent that is aligned with the DVF. We assume that a fraction of the demonstrator agents\u2019 behavior133 is undesirable, specifically, their presence in a group results in a significant reduction of the DVF.134 Further, we assume that the number of demonstrator agents is much larger than the group size of the135 target scenario.136\nOverview of Methods section. To evaluate agents\u2019 contributions in games that only permit spe-137 cific group sizes, we first define the concept of EVs (Def.4.1) for regular characteristic function138 games (Def. 3.1). We then show that our definition extends naturally to characteristic function139 games with constraints on permitted group sizes. We finally derive methods to estimate EVs from140 real-world datasets with limited observations (see Figure 2 for an overview).141\n4.1 EXCHANGE VALUES TO EVALUATE AGENTS\u2019 INDIVIDUAL CONTRIBUTIONS142\nNote that each term of the Shapley Value, denoted \u2206Gm,\u03c0(i), requires computing the difference in143 values between 2 groups of different sizes, with and without an agent i (see Def. 3.2). If we wish144 to only compare groups with the same size, then a natural alternative is to compute the difference in145 values when the agent at position m is replaced with agent i:146\n\u0393Gm,\u03c0(i) = v(S\u03c0(m\u2212 1) \u222a {i})\u2212 v(S\u03c0(m)). (2) We call this quantity the exchange contribution of i, given a permutation of agents \u03c0 sliced at posi-147 tion m. It represents the added value of agent i in a group. Importantly it does not require values of148 groups of different sizes.149\nWe now define the EV analogously to the Shapley Value as the average exchange contribution over150 all permutations of N\\{i} and all non-empty slices.151 Definition 4.1 (Exchange Value). Given a characteristic function game G = (N, v) with |N | = n,152 the Exchange Value of an agent i \u2208 N is denoted by EVi(G) and is given by153\nEVi(G) = ((n\u2212 1)! \u00b7 (n\u2212 1))\u22121 \u00b7 \u2211n\u22121\nm=1 \u2211 \u03c0\u2208\u03a0N\\{i}\u0393 G m,\u03c0(i). (3)\nIn words, the EV of an agent can hence be understood as the expected change in value, when154 substituting the agent with another randomly selected agent, or as comparing the value of all groups155 that include the agent to that of all groups which do not include the agent (see Step 2 in Figure 1).156\nRelationship between Shapley Value and Exchange Value. It can be shown that the Exchange157 Values of all agents can be derived from their Shapley Values by a simple linear transformation:158 we subtract a fraction of the value of the grand coalition N (group of all agents) and scale the159 result by n/n\u22121: EVi(G) = nn\u22121 (SVi(G) \u2212 1/n \u00b7 v(N)). The proof proceeds by computing the160 coefficient of each term v(C), C \u2286 N , in summations (1) and (3) (see Appendix A). Hence, the161 Shapley Value and the Exchange Value order the agents in the same way. Now, recall that the162 Shapley Value is characterized by four axioms, namely, dummy, efficiency, symmetry and linearity163 (Shapley, 1953). The latter two are also satisfied by the Exchange Value: if v(C\u222a{i}) = v(C\u222a{j})164 for all C \u2286 N \\ {i, j}, we have EVi(G) = EVj(G) (symmetry), and if we have two games G1165 and G2 with characteristic functions v1 and v2 over the same set of agents N , then for the combined166 game G = (N, v) with the characteristic function v given by v(C) = v1(C) + v2(C) we have167 EVi(G) = EVi(G1) + EVi(G2) (linearity). The efficiency property of the Shapley Value, i.e.,168 \u2211\ni\u2208N SVi(G) = v(N) implies that \u2211\ni\u2208N EVi(G) = 0. In words, the sum of all agents\u2019 EV is169 zero. The dummy axiom, too, needs to be modified: if an agent i is a dummy, i.e., v(C\u222a{i}) = v(C)170 for every C \u2286 N \\ {i} then for the Shapley value we have SVi(G) = 0 and hence EVi(G) =171 \u22121/n\u22121 \u00b7 v(N), In each case, the proof follows from the relationship between the Shapley Value and172 the Exchange Value and the fact that the Shapley Value satisfies these axioms (see Appendix A).173\n4.1.1 COMPUTING EXCHANGE VALUES IF ONLY CERTAIN GROUP SIZES ARE PERMITTED174\nFor a characteristic function game G = (N, v) the value function v can be evaluated for each possible175 group C \u2286 N . We now consider the case where the value function v is only defined for groups of176 certain sizes m \u2208 M , i.e. v is only defined for a subset of groups of certain sizes.177 Definition 4.2 (Constrained characteristic function game). A constrained characteristic function178 game G\u0304 is given by a tuple (N, v,M), where N = {1, . . . , n} is a finite, non-empty set of agents,179 M \u2286 {0, . . . , n \u2212 1} is a set of feasible group sizes and v : {C \u2208 2N : |C| \u2208 M} \u2192 R is a180 characteristic function, which maps each group C \u2286 N of size |C| \u2208 M to a real number v(C).181\nNote that both the Shapley Value and the EV are generally undefined for constrained characteristic182 function games, as the value function is not defined for groups C of size |C| /\u2208 M . The definition183 of the Shapley Value cannot easily be adapted to constrained characteristic function games, as its184 computation requires evaluating values of groups of different sizes. In contrast, the definition of the185 EV can be straightforwardly adapted to constrained characteristic function games by limiting the186 summation to slices of size m \u2208 M+, where M+ = {m \u2208 M : m > 0}. Hence, we define the187 Constrained EV as the average exchange contribution over all permutations of N\\{i} and over all188 slices of size m \u2208 M+.189 Definition 4.3 (Constrained Exchange Value). Given a constrained characteristic function game190 G\u0304 = (N, v,M) with |N | = n, the Constrained Exchange Value of an agent i \u2208 N is denoted by191 EVi(G\u0304) and is given by EVi(G\u0304) = ((n\u2212 1)! \u00b7 |M+|)\u22121 \u00b7 \u2211 m\u2208M+ \u2211 \u03c0\u2208\u03a0N\\{i}\u0393 G\u0304 m,\u03c0(i).192\nWe refer to the Constrained EV and EV interchangeably, as they are applicable to different settings.193 As outlined in Step 2 in Figure 1, the EV of an agent is a comparison of the value of a group that194 includes the agent and a group that does not include the agent, considering all permitted group sizes.195\n4.2 ESTIMATING EXCHANGE VALUES FROM LIMITED DATA196\nThe EV assesses the contribution of an individual agent and is applicable under group size limi-197 tations in real-world scenarios (see Group-Limited in Figure 2). However, exactly calculating198 EVs is almost always impossible as real-world datasets likely do not contain observations for all199 (combinatorially many) possible groups (Low-Data in Figure 2). We first show a sampling-based200 estimate (Section 4.2) of EVs, which may have a high variance for EVs of agents that are part of a201 only few trajectories (outcomes). Next we introduce a novel method, EV-Clustering (Section 4.2.1),202 which clusters agents that behave similarly and can be used to reduce the variance. When datasets203 are anonymized with one-time-use IDs each demonstrator is only observed as part of one group (see204\nDegenerate in Figure 2), rendering credit assignment degenerate, as explained in Section 4.2.1;205 we propose to address this by incorporating low-level behavior data from the trajectories \u03c4 . If206 some groups are not observed, we can achieve an unbiased estimate of the EV by sampling groups207 uniformly at random. The expected EV is EVi(G\u0304) = Em\u223cU(M+),\u03c0\u223cU(\u03a0N\\{i}) [ \u0393G\u0304m,\u03c0(i) ] . This ex-208 pectation converges to the true EV in the limit of infinite samples.209\n4.2.1 EV-CLUSTERING IDENTIFIES SIMILAR AGENTS210\nIn the case of very few agent observations, the above-introduced sampling estimate has a high vari-211 ance. One way to reduce the variance is by clustering: if we knew that some agents tend to contribute212 similarly to the DVF, then by clustering them and estimating one EV per cluster (instead of one EV213 per agent), each EV estimate will use more samples. Note that, as our focus is on accurately estimat-214 ing EVs, we do not consider clustering agents by behavior here, as two agents may exhibit distinct215 behaviors while still contributing similarly to the DVF.216\nWe propose EV-Clustering, which clusters agents such that the variance in assigned EVs is max-217 imized across all agents. In Appendix A we show that EV-Clustering is equivalent to clustering218 agents by their unobserved individual contribution, under the approximation that the total value of219 a group is the sum of the participating agents\u2019 individual contributions, an assumption frequently220 made for theoretical analysis (Lundberg & Lee, 2017; Covert & Lee, 2021), as it represents the sim-221 plest non-trivial class of cooperative games. Intuitively, if we choose clusters that maximize the EV222 variance across agents, all clusters\u2019 EVs will be maximally distinct. An example of poor clustering223 is a random partition, which will have low variance and thus very similar EVs across clusters.224\nSpecifically, we assign n agents to k \u2264 n clusters K = {1, . . . , k \u2212 1}, with individual cluster225 assignments u = {u0, ..., un\u22121}, where ui \u2208 K. We first combine the observations of all agents226 within the same cluster by defining a clustered value function v\u0303(C) that assigns a value to a group227 of cluster-centroid agents C \u2286 K by averaging over the combined observations, as v\u0303(C) = 1/\u03b7 \u00b7228 \u2211n\u22121\nm=0 \u2211 \u03c0\u2208\u03a0N v(S\u03c0(m)) \u00b7 1({uj | j \u2208 S\u03c0(m)} = C), where \u03b7 is a normalization constant. The229 EV of an agent i is then given as EVi(G\u0303), with G\u0303 = (K, v\u0303), thereby assigning equal EVs to all230 agents within one cluster.231 Definition 4.4 (EV-Clustering). We define the optimal cluster assignments u\u2217 such that the variance232 of EVs is maximised:233\nu\u2217 \u2208 argmaxuVar([EV0(G\u0303), . . . , EVn\u22121(G\u0303)]). (4)\nWe show in Appendix B.1 that this objective is equivalent to clustering agents by their unobserved234 individual contributions, under the approximation of an additive value function.235\n4.2.2 DEGENERACY OF THE CREDIT ASSIGNMENT PROBLEM FOR FULLY-ANONYMIZED DATA236\nIf two agents are observed only once in the dataset and as part of the same group, equal credit must237 be assigned to both due to the inability to separate their contributions. Analogously, when all agents238 are only observed once, credit can only be assigned to groups, resulting in the degenerate scenario239 that all agents in a group are assigned the same credit (e.g. are assigned equal EV). We solve this by240 combining low-level behavior information from trajectories \u03c4 with EV-Clustering (see Sec. 5.1).241\n4.3 EXCHANGE VALUE BASED BEHAVIOR CLONING (EV2BC)242\nHaving defined the EV of an individual agent and different methods to estimate it, we now define a243 variation of Behavior Cloning (Pomerleau, 1991) which takes into account each agent\u2019s contribution244 to the desirability value function (DVF). We refer to this method as EV2BC. Essentially, EV2BC245 imitates only actions of agents that have an EV larger than a tunable threshold parameter.246 Definition 4.5 (EV based Behavior Cloning (EV2BC)). For a set of demonstrator agents N , a247 dataset D, and a DVF, we define the imitation learning loss for EV2BC as248\nLEV 2BC(\u03b8) = \u2212 \u2211 n\u2208N \u2211 (si,ani )\u2208D log(\u03c0\u03b8(ani |si)) \u00b7 1(EV DV Fn > c) (5)\nwhere EV DV Fn denotes the EV of agent n for a DVF and where c is a tunable threshold parameter,249 that trades off between including data of agents with higher contributions to the DVF and reducing250 the total amount of training data used.251\n5 EXPERIMENTS252\nThe environments that we consider only permit certain group sizes, hence we use constrained EVs253 (see Def. 4.3). As the environments are stochastic, we use sampling (see Sec. 4.2) to estimate true254 EVs. We run all experiments for five random seeds and report mean and standard deviation where255 applicable. For more implementation details please refer to the Appendix.256\nIn the following experiments, we first evaluate EVs as a measure of an agent\u2019s contribution to a257 given DVF. We then assess the average estimation error for EVs as the number of observations in258 the dataset D decreases, and how applying clustering decreases this error. We lastly evaluate the per-259 formance of Exchange Value based Behaviour Cloning (EV2BC, see Definition 4.5) for simulated260 and human datasets and compare to relevant baselines, such as standard Behavior Cloning (Pomer-261 leau, 1991) and Offline Reinforcement Learning (Pan et al., 2022).262\nEnvironments. The Tragedy of the Commons (Hardin, 1968) (ToC) refers to a situation where263 multiple individuals deplete a shared resource, and is a social dilemma scenario often studied to264 model the overexploitation of common resources (Dietz et al., 2003; Ostrom, 2009). We model265 ToC as a multi-agent environment and consider three DVFs to represent different interpretations of266 social welfare in the game: the final pool size (vfinal), the total resources consumed (vtotal), and the267 minimum consumption among agents (vmin). Overcooked (Carroll et al., 2019; Hu et al., 2020;268 Shih et al., 2022) is a two-player game simulating a cooperative cooking task requiring teamwork269 and coordination and a common testbed in multi-agent research for studying collaboration. Within270 Overcooked, we consider the configurations Cramped Room and Coordination Ring (displayed in271 Figure 4). For each environment configuration, we generate two datasets by simulating agent behav-272 iors using a near-optimal planning algorithm, where we use a parameter \u03bb to determine an agent\u2019s273 behavior. For \u03bb = 1 agents act (near)-optimal, for \u03bb = \u22121 agents act adversarially. We refer to274 \u03bb as the agent\u2019s trait, as it acts as a proxy for the agent\u2019s individual contribution to the collective275 value function. Each demonstration dataset D is generated by n = 100 agents, and trajectories \u03c4 are276 of length 400. The adversarial dataset Dadv is comprised of 25% adversarial agents with \u03bb = \u22121277 and 75% (near)-optimal agents with \u03bb = 1, while for the dataset D\u03bb agents were uniformly sam-278 pled between \u03bb = \u22121 and \u03bb = 1. The Dhuman dataset was collected from humans playing the279 game (see Carroll et al. (2019)); it is fully anonymized with one-time-use agent identifiers, hence is280 a degenerate dataset (see Figure 2 bottom row). We consider the standard value function given for281 Overcooked as the DVF, i.e. the number of soups prepared by both agents over the course of a trajec-282 tory. The StarCraft Multi-Agent Challenge (Samvelyan et al., 2019) is a cooperative multi-agent283 environment that is partially observable, involves long-term planning, requires strong coordination,284 and is heterogeneous, in which we consider the settings 2s3z, 3s_vs_5z and 6h_vs_8z, which285 involve teams of 3-6 agents. For each, we generate a pool of 200 agents with varying capabilities286 by extracting policies at different epochs, and from training with different seeds. We generate a287 dataset that contains simulated trajectories of 100 randomly sampled groups (out of 109 possible288 groups) and use the environment\u2019s ground truth reward function to assign DVF scores according to289 the collective performance of agents.290\nExchange Values assess an agent\u2019s individual contribution to a collective value function. To291 analyze EVs as a measure for an agent\u2019s individual contribution to a DVF, we consider full datasets292 that contain demonstrations of all possible groups, which allows us to accurately estimate EVs.293\nIn ToC, we find that the ordering of agents broadly reflects our intuition, taking more resources294 negatively impacts the EVs, and agents consuming the average of others have less extreme EVs. The295 color-coded ordering of agents under different DVFs in shown in Figure 7 in App. C. In Overcooked,296 we consider the two simulated datasets (Dadv and D\u03bb) but not the human dataset, as the individual297 contribution is unknown for human participants. We find that EVs of individual agents are strongly298 correlated with their trait parameter \u03bb, which is a proxy for the agent\u2019s individual contribution, and299 provide a plot that shows the relationship between \u03bb and EV in Figue 5 in App. B).300\n5.1 ESTIMATING EVS FROM INCOMPLETE DATA301\nEstimation error for different dataset sizes. We now turn to realistic settings with missing data,302 where EVs must be estimated (Sec. 4.2). For both ToC and Overcooked, we compute the mean303 estimation error in EVs if only a fraction of the possible groups is contained in the dataset. As304 expected, we observe in Fig. 3 that the mean estimation error increases as the fraction of observed305 groups decreases, with the largest estimation error for fully anonymized datasets (see Fig. 3 \u2013 Deg.).306\nEstimating EVs from degenerate datasets. We first use low-level behavior information from the307 given trajectories \u03c4 in D to initialize cluster assignments and then apply EV-Clustering. Specifi-308 cally, we first create behavior-based cluster assignments by applying k-means clustering to vectors309 of concatenated action frequencies in frequently-observed states (see Appendix B for details). We310 then perform EV-Clustering, using the behavior-based cluster assignments to initialize a non-linear311 constrained optimization solver (SLSQP, Kraft (1988)) and adding a small L2 loss term that penal-312 izes solutions deviating from the behavior-based clusters. We observe in Figure 3 that this results in313 a significant decrease in the estimation error of EVs (see \u2013 Deg. clustered). Generally, EV-clustering314 is preferable to behavior clustering, as two agents may have equal contributions to the DVF while315 showing different behaviors; only in cases where only few outcomes per agent are observed is it316 necessary to also use behavior clustering. In the ablation study in Appendix B.1 we investigate317 both methods, finding that both behavior-clustering and EV-Clustering are significant, while be-318 havior clustering is more robust in low-data scenarios (as it incorporates all low-level information319 contained within a trajectory, while EV-Clustering only considers final outcomes).320\nEstimating EVs from degenerate human datasets in Overcooked. In contrast to the simulated321 datasets, no estimation error in EVs can be computed for the human-generated datasets as the ground322 truth EVs are unknown. Also, no latent trait \u03bb that indicates how well a human participant is aligned323 with the DVF is known. However, to evaluate the quality of the estimated EVs for the human324 dataset, we use the keystrokes per second of an agent as a proxy for its individual contribution,325 which we refer to as \u03bbhuman. We here follow Carroll et al. (2019), which found that this proxy326 is highly correlated with overall performance. We estimate EVs for human participants as before,327 relying both on behavior information and DVF scores. We evaluate the quality of computed EVs as328 the inverse of the within-cluster variance in \u03bbhuman. Relative to the average within-cluster variance329 under random cluster assignments, we find that behavior-based clustering results in a reduction of330 16% and 25% in Cramped Room and Coordination ring, respectively, while EV-Clustering reduces331 the within-cluster variance by another 34% and 48% percent, respectively. These findings validate332 that maximizing variance in EVs allows clustering agents by their individual contributions.333\n5.2 IMITATING DESIRED BEHAVIOR BY UTILIZING EVS334\nWe now evaluate EV2BC in both domains, where we set the threshold parameter such that in ToC335 only agents with EVs above the 90th percentile are imitated, and in Overcooked above the 50th336\npercentile. We chose these values because training data in Overcooked is more scarce. We replicate337 the stochastic EV2BC policy for each agent in the environment and evaluate the achieved collective338 DVF score. As baselines, we consider (1) BC, where Behavior Cloning (Pomerleau, 1991) is done339 with the full dataset without correcting for EVs, (2) offline multi-agent reinforcement OMAR (Pan340 et al., 2022) with the reward at the last timestep set to the DVF\u2019s score for a given trajectory (no341 per-step reward is given by the DVF) and (3) Group BC, for which only collective trajectories with342 a DVF score larger than the relevant percentile are included. While EV2BC is based on individual343 agents\u2019 contributions, this last baseline imitates data based on group outcomes. For instance, if a344 collective trajectory includes two aligned agents and one unaligned agent, the latter baseline is likely345 to imitate all three agents. In contrast, our approach would only imitate the two aligned agents.346\nToC results. We imitate datasets of 12 agents and 120 agents, with group sizes of 3 and 10 respec-347 tively, evaluating performance for each of the three DVFs defined for the ToC environment. We do348 not consider the OMAR baseline as policies are not learned but rule-based. Table 2 demonstrates that349 EV2BC outperforms the baselines by a large margin, indicating that considering individual agents\u2019350 EVs to a given DVF leads to significantly improved performance.351\nOvercooked results. We now consider all datasets Dadv, D\u03bb and Dhuman in both Overcooked en-352 vironments. Note that in the standard Overcooked environment, an adversarial agent is limited to353 blocking the other agent, while in many real-world environments, adversaries are likely to be capa-354 ble of more diverse (and possibly severe) actions. We evaluate the performance achieved by agents355 with respect to the DVF (in this case the environments value function of maximizing the number356 of soups) when trained with different imitation learning approaches on the different datasets. We357 evaluate performance for the fully-anonymized datasets, but also consider datasets with more data358 in Table 4 in the Appendix, for which we find an even larger performance gap. EVs are computed359 as detailed in Section 5.1. Table 2 shows that EV2BC clearly outperforms the baseline approaches360 in both environment configurations, with the margin being more significant in the Overcooked+Fire361 environments where adversarial agents can take more powerful actions. We further note that EV2BC362 significantly outperforms baseline approaches on the datasets of human-generated behavior, for363 which EVs were estimated from a fully-anonymized real-world dataset. This demonstrates that364 BC on datasets containing unaligned behavior carries risk of learning wrong behavior, but it can be365 alleviated by weighting the samples using estimated EVs.366\nStarcraft Results. We observe in Table 1 that EV2BC outperforms the baselines by a substantial367 margin, underlining the applicability of our method to larger and more complex settings. Note that368 the OMAR baseline, which is implemented as offline MARL with the DVF as the final-timestep369 reward, did substantially worse than BC.370\n6 CONCLUSION371\nOur work presents a method for training AI agents from diverse datasets of human interactions372 while ensuring that the resulting policy is aligned with a given desirability value function. However,373 it must be noted that quantifying this value function is an active research area.374\nShapley Values and Exchange Values estimate the alignment of an individual with a group value375 function (which must be prescribed separately), and as such can be misused if they are included in a376 larger system that is used to judge those individuals in any way. Discrimination of individuals based377 on protected attributes is generally unlawful, and care must be taken to avoid any discrimination378 by automated means. We demonstrated a novel positive use of these methods by using them to379 train aligned (beneficial) agents, that do not imitate negative behaviors in a dataset. We expect that380 the benefits of addressing the problem of unsafe behavior by AI agents outweigh the downsides of381 misuse of Shapley Values and Exchange Values, which are covered by existing laws.382\nFuture work may address the assumption that individual agents behave similarly across multiple tra-383 jectories and develop methods for a more fine-grained assessment of desired behavior. Additionally,384 exploring how our framework can more effectively utilize data on undesired behavior is an inter-385 esting avenue for further investigation, e.g., developing policies that are constrained to not taking386 undesirable actions. Lastly, future work may investigate applications to real-world domains, such as387 multi-agent autonomy scenarios.388\nReproducibility. To ensure the reproducibility of our work, we will publish the source code with389 the camera-ready version of this work. We provide detailed overviews for all steps of the experi-390 mental evaluation in the Appendix, where we also link to the publicly available code repositories391 that our work used. We further provide information about computational complexity at the end of392 the Appendix.393\nREFERENCES394 Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav395 Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement396 learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.397\nMark Beliaev, Andy Shih, Stefano Ermon, Dorsa Sadigh, and Ramtin Pedarsani. Imitation learning by estimat-398 ing expertise of demonstrators. In International Conference on Machine Learning, pp. 1732\u20131748. PMLR,399 2022.400\nZhangjie Cao and Dorsa Sadigh. Learning from imperfect demonstrations from agents with varying dynamics.401 IEEE Robotics and Automation Letters, 6(3):5231\u20135238, 2021.402\nMicah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, and Anca Dragan. On403 the utility of learning about humans for human-ai coordination. Advances in neural information processing404 systems, 32, 2019.405\nYu-Han Chang, Tracey Ho, and Leslie Kaelbling. All learning is local: Multi-agent learning in global reward406 games. Advances in neural information processing systems, 16, 2003.407\nLetian Chen, Rohan Paleja, and Matthew Gombolay. Learning from suboptimal demonstration via self-408 supervised reward regression. In Conference on robot learning, pp. 1262\u20131277. PMLR, 2021.409\nIan Covert and Su-In Lee. Improving kernelshap: Practical shapley value estimation using linear regression. In410 International Conference on Artificial Intelligence and Statistics, pp. 3457\u20133465. PMLR, 2021.411\nThomas Dietz, Elinor Ostrom, and Paul C Stern. The struggle to govern the commons. science, 302(5652):412 1907\u20131912, 2003.413\nMeta Fundamental AI Research Diplomacy Team (FAIR)\u2020, Anton Bakhtin, Noam Brown, Emily Dinan,414 Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. Human-415 level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378416 (6624):1067\u20131074, 2022.417\nJakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counter-418 factual multi-agent policy gradients. In Proceedings of the AAAI conference on artificial intelligence, vol-419 ume 32, 2018.420\nAmelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,421 Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via422 targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.423\nGarrett Hardin. The tragedy of the commons: the population problem has no technical solution; it requires a424 fundamental extension in morality. science, 162(3859):1243\u20131248, 1968.425\nJerry Zhi-Yang He, Zackory Erickson, Daniel S Brown, Aditi Raghunathan, and Anca Dragan. Learning426 representations that enable generalization in assistive tasks. In Conference on Robot Learning, pp. 2105\u2013427 2114. PMLR, 2023.428\nAlexandre Heuillet, Fabien Couthouis, and Natalia D\u0131\u0301az-Rodr\u0131\u0301guez. Collective explainable ai: Explaining co-429 operative strategies and agent contribution in multiagent reinforcement learning with Shapley values. IEEE430 Computational Intelligence Magazine, 17(1):59\u201371, 2022.431\nHengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. \u201cother-play\u201d for zero-shot coordination.432 In International Conference on Machine Learning, pp. 4399\u20134410. PMLR, 2020.433\nJiechuan Jiang and Zongqing Lu. Offline decentralized multi-agent reinforcement learning. arXiv preprint434 arXiv:2108.01832, 2021.435\nTomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher L Buckley, Jason Phang, Samuel R436 Bowman, and Ethan Perez. Pretraining language models with human preferences. arXiv preprint437 arXiv:2302.08582, 2023.438\nDieter Kraft. A software package for sequential quadratic programming. Forschungsbericht- Deutsche439 Forschungs- und Versuchsanstalt fur Luft- und Raumfahrt, 1988.440\nHoang M Le, Yisong Yue, Peter Carr, and Patrick Lucey. Coordinated multi-agent imitation learning. In441 International Conference on Machine Learning, pp. 1995\u20132003. PMLR, 2017.442\nJiahui Li, Kun Kuang, Baoxiang Wang, Furui Liu, Long Chen, Fei Wu, and Jun Xiao. Shapley counterfactual443 credits for multi-agent reinforcement learning. In Proceedings of the 27th ACM SIGKDD Conference on444 Knowledge Discovery & Data Mining, pp. 934\u2013942, 2021.445\nMichael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine446 Learning Proceedings 1994. 1994. doi: 10.1016/b978-1-55860-335-6.50027-1.447\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in neural448 information processing systems, 30, 2017.449\nDuc Thien Nguyen, Akshat Kumar, and Hoong Chuin Lau. Credit assignment for collective multiagent rl with450 global rewards. Advances in neural information processing systems, 31, 2018.451\nElinor Ostrom. A general framework for analyzing sustainability of social-ecological systems. Science, 325452 (5939):419\u2013422, 2009.453\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,454 Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with455 human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.456\nLing Pan, Longbo Huang, Tengyu Ma, and Huazhe Xu. Plan better amid conservatism: Offline multi-agent re-457 inforcement learning with actor rectification. In International Conference on Machine Learning, pp. 17221\u2013458 17237. PMLR, 2022.459\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,460 R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-461 nay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830,462 2011.463\nDean A. Pomerleau. Efficient Training of Artificial Neural Networks for Autonomous Navigation. Neural464 Computation, 3(1), 1991. ISSN 0899-7667. doi: 10.1162/neco.1991.3.1.88.465\nMikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ466 Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent467 challenge. arXiv preprint arXiv:1902.04043, 2019.468\nFumihiro Sasaki and Ryota Yamashina. Behavioral cloning from noisy demonstrations. In International Con-469 ference on Learning Representations, 2021.470\nLloyd Shapley. A value for n-person games. Contributions to the Theory of Games, pp. 307\u2013317, 1953.471\nAndy Shih, Stefano Ermon, and Dorsa Sadigh. Conditional imitation learning for multi-agent games. In 2022472 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 166\u2013175. IEEE, 2022.473\nJiaming Song, Hongyu Ren, Dorsa Sadigh, and Stefano Ermon. Multi-agent generative adversarial imitation474 learning. Advances in neural information processing systems, 31, 2018.475\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario476 Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Infor-477 mation Processing Systems, 33:3008\u20133021, 2020.478\nRobert Thorndike. Who belongs in the family? Psychometrika, 18(4):267\u2013276, 1953.479\nQi Tian, Kun Kuang, Furui Liu, and Baoxiang Wang. Learning from good trajectories in offline multi-agent480 reinforcement learning. arXiv preprint arXiv:2211.15612, 2022.481\nWei-Cheng Tseng, Tsun-Hsuan Johnson Wang, Yen-Chen Lin, and Phillip Isola. Offline multi-agent rein-482 forcement learning with knowledge distillation. Advances in Neural Information Processing Systems, 35:483 226\u2013237, 2022.484\nJianhong Wang, Yuan Zhang, Tae-Kyun Kim, and Yunjie Gu. Shapley q-value: A local reward approach to485 solve global reward games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,486 pp. 7285\u20137292, 2020.487\nJianhong Wang, Yuan Zhang, Yunjie Gu, and Tae-Kyun Kim. Shaq: Incorporating shapley value theory into488 multi-agent q-learning. Advances in Neural Information Processing Systems, 35:5941\u20135954, 2022.489\nJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. Recipes for safety in open-490 domain chatbots. arXiv preprint arXiv:2010.07079, 2020.491\nMengjiao Yang, Sergey Levine, and Ofir Nachum. Trail: Near-optimal imitation learning with suboptimal data.492 arXiv preprint arXiv:2110.14770, 2021.493\nLantao Yu, Jiaming Song, and Stefano Ermon. Multi-agent adversarial inverse reinforcement learning. In494 International Conference on Machine Learning, pp. 7194\u20137201. PMLR, 2019.495\nSongyuan Zhang, Zhangjie Cao, Dorsa Sadigh, and Yanan Sui. Confidence-aware imitation learning from496 demonstrations with varying optimality. Advances in Neural Information Processing Systems, 34:12340\u2013497 12350, 2021.498\nA APPENDIX TO METHODS499\nA.1 AXIOMATIC PROPERTIES OF THE EXCHANGE VALUE AND ITS RELATIONSHIP WITH THE500 SHAPLEY VALUE501\nFix a characteristic function game G with a set of players N . It is well-known that the Shapley Value satisfies502 the following axioms (Shapley, 1953):503\n(1) Dummy: if an agent i satisfies v(C \u222a {i}) = v(C) for all C \u2286 N \\ {i} then SVi(G) = 0;504 (2) Efficiency: the sum of all agents\u2019 Shapley Values equals to the value of the grand coalition, i.e.,505 \u2211 i\u2208N SVi(G) = v(N);506 (3) Symmetry: for every pair of distinct agents i, j \u2208 N with v(C \u222a {i}) = v(C \u222a {j}) for all C \u2286 N \\ {i, j}507 we have SVi(G) = SVj(G);508\n(4) Linearity: for any pair of games G1 = (N, v1) and G2 = (N, v2) with the same set of agents N , the game509 G = (N, v) whose characteristic funciton v is given by v(C) = v1(C) + v2(C) for all C \u2286 N satisfies510 SVi(G) = SVi(G1) + SVi(G2) for all i \u2208 N .511\nIndeed, the Shapley Value is the only value for characteristic function games that satisfies these axioms (Shap-512 ley, 1953). It is then natural to ask which of these axioms (or their variants) are satisfied by the Exchange513 Value.514\nTo answer this question, we first establish a relationship between the Shapley Value and the Exchange Value.515 Proposition A.1. For any characteristic function game G = (N, v) and every agent i \u2208 N we have516\nEVi(G) = n\nn\u2212 1\n( SVi(G)\u2212 1\nn \u00b7 v(N)\n) . (6)\nProof. Fix an agent i and consider an arbitrary non-empty coalition C \u228a N \\ {i}.517\nIn the expression for the Shapley Value of i the coefficient of v(C) is\n\u2212 1 n! (|C|)!(n\u2212 1\u2212 |C|)! :\nwe subtract the fraction of permutations of N where the agents in C appear in the first |C| positions, followed by i. By the same argument, the coefficient of v(C \u222a {i}) is\n1 n! (|C|)!(n\u2212 1\u2212 |C|)!.\nSimilarly, in the expression for the Exchange Value of i the coefficient of v(C) is\n\u2212 1 (n\u2212 1)!(n\u2212 1) (|C|)!(n\u2212 1\u2212 |C|)! :\neach permutation of N \\ {i} where agents in C appear in the first |C| positions contributes with coefficient \u2212 1\n(n\u22121)!(n\u22121) . By the same argument, the coefficient of v(C \u222a {i}) is 1\n(n\u2212 1)!(n\u2212 1) (|C|)!(n\u2212 1\u2212 |C|)!\nNow, if C = N \\ {i}, in the expression for SVi(G) the coefficient of v(C) is \u2212 1n and the coefficient of518 v(C \u222a {i}) = v(N) is 1\nn . In contrast, in the expression for EVi(G) the coefficient of v(C) is \u2212 1n\u22121 : for each519\nof the (n \u2212 1)! permutations of N \\ {i} we subtract v(C) with coefficient 1 (n\u22121)!(n\u22121) when we replace the520 last agent in that permutation by i. On the other hand, v(N) never appears.521\nIt follows that, for every coalition C \u228a N , if the value v(C) appears in the expression for SHi(G) with weight \u03c9 then it appears in the expression for EVi(G) with weight nn\u22121 \u00b7 \u03c9. Hence\nEVi(G) = n\nn\u2212 1\n( SHi(G)\u2212 1\nn \u00b7 v(N) ) 522\nExample A.2. Consider a characteristic function game G = (N, v), where N = {1, 2} and v is given by v({1}) = 2, v({2}) = 4, v({1, 2}) = 10. We have\nSH1(G) = (2 + (10\u2212 4))/2 = 4, SH2(G) = (4 + (10\u2212 2))/2 = 6 and EV1(G) = 2\u2212 4 = \u22122, EV2(G) = 4\u2212 2 = 2. Note that EV1(G) = 2(SH1(G)\u2212 12v(N)), EV2(G) = 2(SH2(G)\u2212 1 2 v(N)).523\nWe can use Proposition A.1 to show that the Exchange Value satisfies two of the axioms satisfied by the Shapley524 Value, namely, linearity and symmetry.525\nProposition A.3. The Exchange Value satisfies symmetry and linearity axioms.526\nProof. For the symmetry axiom, fix a characteristic function game G = (N, v) and consider two agents i, j \u2208 N with v(C \u222a {i}) = v(C \u222a {j}) for all C \u2286 N \\ {i, j}. We have\nEVi(G) = n\nn\u2212 1\n( SVi(G)\u2212 1\nn \u00b7 v(N)\n) = n\nn\u2212 1\n( SVj(G)\u2212 1\nn \u00b7 v(N)\n) = EVj(G),\nwhere the first and the third equality follow by Proposition A.1, and the second equality follows because the527 Shapley Value satisfies symmetry.528\nFor the linearity axiom, consider a pair of games G1 = (N, v1) and G2 = (N, v2) with the same set of agents529 N and the game G = (N, v) whose characteristic funciton v is given by v(C) = v1(C) + v2(C) for all530 C \u2286 N . Fix an agent i \u2208 N . We have531\nEVi(G) = n\nn\u2212 1\n( SVi(G)\u2212 1\nn \u00b7 (v1(N) + v2(N)) ) = n\nn\u2212 1\n( SVi(G1)\u2212 1\nn \u00b7 v1(N)\n) + n\nn\u2212 1\n( SVi(G2)\u2212 1\nn \u00b7 v2(N) ) = EVi(G1) + EVi(G2).\nAgain, the first and the third equality follow by Proposition A.1, and the second equality follows because the532 Shapley Value satisfies linearity.533\nWhile the Exchange Value does not satisfy the dummy axiom or the efficiency axiom, it satisfies appropriately534 modified versions of these axioms.535\nProposition A.4. For every characteristic function game G it holds that \u2211\ni\u2208N EVi(G) = 0. Moreover, if i is536 a dummy agent, i.e., v(C \u222a {i}) = V (C) for all C \u2286 N \\ {i} then EVi(G) = \u2212 v(N)n\u22121 .537\nProof. We have538 \u2211 i\u2208N EVi(G) = \u2211 i\u2208N n n\u2212 1 ( SVi(G)\u2212 1 n \u00b7 v(N) ) = \u2211 i\u2208N n n\u2212 1SVi(G)\u2212 n n\u2212 1 \u00b7 v(N)\n= n n\u2212 1 \u00b7 v(N)\u2212 n n\u2212 1 \u00b7 v(N) = 0,\nwhere we use Proposition A.1 and the fact that the Shapley Value satisfies the efficiency axiom.539\nNow, fix a dummy agent i. We have\nEVi(G) = n\nn\u2212 1\n( SVi(G)\u2212 1\nn \u00b7 v(N)\n) = \u2212 1\nn\u2212 1 \u00b7 v(N);\nagain, we use Proposition A.1 and the fact that the Shapley Value satisfies the dummy axiom.540\nA.2 DERIVATION OF CLUSTERING OBJECTIVE STATED IN EQ. 4541\nInessential games and EVs. The assumption of an inessential game is commonly made to compute Shap-542 ley Values more efficiently2. In an inessential game, the value of a group is given by the sum of the individual543 contributions of its members, denoted as v(C) = \u2211 i\u2208C vi, where vi is an individual agent\u2019s unobserved544 contribution vi. The EV (see Definition 4.1) of an individual agent i in an inessential game is given as545\nEV i(G) = vi \u2212 1/|N|\u22121 \u00b7 \u2211\nj\u2208N\\{i} vj = (1 + 1/|N|\u22121) \u00b7 vi \u2212 1/|N|\u22121 \u00b7 \u2211 j\u2208N vj ,\nThis expression represents the difference between the individual contribution of agent i, vi, and the average546 individual contribution of all other agents. The second term is independent of i and remains constant across all547 agents.548\n2see, e.g., Covert, I. and Lee, S.I., 2020. Improving kernelshap: Practical shapley value estimation via linear regression. arXiv preprint arXiv:2012.01536.\nDerivation of equivalent clustering objective. We now consider the optimization problem defined by Equation 4, which defines optimal cluster assignments u\u2217 such that the variance in EVs is maximised\nu\u2217 \u2208 argmaxuVar([E\u0303V 0(G\u0303), . . . , E\u0303V n\u22121(G\u0303)]).\nFurther, the clustered value function is defined as v\u0303(C) = 1/\u03b7 \u00b7 \u2211n\u22121\nm=0 \u2211 \u03c0\u2208\u03a0N v(S\u03c0(m)) \u00b7 1({uj | j \u2208 S\u03c0(m)} = C),\nwhere the normalisation constant is defined as \u03b7 = \u2211n\u22121\nm=0 \u2211 \u03c0\u2208\u03a0N\n1({uj | j \u2208 S\u03c0(m)} = C). We denote by549 ki the individual contribution of the agent that represents the agents in cluster i. The value ki is defined as the550 average individual contribution of all agents assigned to the cluster, i.e. ki = 1/\u03f5 \u00b7 \u2211 j\u2208Nvj \u00b7 1(u(i) = u(j)).551\nHere, the normalization constant is given as \u03f5 = \u2211\nj\u2208N 1(u(i) = u(j)).552\nUsing the concept of the clustered value function v\u0303, we can express the EV for all agents assigned cluster i as EV i(G\u0303) = (1 + 1/|K|\u22121) \u00b7 ki \u2212 1/|K|\u22121 \u00b7 \u2211 j\u2208K kj .\nThe second term, which is cluster-independent, can be omitted when computing the variance553 Var([EV 0(G\u0303), . . . , EV n\u22121(G\u0303)]), as the variance is agnostic to a shift in the data distribution. We will omit554 the scaling factor (1 + 1/|K|\u22121) from here onwards.555\nLet nj denote the number of agents assigned to cluster j \u2208 K, with \u2211K\u22121\ni=0 ni = N . By simplifying Equation 4, we obtain:\nVar([EV 0(G\u0303), . . . , EV n\u22121(G\u0303)]) = K\u22121\u2211 i=0 ni \u00b7 ( ki \u2212 \u2211K\u22121 j=0 nj \u00b7kj/N )2 .\nThis allows us to express the objective stated in Equation 4 as\nu\u2217 \u2208 argmaxuVar([k0, . . . , kn\u22121]).\nThe objective stated in Equation 4 is therefore equivalent to assigning agents to clusters such that the variance556 in cluster centroids (centroids computed as the mean of the unobserved individual contributions vi of all agents557 assigned to a given cluster) is maximized.558\n\u03bb, which determines the level of adversarial or optimal actions for a given agent. A value of \u03bb = 1 represents562 a policy that always chose the best action with certainty. As \u03bb decreases, agents are more likely to select non-563 optimal actions. For \u03bb < 0, we invert the cost function to create agents with adversarial behavior. Notably,564 we assign a high cost (or low cost when inverted) to occupying the cell next to the counter in the Overcooked565 environment. Occupying the cell next to the counter enables adversarial agents to block other agents in the566 execution of their tasks.567\nFor human gameplay datasets, we utilized the raw versions of the Overcooked datasets.4 These datasets were568 used as-is, without manual pre-filtering.569\nWe introduce an additional modified version of the Overcooked environment in which agents can take an570 additional action that lights the kitchen on fire with a predefined probability, resulting in an episode reward of571 \u2212200; we refer to this environment as Overcooked+Fire and evaluate on equivalently created datasets Dadv and572 D\u03bb.573\nEVs. To estimate agents\u2019 EVs according to Section 4.2, we used either the full set of all possible groups or a574 fraction of it (see Figure 3 for the relationship between dataset size and EV estimation error). For each observed575 grouption, we conducted 10 rollouts in the environment and calculated the average score across these rollouts576 to account for stochasticity in the environment.577\nImitation learning. For EV2BC, BC, and group-BC, we used the implementation of Behavior Cloning in578 Overcooked as given by the authors of (Carroll et al., 2019)5. We implement the offline multi-agent reinforce-579 ment learning method OMAR (Pan et al., 2022) using the author\u2019s implementation.6 For the OMAR baseline,580 we set the reward at the last timestep to the DVF\u2019s score for a given trajectory, as our work assumes that no per-581 step reward signal is given, in contrast to the standard offline-RL framework. We conducted a hyperparameter582 sweep for the following parameters: learning rate with options {0.01, 0.001, 0.0001}, Omar-coe with options583 {0.1, 1, 10}, Omar-iters with options {1, 3, 10}, and Omar-sigma with options {1, 2, 3}. The best-performing584 parameters were selected based on the evaluation results.585\nImplementation of Overcooked+Fire. We introduce an additional adversarial action, \u201clight kitchen on586 fire,\u201d to the environment. To account for this action in the planning algorithm, we assign it the highest possible587 cost. Taking this action had a 50% chance of resulting in an episode return of \u2212200, regardless of the other588 agent\u2019s performance.589\nB.1 CLUSTERING OF AGENTS IN OVERCOOKED590\nBehavior clustering. The behavior clustering process in the Overcooked environment involves the follow-591 ing steps. Initially, we identify the 200 states that are most frequently visited by all agents in the given set592 of observations. As the action space in Overcooked is relatively small (\u2264 7 actions), we calculate the empir-593 ical action distribution for each state for every agent. These 200 action distributions are then concatenated to594 form a behavior embedding for each agent. To reduce the dimensionality of the embedding, we apply Principal595 Component Analysis (PCA), transforming the initial embedding space into three dimensions. Subsequently, we596 employ the k-means clustering algorithm to assign agents to behavior clusters. The number of clusters (7 for597 Overcooked) is determined using the ELBOW method (Thorndike, 1953), while linear kernels are utilized for598 both PCA and k-means. It is noteworthy that the results are found to be relatively insensitive to the parameters599 used in the dimensionality reduction and clustering steps, thus standard implementations are employed for both600 methods (Pedregosa et al., 2011). Importantly, this clustering procedure focuses exclusively on the observed601 behavior of agents, specifically the actions taken in specific states, and is independent of the scores assigned to602 trajectories by the DVF.603\nEV-Clustering. In contrast to behavior clustering, EV-Clustering (see Section 4.2.1) focuses solely on the604 scores assigned to trajectories by the DVF and disregards agent behavior. The objective of variance clustering605 is to maximize the variance in assigned EVs, as stated in Equation 4. To optimize this objective, we utilize the606 SLSQP non-linear constrained optimization intrroduced by Kraft (1988).607\nWe use soft cluster assignments and enforce constraints to ensure that the total probability is equal to one for608 each agent. The solver is initialized with a uniform distribution and runs until convergence or for a maximum of609 100 steps. Given that the optimization problem may have local minima, we perform 500 random initializations610 and optimizations, selecting the solution with the lowest loss (i.e. the highest variance in assigned EVs).611\n4https://github.com/HumanCompatibleAI/human_aware_rl/tree/master/human_ aware_rl/data/human/anonymized\n5https://github.com/HumanCompatibleAI/overcooked_ai/tree/master/src/ human_aware_rl/imitation\n6https://github.com/ling-pan/OMAR\nCombining Behavior Clustering and EV Clustering. As described in Sections 4.2.2 and 5.1, behavior612 clustering (which utilizes behavior information but disregards DVF scores) and variance clustering (which613 utilizes DVF scores but disregards behavior information) are combined to estimate EVs for degenerate datasets.614 We initialize the SLSQP solver with the cluster assignments obtained from behavior clustering and introduce a615 small loss term in the objective function of Equation 4. This additional loss term, weighted by 0.1 (selected in616 a small sensitivity analysis), penalizes deviations from the behavior clusters. Similar to before, we perform 500617 iterations while introducing a small amount of noise to the initial cluster assignments at each step. The solution618 with the highest variance in assigned EVs is then selected.619\nAblation study. We present an ablation study to examine the impact of different components in the cluster-620 ing approach discussed in Section 5.1. We proposed two sequential clustering methods: behavior clustering and621 variance clustering. This ablation study investigates the performance of both clustering steps when performed622 independently, also under the consideration of the fraction of the data that is observed. We assess performance623 as the within-cluster variance in the unobserved agent-specific latent trait variable \u03bb, where lower within-cluster624 variance indicates higher performance. It is important to note that \u03bb is solely used for evaluating the clustering625 steps and not utilized during the clustering process. The results of the ablation study are depicted in Figure 6.626\nWe first discuss EV-Clustering. EV-Clustering as introduced in Seciton 4 generally leads to a significant de-627 crease in within-cluster variance in the unobserved variable \u03bb. More specifically, the proposed variance clus-628 tering approach (when 50% of data is observed), results in a \u223c 89% reduction of the within-cluster variance629 in \u03bb, which validates the approach of clustering agents by their unobserved individual contributions by maxi-630 mizing the variance in estimated EVs. However, we observe in Figure 6 that, as the fraction of observed data631 decreases, the within-cluster variance increases, indicating a decrease in the quality of clustering. The highest632 within-cluster variance is observed when using only a single observation (\u2019single-obs\u2019), which corresponds to633 a fully-anonymized dataset. This finding is consistent with the fact that a fully-anonymized dataset presents a634 degenerate credit assignment problem, as discussed in Section 4.2.2.635\nWe now discuss behavior clustering. Figure 6 shows that behavior clustering generally results in a very low636 within-cluster variance. However, it is important to note that these results may not directly translate to real-637 world data, as the ablation study uses simulated trajectories. Note that such an ablation study cannot be con-638 ducted for the given real-world human datasets, as these are fully anonymized. In Section 5.1, we demonstrate639 that behavior clustering alone may not be sufficient for fully-anonymized real-world human datasets. Instead,640 a combination of both behavior clustering and variance clustering yields superior results.641\nAdditional results for non-fully anonymized datasets. While the results presented in Section 5.2642 were obtained for fully-anonymised datasets, we ran the same evaluations also for the simulated datasets where643 30% possible groups are observed. As it can be seen in Table 4, EV2BC outperforms baseline approaches by644 an even larger margin.645\nTable 4: Results for 30% of coalitions observed in Overcooked\nImitation method Cramped Room D\u03bb Coordination Ring D\u03bb Cramped Room Dadv Coordination Ring Dadv BC 12.6\u00b1 3.34 18.13\u00b1 6.21 31.7\u00b1 8.96 14.21\u00b1 3.78 Group-BC 64.33\u00b1 6.1 29.4\u00b1 7.01 75.7\u00b1 13.98 16.8\u00b1 5.66 EV-BC (ours) 101.33 \u00b1 14.37 38.3 \u00b1 6.81 138.8 \u00b1 18.6 22.0 \u00b1 6.1\nT ak\ne 1\nT ak\ne 3\nT ak\ne 10\nvfinal\nT ak\ne 1\nxdp l. T ak e 3 xdp l. T ak e 10 xdp l. T ak e 1% T ak e 3% T ak e 10 % T ak e av g. \u22123 T ak e av g. T ak e av g. + 3\nvtotal\nvmin\nFigure 7: Colour-coded ordering of EVs for agents with varying behaviors in Tragedy of the Commons. The brighter, the higher an agent\u2019s contribution to a given value function.\nC TRAGEDY OF THE COMMONS EXPERIMENTS646\nWe model ToC as a multi-agent environment where agents consume from a common pool of resources xt,647 which grows at a fixed rate g = 25% at each time step t: xt+1 = max ( (1 + g) \u00b7 xt \u2212 \u2211 icti, 0 ) , with648 cti as the consumption of the ith agent at time t and x0 = 200 as the starting pool. Hence, if all resources649 are consumed, none can regrow and no agents can consume more resources. The Tragedy of the Commons650 (ToC) environment features 4 different behavior patterns: Take-X consumes X units at every timestep, Take-X-651 x-dpl consumes X units if this does not deplete the pool of resources, Take X% consumes X% of the available652 resources, and TakeAvg consumes the average of the resources consumed by the other agents at the previous653 timestep (0 in the first timestep). For the small-scale experiment of 12 agents, we consider three agents for654 each pattern, with X values selected from the set 1, 3, 10. For the large-scale experiment of 120 agents, we655 simply replicate each agent configuration 10 times. We simulate both experiments for groups of size 3 and 10656 respectively. We generate a simulated dataset using agents with four different behavior patterns. We first collect657 a dataset of observations for a small-scale experiment of 12 agents and simulate ToC for groups of three agents658 for 50 time steps (we later consider a group of 120 agents).659\nDue to the continuous nature of the state and action spaces in ToC, we first discretize both and then apply the660 same clustering methods used in the Overcooked scenario. We proceed by computing EVs for all agents as661 done in Overcooked (see Figure 3 for results). We implement imitation policies by replicating the averaged662 action distributions in the discretized states.663\nD COMPUTATIONAL DEMAND AND REPRODUCIBILITY664\nWe used an Intel(R) Xeon(R) Silver 4116 CPU and an NVIDIA GeForce GTX 1080 Ti (only for training665 BC, EV2BC, group-BC, and OMAR policies). In Overcooked, generating a dataset took a maximum of three666 hours, and estimating EVs from a given dataset takes a few seconds. Behavior clustering consumes a couple667 of minutes, while Variance clustering took up to two hours per configuration (note that it is run 500 times).668 Training of the BC, group-BC, and EV2BC policies took no more than 30 minutes (using a GPU), while the669 OMAR baseline was trained for up to 2 hours. In Tragedy of Commons, each rollout only consumes a couple of670 seconds. Clustering times were comparable to those in Overcooked. Computing imitation policies is similarly671 only a matter of a few minutes.672\nAs this submission is public, we will release the code for all experiments with the camera-ready version.673"
        }
    ],
    "year": 2023
}