{
    "abstractText": "ML-augmented algorithms utilize predictions to achieve performance beyond their worst-case bounds. Producing these predictions might be a costly operation \u2013 this motivated Im et al. (2022) to introduce the study of algorithms which use predictions parsimoniously. We design parsimonious algorithms for caching and MTS with action predictions, proposed by Antoniadis et al. (2023), focusing on the parameters of consistency (performance with perfect predictions) and smoothness (dependence of their performance on the prediction error). Our algorithm for caching is 1-consistent, robust, and its smoothness deteriorates with the decreasing number of available predictions. We propose an algorithm for general MTS whose consistency and smoothness both scale linearly with the decreasing number of predictions. Without the restriction on the number of available predictions, both algorithms match the earlier guarantees achieved by Antoniadis et al. (2023).",
    "authors": [],
    "id": "SP:51e8ee131dbf767b0f47e73c4603ffff821c85c1",
    "references": [
        {
            "authors": [
                "A. Anderson",
                "R. Kumar",
                "A. Tomkins",
                "S. Vassilvitskii"
            ],
            "title": "The dynamics of repeat consumption",
            "venue": "Proceedings of conference World Wide Web \u201914, pages 419\u2013430",
            "year": 2014
        },
        {
            "authors": [
                "A. Antoniadis",
                "T. Gouleakis",
                "P. Kleer",
                "P. Kolev"
            ],
            "title": "Secretary and online matching problems with machine learned advice",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "authors": [
                "A. Antoniadis",
                "C. Coester",
                "M. Eli\u00e1\u0161",
                "A. Polak",
                "B. Simon"
            ],
            "title": "Learning-augmented dynamic power management with multiple states via new ski rental bounds",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "A. Antoniadis",
                "J. Boyar",
                "M. Eli\u00e1\u0161",
                "L.M. Favrholdt",
                "R. Hoeksma",
                "K.S. Larsen",
                "A. Polak"
            ],
            "title": "and B",
            "venue": "Simon. Paging with succinct predictions",
            "year": 2022
        },
        {
            "authors": [
                "A. Antoniadis",
                "P.J. Ganje",
                "G. Shahkarami"
            ],
            "title": "A novel prediction setup for online speedscaling",
            "venue": "SWAT, volume 227 of LIPIcs, pages 9:1\u20139:20. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik",
            "year": 2022
        },
        {
            "authors": [
                "A. Antoniadis",
                "C. Coester",
                "M. Eli\u00e1\u0161",
                "A. Polak",
                "B. Simon"
            ],
            "title": "Online metric algorithms with untrusted predictions",
            "venue": "ACM Trans. Algorithms,",
            "year": 2023
        },
        {
            "authors": [
                "Y. Azar",
                "S. Leonardi",
                "N. Touitou"
            ],
            "title": "Flow time scheduling with uncertain processing time",
            "venue": "Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2021, page 1070\u20131080, New York, NY, USA",
            "year": 2021
        },
        {
            "authors": [
                "Y. Azar",
                "S. Leonardi",
                "N. Touitou"
            ],
            "title": "Distortion-oblivious algorithms for minimizing flow time",
            "venue": "Proceedings of the 2022 ACM-SIAM Symposium on Discrete Algorithms, SODA 2022, Virtual Conference / Alexandria, VA, USA, January 9 - 12, 2022, pages 252\u2013274. SIAM",
            "year": 2022
        },
        {
            "authors": [
                "\u00c9. Bamas",
                "A. Maggiori",
                "L. Rohwedder",
                "O. Svensson"
            ],
            "title": "Learning augmented energy minimization via speed scaling",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "authors": [
                "N. Bansal",
                "C. Coester",
                "R. Kumar",
                "M. Purohit",
                "E. Vee"
            ],
            "title": "Learning-augmented weighted paging",
            "venue": "SODA",
            "year": 2022
        },
        {
            "authors": [
                "Y. Bartal",
                "B. Bollob\u00e1s",
                "M. Mendel"
            ],
            "title": "Ramsey-type theorems for metric spaces with applications to online problems",
            "venue": "J. Comput. Syst. Sci., 72(5):890\u2013921",
            "year": 2006
        },
        {
            "authors": [
                "L.A. Belady"
            ],
            "title": "A study of replacement algorithms for virtual-storage computer",
            "venue": "IBM Syst. J., 5 (2):78\u2013101",
            "year": 1966
        },
        {
            "authors": [
                "G. Bernardini",
                "A. Lindermayr",
                "A. Marchetti-Spaccamela",
                "N. Megow",
                "L. Stougie",
                "M. Sweering"
            ],
            "title": "A universal error measure for input predictions applied to online graph problems",
            "venue": "CoRR, abs/2205.12850",
            "year": 2022
        },
        {
            "authors": [
                "A. Blum",
                "C. Burch"
            ],
            "title": "On-line learning and the metrical task system problem",
            "venue": "Mach. Learn., 39(1):35\u201358",
            "year": 2000
        },
        {
            "authors": [
                "H. B\u00f6ckenhauer",
                "D. Komm",
                "R. Kr\u00e1lovi\u010d",
                "R. Kr\u00e1lovi\u010d",
                "T. M\u00f6mke"
            ],
            "title": "Online algorithms with advice: The tape model",
            "venue": "Inf. Comput., 254:59\u201383",
            "year": 2017
        },
        {
            "authors": [
                "A. Borodin",
                "R. El-Yaniv"
            ],
            "title": "Online computation and competitive analysis",
            "venue": "Cambridge University Press",
            "year": 1998
        },
        {
            "authors": [
                "A. Borodin",
                "N. Linial",
                "M.E. Saks"
            ],
            "title": "An optimal on-line algorithm for metrical task system",
            "venue": "J. ACM, 39(4):745\u2013763",
            "year": 1992
        },
        {
            "authors": [
                "J. Boyar",
                "L.M. Favrholdt",
                "C. Kudahl",
                "K.S. Larsen",
                "J.W. Mikkelsen"
            ],
            "title": "Online Algorithms with Advice: A Survey",
            "venue": "ACM Computing Surveys, 50(2):1\u201334",
            "year": 2017
        },
        {
            "authors": [
                "J. Boyar",
                "L.M. Favrholdt",
                "K.S. Larsen"
            ],
            "title": "Online unit profit knapsack with untrusted predictions",
            "venue": "SWAT, volume 227 of LIPIcs, pages 20:1\u201320:17. Schloss Dagstuhl - Leibniz- Zentrum f\u00fcr Informatik",
            "year": 2022
        },
        {
            "authors": [
                "S. Bubeck",
                "M.B. Cohen",
                "J.R. Lee",
                "Y.T. Lee"
            ],
            "title": "Metrical task systems on trees via mirror descent and unfair gluing",
            "venue": "Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2019, pages 89\u201397",
            "year": 2019
        },
        {
            "authors": [
                "J. Chen",
                "S. Silwal",
                "A. Vakilian",
                "F. Zhang"
            ],
            "title": "Faster fundamental graph algorithms via learned predictions",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "J. Chledowski",
                "A. Polak",
                "B. Szabucki",
                "K.T. Zolna"
            ],
            "title": "Robust learning-augmented caching: An experimental study",
            "venue": "M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 1920\u20131930. PMLR",
            "year": 2021
        },
        {
            "authors": [
                "E. Cho",
                "S.A. Myers",
                "J. Leskovec"
            ],
            "title": "Friendship and mobility: user movement in location-based social networks",
            "venue": "Proceedings of SIGKDD\u201911, pages 1082\u20131090",
            "year": 2011
        },
        {
            "authors": [
                "M. Dinitz",
                "S. Im",
                "T. Lavastida",
                "B. Moseley",
                "S. Vassilvitskii"
            ],
            "title": "Faster matchings via learned duals",
            "venue": "NeurIPS, pages 10393\u201310406",
            "year": 2021
        },
        {
            "authors": [
                "S. Dobrev",
                "R. Kr\u00e1lovi\u010d",
                "D. Pardubsk\u00e1"
            ],
            "title": "Measuring the problem-relevant information in input",
            "venue": "RAIRO - Theor. Inf. Appl., 43(3):585\u2013613",
            "year": 2009
        },
        {
            "authors": [
                "M. Drygala",
                "S.G. Nagarajan",
                "O. Svensson"
            ],
            "title": "Online algorithms with costly predictions",
            "venue": "Proceedings of The 26th International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "P. D\u00fctting",
                "S. Lattanzi",
                "R.P. Leme",
                "S. Vassilvitskii"
            ],
            "title": "Secretaries with advice",
            "venue": "EC, pages 409\u2013429. ACM",
            "year": 2021
        },
        {
            "authors": [
                "F. Eberle",
                "A. Lindermayr",
                "N. Megow",
                "L. N\u00f6lke",
                "J. Schl\u00f6ter"
            ],
            "title": "Robustification of online graph exploration methods",
            "venue": "AAAI, pages 9732\u20139740. AAAI Press",
            "year": 2022
        },
        {
            "authors": [
                "Y. Emek",
                "P. Fraigniaud",
                "A. Korman",
                "A. Ros\u00e9n"
            ],
            "title": "Online computation with advice",
            "venue": "Automata, Languages and Programming, 36th International Colloquium, ICALP 2009, Proceedings, Part I, volume 5555 of Lecture Notes in Computer Science, pages 427\u2013438. Springer",
            "year": 2009
        },
        {
            "authors": [
                "J.C. Ergun",
                "Z. Feng",
                "S. Silwal",
                "D.P. Woodruff",
                "S. Zhou"
            ],
            "title": "Learning-augmented k-means clustering",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022. OpenReview.net",
            "year": 2022
        },
        {
            "authors": [
                "A. Fiat",
                "R.M. Karp",
                "M. Luby",
                "L.A. McGeoch",
                "D.D. Sleator",
                "N.E. Young"
            ],
            "title": "Competitive paging algorithms",
            "venue": "Journal of Algorithms, 12",
            "year": 1016
        },
        {
            "authors": [
                "A. Gupta",
                "D. Panigrahi",
                "B. Subercaseaux",
                "K. Sun"
            ],
            "title": "Augmenting online algorithms with \\varepsilon-accurate predictions",
            "venue": "S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 2115\u20132127. Curran Associates, Inc.",
            "year": 2022
        },
        {
            "authors": [
                "J. Hromkovi\u010d",
                "R. Kr\u00e1lovi\u010d",
                "R. Kr\u00e1lovi\u010d"
            ],
            "title": "Information complexity of online problems",
            "venue": "MFCS, volume 6281 of LNCS, pages 24\u201336. Springer",
            "year": 2010
        },
        {
            "authors": [
                "S. Im",
                "R. Kumar",
                "M.M. Qaem",
                "M. Purohit"
            ],
            "title": "Online knapsack with frequency predictions",
            "venue": "NeurIPS, pages 2733\u20132743",
            "year": 2021
        },
        {
            "authors": [
                "S. Im",
                "R. Kumar",
                "A. Petety",
                "M. Purohit"
            ],
            "title": "Parsimonious learning-augmented caching",
            "venue": "ICML",
            "year": 2022
        },
        {
            "authors": [
                "P. Indyk",
                "F. Mallmann-Trenn",
                "S. Mitrovic",
                "R. Rubinfeld"
            ],
            "title": "Online page migration with ml advice",
            "venue": "Proceedings of The 25th International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "A. Jain",
                "C. Lin"
            ],
            "title": "Back to the future: Leveraging belady\u2019s algorithm for improved cache replacement",
            "venue": "SIGARCH Comput. Archit. News,",
            "year": 2016
        },
        {
            "authors": [
                "Z. Jiang",
                "D. Panigrahi",
                "K. Sun"
            ],
            "title": "Online algorithms for weighted paging with predictions",
            "venue": "ACM Trans. Algorithms, 18(4):39:1\u201339:27",
            "year": 2022
        },
        {
            "authors": [
                "T. Kraska",
                "A. Beutel",
                "E.H. Chi",
                "J. Dean",
                "N. Polyzotis"
            ],
            "title": "The case for learned index structures",
            "venue": "Proceedings of SIGMOD\u201918, pages 489\u2013504",
            "year": 2018
        },
        {
            "authors": [
                "S. Lattanzi",
                "T. Lavastida",
                "B. Moseley",
                "S. Vassilvitskii"
            ],
            "title": "Online scheduling via learned weights",
            "venue": "Proceedings of the 2020 ACM-SIAM Symposium on Discrete Algorithms, SODA 2020, pages 1859\u20131877. SIAM",
            "year": 2020
        },
        {
            "authors": [
                "A. Lindermayr",
                "N. Megow"
            ],
            "title": "Permutation predictions for non-clairvoyant scheduling",
            "venue": "SPAA, pages 357\u2013368. ACM",
            "year": 2022
        },
        {
            "authors": [
                "A. Lindermayr",
                "N. Megow"
            ],
            "title": "Algorithms with predictions",
            "venue": "https: //algorithms-with-predictions.github.io",
            "year": 2022
        },
        {
            "authors": [
                "A. Lindermayr",
                "N. Megow",
                "B. Simon"
            ],
            "title": "Double coverage with machine-learned advice",
            "venue": "M. Braverman, editor, 13th Innovations in Theoretical Computer Science Conference, ITCS 2022, January 31 - February 3, 2022, Berkeley, CA, USA, volume 215 of LIPIcs, pages 99:1\u201399:18. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik",
            "year": 2022
        },
        {
            "authors": [
                "E. Liu",
                "M. Hashemi",
                "K. Swersky",
                "P. Ranganathan",
                "J. Ahn"
            ],
            "title": "An imitation learning approach for cache replacement",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "T. Lykouris",
                "S. Vassilvitskii"
            ],
            "title": "Competitive caching with machine learned advice",
            "venue": "J. ACM, 68 (4):24:1\u201324:25",
            "year": 2021
        },
        {
            "authors": [
                "M. Mitzenmacher",
                "S. Vassilvitskii"
            ],
            "title": "Algorithms with predictions",
            "venue": "Beyond the Worst-Case Analysis of Algorithms, pages 646\u2013662. Cambridge University Press",
            "year": 2020
        },
        {
            "authors": [
                "A. Polak",
                "M. Zub"
            ],
            "title": "Learning-augmented maximum flow",
            "venue": "CoRR, abs/2207.12911",
            "year": 2022
        },
        {
            "authors": [
                "M. Purohit",
                "Z. Svitkina",
                "R. Kumar"
            ],
            "title": "Improving online algorithms via ML predictions",
            "venue": "NeurIPS",
            "year": 2018
        },
        {
            "authors": [
                "D. Rohatgi"
            ],
            "title": "Near-optimal bounds for online caching with machine learned advice",
            "venue": "SODA",
            "year": 2020
        },
        {
            "authors": [
                "S. Sakaue",
                "T. Oki"
            ],
            "title": "Discrete-convex-analysis-based framework for warm-starting algorithms with predictions",
            "venue": "CoRR, abs/2205.09961",
            "year": 2022
        },
        {
            "authors": [
                "Z. Shi",
                "X. Huang",
                "A. Jain",
                "C. Lin"
            ],
            "title": "Applying deep learning to the cache replacement problem",
            "venue": "Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture, MICRO \u201952, page 413\u2013425, New York, NY, USA",
            "year": 2019
        },
        {
            "authors": [
                "D.D. Sleator",
                "R.E. Tarjan"
            ],
            "title": "Amortized efficiency of list update and paging rules",
            "venue": "Commun. ACM, 28(2):202\u2013208",
            "year": 1985
        },
        {
            "authors": [
                "A. Wei"
            ],
            "title": "Better and simpler learning-augmented online caching",
            "venue": "APPROX/RANDOM",
            "year": 2020
        },
        {
            "authors": [
                "A. Zeynali",
                "B. Sun",
                "M.H. Hajiesmaili",
                "A. Wierman"
            ],
            "title": "Data-driven competitive algorithms for online knapsack and set cover",
            "venue": "AAAI, pages 10833\u201310841. AAAI Press",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Caching, introduced by Sleator and Tarjan (1985), is a fundamental problem in online computation important both in theory and practice. Here, we have a fast memory (cache) which can contain up to k different pages and we receive a sequence of requests to pages in an online manner. Whenever a page is requested, it needs to be loaded in the cache. Therefore, if the requested page is already in the cache, it can be accessed at no cost. Otherwise, we suffer a page fault: we have to evict one page from the cache and load the requested page in its place. The page to evict is to be chosen without knowledge of the future requests and our target is to minimize the total number of page faults.\nCaching is a special case of Metrical Task Systems introduced by Borodin et al. (1992) as a generalization of many fundamental online problems. In the beginning, we are given a metric space M of states which can be interpreted as actions or configurations of some system. We start at a predefined state x0 \u2208 M . At time steps t = 1, 2, . . . , we receive a cost function \u2113t : M \u2192 R+ \u222a {0,+\u221e} and we need to make a decision: either to stay at xt\u22121 and pay a cost \u2113t(xt\u22121), or to move to another, possibly cheaper state xt and pay \u2113t(xt) + d(xt\u22121, xt), where the distance d(xt\u22121, xt) represents the transition cost between states xt\u22121 and xt.\nThe online nature of both caching and MTS forces an algorithm to make decisions without knowledge of the future which leads to very suboptimal results in the worst case (Borodin et al., 1992; Sleator and Tarjan, 1985). A recently emerging field of learning-augmented algorithms, introduced in seminal papers by Kraska et al. (2018) and Lykouris and Vassilvitskii (2021), investigates approaches to improve the performance of algorithms using predictions, possibly generated by some ML model. In general, no guarantee on the accuracy of these predictions is assumed. Therefore, the performance of learning-augmented algorithms is usually evaluated using the following three parameters:\nConsistency. Performance with perfect predictions, preferably close to optimum.\nRobustness. Performance with very bad predictions, preferably no worse than what is achievable by known algorithms which do not utilize predictions.\nSmoothness. Algorithm\u2019s performance should deteriorate smoothly with increasing prediction error between the consistency and robustness bound.\nThese three parameters express a desire to design algorithms that work very well when receiving reasonably accurate predictions most of the time and, in the rest of the cases, still satisfy state-of-\nthe-art worst-case guarantees. See the survey by Mitzenmacher and Vassilvitskii (2020) for more information.\nProducing predictions is often a computationally intensive task, therefore it is interesting to understand the interplay between the number of available predictions and the achievable performance. In their inspiring work, Im et al. (2022) initiated the study of learning-augmented algorithms which use the predictions parsimoniously. In their work, they study caching with next-arrival-time predictions introduced by Lykouris and Vassilvitskii (2021). Their algorithm uses O(b logb+1 k)OPT predictions, where OPT is the number of page faults incurred by the offline optimal solution and b \u2208 N is a parameter. It achieves smoothness linear in the prediction error. It satisfies tight consistency bounds: with perfect predictions, it incurs at most O(logb+1 k)OPT page faults and no algorithm can do better. In other words, it achieves a constant competitive ratio with unrestricted access to predictions (b = k) and, with b a small constant, its competitive ratio deteriorates to O(log k) which is comparable to the best competitive ratio achievable without predictions. One of their open questions is whether a similar result could be proved for MTS.\nIn this paper, we study parsimonious algorithms for MTS working with action predictions which were introduced by Antoniadis et al. (2023). Here, each prediction describes the state of an optimal algorithm at the given time step and its error is defined as the distance from the actual state of the optimal algorithm. The total prediction error is the sum of errors of the individual predictions. In the case of caching, action predictions have a very concise representation, see Section 2.1. Unlike next-arrival-time predictions, action predictions can be used for any MTS. Using the method of Blum and Burch (2000), it is easy to achieve near-optimal robustness for any MTS losing only a factor (1 + \u03f5) in consistency and smoothness. Therefore, we study how the reduced number of predictions affects the consistency and smoothness parameters. We consider the following two regimes.\nBounded number of predictions: The algorithm can request a prediction whenever it prefers as far as the total number of requested predictions is bounded by bOPT, where b is a parameter. This regime is similar to Im et al. (2022).\nWell-separated queries to the predictor: The queries to the predictor need to be separated by at least a time steps, for some parameter a. This captures the situation when producing each prediction takes more than one time step."
        },
        {
            "heading": "1.1 OUR RESULTS",
            "text": "We evaluate the algorithm\u2019s performance using competitive ratio which is, roughly speaking, the worst-case ratio between the cost incurred by the algorithm and the cost of the offline optimum, see Section 2 for a formal definition. We say that an algorithm achieves consistency \u03b1 and robustness \u03b2 if its competitive ratio is at most \u03b1 when provided with perfect predictions and at most \u03b2 with arbitrarily bad predictions. For a given function g, we call an algorithm g(\u03b7)-smooth if its competitive ratio is at most g(\u03b7) whenever provided with predictions with the total error at most \u03b7.\nOur first contribution is an algorithm for caching which receives action predictions describing the states of the optimal offline algorithm Belady proposed by Belady (1966). High quality such predictor based on imitation learning was already designed by Liu et al. (2020). Its empirical evaluation within existing algorithms designed for action predictions was performed by Chledowski et al. (2021). Theorem 1.1. Let f be an increasing convex function such that f(0) = 0 and f(i) \u2264 2i \u2212 1 for each i \u2265 0. There is an algorithm for caching requiring O(f(log k))OPT predictions which achieves consistency 1, robustness O(log k), and smoothness O(f\u22121(\u03b7/OPT )), where \u03b7 denotes the total prediction error with respect to Belady and OPT is the number of page faults of Belady.\nIn fact, the number of required predictions is slightly smaller than what is stated in the theorem. Table 1 shows numbers of predictions and achieved smoothness for some natural choices of f . Already with O( \u221a k)OPT predictions, our bounds are comparable to Antoniadis et al. (2023) whose algorithm asks for a prediction in every step, its consistency is constant and its smoothness is logarithmic in \u03b7. The algorithm also works with f(i) = 0. In that case, it asks for at most 2OPT predictions and still remains 1-consistent. However, its smoothness is not very good. We use sliding marking phases and a careful distribution of queries of the predictor over the time horizon. This allows us to avoid dealing with so called \"ancient\" pages considered by Rohatgi (2020) and Antoniadis et al. (2023), resulting in an algorithm with better consistency and a simpler analysis.\nWe discuss tightness of our bounds in Section 7 in the full version of this paper (see Appendix). We show that with, for example, only 0.5OPT available predictions, no algorithm can be better than O(log k)-competitive \u2013 a guarantee comparable to the best classical online algorithms without predictions. We also show that the number of predictions used by our algorithm is close to optimal. Theorem 1.2. Let f be an increasing function. Any f(\u03b7)-smooth algorithm for caching with action predictions, i.e., an algorithm whose competitive ratio with predictions of error \u03b7 is f\u22121(\u03b7) for any \u03b7 > 0, has to use at least f(ln k)OPT predictions.\nFor general MTS, we cannot bound the number of used predictions as a function of OPT. The reason is that any instance of MTS can be scaled to make OPT arbitrarily small, allowing us to use only very few predictions. We propose an algorithm which queries the predictor once in every a time steps, making at most T/a queries in total, where T denotes the length of the input sequence. Theorem 1.3. There is a deterministic algorithm for any MTS which receives a prediction only once per each a time steps and its cost is at most O(a) \u00b7 (OFF+2\u03b7), where OFF denotes the cost of an arbitrary offline algorithm and \u03b7 the error of predictions with respect to this algorithm.\nThis is a more general statement than Theorem 1.1 which requires OFF to be Belady. Considering any offline optimal algorithm OFF, Theorem 1.3 implies a smoothness O(a) \u00b7 (1 + 2\u03b7/OPT) and consistency O(a). Our algorithm is based on work functions. For a = 1, its smoothness is 1 + 2\u03b7/OFF, see Section 4, which improves upon the smoothness bound of 1 + 4\u03b7/OFF by Antoniadis et al. (2023). It is not robust on its own. However, it can be combined with any online algorithm for the given MTS using the result of Blum and Burch (2000) achieving robustness comparable to that algorithm and losing only a factor of (1 + \u03f5) in smoothness and consistency.\nNo algorithm receiving a prediction only once in a time steps can be o(a)-consistent. This follows from the work of Emek et al. (2009) on advice complexity, see Section 7 of the full version (in Appendix) for more details. The same can be shown for smoothness by modifying the lower bound construction of Antoniadis et al. (2023). Theorem 1.4. There is no o(a\u03b7/OPT)-smooth algorithm for MTS with action predictions which receives predictions only once in a time steps.\nWe can modify our algorithm for caching to ensure that the moments when the predictions are queried are separated by at least a time steps, not losing too much of its performance. Theorem 1.5. There is an algorithm for caching which receives prediction at most once in a \u2264 k time steps and using at most O(f(log k))OPT predictions in total which is O(1)-consistent, O(log k)robust and O(f\u22121(a\u03b7/OPT))-smooth.\nIn Section 5, we provide empirical results suggesting that our algorithm\u2019s performance can be comparable to the performance of algorithms imposing no limitations on their use of predictions. Our algorithm may therefore be useful especially with heavy-weight predictors like (Liu et al., 2020)."
        },
        {
            "heading": "1.2 RELATED WORK",
            "text": "The most related work is by Im et al. (2022), who studied caching with next arrival time predictions. A smaller number of predictions affects the consistency of their algorithm: with b(log k/ log b)OPT predictions, they achieve consistency O(log k/ log b) and they show that this is tight. They also show that their algorithm achieves linear smoothness. In contrast, our algorithm is 1-consistent when receiving at least OPT predictions. This demonstrates that action predictions, although not containing more bits, seem to contain useful information about the input instance in a more condensed form. See (Antoniadis et al., 2023) for comparison and connections between these prediction setups. Drygala et al. (2023) study ski rental and bahncard problems with predictions of a fixed cost.\nThere are several other papers on caching with predictions, including (Lykouris and Vassilvitskii, 2021; Rohatgi, 2020; Wei, 2020; Emek et al., 2009; Antoniadis et al., 2023; 2022) which design algorithms asking for a prediction at each time step. Consistency parameters achieved by these algorithms are constants greater than 1. Note that those using black-box methods to achieve robustness are (1 + \u03f5)consistent (e.g. (Wei, 2020)). We can explicitly compare our smoothness bounds to Antoniadis et al. (2023) who use the same kind of predictions: their smoothness is O(1 + log( \u03b7OPT + 1)) with unlimited use of predictions while our algorithm achieves the same smoothness bound with O( \u221a k)OPT predictions. We compare the smoothness of the other algorithms experimentally in Section 5. Antoniadis et al. (2022) study a prediction setup where each prediction is only a single bit, however their algorithms need to receive it in every time step. Gupta et al. (2022) study several problems including caching in a setting where each prediction is correct with a constant probability.\nAntoniadis et al. (2023) proposed an 1-consistent and (1+4\u03b7/OPT)-smooth algorithm for MTS with action predictions which can be robustified by loosing factor (1 + \u03f5) in consistency and smoothness. Getting smoothness bounds sublinear in \u03b7/OPT for specific MTS problems other than caching remains a challenging open problem even with unlimited number of predictions and this holds even for weighted caching. Specific results on weighted caching are by Jiang et al. (2022) who studied it in a setup requiring very verbose predictions and by Bansal et al. (2022) whose bounds depend on the number of weight classes. There is also a consistency/robustness trade-off by Lindermayr et al. (2022) for k-server.\nSince the seminal papers by Kraska et al. (2018) and Lykouris and Vassilvitskii (2021) which initiated the study of learning-augmented algorithms, many computational problems were considered. There are papers on ski rental (Purohit et al., 2018), secretary problem (D\u00fctting et al., 2021), online TSP (Bernardini et al., 2022), energy efficient scheduling (Bamas et al., 2020), flow-time scheduling (Azar et al., 2021; 2022), and online page migration (Indyk et al., 2022). Further related works can be found in References and are discussed in the full version of this paper (see Appendix)."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "Consider an algorithm ALG for MTS which produces a solution x0, x1, . . . , xT for an instance I consisting of cost functions \u21131, . . . , \u2113T . We denote cost(ALG(I)) = \u2211T t=1(\u2113t(xt) + d(xt\u22121, xt)). We say that ALG is r-competitive with respect to an offline algorithm OFF if there is an absolute constant \u03b1 \u2208 R such that E[cost(ALG(I))] \u2264 r \u00b7 cost(OFF(I)) + \u03b1 for any instance I . If ALG is r-competitive with respect to an optimal offline algorithm, we say that ALG is r-competitive and call r the competitive ratio of ALG. In the classical setting (without predictions), the best achievable competitive ratios are \u0398(log k) for caching (Fiat et al., 1991) and of order poly log n for MTS (Bartal et al., 2006; Bubeck et al., 2019), where n is the number of points in the underlying metric space M . We refer to (Borodin and El-Yaniv, 1998) for a textbook treatment."
        },
        {
            "heading": "2.1 ACTION PREDICTIONS FOR MTS",
            "text": "Antoniadis et al. (2023) proposed a prediction setup which they call action predictions, where the predictions tell us what a good algorithm would do. More precisely, at each time t, the algorithm receives a prediction pt of a state where some offline algorithm OFF moves to at time t. The error of prediction pt is then \u03b7t = d(pt, ot), where ot is the real state of OFF at time t. The total prediction error is defined as \u03b7 = \u2211T t=1 \u03b7t.\nConsidering the case of caching, the state corresponds to a cache content, and the prediction error is the number of pages present in the cache of OFF and absent from the predicted cache content. A whole cache content may seem like a huge piece of information, but action predictions for caching can be implemented in a very succinct way. Antoniadis et al. (2023) explain how to represent them with only O(log k) bits per time step when they are received at each time step. Our algorithm asks, in each query, a specific number of indices of pages which are present in its cache but absent from the predicted cache. When we talk about a bound on the number of provided predictions, this bound applies both to the number of such queries as well as to the total number of indices reported by the predictor during the running time of the algorithm. There are predictors which can generate predictions of a similar kind by Jain and Lin (2016); Shi et al. (2019); Liu et al. (2020). See\n(Antoniadis et al., 2023) for a detailed treatment of this prediction setup and a comparison to other setups for caching."
        },
        {
            "heading": "2.2 CACHING: BELADY\u2019S ALGORITHM, MARKING, AND LAZY ALGORITHMS",
            "text": "The classical optimal offline algorithm for caching proposed by Belady (1966) is denoted Belady in this paper. At each page fault, it evicts a page which is going to be requested furthest in the future. In the case of a tie, i.e., if there are several pages in the cache which will not be requested anymore, it chooses one of them arbitrarily. Our caching algorithm assumes that the predictor is trying to simulate Belady. The following useful property allows us to detect errors in the predictions quickly. Observation 2.1. Consider request sequence r1, . . . , rT . For any t \u2264 T , the cost incurred by Belady for r1, . . . , rT until time t is the same as the cost of Belady with input r1, . . . , rt.\nTo see this, it is enough to note that the solution produced by Belady with input r1, . . . , rT agrees until time t with the solution produced by Belady on r1, . . . , rt which breaks ties based on the arrival times in rt+1, . . . , rT .\nWe use properties of marking algorithms in this work. Such algorithms split the input sequence into phases, i.e., maximal subsequences where at most k distinct pages are requested. Usually, the first phase starts in the beginning and the next phase follows just after the end of the previous one. However, we will consider phases starting at arbitrary moments. Let O be the cache content of an algorithm in the beginning of the phase. Whenever a page is requested for the first time during the phase, we call this moment an arrival and we mark the page. At the end of the phase, the set M of marked pages will have size k: some of them belong to O and are called old while those in C = M \\O are called clean. Exactly |C| pages from O remain unmarked until the end of the phase. Marking algorithms is a class of algorithms which never evict a marked page and all of them have cache content M at the end of the phase. Belady is not marking and our algorithm is not marking either, although it uses ideas from marking to achieve desired robustness and smoothness properties. At the end of each phase, we can bound the difference between the cache content of some algorithm and marking. Observation 2.2. Let c be the cost incurred by some algorithm during a marking phase. Then, c \u2265 |M \\ S|, where S is the cache content of the algorithm at the end of the phase and M is the set of k pages requested during the phase.\nThis is because each page in p \u2208 M has to be present in algorithm\u2019s cache when requested during the phase. If p /\u2208 S, then the algorithm must have evicted it during the phase incurring cost 1. Observation 2.3. If a page p is evicted by Belady at time t, then p is not going to be requested in the marking phase containing t anymore.\nIf p is evicted by Belady at time t, then the currently requested page rt and k \u2212 1 pages from the cache are k distinct pages that are requested before the moment when p is requested next time. The current phase then needs to end before that moment.\nWe say that an algorithm is lazy if it evicts only one page at a time and only at a page fault. Belady is lazy while our algorithm, as described, may not be. However, any algorithm can be made lazy without increasing its cost. See (Borodin and El-Yaniv, 1998) for more information about caching. Observation 2.4. The difference in the cache content of two lazy algorithms can increase only if both of them have a page fault. In that case, it can increase by at most 1."
        },
        {
            "heading": "3 BOUNDED NUMBER OF PREDICTIONS",
            "text": "In this section, we prove Theorem 1.1. We propose an algorithm called F&R which consists of two parts: Follower and Robust. It starts with Follower which is 1-consistent, but lacks in smoothness and robustness. At each page fault, Follower recomputes Belady for the part of the sequence seen so far and checks whether it also has a page fault. If yes, it copies the behavior of the predictor (Line 3). Otherwise, it must have received an incorrect prediction before. Therefore, it switches to Robust (Line 5) which is no more 1-consistent, but achieves desired smoothness and robustness. Robust runs for one marking phase and then returns back to Follower. At such moment, the predictor\u2019s and\nthe algorithm\u2019s cache can be very different and Follower may need to lazily synchronize with the predictor (Line 4).\nAlgorithm 1: Follower 1 P := initial cache content; // Prediction for time 0 2 foreach pagefault do 3 if rt /\u2208 P and Belady has a pagefault then query new prediction P and evict any p \u2208 C \\P ; 4 else if rt \u2208 P then evict arbitrary p /\u2208 P ; 5 else Switch to Robust (Algorithm 2);\nAlgorithm Robust runs during a single marking phase starting at the same moment, splitting it into windows as follows (assuming k is a power of 2): The first window W1 starts at the beginning of the phase and lasts k/2 arrivals, i.e., it ends just before the arrival number k/2 + 1. Wi follows the Wi\u22121 and its length is half of the remaining arrivals in the phase. The last window Wlog k+1 = {k} lasts until the end of the phase. Robust comes with an increasing convex function f such that f(0) = 0 and f(i) \u2264 2j \u2212 1. Faster growing f does not further improve our smoothness bounds. Function f determines that we should request f(i)\u2212 f(i\u2212 1) predictions in window i. If the window is too small, we ask for prediction at each time step. Robust starts with the cache content of a marking algorithm whose new phase would start at the same moment (Line 1). In the case of a page fault, it evicts an unmarked page chosen uniformly at random. At arrivals belonging to the sets S and F , it performs synchronization with the predictor and queries the predictor\u2019s state respectively. The synchronization is always performed with respect to the most recent prediction P which, in the case of lazy (or lazified) predictors, implicitly incorporates information from the previous predictions.\nAlgorithm 2: Robustf (one phase) 1 Load k distinct most recently requested pages; 2 S := {k \u2212 2j + 1 | j = log k, . . . , 0}; 3 Wi := [k \u2212 2log k\u2212i+1 + 1, k \u2212 2log k\u2212i] for i = 1, . . . , log k and Wlog k+1 = {k}; 4 Choose F \u2286 {1, . . . , k} such that |F \u2229Wi| = min{f(i)\u2212 f(i\u2212 1), |Wi|} for each i; 5 foreach pagefault during the phase do 6 if it is arrival belonging to F then ask for new prediction P ; 7 if it is arrival belonging to S then synchronize with P ; 8 if requested page is still not in cache then evict random unmarked page; 9 Load all pages marked during the phase;\n10 Switch to Follower (Algorithm 1);\nSynchronization with P (Line 7) works as follows. All pages previously evicted by random evictions return to the cache and the same number of pages not present in P is evicted. We denote Ei = E\u2212i \u222a E+i the set of pages evicted at the beginning of Wi, where pages in E\u2212i are requested during Wi while those in E+i are not. Note that algorithm\u2019s and predictor\u2019s cache may not become the same after the synchronization. Since the algorithm starts with pages in M and loads only clean pages, we have the following observation. Observation 3.1. Let Ci, |Ci| = ci be the set of clean pages arriving before the start of Wi. Then, Ei \u2286 M \u222a Ci and |Ei| = |M \u222a Ci| \u2212 k = ci.\nWe assume that the predictor is lazy and does not load pages that are not requested. Therefore, no page from E+i will be loaded during Wi by the predictor and the same holds for Robust, implying the following. Observation 3.2. For every i = 1, . . . , log k, we have E+i \u2286 Ei+1 and therefore Ei \\ Ei+1 \u2286 E\u2212i .\nSynchronization with the marking cache performed by Robust is to ensure that the difference between the cache of the algorithm and Belady can be bounded by costs incurred by Belady locally using Observation 2.2 instead of diverging over time solely due to incorrect predictions.\nImplementation suggestions. Algorithms are described as to simplify the analysis. Synchronization in Robust (line 7) should be done lazily as to make use of the most recent prediction. At arrivals of\nclean pages, one may evict a page not present in predictor\u2019s cache instead of a random unmarked page; one can also ask for a fresh prediction (at most 2OPT additional queries). The second synchronization with the marking cache in Robust (line 9) can be omitted. With f(i) = 0, one can query the predictor only at clean arrivals, using at most 2OPT predictions in total. We recommend a lazy implementation. Since Robust is not 1-consistent, one may also switch from Follower only once Follower\u2019s cost is at least a constant (e.g. 2 or 3) times higher than the cost of Belady.\nWe denote Hi the ith phase of Robustf and H\u2212i a hypothetical marking phase which ends just before Hi starts. Note that H\u2212i might overlap with Hi\u22121. But H1, H2, . . . are disjoint and we denote Gi,i+1 the time interval between the end of Hi and the beginning of Hi+1. c(Hi) is the number of clean pages during phase Hi. For a given time period X , we define \u2206A(X), \u2206B(X), and \u2206P (X) the costs incurred by F&R, Belady, and the predictor respectively during X and \u03b7(X) the error of predictions received during X .\nHere is the main lemma about the performance of Robust. Its proof is deferred to Section 3.2. Lemma 3.3. Denote Xi = Hi\u22121 \u222a H\u2212i \u222a Hi. During the phase Hi, Robustf receives at most f(log k) + 1 predictions and we have\nE[\u2206A(Hi)] \u2264 O(1)f\u22121 ( \u03b7(Hi)\n\u2206B(Xi)\n) \u2206B(Xi). (1)\nAt the same time, we also have E[\u2206A(Hi)] \u2264 O(log k)\u2206B(Xi) and (2)\n\u2206A(Hi) \u2264 O(k\u03b7(Hi)). (3)"
        },
        {
            "heading": "3.1 ANALYSIS OF FOLLOWER",
            "text": "Lemma 3.4. Consider the gap Gi,i+1 between phases Hi and Hi+1 of Robustf . We have \u2206A(Gi,i+1) \u2264 \u2206B(Gi,i+1) + \u2206B(Hi).\nProof of Theorem 1.1. The cost of Follower until the start of H1 is the same as the cost of Belady. Therefore, by lemmas 3.4 and 3.3 equation 1, the cost of F&R, in expectation, is at most\nOPT + \u2211\ni\n\u2206B(Hi) + \u2211\ni\nO(1)\u2206B(Xi)f \u22121\n( \u03b7(Hi)\n\u2206B(Xi)\n) ,\nwhere the sums are over all phases of Robust and Xi = Hi\u22121 \u222a H\u2212i \u222a Hi. Since phases Hi are disjoint and the same holds for H\u2212i , this expression is at most OPT \u00b7O(f\u22121(\u03b7/OPT)) by concavity of f\u22121, implying the smoothness bound for F&R.\nIf we use bound equation 2 instead of equation 1, we get O(log k)OPT \u2013 the robustness bound. Since there must be at least one error during the execution of Follower to trigger each execution of Robust, equation 3 implies that the cost of F&R is at most OPT+\u03b7O(k). With \u03b7 = 0, this implies 1-consistency of F&R. Follower queries the predictor only at a page fault by OPT and the prediction consists of a single page evicted by the predictor. Robust may ask for up to f(log k) + 1 predictions in each phase, each of them consisting of indices of at most c(Hi) pages from F&R cache not present in the predictor\u2019s cache. This gives both O(f(log k))OPT queries to the predictor as well as O(f(log k))OPT predicted indices in total.\n3.2 ANALYSIS OF Robustf\nThe full version of this section and the proof of Lemma 3.3 can be found in Appendix (Section 3.2), here we include a short overview. Charging a page fault on a page evicted due to predictor\u2019s advice to a single incorrect action prediction can only give us smoothness linear in the prediction error. This is in contrast with next-arrival predictions where algorithms can be analyzed by estimating lengths of eviction chains caused by each incorrect prediction, as proposed by Lykouris and Vassilvitskii (2021). To achieve sublinear smoothness, we need to charge each such page fault to a long interval of incorrect predictions. This is the most challenging part of our analysis because Belady also moves and the same prediction incorrect at one time step may be correct at another time step. We estimate the error of predictions received during each window by introducing window rank which bounds the prediction error from below accounting for the movements of Belady."
        },
        {
            "heading": "4 WELL-SEPARATED QUERIES TO THE PREDICTOR",
            "text": "The full version of this section, which can be found in Appendix, contains a consistent and smooth algorithm for MTS proving Theorem 1.3 and extends our analysis of F&R to the setting where the queries to the predictor need to be separated by at least a time steps, proving Theorem 1.5. In MTS, the cost functions usually do not satisfy any Lipschitz property. Therefore, the difference between the cost of the state reported by the predictor and the state of the optimal algorithm does not need to be proportional to their distance. We show that a state satisfying this property which is close to the predicted state can be found using a classical technique for design of algorithms for MTS called work functions, see (Chrobak and Larmore, 1996) for reference. Then, we use the approach of Emek et al. (2009) to interpolate between predictions received at times t and t+ a. In the case of caching, the performance of F&R in this regime is the same as if it has received a incorrect predictions for each prediction error. Therefore, \u03b7 in its smoothness bound needs to be multiplied by a."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We perform an empirical evaluation of our caching algorithm F&R on the same datasets and with the same predictors as the previous works (Lykouris and Vassilvitskii, 2021; Antoniadis et al., 2023; Im et al., 2022). We use the following datasets.\n\u2022 BrightKite dataset (Cho et al., 2011) contains data from a certain social network. We create a separate caching instance from the data of each user, interpreting check-in locations as pages. We use it with cache size k = 10 and choose instances corresponding to the first 100 users with the longest check-in sequences requiring at least 50 page faults in the optimal policy.\n\u2022 CitiBike dataset contains data about bike trips in a bike sharing platform CitiBike. We create a caching instance from each month in 2017, interpreting starting stations of the trips as pages, and trimming length of each instance to 25 000. We use it with cache size k = 100.\nSome of the algorithms in our comparison use next-arrival predictions while F&R uses action predictions that can be generated from next-arrival predictions. Therefore, we use predictors which predict the next arrival of the requested page and convert it to action predictions. This process was used and described by Antoniadis et al. (2023) and we use their implementation of the predictors. Our algorithm is then provided limited access to the resulting action predictions while the algorithm of Im et al. (2022) has limited access to the original next-arrival predictions.\n\u2022 Synthetic predictions: compute the exact next arrival time computed from the data and add noise to this number. This noise comes from a log-normal distribution with the mean parameter \u00b5 = 0 and the standard deviation parameter \u03c3. We use \u03c3 \u2208 [0, 50].\n\u2022 PLECO predictor proposed by Anderson et al. (2014): This model estimates the probability p of a page being requested in the next time step and we interpret this as a prediction that the next arrival of this page will be in 1/p time steps. The model parameters were fitted to BrightKite dataset and not adjusted before use on CitiBike.\n\u2022 POPU \u2013 a simple predictor used by Antoniadis et al. (2023): if a page appeared in p fraction of the previous requests, we predict its next arrival in 1/p time steps.\nIn our comparison, we include the following algorithms: offline algorithm Belady which we use to compute the optimal number of page faults OPT, standard online algorithms LRU and Marker (Fiat et al., 1991), ML-augmented algorithms using next arrival predictions L&V (Lykouris and Vassilvitskii, 2021), LMark and LnonMark (Rohatgi, 2020), FtPM which, at each step, evicts an unmarked page with the furthest predicted next arrival time, and algorithms for action predictions FtP and T&D (Antoniadis et al., 2023). We use the implementation of all these algorithms published by Antoniadis et al. (2023). We implement algorithm AQ (Im et al., 2022) and our algorithm F&R.\nNotes on implementation of F&R. We follow the recommendations in Section 3 except that Follower switches to Robust whenever its cost is \u03b1 = 1 times higher compared to Belady in the same period. With higher \u03b1, the performance of F&R approaches FtP on the considered datasets. With k = 10 (BrightKite dataset), we use F = [1, 6, 9] corresponding to f(i) = i. Note that, with\nsuch small k, polynomial and exponential f would also give a very similar F . With k = 100 (CitiBike dataset), we use exponential f(i) = 2i+1\u22121. With a-separated queries, Follower uses LRU heuristic when prediction is unavailable, and Robust ignores F , querying the predictor at each page fault separated from the previous query by at least a time steps.\nResults. Figures 1 and 2 contain averages of 10 independent experiments. Figure 1 shows that the performance of F&R with high-quality predictions is superior to the previous ML-augmented algorithms except for FtP which follows the predictions blindly and is also 1-consistent. With high \u03c3, the performance of T&D becomes better. This is true also for F&R with F = [1..10], suggesting that T&D might be more efficient in using erroneous predictions. The second plot shows the total number of times algorithms query the predictor over all instances. Response to such query is a single page missing from predictor\u2019s cache in the case of F&R and T&D and next arrival times of b pages in the case of AQ_b. Note that FtPM is equivalent to the non-parsimonious version of AQ with b = k. F&R makes the smallest number of queries: with perfect predictions, it makes exactly OPT queries and this number decreases with higher \u03c3 as F&R spends more time in Robust.\nFigure 2 shows that F&R performs well in the regime with a-separated queries. While the performance of FtPM with POPU predictor worsens considerably towards Marker already with a = 5, F&R keeps its improvement over Marker even with a = 20. Predictions produced by PLECO seem much less precise as suggested by FtP with PLECO being worse than Marker and smaller number of such predictions either improves (AQ, FtPM) or does not affect performance (F&R) of considered algorithms. Further details of our experimental results are presented in Appendix (Section 5)."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "We present algorithms for MTS and caching with action predictions working in the setting where the number of queries or the frequency of querying the predictor are limited. We have shown that one can achieve theoretical as well as empirical performance comparable to the setting with unlimited access to the predictor, possibly enabling usage of precise but heavy-weight prediction models in environments with scarce computational resources.\nREPRODUCIBILITY STATEMENT\nThe appendix contains a full version of our paper which includes proof of all the theorems and lemmas. We provide textual description of the implementation of our algorithm in Section 5. The code of our implementation and the used datasets can be found in the supplementary material."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Caching, introduced by Sleator and Tarjan (1985), is a fundamental problem in online computation important both in theory and practice. Here, we have a fast memory (cache) which can contain up to k different pages and we receive a sequence of requests to pages in an online manner. Whenever a page is requested, it needs to be loaded in the cache. Therefore, if the requested page is already in the cache, it can be accessed at no cost. Otherwise, we suffer a page fault: we have to evict one page from the cache and load the requested page in its place. The page to evict is to be chosen without knowledge of the future requests and our target is to minimize the total number of page faults.\nCaching is a special case of Metrical Task Systems introduced by Borodin et al. (1992) as a generalization of many fundamental online problems. In the beginning, we are given a metric space M of states which can be interpreted as actions or configurations of some system. We start at a predefined state x0 \u2208 M . At time steps t = 1, 2, . . . , we receive a cost function \u2113t : M \u2192 R+ \u222a {0,+\u221e} and we need to make a decision: either to stay at xt\u22121 and pay a cost \u2113t(xt\u22121), or to move to another, possibly cheaper state xt and pay \u2113t(xt) + d(xt\u22121, xt), where the distance d(xt\u22121, xt) represents the transition cost between states xt\u22121 and xt.\nThe online nature of both caching and MTS forces an algorithm to make decisions without knowledge of the future which leads to very suboptimal results in the worst case (Borodin et al., 1992; Sleator and Tarjan, 1985). A recently emerging field of learning-augmented algorithms, introduced in seminal papers by Kraska et al. (2018) and Lykouris and Vassilvitskii (2021), investigates approaches to improve the performance of algorithms using predictions, possibly generated by some ML model. In general, no guarantee on the accuracy of these predictions is assumed. Therefore, the performance of learning-augmented algorithms is usually evaluated using the following three parameters:\nConsistency. Performance with perfect predictions, preferably close to optimum.\nRobustness. Performance with very bad predictions, preferably no worse than what is achievable by known algorithms which do not utilize predictions.\nSmoothness. Algorithm\u2019s performance should deteriorate smoothly with increasing prediction error between the consistency and robustness bound.\nThese three parameters express a desire to design algorithms that work very well when receiving reasonably accurate predictions most of the time and, in the rest of the cases, still satisfy state-ofthe-art worst-case guarantees. See the survey by Mitzenmacher and Vassilvitskii (2020) for more information.\nProducing predictions is often a computationally intensive task, therefore it is interesting to understand the interplay between the number of available predictions and the achievable performance. In their inspiring work, Im et al. (2022) initiated the study of learning-augmented algorithms which use the predictions parsimoniously. In their work, they study caching with next-arrival-time predictions introduced by Lykouris and Vassilvitskii (2021). Their algorithm uses O(b logb+1 k)OPT predictions, where OPT is the number of page faults incurred by the offline optimal solution and b \u2208 N is a parameter. It achieves smoothness linear in the prediction error. It satisfies tight consistency bounds: with perfect predictions, it incurs at most O(logb+1 k)OPT page faults and no algorithm can do better. In other words, it achieves a constant competitive ratio with unrestricted access to predictions (b = k) and, with b a small constant, its competitive ratio deteriorates to O(log k) which is comparable to the best competitive ratio achievable without predictions. One of their open questions is whether a similar result could be proved for MTS.\nIn this paper, we study parsimonious algorithms for MTS working with action predictions which were introduced by Antoniadis et al. (2023). Here, each prediction describes the state of an optimal algorithm at the given time step and its error is defined as the distance from the actual state of the optimal algorithm. The total prediction error is the sum of errors of the individual predictions. In the case of caching, action predictions have a very concise representation, see Section 2.1. Unlike next-arrival-time predictions, action predictions can be used for any MTS. Using the method of Blum and Burch (2000), it is easy to achieve near-optimal robustness for any MTS losing only a factor (1 + \u03f5) in consistency and smoothness. Therefore, we study how the reduced number of predictions affects the consistency and smoothness parameters. We consider the following two regimes.\nBounded number of predictions: The algorithm can request a prediction whenever it prefers as far as the total number of requested predictions is bounded by bOPT, where b is a parameter. This regime is similar to Im et al. (2022).\nWell-separated queries to the predictor: The queries to the predictor need to be separated by at least a time steps, for some parameter a. This captures the situation when producing each prediction takes more than one time step."
        },
        {
            "heading": "1.1 OUR RESULTS",
            "text": "We evaluate the algorithm\u2019s performance using competitive ratio which is, roughly speaking, the worst-case ratio between the cost incurred by the algorithm and the cost of the offline optimum, see Section 2 for a formal definition. We say that an algorithm achieves consistency \u03b1 and robustness \u03b2 if its competitive ratio is at most \u03b1 when provided with perfect predictions and at most \u03b2 with arbitrarily bad predictions. For a given function g, we call an algorithm g(\u03b7)-smooth if its competitive ratio is at most g(\u03b7) whenever provided with predictions with the total error at most \u03b7.\nOur first contribution is an algorithm for caching which receives action predictions describing the states of the optimal offline algorithm Belady proposed by Belady (1966). High quality such predictor based on imitation learning was already designed by Liu et al. (2020). Its empirical evaluation within existing algorithms designed for action predictions was performed by Chledowski et al. (2021). Theorem 1.1. Let f be an increasing convex function such that f(0) = 0 and f(i) \u2264 2i \u2212 1 for each i \u2265 0. There is an algorithm for caching requiring O(f(log k))OPT predictions which achieves consistency 1, robustness O(log k), and smoothness O(f\u22121(\u03b7/OPT )), where \u03b7 denotes the total prediction error with respect to Belady and OPT is the number of page faults of Belady.\nIn fact, the number of required predictions is slightly smaller than what is stated in the theorem. Table 1 shows numbers of predictions and achieved smoothness for some natural choices of f . Already with O( \u221a k)OPT predictions, our bounds are comparable to Antoniadis et al. (2023) whose algorithm asks for a prediction in every step, its consistency is constant and its smoothness is logarithmic in \u03b7. The algorithm also works with f(i) = 0. In that case, it asks for at most 2OPT predictions and still remains 1-consistent. However, its smoothness is not very good. We use sliding marking phases and a careful distribution of queries of the predictor over the time horizon. This allows us to avoid\nTable 1: Smoothness vs. number of predictions. f(i) # of predictions smoothness 2i \u2212 1 O( \u221a k)OPT O(1 + log( \u03b7OPT + 1)) i2 O(log2 k)OPT O( \u221a\n2 \u03b7OPT )\ni O(log k)OPT O( \u03b7OPT ) 0 2OPT O( k\u03b7OPT )\ndealing with so called \"ancient\" pages considered by Rohatgi (2020) and Antoniadis et al. (2023), resulting in an algorithm with better consistency and a simpler analysis.\nWe discuss tightness of our bounds in Section 7. We show that with, for example, only 0.5OPT available predictions, no algorithm can be better than O(log k)-competitive \u2013 a guarantee comparable to the best classical online algorithms without predictions. We also show that the number of predictions used by our algorithm is close to optimal. Theorem 1.2. Let f be an increasing function. Any f(\u03b7)-smooth algorithm for caching with action predictions, i.e., an algorithm whose competitive ratio with predictions of error \u03b7 is f\u22121(\u03b7) for any \u03b7 > 0, has to use at least f(ln k)OPT predictions.\nFor general MTS, we cannot bound the number of used predictions as a function of OPT. The reason is that any instance of MTS can be scaled to make OPT arbitrarily small, allowing us to use only very few predictions. We propose an algorithm which queries the predictor once in every a time steps, making at most T/a queries in total, where T denotes the length of the input sequence. Theorem 1.3. There is a deterministic algorithm for any MTS which receives a prediction only once per each a time steps and its cost is at most O(a) \u00b7 (OFF+2\u03b7), where OFF denotes the cost of an arbitrary offline algorithm and \u03b7 the error of predictions with respect to this algorithm.\nThis is a more general statement than Theorem 1.1 which requires OFF to be Belady. Considering any offline optimal algorithm OFF, Theorem 1.3 implies a smoothness O(a) \u00b7 (1 + 2\u03b7/OPT) and consistency O(a). Our algorithm is based on work functions. For a = 1, its smoothness is 1 + 2\u03b7/OFF, see Section 4, which improves upon the smoothness bound of 1 + 4\u03b7/OFF by Antoniadis et al. (2023). It is not robust on its own. However, it can be combined with any online algorithm for the given MTS using the result of Blum and Burch (2000) achieving robustness comparable to that algorithm and losing only a factor of (1 + \u03f5) in smoothness and consistency.\nNo algorithm receiving a prediction only once in a time steps can be o(a)-consistent. This follows from the work of Emek et al. (2009) on advice complexity, see Section 7 for more details. The same can be shown for smoothness by modifying the lower bound construction of Antoniadis et al. (2023). Theorem 1.4. There is no o(a\u03b7/OPT)-smooth algorithm for MTS with action predictions which receives predictions only once in a time steps.\nWe can modify our algorithm for caching to ensure that the moments when the predictions are queried are separated by at least a time steps, not losing too much of its performance. Theorem 1.5. There is an algorithm for caching which receives prediction at most once in a \u2264 k time steps and using at most O(f(log k))OPT predictions in total which is O(1)-consistent, O(log k)robust and O(f\u22121(a\u03b7/OPT))-smooth.\nIn Section 5, we provide empirical results suggesting that our algorithm\u2019s performance can be comparable to the performance of algorithms imposing no limitations on their use of predictions. Our algorithm may therefore be useful especially with heavy-weight predictors like (Liu et al., 2020)."
        },
        {
            "heading": "1.2 RELATED WORK",
            "text": "The most related work is by Im et al. (2022), who studied caching with next arrival time predictions. A smaller number of predictions affects the consistency of their algorithm: with b(log k/ log b)OPT predictions, they achieve consistency O(log k/ log b) and they show that this is tight. They also\nshow that their algorithm achieves linear smoothness. In contrast, our algorithm is 1-consistent when receiving at least OPT predictions. This demonstrates that action predictions, although not containing more bits, seem to contain useful information about the input instance in a more condensed form. See (Antoniadis et al., 2023) for comparison and connections between these prediction setups. Drygala et al. (2023) study ski rental and bahncard problems with predictions of a fixed cost.\nThere are several other papers on caching with predictions, including (Lykouris and Vassilvitskii, 2021; Rohatgi, 2020; Wei, 2020; Emek et al., 2009; Antoniadis et al., 2023; 2022) which design algorithms asking for a prediction at each time step. Consistency parameters achieved by these algorithms are constants greater than 1. Note that those using black-box methods to achieve robustness are (1 + \u03f5)consistent (e.g. (Wei, 2020)). We can explicitly compare our smoothness bounds to Antoniadis et al. (2023) who use the same kind of predictions: their smoothness is O(1 + log( \u03b7OPT + 1)) with unlimited use of predictions while our algorithm achieves the same smoothness bound with O( \u221a k)OPT predictions. We compare the smoothness of the other algorithms experimentally in Section 5. Antoniadis et al. (2022) study a prediction setup where each prediction is only a single bit, however their algorithms need to receive it in every time step. Gupta et al. (2022) study several problems including caching in a setting where each prediction is correct with a constant probability.\nAntoniadis et al. (2023) proposed an 1-consistent and (1+4\u03b7/OPT)-smooth algorithm for MTS with action predictions which can be robustified by loosing factor (1 + \u03f5) in consistency and smoothness. Getting smoothness bounds sublinear in \u03b7/OPT for specific MTS problems other than caching remains a challenging open problem even with unlimited number of predictions and this holds even for weighted caching. Specific results on weighted caching are by Jiang et al. (2022) who studied it in a setup requiring very verbose predictions and by Bansal et al. (2022) whose bounds depend on the number of weight classes. There is also a consistency/robustness trade-off by Lindermayr et al. (2022) for k-server.\nSince the seminal papers by Kraska et al. (2018) and Lykouris and Vassilvitskii (2021) which initiated the study of learning-augmented algorithms, many computational problems were considered.\nThere are papers on ski rental (Purohit et al., 2018; Antoniadis et al., 2021), secretary and matching problems (D\u00fctting et al., 2021; Antoniadis et al., 2020), online-knapsack (Im et al., 2021; Zeynali et al., 2021; Boyar et al., 2022), graph exploration (Eberle et al., 2022), online TSP (Bernardini et al., 2022), energy efficient scheduling (Bamas et al., 2020), flow-time scheduling (Azar et al., 2021; 2022), restricted assignment (Lattanzi et al., 2020), non-clairvoyant scheduling Purohit et al. (2018); Lindermayr and Megow (2022), and online page migration (Indyk et al., 2022). In offline setting, there is a work of Dinitz et al. (Dinitz et al., 2021) on matching, Chen et al. (Chen et al., 2022) on graph algorithms, Polak and Zub (Polak and Zub, 2022) on flows, Sakaue and Oki (Sakaue and Oki, 2022) on discrete optimization problems, and Ergun et al. (Ergun et al., 2022) on clustering. We also refer to (Lindermayr and Megow, 2022) to an updated list of results in the area.\nThere are numerous works on advice complexity of online problems, where algorithms are given certain number of bits of information about the future which are guaranteed to be correct. This is different from our setting, where we receive predictions of unknown quality. We refer to the survey by Boyar et al. (2017), work of Dobrev et al. (2009) on caching, Emek et al. (2009) on MTS, and further papers by Hromkovic\u030c et al. (2010); B\u00f6ckenhauer et al. (2017).\nThere are already works on predictors for caching. Jain and Lin (2016) proposed a binary classifier called Hawkey which predicts which pages will be kept in cache by Belady, providing us with action predictions. Their result was later improved by Shi et al. (2019) who designed a model called Glider for the same classification problem. There is a very precise model by Liu et al. (2020) whose main output can be interpreted as an action prediction although it has a second prediction head which produces next arrival predictions. This model is large and relatively slow and served as a motivation for this work. Chledowski et al. (2021) evaluated the performance of existing ML-augmented algorithms with this predictor."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "Consider an algorithm ALG for MTS which produces a solution x0, x1, . . . , xT for an instance I consisting of cost functions \u21131, . . . , \u2113T . We denote cost(ALG(I)) = \u2211T t=1(\u2113t(xt) + d(xt\u22121, xt)). We say that ALG is r-competitive with respect to an offline algorithm OFF if there is an absolute\nconstant \u03b1 \u2208 R such that E[cost(ALG(I))] \u2264 r \u00b7 cost(OFF(I)) + \u03b1 for any instance I . If ALG is r-competitive with respect to an optimal offline algorithm, we say that ALG is r-competitive and call r the competitive ratio of ALG. In the classical setting (without predictions), the best achievable competitive ratios are \u0398(log k) for caching (Fiat et al., 1991) and of order poly log n for MTS (Bartal et al., 2006; Bubeck et al., 2019), where n is the number of points in the underlying metric space M . We refer to (Borodin and El-Yaniv, 1998) for a textbook treatment."
        },
        {
            "heading": "2.1 ACTION PREDICTIONS FOR MTS",
            "text": "Antoniadis et al. (2023) proposed a prediction setup which they call action predictions, where the predictions tell us what a good algorithm would do. More precisely, at each time t, the algorithm receives a prediction pt of a state where some offline algorithm OFF moves to at time t. The error of prediction pt is then \u03b7t = d(pt, ot), where ot is the real state of OFF at time t. The total prediction error is defined as \u03b7 = \u2211T t=1 \u03b7t.\nConsidering the case of caching, the state corresponds to a cache content, and the prediction error is the number of pages present in the cache of OFF and absent from the predicted cache content. A whole cache content may seem like a huge piece of information, but action predictions for caching can be implemented in a very succinct way. Antoniadis et al. (2023) explain how to represent them with only O(log k) bits per time step when they are received at each time step. Our algorithm asks, in each query, a specific number of indices of pages which are present in its cache but absent from the predicted cache. When we talk about a bound on the number of provided predictions, this bound applies both to the number of such queries as well as to the total number of indices reported by the predictor during the running time of the algorithm. There are predictors which can generate predictions of a similar kind by Jain and Lin (2016); Shi et al. (2019); Liu et al. (2020). See (Antoniadis et al., 2023) for a detailed treatment of this prediction setup and a comparison to other setups for caching."
        },
        {
            "heading": "2.2 CACHING: BELADY\u2019S ALGORITHM, MARKING, AND LAZY ALGORITHMS",
            "text": "The classical optimal offline algorithm for caching proposed by Belady (1966) is denoted Belady in this paper. At each page fault, it evicts a page which is going to be requested furthest in the future. In the case of a tie, i.e., if there are several pages in the cache which will not be requested anymore, it chooses one of them arbitrarily. Our caching algorithm assumes that the predictor is trying to simulate Belady. The following useful property allows us to detect errors in the predictions quickly.\nObservation 2.1. Consider request sequence r1, . . . , rT . For any t \u2264 T , the cost incurred by Belady for r1, . . . , rT until time t is the same as the cost of Belady with input r1, . . . , rt.\nTo see this, it is enough to note that the solution produced by Belady with input r1, . . . , rT agrees until time t with the solution produced by Belady on r1, . . . , rt which breaks ties based on the arrival times in rt+1, . . . , rT .\nWe use properties of marking algorithms in this work. Such algorithms split the input sequence into phases, i.e., maximal subsequences where at most k distinct pages are requested. Usually, the first phase starts in the beginning and the next phase follows just after the end of the previous one. However, we will consider phases starting at arbitrary moments. Let O be the cache content of an algorithm in the beginning of the phase. Whenever a page is requested for the first time during the phase, we call this moment an arrival and we mark the page. At the end of the phase, the set M of marked pages will have size k: some of them belong to O and are called old while those in C = M \\O are called clean. Exactly |C| pages from O remain unmarked until the end of the phase. Marking algorithms is a class of algorithms which never evict a marked page and all of them have cache content M at the end of the phase. Belady is not marking and our algorithm is not marking either, although it uses ideas from marking to achieve desired robustness and smoothness properties. At the end of each phase, we can bound the difference between the cache content of some algorithm and marking.\nObservation 2.2. Let c be the cost incurred by some algorithm during a marking phase. Then, c \u2265 |M \\ S|, where S is the cache content of the algorithm at the end of the phase and M is the set of k pages requested during the phase.\nThis is because each page in p \u2208 M has to be present in algorithm\u2019s cache when requested during the phase. If p /\u2208 S, then the algorithm must have evicted it during the phase incurring cost 1. Observation 2.3. If a page p is evicted by Belady at time t, then p is not going to be requested in the marking phase containing t anymore.\nIf p is evicted by Belady at time t, then the currently requested page rt and k \u2212 1 pages from the cache are k distinct pages that are requested before the moment when p is requested next time. The current phase then needs to end before that moment.\nWe say that an algorithm is lazy if it evicts only one page at a time and only at a page fault. Belady is lazy while our algorithm, as described, may not be. However, any algorithm can be made lazy without increasing its cost. See (Borodin and El-Yaniv, 1998) for more information about caching. Observation 2.4. The difference in the cache content of two lazy algorithms can increase only if both of them have a page fault. In that case, it can increase by at most 1."
        },
        {
            "heading": "2.3 MTS AND ADVICE COMPLEXITY",
            "text": "Advice complexity studies the performance of algorithms depending on the number of bits of precise information about the instance available in advance. In the case of MTS, Emek et al. (2009) study the situation when algorithm receives 1a log n bits of information about the state of some optimal offline algorithm, being able to identify its true state once in a time steps. They formulate the following proposition for OFF being an optimal algorithm, but the proof does not use its optimality and it can be any algorithm located at qi at time ia. Proposition 2.5 (Emek et al. (2009)). There is an algorithm which, with knowledge of state qi of algorithm OFF at time ia for i = 1, . . . , T/a, is O(a)-competitive w.r.t. OFF.\nIn our context, we can say that the algorithm from the preceding proposition is O(a)-consistent if q1, . . . , qT/a are states of an optimal solution. However, it is not smooth because it may not be possible to relate the cost of OFF to the prediction error with respect to OPT."
        },
        {
            "heading": "3 BOUNDED NUMBER OF PREDICTIONS",
            "text": "In this section, we prove Theorem 1.1. We propose an algorithm called F&R which consists of two parts: Follower and Robust. It starts with Follower which is 1-consistent, but lacks in smoothness and robustness. At each page fault, Follower recomputes Belady for the part of the sequence seen so far and checks whether it also has a page fault. If yes, it copies the behavior of the predictor (Line 3). Otherwise, it must have received an incorrect prediction before. Therefore, it switches to Robust (Line 5) which is no more 1-consistent, but achieves desired smoothness and robustness. Robust runs for one marking phase and then returns back to Follower. At such moment, the predictor\u2019s and the algorithm\u2019s cache can be very different and Follower may need to lazily synchronize with the predictor (Line 4).\nAlgorithm 1: Follower 1 P := initial cache content; // Prediction for time 0 2 foreach pagefault do 3 if rt /\u2208 P and Belady has a pagefault then query new prediction P and evict any p \u2208 C \\P ; 4 else if rt \u2208 P then evict arbitrary p /\u2208 P ; 5 else Switch to Robust (Algorithm 2);\nAlgorithm Robust runs during a single marking phase starting at the same moment, splitting it into windows as follows (assuming k is a power of 2): The first window W1 starts at the beginning of the phase and lasts k/2 arrivals, i.e., it ends just before the arrival number k/2 + 1. Wi follows the Wi\u22121 and its length is half of the remaining arrivals in the phase. The last window Wlog k+1 = {k} lasts until the end of the phase. Robust comes with an increasing convex function f such that f(0) = 0 and f(i) \u2264 2j \u2212 1. Faster growing f does not further improve our smoothness bounds. Function f determines that we should request f(i)\u2212 f(i\u2212 1) predictions in window i. If the window is too\nsmall, we ask for prediction at each time step. Robust starts with the cache content of a marking algorithm whose new phase would start at the same moment (Line 1). In the case of a page fault, it evicts an unmarked page chosen uniformly at random. At arrivals belonging to the sets S and F , it performs synchronization with the predictor and queries the predictor\u2019s state respectively. The synchronization is always performed with respect to the most recent prediction P which, in the case of lazy (or lazified) predictors, implicitly incorporates information from the previous predictions.\nAlgorithm 2: Robustf (one phase) 1 Load k distinct most recently requested pages; 2 S := {k \u2212 2j + 1 | j = log k, . . . , 0}; 3 Wi := [k \u2212 2log k\u2212i+1 + 1, k \u2212 2log k\u2212i] for i = 1, . . . , log k and Wlog k+1 = {k}; 4 Choose F \u2286 {1, . . . , k} such that |F \u2229Wi| = min{f(i)\u2212 f(i\u2212 1), |Wi|} for each i; 5 foreach pagefault during the phase do 6 if it is arrival belonging to F then ask for new prediction P ; 7 if it is arrival belonging to S then synchronize with P ; 8 if requested page is still not in cache then evict random unmarked page; 9 Load all pages marked during the phase;\n10 Switch to Follower (Algorithm 1);\nSynchronization with P (Line 7) works as follows. All pages previously evicted by random evictions return to the cache and the same number of pages not present in P is evicted. We denote Ei = E\u2212i \u222a E+i the set of pages evicted at the beginning of Wi, where pages in E\u2212i are requested during Wi while those in E+i are not. Note that algorithm\u2019s and predictor\u2019s cache may not become the same after the synchronization. Since the algorithm starts with pages in M and loads only clean pages, we have the following observation.\nObservation 3.1. Let Ci, |Ci| = ci be the set of clean pages arriving before the start of Wi. Then, Ei \u2286 M \u222a Ci and |Ei| = |M \u222a Ci| \u2212 k = ci.\nWe assume that the predictor is lazy and does not load pages that are not requested. Therefore, no page from E+i will be loaded during Wi by the predictor and the same holds for Robust, implying the following.\nObservation 3.2. For every i = 1, . . . , log k, we have E+i \u2286 Ei+1 and therefore Ei \\ Ei+1 \u2286 E\u2212i .\nSynchronization with the marking cache performed by Robust is to ensure that the difference between the cache of the algorithm and Belady can be bounded by costs incurred by Belady locally using Observation 2.2 instead of diverging over time solely due to incorrect predictions.\nImplementation suggestions. Algorithms are described as to simplify the analysis. Synchronization in Robust (line 7) should be done lazily as to make use of the most recent prediction. At arrivals of clean pages, one may evict a page not present in predictor\u2019s cache instead of a random unmarked page; one can also ask for a fresh prediction (at most 2OPT additional queries). The second synchronization with the marking cache in Robust (line 9) can be omitted. With f(i) = 0, one can query the predictor only at clean arrivals, using at most 2OPT predictions in total. We recommend a lazy implementation. Since Robust is not 1-consistent, one may also switch from Follower only once Follower\u2019s cost is at least a constant (e.g. 2 or 3) times higher than the cost of Belady.\nWe denote Hi the ith phase of Robustf and H\u2212i a hypothetical marking phase which ends just before Hi starts. Note that H\u2212i might overlap with Hi\u22121. But H1, H2, . . . are disjoint and we denote Gi,i+1 the time interval between the end of Hi and the beginning of Hi+1. c(Hi) is the number of clean pages during phase Hi. For a given time period X , we define \u2206A(X), \u2206B(X), and \u2206P (X) the costs incurred by F&R, Belady, and the predictor respectively during X and \u03b7(X) the error of predictions received during X .\nHere is the main lemma about the performance of Robust. Its proof is deferred to Section 3.2.\nLemma 3.3. Denote Xi = Hi\u22121 \u222a H\u2212i \u222a Hi. During the phase Hi, Robustf receives at most f(log k) + 1 predictions and we have\nE[\u2206A(Hi)] \u2264 O(1)f\u22121 ( \u03b7(Hi)\n\u2206B(Xi)\n) \u2206B(Xi). (1)\nAt the same time, we also have\nE[\u2206A(Hi)] \u2264 O(log k)\u2206B(Xi) and (2) \u2206A(Hi) \u2264 O(k\u03b7(Hi)). (3)"
        },
        {
            "heading": "3.1 ANALYSIS OF FOLLOWER",
            "text": "Lemma 3.4. Consider the gap Gi,i+1 between phases Hi and Hi+1 of Robustf . We have\n\u2206A(Gi,i+1) \u2264 \u2206B(Gi,i+1) + \u2206B(Hi).\nProof. There are \u2206B(Gi,i+1) page faults served at line 3 because Belady also has those page faults. To bound the cost incurred on line 4, we denote P,B,M the cache contents of the predictor, Belady, and Robust respectively at the end of the phase Hi. The synchronization with P costs at most |(P \\M) \u2229B| if we omit costs incurred by Belady which were already accounted for above. However, (P \\M)\u2229B = (B \\M)\u2229P \u2286 B \\M and |B \\M | \u2264 \u2206B(Hi) by Observation 2.2.\nProof of Theorem 1.1. The cost of Follower until the start of H1 is the same as the cost of Belady. Therefore, by lemmas 3.4 and 3.3 equation 1, the cost of F&R, in expectation, is at most\nOPT + \u2211\ni\n\u2206B(Hi) + \u2211\ni\nO(1)\u2206B(Xi)f \u22121\n( \u03b7(Hi)\n\u2206B(Xi)\n) ,\nwhere the sums are over all phases of Robust and Xi = Hi\u22121 \u222a H\u2212i \u222a Hi. Since phases Hi are disjoint and the same holds for H\u2212i , this expression is at most OPT \u00b7O(f\u22121(\u03b7/OPT)) by concavity of f\u22121, implying the smoothness bound for F&R.\nIf we use bound equation 2 instead of equation 1, we get O(log k)OPT \u2013 the robustness bound. Since there must be at least one error during the execution of Follower to trigger each execution of Robust, equation 3 implies that the cost of F&R is at most OPT+\u03b7O(k). With \u03b7 = 0, this implies 1-consistency of F&R. Follower queries the predictor only at a page fault by OPT and the prediction consists of a single page evicted by the predictor. Robust may ask for up to f(log k) + 1 predictions in each phase, each of them consisting of indices of at most c(Hi) pages from F&R cache not present in the predictor\u2019s cache. This gives both O(f(log k))OPT queries to the predictor as well as O(f(log k))OPT predicted indices in total.\nNote that Robustf can rarely use full O(f(log k)) predictions, because the last windows are not long enough. More precise calculation of numbers of predictions can be found in Appendix A.\n3.2 ANALYSIS OF Robustf\nFirst, we relate the number of clean pages in a robust phase to the costs incurred by Belady.\nObservation 3.5. Consider a phase H denoting C(H) the set of clean pages arriving during H . We have\nc(H) := |C(H)| \u2264 \u2206B(H\u2212) + \u2206B(H).\nProof. There are k + c(H) pages requested during H\u2212 \u222aH . Therefore, any algorithm, and Belady in particular, has to pay cost \u2206B(H\u2212 \u222aH) \u2265 c(H).\nLemma 3.6. Consider phase Hi. Cost incurred by Robust for synchronization with marking in Line 1 is at most \u2206B(Hi\u22121 \u222aH\u2212i ).\nProof. Let M denote the k distinct most recently requested pages \u2013 these are marked pages during H\u2212i . We consider two cases.\nIf H\u2212i \u2229Hi\u22121 = \u2205, then whole H\u2212i was served by Follower. Each p \u2208 M must have been in the cache of both Follower and P when requested and Follower would evict it afterwards only if P did the same. Therefore, Robust needs to load at most \u2206P (H\u2212i ) = \u2206 B(H\u2212i ) pages.\nIf H\u2212i and Hi\u22121 overlap, let M \u2032 denote the set of pages marked during Hi\u22121. At the end of Hi\u22121, Robust loads M \u2032 and Follower loads only pages from M until the end of H\u2212i . Therefore, Robust starting Hi needs to evict only pages from M \u2032 \\M . Now, note that there are |M \u2032 \u222aM | distinct pages requested during Hi\u22121 \u222aH\u2212i and therefore |M \u2032 \\M | = |M \u2032 \u222aM | \u2212 k \u2264 \u2206B(Hi\u22121 \u222aH\u2212i ).\nWe consider costs incurred by Robust during window Wi for i = 1, . . . , log k + 1. Note that E1 = E + 1 = E \u2212 1 = \u2205, since W1 starts at the beginning of the phase and there are no clean pages arriving strictly before W1. Lemma 3.7. Expected cost incurred by Robustf during W1 is at most 2c2. For i = 2, . . . , log k+1, we have E[\u2206A(Wi)] \u2264 |E\u2212i\u22121|+ ci \u2212 ci\u22121 + 2(|E\u2212i |+ ci+1 \u2212 ci), denoting clog k+2 = c(H).\nProof. First, consider the costs during W1. There are c2 \u2212 c1 = c2 clean pages arriving during W1 and Robust has a page fault due to each of them, evicting a random unmarked page. In the worst case, all these clean pages arrive at the beginning of W1. Therefore, at arrival c2 + 1, there are c2 pages evicted which were chosen among unmarked pages uniformly at random. Let Ua denote the set of unmarked pages at arrival a. We have U1 = M (the initial cache content of Robust) and none of those pages get marked during first c2 arrivals. During every arrival a = c2 + 1, . . . , k/2 = S[2]\u2212 1, a single unmarked page is marked and we have |Ua| = k \u2212 (a\u2212 c2). As in the classical analysis of Marker (see (Borodin and El-Yaniv, 1998) and references therein), the probability of the requested unmarked page being evicted is c2/|Ua|. We have\n\u2206A(W1) = c2 +\nS[2]\u22121\u2211\na=c2+1\nc2 |Ua| \u2264 c2 + k 2 \u00b7 c2 k/2 = 2c2.\nFor i \u2265 2, there are ci pages evicted before the start of Wi: those in E+i\u22121 were evicted due to synchronization with the predictor and the rest were evicted in randomized evictions \u2013 those are loaded back to the cache at the beginning of Wi, causing cost ci \u2212 |E+i\u22121| = |E\u2212i\u22121| + ci \u2212 ci\u22121. After this synchronization, all unmarked pages are in the cache except those belonging to Ei.\nDuring Wi, pages from E\u2212i and ci+1 \u2212 ci new clean pages are requested causing page faults which are resolved by evicting a random unmarked page from the cache. In the worst case, these page faults all happen in the beginning of the window, leaving more time for page faults on randomly-evicted pages. Let a\u0304 denote the first arrival after these page faults and Ua\u0304 the set of unmarked pages at that moment. At arrival a\u0304, there are ci+1 pages missing from the cache: pages from E+i which are not going to be requested during Wi and |E\u2212i |+ ci+1 \u2212 ci = ci+1 \u2212 |E+i | unmarked pages were chosen uniformly at random from Ua\u0304 \\Ei = Ua\u0304 \\E+i . This is because only pages which were marked since the beginning of Wi until a\u0304 are those from E\u2212i and they were not present in the cache before they got marked. We compute the expected number of page faults on the randomly evicted pages. Since they are unmarked when evicted, such page faults can happen only on arrivals.\nAt arrival a, the set of unmarked pages Ua has size k \u2212 (a\u2212 ci+1). For any a \u2208 Wi such that a \u2265 a\u0304, we have Ua \u2229E\u2212i = \u2205 and pages in Ua \u2229E+i are evicted with probability 1. So, ci+1 \u2212 |E+i | evicted pages are picked uniformly at random from Ua \\E+i of size at least k\u2212 (a\u2212ci+1)\u2212|E+i |. Therefore, our expected cost is at most\nS[i+1]\u22121\u2211\na=a\u0304\nci+1 \u2212 |E+i | k \u2212 a+ ci+1 \u2212 |E+i | \u2264 \u2211\na\u2208Wi\nci+1 \u2212 |E+i | k \u2212 a \u2264 k 2i \u00b7 ci+1 \u2212 |E + i | k/2i = ci+1 \u2212 |E+i |\nwhich is equal to |E\u2212i\u22121|+ ci \u2212 ci\u22121. Note that k \u2212 a \u2265 k \u2212 S[i+ 1]\u2212 1 = 2log k\u2212i = k/2i.\nFor i = 1, . . . , log k, we define\nrank(Wi) := |Ei+1 \u2229Bi+1| \u2212\u2206B(Wi), where Bi denotes the cache content of Belady at the beginning of Wi. We do not define rank(Wi) for i = 1 + log k. Later, we relate rank to the prediction error. We have the following lemma.\nLemma 3.8. During a phase H , the expected cost of Robustf is at most\n3\nlog k+1\u2211\ni=1\nrank(Wi\u22121) + 3c(H) + 6\u2206 B(H).\nProof. First, we observe that\n|E\u2212i | \u2264 rank(Wi\u22121) + \u2206B(Wi\u22121) + \u2206B(Wi) (4)\nholds for i = 2, . . . , log k+1. This is because |E\u2212i \u2229Bi| \u2264 |Ei \u2229Bi| = rank(Wi\u22121) +\u2206B(Wi\u22121) and |E\u2212i \\ Bi| \u2264 \u2206B(Wi) due to pages from E\u2212i being requested during Wi and Belady having to load them. Combining equation 4 with Lemma 3.7, and summing over all windows, we get the statement of the lemma.\nNow, we relate the rank of a window to the prediction error.\nLemma 3.9. Denote \u03b7i the error of predictions arriving during Wi. We have\n\u03b7(Wi) \u2265 |F \u2229Wi| rank(Wi).\nProof. Prediction error at time t is \u03b7t = |Bt \\Pt|. At the end of Wi, it is at least |Ei+1 \u2229Bi+1|. Due to laziness of the predictor, |Bt \\Pt| can increase only if both predictor and Belady have a page fault: in that case it may increase by 1, see Observation 2.4. Therefore, at any time t during Wi, we have \u03b7t = |Bt \\ Pt| \u2265 |Ei+1 \u2229 Fi+1| \u2212\u2206B(Wi) = rank(Wi). Since we query the predictor at arrivals belonging to F , the total error of received predictions is at least |F \u2229Wi| rank(Wi).\nWe will analyze intervals of windows starting when some particular incorrect prediction was introduced and ending once it was corrected. The following lemma charging the increase of rank to the arriving clean pages and costs incurred by Belady will be used to bound the number of such intervals.\nLemma 3.10. For i = 1, . . . , log k, we have\nrank(Wi)\u2212 rank(Wi\u22121) \u2264 \u2206B(Wi\u22121) + ci+1 \u2212 ci, denoting W0 an empty window before W1 with rank(W0) = 0.\nProof. It is enough to show that\n|Ei+1 \u2229Bi+1| \u2264 |Ei \u2229Bi|+\u2206B(Wi) + ci+1 \u2212 ci.\nSince the right-hand side is always non-negative, we only need to consider the case when the left-hand side is positive. We show how to charge pages in Ei+1 \u2229 Bi+1 either to pages in Ei \u2229 Bi or to \u2206B(Wi) and ci+1 \u2212 ci. Since |Ei| = ci \u2264 ci+1 = |Ei+1|, we can construct an injective map \u03b2 : Ei \u2192 Ei+1, such that \u03b2(p) = p for each p \u2208 Ei \u2229 Ei+1. There are |Ei+1| \u2212 |Ei| = ci+1 \u2212 ci pages p such that \u03b2\u22121(p) is not defined. We show that, for each p \u2208 Ei+1 \u2229Bi+1 for which it is defined, \u03b2\u22121(p) is either a page in Ei \u2229Bi or a page loaded by Belady during Wi. There are two cases.\n\u2022 p \u2208 Ei \u2229 Ei+1 implying \u03b2\u22121(p) = p. Then either p \u2208 Bi and therefore p \u2208 Ei \u2229 Bi, or p \u2208 Bi+1 \\Bi implying that Belady has loaded p during Wi.\n\u2022 p /\u2208 Ei \u2229 Ei+1 implying q = \u03b2\u22121(p) \u2208 Ei \\ Ei+1. By Observation 3.2, q \u2208 E\u2212i , i.e., it must have been requested during Wi. If q \u2208 Bi then, q \u2208 Ei \u2229Bi. Otherwise, Belady must have loaded q during Wi.\nTo sum up: \u03b2 is an injective map and \u03b2\u22121(p) does not exist for at most ci+1 \u2212 ci pages p \u2208 Ei+1 \u2229Bi+1. All other p \u2208 Ei+1 \u2229Bi+1 are mapped by \u03b2\u22121 to a unique page either belonging to Ei \u2229Bi or loaded by Belady during Wi.\nProof of Lemma 3.3. Robustf receives a prediction only at an arrival belonging to F . Since F contains \u2211log k+1 i=1 |F \u2229Wi| \u2264 \u2211log k i=1 (f(i)\u2212f(i\u22121))+ |Wlog k+1| \u2264 f(log k)+1 arrivals, because |Wlog k+1| = 1 and f(0) = 0, there are at most f(log k) + 1 queries to the predictor. To prove equations (1,2,3), we always start with bounds proved in lemmas 3.5, 3.8 and 3.6. In the rest of the proof, we write H instead of Hi and X instead of Hi to simplify the notation.\nTo get equation equation 2, note that rank(Wi) \u2264 c(H) for each window i. Therefore, by lemmas 3.5, 3.8 and 3.6, we can bound E[\u2206A(H)] by\n\u2206B(X) + 3c(H) log k + 3c(H) + 6\u2206B(H) \u2264 O(log k)\u2206B(X).\nTo get equation equation 3, note that \u2206B(H) \u2264 k, c(H) \u2264 k, and rank(Wi) > 0 only if \u03b7i \u2265 |Ei+1 \u2229Bi+1| > 1. Therefore, we get \u2206A(H) \u2264 O(k) + \u03b7(H)O(k). Now we prove equation equation 1. We define Qm = {i | rank(Wi) < m and rank(Wi+1) \u2265 m} and Q = \u2211k m=1 |Qm|. We can bound Q using Lemma 3.10. We have\nQ = k\u2211\nm=1\n|Qm| = log k\u2211\ni=1\nmax{0, rank(Wi)\u2212 rank(Wi\u22121)} \u2264 log k\u2211\ni=1\n(2\u2206B(Wi) + ci+1 \u2212 ci), (5)\nwhich is equal to 2\u2206B(H) + c(H). We bound \u2211log k\nj=1 rank(Wj) as a function of Q and \u03b7(H)\nlog k\u2211\ni=1\nrank(Wi) \u2264 2Q \u00b7 f\u22121( a\u03b7(H)\nQ ). (6)\nThis relation is proved in Proposition 3.11 with a = 1 and, together with equation 5, gives us the desired bound\nE[\u2206A(H)] \u2264 O(1)\u2206B(H)f\u22121(a\u03b7(H) Q ). (7)\nProposition 3.11. log k\u2211\ni=1\nrank(Wi) \u2264 2Q \u00b7 f\u22121( \u03b7(H)\nQ ).\nProof. We rearrange the sum of ranks in the following way. We define Lm = {i | rank(Wi) \u2265 m}, and ai,m, such that Lm = \u22c3 i\u2208Qm(i, i+ ai,m] for each m. We can write\nlog k\u2211\ni=1\nrank(Wi) = k\u2211\nm=1\n|Lm| = k\u2211\nm=1\n\u2211\ni\u2208Qm ai,m. (8)\nOn the other hand, we can write \u03b7i \u2265 \u2211rank(Wi)\nm=1 |F\u2229Wi| (Lemma 3.9) which allows us to decompose the total prediction error \u03b7(H) as follows:\n\u03b7(H) \u2265 k\u2211\nm=1\n\u2211\ni\u2208Lm |F \u2229Wi| =\nk\u2211\nm=1\n\u2211\ni\u2208Qm\nai,m\u2211\nj=1\n|F \u2229Wi+j |.\nLet i\u2217 denote the first window such that |Wi\u2217 | < f(i\u2217) \u2212 f(i\u2217 \u2212 1). If i + ai,m < i\u2217, then\u2211ai,m j=1 |F \u2229Wi+j | = f(i + ai,m) \u2212 f(i) \u2265 f(ai,m) by convexity of f . If this is not the case, we\nclaim that \u2211ai,m\nj=1 |F \u2229Wi+j | \u2265 f(ai,m/2). This is clearly true if i+ \u2308ai,m/2\u2309 < i\u2217. Otherwise, we have \u2211ai,m j=1 |F \u2229Wi+j | \u2265 \u2211ai,m j=\u2308ai,m/2\u2309 |F \u2229Wi+j | \u2265 2\nai,m/2 because i+ ai,m \u2264 k and therefore |Wi+\u2308ai,m/2\u2309| \u2265 2ai,m/2. By our assumptions about f , we have f(ai,m/2) \u2264 2ai,m/2. So, we have the following lower bound on \u03b7(H):\n\u03b7(H) \u2265 k\u2211\nm=1\n\u2211\ni\u2208Qm f(ai,m/2). (9)\nBy convexity of f , this lower bound is smallest if all ami are the same, i.e., equal to the total rank divided by Q = \u2211 m |Qm| and then f(ai,m/2) = \u03b7(H)/Q for each i and m. Combining equation 8 and equation 9, we get\nlog k\u2211\ni=1\nrank(Wi)\n2 =\nk\u2211\nm=1\n\u2211\ni\u2208Qm f\u22121\n( f (ai,m\n2\n)) \u2264 Q \u00b7 f\u22121 (\u03b7(H) Q ) ."
        },
        {
            "heading": "4 WELL-SEPARATED QUERIES TO THE PREDICTOR",
            "text": ""
        },
        {
            "heading": "4.1 MTS",
            "text": "We consider the setting when we are able to receive a prediction once every a time steps, for some parameter a \u2208 N. Without loss of generality, we assume that T is a multiple of a. In time steps ia, where i = 1, . . . , T/a, we receive a prediction pi \u2208 M . State pi itself might be very bad, e.g. \u2113ia(pi) might be infinite. We use work functions to see whether there is a more suitable point nearby pi.\nWork functions. Consider an MTS on a metric space M with a starting state x0 \u2208 M and a sequence of cost functions \u21131, . . . , \u2113T . For each time step t = 1, . . . , T and state x \u2208 M , we define a work function as\nwt(x) = min { d(yt, x) + t\u2211\ni=1\nd(yi\u22121, yi) + \u2113i(yi) } ,\nwhere the minimum is taken over all y0, . . . , yt such that y0 = x0. In other words, it is the cheapest way to serve all the cost functions up to time t and end in state x. Work function is a major tool for design of algorithms for MTS and satisfies the following property.\nObservation 4.1. For any x, y \u2208 M and any time t, we have wt(x) \u2264 wt(y) + d(y, x).\nThis holds because one way to serve the cost functions \u21131, . . . , \u2113t is to follow the best solution which ends in state y and then move to x. If wt(pi) = wt(y) + d(y, pi), we can see that pi is not a very good state, since the best solution ending in pi goes via y. We say that pi is supported by y.\nAlgorithm of Emek et al. (2009). The following algorithm proposed by Emek et al. (2009) in the context of advice complexity receives state of an optimum solution every a time steps.\nAlgorithm 3: One cycle of algorithm by Emek et al. (2009) 1 qi := reported state of OFF at time ia; 2 j := 0, c := 0; 3 for t = ia+ 1, . . . , (i+ 1)a do 4 x\u2032t := argminx\u2208B(qi,2j){d(x, xt\u22121) + \u2113t(x)}; 5 while c+ d(x, x\u2032t) + \u2113t(x\u2032t) > 2j do 6 j := 2 \u2217 j; 7 x\u2032t := argminx\u2208B(qi,2j){d(x, xt\u22121) + \u2113t(x)}; 8 xt := x \u2032 t, c := c+ d(x, xt) + \u2113t(xt);"
        },
        {
            "heading": "4.2 ALGORITHM FTSP",
            "text": "Given qi\u22121 and \u2113t for t = (i\u2212 1)a+ 1, . . . , ia, we define\nwfi(x) = min { d(xia, x) +\ni\u03b1\u2211\nj=(i\u22121)a+1 d(xj\u22121, xj) + \u2113j(xj)\n} ,\nwhere the minimum is taken over x(i\u22121)\u03b1, . . . xi\u03b1 \u2208 M such that x(i\u22121)a = qi\u22121. In fact, it is the work function at the end of an MTS instance with initial state qi\u22121 and request sequence \u2113t for t = (i\u2212 1)a+ 1, . . . , ia. Instead of pi, we choose point\nqi = arg min x\u2208M\n{ wfi(x) \u2223\u2223\u2223\u2223 wfi(x) = wfi(pi)\u2212 d(x, pi) } ,\ni.e., the \"cheapest\" state supporting pi in wfi. After computing qi, we run one cycle of Algorithm 3. This algorithm which we call \u201cFollow the Scarce Predictions\u201d (FtSP), is summarized in Algorithm 4.\nAlgorithm 4: FtSP 1 for i = 0, . . . , T/a do 2 receive prediction pi; 3 use pi to compute qi; 4 run one cycle of Algorithm 3 starting at qi;\nLet Q denote the best (offline) algorithm which is located at qi at time step ia for each i = 1, . . . , T/a. We have\ncost(Q) =\nT/a\u2211\ni=1\nwfi(qi).\nWe can relate the cost of Q to the prediction error using the following lemma. Together with Proposition 2.5, it gives a bound\ncost(ALG) \u2264 O(a)(OFF+2\u03b7), implying Theorem 1.3. Lemma 4.2. Let OFF be an arbitrary offline algorithm and oi denote its state at time ia for i = 1, . . . , T/a. If Q was computed from predictions p1, . . . , pT/a, we have\ncost(Q) \u2264 OFF+2\u03b7, where \u03b7 = \u2211T/a i=1 d(pi, oi) is the prediction error with respect to OFF.\nProof. Denote Ai an algorithm which follows the steps of Q until time ia and then follows the steps of OFF. We have\ncost(Ai) \u2264 cost(Ai\u22121) + wfi(qi)\u2212 wfi(oi) + d(qi, oi) because both Ai and Ai\u22121 are at qi\u22121 at time (i\u2212 1)a, and Ai\u22121 then travels to oi paying wfi(oi) while Ai travels to qi at ia paying wfi(qi) and its costs after ia will be by at most d(qi, oi) larger than the costs of Ai\u22121.\nBy Observation 4.1 and the choice of qi, we have wfi(oi) \u2265 wfi(pi)\u2212 d(oi, pi) = wfi(qi) + d(qi, pi)\u2212 d(oi, pi).\nCombining the two preceding inequalities, we get cost(Ai) \u2264 cost(Ai\u22121) + wfi(qi)\u2212 wfi(qi)\n+ d(oi, pi)\u2212 d(pi, qi) + d(qi, oi) \u2264 cost(Ai\u22121) + 2d(oi, pi),\nwhere the last step follows from the triangle inequality.\nSince OFF = A0 and Q = AT/a, we have\nQ \u2264 OFF+2 T/a\u2211\ni=1\nd(oi, pi) = OFF+2\u03b7."
        },
        {
            "heading": "4.3 CACHING",
            "text": "Follower cannot maintain 1-consistency in this setting. For the sake of theoretical bound, we can do the following: We serve the whole input sequence by subsequent phases of Robustf which is O(1)-consistent with F chosen in such a way that the arrivals in F are separated by at least a. We prove the following replacement of Proposition 3.11. Proposition 4.3.\nlog k\u2211\ni=1\nrank(Wi) \u2264 2Q \u00b7 f\u22121 ( a \u03b7(H)\nQ\n) .\nProof. We rearrange the sum of ranks in the following way. We define Lm = {i | rank(Wi) \u2265 m}, Qm = {i | rank(Wi) < m and rank(Wi+1) \u2265 m}, and ai,m, such that Lm = \u22c3 i\u2208Qm(i, i+ ai,m] for each m. We can write log k\u2211\ni=1\nrank(Wi) = k\u2211\nm=1\n|Lm| = k\u2211\nm=1\n\u2211\ni\u2208Qm ai,m. (10)\nOn the other hand, we can write \u03b7i \u2265 \u2211rank(Wi)\nm=1 |F\u2229Wi| (Lemma 3.9) which allows us to decompose the total prediction error \u03b7(H) as follows:\n\u03b7(H) \u2265 k\u2211\nm=1\n\u2211\ni\u2208Lm |F \u2229Wi| =\nk\u2211\nm=1\n\u2211\ni\u2208Qm\nai,m\u2211\nj=1\n|F \u2229Wi+j |.\nLet i\u2217 denote the first window such that |Wi\u2217 |a < f(i \u2217) \u2212 f(i\u2217 \u2212 1). If i + ai,m < i\u2217, then\u2211ai,m\nj=1 |F \u2229Wi+j | = f(i + ai,m) \u2212 f(i) \u2265 f(ai,m) by convexity of f . If this is not the case, we claim that \u2211ai,m j=1 |F \u2229Wi+j | \u2265 a\u22121f(ai,m/2).\n\u2022 If i+ \u2308ai,m/2\u2309 < i\u2217: we have ai,m\u2211\nj=1\n|F \u2229Wi+j | \u2265 \u2308ai,m/2\u2309\u2211\nj=1\n|F \u2229Wi+j | \u2265 f(ai,m/2).\n\u2022 Otherwise: we have ai,m\u2211\nj=1\n|F\u2229Wi+j | \u2265 ai,m\u2211\nj=\u2308ai,m/2\u2309 |F\u2229Wi+j | \u2265\nai,m\u2211\nj=\u2308ai,m/2\u2309\n1 a |Wi+j | \u2265 1 a 2ai,m/2 \u2265 a\u22121f(ai,m/2)\nBy our assumptions about f saying that f(ai,m/2) \u2264 2ai,m/2.\nSo, we have the following lower bound on \u03b7(H):\na\u03b7(H) \u2265 k\u2211\nm=1\n\u2211\ni\u2208Qm f(ai,m/2). (11)\nBy convexity of f , this lower bound is smallest if all ami are the same, i.e., equal to a\u03b7(H) divided by Q = \u2211 m |Qm| and then f(ai,m/2) = a\u03b7(H)/Q for each i and m. Combining equation 10 and equation 11, we get\nlog k\u2211\ni=1\nrank(Wi)\n2 =\nk\u2211\nm=1\n\u2211\ni\u2208Qm f\u22121\n( f (ai,m\n2\n)) \u2264 Q \u00b7 f\u22121 (a\u03b7(H) Q )\nUsing the proposition above in Equation equation 6 in the proof of Lemma 3.3 gives us the following smoothness bound:\nLemma 4.4. Denote Xi = Hi\u22121 \u222aH\u2212i \u222aHi. During the phase Hi, Robustf receiving at most one prediction in a time steps incurs the cost\nE[\u2206A(Hi)] \u2264 O(1)f\u22121 ( a\u03b7(Hi)\n\u2206B(Xi)\n) \u2206B(Xi).\nTheorem 1.5 follows from summation of the bound above over all phases of Robust and concavity of f\u22121, as in proof of Theorem 1.1."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We perform an empirical evaluation of our caching algorithm F&R on the same datasets and with the same predictors as the previous works (Lykouris and Vassilvitskii, 2021; Antoniadis et al., 2023; Im et al., 2022). We use the following datasets.\n\u2022 BrightKite dataset (Cho et al., 2011) contains data from a certain social network. We create a separate caching instance from the data of each user, interpreting check-in locations as pages. We use it with cache size k = 10 and choose instances corresponding to the first 100 users with the longest check-in sequences requiring at least 50 page faults in the optimal policy.\n\u2022 CitiBike dataset contains data about bike trips in a bike sharing platform CitiBike. We create a caching instance from each month in 2017, interpreting starting stations of the trips as pages, and trimming length of each instance to 25 000. We use it with cache size k = 100.\nSome of the algorithms in our comparison use next-arrival predictions while F&R uses action predictions that can be generated from next-arrival predictions. Therefore, we use predictors which predict the next arrival of the requested page and convert it to action predictions. This process was used and described by Antoniadis et al. (2023) and we use their implementation of the predictors. Our algorithm is then provided limited access to the resulting action predictions while the algorithm of Im et al. (2022) has limited access to the original next-arrival predictions.\n\u2022 Synthetic predictions: compute the exact next arrival time computed from the data and add noise to this number. This noise comes from a log-normal distribution with the mean parameter \u00b5 = 0 and the standard deviation parameter \u03c3. We use \u03c3 \u2208 [0, 50].\n\u2022 PLECO predictor proposed by Anderson et al. (2014): This model estimates the probability p of a page being requested in the next time step and we interpret this as a prediction that the next arrival of this page will be in 1/p time steps. The model parameters were fitted to BrightKite dataset and not adjusted before use on CitiBike.\n\u2022 POPU \u2013 a simple predictor used by Antoniadis et al. (2023): if a page appeared in p fraction of the previous requests, we predict its next arrival in 1/p time steps.\nIn our comparison, we include the following algorithms: offline algorithm Belady which we use to compute the optimal number of page faults OPT, standard online algorithms LRU and Marker (Fiat et al., 1991), ML-augmented algorithms using next arrival predictions L&V (Lykouris and Vassilvitskii, 2021), LMark and LnonMark (Rohatgi, 2020), FtPM which, at each step, evicts an unmarked page with the furthest predicted next arrival time, and algorithms for action predictions FtP and T&D (Antoniadis et al., 2023). We use the implementation of all these algorithms published by Antoniadis et al. (2023). We implement algorithm AQ (Im et al., 2022) and our algorithm F&R.\nNotes on implementation of F&R. We follow the recommendations in Section 3 except that Follower switches to Robust whenever its cost is \u03b1 = 1 times higher compared to Belady in the same period. With higher \u03b1, the performance of F&R approaches FtP on the considered datasets. With k = 10 (BrightKite dataset), we use F = [1, 6, 9] corresponding to f(i) = i. Note that, with such small k, polynomial and exponential f would also give a very similar F . With k = 100 (CitiBike dataset), we use exponential f(i) = 2i+1\u22121. With a-separated queries, Follower uses LRU heuristic when prediction is unavailable, and Robust ignores F , querying the predictor at each page fault separated from the previous query by at least a time steps.\nResults. Figures 1 and 3 contain averages of 10 independent experiments. Figure 1 shows that the performance of F&R with high-quality predictions is superior to the previous ML-augmented algorithms except for FtP which follows the predictions blindly and is also 1-consistent. With high \u03c3, the performance of T&D becomes better. This is true also for F&R with F = [1..10], suggesting that T&D might be more efficient in using erroneous predictions. Figure 2 shows the total number of times algorithms query the predictor over all instances. Response to such query is a single page missing from predictor\u2019s cache in the case of F&R and T&D and next arrival times of b pages in the case of AQ_b. Note that FtPM is equivalent to the non-parsimonious version of AQ with b = k. F&R makes the smallest number of queries: with perfect predictions, it makes exactly OPT queries and this number decreases with higher \u03c3 as F&R spends more time in Robust.\nFigure 3 shows that F&R performs well in regime with a-separated queries. While the performance of FtPM with POPU predictor worsens considerably towards Marker already with a = 5, the performance of F&R worsens only very slowly. On CitiBike dataset, it keeps its improvement over Marker even with a = 20 (note that we use k = 100 with this dataset). Predictions produced by PLECO seem much less precise as suggested by FtP with PLECO being worse than Marker and smaller number of such predictions either improves (AQ, FtPM) or does not affect performance (F&R) of considered algorithms.\nFigures 4 and 5 complements the comparison of F&R to existing ML-augmented algorithms for paging by including those omitted in Figure 1. With smaller \u03c3, it again demonstrates the better consistency of F&R. With higher \u03c3, F&R achieves performance comparable to both L&V and LnonMark, while using a smaller number of predictions. We have decided not to include LMark because its performance as well as number of predictions used were almost the same as of LnonMark. Note that, in the case of both algorithms, the number of used predictions is equal to the number of clean arrivals and therefore it does not change with the prediction error.\nFigures 6 and 7 shows performance of F&R in regime with a-separated queries for different values of a. It shows a significant loss of consistency already with a = 2 compared to a = 1. However, with higher noise parameter \u03c3, the difference in performance does not seem large. In this regime, the focus is on the gap between predictor queries rather than the total number of queries: F&R queries a predictor at each page fault separated from previous query by at least a time steps. However, we decided to include also the plot of the total number of queries (Figure 7) because it shows that with \u03c3 > 20, F&R with a = 1 uses a smaller number of predictions than with a = 2 and even a = 3, while maintaining a better performance. This suggests that the freedom to choose the right moment for a query might be more important for the performance than the total number of used predictions.\nFigure 8 shows experiments with a probabilistic predictor on the BrightKite dataset. In this setting, we consider a predictor that evicts the page requested furthest in the future with a given probability 1\u2212 p. On the other hand, it evicts a random page with probability p. The horizontal axis corresponds to the probability p. We can observe that better consistency of our algorithm compared to T&D is visible for p up to 0.4.\nEach plot and table contains averages of 10 independent experiments. We have seen standard deviations at most 0.004 in the case of figures 1, 4, 6; 0.0015 for Figure 3 on CitiBike dataset and 0.0025 on BrightKite dataset, and 300 for figures 2, 5, 7, counting numbers of used predictions."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "We present algorithms for MTS and caching with action predictions working in the setting where the number of queries or the frequency of querying the predictor are limited. We have shown that one can achieve theoretical as well as empirical performance comparable to the setting with unlimited\naccess to the predictor, possibly enabling usage of precise but heavy-weight prediction models in environments with scarce computational resources."
        },
        {
            "heading": "7 LOWER BOUNDS",
            "text": ""
        },
        {
            "heading": "7.1 CACHING",
            "text": "Proof of the following proposition can be found in (Borodin and El-Yaniv, 1998, Theorem 4.4).\nProposition 7.1 ((Fiat et al., 1991)). For any randomized algorithm ALG for caching there is an input instance on universe of k + 1 pages such that the expected cost of ALG is more than ln k times the cost of the offline optimal solution.\nFor a given algorithm, it constructs an instance consisting of marking phases, each with a single clean page such that the optimal algorithm pays 1 and the online algorithm pays at least ln k.\nImagine an algorithm receiving at most 0.5OPT predictions during this instance. Then, there must be at least 0.5OPT phases during which the algorithm receives no prediction. Its cost is at least ln k in each such phase, giving total cost 0.5OPT ln k.\nTheorem 1.2 is implied by the following more general statement with c = 1 and d = 0.\nTheorem 7.2. Let c \u2265 1 and d \u2265 0 be constants. Any (cf\u22121(\u03b7) + d)-smooth algorithm for caching with action predictions has to use at least f(c\u22121 ln k \u2212 d)OPT predictions.\nProof. Consider a fixed algorithm accepting action predictions. Choose T long enough, an arbitrary prediction for each time step t = 1, . . . , T , and give them to the algorithm at time 0. Having the predictions already, this algorithm becomes a standard randomized algorithm which does not use any further predictions. We use Proposition 7.1 to generate an instance such that E[ALG] \u2265 OPT ln k, where ALG denotes the cost of the algorithm with predictions generated in advance. It is clear that these predictions, generated before the adversary has chosen the input instance, are useless, not helping the algorithm to surpass the worst-case bounds. However, since the universe of pages has size only k + 1, each of the predictions can differ from an optimal algorithm by at most one page.\nIf we want to have E[ALG]OPT \u2264 cf\u22121( \u03b7 OPT ) + d, then we need\n\u03b7\nOPT \u2265 f ( E[ALG] cOPT \u2212 d ) > f ( ln k c \u2212 d ) .\nSince every prediction has error at most 1, we need to receive at least \u03b7 \u2265 f(c\u22121 ln k \u2212 d)OPT predictions."
        },
        {
            "heading": "7.2 MTS",
            "text": "Antoniadis et al. (2023) showed the following lower bound on smoothness of algorithms for general MTS with action predictions.\nProposition 7.3 (Antoniadis et al. (2023)). For \u03b7 \u2265 0 and n \u2208 N, every deterministic (or randomized) online algorithm for MTS on the n-point uniform metric with access to an action prediction oracle with error at least \u03b7 with respect to some optimal offline algorithm has competitive ratio \u2126 ( min { \u03b1n, 1 + \u03b7 OPT }) , where \u03b1n = \u0398(n) (or \u03b1n = \u0398(log n)) is the optimal competitive ratio of deterministic (or randomized) algorithms without prediction.\nWe use this proposition to prove the following theorem from which Theorem 1.4 directly follows.\nTheorem 7.4. For \u03b7 \u2265 0 and n \u2208 N, every deterministic (or randomized) online algorithm for MTS on the n-point uniform metric with access to an action prediction oracle at most once in a time steps with error at least \u03b7 with respect to some optimal offline algorithm has competitive ratio \u2126 ( min { \u03b1n, 1 + a\u03b7 OPT }) , where \u03b1n = \u0398(n) (or \u03b1n = \u0398(log n)) is the optimal competitive ratio of deterministic (or randomized) algorithms without prediction.\nProof. We extend the (n\u2212 1)-point uniform metric from the proposition above by a single point p\u221e whose cost will be +\u221e at each time step, ensuring the optimal algorithm will never be located there. Consider a fixed algorithm and a predictor producing at most one prediction in a time steps with the total prediction error \u03b7. By issuing prediction p\u221e in all missing time steps, we complete predictions for each time step with error at least \u03b7\u2032 \u2265 a\u03b7. By proposition above, the algorithm with completed predictions has competitive ratio at least\n\u2126 ( min { \u03b1n\u22121, 1 + \u03b7\u2032\nOPT\n}) \u2265 \u2126 ( min { \u03b1n, 1 + a\u03b7\nOPT\n}) ,\nsince \u03b1n and \u03b1n\u22121 differ by at most a constant factor."
        },
        {
            "heading": "8 FITF ORACLE",
            "text": "In this section we work with a predictor which tells us which page in our current cache will be requested furthest in the future, we call it a FitF page. Note that this is not the same as the predictions considered in Section 3, where we receive a page not present in Belady\u2019s cache. Belady evicts a FitF page from its current cache content which may be different from the FitF page from the current cache content of our algorithm. Prediction error is the total number of times the predictor reports an incorrect FitF page.\nWe split our algorithm into Follower and Robust part. The Follower (Algorithm 5), checks at each page fault whether Belady starting at the same time with the same cache content also has a page fault. If yes, it evicts a page reported by the predictor. Otherwise, it switches to the Robust part (Algorithm 6).\nLemma 8.1. Consider one execution of Algorithm 5, denoting \u03c3 the request subsequence and \u03c6 the number of incorrect predictions received during this execution. Algorithm 5 pays the same cost as Belady serving \u03c3 and starting with cache content P . There is a tie-breaking rule for Belady such that the cache contents of both algorithms after processing \u03c3 differ in at most \u03c6 pages.\nProof. Whenever the algorithm has a page fault and Belady not, the execution of Algorithm 5 terminates. Therefore, both algorithms have the same cost during the execution.\nAlgorithm 5: Follower with FitF oracle 1 P := starting cache content; 2 foreach pagefault do 3 Compute Belady for the sequence from the beginning of this execution starting with P ; 4 if Belady has page fault as well then 5 p := page in the current cache chosen by the predictor; 6 evict p; 7 else 8 Run one phase of Algorithm 6 starting with the current cache content;\nDenote A and B the cache contents of our algorithm and Belady respectively. We choose the following tie-breaking rule for Belady: whenever the algorithm evicts p \u2208 A \u2229B which is no more requested in \u03c3, Belady evicts p as well. The size of A \\B increases only when the algorithm evicts a predicted page p \u2208 A \u2229B and Belady evicts a different page q \u2208 A \u2229B. This can happen only if the next request of p comes earlier than q by the tie-breaking rule above. Since p, q \u2208 A, the oracle made a prediction error.\nRobust part (Algorithm 6) uses a parameter b which controls the number of predictions used during its execution. It runs for a duration of a single marking phase split into log k windows, as in Section 3, making sure that the number of predictions received in each window is at most the number of clean pages received so far. Evictions of random unmarked pages are used at page faults with no available prediction. At the end, it loads all marked pages. This is to ensure that the difference between the optimal and algorithm\u2019s cache content can be bounded by the cost of the optimal algorithm during the phase (using Observation 2.2) instead of accumulating over repeated executions of Follower and Robust.\nAlgorithm 6: Robust with FitF oracle 1 P := starting cache content; 2 S := [t = k \u2212 2j + 1 | for j = log k, . . . , 0]; 3 Wi := [S[i], S[i+ 1]\u2212 1] for i = 1, . . . , log k + 1 ; // Split the phase into\nwindows 4 foreach pagefault at time t during the phase do 5 ct := number of clean pages which arrived so far; 6 if number of received predictions in the phase is less than bct then 7 if number of received predictions in this window is less than ct then 8 p := page in the current cache chosen by the predictor; 9 evict p;\n10 else 11 evict a random unmarked page;\n12 Once phase has ended, load all marked pages to the cache and run Algorithm 5;\nLemma 8.2. Consider one execution of Algorithm 6 during which it receives \u03c6 incorrect predictions. The expected cost incurred by Algorithm 6 is at most 2\u2206B + 3\u03c6(1 + b\u22121 log k), where \u2206B denotes the cost incurred by Belady starting at the same time with the same cache content.\nProof. There are three kind of page faults:\n1. evicted page is chosen by the predictor\n2. requested page was chosen before by the predictor, evicted page was chosen at random\n3. both evicted and requested pages were chosen at random\nIn the worst case, we can assume that once we run out of budget for predictions, all incorrectly evicted pages are requested in page faults of type 2 and returned to the cache. Now, let g denote the number of pages evicted due to correct predictions \u2013 they are not going to be requested in this phase anymore (Observation 2.3). All other evicted pages are chosen uniformly at random among unmarked pages which were not evicted due to correct predictions. So, until another batch of page faults of type 1, we have only page faults on arrivals and the probability of a page fault on arrival a is at most\nca \u2212 g k \u2212 (a\u2212 ct)\u2212 g ,\nwhere ca is the number of clean pages until arrival a and k \u2212 (a\u2212 ct) is the number of unmarked pages, at most g of them were evicted due to correct predictions.\nWe count the number of page faults in window i for i = 1, . . . , log k + 1. We denote mi the number of page faults of type 1 and resulting into eviction of gi correctly predicted pages. Then, by our assumption, we have mi \u2212 gi page faults of type 2. The expected number of page faults of type 3 depends on when do types 1 and 2 happen. In the worst case, they all happen in the beginning of Wi as well as all arrivals of clean pages. We consider three cases.\nCase A. Prediction budget was not depleted, there were only evictions of type 1.\n\u2206A(Wi) = mi = \u03c6i + gi.\nCase B. There were mi = ci+1 predictions during Wi and we have \u03c6i = ci+1 \u2212 gi. After page faults of type 2, there are at most ci+1 \u2212 gi randomly chosen unmarked pages evicted. Therefore, the expected number of page faults of type 3 is at most\n\u2211\na\u2208Wi\nci+1 \u2212 ga k \u2212 (a\u2212 ci+1)\u2212 ga \u2264 \u2211\na\u2208Wi\nci+1 \u2212 gi k \u2212 (a\u2212 ci+1)\u2212 gi\n\u2264 \u2211\na\u2208Wi\nci+1 \u2212 gi k \u2212 a \u2264 k 2i \u00b7 ci+1 \u2212 gi k/2i = ci+1 \u2212 gi.\nTherefore, counting evictions of types 1, 2, and 3, we have\n\u2206A(Wi) \u2264 (\u03c6i + gi) + \u03c6i + (ci+1 \u2212 gi) \u2264 gi + 3\u03c6i.\nCase C. There were bci+1 predictions since the beginning of the phase. We have mi \u2264 ci+1 and ci+1 \u2212 gi \u2264 1b (bci+1 \u2212 gi) \u2264 1b\u03c6 where \u03c6 is the total number of incorrect predictions received since the beginning of the phase. We have\n\u2206A(Wi) \u2264 ci+1 + (ci+1 \u2212 gi) + (ci+1 \u2212 gi) \u2264 gi + 3(ci+1 \u2212 gi), which is at most gi + 3\u03c6/b.\nNow, the sum of costs over all the windows is at most \u2211\ni\ngi + \u2211\ni\n3\u03c6i + \u2211\ni\n3\u03c6/b+ c \u2264 2c+ 3\u03c6+ 3\u03c6 b log k,\nwhere c = \u2211\ni ci \u2264 \u2206B , because we consider Belady starting with the same cache content as the algorithm which does not contain the clean pages.\nTheorem 8.3. Let b \u2208 {1, . . . , log k} be a parameter. During a request sequence with optimum cost OPT, our algorithm receives at most O(b)OPT predictions and its expected cost is always bounded by O(log k)OPT. If only \u03c6 predictions are incorrect, its expected cost is at most\n( 2 + \u03c6\nOPT (4 + 3b\u22121 log k)\n) OPT .\nMoreover, if \u03c6 = 0, its cost is equal to OPT.\nProof. We split the time horizon into intervals corresponding to executions of Follower and Robust. For each interval i, we denote \u03c6i the number of received incorrect predictions, \u2206Bi the cost incurred by Belady started with the same content as our algorithm and \u2206Oi the cost incurred by the optimal solution during interval i. We denote F the set of intervals during which Follower was executed and R the set of intervals during which Robust was executed. We also define 0 \u2208 R an empty interval in the beginning of the request sequence with \u2206O0 = \u2206 B 0 = 0.\nIn order to prove bounds on robustness and number of used predictions, we provide relations between \u2206Bi and \u2206 O i independent of \u03c6. For each i \u2208 F , we have i \u2212 1 \u2208 R. Interval i \u2212 1 is a marking phase and Robust has all marked pages in the cache at the end (Lemma 8.2). By Observation 2.2, the starting cache content of Follower in interval i differs from optimal cache in at most \u2206Oi\u22121 pages. Therefore, we have \u2206Bi \u2264 \u2206Oi +\u2206Oi\u22121 \u2200i \u2208 F. (12) For each i \u2208 R, we have i\u2212 1 \u2208 F and i\u2212 2 \u2208 R. By Observation 2.4, the difference between the cache of Follower and optimum increases during interval i\u2212 1 by at most \u2206Oi\u22121. Since the starting cache of Follower in interval i\u2212 1 differs from optimal in \u2206Oi\u22122 pages, the starting cache of Robust in interval i differs from optimum by at most \u2206Oi\u22122 +\u2206 O i\u22121. Therefore, we have\n\u2206Bi \u2264 \u2206Oi +\u2206Oi\u22121 +\u2206Oi\u22122 \u2200i \u2208 R. (13)\nUsing equations equation 12 and equation 13, we can bound the number of used predictions as \u2211\ni\u2208F \u2206Bi +\n\u2211 i\u2208R b\u2206Bi \u2264 3bOPT .\nSince \u03c6 \u2264 \u2206Bi , for i \u2208 F , and \u03c6i \u2264 b\u2206Bi for i \u2208 R, we have the following robustness bound:\nALG \u2264 \u2211\ni\u2208F \u2206Bi +\n\u2211\ni\u2208R\n( \u2206Bi + \u03c6i(3 + 3b \u22121 log k) )\n\u2264 \u2211\ni\u2208F \u2206Bi +\n\u2211 i\u2208R \u2206Bi (1 + b)(3 + 3b \u22121 log k)\n\u2264 OPT \u00b7O(log k), where the last inequality follows from equation 12, equation 13, and b \u2264 log k. Now, we analyze smoothness. We can bound \u2206Bi \u2212 \u2206Oi by the difference between optimal and algorithm\u2019s cache in the beginning of the interval i. This is at most \u03c6i\u22121 for each i \u2208 R (Lemma 8.1) and at most \u2206Oi\u22121 for each i \u2208 F by equation 12. Lemmas 8.1 and 8.2 imply\nALG \u2264 \u2211\ni\u2208F \u2206Bi +\n\u2211\ni\u2208R\n( 2\u2206Bi + \u03c6i(3 + 3b \u22121 log k) )\n\u2264 \u2211\ni\u2208F (\u2206Oi +\u2206 O i\u22121) +\n\u2211\ni\u2208R\n( \u2206Oi + \u03c6i\u22121 + \u03c6i(3 + 3b \u22121 log k) )\n\u2264 2OPT+\u03c6(4 + 3b\u22121 log k).\n1-consistency of our algorithm can be seen from the fact that each execution of Robust is triggered by an incorrect prediction. Therefore, with perfect predictions, only Follower is used and behaves the same as Belady."
        },
        {
            "heading": "A COMPUTATIONS FOR TABLE 1",
            "text": "In this section, we present the computations for the numbers of predictions we obtained in Table 1.\nFor f(i) = i and f(i) = i2, we have f(log k) equal to log k and log2 k respectively.\nFor f(i) = 2i \u2212 1, we identify the first window i longer than f(i)\u2212 f(i\u2212 1). Note that the length of window i is k/2i = 2log k\u2212i and this is equal to the sum of lengths of the windows j > i. The total number of predictions used will be therefore f(i) + 2log k\u2212i. For i = log \u221a k + 1, we have f(i)\u2212f(i\u22121) = 2i = 2 12 log k+1 > 2log k\u2212i. Therefore, we use 2i\u22121+2log k\u2212i \u2265 3 \u221a k predictions in each robust phase. Since offline optimum has to pay at least 1 per robust phase, we use at most O( \u221a k)OPT predictions in total.\nFor f(i) = 0, we ask for a prediction at each arrival of a clean page. The number of queries used will therefore be at most the number of clean arrivals, which is at most 2OPT."
        }
    ],
    "year": 2023
}