{
    "abstractText": "We present a novel approach to adapting pre-trained large language models (LLMs) to perform question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire system is trained end-to-end and operates directly on spectrograms, simplifying our architecture. Key to our approach is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a \u2018cross-modal\u2019 chain-of-thought within a single decoding pass. Our method surpasses existing spoken language models in speaker preservation and semantic coherence. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets.",
    "authors": [],
    "id": "SP:1f7a2aeffdfe423aca2dad067e13a523023cef39",
    "references": [
        {
            "authors": [
                "Junyi Ao",
                "Rui Wang",
                "Long Zhou",
                "Chengyi Wang",
                "Shuo Ren",
                "Yu Wu",
                "Shujie Liu",
                "Tom Ko",
                "Qing Li",
                "Yu Zhang"
            ],
            "title": "Speecht5: Unified-modal encoder-decoder pre-training for spoken language processing",
            "venue": "arXiv preprint arXiv:2110.07205,",
            "year": 2021
        },
        {
            "authors": [
                "Rosana Ardila",
                "Megan Branson",
                "Kelly Davis",
                "Michael Henretty",
                "Michael Kohler",
                "Josh Meyer",
                "Reuben Morais",
                "Lindsay Saunders",
                "Francis M Tyers",
                "Gregor Weber"
            ],
            "title": "Common voice: A massivelymultilingual speech",
            "year": 1912
        },
        {
            "authors": [
                "Jonathan Berant",
                "Andrew Chou",
                "Roy Frostig",
                "Percy Liang"
            ],
            "title": "Semantic parsing on freebase from question-answer pairs",
            "venue": "In Proceedings of the 2013 conference on empirical methods in natural language processing,",
            "year": 2013
        },
        {
            "authors": [
                "Zal\u00e1n Borsos",
                "Rapha\u00ebl Marinier",
                "Damien Vincent",
                "Eugene Kharitonov",
                "Olivier Pietquin",
                "Matthew Sharifi",
                "Olivier Teboul",
                "David Grangier",
                "Marco Tagliasacchi",
                "Neil Zeghidour"
            ],
            "title": "AudioLM: a language modeling approach to audio generation",
            "year": 2022
        },
        {
            "authors": [
                "Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Mark Diaz",
                "Orhan Firat",
                "Michele Catasta",
                "Jason Wei",
                "Kathy Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah"
            ],
            "title": "Fiedel. PaLM: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Yu-An Chung",
                "Yu Zhang",
                "Wei Han",
                "Chung-Cheng Chiu",
                "James Qin",
                "Ruoming Pang",
                "Yonghui Wu"
            ],
            "title": "w2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training",
            "venue": "In ASRU,",
            "year": 2021
        },
        {
            "authors": [
                "Ann Clifton",
                "Aasish Pappu",
                "Sravana Reddy",
                "Yongze Yu",
                "Jussi Karlgren",
                "Ben Carterette",
                "Rosie Jones"
            ],
            "title": "The spotify podcast dataset",
            "venue": "arXiv preprint arXiv:2004.04270,",
            "year": 2020
        },
        {
            "authors": [
                "Alexandre D\u00e9fossez",
                "Jade Copet",
                "Gabriel Synnaeve",
                "Yossi Adi"
            ],
            "title": "High fidelity neural audio",
            "venue": "compression. CoRR,",
            "year": 2022
        },
        {
            "authors": [
                "Keqi Deng",
                "Songjun Cao",
                "Yike Zhang",
                "Long Ma"
            ],
            "title": "Improving hybrid CTC/attention end-to-end speech recognition with pretrained acoustic and language models",
            "venue": "In ASRU,",
            "year": 2021
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Heewoo Jun",
                "Christine Payne",
                "Jong Wook Kim",
                "Alec Radford",
                "Ilya Sutskever"
            ],
            "title": "Jukebox: A generative model for music",
            "year": 2005
        },
        {
            "authors": [
                "Yassir Fathullah",
                "Chunyang Wu",
                "Egor Lakomkin",
                "Junteng Jia",
                "Yuan Shangguan",
                "Ke Li",
                "Jinxi Guo",
                "Wenhan Xiong",
                "Jay Mahadeokar",
                "Ozlem Kalinli"
            ],
            "title": "Prompting large language models with speech recognition abilities",
            "venue": "arXiv preprint arXiv:2307.11795,",
            "year": 2023
        },
        {
            "authors": [
                "Daniel Galvez",
                "Greg Diamos",
                "Juan Ciro",
                "Juan Felipe Cer\u00f3n",
                "Keith Achorn",
                "Anjali Gopi",
                "David Kanter",
                "Maximilian Lam",
                "Mark Mazumder",
                "Vijay Janapa Reddi"
            ],
            "title": "The people\u2019s speech: A large-scale diverse english speech recognition dataset for commercial usage",
            "venue": "arXiv preprint arXiv:2111.09344,",
            "year": 2021
        },
        {
            "authors": [
                "Yuan Gong",
                "Hongyin Luo",
                "Alexander H Liu",
                "Leonid Karlinsky",
                "James Glass"
            ],
            "title": "Listen, think, and understand",
            "venue": "arXiv preprint arXiv:2305.10790,",
            "year": 2023
        },
        {
            "authors": [
                "Anmol Gulati",
                "James Qin",
                "Chung-Cheng Chiu",
                "Niki Parmar",
                "Yu Zhang",
                "Jiahui Yu",
                "Wei Han",
                "Shibo Wang",
                "Zhengdong Zhang",
                "Yonghui Wu",
                "Ruoming Pang"
            ],
            "title": "Conformer: Convolution-augmented transformer for speech recognition",
            "venue": "In INTERSPEECH,",
            "year": 2020
        },
        {
            "authors": [
                "Michael Hassid",
                "Tal Remez",
                "Tu Anh Nguyen",
                "Itai Gat",
                "Alexis Conneau",
                "Felix Kreuk",
                "Jade Copet",
                "Alexandre D\u00e9fossez",
                "Gabriel Synnaeve",
                "Emmanuel Dupoux",
                "Roy Schwartz",
                "Yossi Adi"
            ],
            "title": "Textually pretrained speech language models",
            "venue": "CoRR, abs/2305.13009,",
            "year": 2023
        },
        {
            "authors": [
                "Curtis Hawthorne",
                "Andrew Jaegle",
                "Catalina Cangea",
                "Sebastian Borgeaud",
                "Charlie Nash",
                "Mateusz Malinowski",
                "Sander Dieleman",
                "Oriol Vinyals",
                "Matthew M. Botvinick",
                "Ian Simon",
                "Hannah Sheahan",
                "Neil Zeghidour",
                "Jean-Baptiste Alayrac",
                "Jo\u00e3o Carreira",
                "Jesse H. Engel"
            ],
            "title": "General-purpose, long-context autoregressive modeling with perceiver AR",
            "venue": "In ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Wei-Ning Hsu",
                "Benjamin Bolte",
                "Yao-Hung Hubert Tsai",
                "Kushal Lakhotia",
                "Ruslan Salakhutdinov",
                "Abdelrahman Mohamed"
            ],
            "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
            "venue": "IEEE ACM Trans. Audio Speech Lang. Process.,",
            "year": 2021
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Rongjie Huang",
                "Mingze Li",
                "Dongchao Yang",
                "Jiatong Shi",
                "Xuankai Chang",
                "Zhenhui Ye",
                "Yuning Wu",
                "Zhiqing Hong",
                "Jiawei Huang",
                "Jinglin Liu"
            ],
            "title": "Audiogpt: Understanding and generating speech, music, sound, and talking head",
            "venue": "arXiv preprint arXiv:2304.12995,",
            "year": 2023
        },
        {
            "authors": [
                "Kristiina Jokinen",
                "Michael McTear"
            ],
            "title": "Spoken dialogue systems",
            "venue": "Synthesis Lectures on Human Language Technologies,",
            "year": 2009
        },
        {
            "authors": [
                "Norman P Jouppi",
                "George Kurian",
                "Sheng Li",
                "Peter Ma",
                "Rahul Nagarajan",
                "Lifeng Nai",
                "Nishant Patil",
                "Suvinay Subramanian",
                "Andy Swing",
                "Brian Towles"
            ],
            "title": "TPU v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings",
            "venue": "arXiv preprint arXiv:2304.01433,",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Kahn",
                "Morgane Rivi\u00e8re",
                "Weiyi Zheng",
                "Evgeny Kharitonov",
                "Qiantong Xu",
                "Pierre-Emmanuel Mazar\u00e9",
                "Julien Karadayi",
                "Vitaliy Liptchinsky",
                "Ronan Collobert",
                "Christian Fuegen",
                "Tatiana Likhomanenko",
                "Gabriel Synnaeve",
                "Armand Joulin",
                "Abdelrahman Mohamed",
                "Emmanuel Dupoux"
            ],
            "title": "LibriLight: A benchmark for ASR with limited or no supervision",
            "venue": "In ICASSP,",
            "year": 2020
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "year": 2001
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "year": 2014
        },
        {
            "authors": [
                "Yuma Koizumi",
                "Kohei Yatabe",
                "Heiga Zen",
                "Michiel Bacchiani"
            ],
            "title": "WaveFit: an iterative and nonautoregressive neural vocoder based on fixed-point iteration",
            "venue": "In SLT,",
            "year": 2022
        },
        {
            "authors": [
                "Jungil Kong",
                "Jaehyeon Kim",
                "Jaekyoung Bae"
            ],
            "title": "HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis",
            "venue": "Proc. NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Kushal Lakhotia",
                "Eugene Kharitonov",
                "Wei-Ning Hsu",
                "Yossi Adi",
                "Adam Polyak",
                "Benjamin Bolte",
                "Tu-Anh Nguyen",
                "Jade Copet",
                "Alexei Baevski",
                "Abdelrahman Mohamed",
                "Emmanuel Dupoux"
            ],
            "title": "On generative spoken language modeling from raw audio",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Ann Lee",
                "Hongyu Gong",
                "Paul-Ambroise Duquenne",
                "Holger Schwenk",
                "Peng-Jen Chen",
                "Changhan Wang",
                "Sravya Popuri",
                "Yossi Adi",
                "Juan Pino",
                "Jiatao Gu"
            ],
            "title": "Textless speech-to-speech translation on real data",
            "venue": "arXiv preprint arXiv:2112.08352,",
            "year": 2021
        },
        {
            "authors": [
                "Shansong Liu",
                "Atin Sakkeer Hussain",
                "Chenshuo Sun",
                "Ying Shan"
            ],
            "title": "Music understanding llama: Advancing text-to-music generation with question answering and captioning",
            "venue": "arXiv preprint arXiv:2308.11276,",
            "year": 2023
        },
        {
            "authors": [
                "Xubo Liu",
                "Zhongkai Zhu",
                "Haohe Liu",
                "Yi Yuan",
                "Meng Cui",
                "Qiushi Huang",
                "Jinhua Liang",
                "Yin Cao",
                "Qiuqiang Kong",
                "Mark D Plumbley"
            ],
            "title": "Wavjourney: Compositional audio creation with large language models",
            "venue": "arXiv preprint arXiv:2307.14335,",
            "year": 2023
        },
        {
            "authors": [
                "Nobuyuki Morioka",
                "Heiga Zen",
                "Nanxin Chen",
                "Yu Zhang",
                "Yifan Ding"
            ],
            "title": "Residual adapters for few-shot text-to-speech speaker adaptation",
            "venue": "arXiv preprint arXiv:2210.15868,",
            "year": 2022
        },
        {
            "authors": [
                "Tu Anh Nguyen",
                "Beno\u00eet Sagot",
                "Emmanuel Dupoux"
            ],
            "title": "Are discrete units necessary for spoken language modeling",
            "venue": "IEEE J. Sel. Top. Signal Process.,",
            "year": 2022
        },
        {
            "authors": [
                "Tu Anh Nguyen",
                "Eugene Kharitonov",
                "Jade Copet",
                "Yossi Adi",
                "Wei-Ning Hsu",
                "Ali Elkahky",
                "Paden Tomasello",
                "Robin Algayres",
                "Benoit Sagot",
                "Abdelrahman Mohamed"
            ],
            "title": "Generative spoken dialogue language modeling",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2023
        },
        {
            "authors": [
                "Maxwell I. Nye",
                "Anders Johan Andreassen",
                "Guy Gur-Ari",
                "Henryk Michalewski",
                "Jacob Austin",
                "David Bieber",
                "David Dohan",
                "Aitor Lewkowycz",
                "Maarten Bosma",
                "David Luan",
                "Charles Sutton",
                "Augustus Odena"
            ],
            "title": "Show your work: Scratchpads for intermediate computation with language models",
            "venue": "CoRR, abs/2112.00114,",
            "year": 2021
        },
        {
            "authors": [
                "Vassil Panayotov",
                "Guoguo Chen",
                "Daniel Povey",
                "Sanjeev Khudanpur"
            ],
            "title": "Librispeech: an asr corpus based on public domain audio books",
            "venue": "In Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2015
        },
        {
            "authors": [
                "Daniel S Park",
                "William Chan",
                "Yu Zhang",
                "Chung-Cheng Chiu",
                "Barret Zoph",
                "Ekin D Cubuk",
                "Quoc V Le"
            ],
            "title": "SpecAugment: A simple data augmentation method for automatic speech recognition",
            "year": 2019
        },
        {
            "authors": [
                "Daniel S Park",
                "Yu Zhang",
                "Ye Jia",
                "Wei Han",
                "Chung-Cheng Chiu",
                "Bo Li",
                "Yonghui Wu",
                "Quoc V Le"
            ],
            "title": "Improved noisy student training for automatic speech recognition",
            "year": 2005
        },
        {
            "authors": [
                "Linkai Peng",
                "Baorian Nuchged",
                "Yingming Gao"
            ],
            "title": "Spoken language intelligence of large language models for language learning",
            "venue": "arXiv preprint arXiv:2308.14536,",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Yi Ren",
                "Xu Tan",
                "Tao Qin",
                "Sheng Zhao",
                "Zhou Zhao",
                "Tie-Yan Liu"
            ],
            "title": "Almost unsupervised text to speech and automatic speech recognition",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Morgane Rivi\u00e8re",
                "Emmanuel Dupoux"
            ],
            "title": "Towards unsupervised learning of speech features in the wild",
            "venue": "IEEE Spoken Language Technology Workshop (SLT),",
            "year": 2021
        },
        {
            "authors": [
                "Pedro Ortiz Suarez",
                "Victor Sanh",
                "Hugo Lauren\u00e7on",
                "Yacine Jernite",
                "Julien Launay",
                "Margaret Mitchell",
                "Colin Raffel",
                "Aaron Gokaslan",
                "Adi Simhi",
                "Aitor Soroa",
                "Alham Fikri Aji",
                "Amit Alfassy",
                "Anna Rogers",
                "Ariel Kreisberg Nitzav",
                "Canwen Xu",
                "Chenghao Mou",
                "Chris Emezue",
                "Christopher Klamm",
                "Colin Leong",
                "Daniel van Strien",
                "David Ifeoluwa Adelani"
            ],
            "title": "BLOOM: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Shen",
                "Ruoming Pang",
                "Ron J. Weiss",
                "Mike Schuster",
                "Navdeep Jaitly",
                "Zongheng Yang",
                "Zhifeng Chen",
                "Yu Zhang",
                "Yuxuan Wang",
                "RJ-Skerrv Ryan",
                "Rif A. Saurous",
                "Yannis Agiomyrgiannakis",
                "Yonghui Wu"
            ],
            "title": "Natural TTS synthesis by conditioning wavenet on MEL spectrogram predictions",
            "venue": "In ICASSP,",
            "year": 2018
        },
        {
            "authors": [
                "Jonathan Shen",
                "Ye Jia",
                "Mike Chrzanowski",
                "Yu Zhang",
                "Isaac Elias",
                "Heiga Zen",
                "Yonghui Wu"
            ],
            "title": "Nonattentive tacotron: Robust and controllable neural tts synthesis including unsupervised duration modeling",
            "year": 2010
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In NIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Changhan Wang",
                "Morgane Rivi\u00e8re",
                "Ann Lee",
                "Anne Wu",
                "Chaitanya Talnikar",
                "Daniel Haziza",
                "Mary Williamson",
                "Juan Pino",
                "Emmanuel Dupoux"
            ],
            "title": "Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation",
            "venue": "In ACL 2021-59th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V. Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Shu-wen Yang",
                "Po-Han Chi",
                "Yung-Sung Chuang",
                "Cheng-I Jeff Lai",
                "Kushal Lakhotia",
                "Yist Y Lin",
                "Andy T Liu",
                "Jiatong Shi",
                "Xuankai Chang",
                "Guan-Ting Lin"
            ],
            "title": "Superb: Speech processing universal performance benchmark",
            "venue": "arXiv preprint arXiv:2105.01051,",
            "year": 2021
        },
        {
            "authors": [
                "Neil Zeghidour",
                "Alejandro Luebs",
                "Ahmed Omran",
                "Jan Skoglund",
                "Marco Tagliasacchi"
            ],
            "title": "Soundstream: An end-to-end neural audio codec",
            "venue": "IEEE ACM Trans. Audio Speech Lang. Process.,",
            "year": 2022
        },
        {
            "authors": [
                "Aohan Zeng",
                "Xiao Liu",
                "Zhengxiao Du",
                "Zihan Wang",
                "Hanyu Lai",
                "Ming Ding",
                "Zhuoyi Yang",
                "Yifan Xu",
                "Wendi Zheng",
                "Xiao Xia",
                "Weng Lam Tam",
                "Zixuan Ma",
                "Yufei Xue",
                "Jidong Zhai",
                "Wenguang Chen",
                "Peng Zhang",
                "Yuxiao Dong",
                "Jie Tang"
            ],
            "title": "GLM-130B: an open bilingual pre-trained model",
            "year": 2023
        },
        {
            "authors": [
                "Dong Zhang",
                "Shimin Li",
                "Xin Zhang",
                "Jun Zhan",
                "Pengyu Wang",
                "Yaqian Zhou",
                "Xipeng Qiu"
            ],
            "title": "Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities",
            "venue": "arXiv preprint arXiv:2305.11000,",
            "year": 2023
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models",
            "venue": "arXiv preprint arXiv:2205.01068,",
            "year": 2022
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona T. Diab",
                "Xian Li",
                "Xi Victoria Lin",
                "Todor Mihaylov",
                "Myle Ott",
                "Sam Shleifer",
                "Kurt Shuster",
                "Daniel Simig",
                "Punit Singh Koura",
                "Anjali Sridhar",
                "Tianlu Wang",
                "Luke Zettlemoyer"
            ],
            "title": "OPT: open pre-trained transformer language models",
            "venue": "CoRR, abs/2205.01068,",
            "year": 2022
        },
        {
            "authors": [
                "Ziqiang Zhang",
                "Sanyuan Chen",
                "Long Zhou",
                "Yu Wu",
                "Shuo Ren",
                "Shujie Liu",
                "Zhuoyuan Yao",
                "Xun Gong",
                "Lirong Dai",
                "Jinyu Li"
            ],
            "title": "Speechlm: Enhanced speech pre-training with unpaired textual data",
            "venue": "arXiv preprint arXiv:2209.15329,",
            "year": 2022
        },
        {
            "authors": [
                "Zihan Zhao",
                "Yiyang Jiang",
                "Heyang Liu",
                "Yanfeng Wang",
                "Yu Wang"
            ],
            "title": "Librisqa: Pioneering free-form and open-ended spoken question answering with a novel dataset and framework",
            "venue": "arXiv preprint arXiv:2308.10390,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The goal of natural language processing (NLP) is to develop computational models that can understand and generate human language. By capturing the statistical patterns and structures of text-based natural language, language models can predict and generate coherent and meaningful sequences of words. Combined with the Transformer model architecture (Vaswani et al., 2017), large language models (LLMs) trained on web-scale amounts of text, with proportionate compute and size, have demonstrated remarkable success in NLP tasks (Devlin et al., 2019; Brown et al., 2020; Chowdhery et al., 2022; Zhang et al., 2022b; Scao et al., 2022; Zeng et al., 2023). However, transfering these abilities to spoken human language remains a challenging frontier. Spoken dialog systems remain a cascade of separately trained automatic speech recognition (ASR), natural language understanding (NLU) and generation (NLG), and text-to-speech (TTS) systems (Gorin et al., 1997; Jokinen & McTear, 2009), with LLMs now playing the role of a combined NLU and NLG system. However, such cascades introduce latency and additional mechanisms for propagating and rendering non-verbal cues like speaker identity and prosody. Recently, spoken language models (Lakhotia et al., 2021; Kharitonov et al., 2022) and other generative audio models (Dhariwal et al., 2020; Hawthorne et al., 2022; Borsos et al., 2022; Agostinelli et al., 2023) have emerged as a promising avenue for generative speech modeling. These works quantize audio representations (Hsu et al., 2021; Chung et al., 2021; Zeghidour et al., 2022; D\u00e9fossez et al., 2022) into learned discrete tokens compatible with the same next-token cross-entropy objective as text LLMs, a step that Nguyen et al. (2022) argued as necessary for generative quality. In this paper, we introduce Spectron, a novel spoken language model that:\n\u2022 Directly process spectrograms as both input and output. Spectron leverages the audio capabilities of a pre-trained speech encoder through the use of intermediate projection layers.\n\u2022 Demonstrably transfer generative ability from a pre-trained LLM, as shown by competitive performance in semantic coherence and spoken question answering over other end-to-end spoken language models.\nTo quantify this transfer of knowledge, we also introduce two benchmarks for the nascent spoken QA task, which we synthesize from Web Questions and LLaMA-2 generations. Samples are available in the Supplemental Materials.\nOur work shows that the inductive biases from a pre-trained speech encoder and a language model decoder enable end-to-end training and state-of-the-art performance without sacrificing representational fidelity. Key to this is a novel end-to-end training objective which implicitly supervises speech recognition, text continuation, and conditional speech synthesis in a joint manner. The language model transcribes and generates text continuations, acting as an \u2018intermediate scratchpad\u2019 (Nye et al., 2021; Wei et al., 2022) to be conditioned on for audio generation. A novel spectrogram regression loss also supervises the model to match the higher-order temporal and feature deltas of the ground truth, based on the idea that the derivatives of the ground truth express rich, longer-range information about the shape of the signal. Our overall scheme is summarized in Figure 1 and described in the rest of this work."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "The dominant approach to spoken language modeling is to use compact discrete speech representations. This allows the application of text-based language models to speech data. Typically, these representations are created by clustering the outputs of a speech encoder using K-means and using the centroids as tokens. The resulting discrete sequences can be easily modeled using Transformer architectures (Vaswani et al., 2017). Below are notable examples of works using this approach; a comparison table is also presented in Appendix A.2.\nGenerative Spoken Language Modeling (GSLM) (Lakhotia et al., 2021) offers a baseline system that operate on units quantized from pre-trained audio representations, such as HuBERT (Hsu et al., 2021). The quantized units are processed by a Transformer-based model. An additional unit-to-speech decoder converts the generated units into spectrograms. Spectron\u2019s approach is more simple and explicit, where a single model is input with spectograms and outputs raw spectograms.\nTWIST (Hassid et al., 2023) Uses the same unit-to-speech and speech-to-unit systems as GSLM, but warm-starts the spoken language model from a text-based language model. They show that this warm-start improves overall performance, and drastically improves the ability to perform StoryCloze tasks. For the textual language model, they use the state of the art and open sourced OPT (Zhang et al., 2022a) and LLama (Touvron et al., 2023) 1.3B and 7B models respectively and show that scaling spoken langauge models also improves performance.\nAudioLM (Borsos et al., 2022) utilizes two kinds of quantized representations: w2v-BERT (Chung et al., 2021) as semantic tokens and SoundStream (Zeghidour et al., 2022) as acoustic tokens. SoundStream embeddings undergo discretization using residual vector quantization (RVQ), resulting in a hierarchy of vector quantizers. AudioLM utilizes three transformer models, each corresponding to a different layer of token generation. As Spectron does not involve any quantization, our method naturally preserves the input semantics and accustics characteristics. Further, Spectron offers a single model trained with a unified objective as opposed to the multiple components required by AudioLM.\nSpeechGPT (Zhang et al., 2023a) Adapts the LLama 7B to perform speech tasks by using both discrete speech representations and text. They introduce the SpeechInstruct dataset which is used for instruction tuning. SpeechGPT is trained in 3 differnet steps for modality adaptation, cross modal instruction finetuning and chain of modality instruction finetuning (using LoRA (Hu et al., 2021)). The obtained model is capable of generating both speech and text, as well as following instructions in both modalities. Our method, in comparison, is trained using a single reconstruction step and utilizes only publicly available datasets. Using no carefully curated datasets, we demonstrate competitive performance over spoken question answering and superior results for speech continuation, without using any specialized prompts.\nSpoken language understanding (SLU): A number of recent studies explore the usage of pre-trained language models (LMs) for different SLU tasks. (Gong et al., 2023), (Zhao et al., 2023), and (Liu et al., 2023a) fine-tuned LMs on audio data to perform speech-to-text question answering (QA) tasks. These models were able to answer textual questions about the input audio in a direct manner. (Fathullah et al., 2023) showed that adding an audio encoder to an LM and training using the LoRA algorithm (Hu et al., 2021) enables the LM to perform automatic speech recognition (ASR). (Zhang et al., 2022c) aligned text and audio tokens to perform a large number of SLU tasks. (Peng et al., 2023) showed that LMs can be used to answer questions about spoken language using only text. Spectron, in comparison, produces both textual outputs and spectograms using a single autoregressive decoder. Multi-modal text-speech training: Ao et al. (2021) performed joint training on speech and text data to perform multiple tasks, such as text-to-speech (TTS) and ASR. Ren et al. (2019) used unsupervised pre-training on text and speech data to perform TTS for low-resource languages. Textual-guided audio generation: Liu et al. (2023b) uses LMs to generate audio scripts and interacting with audio creation APIs. Huang et al. (2023) augment ChatGPT input/output interface with ASR / TTS capabilities."
        },
        {
            "heading": "3 APPROACH",
            "text": ""
        },
        {
            "heading": "3.1 ARCHITECTURE",
            "text": "We propose a novel architecture for direct speech continuation. The architecture is initialized with a pre-trained speech encoder denoted as E and a pre-trained language decoder denoted as LM. The encoder is prompted with a speech utterance as input, which it encodes into continuous linguistic features. These features fed into the decoder as a prefix, and the whole encoder-decoder is optimized to jointly minimize a cross-entropy loss (for speech recognition and transcript continuation) and a novel reconstruction loss (for speech continuation). During inference, one provides a spoken speech prompt, which is encoded and then decoded to give both text and speech continuations."
        },
        {
            "heading": "3.1.1 INPUT PRE-PROCESSING",
            "text": "During training, the proposed model uses supervised speech utterances, which are pairs of speech x and transcripts y for training. The speech input, denoted as x, is a spectrogram that is split into two segments at position s:\nxp = x\u2264s, xc = x>s. (1)\nThe first segment xp (which we call the prompt) is fed into the speech encoder E to give continuous representations that condition the LM. The second segment xc (the continuation) is used later for a spectrogram reconstruction loss. SpecAugment (Park et al., 2019) is applied for data augmentation. The corresponding transcripts y can be also split at position \u03c6(s):\nyp = y\u2264\u03c6(s), yc = y>\u03c6(s), (2)\nwhere \u03c6(s) maps the feature index s in x to its text token index in y. Note that \u03c6(s) is not needed for our training losses."
        },
        {
            "heading": "3.1.2 SPEECH ENCODER",
            "text": "The speech encoder E is a 600M-parameter Conformer encoder (Gulati et al., 2020) pre-trained on web-scale data (12M hours; Zhang et al., 2023b). It takes the spectrogram of the source speech as input, generating a hidden representation that incorporates both linguistic and acoustic information. The input spectrogram is first subsampled using a convolutional layer and then processed by a series of Conformer blocks. Each Conformer block consists of a feed-forward layer, a self-attention layer, a convolution layer, and a second feed-forward layer. The outputs of the total encoder E are passed through a layer P that projects the hidden representations into the embedding dimension of the language model. We denote these final embeddings\nxlmp = P(E(xp)). (3)"
        },
        {
            "heading": "3.1.3 LANGUAGE MODEL",
            "text": "We use prefix decoder language models with 350M or 1B parameters trained in the manner of PaLM 2 (Google, 2023), which we denote as LM. The LM receives the encoded features of the prompt xlmp as a prefix. Note that this is the only connection between the speech encoder and the LM decoder; i.e., there is no cross-attention between the encoder and the decoder. This late-stage integration is consistent with work in ASR, which found that joint finetuning of a pre-trained speech encoder and a pre-trained LM decoder into a sequence-to-sequence model can improve performance, even if the integration occurs as a single final layer (Deng et al., 2021); more layers did not improve performance, which they attribute to having sufficiently powerful text representations. During training, the decoder is teacher-forced to predict the text transcription yp, text continuation yc, and speech embeddings xpc . To convert the speech embeddings to and from spectrograms, we introduce lightweight modules hpre and hpost, described in the next section. In all, we get next-step predictions for the concatenation of these text tokens and embeddings:\n[y\u0302p, y\u0302c, x\u0302 p c ] = LM(x lm p , [yp, yc, x p c ]). (4)\nBy having the same architecture decode the intermediate text and the spectrograms, we gain two benefits. First, we benefit from the pre-training of the LM in the text domain to continue the prompt in the text domain before synthesizing the speech. Secondly, the predicted text serves as intermediate reasoning, enhancing the quality of the synthesized speech, analogous to improvements in text-based language models when using intermediate scratchpads (Nye et al., 2021) or chain-of-thought (CoT; Wei et al., 2022)."
        },
        {
            "heading": "3.1.4 ACOUSTIC PROJECTION LAYERS",
            "text": "To enable the language model decoder to model speech features, we employ a multi-layer perceptron, the pre-net hpre to project the ground truth spectrogram speech continuations xc to the language model dimension xpc = h\npre(xc). This pre-net hpre compresses the spectrogram input xc into a lower dimension, creating a bottleneck that aids the decoding process. This bottleneck mechanism prevents the model from repetitively generating the same prediction in the decoding process, as demonstrated in previous work (Shen et al., 2018). To project x\u0302pc from the language model dimension to the spectrogram dimension, the model employs a post-net hpost, which is a multi-layer perceptron. This projection is represented by x\u0302c = hpost(x\u0302pc).\nBoth hpre and hpost are two-layer multi-layer perceptrons. Additionally, the input text sequence [yp, yc] is padded at the beginning with a \u201cstart of sequence\u201d (sos) token, while the output sequence is padded with an \u201cend of sequence\u201d (eos) token at the final position."
        },
        {
            "heading": "3.2 TRAINING OBJECTIVE",
            "text": "The training methodology of the proposed approach is depicted in Figure 1. It uses two distinct loss functions: (1) cross-entropy loss, employed for both speech recognition and transcript continuation, and (2) regression loss, employed for speech continuation. During training, all parameters are updated (speech encoder E , projection layer Ps, language model LM, pre-net hpre, and post-net hpost)."
        },
        {
            "heading": "3.2.1 SPEECH RECOGNITION AND TRANSCRIPT CONTINUATION",
            "text": "The first loss term is a combination of a speech recognition loss LASR and a transcript continuation loss LLM, which are given by:\nLASR(yp, y\u0302p) = CE(yp, y\u0302p), LLM(yc, y\u0302c) = CE(yc, y\u0302c), (5)\nwhere CE denotes cross-entropy, which quantifies the dissimilarity between the predicted distribution over y\u0302p, y\u0302c, and the corresponding ground truth distribution over yp, yc. This objective increases the likelihood of the text [yp, yc] under the conditional distribution modeled by the LM."
        },
        {
            "heading": "3.2.2 SPEECH CONTINUATION",
            "text": "The speech continuation objective is formulated as a regression task, predicting spectrogram frame channels independently given previous spectrogram frame predictions and the ASR and LM context. To promote convergence and improve modeling power, we apply `1 and `2 regression losses on the spectrogram (Shen et al., 2020). These losses are applied to the feature-deltas of the spectrogram, and to the time-deltas of the spectrogram up to order K. That is, for a tensor z of dimension T \u00d7 F we define:\n\u2206timek (z) = z[1:T\u2212k,:] \u2212 z[k:T,:], (6) \u2206featk (z) = z[:,1:F\u2212k] \u2212 z[:,k:F ], (7) L1+2(z, z\u2032) = ||z \u2212 z\u2032||1 + ||z \u2212 z\u2032||22. (8)\nFor a ground truth spectrogram xc and the predicted spectrogram x\u0302c, the speech continuation loss is a combination of three objectives:\nLs(xc, x\u0302c) = L1+2(xc, x\u0302c), (9) Lf(xc, x\u0302c) = L1+2(\u2206feat1 (xc),\u2206feat1 (x\u0302c)), (10)\nLt(xc, x\u0302c) = K\u2211 k=1 L1+2(\u2206timek (xc),\u2206timek (x\u0302c)) (11)\nThe overall speech continuation loss is thus given by:\nLRecon.(xc, x\u0302c) = Ls(xc, x\u0302c) + Lf(xc, x\u0302c) + Lt(xc, x\u0302c). (12)"
        },
        {
            "heading": "3.2.3 OVERALL LOSS",
            "text": "Using the above notation, our objective is:\nLtotal(x, y) = LASR(yp, y\u0302p) + LLM(yc, y\u0302c) + LRecon.(xc, x\u0302c). (13)\nSince LASR and LLM are cross-entropy losses and since y = [yp, yc] (Eq.2), the overall speech recognition and transcript continuation loss can be written as:\nLASR(yp, y\u0302p) + LLM(yc, y\u0302c) = CE(y, y\u0302) = LCE(y, y\u0302) (14)\nwhere y\u0302 is the concatenation of y\u0302p and y\u0302c. This simplifies the overall loss to:\nLtotal(x, y) = LCE(y, y\u0302) + \u03bbrLRecon.(xc, x\u0302c), (15)\nwhere \u03bbr is a weighting coefficient. This simplification eliminates the necessity of the text-speech time alignment \u03c6(s). Our approach can be seen as jointly optimizing three capabilities:\nSpeech recognition (LASR): The combined model learns to transcribe speech audio into text. As we use pre-trained speech encoder and pre-trained language model, this objective encourages the alignment and integration of each model\u2019s functionality.\nTranscript continuation (LLM): This reuses, maintains, and leverages the language model\u2019s ability to generate natural text as learned from its training scheme, for example, dialogue for a chat-optimized LM. Depending on the utterance, the decoder may further learn to use paralinguistic cues from the prompt speech to favor certain completions.\nConditional speech synthesis (LRecon.): We reuse the language model\u2019s autoregressive generation ability and direct it toward spectrogram reconstruction. As the teacher-forced transcript is available and the most \u201caccessible\u201d feature, the decoder learns to perform text-to-speech. In this way, the model can synthesize the LM\u2019s arbitrary textual continuations at inference time, including words not found in training. Finally, we expect that good spectrogram-level continuations require the preservation of speaker, prosody, and channel effects from the original speech prompt."
        },
        {
            "heading": "3.3 INFERENCE",
            "text": "In inference, the speech prompt xp is encoded by the speech encoder E then projected by Ps to the LM\u2019s dimension to give xlmp (Eq.3). Utilizing x lm p and the start-of-sentence (sos) token, the language model decodes text in an autoregressive manner: y\u0302 = LM([xlmp , sos]) until eos is emitted, where y\u0302 is a concatenation of the predicted transcript and continuation [y\u0302p, y\u0302c]. Following this, the language model decodes a spectrogram in an autoregressive manner. It predicts the next spectrogram feature estimate x\u0302c(t) using prompt features xlmp , text prediction y\u0302 and past estimated spectrogram features x\u0302c(\u2264 t \u2212 1). Past spectrogram estimates x\u0302c(\u2264 t \u2212 1) are projected to the language model dimension: x\u0302pc(\u2264 t \u2212 1) = hpre(x\u0302c(\u2264 t \u2212 1)). Then, x\u0302pc(t) is predicted at step t: x\u0302pc(t) = LM([x lm p , sos, y\u0302, x\u0302 p c(\u2264 t\u22121)]) The decoded output x\u0302pc(t) is then projected to the spectrogram domain using hpost: x\u0302c(t) = hpost(x\u0302pc(t)). Finally, a vocoder converts the predicted spectrogram x\u0302c into a waveform signal."
        },
        {
            "heading": "4 EXPERIMENTS AND RESULTS",
            "text": ""
        },
        {
            "heading": "4.1 DATA AND PREPROCESSING",
            "text": "To empirically evaluate the performance of the proposed approach, we conducted experiments on the Libri-Light dataset (Kahn et al., 2020). Libri-Light is a 60k hour English dataset consisting of unlabelled read speech from LibriVox audiobooks. For our training objective, the dataset was transcribed using a NST (Park et al., 2020) model trained on LibriSpeech (960 hours). We utilized a frozen neural vocoder called WaveFit (Koizumi et al., 2022) to convert the predicted spectrograms into raw audio. The model architecture and hyperparameters as described in Koizumi et al. (2022). Our proposed model was trained using 64 TPUv4 chips (Jouppi et al., 2023), over a duration of 48 hours. A comprehensive table of hyperparameters can be found in the Supplementary Materials, specifically in Appendix A.1. We consider a predetermined set of 3-second prefixes denoted as s = 3sec. During training utterances with a length of less than 3 seconds are discarded. For Librilight, only 0.04% of utterances are less than 3 seconds. To evaluate our model and the baseline models during testing, we utilize the test-clean test set from LibriSpeech (Panayotov et al., 2015). We employ the first 3 seconds of each utterance in the test set as a prompt to the models, excluding the ground truth transcripts. For semantic and acoustic quality, Spectron was trained with a LM of 350 million parameters, while for the question answering task, Spectron was trained with a LM of 1 billion parameters. The two models are identical except for the LM. In sections 4.2.1 and 4.2.2 a 350M LM was used, in section 4.2.3 a 1B LM was used."
        },
        {
            "heading": "4.2 BASELINES",
            "text": "We compare our method against existing spoken language models:\nGSLM: We evaluate their best model, the HuBERT-L6 configuration with 200 token units for conditional speech continuation. The model was trained on a filtered subset of LibriLight (Rivi\u00e8re & Dupoux, 2021). AudioLM: We utilize the Libri-Light trained model described in their work. The two AudioLM models we compare against differ in the number of SoundStream residual vector quantizer (RVQ) layers they generate. One model generates the top 3 layers (3-RVQ), while the other model generates all 12 layers (12-RVQ). TWIST: We evaluate both the 1.3B version and 7B versions of their models that are based on llama with HuBERT speech representations. Their models where trained on Libri-Light, Spotify podcasts Clifton et al. (2020), People dataset Galvez et al. (2021) and VoxPopuli Wang et al. (2021). SpeechGPT: We evaluate their open-sourced model, which is based upon the llama-7B model with HuBERT speech representations. This model is termed\nSpeechGPT-7B-com, and was trained using all 3 training stages in SpeechGPT. The model was trained using the LibriLight and SpeechInstruct datasets."
        },
        {
            "heading": "4.2.1 SEMANTIC QUALITY",
            "text": "We employ the log-perplexity metric to evaluate the semantic quality of the speech output from the models. We use a state-of-the-art Conformer ASR system (Zhang et al., 2023b) trained on a proprietary English-only dataset to transcribe the speech continuation. Subsequently, we compute the log-perplexity of the predicted transcripts using the GPT-2 Medium model (Radford et al., 2019) from the open-source transformers library (Wolf et al., 2020). The results presented in Table 1 demonstrate the performance gains of our method compared to previous approache such as GSLM, where our method achieves an improvement of 170.91 in log-perplexity. Furthermore, when compared to the state-of-the-art AudioLM method, our approach outperforms both the 3-RVQ and 12-RVQ variants, exhibiting enhancements of 12.88 and 14.20 respectively. Moreover, the results in Table 1 reveal that our method exhibits improved performance compared to existing cascade methods."
        },
        {
            "heading": "4.2.2 ACOUSTIC QUALITY",
            "text": "To evaluate acoustic quality, we use the Naturalness Mean Opinion Score (N-MOS) (Nguyen et al., 2023) metric and the average cosine distance between speaker embeddings as measures of speaker similarity. MOS is computed and reported solely for the speech continuations. The average speaker similarity is calculated between the input prompt and its generated continuation. The Naturalness Mean Opinion Score (N-MOS): Human evaluators are tasked with assigning a rating on a five-point scale to denote the perceived naturalness of a given speech utterance. Spanning from 1 (indicative of poor quality) to 5 (indicative of excellent quality), N-MOS serves as a valuable tool for discerning the quality of speech naturalness. Test where conducted using 20 randomly sampled utterances from the Librispeech test-clean test set. 30 raters have participated in the tests. The prompt where not available to the raters, and they were requested to evaluate only the naturalness of the continuations. Avg. speaker similarity: We compute the speaker similarity between prompt and continuation using the speaker encoder of the PnG NAT TTS model (Morioka et al., 2022). We compute the speaker embeddings of both and measure the cosine similarity between each pair of embeddings. We report the average across the entire test set. As seen in Table 2, our approach performs slightly better than GSLM in terms of N-MOS, with improvements of 0.55. When compared to AudioLM, our approach is comparable to the 3-RVQ version and slightly inferior to the 12-RVQ version, with a decrease of 0.19 in N-MOS. One can see in Table 2 that the results of TWIST are similar to those of GSLM, and Spectron outperforms the 1.3B and 7B versions by 0.4 and 0.65 respectively. SpeechGPT performs slightly inferior to Spectron, which outperforms it by a score of 0.3. Table 3 presents the results for average speaker similarity. Our method demonstrates a significant improvement of 0.31 over the GSLM method. When compared to AudioLM, our method outperforms both the 3-RVQ and 12-RVQ versions, with increases of 0.05 and 0.07 in average speaker similarity, respectively. Moreover, comparing to TWIST 1.3B and 7B, the proposed method improve the average speaker similarity by 0.18 and 0.19, respectively. These results indicate that comparable acoustic quality can be achieved with a simpler approach. Our model is trained end-to-end and utilizes the universal speech representation of spectrograms. Note that SpeechGPT does not intend to preserve speaker identity and its average speaker similarity is lower."
        },
        {
            "heading": "4.2.3 QUESTION ANSWERING",
            "text": "We propose examining whether the models can continue spoken sentences or questions with the appropriate answer. This can can be viewed as spoken generative QA; the correct answer must be produced out of infinite possibilities. Note that except for SpeechGPT, all other methods (including ours) are evaluated in a zero-shot fashion for spoken question answering. Given that the various spoken language models are evaluated with 3-second input contexts, we use TTS to synthesize questions that fit within this duration. The questions are drawn from an existing set WebQuestions and a new test set which we name LLama questions. WebQuestions Berant et al. (2013) is an openended question answering NLP dataset. The dataset contains open ended questions that are supposed to be answerable by Freebase and are centered around a single named entity. LLama question is an open-domain world knowledge QA dataset that we had gathered from the open-source LLama2-70B model (Touvron et al., 2023). We had prompted the model to provide questions and short answers regarding various topics. Overall, we had gathered 300 questions in this manner, and had generally verified the answers (the test set will be released). All of the questions are generated using the publicly available Google Cloud TTS service, using the voice en-US-Neural2-C. To compute answer accuracy, we use a Conformer ASR system (Zhang et al., 2023b) to transcribe the answers of the models. If the textual answer is contained in the output of the ASR, we count the answer as being correct.\nThe results presented in Table 4 demonstrate the performance of the proposed model in comparison to other existing models. Specifically, the proposed model exhibits an accuracy of 22.9% on the LLama test set, while SpeechGPT achieves a comparable accuracy of 21.9%. Notably, despite SpeechGPT\u2019s utilization of a larger model architecture comprising 7 billion parameters, the proposed method leverages a more modest 1 billion parameter model, yet achieves comparable results. In contrast, TWIST models with 1.3 billion and 7 billion parameters demonstrate notably lower accuracies of 1% and 0.5% respectively. Upon careful examination, it becomes evident that these models predominantly generate completions of input questions rather than providing substantive answers. AudioLM 3-RVQ, AudioLM 12-RVQ and GSLM achieved accuracy of 7%, 6.7% and 4%, respectability, which is likely due to the fact that the underlying Transformer architecture is not pre-trained on a large language model. Similarly, on the Web Questions test set, the proposed model attains an accuracy of 6.1%, while SpeechGPT yields a comparable accuracy of 6.5%. Again, TWIST models with 1.3 billion and 7 billion parameters achieve accuracies of 0.7% and 1.1% respectively, further reinforcing the observed trend of completion-centric behavior rather than direct question answering. Additionally, models such as AudioLM 3-RVQ, AudioLM 12-RVQ, and GSLM exhibit accuracies of 2.3%, 2.3%, and 1.5% respectively, which can likely be attributed to the absence of pre-training on a large-scale language model within the underlying Transformer architecture."
        },
        {
            "heading": "4.2.4 AUDIO SAMPLES",
            "text": "Various samples can be found on the supplementary materials. These include generated samples of speech continuation for LibriSpeech dataset as well as spoken question answering Web Questions and LLama Questions test sets."
        },
        {
            "heading": "4.2.5 ABLATION ANALYSIS",
            "text": "To understand the individual impacts of various components within the proposed approach, an ablation study was conducted. We measure the log-perplexity over the test-clean test set of the LibriSpeech dataset (Panayotov et al., 2015). This study involved removing each specific component in isolation. (i) Disabled intermediate loss on text (\u201c-LCE\u201d) (ii) removed spectrogram derivative loss (\u201c-(Lf +Lt)\u201d) (iii) removed pre-training of the language model LM, letting it train from scratch (iv) removed pre-training of the speech encoder E and training it from scratch (v) removed pre-training of both the speech encoder E and language model LM, training the entire model from scratch. The findings are summarized in Table 5. The results demonstrate that each of the aforementioned components contributes to the overall performance enhancement of the proposed approach. Notably, the ASR & LM cross-entropy loss LCE and the spectrogram derivative loss Lf + Lt have the most significant impact, leading to a degradation of 661.81 and 588.35 in the log-perplexity score, respectively. Furthermore, the incorporation of the pre-trained speech encoder and pre-trained language model exhibits a discernible decline in performance, resulting in a degradation of 87.17 and 75.63 in the log-perplexity score, respectively. Notably, when both the speech encoder and pre-trained language model are removed, a degradation of 118.31 in the log-perplexity score is observed."
        },
        {
            "heading": "5 LIMITATIONS AND FUTURE WORK",
            "text": "The limitation of our work is the high time and space complexity of generating spectrogram frames. Since spectrogram frames are computed with a rate of 12.5 ms, generation of long speech utterances is not possible. We hypothesize that potential solutions include generating multiple spectrogram frames from each hidden representation. Another limitation is that text and spectrogram decoding processes are not parallelizable. This hinders the ability to use Spectron in streaming scenarios and introduces a small latency between audio input and output. We leave the development of a parallelized decoding algorithm for future work. We further recognize that biases in the pre-trained language model may be sustained in our model, we refer to Google (2023) for a detailed discussion of ethical considerations for text-based language models."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We proposed Spectron, a neural direct speech continuation model that can be trained end-to-end and operates in the spectrogram domain. We showed that a pre-trained language model can be given speech recognition and generation capabilities post-hoc, by fine-tuning on continuation tasks using a pre-trained speech encoder and a novel training objective. The result is a model that benefits from the pre-training of both models and outperforms previous spoken language models on various metrics."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 TABLE OF HYPER-PARAMETERS\nA.2 EXTENDED COMPARISON TO PREVIOUS METHODS\nGiven the variation in pre-trained models across the methods discussed in this paper, we find it fit to conduct a more comprehensive comparison of their performance. This detailed comparison is presented in Table 7. Regarding the Word Error Rate (WER) assessment for ASR systems, the WER scores are sourced from the SUPERB benchmark paper (Yang et al., 2021). WER are reported on the Librispeech test-clean test set. It\u2019s important to note that these scores rely solely on the speech encoder type due to limited data availability for all utilized models. For instance, models such as mHuBert (Lee et al., 2021) employed in SpeechGPT and the New Frequency HuBert adapted and trained within TWIST exist solely as tokenization models and lack dedicated ASR model forms. The performance comparison of speech encoders referenced in various methods within this paper is depicted in Table 7. Notably, the performance of the speech encoders on the Librispeech test set is comparable. However,\nconcerning the language models (LMs) utilized, more variation is evident among the methods. LMs span a spectrum, ranging from larger models such as SpeechGPT and TWIST employing 7B LMs, to intermediate-sized models like Spectron and AudioLM employing approximately 1B LMs, and finally, GSLM utilizing a smaller 200M parameter LM. It is widely acknowledged that LM performance is significantly influenced by model size (Kaplan et al., 2020). Moreover, diverse datasets have been employed across these systems, including Librispeech, Librilight, SpeechInstruct, VoxPopuli (Wang et al., 2021), Common-Voice (Ardila et al., 2019), Spotify (Clifton et al., 2020), and People (Galvez et al., 2021)."
        }
    ],
    "year": 2023
}