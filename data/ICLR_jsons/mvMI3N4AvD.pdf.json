{
    "abstractText": "Zero-shot text-to-speech (TTS) aims to synthesize voices with unseen speech prompts, which significantly reduces the data and computation requirements for voice cloning by skipping the fine-tuning process. However, the prompting mechanisms of zero-shot TTS still face challenges in the following aspects: 1) previous works of zero-shot TTS are typically trained with single-sentence prompts, which significantly restricts their performance when the data is relatively sufficient during the inference stage. 2) The prosodic information in prompts is highly coupled with timbre, making it untransferable to each other. This paper introduces Mega-TTS, a generic prompting mechanism for zero-shot TTS, to tackle the aforementioned challenges. Specifically, we design a powerful acoustic autoencoder that separately encodes the prosody and timbre information into the compressed latent space while providing high-quality reconstructions. Then, we propose a multi-reference timbre encoder and a prosody latent language model (P-LLM) to extract useful information from multi-sentence prompts. We further leverage the probabilities derived from multiple P-LLM outputs to produce transferable and controllable prosody. Experimental results demonstrate that Mega-TTS could not only synthesize identity-preserving speech with a short prompt of an unseen speaker from arbitrary sources but consistently outperform the fine-tuning method when the volume of data ranges from 10 seconds to 5 minutes. Furthermore, our method enables to transfer various speaking styles to the target timbre in a fine-grained and controlled manner. Audio samples can be found in https://boostprompt.github.io/boostprompt/.",
    "authors": [],
    "id": "SP:a792fac63e7bf5637e02af474068619a2eb72a12",
    "references": [
        {
            "authors": [
                "Bistra Andreeva",
                "Gra\u017cyna Demenko",
                "Bernd M\u00f6bius",
                "Frank Zimmerer",
                "Jeanin J\u00fcgler",
                "Magdalena Oleskowicz-Popiel"
            ],
            "title": "Differences of pitch profiles in germanic and slavic languages",
            "venue": "In Fifteenth Annual Conference of the International Speech Communication Association,",
            "year": 2014
        },
        {
            "authors": [
                "Sercan Arik",
                "Jitong Chen",
                "Kainan Peng",
                "Wei Ping",
                "Yanqi Zhou"
            ],
            "title": "Neural voice cloning with a few samples",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Max Bain",
                "Jaesung Huh",
                "Tengda Han",
                "Andrew Zisserman"
            ],
            "title": "Whisperx: Time-accurate speech transcription of long-form audio",
            "venue": "arXiv preprint arXiv:2303.00747,",
            "year": 2023
        },
        {
            "authors": [
                "Edresson Casanova",
                "Christopher Shulby",
                "Eren G\u00f6lge",
                "Nicolas Michael M\u00fcller",
                "Frederico Santos de Oliveira",
                "Arnaldo Candido Junior",
                "Anderson da Silva Soares",
                "Sandra Maria Aluisio",
                "Moacir Antonelli Ponti"
            ],
            "title": "Sc-glowtts: an efficient zero-shot multi-speaker text-to-speech model",
            "venue": "arXiv preprint arXiv:2104.05557,",
            "year": 2021
        },
        {
            "authors": [
                "Edresson Casanova",
                "Julian Weber",
                "Christopher D Shulby",
                "Arnaldo Candido Junior",
                "Eren G\u00f6lge",
                "Moacir A Ponti"
            ],
            "title": "Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Jiawei Chen",
                "Xu Tan",
                "Jian Luan",
                "Tao Qin",
                "Tie-Yan Liu"
            ],
            "title": "Hifisinger: Towards high-fidelity neural singing voice synthesis",
            "venue": "arXiv preprint arXiv:2009.01776,",
            "year": 2020
        },
        {
            "authors": [
                "Mingjian Chen",
                "Xu Tan",
                "Bohan Li",
                "Yanqing Liu",
                "Tao Qin",
                "Sheng Zhao",
                "Tie-Yan Liu"
            ],
            "title": "Adaspeech: Adaptive text to speech for custom voice",
            "venue": "arXiv preprint arXiv:2103.00993,",
            "year": 2021
        },
        {
            "authors": [
                "Sanyuan Chen",
                "Chengyi Wang",
                "Zhengyang Chen",
                "Yu Wu",
                "Shujie Liu",
                "Zhuo Chen",
                "Jinyu Li",
                "Naoyuki Kanda",
                "Takuya Yoshioka",
                "Xiong Xiao"
            ],
            "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
            "venue": "IEEE Journal of Selected Topics in Signal Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Yutian Chen",
                "Yannis Assael",
                "Brendan Shillingford",
                "David Budden",
                "Scott Reed",
                "Heiga Zen",
                "Quan Wang",
                "Luis C Cobo",
                "Andrew Trask",
                "Ben Laurie"
            ],
            "title": "Sample efficient adaptive text-to-speech",
            "venue": "arXiv preprint arXiv:1809.10460,",
            "year": 2018
        },
        {
            "authors": [
                "Chung-Ming Chien",
                "Jheng-Hao Lin",
                "Chien-yu Huang",
                "Po-chun Hsu",
                "Hung-yi Lee"
            ],
            "title": "Investigating on incorporating pretrained and learnable speaker representations for multi-speaker multi-style text-to-speech",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Seungwoo Choi",
                "Seungju Han",
                "Dongyoung Kim",
                "Sungjoo Ha"
            ],
            "title": "Attentron: Few-shot text-to-speech utilizing attention-based variable-length embedding",
            "venue": "arXiv preprint arXiv:2005.08484,",
            "year": 2020
        },
        {
            "authors": [
                "Erica Cooper",
                "Cheng-I Lai",
                "Yusuke Yasuda",
                "Fuming Fang",
                "Xin Wang",
                "Nanxin Chen",
                "Junichi Yamagishi"
            ],
            "title": "Zero-shot multi-speaker text-to-speech with state-of-the-art neural speaker embeddings",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Erica Cooper",
                "Cheng-I Lai",
                "Yusuke Yasuda",
                "Junichi Yamagishi"
            ],
            "title": "Can speaker augmentation improve multi-speaker end-to-end tts",
            "venue": "arXiv preprint arXiv:2005.01245,",
            "year": 2020
        },
        {
            "authors": [
                "Alexandre D\u00e9fossez",
                "Jade Copet",
                "Gabriel Synnaeve",
                "Yossi Adi"
            ],
            "title": "High fidelity neural audio compression",
            "venue": "arXiv preprint arXiv:2210.13438,",
            "year": 2022
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Zhifang Sui"
            ],
            "title": "A survey for in-context learning",
            "venue": "arXiv preprint arXiv:2301.00234,",
            "year": 2022
        },
        {
            "authors": [
                "Siddharth Gururani",
                "Kilol Gupta",
                "Dhaval Shah",
                "Zahra Shakeri",
                "Jervis Pinto"
            ],
            "title": "Prosody transfer in neural text to speech using global pitch and loudness features",
            "year": 1911
        },
        {
            "authors": [
                "Wei-Ning Hsu",
                "Benjamin Bolte",
                "Yao-Hung Hubert Tsai",
                "Kushal Lakhotia",
                "Ruslan Salakhutdinov",
                "Abdelrahman Mohamed"
            ],
            "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Rongjie Huang",
                "Yi Ren",
                "Jinglin Liu",
                "Chenye Cui",
                "Zhou Zhao"
            ],
            "title": "Generspeech: Towards style transfer for generalizable out-of-domain text-to-speech",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Sung-Feng Huang",
                "Chyi-Jiunn Lin",
                "Da-Rong Liu",
                "Yi-Chen Chen",
                "Hung-yi Lee"
            ],
            "title": "Meta-tts: Metalearning for few-shot speaker adaptive text-to-speech",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Ye Jia",
                "Yu Zhang",
                "Ron Weiss",
                "Quan Wang",
                "Jonathan Shen",
                "Fei Ren",
                "Patrick Nguyen",
                "Ruoming Pang",
                "Ignacio Lopez Moreno",
                "Yonghui Wu"
            ],
            "title": "Transfer learning from speaker verification to multispeaker text-to-speech synthesis",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Kahn",
                "Morgane Rivi\u00e8re",
                "Weiyi Zheng",
                "Evgeny Kharitonov",
                "Qiantong Xu",
                "Pierre-Emmanuel Mazar\u00e9",
                "Julien Karadayi",
                "Vitaliy Liptchinsky",
                "Ronan Collobert",
                "Christian Fuegen"
            ],
            "title": "Libri-light: A benchmark for asr with limited or no supervision",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Minki Kang",
                "Dongchan Min",
                "Sung Ju Hwang"
            ],
            "title": "Any-speaker adaptive text-to-speech synthesis with diffusion models",
            "venue": "arXiv preprint arXiv:2211.09383,",
            "year": 2022
        },
        {
            "authors": [
                "Minki Kang",
                "Dongchan Min",
                "Sung Ju Hwang"
            ],
            "title": "Grad-stylespeech: Any-speaker adaptive text-tospeech synthesis with diffusion models",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Sri Karlapati",
                "Alexis Moinet",
                "Arnaud Joly",
                "Viacheslav Klimkov",
                "Daniel S\u00e1ez-Trigueros",
                "Thomas Drugman"
            ],
            "title": "Copycat: Many-to-many fine-grained prosody transfer for neural text-to-speech",
            "venue": "arXiv preprint arXiv:2004.14617,",
            "year": 2020
        },
        {
            "authors": [
                "Sri Karlapati",
                "Penny Karanasou",
                "Mateusz Lajszczak",
                "Ammar Abbas",
                "Alexis Moinet",
                "Peter Makarov",
                "Ray Li",
                "Arent van Korlaar",
                "Simon Slangen",
                "Thomas Drugman"
            ],
            "title": "Copycat2: A single model for multi-speaker tts and many-to-many fine-grained prosody transfer",
            "venue": "arXiv preprint arXiv:2206.13443,",
            "year": 2022
        },
        {
            "authors": [
                "Eugene Kharitonov",
                "Damien Vincent",
                "Zal\u00e1n Borsos",
                "Rapha\u00ebl Marinier",
                "Sertan Girgin",
                "Olivier Pietquin",
                "Matt Sharifi",
                "Marco Tagliasacchi",
                "Neil Zeghidour"
            ],
            "title": "Speak, read and prompt: High-fidelity text-to-speech with minimal supervision",
            "venue": "arXiv preprint arXiv:2302.03540,",
            "year": 2023
        },
        {
            "authors": [
                "Heeseung Kim",
                "Sungwon Kim",
                "Sungroh Yoon"
            ],
            "title": "Guided-tts: A diffusion model for text-to-speech via classifier guidance",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Jaehyeon Kim",
                "Sungwon Kim",
                "Jungil Kong",
                "Sungroh Yoon"
            ],
            "title": "Glow-tts: A generative flow for text-to-speech via monotonic alignment search",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jaehyeon Kim",
                "Jungil Kong",
                "Juhee Son"
            ],
            "title": "Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Sungwon Kim",
                "Heeseung Kim",
                "Sungroh Yoon"
            ],
            "title": "Guided-tts 2: A diffusion model for high-quality adaptive text-to-speech with untranscribed data",
            "venue": "arXiv preprint arXiv:2205.15370,",
            "year": 2022
        },
        {
            "authors": [
                "Viacheslav Klimkov",
                "Srikanth Ronanki",
                "Jonas Rohnke",
                "Thomas Drugman"
            ],
            "title": "Fine-grained robust prosody transfer for single-speaker neural text-to-speech",
            "venue": "arXiv preprint arXiv:1907.02479,",
            "year": 2019
        },
        {
            "authors": [
                "Jungil Kong",
                "Jaehyeon Kim",
                "Jaekyoung Bae"
            ],
            "title": "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Zvi Kons",
                "Slava Shechtman",
                "Alex Sorin",
                "Carmel Rabinovitz",
                "Ron Hoory"
            ],
            "title": "High quality, lightweight and adaptable tts using lpcnet",
            "year": 1905
        },
        {
            "authors": [
                "Younggun Lee",
                "Taesu Kim"
            ],
            "title": "Robust and fine-grained prosody control of end-to-end speech synthesis",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Naihan Li",
                "Shujie Liu",
                "Yanqing Liu",
                "Sheng Zhao",
                "Ming Liu"
            ],
            "title": "Neural speech synthesis with transformer network",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Yist Y Lin",
                "Chung-Ming Chien",
                "Jheng-Hao Lin",
                "Hung-yi Lee",
                "Lin-shan Lee"
            ],
            "title": "Fragmentvc: Any-to-any voice conversion by end-to-end extracting and fusing fine-grained voice fragments with attention",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Philipos C Loizou"
            ],
            "title": "Speech quality assessment",
            "venue": "In Multimedia analysis, processing and communications,",
            "year": 2011
        },
        {
            "authors": [
                "Xudong Mao",
                "Qing Li",
                "Haoran Xie",
                "Raymond YK Lau",
                "Zhen Wang",
                "Stephen Paul Smolley"
            ],
            "title": "Least squares generative adversarial networks",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Le Matthew",
                "Vyas Apoorv",
                "Shi Bowen",
                "Karrer Brian",
                "Sari Leda",
                "Moritz Rashel",
                "Williamson Mary",
                "Manohar Vimal",
                "Adi Yossi",
                "Mahadeokar Jay",
                "Hsu Wei-Ning"
            ],
            "title": "Voicebox: Text-guided multilingual universal speech generation at scale, 2023",
            "venue": "URL https://voicebox.metademolab. com/",
            "year": 2023
        },
        {
            "authors": [
                "Stephen McAdams"
            ],
            "title": "Musical timbre perception",
            "venue": "The psychology of music, pp",
            "year": 2013
        },
        {
            "authors": [
                "Michael McAuliffe",
                "Michaela Socolof",
                "Sarah Mihuc",
                "Michael Wagner",
                "Morgan Sonderegger"
            ],
            "title": "Montreal forced aligner: Trainable text-speech alignment using kaldi",
            "venue": "In Interspeech,",
            "year": 2017
        },
        {
            "authors": [
                "Dongchan Min",
                "Dong Bok Lee",
                "Eunho Yang",
                "Sung Ju Hwang"
            ],
            "title": "Meta-stylespeech: Multi-speaker adaptive text-to-speech generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Henry B Moss",
                "Vatsal Aggarwal",
                "Nishant Prateek",
                "Javier Gonz\u00e1lez",
                "Roberto Barra-Chicote"
            ],
            "title": "Boffin tts: Few-shot speaker adaptation by bayesian optimization",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Meinard M\u00fcller"
            ],
            "title": "Dynamic time warping. Information retrieval for music and motion, pp",
            "year": 2007
        },
        {
            "authors": [
                "Oliver Niebuhr",
                "Radek Skarnitzl"
            ],
            "title": "Measuring a speaker\u2019s acoustic correlates of pitch\u2013but which? a contrastive analysis based on perceived speaker charisma",
            "venue": "In Proceedings of 19th International Congress of Phonetic Sciences,",
            "year": 2019
        },
        {
            "authors": [
                "Shifeng Pan",
                "Lei He"
            ],
            "title": "Cross-speaker style transfer with prosody bottleneck in neural speech synthesis",
            "venue": "arXiv preprint arXiv:2107.12562,",
            "year": 2021
        },
        {
            "authors": [
                "Vassil Panayotov",
                "Guoguo Chen",
                "Daniel Povey",
                "Sanjeev Khudanpur"
            ],
            "title": "Librispeech: an asr corpus based on public domain audio books",
            "venue": "In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP),",
            "year": 2015
        },
        {
            "authors": [
                "Kaizhi Qian",
                "Yang Zhang",
                "Shiyu Chang",
                "Xuesong Yang",
                "Mark Hasegawa-Johnson"
            ],
            "title": "Autovc: Zero-shot voice style transfer with only autoencoder loss",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Yi Ren",
                "Yangjun Ruan",
                "Xu Tan",
                "Tao Qin",
                "Sheng Zhao",
                "Zhou Zhao",
                "Tie-Yan Liu"
            ],
            "title": "Fastspeech: Fast, robust and controllable text to speech",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yi Ren",
                "Chenxu Hu",
                "Xu Tan",
                "Tao Qin",
                "Sheng Zhao",
                "Zhou Zhao",
                "Tie-Yan Liu"
            ],
            "title": "Fastspeech 2: Fast and high-quality end-to-end text to speech",
            "year": 2006
        },
        {
            "authors": [
                "Yi Ren",
                "Ming Lei",
                "Zhiying Huang",
                "Shiliang Zhang",
                "Qian Chen",
                "Zhijie Yan",
                "Zhou Zhao"
            ],
            "title": "Prosospeech: Enhancing prosody with quantized vector pre-training in text-to-speech",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Shen",
                "Ruoming Pang",
                "Ron J Weiss",
                "Mike Schuster",
                "Navdeep Jaitly",
                "Zongheng Yang",
                "Zhifeng Chen",
                "Yu Zhang",
                "Yuxuan Wang",
                "Rj Skerrv-Ryan"
            ],
            "title": "Natural tts synthesis by conditioning wavenet on mel spectrogram predictions",
            "venue": "IEEE international conference on acoustics, speech and signal processing (ICASSP),",
            "year": 2018
        },
        {
            "authors": [
                "Kai Shen",
                "Zeqian Ju",
                "Xu Tan",
                "Yanqing Liu",
                "Yichong Leng",
                "Lei He",
                "Tao Qin",
                "Sheng Zhao",
                "Jiang Bian"
            ],
            "title": "Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers",
            "venue": "arXiv preprint arXiv:2304.09116,",
            "year": 2023
        },
        {
            "authors": [
                "Atli Thor Sigurgeirsson",
                "Simon King"
            ],
            "title": "Do prosody transfer models transfer prosody\u0192",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "RJ Skerry-Ryan",
                "Eric Battenberg",
                "Ying Xiao",
                "Yuxuan Wang",
                "Daisy Stanton",
                "Joel Shor",
                "Ron Weiss",
                "Rob Clark",
                "Rif A Saurous"
            ],
            "title": "Towards end-to-end prosody transfer for expressive speech synthesis with tacotron",
            "venue": "In international conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Yuhta Takida",
                "Takashi Shibuya",
                "WeiHsiang Liao",
                "Chieh-Hsin Lai",
                "Junki Ohmura",
                "Toshimitsu Uesaka",
                "Naoki Murata",
                "Shusuke Takahashi",
                "Toshiyuki Kumakura",
                "Yuki Mitsufuji"
            ],
            "title": "Sq-vae: Variational bayes on discrete representation with self-annealed stochastic quantization",
            "venue": "arXiv preprint arXiv:2205.07547,",
            "year": 2022
        },
        {
            "authors": [
                "Xu Tan",
                "Tao Qin",
                "Frank Soong",
                "Tie-Yan Liu"
            ],
            "title": "A survey on neural speech synthesis",
            "venue": "arXiv preprint arXiv:2106.15561,",
            "year": 2021
        },
        {
            "authors": [
                "Aaron Van Den Oord",
                "Oriol Vinyals"
            ],
            "title": "Neural discrete representation learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Michael Wagner",
                "Duane G Watson"
            ],
            "title": "Experimental and theoretical advances in prosody: A review",
            "venue": "Language and cognitive processes,",
            "year": 2010
        },
        {
            "authors": [
                "Chengyi Wang",
                "Sanyuan Chen",
                "Yu Wu",
                "Ziqiang Zhang",
                "Long Zhou",
                "Shujie Liu",
                "Zhuo Chen",
                "Yanqing Liu",
                "Huaming Wang",
                "Jinyu Li"
            ],
            "title": "Neural codec language models are zero-shot text to speech synthesizers",
            "venue": "arXiv preprint arXiv:2301.02111,",
            "year": 2023
        },
        {
            "authors": [
                "Yuxuan Wang",
                "RJ Skerry-Ryan",
                "Daisy Stanton",
                "Yonghui Wu",
                "Ron J Weiss",
                "Navdeep Jaitly",
                "Zongheng Yang",
                "Ying Xiao",
                "Zhifeng Chen",
                "Samy Bengio"
            ],
            "title": "Tacotron: Towards end-to-end speech synthesis",
            "venue": "arXiv preprint arXiv:1703.10135,",
            "year": 2017
        },
        {
            "authors": [
                "Yihan Wu",
                "Xu Tan",
                "Bohan Li",
                "Lei He",
                "Sheng Zhao",
                "Ruihua Song",
                "Tao Qin",
                "Tie-Yan Liu"
            ],
            "title": "Adaspeech 4: Adaptive text to speech in zero-shot scenarios",
            "venue": "arXiv preprint arXiv:2204.00436,",
            "year": 2022
        },
        {
            "authors": [
                "Dongchao Yang",
                "Songxiang Liu",
                "Rongjie Huang",
                "Jinchuan Tian",
                "Chao Weng",
                "Yuexian Zou"
            ],
            "title": "Hifi-codec: Group-residual vector quantization for high fidelity audio codec",
            "venue": "arXiv preprint arXiv:2305.02765,",
            "year": 2023
        },
        {
            "authors": [
                "Jingzhou Yang",
                "Lei He"
            ],
            "title": "Towards universal text-to-speech",
            "venue": "In Interspeech,",
            "year": 2020
        },
        {
            "authors": [
                "Dacheng Yin",
                "Chuanxin Tang",
                "Yanqing Liu",
                "Xiaoqiang Wang",
                "Zhiyuan Zhao",
                "Yucheng Zhao",
                "Zhiwei Xiong",
                "Sheng Zhao",
                "Chong Luo"
            ],
            "title": "Retrievertts: Modeling decomposed factors for text-based speech insertion",
            "venue": "arXiv preprint arXiv:2206.13865,",
            "year": 2022
        },
        {
            "authors": [
                "Julian Za\u0131di",
                "Hugo Seut\u00e9",
                "BV Niekerk",
                "M Carbonneau"
            ],
            "title": "Daft-exprt: Robust prosody transfer across speakers for expressive speech synthesis",
            "venue": "arXiv preprint arXiv:2108.02271,",
            "year": 2021
        },
        {
            "authors": [
                "Ziqiang Zhang",
                "Long Zhou",
                "Chengyi Wang",
                "Sanyuan Chen",
                "Yu Wu",
                "Shujie Liu",
                "Zhuo Chen",
                "Yanqing Liu",
                "Huaming Wang",
                "Jinyu Li"
            ],
            "title": "Speak foreign languages with your own voice: Cross-lingual neural codec language modeling",
            "venue": "arXiv preprint arXiv:2303.03926,",
            "year": 2023
        },
        {
            "authors": [
                "Chuanxia Zheng",
                "Andrea Vedaldi"
            ],
            "title": "Online clustered codebook",
            "venue": "arXiv preprint arXiv:2307.15139,",
            "year": 2023
        },
        {
            "authors": [
                "Kun Zhou",
                "Berrak Sisman",
                "Rui Liu",
                "Haizhou Li"
            ],
            "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Yixuan Zhou",
                "Changhe Song",
                "Xiang Li",
                "Luwen Zhang",
                "Zhiyong Wu",
                "Yanyao Bian",
                "Dan Su",
                "Helen Meng"
            ],
            "title": "Content-dependent fine-grained speaker embedding for zero-shot speaker adaptation in text-to-speech synthesis",
            "venue": "arXiv preprint arXiv:2204.00990,",
            "year": 2022
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "2018), voice cloning (Arik et al., 2018), custom voice (Chen et al., 2021), etc. In this paper, we primarily focus on adaptive TTS for different voices. D VISUALIZATION OF ATTENTION MATRICES To further verify the proposed P-LLM and multi-sentence prompting mechanism, we visualize the attention matrices averaged across all layers of P-LLM",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Zero-shot text-to-speech (TTS) aims to synthesize voices with unseen speech prompts, which significantly reduces the data and computation requirements for voice cloning by skipping the fine-tuning process. However, the prompting mechanisms of zero-shot TTS still face challenges in the following aspects: 1) previous works of zero-shot TTS are typically trained with single-sentence prompts, which significantly restricts their performance when the data is relatively sufficient during the inference stage. 2) The prosodic information in prompts is highly coupled with timbre, making it untransferable to each other. This paper introduces Mega-TTS, a generic prompting mechanism for zero-shot TTS, to tackle the aforementioned challenges. Specifically, we design a powerful acoustic autoencoder that separately encodes the prosody and timbre information into the compressed latent space while providing high-quality reconstructions. Then, we propose a multi-reference timbre encoder and a prosody latent language model (P-LLM) to extract useful information from multi-sentence prompts. We further leverage the probabilities derived from multiple P-LLM outputs to produce transferable and controllable prosody. Experimental results demonstrate that Mega-TTS could not only synthesize identity-preserving speech with a short prompt of an unseen speaker from arbitrary sources but consistently outperform the fine-tuning method when the volume of data ranges from 10 seconds to 5 minutes. Furthermore, our method enables to transfer various speaking styles to the target timbre in a fine-grained and controlled manner. Audio samples can be found in https://boostprompt.github.io/boostprompt/."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "In recent years, there has been remarkable progress in the development of text-to-speech (TTS) technology (Shen et al., 2018; Jia et al., 2018; Li et al., 2019; Kim et al., 2020; Ren et al., 2019; 2020; Kim et al., 2021; 2022a). Among them, adaptive TTS systems (Chen et al., 2021; Min et al., 2021; Kim et al., 2022b) are capable of cloning personalized voices given a few minutes of speech data. However, the performance of these systems relies heavily on the quality and quantity of the data utilized during the fine-tuning phases (Tan et al., 2021). Insufficient data during the fine-tuning stages can lead to diminished audio naturalness or speech intelligibility (Kang et al., 2023). Moreover, the computational demands also constrain its application for cloning everyone\u2019s voice.\nTo reduce such a reliance, existing works leverage generative models to perform zero-shot TTS (Cooper et al., 2020a; Casanova et al., 2022; Huang et al., 2022a; Kang et al., 2023; Kharitonov et al., 2023; Wang et al., 2023; Shen et al., 2023; Matthew et al., 2023). These powerful models can effectively synthesize speech given only a single speech prompt, eliminating the need for data preparation and the computational requirements for fine-tuning methods. However, the prompting mechanisms of current solutions still face two primary challenges:\n\u2022 Lack of multi-sentence prompting strategies. Previous works of zero-shot TTS typically employ single-sentence speech prompts during training (Wang et al., 2023; Shen et al., 2023; Matthew et al., 2023). In inference, the information in the single-sentence speech prompt is insufficient to guide the zero-shot TTS systems to imitate the voice variability of a natural\nperson perfectly.1 From another perspective, the performance of fine-tuning methods can be further improved by increasing the amount of data, while zero-shot TTS systems lack an appropriate strategy to extract useful information from multi-sentence speech prompts.\n\u2022 Lack of specialized prompting mechanism for prosodic information. Current solutions for zero-shot TTS primarily concentrate on improving the similarity of timbre and prosody between the generated speech and the prompts. However, they neglect to express various unseen prosodic styles in a controlled manner while also preserving the unique timbre of the given one-sentence prompt. In order to control the prosodic styles, it is necessary to disentangle the prosody information from speech prompts.\nWe address the above challenges by decomposing speech into content, timbre, and prosody. Intuitively, representing speeches for numerous speakers requires a substantial number of codebook entries for timbre modeling (De\u0301fossez et al., 2022; Yang et al., 2023). Through the decoupling of prosody information, a highly compact codebook for prosody modeling can be obtained, which enables our model to effectively handle extremely long prompts and have flexible control over prosodic styles. Therefore, this work proposes Mega-TTS, a generic framework that boosts the prompting mechanisms for zero-shot TTS systems. Specifically, we begin by designing an acoustic autoencoder that can effectively decompose speech into prosody and timbre representations and represent them in a compact latent space. Then, we design a multi-reference timbre encoder (MRTE) and a prosody latent language model (P-LLM) to extract useful information from multi-sentence prompts. In addition to the multi-sentence prompting mechanism, we propose a prosody interpolation technique to control the generation process of prosody codes by utilizing prosody prompts from multiple speakers while maintaining the target speaker\u2019s timbre. By utilizing the probabilities derived from both the prosodic prompts of the target speaker and the auxiliary speaker, the prosodic styles of speech can be generated in a controlled manner.\nExperiments on LibriSpeech test-clean (Panayotov et al., 2015) and ESD (Zhou et al., 2021) datasets show that Mega-TTS outperforms other state-of-the-art fine-tuning and zero-shot TTS models in terms of speaker similarity and speech naturalness. Notably, when the length of the prompt is further extended, our method surpasses the fine-tuning baseline model in the objective and subjective evaluations. The extensive studies on adaptive prosody transfer further highlight the superiority of our proposed prompting mechanisms. The main contributions of this work are summarized as follows:\n\u2022 We design an acoustic autoencoder that separately compresses the prosody and timbre information into the latent space, which allows our model to process prompts of up to 300 seconds in length effectively.\n\u2022 We propose a multi-reference timbre encoder and an auto-regressive prosody language model to extract fine-grained information from multiple reference speeches, which bridges the speaker similarity gap between zero-shot methods and fine-tuning methods.\n\u2022 Experimental results also reveal that the performance of Mega-TTS surpasses the powerful fine-tuning baseline when we have 10 seconds to 5 minutes of data for each unseen speaker, indicating the superiority of our proposed prompting mechanisms.\n\u2022 The proposed prosody interpolation technique ensures the controllability of prosody and is capable of transferring various speaking styles to the desired timbre. For instance, we can transform a voice with a sad tone into a happier one with the auxiliary prosody prompt from another speaker."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "Adaptive TTS Adaptive TTS (Arik et al., 2018; Kons et al., 2019; Moss et al., 2020; Chien et al., 2021) focuses on synthesizing personalized voice for any user with few data. During the adaptation process, a TTS model pre-trained on a multi-speaker speech dataset is typically fine-tuned with few adaptation data for the target voice (Tan et al., 2021). Chen et al. (2018) design independent learned embeddings for each speaker, which requires few data at deployment time to adapt to new speakers rapidly. AdaSpeech (Chen et al., 2021) proposes an acoustic-condition modeling method\n1Although the performance of these systems can be further improved by concatenating multiple sentences into a long prompt, the gap between training and inference still restricts their performance. (See Section 4.2)\nfor high-quality and efficient customization of new voices. There are also some works leveraging the meta-learning approach (Chen et al., 2018; Min et al., 2021; Huang et al., 2022b) and data augmentation (Cooper et al., 2020b; Yang & He, 2020) for speaker adaptation. However, although some works are data-efficient (Min et al., 2021; Huang et al., 2022b) and parameter-efficient (Chen et al., 2021), these systems still suffer from audio quality issues when data size is small, as well as computational cost issues due to hundreds of fine-tuning steps.\nZero-shot TTS Zero-shot adaptation (Jia et al., 2018; Arik et al., 2018; Cooper et al., 2020a; Casanova et al., 2021; Wu et al., 2022; Huang et al., 2022b;a; Casanova et al., 2022) aims to synthesize unseen voices with a speaker encoder that extracts speaker embeddings from the reference audio. This scenario is highly attractive because it does not require any adaptation data or parameters (Kang et al., 2022). The attention-based adaptation method (Choi et al., 2020; Zhou et al., 2022; Yin et al., 2022; Lin et al., 2021) utilizes attention mechanisms to extract fine-grained speech features from reference audios. Among them, Attentron (Choi et al., 2020) proposes to extracts useful style information from arbitrary number of reference audios. However, they do not separately model the timbre and prosody information, lacking controllability over timbre and prosody. Most recently, some works (Kharitonov et al., 2023; Zhang et al., 2023) are proposed to use in-context learning methods (Dong et al., 2022) to efficiently extract speaker information from acoustic prompts and have achieved remarkable results in zero-shot TTS. VALL-E (Wang et al., 2023) proposes the neural codec language model that exhibits strong in-context learning capability for zero-shot speech generation. NaturalSpeech 2 (Shen et al., 2023) introduces in-context learning to latent diffusion model (Rombach et al., 2022), which is achieved by partitioning a speech clip into the prompt and target regions. VoiceBox (Matthew et al., 2023) solves a text-guided speech-infilling task with large-scale data to learn from context information. However, these methods are trained with single-sentence prompts, lacking an appropriate strategy to extract fine-grained information from multi-sentence speech prompts.\nProsody Transfer for Speech Synthesis Prosody transfer (Lee & Kim, 2019; Klimkov et al., 2019; Gururani et al., 2019; Pan & He, 2021; Karlapati et al., 2022) aims to transfer the prosody from a reference utterance to the synthesized target speech, which is essential for producing natural and expressive speech in a controlled manner (Wagner & Watson, 2010). Skerry-Ryan et al. (2018) first integrate a prosody reference encoder into a TTS system based on Tacotron (Wang et al., 2017), which is capable of performing similar-text prosody transfer. Recent works try to transfer prosody in different-text and different-speaker settings (Karlapati et al., 2020; Za\u0131di et al., 2021) with the bottleneck of the prosody encoder. Among them, Daft-Exprt (Za\u0131di et al., 2021) uses a gradient reversal layer to penalize the prosody encoder if its output contains information about the speaker identity from the reference utterance, which enhances the target speaker fidelity for cross-speaker prosody transfer. However, as pointed out by Sigurgeirsson & King (2023), current solutions do not learn a transferable representation of prosody, but rather an utterance-level representation that is relatively dependent on both the reference speaker and reference text."
        },
        {
            "heading": "3 METHOD",
            "text": "This section introduces Mega-TTS. To begin with, we provide an intuitive illustration of how MegaTTS decomposes the timbre and prosody information from speech. Next, we provide detailed explanations of our prompting mechanisms and the two-stage training process of the proposed model."
        },
        {
            "heading": "3.1 DECOMPOSITION FOR PROSODY AND TIMBRE",
            "text": "Problem Formulation Denote H(X) as the Shannon entropy of X and Denote I(Y ;X) as the mutual information. We assume that the mel-spectrogram y can be reconstructed through the following generative process: y = D(zc, zpd, zt, g), where zc and zt denote the fine-grained content and timbre hidden states. g denotes the global style information that contains timbre and prosody. We assume that zpd = (zp, zd) contains the fine-grained prosodic style information of pitch and energy zp and duration zd. zd = Aligner(y) can be obtained by the external alignment tools (McAuliffe et al., 2017) and disentangled from zpd. Denote D as the mel-spectrogram decoder. Our goal is to construct an autoencoder-based model to disentangle speech components.\nDecomposition via Corpus Partition Denote Y = {y1, \u00b7 \u00b7 \u00b7 , yn} as the speech corpus for a certain speaker S. In training, we partition Y into the target mel-spectrogram yt and the other mel-spectrograms y\u0303. Here, we make an important assumption that the mutual information between yt and y\u0303 only contains timbre information H(zt) and global style information H(g) of yt, i.e.,\nI(yt; y\u0303) = H(zt) + H(g) . (1) First, based on the assumption, zt and g can be extracted through Et(y\u0303), and there is no way for Et(y\u0303) to obtain the zp and zc. Second, if we only fed phoneme sequence to Ec, Ec can only pass all the content information zc. Third, since the information zc and zt are available now, the prosody encoder Ep will prioritize removing the fine-grained content and timbre information if it is forced to lose some information by information bottleneck B(\u00b7) (Qian et al., 2019). The bottleneck forces Ep(yt) to pass only the fine-grained prosodic style zp that other encoders cannot supply, hence achieving the decomposition. We provide a detailed explanation of how we ensure the validity of Equation 1 in Appendix A.8. After the decomposition, we describe the detailed designs of our prompting mechanisms in the following subsections."
        },
        {
            "heading": "3.2 COMPRESSIVE ACOUSTIC AUTOENCODER",
            "text": "Note that to store timbre information for thousands of speakers, we need a large number of codebook entries. However, since the prosody and timbre have been decomposed, the prosodic information zp can be compressed into a highly compact codebook, and the timbre information zt can be extracted via a powerful speaker encoder. The decomposition strategy not only allows our model to accommodate extremely long prosody prompts but also enables our model to control the prosodic styles of generated speeches. As shown in Figure 1, we design the vector quantised (VQ) encoder as Ep, the multireference timbre encoder as Et, and the content encoder as Ec. Since Ep mainly captures the prosodic variance information, a GAN-based mel-spectrogram decoder D is adopted to model the highfrequency details in spectrograms, which ensures perceptually high-quality reconstructions. Overall, the first-stage training loss can be formulated as L = Lrec +LVQ +LAdv, where Lrec = \u2225yt \u2212 y\u0302t\u22252 is the reconstruction loss, LVQ is the VQ codebook loss (Van Den Oord et al., 2017), and LAdv is the LSGAN-styled adversarial loss LAdv (Mao et al., 2017) whose objective is to minimize the distribution distance between the predicted mel-spectrograms and the ground truth mel-spectrograms. Among the proposed three encoders, the content encoder is composed of several feed-forward Transformer layers following common practice in non-autoregressive TTS systems (Ren et al., 2019). In the following paragraphs, we describe the details of the prosody and timbre encoders, respectively.\nVector Quantised Encoder The vector quantised encoder Ep consists of two convolution stacks and a vector quantization bottleneck. The first convolution stacks compress mel-spectrograms into hidden states by a factor of r in length, and the second stacks capture the correlation of features. After that, the vector quantization layer utilizes these hidden states to obtain prosody codes u = {u1, u2, ..., un} and hidden states zp. The information bottleneck B(\u00b7) of the VQ encoder is composed of the temporal compression and the vector quantization layer. The detailed instructions for ensuring an appropriate information bottleneck B(\u00b7) can be found in Appendix F.\nMulti-Reference Timbre Encoder Our objective is to extract fine-grained timbre information from multi-sentence speech prompts. Since speakers can change their timbre by using different speaking techniques according to their speaking habits or desired semantic meanings (McAdams, 2013), the timbre encoder needs to extract fine-grained timbre information from multiple prompts that can represent the speakers\u2019 habits. Here, we introduce a multi-reference timbre encoder (MRTE) to achieve this objective. First, we concatenate the reference mel-spectrograms y\u0303 that belong to the target speaker but are different from the target mel-spectrogram. The mel encoder then compresses the concatenated mel-spectrogram into acoustic hidden states zt by a factor of d in length. Subsequently, to extract semantically relevant timbre information from speech prompts, we introduce a timbre-tocontent attention module. This module takes zc as the query and zt as both the key and the value. Finally, we upsample the output of the timbre-to-content attention module to match the length of the target mel-spectrogram using the length regulator (Ren et al., 2019)."
        },
        {
            "heading": "3.3 PROSODY LATENT LANGUAGE MODEL",
            "text": "Unlike previous models that are trained with single-sentence prompts, our prosody latent language model (P-LLM) aims to capture the speaker\u2019s prosodic patterns from multi-sentence prompts effectively. During the second-stage training process, we first extract the compressed prosody hidden states {zp1, zp2, \u00b7 \u00b7 \u00b7 , zpn} and the content hidden states {zc1, zc2, \u00b7 \u00b7 \u00b7 , zcn} from multiple speech clips {s1, s2, \u00b7 \u00b7 \u00b7 , sn} of the target speaker using the proposed compressive acoustic autoencoder. We then concatenate them along the time axis to construct z\u2032p = Concat(zp1, zp2, \u00b7 \u00b7 \u00b7 , zpn) and z\u2032c = Concat(zc1, zc2, \u00b7 \u00b7 \u00b7 , zcn). In order to match the lengths of z\u2032p and z\u2032c in the temporal dimension, we expand z\u2032c to the frame level with duration information zd and compress it r times with a max pooling layer. After that, we transform z\u2032p to prosody code u\n\u2032 and then feed u\u2032 and z\u2032c into the P-LLM, which predicts the prosody code in an auto-regressive manner:\np ( u\u2032 | z\u2032c; \u03b8 ) = L\u220f l=0 p ( u\u2032l | u\u2032<l, z\u2032c; \u03b8 ) , (2)\nwhere \u03b8 is the parameters of P-LLM and L is the length of the concatenated prosody code u\u2032. In training, we set batch size as 1 to increase the maximum number m of prosody codes in each batch as much as possible. If the total number of speech frames from a single speaker is less than m\u00d7 r, we will include speech samples from other speakers in this batch and incorporate speaker-level attention masks into P-LLM. We do not specifically define the speech prompt; instead, we train the language model directly using the concatenated speech samples through the teacher-forcing technique with the cross-entropy loss. To avoid the transition area problems caused by directly concatenating the prompts, we assign the start token and end token to each sentence, which guides P-LLM to continue writing the current sentence and extract useful information from previous sentences. This training strategy enables the model to capture the useful prosody-level information contained in the multi-sentence prompts. Therefore, in the inference stage, users can flexibly improve the generation quality by extending the length of prompts by concatenating the reference speech clips. For duration modeling, we propose a phoneme-level auto-regressive duration model. This model enhances the duration modeling by leveraging the powerful in-context learning capabilities of auto-regressive models. The overall architecture of the auto-regressive duration model remains the same as P-LLM, but we use mean squared error (MSE) loss instead."
        },
        {
            "heading": "3.4 PROSODY INTERPOLATION",
            "text": "Here, we propose a prosody interpolation technique to control or replace the prosodic style of the target speaker in the discrete space while ensuring the quality of timbre reconstruction. We achieve this objective by interpolating the probabilities from multiple P-LLM outputs, which come from multiple speakers. For example, our target speaker has a relatively sad speaking tone, but we want to generate speeches that sound happier for him while preserving his timbre. The solution is to 1) extract prosody latent ua from speeches in a happy tone of other speakers and the sad prosody latent ub from the target speech prompt; 2) utilize two language models to separately decode the target prosody code u\u0302 with the prosodic prompt ua and ub. These language models share the same parameters. In every step t of the decoding process, the probability distributions of the two language models are\ninterpolated with the weight \u03b3, which can be formulated as follows:\np (u\u0302) = T\u220f t=0 ( (1\u2212\u03b3) \u00b7p (u\u0302t | u\u0302<t,ub, Concat(zcb, z\u0302c); \u03b8)+\u03b3 \u00b7p (u\u0302t | u\u0302<t,ua, Concat(zca, z\u0302c); \u03b8) ) , (3)\nwhere zcb and zca are the content information from speech clips sb and sa. z\u0302c is the content information of the target sentence. With our prosody interpolation technique, users can freely control the prosodic style of the generated speech in the inference stage. Moreover, the proposed prosody interpolation algorithm utilizes the autoregressive probability distribution of the language model for prosody transfer. Compared with directly substituting the time-averaged prosody representation ub with ua (Karlapati et al., 2020; Za\u0131di et al., 2021),\nthe prosody latent language model is able to mix ua and ub in a soft and fine-grained manner in the autoregressive generation process."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUP",
            "text": "Training Datasets We train Mega-TTS and all baselines on LibriLight (Kahn et al., 2020), which contains 60K hours of unlabelled speech derived from LibriVox audiobooks. The sample rate is 16KHz for all speech data. We transcribe the speech with the hybrid DNN-HMM ASR model pre-trained on 960 hours labeled LibriSpeech following VALL-E (Wang et al., 2023). We align the phoneme sequence with speech using the external alignment tool (McAuliffe et al., 2017).\nModel Configuration Our Mega-TTS consists of three encoders: a prosody latent language model, a mel decoder, and a discriminator. The prosody encoder, timbre encoder, and decoder consist of 5 convolutional blocks with 512 hidden size and 5 kernel size. The content encoder is an 8-layer Transformer (Vaswani et al., 2017) with 512 hidden size. The GAN discriminator follows the architecture of ML-GAN proposed in Chen et al. (2020). The P-LLM model is a decoder-only architecture that contains 12 Transformer layers with 1024 hidden size, which has 151M parameters. The duration predictor is an 8-layer decoder-only Transformer model with 512 hidden size. The codebook embedding size is 1024, and the hidden size of the codebook vector is 256. The compression rate r and d is set as 8 and 16, respectively. For prosody transfer experiments, \u03b3 is set as 0.8.\nTraining and Inference. In the first training stage, we train the first-stage model on 4 NVIDIA A100 GPUs, with a batch size of 48 sentences on each GPU. In the second stage, we train the P-LLM and duration model on 8 NVIDIA A100 GPUs, with a batch size of 4,000 tokens on each GPU. It means that our model supports 4,000 \u00d7 8 frames of prompts theoretically. We use the Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.999, \u03f5 = 10\u22129 and follow the same learning rate schedule in Vaswani et al. (2017). It takes 600k steps for the first stage model\u2019s training and 300K steps for the second stage model\u2019s training until convergence. The predicted mel-spectrograms are transformed into audio samples using pre-trained HiFi-GAN V1 (Kong et al., 2020).\nObjective Metrics For zero-shot TTS, we evaluate the word error rate (WER), speaker similarity (SIM), and average dynamic time warping (DTW) (Mu\u0308ller, 2007) distance of the pitch for the groundtruth speech and synthesized speech. In terms of the cosine speaker similarity, we use the WavLM model (Chen et al., 2022) fine-tuned for speaker verification2 to compute the cosine speaker similarity score between the ground-truth speech and the synthesized speech. The similarity score is in the range of [\u22121, 1], where a larger value indicates a higher similarity of input samples. We also evaluate the word error rate (WER) for cross-lingual TTS. We use the released HuBERT-Large model (Hsu et al., 2021) fine-tuned on the LibriSpeech 960h dataset to transcribe the generated speech into text. Then,\n2https://huggingface.co/microsoft/wavlm-base-plus-sv\nthe WER between the transcribed text and the original target text is measured. We use all samples in the test set for the objective evaluation. For prosody transfer, we evaluate the WER, SIM, duration error (DE), and the moments (standard deviation (\u03c3), skewness (\u03b3) and kurtosis (\u03ba)) (Andreeva et al., 2014; Niebuhr & Skarnitzl, 2019) of the pitch distribution.\nSubjective Metrics We conduct the MOS (mean opinion score) evaluation on the test set to measure the audio naturalness via Amazon Mechanical Turk. We keep the text content and prompt speech consistent among different models to exclude other interference factors. We randomly choose 50 samples from the test set of each dataset for the subjective evaluation, and each audio is listened to by at least 20 testers. We analyze the MOS in two aspects: QMOS (Quality, clarity, naturalness, and high-frequency details) and SMOS (Speaker similarity in terms of timbre reconstruction and prosodic pattern). We also analyze the CMOS in terms of audio quality and speaker similarity. We tell the testers to focus on one corresponding aspect and ignore the other aspect when scoring."
        },
        {
            "heading": "4.2 RESULTS OF ZERO-SHOT SPEECH SYNTHESIS",
            "text": "In this subsection, we evaluate our model with various lengths of speech prompts and compare our model with zero-shot and fine-tuning baselines to demonstrate the effectiveness of the multi-sentence prompting mechanism. We randomly choose 20 speakers from the LibriSpeech test-clean set and randomly choose 400 seconds of speeches for each of them. We split the 400 seconds of speech into a 300-second prompt set and a 100-second target set. We keep the prompts consistent among different models to exclude other interference factors. We compare the zero-shot speech synthesis performance of Mega-TTS with two systems, including: 1) VALL-E (zero-shot) (Wang et al., 2023), a large-scale zero-shot TTS model using large language models to generate discrete speech codes. Since VALL-E has not been open-sourced yet, we carefully implement it for optimal performance; 2) Baseline (fine-tune), a model that incorporates the GAN used in our Mega-TTS to the FastSpeech 2 backbone (Ren et al., 2020). To make the baseline support adaptive scenarios, we use the powerful speaker encoder from Meta-StyleSpeech (Min et al., 2021) to extract timbre information. We carefully fine-tune the baseline system for 2,000 steps to reach an optimal balance between WER and SIM. Note that all of the systems in this experiment are pre-trained on the LibriLight dataset. We provide further explanation for the selection of the baseline systems in Appendix A.7.\nAnalysis As shown in Table 1, as the amount of adaptation data increases, the performance of Mega-TTS continues to improve. Although the performance of VALL-E improves as the data volume increases from 3 seconds to 10 seconds, the performance significantly drops in the 20-second setting due to the single-sentence prompting mechanisms in training. Moreover, since the compression rate of the Encodec model restricts the length of prompts, VALL-E fails to generate reasonable speeches with prompts longer than 20 seconds in our experiments. From another perspective, when we have 10 seconds or 60 seconds of speeches for each speaker, our Mega-TTS surpasses the fine-tuning baseline\nin terms of speech naturalness and speaker similarity. Additionally, when we have 300 seconds of speeches per speaker, Mega-TTS still outperforms the baseline system in terms of WER and achieves comparable performance with it in terms of speaker similarity. We also visualize the WER and SIM in the fine-tuning process and compare the baseline system with Mega-TTS in Figure 3. Our approach can enhance speaker similarity by utilizing more data like fine-tuning baseline, while maintaining a relatively low word error rate."
        },
        {
            "heading": "4.3 RESULTS OF PROSODY TRANSFER",
            "text": "In this subsection, we evaluate the prosody transfer performance of our model by transferring the emotional styles from the ESD dataset (Zhou et al., 2021) to speakers in the LibriSpeech test-clean dataset. We randomly choose 20 speakers from the LibriSpeech test-clean set and choose 50 sentences for each of them. Then, we randomly select an emotional speech clip from the ESD dataset for each of the sentences in the LibriSpeech test-clean set and use the selected emotional speech as the prosodic reference. We keep the reference speeches consistent among different models to exclude other interference factors. We compare the prosody transfer performance of Mega-TTS with two systems, including: 1) CopyCat (Karlapati et al., 2020), a model that utilizes a reference encoder architecture capable of capturing temporal prosodic representations; 2) Daft-Exprt (Za\u0131di et al., 2021), a model disentangles identity and prosodic information through an adversarial training strategy that enables accurate prosody transfer across speakers. To make fair comparisons, we incorporate the techniques for prosody transfer from CopyCat and Daft-Exprt to the baseline system proposed in the previous subsection and scale up the model capacity to ensure that all models have a comparable number of parameters. All of the systems in this experiment are pre-trained on the LibriLight dataset.\nAnalysis Table 2 demonstrates that compared with CopyCat and Daft-Exprt, the moments (\u03c3, \u03b3, and \u03ba) of the generated speeches of Megs-TTS are closer to the ground-truth audio and the DE is lower than other methods, demonstrating the effectiveness of the proposed prosody interpolation techniques. Besides, we observe that our method can efficiently preserve the original timbre and maintain a high audio quality. We also visualize the prosody distribution before and after the prosody transfer process and compare the baseline system with Mega-TTS in Figure 4.\n4.4 ABLATION STUDIES\nProsody and Timbre Prompts We evaluate different lengths of prompts for the MRTE and P-LLM separately. In Table 3, the SIM score and the speech quality increase with longer timbre prompts while the DTW distance almost remains unchanged. When we increase the length of prosody prompts, the DTW distance decreases while the speaker similarity remains at the same level. It can be seen that the proposed timbre and prosody prompting mechanisms boost the subjective speaker similarity in terms of timbre and prosody modeling separately.\nVQ Encoder and MRTE We test the following four settings: 1) w/o MRTE, which removes the MRTE from our model and does not disentangle the prosody and timbre; 2) w/ VAE, which uses VAE to perform generative prosody modeling; 3) w/ VAE+LDM, which uses VAE and latent diffusion model (LDM) (Rombach et al., 2022) to perform generative prosody modeling. The architecture and prompting mechanism of LDM is based on Natu-\nralSpeech 2 (Shen et al., 2023). All baselines use 10 seconds of prompts. The results are shown in Table 4. For setting 1), it can be observed that the removal of MRTE significantly affects both the audio quality and speaker similarity. This is because the timbre information is absorbed by the VQ codebook and puts great pressure on the P-LLM, which demonstrates the effectiveness of decomposing timbre and prosody information. For setting 3), substituting the VQ encoder and P-LLM with VAE and LDM results in similar performance compared to Ours-10s. However, the performance of w/ VAE+LM is still much inferior to Ours-300s, indicating the superiority of the proposed multi-sentence prompting mechanism."
        },
        {
            "heading": "5 CONCLUSIONS",
            "text": "In this paper, we present Mega-TTS, a framework that boosts the prompting mechanisms for zero-shot TTS systems. With the proposed multi-sentence prompting strategy, our approach outperforms the fine-tuning baseline when 10 seconds to 5 minutes of adaptation data is available for each speaker. Furthermore, our method utilizes a prosody interpolation technique to successfully transfer various prosodic styles to the target speaker while preserving the target speaker\u2019s timbre. Experimental results demonstrate that our method exhibits superior performance in terms of audio naturalness and speaker similarity. Due to space limitations, we include additional discussions in the appendix."
        },
        {
            "heading": "A DETAILED EXPERIMENTAL SETTINGS",
            "text": "A.1 DETAILS IN OBJECTIVE EVALUATIONS\nHere, we provide details of the model used in objective evaluations.\nSpeaker Similarity Model To measure the speaker similarity, we use the WavLM (Chen et al., 2022) model fine-tuned for speaker verification from https://huggingface.co/microsoft/ wavlm-base-plus-sv to extract the speaker embedding. Then the cosine similarity between the synthesized speech\u2019s speaker embedding and the ground-truth speech\u2019s speaker embedding is calculated as the speaker similarity score. The WavLM model is pre-trained on 94,000 hours of speech data and fine-tuned on the VoxCeleb1 dataset using an X-Vector head with an Additive Margin Softmax loss, which achieves 0.84%, 0.928%, and 1.758% EER (Equal Error Rate) on the Vox1-O, Vox1-E, and Vox1-H trial lists.\nASR Model To measure the audio quality and speech intelligibility, we evaluate the word error rate (WER) metric. We use the fine-tuned HuBERT-Large model to transcribe the synthesized speech into text and calculate the WER between the transcribed text and the original target text. The HuBERTLarge model from https://huggingface.co/facebook/hubert-large-ls960-ft is fine-tuned on 960h of Librispeech and achieves 1.5%, 3.0%, 1.9%, and 3.3% WER on the dev-clean, dev-other, test-clean, and test-other set of Librispeech.\nA.2 DETAILS IN SUBJECTIVE EVALUATIONS\nWe perform the audio quality and speaker similarity evaluations on Amazon Mechanical Turk (MTurk). For each dataset, we randomly select 50 samples from the test set and use the TTS systems to generate the audio samples. Each audio has been listened to by at least 20 listeners. For MOS, each tester is asked to evaluate the subjective score of a sentence on a 1-5 Likert scale. For CMOS, listeners are asked to compare pairs of audio generated by systems A and B following Loizou (2011), indicating which of the two audio they prefer. For audio quality evaluation (QMOS and CMOS-Q), we tell listeners to \u201cPlease focus on the speech quality in terms of clarity, naturalness, and high-frequency details, and ignore other factors\u201d. For speaker similarity evaluations (MOS-S), we tell listeners to \u201cPlease focus only on the similarity of the speaker to the reference one in terms of the timbre and prosodic patterns, and ignore the differences of content, grammar, audio quality, or other factors.\u201d. We paid $15 to participants hourly and totally spent about $1200 on participant compensation. We tell the participants that the data will be used in scientific research.\nA.3 DETAILED NETWORK STRUCTURE\nMRTE As shown in Figure 5, the proposed MRTE is composed of two convolution stacks and a downsampling block. To reduce the computational requirements while maintaining the quality of timbre reconstruction, we downsample the timbre hidden states by a factor of d = 16 in length. In training, we randomly sample 2,000 frames from y\u0303 for training efficiency.\nVQ Encoder The bottleneck of our VQ Encoder is composed of a max pooling layer with a stride of 8 and a vector quantised layer. In our experiments, we found that compressing the mel-spectrograms with a compression rate of r = 8 yields superior results compared to phoneme-level compression. We have tried different compression rates (2, 4, 8, 16, 32) and found that r = 8 reached an optimal balance between the reconstruction performance and compression. On the other hand, in the training process, we also found that the vanilla VQ-VAE suffers from codebook collapse (Takida et al., 2022), which means only a small portion of codebook vectors are optimized. It restricts the expressive capacity of the codebook and affects the convergence of the training process. To solve the codebook collapse issue, we adopt a dynamical initialization strategy based on CVQ-VAE (Zheng & Vedaldi, 2023) during training, which ensures the code vectors that are less-used or unused to be modified more than frequently used ones.\nA.4 MODEL CONFIGURATIONS\nWe provide detailed hyper-parameter settings about the model configuration in Table 5.\nA.5 ERROR BARS AND RANDOM SEEDS\nFor the subjective evaluations, we report confidence intervals of the results of MOS tests. For the objective evaluations, we ran the experiments 10 times with 10 different random seeds ([1234, 1111, 2222, 3333, 4444, 5555, 6666, 7777, 8888, 9999]) and obtained the averaged results.\nA.6 SAMPLING STRATEGY FOR P-LLM\nIn all of our experiments, we utilize the top-k sampling strategy for P-LLM, where k is set to 10. The sampling-based method, when used with an appropriate k, enhances the output diversity compared to greedy decoding.\nA.7 ABOUT THE SELECTION OF BASELINES\nVALL-E (Wang et al., 2023), NaturalSpeech 2 (Shen et al., 2023), and VoiceBox (Matthew et al., 2023) are the state-of-the-art zero-shot TTS models. In the experiments of zero-shot TTS, we have tried to carefully reproduce their works but failed to reproduce NaturalSpeech 2 and VoiceBox. Since all of them do not provide the pre-trained models and source code, we only compare Mega-TTS with VALL-E in our experiments.\nA.8 DETAILED DECOMPOSITION STRATEGY\nThe prosody encoder Ep aims to capture fine-grained and local prosodic style zp. For local prosodic style zp, we assume that psd(\u00b7) is a perfect local prosody extractor, and we can obtain the following equation: I(psd(yt), psd(y\u0303)) = 0. The content information zc is also local and fine-grained like zp. On the other hand, the global prosodic information like the averaged volume and pitch can not be captured by Ec, intuitively. And since we have designed an information bottleneck B(\u00b7) for Ep, the global prosodic information will be prioritized by the timbre encoder Et and stored in H(zt). Now that both the local and global prosodic information is appropriately extracted, the validity of Equation 1 and our disentanglement strategy can be ensured."
        },
        {
            "heading": "B ABOUT SCALING UP DATASET SIZE",
            "text": "Scaling up dataset size is crucial for the practical application of zero-shot TTS. Therefore, we crawled 200K hours of audiobook recordings from YouTube and novelfm3. The crawled corpus contains both labelled and unlabelled speeches, and most of them do not have speaker information. To transcribe the unlabelled speech in the wild, we use a powerful ASR model called WhisperX (Bain et al., 2023).\nAnd to obtain the speaker information, we use a released automatic speaker diarization model called pyannote.audio4, which achieves DER=11.24% on the VoxConverse dataset and DER=14.09% on the AISHELL-4 dataset. In this experiment, we do not change the hyperparameter settings of our model. The results are shown in Table 6. It can be seen that increasing the dataset size\ncan improve the speaker similarity of the generated speeches."
        },
        {
            "heading": "C ABOUT THE DEFINITION OF ADAPTIVE TTS",
            "text": "The concept of adaptive TTS encompasses many aspects like the adaption for different voices, languages, styles, and domains (Tan et al., 2021). It is also known as various terms in academia and industry, such as voice adaptation (Chen et al., 2018), voice cloning (Arik et al., 2018), custom voice (Chen et al., 2021), etc. In this paper, we primarily focus on adaptive TTS for different voices.\nD VISUALIZATION OF ATTENTION MATRICES\nTo further verify the proposed P-LLM and multi-sentence prompting mechanism, we visualize the attention matrices averaged across all layers of P-LLM in Figure 6. In this experiment, we separately conduct short-sentence generation and long-sentence generation. For short-sentence generation, we randomly selected two sentences that are shorter than 3 seconds from speaker \u201c908\u201d in the LibriSpeech test-clean set and concatenated them together. The target texts for Figure 6 (a) and (b) are both about 15 words in length. For long-sentence generation, we randomly selected two sentences that are longer than 15 seconds from speaker \u201c908\u201d in the LibriSpeech test-clean set and concatenated them together. The target texts for Figure 6 (c) and (d) are both about 100 words in length. It can be seen that our P-LLM can capture both short-term and long-term information, demonstrating the effectiveness of the P-LLM\u2019s training strategy and the multi-sentence prompting mechanism.\n3https://novelfm.changdunovel.com/ 4https://huggingface.co/pyannote/speaker-diarization"
        },
        {
            "heading": "E LIMITATIONS AND ETHICS IMPACTS",
            "text": "In this section, we begin by discussing the limitations of the proposed method and outlining our strategies for addressing them in future research. Subsequently, we discuss the ethical impacts that might be brought by zero-shot TTS and our measures to address these concerns.\nE.1 LIMITATIONS AND FUTURE WORK\nFirstly, our model is trained on an English dataset and does not support multilingual TTS. We plan to address this problem by introducing more multilingual training data. Secondly, the speech quality can be improved by introducing more high-fidelity training data. Thirdly, a well-designed attention window may further enhance the in-context-learning capability of our P-LLM.\nE.2 ETHICS IMPACTS\nMega-TTS improves the quality and efficiency of zero-shot speech synthesis, which makes it easier for people to synthesize personalized speeches. Under appropriate and legal usage, this technique could facilitate applications like movies, games, podcasts, and other services, making human life more convenient. However, zero-shot TTS may be misused in deepfake-related usages, such as spoofing voices. To handle this, potential solutions like building a corresponding deepfake detection model should be considered. We also plan to add watermarks to the synthesized speeches so that the public can easily tell whether the speeches are synthesized or not. Additionally, restrictions will be included in the license of our project to prevent the misuse of the model."
        },
        {
            "heading": "F DESCRIPTION OF INFORMATION BOTTLENECK",
            "text": "The settings of the information bottleneck B(\u00b7) are crucial for the performance of disentanglement of our proposed method. Intuitively, there are four crucial variables for ensuring an appropriate information bottleneck: the number of codebook embedding, the codebook channel size, the compression rate r of the VQ Encoder, and the downsampling rate d of the MRTE. However, the search space of these hyperparameters is too large. Since the settings of r and d also influence the reconstruction quality, the burden of P-LLM, and the computational requirements, we first consider r and d, fix them, and find the best setting for the hyperparameters of the codebook.\nThe Compression Rate r We have conducted evaluations for different compression rates r of the VQ Encoder. In the experiments, we found that a lower compression rate would result in better reconstruction performance for the compressive acoustic autoencoder, but it would impose a heavier burden on P-LLM since the token sequence is longer. As shown in Table 10, although r = 2 achieves the highest objective similarity score, the subjective speech quality and similarity significantly decrease, which means the final quality of generation is affected. Therefore, we use r = 8 to reach an optimal balance between the reconstruction performance and compression.\nThe Downsampling Rate d We have conducted evaluations for different downsampling rates d of the MRTE. The results are shown in Figure 11. It can be seen that when the downsampling rate d of the MRTE is low, the mel-spectrogram sequence can provide more information to the timbre encoder, resulting in better reconstruction. However, a low downsampling ratio puts a significant computational burden on the attention operation in MRTE. To reduce the computational requirements while maintaining the quality of timbre reconstruction, we choose d = 16 for our Mega-TTS.\nThe Codebook Since we have the optimal hyperparameters r and d, we will start choosing the parameters of the codebook to achieve the optimal performance of disentanglement. In this experiment, we train our compressive acoustic autoencoder with different sizes of codebooks and evaluate their performance. Here we use a relatively harsh approach for the evaluation. Specifically, we use the proposed encoders to extract the timbre, content, and prosody embeddings of the test samples. Then, we randomly shuffle the timbre embedding sequence and reconstruct the melspectrogram with the original content, original prosody, and shuffled timbre information. We calculate the objective speaker similarity (SIM) between the shuffled ground-truth speech and the generated speech. We also calculate the CMOS-Q and CMOS-S as the subjective metrics (note that we also ask the users to consider the prosodic similarity in the CMOS-S tests as described in Section A.2). We do not measure the DTW distance of the pitch curve due to the fact that the average pitch value may vary greatly among different speakers. But our VQ Encoder aims at disentangling the fine-grained prosodic patterns (not the average pitch value). The results are shown in Table 9. It can be seen that when the channel size is 256 and the number of VQ Embedding is 1024, the disentanglement performance achieves the best.\nThe Information Bottleneck with Different Amount of Data Intuitively, the performance of the information bottleneck might be very sensitive to the size of the dataset. Therefore, we conduct experiments analyzing the relationship between dataset size and the hyperparameters. The results are presented in Appendix G. Although the hyperparameters do not change across these experiments, we find that the model consistently performs well in scenarios with varying amounts of available data."
        },
        {
            "heading": "G SCALING WITH DIFFERENT SIZES OF TRAINING DATA",
            "text": "Here we evaluated the performance of our Mega-TTS scale with varying amounts of available data. In this experiment, all of the systems use 3 seconds of speech prompts. The results are shown in the following table. We can see that Mega-TTS performs well with different sizes of training data, while VALL-E fails to obtain satisfying results when the data is insufficient. We also scale our Mega-TTS with 200K hours of speeches and the results can be found in Appendix B."
        },
        {
            "heading": "H THE STRATEGY OF PROSODY MODELING",
            "text": "In this section, we conduct experiments to verify the performance of the phoneme-level, word-level, and stride-8-level prosody modeling. Stride-8 means that the stride of the pooling layer inside the VQ encoder is set to 8. It is worth noting that ProsoSpeech (Ren et al., 2022) utilizes word-level prosody modeling. Both of us use the auto-regressive Transformer for prosody modeling. However, ProsoSPeech aims to improve the naturalness of prosody modeling. Compared with it, our P-LLM aims at improving the similarity of speaker-relevant prosodic patterns, which extracts fine-grained prosodic information from latent prosodic prompts by leveraging the powerful in-context learning capability of LLM. The experimental results are shown in the following table. It can be seen that the stride-8-level prosody modeling achieves the best performance. Intuitively speaking, the phonemelevel prosody modeling provides finer-grained information for better reconstruction while word-level prosody modeling provides more semantic information. Both of these methods would be easily afftected by the alignment accuracy of the speeches in training and inference stages. In order to enhance the stability and performance of the proposed model, we use stride-8-level prosody modeling."
        },
        {
            "heading": "I SPECIAL CASES FOR ASSUMPTION 1",
            "text": "In practical scenarios, there is a special case for Assumption 1: the timbre of a speaker may vary significantly over different time periods. To address this special case, we select y\u0303 randomly from regions near yt as much as possible, ensuring that the timbre information of y\u0303 is close to that of yt."
        },
        {
            "heading": "J DIFFERENT LENGTHS OF CONTEXT DURING TRAINING",
            "text": "Here we make ablation studies for different lengths of context during Mega-TTS\u2019s training process. We separately train P-LLM with different numbers of the contextual VQ code tokens and train our compressive acoustic autoencoder with different numbers of the contextual mel-spectrogram frames for MRTE. The results are shown in Figure 7. It can be seen that when we increase the length of context, the performance of the model during training significantly improves, demonstrating the effectiveness of our multi-reference training strategy."
        },
        {
            "heading": "K EXPLANATIONS ABOUT MORE CASES OF ROBUST SPEECH SYNTHESIS",
            "text": "In our Mega-TTS, we employ a language model only for prosody modeling, enabling our model to benefit from the advantages of in-context learning provided by the LLM model. This approach also helps to address the robustness issues (word skipping or repeating) associated with the autoregressive TTS model. Therefore, we make explanations about more cases of robust speech synthesis, to demonstrate our method\u2019s necessity. In commercial scenarios, news reporting, and other formal scenarios, robustness is a crucial factor. Just a few repeating or skipping words can have significant negative impacts. These situations are better suited for models with duration models that ensure robustness, such as FastSpeech [4], Glow-TTS [5], and our Mega-TTS. However, for models like tacotron [6], word omissions or repetitions can significantly affect the listening experience. On the other hand, in some scenarios, robustness is relatively less important. For example, occasional missing words or repetitions in dialogue scenes can also be natural."
        },
        {
            "heading": "L RESULTS WITH NOISY REFERENCE PROMPTS",
            "text": "To verify our model\u2019s robustness against noisy reference prompts, we conduct experiments on LibriSpeech test-other set. The experimental setup for this experiment is consistent with the one described in Section 4.2. The results are shown in Table 12. It can be seen that Mega-TTS maintains excellent performance with noisy reference prompts."
        }
    ],
    "year": 2023
}