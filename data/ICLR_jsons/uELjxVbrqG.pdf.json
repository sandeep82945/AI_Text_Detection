{
    "abstractText": "The current face recognition (FR) algorithms has achieved a high level of accuracy, making further improvements increasingly challenging. While existing FR algorithms primarily focus on optimizing margins and loss functions, limited attention has been given to exploring the feature representation space. Therefore, this paper endeavors to improve FR performance in the view of feature representation space. Firstly, we consider two FR models that exhibit distinct performance discrepancies, where one model exhibits superior recognition accuracy compared to the other. We implement orthogonal decomposition on the features from the superior model along those from the inferior model and obtain two sub-features. Surprisingly, we find the sub-feature orthogonal to the inferior still possesses a certain level of face distinguishability. We adjust the modulus of the sub-features and recombine them through vector addition. Experiments demonstrate this recombination is likely to contribute to an improved facial feature representation, even better than features from the original superior model. Motivated by this discovery, we further consider how to improve FR accuracy when there is only one FR model available. Inspired by knowledge distillation, we incorporate the intraclass incoherence constraint (IIC) to solve the problem. Experiments on various FR benchmarks show the existing state-of-the-art method with IIC can be further improved, highlighting its potential to further enhance FR performance.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuanqing Huang"
        },
        {
            "affiliations": [],
            "name": "Yinggui Wang"
        },
        {
            "affiliations": [],
            "name": "Le Yang"
        },
        {
            "affiliations": [],
            "name": "Lei Wang"
        }
    ],
    "id": "SP:a1952b35d4813351f03be88d01773338a1893e5e",
    "references": [
        {
            "authors": [
                "Qiong Cao",
                "Li Shen",
                "Weidi Xie",
                "Omkar M. Parkhi",
                "Andrew Zisserman"
            ],
            "title": "VGGFace2: A dataset for recognising faces across pose and age",
            "venue": "Proceedings - 13th IEEE International Conference on Automatic Face and Gesture Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Jie Chang",
                "Zhonghao Lan",
                "Changmao Cheng",
                "Yichen Wei"
            ],
            "title": "Data uncertainty learning in face recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Jiankang Deng",
                "Jia Guo",
                "Niannan Xue",
                "Stefanos Zafeiriou"
            ],
            "title": "ArcFace: Additive angular margin loss for deep face recognition",
            "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Chi Nhan Duong",
                "Khoa Luu",
                "Kha Gia Quach",
                "Ngan Le"
            ],
            "title": "Shrinkteanet: Million-scale lightweight face recognition via shrinking teacher-student networks",
            "year": 1905
        },
        {
            "authors": [
                "Yushu Feng",
                "Huan Wang",
                "Haoji Roland Hu",
                "Lu Yu",
                "Wei Wang",
                "Shiyan Wang"
            ],
            "title": "Triplet distillation for deep face recognition",
            "venue": "IEEE International Conference on Image Processing (ICIP),",
            "year": 2020
        },
        {
            "authors": [
                "Yandong Guo",
                "Lei Zhang",
                "Yuxiao Hu",
                "Xiaodong He",
                "Jianfeng Gao"
            ],
            "title": "Ms-celeb-1m: A dataset and benchmark for large-scale face recognition",
            "venue": "In European conference on computer vision,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531,",
            "year": 2015
        },
        {
            "authors": [
                "Yuge Huang",
                "Pengcheng Shen",
                "Ying Tai",
                "Shaoxin Li",
                "Xiaoming Liu",
                "Jilin Li",
                "Feiyue Huang",
                "Rongrong Ji"
            ],
            "title": "Improving face recognition from hard samples via distribution distillation loss",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Yuge Huang",
                "Yuhan Wang",
                "Ying Tai",
                "Xiaoming Liu",
                "Pengcheng Shen",
                "Shaoxin Li",
                "Jilin Li",
                "Feiyue Huang"
            ],
            "title": "Curricularface: adaptive curriculum learning loss for deep face recognition",
            "venue": "In proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Yuge Huang",
                "Jiaxiang Wu",
                "Xingkun Xu",
                "Shouhong Ding"
            ],
            "title": "Evaluation-oriented knowledge distillation for deep face recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Minchul Kim",
                "Anil K Jain",
                "Xiaoming Liu"
            ],
            "title": "Adaface: Quality adaptive margin for face recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Shen Li",
                "Jianqing Xu",
                "Xiaqing Xu",
                "Pengcheng Shen",
                "Shaoxin Li",
                "Bryan Hooi"
            ],
            "title": "Spherical confidence learning for face recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Weiyang Liu",
                "Yandong Wen",
                "Zhiding Yu",
                "Ming Li",
                "Bhiksha Raj",
                "Le Song"
            ],
            "title": "SphereFace: Deep hypersphere embedding for face recognition",
            "venue": "Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Xuxing Liu",
                "Xiaoqin Tang",
                "Shanxiong Chen"
            ],
            "title": "Learning a similarity metric discriminatively with application to ancient character recognition",
            "venue": "In International Conference on Knowledge Science, Engineering and Management,",
            "year": 2021
        },
        {
            "authors": [
                "Brianna Maze",
                "Jocelyn Adams",
                "James A. Duncan",
                "Nathan Kalka",
                "Tim Miller",
                "Charles Otto",
                "Anil K. Jain",
                "W. Tyler Niggel",
                "Janet Anderson",
                "Jordan Cheney",
                "Patrick Grother"
            ],
            "title": "IARPA janus benchmark-C: Face dataset and protocol",
            "venue": "Proceedings - 2018 International Conference on Biometrics, ICB 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Qiang Meng",
                "Shichao Zhao",
                "Zhida Huang",
                "Feng Zhou"
            ],
            "title": "Magface: A universal representation for face recognition and quality assessment",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Stylianos Moschoglou",
                "Athanasios Papaioannou",
                "Christos Sagonas",
                "Jiankang Deng",
                "Irene Kotsia",
                "Stefanos Zafeiriou"
            ],
            "title": "AgeDB: The First Manually Collected, In-the-Wild Age Database",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops,",
            "year": 2017
        },
        {
            "authors": [
                "Adriana Romero",
                "Nicolas Ballas",
                "Samira Ebrahimi Kahou",
                "Antoine Chassang",
                "Carlo Gatta",
                "Yoshua Bengio"
            ],
            "title": "Fitnets: Hints for thin deep nets",
            "venue": "arXiv preprint arXiv:1412.6550,",
            "year": 2014
        },
        {
            "authors": [
                "Florian Schroff",
                "Dmitry Kalenichenko",
                "James Philbin"
            ],
            "title": "Facenet: A unified embedding for face recognition and clustering",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Soumyadip Sengupta",
                "Jun Cheng Chen",
                "Carlos Castillo",
                "Vishal M. Patel",
                "Rama Chellappa",
                "David W. Jacobs"
            ],
            "title": "Frontal to profile face verification in the wild",
            "venue": "IEEE Winter Conference on Applications of Computer Vision,",
            "year": 2016
        },
        {
            "authors": [
                "Yichun Shi",
                "Anil K Jain"
            ],
            "title": "Probabilistic face embeddings",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Sungho Shin",
                "Joosoon Lee",
                "Junseok Lee",
                "Yeonguk Yu",
                "Kyoobin Lee"
            ],
            "title": "Teaching where to look: Attention similarity knowledge distillation for low resolution face recognition",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Kihyuk Sohn"
            ],
            "title": "Improved deep metric learning with multi-class n-pair loss objective",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "David Svitov",
                "Sergey Alyamkin"
            ],
            "title": "Margindistillation: Distillation for margin-based softmax",
            "venue": "arXiv preprint arXiv:2003.02586,",
            "year": 2020
        },
        {
            "authors": [
                "Philipp Terh\u00f6rst",
                "Marco Huber",
                "Naser Damer",
                "Florian Kirchbuchner",
                "Arjan Kuijper"
            ],
            "title": "Unsupervised enhancement of soft-biometric privacy with negative face recognition",
            "venue": "arXiv preprint arXiv:2002.09181,",
            "year": 2020
        },
        {
            "authors": [
                "Feng Wang",
                "Jian Cheng",
                "Weiyang Liu",
                "Haijun Liu"
            ],
            "title": "Additive margin softmax for face verification",
            "venue": "IEEE Signal Processing Letters,",
            "year": 2018
        },
        {
            "authors": [
                "Hao Wang",
                "Yitong Wang",
                "Zheng Zhou",
                "Xing Ji",
                "Dihong Gong",
                "Jingchao Zhou",
                "Zhifeng Li",
                "Wei Liu"
            ],
            "title": "CosFace: Large Margin Cosine Loss for Deep Face Recognition",
            "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Jian Wang",
                "Feng Zhou",
                "Shilei Wen",
                "Xiao Liu",
                "Yuanqing Lin"
            ],
            "title": "Deep metric learning with angular loss",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Xiaobo Wang",
                "Shuo Wang",
                "Shifeng Zhang",
                "Tianyu Fu",
                "Hailin Shi",
                "Tao Mei"
            ],
            "title": "Support vector guided softmax loss for face recognition",
            "venue": "arXiv preprint arXiv:1812.11317,",
            "year": 2018
        },
        {
            "authors": [
                "Dong Yi",
                "Zhen Lei",
                "Shengcai Liao",
                "Stan Z. Li"
            ],
            "title": "Learning face representation from scratch",
            "year": 2014
        },
        {
            "authors": [
                "Nanhai Zhang",
                "Weihong Deng"
            ],
            "title": "Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments",
            "venue": "International Conference on Biometrics, ICB 2016,",
            "year": 2016
        },
        {
            "authors": [
                "Xiao Zhang",
                "Rui Zhao",
                "Yu Qiao",
                "Xiaogang Wang",
                "Hongsheng Li"
            ],
            "title": "Adacos: Adaptively scaling cosine logits for effectively learning deep face representations",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Ying Zhang",
                "Tao Xiang",
                "Timothy M Hospedales",
                "Huchuan Lu"
            ],
            "title": "Deep mutual learning",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Borui Zhao",
                "Quan Cui",
                "Renjie Song",
                "Yiyu Qiu",
                "Jiajun Liang"
            ],
            "title": "Decoupled knowledge distillation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Face recognition (FR) has been receiving increasing attention in the field of machine learning. With the development of deep learning and the availability of large-scale face datasets, the accuracy of face recognition tasks is steadily improving. Many FR algorithms are devoted to optimizing margins and loss functions. Margin-based methods such as SphereFace (Liu et al., 2017), CosFace (Wang et al., 2018b) and ArcFace (Deng et al., 2019) introduce different forms of margin functions. Adaptive loss functions-based methods such as Curricularface (Huang et al., 2020b), MagFace (Meng et al., 2021) and AdaFace (Kim et al., 2022) include adaptiveness in the training objective for mining hard samples (Lin et al., 2017), scheduling difficulty during training (Huang et al., 2020b), and finding optimal hyperparameters (Zhang et al., 2019). Nevertheless, the exploration of the feature representation space in face recognition has been somehow overlooked. From the perspective of feature representation, an advanced FR model makes the features of face images increasingly distinguishable and attempts to search the optimal feature space with the most representational capability for recognition.\nTherefore, we are absorbed in interpreting the performace of FR from the view of the feature space and further considering how to enhance the performance of an existing face recognition model. Firstly, we compare two FR models that exhibit distinct performance discrepancies. As shown in Fig. 1(a), for the clarity of presentation, we use two-dimensional vectors to illustrate the feature representations space of different FR models at the feature level, and choose ArcFace (Deng et al., 2019) and CosFace (Wang et al., 2018b) as examples. Here, d represents an optimal but unknown feature representation space for FR. a and b represent the feature space of ArcFace and CosFace.\n*These authors contributed equally to this work. \u2020Corresponding author.\nSince it is generally believed that ArcFace perform better than CosFace, a should be closer to d, compared with b (i.e. \u2220\u03b1 < \u2220\u03b2). We perform orthogonal decomposition on the feature obtained from ArcFace and get two sub-features, where one of the sub-features p is parallel to b (we call the sub-feature pro-feature). The other sub-feature c is orthogonal to b (we call it innovation).\nIn Table 1, we evaluate FR performance over features from ArcFace, CosFace, innovation and profeature decomposed by ArcFace, respectively (depicted as in Fig. 1(a)). The results show that ArcFace has the best performance in most cases, while innovation exhibits certain capabilities in distinguishing faces since its average accuracy outperforms CosFace. Unexpectedly, the accuracy of pro-feature is far lower than that of CosFace. The reasons behind this may be explained as follows. The angle between the feature from ArcFace and the feature from CosFace may be less than 90 degrees in some cases and more than 90 degrees in other cases. When it is less than 90 degrees, the direction of the pro-feature is exactly aligned with that from CosFace, and the recognition accuracy is thus similar. While it is greater than 90 degrees, the direction of the pro-feature would become opposite to the direction of features from CosFace, rendering poor recognition accuracy and degrading the average FR performance. However, it does not influence the accuracy of innovation since innovation is always orthogonal to features from CosFace. Such discovery can lead to a conclusion that the high-precision model (ArcFace) performs better than the low-precision model (CosFace) because the former contains innovations that are irrelevant to the feature from the low-precision model, while these innovations do have a promoting effect on face recognition. In other words, we can optimize the features extracted from an available FR model by incorporating appropriate innovations to enhance its performance.\nWith the above analysis in mind, innovation motivates us to improve FR accuracy in the scenario where only a single FR model is available, as shown in Fig. 1(b). We attempt to utilize intra-class irrelevance on the feature from an existing state-of-the-art model such that its innovation can be learned (denoted by a\u22a5 in Fig. 1(b)), properly scaled, and added to its original feature to get an improved feature representation space(denoted by anew in Fig. 1(b)). Consequently, the improved feature representation would be further closer to the optimal feature d.\nIn general, the contributions of this work are as follows:\n\u2022 The concept of innovation in FR is put forward for the first time. Considering two FR models, we perform orthogonal decomposition on the features from the superior model with higher recognition accuracy. The sub-feature parallel to the features from the inferior model is pro-feature, and the other sub-feature orthogonal to features from the inferior model is innovation. Experiments show that innovation still has a high level of face distinguishability.\n\u2022 Moreover, innovation inherently has a positive impact on face recognition. We adjust the modulus of innovation, and then synthesize new features by recombining the modified innovation and pro-feature. Experiments indicate this synthesized features are likely to have better representational capability for recognition than the original feature.\n\u2022 Furthermore, we consider how to enhance FR performance when there is only one FR model available. Inspired by knowledge distillation, we adopt a framework incorporating the intra-class irrelevance constraint. In our framework, an existing state-of-the-art FR model serves as the teacher network and the student has the same architecture as the teacher. However, the student network not only possesses the capability for recognition but also learns a dissimilar feature representation space. Experiments show that the performance of the student network outperforms the teacher network by incorporating intra-class irrelevance."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 FACE RECOGNITION",
            "text": "Face recognition algorithms are generally divided into methods with metric-learning-based loss functions, methods with adaptive loss functions and those with uncertainty modeling. Early methods with metric learning include contrastive loss (Liu et al., 2021), triplet loss (Schroff et al., 2015), pair loss (Sohn, 2016) and angular loss (Wang et al., 2017). Then, in this category, some algorithms with higher accuracy have been proposed, which include SphereFace (Liu et al., 2017), AM-softmax (Wang et al., 2018a), SV-AM-Softmax (Wang et al., 2018c), CosFace (Wang et al., 2018b), ArcFace (Deng et al., 2019).\nMethods with adaptive loss functions introduce an element of adaptiveness in the training objective for either hard sample mining (Lin et al., 2017), finding optimal hyperparameters (Zhang et al., 2019), or brings the idea of curriculum learning into the loss function (Huang et al., 2020b). In this category, MagFace (Meng et al., 2021) and AdaFace (Kim et al., 2022) are algorithms with higher accuracy. MagFace explores the idea of applying different margins based on recognizability. It applies large angular margins to high-norm features on the premise that high-norm features are easily recognizable. AdaFace develops a new loss function that emphasizes samples of different difficulties based on their image quality, and it achieves this in the form of an adaptive margin function by approximating the image quality with feature norms. Algorithms with uncertainty modeling include probabilistic face embeddings (PFE (Shi & Jain, 2019)), sphere confidence face (SCF (Li et al., 2021)), and data uncertainty learning in FR (DUL (Chang et al., 2020)). PFE is the first work to consider data uncertainty in FR, and it estimates a Gaussian distribution, instead of a fixed point, in the latent space. While being effective, PFE is limited in that it does not learn the embedded feature (mean) but only the uncertainty. DUL applies data uncertainty learning (DUL) to face recognition such that feature (mean) and uncertainty (variance) are learned simultaneously. Besides, SCF theoretically and empirically identifies two main failures of PFE when it is applied to spherical deterministic embeddings aforementioned. To address these issues, SCF develop a novel framework for face confidence learning in spherical space, and extend the von Mises Fisher density to its r-radius counterpart and derive a new optimization objective in closed form.\nTerho\u0308rst et al. (2020) introduced a seemingly similar concept, called \u201dnegative face representation\u201d. However, this method was primarily focused on privacy protection, as its \u201dnegative face representation\u201d solely influenced the face verification stage and did not impact the embedding mapping process where facial features are extracted. Furthermore, it did not contribute to improving the recognition accuracy of the FR model. In contrast, our method directly influences the face-to-embedding mapping process, leading to an improvement in the recognition accuracy of the final model."
        },
        {
            "heading": "2.2 KNOWLEDGE DISTILLATION",
            "text": "The method in this paper uses the framework of knowledge distillation. The general methods of knowledge distillation and the methods used in face recognition knowledge distillation are briefly introduced below.\nKnowledge distillation has been actively investigated and widely used in many computer vision tasks. The basic idea proposed by Hinton et al. (2015) minimizes the KL divergence of softened class probabilities between the teacher and the student. Later variants of distillation strategies are proposed to utilize diverse information from the teacher model, which can be divided into two types, i.e., logit distillation and feature distillation. As for the feature distillation, typical algorithms include FitNet (Romero et al., 2014), ShrinkTeaNet (Duong et al., 2019), TripletDistillation (Feng et al., 2020), and MarginDistillation (Svitov & Alyamkin, 2020). Different from feature-based distillation methods, logit distillation does not require the student to mimic the teacher\u2019s representation space, but rather to preserve the high semantic consistency with the teacher. The classical methods include the Kullback-Leibler Divergence-based algorithm (Hinton et al., 2015), the mutual learning based algorithm (Zhang et al., 2018), and DKD (Zhao et al., 2022), which firstly decouple the classical KD Loss into target class knowledge distillation and non-target class knowledge distillation.\nThe typical methods used in face recognition knowledge distillation include evaluation-oriented knowledge distillation for FR (EKD) (Huang et al., 2022), and improved FR via distribution distillation loss (DDL) (Huang et al., 2020a). EKD introduces the evaluation-oriented method that optimizes the student model\u2019s critical relations that cause the TPR and FPR difference between the teacher and student models. DDL uses the distillation loss to minimize the gap between easy and hard sample features. A recent research Shin et al. (2022) introduced attention similarity by a knowledge distillation framework where both the student and teacher networks are of the same size. However, it still follows a traditional paradigm of knowledge distillation, attempting the student network to learn knowledge that is similar to the teacher network. In contrast, our proposed method aims for the student network to learn different knowledge from the teacher network."
        },
        {
            "heading": "3 PROPOSED APPROACH",
            "text": "The fundamental goal of FR algorithms is to learn the optimal feature space with the most representational capability for recognition. In this paper, we suppose the feature representation space change reflects in the accuracy improvement. Firstly, we provide a detailed analysis of the feature space by feature decomposition and recombination in Section 3.1. Experimental results show that such recombination is likely to contribute to an improved facial feature space. However, it is a challenge to find innovation in a scenario where there is only one single model available. To solve the problem, we adopt a framework inspired by knowledge distillation in Section 3.1. We utilize intra-class incoherence constraint (IIC) to learn innovation and finally obtain a better facial feature representation from an existing model."
        },
        {
            "heading": "3.1 FEATURE DECOMPOSITION AND RECOMBINATION",
            "text": "Firstly, we review the feature decomposition in Introduction, and compare two FR models that exhibit distinct performance discrepancies. We perform orthogonal decomposition on the feature from the superior model and obtain two sub-features, pro-feature and innovation. The pro-feature is parallel to the feature from the inferior model, and innovation is orthogonal to the feature from the inferior model. Let a be the feature extracted from the superior model, b be the feature obtained by the inferior model, and c be innovation. According to the Gram-Schmidt Orthogonalization, we have\nc = a\u2212 \u27e8a, b\u27e9 \u2225b\u2225\u2225b\u2225 b, (1)\nwhere \u27e8a, b\u27e9 denotes the inner product operation of a and b, \u2225 \u00b7 \u2225 represents the L2-norm. Table 1 indicates that innovation also has a certain level of face distinguishability. This indicates that the feature a indeed contains useful information irrelevant to the feature b. In other words, with the help of innovation, the feature a gets closer to the optimal direction d. Since the optimal feature direction is not known, it is natural to investigate whether we can change the modulus of innovation,\nand synthesize a new feature anew using modified innovation and pro-feature, with the hope that anew (depicted in Fig. 1(b)) would be closer to the optimal direction than a.\nAccording to Fig. 2, there are two cases for synthesizing features. In the first case, as shown in Fig. 2(a), a and b are on different sides of d. In this case, it is necessary to reduce the modulus of innovation (or increase the modulus of pro-feature), so that anew can be closer to the direction d. However, excessive reduction of the modulus of innovation (or increase the modulus of pro-feature too much) will lead to the angle between anew (a\u2212 or a+) and d being greater than that between a and d, which will eventually impact the recognition accuracy. In the second case, as shown in Fig. 2(b), a and b are on the same sides of d. In this case, it is necessary to increase the modulus of innovation (or reduce the modulus of pro-feature), so that anew may be closer to the direction d. Similarly, extra care is required when increasing the modulus. We take an example where ArcFace servers as a and CosFace servers as b, and show the performance of the above recombinations in Table 2. We can observe that this recombination is likely to contribute to an improved facial feature representation, even better than features from ArcFace.\nMotivated by this discovery, we further investigate a more practical scenario where only a single model is available (like a in Fig. 1(b)), but its improved model (anew in Fig. 1(b)) and innovation (a\u22a5 Fig. 1(b)) are unknown. We need to search for innovation from the existing feature space and properly scale it to synthesize a better feature. Through the aforementioned analysis, we identify two key characteristics of innovation. Firstly, the innovation a\u22a5 is orthogonal to a. Secondly, it facilitates an improvement in recognition accuracy."
        },
        {
            "heading": "3.2 FR WITH INTRA-CLASS INCOHERENCE",
            "text": "Our purpose is to learn the superior feature containing innovation from an existing model feature. Based on the aforementioned observations and analysises, we propose a novel FR training paradigm incorporating intra-class incoherence.\nSpecifically, we first consider utilizing the knowledge distillation(KD) framework to implement intra-class incoherence. For traditional KD, the teacher network is a model trained with the face recognition algorithm with better performance. The student network is a network that needs to be trained. The teacher network supervises the student network at the feature and logit levels. However, due to the different network structures of the teacher and student networks, the feature dimensions are different, and the supervision on the feature level sometimes is optional.\nNevertheless, different from the traditional knowledge distillation methods, the student network in our framework keeps the same size as the teacher to avoid impairing the ability of recognition. As shown in Fig 3, the two models are the same in architecture, and we expect the to-be-trained model (the student network) to perform better than the pre-trained model (the teacher network). Another noteworthy difference is that the general KD is to learn the relevant knowledge of a teacher network, while our method obtains the irrelevant information of a teacher network with intra-class irrelevant constraints (i.e. feature dissimilarity).\nTherefore, the loss function LS for student network training is composed of the face recognition loss function LFR and the intra-class incoherence cost Ldissim.\nLS = LFR + \u03b3Ldissim, (2)\nwhere \u03b3 is the weight, Ldissim is the cosine similarity of fT and fS , and fT and fS are the feature embeddings of the same face image obtained through the teacher and student networks. The term LFR is to preserve the capability for face recognition and Ldissim is to impose IIC.\nTo introduce innovation of different levels when training, we take such a strategy: in addition to the final feature embeddings of the feature extraction network, the proposed intra-class irrelevance is also applied to different intermediate feature levels. Specifically, we take IR-SE-50 as the feature extraction network for experiments. As shown in Fig. 3, IR-SE-50 also has four blocks. We investigate updating the four blocks\u2019 outputs using intra-class incoherence via\nLS = LFR + \u03b3i.Ldissimi , (i = 1, . . . , N) , (3)\nwhere \u03b3i is the weight, Ldissimi is the cosine similarity between fTi and fSi , fTi and fSi are the features of the same face image obtained through the ith block of the teacher network and student network, N denotes the number of blocks in the feature extraction network.\nWe also consider the case where the intra-class incoherence is applied to the outputs of the four modules and the final output embedding of the feature extraction network in the following hybrid\nway:\nLS = LFR + N+1\u2211 i=1 \u03b3i.Ldissimi , (4)\nwhere N + 1 denotes the number of blocks and the output layer in the feature extraction network. By controlling whether \u03b3i is zero, we can examine the effect of intra-class incoherence constraints over an arbitrary combination of different output layers on FR accuracy. See Section 4.3 for detailed ablation studies and hyper-parameter preference."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 DATASETS AND IMPLEMENTATION DETAILS",
            "text": "We use CASIA (Yi et al., 2014) and MS1M-ArcFace (also known as MS1MV2) (Guo et al., 2016) as the training datasets. 7 common benchmarks including LFW (Zhang & Deng, 2016), CFP-FP (Sengupta et al., 2016), CPLFW(Zhang & Deng, 2016), AgeDB (Moschoglou et al., 2017), CALFW (Zhang & Deng, 2016), Vggface2 (Cao et al., 2018), and IJB-C (Maze et al., 2018) are used to evaluate the performance of different algorithms following the standard evaluation protocols. For these test sets, the images show variations in lighting, pose, or age.\nWe train the model with ResNet50 and ResNet100 (He et al., 2016) as the backbone and batch size of 512 using the metric and loss functions similar to the specified definition in their original text. The head of the baseline model is: BackBone-Flatten-FC-BN with embedding dimensions of 512 and the dropout probability of 0.4 to output the embedding feature. Unless specified otherwise, models are trained for 50 epochs using the SGD optimizer with a momentum of 0.9, and a weight decay of 0.0001. The model is trained with SGD with an initial learning rate of 0.1 and step scheduling at 10, 20, 30 and 40 epochs. For the scale parameters, we set it to 64, following the suggestion of Wang et al. (2018b)."
        },
        {
            "heading": "4.2 COMPARISON WITH SOTA METHODS AND ANALYSIS",
            "text": "In this experiment, we adopt some common baseline models such as ArcFace (Deng et al., 2019) and CosFace (Wang et al., 2018b). Besides, some recent state-of-the-art methods such as MagFace (Meng et al., 2021) and AdaFace (Kim et al., 2022) also serve as the baseline models for comparison, and this paper adopts the training paradigm with IIC in a manner consistent with the original literature. Since the proposed method is based on a pre-trained model, the parameters of the teacher network are downloaded from their official offered checkpoints if available.\nFirstly, we train a relatively small network with ResNet50 (He et al., 2016) as the backbone on CASIA (Yi et al., 2014). The experimental results are shown in Table 3.\nWe observe that IIC proposed in this paper has significant improvements on all face recognition algorithms. Even the recognition accuracy of the SoTA method AdaFace is also improved. By comparing the results of Table 2 and Table 3 (results of ArcFace), it is evident that the improvement in recognition accuracy achieved by IIC surpasses that listed in Table 2. Then we are curious about\nthe performance of IIC on some large-scale datasets and more complex networks. Therefore, we train a ResNet100 network as the backbone using MS1MV2 as the training dataset and show the results in Table 4.\nTo provide a comprehensive evaluation, we present the performance of state-of-the-art (SoTA) methods on the IJB-C benchmark in Table 5. In this setting, we utilize the MS1MV2 dataset for training. From the results in Table 5, it is evident that our method outperforms the baseline models in most cases, particularly when a low false positive rate (FPR) is required. Notably, when the FPR is set 1e-6, the models trained with the intra-class incoherence constraint (IIC) significantly outperform all baseline methods.\nHowever, comparing the results in Tab 4 and Tab 3, we find that IIC plays a more effective role in a relatively smaller dataset. This observation motivates us to investigate the underlying reasons. Through careful analysis of the experimental results, we associate it with the technique of data augmentation. Data augmentation is a technique of artificially increasing the training set by creating modified copies of a dataset using existing data. It introduces perturbations to the data, making it challenging for the model to simply recognize them. Although data augmentation may seemingly increase the training difficulty, it actually enables the model to better learn the underlying feature representations. Furthermore, data augmentation has been shown to effectively enhance the performance of models when dealing with limited data. These explanations can also be applied to IIC. Our proposed method encourages the FR model to explore a different feature space and learn a better feature representation. Therefore, we hypothesize that IIC serves as a form of \u201dfeature augmentation\u201d. To validate this hypothesis, we conduct an ablation study in Sec 4.3."
        },
        {
            "heading": "4.3 ABLATION STUDY AND HYPER-PARAMETER PREFERENCE",
            "text": "In this section, if not specified, we use the CASIA dataset to train the model in which ArcFace serves as the basic face recognition algorithm.\nFirstly, we investigate the impact of different weights on the performance of Eq. 2. The detailed results are shown in Appendix Table 6. We find that the average precision on all test sets is improved. For different weights in Table 6, the performance gap is marginal. For convenience, the previous experiments also have a similar treatment. In the absence of special instructions, the default weight is 1.0.\nBesides, we examine the impact of the different initialization methods for student networks on recognition accuracy, namely, random initialization and initialization with the teacher network\u2019s weight. We provide the results in Appendix Table 7. It is obvious that the recognition accuracy of initialization with the teacher network\u2019s weight is the best. Unless otherwise specified, the weight of the teacher network is used for initialization by default. We also investigate the case where the student network has no intra-class irrelevant constraints, that is, only the weight of the teacher network is loaded for initialization and the model is trained solely by FR loss. It can be seen from Table 7 that the absence of intra-class irrelevant constraints does not improve the performance, but the performance will decline slightly. We suppose it is because that the data are reused and the model training may be overfitting.\nThen, we investigate the constraint of intra-class irrelevant on the outputs of four blocks, the output of the last layer of the feature extraction network, and all five outputs, respectively. The results are shown in Appendix Table 8. We find that the average accuracy is improved by adding the IIC to the output of each module. However, when all the features are imposed on IIC at the same time, the performance is degraded. The possible reason is that imposing too many restrictions would affect the capability of recognition. Since the performance of applying IIC to the outputs of the four modules and the last layer has little difference, we prefer a more convenient way. If not specified, the following experiment applies IIC to the last layer output of the feature extraction network.\nTo further verify the accuracy gained brought by innovation, we also perform orthogonal decomposition on the features learned from the model with IIC to investigate the ability of face recognition of innovation. We provide the experimental results in Appendix Table 9. It shows that the learned innovation does have certain facial feature representation abilities. Combined with Tab 3, we can infer that the feature representation space obtained by IIC plays a role in performance improvement.\nFinally, we validate the supposition mentioned in the Section 4.2. We use a small portion (i.e. 1/10) of MS1MV2 to train a ResNet50 as the backbone, with AdaFace serving as the basic face recognition algorithm. We provide the results in Appendix Table 10. It confirms our hypothesis that our proposed method can be regarded as a form of \u201dfeature augmentation\u201d."
        },
        {
            "heading": "5 LIMITATION",
            "text": "The method proposed in this paper is to find a more expressive feature representation space based on a pre-trained model. However, the method is currently applicable to face recognition tasks, and we think it has the potential to be extended to more general tasks and more general datasets. Though the improvement of recognition accuracy by IIC is greater than that listed in Table 2, it is not ruled out that a better feature expression space can be found by a suitable optimization algorithm."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "Generally, traditional face recognition algorithms are to enhance FR performance by increasing intra-class correlation and inter-class incoherence. This paper does the opposite, and the proposed method further improves the accuracy of the face recognition algorithm by adding intra-class incoherence. Firstly, this paper analyzes the reasons for the improvement of face recognition accuracy from the feature representation level for different face recognition algorithms. Based on detailed analysis, we adopt the knowledge distillation framework to introduce innovation by adding intraclass irrelevant constraints (IIC), attempting to obtain a more representative feature space. Experiments show that the existing state-of-the-art method with IIC can be further improved, highlighting its potential to further enhance FR performance."
        }
    ],
    "title": "INTRA-CLASS INCOHERENCE",
    "year": 2024
}