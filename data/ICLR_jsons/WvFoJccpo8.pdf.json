{
    "abstractText": "Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced numbers of parameters for quantization and adaptation, and the solution is to use group-wise operators to increase the number of parameters for quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM\u2019s weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model families and validate its effectiveness in different fine-tuning datasets and downstream scenarios. The code is made available at https://github.com/yuhuixu1993/qa-lora.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuhui Xu"
        },
        {
            "affiliations": [],
            "name": "Lingxi Xie"
        },
        {
            "affiliations": [],
            "name": "Xiaotao Gu"
        },
        {
            "affiliations": [],
            "name": "Xin Chen"
        },
        {
            "affiliations": [],
            "name": "Heng Chang"
        },
        {
            "affiliations": [],
            "name": "Hengheng Zhang"
        },
        {
            "affiliations": [],
            "name": "Zhensu Chen"
        },
        {
            "affiliations": [],
            "name": "Xiaopeng Zhang"
        },
        {
            "affiliations": [],
            "name": "Qi Tian"
        }
    ],
    "id": "SP:f5fb4c772f98e2c4846e549c4b6a6491c7f3cc61",
    "references": [
        {
            "authors": [
                "Yonatan Bisk",
                "Rowan Zellers",
                "Ronan Le Bras",
                "Jianfeng Gao",
                "Yejin Choi"
            ],
            "title": "Piqa: Reasoning about physical commonsense in natural language",
            "venue": "In Thirty-Fourth AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg",
                "Harsha Nori",
                "Hamid Palangi",
                "Marco Tulio Ribeiro",
                "Yi Zhang"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "venue": "arXiv preprint arXiv:2303.12712,",
            "year": 2023
        },
        {
            "authors": [
                "Yupeng Chang",
                "Xu Wang",
                "Jindong Wang",
                "Yuan Wu",
                "Kaijie Zhu",
                "Hao Chen",
                "Linyi Yang",
                "Xiaoyuan Yi",
                "Cunxiang Wang",
                "Yidong Wang"
            ],
            "title": "A survey on evaluation of large language models",
            "venue": "arXiv preprint arXiv:2307.03109,",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Christopher Clark",
                "Kenton Lee",
                "Ming-Wei Chang",
                "Tom Kwiatkowski",
                "Michael Collins",
                "Kristina Toutanova"
            ],
            "title": "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
            "year": 1905
        },
        {
            "authors": [
                "Peter Clark",
                "Isaac Cowhey",
                "Oren Etzioni",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Carissa Schoenick",
                "Oyvind Tafjord"
            ],
            "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge",
            "venue": "arXiv preprint arXiv:1803.05457,",
            "year": 2018
        },
        {
            "authors": [
                "Tim Dettmers",
                "Mike Lewis",
                "Younes Belkada",
                "Luke Zettlemoyer"
            ],
            "title": "LLM.int8(): 8-bit matrix multiplication for transformers at scale",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tim Dettmers",
                "Artidoro Pagnoni",
                "Ari Holtzman",
                "Luke Zettlemoyer"
            ],
            "title": "Qlora: Efficient finetuning of quantized llms",
            "venue": "arXiv preprint arXiv:2305.14314,",
            "year": 2023
        },
        {
            "authors": [
                "Tim Dettmers",
                "Ruslan Svirschevski",
                "Vage Egiazarian",
                "Denis Kuznedelev",
                "Elias Frantar",
                "Saleh Ashkboos",
                "Alexander Borzunov",
                "Torsten Hoefler",
                "Dan Alistarh"
            ],
            "title": "Spqr: A sparse-quantized representation for near-lossless llm weight compression",
            "venue": "arXiv preprint arXiv:2306.03078,",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "In North American Chapter of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Steven K Esser",
                "Jeffrey L McKinstry",
                "Deepika Bablani",
                "Rathinakumar Appuswamy",
                "Dharmendra S Modha"
            ],
            "title": "Learned step size quantization",
            "year": 1902
        },
        {
            "authors": [
                "Elias Frantar",
                "Dan Alistarh"
            ],
            "title": "Sparsegpt: Massive language models can be accurately pruned in one-shot",
            "venue": "International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Elias Frantar",
                "Saleh Ashkboos",
                "Torsten Hoefler",
                "Dan Alistarh"
            ],
            "title": "GPTQ: Accurate post-training compression for generative pretrained transformers",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Leo Gao",
                "Jonathan Tow",
                "Stella Biderman",
                "Sid Black",
                "Anthony DiPofi",
                "Charles Foster",
                "Laurence Golding",
                "Jeffrey Hsu",
                "Kyle McDonell",
                "Niklas Muennighoff"
            ],
            "title": "A framework for few-shot language model evaluation",
            "venue": "Version v0",
            "year": 2021
        },
        {
            "authors": [
                "Muhammad Usman Hadi",
                "Rizwan Qureshi",
                "Abbas Shah",
                "Muhammad Irfan",
                "Anas Zafar",
                "Muhammad Bilal Shaikh",
                "Naveed Akhtar",
                "Jia Wu",
                "Seyedali Mirjalili"
            ],
            "title": "Large language models: A comprehensive survey of its applications, challenges, limitations, and future prospects",
            "venue": "TechRxiv,",
            "year": 2023
        },
        {
            "authors": [
                "Junxian He",
                "Chunting Zhou",
                "Xuezhe Ma",
                "Taylor Berg-Kirkpatrick",
                "Graham Neubig"
            ],
            "title": "Towards a unified view of parameter-efficient transfer learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Steven Basart",
                "Andy Zou",
                "Mantas Mazeika",
                "Dawn Song",
                "Jacob Steinhardt"
            ],
            "title": "Measuring massive multitask language understanding",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Edward J. Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Zhiqiang Hu",
                "Yihuai Lan",
                "Lei Wang",
                "Wanyu Xu",
                "Ee-Peng Lim",
                "Roy Ka-Wei Lee",
                "Lidong Bing",
                "Soujanya Poria"
            ],
            "title": "Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models",
            "year": 1933
        },
        {
            "authors": [
                "Jeonghoon Kim",
                "Jung Hyun Lee",
                "Sungdong Kim",
                "Joonsuk Park",
                "Kang Min Yoo",
                "Se Jung Kwon",
                "Dongsoo Lee"
            ],
            "title": "Memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization",
            "venue": "arXiv preprint arXiv:2305.14152,",
            "year": 2023
        },
        {
            "authors": [
                "Abdullatif K\u00f6ksal",
                "Timo Schick",
                "Anna Korhonen",
                "Hinrich Sch\u00fctze"
            ],
            "title": "Longform: Optimizing instruction tuning for long text generation with corpus extraction",
            "venue": "arXiv preprint arXiv:2304.08460,",
            "year": 2023
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang"
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "arXiv preprint arXiv:2101.00190,",
            "year": 2021
        },
        {
            "authors": [
                "Ji Lin",
                "Jiaming Tang",
                "Haotian Tang",
                "Shang Yang",
                "Xingyu Dang",
                "Song Han"
            ],
            "title": "Awq: Activation-aware weight quantization for llm compression and acceleration",
            "venue": "arXiv preprint arXiv:2306.00978,",
            "year": 2023
        },
        {
            "authors": [
                "Zechun Liu",
                "Barlas Oguz",
                "Changsheng Zhao",
                "Ernie Chang",
                "Pierre Stock",
                "Yashar Mehdad",
                "Yangyang Shi",
                "Raghuraman Krishnamoorthi",
                "Vikas Chandra"
            ],
            "title": "Llm-qat: Data-free quantization aware training for large language models",
            "venue": "arXiv preprint arXiv:2305.17888,",
            "year": 2023
        },
        {
            "authors": [
                "Shayne Longpre",
                "Le Hou",
                "Tu Vu",
                "Albert Webson",
                "Hyung Won Chung",
                "Yi Tay",
                "Denny Zhou",
                "Quoc V Le",
                "Barret Zoph",
                "Jason Wei"
            ],
            "title": "The flan collection: Designing data and methods for effective instruction tuning",
            "venue": "arXiv preprint arXiv:2301.13688,",
            "year": 2023
        },
        {
            "authors": [
                "Xinyin Ma",
                "Gongfan Fang",
                "Xinchao Wang"
            ],
            "title": "Llm-pruner: On the structural pruning of large language models",
            "venue": "arXiv preprint arXiv:2305.11627,",
            "year": 2023
        },
        {
            "authors": [
                "Sourab Mangrulkar",
                "Sylvain Gugger",
                "Lysandre Debut",
                "Younes Belkada",
                "Sayak Paul"
            ],
            "title": "Peft: State-of-the-art parameter-efficient fine-tuning methods",
            "venue": "https://github.com/ huggingface/peft,",
            "year": 2022
        },
        {
            "authors": [
                "Todor Mihaylov",
                "Peter Clark",
                "Tushar Khot",
                "Ashish Sabharwal"
            ],
            "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
            "year": 2018
        },
        {
            "authors": [
                "Gunho Park",
                "Baeseong Park",
                "Se Jung Kwon",
                "Byeongwook Kim",
                "Youngjoo Lee",
                "Dongsoo Lee"
            ],
            "title": "nuqmm: Quantized matmul for efficient inference of large-scale generative language models",
            "venue": "arXiv preprint arXiv:2206.09557,",
            "year": 2022
        },
        {
            "authors": [
                "Keisuke Sakaguchi",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi"
            ],
            "title": "Winogrande: An adversarial winograd schema challenge at scale",
            "year": 1907
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "venue": "arXiv preprint arXiv:2211.05100,",
            "year": 2022
        },
        {
            "authors": [
                "Sheng Shen",
                "Zhen Dong",
                "Jiayu Ye",
                "Linjian Ma",
                "Zhewei Yao",
                "Amir Gholami",
                "Michael W Mahoney",
                "Kurt Keutzer"
            ],
            "title": "Q-bert: Hessian based ultra low precision quantization of bert",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Mingjie Sun",
                "Zhuang Liu",
                "Anna Bair",
                "J Zico Kolter"
            ],
            "title": "A simple and effective pruning approach for large language models",
            "venue": "arXiv preprint arXiv:2306.11695,",
            "year": 2023
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto"
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https://github.com/tatsu-lab/stanford_alpaca,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "arXiv preprint arXiv:2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Mojtaba Valipour",
                "Mehdi Rezagholizadeh",
                "Ivan Kobyzev",
                "Ali Ghodsi"
            ],
            "title": "Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation",
            "venue": "arXiv preprint arXiv:2210.07558,",
            "year": 2022
        },
        {
            "authors": [
                "Qichao Wang",
                "Tian Bian",
                "Yian Yin",
                "Tingyang Xu",
                "Hong Cheng",
                "Helen M Meng",
                "Zibin Zheng",
                "Liang Chen",
                "Bingzhe Wu"
            ],
            "title": "Language agents for detecting implicit stereotypes in text-to-image models at scale",
            "venue": "arXiv preprint arXiv:2310.11778,",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560,",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Yi Tay",
                "Rishi Bommasani",
                "Colin Raffel",
                "Barret Zoph",
                "Sebastian Borgeaud",
                "Dani Yogatama",
                "Maarten Bosma",
                "Denny Zhou",
                "Donald Metzler",
                "Ed H. Chi",
                "Tatsunori Hashimoto",
                "Oriol Vinyals",
                "Percy Liang",
                "Jeff Dean",
                "William Fedus"
            ],
            "title": "Emergent abilities of large language models",
            "venue": "In Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Xiuying Wei",
                "Yunchen Zhang",
                "Xiangguo Zhang",
                "Ruihao Gong",
                "Shanghang Zhang",
                "Qi Zhang",
                "Fengwei Yu",
                "Xianglong Liu"
            ],
            "title": "Outlier suppression: Pushing the limit of low-bit transformer language models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Lilian Weng"
            ],
            "title": "Large transformer model inference optimization. Lil\u2019Log, Jan 2023",
            "venue": "URL https: //lilianweng.github.io/posts/2023-01-10-inference-optimization/",
            "year": 2023
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Tim Dettmers",
                "Luke Zettlemoyer",
                "Ari Morcos",
                "Ali Farhadi",
                "Ludwig Schmidt"
            ],
            "title": "Stable and low-precision training for large-scale vision-language models",
            "venue": "arXiv preprint arXiv:2304.13013,",
            "year": 2023
        },
        {
            "authors": [
                "Xiaoxia Wu",
                "Zhewei Yao",
                "Yuxiong He"
            ],
            "title": "Zeroquant-fp: A leap forward in llms post-training w4a8 quantization using floating-point formats",
            "venue": "arXiv preprint arXiv:2307.09782,",
            "year": 2023
        },
        {
            "authors": [
                "Guangxuan Xiao",
                "Ji Lin",
                "Mickael Seznec",
                "Hao Wu",
                "Julien Demouth",
                "Song Han"
            ],
            "title": "Smoothquant: Accurate and efficient post-training quantization for large language models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Zhewei Yao",
                "Reza Yazdani Aminabadi",
                "Minjia Zhang",
                "Xiaoxia Wu",
                "Conglong Li",
                "Yuxiong He"
            ],
            "title": "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Junchi Yu",
                "Ran He",
                "Zhitao Ying"
            ],
            "title": "Thought propagation: An analogical approach to complex reasoning with large language models",
            "venue": "In The Twelfth International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ari Holtzman",
                "Yonatan Bisk",
                "Ali Farhadi",
                "Yejin Choi"
            ],
            "title": "Hellaswag: Can a machine really finish your sentence? CoRR, abs/1905.07830, 2019",
            "venue": "URL http://arxiv.org/ abs/1905.07830",
            "year": 1905
        },
        {
            "authors": [
                "Aohan Zeng",
                "Xiao Liu",
                "Zhengxiao Du",
                "Zihan Wang",
                "Hanyu Lai",
                "Ming Ding",
                "Zhuoyi Yang",
                "Yifan Xu",
                "Wendi Zheng",
                "Xiao Xia"
            ],
            "title": "Glm-130b: An open bilingual pre-trained model",
            "venue": "International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin",
                "Todor Mihaylov",
                "Myle Ott",
                "Sam Shleifer",
                "Kurt Shuster",
                "Daniel Simig",
                "Punit Singh Koura",
                "Anjali Sridhar",
                "Tianlu Wang",
                "Luke Zettlemoyer"
            ],
            "title": "Opt: Open pre-trained transformer language models, 2022",
            "venue": "URL https://arxiv. org/abs/2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Wayne Xin Zhao",
                "Kun Zhou",
                "Junyi Li",
                "Tianyi Tang",
                "Xiaolei Wang",
                "Yupeng Hou",
                "Yingqian Min",
                "Beichen Zhang",
                "Junjie Zhang",
                "Zican Dong",
                "Yifan Du",
                "Chen Yang",
                "Yushuo Chen",
                "Zhipeng Chen",
                "Jinhao Jiang",
                "Ruiyang Ren",
                "Yifan Li",
                "Xinyu Tang",
                "Zikang Liu",
                "Peiyu Liu",
                "Jian-Yun Nie",
                "JiRong Wen"
            ],
            "title": "A survey of large language models",
            "venue": "arXiv preprint arXiv:2303.18223,",
            "year": 2023
        },
        {
            "authors": [
                "Wayne Xin Zhao",
                "Kun Zhou",
                "Junyi Li",
                "Tianyi Tang",
                "Xiaolei Wang",
                "Yupeng Hou",
                "Yingqian Min",
                "Beichen Zhang",
                "Junjie Zhang",
                "Zican Dong"
            ],
            "title": "A survey of large language models",
            "venue": "arXiv preprint arXiv:2303.18223,",
            "year": 2023
        },
        {
            "authors": [
                "Aojun Zhou",
                "Ke Wang",
                "Zimu Lu",
                "Weikang Shi",
                "Sichun Luo",
                "Zipeng Qin",
                "Shaoqing Lu",
                "Anya Jia",
                "Linqi Song",
                "Mingjie Zhan"
            ],
            "title": "Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification",
            "venue": "arXiv preprint arXiv:2308.07921,",
            "year": 2023
        },
        {
            "authors": [
                "Xunyu Zhu",
                "Jian Li",
                "Yong Liu",
                "Can Ma",
                "Weiping Wang"
            ],
            "title": "A survey on model compression for large language models",
            "venue": "arXiv preprint arXiv:2308.07633,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Recently, large language models (LLMs) (Brown et al., 2020; Scao et al., 2022; Zhang et al., 2022; Touvron et al., 2023a; Chowdhery et al., 2022; OpenAI, 2023; Zeng et al., 2023) have shown unprecedented performance across a wide range of language understanding tasks (Wei et al., 2022a) and served as the foundation of state-of-the-art chat systems (Bubeck et al., 2023). The diversity of real-world applications calls for a pipeline in which LLMs can be fine-tuned to fit different scenarios and quantized to be deployed onto edge devices (e.g., mobile phones), and the key issue is to get rid of the heavy computational burden brought by the large number of parameters of LLMs.\nThere are two lines of research for this purpose. The first one is parameter-efficient fine-tuning (PEFT) (Houlsby et al., 2019; Li & Liang, 2021; Liu et al., 2021; He et al., 2022; Hu et al., 2021) which introduced a small number of learnable parameters while keeping most pre-trained parameters unchanged. Among them, low-rank adaptation (LoRA) (Hu et al., 2021), a popular PEFT algorithm, proposed to fine-tune low-rank matrices to complement the pre-trained weights. Despite the comparable performance to full-parameter fine-tuning, the memory usage of LoRA is still large, especially when the base LLM is large (e.g., LLaMA-65B). The second one studies parameter quantization (Yao et al., 2022; Dettmers et al., 2022; Wei et al., 2022b; Frantar et al., 2023; Lin et al., 2023; Xiao et al., 2023; Dettmers et al., 2023b) where the trained weights are quantized into low-bit integers or floating point numbers. Although these methods can alleviate the computational burden, they often report unsatisfying accuracy especially when the quantization bit width is low.\nHence, it is an important topic to integrate PEFT with quantization. A naive solution is to perform post-training quantization (PTQ) after PEFT, but it reports unsatisfying accuracy especially when the quantization bit width is low. Advanced methods exist, but they are either computationally expensive in the fine-tuning stage (Liu et al., 2023) or unable to maintain the quantized property after fine-tuning (Dettmers et al., 2023a). In this paper, we propose a simple yet effective method for quantization-aware low-rank adaptation (QA-LoRA). Our idea is based on the imbalanced numbers\nof parameters for quantization and adaptation. Specifically, each column of the pre-trained weight matrix is accompanied by only one pair of scaling and zero parameters but many more LoRA parameters. This imbalance not only results in large quantization errors (which harm the LLM\u2019s accuracy), but also makes it difficult to integrate the auxiliary weights into the main model. QA-LoRA addresses the issue by introducing group-wise operators to increase the number of parameters for low-bit quantization (each group is quantized individually) and decrease that of LoRA (each group shares the adaptation parameters). QA-LoRA enjoys two-fold benefits: (i) an efficient fine-tuning stage thanks to the LLM\u2019s weights being quantized into low-bit integers; (ii) a lightweight, finetuned model without the need for PTQ which often incurs loss of accuracy.\nWe evaluate QA-LoRA on the LLaMA and LLAMA2 model families (Touvron et al., 2023a;b) and validate it on various language understanding benchmarks. Figure 1 shows the comparison of 5-shot accuracy on the MMLU benchmark between QA-LoRA and the direct baseline, QLoRA (Dettmers et al., 2023a). QA-LoRA consistently outperforms QLoRA with PTQ on top of LLMs of different scales (the advantage becomes more significant when the quantization bit width is lower) and is on par with QLoRA without PTQ. Note that during inference, QA-LoRA has exactly the same complexity as QLoRA with PTQ and is much more efficient than QLoRA without PTQ. Hence, QA-LoRA is an effective and off-the-shelf method for joint quantization and adaptation of LLMs."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Large language models (LLMs) (Devlin et al., 2019; Brown et al., 2020; Zhao et al., 2023a; Hadi et al., 2023; Yu et al., 2023) have emerged as a dominant paradigm in natural language processing. It achieved the state-of-the-art on various tasks (Zhao et al., 2023b; Zhou et al., 2023; Wang et al., 2023) and served as the fundamental of chat systems (OpenAI, 2023). However, their deployment in real-world scenarios is hindered by the high computational and memory requirements during inference (Chang et al., 2023). To tackle this issue, various methods have been proposed, including distillation (Liu et al., 2023), quantization (Yao et al., 2022; Dettmers et al., 2022; Wei et al., 2022b; Frantar et al., 2023; Lin et al., 2023; Xiao et al., 2023), pruning (Frantar & Alistarh, 2023; Ma et al., 2023; Sun et al., 2023), etc. (Weng, 2023). This paper mainly focuses on the quantization of LLMs.\nFine-tuning LLMs with adapters. Parameter efficient fine-tuning (PEFT) is an important topic for LLMs. One of the most popular approaches is low-rank adaptation (LoRA) (Hu et al., 2021; Valipour et al., 2022), where the key insight is to decompose the adapter weights into the multiplication of two low-rank (and thus parameter-efficient) matrices. LoRA has claimed comparable performance to full fine-tuning while using much fewer learnable parameters. Meanwhile, there are also other branches of adapters for LLMs such as the series adapter (Houlsby et al., 2019) and parallel adapter (He et al., 2022). Please refer to (Mangrulkar et al., 2022; Hu et al., 2023) for a review of these adapters.\nQuantization of LLMs. Quantization is a compression technique that reduces the bit width of the parameters and/or activations of LLMs to improve their efficiency and scalability (Xiao et al., 2023; Dettmers et al., 2022; 2023a). Existing methods mostly focused on preserving or restoring the accuracy of quantized LLMs during the inference stage (Zhu et al., 2023), where the key is to\nreduce the memory footprint and computational costs without re-training the LLMs. One of the main challenges is to handle the outliers in the parameter distribution (Xiao et al., 2023), which can cause significant errors when quantized. To address this issue, some methods proposed to use either adaptive or dynamic quantization schemes that adjust the quantization range or precision according to the parameters (Xiao et al., 2023; Dettmers et al., 2022). Other methods used sophisticated grouping or clustering techniques to partition the parameters into different groups and applied different quantization strategies for each group (Park et al., 2022; Yao et al., 2022; Wu et al., 2023).\nJoint adaptation and quantization. This paper aims to achieve the objectives of both parameterefficient adaptation and computation-efficient tuning and deployment, which can further improve the efficiency and scalability of LLMs as well as mitigate the negative impact of quantization errors. However, this also poses additional challenges, such as propagating gradients through discrete values and optimizing the quantization parameters. To overcome these challenges, lossy quantization methods proposed to use stochastic rounding (Shen et al., 2020) or learned rounding (Esser et al., 2019) to approximate the gradients and update the parameters, but applying these methods to LLMs is often difficult. Other methods proposed to use switchback layers (Wortsman et al., 2023) or mixed-precision inference (Dettmers et al., 2023a) to alternate between quantized and full/halfprecision values, which often result in low inference speed.\nTo the best of our knowledge, the most related work is QLoRA (Dettmers et al., 2023a) which squeezed the pre-trained weights into NF4 and added LoRA. However, QLoRA added the adaption weights back to pre-trained weights and turned them into FP16 again, and thus the deployed model is still slow. We solve this problem with the proposed QA-LoRA approach."
        },
        {
            "heading": "3 THE PROPOSED APPROACH",
            "text": ""
        },
        {
            "heading": "3.1 BASELINE: LOW-RANK ADAPTATION AND LOW-BIT QUANTIZATION",
            "text": "We follow the notation system used in LoRA (Hu et al., 2021) which assumed pre-trained weights to form a matrix W and the features form a vector x. The definition is easily applied to a wide range of scenarios and extended into x is a set of vectors (e.g., a feature matrix). Let the size of W be Din \u00d7 Dout and x has the length of Din, and thus the computation is easily written as y = W\u22a4x where y is the output vector with a length of Dout.\nThe key idea of LoRA is to introduce a pair of matrices, A and B, to supplement W. A and B have sizes of Din \u00d7 Dint and Dint \u00d7 Dout, respectively, so that their multiplication, AB, has the same size as W. The intermediate dimensionality is often small (i.e., Dint \u226a min{Din, Dout}), making AB a low-rank matrix compared to W. During fine-tuning, we compute y = W\u22a4x+ s \u00b7 (AB)\u22a4x, where s is the coefficient for weight tuning, and W is fixed while A and B can be adjusted, arriving at the goal of parameter-efficient fine-tuning. After fine-tuning, the computation is reformulated into y = (W + s \u00b7AB)\u22a4x, where W is replaced by W\u2032 = W + s \u00b7AB for fast inference. Another effective way to reduce computational costs lies in low-bit quantization. We only consider the quantization of weights throughout this paper. In particular, we apply a simple method named min-max quantization. Mathematically, given the bit width N and a pre-trained weight matrix W, we compute the minimum and maximum values across all elements of W, denoted as min(W) and max(W), respectively. Then, W is quantized into W\u0303 by computing\nW\u0303 = \u03b1 \u00b7 W\u0302 + \u03b2 .= \u03b1 \u00b7 \u230a W \u2212 \u03b2\n\u03b1\n\u2309 + \u03b2, (1)\nwhere \u03b1 = (max(W) \u2212 min(W))/(2N \u2212 1) and \u03b2 = min(W) are called the scaling and zero factors, respectively; \u230a\u00b7\u2309 denotes the integer rounding operation. All elements in W\u0302 are in the set of {0, 1, . . . , 2N \u22121} and thus stored as B-bit integers. The computation, y = W\u22a4x, is approximated\nas y = W\u0303\u22a4x = \u03b1\u00b7 \u230a W\u2212\u03b2\n\u03b1\n\u2309\u22a4 x+\u03b2x. The quantization brings two-fold benefits, namely, the storage\nof W is reduced (e.g., from FP16 to INT4) and the computation of W\u22a4x becomes faster. The cost is that W\u0303 is an approximation of W, which may harm the accuracy of language understanding.\nTo reduce the quantization loss between W and W\u0303, an effective strategy is to perform an individual quantization for each column of W. Let W = [wi,j ]Din\u00d7Dout , where i \u2208 {1, . . . , Din} and j \u2208\n{1, . . . , Dout} are iterative variables. Let \u03b1j and \u03b2j be the scaling and zero factors computed on the j-th column, wj . Hence, Equation 1 is updated as W\u0303 = [w\u0303j ]Dout = [ \u03b1j \u00b7 \u230a wj\u2212\u03b2j\n\u03b1j\n\u2309 + \u03b2j ] Dout ,\nand the computation is rewritten as y = W\u0303\u22a4x = [ \u03b1j \u00b7 \u230a wj\u2212\u03b2j\n\u03b1j\n\u2309\u22a4 x+ \u03b2jx ] Dout . Compared to\nthe original (holistic) quantization, the computational cost is unchanged while the storage cost of the scaling and zero factors increases from 2 to 2Dout floating point numbers. This is negligible compared to the reduced cost of storing the full-precision W."
        },
        {
            "heading": "3.2 OBJECTIVE: EFFICIENT ADAPTATION AND DEPLOYMENT",
            "text": "As shown in Figure 2, we aim to achieve two goals. First, during the fine-tuning stage, the pretrained weights W are quantized into low-bit representation so that LLMs can be fine-tuned on as few GPUs as possible. Second, after the fine-tuning stage, the fine-tuned and merged weights W\u2032 are still in a quantized form so that LLMs can be deployed with computational efficiency.\nWe note that QLoRA (Dettmers et al., 2023a), a recently proposed variant of LoRA, achieved the first goal. The idea is to quantize W from FP16 to NF4 (a highly squeezed type of floating point numbers) during the fine-tuning stage. We learn from QLoRA that joint optimization of quantization and adaptation is tractable because the accuracy loss between W and W\u0303 is compensated by the lowrank weights, s \u00b7AB. After fine-tuning, the side weights s \u00b7AB must be added back to W\u0303, making the final weights W\u2032 in FP16 again. Indeed, one can perform post-training quantization (PTQ) upon W\u2032, but this strategy can cause a significant loss in accuracy especially when the bit width is low. Please refer to the experiments for details. Additionally, there is no operator-level optimization for NF4 yet, making it difficult to accelerate the fine-tuning and inference stages. In brief, the only benefit brought by QLoRA is the reduced memory cost for fine-tuning."
        },
        {
            "heading": "3.3 SOLUTION: GROUP-WISE QUANTIZATION WITH LOW-RANK ADAPTATION",
            "text": "From the above analysis, the key to achieving the second goal lies in that W\u0303 (i.e., the quantized W) and s \u00b7AB can be merged without using high-precision numbers (e.g., FP16). We first note that this is impossible in the original setting, i.e., W is quantized into W\u0303 in a column-wise manner while both A and B are unconstrained.\nWe write down the condition using the mathematical language. Since W\u2032 = W\u0303 + s \u00b7 AB, we have w\u2032i,j = w\u0303i,j + s \u00b7 \u2211 k ai,kbk,j for all (i, j). Here, for any j, all w\u0303i,j are represented using the same set of scaling and zero factors, i.e., there exist \u03b1j and \u03b2j so that w\u0303i,j = \u03b1j \u00d7 w\u0302i,j + \u03b2j , w\u0302i,j \u2208 {0, 1, . . . , 2N\u22121}. After each w\u0303i,j is added by s\u00b7 \u2211 k ai,kbk,j (abbreviated as ci,j), if we want to keep the property for quantization, we must guarantee that for any j, all possible values of ci,j\nAlgorithm 1 QA-LoRA Pseudocode in the PyTorch-like style\n# D_in, D_out, D_int: the input, output, and low-rank adaptation dimensions # L: the quantization group numbers of weights W (D_in // L is the group size) # s: the coefficient for adaptation; N: the bit width of quantization\nQA = nn.AvgPool1d(D in//L) lora_A = nn.Parameter(torch.empty((D_int, L))) lora_B = nn.Parameter(torch.empty((D_out, D_int)))\ndef qalora_forward(x, W, lora_A, lora_B): W_tilde = pre_quantization(W, alpha, beta) result = x @ W_tilde result += (QA(x)*(D in//L)) @ lora_A.transpose(0,1) @ lora_B.transpose(0,1) * s return result\ndef pre_quantization(W, alpha, beta): W_hat = torch.round(W / alpha) + beta return alpha * (W_hat - beta)\ndef merge_with_quantization(beta, lora_A, lora_B): beta_new = beta - s * (lora_B @ lora_A).transpose(0,1) / alpha return beta_new\nform an arithmetic set with the common difference being \u03b1j1. This is intractable in continuous and gradient-based optimization unless we ask that ci,j is a constant, i.e., c1,j = . . . = ci,j = . . . , cDin,j for any j. This is equivalent to set all row vectors of A to be same, i.e., a1 \u2261 . . . \u2261 ai \u2261 . . . \u2261 aDin , where \u2261 denotes element-wise equivalence between two vectors. The above strategy, while tractable, leads to a significant accuracy drop in practice. In particular, with all rows of A being the same vector, we have rank(A) = 1 and thus rank(AB) = 1, whereas the rank of AB is correlated to the ability of fine-tuning W\u0303 in new data (Hu et al., 2021; Valipour et al., 2022; Dettmers et al., 2023a). To address this issue, a straightforward idea is to relax the constraints for both quantization and adaptation.\nWe partition each column of W into L groups where, for ease of implementation, we set L to be a divisor of Din. Instead of quantizing each column of W entirely, we use an individual pair of scaling and zero factors for quantization, i.e., the l-th group of factors, \u03b1l,j and \u03b2l,j , are computed for Din/L elements in the j-th column. Correspondingly, we only require the row vectors of A within the same group to have the same value. In our implementation, this is achieved by doing summation within each group of the input vector, x. This parameter-free operation reduces the dimension of x from Din to L, hence we can set A to be a L\u00d7Dint matrix without further constraints. The proposed approach is named quantization-aware low-rank adaptation (QA-LoRA). It is implemented by inserting/modifying a few lines of code beyond QLoRA, as shown in Algorithm 12. Compared to LoRA, QA-LoRA enjoys advantages in time and memory consumption. Compared to QLoRA, QA-LoRA requires extra storage for L\u00d7Dout pairs of scaling and zero factors but reduces the number of parameters of A from Din \u00d7 Dint to L \u00d7 Dint \u2013 since we often set L \u226a Din, the above change is negligible. Additionally, QA-LoRA is much faster in inference because it merges s \u00b7AB into W\u0303 while keeping the merged matrix W\u2032 quantized in low bits. The insight of QA-LoRA: balance. QA-LoRA is very similar to a variant of QLoRA in which NF4 quantization is replaced by INT43). In this version, the number of parameters for quantization (Dout pairs of scaling and zero factors) is much smaller than that for adaptation (Din\u00d7Dint+Dint\u00d7 Dout parameters), i.e., there is a significant imbalance here. We introduce group-wise operations, increasing the number of parameters for quantization from Dout to L\u00d7Dout, meanwhile decreasing that for adaptation from Din \u00d7Dint +Dint \u00d7Dout to L\u00d7Dint +Dint \u00d7Dout. As we shall see in experiments, a moderate L can achieve satisfying accuracy of language understanding meanwhile preserving computational efficiency.\n1The exact conditions are two-fold. For any j, there exists a new zero factor \u03b2\u2032j and a set of integers ci,j so that ci,j = \u03b1j \u00d7 c\u0302i,j + \u03b2\u2032j . Additionally, the difference between the minimum and maximum of w\u0302i,j + c\u0302i,j is not greater than 2B \u2212 1 so that the summed weights can still be quantized into B-bit integers.\n2The merge with quantization function is called after the training procedure for merging weights. 3We implemented this version of QLoRA, and it reports very similar (\u00b10.5%) accuracy compared to the\noriginal QLoRA in the few-shot experiments for MMLU."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 SETTINGS",
            "text": "Foundation models. We establish QA-LoRA upon the LLaMA (Touvron et al., 2023a) and LLaMa2 (Touvron et al., 2023b) families. In particular, we fine-tune the 7B, 13B, 33B, and 65B models of LLaMA and the 7B and 13B models of LLaMA2.\nEvaluation metrics. Following QLoRA (Dettmers et al., 2023a), we evaluate both the zeroshot and few-shot performance of the LLMs on Massively Multitask Language Understanding (MMLU) benchmark (Hendrycks et al., 2021). It consists of 57 language tasks including humanities, STEM, social science, etc. We use the official MMLU evaluation script and prompts4. We further assess the zero-shot common sense reasoning ability on tasks covering HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), WinoGrande (Sakaguchi et al., 2019), ARC (Clark et al., 2018), BoolQ (Clark et al., 2019), and OpenBookQA (Mihaylov et al., 2018). We adopt lm-evalharness (Gao et al., 2021) to produce the Common Sense QA results.\nQuantization. We adopt GPTQ (Frantar et al., 2023) in the quantization step, and our approach is open to other PTQ methods such as (Lin et al., 2023; Dettmers et al., 2023b). We use the same settings to quantize the QLoRA fine-tuned models and pre-trained LLaMA models. In the main experiments, we conduct a group-wise asymmetric quantization (with a group size of 32). We set the act-order variable to be false and the true-sequential variable to be true.\nDatasets and training details. We choose Alpaca (Taori et al., 2023) and FLAN v2 (Longpre et al., 2023) as our fine-tuning datasets. Alpaca contains 52K instruction-following data generated from text-davinci-003 (GPT 3.5) (Wang et al., 2022). FLAN v2 is a collection of 1,836 tasks combining the mixture with CoT, Muffin, T0-SF, and NIV2. To save the tuning cost, we randomly sample a 320K subset from the FLAN v2 collection. Following QLoRA (Dettmers et al., 2023a), we use a paged AdamW optimizer, a maximum gradient norm of 0.3, and a batch size of 16 in the tuning period. We choose the constant learning rate schedule and set the learning rate to be 2 \u00d7 10\u22125 for the 7B and 13B models and 1\u00d7 10\u22125 for the 33B and 65B models. The number of fine-tuning steps is 10K for Alpaca and 20K for FLAN v2. All experiments are conducted on Tesla V100 GPUs. We use one GPU for the 7B, 13B, and 33B models and two GPUs for the 65B models."
        },
        {
            "heading": "4.2 MAIN RESULTS AND EFFICIENCY",
            "text": "Comparison against recent competitors on LLaMA for MMLU. We first apply QA-LoRA to fine-tune the LLaMA models for MMLU. Table 1 summarizes the results with respect to different model sizes, fine-tuning datasets, and bit widths. Besides the base LLaMA models, we also compare QA-LoRA against QLoRA (Dettmers et al., 2023a), the most related work, and PEQA (Kim et al., 2023), a recent quantization method that does not use LoRA. We report both the original QLoRA (the inference stage involves FP16 computation) and the variant after GPTQ (for fair comparison). QA-LoRA consistently outperforms both competitors (QLoRA w/ GPTQ and PEQA) in either 0- shot and 5-shot accuracy. The advantage is more significant when the model size is small (e.g., 7B and 13B) or the bit width is small (e.g., INT3 or even INT2 is used), demonstrating that QALoRA is a strong solution in the scenarios that require computational efficiency. In some cases, the INT4 version of QA-LoRA performs even better than the original version of QLoRA meanwhile the inference speed is much faster (see the next paragraph). We further demonstrate some examples of QA-LoRA in Appendix A, where one can see the qualitative comparison and QA-LoRA beyond QLoRA w/ GPTQ. QA-LoRA mainly benefits from the quantization-aware adaptation; otherwise, the post-training quantization will not be compensated, resulting in unstable results.\nApplication to large models. As shown in Table 1, in large models (e.g., 33B and 65B), QALoRA still achieves improvement over the baseline, QLoRA with GPTQ. Upon these results, we discuss the need for QA-LoRA in large models in the following aspects. (1) QA-LoRA reduces the computational costs of fine-tuning large models, e.g., using QA-LoRA, only 1 and 2 V100 GPUs are needed for fine-tuning the 33B and 65B models. (2) When larger models are used, there can be increasing needs for low-bit (e.g., INT3 and INT2) quantization, especially when the large models are to be deployed to edge devices. QA-LoRA shows significant advantages in such scenarios.\n4https://github.com/hendrycks/test\nThe efficiency of QA-LoRA. A clear advantage of QA-LoRA lies in its computational efficiency. Table 2 compares QA-LoRA to QLoRA in terms of the learnable parameters and training time during the fine-tuning stage.\nThe reason behind the fewer amounts of parameters, compared to QLoRA, lies in the reduction of the dimensionality of A. Compared to LoRA and QLoRA where A has Din \u00d7 Dint parameters, QA-LoRA reduces the number to L \u00d7Dint where L is the group size and L \u226a Din. This reduces the number of parameters in QA-LoRA by around 1/2 (originally, A and B have similar numbers of parameters). QA-LoRA achieves higher fine-tuning accuracy with fewer parameters.\nRegarding the time cost of fine-tuning, it is not largely impacted by the parameters because the amount of LoRA parameters is much smaller than that of the LLM itself (e.g., 89M or 160M vs. 7B). To verify this point, we double Dint which also doubles the number of parameters, surpassing that of QLoRA, but QA-LoRA is still much faster than QLoRA (see the table below). The significant advantage of QA-LoRA in training time mainly comes from the use of INT4 quantization. Compared to NF4 quantization used by QLoRA, INT4 operators have been optimized by CUDA and are much faster in execution. Additionally, during the inference stage, QA-LoRA is also more than 50% faster than QLoRA because the fine-tuned model (after weight integration) is still in INT4, unlike QLoRA that converts it back to FP16.\nCommonsense QA results. We also evaluate QA-LoRA for 0-shot commonsense QA based on LLaMA-7B. Results are summarized in Table 3. Similar to the MMLU results, the 4-bit QA-LoRA is comparable with the mixed-precision QLoRA and outperforms the post-quantized QLoRA by an average of 2.0%. The advantage becomes more significant in low-bit scenarios, e.g., the 2-bit QA-LoRA reports a remarkable accuracy gain of 15.0% over the 2-bit post-quantized QLoRA.\nOn LLaMA2 models. We further validate the effectiveness of our method on LLaMA2 (Touvron et al., 2023b). As shown in Table 4, we fine-tune the 7B and 13B models of LLaMA2 and test them on MMLU. Compared to the original FP16 models, the INT4 models fine-tuned with FLAN v2 are consistently better, while those with Alpaca report slightly lower accuracy. These experiments validate that QA-LoRA is generalized to other pre-trained model families."
        },
        {
            "heading": "4.3 ABLATIVE STUDIES",
            "text": "Impact of the quantization group size. We investigate different settings of L, the hyper-parameter that controls the numbers of parameters for both quantization and low-rank adaptation. Results are reported in Table 6 (see Appendix B), where group size (i.e., Din/L is displayed instead of L). Recall that a larger L (corresponding to a smaller group size) implies a larger number of parameters, i.e., a smaller quantization loss, and a larger number of adaptation parameters. Meanwhile, it also requires a larger number of storage and computation, though negligible as long as L \u226b 1. One can observe that a larger L (e.g., group size is 32) often leads to higher accuracy, and the advantage becomes more significant when the quantization bit width is small, implying that a larger quantization loss needs to be compensated by a larger number of parameters.\nImpact of Dint. We diagnose the performance with respect to Dint in Table 7 (see Appendix B). We find that the MMLU accuracy is not largely impacted by the value of Dint unless it is too small.\nImpact of fine-tuning datasets. We also evaluate QA-LoRA on more datasets such as Selfinstruct (Wang et al., 2022), Longform (Ko\u0308ksal et al., 2023), and Chip2 (LAION, 2023). Results are summarized in Table 5. Compared to Alpaca and FLAN v2, these datasets are relatively small, and thus the fine-tuned models report a bit weaker accuracy on MMLU. Note that, with LLaMA-13B as the foundation model, QA-LoRA consistently outperforms QLoRA with mixed precision, meanwhile being much faster in the inference stage.\nImpact of the size of fine-tuning datasets. Lastly, we evaluate QA-LoRA on different subsets of FLAN v2. The dataset size varies from 160K, 240K, 320K, 400K, and 480K. LLaMA-7B is used as the foundation model. As shown in Figure 3, low-bit quantization asks for more data, yet 320K is sufficient for both the INT2 and INT4 variants of QA-LoRA."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we propose QA-LoRA as an efficient method that introduces quantization awareness into the low-rank adaptation of LLMs. At the core of QA-LoRA lies the group-wise operations for both quantization and low-rank adaptation, and the key insight comes from balancing the numbers of parameters of both sides. QA-LoRA is easily implemented, generalized across various foundation models and language understanding tasks, and computationally efficient in both fine-tuning and inference stages. Extensive experiments on LLaMA validate the effectiveness of QA-LoRA.\nETHICS STATEMENT\nThis paper is built upon pre-trained large language models (e.g., LLaMA and LLaMA2) and existing datasets for instruct fine-tuning (e.g., Alpaca and FLAN v2). We do not introduce any new data and thus do not involve human annotation. This paper has no additional ethical concerns beyond a large corpus of research in LLMs."
        },
        {
            "heading": "A QUALITATIVE STUDIES",
            "text": "In this section, we show a few examples of dialog. We compare our method, QA-LoRA, to the direct competitor, QLoRA, under different quantization bit widths. All QLoRA models are postprocessed with GPTQ as described in the main text. We highlight inaccurate answers in blue and totally unacceptable answers in red.\nOverall, QLoRA with GPTQ shows unstable behaviors across different cases. For example, in the second case, it crashes in 4-bit and 2-bit quantization but works well in 3-bit quantization. This mainly owes to the uncontrollable quantization loss in post-processing and such loss cannot be amended by any of the subsequent stages.\nInstruction: Suggest five ways to budget your expenses. QLoRA (4-bit): 1. Write down all of your current expenses. 2. Estimate your future expenses. 3. Cut down on non-essential spending. 4. Rate your expenses according to their importance. 5. Categorize your expenses to make it easier to. Ours (4-bit): 1. Plan a monthly budget and stick to it. 2. Track your spending. 3. Narrow down your wants and priorities. 4. Cut expenses by shopping around. 5. Eat at home as much as possible. \u2014\u2014\u2014\u2014 QLoRA (3-bit): 1. Gather and document your financial information (income, expenses, and assets) 2. Estimate your cash flow (income, expenses, and assets) 3. Allocate your cash flow to the needed/wanted category. 4. Monitor your spending Ours (3-bit): To budget expenses, one should: (1) keep track of spending; (2) have a spending plan; (3) set weekly goals; (4) only use cash; and (5) try to overspend one area to cover another. \u2014\u2014\u2014\u2014 QLoRA (2-bit): the maximum number of times 2026 #03289-012-052112. 03289- 0222500032815-0451 2291-253222. Ours (2-bit): 1. Plan a monthly budget and stick to it. 2. Track your spending. 3. Narrow down your wants and priorities. 4. Cut expenses by shopping around. 5. Eat at home as much as possible.\nInstruction: How do you respond to a negative comment on social media? QLoRA (4-bit): I\u2019m sorry you feel that way. Ours (4-bit): Negative comments on social media can be hard to respond to, but I would first try to understand where the person is coming from and why they are saying what they are saying. I would then strive to reply in a way that does not further escalate the discussion, but instead responds with a mature. \u2014\u2014\u2014\u2014 QLoRA (3-bit): I would first thank the person for taking the time to share their thoughts. I would then say that I value their opinion and that I am here to listen and learn. I would then go on to explain why I feel the way I do and invite them to an open discussion. Ours (3-bit): I response to a negative comment on social media by thanking the person for their feedback, clarifying if they are basing their comment off of a bad experience, and inviting them to contact me directly so we can work through any issues or concerns they may have. \u2014\u2014\u2014\u2014 QLoRA (2-bit): Demostimeta for the Parts of the Their work of for the parts of Their work of For the parts of3.4.2004 G Mach Rack 2002-2004 G 2. Ours (2-bit): Negative comments on social media can be hard to respond to, but I would first try to understand where the person is coming from and why they are saying what they are saying. I would then strive to reply in a way that does not further escalate the discussion, but instead responds with a mature.\nInstruction: Categorize the following fruits according to color. Input: Apple, Banana, Avocado, Peach. QLoRA (4-bit): of the same color: Red (Apple, Banana), Green (Avocado, Peach) Other: Yellow (Peach).\nOurs (4-bit): Apple - Red. Banana - Yellow. Avocado - Green. Peach - Pink. \u2014\u2014\u2014\u2014 QLoRA (3-bit): is yellow is green is black is white. Ours (3-bit): - Apple (Red)- Banana (Yellow)- Avocado (Green)- Peach (Pink). \u2014\u2014\u2014\u2014 QLoRA (2-bit): Apple, mash up, and avocado. Ours (2-bit): Apple - Red. Banana - Yellow. Avocado - Green. Peach - Pink.\nInstruction: Generate an HTML page that contains the text \\\u201dWelcome to my page\\\u201d. QLoRA (4-bit): \\n\\n<html>\\n <head>\\n <title>Welcome to my page</title>\\n </head>\\n <body>\\n <h1>Welcome to my page</h1>\\n </body>\\n</html>\\n. Ours (4-bit): Welcome to my page.\\n<html>\\n<body>\\n<p>Welcome to my page.</p>\\n</body>\\n</html>\\n. \u2014\u2014\u2014\u2014 QLoRA (3-bit): <html>\\n <head>\\n <title>My Page</title>\\n </head>\\n <body>\\n <p>Welcome to my page!</p>\\n </body>\\n</html> \\n package org.michael. Ours (3-bit): <HTML>\\n <P>Welcome to my page</P>\\n <P>Lorem Ipsum is simply placeholder text.</P>\\n</HTML>\\n\\n Note that this response may also contain other elements such as stylesheets and script, if they were defined in the task. \u2014\u2014\u2014\u2014 QLoRA (2-bit): The page has the text \\\u201dWelcome to my page\\\u201d as it.\\n\\n28. A reint: a 28!313473-020-520. The\\n313473-020/2 Ours (2-bit): Welcome to my page.\\n<html>\\n<body>\\n<p>Welcome to my page.</p>\\n</body>\\n</html>\\n.\nInstruction: Create a dialogue between two people having a conversation about going to the beach. QLoRA (4-bit): of course, I\u2019m in! what time were you thinking of meeting up? I was thinking of going around noon. sounds good, I\u2019ll see you then! Ours (4-bit): .... (beach conversations A and B)\\n.... B: Going to the beach sounds like a blast! A: It does, isn\u2019t it? B: Yep! A: so what are we doing? B: Planning a full itinerary. \u2014\u2014\u2014\u2014 QLoRA (3-bit): \u201dPerson 1: \u201dI\u2019m so excited to go to the beach! The sun, sand, and water are all so soothing. I can\u2019t wait to get there and dive in!\u201d Person 2: \u201dMe too! I can\u2019t wait to experience that feeling of being\u201d. Ours (3-bit): \u201dSo which beach should we go to, Fiji or Bora Bora?\u201d\\n, \u201dFiji has more coral reefs, Bora Bora has more beaches, right?\u201d\\n, \u201dYeah, Bora Bora also has the world\u2019s most\u201d \u2014\u2014\u2014\u2014 QLoRA (2-bit): Such as a user 31 is 4 0-5 Gigs Sector 4 (22.12.2018) - [P] / f / Beyond there is a lot of and. Ours (2-bit): .... (beach conversations A and B)\\n.... B: Going to the beach sounds like a blast! A: It does, isn\u2019t it? B: Yep! A: so what are we doing? B: Planning a full itinerary."
        },
        {
            "heading": "B ADDITIONAL EXPERIMENTAL RESULTS",
            "text": "We plot some important results in Table 1 in Figures 4\u20137 to ease the readers to compare the our model with the baseline, QLoRA. Note that Figure 5 is identical to Figure 1.\nWe also provide the diagnosis on the L (group size) and Dint (the hidden dimensionality of adaptation) hyper-parameters in Tables 6 and 7."
        },
        {
            "heading": "C MODEL SIZE",
            "text": "We report the sizes of the final models of QLoRA and QA-LoRA in Table 8. Please note that there are two ways for post-processing in QLoRA, i.e., the unmerged (W\u0303 and s \u00b7 AB are stored individually, which saves memory but the inference is slow) and merged (s \u00b7 AB is added to W\u0303, which is faster in inference but requires large memory because the matrix must be stored in FP16). QA-LoRA enjoys both low memory usage and a fast inference speed. A side note: In 33B and 65B models, setting L = 32 in QA-LoRA results in slightly larger model sizes compared to QLoRA, but one can set L = 128 which causes a negligible accuracy drop.\nNote that the final model size of QA-LoRA is exactly the size of W\u2032 (or equivalently, W\u0303) because s \u00b7 AB is merged into W\u0303 after adaptation. Take the 7B model with L = 32 as an example. The baseline, the unmerged version of QLoRA, is sized 4.6G, in which W\u0303 is sized 4.0G and A and B combined is sized 0.6G. QA-LoRA increases the first amount to 4.3G and eliminates the second amount."
        }
    ],
    "title": "QA-LORA: QUANTIZATION-AWARE LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS",
    "year": 2024
}