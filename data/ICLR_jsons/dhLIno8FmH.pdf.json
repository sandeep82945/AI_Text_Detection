{
    "abstractText": "Electroencephalography (EEG) signals, known for convenient non-invasive acquisition but low signal-to-noise ratio, have recently gained substantial attention due to the potential to decode natural images. This paper presents a self-supervised framework to demonstrate the feasibility of learning image representations from EEG signals, particularly for object recognition. The framework utilizes image and EEG encoders to extract features from paired image stimuli and EEG responses. Contrastive learning aligns these two modalities by constraining their similarity. Our approach achieves state-of-the-art results on a comprehensive EEG-image dataset, with a top-1 accuracy of 15.6% and a top-5 accuracy of 42.8% in 200-way zero-shot tasks. Moreover, we perform extensive experiments to explore the biological plausibility by resolving the temporal, spatial, spectral, and semantic aspects of EEG signals. Besides, we introduce attention modules to capture spatial correlations, providing implicit evidence of the brain activity perceived from EEG data. These findings yield valuable insights for neural decoding and brain-computer interfaces in real-world scenarios. Code available at https://github.com/eeyhsong/NICE-EEG.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yonghao Song"
        },
        {
            "affiliations": [],
            "name": "Bingchuan Liu"
        },
        {
            "affiliations": [],
            "name": "Xiang Li"
        },
        {
            "affiliations": [],
            "name": "Nanlin Shi"
        },
        {
            "affiliations": [],
            "name": "Yijun Wang"
        },
        {
            "affiliations": [],
            "name": "Xiaorong Gao"
        }
    ],
    "id": "SP:d54d63192fb0e4568c4a4350a3ccb067c4df1e45",
    "references": [
        {
            "authors": [
                "Hamad Ahmed",
                "Ronnie B. Wilbur",
                "Hari M. Bharadwaj",
                "Jeffrey Mark Siskind"
            ],
            "title": "Object classification from randomized EEG trials",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Emily J. Allen",
                "Ghislain St-Yves",
                "Yihan Wu",
                "Jesse L. Breedlove",
                "Jacob S. Prince",
                "Logan T. Dowdle",
                "Matthias Nau",
                "Brad Caron",
                "Franco Pestilli",
                "Ian Charest",
                "J. Benjamin Hutchinson",
                "Thomas Naselaris",
                "Kendrick Kay"
            ],
            "title": "A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence",
            "venue": "Nature Neuroscience,",
            "year": 2022
        },
        {
            "authors": [
                "Pinglei Bao",
                "Liang She",
                "Mason McGill",
                "Doris Y. Tsao"
            ],
            "title": "A map of object space in primate inferotemporal cortex",
            "venue": "Nature,",
            "year": 2020
        },
        {
            "authors": [
                "Andr\u00e9 Moraes Bastos",
                "Julien Vezoli",
                "Conrado Arturo Bosman",
                "Jan-Mathijs Schoffelen",
                "Robert Oostenveld",
                "Jarrod Robert Dowdall",
                "Peter De Weerd",
                "Henry Kennedy",
                "Pascal Fries"
            ],
            "title": "Visual Areas Exert Feedforward and Feedback Influences through Distinct Frequency Channels",
            "year": 2015
        },
        {
            "authors": [
                "Yohann Benchetrit",
                "Hubert Banville",
                "Jean-R\u00e9mi King"
            ],
            "title": "Brain decoding: Toward real-time reconstruction of visual perception",
            "year": 2023
        },
        {
            "authors": [
                "Shaked Brody",
                "Uri Alon",
                "Eran Yahav"
            ],
            "title": "How Attentive are Graph Attention Networks",
            "year": 2022
        },
        {
            "authors": [
                "Xinyu Cheng",
                "Wei Wei",
                "Changde Du",
                "Shuang Qiu",
                "Sanli Tian",
                "Xiaojun Ma",
                "Huiguang He"
            ],
            "title": "VigilanceNet: Decouple Intra- and Inter-Modality Learning for Multimodal Vigilance Estimation in RSVP-Based BCI",
            "venue": "In Proceedings of the 30th ACM International Conference on Multimedia,",
            "year": 2022
        },
        {
            "authors": [
                "Radoslaw M. Cichy",
                "Aude Oliva"
            ],
            "title": "A M/EEG-fMRI Fusion Primer: Resolving Human Brain Responses",
            "venue": "in Space and Time. Neuron,",
            "year": 2020
        },
        {
            "authors": [
                "Radoslaw Martin Cichy",
                "Dimitrios Pantazis",
                "Aude Oliva"
            ],
            "title": "Resolving human object recognition in space and time",
            "venue": "Nature Neuroscience,",
            "year": 2014
        },
        {
            "authors": [
                "Djork-Arn\u00e9 Clevert",
                "Thomas Unterthiner",
                "Sepp Hochreiter"
            ],
            "title": "Fast and accurate deep network learning by exponential linear units (elus)",
            "venue": "In International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "Joel Dapello",
                "Kohitij Kar",
                "Martin Schrimpf",
                "Robert Baldwin Geary",
                "Michael Ferguson",
                "David Daniel Cox",
                "James J. DiCarlo"
            ],
            "title": "Aligning Model and Macaque Inferior Temporal Cortex Representations Improves Model-to-Human Behavioral Alignment and Adversarial Robustness",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "James J. DiCarlo",
                "David D. Cox"
            ],
            "title": "Untangling invariant object recognition",
            "venue": "Trends in Cognitive Sciences,",
            "year": 2007
        },
        {
            "authors": [
                "Yi Ding",
                "Neethu Robinson",
                "Chengxuan Tong",
                "Qiuhao Zeng",
                "Cuntai Guan"
            ],
            "title": "LGGNet: Learning From Local-Global-Graph Representations for Brain\u2013Computer Interface",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems, pp",
            "year": 2023
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Changde Du",
                "Kaicheng Fu",
                "Jinpeng Li",
                "Huiguang He"
            ],
            "title": "Decoding Visual Neural Representations by Multimodal Learning of Brain-Visual-Linguistic Features",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 1939
        },
        {
            "authors": [
                "Kaicheng Fu",
                "Changde Du",
                "Shengpei Wang",
                "Huiguang He"
            ],
            "title": "Multi-View Multi-Label Fine-Grained Emotion Decoding From Human Brain Activity",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaorong Gao",
                "Yijun Wang",
                "Xiaogang Chen",
                "Shangkai Gao"
            ],
            "title": "Interface, interaction, and intelligence in generalized brain\u2013computer interfaces",
            "venue": "Trends in Cognitive Sciences,",
            "year": 2021
        },
        {
            "authors": [
                "Alessandro T. Gifford",
                "Kshitij Dwivedi",
                "Gemma Roig",
                "Radoslaw M. Cichy"
            ],
            "title": "A large and rich EEG dataset for modeling human visual object recognition",
            "year": 2022
        },
        {
            "authors": [
                "Matthias Guggenmos",
                "Philipp Sterzer",
                "Radoslaw Martin Cichy"
            ],
            "title": "Multivariate pattern analysis for MEG: A comparison of dissimilarity measures",
            "year": 2018
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep Residual Learning for Image Recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Martin N Hebart",
                "Oliver Contier",
                "Lina Teichmann",
                "Adam H Rockter",
                "Charles Y Zheng",
                "Alexis Kidder",
                "Anna Corriveau",
                "Maryam Vaziri-Pashkam",
                "Chris I Baker"
            ],
            "title": "THINGS-data, a multimodal collection of large-scale datasets for investigating object representations in human brain",
            "venue": "and behavior. eLife,",
            "year": 2023
        },
        {
            "authors": [
                "C.S. Herrmann",
                "T. Demiralp"
            ],
            "title": "Human EEG gamma oscillations in neuropsychiatric disorders",
            "venue": "Clinical Neurophysiology,",
            "year": 2005
        },
        {
            "authors": [
                "Jun Kai Ho",
                "Tomoyasu Horikawa",
                "Kei Majima",
                "Fan Cheng",
                "Yukiyasu Kamitani"
            ],
            "title": "Inter-individual deep image reconstruction via hierarchical neural code conversion",
            "year": 2023
        },
        {
            "authors": [
                "Tomoyasu Horikawa",
                "Yukiyasu Kamitani"
            ],
            "title": "Generic decoding of seen and imagined objects using hierarchical visual features",
            "venue": "Nature Communications,",
            "year": 2017
        },
        {
            "authors": [
                "Xiaoxuan Jia",
                "Ha Hong",
                "James J DiCarlo"
            ],
            "title": "Unsupervised changes in core object recognition behavior are predicted by neural plasticity in inferior temporal cortex. eLife, 10:e60830",
            "venue": "ISSN 2050-084X. doi: 10.7554/eLife.60830",
            "year": 2021
        },
        {
            "authors": [
                "Yukiyasu Kamitani",
                "Frank Tong"
            ],
            "title": "Decoding the visual and subjective contents of the human brain",
            "venue": "Nature Neuroscience,",
            "year": 2005
        },
        {
            "authors": [
                "Kendrick N. Kay",
                "Thomas Naselaris",
                "Ryan J. Prenger",
                "Jack L. Gallant"
            ],
            "title": "Identifying natural images from human brain activity",
            "year": 2008
        },
        {
            "authors": [
                "Christian Keysers",
                "David I Perrett"
            ],
            "title": "Visual masking and RSVP reveal neural competition",
            "venue": "Trends in Cognitive Sciences,",
            "year": 2002
        },
        {
            "authors": [
                "Reinmar Kobler",
                "Jun-ichiro Hirayama",
                "Qibin Zhao",
                "Motoaki Kawanabe"
            ],
            "title": "SPD domain-specific batch normalization to crack interpretable unsupervised domain adaptation in EEG",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Vernon J. Lawhern",
                "Amelia J. Solon",
                "Nicholas R. Waytowich",
                "Stephen M. Gordon",
                "Chou P. Hung",
                "Brent J. Lance"
            ],
            "title": "EEGNet: A compact convolutional neural network for EEG-based brain\u2013computer interfaces",
            "venue": "Journal of Neural Engineering,",
            "year": 2018
        },
        {
            "authors": [
                "Ren Li",
                "Jared S. Johansen",
                "Hamad Ahmed",
                "Thomas V. Ilyevsky",
                "Ronnie B. Wilbur",
                "Hari M. Bharadwaj",
                "Jeffrey Mark Siskind"
            ],
            "title": "The Perils and Pitfalls of Block Design for EEG Classification Experiments",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Sikun Lin",
                "Thomas Sprague",
                "Ambuj K Singh"
            ],
            "title": "Mind Reader: Reconstructing complex images from brain activities",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Bingchuan Liu",
                "Xiaogang Chen",
                "Nanlin Shi",
                "Yijun Wang",
                "Shangkai Gao",
                "Xiaorong Gao"
            ],
            "title": "Improving the Performance of Individually Calibrated SSVEP-BCI by Task- Discriminant Component Analysis",
            "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Dongjun Liu",
                "Weichen Dai",
                "Hangkui Zhang",
                "Xuanyu Jin",
                "Jianting Cao",
                "Wanzeng Kong"
            ],
            "title": "BrainMachine Coupled Learning Method for Facial Emotion Recognition",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 1939
        },
        {
            "authors": [
                "Georgios Michalareas",
                "Julien Vezoli",
                "Stan van Pelt",
                "Jan-Mathijs Schoffelen",
                "Henry Kennedy",
                "Pascal Fries"
            ],
            "title": "Alpha-Beta and Gamma Rhythms Subserve Feedback and Feedforward Influences among Human Visual Cortical Areas",
            "year": 2016
        },
        {
            "authors": [
                "Yoichi Miyawaki",
                "Hajime Uchida",
                "Okito Yamashita",
                "Masa-aki Sato",
                "Yusuke Morito",
                "Hiroki C. Tanabe",
                "Norihiro Sadato",
                "Yukiyasu Kamitani"
            ],
            "title": "Visual Image Reconstruction from Human Brain Activity using a Combination of Multiscale Local Image Decoders",
            "year": 2008
        },
        {
            "authors": [
                "Simone Palazzo",
                "Concetto Spampinato",
                "Isaak Kavasidis",
                "Daniela Giordano",
                "Joseph Schmidt",
                "Mubarak Shah"
            ],
            "title": "Decoding Brain Representations by Multimodal Learning of Neural Activity and Visual Features",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Yue-Ting Pan",
                "Jing-Lun Chou",
                "Chun-Shu Wei"
            ],
            "title": "MAtt: A Manifold Attention Network for EEG Decoding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Alexandre Sayal",
                "Teresa Sousa",
                "Jo\u00e3o V. Duarte",
                "Gabriel N. Costa",
                "Ricardo Martins",
                "Miguel Castelo-Branco"
            ],
            "title": "Identification of competing neural mechanisms underlying positive and negative perceptual hysteresis in the human visual system",
            "year": 2020
        },
        {
            "authors": [
                "Robin Tibor Schirrmeister",
                "Jost Tobias Springenberg",
                "Lukas Dominique Josef Fiederer",
                "Martin Glasstetter",
                "Katharina Eggensperger",
                "Michael Tangermann",
                "Frank Hutter",
                "Wolfram Burgard",
                "Tonio Ball"
            ],
            "title": "Deep learning with convolutional neural networks for EEG decoding and visualization",
            "venue": "Human Brain Mapping,",
            "year": 2017
        },
        {
            "authors": [
                "Steffen Schneider",
                "Jin Hwa Lee",
                "Mackenzie Weygandt Mathis"
            ],
            "title": "Learnable latent embeddings for joint behavioural and neural analysis",
            "venue": "Nature,",
            "year": 2023
        },
        {
            "authors": [
                "Ramprasaath R. Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization",
            "venue": "IEEE International Conference on Computer Vision (ICCV),",
            "year": 2017
        },
        {
            "authors": [
                "Guohua Shen",
                "Tomoyasu Horikawa",
                "Kei Majima",
                "Yukiyasu Kamitani"
            ],
            "title": "Deep image reconstruction from human brain activity",
            "venue": "PLOS Computational Biology,",
            "year": 2019
        },
        {
            "authors": [
                "Nanlin Shi",
                "Xiang Li",
                "Bingchuan Liu",
                "Chen Yang",
                "Yijun Wang",
                "Xiaorong Gao"
            ],
            "title": "RepresentativeBased Cold Start for Adaptive SSVEP-BCI",
            "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering,",
            "year": 2023
        },
        {
            "authors": [
                "Yonghao Song",
                "Qingqing Zheng",
                "Bingchuan Liu",
                "Xiaorong Gao"
            ],
            "title": "EEG Conformer: Convolutional Transformer for EEG Decoding and Visualization",
            "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering,",
            "year": 2023
        },
        {
            "authors": [
                "C. Spampinato",
                "S. Palazzo",
                "I. Kavasidis",
                "D. Giordano",
                "N. Souly",
                "M. Shah"
            ],
            "title": "Deep Learning Human Mind for Automated Visual Classification",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Yu Takagi",
                "Shinji Nishimoto"
            ],
            "title": "High-resolution image reconstruction with latent diffusion models from human brain activity",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation Learning with Contrastive Predictive Coding, January 2019",
            "year": 2019
        },
        {
            "authors": [
                "Laurens van der Maaten",
                "Geoffrey Hinton"
            ],
            "title": "Visualizing Data using t-SNE",
            "venue": "Journal of Machine Learning Research,",
            "year": 2008
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is All you Need",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio"
            ],
            "title": "Graph Attention Networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Junke Wang",
                "Dongdong Chen",
                "Zuxuan Wu",
                "Chong Luo",
                "Luowei Zhou",
                "Yucheng Zhao",
                "Yujia Xie",
                "Ce Liu",
                "Yu-Gang Jiang",
                "Lu Yuan"
            ],
            "title": "OmniVL: One foundation model for image-language and video-language tasks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Francis R. Willett",
                "Donald T. Avansino",
                "Leigh R. Hochberg",
                "Jaimie M. Henderson",
                "Krishna V. Shenoy"
            ],
            "title": "High-performance brain-to-text communication via handwriting",
            "venue": "Nature,",
            "year": 2021
        },
        {
            "authors": [
                "Rui Xu",
                "Wenjing Zhou",
                "Xin Xu",
                "Zhipei Ling",
                "Hesheng Liu",
                "Bo Hong"
            ],
            "title": "A temporal hierarchy of object processing in human visual cortex",
            "year": 2023
        },
        {
            "authors": [
                "Shlomit Yuval-Greenberg",
                "Orr Tomer",
                "Alon S. Keren",
                "Israel Nelken",
                "Leon Y. Deouell"
            ],
            "title": "Transient Induced Gamma-Band Response in EEG as a Manifestation of Miniature Saccades",
            "year": 2008
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Our daily life relies on accurately and rapidly identifying objects in complex visual environments (DiCarlo & Cox, 2007). Researchers have pursued decoding natural images from brain activity, aiming to deepen our understanding of the brain and create user-friendly brain-computer interfaces (BCIs) (Kamitani & Tong, 2005; Kay et al., 2008; Gao et al., 2021). Functional magnetic resonance imaging (fMRI), which records blood oxygen level-dependent signals, has been a popular choice for categorizing objects observed by humans (Du et al., 2023; Horikawa & Kamitani, 2017; Allen et al., 2022). Despite its high spatial resolution, fMRI typically requires several seconds to provide a stable response to a single stimulus, limiting its real-time applicability in daily interactions (Lin et al., 2022). Similarly, Magnetoencephalogram (MEG), with high time resolution, has been employed for this purpose, but it is hindered by cost and large devices (Cichy et al., 2014; Hebart et al., 2023).\nElectroencephalogram (EEG) has emerged as a valuable tool for decoding images based on visualevoked brain activities (Spampinato et al., 2017). EEG has high time resolution, low cost, and good portability, but the low signal-to-noise ratio takes problems (Pan et al., 2022; Kobler et al., 2022). Some efforts have yielded impressive classification results and captured saliency maps using EEG signals (Palazzo et al., 2021). However, its flawed block-design experiments group images of the same class within one block, resulting in classification that relies on block-level temporal correlation rather than stimulus-related activity (Li et al., 2021). Some studies have achieved above-chance performance but were restricted to single-subject data (Ahmed et al., 2021). Moreover, available datasets often feature a limited number of image categories, posing challenges for real-world image decoding tasks. The latest study has significantly expanded the situation by amassing a large and diverse EEG dataset comprising 16,740 image stimuli spanning 1,854 concepts, employing rapid serial visual presentation (Gifford et al., 2022). While demonstrating the feasibility of image-to-EEG encoding and category separability, this work primarily focused on analyzing electrodes in the occipital and parietal regions, overlooking the inferior temporal cortex plays a necessary role in object recognition (Dapello et al., 2023). In addition, the work focuses on the peak of pairwise decoding about 110 ms after stimuli onset. Generally, this latency involves visual conduction and primary visual processing by a large\nmargin, rather than semantic understanding Xu et al. (2023). In short, the issues of multi-class object recognition and biological plausibility warrant further exploration.\nPattern recognition plays a pivotal role in EEG analysis. Existing work has predominantly relied on supervised learning, often dealing with limited data from a few categories (Cheng et al., 2022). Applying machine learning methods directly, such as support vector machines and deep neural networks, has proven challenging in obtaining intrinsic representations (Fu et al., 2022; Liu et al., 2021). People have performed self-supervised learning and leveraged information from other modalities in computer vision research (Wang et al., 2022; Saharia et al., 2022). Text replaces the traditional hard labels to enable self-supervised learning to produce superior image representations, often leading to impressive zero-shot results on downstream tasks (Radford et al., 2021). Researchers have also used contrastive learning to obtain consistent embedding of neural recordings across multiple animals (Schneider et al., 2023). Building upon these insights, we explore the intriguing possibility of recognizing visual objects from EEG signals in a self-supervised manner. Another issue pertains to the limitations of commonly employed EEG feature extractors, typically structured around convolutional layers applied separately along temporal and spatial dimensions of raw EEG signals (Schirrmeister et al., 2017; Lawhern et al., 2018; Song et al., 2023). This organization disrupts the correlations between electrode channels, hindering our perceiving spatial properties of brain activity (Ding et al., 2023).\nTo address the above limitations, we introduce a self-supervised framework to decode image representations from EEG signals, focusing on object recognition. Our approach uses contrastive learning as the bridge connecting image stimuli and EEG responses. We feed image-EEG pairs to the model and process them by an image encoder and an EEG encoder separately. These modalities are aligned by constraining their cosine similarity, resulting in the ability to perform zero-shot EEG decoding after training, achieving remarkable classification accuracy for previously unseen image categories. With the framework, we provide an overall resolving of the visual object recognition processing in our brain from spatial, temporal, spectral, and semantic aspects with comparative experiments. The results are consistent with established neuroscientific knowledge, further demonstrating the feasibility of EEG-based image decoding. Furthermore, we present two spatial modules with self-attention and graph attention that integrate with the EEG encoder (Dosovitskiy et al., 2021; Velic\u030ckovic\u0301 et al., 2018), helping us to emphasize key brain regions relevant to object recognition.\nOur main contributions can be summarized as follows:\n\u2022 We propose a self-supervised framework for EEG-based object recognition with contrastive learning. Remarkable zero-shot performance has been achieved on large and rich datasets.\n\u2022 We demonstrate the feasibility of investigating natural image information from EEG signals. Extensive experiments affirm the biological plausibility, which brings a resolving of human object recognition from temporal, spatial, spectral, and semantic aspects.\n\u2022 We apply two plug-and-play modules to capture spatial correlations among EEG channels, offering evidence that the model discerns the spatial dynamics of object recognition."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": "Decoding visual information from our brain has been a long-standing pursuit in neuroscience and computer science (Miyawaki et al., 2008; Jia et al., 2021). While some progress has been made in decoding steady-state visual stimuli, the accurate and rapid decoding of semantic information in natural images has remained a challenge (Shi et al., 2023). fMRI has been widely used to estimate semantic and shape information from visual processing in the brain (Ho et al., 2023; Takagi & Nishimoto, 2023). However, the demand for high-speed and practical applications in brain-computer interaction necessitates alternative approaches. EEG has emerged as a promising option due to its high temporal resolution and portability (Willett et al., 2021). But general performance on different subjects and biological plausibility remain unsolved (Ahmed et al., 2021). Additionally, prior approaches have often relied on supervised learning methods with limited image categories, overlooking the intrinsic relationship between image stimuli and brain responses (Shen et al., 2019; Li et al., 2021; Liu et al., 2023). These limitations hinder their practicality in real-world scenarios with generalization to new categories. Motivated by these challenges, we present an EEG-based image decoding framework that employs self-supervised learning, enabling the model to achieve zero-shot generalization in object recognition tasks, further demonstrating the feasibility."
        },
        {
            "heading": "3 METHODS",
            "text": ""
        },
        {
            "heading": "3.1 OVERVIEW",
            "text": "We propose a self-supervised framework, Natural Image Contrast EEG (NICE), to decode images from EEG signals. The overall framework is depicted in Fig. 1(A).\nDuring training, stimulus-response pairs comprising images and EEG signals are fed into the framework. The image and EEG encoder extract features from their respective modalities. Contrastive learning is employed to align these features, optimizing the similarity between matched pairs and reducing it for unmatched pairs. This self-supervised approach enables the model to learn shared information between the two modalities instead of traditional supervised learning with predefined labels. Before the test process, we use a few images that belong to the image concepts (classes) to be classified, and have not appeared in the EEG collection experiments. These images are processed and averaged to obtain one template for each concept. Next, the EEG encoder processes the test signals, and their similarity with all templates is calculated for matching results.\nFor the EEG encoder, we adapt temporal-spatial convolution (TSConv), which is widely used in EEG analysis, as shown in Fig. 1(B). In addition, we integrate two plug-and-play spatial modules with self-attention and graph attention. These modules are instrumental in preserving the spatial characteristics of EEG channels, reflecting intrinsic patterns of brain activity. Note that the image encoder has been pre-trained on other image datasets. The leading studies in computer vision help us with larger sample space, when we cannot easily collect numbers of brain responses."
        },
        {
            "heading": "3.2 NETWORK ARCHITECTURE",
            "text": ""
        },
        {
            "heading": "3.2.1 EEG ENCODER",
            "text": "Researchers usually arrange the raw EEG trials into two dimensions C \u21e5 T , in which C denotes electrode channels, and T denotes time samples. Convolution along the two dimensions is widely used in deep learning-based EEG analysis models. In the same way, we present a very concise network architecture as shown in Table 1. One-dimension convolution is first applied to capture the temporal features with k kernels of size (1,m1) and stride of (1, 1). An average pooling layer with a kernel size of (1,m2) and stride of (1, s) is introduced to alleviate overfitting and smooth the temporal features. Another one-dimension convolution is then used for spatial features keeping k kernels of size (ch, 1) and stride of (1, 1), where ch usually equals the number of electrodes. After\neach convolutional layer, batch normalization and exponential linear units (ELU) activation functions are used for better training and nonlinearity (Clevert et al., 2016). Finally, a linear layer is added as a projector to transform the features to the same size as the output of the image encoder."
        },
        {
            "heading": "3.2.2 IMAGE ENCODER",
            "text": "The image features are extracted with the pairing EEG features for contrastive learning. There have been some excellent computer vision models that can obtain image features with semantic discrimination. We prefer directly using models pre-trained on other image datasets to get a large sample space. The feature extraction parts of Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021), Vision Transfomer (ViT) (Dosovitskiy et al., 2021), and Residual Neural Networks (ResNet) (He et al., 2016) are tried separately in our framework for demonstration. All the images for stimuli can be processed in advance, thus speeding up our model training."
        },
        {
            "heading": "3.2.3 CONTRASTIVE LEARNING",
            "text": "The framework is constructed with contrastive learning, shown in Algorithm 1. The outputs of the image and EEG encoder are normalized separately. Then, the dot product is used to evaluate the similarity of all image-EEG feature pairs. A scaled temperature parameter is used to adjust distribution probability. InfoNCE loss (Radford et al., 2021; van den Oord et al., 2019) is employed as the objective function to increase the similarities between matched pairs and decrease those between unmatched pairs. After adequate training, the EEG encoder can extract representations similar to corresponding images. The self-supervised strategy allows us to learn inherent patterns from EEG signals without labels, rather than directly separate different classes with supervised learning.\nAlgorithm 1 Natural Image Contrast EEG framework 1: Input: (Image, EEG) - stimulus & response 2: Model: Enc_img - CLIP & Enc_eeg - TSConv\n3: # E - (batch, channel, electrode, sample) . batch of input EEG 4: # I - (batch, channel, height, width) . batch of input images 5: # \u2327 - learned temperature parameter\n6: # extract normalized representations from the raw image and EEG 7: E_f = Norm(Linear(Enc_eeg(E))) 8: I_f = Norm(Enc_img(I)) . can be obtained before training\n9: # scaled pairwise cosine similarity 10: logits = dot(E_f, I_f.t) * exp(\u2327)"
        },
        {
            "heading": "11: # symmetric loss function",
            "text": ""
        },
        {
            "heading": "12: labels = arange(batch)",
            "text": ""
        },
        {
            "heading": "13: loss_e = cross_entropy_loss(logits, labels, axis=0)",
            "text": ""
        },
        {
            "heading": "14: loss_i = cross_entropy_loss(logits, labels, axis=1)",
            "text": "15: loss = (loss_e + loss_i) / 2"
        },
        {
            "heading": "3.3 PLUG-AND-PLAY MODULE",
            "text": ""
        },
        {
            "heading": "3.3.1 SELF-ATTENTION",
            "text": "We utilize two approaches, self-attention (SA) Vaswani et al. (2017), and graph attention (GA) Velic\u030ckovic\u0301 et al. (2018), to supplement the EEG encoder as \"spatial filters\" to encapsulate electrode correlations and reflect the spatial dynamics of brain activity. We use SA on the electrode channels to evaluate the spatial correlations of EEG data. The input xin 2 RC\u21e5T is linearly transformed into equal-sized with weight metrics Wq , Wk, Wv:\nx0in = Softmax( Wqxin \u00b7 (Wkxin)Tp\nd ) \u00b7Wvxin (1)\nwhere x0in 2 RC\u21e5T denotes the output after the SA module, d, the time length of EEG data, is a scaled factor to accommodate the Softmax function."
        },
        {
            "heading": "3.3.2 GRAPH ATTENTION",
            "text": "We employ the GA module to update each electrode with all the others, using the implementation from Brody et al. (2022). We treat each electrode as a node ni 2 R1\u21e5T , i = 1, ..., ch, which has edges to all the other electrodes Ni. The process to update one electrode is as follows:\nn0i = \u21b5i,iWni + X\nj2Ni\n\u21b5i,jWnj (2)\nwhere n0i denotes each node after processing, \u21b5i,j is attention coefficients indicating importance of node j features to node i, and W is the weight of linear transformation. Calculate \u21b5i,j as:\n\u21b5i,j = exp(aT LeakyReLU(W [ni knj ]))P\nk2Ni[{i} exp(a T LeakyReLU(W [ni knk]))\n(3)\nwhere a 2 R2T denotes the weight of a feedforward layer for attention calculation, ()T is the transposition operator, and k is the concatenation operator. LeakyReLU with a slope of 0.2 is used for nonlinearity in calculation. Residual connection is employed by integrating the input and output of the SA and GA modules separately, contributing to stable training (He et al., 2016)."
        },
        {
            "heading": "4 EXPERIMENTS AND RESULTS",
            "text": ""
        },
        {
            "heading": "4.1 DATASETS AND PREPROCESSING",
            "text": "The dataset (Gifford et al., 2022) contains EEG data from ten participants with a time-efficient rapid serial visual presentation (RSVP) paradigm. The training set includes 1654 concepts\u21e510 images\u21e54 repetitions. The test set includes 200 concepts\u21e51 image\u21e580 repetitions. Images for training and testing appear in a pseudo-randomized order, and a target image is used to reduce eye blinks and other artifacts. Each image displays 100 ms, followed by a 100 ms blank screen. Raw EEG data filtered to [0.1, 100] Hz has 63 channels and a sample rate of 1000 Hz.\nFor preprocessing, we epoched EEG data into trials ranging from 0 to 1000 ms after stimuli onset. Baseline correction was performed with the mean of 200 ms pre-stimulus data. All electrodes were preserved for analysis with down-sampling to 250 Hz, and multivariate noise normalization was performed with training data (Guggenmos et al., 2018). We averaged all EEG repetitions of one image to ensure the signal-to-noise ratio and compared the impact of repetitions. Images were resized to 224\u21e5224 and normalized before being processed by the image encoder."
        },
        {
            "heading": "4.2 EXPERIMENT DETAILS",
            "text": "Our method is implemented with PyTorch on a Geforce 4090 GPU. We randomly select 740 trials from training data as the validation set in each run of the code. Best models are saved when the validation loss reaches a minimum of 200 epochs in the training process. We compute the results of the test set once after training. It takes about 5 minutes per subject to train with a batch size of 1000, since we get the image features in advance. The k in TSConv is set to 40, m1 to 25, m2 to 51, and s to 5 by pre-experiments. Adam optimizer is used with the learning rate, 1 and 2 of 0.0002, 0.5, and 0.999, respectively. Wilcoxon Signed-Rank Test is employed to analyze the statistical significance."
        },
        {
            "heading": "4.3 OVERALL PERFORMANCE",
            "text": "The decoding results are outlined in Table 2, with a sanity check in Appendix A.1. We evaluated the base framework (NICE) alongside its variants enhanced with self-attention (NICE-SA) and graph attention (NICE-GA). The evaluation was on the latest and most extensive image-EEG dataset, with only one state-of-the-art BraVL (Du et al., 2023) for comparison. This dataset posed a 200-way zero-shot task, comprising 200 untrained image concepts, with a chance level of 0.5%. In subjectdependent experiments, NICE achieved a top-1 accuracy of 13.8% and a top-5 accuracy of 39.5%, surpassing BraVL by 8.0% and 22.0%, respectively. The introduction of SA and GA improved the top-1 by 0.9% (p < 0.05) and 1.8% (p < 0.01), separately. Besides, in subject-independent experiments, our method achieved top-1 of 6.2% and top-5 of 21.4%. NICE-SA improved the top-1 by 0.8% (p > 0.05), while NICE-GA only improved for several subjects, with the average top-1 decreasing by 0.3%. We employed base NICE for fair comparison in the following experiments."
        },
        {
            "heading": "4.4 ENCODER COMPARISON",
            "text": "The comparison of EEG encoders and image encoders is present in Table 3. We carefully selected several representative methods, including ShallowNet, DeepNet (Schirrmeister et al., 2017), Conformer (Song et al., 2023), and EEGNet (Lawhern et al., 2018). Our custom-designed TSConv, which utilizes temporal and spatial convolution, outperformed these methods. The average top-1 accuracy of TSConv is 5.9% higher than ShallowNet (p < 0.001), 3.9% higher than DeepNet (p < 0.001), 2.7% higher than Conformer (p < 0.001), and 0.7% higher than EEGNet (p > 0.05). The robustness of TSConv reflected by standard deviation was not superior.\nWe utilized image encoders, including ResNet-50 pre-trained on ImageNet-1k, ViT-B/16 pre-trained on ImageNet-21k and fine-tuned on ImageNet-1k, and CLIP-ViT-L/14 pre-trained on 400 million image-text pairs. CLIP pre-trained on huge amounts of data helped us achieve the highest results. Interestingly, ViT also worked well as the image encoder, with an average top-1 accuracy 1.7% lower than CLIP (p > 0.05). The results of ResNet were 6.9% lower (p < 0.001).\nWe also showed the impact of pre-training compared with non-pre-trained ResNet-50 and ViT-B/16. Pre-trained ViT outperformed the non-pre-trained by 4.2% (p < 0.01). Surprisingly, ResNet pretrained with smaller ImageNet-1k decreased by 1.9% (p < 0.05). Note that non-pre-trained models performed well but incurred substantially higher computational costs, with details in Appendix A.2."
        },
        {
            "heading": "4.5 TEMPORAL, SPATIAL, AND SPECTRAL DYNAMICS",
            "text": "Beyond the self-supervised framework, we try to demonstrate the biological plausibility by resolving the visual processing of EEG signals. We conducted a detailed analysis from the temporal, spatial, and spectral dynamics perspective in Fig. 2. The topographies were first plotted with each 100 ms in Fig. 2(A) by averaging all training trials from the first subject. Visual masking (Keysers & Perrett, 2002) would be alleviated because the data used were collected in many rapid series sequences. A clear response could be observed on the temporal cortex in 100-600 ms after the stimulus, although the 200 ms stimulus onset asynchrony still caused periodic responses on the occipital cortex. Besides, the parietal cortex also had a response after 100 ms. The phenomenon is consistent with the bottom-up hierarchy of visual system (DiCarlo & Cox, 2007), that the visual stimulus is processed sequentially by the V1, V2, V4 on the occipital cortex, and inferotemporal (IT) on the temporal cortex along the ventral stream for object recognition (Bao et al., 2020).\nWe explored the active time range in three ways: incrementing forward, decrementing backward, and segmentation, as in Fig. 2(B). It could be seen that the region of interest was in 100 to 600 ms. The average top-1 accuracy of [0, 1000] ms was 0.1% lower than that of [100, 1000] ms without significance (p > 0.05). These findings suggest that the initial 100 ms following stimulus onset contained limited information, possibly due to the hysteresis effect in the visual pathway (Sayal et al., 2020). Besides, the signal after 600 ms brought a negative effect, probably due to the noise from other stimuli and cognitive processing, emerging a response increasing on the frontal lobe as in Fig. 2(A).\nWe conducted an ablation study involving electrodes from different cortices as in Fig. 2(C). The average accuracy sharply declined 3.8% (p < 0.01) without occipital electrodes. Ablation of temporal,\nparietal electrodes and the central area near the motor cortex decreased the results by 1.9% (p < 0.05), 1.6% (p < 0.05), and 0.5% (p > 0.05) separately. The accuracy improved by 0.2% (p > 0.05) to discard frontal electrodes. The results are still reasonable with the low spatial resolution of EEG.\nWe plotted time-frequency maps in Fig. 2(D) by averaging all training trials from the first subject. The main components were below 30 Hz from the occipital, temporal, and parietal region data. High-frequency responses could be observed from electrodes on the temporal cortex. Interestingly, the frequency responses had an upward trend along with visual processing.\nWe further conducted classification tests with different frequency bands in Fig. 2(E). Theta and delta bands near 4 Hz performed well, where theta was more stable for different subjects. The beta and alpha bands also demonstrated performance above the chance level, coarsely aligning with previous findings (Bastos et al., 2015; Michalareas et al., 2016). However, the gamma band could hardly help us in decoding, for which we have two speculations. Capturing pure gamma oscillations from EEG can be challenging due to the susceptibility of high-frequency artifacts (Fries et al., 2008; Yuval-Greenberg et al., 2008). Besides, the amplitude of the gamma band is quite low and easily modulated by other cognitive processing, such as attention and memory (Herrmann & Demiralp, 2005). This may explain why some studies with MEG have also disregarded the analysis of the gamma band (Hebart et al., 2023). Inspired by the latest work Benchetrit et al. (2023), we provided preliminary analysis on MEG data for reference in Appendix A.5."
        },
        {
            "heading": "4.6 SEMANTIC SIMILARITY",
            "text": "We show the semantic similarity and classification visualization in Fig. 3. One concern is that the discrimination we obtain from EEG is not due to semantic information but only the basic visual components, such as color, brightness, contrast, etc. We performed representational similarity analysis (RSA) (Cichy & Oliva, 2020), and got the similarity matrices by averaging all subjects, and categorizing the 200 concepts in the test set into animal, food, vehicle, tool, and others. As shown in Fig. 3(A), we could observe distinct intra-category aggregation. This meant the obtained EEG representations were closer to the image representations within the corresponding semantic category.\nThe images in the test set were used for visualization in Fig. 3(B). We randomly chose several top-5 decoding results from the first subject. It could be noticed that the predicted results were semantically similar to the ground truth, such as the bike was predicted into several vehicles with wheels."
        },
        {
            "heading": "4.7 DATA SIZE AND REPETITION",
            "text": "We compared the impact of the data size and repetition of the training and test sets, which may be crucial for the performance, in Fig. 4. There were 1654 concepts\u21e510 image conditions repeated four times in the training set. We averaged the four repetitions of each condition for training. Two cases were compared in Fig. 4(A), adding conditions by 25% intervals with all repetitions, and adding repetitions by 25% with all conditions. Increasing conditions, i.e., data size, contributed significantly to decoding accuracy from 25% to 50% (p < 0.01), from 50% to 75% (p < 0.05), and from 75% to 100% (p < 0.05). There were also significant contributions by increasing repetitions from 25% to 50% (p < 0.01), from 50% to 75% (p < 0.05), except from 75% to 100% (p > 0.05). The results suggest that increasing repetitions is of limited help, while more data promises further improvements.\nFrom another view, repetitions of test data determine the practice efficiency. We compared the number of repetitions for averaging, from 5 to 80, with an interval of 5 in Fig. 4(B). Ten repetitions achieved a 9.9% (30.1%) accuracy, which tended to stabilize above 13.0% (37.0%) after twenty-five repetitions."
        },
        {
            "heading": "4.8 EFFECT OF SPATIAL MODULES",
            "text": "We tried to provide additional evidence of brain responses facilitated by spatial modules. Gradientweighted class activation mapping (Grad-CAM) (Selvaraju et al., 2017) was used to visualize the interest region of SA and GA in Fig. 4(C) and Appendix A.3. The original data was largely affected by the response on the frontal lobe. Surprisingly, self and graph attention focuses on the temporal and occipital lobes, providing implicit evidence that object recognition from EEG is a feasible endeavor."
        },
        {
            "heading": "5 DISCUSSION AND CONCLUSION",
            "text": "There have been some studies achieving commendable results in image reconstruction with fMRI. We turn to more convenient and fast EEG signals and focus on the object recognition tasks, in which the semantic information is the significant gain by natural image decoding compared to visual decoding of contrast, color, etc. We have tried to demonstrate the feasibility and plausibility of EEG-based image decoding from three folds, zero-shot classification performance, detailed resolving of the brain activity, and model interpreting. There are also some limitations of this paper. Firstly, we observed that a stable response required multiple repetitions. This could be attributed to the brief 100 ms stimulus duration, which may make it susceptible to being missed or disrupted by stimuli before and after. We will aim to identify a more optimal window length for stimulus presentation. Secondly, we have yet to capture useful information from the gamma band, which deserves further investigation.\nIn conclusion, we propose a self-supervised framework to decode natural images from EEG for object recognition. Our framework has achieved remarkable results in zero-shot tasks with rich biological evidence. The results provide new inspiration for practical brain-computer interfaces."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported in part by the National Natural Science Foundation of China under Grant U2241208, and the Key Research and Development Program of Ningxia under Grant 2023BEG02063."
        }
    ],
    "year": 2024
}