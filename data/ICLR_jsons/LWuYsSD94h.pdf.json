{
    "abstractText": "We investigate learning the equilibria in non-stationary multi-agent systems and address the challenges that differentiate multi-agent learning from single-agent learning. Specifically, we focus on games with bandit feedback, where testing an equilibrium can result in substantial regret even when the gap to be tested is small, and the existence of multiple optimal solutions (equilibria) in stationary games poses extra challenges. To overcome these obstacles, we propose a versatile black-box approach applicable to a broad spectrum of problems, such as generalsum games, potential games, and Markov games, when equipped with appropriate learning and testing oracles for stationary environments. Our algorithms can achieve \u00d5 ( \u2206T 3/4 ) regret when the degree of nonstationarity, as measured by total variation \u2206, is known, and \u00d5 ( \u2206T 4/5 ) regret when \u2206 is unknown, where T is the number of rounds. Meanwhile, our algorithm inherits the favorable dependence on number of agents from the oracles. As a side contribution that may be independent of interest, we show how to test for various types of equilibria by a black-box reduction to single-agent learning, which includes Nash equilibria, correlated equilibria, and coarse correlated equilibria.",
    "authors": [],
    "id": "SP:547a6cd1b76d90d6f68cd342543ce6266024cf8f",
    "references": [
        {
            "authors": [
                "Ilan Adler"
            ],
            "title": "The equivalence of linear programs and zero-sum games",
            "venue": "International Journal of Game Theory,",
            "year": 2013
        },
        {
            "authors": [
                "Ioannis Anagnostides",
                "Ioannis Panageas",
                "Gabriele Farina",
                "Tuomas Sandholm"
            ],
            "title": "On the convergence of no-regret learning dynamics in time-varying games",
            "venue": "arXiv preprint arXiv:2301.11241,",
            "year": 2023
        },
        {
            "authors": [
                "Peter Auer",
                "Nicolo Cesa-Bianchi",
                "Yoav Freund",
                "Robert E Schapire"
            ],
            "title": "The nonstochastic multiarmed bandit problem",
            "venue": "SIAM journal on computing,",
            "year": 2002
        },
        {
            "authors": [
                "Peter Auer",
                "Pratik Gajane",
                "Ronald Ortner"
            ],
            "title": "Adaptively tracking the best bandit arm with an unknown number of distribution changes",
            "venue": "In Conference on Learning Theory,",
            "year": 2019
        },
        {
            "authors": [
                "Yu Bai",
                "Chi Jin",
                "Tiancheng Yu"
            ],
            "title": "Near-optimal reinforcement learning with self-play",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yu Bai",
                "Chi Jin",
                "Song Mei",
                "Tiancheng Yu"
            ],
            "title": "Near-optimal learning of extensive-form games with imperfect information",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Omar Besbes",
                "Yonatan Gur",
                "Assaf Zeevi"
            ],
            "title": "Stochastic multi-armed-bandit problem with nonstationary rewards",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Adrian Rivera Cardoso",
                "Jacob Abernethy",
                "He Wang",
                "Huan Xu"
            ],
            "title": "Competing against nash equilibria in adversarially changing zero-sum games",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Xi Chen",
                "Xiaotie Deng",
                "Shang-Hua Teng"
            ],
            "title": "Settling the complexity of computing two-player nash equilibria",
            "venue": "Journal of the ACM (JACM),",
            "year": 2009
        },
        {
            "authors": [
                "Yifang Chen",
                "Chung-Wei Lee",
                "Haipeng Luo",
                "Chen-Yu Wei"
            ],
            "title": "A new algorithm for non-stationary contextual bandits: Efficient, optimal and parameter-free",
            "venue": "In Conference on Learning Theory,",
            "year": 2019
        },
        {
            "authors": [
                "Wang Chi Cheung",
                "David Simchi-Levi",
                "Ruihao Zhu"
            ],
            "title": "Reinforcement learning for non-stationary markov decision processes: The blessing of (more) optimism",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Wang Chi Cheung",
                "David Simchi-Levi",
                "Ruihao Zhu"
            ],
            "title": "Hedging the drift: Learning to optimize under nonstationarity",
            "venue": "Management Science,",
            "year": 2022
        },
        {
            "authors": [
                "Yun Kuen Cheung",
                "Georgios Piliouras"
            ],
            "title": "Chaos, extremism and optimism: Volume analysis of learning in games, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Qiwen Cui",
                "Zhihan Xiong",
                "Maryam Fazel",
                "Simon S Du"
            ],
            "title": "Learning in congestion games with bandit feedback",
            "venue": "arXiv preprint arXiv:2206.01880,",
            "year": 2022
        },
        {
            "authors": [
                "Qiwen Cui",
                "Kaiqing Zhang",
                "Simon S Du"
            ],
            "title": "Breaking the curse of multiagents in a large state space: Rl in markov games with independent linear function approximation",
            "venue": "arXiv preprint arXiv:2302.03673,",
            "year": 2023
        },
        {
            "authors": [
                "Constantinos Daskalakis",
                "Noah Golowich",
                "Kaiqing Zhang"
            ],
            "title": "The complexity of markov equilibrium in stochastic games",
            "venue": "arXiv preprint arXiv:2204.03991,",
            "year": 2022
        },
        {
            "authors": [
                "Christian Schroeder de Witt",
                "Bei Peng",
                "Pierre-Alexandre Kamienny",
                "Philip Torr",
                "Wendelin B\u00f6hmer",
                "Shimon Whiteson"
            ],
            "title": "Deep multi-agent reinforcement learning for decentralized continuous cooperative control",
            "venue": "arXiv preprint arXiv:2003.06709,",
            "year": 2003
        },
        {
            "authors": [
                "Dongsheng Ding",
                "Chen-Yu Wei",
                "Kaiqing Zhang",
                "Mihailo Jovanovic"
            ],
            "title": "Independent policy gradient for large-scale markov potential games: Sharper rates, function approximation, and game-agnostic convergence",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Benoit Duvocelle",
                "Panayotis Mertikopoulos",
                "Mathias Staudigl",
                "Dries Vermeulen"
            ],
            "title": "Multiagent online learning in time-varying games",
            "venue": "Mathematics of Operations Research,",
            "year": 2022
        },
        {
            "authors": [
                "Aur\u00e9lien Garivier",
                "Eric Moulines"
            ],
            "title": "On upper-confidence bound policies for switching bandit problems",
            "venue": "Algorithmic Learning Theory,",
            "year": 2011
        },
        {
            "authors": [
                "Yingya Guo",
                "Qi Tang",
                "Yulong Ma",
                "Han Tian",
                "Kai Chen"
            ],
            "title": "Distributed traffic engineering in hybrid software defined networks: A multi-agent reinforcement learning framework, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Am\u00e9lie Heliou",
                "Johanne Cohen",
                "Panayotis Mertikopoulos"
            ],
            "title": "Learning with bandit feedback in potential games",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Shinji Ito"
            ],
            "title": "A tight lower bound and efficient reduction for swap regret",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Chi Jin",
                "Qinghua Liu",
                "Yuanhao Wang",
                "Tiancheng Yu"
            ],
            "title": "V-learning\u2013a simple, efficient, decentralized algorithm for multiagent rl",
            "venue": "arXiv preprint arXiv:2110.14555,",
            "year": 2021
        },
        {
            "authors": [
                "Yun Geon Kim",
                "Seokgi Lee",
                "Jiyeon Son",
                "Heechul Bae",
                "Byung Do Chung"
            ],
            "title": "Multi-agent system and reinforcement learning approach for distributed intelligence in a flexible smart manufacturing system",
            "venue": "Journal of Manufacturing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tadashi Kozuno",
                "Pierre M\u00e9nard",
                "R\u00e9mi Munos",
                "Michal Valko"
            ],
            "title": "Model-free learning for two-player zero-sum partially observable markov games with perfect recall",
            "venue": "arXiv preprint arXiv:2106.06279,",
            "year": 2021
        },
        {
            "authors": [
                "Stefanos Leonardos",
                "Will Overman",
                "Ioannis Panageas",
                "Georgios Piliouras"
            ],
            "title": "Global convergence of multi-agent policy gradient in markov potential games",
            "venue": "arXiv preprint arXiv:2106.01969,",
            "year": 2021
        },
        {
            "authors": [
                "Qinghua Liu",
                "Tiancheng Yu",
                "Yu Bai",
                "Chi Jin"
            ],
            "title": "A sharp analysis of model-based reinforcement learning with self-play",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Qinghua Liu",
                "Csaba Szepesv\u00e1ri",
                "Chi Jin"
            ],
            "title": "Sample-efficient reinforcement learning of partially observable markov games",
            "venue": "arXiv preprint arXiv:2206.01315,",
            "year": 2022
        },
        {
            "authors": [
                "Weichao Mao",
                "Kaiqing Zhang",
                "Ruihao Zhu",
                "David Simchi-Levi",
                "Tamer Basar"
            ],
            "title": "Near-optimal model-free reinforcement learning in non-stationary episodic mdps",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Weichao Mao",
                "Lin Yang",
                "Kaiqing Zhang",
                "Tamer Basar"
            ],
            "title": "On improving model-free algorithms for decentralized multi-agent reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Deepan Muthirayan",
                "Chinmay Maheshwari",
                "Pramod P. Khargonekar",
                "Shankar Sastry"
            ],
            "title": "Competing bandits in time varying matching markets, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Noam Nisan",
                "Tim Roughgarden",
                "Eva Tardos",
                "Vijay V Vazirani"
            ],
            "title": "Algorithmic game theory",
            "venue": "Cambridge university press,",
            "year": 2007
        },
        {
            "authors": [
                "Ioannis Panageas",
                "Stratis Skoulakis",
                "Luca Viano",
                "Xiao Wang",
                "Volkan Cevher"
            ],
            "title": "Semi bandit dynamics in congestion games: Convergence to nash equilibrium and no-regret guarantees, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Jorge I Poveda",
                "Miroslav Krsti\u0107",
                "Tamer Basar"
            ],
            "title": "Fixed-time seeking and tracking of time-varying nash equilibria in noncooperative games",
            "venue": "American Control Conference (ACC),",
            "year": 2022
        },
        {
            "authors": [
                "Ziang Song",
                "Song Mei",
                "Yu Bai"
            ],
            "title": "When can we learn general-sum markov games with a large number of players sample-efficiently",
            "venue": "arXiv preprint arXiv:2110.04184,",
            "year": 2021
        },
        {
            "authors": [
                "Ziang Song",
                "Song Mei",
                "Yu Bai"
            ],
            "title": "Sample-efficient learning of correlated equilibria in extensiveform games",
            "venue": "arXiv preprint arXiv:2205.07223,",
            "year": 2022
        },
        {
            "authors": [
                "Zitao Song",
                "Xuyang Jin",
                "Chenliang Li"
            ],
            "title": "Safe-finrl: A low bias and variance deep reinforcement learning implementation for high-freq stock trading, 2022b",
            "year": 2022
        },
        {
            "authors": [
                "Brian Swenson",
                "Ryan Murray",
                "Soummya Kar"
            ],
            "title": "On best-response dynamics in potential games, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Igor Babuschkin",
                "Wojciech M Czarnecki",
                "Micha\u00ebl Mathieu",
                "Andrew Dudzik",
                "Junyoung Chung",
                "David H Choi",
                "Richard Powell",
                "Timo Ewalds",
                "Petko Georgiev"
            ],
            "title": "Grandmaster level in starcraft ii using multi-agent reinforcement learning",
            "year": 2019
        },
        {
            "authors": [
                "Yuanhao Wang",
                "Qinghua Liu",
                "Yu Bai",
                "Chi Jin"
            ],
            "title": "Breaking the curse of multiagency: Provably efficient decentralized multi-agent rl with function approximation",
            "venue": "arXiv preprint arXiv:2302.06606,",
            "year": 2023
        },
        {
            "authors": [
                "Chen-Yu Wei",
                "Haipeng Luo"
            ],
            "title": "Non-stationary reinforcement learning without prior knowledge: An optimal black-box approach",
            "venue": "In Conference on Learning Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiqing Zhang",
                "Zhuoran Yang",
                "Tamer Basar"
            ],
            "title": "Multi-agent reinforcement learning: A selective overview of theories and algorithms",
            "venue": "Handbook of reinforcement learning and control,",
            "year": 2021
        },
        {
            "authors": [
                "Mengxiao Zhang",
                "Peng Zhao",
                "Haipeng Luo",
                "Zhi-Hua Zhou"
            ],
            "title": "No-regret learning in time-varying zero-sum games",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Peng Zhao",
                "Lijun Zhang",
                "Yuan Jiang",
                "Zhi-Hua Zhou"
            ],
            "title": "A simple approach for non-stationary linear bandits",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Liu"
            ],
            "title": "2021), general-sum Markov games (Jin et",
            "year": 2022
        },
        {
            "authors": [
                "Cheung"
            ],
            "title": "2020) develop a Bandit-overReinforcement-Learning framework for non-stationary RL that generalizes the Bandit-over-Bandit",
            "venue": "non-stationary bandits (Auer et al.,",
            "year": 2002
        },
        {
            "authors": [
                "Cardoso"
            ],
            "title": "zero-sum, general-sum and potential games, and shares a similar dynamic regret notion as ours",
            "year": 2019
        },
        {
            "authors": [
                "Auer"
            ],
            "title": "Here we restate the idea as follows. Consider the multi-armed bandit setting. There are K arms, T episodes and L abrupt changes. The regret can be decomposed as \u2022 Most of the time, we run the standard UCB",
            "year": 2019
        },
        {
            "authors": [
                "Cui"
            ],
            "title": "2023) to the non-stationary game case. For simplicity, we will follow the proof in Cui et al. (2023) in general and only point out critical changes. Note that they use k as epoch index while we have been using k as episode index. For consistency, we will use \u03ba as the episode index in this section. As a reminder, we will use r\u03ba, P\u03ba and M\u03ba to denote the reward function, the transition kernel and the game at episode \u03ba",
            "year": 2023
        },
        {
            "authors": [
                "\u2223\u2223\u2223\u2223 \u2264 \u2206h"
            ],
            "title": "The rest of the proof follows Cui et al. (2023)",
            "venue": "Lemma 12. (Modified Lemma 40 in Cui et al",
            "year": 2023
        },
        {
            "authors": [
                "\u2206h. Proof"
            ],
            "title": "The proof follows the proof for Lemma 11. Lemma 13. (Modified Lemma 41 in Cui et al. (2023)) Under the good event G, for all k \u2208 [K",
            "year": 2023
        },
        {
            "authors": [
                "Cui"
            ],
            "title": "2023) holds directly with the modified update rule. As a result, following Theorem 4 in Cui et al. (2023), the same sample complexity result holds for learning an \u03b5+\u00d5(HS\u2206)CCE",
            "year": 2023
        },
        {
            "authors": [
                "HS. E"
            ],
            "title": "MARKOV POTENTIAL GAMES (NE) This setting is rather straightforward. Algorithm 3 in Song et al. (2021) serves as a base algorithm. By noticing that any weighted average of the samples of rewards shifts by no more than O(\u2206) in the non-stationary environment and by the very similar argument we made in Lemma 3 or proof",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "( \u22061/4T 3/4 ) regret when the degree of nonstationarity, as measured\nby total variation \u2206, is known, and O\u0303 ( \u22061/5T 4/5 ) regret when \u2206 is unknown, where T is the number of rounds. Meanwhile, our algorithm inherits the favorable dependence on number of agents from the oracles. As a side contribution that may be independent of interest, we show how to test for various types of equilibria by a black-box reduction to single-agent learning, which includes Nash equilibria, correlated equilibria, and coarse correlated equilibria."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Multi-agent reinforcement learning (MARL) studies the interactions of multiple agents in an unknown environment with the aim of maximizing their long-term returns (Zhang et al., 2021). This field has applications in diverse areas such as computer games (Vinyals et al., 2019), robotics (de Witt et al., 2020), and smart manufacturing (Kim et al., 2020). Although various algorithms have been developed for MARL, it is typically assumed that the underlying repeated game is stationary throughout the entire learning process. However, this assumption often fails to represent real-world scenarios where the environment is evolving throughout the learning process.\nNon-stationarity is ubiquitous. For example, in traffic routing (Guo et al., 2023), financial markets (Song et al., 2022b), and matching markets (Muthirayan et al., 2023) non-stationarity of the underlying environment plays a crucial role in multi-agent interactions. The task of learning within a nonstationary multi-agent system, while crucial, poses additional challenges when attempts are made to generalize non-stationary single-agent reinforcement learning (RL), especially for the bandit feedback case where minimal information is revealed to the agents (Anagnostides et al., 2023). In addition, the various multi-agent settings, such as zero-sum, potential, and general-sum games, along with normal-form and extensive-form games, and fully observable or partially observable Markov games, further complicate the design of specialized algorithms. Thus it is desirable to design algorithms that build on learning oracles for stationary environments, and exploit these oracles (as plug-in \u201cblack boxes\u201d) to tackle the more challenging non-stationary case.\nIn this work, we take the first step towards understanding non-stationary MARL with bandit feedback. First, we point out several challenges that differentiate non-stationary MARL from non-stationary single-agent RL, and bandit feedback from full-information feedback. Subsequently, we propose black-box algorithms with sub-linear dynamic regret in arbitrary non-stationary games, provided there is access to learning algorithms in the corresponding (near-)stationary environment. This versatile approach allows us to leverage existing algorithms for various stationary games, while facilitating seamless adaptation to future algorithms that may offer improved guarantees."
        },
        {
            "heading": "1.1 MAIN CONTRIBUTIONS AND NOVELTY",
            "text": "1. Identifying challenges in non-stationary games with bandit feedback (Section 3). First, we point out that bandit feedback is incompatible with online-learning based algorithms as the gradient of reward is hard to estimate. Then, we show that bandit feedback complicates the application of testbased algorithms as testing an arbitrary small gap can incur O(1) regret each term. Non-uniqueness of equilibria makes replay-based test difficult as well. Additionally, we point out that it is non-trivial to generalize an algorithm for non-stationary Markov games to a parameter-free version since the objective for games is very different from that of multi-armed bandits.\n2. Generic black-box approach for non-stationary games. Our approach is a black-box reduction that can transform any base algorithm designed for (near-)stationary games into an algorithm capable of learning in a non-stationary environment. This approach inherits favorable properties of the base algorithm, like breaking the curse of multi-agents, and directly adapts to future algorithmic advances.\n3. Restart-based algorithm when non-stationarity budget is known (Section 4). When we know a bound on the degree of non-stationarity, often measured by number of switches or total variation (which from here on, we refer to as the \u201cnonstationarity budget\u201d), we design a simple restart-based algorithm achieving sublinear dynamic equilibrium regret of O\u0303(L1/4T 3/4) or O\u0303(\u22061/4T 3/4), where L is the switching number and \u2206 is the total variation non-stationarity budget. In words, this result implies that all the players follow a near-equilibrium strategy in most episodes.\n4. Multi-scale testing algorithm when non-stationarity budget is unknown (Section 5). We also propose a multi-scale testing algorithm to optimize the regret when the non-stationarity budget is unknown, which can adaptively avoid the strategy deviating from equilibrium for too many rounds. The algorithm achieves the same O\u0303(L1/4T 3/4) regret for unknown switching number L, and a marginally higher O\u0303(\u22061/5T 4/5) regret for unknown total variation budget \u2206. The testing algorithms are newly designed and the scheduling is specially designed for the PAC assumptions, which is different from that in Wei & Luo (2021) where regret assumptions are made.\nWhile the ultimate goal is to design no-regret algorithms for each agent, i.e. achieving no-regret no matter what policy other players adopt (like Panageas et al. (2023)), our setting is already applicable in various real-world cases even without yet achieving this desired property, this is discussed with a concrete example below. We leave the problem of finding no-regret algorithms for each individual for future work.\nExample (traffic routing with navigation). In traffic routing using navigation applications (Guo et al., 2023), being able to track Nash Equilibrium is advantageous. Assume all drivers use the same navigation application which runs our algorithm. It is reasonable to assume that drivers adhere to the application\u2019s suggestions. After following the route recommended by the application, the drivers all find that their routes are not improvable because all drivers are committing to the equilibrium; this makes drivers satisfied with the algorithm\u2019s recommendation."
        },
        {
            "heading": "1.2 RELATED WORK",
            "text": "Due to the space limit, we defer the more comprehensive related work to the appendix.\n(Stationary) Multi-agent reinforcement learning. Numerous works have been devoted to learning equilibria in (stationary) multi-agent systems, including zero-sum Markov games (Bai et al., 2020), general-sum Markov games (Jin et al., 2021), Markov potential games (Cui et al., 2023), congestion games (Cui et al., 2022), extensive-form games (Kozuno et al., 2021), and partially observable Markov games (Liu et al., 2022). These works aim to learn equilibria with bandit feedback efficiently, measured by either regret or sample complexity.\nNon-stationary (single-agent) reinforcement learning. The study of non-stationary reinforcement learning originated from non-stationary bandits (Auer et al., 2002). Auer et al. (2019) and Chen et al. (2019) first achieve near-optimal dynamic regret without knowing the non-stationary budget for bandits. The most relevant work is Wei & Luo (2021), which also proposes a black-box approach with multi-scale testing and achieves optimal regret in various single-agent settings."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "We consider the multi-agent general-sum Markov games framework, which covers a wide range of problems, and is described by the tupleM = (S,A = A1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Am, H,P, {ri}mi=1), where S is the state space with cardinality S, m is the number of the players, Ai is the action space for player i with cardinality Ai, H is the length of the horizon, P = {Ph}Hh=1 is the collection of the transition kernels. Ph : S \u00d7 A \u2192 \u2206(S) and Ph(\u00b7 | s,a) is the next state distribution given the current state s and joint action a = (a1, \u00b7 \u00b7 \u00b7 , am) at step h. ri = {rh,i}Hh=1 is the collection of random reward functions for player i; rh,i : S \u00d7 A \u2192 [0, 1], and its mean is denoted as Rh,i. At the beginning of each episode, the players start at a fixed initial state s1.1 At each step h \u2208 [H], each player observes the current state sh and chooses action ah,i simultaneously. Then player i \u2208 [m] will receive her own reward realization r\u0303h,i \u223c rh,i(sh,ah) where ah = (ah,1, \u00b7 \u00b7 \u00b7 , ah,m) and the state will transition according to sh+1 \u223c Ph(\u00b7 | sh,ah). The game will terminate when h = H + 1. We consider the bandit feedback setting where only the reward of the chosen action is revealed to the player.\nHere we discuss the generality of Markov games. When the horizonH = 1, multi-player general-sum Markov games reduce to multi-player general-sum matrix games, which include zero-sum games, potential games, congestion games, etc. (Nisan et al., 2007). If we pose different assumptions on the Markov game structure, we can obtain zero-sum Markov games (Bai et al., 2020), Markov potential games (Leonardos et al., 2021), extensive-form games (Kozuno et al., 2021). If the state sh is not directly observable, the Markov games are modeled by partially observable Markov games (Liu et al., 2022). A detailed section on preliminaries for different games is deferred to the appendix.\nPolicy. A Markov joint policy is defined as \u03c0 = {\u03c0h}Hh=1 where \u03c0h : S \u2192 \u2206(A) is the policy at step h. We will use \u03c0\u2212i to denote that all players except for player i are following policy \u03c0. A special case of Markov joint policy is Markov product policy, which satisfies that there exist policies {\u03c0i}mi=1 such that for all h \u2208 [H] and (s,a) \u2208 S \u00d7A, we have \u03c0h(a | s) = \u220fm i=1 \u03c0h,i(ai | s), where \u03c0i = {\u03c0h,i}Hh=1 is the collection of Markov policies \u03c0h,i : S \u2192 \u2206(Ai) for player i. In words, a Markov product policy can be factorized into individual policies such that they are uncorrelated.\nValue function. Given a Markov game M \u2208 M and a policy \u03c0, the value function for player i is defined as VMi (\u03c0) := E\u03c0 [\u2211H h=1 rh,i(sh,ah) \u2223\u2223\u2223M], where the expectation is taken over the randomness in both the policy and the environment.\nBest response and strategy modification. Given a policy \u03c0 and Markov game M , the best response value for player i is VMi (\u2020, \u03c0\u2212i) := max\u03c0\u2032i\u2208\u03a0i V M i (\u03c0 \u2032 i, \u03c0\u2212i), which is the maximum achievable expected return for player i if all the other players are following \u03c0\u2212i. Equivalently, best response is the optimal policy in the induced Markov decision process (MDP), i.e., Markov game with only one player.\nA strategy modification \u03c8i = {\u03c8h,i}Hh=1 is a collection of mappings \u03c8h,i : S \u00d7Ai \u2192 Ai that maps the joint state-action space to the action space.2 For policy \u03c0, \u03c8i \u22c4 \u03c0 is the modified policy such that\n(\u03c8i \u22c4 \u03c0)h(a | s) = \u2211\na\u2032:\u03c8h,i(a\u2032i|s)=ai,a\u2032\u2212i=a\u2212i\n\u03c0h(a \u2032 | s).\nIn other words, \u03c8i \u22c4 \u03c0 is a policy such that if \u03c0 assigns each player j a random action aj at state s and step h, then \u03c8i \u22c4 \u03c0 assigns action \u03c8h,i(ai | s) to player i while all the other players are following the action assigned by policy. We will use \u03a8i to denote all the possible strategy modifications for player i. As \u03a8i contains all the constant strategy modifications, we have\nmax \u03c8i\u2208\u03a8i VMi (\u03c8i \u22c4 \u03c0) \u2265 max \u03c0\u2032i VMi (\u03c0 \u2032 i, \u03c0\u2212i) = V M i (\u2020, \u03c0\u2212i),\nwhich means that the best strategy modification is always no worse than the best response."
        },
        {
            "heading": "Notions of equilibria.",
            "text": "1It is straightforward to generalize to stochastic initial state by adding a dummy state s0 that transition to the random initial state.\n2We only consider deterministic strategy modification as the optimal strategy modification can always be deterministic (Jin et al., 2021).\nDefinition 1. For Markov game M , policy \u03c0 is an \u03f5-approximate Nash equilibrium (NE) if it is a product policy and\nNEGapM (\u03c0) = max i\u2208[m]\n( VMi (\u2020, \u03c0\u2212i)\u2212 VMi (\u03c0) ) \u2264 \u03f5.\nLearning Nash equilibrium (NE) is neither computationally nor statistically efficient for general-sum normal-form games (Chen et al., 2009), while it is tractable for games with special structures, such as potential games (Monderer & Shapley, 1996) and two-player zero-sum games (Adler, 2013). Definition 2. For Markov game M , policy \u03c0 is an \u03f5-approximate coarse correlated equilibrium (CCE) if\nCCEGapM (\u03c0) = max i\u2208[m]\n( VMi (\u2020, \u03c0\u2212i)\u2212 VMi (\u03c0) ) \u2264 \u03f5.\nThe only difference between CCE and NE is that CCE is not required to be a product policy. This relaxation allows tractable algorithms for learning CCE. Definition 3. For Markov game M , policy \u03c0 is an \u03f5-approximate correlated equilibrium (CE) if\nCEGapM (\u03c0) = max i\u2208[m] ( max \u03c8i\u2208\u03a8i VMi (\u03c8i \u22c4 \u03c0)\u2212 VMi (\u03c0) ) \u2264 \u03f5.\nCorrelated equilibrium generalizes the best response used in CCE to best strategy modification. It is known that each NE is a CE and each CE is a CCE. For conciseness, we use \u03f5-EQ to denote \u03f5-approximate NE/CE/CCE.\nNon-stationarity measure. Here we formalize the non-stationary Markov game. There are T total episodes and at each episode t, the players are following some policy \u03c0t an unknown Markov game M t. The non-stationarity degree of the environment is measured by the cumulative difference between two consecutive models, defined as follows. Definition 4. The non-stationarity degree of Markov games (M1,M2, \u00b7 \u00b7 \u00b7 ,MT ) is measured by total variation \u2206 or number of switches L, which are respectively defined as\n\u2206 = T\u22121\u2211 t=1 \u2225\u2225M t+1 \u2212M t\u2225\u2225 1 , L = T\u22121\u2211 t=1 1[M t \u0338=M t+1].\nHere, the total variation distance between two Markov games is defined as\n\u2225\u2225M \u2212M \u2032\u2225\u2225 1 := H\u2211 h=1 ( max s\u2208S,a\u2208A \u2225\u2225\u2225PMh (\u00b7|s,a)\u2212 PM\u2032h (\u00b7|s,a)\u2225\u2225\u2225 1 + max s\u2208S,a\u2208A,i\u2208[m] \u2223\u2223\u2223RMh,i(s,a)\u2212RM\u2032h,i (s,a)\u2223\u2223\u2223) We also define\n\u2206[t1,t2] = t2\u22121\u2211 t=t1 \u2225\u2225M t+1 \u2212M t\u2225\u2225 1 , L[t1,t2] = t2\u22121\u2211 t=t1 1[M t \u0338= M t+1].\nWhen the non-stationarity measure go to zero, we go back to the traditional stationary setting.\nDynamic regret. We generalize the standard dynamic regret in non-stationary single-agent RL to non-stationary MARL. Definition 5. The dynamic equilibrium regret is defined as\nRegret(T ) = T\u2211 t=1 GapM t (\u03c0t),\nwhere Gap(\u00b7) can be NEGap, CCEGap or CEGap.\nA small dynamic regret implies that for most episodes t \u2208 [T ], the policy \u03c0t is an approximate equilibrium for model M t. The same dynamic regret is used in Anagnostides et al. (2023) for matrix games. In the literature, Cardoso et al. (2019) and Zhang et al. (2022) propose NE-regret and dynamic NE-regret for two-player zero-sum games where the comparator is the best NE value in hindsight\nand the best dynamic NE value. However, these regret notions cannot be generalized to general-sum games as the NE/CE/CCE values become non-unique. Zhang et al. (2022) also considers duality gap as a performance measure, which coincides with our dynamic regret where Gap is NEGap.\nBase algorithms. Our algorithm will use black-box oracles that can learn and test equilibria in a (near-)stationary environment. Details of the base algorithms are shown in Appendix. Assumption 1. (PAC guarantee for learning equilibrium) We assume that we have access to an oracle LEARN_EQ such that with probability 1\u2212 \u03b4, in an environment with non-stationarity \u2206 as defined in Definition 4, it can output an (\u03f5+ c\u22061 \u2206)-EQ of a game with at most C1(\u03f5, \u03b4) samples. Assumption 2. (PAC guarantee for testing equilibrium) We assume that we have access to an oracle TEST_EQ such that given a policy \u03c0, with probability 1 \u2212 \u03b4, in an environment with nonstationarity \u2206 as defined in Definition 4, it outputs False when \u03c0 is not a (2\u03f5 + c\u22062 \u2206)-EQ for all t = 1, . . . , C2(\u03f5, \u03b4) and outputs True when \u03c0 is an (\u03f5\u2212 c\u22062 \u2206)-EQ for all t = 1, . . . , C2(\u03f5, \u03b4).\nThere exist various algorithms (see Table 1) providing PAC guarantees for learning equilibrium in stationary games, which satisfies Assumption 1 when non-stationarity degree \u2206 = 0. We will show that most of these algorithms enjoy an additive error w.r.t. non-stationarity degree \u2206 in Appendix E and discuss how to construct oracles satisfying Assumption 2 in Section 5.1. For simplicity, we will omit \u03b4 in C1(\u03f5, \u03b4) and C2(\u03f5, \u03b4) as they only have poly-logarithmic dependence on \u03b4 for all the oracle realizations in this work. Furthermore, since the dependence of C1(\u03f5), C2(\u03f5) on \u03f5 are all polynomial, we denote C1(\u03f5) = c1\u03f5\u03b1, C2(\u03f5) = c2\u03f5\u22122. Here c1, c2 does not depend on \u03f5 and \u03b1 is a constant depending on the oracle algorithm. In Table 1, \u03b1 = \u22122 or \u03b1 = \u22123, where \u03b1 is the exponent in C1(\u03f5)."
        },
        {
            "heading": "3 CHALLENGES IN NON-STATIONARY GAMES",
            "text": "In this section, we discuss the major difficulties generalizing single-agent non-stationary algorithms to non-stationary Markov games. There are two major lines of work in the single-agent setting. The first line of work uses online learning techniques to tackle non-stationarity. There exist works generalizing online learning algorithms to the multi-agent setting. However most of them apply only to the full-information setting. In the bandit feedback setting, it is hard to estimate the gradient of the objective function. The other line of work uses explicit tests to determine notable changes of the environment and restart the whole algorithm accordingly. This paper also adpots this paradigm.\nThe first type of test is to play a sub-optimal action a consecutively to determine whether it has become optimal (Auer et al., 2019; Chen et al., 2019). For simplicity, let us think of learning NE in the environment with abrupt changes (switching number as the non-stationary measure). In order to assure a has not become a new optimal action, one needs to spend 1/D2 steps to play a and secure its value up to confidence bound D where D is the suboptimality.3 The regret incurred in this testing process is D \u00b7 1/D2 = 1/D. In the multi-agent setting, if one wants to repeat the process by testing\n31/D2 is the statistical lower bound.\n(a\u2032i, a\u2212i) to assure a is a NE, the timesteps needed is still 1/D 2 where D is the empirical reward difference of (a\u2032i, a\u2212i) and a. However, the gap of (a \u2032 i, a\u2212i) depends on its own unilateral deviations, which can be O(1) in general. Hence the regret incurred can be 1/D2, sabotaging the test process (example in Figure 1) and greatly increase the regret.\nThe second type of test restarts the learning algorithm for a small amount of time and checks for abnormality in the replay (Wei & Luo, 2021). In the multi-agent setting, since equilibrium is not unique in all games, different runs of the same algorithm can converge to different equilibria even in a stationary environment. Hence test of this type fails to detect abnormality in the base algorithm.\nAnother method worth mentioning was invented in Garivier & Moulines (2011). This method proposes to forget old history through putting a discount weight on old feedback or imposing a sliding window based on which we calculate the empirical estimate of value of actions. There is no obvious obstacle in generalizing it to the multi-agent setting but it is hard to derive a parameter-free version. Cheung et al. (2020) uses the Bandit-Over-RL technique to get a parameter-free version for the single-agent setting based on the sliding-window idea. However, the Bandit-Over-RL technique does not generalize to the multi-agent setting as the learning objective is totally different. A more detailed version of the challanges mentioned is presented in the Appendix B."
        },
        {
            "heading": "4 WARM-UP:",
            "text": ""
        },
        {
            "heading": "KNOWN NON-STATIONARY BUDGET",
            "text": "We first present an algorithm for MARL against non-stationary environments with known nonstationarity budget to serve as a starting point.\nAlgorithm 1 Restarted Explore-then-Commit for Non-stationary MARL 1: Input: number of episodes T ; non-stationarity budget \u2206; confidence level \u03b4; parameter T1 2: while episode T is not reached do 3: Run LEARN_EQ with accuracy \u03f5 and confidence level \u03b4, and receive the output \u03c0. 4: Execute \u03c0 for T1 episodes.\nInitially, the algorithm starts a LEARN_EQ algorithm, intending to learn an \u03f5-EQ policy \u03c0. After that, it commits to \u03c0 for T1 episodes. Subsequently, the algorithm repeats this learn-then-commit pattern until the end. The restart mechanism guarantees that the non-stationarity in the environment can at most affect T1 episodes. By carefully tuning T1, we can achieve a sublinear regret. This algorithm admits a performance guarantee as follows. Proposition 1. With probability 1\u2212 T\u03b4, the regret of Algorithm 1 satisfies\nRegret(T ) \u2264 4TC1(\u03f5) T1\n+ T\u03f5+ 2max { c\u22061 , H } T1\u2206.\nRemark 1. Let us look at the meaning of each term in this bound. The first term comes from all LEARN_EQ. The second and third terms come from committing to the learned policy.\nCorollary 1. With probability 1\u2212 T\u03b4, the regret of Algorithm 1 satisfies\nRegret(T ) \u2264\n{ 13 ( \u2206c1max { c\u22061 , H })1/4 T 3/4, \u03b1 = \u22122,\n13 ( \u2206c1max { c\u22061 , H })1/5 T 4/5, \u03b1 = \u22123,\nby setting\nT1 =\n\u2308\u221a TC1(\u03f5)\nmax { c\u22061 , H } \u2206\n\u2309 , \u03f5 = { ( \u2206c1max { c\u22061 , H } /T )1/4\n, \u03b1 = \u22122,( \u2206c1max { c\u22061 , H } /T )1/5 , \u03b1 = \u22123.\nExample 1. As a concrete example, for learning CCE in general-sum Markov games, Algorithm 1 achieves O(A1/4max\u22061/4T 3/4) regret. We can see that this algorithm breaks the curse of multi-agents (dependence on the number of players) which is a nice property inherited from the base algorithm. In addition, as long as the base algorithm is decentralized, Algorithm 1 will also be decentralized. When the environment is stationary, Algorithm 1 reduces to the vanilla Explore-then-Commit algorithm, achieving O(A1/4maxT 3/4) regret."
        },
        {
            "heading": "5 UNKNOWN NON-STATIONARITY BUDGET",
            "text": "In this section, we generalize Algorithm 1 to a parameter-free version, which achieves a similar regret bound without the knowledge of the non-stationarity budget and the time horizon T . If the non-stationarity budget is unknown, we cannot determine the appropriate rate to restart in advance as in Algorithm 1. Hence, we use multi-scale testing to monitor the performance of the committed policy and restart adaptively."
        },
        {
            "heading": "5.1 BLACK-BOX ALGORITHMS FOR TESTING EQUILIBRIA",
            "text": "In this section, we present the construction of the testing algorithms TEST_EQ that satisfies Assumption 2 by a black-box reduction to single-agent algorithms, which is able to test whether a policy is an equilibrium in a (near-)stationary game. We make the following assumption on the single-agent learning oracle which is implied by Assumption 1 Wei & Luo (2021) used for single-agent learning. Assumption 3. (PAC guarantee for single-agent RL) We assume that we have access to an oracle LEARN_OP such that with probability 1\u2212 \u03b4, in a single-agent environment with non-stationarity \u2206, it can output an (\u03f5+ c\u22063 \u2206)-optimal policy with C3(\u03f5, \u03b4) samples.\nThe construction of TEST_EQ is described in Algorithm 2. Note that here we only consider Markov policies and the best response to a Markov policy is the optimal policy in the induced single-agent MDP. First, we get an estimate of Vi(\u03c0) for all i up to an error bound of \u03f5/6 by standard concentration inequalities. Then, for each player i, we run LEARN_OP and by Assumption 3, \u03c0\u2032i is an \u03f5/6-optimal policy in the MDP induced by other players following \u03c0\u2212i. In other words, \u03c0\u2032i is an \u03f5/6-best response to \u03c0\u2212i. After that we run (\u03c0\u2032i, \u03c0\u2212i) and estimate the policy value V\u0302i(\u03c0 \u2032 i, \u03c0\u2212i) for players i up to \u03f5/6 error bound. Finally the algorithm decides the output according to the empirical estimate of the gap.\nTo test a CE, we need to learn the best strategy modification in the induced MDP. Here we design an MDP M \u2032 such that learning the best strategy modification with random recommendation policy \u03c0 in MDP M = (S,A, P, r,H) is equivalent to learning the optimal policy in M \u2032, where the randomness in \u03c0 could be correlated with the transition. In M \u2032, the state space is S \u2032 = S \u00d7 A, the action space is A, the transition is P \u2032h((sh+1, bh+1) | (sh, bh), ah) = Ph(sh+1 | sh, \u03c0h(sh) = bh, ah) \u00b7 \u03c0h+1(bh+1 | sh+1) and the reward is r\u2032h(\u00b7 | (sh, bh), ah) = rh(\u00b7 | sh, \u03c0h(sh) = bh, ah). The following proposition shows that learning the best strategy modification to recommendation policy \u03c0 in MDP M is equivalent to learning the optimal policy in MDP M \u2032. Note that the correlation between \u03c0 and M \u2032 complicates the proof of correctness to the seemingly straightforward construction. Proposition 2. MDP M \u2032 is induced by MDP M and recommendation policy \u03c0, then the optimal policy in M \u2032 corresponds to a best strategy modification to recommendation policy \u03c0 in M .\nNote that the state space in M \u2032 is enlarged by a factor of A, which means the sample complexity for testing CE is A times larger than CCE, which coincides with the fact that the minimax swap regret is\u221a A times larger than the minimax external regret (Ito, 2020).\nProposition 3. As long as LEARN_OP satisfies Assumption 3, Algorithm 2 satisfies Assumption 2. Furthermore, there exists algorithm satisfying c\u22062 = O(H 2)."
        },
        {
            "heading": "5.2 MULTI-SCALE TEST SCHEDULING",
            "text": "In this section, we introduce how to schedule TEST_EQ during the committing phase. The scheduling is motivated by MALG in Wei & Luo (2021), with modifications to the multi-agent setting.\nWe consider a block of timesteps with length 2n for some integer n. The block starts with a LEARN_EQ with \u03f5 = 2\u2212n/4 and is followed by the committing phase when the agents commit to\nAlgorithm 2 TEST_EQ 1: Input: Joint Markov policy \u03c0, failure probability \u03b4, tolerance \u03f5. 2: Run \u03c0 for O\u0303(\u03f5\u22122) episodes and estimate the policy value V\u0302i(\u03c0) with confidence level \u03f5/6 for all\nplayers i \u2208 [m]. 3: for i = 1, 2, . . . ,m do 4: Let players [m]/{i} follow \u03c0\u2212i and player i run LEARN_OP with \u03b4 and \u03f5/6. Receive best\nreponse policy \u03c0\u2032i or best strategy modification \u03c8i \u22c4 \u03c0 for (NE,CCE) or CE. 5: Run (\u03c0\u2032i, \u03c0\u2212i) or \u03c8i \u22c4\u03c0 for O\u0303(\u03f5\u22122) episodes and estimate the best response value V\u0302i(\u03c0\u2032i, \u03c0\u2212i)\nor the best strategy modification value V\u0302i(\u03c8i \u22c4 \u03c0) with confidence level \u03f5/6 for players i. 6: if maxi\u2208[m] ( V\u0302i(\u03c0 \u2032 i, \u03c0\u2212i)\u2212 V\u0302i(\u03c0) ) \u2264 3\u03f5/2 or maxi\u2208[m] ( V\u0302i(\u03c8i \u22c4 \u03c0)\u2212 V\u0302i(\u03c0) ) \u2264 3\u03f5/2 then 7: return True 8: else 9: return False\nthe learned policy. During the committing phase, TEST_EQ starts randomly for different gaps with different probabilities at each step. That is, we intend to test larger changes more quickly by testing for them more frequently (by setting the probability higher) so that the detection is adaptive to the severity of changes. Denote the episode index in this block by \u03c4 . In the committing phase, if \u03c4 is an integer multiple of 2c+q for some q \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , Q}, with probability p(q) = 1/(\u03f5(q)2n/2) we start a test for gap \u03f5(q) = \u221a c2/2q so that the length of test is 2q , where Q, c are defined as\nQ = min {\u230a\nlog2 ( c22 n/2\u22121 )\u230b , n\u2212 c } + , c = \u2308 1 + log2 max { 5 \u221a c2, 2 log 1 \u03b4 }\u2309 .\nThe gaps we intend to test are approximately {\u221a 2\u03f5, 2\u03f5, 2 \u221a 2\u03f5, \u00b7 \u00b7 \u00b7 } . It is possible that TEST_EQ for\ndifferent \u03f5(q) are overlapped. In this case, we prioritize the running of TEST_EQ for larger \u03f5(q) and pause those for smaller \u03f5(q). After the shorter TEST_EQ ends, we resume the longer ones until they are completed. In addition, if a TEST_EQ for \u03f5(q) spans for more than 2c+q episodes, it is aborted. To better illustrate the scheduling, we construct an example shown in Figure 2. It can be proved that with high probability no TEST_EQ is aborted (Lemma 5), i.e. the 2c multiplication in length reserves enough space for all TEST_EQ. Note that the original MALG (Wei & Luo (2021)) does not work here because the length of each scheduled TEST_EQ can be reduced greatly and there is no guarantee how a TEST_EQ with reduced length would work. The scheduling is formally stated in Algorithm 1. Lemma 1. With probability 1\u2212 3QT\u03b4, the regret inside this block Regret = O\u0303 ( 2 3n/4\n+ c2 min\n{ 2 2n/3 ( c \u2206 2 \u2206[1,En] )1/3 , 2 5n/8 ( c \u2206 2 \u2206[1,En] )1/2} + 2 n/2 c 3/2 2 + 2 \u2212\u03b1n/4 c1 ) . (1)\nRemark 2. The common way to bound the regret with total variation is to divide the block into several near-stationary intervals [C1(\u03f5) + 1, 2n] = I1 \u222a I2 \u222a \u00b7 \u00b7 \u00b7 \u222a IK . In each interval the near-stationarity ensure all TEST_EQ to work properly and hence the regret is bounded. This is because if the regret is to big for a long time TEST_EQ would detect it. After that we bound K and finally bound the regret of a block using H\u00f6lder\u2019s inequality. While prior works (Chen et al., 2019) partition the intervals according to \u2206Ik = O ( |Ik|\u22121/2 ) , we set \u2206Ik = O ( max { |Ik|\u22121/2 , 2\u2212n/4 }) . This\ngreatly changes the subsequent calculations and makes the regret better in our case, please refer to the appendix for more details."
        },
        {
            "heading": "5.3 MAIN ALGORITHM",
            "text": "The main algorithm consists of blocks with doubling lengths. The first block is the shortest block that can accomodate a whole LEARN_EQ in it. The doubling structure is not only important to making the algorithm parameter free of \u2206, but also to that of T (see Appendix for more details). The performance guarantee of this algorithm is stated in Theorem 1 and Theorem 2. For simplicity, let \u2206\u0303J = c \u2206 2 \u2206J and \u2206\u030cJ = max { c\u22061 , c \u2206 2 } \u2206J . Theorem 1. With probability 1\u2212 3QT\u03b4, the regret of Algorithm 3 is\nRegret(T ) =  O\u0303 ( \u2206\u030c1/5T 4/5 + c2 min { \u2206\u03031/3T 2/3, \u2206\u03031/2T 5/8 } + ( c1 + c 3/2 2 ) \u2206\u030c2/5T 3/5 ) \u03b1 = \u22122 O\u0303 ( c1\u2206\u030c 1/5T 4/5 + c2 min { \u2206\u03031/3T 2/3, \u2206\u03031/2T 5/8 } + c 3/2 2 \u2206\u030c 2/5T 3/5 ) \u03b1 = \u22123\nAlgorithm 3 Multi-scale Testing for Non-stationary MARL 1: Input: failure probability \u03b4. 2: N \u2190 min { n | 2n \u2265 C1(2n/2) } 3: for n = N,N + 1, \u00b7 \u00b7 \u00b7 do 4: Schedule a block sized 2n according to Section 5.2 (Algorithm 1). 5: Run LEARN_EQ with accuracy \u03f5 = 2\u2212n/4 and receive \u03c0. 6: Run the committing phase according to the schedule. If any TEST_EQ returns False, go to\nLine 3 and restart the for loop.\nRemark 3. The main idea of the proof is as follows. The restarts divide the whole time horizon into consecutive segments [1, T ] = J1 \u222a J2 \u222a \u00b7 \u00b7 \u00b7 \u222a JJ . In each segment Jj between restarts, the regret can be bounded by adding up Formula 1 for all blocks as\nRegret (Jj) = O\u0303 ( |Jj |3/4 + c2 min { |Jj |2/3 \u2206\u0303Jj 1/3, |Jj |5/8 \u2206\u0303Jj 1/2 } + c 3/2 2 |Jj | 1/2 + c1 |Jj |\u2212\u03b1/4 ) .\nIt can be proved that the number of segments is bounded by J = O ( T 1/5\u2206\u030c4/5 ) . Using H\u00f6lder\u2019s\ninequality, we get the conclusion.\nTheorem 2. With probability 1\u2212 3QT\u03b4, the regret of Algorithm 3 is\nRegret(T ) =  O\u0303 ( L1/4T 3/4 + ( c1 + c 3/2 2 ) L1/2T 1/2 ) \u03b1 = \u22122 O\u0303 ( c1L 1/4T 3/4 + c 3/2 2 L 1/2T 1/2 ) \u03b1 = \u22123\nRemark 4. Algorithm 3 breaks the curse of multi-agent as long as the base algorithms do. If the base algorithm is decentralized, all players are informed to restart when a change is detected and no further communication is needed. In this sense very few extra communications are needed in Algorithm 3."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "In this work, we propose black-box reduction approaches for learning equilibria in non-stationary multi-agent reinforcement learning, both with and without knowledge of parameters, that offer favorable performance guarantees. We conclude this paper by posing two open questions. First, it remains unknown how to design algorithms with no-regret oracles, which would further minimize the regret in learning. Second, the lower bound of regret for learning in non-stationary multi-agent systems is currently unknown."
        },
        {
            "heading": "A.1 RELATED WORK",
            "text": "(Stationary) Multi-agent reinforcement learning. Numerous works have been devoted to learning equilibria in (stationary) multi-agent systems, including zero-sum Markov games (Bai et al., 2020; Liu et al., 2021), general-sum Markov games (Jin et al., 2021; Mao et al., 2022; Song et al., 2021; Daskalakis et al., 2022; Wang et al., 2023; Cui et al., 2023), Markov potential games (Leonardos et al., 2021; Song et al., 2021; Ding et al., 2022; Cui et al., 2023), congestion games (Cui et al., 2022), extensive-form games (Kozuno et al., 2021; Bai et al., 2022; Song et al., 2022a), and partially observable Markov games (Liu et al., 2022). These works aim to learn equilibria with bandit feedback efficiently, measured by either regret or sample complexity. There also exists a rich literature on asymptotic convergence of different learning dynamics in known games and non-asymptotic convergence with full-information feedback (Swenson et al. (2018); Heliou et al. (2017); Cheung & Piliouras (2020)).\nNon-stationary (single-agent) reinforcement learning. The study of non-stationary reinforcement learning originated from non-stationary bandits (Auer et al., 2002; Besbes et al., 2014; Chen et al., 2019; Zhao et al., 2020; Wei & Luo, 2021; Cheung et al., 2022; Garivier & Moulines, 2011). Auer et al. (2019) and Chen et al. (2019) first achieve near-optimal dynamic regret without knowing the non-stationary budget. For non-stationary RL, Cheung et al. (2020) develop a Bandit-overReinforcement-Learning framework for non-stationary RL that generalizes the Bandit-over-Bandit approach (Cheung et al., 2022). The most relevant work is Wei & Luo (2021), which also proposes a black-box approach with multi-scale testing and achieves optimal regret in various single-agent settings. We refer readers to Wei & Luo (2021) for a more comprehensive literature review on non-stationary reinforcement learning.\nNon-stationary multi-agent reinforcement learning. Most of the previous works have been focused on the full-information feedback setting, which is considerably easier than the bandit feedback setting as testing becomes unnecessary (Cardoso et al., 2019; Zhang et al., 2022; Anagnostides et al., 2023; Duvocelle et al., 2022; Poveda et al., 2022). For two-player zero-sum matrix games, Zhang et al. (2022) proposes a meta-algorithm over a group of base algorithms to tackle with unknown parameters. Anagnostides et al. (2023) studies the convergence of no-regret learning dynamics in non-stationary matrix games, including zero-sum, general-sum and potential games, and shares a similar dynamic regret notion as ours. Notably, Cardoso et al. (2019) also studies the bandit feedback case and aims to minimize NE-regret, while the regret is comparing with the best NE in hindsight instead of a dynamic regret."
        },
        {
            "heading": "B CHALLENGES IN NON-STATIONARY GAMES",
            "text": "In this section, we discuss the challenges in non-stationary games in more detail."
        },
        {
            "heading": "B.1 CHALLENGES IN TEST-BASED ALGORITHMS",
            "text": "The idea of achieving optimal regret using consecutive testing in a parameter-free fashion was first proposed in Auer et al. (2019). Here we restate the idea as follows. Consider the multi-armed bandit setting. There are K arms, T episodes and L abrupt changes. The regret can be decomposed as\n\u2022 Most of the time, we run the standard UCB algorithm. If we always restart the UCB algorithm right after each abrupt change, the accumulated regret is upper bounded by O (\u221a K(T/L)L ) = O (\u221a KTL ) .\n\u2022 Intending to detect changes on one arm that make the optimal arm incur D regret, the algorithm starts a test at each step with probability pD = D \u221a l/KT where l is the number of\nchanges detected thus far. The test should last nD = O(1/D2) steps to make the confidence bound no larger than D. In expectation, the test incurs pDTnD\u2206 = O ( \u2206 D \u221a lT K ) regret.\nHere \u2206 is the real gap of the detected arm. To cover all possible \u2206, we may detect for gaps of size D = D0, 2D0, 4D0, \u00b7 \u00b7 \u00b7 . D0 is the smallest gap that is worth noticing4. This incurs\nO (\u221a LT K K ) = O (\u221a KTL ) regret.\n\u2022 The expected number of episodes before we start to detect for a change of size D is D/pD = \u221a KT/l. Summing over all changes, this part incurs O (KTL) regret\nIn all, the scheme suffer O (\u221a KLT ) regret, which is optimal. In the game setting, however, the\nsecond part can become 1D0 \u221a lT KK and we will no longer have a no-regret algorithm."
        },
        {
            "heading": "B.2 CHALLENGES IN BANDIT-OVER-RL ALGORITHMS",
            "text": "The high-level idea of BORL is as follows (Cheung et al., 2020). First partition the whole time horizon T into intervals with length H . Each interval is one step for an adversarial bandit algorithm A. Inside each interval, one instance of the base algorithm is run, with the tunable parameter selected by A. The arms for A are the possible parameters of the base algorithm and the reward is the total reward from one interval. Let the action at timestep t be at and r(at) be its expected reward, a\u2217t be the optimal action at timestep t and R(w) be the expected return from one interval if we chooses parameter w. The regret can then be decomposed as\nT\u2211 t=1 [r (a\u2217t )\u2212 r (at)] =  T\u2211 t=1 r (a\u2217t )\u2212 T/H\u2211 h=1 R(w\u2217) + T/H\u2211 h=1 R(w\u2217)\u2212 T\u2211 t=1 r(at)  where w\u2217 is the best parameter. The first term is bounded by the base algorithm regret upper bound and the second term is bounded by the adversarial bandit regret guarantee. If we apply the same to minimize, for example, the Nash regret\nT\u2211 t=1 max i\u2208[m] ( VMi (\u2020, \u03c0\u2212i)\u2212 VMi (\u03c0) ) ,\nwe easily find the max hinders the same decomposition. Even if we drop the max and focus on individual regret, the decomposition is T\u2211 t=1 [ VMi (\u2020, \u03c0\u2212i)\u2212 VMi (\u03c0) ] =  T\u2211 i=1 VMi (\u2020, \u03c0\u2212i)\u2212 T/H\u2211 h=1 R(w\u2217) + T/H\u2211 h=1 R(w\u2217)\u2212 T\u2211 t=1 VMi (\u03c0)  4We can take D0 = \u221a K/T because even if each step we suffer an extra D0 regret, the total regret will still remain.\nwhere the first term loses meaning. The fundamental reason is that in MAB, at timestep t, any action is competing with a fixed action a\u2217t , while in a game, a policy \u03c0 is competing with argmax\u03c0\u2032 i VMi (\u03c0 \u2032 i, \u03c0\u2212i), which depends on \u03c0 itself. This difficulty can also be seen from Figure 1."
        },
        {
            "heading": "C OMITTED PROOFS IN SECTION 4",
            "text": "In this section, we analyze the performance of Algorithm 1. For convenience, we denote the intervals corresponding to each LEARN_EQ by I1, I2, \u00b7 \u00b7 \u00b7 , IK and the committing phases asJ1,J2, \u00b7 \u00b7 \u00b7 ,JK . The committed policy are \u03c01, \u03c02, \u00b7 \u00b7 \u00b7 , \u03c0K respectively. Here K = \u2308T/(C1(\u03f5) + T1)\u2309 and JK can be empty. Lemma 2. If x > 1, x/2 < \u2308x\u2309 < x+ 1. Remark 5. It is a basic algebraic lemma that will be used very often to get over the roundings. Lemma 3. If \u03c0 is an \u03f5-EQ of episode t, then it is also an ( \u03f5+ 2H\u2206[t,t\u2032] ) -equilibrium for any episode t\u2032 > t.\nProof. To facilitate this proof, we define some more notations. The value function of player i at timestep h0, episode t, state s is defined to be\nV \u03c0,Mh0,i (s) = E\u03c0  H\u2211 h=h0 rh,i(sh,ah) |M, sh0 = s  . (2) Here M is the model at episode t. We also denote the model at episode t\u2032 by M \u2032. We have the recursion\nV \u03c0,Mh0,i (s) = \u2211 a \u03c0(a | s) [\u2211 s\u2032 PMh0 ( s\u2032|s,a ) V \u03c0,Mh0+1,i(s \u2032) +RMh0,i(s,a) ] .\nAssume \u2223\u2223\u2223V \u03c0,Mh0+1,i(s)\u2212 V \u03c0,M \u2032h0+1,i(s)\u2223\u2223\u2223 \u2264 H H\u2211 h=h0+1 (\u2225\u2225\u2225PMh \u2212 PM \u2032h \u2225\u2225\u2225 1 + \u2225\u2225\u2225RMh \u2212RM \u2032h \u2225\u2225\u2225 1 ) , then we have\u2223\u2223\u2223V \u03c0,Mh0,i (s)\u2212 V \u03c0,M \u2032h0,i (s)\u2223\u2223\u2223 \u2264 \u2211 a \u03c0(a | s) [\u2211 s\u2032 ( PMh0 ( s\u2032|s,a ) \u2212 PM \u2032 h0 ( s\u2032|s,a )) V \u03c0,M \u2032 h0+1,i (s\u2032) ]\n+ \u2211 a \u03c0(a | s) [\u2211 s\u2032 PMh0 ( s\u2032|s,a ) ( V \u03c0,M \u2032 h0+1,i (s\u2032)\u2212 V \u03c0,Mh0+1,i(s \u2032) )]\n+ \u2211 a \u03c0(a | s) [ RMh0,i(s,a)\u2212R M \u2032 h0,i(s,a) ]\n\u2264H \u2223\u2223\u2223PMh0 (s\u2032|s,a)\u2212 PM \u2032h0 (s\u2032|s,a)\u2223\u2223\u2223+ \u2223\u2223\u2223V \u03c0,Mh0+1,i(s)\u2212 V \u03c0,M \u2032h0+1,i(s)\u2223\u2223\u2223+ \u2223\u2223\u2223RMh0,i(s,a)\u2212RM \u2032h0,i(s,a)\u2223\u2223\u2223 \u2264H H\u2211\nh=h0\n(\u2225\u2225\u2225PMh \u2212 PM \u2032h \u2225\u2225\u2225 1 + \u2225\u2225\u2225RMh \u2212RM \u2032h \u2225\u2225\u2225 1 ) .\nSince the assumption holds trivially for h0 = H , by induction we get\u2223\u2223\u2223V \u03c0,M1 (s)\u2212 V \u03c0,M \u20321 (s)\u2223\u2223\u2223 \u2264 \u2206[t,t\u2032] . Finally by definition of the equilibria, we get the conclusion.\nLemma 4. With probability 1\u2212 T\u03b4, \u03c0k is ( \u03f5+ c\u22061 \u2206Ik ) -approximate equilibrium in the last episode of Ik for all k \u2208 [K].\nProof. This is by the union bound and K \u2264 T .\nThe following theorem is conditioned on this high-probability event.\nProposition 1. With probability 1\u2212 T\u03b4, the regret of Algorithm 1 satisfies\nRegret(T ) \u2264 4TC1(\u03f5) T1\n+ T\u03f5+ 2max { c\u22061 , H } T1\u2206.\nProof. According to Assumption 1, \u03c0k is an ( \u03f5+ c\u22061 \u2206Ik ) -approximate equilibrium for the last\nepisode of Ik. Hence it is an ( \u03f5+ 2max { c\u22061 , H } \u2206Ik\u222aJk ) -approximate equilibrium for any episode in Jk according to Lemma 3. In the proof we omit the max with H and recover it in the conclusion.\nRegret(T ) = K\u2211 k=1 ( |Ik|+ |Jk| ( \u03f5+ 2c\u22061 \u2206Ik\u222aJk )) \u2264K\u2308C1(\u03f5)\u2309+ T\u03f5+ 2c\u22061 T1\u2206\n\u22644TC1(\u03f5) T1 + T\u03f5+ 2c\u22061 T1\u2206.\nCorollary 1. With probability 1\u2212 T\u03b4, the regret of Algorithm 1 satisfies\nRegret(T ) \u2264\n{ 13 ( \u2206c1max { c\u22061 , H })1/4 T 3/4, \u03b1 = \u22122,\n13 ( \u2206c1max { c\u22061 , H })1/5 T 4/5, \u03b1 = \u22123,\nby setting\nT1 =\n\u2308\u221a TC1(\u03f5)\nmax { c\u22061 , H } \u2206\n\u2309 , \u03f5 = { ( \u2206c1max { c\u22061 , H } /T )1/4\n, \u03b1 = \u22122,( \u2206c1max { c\u22061 , H } /T )1/5 , \u03b1 = \u22123.\nProof. As before, we omit the max with H in the proof.\nRegret(T ) \u22648TC1(\u03f5)\n\u221a c\u22061 \u2206\nTC1(\u03f5) + T\u03f5+ 4c\u22061 \u2206\n\u221a TC1(\u03f5)\nc\u22061 \u2206 =12 \u221a c\u22061 T\u2206C1(\u03f5) + T\u03f5\n=12 \u221a c\u22061 T\u2206c1\u03f5 \u03b1/2 + T\u03f5\nApplying Lemma 2, we get the desired conclusion."
        },
        {
            "heading": "D OMITTED PROOFS IN SECTION 5",
            "text": "In Section D.1 we present the proof for Proposition 2 and Proposition 3. In Section D.2 and D.3, we analyze the performance of Algorithm 3. We first analyze the performance of single block in Section D.2 and then present the subsequent proof in Section D.3. For convenience, the episodes in Section D.2 refer to \u03c4 and the episodes in Section D.3 refer to t. Before presenting the proof, we first describe the multi-scale scheduling formally in Protocol 1."
        },
        {
            "heading": "D.1 PROOFS REGARDING CONSTRUCTION OF TEST_EQ",
            "text": "Proposition 2. MDP M \u2032 is induced by MDP M and recommendation policy \u03c0, then the optimal policy in M \u2032 corresponds to a best strategy modification to recommendation policy \u03c0 in M .\nProtocol 1 Scheduling TEST_EQ in a block with length 2n\n1: Input: Joint Markov policy \u03c0, failure probability \u03b4, tolerance \u03f5. 2: for \u03c4 = 0, 1, . . . , 2n \u2212 1 do 3: for q = 0, 1, \u00b7 \u00b7 \u00b7 , Q do 4: if \u03c4 is a multiple of 2c+q then 5: With probability p(q), schedule a TEST_EQ for \u03f5(q) starting from \u03c4 .\nProof. To facilitate the proof, we define some notations here. We define the value function of policy \u03c0 in an MDP M at timestep h0 and state s as\nV \u03c0,Mh0 (s) = E\u03c0  H\u2211 h=h0 rh(sh, ah) |M, sh0 = s  . The mean reward from rh(\u00b7|s, a) is denoted as Rh(s, a). Let \u03c0\u2032 be a policy in M , then\nV \u03c0 \u2032,M \u2032 h ((s, b)) = \u2211 a \u03c0\u2032 (a | (s, b))  \u2211 (s\u2032,b\u2032) P \u2032h ( (s\u2032, b\u2032)|(s, b), a ) V \u03c0 \u2032,M \u2032 h+1 ( (s\u2032, b\u2032) ) +R\u2032h((s, b), a)  Additionally, the Q-function of a state-action pair (s, b) under policy \u03c0 at timestep h0 for agent i in Markov game M is defined as\nQ\u03c0,Mh0,i (s, b) = E\u03c0  H\u2211 h=h0 rh,i(sh,ah) |M, sh0 = s, ah0,i = b  . Assume \u03c0\u2032 is a deterministic policy and \u03c8i is a strategy modification such that its choice is the same as the choice of \u03c0\u2032, then\nQ\u03c8i\u22c4\u03c0,Mh0,i (s, \u03c8i(b)) = \u2211 (s\u2032,b\u2032) Ph(s\u2032|s, \u03c0h(s) = b, \u03c8i(b))\u03c0h+1(b\u2032 | s\u2032)Q\u03c8i\u22c4\u03c0,Mh0+1,i (s \u2032, \u03c8i(b))\n+Rh0(s, \u03c8i(b) | \u03c0h(s) = b)\nby definition of M \u2032 we can directly see that\nV \u03c0 \u2032,M \u2032 h ((s, b)) = Q \u03c8i\u22c4\u03c0,M h,i (s, \u03c8i(b)) (3)\nHence the optimal policy of M \u2032 corresponds to a best strategy modification to recommendation policy.\nProposition 3. As long as LEARN_OP satisfies Assumption 3, Algorithm 2 satisfies Assumption 2. Furthermore, there exists algorithm satisfying c\u22062 = O(H 2).\nProof. We first consider the NE and CCE case. The main logic has been stated in the main text. We restate it here with environmental changes involved. Denote the intervals that run Line 2, 4, 5 by I,J ,K respectively. Then with high probability, the estimation of V\u0302i(\u03c0) departs from the true value by at most \u03f5/6 + \u2206I and that of V\u0302i(\u03c0\u2032i, \u03c0\u2212i) is at most \u03f5/3 + c \u2206 3 \u2206J +\u2206K. Combine all the error we get the conclusion. In terms of sample complexity\nC2(\u03f5) = O\u0303 ( mC3(\u03f5) + \u03f5 \u22122) = O\u0303 (mC3(\u03f5)) . The last equality use the information-theoretic lower bound C3(\u03f5) = \u2126(\u03f5\u22122). Then we consider the CE case. By Equation 3 we can prove the correctness of this algorithm using the same argument as before. In terms of sample complexity, it is the same as before except that we need to change the size of state space from |S| to |S| |A|. Finally, c\u22062 = c\u22063 . By Wei & Luo (2021), we know that we have c\u22062 = c \u2206 3 = O(H\n2). To be more specific, we restate the Assumption 1 of Wei & Luo (2021) as follows. Let ft : \u03a0\u2192 [0, 1] be the reward function at timestep t where \u03a0 is the policy set of the player and t \u2208 [T ]. At timestep t, f\u22c6t = max\u03c0\u2208\u03a0 ft(\u03c0) is the optimal mean reward and Rt is the\nmean reward received by the player. The algorithm outputs an auxiliary quantity f\u0303t at timestep t satisfying that with probability 1\u2212 \u03b4/T , for all timesteps t, for some function \u03c1 : [T ]\u2192 R,\nf\u0303t \u2265 min \u03c4\u2208[1,t]\nf\u22c6\u03c4 \u2212 \u039b[1,t], 1\nt t\u2211 \u03c4=1 ( f\u0303\u03c4 \u2212R\u03c4 ) \u2264 \u03c1(t) + \u039b[1,t].\nHere \u039b is the non-stationarity measure. Using the fact that\nmax \u03c4\u2208[1,t] f\u22c6\u03c4 \u2212 min \u03c4\u2208[1,t] f\u22c6\u03c4 \u2264 \u039b[1,t]\nwe have that for any timestep s \u2208 [1, t],\n1\nt t\u2211 \u03c4=1 ( max \u03c4 \u2032\u2208[1,t] f\u22c6\u03c4 \u2032 \u2212R\u03c4 ) \u2264 \u03c1(t) + 3\u039b[1,t].\nFor single-agent RL, Q-UCB algorithm achieves \u03c1(t) = O\u0303 (\u221a H5SA/t+H3SA/t ) and \u039b1,t =\nO\u0303(H2\u2206). Thus, the average policy of behavior policies in [1, t] satisfies Assumption 2 with C3(\u03f5, \u03b4) = O\u0303(H 5SA/\u03f52) and c\u22063 = O\u0303(H 2)."
        },
        {
            "heading": "D.2 SINGLE BLOCK ANALYSIS",
            "text": "Divide [C1(\u03f5) + 1, 2n] into I1 \u222a I2 \u222a \u00b7 \u00b7 \u00b7 \u222a IK such that Ik = [sk, ek], s1 = C1(\u03f5) + 1, eK = 2n, ek + 1 = sk+1 and\n\u2206Ik \u2264 1\nc\u22062 max { 1\u221a |Ik| , 2\u2212n/4\u22121 } Intervals with such property are called near-stationary. Let En \u2208 Il be the last episode (The block may be ended due to a failed TEST_EQ). Define e\u2032k = min{En, ek}, I \u2032k = [si, e\u2032k]. If k > l, I \u2032k = \u2205. For convenience, we denote \u03c4n = C1(\u03f5) + 1 in the following proof. Definition 6. For k \u2208 [K], q \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , Q}, let\n\u03c4k(q) = min { \u03c4 \u2208 I \u2032k| \u03c0 is not a 2\u03f5(q)-EQ at \u03c4 } , \u03bek(q) = [ e\u2032k \u2212 \u03c4k(q) + 1 ] + .\nFirst, we are going to show that with high probability no TEST_EQ is aborted. Lemma 5. With probability 1\u2212 2QT\u03b4, for any TEST_EQ instance testing gap \u03f5(q) maintained from s to e, it returns fail if the policy is not (2\u03f5(q)+ c\u22062 \u2206[s,e])-NE/CCE for any \u03c4 \u2208 [s, e]. In equivalence, e\u2212 s < 2c+q and all TEST_EQ function as desired.\nProof. By union bound, the probability all TEST_EQ function as desired is 1 \u2212 QT\u03b4. There are 2q\u2212r possible starting points for a test occupying 2r episodes. For each of them, TEST_EQ exists with probability 1/(\u03f5(r)2n/2). By Bernstein\u2019s inequality, with probability 1\u2212 \u03b4, the number of such tests is upper-bounded by\n2q\u2212r 1\n\u03f5(r)2n/2 +\n\u221a 2 \u00b7 2q\u2212r 1\n\u03f5(r)2n/2 log\n1 \u03b4 + log 1 \u03b4\n\u22642 \u00b7 2q\u2212r 1 \u03f5(r)2n/2 + 2 log 1 \u03b4\n= 2q\u2212r/2+1 \u221a c22n/2 + 2 log 1 \u03b4 .\nBy union bound, this inequality holds for all TEST_EQ with probability 1\u2212QT\u03b4. So the total length of all shorter tests is upper bounded by\nq\u22121\u2211 r=0 ( 2q\u2212r/2+1 \u221a c22n/2 + 2 log 1 \u03b4 ) 2r\n\u22642q+1 2 q\u22121 2 \u2212 1\u221a 2\u2212 1 1 \u221a c22n/2 + 2 log 1 \u03b4 (2q \u2212 1) \u22645 \u221a c2 ( 2 q\u22121 2 \u2212 1 ) + log 1\n\u03b4 2q+1 \u2264max { 5 \u221a c2, 2 log 1\n\u03b4\n} 2q\nHere we use 2q < 2Q < c22n/2. Using the union bound, we get the conclusion.\nIn subsequent proofs, we condition on the high probability event described in this lemma. Lemma 6. With probability 1\u2212Q\u03b4, for all r \u2208 [Q].\nl\u2211 k=1 [ \u03bek(r)\u2212 2c+r ] + \u2264 2c+r\u22121\u03f5(r \u2212 2) \u221a 2n log 1 \u03b4 = 2c \u221a 2r+nc2\nProof. For each r \u2208 [Q].\n2\u2212c\u2212r+2 l\u2211\nk=1\n[ \u03bek(r)\u2212 2c+r ] +\n=2\u2212c\u2212r+2 l\u2211\nk=1\n[ e\u2032k \u2212 \u03c4k(r) + 1\u2212 2c+r ] +\n\u2264 K\u2211\nk=1 \u2211 \u03c4\u2208Ik 1 [ \u03c4 \u2208 [\u03c4k(r), e\u2032k \u2212 2c+r\u22121], \u03c4 mod 2c+r\u22122 \u2261 0 ] =\n2n\u2211 \u03c4=\u03c4n 1 [ \u03c4 \u2208 [\u03c4k(r), e\u2032k \u2212 2c+r\u22121], \u03c4 mod 2c+r\u22122 \u2261 0 ] \u2264\n2n\u2211 \u03c4=\u03c4n 1 [ \u03c4 \u2208 [\u03c4k(r), e\u2032k \u2212 2c+r\u22121], \u03c4 mod 2c+r\u22122 \u2261 0 and there is no test for \u03f5(r)/2 starting at any t \u2208 [\u03c4n, \u03c4 ] ] +\n2n\u2211 \u03c4=\u03c4n 1 [ \u03c4 \u2208 [1, En \u2212 2c+r\u22121] and there is a test for \u03f5(r)/2 starting at some t \u2208 [\u03c4n, \u03c4 ] ] \u2264 [ 1 + log(1/\u03b4)\n\u2212 log(1\u2212 1/ ( \u03f5(r \u2212 2) \u221a 2n ) )\n] + 0 \u2264 2\u03f5(r \u2212 2) \u221a 2n log 1\n\u03b4\nThe first inequality holds because in an interval of length w, there are at least (w + 2\u2212 2u)/u points whose indices are multiples of u. The third inequality holds with probability 1\u2212 \u03b4. The first sum is bounded using the fact the test is started i.i.d. with constant probability 1/ ( \u03f5(r \u2212 2) \u221a 2n ) . In the second sum, the condition implies that the ending time of the test is before t+ 2c+r\u22122 \u2212 1 \u2264 ei \u2212 2c+r\u22122 \u2212 1 \u2264 ei so the test is within Ik and t + 2c+r\u22122 \u2212 1 \u2264 \u03c4 + 2c+r\u22122 \u2212 1 < En so the test ends before the block ends. However, the test is for \u03f5(r) and the variation during the test is bounded by \u2206Ik < 2 \u2212n/4 = \u03f5 < \u03f5(r), so such TEST_EQ must return Fail.\nIn subsequent proofs, we further condition on the high probability event described in this lemma. Lemma 7. The total number of near-stationary intervals\nl \u2264 1 + 2min { 2n/3 ( c\u22062 \u2206[1,En] )2/3 , 2n/4c\u22062 \u2206[1,En] } (4)\nProof. We divide [\u03c4n, En] = I1 \u222a I2 \u222a \u00b7 \u00b7 \u00b7 \u222a Il in such a way that [sk, ek] is near-stationary but [sk, ek + 1] is not near-stationary. Then\n\u2206[\u03c4n,En] \u2265 l\u22121\u2211 k=1 \u2206[sk,ek+1]\n\u2265 1 c\u22062 l\u22121\u2211 k=1 max { 1\u221a ek \u2212 sk + 2 , 2\u2212n/4\u22121 }\n\u2265 1 c\u22062 max { l\u22121\u2211 k=1\n1\n2 \u221a ek \u2212 sk + 1\n, (l \u2212 1)2\u2212n/4\u22121 }\nHence by H\u00f6lder\u2019s inequality\nl \u22641 + min  ( l\u22121\u2211 k=1 (ek \u2212 sk + 1)\u22121/2 )2/3( l\u22121\u2211 k=1 (ek \u2212 sk + 1) )1/3 , 2n/4+1c\u22062 \u2206[\u03c4n,En]  \u22641 + 2min {( c\u22062 \u2206[\u03c4n,En] )2/3 |[\u03c4n, En]|1/3 , 2n/4c\u22062 \u2206[\u03c4n,En]\n} \u22641 + 2min { 2n/3 ( c\u22062 \u2206[1,En] )2/3 , 2n/4c\u22062 \u2206[1,En] }\nLemma 8. With probability 1\u2212 3QT\u03b4\nRegret([1, En]) \u2264 23n/4+4 + 4Q ( 2n/2+c \u221a c2l + 2 c+n/2c2 ) + c2 log 1\n\u03b4 2n/2+1 + c12\n\u2212\u03b1n/4\nProof. First we consider the regret generated by TEST_EQ. We need to count the number of steps all the tests go for. Similar to the calculation in Lemma 6. The number of tests with length 2q is upper bounded by\n2n\u2212r/2+1 c \u221a c22n/2 + 2 log 1 \u03b4 .\nSo the total length of all TEST_EQ is upper bounded by\nQ\u2211 r=0\n( 2n\u2212r/2+1\n2c \u221a c22n/2\n+ 2 log 1\n\u03b4\n) 2r\n\u22642n+1 2 Q/2 \u2212 1\u221a 2\u2212 1\n1\nc \u221a c22n/2\n+ 2 log 1\n\u03b4\n( 2Q \u2212 1 ) \u2264 5 2c 23n/4 + c2 log 1 \u03b4 2n/2+1\nThen we consider the regret generated by committing.\u2211 \u03c4\u2208I\u2032\nk\nGapM t ( \u03c0t )\n\u2264 \u2211 \u03c4\u2208I\u2032\nk\n( 1 [ GapM t ( \u03c0t ) \u2264 2\u03f5(Q) ] 2\u03f5(Q)\n+ Q\u22121\u2211 r=0 1 [ 2\u03f5(r + 1) \u2264 GapM t ( \u03c0t ) \u2264 2\u03f5(r) ] 2\u03f5(r) + 1 [ GapM t ( \u03c0t ) > \u03f5(0) ] 1\n)\n\u22642|I \u2032k|\u03f5(Q) + 2 Q\u22121\u2211 r=0 \u03f5(r)\u03bei(r + 1) + 2\u03f5(0)\u03bei(0)\n\u22642|I \u2032k|\u03f5(Q) + 4 Q\u2211 r=0 \u03f5(r)\u03bei(r)\nIn the second inequaility we use \u03f5(0) = \u221a c2 > 1 and in the third inequality we use \u03f5(r) \u2264 2\u03f5(r+1). Summing over all intervals we have\nRegret([1, En]) \u2264 2n+1\u03f5(Q) + 4 Q\u2211 r=0 l\u2211 k=1 \u03f5(r)\u03bek(r).\nFurthermore l\u2211\nk=1\n\u03f5(r)\u03bek(r) = l\u2211 k=1 \u03f5(r)min{\u03bek(r), 2c+r}+ l\u2211 k=1 \u03f5(r) [ \u03bek(r)\u2212 2c+r ] +\n\u22642c l\u2211\nk=1\n\u221a c2min{\u03bek(r), 2c+r}+ l\u2211 k=1 \u03f5(r) [ \u03bek(r)\u2212 2c+r ] +\n\u22642c l\u2211\nk=1\n\u221a c2 \u2223\u2223I \u2032k\u2223\u2223+ 2c+n/2c2\nThe last inequality uses Lemma 6. Hence\nRegret([1, En]) \u22642n+1\u03f5(Q) + 4Q ( 2c\nl\u2211 k=1 \u221a c2 |I\u2032k|+ 2 c+n/2c2 ) + 5 2c 23n/4 + c2 log 1 \u03b4 2n/2+1 + C1(\u03f5)\n\u226423n/4+4 + 4Q 2c \u221a\u221a\u221a\u221ac2l l\u2211\nk=1\n|I\u2032k|+ 2 c+n/2c2 + c2 log 1 \u03b4 2n/2+1 + c12 \u2212\u03b1n/4\n\u226423n/4+4 + 4Q ( 2n/2+c \u221a c2l + 2 c+n/2c2 ) + c2 log 1\n\u03b4 2n/2+1 + c12\n\u2212\u03b1n/4\nTo keep the notation clean, from now on we make frequent use of the big-O notation and hide the dependencies on logarithmic factors on relevant variables. We also assume \u2206 is always large enough so that we can drop the 1 in Inequality 4. Lemma 1. With probability 1\u2212 3QT\u03b4, the regret inside this block\nRegret = O\u0303 ( 2 3n/4\n+ c2 min\n{ 2 2n/3 ( c \u2206 2 \u2206[1,En] )1/3 , 2 5n/8 ( c \u2206 2 \u2206[1,En] )1/2} + 2 n/2 c 3/2 2 + 2 \u2212\u03b1n/4 c1 ) . (1)\nProof. We may restate the bounds in Lemma 7 and 8 as\nl = O ( min { 2n/3 ( c\u22062 \u2206 )2/3 [1,En] , 2n/4 ( c\u22062 \u2206[1,En] )})\nRegret([1, En]) = O\u0303 ( 23n/4 + 2n/2c2 \u221a l + 2n/2c 3/2 2 + 2 \u2212\u03b1n/4c1 )\nCombine them together we get\nRegret([1, En]) = O\u0303 ( 23n/4 + c2 min { 22n/3 ( c\u22062 \u2206[1,En] )1/3 , 25n/8 ( c\u22062 \u2206[1,En] )1/2} + 2n/2c 3/2 2 + 2 \u2212\u03b1n/4c1 ) (5)"
        },
        {
            "heading": "D.3 PROOF FOR THEOREM 1",
            "text": "Due to the doubling structure inside each segment, from Formula 5 we get Regret (Jj) = O\u0303 ( |Jj |3/4 + c2min { |Jj |2/3 ( c\u22062 \u2206Jj )1/3 , |Jj |5/8 ( c\u22062 \u2206Jj )1/2} + c 3/2 2 |Jj | 1/2 + c1 |Jj |\u2212\u03b1/4 )\nLemma 9.\nJ = O ( T 1/5 ( max { c\u22061 , c \u2206 2 } \u2206 )4/5) .\nProof. For any segment Jj , max { c\u22061 , c \u2206 2 } \u2206Jj \u2265 \u03f5(Q)\u2212 \u03f5 \u2265 (\u221a 2\u2212 1 ) |Jj |\u22121/4\nsince the ending of a segment is caused by a False returned by TEST_EQ. Then by the same logic as in Lemma 7 we get the conclusion\nHence by H\u00f6lder inequality Regret(T ) =O\u0303 ( J1/4T 3/4 + c2 min { T 2/3\u2206\u03031/3, T 5/8\u2206\u03031/2 } + c 3/2 2 J 1/2T 1/2 + c1J 1+\u03b1/4T\u2212\u03b1/4 ) =  O\u0303 ( \u2206\u030c1/5T 4/5 + c2 min { \u2206\u03031/3T 2/3, \u2206\u03031/2T 5/8 } + ( c1 + c 3/2 2 ) \u2206\u030c2/5T 3/5 ) \u03b1 = \u22122 O\u0303 ( c1\u2206\u030c 1/5T 4/5 + c2 min { \u2206\u03031/3T 2/3, \u2206\u03031/2T 5/8 } + c 3/2 2 \u2206\u030c 2/5T 3/5 ) \u03b1 = \u22123"
        },
        {
            "heading": "E BASE ALGORITHMS SATISFYING ASSUMPTION 1",
            "text": "In table 2 we summarize the results of this section."
        },
        {
            "heading": "E.1 TWO-PLAYER ZERO-SUM MATRIX GAMES (NE)",
            "text": "In this part we consider the following algorithm: each player independently runs an optimal adversarial multi-armed bandit algorithm (e.g. EXP.3) and finally output the product of respective average policies of the whole time horizon. We will prove that this algorithm satisfies Assumption 1 in terms of learning NE in two-player zero-sum matrix games.\nProof. We adopt some new notations in this proof. Let Rt \u2208 [0, 1]A\u00d7B be the reward matrix at episode t. The policy of the max and min players are represented by xt \u2208 [0, 1]A, yt \u2208 [0, 1]B . Each entry represents the probability they choose the corresponding action. The reward received by the max and min players are respectively xt\u22a4Rtyt and \u2212xt\u22a4Rtyt. With probability 1\u2212 \u03b4 the adversarial MAB algorithms satisfy\n1\nT T\u2211 t=1 xt \u22a4 Rtyt \u2212min y 1 T T\u2211 t=1 xt \u22a4 Rty \u2264 cadv \u221a AT\nmax x\n1\nT T\u2211 t=1 x\u22a4Rtyt \u2212 1 T T\u2211 t=1 xt \u22a4 Rtyt \u2264 cadv \u221a BT\nwhere cadv = O\u0303(1). The output policy x = \u2211T t=1 x t/T and y = \u2211T t=1 y t/T satisfy\nVM T max(\u2020, y) + VM T min (x, \u2020) = maxx x \u22a4RT y +min y x\u22a4RT y \u2264 cadv\n\u221a BT +\u2206+ cadv \u221a AT +\u2206\nBy the definition of zero-sum game\nNEGAP(x, y) \u2264 VM\nT max(\u2020, y)\u2212 VM T max(x, y) + V MT min (x, \u2020)\u2212 VM T min (x, y)\n2\n= VM\nT max(\u2020, y) + VM T min (x, \u2020) 2\n= O\u0303 (\u221a (A+B)T ) + 2\u2206.\nHence this algorithm satisfies Assumption 1 with C1(\u03f5) = O\u0303 ( (A+B)\u03f5\u22122 ) , c\u22061 = 2."
        },
        {
            "heading": "E.2 MULTI-PLAYER GENERAL-SUM MATRIX GAMES (CCE)",
            "text": "In this part we consider the following algorithm: each player independently runs an optimal adversarial multi-armed bandit algorithm (e.g. EXP.3) and finally output the average joint policy of the whole time horizon. We will prove that this algorithm satisfies Assumption 1 in terms of learning CCE in multi-player general-sum matrix games.\nProof. We define the loss of player i at episode t by playing ai as lti(ai) = 1\u2212 Ea\u2212i\u223c\u03c0t\u2212i [ ri(ai, a\u2212i) |M t ] then with probability 1\u2212 \u03b4, the adversarial MAB algorithm satisfies\nT\u2211 t=1 \u27e8\u03c0t(\u00b7), lti(\u00b7)\u27e9 \u2212 min ai\u2208Ai T\u2211 t=1 lti(ai) \u2264 cadv \u221a AiT , cadv = O\u0303(1)\nFor convenience, we denote the reward function at timestep t by rt. Let the output policy \u03c0 =\u2211T t=1 \u03c0 t/T , we have\nVM T\ni (\u03c0) =Ea\u223c\u03c0 [ rTi (a) ] = 1\nT T\u2211 t=1 Eai\u223c\u03c0tiEa\u2212i\u223c\u03c0t\u2212i [ rTi (ai, a\u2212i) ] =1\u2212 1\nT T\u2211 t=1 Eai\u223c\u03c0ti [ lti(ai) ] + 1 T T\u2211 t=1 Eai\u223c\u03c0tiEa\u2212i\u223c\u03c0t\u2212i [ rTi (ai, a\u2212i)\u2212 rti(ai, a\u2212i) ] \u22651\u2212 1\nT min ai\u2208Ai T\u2211 t=1 lti(ai)\u2212 cadv \u221a Ai/T + 1 T T\u2211 t=1 Eai\u223c\u03c0tiEa\u2212i\u223c\u03c0t\u2212i [ rTi (ai, a\u2212i)\u2212 rti(ai, a\u2212i) ] \u2265 1 T max ai\u2208Ai T\u2211 t=1 Ea\u2212i\u223c\u03c0t\u2212i [ rti(ai, a\u2212i) ] \u2212 cadv \u221a Ai/T \u2212\u2206\n\u2265 1 T max ai\u2208Ai T\u2211 t=1 Ea\u2212i\u223c\u03c0t\u2212i [ rTi (ai, a\u2212i) ] \u2212\u2206\u2212 cadv \u221a Ai/T \u2212\u2206\n\u2265VM T i (\u2020, \u03c0\u2212i)\u2212 cadv \u221a Ai/T \u2212 2\u2206\nBy definition of CCE we know this algorithm satisfies Assumption 1 with C1(\u03f5) = O\u0303 ( Amax\u03f5 \u22122) , c\u22061 = 2"
        },
        {
            "heading": "E.3 MULTI-PLAYER GENERAL-SUM MATRIX GAMES (CE)",
            "text": "This part is very similar to the last part. Instead of using standard adversarial bandit algorithms, we use no-swap-regret algorithm for adversarial bandits (for example, Ito (2020)) and the proof is almost the same. We can achieve with probability 1\u2212 \u03b4,\nT\u2211 t=1 \u27e8\u03c0t(\u00b7), lti(\u00b7)\u27e9 \u2212min \u03c8i T\u2211 t=1 \u27e8(\u03c8i \u22c4 \u03c0t)(\u00b7), lti(\u00b7)\u27e9 \u2264 cadvAi \u221a T , cadv = O\u0303(1)\nwhere \u03c8i is a strategy modification. By substituting all min,max related terms correspondingly we get the proof for CE and C1(\u03f5) = O\u0303 ( A2max\u03f5 \u22122)"
        },
        {
            "heading": "E.4 CONGESTION GAMES (NE)",
            "text": "In this part we will show the Nash-UCB algorithm proposed in Cui et al. (2022) satisfies Assumption 1. We carry out the proof by pointing out the modifications we need to make in their proof. In their proof, k stands for the episode index instead of t and K is the total episodes instead of T . Lemma 10. (Modified Lemma 3 in Cui et al. (2022)) With high probability,\u2223\u2223\u2223r\u0303ki \u2212 ri\u2223\u2223\u2223 (a) \u2264 max\ni\u2208[m] \u2225Ai(a)\u2225(V k)\u22121\n\u221a \u03b2\u0303k, \u03b2\u0303k = O(mF +Km\u2206 2)\nProof. We denote the average reward vector by \u03b8 and the reward vector of the last epsiode by \u03b8T , other notations are similar, then\u2223\u2223\u2223r\u0303ki \u2212 rTi \u2223\u2223\u2223 (a) \u2264\u2225Ai(a)\u2225(V k)\u22121 \u2225\u2225\u2225\u03b8\u0302 \u2212 \u03b8T\u2225\u2225\u2225\nV k \u2264\u2225Ai(a)\u2225(V k)\u22121 (\u2225\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2225\u2225\u2225 V k + \u2225\u2225\u2225\u03b8 \u2212 \u03b8T\u2225\u2225\u2225 V k ) \u2264\u2225Ai(a)\u2225(V k)\u22121 (\u2225\u2225\u03b8\u2225\u2225 2 + \u221a log detV k + \u03b9\u0303+ \u221a Km\u2206 )\nThe rest of the proof is carried out with the new \u03b2\u0303k and finally the regret becomes Nash-Regret(K) = O\u0303 ( mF 3/2 \u221a K +mFK\u2206 ) .\nFinally this algorithm can be converted into a version with sample complexity guarantee and C1(\u03f5) = m2F 3\u03f5\u22122, c\u22061 = mF as stated in the original paper using the certified policy trick from Bai et al. (2020)."
        },
        {
            "heading": "E.5 MULTI-PLAYER GENERAL-SUM MARKOV GAMES (CCE,CE)",
            "text": "In this part we will show how to adapt the proof in Cui et al. (2023) to the non-stationary game case. For simplicity, we will follow the proof in Cui et al. (2023) in general and only point out critical changes. Note that they use k as epoch index while we have been using k as episode index. For consistency, we will use \u03ba as the episode index in this section. As a reminder, we will use r\u03ba, P\u03ba and M\u03ba to denote the reward function, the transition kernel and the game at episode \u03ba.\nWe use the superscript \u03ba in E\u03ba[\u00b7] to denote that the underlying game is M\u03ba. We further use \u03bakh(j; s) to denote the episode index when state s is visited for the jth time at step h and epoch k in the no-regret learning phase (Line 12 in Algorithm 3), and we use \u03bakh(j; s) to denote the episode index when state s is visited for the jth time at step h and epoch k in the no-regret learning phase (Line 12 in Algorithm 3). We will change the algorithm in Line 34 where we replace nkh(sh) with \u2211k\u22121 l=1 T k h (sh).\nWe will modify all the lemmas in the proof below. We use Nk to denote \u2211k l=1 n k.\nFirst, we will replace E a\u223c\u03c0k,t k h (j;s) h (\u00b7|s) [\u00b7] with E\u03ba\nk h (j;s)\na\u223c\u03c0k,t k h (j;s) h (\u00b7|s) [\u00b7] in all the lemmas, which takes the\nexpectation with the underlying game when \u03c0k,t k h (j;s)\nh (\u00b7 | s) is used. It is easy to verify that Lemma 35, Lemma 36, Lemma 37 hold after the modification.\nSecond, we will replace nkh(s) with \u2211k\u22121 l=1 T l h(s) and n kd\u03c0 k h (s) with \u2211nk J=1 d \u03c0k\nh (s; k, J), where d\u03c0 l\nh (s; k, J) is the visiting density for model at epoch k and J th trajectory sampled in the policy cover update phase. In addition, we also add the following argument in the lemma:\nnkh(s) \u2228 Trig \u2265 1\n2 k\u22121\u2211 l=1 nl Nk\u22121 Nk\u22121\u2211 j=1 d\u03c0 l h (s; k, j)  \u2228 TTrig, where d\u03c0 l\nh (s; k, j) is the visiting density for model at epoch k and jth trajectory sampled in the no-regret learning phase. It is easy to verify that Lemma 38 hold after the modification.\nThird, we will consider a baseline model M0, which can be the game at any episode, and use V \u03c0h,i(s) to denote the corresponding value function. Now we show that Lemma 39, Lemma 40 and Lemma 41 holds with an addition tolerance \u2206. Lemma 11. (Modified Lemma 39 in Cui et al. (2023)) Under the good event G, for all k \u2208 [K], h \u2208 [H], i \u2208 [m], s \u2208 S, we have\nV k h,i(s) \u2265 V \u2020,\u03c0k\u2212i h,i (s)\u2212 H\u2211 h\u2032=h \u2206h.\nProof. Note that we have\u2223\u2223\u2223\u2223E\u03bakh(j;s)a\u223c\u03c0k,tkh(j;s)h (\u00b7|s) [ rh,i(s,a) + V k h+1,i(s \u2032) ] \u2212 EM 0 a\u223c\u03c0k,t k h (j;s) h (\u00b7|s) [ rh,i(s,a) + V k h+1,i(s \u2032) ]\u2223\u2223\u2223\u2223 \u2264 \u2206h.\nThe rest of the proof follows Cui et al. (2023).\nLemma 12. (Modified Lemma 40 in Cui et al. (2023)) Under the good event G, for all k \u2208 [K], h \u2208 [H], i \u2208 [m], s \u2208 S, we have\nV kh,i(s) \u2264 V \u03c0 k h,i(s) + H\u2211 h\u2032=h \u2206h.\nProof. The proof follows the proof for Lemma 11.\nLemma 13. (Modified Lemma 41 in Cui et al. (2023)) Under the good event G, for all k \u2208 [K], i \u2208 [m], we have\nV k 1,i(s1)\u2212 V k1,i(s1) \u2264 O\u0303\n( EM 0\n\u03c0k [ H\u2211 h=1 \u221a H2AiTTrig nkh(sh) \u2228 TTrig ]) + 2\u2206.\nProof. The proof follows the proof for Lemma 11.\nLemma 14. (Modified Lemma 42 in Cui et al. (2023)) Under the good event G, for all i \u2208 [m], we have\nK\u2211 k=1 nk max i\u2208[m] ( V k 1,i(s1)\u2212 V \u03c0 k 1,i(s1) ) \u2264 O\u0303 ( H2 \u221a SAmaxTTrigN ) .\nProof. By Lemma 13 and the proof in Cui et al. (2023), we only need to bound\u2211K k=1 n kEM 0 \u03c0k \u221a 1\nnk h (sh)\u2228TTrig . By the definition of \u2206, we can easily prove that\n\u2211 s\u2208S \u2223\u2223\u2223\u2223\u2223\u2223 n k Nk Nk\u2211 j=1 d\u03c0 k h (s; k + 1, j)\u2212 nk\u2211 J=1 d\u03c0 k h (s; k, J) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 nk\u2206, \u2211 s\u2208S \u2223\u2223\u2223\u2223\u2223\u2223nkd\u03c0kh (s)\u2212  k\u2211 l=1 nl Nk Nk\u2211 j=1 d\u03c0 l h (s; k + 1, j)\u2212 k\u22121\u2211 l=1 nl Nk\u22121 Nk\u22121\u2211 j=1 d\u03c0 l h (s; k, j) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 Nk\u2206.\nand we have \u2211 s\u2208S  k\u2211 l=1 nl Nk Nk\u2211 j=1 d\u03c0 l h (s; k + 1, j)\u2212 k\u22121\u2211 l=1 nl Nk\u22121 Nk\u22121\u2211 j=1 d\u03c0 l h (s; k, j) \u22122 k\u22121\u2211 l=1 nl Nk\u22121 Nk\u22121\u2211 j=1 d\u03c0 l h (s; k, j) \u2264 4Nk\u2206.\nThen we have K\u2211 k=1 nkEM 0 \u03c0k \u221a 1 nkh(sh) \u2228 TTrig\n= K\u2211 k=1 nk \u2211 s\u2208S d\u03c0 k h (s)\n\u221a 1\nnkh(s) \u2228 TTrig\n\u2264 \u2211 s\u2208S K\u2211 k=1 nkd\u03c0 k h (s)\n\u221a 2\n( \u2211k\u22121 l=1 nl Nk\u22121 \u2211Nk\u22121 j=1 d \u03c0l h (s; k, j)) \u2228 TTrig (Lemma 38 in Cui et al. (2023))\n\u2264NK\u2206+ \u2211 s\u2208S K\u2211 k=1  k\u2211 l=1 nl Nk Nk\u2211 j=1 d\u03c0 l h (s; k + 1, j)\u2212 k\u22121\u2211 l=1 nl Nk\u22121 Nk\u22121\u2211 j=1 d\u03c0 l h (s; k, j)  \u221a\n2 ( \u2211k\u22121 l=1 nl Nk\u22121 \u2211Nk\u22121 j=1 d \u03c0l h (s; k, j)) \u2228 TTrig\n\u22642NK\u2206+ \u2211 s\u2208S \u221a\u221a\u221a\u221a32 K\u2211 l=1 nl NK NK\u2211 j=1 d\u03c0 l h (s;K, j) (Lemma 38 and Lemma 53 in Cui et al. (2023)) \u22642NK\u2206+ \u221a 32SN.\nLemma 43 in Cui et al. (2023) holds directly with the modified update rule. As a result, following Theorem 4 in Cui et al. (2023), the same sample complexity result holds for learning an \u03f5+O\u0303(HS\u2206)CCE. Hence C1(\u03f5) = H6S2Amax\u03f5\u22122, c\u22061 = HS."
        },
        {
            "heading": "E.6 MARKOV POTENTIAL GAMES (NE)",
            "text": "This setting is rather straightforward. Algorithm 3 in Song et al. (2021) serves as a base algorithm. By noticing that any weighted average of the samples of rewards shifts by no more than O(\u2206) in the non-stationary environment and by the very similar argument we made in Lemma 3 or proof of Theorem 1 in Mao et al. (2021) we can see C1(\u03f5) = m2H4SAmax\u03f5\u22123, c\u22061 = O(H 2)."
        }
    ],
    "year": 2023
}