{
    "abstractText": "Equilibrium propagation (EP) is a compelling alternative to the backpropagation of error algorithm (BP) for computing gradients of neural networks on biological or analog neuromorphic substrates. Still, the algorithm requires weight symmetry and infinitesimal equilibrium perturbations, i.e., nudges, to yield unbiased gradient estimates. Both requirements are challenging to implement in physical systems. Yet, whether and how weight asymmetry contributes to bias is unknown because, in practice, its contribution may be masked by a finite nudge. To address this question, we study generalized EP, which can be formulated without weight symmetry, and analytically isolate the two sources of bias. For complex-differentiable non-symmetric networks, we show that bias due to finite nudge can be avoided by estimating exact derivatives via a Cauchy integral. In contrast, weight asymmetry induces residual bias through poor alignment of EP\u2019s neuronal error vectors compared to BP resulting in low task performance. To mitigate the latter issue, we present a new homeostatic objective that directly penalizes functional asymmetries of the Jacobian at the network\u2019s fixed point. This homeostatic objective dramatically improves the network\u2019s ability to solve complex tasks such as ImageNet 32\u00d732. Our results lay the theoretical groundwork for studying and mitigating the adverse effects of imperfections of physical networks on learning algorithms that rely on the substrate\u2019s relaxation dynamics.",
    "authors": [
        {
            "affiliations": [],
            "name": "Axel Laborieux"
        },
        {
            "affiliations": [],
            "name": "Friedemann Zenke"
        }
    ],
    "id": "SP:8b5025ff423b40413c97ea87799572484b541b59",
    "references": [
        {
            "authors": [
                "Vanhoucke",
                "Vijay Vasudevan",
                "Fernanda Vi\u00e9gas",
                "Oriol Vinyals",
                "Pete Warden",
                "Martin Wattenberg",
                "Martin Wicke",
                "Yuan Yu",
                "Xiaoqiang Zheng"
            ],
            "title": "TensorFlow: Large-scale machine learning on heterogeneous systems",
            "year": 2015
        },
        {
            "authors": [
                "Mohamed Akrout",
                "Collin Wilson",
                "Peter Humphreys",
                "Timothy Lillicrap",
                "Douglas B Tweed"
            ],
            "title": "Deep Learning without Weight Transport",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Luis B Almeida"
            ],
            "title": "A learning rule for asynchronous perceptrons with feedback in a combinatorial environment. In Artificial neural networks: concept learning, pp. 102\u2013111",
            "year": 1990
        },
        {
            "authors": [
                "Vidyesh Rao Anisetti",
                "Ananth Kandala",
                "Benjamin Scellier",
                "JM Schwarz"
            ],
            "title": "Frequency propagation: Multi-mechanism learning in nonlinear physical networks",
            "venue": "arXiv preprint arXiv:2208.08862,",
            "year": 2022
        },
        {
            "authors": [
                "Walter Appel"
            ],
            "title": "Mathematics for physics and physicists",
            "year": 2007
        },
        {
            "authors": [
                "Shaojie Bai",
                "J Zico Kolter",
                "Vladlen Koltun"
            ],
            "title": "Deep equilibrium models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Shaojie Bai",
                "Vladlen Koltun",
                "J Zico Kolter"
            ],
            "title": "Stabilizing equilibrium models by jacobian regularization",
            "venue": "arXiv preprint arXiv:2106.14342,",
            "year": 2021
        },
        {
            "authors": [
                "Pierre Baldi"
            ],
            "title": "Gradient descent learning algorithm overview: A general dynamical systems perspective",
            "venue": "IEEE Transactions on neural networks,",
            "year": 1995
        },
        {
            "authors": [
                "Pierre Baldi",
                "Fernando Pineda"
            ],
            "title": "Contrastive learning and neural oscillations",
            "venue": "Neural computation,",
            "year": 1991
        },
        {
            "authors": [
                "Sergey Bartunov",
                "Adam Santoro",
                "Blake Richards",
                "Luke Marris",
                "Geoffrey E Hinton",
                "Timothy Lillicrap"
            ],
            "title": "Assessing the scalability of biologically-motivated deep learning algorithms and architectures",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Irem Boybat",
                "Manuel Le Gallo",
                "SR Nandakumar",
                "Timoleon Moraitis",
                "Thomas Parnell",
                "Tomas Tuma",
                "Bipin Rajendran",
                "Yusuf Leblebici",
                "Abu Sebastian",
                "Evangelos Eleftheriou"
            ],
            "title": "Neuromorphic computing with multi-memristive synapses",
            "venue": "Nature communications,",
            "year": 2018
        },
        {
            "authors": [
                "James Bradbury",
                "Roy Frostig",
                "Peter Hawkins",
                "Matthew James Johnson",
                "Chris Leary",
                "Dougal Maclaurin",
                "George Necula",
                "Adam Paszke",
                "Jake VanderPlas",
                "Skye Wanderman-Milne",
                "Qiao Zhang"
            ],
            "title": "JAX: composable transformations of Python+NumPy programs, 2018",
            "venue": "URL http://github.com/google/jax",
            "year": 2018
        },
        {
            "authors": [
                "Connor Bybee",
                "E Paxon Frady",
                "Friedrich T Sommer"
            ],
            "title": "Deep learning in spiking phasor neural networks",
            "venue": "arXiv preprint arXiv:2204.00507,",
            "year": 2022
        },
        {
            "authors": [
                "David G. Clark",
                "L.F. Abbott",
                "SueYeon Chung"
            ],
            "title": "Credit Assignment Through Broadcasting a Global Error Vector",
            "year": 2021
        },
        {
            "authors": [
                "Claudia Clopath",
                "Wulfram Gerstner"
            ],
            "title": "Voltage and spike timing interact in stdp\u2013a unified model",
            "venue": "Frontiers in synaptic neuroscience,",
            "year": 2010
        },
        {
            "authors": [
                "Yang Dan",
                "Mu-ming Poo"
            ],
            "title": "Spike timing-dependent plasticity of neural circuits",
            "year": 2004
        },
        {
            "authors": [
                "Corentin Delacour",
                "Stefania Carapezzi",
                "Madeleine Abernot",
                "Gabriele Boschetto",
                "Nadine Azemard",
                "Jeremie Salles",
                "Thierry Gil",
                "Aida Todri-Sanial"
            ],
            "title": "Oscillatory neural networks for edge ai computing",
            "venue": "IEEE Computer Society Annual Symposium on VLSI (ISVLSI),",
            "year": 2021
        },
        {
            "authors": [
                "Giorgia Dellaferrera",
                "Gabriel Kreiman"
            ],
            "title": "Error-driven Input Modulation: Solving the Credit Assignment Problem without a Backward Pass",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Stefan Elfwing",
                "Eiji Uchibe",
                "Kenji Doya"
            ],
            "title": "Sigmoid-weighted linear units for neural network function approximation in reinforcement learning",
            "venue": "Neural Networks,",
            "year": 2018
        },
        {
            "authors": [
                "Andreas K Engel",
                "Pascal Fries",
                "Wolf Singer"
            ],
            "title": "Dynamic predictions: oscillations and synchrony in top\u2013down processing",
            "venue": "Nature Reviews Neuroscience,",
            "year": 2001
        },
        {
            "authors": [
                "Maxence Ernoult",
                "Julie Grollier",
                "Damien Querlioz",
                "Yoshua Bengio",
                "Benjamin Scellier"
            ],
            "title": "Updates of equilibrium prop match gradients of backprop through time in an rnn with static input",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Maxence Ernoult",
                "Julie Grollier",
                "Damien Querlioz",
                "Yoshua Bengio",
                "Benjamin Scellier"
            ],
            "title": "Equilibrium propagation with continual weight updates",
            "venue": "arXiv preprint arXiv:2005.04168,",
            "year": 2020
        },
        {
            "authors": [
                "Maxence Ernoult",
                "Fabrice Normandin",
                "Abhinav Moudgil",
                "Sean Spinney",
                "Eugene Belilovsky",
                "Irina Rish",
                "Blake Richards",
                "Yoshua Bengio"
            ],
            "title": "Towards scaling difference target propagation by learning backprop targets",
            "venue": "arXiv preprint arXiv:2201.13415,",
            "year": 2022
        },
        {
            "authors": [
                "E Paxon Frady",
                "Friedrich T Sommer"
            ],
            "title": "Robust computation with rhythmic spike patterns",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2019
        },
        {
            "authors": [
                "Charlotte Frenkel",
                "Giacomo Indiveri"
            ],
            "title": "Reckon: A 28nm sub-mm2 task-agnostic spiking recurrent neural network processor enabling on-chip learning over second-long timescales",
            "venue": "IEEE International Solid-State Circuits Conference (ISSCC),",
            "year": 2022
        },
        {
            "authors": [
                "Will Greedy",
                "Heng Wei Zhu",
                "Joseph Pemberton",
                "Jack Mellor",
                "Rui Ponte Costa"
            ],
            "title": "Single-phase deep learning in cortico-cortical networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Stephen Grossberg"
            ],
            "title": "Competitive learning: From interactive activation to adaptive resonance",
            "venue": "Cognitive science,",
            "year": 1987
        },
        {
            "authors": [
                "Manu Srinath Halvagal",
                "Friedemann Zenke"
            ],
            "title": "The combination of hebbian and predictive plasticity learns invariant object representations in deep sensory networks. bioRxiv, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Heek",
                "Anselm Levskaya",
                "Avital Oliver",
                "Marvin Ritter",
                "Bertrand Rondepierre",
                "Andreas Steiner",
                "Marc van Zee"
            ],
            "title": "Flax: A neural network library and ecosystem for JAX, 2020",
            "venue": "URL http://github.com/google/flax",
            "year": 2020
        },
        {
            "authors": [
                "Geoffrey Hinton"
            ],
            "title": "The forward-forward algorithm: Some preliminary investigations",
            "venue": "arXiv preprint arXiv:2212.13345,",
            "year": 2022
        },
        {
            "authors": [
                "Rasmus H\u00f8ier",
                "D. Staudt",
                "Christopher Zach"
            ],
            "title": "Dual propagation: Accelerating contrastive hebbian learning with dyadic neurons",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "John J Hopfield"
            ],
            "title": "Neurons with graded response have collective computational properties like those of two-state neurons",
            "venue": "Proceedings of the national academy of sciences,",
            "year": 1984
        },
        {
            "authors": [
                "Michael F Hutchinson"
            ],
            "title": "A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines",
            "venue": "Communications in Statistics-Simulation and Computation,",
            "year": 1989
        },
        {
            "authors": [
                "Giacomo Indiveri",
                "Bernab\u00e9 Linares-Barranco",
                "Tara Julia Hamilton",
                "Andr\u00e9 van Schaik",
                "Ralph EtienneCummings",
                "Tobi Delbruck",
                "Shih-Chii Liu",
                "Piotr Dudek",
                "Philipp H\u00e4fliger",
                "Sylvie Renaud"
            ],
            "title": "Neuromorphic silicon neuron circuits",
            "venue": "Frontiers in neuroscience,",
            "year": 2011
        },
        {
            "authors": [
                "Adrien Journ\u00e9",
                "Hector Garcia Rodriguez",
                "Qinghai Guo",
                "Timoleon Moraitis"
            ],
            "title": "Hebbian deep learning without feedback",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Jack Kendall",
                "Ross Pantone",
                "Kalpana Manickavasagam",
                "Yoshua Bengio",
                "Benjamin Scellier"
            ],
            "title": "Training end-to-end analog neural networks with equilibrium propagation",
            "venue": "arXiv preprint arXiv:2006.01981,",
            "year": 2020
        },
        {
            "authors": [
                "Adam Kohan",
                "Edward A. Rietman",
                "Hava T. Siegelmann"
            ],
            "title": "Signal Propagation: The Framework for Learning and Inference in a Forward Pass",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Adam A. Kohan",
                "Edward A. Rietman",
                "Hava T. Siegelmann"
            ],
            "title": "Error Forward-Propagation: Reusing Feedforward Connections to Propagate Errors in Deep Learning, August 2018",
            "year": 2018
        },
        {
            "authors": [
                "John F Kolen",
                "Jordan B Pollack"
            ],
            "title": "Backpropagation without weight transport",
            "venue": "IEEE International Conference on Neural Networks (ICNN\u201994),",
            "year": 1994
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Technical report, University of Toronto,",
            "year": 2009
        },
        {
            "authors": [
                "Axel Laborieux",
                "Friedemann Zenke"
            ],
            "title": "Holomorphic equilibrium propagation computes exact gradients through finite size oscillations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Axel Laborieux",
                "Maxence Ernoult",
                "Benjamin Scellier",
                "Yoshua Bengio",
                "Julie Grollier",
                "Damien Querlioz"
            ],
            "title": "Scaling equilibrium propagation to deep convnets by drastically reducing its gradient estimator bias",
            "venue": "Frontiers in neuroscience,",
            "year": 2021
        },
        {
            "authors": [
                "Julien Launay",
                "Iacopo Poli",
                "Fran\u00e7ois Boniface",
                "Florent Krzakala"
            ],
            "title": "Direct feedback alignment scales to modern deep learning tasks and architectures",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Dong-Hyun Lee",
                "Saizheng Zhang",
                "Asja Fischer",
                "Yoshua Bengio"
            ],
            "title": "Difference target propagation",
            "venue": "In Joint european conference on machine learning and knowledge discovery in databases,",
            "year": 2015
        },
        {
            "authors": [
                "Qianli Liao",
                "Joel Z. Leibo",
                "Tomaso Poggio"
            ],
            "title": "How Important is Weight Symmetry in Backpropagation",
            "year": 2016
        },
        {
            "authors": [
                "Renjie Liao",
                "Yuwen Xiong",
                "Ethan Fetaya",
                "Lisa Zhang",
                "KiJung Yoon",
                "Xaq Pitkow",
                "Raquel Urtasun",
                "Richard Zemel"
            ],
            "title": "Reviving and improving recurrent back-propagation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Timothy P Lillicrap",
                "Daniel Cownden",
                "Douglas B Tweed",
                "Colin J Akerman"
            ],
            "title": "Random synaptic feedback weights support error backpropagation for deep learning",
            "venue": "Nature communications,",
            "year": 2016
        },
        {
            "authors": [
                "Timothy P Lillicrap",
                "Adam Santoro",
                "Luke Marris",
                "Colin J Akerman",
                "Geoffrey Hinton"
            ],
            "title": "Backpropagation and the brain",
            "venue": "Nature Reviews Neuroscience,",
            "year": 2020
        },
        {
            "authors": [
                "David Lipshutz",
                "Charles Windolf",
                "Siavash Golkar",
                "Dmitri Chklovskii"
            ],
            "title": "A Biologically Plausible Neural Network for Slow Feature Analysis",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Sindy L\u00f6we",
                "Peter O\u2019Connor",
                "Bastiaan Veeling"
            ],
            "title": "Putting an end to end-to-end: Gradient-isolated learning of representations",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Alexander Meulemans",
                "Francesco Carzaniga",
                "Johan Suykens",
                "Jo\u00e3o Sacramento",
                "Benjamin F Grewe"
            ],
            "title": "A theoretical framework for target propagation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Alexander Meulemans",
                "Matilde Tristany Farinha",
                "Javier Garc\u00eda Ord\u00f3\u00f1ez",
                "Pau Vilimelis Aceituno",
                "Jo\u00e3o Sacramento",
                "Benjamin F Grewe"
            ],
            "title": "Credit assignment in neural networks through deep feedback control",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Arild N\u00f8kland"
            ],
            "title": "Direct feedback alignment provides learning in deep neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Alexandre Payeur",
                "Jordan Guerguiev",
                "Friedemann Zenke",
                "Blake A Richards",
                "Richard Naud"
            ],
            "title": "Burstdependent synaptic plasticity can coordinate learning in hierarchical circuits",
            "venue": "Nature neuroscience,",
            "year": 2021
        },
        {
            "authors": [
                "Fernando Pineda"
            ],
            "title": "Generalization of back propagation to recurrent and higher order neural networks",
            "venue": "In Neural information processing systems,",
            "year": 1987
        },
        {
            "authors": [
                "Maria Refinetti",
                "St\u00e9phane d\u2019Ascoli",
                "Ruben Ohana",
                "Sebastian Goldt"
            ],
            "title": "Align, then memorise: The dynamics of learning with feedback",
            "year": 2021
        },
        {
            "authors": [
                "Mengye Ren",
                "Simon Kornblith",
                "Renjie Liao",
                "Geoffrey Hinton"
            ],
            "title": "Scaling forward gradient with local losses",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "David E Rumelhart",
                "Geoffrey E Hinton",
                "Ronald J Williams"
            ],
            "title": "Learning representations by back-propagating",
            "venue": "errors. Nature,",
            "year": 1986
        },
        {
            "authors": [
                "Franco Scarselli",
                "Marco Gori",
                "Ah Chung Tsoi",
                "Markus Hagenbuchner",
                "Gabriele Monfardini"
            ],
            "title": "The graph neural network model",
            "venue": "IEEE transactions on neural networks,",
            "year": 2008
        },
        {
            "authors": [
                "Benjamin Scellier",
                "Yoshua Bengio"
            ],
            "title": "Equilibrium propagation: Bridging the gap between energybased models and backpropagation",
            "venue": "Frontiers in computational neuroscience,",
            "year": 2017
        },
        {
            "authors": [
                "Benjamin Scellier",
                "Anirudh Goyal",
                "Jonathan Binas",
                "Thomas Mesnard",
                "Yoshua Bengio"
            ],
            "title": "Generalization of equilibrium propagation to vector field dynamics",
            "venue": "arXiv preprint arXiv:1808.04873,",
            "year": 2018
        },
        {
            "authors": [
                "Catherine D Schuman",
                "Thomas E Potok",
                "Robert M Patton",
                "J Douglas Birdwell",
                "Mark E Dean",
                "Garrett S Rose",
                "James S Plank"
            ],
            "title": "A survey of neuromorphic computing and neural networks in hardware",
            "venue": "arXiv preprint arXiv:1705.06963,",
            "year": 2017
        },
        {
            "authors": [
                "A Stergiou",
                "R Poppe",
                "G Kalliatakis"
            ],
            "title": "Refining activation downsampling with softpool. arxiv 2021",
            "venue": "arXiv preprint arXiv:2101.00440,",
            "year": 2021
        },
        {
            "authors": [
                "Christopher H Stock",
                "Sarah E Harvey",
                "Samuel A Ocko",
                "Surya Ganguli"
            ],
            "title": "Synaptic balancing: A biologically plausible local learning rule that provably increases neural network noise robustness without sacrificing task performance",
            "venue": "PLOS Computational Biology,",
            "year": 2022
        },
        {
            "authors": [
                "Chetan Singh Thakur",
                "Jamal Lottier Molin",
                "Gert Cauwenberghs",
                "Giacomo Indiveri",
                "Kundan Kumar",
                "Ning Qiao",
                "Johannes Schemmel",
                "Runchun Wang",
                "Elisabetta Chicca",
                "Jennifer Olson Hasler"
            ],
            "title": "Large-scale neuromorphic spiking array processors: A quest to mimic the brain",
            "venue": "Frontiers in neuroscience,",
            "year": 2018
        },
        {
            "authors": [
                "James CR Whittington",
                "Rafal Bogacz"
            ],
            "title": "Theories of error back-propagation in the brain",
            "venue": "Trends in cognitive sciences,",
            "year": 2019
        },
        {
            "authors": [
                "Ezekiel Williams",
                "Colin Bredenberg",
                "Guillaume Lajoie"
            ],
            "title": "Flexible phase dynamics for bio-plausible contrastive learning",
            "venue": "arXiv preprint arXiv:2302.12431,",
            "year": 2023
        },
        {
            "authors": [
                "Laurenz Wiskott",
                "Terrence J Sejnowski"
            ],
            "title": "Slow feature analysis: Unsupervised learning of invariances",
            "venue": "Neural computation,",
            "year": 2002
        },
        {
            "authors": [
                "Han Xiao",
                "Kashif Rasul",
                "Roland Vollgraf"
            ],
            "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
            "venue": "arXiv preprint arXiv:1708.07747,",
            "year": 2017
        },
        {
            "authors": [
                "Will Xiao",
                "Honglin Chen",
                "Qianli Liao",
                "Tomaso Poggio"
            ],
            "title": "Biologically-plausible learning algorithms can scale to large datasets, December 2018",
            "year": 2018
        },
        {
            "authors": [
                "Su-in Yi",
                "Jack D Kendall",
                "R Stanley Williams",
                "Suhas Kumar"
            ],
            "title": "Activity-difference training of deep neural networks using memristor crossbars",
            "venue": "Nature Electronics,",
            "year": 2023
        },
        {
            "authors": [
                "Friedemann Zenke",
                "Wulfram Gerstner",
                "Surya Ganguli"
            ],
            "title": "The temporal paradox of Hebbian learning and homeostatic plasticity",
            "venue": "Curr Opin Neurobiol,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Virtually all state-of-the-art artificial intelligence (AI) models are trained using the backpropagation of error algorithm (BP) (Rumelhart et al., 1986). In BP, neural activations are evaluated during the forward pass, and gradients are obtained through a corresponding backward pass. While BP can be implemented efficiently on digital hardware which simulates neural networks, it is less suitable for physical neural networks such as brains or neuromorphic substrates, which operate at lower energy costs. This limitation is mainly due to two technical requirements of the backward pass in BP. First, the backward pass is linear while leaving neuronal activity unaffected (Lillicrap et al., 2020; Whittington & Bogacz, 2019). Second, back-propagation requires the transpose of connectivity matrices, often referred to as \u201cweight symmetry\u201d (Grossberg, 1987). For these reasons, many alternative learning algorithms that can run on dedicated neuromorphic hardware have been designed (Boybat et al., 2018; Thakur et al., 2018; Indiveri et al., 2011; Schuman et al., 2017; Frenkel & Indiveri, 2022; Yi et al., 2023).\nOne such alternative learning algorithm is equilibrium propagation (EP)(Scellier & Bengio, 2017), which can provably estimate gradients using only the network\u2019s own dynamics. Classic EP requires an energy-based model (EBM), such as a differentiable Hopfield network (Hopfield, 1984), which relaxes to an equilibrium for any given input. EP computes gradients by comparing neuronal activity at the free equilibrium to the activity at a second equilibrium that is \u201cnudged\u201d toward a desired target output. Preliminary hardware realization dedicated to EP suggests that it could reduce the energy cost of AI training by four orders of magnitude (Yi et al., 2023). Unlike BP in feed-forward networks, EP uses the same network to propagate feed-forward and feed-back activity, thereby dispensing with\nthe need to linearly propagate error vectors. In particular, EP is a promising candidate for learning through neuronal oscillations (Baldi & Pineda, 1991; Laborieux & Zenke, 2022; Anisetti et al., 2022; Delacour et al., 2021), which is why we focus on EP in this work.\nNevertheless, EP requires weight symmetry and vanishing nudge for unbiased gradient estimates, which limits its potential for neuromorphics compared to other better performing BP alternatives (Ren et al., 2023; Journ\u00e9 et al., 2023; Payeur et al., 2021; Greedy et al., 2022; H\u00f8ier et al., 2023). While Laborieux & Zenke (2022) showed that an oscillation-based extension of EP called holomorphic EP (hEP) removes the bias by integrating oscillations of neuronal activity, this approach has only been demonstrated using symmetric weights. Although a generalization of EP to non-symmetric dynamical systems exists, the approach has only been demonstrated on simple tasks like MNIST (Scellier et al., 2018; Ernoult et al., 2020; Kohan et al., 2018), while it fails to train on CIFAR-10 (Laborieux et al., 2021). Previous work investigated the effect of non-symmetric feedback on training with BP (Lillicrap et al., 2016; N\u00f8kland, 2016; Launay et al., 2020; Clark et al., 2021; Refinetti et al., 2021) and approximate BP (Hinton, 2022; Dellaferrera & Kreiman, 2022; L\u00f6we et al., 2019), but little attention has been given to effect of weight symmetry for EP. In this article we fill this gap. Overall, our main contributions are:\n\u2022 We provide a comprehensive analysis of the individual sources of bias in the gradient estimate from weight asymmetry and finite-size nudge in generalized EP.\n\u2022 An extension of hEP (Laborieux & Zenke, 2022) to non symmetric complex-differentiable dynamical systems, that can be estimated through continuous oscillations.\n\u2022 We propose a new homeostatic loss that reduces the asymmetry of the Jacobian at the free equilibrium point without enforcing perfect weight symmetry.\n\u2022 An empirical demonstration that hEP with homeostatic loss scales to ImageNet 32\u00d732, with a small performance gap compared to the symmetric case."
        },
        {
            "heading": "2 BACKGROUND",
            "text": ""
        },
        {
            "heading": "2.1 GRADIENTS IN CONVERGING DYNAMICAL SYSTEMS",
            "text": "Let us first recall how gradients are computed in converging dynamical systems (Baldi, 1995). Let F be a differentiable dynamical system with state variable u and parameters \u03b8. A subset of units receive static input currents x. The dynamics of the system are governed by:\ndu dt = F (\u03b8,u,x). (1)\nWe further assume that the dynamics of Eq. (1) converge to a fixed point of the state u. This phase of the dynamics is defined as the \u201cfree phase\u201d, because there are no contributions from any target y associated to x. While stable fixed points provably exists for EBMs (Hopfield, 1984), there is no guarantee to converge for arbitrary dynamical systems. However, a fixed point exists if F is a contraction map (Scarselli et al., 2008; Liao et al., 2018). In the following, we just assume the existence of a fixed point and denote it by u\u22170:\n0 = F (\u03b8,u\u22170,x). (2)\nIn u\u22170, the superscript \u2217 indicates convergence to a fixed point, and the subscript 0 stands for the \u201cfree\u201d phase. We further assume that u\u22170 is an implicit function of \u03b8, such that its derivative exist by means of the implicit function theorem (IFT). Given an objective function L that measures the proximity of a subset of u\u2217 (output units) to a target y, the gradient of L with respect to \u03b8 is obtained by the chain rule through the fixed point dLd\u03b8 = dL du\u22170 du\u22170 d\u03b8 , where dL du\u22170 is the error at the output units, and du \u2217 0 d\u03b8 is obtained by differentiating Eq. (2) with respect to \u03b8:\n0 = \u2202F\n\u2202\u03b8 (u\u22170) + JF (u \u2217 0) \u00b7 du\u22170 d\u03b8 ,\ndu\u22170 d\u03b8 = \u2212JF (u\u22170)\u22121 \u00b7 \u2202F \u2202\u03b8 (u\u22170). (3)\nWhere JF (u\u22170) := \u2202F \u2202u (\u03b8,u \u2217 0,x) is the Jacobian of the network at the fixed point (Fig. 1). Replacing du\u22170 d\u03b8 in the expression of the gradient dL d\u03b8 and transposing the whole expression yields (Baldi, 1995):\ndL d\u03b8 \u22a4 = \u2202F \u2202\u03b8 (u\u22170)\n\u22a4\ufe38 \ufe37\ufe37 \ufe38 \u201cpre-synaptic\" \u00b7\n( \u2212JF (u\u22170)\u2212\u22a4 \u00b7\ndL du\u22170\n\u22a4 )\n\ufe38 \ufe37\ufe37 \ufe38 \u201cpost-synaptic\" \u03b4\n. (4)\nWe refer to the part of the expression in parenthesis as the neuronal error vector \u03b4. In the widely used reverse-mode automatic differentiation (AD), a linear system involving the transpose of the Jacobian is solved to compute \u03b4 from the output error dLdu\u22170 , and finally multiply with \u2202F \u2202\u03b8 (u \u2217 0), which is a sparse matrix involving pre-synaptic variables. In feed forward networks, the lower-triangular structure of the Jacobian allows computing \u03b4 recursively layer by layer from the output (Fig. 1c) as exploited in BP (Rumelhart et al., 1986). In converging dynamical systems, recurrent backpropagation (RBP) and variants (Almeida, 1990; Pineda, 1987; Liao et al., 2018) obtain \u03b4 as the fixed point of an auxiliary linear dynamical system using JF (u\u22170)\n\u22a4. In deep equilibrium models (Bai et al., 2019) \u03b4 is found with a root-finding algorithm. In the next section, we review how \u03b4 is obtained in EP."
        },
        {
            "heading": "2.2 NEURONAL ERRORS IN EQUILIBRIUM PROPAGATION",
            "text": "The EP (Scellier & Bengio, 2017) gradient formula for EBMs can be derived without explicit use of the energy function formalism (Scellier et al., 2018), by noticing a similar pattern between the derivative of u\u22170 with respect to parameters \u03b8 (Eq. (3)) and \u03b4 in Eq. (4). While du\u22170 d\u03b8 is obtained by inversion of JF (u\u22170), \u03b4 is obtained by inversion of JF (u \u2217 0)\n\u22a4. However, in EBMs for which there exists a scalar energy function E such that F = \u2212\u2202E\u2202u , we have precisely JF (u \u2217 0) = JF (u \u2217 0) \u22a4 (Fig. 1a) due to the Schwarz theorem on symmetric second derivatives. The remaining difference between Eqs. (3) and (4) is the quantity to which the inverted Jacobian is applied: \u2202F\u2202\u03b8 (u \u2217 0) in Eq. (3) and dLdu\u22170 \u22a4 in Eq. (4). The solution is to define a parameter \u03b2 such that these quantities are equal:\nDefinition 1 (Nudge parameter \u03b2). Let \u03b2 be a scalar parameter that is equal to zero during the free phase, justifying the notation u\u22170, and such that \u2202F \u2202\u03b2 (u \u2217 0) = dL du\u22170 \u22a4 . When \u03b2 \u0338= 0, the target y contributes to the dynamics through dLdu\u22170 . The term \u201cteaching amplitude\u201d refers to |\u03b2|."
        },
        {
            "heading": "Second hidden layer",
            "text": "By applying the same derivation as Eq. (3) for the new parameter \u03b2, we have:\ndu\u2217\nd\u03b2 \u2223\u2223\u2223\u2223 \u03b2=0 = \u2212JF (u\u22170)\u22121 \u00b7 \u2202F \u2202\u03b2 (u\u22170) = \u2212 JF (u\u22170)\u2212\u22a4\ufe38 \ufe37\ufe37 \ufe38\nfor EBMs\n\u00b7 dL du\u22170\n\u22a4\n\ufe38 \ufe37\ufe37 \ufe38 by Def. 1 = \u03b4. (5)\nTherefore, the EP neuronal error vector is the derivative of the fixed point u\u22170 with respect to \u03b2 in \u03b2 = 0, which we denote by d\u03b2u\u2217 for short. Note that the derivation assumes that u\u22170 is an implicit function of \u03b2. In practice, d\u03b2u\u2217 is approximated by finite differences after letting the network settle to a second equilibrium d\u03b2u\u2217 \u2248 (u\u2217\u03b2 \u2212 u\u22170)/\u03b2, which contains a bias due to the finite \u03b2 \u0338= 0 used in the second phase. The EP gradient estimate can then be written without using the energy function by replacing \u03b4 by d\u03b2u\u2217 in Eq. (4):\n\u2207\u0303\u03b8 := \u2202F\n\u2202\u03b8 (u\u22170) \u22a4 \u00b7 d\u03b2u\u2217. (6)\nImportantly, the quantities d\u03b2u\u2217 and \u2207\u0303\u03b8 are defined for any differentiable dynamical system (Scellier et al., 2018), not just for EBMs. However, d\u03b2u\u2217 coincides with \u03b4, and \u2207\u0303\u03b8 with dLd\u03b8 (Eq. (4)), only for EBMs as their Jacobians JF (u\u22170) are symmetric. We provide more details on the relation between Eq. (6) and the usual gradient formula involving the energy function (Scellier & Bengio, 2017) in Appendix B. Intuitively, d\u03b2u\u2217 can be seen as the replacement for \u03b4 that the system can obtain using its own dynamics. When JF (u\u22170) \u0338= JF (u\u22170)\u22a4 (Fig. 1b), the asymmetric part of the Jacobian contributes an additional bias to the estimate of dLd\u03b8 . However, its contribution has never been studied independently because it is masked by the finite nudge \u03b2 in practice. It is worth noting that d\u03b2u\u2217 = 0 for feed forward networks due to the absence of feedback connections (Fig. 1c)."
        },
        {
            "heading": "3 THEORETICAL RESULTS",
            "text": ""
        },
        {
            "heading": "3.1 GENERALIZED HOLOMORPHIC EP",
            "text": "To study individual bias contributions we build on Holomorphic EP (Laborieux & Zenke, 2022), which, assuming weight symmetry, allows computing unbiased gradients with finite nudges. As we will see, the property generalizes to asymmetric weights and d\u03b2u\u2217 can be exactly computed with\nfinite teaching amplitude |\u03b2| > 0 for any complex differentiable (holomorphic) vector field F . We use the fact that for holomorphic functions, derivatives at a given point are equal to the Cauchy integral over a path around the point of interest \u03b2 = 0 (Appel, 2007) (see Appendix A.1 for more details):\ndu\u2217\nd\u03b2 \u2223\u2223\u2223\u2223 \u03b2=0 = 1 2i\u03c0 \u222e \u03b3 u\u2217\u03b2 \u03b22 d\u03b2 = 1 T |\u03b2| \u222b T 0 u\u2217\u03b2(t)e \u22122i\u03c0t/Tdt. (7)\nHere we used the teaching signal \u03b2(t) = |\u03b2|e2i\u03c0t/T , t \u2208 [0, T ] for the path \u03b3, and performed the change of variable d\u03b2 = \u03b2(t) 2i\u03c0T dt, where i \u2208 C is the imaginary unit. Moreover, Eq. (7) assumes that the fixed point u\u2217 is an implicit function of \u03b2 and well-defined on the entire path \u03b3. In other words, we assume that the system is at equilibrium for all t \u2208 [0, T ], which can be achieved when the timescale of the dynamical system is shorter than the one of the time-dependent nudge \u03b2(t). In numerical simulations, we estimate the integral with a discrete number of N points: t/T = k/N, k \u2208 [0, ..., N \u2212 1] (see Appendix E.1). Regardless of whether the dynamical system has a symmetric Jacobian at the free fixed point or not (Fig. 1), the exact value of d\u03b2u\u2217 can be computed for a finite range of teaching amplitudes |\u03b2| (Fig. 2a,b)."
        },
        {
            "heading": "3.2 ESTIMATING GRADIENTS IN CONTINUOUS TIME",
            "text": "Although Eq. (7) implies that d\u03b2u\u2217 is obtained at the end of one period of the teaching signal (t = T ), it can also estimated continuously by letting the integral run over multiple oscillation cycles of the teaching signal \u03b2(t) (Fig. 2 f):\n1\nt|\u03b2| \u222b t 0 u\u2217\u03b2(\u03c4)e \u22122i\u03c0\u03c4/Td\u03c4 \u2212\u2192 t\u2192\u221e du\u2217 d\u03b2 \u2223\u2223\u2223\u2223 \u03b2=0 . (8)\nThis relation is made obvious by splitting the integral over all the completed periods, such that the relative contribution of the remainder integral asymptotically approaches 0 as t\u2192\u221e (see Appendix C.1). The advantage of this approach for physical systems is that d\u03b2u\u2217 need not be accessed at the precise times t = T , thereby dispensing with the need for separate phases (Williams et al., 2023). Moreover, the free fixed point does not need to be evaluated to obtain d\u03b2u\u2217. However, the pre-synaptic term \u2202F\u2202\u03b8 (u \u2217 0) in Eq. (6) a priori still requires evaluating the free fixed point. We will see next that we can also estimate it continuously in time thanks to the Mean Value Theorem from complex analysis (Appel, 2007) (Fig. 2e), Appendix C.1):\n1\nt \u222b t 0 \u2202F \u2202\u03b8 (u\u2217\u03b2(\u03c4))d\u03c4 \u2212\u2192t\u2192\u221e \u2202F \u2202\u03b8 (u\u22170). (9)\nOverall, the gradient estimate \u2207\u0303\u03b8 in the time-continuous form corresponds to the average of presynaptic activation multiplied by the amplitude of the post-synaptic potential, consistent with models of STDP (Dan & Poo, 2004; Clopath & Gerstner, 2010). In the next section, we isolate the contribution to the bias brought by the asymmetry of the Jacobian JF (u\u22170)."
        },
        {
            "heading": "3.3 ISOLATING BIAS FROM JACOBIAN ASYMMETRY",
            "text": "We are now in the position to quantify the bias originating from asymmetric Jacobians in the computation of the neuronal error vector of hEP. By isolating \u2202F\u2202\u03b2 and dL du\u22170\non both ends of Eq. (5), and because they are equal by Definiton 1, we see that both the neuronal error vectors of hEP and RBP are linked by the following relation:\nd\u03b2u \u2217 = JF (u \u2217 0) \u22121JF (u \u2217 0) \u22a4\u03b4. (10)\nThus, for asymmetric Jacobians, \u03b4 and d\u03b2u\u2217 are not aligned which introduces bias in the resulting gradient estimates. To further quantify this bias, we introduce S and A the symmetric and skewsymmetric parts of JF , such that JF (u\u22170) = S +A and JF (u \u2217 0)\n\u22a4 = S \u2212A. Then we can show that when the Frobenius norm \u2223\u2223\u2223\u2223\u2223\u2223S\u22121A\u2223\u2223\u2223\u2223\u2223\u2223\u2192 0, d\u03b2u\u2217 and \u03b4 are linked by (see Appendix C.2 for a proof): d\u03b2u\n\u2217 = \u03b4 \u2212 2S\u22121A\u03b4 + o(S\u22121A\u03b4). (11) This expression highlights the direct dependence of the bias on A and further suggests a way to improve the Jacobian\u2019s symmetry, thus reducing the bias by directly decreasing the norm of A."
        },
        {
            "heading": "3.4 INCREASING FUNCTIONAL SYMMETRY THROUGH JACOBIAN HOMEOSTASIS.",
            "text": "To reduce the norm of A we note that |||A|||2 = Tr(A\u22a4A) = 12 Tr(J \u22a4 F JF ) \u2212 12 Tr(J 2 F ), where the dependence of JF on u\u2217 is omitted. Similar to work by Bai et al. (2021) on deep equilibrium models, we can use the classical Hutchinson trace estimator (Hutchinson, 1989) (see Appendix A.2) to obtain the following objective for minimizing Jacobian asymmetry and, therefore, improving \u201cfunctional symmetry\u201d:\nLhomeo := E \u03b5\u223cN (0,I)\n[ \u2225JF\u03b5\u22252 \u2212 \u03b5\u22a4J2F\u03b5 ] . (12)\nIf we break down the two terms present in this objective, we see that \u2225JF\u03b5\u22252 should be minimized, which corresponds to making the network more robust to random perturbations by minimizing the Frobenius norm of the Jacobian. Interestingly, Stock et al. (2022) showed how a local heterosynaptic learning rule could optimize this objective locally through synaptic balancing (see Eqs. (12)\u2013(14) in Stock et al. (2022)), whereby neurons try to balance their net inputs with their output activity. The second term in Eq. (12) comes with a minus sign and should therefore be maximized. It is, thus, akin to a reconstruction loss, similar to the training of feedback weights in target propagation (TP) (Meulemans et al., 2020; Ernoult et al., 2022; Lee et al., 2015), which can also be local (see e.g. Eq. (13) of Meulemans et al. (2021)), and reminiscent of predictive slow feature learning (Wiskott & Sejnowski, 2002; Lipshutz et al., 2020; Halvagal & Zenke, 2022). However, in our case all weights are trained to optimize Lhomeo, unlike TP approaches which only train feedback weights. Overall, Lhomeo is a plausible objective for increasing and maintaining functional symmetry of the Jacobian in any dynamical system. Finally, it is important to note that while perfect weight symmetry implies functional symmetry, the converse is not true. For instance, Eq. (12) can be optimized in arbitrary systems without reciprocal connectivity. This distinction makes Lhomeo more general than alternative strategies acting on weight symmetry directly (Kolen & Pollack, 1994; Akrout et al., 2019)."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In the following experiments, we used the setting of convergent recurrent neural networks (Ernoult et al., 2019), as well as the linear readout for optimizing the cross-entropy loss (Laborieux et al., 2021) (see appendix E.2). Simulations were implemented in JAX (Bradbury et al., 2018) and Flax (Heek et al., 2020) and datasets obtained through the Tensorflow Datasets API (Abadi et al., 2015). Our code is available on GitHub1 and hyperparameters can be found in Appendix E.3.\nHolomorphic EP matches automatic differentiation at computing d\u03b2u\u2217. We sought to understand the amount of bias due to the finite nudge \u03b2 when using different ways of estimating the neuronal error vector d\u03b2u\u2217. To this end, we trained a two-hidden layer network with independent forward and backward connections (Fig. 1b) on the Fashion MNIST dataset (Xiao et al., 2017) using stochastic gradient descent for 50 epochs. To investigate the evolution of weight symmetry during training, the forward and backward connections were initialized symmetrically. We compared different ways of estimating the neuronal error vector d\u03b2u\u2217. As the ideal \u201cceiling\u201d case, we computed the ground truth using forward-mode automatic differentiation in JAX. Additionally, we computed the classic one-sided EP estimate (Scellier et al., 2018) denoted by \u201cClassic\u201d in Table 1, as well as the Cauchy integral estimate (Eq. 7) computed with various number N of points (see Appendix E.1). In all cases we ran simulations with two teaching amplitudes |\u03b2| = 0.05 and |\u03b2| = 0.5. We report the final validation errors in Table 1. We observed that the bias of the nudge is already noticeable at low teaching amplitude when comparing to better estimates obtained using higher N or the ground truth. The bias became more noticeable when the teaching amplitude was increased. The lowest average validation error was obtained with N = 6 points and |\u03b2| = 0.5, matching the ground truth within statistical uncertainty, and consistent with our theory. Finally, to test the potential of running hEP in continuous time, we implemented the continuous-time estimates of Eqs (8) and (9) where the average spanned five oscillation periods and N = 4 points were used. Crucially, there was no free phase in this setting. Despite longer numerical simulation times in this case (see Appendix E.4), we observed that the continuous-time estimate only suffered from moderate performance drop with respect to the ground truth d\u03b2u\u2217 despite the complete absence of the free phase, thereby confirming our predictions.\n1https://github.com/Laborieux-Axel/generalized-holo-ep\nJacobian asymmetry leads to different learning dynamics. To see whether and how Jacobian asymmetry influences training, we studied the dynamics of the neuronal error vectors d\u03b2u\u2217, obtained through hEP, and \u03b4 from RBP respectively. We used the same experimental setting as in the previous paragraph and trained neural networks with varying degrees of initial weight asymmetry. The degree of initial weight asymmetry was controlled by setting the backward weights wb as wb \u2190 sin(\u03b1)wb + cos(\u03b1)w\u22a4f , where wf are the forward weights. Throughout training, we recorded the evolution of the angle \u03b1, and the cosine between both neuronal error vectors (Fig. 3).\nWe observed that networks trained with RBP all performed comparably in terms of loss (Fig. 3c) whereas their weight symmetry reduced quicker than in networks trained with hEP (Fig. 3a,b). Furthermore, RBP trained networks settled in a parameter regime in which the cosine between \u03b4 and d\u03b2u\n\u2217 was negative for all except the output layer (Fig. 3e,f). In contrast, the performance of networks trained with hEP strongly depended on the initial degree of symmetry (Fig. 3d), and although the cosine angle with RBP decreased during training, it remained positive throughout. These findings could suggest that learning with hEP alone reaches a parameter regime in which the cosine with \u03b4 is too low to further decrease the loss while maintaining the current performance level. In summary, even networks that start with symmetric weights, lose this symmetry during training, which leads to increasing bias with respect to the ground-truth neuronal error vector and impaired task performance. In the next paragraph, we investigate whether and to what extent the homeostatic loss introduced above (cf Eq. (12)) can mitigate this problem.\nTable 1: Validation error in % \u00b1 stddev on Fashion MNIST (n = 5) for different values of |\u03b2|.\nTeach. amp. Classic N = 2 N = 4 N = 6 Continuous-time True d\u03b2u\u2217\n|\u03b2| = 0.05 15.8 \u00b1 0.6 14.6 \u00b1 0.4 14.7 \u00b1 0.7 14.5 \u00b1 0.6 15.8 \u00b1 0.8 14.7 \u00b1 0.6|\u03b2| = 0.5 38.4 \u00b1 6.2 16.3 \u00b1 0.7 14.8 \u00b1 0.8 14.3 \u00b1 0.6 17.3 \u00b1 1.0\na)\nb)\nc)\nf)d)\ne)Training uses\nTraining uses\nInitial =0\nInitial =0d\nFigure 3: Comparison of training dynamics of a two-hidden-layer MLP on Fashion MNIST using RBP (top row) and generalized hEP (bottom row). a,b) Evolution of the angle between forward and backward connections during training on for varying initial angle. Training tends to reduce weight symmetry. c,d) Learning curves. The training loss is dependent on initial angle only for generalized hEP. e,f) Evolution of the cosine similarity between neuronal error vectors of RBP and hEP over training. Curves are averaged over three seeds and shaded areas denote \u00b1 1 stddev.\nJacobian homeostasis improves functional symmetry and enables training on larger datasets. To study the effect of the homeostatic loss (Eq. (12)), we added it scaled by the hyper parameter \u03bbhomeo to the cross entropy loss (see Appendix E.3). For all experiments we estimated the average in Eq. (12) using 5 stochastic realizations of a Gaussian noise \u03b5 per sample in the mini-batch. With these settings, we trained the same network as in the previous paragraphs with the homeostatic loss\na) b) c) d)\non Fashion MNIST (Fig. 4a) and found a small reduction in validation error. We further observed that the added homeostatic loss increased the symmetry of the Jacobian, as defined by the symmetry measure |||S|||/(|||S|||+ |||A|||) (Section 3), over the course of training. In line with this observation, the alignment between \u03b4 and d\u03b2u\u2217 (Fig. 4c-d) also increased for each layer.\nBecause the homeostatic loss is defined on the Jacobian instead of the weights, we hypothesized that its effect may be more general than merely increasing weight symmetry. To test this idea, we introduced a network architecture without feedback connections between adjacent layers, thereby precluding the possibility for symmetric feedback. Specifically, we trained a multi-layer architecture in which the output units fed back to the first hidden layer directly (Fig. 4e) (Kohan et al., 2018; 2023; Dellaferrera & Kreiman, 2022). After training this antisymmetric architecture, we observed the same key effects on the validation error, Jacobian symmetry, and error vector alignment (Fig. 4g,h) as in the reciprocally connected network, thereby confirming our hypothesis that the homeostatic loss is more general than weight alignment (Kolen & Pollack, 1994). In addition, we show in Appendix D that the homeostatic loss generalize to predictive coding networks (Whittington & Bogacz, 2019).\nSince, the improvement in validation error was measurable, but small in the network architectures we studied (Fig. 4b,f), we wondered whether the positive effect of the homeostatic loss would be more pronounced on more challenging tasks. To this end, we extended a recurrent convolutional architecture (Fig. 4i; Laborieux & Zenke, 2022) to asymmetric feedback weights (see Appendix E.2 for further details). We trained this architecture using generalized hEP on CIFAR-10, CIFAR-100 (Krizhevsky, 2009) vision benchmarks both with and without homeostatic loss.\nWithout the homeostatic loss, the network trained on CIFAR-10 reached 60.4% validation accuracy (Table 2). With the homeostatic loss, the performance increased to 84.3 (Table 2), with only approximate weight symmetry (Fig. 4k). Strikingly, this is only a reduction by 4.3% in accuracy in comparison to the symmetric architecture (Laborieux & Zenke, 2022). When using N = 2 instead of the ground truth for estimating d\u03b2u\u2217, we observed an additional drop by 2.9% points due to the finite nudge bias. We also noticed that the homeostatic loss has no measurable effect on performance when training the asymmetric network with RBP (Table 2), suggesting that the homeostatic loss is only beneficial while not restricting model performance. Finally, we reproduced similar findings on\nCIFAR-100 and ImageNet 32\u00d7 32, with a remaining 5% gap on Top-5 validation accuracy from the perfectly symmetric architecture. Together these findings suggest that the addition of homeostatic objectives is increasingly important for training on larger datasets and further highlights the need additional homeostatic processes in the brain (Zenke et al., 2017; Stock et al., 2022)."
        },
        {
            "heading": "5 DISCUSSION",
            "text": "We have studied a generalized form of EP in the absence of perfect weight symmetry for finite nudge amplitudes. We found that relaxing the strong algorithmic assumptions underlying EP rapidly degraded its ability to estimate gradients in larger networks and on real-world tasks. We identified two main sources of bias in generalized EP (Scellier et al., 2018) with adverse effect on performance: The finite nudge amplitude and the Jacobian asymmetry. Further, we illustrate how both issues can be overcome through oscillations by combining holomorphic EP (Laborieux & Zenke, 2022) with a new form of Jacobian homeostasis that encourages functional symmetry, but without imposing strict weight symmetry. Finally, we show that our strategy allows training deep dynamical networks without perfect weight symmetry on ImageNet 32\u00d7 32 with only a small gap in performance to ideal symmetric networks.\nThe role of weight asymmetry for computing gradients has received a lot of attention (Kolen & Pollack, 1994; Lillicrap et al., 2016; Launay et al., 2020; N\u00f8kland, 2016; Payeur et al., 2021; Greedy et al., 2022). Our work corroborates previous findings showing that learning with fully asymmetric weights results in poor alignment with back propagation which limits it to small datasets. Reminiscent of work on Feedback Alignment (Lillicrap et al., 2016), training becomes more challenging in deeper networks and on larger datasets (Xiao et al., 2018; Liao et al., 2016; Bartunov et al., 2018). Although our findings do not allow completely dispensing with the weight symmetry requirement, they are more general since Jacobian symmetry and weight symmetry are not the same, a realization that may prove useful for future algorithmic developments.\nNevertheless, several questions pertaining to plausibility remain open. For instance, removing the bias with hEP requires the neurons to oscillate in the complex plane, which restricts its use to oscillationbased learning and makes its biological interpretation challenging. It could be implemented through phase coding (Frady & Sommer, 2019; Bybee et al., 2022), which would require an additional fast carrier frequency, suggesting potential links to the beta and gamma rhythm in neurobiology (Engel et al., 2001). Another limitation of EP is the convergence to an equilibrium. This requirement makes EP costly on digital computers, which simulate the substrate physics, whereas analog substrates could achieve this relaxation following the laws of physics (Yi et al., 2023; Kendall et al., 2020).\nIn summary, our work further bridges the gap between EP\u2019s assumptions and constraints found in physical neural networks, and opens new avenues for understanding and designing oscillation-based learning algorithms for power efficient learning systems."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We thank all members of the Zenke Lab for comments and discussions. We thank Maxence Ernoult, Nicolas Zucchet, Jack Kendall and Benjamin Scellier for helpful feedback. This project was supported by the Swiss National Science Foundation [grant numbers PCEFP3_202981 and TMPFP3_210282], EU\u2019s Horizon Europe Research and Innovation Programme (CONVOLVE, grant agreement number 101070374) funded through SERI (ref 1131-52302), and the Novartis Research Foundation. The authors declare no competing interests."
        },
        {
            "heading": "A MATHEMATICAL BACKGROUND",
            "text": ""
        },
        {
            "heading": "A.1 COMPLEX ANALYSIS",
            "text": "Here we provide intuitions and the minimal theoretical background on complex analysis to allow appreciating the results presented in Sections 3.1 and 3.2. We refer the interested reader to (Appel, 2007) for a more complete introduction.\nThe notion of complex differentiability in a point z0 \u2208 C is defined for a function f : z \u2208 C 7\u2192 f(z) \u2208 C by the existence of the limit:\nlim z\u2192z0 f(z)\u2212 f(z0) z \u2212 z0 . (13)\nThe limit is written f \u2032(z0), and the function f is said to be \u201cholomorphic\u201d in z0. Although the definition is similar to R-differentiability, the fact that the limit is taken with a complex number that can go to z0 from all directions in the complex plane makes the variation of f more \u201crigid\u201d. However, most usual functions on real numbers can be extended to complex inputs and outputs and are holomorphic, such as the exponential, the sigmoid, all polynomials, trigonometric functions, softmax, and logarithm. Non-holomorphic functions include functions that, for instance, use the absolute value or min/max operators.\nIn exchange for the rigidness of holomorphic functions, one obtains the possibility to express their derivatives at any order through integrals as described by the Cauchy formulas.\nf (n)(z0) = n!\n2i\u03c0 \u222e \u03b3\nf(z)\n(z \u2212 z0)n+1 dz, (14)\nwhere n! is the factorial product of integers up to n, i is the imaginary unit, and the integral is taken over a path \u03b3 \u2282 C going around z0 once and counterclockwise, and provided that f is holomorphic on a set that includes the path. For instance, for n = 0 and the path \u03b3 : \u03b8 \u2208 [\u2212\u03c0, \u03c0] 7\u2192 z0 + rei\u03b8, doing the change of variable dz = riei\u03b8d\u03b8 yields the mean value property linking the value of f in z0 to its variation on a circle of radius r going around z0:\nf(z0) = 1\n2i\u03c0 \u222b \u03c0 \u2212\u03c0 f(z0 + re i\u03b8) z0 + rei\u03b8 \u2212 z0 riei\u03b8d\u03b8,\nf(z0) = 1\n2\u03c0 \u222b \u03c0 \u2212\u03c0 f(z0 + re i\u03b8)d\u03b8. (15)\nAnd f \u2032(z0) can be computed by plugging n = 1 in Eq. (14), and the same change of variable:\nf \u2032(z0) = 1\n2i\u03c0 \u222e \u03b3 f(z) (z \u2212 z0)2 dz,\nf \u2032(z0) = 1\n2i\u03c0 \u222b \u03c0 \u2212\u03c0 f(z0 + re i\u03b8) (z0 + rei\u03b8 \u2212 z0)2 riei\u03b8d\u03b8,\nf \u2032(z0) = 1\n2\u03c0r \u222b \u03c0 \u2212\u03c0 f(z0 + re i\u03b8)e\u2212i\u03b8d\u03b8. (16)\nImportantly, these formulas are integrals over a non-vanishing radius r > 0 where the integrands only involve the function f , which is a promising conceptual step toward biological plausiblity and hardware design since integrals are easier to implement in noisy substrates than finite differences."
        },
        {
            "heading": "A.2 HUTCHINSON TRACE ESTIMATOR",
            "text": "The Hutchinson trace estimator (Hutchinson, 1989) refers to the following \u201ctrick\u201d to estimate the trace of a square matrix M = [mij ] : suppose we have a random vector \u03b5 \u2208 Rn such that E[\u03b5\u03b5\u22a4] = I ,\nwhere I is the identity matrix. Then we have:\nE[\u03b5\u22a4M\u03b5] = E \u2211 ij mij\u03b5i\u03b5j  = \u2211 ij mijE [\u03b5i\u03b5j ]\n= \u2211 ij mijIij\n= \u2211 i mii = Tr(M). (17)\nSince the Frobenius norm can be expressed with the trace operator, it can be estimated in the same way. For instance, the Frobenius norm of the asymmetric part A of the Jacobian JF , A = 12 (JF \u2212J \u22a4 F ) is given by:\n|||A|||2 = Tr(A\u22a4A)\n= 1 4 Tr ( (J\u22a4F \u2212 JF )(JF \u2212 J\u22a4F ) ) = 1\n4 Tr ( J\u22a4F JF \u2212 J\u22a4F J\u22a4F \u2212 JFJF + JFJ\u22a4F ) = 1\n2 Tr(J\u22a4F JF )\u2212\n1 2 Tr(J2F )\n= 1\n2 E[\u03b5\u22a4J\u22a4F JF\u03b5]\u2212\n1 2 E[\u03b5\u22a4J2F\u03b5] using Eq. (17)\n= 1 2 E [ \u2225JF\u03b5\u22252 \u2212 \u03b5\u22a4J2F\u03b5 ] . (18)\nTo obtain the fourth line, we used the linear property of the trace operator, the fact that the trace is unchanged by the matrix transposition, and fact that Tr(M1M2) = Tr(M2M1) for any two matrices M1 and M2, which is a consequence of the symmetry of the Frobenius inner product. The final quantity in Eq. (18) can be estimated efficiently with vector-Jacobian product routines available in automatic differentiation frameworks such as JAX or PyTorch, without instantiating the full Jacobian matrix."
        },
        {
            "heading": "B RELATION BETWEEN THE ENERGY AND VECTOR FIELD GRADIENT FORMULAS",
            "text": "Here we briefly provide an intuition for how the EP gradient formula obtained with the energy function formalism (Scellier & Bengio, 2017) relates to the EP gradient formula obtained directly with the vector field (Eq. (6), Scellier et al. (2018)). Although the formulas seem different, they are in fact equivalent. The main takeaway is that the connections wij and wji are fused into a single parameter in the energy function formalism, whereas the vector field formalism distinguishes the two. For a continuous Hopfield model with two-body interaction terms of the form \u2212\u03c3(ui)wij\u03c3(uj), the energy-based (EB) gradient of the loss with respect to wij is (Scellier & Bengio, 2017):\ndL dwij \u2223\u2223\u2223\u2223 EB = d\u03c3(u\u2217i )\u03c3(u \u2217 j ) d\u03b2 \u2223\u2223\u2223\u2223 \u03b2=0 .\nIf we use the product rule of derivatives, we obtain:\ndL dwij \u2223\u2223\u2223\u2223 EB = \u03c3(u\u2217i )\u03c3 \u2032(u\u2217j ) du\u2217j d\u03b2 \u2223\u2223\u2223\u2223 \u03b2=0 + \u03c3(u\u2217j )\u03c3 \u2032(u\u2217i ) du\u2217i d\u03b2 \u2223\u2223\u2223\u2223 \u03b2=0 .\nBoth terms in the sum corresponds respectively to the vector field (VF) gradient formula for wij and wji. Here \u2202F\u2202wij = \u03c3(u \u2217 i )\u03c3 \u2032(u\u2217j ). Therefore, we have:\ndL dwij \u2223\u2223\u2223\u2223 EB = dL dwij \u2223\u2223\u2223\u2223 VF + dL dwji \u2223\u2223\u2223\u2223 VF ."
        },
        {
            "heading": "C THEORETICAL PROOFS",
            "text": ""
        },
        {
            "heading": "C.1 CONVERGENCE OF THE CONTINUOUS-TIME ESTIMATE",
            "text": "We recall the limit:\n1\nt|\u03b2| \u222b t 0 u\u2217\u03b2(\u03c4)e \u22122i\u03c0\u03c4/Td\u03c4 \u2212\u2192 t\u2192\u221e du\u2217 d\u03b2 \u2223\u2223\u2223\u2223 \u03b2=0 .\nWe split the running time t into an integer k amount of periods t = kT + t\u2032. Then by splitting the integral we have:\n1\nt|\u03b2| \u222b t 0 u\u2217\u03b2(\u03c4)e \u22122i\u03c0\u03c4/Td\u03c4 = 1 t|\u03b2| \u222b kT+t\u2032 0 u\u2217\u03b2(\u03c4)e \u22122i\u03c0\u03c4/Td\u03c4\n= 1\nt|\u03b2|\n( k \u222b T 0 u\u2217\u03b2(\u03c4)e \u22122i\u03c0\u03c4/Td\u03c4 + \u222b t\u2032 0 u\u2217\u03b2(\u03c4)e \u22122i\u03c0\u03c4/Td\u03c4 )\n= 1\nt|\u03b2|\n( kT |\u03b2| du \u2217\nd\u03b2 \u2223\u2223\u2223\u2223 \u03b2=0 + \u222b t\u2032 0 u\u2217\u03b2(\u03c4)e \u22122i\u03c0\u03c4/Td\u03c4 )\n= t\u2212 t\u2032\nt\ndu\u2217\nd\u03b2 \u2223\u2223\u2223\u2223 \u03b2=0 + 1 t|\u03b2| \u222b t\u2032 0 u\u2217\u03b2(\u03c4)e \u22122i\u03c0\u03c4/Td\u03c4. (19)\nSince 0 < t\u2032 < T , the first term converges toward d\u03b2u\u2217, and the second term is a bounded integral divided by t, and therefore goes to 0 as t\u2192\u221e. For the pre-synaptic term we recall the statement:\n1\nt \u222b t 0 \u2202F \u2202\u03b8 (u\u2217\u03b2(\u03c4))d\u03c4 \u2212\u2192t\u2192\u221e \u2202F \u2202\u03b8 (u\u22170).\nThe proof is the same as above, once we have shown that:\n1\nT \u222b T 0 \u2202F \u2202\u03b8 (u\u2217\u03b2(\u03c4))d\u03c4 = \u2202F \u2202\u03b8 (u\u22170). (20)\nBy applying the Mean Value Theorem (Eq. (15)) (Appel, 2007) to the function \u03c6 : \u03b2 \u2208 Du\u2217 7\u2192 \u2202F \u2202\u03b8 (u \u2217 \u03b2) in \u03b2 = 0, we have that:\n\u2202F \u2202\u03b8 (u\u22170) = 1 2\u03c0 \u222b 2\u03c0 0 \u2202F \u2202\u03b8 (u\u2217rei\u03b1) d\u03b1,\nwhere r is a radius such that the circle is contained in the domain of definition Du\u2217 of \u03c6. Then we apply the equation in r = |\u03b2| and do the change of variable \u03b1 = 2\u03c0\u03c4/T , which gives Eq. (20)."
        },
        {
            "heading": "C.2 BIAS TERM FROM JACOBIAN ASYMMETRY",
            "text": "We recall that d\u03b2u\u2217 and \u03b4 are linked by:\nd\u03b2u \u2217 = JF (u \u2217 0) \u22121JF (u \u2217 0) \u22a4\u03b4. (21)\nLet S and A be the symmetric and skew-symmetric parts of JF , such that S = (JF (u\u22170) + JF (u \u2217 0) \u22a4)/2 and A = (JF (u\u22170) \u2212 JF (u\u22170)\u22a4)/2. Since JF (u\u22170) is assumed invertible and |||A|||\nis assumed small, we have that S is invertible and:\nJF (u \u2217 0) \u22121JF (u \u2217 0) \u22a4 = (S +A)\u22121(S \u2212A) = (I + S\u22121A)\u22121S\u22121(S \u2212A) = (I + S\u22121A)\u22121(I \u2212 S\u22121A)\n= ( \u221e\u2211 k=0 (\u22121)k(S\u22121A)k ) (I \u2212 S\u22121A)\n= \u221e\u2211 k=0 (\u22121)k(S\u22121A)k \u2212 \u221e\u2211 k=0 (\u22121)k(S\u22121A)k+1\n= \u221e\u2211 k=0 (\u22121)k(S\u22121A)k + \u221e\u2211 k=1 (\u22121)k(S\u22121A)k\n= I \u2212 2S\u22121A+ 2 \u221e\u2211 k=2 (\u22121)k(S\u22121A)k.\nWe used the Neumann Series to compute the inverse (I + S\u22121A)\u22121, which converges given the assumption \u2223\u2223\u2223\u2223\u2223\u2223S\u22121A\u2223\u2223\u2223\u2223\u2223\u2223\u2192 0. From Eq (21) we can write: d\u03b2u \u2217 = \u03b4 \u2212 2S\u22121A\u03b4 + o(S\u22121A\u03b4)"
        },
        {
            "heading": "D APPLICATION TO PREDICTIVE CODING NETWORKS",
            "text": "To demonstrate that our approach of regularizing the Jacobian to improve its symmetry is not restricted to the networks trained in the main manuscript, and generalize to other dynamical systems, we run an additional training experiment on a different model architecture, namely predictive coding networks (PCNs) (Whittington & Bogacz, 2019).\nThe key feature of the predictive coding network architecture is the presence in each layer l of explicit error neurons \u03f5l in addition to value neurons ul (Fig. 5). The evolution of the error and value neurons are given by:\n\u03f5l = ul \u2212wfl \u03c3(ul\u22121), dul dt = \u2212\u03f5l + \u03c3\u2032(ul)\u2299wbl+1\u03f5l+1,\nwhere\u2299 denotes the element-wise product, and the superscripts f and b denote forward and backward weights respectively. We train such a network with generalized hEP on Fashion MNIST with and without the homeostatic loss, and observe the same behavior as in Fig. 4 that optimizing the homeostatic loss for the PCN improves the symmetry of the Jacobian of the network, which translates into better validation error (Fig. 5). The hyperparameters used in this experiment are reported in Table 3."
        },
        {
            "heading": "E SIMULATION DETAILS",
            "text": ""
        },
        {
            "heading": "E.1 NUMERICAL ESTIMATE OF THE CAUCHY INTEGRAL",
            "text": "For evaluating the integral in Eq. (7), we set an integer number of N > 1 values of \u03b2:\n\u03b2k = |\u03b2|e2i\u03c0k/N , k \u2208 [0, 1, ..., N \u2212 1].\nThe integral is then approximated by:\n1\nT |\u03b2| \u222b T 0 u\u2217\u03b2(\u03c4)e \u22122i\u03c0\u03c4/Td\u03c4 \u2248 1 N |\u03b2| N\u2211 k=0 u\u2217\u03b2ke \u22122i\u03c0k/N . (22)\nThe leading term of the bias when using N points is |\u03b2|N/(N + 1)!, where ! is the factorial product. However, unless specified we used forward mode automatic differentiation to compute the ground truth d\u03b2u\u2217 without bias."
        },
        {
            "heading": "E.2 ARCHITECTURE AND DYNAMICS",
            "text": "Multi-layer networks. The disrete dynamics of the multi-layer networks of experiments in Figs. 2, 3, 4 on Fashion MNIST read: u1 \u2190 winx+ \u2211 l\u2032 to 1 w1,l\u2032\u03c3(ul\u2032) + b1, ul \u2190 \u2211 l\u2032 to l wl,l\u2032\u03c3(ul\u2032) + bl,\nuL \u2190 wfL,L\u22121\u03c3(uL\u22121) + bL + \u03b2w\u22a4ro (y \u2212 softmax (wro\u03c3(uL) + bro)) ,\nwhere wl,l\u2032 is the weight matrix going from layer l\u2032 to l, and bl the bias of layer l. (wro,bro) is a 10\u00d7 10 readout layer of the parameterized cross-entropy loss of Laborieux et al. (2021). In particular, it does not belong to the network connections since it does not influence the free dynamics (when \u03b2 = 0). The activation \u03c3 is the shifted sigmoid x 7\u2192 1/(1 + e\u22124x+2).\nConvolutional networks. For the convolutional neural network experiments, the discrete dynamics read:  u1 \u2190 P(wf1 \u2217 x) + P\u0303(u1,wb2,u2) + b1, u2 \u2190 P(wf2 \u2217 \u03c3(u1)) + P\u0303(u2,wb3,u3) + b2, u3 \u2190 P(wf3 \u2217 \u03c3(u2)) + P\u0303(u3,wb4,u4) + b3,\nu4 \u2190 P(wf4 \u2217 \u03c3(u3)) + b4 + \u03b2w\u22a4ro (y \u2212 softmax (wro\u03c3(uL) + bro)) .\nWhere the superscripts f , b respectively mean forward and backward weights. P denotes Softmax pooling (Stergiou et al., 2021). The backward convolutional module going from ul+1 to ul is defined as:\nP\u0303(ul,wb,ul+1) := \u2202\n\u2202\u03c3(ul)\n[ ul+1 \u00b7 P(wb \u2217 \u03c3(ul)) ] .\nThe activation \u03c3 is a sigmoid-weighted linear unit (Elfwing et al., 2018) defined by: \u03c3(x) := (x 2 ) 1 1 + e\u2212x + ( 1\u2212 x 2 ) 1 1 + e\u2212x+2 .\nThe shapes of u1,u2,u3,u4 are respectively (16, 16, 128), (8, 8, 256), (4, 4, 512), (1, 1, 512). The weights are all 3\u00d7 3 kernel with no strides. The paddings are all \u2018same\u2019 except for the last layer. The Softmax pooling has stride 2 and window-size 2\u00d7 2. It is defined by (Stergiou et al., 2021):\ny = \u2211 i\u2208R\n( exi\u2211\nj\u2208R e xj\n) xi,\nwhere R is the 2\u00d7 2 window."
        },
        {
            "heading": "E.3 HYPERPARAMETERS",
            "text": "Choice of hyperparameters. The learning rate was the main hyperparameter searched for by coarse grid search. The number of time steps was tuned such that the networks have enough time to equilibrate by measuring the norm of the vector field. The coefficient for the homeostatic loss was searched by grid search over the range of 0.1, 1.0, and 10.0. Hyperparameters for the CIFAR-10/100 and ImageNet 32\u00d7 32 were chosen based on those reported in the literature for similar experiments (Laborieux et al., 2021; Laborieux & Zenke, 2022).\nIn the case of continuous-time estimates (Table 1, Eqs. (8), (9)), we used 60 time steps for each of the N = 4 \u03b2 values, and five periods, for a total to 1200 time steps per batch."
        },
        {
            "heading": "E.4 HARDWARE AND SIMULATION TIMES",
            "text": "Simulations on Fashion MNIST were run on single RTX 5000 GPU, for 10mins per run and 1 hour for the continuous-time estimate. The convolutional network simulations were run on an in-house cluster consisting of 5 nodes with 4 v100 NVIDIA GPUs each, one node with 4 A100 NVIDIA GPUs, and one node with 8 A40 NVIDIA GPUs. Runs on CIFAR-10/100 took about 8 hours each, and the Imagenet 32\u00d7 32 runs took 72 hours each. Each run was parallelized over 4 or 8 GPUs by splitting the batch. The free phase with the convolutional architecture was done in half precision, which provided a 2\u00d7 speed up compared to using full precision."
        }
    ],
    "title": "IMPROVING EQUILIBRIUM PROPAGATION WITHOUT WEIGHT SYMMETRY THROUGH JACOBIAN HOMEOSTASIS",
    "year": 2024
}