{
    "abstractText": "Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network\u2019s robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called \u201cForget to Mitigate Overfitting (FOMO)\". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model\u2019s information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last robust test accuracy while improving the state-of-the-art robustness. Furthermore, FOMO provides a better trade-off between standard and robust accuracy, outperforming baseline adversarial methods. Finally, our framework is robust to AutoAttacks and increases generalization in many real-world scenarios.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Vijaya Raghavan"
        },
        {
            "affiliations": [],
            "name": "T Ramkumar"
        },
        {
            "affiliations": [],
            "name": "Bahram Zonooz"
        },
        {
            "affiliations": [],
            "name": "Elahe Arani"
        }
    ],
    "id": "SP:eef756ee26835f4789f4447de578995d05e61e29",
    "references": [
        {
            "authors": [
                "Ibrahim Alabdulmohsin",
                "Hartmut Maennel",
                "Daniel Keysers"
            ],
            "title": "The impact of reinitialization on generalization in convolutional neural networks",
            "venue": "arXiv preprint arXiv:2109.00267,",
            "year": 2021
        },
        {
            "authors": [
                "Devansh Arpit",
                "Stanis\u0142aw Jastrz\u0119bski",
                "Nicolas Ballas",
                "David Krueger",
                "Emmanuel Bengio",
                "Maxinder S Kanwal",
                "Tegan Maharaj",
                "Asja Fischer",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "A closer look at memorization in deep networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Can Bakiskan",
                "Metehan Cekic",
                "Upamanyu Madhow"
            ],
            "title": "Early layers are more important for adversarial robustness",
            "venue": "In ICLR 2022 Workshop on New Frontiers in Adversarial Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Robert A Bjork",
                "Ted W Allen"
            ],
            "title": "The spacing effect: Consolidation or differential encoding",
            "venue": "Journal of Verbal Learning and Verbal Behavior,",
            "year": 1970
        },
        {
            "authors": [
                "Robert A Bjork",
                "Elizabeth L Bjork"
            ],
            "title": "Forgetting as the friend of learning: Implications for teaching and self-regulated learning",
            "venue": "Advances in Physiology Education,",
            "year": 2019
        },
        {
            "authors": [
                "Nicholas Carlini",
                "David Wagner"
            ],
            "title": "Towards evaluating the robustness of neural networks",
            "venue": "In 2017 ieee symposium on security and privacy (sp),",
            "year": 2017
        },
        {
            "authors": [
                "Yair Carmon",
                "Aditi Raghunathan",
                "Ludwig Schmidt",
                "John C Duchi",
                "Percy S Liang"
            ],
            "title": "Unlabeled data improves adversarial robustness",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Chen Chen",
                "Jingfeng Zhang",
                "Xilie Xu",
                "Tianlei Hu",
                "Gang Niu",
                "Gang Chen",
                "Masashi Sugiyama"
            ],
            "title": "Guided interpolation for adversarial training",
            "venue": "arXiv preprint arXiv:2102.07327,",
            "year": 2021
        },
        {
            "authors": [
                "Tianlong Chen",
                "Zhenyu Zhang",
                "Sijia Liu",
                "Shiyu Chang",
                "Zhangyang Wang"
            ],
            "title": "Robust overfitting may be mitigated by properly learned smoothening",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Francesco Croce",
                "Matthias Hein"
            ],
            "title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Yinpeng Dong",
                "Ke Xu",
                "Xiao Yang",
                "Tianyu Pang",
                "Zhijie Deng",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "Exploring memorization in adversarial training",
            "venue": "arXiv preprint arXiv:2106.01606,",
            "year": 2021
        },
        {
            "authors": [
                "Robert Geirhos",
                "Carlos RM Temme",
                "Jonas Rauber",
                "Heiko H Sch\u00fctt",
                "Matthias Bethge",
                "Felix A Wichmann"
            ],
            "title": "Generalisation in humans and deep neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "arXiv preprint arXiv:1412.6572,",
            "year": 2014
        },
        {
            "authors": [
                "Oliver Hardt",
                "Karim Nader",
                "Lynn Nadel"
            ],
            "title": "Decay happens: the role of active forgetting in memory",
            "venue": "Trends in cognitive sciences,",
            "year": 2013
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Thomas Dietterich"
            ],
            "title": "Benchmarking neural network robustness to common corruptions and perturbations",
            "venue": "arXiv preprint arXiv:1903.12261,",
            "year": 2019
        },
        {
            "authors": [
                "Shuntaro Izawa",
                "Srikanta Chowdhury",
                "Toh Miyazaki",
                "Yasutaka Mukai",
                "Daisuke Ono",
                "Ryo Inoue",
                "Yu Ohmura",
                "Hiroyuki Mizoguchi",
                "Kazuhiro Kimura",
                "Mitsuhiro Yoshioka"
            ],
            "title": "Rem sleep\u2013active mch neurons are involved in forgetting hippocampus-dependent memories",
            "year": 2019
        },
        {
            "authors": [
                "Pavel Izmailov",
                "Dmitrii Podoprikhin",
                "Timur Garipov",
                "Dmitry Vetrov",
                "Andrew Gordon Wilson"
            ],
            "title": "Averaging weights leads to wider optima and better generalization",
            "venue": "arXiv preprint arXiv:1803.05407,",
            "year": 2018
        },
        {
            "authors": [
                "Giri P Krishnan",
                "Timothy Tadros",
                "Ramyaa Ramyaa",
                "Maxim Bazhenov"
            ],
            "title": "Biologically inspired sleep algorithm for artificial neural networks",
            "year": 1908
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "Temporal ensembling for semi-supervised learning",
            "venue": "arXiv preprint arXiv:1610.02242,",
            "year": 2016
        },
        {
            "authors": [
                "Saehyung Lee",
                "Hyungyu Lee",
                "Sungroh Yoon"
            ],
            "title": "Adversarial vertex mixup: Toward better adversarially robust generalization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Lin Li",
                "Michael W. Spratling"
            ],
            "title": "Data augmentation alone can improve adversarial training",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Tao Li",
                "Yingwen Wu",
                "Sizhe Chen",
                "Kun Fang",
                "Xiaolin Huang"
            ],
            "title": "Subspace adversarial training",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "arXiv preprint arXiv:1706.06083,",
            "year": 2017
        },
        {
            "authors": [
                "Preetum Nakkiran",
                "Gal Kaplun",
                "Yamini Bansal",
                "Tristan Yang",
                "Boaz Barak",
                "Ilya Sutskever"
            ],
            "title": "Deep double descent: Where bigger models and more data hurt",
            "venue": "Journal of Statistical Mechanics: Theory and Experiment,",
            "year": 2021
        },
        {
            "authors": [
                "Yuval Netzer",
                "Tao Wang",
                "Adam Coates",
                "Alessandro Bissacco",
                "Bo Wu",
                "Andrew Y Ng"
            ],
            "title": "Reading digits in natural images with unsupervised feature learning",
            "year": 2011
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Zhiyuan Li",
                "Srinadh Bhojanapalli",
                "Yann LeCun",
                "Nathan Srebro"
            ],
            "title": "Towards understanding the role of over-parametrization in generalization of neural networks",
            "year": 1805
        },
        {
            "authors": [
                "Leslie Rice",
                "Eric Wong",
                "Zico Kolter"
            ],
            "title": "Overfitting in adversarially robust deep learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Ludwig Schmidt",
                "Shibani Santurkar",
                "Dimitris Tsipras",
                "Kunal Talwar",
                "Aleksander Madry"
            ],
            "title": "Adversarially robust generalization requires more data",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Yichun Shuai",
                "Binyan Lu",
                "Ying Hu",
                "Lianzhang Wang",
                "Kan Sun",
                "Yi Zhong"
            ],
            "title": "Forgetting is regulated through rac activity in drosophila",
            "year": 2010
        },
        {
            "authors": [
                "Shoaib Ahmed Siddiqui",
                "Thomas Breuel"
            ],
            "title": "Identifying layers susceptible to adversarial attacks",
            "venue": "arXiv preprint arXiv:2107.04827,",
            "year": 2021
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian Goodfellow",
                "Rob Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "arXiv preprint arXiv:1312.6199,",
            "year": 2013
        },
        {
            "authors": [
                "Guillermo Valle-Perez",
                "Chico Q Camargo",
                "Ard A Louis"
            ],
            "title": "Deep learning generalizes because the parameter-function map is biased towards simple functions",
            "venue": "arXiv preprint arXiv:1805.08522,",
            "year": 2018
        },
        {
            "authors": [
                "Dongxian Wu",
                "Shu-Tao Xia",
                "Yisen Wang"
            ],
            "title": "Adversarial weight perturbation helps robust generalization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jason Yosinski",
                "Jeff Clune",
                "Yoshua Bengio",
                "Hod Lipson"
            ],
            "title": "How transferable are features in deep neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Chaojian Yu",
                "Bo Han",
                "Li Shen",
                "Jun Yu",
                "Chen Gong",
                "Mingming Gong",
                "Tongliang Liu"
            ],
            "title": "Understanding robust overfitting of adversarial training and beyond",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Sergey Zagoruyko",
                "Nikos Komodakis"
            ],
            "title": "Wide residual networks",
            "venue": "arXiv preprint arXiv:1605.07146,",
            "year": 2016
        },
        {
            "authors": [
                "Runtian Zhai",
                "Tianle Cai",
                "Di He",
                "Chen Dan",
                "Kun He",
                "John Hopcroft",
                "Liwei Wang"
            ],
            "title": "Adversarially robust generalization just requires more unlabeled data",
            "year": 1906
        },
        {
            "authors": [
                "Chiyuan Zhang",
                "Samy Bengio",
                "Moritz Hardt",
                "Benjamin Recht",
                "Oriol Vinyals"
            ],
            "title": "Understanding deep learning (still) requires rethinking generalization",
            "venue": "Communications of the ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Haichao Zhang",
                "Wei Xu"
            ],
            "title": "Adversarial interpolation training: A simple approach for improving model robustness",
            "year": 2019
        },
        {
            "authors": [
                "Hongyang Zhang",
                "Yaodong Yu",
                "Jiantao Jiao",
                "Eric Xing",
                "Laurent El Ghaoui",
                "Michael Jordan"
            ],
            "title": "Theoretically principled trade-off between robustness and accuracy",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Huan Zhang",
                "Hongge Chen",
                "Zhao Song",
                "Duane Boning",
                "Inderjit S Dhillon",
                "Cho-Jui Hsieh"
            ],
            "title": "The limitations of adversarial training and the blind-spot attack",
            "venue": "arXiv preprint arXiv:1901.04684,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep neural networks (DNNs) have demonstrated outstanding performance in various domains, ranging from computer vision to natural language processing and speech recognition. However, recent studies (Szegedy et al., 2013; Goodfellow et al., 2014) have revealed the susceptibility of DNNs to adversarial attacks. These attacks are initiated by adding small, yet intentionally crafted, imperceptible perturbations to input data, resulting in erroneous predictions by DNNs. The adversarial attacks pose a serious security threat in applications such as autonomous vehicles, medical diagnosis, and other areas where DNNs are used to make critical decisions. Adversarial Training (AT) (Madry et al., 2017; Zhang et al., 2019a) has emerged as a promising solution to address the issue of robustness and security of DNNs against adversarial attacks. It involves training DNNs using adversarial examples to improve their resilience against such attacks.\nRecently, Rice et al. (2020) reported \u201crobust overfitting\" in AT, where the robust performance on test data degrades with further training. Figure 1 (left) illustrates this phenomenon, where the adversarial test accuracy significantly lags behind the adversarial train accuracy, leading to robust overfitting. Although this phenomenon is present in AT, conventional methods to prevent benign overfitting in standard training, such as explicit regularization and data augmentation, do not improve performance compared to the best accuracy achieved with early stopping in AT. While early stopping is a useful technique to prevent robust overfitting, it may not be desirable due to the occurrence of the \u201cdouble descent\" 2 phenomenon in AT (Rice et al., 2020; Nakkiran et al., 2021). Thus, the potential existence of robust overfitting in AT, and the failure of conventional methods to mitigate it, present a striking lacuna in building robust machine learning systems.\n\u2217Contributed equally. 1Code is available at https://github.com/NeurAI-Lab/FOMO. 2Double descent is a phenomenon in deep learning where a model\u2019s test error initially increases, decreases,\nand then increases again as model complexity or dataset size increases (Nakkiran et al., 2021).\n60 80 100 120 140 160 180 200 Epochs\n30\n40\n50\n60\n70\n80\n90\nAc cu\nra cy\nPGD-AT (Baseline)\n60 80 100 120 140 160 180 200 Epochs\nFOMO (Ours) Robust Test Acc Robust Train Acc Robust Test Acc (Stable model)\nFigure 1: Comparison of robust overfitting in baseline PGD-AT (left) and our method FOMO (right), highlighting the gap between robust training and test accuracy on CIFAR-10 with PreAct-ResNet18. FOMO significantly reduces robust overfitting compared to the baseline\u2019s best early-stop checkpoint.\nUnlike DNNs, humans excel in generalization in dynamic environments, facilitated by the interplay of remembering, forgetting, and relearning processes in the brain (Richards & Frankland, 2017). As the eminent psychologist William James noted, \"If we remembered everything, we should on most occasions be as ill off as if we remembered nothing.\" Paradoxically, one condition for remembering is that we should forget. Similarly, our brain has the remarkable ability to remember and actively forget information as needed, which is necessary for learning and achieving generalization. Although the underlying mechanism of active forgetting remains elusive, neuroscience and cognitive psychology research (Gravitz, 2019; Izawa et al., 2019) provides growing evidence that the brain actively forgets by pruning neurons, shaping the learning-memory process (Shuai et al., 2010; Hardt et al., 2013; Davis & Zhong, 2017; Richards & Frankland, 2017). Furthermore, the interaction between forgetting, remembering, and relearning is reinforced by the presence of multiple memory systems. Consolidation through long-term memory storage enables us to preserve crucial knowledge for future use and retrieval, while relearning reinforces previously learned information (Bjork & Allen, 1970). Together, these three components work in harmony in the regulation of the learning process to achieve better generalizability in the real world. Therefore, emulating these aspects in DNNs might hold the key to achieving robust generalization in AT.\nTherefore, we propose a general learning paradigm, which we refer to as FOrgetting for Mitigating Overfitting (FOMO), to address the problem of robust overfitting of parameterized networks during AT. We consciously simulate the process of active forgetting in the DNNs by re-randomizing a random subset of weights periodically during AT. Each forgetting phase is followed by a relearning phase, which we call \u2019interleaved training\u2019. Our method alternates between the forgetting and relearning phases while consolidating generalized features. With extensive experiments on multiple datasets, we show that our proposed training paradigm boosts the robust performance and generalization of AT models to a greater extent by alleviating robust overfitting. Our main contributions are as follows;\n\u2022 FOMO, an adversarial training paradigm to improve the performance and generalization of DNNs through the lens of active forgetting and relearning.\n\u2022 We demonstrate the efficacy of FOMO against the AutoAttacks. \u2022 Our method alleviates robust overfitting and achieves significant results across multiple\narchitectures and datasets. \u2022 Our proposed training paradigm is robust to natural corruptions and leads to flatter minima."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 ADVERSARIAL LEARNING",
            "text": "Adversarial training (AT) has been shown to be an effective method of countering adversarial attacks; thus we define adversarial learning settings as envisioned in Madry et al. (2017). We consider a classification task with a given K-class dataset D = {(xi, yi)}ni=1 \u2286 X \u00d7 Y , where x \u2208 Rd represents an input sampled from a certain data-generating distribution P in an i.i.d. manner, and Y := 1, . . . ,K represents a set of possible class labels. Let f\u03b8 \u2208 Rd be a neural network modeled to predict classes. The notion of adversarial robustness requires f\u03b8 to perform well not only on P but also on the worst-case distribution near P under a certain distance metric. More concretely, the\nadversarial robustness we primarily focus on in this paper is the \u2113p-robustness: that is, for a given p \u2265 1 and a small \u03f5 > 0, we aim to train a classifier f\u03b8 that correctly classifies x + \u03b4 for any ||\u03b4||p \u2264 \u03f5 as y, where (x, y) \u223c P . In AT, the training data is sampled from adversarial regions incorporated to train the classifier. Madry et al. (2017) formulated AT as a min-max optimization problem:\nmin \u03b8 n\u2211 i=1 max \u2225\u03b4\u2225p\u2264\u03f5 Ladv(f\u03b8(xi + \u03b4), yi), (1)\nwhere f\u03b8 is the DNN with parameters \u03b8, and Ladv(.) is the typical classification loss function (e.g., the cross-entropy (CE) loss). The inner maximization is to find an adversarial example xi0 that maximizes the loss. The outer minimization is to optimize network parameters \u03b8 that minimize the loss on adversarial examples."
        },
        {
            "heading": "2.2 ROBUST GENERALIZATION.",
            "text": "Unlike in standard training, where longer training results in near-zero training and test error, Rice et al. (2020) observed that the test error increases in adversarial training. This phenomenon is called robust overfitting. Schmidt et al. (2018) established that achieving an adversarially robust generalization is challenging and requires more training data. Zhang et al. (2019b) pointed out the limitations of adversarial training to blind spot attacks. Subsequently, several empirical approaches have been proposed to improve generalization, such as adversarial training with semi/unsupervised learning (Carmon et al., 2019; Zhai et al., 2019; Zhang & Xu, 2019), AVmixup (Lee et al., 2020), and robust local feature (Chen et al., 2021). However, we distance ourselves from these works, as these data interpolation methods rely heavily on the requirement of large datasets to mitigate overfitting.\nIn contrast, Rice et al. (2020) systematically investigated various techniques used in deep learning, including \u21131 and \u21132 regularization, cutout, mixup, and early stopping, and found that early stopping was the most effective approach to remedy robust overfitting. While early stopping may not be the optimal solution for robust overfitting, other approaches mitigate this by promoting model flatness (Wu et al., 2020; Chen et al., 2020). Chen et al. (2020) achieves model flatness by leveraging knowledge distillation to smooth the logits space while applying stochastic weight averaging (Izmailov et al., 2018) to smoothen the weights space. However, their method is computationally expensive as it necessitates pre-training additional models to mitigate overfitting. Dong et al. (2021) incorporates the temporal ensembling (TE) technique (Laine & Aila, 2016) into the AT frameworks to regularize the predictions of adversarial examples from becoming overconfident. On the other hand, Adversarial Weight Perturbation (AWP) (Wu et al., 2020) explicitly regularizes the flatness of the weight loss landscape by proposing a double-perturbation mechanism that adversarially perturbs both inputs and weights. However, AWP requires an additional maximization step to compute the adversarial noise to perturb the network weights during AT. IDBH (Li & Spratling, 2023) underscores the importance of diversity and hardness in data augmentation, showing that diversity improves accuracy and robustness, while hardness enhances robustness in adversarial training. Unlike the other approaches, we tackle the problem of robust overfitting in AT through the lens of active forgetting. Given the ability of DNNs to memorize noise in the training data (Dong et al., 2021), we hypothesize that actively forgetting and relearning during AT may help consolidate generalizable features and achieve robust generalization."
        },
        {
            "heading": "2.3 BIOLOGICAL UNDERPINNINGS FOR ROBUST GENERALIZATION",
            "text": "We begin by motivating our approach through an examination of learning dynamics within the human brain. Intelligent decision-making in noisy, dynamic environments emerges from the interplay between memory retention and forgetting mechanisms. As highlighted by Davis & Zhong (2017), the ability of humans to generalize from new experiences is, in part, attributable to the phenomenon of active forgetting. Active forgetting plays a pivotal role in the selective regulation and rebalancing of the learning-memory process, thereby guarding against overfitting to individual experiences (Gravitz, 2019). These insights from neuroscience provide substantial evidence for the existence of a symbiotic relationship between generalization and active forgetting in biological neural networks, a relationship that remains notably absent in DNNs.\nIn line with this, Richards & Frankland (2017) present compelling evidence supporting the essential role of forgetting in facilitating adaptive behavior within dynamic settings. Active forgetting within memory models offers functional advantages for robust generalization in such contexts, including (1) bolstering behavioral flexibility by diminishing the impact of obsolete information on memorydriven decision-making, and (2) averting overfitting to past events, thereby fostering generalization. Consequently, we underscore the importance of incorporating active forgetting mechanisms into computational models to effectively mitigate robust overfitting and promote robust generalization."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "We propose FOrget to Mitigate Overfitting (FOMO), a training paradigm to improve generalization in adversarial learning. FOMO interchanges between the consolidation, forgetting, and relearning phases. More details can be found in Figure 2 and in Algorithm 1 in Appendix"
        },
        {
            "heading": "3.1 FORGETTING",
            "text": "What is forgetting? We define the \"forgetting step\" as any process that results in a reduction in robust training accuracy. Specifically, let us denote the training dataset as D and a neural network parameterized by \u03b8 as f\u03b8. The robust training accuracy of the network f\u03b8 is expressed as AccR(f\u03b8) = 1 n \u2211n i=1 If\u03b8(xi + \u03b4) = yi. Now, consider the robust chance accuracy denoted as C for a randomly initialized neural network (f \u2032\u03b8) on the dataset D. We define a forgetting step as any function satisfying two conditions: (i) P (C < AccR(f \u2032\u03b8) < AccR(f\u03b8)) = 1, ensuring that the forgetting stage allows relearning due to a decrease in accuracy, and (ii) mutual information I(f\u03b8;D) > 0 for the network concerning the dataset remains positive after the forgetting step. This concept captures the notion that forgetting entails the partial removal of information from the network, not complete erasure. Put simply, after any forgetting operation, the training accuracy of the adversarially trained network should fall between the chance accuracy and the robust accuracy achieved before forgetting. Thus, the forgetting step aims to strike a balance between preserving essential information from the previous step and facilitating relearning.\nWhere to forget? Now that we have established the process of forgetting, the subsequent question arises: Where in the neural architecture should forgetting take place to achieve generalization? Although our goal is to mitigate overfitting in AT, our aim is to prioritize characteristics that are simple, general, and beneficial for generalization (Valle-Perez et al., 2018). Therefore, we limit the forgetting process to the later layers of the network. For example, in the case of PreAct-ResNet18 (He et al., 2016), the last two layers are considered as later layers. This is because these layers have more capacity to memorize and tend to overfit the training data, while the earlier layers in the network tend to learn more generalized representations (Dong et al., 2021; Neyshabur et al., 2018;\nArpit et al., 2017; Yosinski et al., 2014; Zhang et al., 2021; Geirhos et al., 2018). Furthermore, studies in AT such as (Bakiskan et al., 2022; Siddiqui & Breuel, 2021) have shown that the network depth has a significant impact on adversarial robustness, and early layers tend to contribute more to robustness than later layers. Therefore, by limiting the forgetting process to the last two layers, we aim to regularize the weights in the later layers while retaining more generalized features learned in the earlier layers to facilitate robust generalization.\nHow to forget? Having established the concept of forgetting and identified where in a neural network to apply forgetting, the next critical step is to determine how to effectively emulate the forgetting process in DNNs. To this end, we consider a deep network (f\u03b8) with l layers, each with parameters \u03b8l. Initially, the network undergoes adversarial training during a warm-up period, employing the adversarial example generation technique outlined in Section 2.1.\nOur forgetting approach starts by splitting the network f\u03b8 into two hypotheses during the AT as outlined by a binary mask M : (a) Retain hypothesis F\u25b3, and (b) Reset hypothesis F\u25bd. The network parameters \u03b8l belonging to the retain and reset hypotheses are selected randomly using a binary mask Ml such that sum(Ml) = s \u2217 |\u03b8l|, where s is the sparsity rate that determines the percentage of parameters belonging to the reset hypothesis F\u25bd. We prefer random selection as it is simple and efficient. Note that the forgetting step applies only to the parameters of the later layers defined by the layer threshold L. Therefore, by default, the parameters \u03b8l corresponding to the early layers of the network belong to the retain hypothesis F\u25b3 throughout the AT. Finally, these hypotheses are outlined by a binary mask; 1 for F\u25b3 and 0 for F\u25bd, that is, F\u25b3 = M \u2299 f\u03b8 and F\u25bd = (1\u2212M)\u2299 f\u03b8. After randomly selecting the subset of weights belonging to the retain and reset hypothesis, the parameters belonging to F\u25b3 from the previous learning step are retained, while F\u25bd is reinitialized or reset to a random value sampled from a uniform distribution. Formally, we reinitialize parameters in the layer, as follows \u03b8l = Ml \u2299 \u03b8l + (1 \u2212Ml) \u2299 \u03b8rl , where \u03b8rl is a randomly initialized tensor. Thus, we emulate the aspect of active forgetting through random reinitialization of the connections and regularize the parameters in the later layers throughout the AT to achieve robust generalization.\nFinally, the new network, after forgetting, contains an amalgamation of retained and reinitialized parameters that undergo relearning until the onset of the next forgetting step."
        },
        {
            "heading": "3.2 CONSOLIDATION",
            "text": "Since our FOMO method alternates between the forgetting and relearning phases during the AT, it is important to assimilate the generalized information that is relearned after each forgetting step. As the information is encoded in the parameters of the network (Krishnan et al., 2019), we intend to consolidate this information after each relearning step by employing another network f\u03d5 called stable model similar to f\u03b8. The knowledge learned by f\u03b8 is consolidated in the stable model after each learning session (before each forgetting step), thereby serving as a long-term memory. This newly learned information is consolidated in the stable model by taking an exponential moving average of the f\u03b8 weights with decay parameter \u03b1c: \u03d5 = \u03b1c\u03d5+(1\u2212\u03b1c)\u03b8, where \u03d5 and \u03b8 correspond to the weights of the stable and the current network, respectively. It should be noted that the stable model is not subjected to training, while the forgetting operation is exclusively applied to f\u03b8. Thus, this consolidation step can be considered as forming a self-ensemble of the intermediate model states obtained after multiple relearning steps that leads to generalized representations."
        },
        {
            "heading": "3.3 RELEARNING",
            "text": "Recent studies in cognitive neuroscience provide significant evidence of the existence of a symbiotic relationship between learning and forgetting (Gravitz, 2019; Richards & Frankland, 2017). The \"spacing effect\" is a well-known example of this relationship, which shows that long-term recall is improved when learning sessions are spaced out rather than massed together (Bjork & Bjork, 2019). Also, Bjork & Allen (1970) showed that the reduced accessibility of information between learning sessions is the key to regulating the important information in long-term memory. Therefore, we exploit this symbiotic relationship in AT by introducing an interleaved training session after each forgetting step. The network with new initialization undergoes a relearning phase wherein it is trained adversarially for er epochs, where er is less than the total number of AT epochs.\nRegularization. To expedite the relearning process, we propose a consistency loss that regularizes the output of the stable model. As shown in Equation 2, the function of this consistency regularization is to provide guidance to f\u03b8 after each forgetting step. \u03bb1 and \u03bb2 in the Equation 2 denotes loss balancing parameters. The network (f\u03b8) is updated so that it acquires new knowledge while aligning its decision boundary with the stable model that contains information consolidated across multiple relearning steps. This further prevents f\u03b8 from robust overfitting during the relearning phase. The overall loss LFOMO used to train the network during relearning phase is shown in Equation 3. We also detach the gradients from the stable model. Thus this regularization provides more detailed supervision from a stable model than LCE loss, which helps to avoid overfitting. Once the network completes the relearning phase, the newly acquired knowledge is integrated into the stable model, which then proceeds to the forgetting step.\nLCR = \u03bb1 \u00b7DKL(f\u03b8(x)||f\u03d5(x)) + \u03bb2 \u00b7DKL(f\u03b8(x+ \u03b4)||f\u03d5(x+ \u03b4)) (2)\nLFOMO(x, y; \u03b8) = Ladv(f\u03b8(x+ \u03b4), y) + LCR (3)\nThus, by cycling through forgetting, relearning, and consolidation, we introduce behavioral flexibility that helps in learning generalized information. Finally, for inference, we rely on the stable model, which serves as a long-term memory and holds the generalized knowledge that is consolidated after multiple relearning steps during the AT."
        },
        {
            "heading": "4 TACKLING ROBUST OVERFITTING",
            "text": "We compared our proposed method against previous AT methods on the CIFAR-10 dataset, utilizing two popular backbone networks, namely PreAct-ResNet18 (He et al., 2016) and WideResNet-3410 (Zagoruyko & Komodakis, 2016). The performance of each approach was evaluated on the test set against the PGD-20 attack. As presented in Table 1, our experimental results demonstrate that FOMO outperforms all the baseline approaches regarding the best and last robust test accuracy for all architectures. Specifically, our method achieves a significant improvement of 12.02% and 11.07% in last-epoch robust test accuracy over PGD-AT on PreAct-ResNet18 and WideResNet34-10, respectively. Notably, the last-epoch robustness of FOMO even consistently outperforms the best-epoch robustness of previous approaches that employ early stopping. Thus, early stopping may not be necessary, saving computation that goes into validating every epoch during AT. Moreover, FOMO achieves a better trade-off between standard and robust generalization than many AT methods. Lastly, FOMO mitigates robust overfitting by reducing the gap between the best and last\nrobust test accuracy. Therefore, iterating between forgetting, relearning, and consolidation learns the generalized features that facilitate robust generalization in AT.\nPerformance across different datasets. To further evaluate the scalability of our proposed method, we conducted experiments on two additional benchmark datasets, CIFAR-100 and SVHN, which are more complex than CIFAR-10. The results, presented in Table 2, indicate that our method achieves the highest level of robustness in both the best and the final epoch, demonstrating its ability to effectively scale to larger datasets compared to other baselines. Thus, selectively forgetting information periodically through weight reinitialization regulates the weights and brings discernable benefits to the model regarding robust generalization."
        },
        {
            "heading": "5 TRAINING ROBUST ACCURACY",
            "text": "Here, we study the impact of FOMO on robust training accuracy. The variation in robust accuracy on the training data during AT is shown in Figure 1. Our proposed method (FOMO) effectively suppresses the robust training accuracy from the level attained by PGD-AT (from 92% to 69%). This illustrates that randomly forgetting a percentage of parameters in the later layers during AT inhibits the model from overfitting to the training data, leading to a significant reduction in the robust generalization gap (from 47.56% to approximately 12.54%) and, thus, mitigating robust overfitting."
        },
        {
            "heading": "6 EVALUATION WITH AUTOATTACK",
            "text": "DNNs are deployed in real-world settings where they face more realistic and challenging scenarios, including sophisticated attacks that can target the model. It is, therefore, essential to evaluate proposed approaches against strong attacks that can effectively compromise the model\u2019s robustness. By doing so, we can ensure that the adversarial methods can effectively enhance the model\u2019s security and resilience in real-world settings. Recently, AutoAttack (Croce & Hein, 2020) has been more effective in uncovering vulnerabilities in DNNs, making it a popular choice for evaluating the robustness of models. It uses an ensemble of several state-of-the-art white-box and black-box attacks to generate adversarial examples that are more transferable and harder to defend against. As shown in Table 3, we evaluate our proposed method against the AutoAttack on CIFAR-10 and CIFAR-100 datasets with the PreActResNet-18 architecture. Compared to the standard AT (PGD-AT) and other robust generalization methods, our approach largely alleviates robust overfitting under Auto-Attack on both datasets. These results indicate that training with FOMO leads to robust, consistently generalizable features across challenging adversarial attacks."
        },
        {
            "heading": "7 ROBUSTNESS AGAINST COMMON CORRUPTIONS",
            "text": "DNNs are commonly deployed in real-world settings, where they are exposed to dynamic environments influenced by factors such as lighting and weather. As a result, it is crucial to assess the robustness of DNNs to handle data distributions susceptible to natural corruption in real-world settings. Here, we evaluated the efficacy of our method on the corrupted CIFAR-10 dataset (Hendrycks\n& Dietterich, 2019), which includes 19 types of corruptions. Models are trained on clean images and tested on CIFAR-10-C. The mean corruption accuracy (mCA) of each method is presented in Figure 3 (Right). The results reveal that the mCA consistently improves with FOMO compared to PGD-AT across corruptions. Periodically iterating between forgetting, relearning, and consolidation during AT brings discernible benefits regarding robustness to natural corruptions."
        },
        {
            "heading": "8 ROBUSTNESS TO INCREASE IN ADVERSARIAL ATTACK STRENGTH",
            "text": "To further assess the efficacy, we conducted experiments employing a PGD-20 attack with perturbation strengths incrementally spanning from an \u03f5 value of 0.25/255 to 8/255. The outcomes, depicted in Figure 3 (Left), reveal a pronounced decline in PGD-AT\u2019s robustness as perturbation strength increases, whereas FOMO consistently surpasses baseline adversarial techniques across all strength levels, showcasing its stable performance. These results suggest that the process of forgetting and relearning within the FOMO framework consolidates high-level abstractions capable of withstanding minor data perturbations, in contrast to standard AT (PGD-AT) and AWP methodologies."
        },
        {
            "heading": "9 CONVERGENCE TO FLATTER MINIMA",
            "text": "DNNs that converge to flatter minima in the loss landscape demonstrate superior generalization, according to Neyshabur et al. (2018). Additionally, models that reside in flatter minima are more resilient since slight perturbations do not significantly affect their predictions. To evaluate the robustness of our method, we incorporate independent Gaussian noise into all parameters of the trained CIFAR-10 model, as outlined in Alabdulmohsin et al. (2021).\nFigure 4 illustrates that the solution achieved by FOMO exhibits greater robustness to model perturbations compared to standard AT. Our method demonstrates significantly reduced sensitivity to perturbations. Specifically, for any level of noise introduced into the model parameters \u03b8, FOMO training accuracy exhibits smaller variations than standard training, suggesting that the FOMO solution may reside\nwithin flatter local minima. We posit that the training regimen involving alternating stages of for-\ngetting and relearning induces a broader valley, potentially elucidating our model\u2019s capacity to consolidate generalizable features."
        },
        {
            "heading": "10 ABLATION",
            "text": "We analyze the effect of forgetting, consolidation, and consistency regularization on the overall performance by incrementally adding each component to the baseline AT (PGD-AT). Table 4 reports the robust performance of the PreActResNet-18 network against PGD-20 and AA on the CIFAR-10 dataset.\nFirstly, we evaluate the effect of forgetting by comparing the performance of our method with and without forgetting (PGD-AT). We periodically forget 3.5% parameters corresponding to the parameters in the later layers ( 3 and 4 in PreAct-ResNet18) every five epochs. Our observations indicate a substantial reduction in the gap between the best and last robust test accuracy when the forgetting component is incorporated, compared to PGD-AT. Introducing the forgetting component aids in the regularization of the weights and facilitates the release of the network\u2019s capacity to learn generalized features during relearning.\nFurthermore, we extend our investigation by introducing a stable model for consolidating the features. Our findings indicate a notable improvement (approximately 3%) in the best test robust accuracy compared to PGD-AT. This improvement is attributed to the ability of the stable model to consolidate critical information learned during each relearning phase.\nLastly, we assess the role of the consistency regularization loss on the AT. Our observations reveal that the loss component is vital in mitigating robust overfitting by decreasing the gap between the best and last accuracy to -0.22. Thus, consistency regularization guides the network after each forgetting step by providing more precise supervision from a stable model, which helps to prevent overfitting to training labels. This further incentivizes the learning model to acquire generalized features consolidated in the stable model at the end of each relearning phase, resulting in an improved final performance. Our ablation study indicates that all individual components included in our proposed method are essential for robust generalization."
        },
        {
            "heading": "11 CONCLUSION",
            "text": "We introduce Forget to Mitigate Overfitting (FOMO), an adversarial training paradigm, to improve DNN performance and generalization through the lens of active forgetting. FOMO alternates between the forgetting phase, which periodically forgets undesirable information in the model through the reinitialization of weights, and the relearning phase, which emphasizes learning generalizable features. These features are later consolidated into a stable model after each relearning phase. Empirical results show that the proposed framework improves both standard and robust performance and generalization across a wide range of architectures, datasets, and perturbation types. Our framework is robust to auto attacks and increases generalization in many real-world scenarios. Overall, FOMO presents a promising solution for achieving better robust generalization in adversarial training. In our future work, we will aim to further develop a theoretical understanding of this issue and how it relates to the effectiveness of AT, as the underlying cause of this robust overfitting phenomenon is not yet fully understood."
        },
        {
            "heading": "A APPENDIX",
            "text": "Algorithm 1 Adversarial Training with FOMO Require: Training data D = (xi, yi)i = 1n, number of training epochs N , batch size B, base\nadversarial training algorithm A with \u03f5, perturbation norm and steps, network f\u03b8, stable model f\u03d5, warm-up epochs ewarm\u2212up, number of epochs the network undergoes relearning er, decay parameter \u03b1c, s sparsity rate that determines the percentage of parameters to be forgotten in the later layers defined by layer threshold L. 1: Initialize model parameters \u03b8 2: for epoch\u2190 1 to N do 3: if epoch > ewarm\u2212up and epoch%er == 0 then 4: Consolidate(f\u03b8, f\u03d5, \u03b1c) 5: Random_forgetting(f\u03b8, s) \u25b7 Randomly reinitialize the parameters in the later layers 6: Sample a mini-batch B = (xi, yi) from D 7: B\u0302 = A(f\u03b8, Ladv , \u03f5, steps, norm) \u25b7 adv samples 8: B\u0302\u2032 \u2190 f\u03b8(B\u0302) \u25b7 forward pass with adv samples 9: B\u2032 \u2190 f\u03b8(B) \u25b7 forward pass with std samples 10: if epoch > ewarm\u2212up then 11: B\u0302\u2032\u2032 \u2190 f\u03d5(B\u0302) \u25b7 forward pass using stable model 12: B\u2032\u2032 \u2190 f\u03d5(B) 13: LCR = \u03bb1 \u00b7DKL(B\u0302\u2032||B\u0302\u2032\u2032) + \u03bb2 \u00b7DKL(B\u2032||B\u2032\u2032) 14: LFOMO = Ladv + LCR \u25b7 refer Eq 2 & Eq 3 15: else 16: LFOMO = Ladv 17: Compute the gradients 18: Update the parameters f\u03b8\nA.1 IMPLEMENTATION DETAILS\nWe follow the standard adversarial training (AT) procedure used in previous research (Wu et al., 2020). The model was trained for a total of 200 epochs using the stochastic gradient descent (SGD) optimization algorithm with a momentum of 0.9, a weight decay of 5\u00d710\u22124, and an initial learning rate of 0.1. For standard AT, we reduced the learning rate by a factor of 10 at the 100th and 150th epochs, respectively. We applied standard data augmentation techniques, including random cropping with 4-pixel padding and random horizontal flipping, to the CIFAR-10 and CIFAR-100 datasets, while no data augmentation was applied to the SVHN dataset.\nFor training the other baseline methods (Wu et al., 2020; Chen et al., 2020; Dong et al., 2021), we used the exact same procedure and hyperparameters as specified in those methods. For the FOMO method proposed in Section 3, we began at epoch 105 (ewarm\u2212up), a little later than the first LR decay where robust overfitting often occurs. For PreActResNet-18, we forgot a fixed s = 3.5% of the parameters in the later layers (Block-3 and Block-4) of the architecture, while for widerResNet34-10, we forgot 5% as it has a larger capacity to memorize. Each forgetting step was followed by a relearning phase that lasted for er = 5 epochs. The relationship between s and er is studied in Section A.6. For the consolidation step, we chose a decay rate of the stable model of \u22a3c = 0.999. During the relearning phase, the stable model through the regularization loss (LCR), and we chose regularization strengths of \u03bb1 and \u03bb2 equal to 1. We ran the experiments for three seeds, and the average of the results is reported in the table.\nDatasets. For our experiments, we use three datasets: CIFAR-10 (Krizhevsky et al., 2009), CIFAR100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011). We randomly split the original training sets for these datasets into a training set and a validation set in a 9:1 ratio. Our ablation studies and visualizations are mainly based on the CIFAR-10 dataset.\nBaseline. We compare the results against various baseline methods such as vanilla PGD-AT (Madry et al., 2017), TRADES (Zhang et al., 2019a), KD+SWA (Chen et al., 2020), PGD-AT+TE (Dong et al., 2021), AWP Wu et al. (2020) that are proposed to mitigate robust overfitting. To assess the model\u2019s robust overfitting ability, we compare the robust test accuracy between the best-epoch and the last-epoch. The difference between the best and the final robust test accuracy is denoted as \u2206.\nThe results are averaged over three seeds. In addition to robust overfitting, achieving an optimal balance between natural and robust test accuracy is crucial for an effective AT method. However, there is currently no standardized method for measuring this trade-off in the adversarial trade-off literature. Therefore, we propose a trade-off measure that offers a formal approach to compare how well different methods maintain this balance. The Trade-off is measured as follows:\nTrade-off = 2\u00d7NAL \u00d7RAL NAL +RAL\n(4)\nwhere NAL and RAL stand for last-epoch natural test accuracy and robust test accuracy, respectively.\nA.2 COMPARISON WITH MLCAT\nYu et al. (2022) proposed a method called MLCAT to alleviate robust overfitting by analyzing the roles of easy and hard samples and regularizing samples with small loss values using non-robust features. In contrast, we approach this problem from the lens of active forgetting, which provides a different perspective for addressing robust overfitting.\nAccording to the results presented in Table 5, our FOMO approach outperforms MLCAT consistently in terms of robust improvement against AutoAttack across datasets. Furthermore, FOMO exhibits lower levels of robust overfitting compared to both variants of MLCAT. MLCATLS corresponds to loss scaling, and MLCATWP corresponds to weight perturbation. As a result, FOMO is not only more effective than MLCAT, but also shows a reduced tendency toward overfitting. Thus, iterating periodically between forgetting, relearning, and consolidation during AT brings discernible benefits with respect to the robustness to AT.\nA.3 EXTENDED COMPARISON WITH BASELINES ON LARGE DATASETS\nWe conducted extensive comparisons with KD-SWA, weight averaging (WA), and model ensembling approaches on CIFAR10/100, SVHN, and Tiny-ImageNet consisting of 64x64 images. FOMO algorithm outperforms these baselines, demonstrating superior robustness and im- proved performance as shown in Table 6. Furthermore, as shown in Table 7, FOMO outperforms subspace adversarial training (Sub-AT) (Li et al., 2022). This shows the effectiveness of FOMO even on larger datasets.\nA.4 EVALUATION ACROSS PERTURBATIONS\nWe assess the versatility of our proposed FOMO framework by evaluating across perturbation (\u21132 norm ) using PreActResNet-18 He et al. (2016) on CIFAR-10. Table 8 demonstrates the best and\nfinal performance of the FOMO network compared against vanilla adversarial training (PGD-AT) with \u21132 perturbation norm. We use \u03f5 of 128/255 and a step size of 15/255 for evaluation with \u21132 perturbation. The training/test attacks are PGD-10/PGD20, respectively. The results demonstrate that FOMO significantly outperforms vanilla adversarial training across perturbations. Therefore, forgetting and relearning are more effective in mitigating robust overfitting across perturbations.\nA.5 EFFECT OF FORGETTING IN DIFFERENT LAYERS ON ROBUST OVERFITTING\nTraining DNNs adversarially often results in a predominant phenomenon known as robust overfitting. Current learning techniques generally analyze learning behavior by treating the network as a whole unit, which disregards the ability of individual layers to learn adversarial data distributions. We suggest that different layers possess unique capacities to learn information, and it is crucial to comprehend these patterns to develop a training scheme that mitigates robust overfitting. Therefore, we investigate the layer-wise characteristics of a network by analyzing the effect of forgetting parameters at different layers on robust forgetting. For this analysis, we use the PreActResNet18 architecture, where each block (4 blocks) is considered a layer along with an additional linear classification layer. Figure 5 demonstrates the effect of forgetting in different layers on the robust train and test accuracy.\nOur analysis revealed that forgetting on early layers often results in decreased performance on robust test accuracy. Additionally, it fails to regularize the training accuracy, leading to a larger robust generalization gap. This is possibly due to the lower capacity of earlier layers to accommodate new information. Moreover, since earlier layers learn generalized features compared to later layers, forgetting them results in a loss of generalization. On the other hand, later layers have more capacity to memorize and tend to overfit the training data. Therefore, forgetting layers 3 and 4 leads to a reduced robust generalization gap, which mitigates robust overfitting. By limiting the forgetting process to the last two layers, we aim to regularize the weights in the later layers while retaining more generalized features learned in the earlier layers to facilitate robust generalization.\nA.6 STUDY THE SYMBIOTIC RELATIONSHIP BETWEEN FORGETTING AND RELEARNING\nRecent advances in cognitive neuroscience have shed light on the interdependent nature of learning and forgetting, with mounting evidence indicating a symbiotic relationship between the two phenomena Bjork & Allen (1970); Bjork & Bjork (2019). In the context of DNNs, the dynamics of forgetting and relearning are of particular interest, as they have been shown to play a critical role in mitigating the deleterious effects of overfitting. Specifically, the percentage of forgotten parameters and the duration of the relearning phase (er) are important factors to consider to achieve robust overfitting.\nTo investigate the impact of forgetting and relearning on overfitting, we varied the percentage of forgotten parameters (3%, 10%, 30%, and 50%) and adjusted the duration of the relearning phase over six different intervals (1, 5, 15, 20, and 25 epochs) within a total training duration of 200 epochs. Figure 6 presents the relationship between forgetting different parameters and the relearning phase during the training process.\nOur results suggest that forgetting only a small percentage of parameters can effectively mitigate overfitting when combined with a relatively short relearning phase. However, forgetting many parameters with short relearning phases can make training DNNs challenging. This implies that the percentage of parameters forgotten directly correlates with the duration of the relearning phase; the higher the percentage of parameters forgotten, the longer the relearning interval required to relearn the necessary information from the previous step. Therefore, it is essential to balance the amount\nof forgetting and the duration of the relearning phase to enable effective relearning and retain the critical information necessary for optimal performance.\nA.7 COMPUTATIONAL EFFICIENCY\nHere, we compare the computational efficiency of FOMO with the baseline methods. To ensure a fair comparison, all methods were integrated into a universal training framework, and each test was performed on a single NVIDIA GeForce 3090 GPU. Table 9 compares the computational time per epoch of FOMO with the considered baselines. Notably, FOMO and our baselines were trained for the same number of epochs (i.e., 200 epochs for CIFAR-10/100). From Table 9, it is evident that FOMO imposes almost no additional computational cost compared to vanilla PGD-AT, with specific values of 137.1s for FOMO and 132.6s for vanilla PGD-AT per epoch. This implies that FOMO is an efficient training method in practical terms. It is important to note that KD+SWA, a formidable method designed to counter robust overfitting, comes with an increased computational cost. This arises from its approach, which entails the pretraining of both a robust and a non-robust classifier, serving as the adversarial and standard teacher, respectively. In addition, the method incorporates the process of distilling the knowledge of these teachers. Moreover, KD+SWA employs stochastic weight averaging to smooth the weights of the model, further contributing to its computational demands. We believe that this addition improves practical insight into the efficiency of FOMO and its comparison with existing methods.\nA.8 PERFORMANCE UNDER CW ATTACK\nTable 10 presents the evaluation results under CW attack (Carlini & Wagner, 2017) on CIFAR10/100 using the PreActResNet-18 architecture. The robust accuracy is assessed under CW attacks, and checkpoints with the best robust accuracy under PGD-20 attacks on the validation set are selected for comparison. FOMO consistently outperforms both baselines across different datasets and attack scenarios, demonstrating its effectiveness in enhancing robust accuracy under CW attacks. The best and final robust accuracies for FOMO are generally close, indicating that FOMO maintains its performance during training and does not suffer from significant overfitting under these attacks. These results emphasize the promising performance of FOMO in mitigating adversarial attacks, particularly under CW attack scenarios."
        }
    ],
    "title": "THE EFFECTIVENESS OF RANDOM FORGETTING FOR ROBUST GENERALIZATION",
    "year": 2024
}