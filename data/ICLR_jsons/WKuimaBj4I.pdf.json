{
    "abstractText": "We study principal-agent problems in which a principal commits to an outcomedependent payment scheme\u2014called contract\u2014in order to induce an agent to take a costly, unobservable action leading to favorable outcomes. We consider a generalization of the classical (single-round) version of the problem in which the principal interacts with the agent by committing to contracts over multiple rounds. The principal has no information about the agent, and they have to learn an optimal contract by only observing the outcome realized at each round. We focus on settings in which the size of the agent\u2019s action space is small. We design an algorithm that learns an approximately-optimal contract with high probability in a number of rounds polynomial in the size of the outcome space, when the number of actions is constant. Our algorithm solves an open problem by Zhu et al. (2023). Moreover, it can also be employed to provide a \u00d5(T ) regret bound in the related online learning setting in which the principal aims at maximizing their cumulative utility over rounds, considerably improving previously-known regret bounds.",
    "authors": [
        {
            "affiliations": [],
            "name": "Francesco Bacchiocchi"
        },
        {
            "affiliations": [],
            "name": "Matteo Castiglioni"
        },
        {
            "affiliations": [],
            "name": "Alberto Marchesi"
        },
        {
            "affiliations": [],
            "name": "Nicola Gatti"
        }
    ],
    "id": "SP:882cae9abfe6fb15f9ce9d0916ca86c01fa0745a",
    "references": [
        {
            "authors": [
                "Tal Alon",
                "Paul D\u00fctting",
                "Inbal Talgam-Cohen"
            ],
            "title": "Contracts with private cost per unit-of-effort",
            "venue": "In Proceedings of the 22nd ACM Conference on Economics and Computation,",
            "year": 2021
        },
        {
            "authors": [
                "Tal Alon",
                "Paul Duetting",
                "Yingkai Li",
                "Inbal Talgam-Cohen"
            ],
            "title": "Bayesian analysis of linear contracts",
            "venue": "In Proceedings of the 24th ACM Conference on Economics and Computation, EC \u201923,",
            "year": 2023
        },
        {
            "authors": [
                "Yu Bai",
                "Chi Jin",
                "Huan Wang",
                "Caiming Xiong"
            ],
            "title": "Sample-efficient learning of stackelberg equilibria in general-sum games",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hamsa Bastani",
                "Mohsen Bayati",
                "Mark Braverman",
                "Ramki Gummadi",
                "Ramesh Johari"
            ],
            "title": "Analysis of medicare pay-for-performance contracts",
            "venue": "Available at SSRN",
            "year": 2016
        },
        {
            "authors": [
                "Gabriel Carroll"
            ],
            "title": "Robustness and linear contracts",
            "venue": "American Economic Review,",
            "year": 2015
        },
        {
            "authors": [
                "Matteo Castiglioni",
                "Alberto Marchesi",
                "Nicola Gatti"
            ],
            "title": "Bayesian agency: Linear versus tractable contracts",
            "venue": "Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Matteo Castiglioni",
                "Alberto Marchesi",
                "Nicola Gatti"
            ],
            "title": "Designing menus of contracts efficiently: The power of randomization",
            "venue": "Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Matteo Castiglioni",
                "Alberto Marchesi",
                "Nicola Gatti"
            ],
            "title": "Multi-agent contract design: How to commission multiple agents with individual outcomes",
            "venue": "In Proceedings of the 24th ACM Conference on Economics and Computation,",
            "year": 2023
        },
        {
            "authors": [
                "Alon Cohen",
                "Argyrios Deligkas",
                "Moran Koren"
            ],
            "title": "Learning approximately optimal contracts",
            "venue": "In Algorithmic Game Theory: 15th International Symposium, SAGT 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Lin William Cong",
                "Zhiguo He"
            ],
            "title": "Blockchain disruption and smart contracts",
            "venue": "The Review of Financial Studies,",
            "year": 2019
        },
        {
            "authors": [
                "Ramiro Deo-Campo Vuong",
                "Shaddin Dughmi",
                "Neel Patel",
                "Aditya Prasad"
            ],
            "title": "On supermodular contracts and dense subgraphs",
            "venue": "In Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),",
            "year": 2024
        },
        {
            "authors": [
                "Paul D\u00fctting",
                "Tim Roughgarden",
                "Inbal Talgam-Cohen"
            ],
            "title": "Simple versus optimal contracts",
            "venue": "In Proceedings of the 2019 ACM Conference on Economics and Computation,",
            "year": 2019
        },
        {
            "authors": [
                "Paul Dutting",
                "Tim Roughgarden",
                "Inbal Talgam-Cohen"
            ],
            "title": "The complexity of contracts",
            "venue": "SIAM Journal on Computing,",
            "year": 2021
        },
        {
            "authors": [
                "Paul D\u00fctting",
                "Tomer Ezra",
                "Michal Feldman",
                "Thomas Kesselheim"
            ],
            "title": "Combinatorial contracts",
            "venue": "IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS),",
            "year": 2021
        },
        {
            "authors": [
                "Paul D\u00fctting",
                "Tomer Ezra",
                "Michal Feldman",
                "Thomas Kesselheim"
            ],
            "title": "Multi-agent contracts",
            "venue": "In Proceedings of the 55th Annual ACM Symposium on Theory of Computing,",
            "year": 2023
        },
        {
            "authors": [
                "Paul Dutting",
                "Michal Feldman",
                "Yoav Gal Tzur"
            ],
            "title": "Combinatorial contracts beyond gross substitutes",
            "venue": "In Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),",
            "year": 2024
        },
        {
            "authors": [
                "Guru Guruganesh",
                "Jon Schneider",
                "Joshua R Wang"
            ],
            "title": "Contracts under moral hazard and adverse selection",
            "venue": "In Proceedings of the 22nd ACM Conference on Economics and Computation,",
            "year": 2021
        },
        {
            "authors": [
                "Guru Guruganesh",
                "Jon Schneider",
                "Joshua Wang",
                "Junyao Zhao"
            ],
            "title": "The power of menus in contract design",
            "venue": "In Proceedings of the 24th ACM Conference on Economics and Computation, EC \u201923,",
            "year": 2023
        },
        {
            "authors": [
                "Minbiao Han",
                "Michael Albert",
                "Haifeng Xu"
            ],
            "title": "Learning in online principle-agent interactions: The power of menus",
            "venue": "arXiv preprint arXiv:2312.09869,",
            "year": 2023
        },
        {
            "authors": [
                "Chien-Ju Ho",
                "Aleksandrs Slivkins",
                "Jennifer Wortman Vaughan"
            ],
            "title": "Adaptive contract design for crowdsourcing markets: Bandit algorithms for repeated principal-agent problems",
            "venue": "In Proceedings of the Fifteenth ACM Conference on Economics and Computation,",
            "year": 2014
        },
        {
            "authors": [
                "Niklas Lauffer",
                "Mahsa Ghasemi",
                "Abolfazl Hashemi",
                "Yagiz Savas",
                "Ufuk Topcu"
            ],
            "title": "No-regret learning in dynamic stackelberg games",
            "venue": "IEEE Transactions on Automatic Control,",
            "year": 2023
        },
        {
            "authors": [
                "Joshua Letchford",
                "Vincent Conitzer",
                "Kamesh Munagala"
            ],
            "title": "Learning and approximating the optimal strategy to commit to",
            "venue": "In Algorithmic Game Theory: Second International Symposium,",
            "year": 2009
        },
        {
            "authors": [
                "Binghui Peng",
                "Weiran Shen",
                "Pingzhong Tang",
                "Song Zuo"
            ],
            "title": "Learning optimal strategies to commit to",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Banghua Zhu",
                "Stephen Bates",
                "Zhuoran Yang",
                "Yixin Wang",
                "Jiantao Jiao",
                "Michael I. Jordan"
            ],
            "title": "The sample complexity of online contract design",
            "venue": "In Proceedings of the 24th ACM Conference on Economics and Computation,",
            "year": 2023
        },
        {
            "authors": [
                "e.g",
                "(D\u00fctting et al",
                "Alon"
            ],
            "title": "Learning in principal-agent problems The work that is most related to ours is (Zhu et al., 2023), which investigate a setting very similar to ours, though from an online learning perspective",
            "venue": "Guruganesh et al.,",
            "year": 2021
        },
        {
            "authors": [
                "Zhu"
            ],
            "title": "T 1\u22121/(2m+1)) when the principal is restricted to contracts in [0, 1], where m is the number of outcomes. They also show that their regret bound can be improved to \u00d5(T 1\u22121/(m+2)) by making additional structural assumptions on the problem instances, including the first-order stochastic dominance (FOSD) condition",
            "year": 2023
        },
        {
            "authors": [
                "Ho"
            ],
            "title": "on the number of outcomes m at the exponent of T is necessary in their setting. Notice that the regret bound by Zhu et al. (2023) is \u201cclose\u201d to a linear dependence on T , even when m is very small. In contrast, in Section 5 we show how our algorithm can be exploited to achieve a regret bound whose dependence on T is of the order of \u00d5(T ) (independent of m), when the number of agent\u2019s actions n is constant",
            "year": 2014
        },
        {
            "authors": [
                "Cohen"
            ],
            "title": "2022) study a repeated principal-agent interaction with a risk-averse agent, providing a no-regret algorithm that relies on the FOSD condition too. Learning in Stackelberg games The learning problem faced",
            "year": 2022
        },
        {
            "authors": [
                "Peng"
            ],
            "title": "by proposing an algorithm with more robust performances, being them independent of the volume of the smallest best-response region. The algorithm proposed in this paper borrows some ideas from that of Peng et al. (2019)",
            "year": 2009
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The computational aspects of principal-agent problems have recently received a growing attention in algorithmic game theory (Du\u0308tting et al., 2019; Alon et al., 2021; Guruganesh et al., 2021; Dutting et al., 2021; Du\u0308tting et al., 2022; Castiglioni et al., 2022; Alon et al., 2023; Castiglioni et al., 2023a;b; Du\u0308tting et al., 2023; Guruganesh et al., 2023; Deo-Campo Vuong et al., 2024; Dutting et al., 2024). Such problems model the interaction between a principal and an agent, where the latter takes a costly action inducing some externalities on the former. In hidden-action models, the principal cannot observe the agent\u2019s action, but only an outcome that is stochastically obtained as an effect of such an action and determines a reward for the principal. The goal of the principal is to incentivize the agent to take an action leading to favorable outcomes. This is accomplished by committing to a contract, which is a payment scheme defining a payment from the principal to the agent for every outcome.\nNowadays, thanks to the flourishing of digital economies, principal-agent problems find applications in a terrific number of real-world scenarios, such as, e.g., crowdsourcing platforms (Ho et al., 2014), blockchain-based smart contracts (Cong & He, 2019), and healthcare (Bastani et al., 2016). Most of the works on principal-agent problems focus on the classical model in which the principal knows everything about the agent, including costs and probability distributions over outcomes associated with agent\u2019s actions. Surprisingly, the study of settings in which the principal does not know all agent\u2019s features and has to learn them from experience has been neglected by almost all the previous works, with the exception of (Ho et al., 2014; Zhu et al., 2023; Cohen et al., 2022; Han et al., 2023).\nIn this paper, we consider a generalization of the classical hidden-action principal-agent problem in which the principal repeatedly interacts with the agent over multiple rounds. At each round, the principal commits to a contract, the agent plays an action, and this results in an outcome that is observed by the principal. The principal has no prior information about the agent\u2019s actions. Thus, they have to learn an optimal contract by only observing the outcome realized at each round. Our goal is to design algorithms which prescribe the principal a contract to commit to at each round, in order to learn an \u201capproximately-optimal\u201d contract with high probability by using the minimum possible number of rounds. This can be seen as the problem of establishing the sample complexity of learning optimal contracts in hidden-action principal-agent settings.\nHo et al. (2014) were the first to consider the problem of learning optimal contracts, though they focus on instances with a very specific structure, as the more recent work by Cohen et al. (2022). Recently, Zhu et al. (2023) addressed the problem of learning optimal contracts in general principalagent settings. They provide an algorithm whose cumulative regret with respect to playing an optimal contract is upper bounded by O\u0303( \u221a m \u00b7 T 1\u22121/(2m+1)), with an (almost-matching) lower bound of \u2126(T 1\u22121/(m+2)), where m is the number of outcomes and T the number of rounds. The result of Zhu et al. (2023) is very unpleasant when m is large, since in such a case the regret grows almost linearly in T . Moreover, in the instances used in their lower bound, the number of agent\u2019s actions is exponential in m, leaving as an open problem establishing whether better guarantees can be obtained when the action space is small (see also the discussion in (Zhu et al., 2023)).\nOriginal contributions We provide an algorithm that finds an \u201capproximately-optimal\u201d contract with high probability, requiring a number of rounds that grows polynomially in the size of the problem instance (including the number of outcomes m) when the number of agent\u2019s actions is a constant. The algorithm can be easily exploited to achieve cumulative regret (with respect to an optimal contract) upper bounded by O\u0303(mn \u00b7 T 4/5), which polynomially depends on the instance size when the number of agent\u2019s actions n is constant. This solves the open problem recently stated by Zhu et al. (2023). Our algorithm works by \u201capproximately identifying\u201d a covering of contracts into best-response regions, each one representing a set of contracts in which a given agent\u2019s action is a best response. The algorithm does so by progressively refining upper and lower bounds for such regions, until they coincide. One of the main challenges faced by our algorithm is that, after each round, the principal only observes an outcome stochastically determined by the agent\u2019s best response, rather than observing the action itself. This makes impossible identifying the action played by the agent. Our algorithm overcomes such an issue by working with meta-actions, which group together agent\u2019s actions associated with \u201csimilar\u201d distributions over outcomes. Such meta-actions need to be \u201cdiscovered\u201d online by the algorithm, which thus has to update the set of meta-actions on the basis of the observed feedbacks. As a result, our algorithm has to adopt a trial-and-error approach which re-starts the covering procedure each time a new agent\u2019s meta-action is discovered.\nRelation to repeated Stackelberg games Our work is related to the problem of learning an optimal commitment in repeated Stackelberg games (Letchford et al., 2009; Peng et al., 2019). However, differently from Stackelberg games, in principal-agent problems the principal cannot observe the action undertaken by the agent. This prevents us from relying on existing techniques (see, e.g., Peng et al. (2019)) which identify the hyperplanes defining the best-response regions of the follower. As a consequence, we have to work with meta-actions that only \u201capproximate\u201d the set of actions and we can only compute \u201capproximate\u201d separating hyperplanes. Such approximations are made effective by the particular structure of principal-agent problems, in which an approximately incentive compatible contract can be turned into an incentive compatible one by only suffering a small utility loss (see, e.g., (Dutting et al., 2021; Zhu et al., 2023)). This is not the case for Stackelberg games."
        },
        {
            "heading": "2 PRELIMINARIES ON HIDDEN-ACTION PRINCIPAL-AGENT PROBLEMS",
            "text": "An instance of the hidden-action principal-agent problem is characterized by a tuple (A,\u2126), where A is a finite set of n := |A| actions available to the agent, while \u2126 is a finite set of m := |\u2126| possible outcomes. Each agent\u2019s action a \u2208 A determines a probability distribution Fa \u2208 \u2206\u2126 over outcomes, and it results in a cost ca \u2208 [0, 1] for the agent.1 We denote by Fa,\u03c9 the probability with which action a results in outcome \u03c9 \u2208 \u2126, as prescribed by Fa. Thus, it must be the case that \u2211 \u03c9\u2208\u2126 Fa,\u03c9 = 1 for all a \u2208 A. Each outcome \u03c9 \u2208 \u2126 is characterized by a reward r\u03c9 \u2208 [0, 1] for the principal. Thus, when the agent selects action a \u2208 A, the principal\u2019s expected reward is \u2211 \u03c9\u2208\u2126 Fa,\u03c9r\u03c9 .\nThe principal commits to an outcome-dependent payment scheme with the goal of steering the agent towards desirable actions. Such a payment scheme is called contract and it is encoded by a vector p \u2208 Rm+ defining a payment p\u03c9 \u2265 0 from the principal to the agent for each outcome \u03c9 \u2208 \u2126.2 Given a contract p \u2208 Rm+ , the agent plays a best-response action that is: (i) incentive compatible (IC), i.e., it\n1Given a finite set X , we denote by \u2206X the set of all the probability distributions over the elements of X . 2As it is customary in contract theory (Carroll, 2015), in this work we assume that the agent has limited\nliability, meaning that the payments can only be from the principal to the agent, and not viceversa.\nmaximizes their expected utility; and (ii) individually rational (IR), i.e., it has non-negative expected utility. We assume w.l.o.g. that there always exists an action a \u2208 A with ca = 0. Such an assumption ensures that there is an action providing the agent with positive utility. This guarantees that any IC action is also IR and allows us to focus w.l.o.g. on IC only.\nWhenever the principal commits to p \u2208 Rm+ , the agent\u2019s expected utility by playing an action a \u2208 A is equal to \u2211 \u03c9\u2208\u2126 Fa,\u03c9p\u03c9 \u2212 ca, where the first term is the expected payment from the principal to the agent when selecting action a. Then, the set A(p) \u2286 A of agent\u2019s best-response actions in a given contract p \u2208 Rm+ is formally defined as follows: A(p) := argmaxa\u2208A \u2211 \u03c9\u2208\u2126 Fa,\u03c9p\u03c9 \u2212 ca. Given an action a \u2208 A, we denote with Pa \u2286 Rm+ the best-response set of action a, which is the set of all the contracts that induce action a as agent\u2019s best response. Formally, Pa := { p \u2208 Rm+ | a \u2208 A(p) } .\nAs it is customary in the literature (see, e.g., (Du\u0308tting et al., 2019)), we assume that the agent breaks ties in favor of the principal when having multiple best responses available. We let a\u22c6(p) \u2208 A(p) be the action played by the agent in a given p \u2208 Rm+ , which is an action a \u2208 A(p) maximizing the principal\u2019s expected utility \u2211 \u03c9\u2208\u2126 Fa,\u03c9 (r\u03c9 \u2212 p\u03c9). For ease of notation, we introduce the function u : Rm+ \u2192 R to encode the principal\u2019s expected utility under all the possible contracts; formally, the function u is defined so that u(p) = \u2211 \u03c9\u2208\u2126 Fa\u22c6(p),\u03c9 (r\u03c9 \u2212 p\u03c9) for every p \u2208 Rm+ .\nIn the classical (single-round) hidden-action principal-agent problem, the goal of the principal is find a contract p \u2208 Rm+ that maximizes the expected utility u(p). By letting OPT := maxp\u2208Rm+ u(p), we say that a contract p \u2208 Rm+ is optimal if u(p) = OPT. Moreover, given an additive approximation error \u03c1 > 0, we say that p is \u03c1-optimal whenever u(p) \u2265 OPT\u2212 \u03c1."
        },
        {
            "heading": "3 LEARNING OPTIMAL CONTRACTS",
            "text": "We study settings in which the principal and the agent interact over multiple rounds, with each round involving a repetition of the same instance of hidden-action principal-agent problem. The principal has no knowledge about the agent, and their goal is to learn an optimal contract. At each round, the principal-agent interaction goes as follows: (i) The principal commits to a contract p \u2208 Rm+ . (ii) The agent plays a\u22c6(p), which is not observed by the principal. (iii) The principal observes the outcome \u03c9 \u223c Fa\u22c6(p) realized by the agent\u2019s action. The principal only knows the set \u2126 of possible outcomes and their associated rewards r\u03c9, while they do not know anything about agent\u2019s actions A, their associated distributions Fa, and their costs ca.\nThe goal is to design algorithms for the principal that prescribe how to select a contract at each round in order to learn an optimal contract. Ideally, we would like an algorithm that, given an additive approximation error \u03c1 > 0 and a probability \u03b4 \u2208 (0, 1), is guaranteed to identify a \u03c1-optimal contract with probability at least 1 \u2212 \u03b4 by using the minimum possible number of rounds.3 The number of rounds required by such an algorithm can be seen as the sample complexity of learning optimal contracts in hidden-action principal-agent problems (see (Zhu et al., 2023)).\nThe following theorem shows that the goal defined above is too demanding.\nTheorem 1. For any number of rounds N \u2208 N, there is no algorithm that is guaranteed to find a \u03ba-optimal contract with probability greater than or equal to 1\u2212 \u03b4 by using less than N rounds, where \u03ba, \u03b4 > 0 are some suitable absolute constants.\nTheorem 1 follows by observing that there exist instances in which an approximately-optimal contract may prescribe a large payment on some outcome. Thus, for any algorithm and number of rounds N , the payments used up to round N may not be large enough to learn such an approximately-optimal contract. To circumvent Theorem 1, in the rest of the paper we focus on designing algorithms for the problem introduced in the following Definition 1. Such a problem relaxes the one introduced above by asking to find a contract whose seller\u2019s expected utility is \u201capproximately\u201d close to that of a utility-maximizing contract among those with bounded payments.\nDefinition 1 (Learning an optimal bounded contract). The problem of learning an optimal bounded contract reads as follows: Given a bound B \u2265 1 on payments, an additive approximation error \u03c1 > 0,\n3Such a learning goal can be seen as instantiating the PAC-learning framework into principal-agent settings.\nand a probability \u03b4 \u2208 (0, 1), find a contract p \u2208 [0, B]m such that the following holds: P { u(p) \u2265 max\np\u2032\u2208[0,B]m u(p\u2032)\u2212 \u03c1\n} \u2265 1\u2212 \u03b4.\nLet us remark that the problem introduced in Definition 1 does not substantially hinder the generality of the problem of learning an optimal contract. Indeed, since a contract achieving principal\u2019s expected utility OPT can be found by means of linear programming (Dutting et al., 2021), it is always the case that an optimal contract uses payments which can be bounded above in terms of the number of bits required to represent the probability distributions over outcomes. Moreover, all the previous works that study the sample complexity of learning optimal contracts focus on settings with bounded payments; see, e.g., (Zhu et al., 2023). The latter only considers contracts in [0, 1]m, while we address the more general case in which the payments are bounded above by a given B \u2265 1.\n4 THE DISCOVER-AND-COVER ALGORITHM\nIn this section, we provide an algorithm for the problem introduced in Definition 1, which we call Discover-and-Cover algorithm (Algorithm 1). Our main result (Theorem 2) is an upper bound on the number of rounds required by the algorithm, which we show to grow polynomially in the size of the problem instance when the number of agent\u2019s actions n is constant.\nThe core idea behind Algorithm 1 is to learn an optimal bounded contract p \u2208 [0, B]m by \u201capproximately\u201d identifying the best-response regions Pa. Notice that, since at the end of each round the principal only observes the outcome realized by the agent\u2019s action, rather than the action itself, identifying such best-response regions exactly is not possible. Moreover, the principal does not even know the set of actions available to the agent, which makes the task even more challenging.\nAlgorithm 1Discover-and-Cover Require: \u03c1 \u2208 (0, 1), \u03b4 \u2208 (0, 1), B \u2265 1 1: Set \u03f5, \u03b1, q, and \u03b7 as in Appendix C.1 2: P \u2190 {p \u2208 Rm+ | \u2225p\u22251 \u2264 mB} 3: D \u2190 \u2205, F \u2190 \u2205 4: do 5: {Ld}d\u2208D \u2190 Try-Cover() 6: while {Ld}d\u2208D = \u2205 7: return Find-Contract({Ld}d\u2208D) Algorithm 1 builds a set D of meta-actions, where each meta-action d \u2208 D identifies a set A(d) \u2286 A of one or more (unknown) agent\u2019s actions. A meta-action groups together \u201csimilar\u201d actions, in the sense that they induce similar distributions over outcomes. The algorithm incrementally discovers new elements ofD by trying to cover a suitably-defined set P of contracts (see Line 2) by means of approximate best-response regions Ld, one for each d \u2208 D. In particular, Ld is a suitably-defined polytope that \u201capproximately describes\u201d the union of all the bestresponse regions Pa of actions a \u2208 A(d) associated with d. Each time D is updated, the algorithm calls the Try-Cover procedure (Algorithm 3), whose aim is to cover P with approximate best-response regions. Try-Cover works by iteratively finding hyperplanes that separate such regions, by \u201ctesting\u201d suitable contracts through the Action-Oracle procedure (Algorithm 2). Given a contract p \u2208 P , Action-Oracle checks whether it is possible to safely map the agent\u2019s best response a\u22c6(p) to an already-discovered meta-action d \u2208 D or not. In the latter case, Action-Oracle refines the set D by either adding a new meta-action or merging a group of meta-actions if their associated actions can be considered as \u201csimilar\u201d. Whenever the set D changes, Try-Cover is terminated (returning \u2205). The algorithm continues trying to cover P until a satisfactory cover is found. Finally, such a cover is used to compute a contract p \u2208 [0, B]m to be returned, by means of the procedure Find-Contract (Algorithm 4).\nIn the rest of this section, we describe the details of all the procedures used by Algorithm 1. In particular, Section 4.1 is concerned with Action-Oracle, Section 4.2 with Try-Cover, and Section 4.3 with Find-Contract. Finally, Section 4.4 concludes the analysis of Algorithm 1 by putting everything together so as to prove the guarantees of the algorithm.\n4.1 ACTION-ORACLE\nGiven a contract p \u2208 P as input, the Action-Oracle procedure (Algorithm 2) determines whether it is possible to safely map the agent\u2019s best response a\u22c6(p) to some meta-action d \u2208 D. If this is the case, then the procedure returns such a meta-action. Otherwise, the procedure has to properly refine the set D of meta-actions, as we describe in the rest of this subsection.\nIn the first phase (Lines 1\u20136), Algorithm 2 prescribes the principal to commit to the contract p \u2208 P given as input for q \u2208 N consecutive rounds (see Appendix C.1 for the definition of q). This is done to build an empirical distribution F\u0303 \u2208 \u2206\u2126 that estimates the (true) distribution over outcomes Fa\u22c6(p) induced by the agent\u2019s best response a\u22c6(p). In the second phase (Lines 7\u201321), such an empirical distribution is compared with those previously computed, in order to decide whether a\u22c6(p) can be safely mapped to some d \u2208 D or not. To perform such a comparison, all the empirical distributions computed by the algorithm are stored in a dictionary F (initialized in Line 3 of Algorithm 1), where F [d] contains all the F\u0303 \u2208 \u2206\u2126 associated with d \u2208 D.\nThe first phase of Algorithm 2 ensures that \u2225F\u0303\u2212Fa\u22c6(p)\u2225\u221e \u2264 \u03f5 holds with sufficiently high probability. For ease of presentation, we introduce a clean event that allows us to factor out from our analysis the \u201cwith high probability\u201d statements. In Section 4.4, we will show that such an event holds with sufficiently high probability given how the value of q is chosen. Definition 2 (Clean event). Given any \u03f5 > 0, we let E\u03f5 be the event in which Algorithm 2 always computes some F\u0303 \u2208 \u2206\u2126 such that \u2225F\u0303 \u2212 Fa\u22c6(p)\u2225\u221e \u2264 \u03f5, where p \u2208 P is the input to the algorithm.\nAlgorithm 2 Action-Oracle Require: p \u2208 P 1: I\u03c9 \u2190 0 for all \u03c9 \u2208 \u2126 \u25b7 Phase 1: Estimate 2: for \u03c4 = 1, . . . , q do 3: Commit to p and observe \u03c9 \u2208 \u2126 4: I\u03c9 \u2190 I\u03c9 + 1 5: end for 6: Build F\u0303 \u2208 \u2206\u2126 : F\u0303\u03c9 = I\u03c9/q \u2200\u03c9 \u2208 \u2126 7: D\u22c4 \u2190 \u2205 \u25b7 Phase 2: Compare 8: for d \u2208 D do 9: if \u2203F \u2208 F [d] : \u2225F \u2212 F\u0303\u2225\u221e \u2264 2\u03f5 then 10: D\u22c4 \u2190 D\u22c4 \u222a {d} 11: end if 12: end for 13: if |D\u22c4| = 1 then 14: F [d]\u2190 F [d] \u222a {F\u0303}\u25b7 unique d \u2208 D\u22c4 15: return d 16: end if 17: D \u2190 (D \\ D\u22c4) \u222a {d\u22c4} \u25b7 d\u22c4 is new 18: F [d\u22c4]\u2190 \u22c3 d\u2208D\u22c4 F [d] \u222a {F\u0303} 19: Let F\u0303d\u22c4 be equal to F\u0303 20: Clear F [d] for all d \u2208 D\u22c4 21: return \u22a5 In the second phase, Algorithm 2 searches for all the d \u2208 D such that F [d] contains at least one empirical distribution which is \u201csufficiently close\u201d to the F\u0303 that has just been computed by the algorithm. Formally, the algorithm looks for all the meta-actions d \u2208 D such that \u2225F \u2212 F\u0303\u2225\u221e \u2264 2\u03f5 for at least one F \u2208 F [d]. Then, three cases are possible: (i) If the algorithm finds a unique d \u2208 D with such a property (case |D\u22c4| = 1), then d is returned since a\u22c6(p) can be safely mapped to d. (ii) If the algorithm does not find any d \u2208 D with such a property (case D\u22c4 = \u2205), then a new metaaction d\u22c4 is added to D. (iii) If the algorithm finds more than one d \u2208 D with such a property (|D\u22c4| > 1), then all the meta-actions in D\u22c4 are merged into a single (new) meta-action d\u22c4. In Algorithm 2, the last two cases above are jointly managed by Lines 17\u201321, and in both cases the algorithm returns the special value \u22a5. This alerts the calling procedure that the set D has changed, and, thus, the Try-Cover procedure needs to be re-started. Notice that Algorithm 2 also takes care of properly updating the dictionary F , which is done in Lines 14 and 20. Moreover, whenever Algorithm 2 adds a new meta-action d\u22c4 into D, this is also associated with a particular empirical distribution F\u0303d\u22c4 , which is set to be equal to F\u0303 (Line 19). We remark that the symbols F\u0303d for d \u2208 D have only been introduced for ease of exposition, and they do not reference actual variables declared in the algorithm. Operationally, one can replace each appearance of F\u0303d in the algorithms with any fixed empirical distribution contained in the dictionary entry F [d]. The first crucial property that Algorithm 2 guarantees is that only \u201csimilar\u201d agent\u2019s best-response actions are mapped to the same meta-action. In order to formally state such a property, we first need to introduce the definition of set of agent\u2019s actions associated with a meta-action in D. Definition 3 (Associated actions). Given a set D of meta-actions and a dictionary F of empirical distributions computed by means of Algorithm 2, we let A : D \u2192 2A be a function such that A(d) represents the set of actions associated with the meta-action d \u2208 D, defined as follows:\nA(d) = { a \u2208 A | \u2225F\u0303d \u2212 Fa\u2225\u221e\u2264 5\u03f5n \u2227 \u2203p \u2208 P : a = a\u22c6(p) } .\nThe set A(d) encompasses all the agent\u2019s actions a \u2208 A whose distributions over outcomes Fa are \u201csimilar\u201d to the empirical distribution F\u0303d associated with the meta-action d \u2208 D, where the entity of the similarity is defined in a suitable way depending on \u03f5 and the number of agent\u2019s actions n. Moreover, notice that the set A(d) only contains agent\u2019s actions that can be induced as best response for at least one contract p \u2208 P . This is needed in order to simplify the analysis of the algorithm. Let\nus also remark that an agent\u2019s action may be associated with more than one meta-action. Equipped with Definition 3, the property introduced above can be formally stated as follows: Lemma 1. Given a set D of meta-actions and a dictionary F of empirical distributions computed by means of Algorithm 2, under the event E\u03f5, if Algorithm 2 returns a meta-action d \u2208 D for a contract p \u2208 P given as input, then it holds that a\u22c6(p) \u2208 A(d).\nLemma 1 follows from the observation that, under the event E\u03f5, the empirical distributions computed by Algorithm 2 are \u201cclose\u201d to the true ones, and, thus, the distributions over outcomes of all the actions associated with d are sufficiently \u201cclose\u201d to each other. A non-trivial part of the proof of Lemma 1 is to show that Algorithm 2 does not put in the same entry F [d] empirical distributions that form a \u201cchain\u201d growing arbitrarily in terms of \u2225\u00b7\u2225\u221e norm, but instead the length of such \u201cchains\u201d is always bounded by 5\u03f5n. The second crucial property is made formal by the following lemma: Lemma 2. Under the event E\u03f5, Algorithm 2 returns \u22a5 at most 2n times.\nIntuitively, Lemma 2 follows from the fact that Algorithm 2 increases the cardinality of the set D by one only when \u2225F \u2212 F\u0303\u2225\u221e > 2\u03f5 for all F \u2208 F [d] and d \u2208 D. In such a case, under the event E\u03f5 the agent\u2019s best response is an action that has never been played before. Thus, the cardinality of D can be increased at most n times. Moreover, in the worst case the cardinality of D is reduced by one for n times, resulting in 2n being the maximum number of times Algorithm 2 returns \u22a5. In the following, we formally introduce the definition of cost of a meta-action. Definition 4 (Cost of a meta-action). Given a set D of meta-actions and a dictionary F of empirical distributions computed by means of Algorithm 2, we let c : D \u2192 [0, 1] be a function such that c(d) represents the cost of meta-action d \u2208 D, defined as c(d) = mina\u2208A(d) ca.\nThen, by Holder\u2019s inequality, we can prove that the two following lemmas hold: Lemma 3. Given a set D of meta-actions and a dictionary F of empirical distributions computed by means of Algorithm 2, under the event E\u03f5, for every meta-action d \u2208 D and associated action a \u2208 A(d) it holds that |c(d)\u2212 ca| \u2264 4B\u03f5mn.\nLemma 4. Given a set D of meta-actions and a dictionary F of empirical distributions computed by means of Algorithm 2, under the event E\u03f5, for every meta-action d \u2208 D, associated action a \u2208 A(d), and contract p \u2208 P it holds | \u2211 \u03c9\u2208\u2126 F\u0303d,\u03c9 p\u03c9 \u2212 c(d)\u2212 \u2211 \u03c9\u2208\u2126 Fa,\u03c9 p\u03c9 + ca| \u2264 9B\u03f5mn.\n4.2 TRY-COVER\nIn this section, we present key component of Algorithm 1, which is the Try-Cover procedure (Algorithm 3). It builds a cover {Ld}d\u2208D of P made of approximate best-response regions for the meta-actions in the current set D. The approximate best-response region Ld for a meta-action d \u2208 D must be such that: (i) all the best-response regions Pa of actions a \u2208 A(d) associated with d are contained in Ld; and (ii) for every contract in Ld there must be an action a \u2208 A(d) that is induced as an \u201capproximate best response\u201d by that contract. Notice that working with approximate best-response regions is unavoidable since the algorithm has only access to estimates of the (true) distributions over outcomes induced by agent\u2019s actions.\nThe core idea of the algorithm is to progressively build two polytopes Ud and Ld\u2014called, respectively, upper bound and lower bound\u2014for each meta-action d \u2208 D. During the execution, the upper bound Ud is continuously shrunk in such a way that a suitable approximate best-response region for d is guaranteed to be contained in Ud, while the lower bound Ld is progressively expanded so that it is contained in such a region. The algorithm may terminate in two different ways. The first one is when Ld = Ud for every d \u2208 D. In such a case, the lower bounds {Ld}d\u2208D constitutes a cover of P made of suitable approximate best-response regions for the meta-actions in D, and, thus, it is returned by the algorithm. The second way of terminating occurs any time a call to the Action-Oracle procedure is not able to safely map the agent\u2019s best response under the contract given as input to a meta-action (i.e., Action-Oracle returns \u22a5). In such a case, the algorithm gives back control to Algorithm 1 by returning \u2205, and the latter in turn re-starts the Try-Cover procedure from scratch since the last call to Action-Oracle has updated D. This happens in Lines 5, 14, and 22. The ultimate goal of Algorithm 3 is to reach termination with Ld = Ud for every d \u2208 D. Intuitively, the algorithm tries to \u201cclose the gap\u201d between the lower boundLd and the upper bound Ud for each d \u2208\nD by discovering suitable approximate halfspaces whose intersections define the desired approximate best-response regions. Such halfspaces are found in Line 21 by means of the Find-HS procedure (Algorithm 6), whose description and analysis is deferred to Appendix B. Intuitively, such a procedure searches for an hyperplane that defines a suitable approximate halfspace by performing a binary search (with parameter \u03b7) on the line connecting two given contracts, by calling Action-Oracle on the contract representing the middle point of the line at each iteration.\nAlgorithm 3 Try-Cover 1: Ld \u2190 \u2205, Ud \u2190 P for all d \u2208 D 2: Dd \u2190 {d} for all d \u2208 D 3: H\u0303ij \u2190 \u2205, \u2206c\u0303ij \u2190 0 for all di, dj \u2208 D 4: d\u2190 Action-Oracle(p) \u25b7 For any p 5: if d = \u22a5 then return \u2205 6: Ld \u2190 {p}, C \u2190 {d} 7: while C =\u0338 \u2205 do 8: Take any di \u2208 C 9: while Ldi \u0338= Udi do 10: Vdi \u2190 V (Udi) \u25b7 Vertexes of Udi 11: do 12: Take any p \u2208 Vdi 13: dj \u2190 Action-Oracle(p) 14: if dj = \u22a5 then return \u2205 15: if Ldj = \u2205 then 16: Ldj \u2190 {p}, C \u2190 C\u222a{dj} 17: end if 18: if dj \u2208 Ddi then 19: Ldi \u2190 co(Ldi , p) 20: else 21: (H\u0303, dk) Find-HS(di, p) 22: if dk = \u22a5 then return \u2205 23: H\u0303ik \u2190 H\u0303 24: Udi \u2190 Udi \u2229 H\u0303ik 25: Ddi \u2190 Ddi \u222a {dk} 26: end if 27: Vdi \u2190 Vdi \\ {p} 28: while Vdi \u0338= \u2205 \u2227 dj \u2208 Ddi 29: end while 30: C \u2190 C \\ {di} 31: end while 32: return {Ld}d\u2208D The halfspaces that define the approximate best-response regions computed by Algorithm 3 intuitively identify areas of P in which one meta-action is \u201csupposedly better\u201d than another one. In particular, Algorithm 3 uses the variable H\u0303ij to store the approximate halfspace in which di \u2208 D is \u201csupposedly better\u201d than dj \u2208 D, in the sense that, for every contract in such a halfspace, each action associated with di provides (approximately) higher utility to the agent than all the actions associated with dj . Then, the approximate best-response region for di \u2208 D is built by intersecting a suitably-defined group of H\u0303ij , for some dj \u2208 D with j \u0338= i. In order to ease the construction of the approximate halfspaces in the Find-HS procedure, Algorithm 3 also keeps some variables \u2206c\u0303ij , which represent estimates of the difference between the costs c(di) and c(dj) of di and dj , respectively. These are needed to easily compute the intercept values of the hyperplanes defining H\u0303ij .\nNext, we describe the procedure used by Algorithm 3 to reach the desired termination. In the following, for clarity of exposition, we omit all the cases in which the algorithm ends prematurely after a call to Action-Oracle. Algorithm 3 works by tracking all the d \u2208 D that still need to be processed, i.e., those such that Ld \u0338= Ud, into a set C. At the beginning of the algorithm (Lines 1\u20136), all the variables are properly initialized. In particular, all the upper bounds are initialized to the set P , while all the lower bounds are set to \u2205. The set C is initialized to contain a d \u2208 D obtained by calling Action-Oracle for any contract p \u2208 P , with the lower bound Ld being updated to the singleton {p}. Moreover, Algorithm 3 also maintains some subsets Dd \u2286 D of meta-actions, one for each d \u2208 D. For any di \u2208 D, the set Ddi is updated so as to contain all the dj \u2208 D such that the halfspace H\u0303ij has already been computed. Each set Dd is initialized to be equal to {d}, as this is useful to simplify the pseudocode of the algorithm. The main loop of the algorithm (Line 7) iterates over the meta-actions in C until such a set becomes empty. For every di \u2208 C, the algorithm tries to \u201cclose the gap\u201d between the lower bound Ldi and the upper bound Udi by using the loop in Line 9 In particular, the algorithm does so by iterating over all the vertices V (Udi) of the polytope defining the upper bound Udi (see Line 10). For every p \u2208 V (Udi), the algorithm first calls Action-Oracle with p as input. Then:\n\u2022 If the returned dj belongs to Ddi , then the algorithm takes the convex hull between the current lower bound Ldi and p as a new lower bound for di. This may happen when either dj = di (recall that di \u2208 Ddi by construction) or dj \u0338= di. Intuitively, in the former case the lower bound can be \u201csafely expanded\u201d to match the upper bound at the currently-considered vertex p, while in the latter case such \u201cmatching\u201d the upper bound may introduce additional errors in the definition of Ldi . Nevertheless, such an operation is done in both cases, since this not hinders the guarantees of the algorithm, as we formally show in Lemma 6. Notice that handling the case dj \u0338= di as in Algorithm 3 is crucial to avoid that multiple versions of the approximate halfspace H\u0303ij are created during the execution of the algorithm.\n\u2022 If the returned dj does not belong to Ddi , the algorithm calls the Find-HS procedure to find a new approximate halfspace. Whenever the procedure is successful, it returns an approximate\nhalfspace H\u0303 that identifies an area of P in which di is \u201csupposedly better\u201d than another meta-action dk \u2208 D \\ Ddi , which is returned by the Find-HS procedure as well. Then, the returned halfspace H\u0303 is copied into the variable H\u0303ik (Line 23), and the upper bound Udi is intersected with the latter (Line 24). Moreover, dk is added to Ddi (Line 25), in order to record that the halfspace H\u0303ik has been found. After that, the loop over vertexes is re-started, as the upper bound has been updated.\nWhenever the loop in Line 9 terminates, Ldi = Udi for the current meta-action di, and, thus, di is removed from C. Moreover, if a call to Action-Oracle returns a meta-action dj \u2208 D such that dj /\u2208 C and Ldj = \u2205, then dj is added to C and Ldj is set to {p}, where p \u2208 P is the contract given to Action-Oracle (see Lines 15\u201316). This ensures that all the meta-actions are eventually considered. Next, we prove two crucial properties which are satisfied by Algorithm 3 whenever it returns {Ld}d\u2208D. The first one is formally stated in the following lemma: Lemma 5. Under the event E\u03f5, when Algorithm 3 returns {Ld}d\u2208D, it holds that \u22c3 d\u2208D Ld = P .\nIntuitively, Lemma 5 states that Algorithm 3 terminates with a correct cover of P , and it follows from the fact that, at the end of the algorithm, \u22c3 d\u2208D Ud = P and Ld = Ud for every d \u2208 D. The second crucial lemma states the following: Lemma 6. Under the event E\u03f5, when Algorithm 3 returns {Ld}d\u2208D, for every meta-action d \u2208 D, contract p \u2208 Ld and action a\u2032 \u2208 A(d), there exists a \u03b3 that polynomially depends on m, n, \u03f5, and B such that: \u2211\n\u03c9\u2208\u2126 Fa\u2032,\u03c9 p\u03c9 \u2212 ca\u2032 \u2265 \u2211 \u03c9\u2208\u2126 Fa,\u03c9 p\u03c9 \u2212 ca \u2212 \u03b3 \u2200a \u2208 A.\nLemma 6 states that {Ld}d\u2208D defines a cover of P made of suitable approximate best-response regions for the meta-actions in D. Indeed, for every meta-action d \u2208 D and contract p \u2208 Ld, playing any a \u2208 A(d) associated with d is an \u201capproximate best response\u201d for the agent, in the sense that the agent\u2019s utility only decreases by a small amount with respect to playing the (true) best response a\u22c6(p). Finally, the following lemma bounds the number of rounds required by Algorithm 3.\nLemma 7. Under event E\u03f5, Algorithm 3 requires at most O ( n2q ( log (Bm/\u03b7) + ( m+n+1\nm\n))) rounds.\nLemma 7 follows from the observation that the main cost, in terms of number of rounds, incurred by Algorithm 3 is to check all the vertexes of the upper bounds Ud. The number of such vertexes can be bound by ( n+m+1\nm\n) , thanks to the fact that the set P being covered by Algorithm 3 has m+ 1\nvertexes. Notice that, using P instead of [0, B]m is necessary, since the latter has a number vertexes which is exponential in m. Nevertheless, Algorithm 1 returns a contract p \u2208 [0, B]m by means of the Find-Contract procedure, which we are going to describe in the following subsection.\n4.3 FIND-CONTRACT\nThe Find-Contract procedure (Algorithm 4) finds a contract p \u2208 [0, B]m that approximately maximizes the principal\u2019s expected utility over [0, B]m by using the cover {Ld}d\u2208D of P made by approximate best-response regions given as input (obtained by running Try-Cover).\nAlgorithm 4 Find-Contract Require: {Ld}d\u2208D 1: while d \u2208 D do 2: p\u22c6d argmax\np\u2208[0,B]m\u2229Ld\n\u2211 \u03c9\u2208\u2126 F\u0303d,\u03c9 (r\u03c9 \u2212 p\u03c9)\n3: end while 4: d\u22c6 \u2190 argmax\nd\u2208D\n\u2211 \u03c9\u2208\u2126 F\u0303d,\u03c9(r\u03c9 \u2212 p \u22c6 d,\u03c9)\n5: p\u22c6 \u2190 p\u22c6d\u22c6 6: p\u03c9 \u2190 (1\u2212 \u221a \u03f5)p\u22c6\u03c9 + \u221a \u03f5r\u03c9 for all \u03c9 \u2208 \u2126 7: return p\nFirst, for every d \u2208 D, Algorithm 4 computes p\u22c6d which maximizes an empirical estimate of the principal\u2019s expected utility over the polytope Ld \u2229 [0, B]m. This is done in Line 2 by solving a linear program with constraints defined by the hyperplanes identifying Ld\u2229 [0, B]m and objective function defined by the principal\u2019s expected utility when outcomes are generated by F\u0303d. Then, Algorithm 4 takes the best contract (according to the empirical distributions F\u0303d) among all the p\u22c6d, which is the contract p\n\u22c6 defined in Line 5, and it returns a suitable convex combination of such a contract and a vector whose components are defined by principal\u2019s rewards (see Line 6). The following lemma formally proves the guarantees in terms of principal\u2019s expected utility provided by Algorithm 4: Lemma 8. Under the event E\u03f5, if {Ld}d\u2208D is a cover of P computed by Try-Cover, Algorithm 4 returns a contract p \u2208 [0, B]m such that u(p) \u2265 maxp\u2032\u2208[0,B]m u(p\u2032)\u2212 \u03c1.\nThe main challenge in proving Lemma 8 is that, for the contract p\u22c6 computed in Line 5, the agent\u2019s best response may not be associated with any meta-action in D, namely a\u22c6(p\u22c6) \u0338\u2208 A(d) for every d \u2208 D. Nevertheless, by means of Lemma 6, we can show that a\u22c6(p\u22c6) is an approximate best response to p\u22c6. Moreover, the algorithm returns the contract defined in Line 6, i.e., a convex combination of p\u22c6 and the principal\u2019s reward vector. Intuitively, paying the agent based on the principal\u2019s rewards aligns the agent\u2019s interests with those of the principal. This converts the approximate incentive compatibility of a\u22c6(p\u22c6) for the contract p\u22c6 into a loss in terms of principal\u2019s expected utility."
        },
        {
            "heading": "4.4 PUTTING IT ALL TOGETHER",
            "text": "We conclude the section by putting all the results related to the procedures involved in Algorithm 1 together, in order to derive the guarantees of the algorithm.\nFirst, by Lemma 2 the number of calls to the Try-Cover procedure is at most 2n. Moreover, by Lemma 7 and by definition of q and \u03b7, the number of rounds required by each call to Try-Cover is at most O\u0303(mn \u00b7 I \u00b7 1/\u03c14 log(1/\u03b4)) under the event E\u03f5, where I is a term that depends polynomially in m, n, and B. Finally, by Lemma 8 the contract returned by Algorithm 1\u2014the result of a call to the Find-Contract procedure\u2014has expected utility at most \u03c1 less than the best contact in [0, B]m, while the probability of the clean event E\u03f5 can be bounded below by means of a concentration argument. All the observations above allow us to state the following main result. Theorem 2. Given \u03c1 \u2208 (0, 1), \u03b4 \u2208 (0, 1), and B \u2265 1 as inputs, with probability at least 1\u2212 \u03b4 the Discover-and-Cover algorithm (Algorithm 1) is guaranteed to return a contract p \u2208 [0, B]m such that u(p) \u2265 maxp\u2032\u2208[0,B]m u(p\u2032)\u2212 \u03c1 in at most O\u0303(mn \u00b7 I \u00b7 1/\u03c14 log(1/\u03b4)) rounds, where I is a term that depends polynomially in m, n, and B.\nNotice that the number of rounds required by Algorithm 1is polynomial in the instance size (including the number of outcomes m) when the number of agent\u2019s actions n is constant."
        },
        {
            "heading": "5 CONNECTION WITH ONLINE LEARNING IN PRINCIPAL-AGENT PROBLEMS",
            "text": "In this section, we show that our Discover-and-Cover algorithm can be exploited to derive a no-regret algorithm for the related online learning setting in which the principal aims at maximizing their cumulative utility. In such a setting, the principal and the agent interact repeatedly over a given number of rounds T , as described in Section 3.\nAlgorithm 5 No-regret algorithm Require: \u03b4 \u2208 (0, 1), B \u2265 1 1: Set \u03c1 as in proof of Theorem 3 2: for t = 1, . . . , T do 3: if Algorithm 1 not terminated yet then 4: pt \u2190 p \u2208 P prescribed by Alg. 1 5: else 6: pt \u2190 p \u2208 [0, B]m returned by Alg. 1 7: end if 8: end for At each t = 1, . . . , T , the principal commits to a contract pt \u2208 Rm+ , the agent plays a best response a\u22c6(pt), and the principal observes the realized outcome \u03c9t \u223c Fa\u22c6(pt) with reward r\u03c9t . Then, the performance in terms of cumulative expected utility by employing the contracts {pt}Tt=1 is measured by the cumulative (Stackelberg) regret RT := T \u00b7 maxp\u2208[0,B]m u(p) \u2212 E [\u2211T t=1 u(p t) ] ,\nwhere the expectation is over the randomness of the environment. As shown in Section 4, Algorithm 1 learns an approximately-optimal bounded contract with high probability by using the number of rounds prescribed by Theorem 2. Thus, by exploiting Algorithm 1, it is possible to design an explore-then-commit algorithm ensuring sublinear regret RT with high probability; see Algorithm 5.\nTheorem 3. Given \u03b1 \u2208 (0, 1), Algorithm 5 achieves RT \u2264 O\u0303 ( mn \u00b7 I \u00b7 log(1/\u03b4) \u00b7 T 4/5 ) with probability at least 1\u2212 \u03b4, where I is a term that depends polynomially on m, n, and B.\nNotice that, even for a small number of outcomes m (i.e., any m \u2265 3), our algorithm achieves better regret guarantees than those obtained by Zhu et al. (2023) in terms of the dependency on the number of rounds T . Specifically, Zhu et al. (2023) provide a O\u0303(T 1\u22121/(2m+1)) regret bound, which exhibits a very unpleasant dependency on the number of outcomes m at the exponent of T . Conversely, our algorithm always achieves a O\u0303(T 4/5) dependency on T , when the number of agent\u2019s actions n is small. This solves a problem left open in the very recent paper by Zhu et al. (2023)."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This paper is supported by the Italian MIUR PRIN 2022 Project \u201cTargeted Learning Dynamics: Computing Efficient and Fair Equilibria through No-Regret Algorithms\u201d, by the FAIR (Future Artificial Intelligence Research) project, funded by the NextGenerationEU program within the PNRRPE-AI scheme (M4C2, Investment 1.3, Line on Artificial Intelligence), and by the EU Horizon project ELIAS (European Lighthouse of AI for Sustainability, No. 101120237)."
        },
        {
            "heading": "A RELATED WORKS",
            "text": "In this section, we survey all the previous works that are most related to ours. Among the works addressing principal-agent problems, we only discuss those focusing on learning aspects. Notice that there are several works studying computational properties of principal-agent problems which are not concerned with learning aspects, such as, e.g., (Du\u0308tting et al., 2019; Alon et al., 2021; Guruganesh et al., 2021; Dutting et al., 2021; Du\u0308tting et al., 2022; Castiglioni et al., 2022; 2023b).\nLearning in principal-agent problems The work that is most related to ours is (Zhu et al., 2023), which investigate a setting very similar to ours, though from an online learning perspective (see also our Section 5). Zhu et al. (2023) study general hidden-action principal-agent problem instances in which the principal faces multiple agent\u2019s types. They provide a regret bound of the order of O\u0303( \u221a m \u00b7T 1\u22121/(2m+1)) when the principal is restricted to contracts in [0, 1]m, where m is the number of outcomes. They also show that their regret bound can be improved to O\u0303(T 1\u22121/(m+2)) by making additional structural assumptions on the problem instances, including the first-order stochastic dominance (FOSD) condition. Moreover, Zhu et al. (2023) provide an (almost-matching) lower bound of \u2126(T 1\u22121/(m+2)) that holds even with a single agent\u2019s type, thus showing that the dependence on the number of outcomes m at the exponent of T is necessary in their setting. Notice that the regret bound by Zhu et al. (2023) is \u201cclose\u201d to a linear dependence on T , even when m is very small. In contrast, in Section 5 we show how our algorithm can be exploited to achieve a regret bound whose dependence on T is of the order of O\u0303(T 4/5) (independent of m), when the number of agent\u2019s actions n is constant. Another work that is closely related to ours is the one by Ho et al. (2014), who focus on designing a no-regret algorithm for a particular repeated principal-agent problem. However, their approach relies heavily on stringent structural assumptions, such as the FOSD condition. Finally, Cohen et al. (2022) study a repeated principal-agent interaction with a risk-averse agent, providing a no-regret algorithm that relies on the FOSD condition too.\nLearning in Stackelberg games The learning problem faced in this paper is closely related to the problem of learning an optimal strategy to commit to in Stackelberg games, where the leader repeatedly interacts with a follower by observing the follower\u2019s best-response action at each iteration. Letchford et al. (2009) propose an algorithm for the problem requiring the leader to randomly sample a set of available strategies in order to determine agent\u2019s best-response regions. The performances of such an algorithm depend on the volume of the smallest best-response region, and this considerably limits its generality. Peng et al. (2019) build upon the work by Letchford et al. (2009) by proposing an algorithm with more robust performances, being them independent of the volume of the smallest best-response region. The algorithm proposed in this paper borrows some ideas from that of Peng et al. (2019). However, it requires considerable additional machinery to circumvent the challenge posed by the fact that the principal does not observe agent\u2019s best-response actions, but only outcomes randomly sampled according to them. Other related works in the Stackelberg setting are (Bai et al., 2021), which proposes a model where both the leader and the follower learn through repeated interaction, and (Lauffer et al., 2023), which considers a scenario where the follower\u2019s utility is unknown to the leader, but it can be linearly parametrized.\nAssumptions relaxed compared to Stackelberg games In our work, we relax several limiting assumptions made in repeated Stackelber games (see, e.g., (Letchford et al., 2009; Peng et al., 2019)) to learn an optimal commitment. Specifically, in repeated Stackelberg games either the best response regions have at least a constant volume or they are empty. Thanks to Lemma 8, we effectively address this limitation, showing that even when an optimal contract belongs to a zero-measured best-response region, we can still compute an approximately optimal solution. Furthermore, in Stackelberg games it is assumed that in cases where there are multiple best responses for the follower, any of them can be arbitrarily chosen. In contrast, we assume that the agent always breaks ties in favor of the leader as it is customary in the Stackelberg literature. Finally, our approach does not require the knowledge of the number of actions of the agent, differently from previously proposed algorithms.\nB DETAILS ABOUT THE FIND-HS PROCEDURE\nIn this section, we describe in details the Find-HS procedure (Algorithm 6). The procedure takes as inputs a meta-action di \u2208 D and a contract p \u2208 P , and it tries to find one of the approximate halfspaces defining a suitable approximate best-response region for the meta-action di. It may terminate either with a pair (H\u0303, dk) such that H\u0303 is an approximate halfspace in which di is \u201csupposedly better\u201d than the meta-action dk \u2208 D or with a pair (\u2205,\u22a5), whenever a call to Action-Oracle returned\u22a5. Let us recall that, for ease of presentation, we assume that Algorithm 6 has access to all the variables declared in the Try-Cover procedure (Algorithm 3), namely Ld, Ud, Dd, H\u0303ij , and \u2206c\u0303ij .\nAlgorithm 6 Find-HS Require: di, p\n1: p1 \u2190 any contract in Ldi 2: p2 \u2190 p 3: dj \u2190 Action-Oracle(p1) 4: dk \u2190 Action-Oracle(p2) 5: y \u2190 18B\u03f5mn2 + 2n\u03b7 \u221a m 6: while \u2225p1 \u2212 p2\u22252 > \u03b7 do 7: p\u2032\u03c9 \u2190 (p1\u03c9 + p2\u03c9)/2 for all \u03c9 \u2208 \u2126 \u25b7 Middle point of the current line segment 8: d\u2190 Action-Oracle(p\u2032) 9: if d = \u22a5 then return (\u2205,\u22a5) \u25b7 Force termination of Try-Cover 10: if Ld = \u2205 then \u25b7 Add d to to-be-processed meta-actions in Try-Cover 11: Ld \u2190 {p\u2032}, C \u2190 C \u222a {d} 12: end if 13: if d \u2208 Ddi then 14: p1\u03c9 \u2190 p\u2032\u03c9 for all \u03c9 \u2208 \u2126 15: dj \u2190 d 16: else 17: p2\u03c9 \u2190 p\u2032\u03c9 for all \u03c9 \u2208 \u2126 18: dk \u2190 d 19: end if 20: end while 21: p\u2032\u03c9 \u2190 (p1\u03c9 + p2\u03c9)/2 for all \u03c9 \u2208 \u2126 22: if dj = di then \u25b7 The approximate halfspace H\u0303ik is the first one for Ldi 23: \u2206c\u0303ik \u2190\n\u2211 \u03c9\u2208\u2126 ( F\u0303dj ,\u03c9 \u2212 F\u0303dk,\u03c9 ) p\u2032\u03c9\n24: else \u25b7 The approximate halfspace H\u0303ij has already been computed 25: \u2206c\u0303ik \u2190 \u2206c\u0303ij + \u2211 \u03c9\u2208\u2126 ( F\u0303dj ,\u03c9 \u2212 F\u0303dk,\u03c9 ) p\u2032\u03c9 26: end if 27: H\u0303 := { p \u2208 Rm+ | \u2211 \u03c9\u2208\u2126 ( F\u0303di,\u03c9 \u2212 F\u0303dk,\u03c9 ) p\u03c9 \u2265 \u2206c\u0303ik \u2212 y\n} 28: return (H\u0303, dk)\nAlgorithm 6 works by performing a binary search on the line segment connecting p with any contract in the (current) lower bound Ldi of di (computed by the Try-Cover procedure). At each iteration of the search, by letting p1, p2 \u2208 P be the two extremes of the (current) line segment, the algorithm calls the Action-Oracle procedure on the contract p\u2032 \u2208 P defined as the middle point of the segment. Then, if the procedure returns a meta-action d \u2208 D that belongs to Ddi (meaning that the approximate halfspace H\u0303ij has already been computed), the algorithm sets p1 to p\u2032, while it sets p2 to p\u2032 otherwise. The binary search terminates when the length of the line segment is below a suitable threshold \u03b7 \u2265 0. After the search has terminated, the algorithm computes an approximate halfspace. First, the algorithm computes the estimate \u2206c\u0303ik of the cost difference between the meta-actions di and dk, where the latter is the last meta-action returned by Action-Oracle that does not belong to the set Ddi . Such an estimate is computed by using the two empirical distributions F\u0303dj and F\u0303dk that the Action-Oracle procedure associated with the meta-actions dj \u2208 Ddi and dk, respectively. In particular, \u2206c\u0303ik is computed as the sum of \u2206c\u0303ij\u2014the estimate of the cost difference between meta-actions di and dj that has been computed during a previous call to Algorithm 6\u2014and an estimate of the cost difference between the meta-actions dj and dk, which can be computed by using the middle point of the line segment found by the binary search procedure (see Lines 23 and 25). Then, the desired approximate halfspace is the one defined by the hyperplane passing through the middle point p\u2032 \u2208 P of the line segment computed by the binary search with coefficients given by the (m+ 1)-dimensional vector [F\u0303\u22a4di \u2212 F\u0303 \u22a4 dk ,\u2206c\u0303ik \u2212 y ] (see Line 27), where y \u2265 0 is a suitably-defined value chosen so as to ensure that the approximate best-response regions satisfy the desired properties (see Lemma 10).\nNotice that Algorithm6 also needs some additional elements in order to ensure that the (calling) Try-Cover procedure is properly working. In particular, any time the Action-Oracle procedure returns \u22a5, Algorithm 6 terminates with (\u2205,\u22a5) as a return value (Line 9). This is needed to force the termination of the Try-Cover procedure (see Line 22 in Algorithm 3). Moreover, any time the Action-Oracle procedure returns a meta-action d /\u2208 C such that Ld = \u2205, Algorithm 6 adds such a meta-action to C and it initializes its lower bound to the singleton {p\u2032}, where p\u2032 is the contract given as input to Action-Oracle (see Lines 10\u201311). This is needed to ensure that all the meta-actions in D are eventually considered by the (calling) Try-Cover procedure. Before proving the main properties of Algorithm6, we introduce the following useful definition: Definition 5. Given y \u2265 0, a set of meta-actions D and a dictionary F of empirical distributions computed by means of Algorithm 2, for every pair of meta-actions di, dj \u2208 D, we let Hyij \u2286 P be the set of contracts defined as follows:\nHyij :=\n{ p \u2208 P |\n\u2211 \u03c9\u2208\u2126 F\u0303di,\u03c9p\u03c9 \u2212 c(di) \u2265 \u2211 \u03c9\u2208\u2126 F\u0303dj ,\u03c9p\u03c9 \u2212 c(dj)\u2212 y\n} .\nFurthermore, we let Hij := H0ij .\nNext, we prove some technical lemmas related to Algorithm 6. Lemma 9 provides a bound on the gap between the cost difference \u2206c\u0303ik estimated by Algorithm 6 and the \u201ctrue\u201d cost difference between the meta-actions di and dk (see Defintion 4). Lemma 10 shows that the approximate halfspace computed by the algorithm is always included in the halfspace introduced in Definition 5 for a suitably-defined value of the parameter y \u2265 0. Finally, Lemma 11 provides a bound on the number of rounds required by the algorithm in order to terminate. Lemma 9. Under the event E\u03f5, Algorithm 6 satisfies |\u2206c\u0303ik\u2212 c(di)+ c(dk)| \u2264 18B\u03f5mn2+2n\u03b7 \u221a m.\nProof. We start by proving that, during any execution of Algorithm 6, the following holds:\u2223\u2223\u2223\u2223\u2223\u2211 \u03c9\u2208\u2126 ( F\u0303dj ,\u03c9 \u2212 F\u0303dk,\u03c9 ) p\u2032\u03c9 \u2212 c(dj) + c(dk) \u2223\u2223\u2223\u2223\u2223 \u2264 18B\u03f5mn+ 2\u03b7\u221am, (1) where dj , dk \u2208 D are the meta-actions resulting from the binary search procedure and p\u2032 \u2208 P is the middle point point of the segment after the binary search stopped. In order to prove Equation 1, we first let a1 := a\u22c6(p1) and a2 := a\u22c6(p2), where p1, p2 \u2208 P are the two extremes of the line segment\nresulting from the binary search procedure. By Lemma 1, under the event E\u03f5, it holds that a1 \u2208 A(dj) and a2 \u2208 A(dk) by definition. Moreover, we let p\u2217 \u2208 P be the contract that belongs to the line segment connecting p1 to p2 and such that the agent is indifferent between actions a1 and a2, i,e.,\u2211\n\u03c9\u2208\u2126\n( Fa1,\u03c9 \u2212 Fa2,\u03c9 ) p\u22c6\u03c9 = ca1 \u2212 ca2 .\nThen, we can prove the following:\u2223\u2223\u2223\u2223\u2223\u2211 \u03c9\u2208\u2126 ( F\u0303dj ,\u03c9 \u2212 F\u0303dk,\u03c9 ) p\u2032\u03c9 \u2212 c(dj) + c(dk) \u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223\u2211 \u03c9\u2208\u2126 ( F\u0303dj ,\u03c9 \u2212 F\u0303dk,\u03c9 ) p\u2032\u03c9 \u2212 c(dj) + ca1 \u2212 ca1 + ca2 \u2212 ca2 + c(dk) \u2223\u2223\u2223\u2223\u2223 \u2264\n\u2223\u2223\u2223\u2223\u2223\u2211 \u03c9\u2208\u2126 ( F\u0303dj ,\u03c9 \u2212 F\u0303dk,\u03c9 ) p\u2032\u03c9 \u2212 ca1 + ca2 \u2223\u2223\u2223\u2223\u2223+ 8B\u03f5mn =\n\u2223\u2223\u2223\u2223\u2223\u2211 \u03c9\u2208\u2126 ( F\u0303dj ,\u03c9 \u2212 F\u0303dk,\u03c9 ) p\u2032\u03c9 \u2212 \u2211 \u03c9\u2208\u2126 ( Fa1,\u03c9 \u2212 Fa2,\u03c9 ) p\u2217\u03c9 \u2223\u2223\u2223\u2223\u2223+ 8B\u03f5mn =\n\u2223\u2223\u2223\u2223\u2223\u2211 \u03c9\u2208\u2126 ( F\u0303dj ,\u03c9 p \u2032 \u03c9 + Fa1,\u03c9 p \u2032 \u03c9 \u2212 Fa1,\u03c9 p\u2032\u03c9 \u2212 Fa1,\u03c9 p\u2217\u03c9 ) + \u2211 \u03c9\u2208\u2126 ( F\u0303dk,\u03c9 p \u2032 \u03c9 + Fa2,\u03c9 p \u2032 \u03c9 \u2212 Fa2,\u03c9 p\u2032\u03c9 \u2212 Fa2,\u03c9 p\u2217\u03c9\n)\u2223\u2223\u2223\u2223\u2223+ 8B\u03f5mn \u2264 \u2225F\u0303dj \u2212 Fa1\u2225\u221e\u2225p\u2032\u22251 + \u2225F\u0303dk \u2212 Fa2\u2225\u221e\u2225p\u2032\u22251 + (\u2225Fa1\u22252 + \u2225Fa2\u22252) \u2225p\u2032 \u2212 p\u2217\u22252 + 8B\u03f5mn \u2264 18B\u03f5mn+ 2\u03b7 \u221a m.\nThe first inequality above is a direct consequence of Lemma 3 and an application of the triangular inequality, since a1 \u2208 A(dj) and a2 \u2208 A(dk) under the event E\u03f5. The second inequality follows by employing Holder\u2019s inequality. The third inequality holds by employing Lemma 1 and Definition 3, by observing that \u2225p\u22251 \u2264 Bm and \u2225Fa\u2225 \u2264 \u221a m for all a \u2208 A. Moreover, due to the definitions of p\u2032 and p\u2217, and given how the binary search performed by Algorithm 6 works, it is guaranteed that \u2225p\u2032 \u2212 p\u2217\u22252 \u2264 \u03b7. Next, we prove that, after any call to Algorithm 6, the following holds:\n|\u2206c\u0303ik \u2212 c(ci) + c(dk)| \u2264 |Ddi | ( 18B\u03f5mn+ 2\u03b7 \u221a m ) ,\nwhere di, dk \u2208 D are the meta-actions defined by the binary search procedure in Algorithm 6. We prove the statement by induction on the calls to Algorithm 6 with the meta-action di as input. The base case is the first time Algorithm 6 is called with di as input. In that case, the statement is trivially satisfied by Equation 1 and the fact that Ddi = {di}. Let us now consider a generic call to Algorithm 6 with the meta-action di as input. Then, we can write the following:\n|\u2206c\u0303ik \u2212 c(ci) + c(dk)| = \u2223\u2223\u2223\u2223\u2223\u2206c\u0303ij +\u2211 \u03c9\u2208\u2126 ( F\u0303dj ,\u03c9 \u2212 F\u0303dk,\u03c9 ) p\u2032\u03c9 \u2212 c(di) + c(dk) \u2223\u2223\u2223\u2223\u2223 \u2264 |\u2206c\u0303ij \u2212 c(di) + c(dj)|+\n\u2223\u2223\u2223\u2223\u2223\u2211 \u03c9\u2208\u2126 ( F\u0303dj ,\u03c9 \u2212 F\u0303dk,\u03c9 ) p\u2032\u03c9 \u2212 c(dj) + c(dk) \u2223\u2223\u2223\u2223\u2223 \u2264 (|Ddi | \u2212 1) ( 18B\u03f5mn+ 2\u03b7 \u221a m ) + 18B\u03f5mn+ 2\u03b7 \u221a m\n= |Ddi | ( 18B\u03f5mn+ 2\u03b7 \u221a m ) ,\nwhere the first inequality holds by applying the triangular inequality and the second one by using the inductive hypothesis.\nLemma 10. Let (H\u0303, dk) be the pair returned by Algorithm 6 when given as inputs Ldi and p \u2208 P for some meta-action di \u2208 D. Then, under the event E\u03f5, it holds:\nHik \u2286 H\u0303 \u2286 Hyik, with y = 18B\u03f5mn2 + 2n\u03b7 \u221a m.\nProof. We start by showing that Hik is a subset of H\u0303ik. Let p be a contract belonging to Hik, then according to Definition 5 the following inequality holds:\u2211\n\u03c9\u2208\u2126 F\u0303di,\u03c9p\u03c9 \u2212 \u2211 \u03c9\u2208\u2126 F\u0303dk,\u03c9p\u03c9 \u2265 \u2206c\u0303ik \u2212 y,\nwith y = 18B\u03f5mn2 + 2n\u03b7 \u221a m as prescribed by Lemma 9. This shows that p \u2208 H\u0303ik, according to Definition 5. We now consider the case in which p \u2208 H\u0303ik. Using the definition of H\u0303ik and Lemma 9, we have: \u2211\n\u03c9\u2208\u2126 F\u0303di,\u03c9p\u03c9 \u2212 \u2211 \u03c9\u2208\u2126 F\u0303dk,\u03c9p\u03c9 = \u2206c\u0303ik \u2265 c(di)\u2212 c(dk)\u2212 y,\nshowing that p \u2208 Hyik with y = 18B\u03f5mn2 + 2n\u03b7 \u221a m.\nLemma 11. Under the event E\u03f5, the number of rounds required by Algorithm 6 to terminate with an approximate separating hyperplane is at most O (q log (Bm/\u03b7)).\nProof. The lemma can be proven by observing that the number of rounds required by the binary search performed in Line 6 of Algorithm 6 is at most O(log(D/\u03b7)), where D represents the distance over which the binary search is performed. In our case, we have D \u2264 \u221a 2Bm, which represents the maximum distance between two contracts in P . Additionally, noticing that we invoke the Action-Oracle algorithm at each iteration, the total number of rounds required by Find-HS is O(q log( \u221a 2Bm \u03b7 )), as the number of rounds required by Action-Oracle is bounded by q.\nC ADDITIONAL DETAILS ABOUT THE DISCOVER-AND-COVER ALGORITHM\nIn this section, we provide all the details about the Discover-and-Cover (Algorithm 1) algorithm that are omitted from the main body of the paper. In particular:\n\u2022 Appendix C.1 gives a summary of the definitions of the parameters required by Algorithm 1, which are set as needed in the proofs provided in the rest of this section.\n\u2022 Appendix C.2 provides the proofs of the lemmas related to the Action-Oracle procedure.\n\u2022 Appendix C.3 provides the proofs of the lemmas related to the Try-Cover procedure. \u2022 Appendix C.4 provides the proofs of the lemmas related to the Find-Contract proce-\ndure. \u2022 Appendix C.5 provides the proof of the final result related to Algorithm 1 (Theorem 2).\nC.1 DEFINITIONS OF THE PARAMETERS IN ALGORITHM 1\nThe parameters required by Algorithm 1 are defined as follows:\n\u2022 \u03f5 := \u03c12\n322Bm2n2\n\u2022 \u03b7 := \u03f5 \u221a mn\n2\n\u2022 \u03b1 := \u03b4 2n3 [ log (\n2Bm \u03b7\n) + ( m+n+1\nm )] \u2022 q := \u2308 1\n2\u03f52 log\n( 2m\n\u03b1\n)\u2309\nC.2 PROOFS OF THE LEMMAS RELATED TO THE ACTION-ORACLE PROCEDURE\nLemma 1. Given a set D of meta-actions and a dictionary F of empirical distributions computed by means of Algorithm 2, under the event E\u03f5, if Algorithm 2 returns a meta-action d \u2208 D for a contract p \u2208 P given as input, then it holds that a\u22c6(p) \u2208 A(d).\nProof. For ease of presentation, in this proof we adopt the following additional notation. Given a set D of meta-actions and a dictionary F of empirical distributions computed by means of Algorithm 2, for every meta-action d \u2208 D, we let Pd \u2286 P be the set of all the contracts that have been provided as input to Algorithm 2 during an execution in which it computed an empirical distribution that belongs to F [d]. Moreover, we let I(d) \u2286 A be the set of all the agent\u2019s actions that have been played as a best response by the agent during at least one of such executions of Algorithm 2. Formally, by exploiting the definition of Pd, we can write I(d) := \u22c3 p\u2208Pd a \u22c6(p).\nFirst, we prove the following crucial property of Algorithm 2:\nProperty 4. Given a setD of meta-actions and a dictionary F of empirical distributions computed by means of Algorithm 2, under the event E\u03f5, ti holds that I(d) \u2229 I(d\u2032) = \u2205 for every pair of (different) meta-actions d \u0338= d\u2032 \u2208 D.\nIn order to show that Property 4 holds, we assume by contradiction that there exist two different meta-actions d \u0338= d\u2032 \u2208 D such that a \u2208 I(d) and a \u2208 I(d\u2032) for some agent\u2019s action a \u2208 A, namely that I(d) \u2229 I(d\u2032) \u0338= \u2205. This implies that there exist two contracts p \u2208 Pd and p\u2032 \u2208 Pd\u2032 such that a\u22c6(p) = a\u22c6(p\u2032) = a. Let F\u0303 \u2208 F [d] and F\u0303 \u2032 \u2208 F [d\u2032] be the empirical distributions over outcomes computed by Algorithm 2 when given as inputs the contracts p and p\u2032, respectively. Then, under the event E\u03f5, we have that \u2225F\u0303 \u2212 F\u0303 \u2032\u2225\u221e \u2264 2\u03f5, since the two empirical distributions are generated by sampling from the same (true) distribution Fa. Clearly, the way in which Algorithm 2 works implies that such empirical distributions are associated with the same meta-action and put in the same entry of the dictionary F . This contradicts the assumption that d \u0338= d\u2032, proving that Property 4 holds. Next, we show the following additional crucial property of Algorithm 2:\nProperty 5. LetD be a set of meta-actions and F be a dictionary of empirical distributions computed by means of Algorithm 2. Suppose that the following holds for every d \u2208 D and a, a\u2032 \u2208 I(d):\n\u2225Fa \u2212 Fa\u2032\u2225\u221e \u2264 4\u03f5(|I(d)| \u2212 1). (2)\nThen, if Algorithm 2 is run again, under the event E\u03f5 the same condition continues to hold for the set of meta-actions and the dictionary obtained after the execution of Algorithm 2.\nFor ease of presentation, in the following we call Dold and Fold the set of meta-actions and the dictionary of empirical distributions, respectively, before the last execution of Algorithm 2, while we call Dnew and Fnew those obtained after the last run of Algorithm 2. Moreover, in order to avoid any confusion, given Dold and Fold, we write Iold(d) in place of I(d) for any meta-action d \u2208 Dold. Similarly, given Dnew and Fnew, we write Inew(d) in place of I(d) for any d \u2208 Dnew. In order to show that Property 5 holds, let p \u2208 P be the contract given as input to Algorithm 2 during its last run. Next, we prove that, no matter how Algorithm 2 updates Dold and Fold in order to obtain Dnew and Fnew, the condition in Property 5 continues to hold. We split the proof in three cases.\n1. If |D\u22c4| = 0, then Dnew = Dold \u222a {d\u22c4}, where d\u22c4 is a new meta-action. Since Fnew is built by adding a new entry Fnew[d\u22c4] = {F\u0303} to Fold while leaving all the other entries unchanged, it holds that Inew(d\u22c4) = {a\u22c6(p)} and Inew(d) = Iold(d) for all d \u2208 Dold. As a result, the condition in Equation 2 continues to hold for all the meta-actions d \u2208 Dnew \\{d\u22c4}. Moreover, for the meta-action d\u22c4, the following holds:\n0 = \u2225Fa\u22c6(p) \u2212 Fa\u22c6(p)\u2225\u221e \u2264 4\u03f5(|Inew(d\u22c4)| \u2212 1) = 0,\nas |Inew(d\u22c4)| = 1. This proves that the condition in Equation 2 also holds for d\u22c4.\n2. If |D\u22c4| = 1, then Dnew = Dold. We distinguish between two cases.\n(a) In the case in which a\u22c6(p) \u2208 \u22c3\nd\u2208Dold I old(d), Property 4 immediately implies that\nInew(d) = Iold(d) for all d \u2208 Dnew = Dold. Indeed, if this is not the case, then there would be two different meta-actions d \u0338= d\u2032 \u2208 Dnew = Dold such that Inew(d) = Iold(d) \u222a {a\u22c6(p)} (since Fnew[d] = Fold[d] \u222a {F\u0303}) and a\u22c6(p) \u2208 Inew(d\u2032) = Iold(d\u2032), contradicting Property 4. As a result, the condition in Equation 2 continues to holds for all the meta-actions after the execution of Algorithm 2.\n(b) In the case in which a\u22c6(p) /\u2208 \u22c3\nd\u2208Dold I old(d), the proof is more involved. Let d \u2208\nDnew = Dold be the (unique) meta-action in the set D\u22c4 computed by Algorithm 2. Notice that Inew(d) = Iold(d) \u222a {a\u22c6(p)} by how Algorithm 2 works. As a first step, we show that, for every pair of actions a, a\u2032 \u2208 Inew(d) with a = a\u22c6(p), Equation 2 holds. Under the event E\u03f5, it holds that \u2225Fa\u22c6(p) \u2212 F\u0303\u2225\u221e \u2264 \u03f5, where F\u0303 is the empirical distribution computed by Algorithm 2. Moreover, since the meta-action d has been added to D\u22c4 by Algorithm 2, there exists an empirical distribution F \u2208 F [d] such that \u2225F\u0303 \u2212 F\u2225\u221e \u2264 2\u03f5, and, under the event E\u03f5, there exists an action a\u2032\u2032 \u2208 Iold(d) such that \u2225F \u2212 Fa\u2032\u2032\u2225\u221e \u2264 \u03f5. Then, by applying the triangular inequality we can show that:\n\u2225Fa\u22c6(p) \u2212 Fa\u2032\u2032\u2225\u221e \u2264 \u2225Fa\u22c6(p) \u2212 F\u0303\u2225\u221e + \u2225F\u0303 \u2212 F\u2225\u221e + \u2225F \u2212 Fa\u2032\u2032\u2225\u221e \u2264 4\u03f5.\nBy using the fact that the condition in Equation 2 holds for Dnew and Fnew, for every action a\u2032 \u2208 Inew(d) it holds that:\n\u2225Fa\u22c6(p) \u2212 Fa\u2032\u2225\u221e \u2264 \u2225Fa\u22c6(p) \u2212 Fa\u2032\u2032\u2225\u221e + \u2225Fa\u2032\u2032 \u2212 Fa\u2032\u2225\u221e \u2264 4\u03f5(|Iold(d)| \u2212 1) + 4\u03f5 \u2264 4\u03f5|Iold(d)| = 4\u03f5(|Inew(d)| \u2212 1),\nwhere the last equality holds since |Inew(d)| = |Iold(d)|+1, as a\u22c6(p) /\u2208 \u22c3\nd\u2208Dold I old(d).\nThis proves that Equation 2 holds for every pair of actions a, a\u2032 \u2208 Inew(d) with a = a\u22c6(p). For all the other pairs of actions a, a\u2032 \u2208 Inew(d), the equation holds since it was already satisfied before the last execution of Algorithm 2. Analogously, given that Inew(d) = Iold(d) for all the meta-actions in Dnew \\ {d}, we can conclude that the condition in Equation 2 continues to holds for such meta-actions as well.\n3. If |D\u22c4| > 1, then Dnew = ( Dold \\ D\u22c4 ) \u222a {d\u22c4}, where d\u22c4 is a new meta-action. Clearly, the\ncondition in Equation 2 continues to hold for all the meta-actions in Dold \\ D\u22c4. We only need to show that the condition holds for d\u22c4. We distinguish between two cases.\n(a) In the case in which a\u22c6(p) \u2208 \u22c3\nd\u2208Dold I old(d), let us first notice that a\u22c6(p) \u2208 Iold(d)\nfor some d \u2208 D\u22c4, otherwise Property 4 would be violated (as a\u22c6(p) \u2208 Inew(d\u22c4) by definition). Moreover, it is easy to see that Inew(d\u22c4) = \u22c3 d\u2208D\u22c4 I\nold(d) and, additionally, Iold(d) \u2229 Iold(d\u2032) = \u2205 for all d \u0338= d\u2032 \u2208 D\u22c4, given how Algorithm 2 works and thanks to Property 4. In the following, for ease of presentation, we assume w.l.o.g. that the meta-actions in D\u22c4 are indexed by natural numbers so that D\u22c4 := {d1, . . . d|D\u22c4|} and a\u22c6(p) \u2208 Iold(d1). Next, we show that Equation 2 holds for every pair a, a\u2032 \u2208 Inew(d\u22c4). First, by employing an argument similar to the one used to prove Point 2.2, we can show that, for every dj \u2208 D\u22c4 with j > 1, there exists an action a\u2032\u2032 \u2208 Iold(dj) such that \u2225Fa\u22c6(p) \u2212 Fa\u2032\u2032\u2225\u221e \u2264 4\u03f5. Then, for every pair of actions a, a\u2032 \u2208 Inew(d\u22c4) such that a \u2208 Iold(d1) and a\u2032 \u2208 Iold(dj) for some dj \u2208 D\u22c4 with j > 1, the following holds:\n\u2225Fa \u2212 Fa\u2032\u2225\u221e \u2264 \u2225Fa \u2212 Fa\u22c6(p)\u2225\u221e + \u2225Fa\u22c6(p) \u2212 Fa\u2032\u2032\u2225\u221e + \u2225Fa\u2032\u2032 \u2212 Fa\u2032\u2225\u221e \u2264 4\u03f5(|Iold(d1)| \u2212 1) + 4\u03f5+ 4\u03f5(|Iold(dj)| \u2212 1) = 4\u03f5(|Iold(d1)|+ |Iold(dj)| \u2212 1) \u2264 4\u03f5(|Inew(d\u22c4)| \u2212 1),\nwhere the first inequality follows from an application of the triangular inequality, the second one from the fact that Equation 2 holds before the last execution of Algorithm 2, while the last inequality holds since |Inew(d\u22c4)| = \u2211 d\u2208D\u22c4 |Iold(d)| given that Iold(d)\u2229\nIold(d\u2032) = \u2205 for all d \u0338= d\u2032 \u2208 D\u22c4 thanks to Property 4. As a final step, we show that Equation 2 holds for every pair of actions a, a\u2032 \u2208 Inew(d\u22c4) such that a \u2208 Iold(di) and a\u2032 \u2208 Iold(dj) for some di, dj \u2208 D\u22c4 with i \u0338= j > 1. By the fact that di, dj \u2208 D\u22c4 and triangular inequality, it follows that there exist a\u2032\u2032 \u2208 Iold(di) and a\u2032\u2032\u2032 \u2208 Iold(dj) such that \u2225Fa\u2032\u2032 \u2212 F\u0303\u2225\u221e \u2264 3\u03f5 and \u2225Fa\u2032\u2032\u2032 \u2212 F\u0303\u2225\u221e \u2264 3\u03f5 under E\u03f5. Then, with steps similar to those undertaken above, we can prove the following:\n\u2225Fa \u2212 Fa\u2032\u2225\u221e \u2264 \u2225Fa \u2212 Fa\u2032\u2032\u2225\u221e + \u2225Fa\u2032\u2032 \u2212 F\u0303\u2225\u221e + \u2225F\u0303 \u2212 Fa\u2032\u2032\u2032\u2225\u221e + \u2225Fa\u2032\u2032\u2032 \u2212 Fa\u2032\u2225\u221e \u2264 4\u03f5(|Iold(di)| \u2212 1) + 3\u03f5+ 3\u03f5+ 4\u03f5(|Iold(dj)| \u2212 1) = 4\u03f5(|Iold(di)|+ |Iold(dj)| \u2212 1) + 2\u03f5 \u2264 4\u03f5(|Inew(d\u22c4)| \u2212 1),\nwhich shows that Equation 2 is satisfied for all the pairs of actions a, a\u2032 \u2208 Inew(d\u22c4). (b) In the case in which a\u22c6(p) /\u2208 \u22c3 d\u2208Dold I\nold(d), let us first observe that Inew(d\u22c4) =\u22c3 d\u2208D\u22c4 I\nold(d) \u222a {a\u22c6(p)} and Iold(d) \u2229 Iold(d\u2032) = \u2205 for all d \u0338= d\u2032 \u2208 D\u22c4, given how Algorithm 2 works and thanks to Property 4. Next, we show that Equation 2 holds for every pair of actions a, a\u2032 \u2208 Inew(d\u22c4). As a first step, we consider the case in which a = a\u22c6(p), and we show that Equation 2 holds for every a\u2032 \u2208 Inew(d\u22c4) such that a\u2032 \u2208 Iold(d) for some d \u2208 D\u22c4. In order to show this, we first observe that, given how Algorithm 2 works, there exists F \u2208 Fold[d] such that \u2225F\u0303 \u2212 F\u2225 \u2264 2\u03f5, and, under the event E\u03f5, there exists an action a\u2032\u2032 \u2208 Iold(d) such that \u2225Fa\u2032\u2032 \u2212 F\u2225 \u2264 \u03f5. Then: \u2225Fa\u22c6(p) \u2212 Fa\u2032\u2225\u221e \u2264 \u2225Fa\u22c6(p) \u2212 F\u0303\u2225\u221e + \u2225F\u0303 \u2212 F\u2225\u221e + \u2225F \u2212 Fa\u2032\u2032\u2225\u221e + \u2225Fa\u2032\u2032 \u2212 Fa\u2032\u2225\u221e\n\u2264 4\u03f5+ 4\u03f5(|Iold(d)| \u2212 1) \u2264 4\u03f5(|Inew(d\u22c4)| \u2212 1),\nwhere the last inequality holds since |Inewd\u22c4 | = \u2211\nd\u2208D\u22c4 |Iold(d)| as Iold(d)\u2229Iold(d\u2032) = \u2205 for all d \u0338= d\u2032 \u2208 D\u22c4. As a second step, we consider the case in which a \u2208 Iold(d) and a\u2032 \u2208 Iold(d\u2032) for some pair d \u0338= d\u2032 \u2208 D\u22c4. Given how Algorithm 2 works, there exist F \u2208 Fold[d] and F \u2032 \u2208 Fold[d\u2032] such that \u2225F \u2212 F\u0303\u2225 \u2264 2\u03f5 and \u2225F \u2032\u2212 F\u0303\u2225 \u2264 2\u03f5. Moreover, by the fact that d, d\u2032 \u2208 D\u22c4 and the triangular inequality, it follows that there exist a\u2032\u2032 \u2208 Iold(d) and a\u2032\u2032\u2032 \u2208 Iold(d\u2032) such that \u2225Fa\u2032\u2032 \u2212 F\u0303\u2225\u221e \u2264 3\u03f5 and \u2225Fa\u2032\u2032\u2032 \u2212 F\u0303\u2225\u221e \u2264 3\u03f5 under the event E\u03f5. Then, the following holds: \u2225Fa \u2212 Fa\u2032\u2225\u221e \u2264 \u2225Fa \u2212 Fa\u2032\u2032\u2225\u221e + \u2225Fa\u2032\u2032 \u2212 F\u0303\u2225\u221e + \u2225F\u0303 \u2212 Fa\u2032\u2032\u2032\u2225\u221e + \u2225Fa\u2032\u2032\u2032 \u2212 Fa\u2032\u2225\u221e\n\u2264 4\u03f5(|Iold(d)| \u2212 1) + 3\u03f5+ 3\u03f5+ 4\u03f5(|Iold(d\u2032)| \u2212 1) = 4\u03f5(|Iold(d)|+ |Iold(d\u2032)| \u2212 1) + 2\u03f5 \u2264 4\u03f5(|Inew(d\u22c4)| \u2212 1).\nFinally, by observing that the size of I(d) for each possible set of meta-actions D is bounded by the number of agent\u2019s actions n, for each a, a\u2032 \u2208 I(d) and d \u2208 D we have that:\n\u2225Fa \u2212 Fa\u2032\u2225 \u2264 4n\u03f5. (3) Then, under the event E\u03f5, by letting a\u2032 \u2208 I(d) be the action leading to the empirical distribution F\u0303d computed at Line 19 in Algorithm 2 we have \u2225F\u0303d \u2212 Fa\u2032\u2225\u221e \u2264 \u03f5. Finally, combining the two inequalities, we get:\n\u2225F\u0303d \u2212 Fa\u22c6(p)\u2225\u221e \u2264 \u2225F\u0303d \u2212 Fa\u2032\u2225\u221e + \u2225Fa\u2032 \u2212 Fa\u22c6(p)\u2225\u221e \u2264 4\u03f5n+ \u03f5 \u2264 5\u03f5n. This implies that a\u22c6(p) \u2208 A(d).\nTo conclude the proof we show that I(d) \u2286 A(d). Let a\u2032 \u2208 I(d) be an arbitrary action, and let F\u0303d be the empirical distribution computed by Algorithm 2 by sampling from Fa. Then for each a\u2032\u2032 \u2208 I(d) we have: \u2225F\u0303d \u2212 Fa\u2032\u2032\u2225\u221e \u2264 \u2225F\u0303d \u2212 Fa\u2032\u2225\u221e + \u2225Fa\u2032 \u2212 Fa\u2032\u2032\u2225\u221e \u2264 4\u03f5n+ \u03f5 \u2264 5\u03f5n. Since the latter argument holds for all the actions a \u2208 I(d) this hows that\n\u2225Fa \u2212 Fa\u2032\u2225\u221e \u2264 4n\u03f5, (4) for all the actions a, a\u2032 \u2208 A(d).\nLemma 2. Under the event E\u03f5, Algorithm 2 returns \u22a5 at most 2n times.\nProof. As a first step, we observe that under the event E\u03f5, the size of D increases whenever the principal observes an empirical distribution F\u0303 that satisfies \u2225F\u0303 \u2212 F\u0303 \u2032\u2225\u221e \u2265 2\u03f5 for every F\u0303 \u2032 \u2208 F . This condition holds when the agent chooses an action that has not been selected before. This is because, if the principal commits to a contract implementing an action the agent has already played, the resulting estimated empirical distribution F\u0303 must satisfy \u2225F\u0303 \u2212 F\u2225\u221e \u2264 2\u03f5 for some d \u2208 D and F \u2208 Fd, as guaranteed by the event E\u03f5. Consequently, the cardinality of D can increase at most n times, which corresponds to the total number of actions. Furthermore, we observe that the cardinality of D can decrease by merging one or more meta-actions into a single one, with the condition |D| \u2265 1. Therefore, in the worst case the cardinality of D is reduced by one n times, resulting in 2n being the maximum number of times Algorithm 2 returns \u22a5.\nLemma 3. Given a set D of meta-actions and a dictionary F of empirical distributions computed by means of Algorithm 2, under the event E\u03f5, for every meta-action d \u2208 D and associated action a \u2208 A(d) it holds that |c(d)\u2212 ca| \u2264 4B\u03f5mn.\nProof. Let d \u2208 D be a meta-action and assume that the event E\u03f5 holds. Let a\u2032 \u2208 argmina\u2208A(d) ca and let p\u2032 be a contract such that a\u22c6(p\u2032) = a\u2032. By employing Definition 3, we know that for any action a \u2208 A(d), there exists a contract p \u2208 P such that a\u22c6(p) = a. Then the following inequalities hold:\n4B\u03f5mn \u2265 \u2225Fa \u2212 Fa\u2032\u2225\u221e\u2225p\u22251 \u2265 \u2211 \u03c9\u2208\u2126 (Fa,\u03c9 \u2212 Fa\u2032,\u03c9) p\u03c9 \u2265 ca \u2212 ca\u2032 , (5)\nand, similarly, 4B\u03f5mn \u2265 \u2225Fa\u2032 \u2212 Fa\u2225\u221e\u2225p\u2032\u22251 \u2265 \u2211 \u03c9\u2208\u2126 (Fa\u2032,\u03c9 \u2212 Fa,\u03c9) p\u2032\u03c9 \u2265 ca\u2032 \u2212 ca. (6)\nIn particular, in the chains of inequalities above, the first \u2018\u2265\u2019 holds since \u2225Fa \u2212 Fa\u2032\u2225\u221e \u2264 4\u03f5n by Lemma 1 and the fact that the norm \u2225 \u00b7 \u22251 of contracts in P is upper bounded by Bm. Finally, by applying Equations 5 and 6, we have that:\n|ca \u2212 ca\u2032 | = |ca \u2212 c(d)| \u2264 4B\u03f5mn,\nfor every possible action a \u2208 A(d) since ca\u2032 = c(d) by definition, concluding the proof.\nLemma 4. Given a set D of meta-actions and a dictionary F of empirical distributions computed by means of Algorithm 2, under the event E\u03f5, for every meta-action d \u2208 D, associated action a \u2208 A(d), and contract p \u2208 P it holds | \u2211 \u03c9\u2208\u2126 F\u0303d,\u03c9 p\u03c9 \u2212 c(d)\u2212 \u2211 \u03c9\u2208\u2126 Fa,\u03c9 p\u03c9 + ca| \u2264 9B\u03f5mn.\nProof. To prove the lemma we observe that for each action a \u2208 A(d) it holds:\u2223\u2223\u2223\u2223\u2223\u2211 \u03c9\u2208\u2126 F\u0303d,\u03c9p\u03c9 \u2212 c(d)\u2212 \u2211 \u03c9\u2208\u2126 Fa,\u03c9p\u03c9 + ca \u2223\u2223\u2223\u2223\u2223 \u2264 \u2225F\u0303d \u2212 Fa\u2225\u221e\u2225p\u22251 + |c(d)\u2212 ca| \u2264 5B\u03f5mn+ 4B\u03f5mn = 9B\u03f5mn,\nwhere the first inequality holds by applying the triangular inequality and Holder\u2019s inequality. The second inequality holds by employing Lemma 3 and leveraging Definition 3 that guarantees \u2225F\u0303d \u2212 Fa\u2225\u221e \u2264 5\u03f5n for every action a \u2208 A(d). Additionally, we observe that the \u2225 \u00b7 \u22251 norm of contracts in P is upper bounded by Bm, which concludes the proof.\nWe conclude the subsection by providing an auxiliary lemmas related to the Action-Oracle procedure that will be employed to bound the probability of the clean event E\u03f5 in the proof of Theorem 2. Lemma 12. Given \u03b1, \u03f5 \u2208 (0, 1), let F\u0303 \u2208 \u2206\u2126 be the empirical distribution computed by Algorithm 2 with q := \u2308 1 2\u03f52 log ( 2m \u03b1 )\u2309 for a contract p \u2208 P given as input. Then, it holds that E[F\u0303\u03c9] = Fa\u22c6(p),\u03c9 for all \u03c9 \u2208 \u2126 and, with probability of at least 1\u2212 \u03b1, it also holds that \u2225F\u0303 \u2212 Fa\u22c6(p)\u2225\u221e \u2264 \u03f5.\nProof. By construction, the empirical distribution F\u0303 \u2208 \u2206\u2126 is computed from q i.i.d. samples drawn according to Fa\u22c6(p), with each \u03c9 \u2208 \u2126 being drawn with probability Fa\u22c6(p),\u03c9 . Therefore, the empirical distribution F\u0303 is a random vector supported on \u2206\u2126, whose expectation is such that E[F\u0303\u03c9] = Fa\u22c6(p),\u03c9 for all \u03c9 \u2208 \u2126. Moreover, by Hoeffding\u2019s inequality, for every \u03c9 \u2208 \u2126, we have that:\nP { |F\u0303\u03c9 \u2212 E[F\u0303\u03c9]| \u2265 \u03f5 } = P { |F\u0303\u03c9 \u2212 Fa\u22c6(p),\u03c9| \u2265 \u03f5 } \u2265 1\u2212 2e\u22122q\u03f5 2 . (7)\nThen, by employing a union bound and Equation 7 we have that:\nP { \u2225F\u0303 \u2212 Fa\u22c6(p)\u2225\u221e \u2264 \u03f5 } = P {\u22c2 \u03c9\u2208\u2126 { |F\u0303\u03c9 \u2212 Fa\u22c6(p),\u03c9| \u2264 \u03f5 }} \u2265 1\u2212 2me\u22122q\u03f5 2 \u2265 1\u2212 \u03b1,\nwhere the last inequality holds by definition of q.\nC.3 PROOFS OF THE LEMMAS RELATED TO THE TRY-COVER PROCEDURE\nTo prove Lemma 5, we first introduce Definition 6 for a given a set of meta-actions D. This definition associates to each meta-acxxtion d the set of contracts in which the agent\u2019s utility, computed employing the empirical distribution over outcomes returned by the Action-Oracle procedure and the cost of a meta-action introduced in Definition 4, is greater or equal to the utility computed with the same quantities for all the remaining meta-actions in D. Formally we have that: Definition 6. Given a set of meta-actions D, we let Pdi(D) \u2286 P be the set defined as follows:\nPdi(D) :=\n{ p \u2208 P |\n\u2211 \u03c9\u2208\u2126 F\u0303di,\u03c9p\u03c9 \u2212 c(di) \u2265 \u2211 \u03c9\u2208\u2126 F\u0303dj ,\u03c9p\u03c9 \u2212 c(dj) \u2200dj \u2208 D\n} .\nIt is important to notice that we can equivalently formulate Definition 6 by means of Definition 5. More specifically, given a set of met actions D, for each di \u2208 D we let Pdi(D) := \u2229j\u2208DHij be the intersection of a subset of the halfspaces introduced in Definition 5.\nAs a second step, we introduce two useful lemmas. Lemma 13 shows that for any set of meta-actions D, the union of the sets Pd(D) over all d \u2208 D is equal to P . On the other hand, Lemma 14 shows that the set Pd(D) is a subset of the upper bounds Ud computed by the Try-Cover procedure. Lemma 13. Given a set of meta-actions D it always holds \u222ad\u2208DPd(D) = P .\nProof. The lemma follows observing that, for each p \u2208 P , there always exits a di \u2208 D such that:\u2211 \u03c9\u2208\u2126 F\u0303di,\u03c9p\u03c9 \u2212 c(di) \u2265 \u2211 \u03c9\u2208\u2126 F\u0303dj ,\u03c9p\u03c9 \u2212 c(dj) \u2200dj \u2208 D.\nThis is due to the fact that the cardinality of D is always ensured to be greater than or equal to one. Therefore, for each contract p \u2208 P , there exists a meta-action d such that p \u2208 Pd(D), thus ensuring that \u222ad\u2208DPd(D) = P . This concludes the proof.\nLemma 14. Under the event E\u03f5, it always holds Pd(D) \u2286 Ud for each meta-action d \u2208 D.\nProof. To prove the lemma we observe that, for any meta-action di \u2208 D, the following inclusions hold:\nUdi = \u2229j\u2208Ddi H\u0303ij \u2283 \u2229j\u2208DdiHij \u2283 \u2229j\u2208DHij = Pdi(D).\nThe first inclusion holds thanks to the definition of the halfspace H\u0303ij and employing Lemma 10, which entails under the event E\u03f5. The second inclusion holds because Ddi is a subset of D for each di \u2208 D. Finally, the last equality holds because of the definition of Pdi(D).\nLemma 5. Under the event E\u03f5, when Algorithm 3 returns {Ld}d\u2208D, it holds that \u22c3 d\u2208D Ld = P .\nProof. As a first step we notice that, if Algorithm 3 returns {Ld}d\u2208D, then the set D has not been updated during its execution and, thus, we must have Ld = Ud for all d \u2208 D. In addition, we notice that, under the event E\u03f5, thanks to Lemma 13 and Lemma 14 the following inclusion holds:\u22c3\ndi\u2208D\nUdi \u2287 \u22c3\ndi\u2208D\nPdi(D) = P.\nThen, by putting all together we get:\u22c3 di\u2208D Ldi = \u22c3 di\u2208D Udi = \u22c3 di\u2208D Pi(D) = P,\nconcluding the proof.\nLemma 6. Under the event E\u03f5, when Algorithm 3 returns {Ld}d\u2208D, for every meta-action d \u2208 D, contract p \u2208 Ld and action a\u2032 \u2208 A(d), there exists a \u03b3 that polynomially depends on m, n, \u03f5, and B such that: \u2211\n\u03c9\u2208\u2126 Fa\u2032,\u03c9 p\u03c9 \u2212 ca\u2032 \u2265 \u2211 \u03c9\u2208\u2126 Fa,\u03c9 p\u03c9 \u2212 ca \u2212 \u03b3 \u2200a \u2208 A.\nProof. In order to prove the lemma, we rely on the crucial observation that, for any vertex p \u2208 V (Ldi) of the lower bound of a meta-action di \u2208 D, the Action-Oracle procedure called by Algorithm 3 with p as input either returned di or another meta-action dj \u2208 D such that p \u2208 H\u0303ij with dj \u2208 Ddi . First, we consider the case in which the meta-action returned by Action-Oracle in the vertex p is equal to di. In such a case, a\u22c6(p) \u2208 A(di) thanks to Lemma 1 and the following holds:\u2211\n\u03c9\u2208\u2126 F\u0303di,\u03c9 p\u03c9 \u2212 c(di) \u2265 \u2211 \u03c9\u2208\u2126 Fa\u22c6(p),\u03c9 p\u03c9 \u2212 ca\u22c6(p) \u2212 9B\u03f5mn,\nby means of Lemma 4. Next, we consider the case in which the meta-action returned by Algorithm 2 is equal to dj with p that belongs to H\u0303ij . In such a case the following inequalities hold:\u2211\n\u03c9\u2208\u2126 F\u0303di,\u03c9 p\u03c9 \u2212 c(di) \u2265 \u2211 \u03c9\u2208\u2126 F\u0303dj ,\u03c9 p\u03c9 \u2212 c(dj)\u2212 y\n\u2265 \u2211 \u03c9\u2208\u2126 Fa\u22c6(p),\u03c9 p\u03c9 \u2212 ca\u22c6(p) \u2212 9B\u03f5mn\u2212 y,\nwhere the first inequality follows by Lemma 10 that guarantees that p belongs to H\u0303ij \u2286 Hyij with y = 18B\u03f5mn2 + 2n\u03b7 \u221a m, while the second inequality holds because of Lemma 4, since a\u22c6(p) \u2208 A(dj) thanks to Lemma 1 . Finally, by putting together the inequalities for the two cases considered above and employing Lemma 4, we can conclude that, for every vertex p \u2208 V (Ld) of the lower bound of a meta-action d \u2208 D, it holds: \u2211\n\u03c9\u2208\u2126 Fa,\u03c9 p\u03c9 \u2212 ca \u2265 \u2211 \u03c9\u2208\u2126 F\u0303d,\u03c9 p\u03c9 \u2212 c(d)\u2212 9B\u03f5mn\n\u2265 \u2211 \u03c9\u2208\u2126 Fa\u22c6(p),\u03c9 p\u03c9 \u2212 ca\u22c6(p) \u2212 \u03b3, (8)\nfor each action a \u2208 A(d) by setting \u03b3 := 27B\u03f5mn2 + 2n\u03b7 \u221a m.\nMoreover, by noticing that each lower bound Ld is a convex polytope, we can employ the Carathe\u0301odory\u2019s theorem to decompose each contract p \u2208 Ld as a convex combination of the vertices of Ld. Formally: \u2211\np\u2032\u2208V (Ld)\n\u03b1(p\u2032) p\u2032\u03c9 = p\u03c9 \u2200\u03c9 \u2208 \u2126, (9)\nwhere \u03b1(p\u2032) \u2265 0 is the weight given to vertex p\u2032 \u2208 V (Ld), so that it holds \u2211\np\u2032\u2208V (Ld) \u03b1(p \u2032) = 1.\nFinally, for every p \u2208 Ld and action a \u2208 A(d) we have:\u2211 \u03c9\u2208\u2126 Fa\u22c6(p),\u03c9 p\u03c9 \u2212 ca\u22c6(p) = \u2211 \u03c9\u2208\u2126 Fa\u22c6(p),\u03c9  \u2211 p\u2032\u2208V (Ld) \u03b1(p\u2032) p\u2032\u03c9 \u2212 ca\u22c6(p)\n= \u2211\np\u2032\u2208V (Ld)\n\u03b1(p\u2032) (\u2211 \u03c9\u2208\u2126 Fa\u22c6(p),\u03c9 p \u2032 \u03c9 \u2212 ca\u22c6(p) )\n\u2264 \u2211\np\u2032\u2208V (Ld)\n\u03b1(p\u2032) (\u2211 \u03c9\u2208\u2126 Fa,\u03c9 p \u2032 \u03c9 \u2212 ca + \u03b3 )\n= \u2211 \u03c9\u2208\u2126 Fa,\u03c9  \u2211 p\u2032\u2208V (Ld) \u03b1(p\u2032) p\u2032\u03c9 \u2212 ca + \u03b3 = \u2211 \u03c9\u2208\u2126 Fa,\u03c9 p\u03c9 \u2212 ca + \u03b3,\nwhere the first and the last equalities hold thanks of Equation 9, while the second and the third equalities hold since \u2211 p\u2032\u2208V (Ld) \u03b1(p\n\u2032) = 1. Finally, the inequality holds thanks to Inequality 8 by setting \u03b3 := 27B\u03f5mn2 + 2n\u03b7 \u221a m.\nLemma 7. Under event E\u03f5, Algorithm 3 requires at most O ( n2q ( log (Bm/\u03b7) + ( m+n+1\nm\n))) rounds.\nProof. As a first step, we observe that Algorithm 3 terminates in a finite number of rounds. By the way in which Algorithm 3 intersects the upper bounds with the halfspaces computed with the help of the Find-HS procedure, the algorithm terminates with Ld = Ud for all d \u2208 D after a finite number of rounds as long as, for each meta-action di \u2208 D, the halfspaces H\u0303ij with dj \u2208 D are computed at most once. It is easy to see that this is indeed the case. Specifically, for every meta-actions di \u2208 D, Algorithm 6 is called to build the halfspace H\u0303ij only when Algorithm 2 returns dj with dj \u0338\u2208 Ddi for a vertex p \u2208 V (Udi) of the upper bound Udi . If the halfspace has already been computed, then dj \u2208 Ddi by the way in which the set Ddi is updated. As a result, if Algorithm 2 called on a vertex of the upper bound Udi returns the meta-action dj \u2208 Ddi , then Algorithm 3 does not compute the halfspace again.\nFor every di \u2208 D, the number of vertices of the upper bound Udi is at most ( m+n+1\nm\n) = O(mn),\nsince the halfspaces defining the polytope Udi are a subset of the m+ 1 halfspaces defining P and the halfspaces H\u0303ij with dj \u2208 D. The number of halfspaces H\u0303ij is at most n for every meta-action di \u2208 D. Consequently, since each vertex lies at the intersection of at most m linearly independent hyperplanes, the total number of vertices of the upper bound Udi is bounded by ( m+n+1\nm\n) = O(mn),\nfor every di \u2208 D We also observe that, for every di \u2208 D, the while loop in Line 28 terminates with at most V (Udi) =( m+n+1\nm\n) = O(mn) iterations. This is because, during each iteration, either the algorithm finds a new\nhalfspace or it exits from the loop. At each iteration of the loop, the algorithm invokes Algorithm 2, which requires q rounds. Moreover, finding a new halfspace requires a number of rounds of the order of O (q log (Bm/\u03b7)), as stated in Lemma 11. Therefore, the total number of rounds required by the execution of the while loop in Line 28 is at most O ( q ( log (Bm/\u03b7) + ( m+n+1\nm\n))) .\nLet us also notice that, during each iteration of the while loop in Line 9, either the algorithm finds a new halfspace or it exits from the loop. This is because, if no halfspace is computed, the algorithm does not update the boundaries of Udi , meaning that the meta-action implemented in each vertex of Ldi is either di or some dj belonging to Ddi . Moreover, since the number of halfspaces H\u0303ij is bounded by n for each Udi , the while loop in Line 9 terminates in at most n steps. As a result, the number of rounds required by the execution of the while loop in Line 9 is of the order of O ( nq ( log(Bm/\u03b7) + ( m+n+1\nm\n))) , being the while loop in Lines 28 nested within the one in Line 9.\nFinally, we observe that the while loop in Line 7 iterates over the set of meta-actions actions D, which has cardinality at most n. Therefore, the total number of rounds required to execute the entire algorithm is of the order of O ( n2q ( log (Bm/\u03b7) + ( m+n+1\nm\n))) , which concludes the proof.\nC.4 PROOFS OF THE LEMMAS RELATED TO THE FIND-CONTRACT PROCEDURE\nLemma 8. Under the event E\u03f5, if {Ld}d\u2208D is a cover of P computed by Try-Cover, Algorithm 4 returns a contract p \u2208 [0, B]m such that u(p) \u2265 maxp\u2032\u2208[0,B]m u(p\u2032)\u2212 \u03c1.\nProof. In the following, we define po \u2208 [0, B]m as the optimal contract, while we let p\u2113 := (1 \u2212\u221a \u03b3)po + \u221a \u03b3r, where \u03b3 is defined as in Lemma 6. Additionally, we define do \u2208 D as one of the\nmeta-actions such that po \u2208 Ldo . Similarly, we let d\u2113 \u2208 D be one of the meta-actions such that p\u2113 \u2208 Ld\u2113 . It is important to note that p\u2113 \u2208 [0, B]m since \u2225r\u2225\u221e \u2264 1. Furthermore, Lemma 5 ensures that there exists at least one d\u2113 \u2208 D such that p\u2113 \u2208 Ld\u2113 . As a first step, we prove that, for each ai \u2208 A(d\u2113), it holds:\n\u03b3 \u2265 \u2211 \u03c9\u2208\u2126 (Fa\u22c6(p\u2113),\u03c9 \u2212 Fai,\u03c9)p\u2113\u03c9 + cai \u2212 ca\u22c6(p\u2113)\n\u2265 \u2211 \u03c9\u2208\u2126 (Fa\u22c6(po),\u03c9 \u2212 Fai,\u03c9)p\u2113\u03c9 + cai \u2212 ca\u22c6(po)\n= \u2211 \u03c9\u2208\u2126 (Fa\u22c6(po),\u03c9 \u2212 Fai,\u03c9)po\u03c9 + cai \u2212 ca\u22c6(po) + \u221a \u03b3 \u2211 \u03c9\u2208\u2126 (Fa\u22c6(po),\u03c9 \u2212 Fai,\u03c9)(r\u03c9 \u2212 po\u03c9)\n\u2265 \u221a\u03b3 \u2211 \u03c9\u2208\u2126 (Fa\u22c6(po),\u03c9 \u2212 Fai,\u03c9)(r\u03c9 \u2212 po\u03c9),\nwhere the first inequality holds because of Lemma 6 since p\u2113 \u2208 Ld\u2113 , while the second and the third inequalities hold because of the definition of best-response and the equality holds because of the definition of p\u2113 \u2208 Ld\u2113 . Then, by rearranging the latter inequality, we can show that for each action ai \u2208 A(d\u2113) the following holds: \u2211\n\u03c9\u2208\u2126 Fai,\u03c9(r\u03c9 \u2212 po\u03c9) \u2265 \u2211 \u03c9\u2208\u2126 Fa\u22c6(po),\u03c9(r\u03c9 \u2212 po\u03c9)\u2212 \u221a \u03b3. (10)\nFurthermore, for each action ai \u2208 A(d\u2113), we have that:\u2211 \u03c9\u2208\u2126 Fai,\u03c9 ( r\u03c9 \u2212 p\u2113\u03c9 ) = \u2211 \u03c9\u2208\u2126 Fai,\u03c9 (r\u03c9 \u2212 ((1\u2212 \u221a \u03b3)po\u03c9 + \u221a \u03b3r\u03c9))\n= \u2211 \u03c9\u2208\u2126 Fai,\u03c9 (r\u03c9 \u2212 po\u03c9)\u2212 \u221a \u03b3 \u2211 \u03c9\u2208\u2126 Fai,\u03c9 (r\u03c9 \u2212 po\u03c9)\n\u2265 \u2211 \u03c9\u2208\u2126 Fai,\u03c9 (r\u03c9 \u2212 po\u03c9)\u2212 \u221a \u03b3\n\u2265 \u2211 \u03c9\u2208\u2126 Fa\u22c6(po),\u03c9(r\u03c9 \u2212 po\u03c9)\u2212 2 \u221a \u03b3,\nwhere the first equality holds because of the definition of p\u2113 \u2208 Ld\u2113 , while the first inequality holds since \u2225r\u2225\u221e \u2264 1 and the latter inequality because of Equation 10. Putting all together we get:\u2211\n\u03c9\u2208\u2126 Fai,\u03c9\n( r\u03c9 \u2212 p\u2113\u03c9 ) \u2265 \u2211 \u03c9\u2208\u2126 Fa\u22c6(po),\u03c9(r\u03c9 \u2212 po\u03c9)\u2212 2 \u221a \u03b3\n= OPT\u2212 2\u221a\u03b3, (11)\nfor each ai \u2208 A(d\u2113) with p\u2113 \u2208 Ld\u2113 . Now, we show that the principal\u2019s utility in the contract returned by Algorithm 4 is close to the optimal one. To do that we let {F\u0303d}d\u2208D be the set of empirical distributions employed by Algorithm 4. Furthermore, we let p\u22c6 \u2208 Ld\u22c6 be the contract computed in Line 5 of Algorithm 4. Analogously, we let pf := (1 \u2212 \u221a\u03b3)p\u22c6 + \u221a\u03b3r be the final contract the principal commits to. Then, for each ai \u2208 A(d\u22c6), we have:\nu(pf ) \u2265 \u2211 \u03c9\u2208\u2126 Fai,\u03c9 (r\u03c9 \u2212 p\u22c6\u03c9)\u2212 2 \u221a \u03b3\n\u2265 \u2211 \u03c9\u2208\u2126 F\u0303d\u22c6,\u03c9 (r\u03c9 \u2212 p\u22c6\u03c9)\u2212 2 \u221a \u03b3 \u2212 5\u03f5mn,\nwhere the first inequality follows from Proposition A.4 by Dutting et al. (2021) and u(p\u22c6) \u2264 1, while the second inequality holds by means of Definition 3 as ai \u2208 A(d\u22c6). Analogously, for each ai \u2208 A(d\u2113), we have: \u2211\n\u03c9\u2208\u2126 F\u0303d\u22c6,\u03c9 (r\u03c9 \u2212 p\u22c6\u03c9) \u2265 \u2211 \u03c9\u2208\u2126 F\u0303d\u2113,\u03c9 ( r\u03c9 \u2212 p\u2113\u03c9 ) \u2265 \u2211 \u03c9\u2208\u2126 Fai,\u03c9 ( r\u03c9 \u2212 p\u2113\u03c9 ) \u2212 5\u03f5mn\n\u2265 OPT\u2212 2\u221a\u03b3 \u2212 5\u03f5mn, where the first inequality holds because of the optimality of p\u22c6, the second inequality holds because of Definition 3 since ai \u2208 A(d\u2113), while the third inequality holds because of Equation 11. Finally, by putting all together we get:\nu(pf ) \u2265 OPT\u2212 4 \u221a 27B\u03f5mn2 + 2n\u03b7 \u221a m\u2212 10\u03f5mn\n\u2265 OPT\u2212 32 \u221a B\u03f5m2n2,\nwhere we employ the definition of \u03b3 as prescribed by Lemma 6. As a result, in order to achieve a \u03c1-optimal solution we set:\n\u03f5 := \u03c12\n322Bm2n2 ,\nwhile \u03b7 := \u03f5 \u221a mn/2.\nC.5 PROOF OF THEOREM 2\nTheorem 2. Given \u03c1 \u2208 (0, 1), \u03b4 \u2208 (0, 1), and B \u2265 1 as inputs, with probability at least 1\u2212 \u03b4 the Discover-and-Cover algorithm (Algorithm 1) is guaranteed to return a contract p \u2208 [0, B]m such that u(p) \u2265 maxp\u2032\u2208[0,B]m u(p\u2032)\u2212 \u03c1 in at most O\u0303(mn \u00b7 I \u00b7 1/\u03c14 log(1/\u03b4)) rounds, where I is a term that depends polynomially in m, n, and B.\nProof. First, we notice that to achieve \u03c1-optimal solution under the event E\u03f5, as observed in Lemma 8, we must set:\n\u03f5 := \u03c12\n322Bm2n2 and \u03b7 := \u03f5\n\u221a mn/2. (12)\nTo ensure that Algorithm 1 returns a \u03c1-optimal solution with a probability of at least 1\u2212 \u03b4, we need to set the remaining parameters \u03b1 and q in a way that P(E\u03f5) \u2265 1\u2212 \u03b4. Intuitively, the probability of the event E\u03f5 corresponds to the probability that, whenever Algorithm 2 is invoked by Algorithm 3, it returns an empirical distribution sufficiently close to the actual one.\nFirst, we observe that given \u03f5, \u03b1 and a distribution over outcomes F , Algorithm 2 computes an empirical distribution F\u0303 satisfying \u2225F\u0303 \u2212 F\u2225\u221e \u2264 \u03f5 with a probability of at least 1\u2212 \u03b1, in a number of rounds q = \u2308 1 2\u03f52 log ( 2m \u03b1 )\u2309 as prescribed by Lemma 12.\nTo ensure that Algorithm 2 returns an empirical distribution that closely approximates the true distribution each time it is called, we need to bound the number of times the Discover-and-Cover procedure invokes Algorithm 2. By applying Lemma 7, we have that the maximum number of times the Action-Oracle algorithm is called by the Try-Cover algorithm is bounded by n2 ( log (2Bm/\u03b7) + ( m+n+1\nm\n)) . Additionally, according to Lemma 2, the Try-Cover procedure is\ninvoked at most 2n times during the execution of the Discover-and-Cover algorithm. Consequently, the number of times Algorithm 2 is invoked is bounded by 2n3 ( log (2Bm/\u03b7) + ( m+n+1\nm\n)) .\nBy applying a union bound over all the times Algorithm 2 is invoked and considering that each time it returns an empirical distribution that is within \u03f5 distance in the \u2225 \u00b7 \u2225\u221e norm from the actual distribution with probability at least 1\u2212 \u03b1, we can conclude that the event E\u03f5 occurs with probability at least:\nP(E\u03f5) \u2265 1\u2212 2\u03b1n3 ( log ( 2Bm\n\u03b7\n) + ( m+ n+ 1\nm\n)) .\nAs a result, by setting:\n\u03b1 := \u03b4 2n3 ( log (2Bm/\u03b7) + ( m+n+1\nm )) , (13) with \u03b7 defined as above guarantees that P(E\u03f5) \u2265 1\u2212 \u03b4. Thus, the number of rounds q required by Algorithm 2 is equal to:\nq :=\n\u2308 1\n2\u03f52 log\n( 2m\n\u03b1\n)\u2309 ,\nwith \u03f5, \u03b1 defined as in Equations 12 and 13. Then, by employing Lemma 2 and Lemma 7, the number of rounds to execute Algorithm 1 is of the order of O ( qn3 ( log (2Bm/\u03b7) + ( m+n+1\nm\n))) .\nFinally, by definition of the parameters \u03b1, \u03f5, and q , the total number of rounds required by Algorithm 1 to return a \u03c1-optimal solution with probability at least 1\u2212 \u03b4 is at most:\nO\u0303 ( mn B2m4n8\n\u03c14 log\n( 1\n\u03b4\n)) ,\nwhich concludes the proof."
        },
        {
            "heading": "D OTHER OMITTED PROOFS",
            "text": "In this section, we provide all the remaining omitted proofs. Theorem 1. For any number of rounds N \u2208 N, there is no algorithm that is guaranteed to find a \u03ba-optimal contract with probability greater than or equal to 1\u2212 \u03b4 by using less than N rounds, where \u03ba, \u03b4 > 0 are some suitable absolute constants. Proof. We consider a group of instances parametrized by a parameter \u03f5 \u2208 ( 0, 180 ) . In each instance, we let A = {a1, a2} be the set of actions while we let \u2126 = {\u03c91, \u03c92, \u03c93} be the set of outcomes. Furthermore, the distributions over the outcomes of the two actions are defined as follows: Fa1 =( 1 2 , 0, 1 2 ) and Fa2 = (0, \u03f5, 1\u2212 \u03f5) with associated cost of ca1 = 0 and ca2 = 14 , respectively. In all the instances the principal\u2019s reward is given by r = (0, 0, 1) while the optimal contract is equal to p\u2217 = (0, 14\u03f5 , 0), resulting in a principal\u2019s expected utility of u(p \u2217) = 34 \u2212 \u03f5.\nAs a first step, we show that if p\u03c92 \u2264 18\u03f5 , then the principal\u2019s utility is at most 9 80 -optimal. To show that, we first consider the case in which the agent selects action a1. In such a case, the highest expected utility achieved by the principal is at most 12 , which occurs when they commit to the null contract p = (0, 0, 0). Clearly, the utility achieved in p = (0, 0, 0) is not 980 -optimal, for each possible \u03f5 \u2208 (0, 180 ). Then, we consider the case in which the agent selects action a2. In this scenario, we observe that the agent selects such an action only when the contract committed by the principal is such that p\u03c93 > 1 4 , resulting in an expected principal\u2019 s utility of at most:\nu(p) = \u2211 \u03c9\u2208\u2126 Fa2,\u03c9(r\u03c9 \u2212 p\u2032\u03c9) \u2264 \u2212 1 8 + (1\u2212 \u03f5) ( 1\u2212 1 4 ) \u2264 \u22121 8 + 3 4 = 5 8 ,\nwhich is not 980 -optimal, for any value of \u03f5 \u2208 (0, 1 80 ). Consequently, for each possible action selected by the agent, if p\u03c93 \u2264 14 , then the expected utility of the principal\u2019s utility cannot be 9 80 -optimal.\nTo conclude the proof, we consider two instances characterized by \u03f51 = 180N log(2N) and \u03f52 = 1 80N2 , for an arbitrary fixed N \u2265 1. In the following, we let P\u03f51 and P\u03f52 be the probability measures induced by the N-rounds interconnection of an arbitrary algorithm executed in the first and in the second instances, respectively. Furthermore, we denote with KL(P\u03f51 ,P\u03f52) the Kullback-Leibler divergence between these two measures. Then, by applying the Kullback-Leibler decomposition, with a simple calculation we can show that:\nKL(P\u03f51 ,P\u03f52) \u2264 E\u03f51 [ N\u2211 t=1 KL(F \u03f51a2 , F \u03f52 a2 ) ]\n\u2264 N(\u03f51 log(\u03f51/\u03f52) + (1\u2212 \u03f51) log((1\u2212\u03f51)/(1\u2212\u03f52))) \u2264 2/79,\nwhere we let F \u03f51ai and F \u03f52 ai be the distributions over outcomes of action ai \u2208 A in the first and in the second instances, respectively.\nWe now introduce the event I, defined as the event in which the final contract returned by a given algorithm satisfies the condition p\u03c92 \u2265 18\u03f52 . We observe that if the event I holds in the first instance, then the learned contract provides a negative principal\u2019s utility. On the contrary, if such an event does not hold in the second instance, the final contract is not 980 -optimal, as previously observed. Then, by the Pinsker\u2019s inequality we have that:\nP\u03f52(Ic) + P\u03f51(I) \u2265 1\n2 exp (\u2212KL(P\u03f51 ,P\u03f52)) =\n1 2 exp (\u22122/79). (14)\nConsequently, there exists no algorithm returning a 9/80-optimal with a probability greater or equal to 14 exp (\u22122/79), thus concluding the proof.\nTheorem 3. Given \u03b1 \u2208 (0, 1), Algorithm 5 achieves RT \u2264 O\u0303 ( mn \u00b7 I \u00b7 log(1/\u03b4) \u00b7 T 4/5 ) with probability at least 1\u2212 \u03b4, where I is a term that depends polynomially on m, n, and B.\nProof. Thanks to Theorem 2, we know that by employing an appropriate number of rounds, the solution returned by Algorithm 1 is \u03c1-optimal with probability at least 1\u2212 \u03b4, for given values of \u03c1 and \u03b4 greater than zero. Furthermore, we notice that the per-round regret suffered by Algorithm 5 is bounded by B + 1 during the execution of Algorithm 1, and it is at most \u03c1 for the remaining rounds. Formally, we have that:\nRT \u2264 O\u0303 ( mn B3m4n8\n\u03c14 log\n( 1\n\u03b4\n) + T\u03c1 ) .\nThus, by setting \u03c1 = mn/5B3/5mn8/5T\u22121/5 as input to Algorithm 1, with probability at least 1\u2212 \u03b4 the cumulative regret is bounded by:\nRT \u2264 O\u0303 ( mnB3/5n8/5 log ( 1\n\u03b4\n) T 4/5 ) ,\nconcluding the proof."
        }
    ],
    "title": "LEARNING OPTIMAL CONTRACTS: HOW TO EXPLOIT SMALL ACTION SPACES",
    "year": 2024
}