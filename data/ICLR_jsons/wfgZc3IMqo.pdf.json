{
    "abstractText": "Deep neural networks and large-scale datasets have revolutionized the field of machine learning. However, these large networks are susceptible to overfitting to label noise, resulting in reduced generalization. To address this challenge, two promising approaches have emerged: i) loss reweighting, which reduces the influence of noisy examples on the training loss, and ii) label correction that replaces noisy labels with estimated true labels. These directions have been pursued separately or combined as independent methods, lacking a unified approach. In this work, we present a unified method that seamlessly combines loss reweighting and label correction to enhance robustness against label noise in classification tasks. Specifically, by leveraging ideas from compositional data analysis in statistics, we frame the problem as a regression task, where loss reweighting and label correction can naturally be achieved with a shifted Gaussian label noise model. Our unified approach achieves strong performance compared to recent baselines on several noisy labelled datasets. We believe this work is a promising step towards robust deep learning in the presence of label noise. Our code will be available at: anonymous.4open.science/r/SGN-456E/.",
    "authors": [
        {
            "affiliations": [],
            "name": "LOSS REWEIGHTING"
        }
    ],
    "id": "SP:858127574f7212c128bbeefc81c2c8031de54aaa",
    "references": [
        {
            "authors": [
                "J. Aitchison"
            ],
            "title": "The Statistical Analysis of Compositional Data. Monographs on Statistics and Applied Probability",
            "venue": "URL https: //books.google.se/books?id=N1LOngEACAAJ",
            "year": 2011
        },
        {
            "authors": [
                "John Aitchison"
            ],
            "title": "The statistical analysis of compositional data",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological),",
            "year": 1982
        },
        {
            "authors": [
                "John Aitchison"
            ],
            "title": "Principal component analysis of compositional data",
            "venue": "Biometrika, 70(1):57\u201365,",
            "year": 1983
        },
        {
            "authors": [
                "John Aitchison"
            ],
            "title": "A concise guide to compositional data analysis",
            "venue": "In Compositional Data Analysis Workshop,",
            "year": 2005
        },
        {
            "authors": [
                "Abdulaziz Alenazi"
            ],
            "title": "A review of compositional data analysis and recent advances",
            "venue": "Communications in Statistics-Theory and Methods,",
            "year": 2023
        },
        {
            "authors": [
                "G\u00f6rkem Algan",
                "Ilkay Ulusoy"
            ],
            "title": "Image classification with deep learning in the presence of noisy labels: A survey",
            "venue": "Knowledge-Based Systems,",
            "year": 2021
        },
        {
            "authors": [
                "J Atchison",
                "Sheng M Shen"
            ],
            "title": "Logistic-normal distributions: Some properties and uses",
            "year": 1980
        },
        {
            "authors": [
                "Yingbin Bai",
                "Erkun Yang",
                "Bo Han",
                "Yanhua Yang",
                "Jiatong Li",
                "Yinian Mao",
                "Gang Niu",
                "Tongliang Liu"
            ],
            "title": "Understanding and improving early stopping for learning with noisy labels",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Lucas Beyer",
                "Olivier J H\u00e9naff",
                "Alexander Kolesnikov",
                "Xiaohua Zhai",
                "A\u00e4ron van den Oord"
            ],
            "title": "Are we done with imagenet",
            "venue": "arXiv preprint arXiv:2006.07159,",
            "year": 2020
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Mark Sellke"
            ],
            "title": "A universal law of robustness via isoperimetry",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Pengfei Chen",
                "Guangyong Chen",
                "Junjie Ye",
                "Pheng-Ann Heng"
            ],
            "title": "Noise against noise: stochastic label noise helps combat inherent label noise",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Mark Collier",
                "Basil Mustafa",
                "Efi Kokiopoulou",
                "Rodolphe Jenatton",
                "Jesse Berent"
            ],
            "title": "Correlated input-dependent label noise in large-scale image classification",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Padraig Cunningham",
                "Sarah Jane Delany"
            ],
            "title": "k-nearest neighbour classifiers-a tutorial",
            "venue": "ACM computing surveys (CSUR),",
            "year": 2021
        },
        {
            "authors": [
                "Juan Jos\u00e9 Egozcue",
                "Vera Pawlowsky-Glahn",
                "Gl\u00f2ria Mateu-Figueras",
                "Carles Barcelo-Vidal"
            ],
            "title": "Isometric logratio transformations for compositional data analysis",
            "venue": "Mathematical geology,",
            "year": 2003
        },
        {
            "authors": [
                "Juan Jos\u00e9 Egozcue",
                "Carles Barcel\u00f3-Vidal",
                "Josep Antoni Mart\u0131\u0301n-Fern\u00e1ndez",
                "Eusebi Jarauta-Bragulat",
                "Jos\u00e9 Luis D\u0131\u0301az-Barrero",
                "Gl\u00f2ria Mateu-Figueras"
            ],
            "title": "Elements of simplicial linear algebra and geometry",
            "venue": "Compositional data analysis: Theory and applications,",
            "year": 2011
        },
        {
            "authors": [
                "Erik Englesson",
                "Hossein Azizpour"
            ],
            "title": "Generalized jensen-shannon divergence loss for learning with noisy labels",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Erik Englesson",
                "Amir Mehrpanah",
                "Hossein Azizpour"
            ],
            "title": "Logistic-normal likelihoods for heteroscedastic label noise",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Stuart Geman",
                "Elie Bienenstock",
                "Ren\u00e9 Doursat"
            ],
            "title": "Neural networks and the bias/variance dilemma",
            "venue": "Neural computation,",
            "year": 1992
        },
        {
            "authors": [
                "Aritra Ghosh",
                "Himanshu Kumar",
                "P Shanti Sastry"
            ],
            "title": "Robust loss functions under label noise for deep neural networks",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Purvi Goel",
                "Li Chen"
            ],
            "title": "On the robustness of monte carlo dropout trained with noisy labels",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Guo",
                "Mohammad Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Bo Han",
                "Quanming Yao",
                "Xingrui Yu",
                "Gang Niu",
                "Miao Xu",
                "Weihua Hu",
                "Ivor Tsang",
                "Masashi Sugiyama"
            ],
            "title": "Co-teaching: Robust training of deep neural networks with extremely noisy labels",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Bo Han",
                "Quanming Yao",
                "Tongliang Liu",
                "Gang Niu",
                "Ivor W Tsang",
                "James T Kwok",
                "Masashi Sugiyama"
            ],
            "title": "A survey of label-noise representation learning: Past, present and future",
            "year": 2011
        },
        {
            "authors": [
                "Tianyang Hu",
                "Jun Wang",
                "Wenjia Wang",
                "Zhenguo Li"
            ],
            "title": "Understanding square loss in training overparametrized neural network classifiers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Peter J Huber"
            ],
            "title": "Robust statistics, volume 523",
            "year": 2004
        },
        {
            "authors": [
                "Like Hui",
                "Mikhail Belkin"
            ],
            "title": "Evaluation of neural architectures trained with square loss vs crossentropy in classification",
            "year": 2006
        },
        {
            "authors": [
                "Pavel Izmailov",
                "Dmitrii Podoprikhin",
                "Timur Garipov",
                "Dmitry Vetrov",
                "Andrew Gordon Wilson"
            ],
            "title": "Averaging weights leads to wider optima and better generalization",
            "venue": "arXiv preprint arXiv:1803.05407,",
            "year": 2018
        },
        {
            "authors": [
                "Arthur Jacot",
                "Franck Gabriel",
                "Cl\u00e9ment Hongler"
            ],
            "title": "Neural tangent kernel: Convergence and generalization in neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Lu Jiang",
                "Zhengyuan Zhou",
                "Thomas Leung",
                "Li-Jia Li",
                "Li Fei-Fei"
            ],
            "title": "Mentornet: Learning datadriven curriculum for very deep neural networks on corrupted labels",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Alex Kendall",
                "Yarin Gal"
            ],
            "title": "What uncertainties do we need in bayesian deep learning for computer vision",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Sotiris B Kotsiantis"
            ],
            "title": "Decision trees: a recent overview",
            "venue": "Artificial Intelligence Review,",
            "year": 2013
        },
        {
            "authors": [
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "Temporal ensembling for semi-supervised learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "HO Lancaster"
            ],
            "title": "The helmert matrices",
            "venue": "The American Mathematical Monthly,",
            "year": 1965
        },
        {
            "authors": [
                "Junnan Li",
                "Richard Socher",
                "Steven CH Hoi"
            ],
            "title": "Dividemix: Learning with noisy labels as semisupervised learning",
            "venue": "arXiv preprint arXiv:2002.07394,",
            "year": 2020
        },
        {
            "authors": [
                "Mingchen Li",
                "Mahdi Soltanolkotabi",
                "Samet Oymak"
            ],
            "title": "Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks",
            "venue": "In International conference on artificial intelligence and statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Wen Li",
                "Limin Wang",
                "Wei Li",
                "Eirikur Agustsson",
                "Luc Van Gool"
            ],
            "title": "Webvision database: Visual learning and understanding from web data",
            "venue": "arXiv preprint arXiv:1708.02862,",
            "year": 2017
        },
        {
            "authors": [
                "Sheng Liu",
                "Jonathan Niles-Weed",
                "Narges Razavian",
                "Carlos Fernandez-Granda"
            ],
            "title": "Early-learning regularization prevents memorization of noisy labels. Advances in neural information processing",
            "year": 2033
        },
        {
            "authors": [
                "Sheng Liu",
                "Zhihui Zhu",
                "Qing Qu",
                "Chong You"
            ],
            "title": "Robust training under label noise by overparameterization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Yangdi Lu",
                "Yang Bo",
                "Wenbo He"
            ],
            "title": "Noise attention learning: Enhancing noise robustness by gradient scaling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Michal Lukasik",
                "Srinadh Bhojanapalli",
                "Aditya Menon",
                "Sanjiv Kumar"
            ],
            "title": "Does label smoothing mitigate label noise",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Xingjun Ma",
                "Hanxun Huang",
                "Yisen Wang",
                "Simone Romano",
                "Sarah Erfani",
                "James Bailey"
            ],
            "title": "Normalized loss functions for deep learning with noisy labels",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Ricardo A Maronna",
                "R Douglas Martin",
                "Victor J Yohai",
                "Mat\u0131\u0301as Salibi\u00e1n-Barrera"
            ],
            "title": "Robust statistics: theory and methods (with R)",
            "year": 2019
        },
        {
            "authors": [
                "Hannes Nickisch",
                "Carl Edward Rasmussen"
            ],
            "title": "Approximations for binary gaussian process classification",
            "venue": "Journal of Machine Learning Research,",
            "year": 2008
        },
        {
            "authors": [
                "David A Nix",
                "Andreas S Weigend"
            ],
            "title": "Estimating the mean and variance of the target probability distribution",
            "venue": "In Proceedings of 1994 ieee international conference on neural networks (ICNN\u201994),",
            "year": 1994
        },
        {
            "authors": [
                "Giorgio Patrini",
                "Alessandro Rozza",
                "Aditya Krishna Menon",
                "Richard Nock",
                "Lizhen Qu"
            ],
            "title": "Making deep neural networks robust to label noise: A loss correction approach",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Vera Pawlowsky-Glahn",
                "Juan Jos\u00e9 Egozcue"
            ],
            "title": "Compositional data and their analysis: an introduction",
            "venue": "Geological Society, London, Special Publications,",
            "year": 2006
        },
        {
            "authors": [
                "Kaare Brandt Petersen",
                "Michael Syskind Pedersen"
            ],
            "title": "The matrix cookbook",
            "venue": "Technical University of Denmark,",
            "year": 2008
        },
        {
            "authors": [
                "Carl Edward Rasmussen",
                "Christopher KI Williams"
            ],
            "title": "Gaussian processes for machine learning, volume 1",
            "year": 2006
        },
        {
            "authors": [
                "Scott Reed",
                "Honglak Lee",
                "Dragomir Anguelov",
                "Christian Szegedy",
                "Dumitru Erhan",
                "Andrew Rabinovich"
            ],
            "title": "Training deep neural networks on noisy labels with bootstrapping",
            "venue": "arXiv preprint arXiv:1412.6596,",
            "year": 2014
        },
        {
            "authors": [
                "Mengye Ren",
                "Wenyuan Zeng",
                "Bin Yang",
                "Raquel Urtasun"
            ],
            "title": "Learning to reweight examples for robust deep learning",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Andrzej Rusiecki"
            ],
            "title": "Standard dropout as remedy for training deep neural networks with label noise",
            "venue": "Theory and Applications of Dependable Computer Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jun Shu",
                "Qi Xie",
                "Lixuan Yi",
                "Qian Zhao",
                "Sanping Zhou",
                "Zongben Xu",
                "Deyu Meng"
            ],
            "title": "Metaweight-net: Learning an explicit mapping for sample weighting",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Jun Shu",
                "Xiang Yuan",
                "Deyu Meng",
                "Zongben Xu"
            ],
            "title": "Cmw-net: Learning a class-aware sample weighting mapping for robust deep learning",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Hwanjun Song",
                "Minseok Kim",
                "Jae-Gil Lee"
            ],
            "title": "Selfie: Refurbishing unclean samples for robust deep learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Hwanjun Song",
                "Minseok Kim",
                "Dongmin Park",
                "Yooju Shin",
                "Jae-Gil Lee"
            ],
            "title": "Learning from noisy labels with deep neural networks: A survey",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Sainbayar Sukhbaatar",
                "Joan Bruna",
                "Manohar Paluri",
                "Lubomir Bourdev",
                "Rob Fergus"
            ],
            "title": "Training convolutional networks with noisy labels",
            "venue": "arXiv preprint arXiv:1406.2080,",
            "year": 2014
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jon Shlens",
                "Zbigniew Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Antti Tarvainen",
                "Harri Valpola"
            ],
            "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Sunil Thulasidasan",
                "Tanmoy Bhattacharya",
                "Jeff Bilmes",
                "Gopinath Chennupati",
                "Jamal MohdYusof"
            ],
            "title": "Combating label noise in deep learning using abstention",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Michail Tsagris",
                "Connie Stewart"
            ],
            "title": "A folded model for compositional data analysis",
            "venue": "Australian & New Zealand Journal of Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Ruxin Wang",
                "Tongliang Liu",
                "Dacheng Tao"
            ],
            "title": "Multiclass learning with partially corrupted labels",
            "venue": "IEEE transactions on neural networks and learning systems,",
            "year": 2017
        },
        {
            "authors": [
                "Yisen Wang",
                "Xingjun Ma",
                "Zaiyi Chen",
                "Yuan Luo",
                "Jinfeng Yi",
                "James Bailey"
            ],
            "title": "Symmetric cross entropy for robust learning with noisy labels",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Yixin Wang",
                "Alp Kucukelbir",
                "David M Blei"
            ],
            "title": "Robust probabilistic modeling with bayesian data reweighting",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Jiaheng Wei",
                "Hangyu Liu",
                "Tongliang Liu",
                "Gang Niu",
                "Yang Liu"
            ],
            "title": "Understanding generalized label smoothing when learning with noisy labels. 2021a",
            "year": 2021
        },
        {
            "authors": [
                "Jiaheng Wei",
                "Zhaowei Zhu",
                "Hao Cheng",
                "Tongliang Liu",
                "Gang Niu",
                "Yang Liu"
            ],
            "title": "Learning with noisy labels revisited: A study using real-world human annotations",
            "venue": "arXiv preprint arXiv:2110.12088,",
            "year": 2021
        },
        {
            "authors": [
                "Jiaheng Wei",
                "Hangyu Liu",
                "Tongliang Liu",
                "Gang Niu",
                "Masashi Sugiyama",
                "Yang Liu"
            ],
            "title": "To smooth or not? When label smoothing meets noisy labels",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Tong Xiao",
                "Tian Xia",
                "Yi Yang",
                "Chang Huang",
                "Xiaogang Wang"
            ],
            "title": "Learning from massive noisy labeled data for image classification",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Cheng Zhang",
                "Judith B\u00fctepage",
                "Hedvig Kjellstr\u00f6m",
                "Stephan Mandt"
            ],
            "title": "Advances in variational inference",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Chiyuan Zhang",
                "Samy Bengio",
                "Moritz Hardt",
                "Benjamin Recht",
                "Oriol Vinyals"
            ],
            "title": "Understanding deep learning requires rethinking generalization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N Dauphin",
                "David Lopez-Paz"
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "arXiv preprint arXiv:1710.09412,",
            "year": 2017
        },
        {
            "authors": [
                "Yikai Zhang",
                "Songzhu Zheng",
                "Pengxiang Wu",
                "Mayank Goswami",
                "Chao Chen"
            ],
            "title": "Learning with feature-dependent label noise: A progressive approach",
            "venue": "arXiv preprint arXiv:2103.07756,",
            "year": 2021
        },
        {
            "authors": [
                "Zhilu Zhang",
                "Mert Sabuncu"
            ],
            "title": "Generalized cross entropy loss for training deep neural networks with noisy labels",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Zizhao Zhang",
                "Han Zhang",
                "Sercan O Arik",
                "Honglak Lee",
                "Tomas Pfister"
            ],
            "title": "Distilling effective supervision from severe label noise",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Xiong Zhou",
                "Xianming Liu",
                "Junjun Jiang",
                "Xin Gao",
                "Xiangyang Ji"
            ],
            "title": "Asymmetric loss functions for learning with noisy labels",
            "venue": "In International conference on machine learning,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep neural networks (DNNs) fueled by large-scale labeled datasets such as ImageNet, have achieved remarkable performance in recent years. However, continuing progress with even larger labelled datasets faces a persistent challenge: noisy labels. Even the manually annotated ImageNet dataset contains label noise (Beyer et al., 2020), and using scalable but automatic annotation techniques will lead to higher levels of noise. This poses a problem as DNNs overfit to label noise, which results in suboptimal generalization Zhang et al. (2017a). Common regularization techniques, including weight decay, help but do not solve the problem. Therefore, designing learning algorithms that are robust to label noise is an important research endeavor.\nCurrent research study several categories of methods for reducing negative impacts of label noise (Han et al., 2020; Algan & Ulusoy, 2021; Song et al., 2022), this includes robust loss functions, sample selection, regularization, among others. In this work, we consider the two prominent directions of loss reweighting and label correction.\nLoss reweighting. The key idea here is to reduce the contribution of noisy labelled examples to the total loss. This has been shown to effectively delay overfitting to noisy labels. However, intuitively, during later stages of training when most of the correctly labeled examples have been fit, the noisy labels will gradually dominate the gradients, resulting in overfitting, but at a much slower rate.\nLabel correction. The key idea here is to correct the noisy labels by replacing the given labels with estimates of what the ground-truth label is. Clearly, with perfect correction, the problem of noisy labels is solved. Unfortunately, this is easier said than done. A common method is to estimate the true labels during training with DNNs, therefore, intuitively, too early in the learning phase the predictions of the DNN are not accurate, while too late in the training the network has overfit to the noisy labels. Therefore, there is only a short span during learning where the labels are reliable.\nWe believe these directions are complementary and combining them should be especially fruitful. The proposed high-level approach is simple: we adopt loss reweighting to delay overfitting to noisy labels, so that our label correction procedure has ample time to estimate and correct labels.\nThese directions have been pursued separately or combined as independent methods, lacking a unified approach. In this work, we present a unified method that seamlessly combines loss reweighting and label correction to enhance robustness against label noise in classification tasks. More precisely, our main contributions are:\n\u2022 We propose an adaptation of the log-ratio transform approach from compositional data analysis in statistics to the classification task (Section 3.1). This includes turning the classification dataset to a regression dataset (Section 3.2) and to transform regression predictions to classification predictions (Section 3.4).\n\u2022 With this novel view, we solve the label noise problem in classification as a regression problem. We present a unified probabilistic regression method that seamlessly combines loss reweighting and label correction (Section 3.3). We naturally achieve loss reweighting by learning the mean and covariance of per-example Gaussian distributions. To achieve label correction, we naturally extend this approach by using a shifted (non-zero mean) Gaussian noise model, which we show indirectly changes the label.\n\u2022 Finally, we perform extensive experiments, that increases our understanding of the method and shows its strong performance compared to baselines on several datasets (Section 4)."
        },
        {
            "heading": "2 COMPOSITIONAL DATA ANALYSIS WITH LOG-RATIO TRANSFORMS",
            "text": "In this section, we provide background information about the statistical field of compositional data analysis Aitchison (2011), and in particular the log-ratio transform approach.\nCompositional Data Compositional data is a collection of compositions, which are non-negative real vectors that sum to a constant, usually 1. Indeed, in this case, compositions are categorical distributions in the probability simplex \u2206D\u22121 = {[p1, p2, . . . , pD] \u2208 RD | pi \u2265 0, \u2211D i=1 pi = 1} where D denotes the number of components. Compositional data naturally arises in many fields of study (Aitchison, 2005; Tsagris & Stewart, 2020), e.g., in geology when studying [sand, silt, clay] compositions of sedimentary rock. As compositions are constrained variables, compositional data cannot be analyzed with statistical techniques designed for unconstrained variables (Pawlowsky-Glahn & Egozcue, 2006). Next, we describe a suitable method for analyzing compositional data.\nThe Log-Ratio Transform Approach Aitchison (1982) recognized that all relevant information for compositions is captured in the relations (ratios) between components, and proposed a framework for analyzing compositional data in terms of ratios. The core idea is: i) use a log-ratio transform to view compositions in the constrained simplex space as vectors in an unconstrained multivariate real space, ii) apply standard statistical techniques for unconstrained variables, iii) use the inverse transform to map results back to the simplex. Statistical modeling of compositional data typically assume that the transformed data are normally distributed, leading to a Logistic-Normal distribution Atchison & Shen (1980). Clearly, in this approach, the transform has a key role, several of which we will describe next.\nTransforms There are many transforms that can be used to transform compositional data to a real space, see e.g. (Alenazi, 2023). In this work, we focus on the most common log-ratio transforms. Here, we provide intuition for these transforms, and leave details to the work of Egozcue et al. (2011).\nThe additive log-ratio (alr) transform (Aitchison, 1982) calculates the logarithm of the ratio of D\u2212 1 components with the remaining component: alr(p) = log( [ p1, p2, . . . , pD\u22121 ] /pD). The choice of which component to divide by is arbitrary and introduces an asymmetry.\nThe centered log-ratio (clr) transform (Aitchison, 1983) solves the issue of asymmetry by dividing the components of p by its geometric mean g(p): clr(p) = log( [ p1, p2, . . . , pD ] /g(p)). The transformed clr data sum to zero and are therefore constrained to a D \u2212 1 dimensional hyper-plane of RD. The isometric log-ratio (ilr) transform (Egozcue et al., 2003) reparametrizes the clr transformed data in an orthonormal bases of the D \u2212 1 dimensional hyper-plane: ilr(p) = V T clr(p), where V is a D \u00d7 D \u2212 1 dimensional matrix providing the change of basis. Although the ilr transform is symmetric and its image is the full RD\u22121, it shares the problem of zero components with other logratio transforms, as the logarithm of zero is undefined. In this work, we use the simple label smoothing technique (Szegedy et al., 2016) to solve this issue, but several other solutions exist (Alenazi, 2023)."
        },
        {
            "heading": "3 METHOD",
            "text": "In this section, we first provide an overview of our idea to adapt the three-step log-ratio transform approach for classification (Section 3.1). The following sections go over these three steps in detail: i) transforming the classification dataset to a regression dataset (Section 3.2), ii) our regression learning algorithm with loss reweighting and label correction (Section 3.3), iii) transforming regression predictions to classification predictions (Section 3.4)."
        },
        {
            "heading": "3.1 OVERVIEW: THE LOG-RATIO TRANSFORM APPROACH FOR CLASSIFICATION TASKS",
            "text": "To achieve a unified approach with loss reweighting and label correction for classification tasks, we propose the following three-step process based on the log-ratio transform approach (Section 2):\nStep 1. We turn the classification dataset to a compositional dataset via label smoothing. Then, the compositional dataset is transformed to a regression dataset via the ilr transform.\nDclass\u03b7 = {(xi, yi)}Ni=1 \u2192 Dcomp\u03b7 = {(xi, LS(yi))}Ni=1 \u2192 Dreg\u03b7 = {(xi, ilr(LS(yi)))}Ni=1 (1)\nStep 2. Given the regression dataset Dreg\u03b7 , we use our robust regression algorithm that achieves loss reweighting and label correction via a shifted (\u2206) Gaussian noise model.\nilr(LS(yi)) = t(xi) = \u00b5(xi) + \u03f5(xi), \u03f5(xi) \u223c N (\u2206(xi),\u03a3(xi)) (2)\nStep 3. We transform any regression prediction \u00b5\u0302 \u2208 RK\u22121, to the probability simplex with the inverse ilr transform, and use the most likely class as the class prediction\n\u00b5\u0302 \u2192 \u03c0\u0302 = ilr\u22121(\u00b5\u0302) \u2192 y\u0302 = argmax k \u03c0\u0302k (3)\nAn overview of our approach is shown in Figure 1. Next, we go over each of these steps in detail."
        },
        {
            "heading": "3.2 STEP 1: TRANSFORMING CLASSIFICATION DATASETS TO REGRESSION DATASETS",
            "text": "The log-ratio transform approach transforms compositional datasets to regression datasets. Hence, we can get regression datasets by converting classification datasets to compositional datasets. Next, we discuss using label smoothing for this purpose and provide details about the ilr transform we use.\nClass ID to Interior Simplex It is common to view the class ID labels in classification as part of the simplex via the one-hot encoding, effectively turning the classification dataset into a compositional dataset. Unfortunately, this is not enough for our purposes, as the log-ratio transforms cannot deal with zero components. To ensure that all components are non-zero (interior of simplex), we simply use the well-known label smoothing (Szegedy et al., 2016) technique, which is defined as\nLS(y) = (1\u2212 \u03b3)\u03b4y + \u03b3u (4)\nwhere \u03b4y \u2208 \u2206K\u22121 is a one-hot encoding of y (component y is 1 while the rest are zero), and u is the uniform distribution over K classes (all components equal to 1/K), and \u03b3 is a small positive number. We motivate the use of label smoothing as it is simple to understand, implement, and has been shown to help against label noise for the standard cross-entropy loss Lukasik et al. (2020); Wei et al. (2022).\nThe Isometric Log-Ratio Transform In Section 2, we described the additive, centered, and the isometric log-ratio transforms. In this work, we use the ilr transform (Egozcue et al., 2003), as it does not have issues the other transforms have: asymmetry (alr) and the image being on a lower dimensional plane (clr). To use the ilr transform, one has to choose an orthonormal basis and its corresponding matrix V . Here, we use a basic approach of having V t be a K \u00d7 K dimensional Helmert matrix in the strict sense (Lancaster, 1965) without its first row. The columns of this K \u00d7K \u2212 1 matrix V correspond to the orthogonal basis vectors of the clr plane."
        },
        {
            "heading": "3.3 STEP 2: REGRESSION ANALYSIS WITH LOSS REWEIGHTING AND LABEL CORRECTION",
            "text": "With label smoothing and the ilr transform, we can transform the classification task into a regression task, allowing us to use classical regression techniques. Here, we first describe a statistical regression technique that naturally achieves loss attenuation and then uncover its potential to do label correction as well. This is then followed by details on how to achieve this label correction in practice.\nLoss Reweighting via a Gaussian Noise Model Nix & Weigend (1994) proposed to model the observed regression target t as having a true mean \u00b5 with some added noise \u03f5: t(x) = \u00b5(x) + \u03f5(x), where \u03f5 is normally distributed with zero mean and input-dependent variance \u03c32(x). This results in a Gaussian likelihood p(t|x) = N (\u00b5(x), \u03c32(x)) per example. The authors propose to have neural network with two output heads with parameters \u03b8 to output \u00b5\u0302 = \u00b5\u03b8 and \u03c3\u03022 = \u03c32\u03b8 per example, and the parameters of the network are found through maximum likelihood estimation. That is\nargmin \u03b8 \u2212 N\u2211 i=1 logN (ti;\u00b5\u03b8(xi), \u03c32\u03b8(xi)) = argmin \u03b8 N\u2211 i=1 (ti \u2212 \u00b5\u03b8(xi))2 2\u03c32\u03b8(xi) + 1 2 log \u03c32\u03b8(xi) (5)\nKendall & Gal (2017) recognized that this objective, when learning \u03c32\u03b8(xi) does loss reweighting (they called it attenuation). Clearly, if the variance \u03c32\u03b8(xi) is high for noisy examples, their influence on total loss is reduced. Here, the last term of the objective in Equation 5 naturally arises from the probabilistic noise model and importantly keeps the network from outputting the trivial solution of high variance (low weight) for all examples. In this work, we use this approach but in a multivariate setting using deep neural networks to estimate the mean \u00b5\u03b8 and covariance \u03a3\u03b8 per-example. To efficiently and effectively parameterize covariance matrices, we follow the works of Collier et al. (2021); Englesson et al. (2023), see Appendix A for details.\nUncovering Label Correction via a Shifted Gaussian Noise Model The Gaussian noise model above achieves loss reweighting, but not yet label correction. Unfortunately, loss reweighting is not enough for robustness as the ML estimate for \u00b5\u03b8 is \u00b5\u0302ML(xi) = ti for all i = 1, . . . , N , i.e. it would fit all (including noisy) labels. Here, we propose the following shifted noise model: t = \u00b5+ \u03f5, \u03f5 \u223c N (\u2206, \u03c32), that clearly changes the label in the training objective\n\u2212 logN (t;\u00b5\u03b8 +\u2206, \u03c32\u03b8) = (t\u2212 (\u00b5\u03b8 +\u2206))2\n2\u03c32\u03b8 +\n1 2 log \u03c32\u03b8\n= ((t\u2212\u2206)\u2212 \u00b5\u03b8)2\n2\u03c32\u03b8 +\n1 2 log \u03c32\u03b8 (6)\n= \u2212 logN (t\u2212\u2206;\u00b5\u03b8, \u03c32\u03b8) Consider an example with a noisy target t where the correct label is \u00b5. We can correct the label by having t\u2212\u2206 = \u00b5, or, equivalently, \u2206 = t\u2212 \u00b5. Therefore, including a shift \u2206 in the noise model indirectly incorporates label correction. Although intuitively simple, how to estimate the shift is far from trivial, for which a practical algorithm is described next.\nA Practical Algorithm to Estimate the Shift To do label correction with \u2206, we want \u2206 = t\u2212 \u00b5. As the observed target t is known, it is enough to estimate the ground-truth label \u00b5 and then calculate the difference. Clearly, this is a challenging task, since if we were able to accurately estimate \u00b5, the problem of noisy labels would be solved. To achieve good estimates of ground-truth labels, we rely on the predictions of a network with moving averaged weights of the original network from previous training steps. Despite being a simple approach, these networks have been shown to generalize better (Izmailov et al., 2018), and have been successfully used in semi-supervised (Tarvainen & Valpola, 2017) and self-supervised (Grill et al., 2020) learning methods.\nIn particular, for each training example (xi, ti), we estimate \u2206\u03b8\u0304(xi) = ti \u2212 \u00b5\u03b8\u0304(xi), where \u00b5\u03b8\u0304 is the prediction of the network with exponentially moving averaged (EMA) weights \u03b8\u0304 of the original network parameters \u03b8. Substituting \u2206 with \u2206\u03b8\u0304 in Equation 6, we get\n\u2212 logN (t;\u00b5\u03b8 +\u2206\u03b8\u0304, \u03c32\u03b8) = \u2212 logN (\u00b5\u03b8\u0304;\u00b5\u03b8, \u03c32\u03b8) (7)\nThat is, the introduction of the shift \u2206\u03b8\u0304 effectively changes the label from the observed t to the estimated ground-truth \u00b5\u03b8\u0304. This is a desired capability of any label correction method, but it also introduces a problem early in training, as the EMA network\u2019s predictions are poor estimates of the ground-truth label at this point. To remedy this, we could start introducing \u2206\u03b8\u0304 after a few warm-up epochs, as proposed by Grill et al. (2020). However, instead of abruptly changing the label in this way, we use a smooth transition using the following weighting scheme: t = \u00b5+ (1\u2212\u03b1e)\u2206, where \u03b1 is a hyperparameter close to 1 and e is the current training epoch. In this way, the weight (1\u2212 \u03b1e) is close to zero early in training and gradually goes to 1, causing the labels to transition from the observed to the estimated ground-truth labels during the training process."
        },
        {
            "heading": "3.4 STEP 3: TRANSFORMING REGRESSION PREDICTIONS TO CLASS IDS",
            "text": "In this section, we complete the method by considering the last step: converting these predictions back to classification predictions.\nNoise-Free Predictions The networks output the estimated true mean \u00b5\u0302 and the estimated noise variance \u03c3\u03022. For predictions on the training set, this noise variance is important to achieve loss reweighting, but for unseen data our goal is to predict the noise-free true mean. Therefore, at test time, we do not use the predicted \u03c3\u03022 and consider the estimated true mean \u00b5\u0302 as the prediction of the network. As the ilr transform is one-to-one, we utilize its inverse to map \u00b5\u0302 to a categorical distribution in the simplex: \u03c0\u0302 = ilr\u22121(\u00b5\u0302). To obtain a class ID prediction, the standard approach of using the most likely component is used: y\u0302 = argmaxk \u03c0\u0302k.\nEMA Predictions A strong advantage of estimating the ground-truth target with an EMA network compared to, e.g., with the same network (Reed et al., 2014) or with a separate buffer (Liu et al., 2020), is that we have the option to use the predictions of this network at no extra cost at test time. As the EMA network has been shown to generalize better (Izmailov et al., 2018), this is a natural choice. We perform an ablation study to quantify the improvements this leads to in Section 4.4."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "First, we describe our experimental setup (Section 4.1), followed by results on synthetically (Section 4.2) and naturally (Section 4.3) noisy datasets. Finally, we perform experiments to better understand our method (Section 4.4). For details about the CIFAR, CIFAR-N, Clothing1M, and WebVision datasets, see Appendix A.4. We refer to our method as Shifted Gaussian Noise (SGN)."
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUP",
            "text": "For the experiments on the CIFAR (including CIFAR-N) datasets, we implement all baselines in the same shared code base to have an as conclusive comparison as possible. To achieve the best possible performance in this setup, we do a search for method-specific hyperparameters for each method based on a noisy validation set. For more information about the search and the optimal hyperparameters for each method, see Appendix A.3. For full details of the training setup on the CIFAR, CIFAR-N, Clothing1M, and WebVision datasets, see Appendix A.2.\nThe results on all datasets report the mean and standard deviation of validation/test accuracy calculated based on five different network training runs with different seeds, evaluated at the end of training, if not stated otherwise. The random seeds affect both network initialization, synthetic noise generation, data augmentation, and the order of the data loaders."
        },
        {
            "heading": "4.2 SYNTHETIC LABEL NOISE EXPERIMENTS",
            "text": "To evaluate the robustness of our approach in a setup where we have full control over the amount of label noise, we perform experiments on the CIFAR datasets with synthetic noise manually added to\nC IF\nA R\n-1 0 CE 90.67 \u00b1 0.80 73.54 \u00b1 1.01 56.56 \u00b1 1.44 39.44 \u00b1 1.87 81.35 \u00b1 1.26 76.01 \u00b1 2.67 71.89 \u00b1 1.67 GCE 90.83 \u00b1 0.44 87.55 \u00b1 0.41 84.72 \u00b1 0.82 79.25 \u00b1 0.93 85.68 \u00b1 0.69 83.97 \u00b1 0.52 72.90 \u00b1 1.61 LS 89.78 \u00b1 0.39 79.09 \u00b1 0.96 64.27 \u00b1 1.50 43.57 \u00b1 3.13 81.99 \u00b1 1.22 76.49 \u00b1 1.17 71.66 \u00b1 1.78 HET 90.82 \u00b1 0.42 77.16 \u00b1 0.94 62.85 \u00b1 1.88 44.20 \u00b1 3.05 81.55 \u00b1 0.80 77.05 \u00b1 0.33 72.69 \u00b1 0.89 NAN 89.61 \u00b1 0.93 83.86 \u00b1 1.03 79.80 \u00b1 0.59 73.58 \u00b1 0.41 84.32 \u00b1 1.05 76.79 \u00b1 2.28 72.90 \u00b1 1.92 LN 90.17 \u00b1 0.55 86.13 \u00b1 1.03 81.37 \u00b1 1.97 76.08 \u00b1 0.63 87.64 \u00b1 0.78 86.91 \u00b1 1.03 82.18 \u00b1 1.30 ELR 91.78 \u00b1 0.26 90.15 \u00b1 0.54 88.19 \u00b1 0.68 81.87 \u00b1 2.42 90.59 \u00b1 0.36 89.72 \u00b1 0.22 87.37 \u00b1 0.55 SOP 91.57 \u00b1 0.38 89.86 \u00b1 0.45 88.45 \u00b1 0.51 85.56 \u00b1 0.93 89.84 \u00b1 0.55 87.60 \u00b1 0.65 83.90 \u00b1 1.04 NAL 92.80 \u00b1 0.23 89.79 \u00b1 0.47 86.25 \u00b1 0.28 78.82 \u00b1 0.43 90.80 \u00b1 0.76 89.51 \u00b1 0.76 84.36 \u00b1 1.19 SGN (Ours) 94.12 \u00b1 0.22 93.02 \u00b1 0.17 91.29 \u00b1 0.25 86.03 \u00b1 1.19 93.35 \u00b1 0.21 92.71 \u00b1 0.11 91.26 \u00b1 0.27\nC IF\nA R\n-1 00 CE 64.87 \u00b1 0.88 47.39 \u00b1 0.43 33.62 \u00b1 0.79 20.04 \u00b1 0.58 50.98 \u00b1 0.88 44.04 \u00b1 0.73 36.95 \u00b1 0.58 GCE 64.33 \u00b1 0.83 61.67 \u00b1 0.67 53.96 \u00b1 1.40 42.85 \u00b1 0.79 59.63 \u00b1 1.28 49.21 \u00b1 0.53 36.78 \u00b1 0.50 LS 65.39 \u00b1 0.40 57.08 \u00b1 0.70 44.03 \u00b1 1.20 26.13 \u00b1 1.45 55.47 \u00b1 0.76 44.70 \u00b1 0.73 38.56 \u00b1 0.66 HET 65.18 \u00b1 0.90 54.83 \u00b1 0.46 41.49 \u00b1 1.53 22.42 \u00b1 0.95 61.29 \u00b1 0.46 56.44 \u00b1 0.53 45.75 \u00b1 1.02 NAN 64.25 \u00b1 0.64 56.93 \u00b1 0.77 50.03 \u00b1 0.62 40.45 \u00b1 0.41 56.40 \u00b1 1.07 52.78 \u00b1 0.85 40.59 \u00b1 0.84 LN 64.88 \u00b1 0.98 60.58 \u00b1 1.07 55.55 \u00b1 1.30 46.43 \u00b1 1.15 64.31 \u00b1 0.98 64.07 \u00b1 0.77 61.20 \u00b1 1.22 ELR 67.74 \u00b1 0.61 64.70 \u00b1 0.85 59.92 \u00b1 0.95 48.85 \u00b1 0.85 66.32 \u00b1 0.88 65.99 \u00b1 1.16 63.80 \u00b1 0.35 SOP 62.50 \u00b1 0.76 61.40 \u00b1 1.18 60.92 \u00b1 1.34 50.80 \u00b1 0.74 54.19 \u00b1 0.48 47.22 \u00b1 1.27 39.20 \u00b1 0.60 NAL 69.59 \u00b1 0.37 64.27 \u00b1 0.18 57.09 \u00b1 0.51 46.23 \u00b1 0.45 66.59 \u00b1 0.48 64.46 \u00b1 0.62 58.01 \u00b1 0.79 SGN (Ours) 73.88 \u00b1 0.34 71.79 \u00b1 0.26 66.86 \u00b1 0.35 56.83 \u00b1 0.57 72.83 \u00b1 0.31 72.16 \u00b1 0.86 71.01 \u00b1 0.71\na given percentage of training examples. Here, we describe our process of adding label noise, the baselines we compare with, and finally our experimental results.\nLabel Noise Process We consider two types of synthetic noise: symmetric and asymmetric. Symmetric noise uniformly samples a new label from any class. Asymmetric noise circularly cycle labels to the next class on CIFAR-100, and as follows on CIFAR-10: bird \u2192 airplane, cat \u2194 dog, deer \u2192 horse, and truck \u2192 automobile. For more details on these noise types, see Appendix A.11.\nBaselines We compare our novel method with an extensive set of baselines, from the standard crossentropy loss, to strong representatives from the categories of regularization, robust loss functions, loss reweighting and label correction methods. We compare with the robust GCE (Zhang & Sabuncu,\n2018) loss and ELR (Liu et al., 2020) that regularizes the prediction to be closer to the estimated ground-truth label. As label smoothing (LS) increases robustness to label noise (Lukasik et al., 2020), and our method uses it to get to a compositional dataset, it is a natural baseline. Furthermore, as our method uses a Gaussian label noise model, we compare with NAN (Chen et al., 2020) and HET (Collier et al., 2021) that adds Gaussian noise to the one-hot encoded labels and pre-softmax logits, respectively. Finally, we also compare with methods that have some form of loss reweighting and or label correction, including SOP (Liu et al., 2022), NAL (Lu et al., 2022) and LN (Englesson et al., 2023). Detailed connections to the most related works are discussed in Appendix A.6.\nResults In Table 1, we show the results for various levels of symmetric and asymmetric noise on CIFAR-10 and CIFAR-100. Our method (SGN) performs significantly better than other methods on both datasets for all noise rates (expect 60% symmetric noise) and noise types. Interestingly, SGN significantly outperforms CE when there is no added label noise. We attribute this to better generalization of the EMA network, and to inherent label noise in the datasets, as other label estimating method often also perform better. Remarkably, comparing SGN on no noise with 40% asymmetric noise, the generalization degradation is less than 3% on both datasets, which we attribute to an effective label correction, see Section 4.4. The most challenging setup for all methods seem to be high levels of symmetric noise. In our case, we believe this is mainly due to our method predicting the covariance matrices of the Gaussian distributions. As there is no structure in the label noise, the network has to memorize the noise to do an effective loss reweighting (Englesson et al., 2023). Despite this, SGN with 40% noise remarkably outperforms CE when trained with 0% added noise."
        },
        {
            "heading": "4.3 NATURAL LABEL NOISE EXPERIMENTS",
            "text": "Synthetic label noise is excellent for conducting experiments with controlled levels of label noise. However, this comes at the cost of the structure of the noise potentially being different compared to label noise naturally arising from standard annotation processes. In this section, we turn our attention to this type of noise by conducting experiments on datasets that have been annotated by humans, or automatically with search engines.\nResults We report the results on CIFAR-10N (Aggregate, Random 1-3, and Worst label sets) and CIFAR-100N datasets in Table 2. Our method shows strong robustness in all six cases. We find that our method achieves the largest improvements compared to baselines on the most challenging setups, i.e., \u201cWorst\u201d (40% noise) on CIFAR-10N and CIFAR-100N. For example, on CIFAR-100N, our method achieves a four percentage point improvement compared to the best baseline (NAL). However, ELR, SOP and NAL baselines are all showing strong performance on these datasets.\nIn Table 3 we report strong results on Clothing1M and WebVision. All baseline results (except NAL) are gathered from the work of Liu et al. (2022). These results correspond to accuracies of a single network evaluated when the best validation accuracy is achieved. NAL results are from Lu et al. (2022), but has a different evaluation setup on WebVision.1 We provide results following our standard\n1The official code evaluates all accuracies in each epoch and tracks the best accuracies separately in training.\nevaluation setup, i.e.., mean and standard deviation at the end of training. We believe the use of early stopping, without error bars, makes the results noisy and comparisons inconclusive. However, as early stopping improves performance, we also report results following the first setup of Liu et al. (2022). We believe this highlights the importance of having common training and evaluation setups."
        },
        {
            "heading": "4.4 EMPIRICAL ANALYSIS AND DISCUSSIONS",
            "text": "Here, we dissect the impact of different parts of the method, and its label correction behavior.\nAblation Study To better understand our method, we perform an ablation study where we systematically deactivate key components of our method: loss reweighting (LR), label correction (LC), and the use of EMA network predictions (EP). As the covariance in the Gaussian likelihood causes the loss reweighting effect, we can deactivate it by using an identity matrix. As the shift \u2206 is responsible for the label correction, we can deactivate it by setting \u03b1 = 1. We can deactivate the EMA predictions by instead using the predictions of the main network. We report results for different settings on the CIFAR and CIFAR-N datasets in Table 4. Without EMA network predictions, and we see that activating LR and LC each provides clear improvements on all datasets and noise types. For example, the activation of LR and then LC improve performance by 20 and 4 pp on CIFAR-100 with asymmetric noise, respectively. Similar improvements are observed when EMA predictions are used. We find that in every case, EMA network predictions provide significant generalization improvements. This study provides strong evidence that the components of our method naturally work well together to provide significant and separate generalization improvements.\nLabel Correction Analysis To better understand the label correction part of our method, we compare the predictions of the EMA network with the clean (original) labels. In Figures 2a and 2b, we show the clean training accuracy of the EMA network predictions during training with synthetic label noise on the CIFAR-100 dataset. Our method struggles to correct labels on high levels of symmetric noise, but does remarkably well on asymmetric noise. We attribute this to the network having to\nmemorize the noise to effectively do loss reweighting on the structureless symmetric noise Englesson et al. (2023). Despite this, Table 1 shows that our method has the strongest robustness to symmetric label noise. We provide similar figures for CIFAR-10 in Section A.5, showing better LC performance. Finally, the 40% curve in Figure 2b shows that it takes time to converge. This, and the flatness of other curves, likely explains why training for longer improves the performance.\nTo better understand the impact of the gradual introduction of \u2206 via the weighting (1\u2212 \u03b1e)\u2206 in our noise model, we plot the final clean training accuracy of the EMA network predictions when trained with different values of \u03b1, see Figure 2c. We find that varying \u03b1 has a considerable effect on the final accuracy, where the optimal \u03b1 seems to be close, but not too close to 1. Intuitively, a too low \u03b1 could lead to using the EMA network predictions too early in training when they are not yet accurate. Conversely, a too large \u03b1 could lead to relying on the noisy labels for too long."
        },
        {
            "heading": "5 RELATED WORK",
            "text": "Robustness to label noise is an active area of research, with several important categories of methods. Unfortunately, we cannot cover all of them here, but luckily there are excellent surveys available (Han et al., 2020; Algan & Ulusoy, 2021; Song et al., 2022). Here, we focus on the following categories: regularization, robust loss functions, loss correction, loss reweighting, and label correction.\nRegularization A natural idea to try against label noise is standard regression techniques, several of which have been shown to help: label smoothing (Szegedy et al., 2016; Lukasik et al., 2020; Wei et al., 2021a), data augmentation (Zhang et al., 2017b), dropout (Rusiecki, 2020; Goel & Chen, 2021), and early stopping (Li et al., 2020b; Bai et al., 2021), or adding noise to labels (Chen et al., 2020).\nRobust Loss Functions These methods propose replacing the standard CE loss with a robust alternative, often with theoretical guarantees on robustness based on simplifying assumptions, e.g., class-dependent noise. In their seminal work Ghosh et al. (2017) proposed such a theoretical framework that has led to several new loss function (Zhang & Sabuncu, 2018; Wang et al., 2019; Ma et al., 2020; Englesson & Azizpour, 2021), and extension of the theory (Zhou et al., 2021).\nLoss Correction Another natural idea is to model the label noise mathematically, by relating the true and noisy label distribution via a transition matrix (Sukhbaatar et al., 2014; Patrini et al., 2017). As for robust loss functions, these methods also have theoretical robustness guarantees.\nLoss Reweighting An intuitive idea is to down weight the influence noisy labelled examples has on the training loss, leading to less overfitting. How to estimate such weights has been studied extensively (Wang et al., 2017b;a; Jiang et al., 2018; Ren et al., 2018; Thulasidasan et al., 2019; Lu et al., 2022). To avoid down weighting all examples, an extra loss term is often designed. Interestingly, modelling label noise in the pre-softmax logit space neatly solves this problem (Kendall & Gal, 2017; Collier et al., 2021). In the same direction, Englesson et al. (2023) proposes to maximize a Logistic-Normal likelihood, showing it exhibits loss reweighting properties. In fact, this loss reweighting method corresponds to a special case of our adapted log-ratio transform approach.\nLabel Correction Yet another intuitive idea is to directly fix the cause of the issues: the noisy labels. In fact, a common issue with most of the above methods is that the global optima of the training loss is still to match all (including noisy) labels. Estimating ground-truth labels is an ambitious goal, but often hard to do in practice. Some methods do so with the same network during training (Reed et al., 2014; Song et al., 2019; Zhang et al., 2021), via running averages of previous predictions (Laine & Aila, 2017; Liu et al., 2020; Lu et al., 2022), learned via buffers (Liu et al., 2022), or via meta-learning (Shu et al., 2019; Zhang et al., 2020; Shu et al., 2023)."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "Our goal was to achieve a unified approach to loss reweighting and label correction for robustness against label noise in classification. We proposed to adapt the log-ratio transform approach from compositional data analysis in statics to the classification task, allowing us to use regression techniques to solve the problem. Therefore, we successfully achieved our goal by also proposing a regression method based on a shifted Gaussian label noise model that naturally achieved loss reweighting and label correction. We performed extensive experiments, which increased our understanding of our novel method and showed its strong robustness against label noise. We believe this work provides a promising direction towards robust deep learning in the presence of label noise."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 ESTIMATING THE MEAN AND COVARIANCE",
            "text": "A practical way to estimate the covariance matrix of a Gaussian distribution has been proposed by Collier et al. (2021); Englesson et al. (2023). We follow this approach and have a two-headed deep neural network predict \u00b5\u03b8 and also an efficient low-rank approximation of a covariance matrix, where the covariance is represented as follows\n\u03a3\u03b8(x) = S\u03b8(x)S\u03b8(x) T (8)\nS\u03b8(x) = r\u03b8(x)r\u03b8(x) T + I (9)\nwhere r\u03b8 \u2208 RK\u22121 is outputted by the network. A benefit of this particular parametrization is that evaluation of can be evaluated efficiently. To see, this we first rewrite the negative log-likelihood of a multivariate distribution in terms of the scale matrix S\u03b8(x) instead of \u03a3\u03b8(x)\n\u2212 logN (t(x);\u00b5\u03b8(x),\u03a3\u03b8(x) = 1 2 (t(x)\u2212 \u00b5\u03b8(x))T\u03a3\u22121\u03b8 (x)(t(x)\u2212 \u00b5\u03b8(x)) + 1 2 log |\u03a3\u03b8(x)|+ const\n(10)\n= 1\n2 (\u2225S\u22121\u03b8 (x)(t(x)\u2212 \u00b5\u03b8(x))\u2225 2 2) + log |S\u03b8(x)|+ const (11)\nTo evaluate this loss, we need to calculate the inverse and determinant of S. In general, this is computationally challenging, but with this particular parameterization we can efficiently calculate the inverse and determinant using the Woodbury matrix identity, and the matrix determinant lemma (see Equations 156 and 24 in (Petersen et al., 2008)), respectively.\nS\u22121\u03b8 (x) = (r\u03b8(x)r\u03b8(x) T + I)\u22121 = I \u2212 r\u03b8(x)r\u03b8(x)\nT\n1 + r\u03b8(x)Tr\u03b8(x) (12)\n|S\u22121\u03b8 (x)| = |r\u03b8(x)r\u03b8(x) T + I| = 1 + r\u03b8(x)Tr\u03b8(x) (13)\nThus, the inverse and determinant computations reduce to simple matrix additions and inner and outer products of vectors, which can be computed efficiently. In practice, we conveniently make use of these loss evaluation optimizations by calling the \u201clog prob\u201d method of the multivariate normal distribution (MultivariateNormalDiagPlusLowRank) in TensorFlow Probability (Dillon et al., 2017)."
        },
        {
            "heading": "A.2 TRAINING DETAILS",
            "text": "CIFAR and CIFAR-N All methods use the same WideResNet (WRN-28-2) architecture, with a constant learning rate (0.01), SGD with momentum (0.9) and weight decay (5e-4), batch size of 128, and standard data augmentation (crop and flip). We used 300 training epochs, but found that the baselines that estimate shifts/labels, ELR (Liu et al., 2020), SOP (Liu et al., 2022), NAL (Lu et al., 2022), and ours, benefited from more training epochs and were trained for twice as long. Training for longer might be an advantage with noise-free training, but with label noise it is typically more challenging, as there is more time to overfit.\nClothing1M and WebVision Due to Clothing1M and WebVision being much larger-scale datasets, we resort to only training our method. For WebVision, we train on the commonly used subset called mini WebVision (Jiang et al., 2018). We use the InceptionResNetV2 architecture with the same training setup as for CIFAR, except we use a learning rate, weight decay, and batch size of 0.02, 2e-3, 64, respectively. \u03b1 of 0.999. Also, we found that a temperature scaling of 0.3 and a linear warm-up of the learning rate (first epoch) improved the results. On Clothing1M, we use the standard approach of\nusing a ResNet-50 network initialized with weights from pre-training on ImageNet. We use a learning rate, weight decay, and batch size of 0.001, 0.01, and 32, respectively. \u03b1 of 0.7. Trained for 10 epochs, where the learning rate was scaled at epoch 5 by a factor of 0.1. Standard data augmentation (crop and flip). In each epoch, a 265664 class balanced subset of the dataset is sampled, as in the work of Li et al. (2020a).\nThe EMA update rate is set to 0.9999 in all experiments."
        },
        {
            "heading": "A.3 HYPERPARAMETERS",
            "text": "We choose the range of values to search over is based on the original papers. For HET, we search for temperatures in [0.1, 0.5, 1.0, 10.0, 20.0], and over the number of factors R of the covariance matrix in [1, 2, 4]. LN searches over temperatures and \u03bbs in [0.5, 1.0]. Our method searches over the \u2206 weighting parameter \u03b1 in [0.991, 0.993, 0.995, 0.997, 0.999]. We treat the label smoothing parameter \u03b3 for LN as fixed, and set it to 0.001 in all experiments on CIFAR, and 0.1 and 0.01 on Clothing1M and WebVision, respectively. For GCE, we search over q in [0.1, 0.3, 0.5, 0.7, 0.9]. For NAN, the search is over \u03c3 in [0.1, 0.2, 0.5, 0.75, 1.0]. For LS, we search for values in [0.1, 0.3, 0.5, 0.7, 0.9]. For ELR, we search over \u03bb in [1, 3, 7] and \u03b2 in [0.7, 0.9, 0.99]. For SOP, we search over learning rates for u in [0.1, 1, 10] and v in [1, 10, 100]. For NAL, we search for \u03bb in [0.5, 10, 50] and \u03b2 in [0.97, 0.98, 0.99].\nAll searches are done with a single seed, and the hyperparameters with the highest noisy validation accuracy at the end of training are used to train four more networks with different seeds. The hyperparameters used for different noise rates, noise types, and datasets are shown in Tables 5, and 6."
        },
        {
            "heading": "A.4 DATASETS",
            "text": "We conduct experiments on the CIFAR-N (Wei et al., 2021b), Clothing1M (Xiao et al., 2015), and (mini) WebVision Li et al. (2017) datasets. The CIFAR-N datasets provide new sets of human labels for the training sets of CIFAR-10 and CIFAR-100, respectively. On the CIFAR-N datasets, we follow the same thorough training setup as for the CIFAR datasets. Clothing1M is a dataset crawled from the web and has a training set of one million images of 14 different types of clothes. The training set\nhas noisy labels due to the automatic assigning of labels based on text information accompanying each image. WebVision v1 is another large-scale dataset with over two million training examples collected by using search engines to query images related to the thousand classes of the ILSVRC 2012 dataset. To speed up experiments, a smaller subset was proposed called mini WebVision (Jiang et al., 2018) that only uses the first 50 classes (of Google subset). Due to the time necessary to conduct experiments on Clothing1M and (mini) WebVision, we forgo our thorough training setup on the CIFAR related datasets, and only train our method. Here, we also compare with two new baselines, Forward (Patrini et al., 2017), and Co-teaching (Han et al., 2018)."
        },
        {
            "heading": "A.5 ADDITIONAL LABEL CORRECTION EXPERIMENTS",
            "text": "In Figure 3, similarly to Figure 2, we plot the clean training accuracy of the EMA network predictions during training when training on various amounts and types of noise on CIFAR-10. We observe a significant improvement in the label correction ability, especially on symmetric noise, compared to the challenging CIFAR-100 dataset."
        },
        {
            "heading": "A.6 CONNECTIONS TO RELATED WORKS",
            "text": "In this section, we use the notation that p(y|x) and p\u0303(y|x) corresponds to the true and noisy label distributions from the underlying generation process, respectively.\nELR (Liu et al., 2020) relies on networks to have not overfitted to noisy labels early in training. They propose to estimate ground-truth labels by averaging per-example predictions of the network during\ntraining, which are stored in separate data buffers. In addition to the cross-entropy loss, a term that encourages consistency between the predictions of the network and the estimated labels is used. Due to the memory usage, we believe using buffers to estimate labels do not scale to large datasets and or large number of classes, while our approach of using EMA networks do. Furthermore, the buffers are useless at test time, whereas EMA networks can generalize significantly better than the original network.\nAdditive Noise Models SOP (Liu et al., 2022) modifies the categorical network prediction by adding a shift followed by a normalization to get the new prediction for a cross-entropy loss. We interpret this approach as the following additive noise model: p\u0303(y|x) = N(p(y|x) + s), where N is a normalization function and s is their unconstrained shift stored in per-example buffers. Intriguingly, this resembles our approach of predicting a shift \u2206, and can therefore do label correction, but because their addition of the unconstrained shift is done in the simplex, it requires an ad-hoc re-normalization. Furthermore, to effectively learn the shift, gradients of another loss function had to be used, which our method avoids by predicting \u2206. As the shift is stored in buffers, it has the same buffer-related issues as ELR.\nLN (Englesson et al., 2023) proposes the following noise model: p\u0303(y|x) = SC(\u00b5+ \u03f5), \u03f5 \u223c N (0,\u03a3), where p(y|x) = SC(\u00b5), and SC(\u00b7) is the softmax centered function. They show how the noise model leads to a Logistic-Normal likelihood (Atchison & Shen, 1980), that has loss reweighting properties. Interestingly, this is a special case of our adapted log-ratio transform approach, corresponding to using the asymmetric alr transform and without modelling \u2206, i.e., no label correction.\nNAL (Lu et al., 2022) interpolates the one-hot encoded label with the categorical network prediction and use this as the prediction in a cross-entropy loss, where the interpolation parameter is also outputted by the network. Clearly, the influence of noisy examples can be reduced by using an interpolation weight that results in the prediction being close to the label. We interpret this as the following additive noise model: p\u0303(y|x) = (1\u2212 \u03b3)p(y|x) + \u03b3\u03b4y = p(y|x) + \u03b3(\u03b4y \u2212 p(y|x)), where \u03b4y \u2212 p(y|x) resembles our \u2206, but in the simplex. Notably, unlike SOP, no normalization is needed as the interpolation stays in the simplex. Additionally, they combine their method with an independent label correction approach. Ground-truth labels are estimated similarly to ELR, and therefore this method has the same buffer-related limitations.\nCMW-Net (Shu et al., 2023) is a loss reweighting method that uses meta-learning to learn the weights, similar to the work of Shu et al. (2019). The main difference is CMW-Net also incorporates extra (task/class) information, leading to more flexibility in learning weighting schemes. Interestingly, Shu et al. (2023) also enhance CMW-Net with label correction (denoted CMW-Net-SL). Instead of learning a loss weight, CMW-Net-SL learns an interpolation parameter between the original one-hot label and a soft label estimate (Equation 14 can be rewritten to the form in Equation 13, in the same way as Equation 12 was), and the new interpolated label is then used in a standard CE loss (no loss reweighting). Similar to NAL, we interpret this as the following additive noise model: p\u0303(y|x) = (1 \u2212 v)p(y|x) + v\u03b4y = p(y|x) + v(\u03b4y \u2212 p(y|x)), where \u03b4y \u2212 p(y|x) resembles our \u2206. Therefore, although the proposed approaches are interesting and novel, we argue neither CMW-Net (loss reweighting) nor CMW-Net-SL (label correction) unifies loss reweighting and label correction."
        },
        {
            "heading": "A.7 DISCUSSION: A CASE FOR CLASSIFICATION AS REGRESSION",
            "text": "Our goal in this section is to make a strong case for why transforming classification to regression is an interesting and potentially promising new research direction.\nProbabilistic Classifiers and Regression The goal of classification is to correctly predict which class unseen examples belong to. There are non-probabilistic classification algorithms that directly predict class IDs, e.g., decision trees (Kotsiantis, 2013), nearest neighbours (Cunningham & Delany, 2021), etc. However, in probabilistic classification, the labels (one-hot encodings) and predictions lives in a continuous multidimensional real space, i.e., the probability simplex. Thus, standard softmax classifiers are essentially regressors. In this work, we just acknowledge this fact and exploit it to map the constrained real space (simplex) to an equivalent unconstrained real space by adapting the well-founded log-ratio transform approach. We are excited about this, as then the entire regression literature is at our disposal. To be clear, viewing classification as regression does not by default solve\nall our problems or necessarily perform better, but it opens up a new and potentially fruitful research direction to explore.\nA Promising Example To show the promise of this direction, we make some simple (shifted Gaussian) noise assumptions in this unconstrained real space that naturally leads to our loss, which we show have properties similar to well-established robustness techniques developed for classification: loss reweighting and label correction. In contrast to classification approaches that combine separate loss reweighting and label correction methods, our approach is simple to understand and analyse from a probabilistic perspective, which is directly due to the regression view, as it opens up the ability to use the Gaussian likelihood.\nStronger Robustness Although our simple Gaussian noise model achieved strong results compared to many recent baselines, we have reason to believe even stronger robustness is possible. For example, we are interested in exploring the subfield of robust statistics (Huber, 2004; Maronna et al., 2019), that have developed regression methods specifically for robustness against outliers and label noise.\nUncertainty Estimation We believe viewing classification as regression is not only exciting from a robustness to label noise perspective, but also from an uncertainty perspective. A common/standard prior in Bayesian statistics is the Gaussian, which is problematic as it is not a conjugate prior for the standard categorical likelihood in classification, leading to the use of approximate inference techniques (Zhang et al., 2018; Nickisch & Rasmussen, 2008). By viewing classification as regression, we get access to the Gaussian likelihood with which all relevant integrals are Gaussian and can be evaluated analytically, see e.g., Gaussian processes regression (Rasmussen et al., 2006). This could lead to more efficient algorithms with better noise robustness (related to aleatoric uncertainty) and (epistemic) uncertainty estimates for classification. We are excited to explore this in future work.\nDeep Learning Theory In the theoretical literature on deep learning, taking a regression perspective of classification can in many cases ease the theoretical analysis, e.g., the law of robustness (Bubeck & Sellke, 2021), the neural tangent kernel (Jacot et al., 2018), and the bias-variance tradeoff (Geman et al., 1992), etc. We hope our approach to transform classification to regression could help connect these theories to classification in a principled way.\nSquared Loss for Classification Recently, there has been a growing amount of evidence for the practical usefulness of using regression losses for classification, especially the squared error loss, which correspond to a Gaussian likelihood with an identity covariance matrix applied on the one-hot encoded versions of the labels. For example, Hui & Belkin (2020) compared using the squared error regression loss with the standard cross-entropy (CE) loss on several classification tasks. They concluded: \u201cOur empirical results suggest amending best practices of deep learning to include training with square loss for classification problems on equal footing with cross-entropy or even as a preferred option.\u201d. Another example is the work of Hu et al. (2022) that theoretically and empirically studied the squared loss for training deep learning classifiers. They found several good properties: fast convergence rates, strong adversarial robustness, and low calibration error. We believe our adapted log-ratio transform approach is a sound alternative to directly regressing on the one-hot labels, as the regression predictions can naturally be mapped back to the simplex via the inverse transform.\nWe believe our work provides a promising direction towards robust deep learning in the presence of label noise, and we are excited to further improve robustness by exploring the vast (robust) regression literature."
        },
        {
            "heading": "A.8 EVOLUTION OF VALIDATION ACCURACY",
            "text": "To investigate any potential overfitting problem, we study the clean validation accuracy during training for the EMA network for various symmetric and asymmetric noise rates on CIFAR-10 and CIFAR-100, see Figure 4. We find there are no signs of overfitting. In fact, based on the training curves, the performance of SGN could potentially be improved further if training for longer on the higher noise rates."
        },
        {
            "heading": "A.9 ADDITIONAL ABLATION STUDY",
            "text": "In Table 1, we found that SGN outperforms CE when no noise is added. To better understand the reason for this, we perform an ablation study, similar to Table 4, where we study the performance of SGN when we systematically deactivate its components: loss reweighting, label correction, and EMA predictions. The results can be found in Table 7. We find that the main reason for the improvement over CE is due to the EMA predictions. As there is no added label noise, and loss reweighting\nand label correction are directly related to modelling label noise, they do not bring the same strong improvements as was observed for label noise in Table 4."
        },
        {
            "heading": "A.10 HYPERPARAMETER SENSITIVITY ON GENERALIZATION",
            "text": "To investigate the sensitivity of our method to the only hyperparameter \u03b1 that the gradual introduction of the shift \u2206, we plot the clean validation accuracy on CIFAR-100 for various settings of \u03b1 of the EMA network, see Figure 5. We find \u03b1 = 0.995 performs well in all setups, and that \u03b1 = 1 (no shift) performs much worse, showing the importance of the shift on robustness."
        },
        {
            "heading": "A.11 SYMMETRIC AND ASYMMETRIC NOISE",
            "text": "Symmetric noise of X% means that there is a X% risk for each example that their label is replaced by a uniformly random new label. Similarly, for asymmetric noise, there is a risk the label is replaced, but now to a perceptually similar class, e.g., images with cat labels are replaced with a dog label.\nIn the symmetric case, even after applying noise with a noise rate above 50%, out of all examples with a particular label, the majority of them will still be correctly labelled. An example will make this clearer, consider the case with three classes, A, B and C, with 50% noise (and for simplicity, the randomly selected new label cannot be the original label). Then, after the noise process, out of the examples that originally had a label of A, 50% are still As, 25% now are Bs, and 25% are Cs. With a similar argument for other classes, we find that out of all examples with a label of A after the noise process, 50% of the examples were originally As, 25% Bs, 25% Cs. In fact, as long as the noise rate is below (K-1)/K, the true class will still be in majority after the noise process, so there is still a signal to learn from.\nThe asymmetric noise case is different. As some classes are only changed to a particular other class (e.g., cat to dog), for the true class to still be in majority after the noise process, the noise rate must be below 50%.\nTo summarize, the noise rates must be less than K\u22121K and 1 2 for symmetric and asymmetric noise, respectively. This makes it possible to have larger than 50% symmetric noise rates on datasets such as the CIFAR datasets (K = 10 and K = 100)."
        }
    ],
    "year": 2023
}