{
    "abstractText": "A growing literature in computational neuroscience leverages gradient descent and learning algorithms that approximate it to study synaptic plasticity in the brain. However, the vast majority of this work ignores a critical underlying assumption: the choice of distance for synaptic changes \u2013 i.e. the geometry of synaptic plasticity. Gradient descent assumes that the distance is Euclidean, but many other distances are possible, and there is no reason that biology necessarily uses Euclidean geometry. Here, using the theoretical tools provided by mirror descent, we show that the distribution of synaptic weights will depend on the geometry of synaptic plasticity. We use these results to show that experimentally-observed log-normal weight distributions found in several brain areas are not consistent with standard gradient descent (i.e. a Euclidean geometry), but rather with nonEuclidean distances. Finally, we show that it should be possible to experimentally test for different synaptic geometries by comparing synaptic weight distributions before and after learning. Overall, our work shows that the current paradigm in theoretical work on synaptic plasticity that assumes Euclidean synaptic geometry may be misguided and that it should be possible to experimentally determine the true geometry of synaptic plasticity in the brain.",
    "authors": [
        {
            "affiliations": [],
            "name": "ON THE"
        },
        {
            "affiliations": [],
            "name": "Roman Pogodin"
        },
        {
            "affiliations": [],
            "name": "Jonathan Cornford"
        },
        {
            "affiliations": [],
            "name": "Arna Ghosh"
        }
    ],
    "id": "SP:a8d92d2052cdda37698724679ba6c6e63e8b38b0",
    "references": [
        {
            "authors": [
                "Mohamed Akrout",
                "Collin Wilson",
                "Peter Humphreys",
                "Timothy Lillicrap",
                "Douglas B Tweed"
            ],
            "title": "Deep learning without weight transport",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Shun-ichi Amari"
            ],
            "title": "Differential-geometrical methods in statistics",
            "venue": "Lecture Notes on Statistics,",
            "year": 1985
        },
        {
            "authors": [
                "Shun-Ichi Amari"
            ],
            "title": "Natural gradient works efficiently in learning",
            "venue": "Neural computation,",
            "year": 1998
        },
        {
            "authors": [
                "Navid Azizan",
                "Babak Hassibi"
            ],
            "title": "Stochastic gradient/mirror descent: Minimax optimality and implicit regularization",
            "venue": "arXiv preprint arXiv:1806.00952,",
            "year": 2018
        },
        {
            "authors": [
                "Navid Azizan",
                "Sahin Lale",
                "Babak Hassibi"
            ],
            "title": "Stochastic mirror descent on overparameterized nonlinear models",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Shahab Bakhtiari",
                "Patrick Mineault",
                "Timothy Lillicrap",
                "Christopher Pack",
                "Blake Richards"
            ],
            "title": "The functional specialization of visual cortex emerges from training parallel pathways with selfsupervised predictive learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Amir Beck",
                "Marc Teboulle"
            ],
            "title": "Mirror descent and nonlinear projected subgradient methods for convex optimization",
            "venue": "Operations Research Letters,",
            "year": 2003
        },
        {
            "authors": [
                "Patrick Billingsley"
            ],
            "title": "Convergence of probability measures",
            "year": 1999
        },
        {
            "authors": [
                "JR Blum",
                "H Chernoff",
                "M Rosenblatt",
                "H Teicher"
            ],
            "title": "Central limit theorems for interchangeable processes",
            "venue": "Canadian Journal of Mathematics,",
            "year": 1958
        },
        {
            "authors": [
                "Blake Bordelon",
                "Cengiz Pehlevan"
            ],
            "title": "Self-consistent dynamical field theory of kernel evolution in wide neural networks",
            "venue": "arXiv preprint arXiv:2205.09653,",
            "year": 2022
        },
        {
            "authors": [
                "Lev M Bregman"
            ],
            "title": "The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming",
            "venue": "USSR computational mathematics and mathematical physics,",
            "year": 1967
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck"
            ],
            "title": "Convex optimization: Algorithms and complexity",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Gy\u00f6rgy Buzs\u00e1ki",
                "Kenji Mizuseki"
            ],
            "title": "The log-dynamic brain: how skewed distributions affect network operations",
            "venue": "Nature Reviews Neuroscience,",
            "year": 2014
        },
        {
            "authors": [
                "Simone Carlo Surace",
                "Jean-Pascal Pfister",
                "Wulfram Gerstner",
                "Johanni Brea"
            ],
            "title": "On the choice of metric in gradient-based theories of brain function",
            "venue": "arXiv e-prints, pp",
            "year": 2018
        },
        {
            "authors": [
                "Xinlei Chen",
                "Kaiming He"
            ],
            "title": "Exploring simple siamese representation learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Lenaic Chizat",
                "Edouard Oyallon",
                "Francis Bach"
            ],
            "title": "On lazy training in differentiable programming",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "David Clark",
                "LF Abbott",
                "SueYeon Chung"
            ],
            "title": "Credit assignment through broadcasting a global error vector",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Sven Dorkenwald",
                "Nicholas L Turner",
                "Thomas Macrina",
                "Kisuk Lee",
                "Ran Lu",
                "Jingpeng Wu",
                "Agnes L Bodor",
                "Adam A Bleckert",
                "Derrick Brittain",
                "Nico Kemnitz"
            ],
            "title": "Binary and analog variation of synapses between cortical pyramidal neurons",
            "venue": "Elife, 11:e76120,",
            "year": 2022
        },
        {
            "authors": [
                "John C Duchi",
                "Shai Shalev-Shwartz",
                "Yoram Singer",
                "Ambuj Tewari"
            ],
            "title": "Composite objective mirror descent",
            "venue": "In COLT,",
            "year": 2010
        },
        {
            "authors": [
                "Timo Flesch",
                "Keno Juechems",
                "Tsvetomira Dumbalska",
                "Andrew Saxe",
                "Christopher Summerfield"
            ],
            "title": "Orthogonal representations for robust context-dependent task performance in brains",
            "venue": "and neural networks. Neuron,",
            "year": 2022
        },
        {
            "authors": [
                "Nicolas Fr\u00e9maux",
                "Wulfram Gerstner"
            ],
            "title": "Neuromodulated spike-timing-dependent plasticity, and theory of three-factor learning rules",
            "venue": "Frontiers in neural circuits,",
            "year": 2016
        },
        {
            "authors": [
                "Udaya Ghai",
                "Elad Hazan",
                "Yoram Singer"
            ],
            "title": "Exponentiated gradient meets gradient descent",
            "venue": "In Algorithmic learning theory,",
            "year": 2020
        },
        {
            "authors": [
                "Suriya Gunasekar",
                "Blake E Woodworth",
                "Srinadh Bhojanapalli",
                "Behnam Neyshabur",
                "Nati Srebro"
            ],
            "title": "Implicit regularization in matrix factorization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Suriya Gunasekar",
                "Jason Lee",
                "Daniel Soudry",
                "Nathan Srebro"
            ],
            "title": "Characterizing implicit bias in terms of optimization geometry",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Arthur Jacot",
                "Franck Gabriel",
                "Cl\u00e9ment Hongler"
            ],
            "title": "Neural tangent kernel: Convergence and generalization in neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Jyrki Kivinen",
                "Manfred K Warmuth"
            ],
            "title": "Exponentiated gradient versus gradient descent for linear predictors",
            "venue": "information and computation,",
            "year": 1997
        },
        {
            "authors": [
                "Jonas Kubilius",
                "Martin Schrimpf",
                "Ha Hong",
                "Najib J. Majaj",
                "Rishi Rajalingham",
                "Elias B. Issa",
                "Kohitij Kar",
                "Pouya Bashivan",
                "Jonathan Prescott-Roy",
                "Kailyn Schmidt",
                "Aran Nayebi",
                "Daniel Bear",
                "Daniel L.K. Yamins",
                "James J. DiCarlo"
            ],
            "title": "Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs",
            "venue": "In Neural Information Processing Systems (NeurIPS). Curran Associates,",
            "year": 2019
        },
        {
            "authors": [
                "\u0141ukasz Ku\u015bmierz",
                "Takuya Isomura",
                "Taro Toyoizumi"
            ],
            "title": "Learning with three factors: modulating hebbian plasticity with errors",
            "venue": "Current opinion in neurobiology,",
            "year": 2017
        },
        {
            "authors": [
                "Tor Lattimore",
                "Andras Gyorgy"
            ],
            "title": "Mirror descent and the information ratio",
            "venue": "In Conference on Learning Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Tor Lattimore",
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Bandit algorithms",
            "year": 2020
        },
        {
            "authors": [
                "Guillaume Leclerc",
                "Andrew Ilyas",
                "Logan Engstrom",
                "Sung Min Park",
                "Hadi Salman",
                "Aleksander Madry"
            ],
            "title": "FFCV: Accelerating training by removing data bottlenecks",
            "venue": "https://github.com/ libffcv/ffcv/,",
            "year": 2022
        },
        {
            "authors": [
                "Jaehoon Lee",
                "Lechao Xiao",
                "Samuel Schoenholz",
                "Yasaman Bahri",
                "Roman Novak",
                "Jascha SohlDickstein",
                "Jeffrey Pennington"
            ],
            "title": "Wide neural networks of any depth evolve as linear models under gradient descent",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Robert B Levy",
                "Alex D Reyes"
            ],
            "title": "Spatial profile of excitatory and inhibitory synaptic connectivity in mouse primary auditory cortex",
            "venue": "Journal of Neuroscience,",
            "year": 2012
        },
        {
            "authors": [
                "Qianli Liao",
                "Joel Leibo",
                "Tomaso Poggio"
            ],
            "title": "How important is weight symmetry in backpropagation",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2016
        },
        {
            "authors": [
                "Timothy P Lillicrap",
                "Daniel Cownden",
                "Douglas B Tweed",
                "Colin J Akerman"
            ],
            "title": "Random synaptic feedback weights support error backpropagation for deep learning",
            "venue": "Nature communications,",
            "year": 2016
        },
        {
            "authors": [
                "Yonatan Loewenstein",
                "Annerose Kuras",
                "Simon Rumpel"
            ],
            "title": "Multiplicative dynamics underlie the emergence of the log-normal distribution of spine sizes in the neocortex in vivo",
            "venue": "Journal of Neuroscience,",
            "year": 2011
        },
        {
            "authors": [
                "Ningning Ma",
                "Xiangyu Zhang",
                "Hai-Tao Zheng",
                "Jian Sun"
            ],
            "title": "Shufflenet v2: Practical guidelines for efficient cnn architecture design",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "James Martens"
            ],
            "title": "New insights and perspectives on the natural gradient method",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Alexander G de G Matthews",
                "Mark Rowland",
                "Jiri Hron",
                "Richard E Turner",
                "Zoubin Ghahramani"
            ],
            "title": "Gaussian process behaviour in wide deep neural networks",
            "venue": "arXiv preprint arXiv:1804.11271,",
            "year": 2018
        },
        {
            "authors": [
                "Joshua B Melander",
                "Aran Nayebi",
                "Bart C Jongbloets",
                "Dale A Fortin",
                "Maozhen Qin",
                "Surya Ganguli",
                "Tianyi Mao",
                "Haining Zhong"
            ],
            "title": "Distinct in vivo dynamics of excitatory synapses onto cortical pyramidal neurons and parvalbumin-positive interneurons",
            "venue": "Cell reports,",
            "year": 2021
        },
        {
            "authors": [
                "Mohamad Amin Mohamadi",
                "Wonho Bae",
                "Danica J Sutherland"
            ],
            "title": "Making look-ahead active learning strategies feasible with neural tangent kernels",
            "venue": "arXiv preprint arXiv:2206.12569,",
            "year": 2022
        },
        {
            "authors": [
                "Aran Nayebi",
                "Daniel Bear",
                "Jonas Kubilius",
                "Kohitij Kar",
                "Surya Ganguli",
                "David Sussillo",
                "James J DiCarlo",
                "Daniel L Yamins"
            ],
            "title": "Task-driven convolutional recurrent models of the visual system",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Arkadij Semenovi\u010d Nemirovskij",
                "David Borisovich Yudin"
            ],
            "title": "Problem complexity and method efficiency in optimization",
            "year": 1983
        },
        {
            "authors": [
                "Yann Ollivier",
                "Ludovic Arnold",
                "Anne Auger",
                "Nikolaus Hansen"
            ],
            "title": "Information-geometric optimization algorithms: A unifying picture via invariance principles",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2017
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, highperformance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Bill Podlaski",
                "Christian K Machens"
            ],
            "title": "Biological credit assignment through dynamic inversion of feedforward networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Roman Pogodin",
                "Peter Latham"
            ],
            "title": "Kernelized information bottleneck leads to biologically plausible 3-factor hebbian learning in deep networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yi Ren",
                "Shangmin Guo",
                "Wonho Bae",
                "Danica J Sutherland"
            ],
            "title": "How to prepare your task head for finetuning",
            "venue": "arXiv preprint arXiv:2302.05779,",
            "year": 2023
        },
        {
            "authors": [
                "Blake A Richards",
                "Timothy P Lillicrap",
                "Philippe Beaudoin",
                "Yoshua Bengio",
                "Rafal Bogacz",
                "Amelia Christensen",
                "Claudia Clopath",
                "Rui Ponte Costa",
                "Archy de Berker",
                "Surya Ganguli"
            ],
            "title": "A deep learning framework for neuroscience",
            "venue": "Nature neuroscience,",
            "year": 2019
        },
        {
            "authors": [
                "Blake Aaron Richards",
                "Konrad Paul Kording"
            ],
            "title": "The study of plasticity has always been about gradients",
            "venue": "The Journal of Physiology,",
            "year": 2023
        },
        {
            "authors": [
                "Daniel L Ruderman"
            ],
            "title": "The statistics of natural images. Network: computation",
            "venue": "in neural systems,",
            "year": 1994
        },
        {
            "authors": [
                "Martin Schrimpf",
                "Jonas Kubilius",
                "Ha Hong",
                "Najib J Majaj",
                "Rishi Rajalingham",
                "Elias B Issa",
                "Kohitij Kar",
                "Pouya Bashivan",
                "Jonathan Prescott-Roy",
                "Franziska Geiger"
            ],
            "title": "Brain-score: Which artificial neural network for object recognition is most brain-like? BioRxiv",
            "year": 2018
        },
        {
            "authors": [
                "David PA Schulz",
                "Maneesh Sahani",
                "Matteo Carandini"
            ],
            "title": "Five key factors determining pairwise correlations in visual cortex",
            "venue": "Journal of neurophysiology,",
            "year": 2015
        },
        {
            "authors": [
                "Jonathan Schwarz",
                "Siddhant Jayakumar",
                "Razvan Pascanu",
                "Peter E Latham",
                "Yee Teh"
            ],
            "title": "Powerpropagation: A sparsity inducing weight reparameterisation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Shai Shalev-Shwartz"
            ],
            "title": "Online learning and online convex optimization",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2012
        },
        {
            "authors": [
                "Sen Song",
                "Per Jesper Sj\u00f6str\u00f6m",
                "Markus Reigl",
                "Sacha Nelson",
                "Dmitri B Chklovskii"
            ],
            "title": "Highly nonrandom features of synaptic connectivity in local cortical circuits",
            "venue": "PLoS biology,",
            "year": 2005
        },
        {
            "authors": [
                "Mingxing Tan",
                "Quoc Le"
            ],
            "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Dimitra Vardalaki",
                "Kwanghun Chung",
                "Mark T Harnett"
            ],
            "title": "Filopodia are a structural substrate for silent synapses in adult",
            "venue": "neocortex. Nature,",
            "year": 2022
        },
        {
            "authors": [
                "Roman Vershynin"
            ],
            "title": "High-dimensional probability: An introduction with applications in data science, volume 47",
            "year": 2018
        },
        {
            "authors": [
                "Daniel LK Yamins",
                "Ha Hong",
                "Charles F Cadieu",
                "Ethan A Solomon",
                "Darren Seibert",
                "James J DiCarlo"
            ],
            "title": "Performance-optimized hierarchical models predict neural responses in higher visual cortex",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2014
        },
        {
            "authors": [
                "Chiyuan Zhang",
                "Samy Bengio",
                "Moritz Hardt",
                "Benjamin Recht",
                "Oriol Vinyals"
            ],
            "title": "Understanding deep learning (still) requires rethinking generalization",
            "venue": "Communications of the ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Weishun Zhong",
                "Ben Sorscher",
                "Daniel D Lee",
                "Haim Sompolinsky"
            ],
            "title": "A theory of learning with constrained weight-distribution",
            "venue": "arXiv preprint arXiv:2206.08933,",
            "year": 2022
        },
        {
            "authors": [
                "Matthews"
            ],
            "title": "It\u2019s a welldefined stochastic process since N is finite, meaning that \u039e is a weighted sum of N stochastic processes. However, it formalizes convergence of a finite-dimensional vector \u03be to something infinite-dimensional. That is, we\u2019ll work with convergence of the process \u039e to a limit process \u039e in distribution.3",
            "year": 2018
        },
        {
            "authors": [
                "Matthews"
            ],
            "title": "2018) for the discussion on what \u201din distribution\u201d means for stochastic processes",
            "year": 2018
        },
        {
            "authors": [
                "Matthews"
            ],
            "title": "A \u2282 Z>0, the projection of the sequence of the scaled stochastic processes \u039e\u0303 w.r.t. A , \u03b1 converges to a Gaussian in distribution (with parameters found in Lemma 3). Therefore, \u039e\u0303 converges to a Gaussian process in distribution by Lemma",
            "year": 2018
        },
        {
            "authors": [
                "MNIST LeCun"
            ],
            "title": "The networks were trained on the train set, and then finetuned on a subset of the test set (same procedure as for deep networks) for N = D (the number of weights scales quadratically with the hidden size, so N equals the number of hidden units). The Gaussian fits, and non-Gaussianity of weight changes measured with a wrong potential were remarkably similar to our theoretical predictions and behavior of deep networks (Fig. 7A). Note that for networks",
            "year": 2010
        },
        {
            "authors": [
                "Powerpropagation Schwarz"
            ],
            "title": "2021) reparametrized network\u2019s weights w as w = \u03b8 |\u03b8|\u03b1\u22121, such that gradient descent is done over \u03b8. The original weights are therefore updated for a loss L as (assuming all terms remain positive for simplicity)",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Many computational neuroscience studies use gradient descent to train models that are then compared to the brain Schrimpf et al. (2018); Nayebi et al. (2018); Yamins et al. (2014); Bakhtiari et al. (2021); Flesch et al. (2022), and many others explore how synaptic plasticity could approximate gradient descent Richards et al. (2019). One aspect of this framework that is often not explicitly considered is that in order to follow the gradient of a loss in the synaptic weight space, one must have a means of measuring distance in the synaptic space, i.e. of determining what constitutes a large versus a small change in the weights Carlo Surace et al. (2018). In other words, whenever we build a neural network model we are committing to a synaptic geometry. Standard gradient descent assumes Euclidean geometry, meaning that distance in synaptic space is equivalent to the L2-norm of weight changes.\nThere are however other distances that can be used. For example, in natural gradient descent the Kullback-Leibler divergence of the network\u2019s output distributions is used as the distance Martens (2020). More broadly, the theory of mirror descent provides tools for analyzing and building algorithms that use different distances in parameter space Nemirovskij & Yudin (1983); Beck & Teboulle (2003), which has proven useful in a variety of applications Shalev-Shwartz et al. (2012); Bubeck et al. (2015); Lattimore & Gyorgy (2021). However, within computational neuroscience the question of synaptic geometry is often overlooked: most models use Euclidean geometry without considering other options Carlo Surace et al. (2018). This assumption has no basis in neuroscience data, so how could we possibly determine the synaptic geometry used by the brain?\n\u2217Equal contribution. 1Dept. CS and OR; 2Dept. Maths and Stats; 3Dept. of Neurology & Neurosurgery, School of Computer Science, Montreal Neurological Institute; 4Learning in Machines and Brains Program.\nHere, using tools from mirror descent Nemirovskij & Yudin (1983); Beck & Teboulle (2003), we show that synaptic geometry can be determined by observing the distribution of synaptic weight changes during learning. Specifically, we prove that in situations where synaptic changes are relatively small the distribution of synaptic weights depends on the synaptic geometry (with mild assumptions about the loss function and the dataset). We use this result to show that the geometry defines a dual space in which the total synaptic changes are Gaussian. As a result, if one can find a dual space in which experimentally observed synaptic changes are Gaussian, then one knows the synaptic geometry. Applying this framework to existing neural data, which suggests that synaptic weights are log-normally distributed Song et al. (2005); Loewenstein et al. (2011); Melander et al. (2021); Buzsa\u0301ki & Mizuseki (2014), we conclude that the brain is unlikely to use a Euclidean synaptic geometry. Moreover, we show how to use our findings to make experimental predictions. In particular, we show that it should be possible to use experimentally observed weight distributions before and after learning to rule out different candidate geometries. Altogether, our work provides a novel theoretical insight for reasoning about the learning algorithms of the brain."
        },
        {
            "heading": "1.1 RELATED WORK",
            "text": "There is a large and growing literature on approximating gradient-based learning in the brain Lillicrap et al. (2016); Liao et al. (2016); Akrout et al. (2019); Podlaski & Machens (2020); Clark et al. (2021). The vast majority of this work assumes Euclidean synaptic geometry Carlo Surace et al. (2018). Notably, even if the brain does not estimate gradients directly, as long as synaptic weight updates are relatively small, then the brain\u2019s learning algorithm must be non-orthogonal to some gradient in expectation Richards & Kording (2023). As such, our work is relevant to neuroscience regardless of the specific learning algorithm used by the brain.\nOur work draws strongly from the rich and long-standing literature on mirror descent, which was originally introduced 40 years ago for convex optimization Nemirovskij & Yudin (1983). Recent years have seen a lot of work in this area Beck & Teboulle (2003); Duchi et al. (2010); Bubeck et al. (2015); Ghai et al. (2020); Lattimore & Gyorgy (2021), especially in online optimization settings such as bandit algorithms Shalev-Shwartz et al. (2012); Lattimore & Szepesva\u0301ri (2020). More recently, researchers have started to apply mirror descent to deep networks and have used it to try to develop better performing algorithms than gradient descent Azizan & Hassibi (2018); Azizan et al. (2021). This work is related to natural gradient descent which also explores non-Euclidean geometries Amari (1985; 1998); Ollivier et al. (2017).\nFinally, our work is relevant to experimental neuroscience literature on synaptic weight distributions. Using a variety of techniques, including patch clamping Song et al. (2005) and fluorescent imaging Melander et al. (2021); Loewenstein et al. (2011); Vardalaki et al. (2022), neuroscientists have explored the distributions of synaptic strengths across a variety of brain regions and species. This work has generally reported log-normal distributions in synaptic weights Song et al. (2005); Loewenstein et al. (2011); Melander et al. (2021), though, a recent study observed a more complicated bimodal distribution in log-space Dorkenwald et al. (2022). Moreover, the parameters of log-normal distributions observed in primary auditory cortex Levy & Reyes (2012) are close to optimal in terms of perceptron capacity Zhong et al. (2022). Our work connects this experimental literature to our theoretical understanding of learning in the brain \u2013 it makes it possible to test theories of synaptic geometry using weight distribution data."
        },
        {
            "heading": "2 MIRROR DESCENT FRAMEWORK",
            "text": "In order to derive our core results we will rely on tools from mirror descent. To introduce it, we first revisit gradient descent. Assume we have a loss function l(w). In gradient descent, we choose the next point in weight space wt+1 from the current point wt by minimizing a linearized version of l(w) with a penalty for taking large steps in weight space (where the strength of the penalty is controlled by the learning rate \u03b7). Importantly, penalizing large steps in weight space necessitates a distance function. If we choose the squared L2-norm as our distance function we obtain:\nwt+1 = argmin w g(w,wt) , g(w,wt) = l(wt) +\u2207l(wt)\u22a4(w \u2212wt) + 1 2 \u03b7 \u2225w \u2212wt\u222522 . (1)\nThis choice of distance function results in the standard gradient descent update when we solve the unconstrained optimization problem in Eq. (1):\nwt+1 = wt \u2212 \u03b7\u2207l(wt) . (2) In mirror descent, we consider a more general set of distance functions provided by the Bregman divergence D\u03d5(w, w\u0303) Bregman (1967). For a strictly convex function, called a potential, \u03d5(w):\nD\u03d5(w, w\u0303) = \u03d5(w)\u2212 \u03d5(w\u0303)\u2212\u2207\u03d5(w\u0303)\u22a4(w \u2212 w\u0303) . (3) Re-writing Eq. (1) into this more general form we get:\nwt+1 = argmin w g\u03d5(w,w t) , g\u03d5(w,w t) = l(wt) +\u2207l(wt)\u22a4(w \u2212wt) + 1 \u03b7 D\u03d5(w,w t) . (4)\nWe can express the update in closed form using the gradient of the potential:\n\u2207\u03d5(wt+1) = \u2207\u03d5(wt)\u2212 \u03b7\u2207l(wt) . (5) As \u03d5 is strictly convex, \u2207\u03d5 is invertible, so:\nwt+1 = \u2207\u03d5\u22121 ( \u2207\u03d5(wt)\u2212 \u03b7\u2207l(wt) ) . (6)\nThe update moves w from the \u201cprimal\u201d space to the \u201cdual\u201d via \u2207\u03d5(w), performs a regular gradient descent update in the dual space, and then projects back to the primal space (Fig. 1). We consider p-norms \u03d5(w) = 1p\u2225w\u2225 p p and negative entropy\n\u03d5(w)= \u2211\ni|wi| log |wi|. The former for p=2 recovers standard gradient descent. The latter leads to the exponentiated gradient algorithm Kivinen & Warmuth (1997) (\u2299 denotes element-wise product):\nwt+1 = wt \u2299 e(\u2212\u03b7\u2207l(w t)\u2299 signwt) , (7)\nFig. 2 shows how different potentials change problem geometry and, therefore, solutions.\nEquation (5) already predicts our main result: if updates in the dual space are approximately independent, or contain noise, over time the sum of the updates will look Gaussian by the central limit theorem (since they sum linearly in the dual space). In fact, Loewenstein et al. (2011) showed that synaptic weights (of rodent auditory cortex) stay log-normal over time (implying Gaussian changes in the log space) and follow multiplicative dynamics in synaptic weight space. This is consistent with the negative entropy potential and in particular Eq. (7). This data, however, wasn\u2019t collected when training mice on a specific task. It is not clear if learning-driven dynamics follows the same update geometry. Here, we develop a theory for distinguishing synaptic geometries during training.\nEq. (6) allows us to understand synaptic weight updates via two independent terms: a credit signal and an intrinsic synaptic geometry. The credit signal is the gradient \u2207l(wt) \u2013 or a more biologically plausible approximation. The synaptic geometry is determined by the potential \u03d5; it captures how the credit signal \u2207l(wt) is used to update synaptic weights. Note however, we are not necessarily proposing that this separation is reflected in underlying neurobiological processes, just that it can be used as a model for understanding weight updates. Most studies of biologically plausible deep learning Lillicrap et al. (2016); Liao et al. (2016); Akrout et al. (2019); Podlaski & Machens (2020); Clark et al. (2021) use Euclidean distance by default, effectively ignoring\nthe second term (although see Carlo Surace et al. (2018) and Schwarz et al. (2021)). Here we concentrate on the second term, and derive results that are independent of the first term. Therefore, and crucially, previous theoretical studies that derive biologically plausible estimates of credit signals are fully compatible with our work."
        },
        {
            "heading": "2.1 IMPLICIT BIAS IN MIRROR DESCENT",
            "text": "Gradient descent in overparametrized linear regression finds the minimum norm solution: a set of weights w closest to the initial weights w0, according to the 2-norm distance Gunasekar et al. (2017); Zhang et al. (2021). This bias towards small changes is referred to as the implicit bias of the algorithm. Recently, a similar result was obtained for mirror descent, wherein the implicit bias will depend on the potential \u03d5 Gunasekar et al. (2018). Here we discuss this result and its applicability to gradient-based learning in deep networks.\nLinear regression. To begin, consider a linear problem: predict yn from xn as y\u0302 n=(xn)\u22a4w. For N points and D-dimensional weights, we write this as y =Xw. Gunasekar et al. (2018) showed that for losses l(y\u0302, y) with a unique finite root (l(y\u0302, y)\u2192 0 iff y\u0302 \u2192 y), the mirror descent solution w\u221e (assuming it exists) is the closest w to the initial weights w0 w.r.t. the Bregman divergence D\u03d5:\nw\u221e = argmin w: y=X\u22a4w D\u03d5(w,w 0) . (8)\nAn example of such a setup is the MSE loss l(y\u0302, y) = (y\u0302 \u2212 y)2/2 and an overparametrized model with D \u2265 N . For a fixed y, this gives rise to an anti-Hebbian gradient \u2212(y\u0302 \u2212 y)x. Deep networks. A crucial condition is hidden in the proof of the above result: the gradient updates \u2207l(y\u0302 n, yn) have to span the space of xn. For a linear model y\u0302 = x\u22a4w, this is naturally satisfied as \u2207l(y\u0302 n, yn) = \u2202 l(y\u0302\nn,yn) \u2202 y\u0302 n x\nn. For deep networks, Azizan et al. (2021) showed a similar to Eq. (8) result for solutions close to the initial weights, and empirically noted that different potentials result in different final weights. Thus, we can make a similar assumption: if the solution is close to the initial weights, then we can linearize a given deep network f(w,x) around the initial weights w0:\nf lin(w,x,w0) = f(w0,x) +\u2207f(w0,x)\u22a4(w \u2212w0) . (9) This function is linear in the weights w, but not in the inputs x. Intuitively, if w doesn\u2019t change a lot from w0, a linearized network should behave similarly to the original one with respect to weight changes. The linear approximation moves us back to a setting akin to linear regression except that the gradients now span \u2207f(w0,xi) rather than xi. Thus, for linearized networks, Eq. (8) becomes:\nw\u221e = argmin w: y=f lin(w,x,w0) D\u03d5(w,w 0) . (10)\nOne may wonder whether it is appropriate to assume that a solution exists near the initial weights when considering real brains. There are two reasons that we argue this is appropriate in our context. First, in biology, animals are never true \u201cblank slates\u201d, they instead come with both life experience and evolutionary priors. As such, it is not unreasonable to think that in real brains the solution in synaptic weight space often sits close to the initial weights, and moreover, there would be strong evolutionary pressure for this to be so. Second, neural tangent kernel (NTK; Jacot et al. (2018)) theory shows that, with some assumptions, infinite width networks are identical to their linearized versions, and for finite width networks, the linear approximation gets better as width increases Lee et al. (2019) (although learning dynamics don\u2019t always follow NTK theory, see e.g. Bordelon & Pehlevan (2022)). Thus, for very large networks (such as the mammalian brain) it is not unreasonable to think that a linear approximation may be appropriate.\nBelow, we will develop theory for Eq. (8) (linear regression), and then experimentally show that the results hold for fine-tuning of deep networks (which are close to linearized networks in Eq. (10))."
        },
        {
            "heading": "3 WEIGHT DISTRIBUTIONS IN MIRROR DESCENT",
            "text": "Our goal in this section is to derive a solution for the distribution of the final synaptic weights w\u221e as a function of the potential \u03d5. To do this, we begin by noting that Eq. (8) can be solved using Lagrange multipliers \u03bb \u2208 RN that enforce y = Xw; the Lagrangian is:\nL(w, \u03bb) = D\u03d5(w,w0) + (y \u2212Xw)\u22a4\u03bb . (11) Solve for the minimum of L by differentiating it w.r.t. w and setting it to 0:\n\u2202 L(w, \u03bb)\n\u2202w = \u2207\u03d5(w)\u2212\u2207\u03d5(w0)\u2212X\u22a4\u03bb = 0 . (12)\nSince \u2207\u03d5\u22121 is the inverse of \u2207\u03d5, we obtain: w\u221e = \u2207\u03d5\u22121 ( \u2207\u03d5(w0) +X\u22a4\u03bb ) , (13)\nwhere \u03bb will be chosen to satisfy y = Xw\u221e.\nTo obtain a closed-form solution, we can linearize Eq. (13) around \u2207\u03d5(w0) using the fact that the mapping with the potential is invertible, so \u2207\u03d5\u22121 ( \u2207\u03d5(w0) ) = w0:\nw\u221e \u2248 \u2207\u03d5\u22121 ( \u2207\u03d5(w0) ) +\u22072\u03d5\u22121 ( \u2207\u03d5(w0) ) X\u22a4\u03bb = w0 +H\u03d5\u22121X \u22a4\u03bb , (14)\nwhere we denoted H\u03d5\u22121 = \u22072\u03d5\u22121 ( \u2207\u03d5(w0) ) . For potentials that couple weights together, the Hessian will be non-diagonal. However, we use \u201clocal\u201d potentials in which each entry i of \u2207\u03d5(w) depends only on wi (i.e. \u03d5(w) = \u2211 i f(wi) for some function f ). For such potentials, the Hessian becomes diagonal. Since the mapping w.r.t. the potential is invertible, we can use the inverse function theorem to compute the Hessian H\u03d5\u22121 : H\u03d5\u22121 = \u22072\u03d5\u22121 (z) = ( \u22072\u03d5(w0) )\u22121 for z = \u2207\u03d5(w0), assuming \u03d5 is twice continuously differentiable and H\u03d5\u22121 is non-singular. Since H\u03d5\u22121 is ultimately a function of \u03d5, we have the structure of our solution. However, we need to solve for \u03bb in order to satisfy y = Xw\u221e. Assuming that H\u03d5\u22121 is positive-definite and that X \u2208 RN\u00d7D is rank N (D \u2265 N ) for the sake of invertibility, \u03bb can be approximated by \u03bb\u0302 as:\n\u03bb \u2248 \u03bb\u0302 = ( XH\u03d5\u22121X \u22a4)\u22121 (y \u2212Xw0) . (15) With Eq. (15), we can now state the main result. Intuitively, what this result will show is that (1) we can know the shape of the distribution of the final weights, (2) that shape is independent of the loss function and the dataset, but not the initial weights. More formally, this is stated as: Theorem 1 (Informal). Consider N i.i.d. samples yn,xn, such that: xn \u2208 RD are zero-mean and bounded; pairwise correlations cij =Exni xnj and c\u2032ij =Cov((xni )2, (xnj )2) between entries of a single xn decay quickly enough so \u2211\u221e j=1 |cij | \u2264 const and \u2211\u221e j=1 |c\u2032ij | \u2264 const for all i; yn = (xn)\u22a4w\u2217; the teacher weights w\u2217 and the initial weights w0 have zero-mean, O(1/D) variance, i.i.d. entries, and finite 8th moment; \u22072\u03d5\u22121(w0i ) has finite 1st and 2nd moments.\nThen for \u03bb\u0302 = ( XH\u03d5\u22121X \u22a4)\u22121 (y \u2212Xw0), individual entries of X\u22a4\u03bb\u0302 converge (in distribution) to a Gaussian with a constant variance \u03c32\u03bb as D, N \u2192 \u221e with N = o(D1/(5+\u03b4)) (for any \u03b4 > 0):\nDh\u221a N\n( X\u22a4\u03bb\u0302 ) i \u2212\u2192d N (0, \u03c32\u03bb) , (16)\nwhere h = E [ \u22072\u03d5\u22121(w0i ) ] (a scaling factor that is identical for all entries, which is not equivalent to H\u03d5\u22121 ), and \u03c32\u03bb depends on the distributions of inputs and initial weights. The whole vector X \u22a4\u03bb converges to a Gaussian process (over discrete indices) with weak correlations for distant points. Proof sketch. First, we show that scaled ( XH\u03d5\u22121X\n\u22a4)\u22121 behaves like an identity for large N,D. Then, we show that (X\u22a4(y \u2212Xw0))i is a sum of N exchangeable variables (i.e. any permutation of these points has the same distribution) that satisfy conditions for a central limit theorem for such variables. Finally, we convert this result into a convergence to a Gaussian process.\nWe postpone the full proof, along with a version of the theorem for generic labels (without the teacher weights w\u2217), to Appendix A. To unpack this theorem a bit more for the reader, combined with Eq. (13), the theorem shows that given some initial weights, w0, the distribution of w\u221e will depend on that initial distribution plus a term that converges to a Gaussian (in the dual space), as long as the loss has a unique finite root (defined before Eq. (8)) and the dataset correlations are small (cij , c\u2032ij in Theorem 1). As such, the distribution of the solution weights will depend on the initial weights and the potential, but not on the loss or data. To clarify the assumptions used, we need the pairwise correlations cij , c\u2032ij between distant inputs to be small. This is a common feature in natural sensory data Ruderman (1994) and neural activity Schulz et al. (2015). Finally, we note that the N scaling with D results from a generic large deviations bound and could likely be improved under some assumptions. In Section 4, we experimentally verify that N = D0.5 and N = D0.75 result in Gaussian behavior in the tested settings.\nPractical usage of our result. Since we expect the weight change in the dual space to look Gaussian, we also expect the weight change in the dual space of a wrong potential to not be Gaussian. If we denote the true potential \u2207\u03d5 = f and another potential \u2207\u03d5\u2032 = f \u2032, our result states that the change in the dual space is Gaussian: f(w\u221e)\u2212f(w0)=\u03be, where \u03be is Gaussian. If we use the wrong potential, i.e. f \u2032, we get f \u2032(w\u221e)=f \u2032(f\u22121(f(w0)+\u03be)). If f and f \u2032 are similar, then we f \u2032(f\u22121(\u00b7)) is approximately an identity, so we would see a (slightly less than for f ) Gaussian change. If not, the change will be non-Gaussian due to the nonlinear f \u2032(f\u22121(\u00b7)) transformation. Therefore we will be able to determine the potential used for training by finding the one with the most Gaussian change.\nApplicability to other learning rules. Our result in Theorem 1 is made possible by two components: the structure of mirror descent solutions (see Eq. (8)) and the Gaussian behavior of large sums. However, the mirror descent framework can be applied to any learning rule, if we replace the gradient w.r.t. the loss in Eq. (5) with a generic error term gt: \u2207\u03d5(wt+1) = \u2207\u03d5(wt)\u2212\u03b7 gt. This way, for small and approximately independent (e.g. exchangeable) error terms gt their total contribution to the weight change would also follow the central limit theorem, resulting in a small Gaussian weight change. However, for generic error terms we cannot rely on Eq. (8) and can only provide generic conditions for Guassian behavior. Therefore, while our work leverages gradient-based optimization, the intuition we present applies more broadly.\nTheorem applicability in lazy and rich regimes. The variance of the Gaussian term in Theorem 1 depends on the choice of the potential (via h). This means that the magnitude of the change from w0 to w\u221e will depend on \u03d5. Moreover, if the \u201clearned\u201d term X\u22a4\u03bb in Eq. (13) is not small compared to \u2207\u03d5(w0), our theory will not be applicable since the linearization will not be valid. Therefore, the applicability of our theorem is related to the question of \u201crich\u201d versus \u201clazy\u201d regimes of learning. Our theory is valid only in the lazy regime, in which learning is mostly linear Chizat et al. (2019).\nHowever, whether we are in the rich or lazy regime turns out to depend on the potential. Assume for simplicity that all initial weights are the same and positive: w0d =w 0 =\u03b1/ \u221a D. The standard lazy regime corresponds to large weights with \u03b1=1, and the standard rich regime corresponds to small weights with \u03b1=1/ \u221a D. We can show (see Appendix A.1 for a derivation) that for p-norms, the dual\nweights are asymptotically larger than the weight changes as long as \u03b1\u226b \u221a\nN/D (for N data points and D weights). For negative entropy, this bound is more loose: \u03b1\u226b \u221a N/(D logD). Therefore, for the same weight initialization and dataset size, negative entropy would typically produce smaller updates. For the standard for deep networks initialization with \u03b1\u22481, our theory should be applicable to datasets with N\u226aD (e.g. N=D0.5)."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "Here we empirically verify our theory under conditions relevant for neuroscientific experiments. We use PyTorch Paszke et al. (2019) and FFCV library for fast data loading Leclerc et al. (2022). The experiments were performed on a local cluster with A100 NVIDIA GPUs. Experimental details are provided in Appendix B. Code is available at github.com/romanpogodin/synaptic-weight-distr."
        },
        {
            "heading": "4.1 LINEAR REGRESSION",
            "text": "We begin by testing whether the theorem holds in the case of linear regression, where we know that Eq. (8) holds. However, for experimental applicability, we want to verify that the histogram of \u03bei over entries i is Gaussian even when we draw it from a single network (rather than several initializations of a network), which would be the condition that holds in real neural experiments. As well, we want to verify that the change of weights \u03bei=\u2207\u03d5(w\u221ei )\u2212\u2207\u03d5(w0i ) is smaller than \u2207\u03d5(w0i ). For each potential, we draw xi \u223c N (0, 1) + U(\u22120.5, 0.5) such that the correlation structure is as follows: Exi xj = (\u22121)|i\u2212j|/(1+ |i\u2212j|c) for c = 1, 2. Note that c = 1 is a more relaxed condition than the correlation assumption used in proving Theorem 1. This is, therefore, a test of whether the theorem applies more broadly. In our experiments here, the initial weights are drawn from wi \u223c N (0, 1/D) for width D, and labels are drawn as y = \u00b11 (equal probability). We then optimize (y\u2212\u2211\ni xiwi) 2 for networks of different widths D and N = D0.5, D0.75. Strictly speaking only N = o(D1/(5+\u03b4)) satisfies Theorem 1, so again, we are testing whether the theorem holds empirically when we relax our assumptions. We measure two things: the magnitude of weight changes in the\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nNE1.1 1.5 2.0 2.5 3.0\nlog-normal initialization\nNE1.1 1.5 2.0 2.5 3.0 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\nnormal initializationCDF difference magnitude of change \u0394CDF \u0394CDF A B Gaussian for:\nNE 2-norm 3-norm\n3 410 10 number of weights\n2 3 410 10 10 number of weights evaluated potential evaluated potential\n0.75 0.5 N = D N = D\n0.75 0.5 N = D N = D\n0.0\n0.5\n1.0 \u0394\u03c6(w)\n2\n\u0394CDF\n10\n0.0\n0.1\n0.2\nFigure 3: A. Linear regression solutions for negative entropy (NE) and fast correlation decay (c=2). Left: integral of absolute CDF difference (\u2206CDF) between normalized uncentered weights and N (0, 1). Right: magnitude of weight changes relative to the initial weights in the dual space (\u2206\u03d5). Solid line: median over 30 seeds; shaded area: 5/95% percentiles; pink: N=D0.5; blue: N=D0.75. B. \u2206CDF for Gaussian (left) and log-normal (right) weight initializations and a Gaussian addition w.r.t. \u03d5, evaluated on another potential \u03d5\u2032 (e.g. blue lines are sampled for NE but evaluated on every potential). Solid line: mean over 30 seeds; shaded areas: mean \u00b1 standard deviation.\ndual space \u2206\u03d5(w) = \u2225\u03be\u22252 / \u2225\u2207\u03d5(w0)\u22252 (i.e. \u03be normalized relative to the initial weights) and \u2206CDF = \u222b dt |eCDF(t)\u2212 CDF(t)| (i.e. the difference between the empirical cumulative density function, eCDF(t) and the standard normal CDF(t))1.\nFirst, we find that \u2206CDF is always relatively small (\u2264 0.2) and converges towards zero as the number of weights increases (Fig. 3A for negative entropy with c = 2. The full results are postponed to the appendix; see Fig. 6). Hence, we find that the weight changes in the dual space behave like a Gaussian when we know what potential was used for training, confirming Theorem 1. Second, we find that the magnitude of weight changes in the dual space \u2206\u03d5(w) is much smaller than 1 for large widths, justifying the linearization in Eq. (14)."
        },
        {
            "heading": "4.2 ROBUSTNESS TO POTENTIAL CHANGE",
            "text": "We also need to test what happens if we don\u2019t know the true potential used for training, here denoted \u03d5, which is the case for neural data. As discussed after Theorem 1, for \u03d5\u2032 close to the true potential we should see Gaussian changes, and for \u03d5\u2032 far from \u03d5 \u2013 non-Gaussian ones. We test negative entropy and 2/3-norms as \u03d5, and p-norm with p \u2208 [1.1, 3] and negative entropy as \u03d5\u2032; w0 is drawn from either Gaussian or log-normal distributions with approximately the same variance. We draw a Gaussian \u03be\u03d5 to compute w\u221e using \u03d5 from \u03be\u03d5=\u2207\u03d5(w\u221e)\u2212\u2207\u03d5(w0). We find that the \u201cempirical\u201d change for \u03d5\u2032, \u03be\u03d5\u2032 =\u2207\u03d5\u2032(w\u221e)\u2212\u2207\u2032\u03d5(w0), is indeed Gaussian only for small variations from the true potential \u03d5 (Fig. 3B). In particular, the distinction between negative entropy and other, nonmultiplicative updates, is very pronounced.2 For \u03d5= 3-norm and log-normal initialization, the range for a Gaussian \u03be\u03d5\u2032 almost reached p = 2.3 (Fig. 3B, bottom, orange line). Thus, if we hypothesize a potential, \u03d5\u2032, that is similar to the true potential \u03d5, we get a nearly Gaussian \u03be\u03d5\u2032 . In contrast, if we have the wrong form for the potential (e.g. the potential is the negative entropy but we hypothesize a 3-norm potential), then we get a clearly non-Gaussian distribution. As such, we can use the distribution of \u03be\u03d5\u2032 to test hypotheses about synaptic geometry."
        },
        {
            "heading": "4.3 FINETUNING OF DEEP NETWORKS",
            "text": "We expect our theory to work for networks that behave similarly to their linearization during learning. One such example is a pretrained trained network that\u2019s being finetuned on new data. If the new data comes from a similar distribution as the pretraining data, then the linearized network should approximate its non-linearized counterpart well with respect to weight changes Mohamadi et al. (2022); Ren et al. (2023). This is also the scenario we expect to see in neural data if we are training an animal on an ethologically relevant task that matches its prior life experience and innate capabilities to a reasonable degree.\n1We use normalized but uncentered weight distributions, since we predict the change \u03be to be zero-mean. 2Negative entropy is placed at p = 1, as for weights that sum to 1, p\u2212norms are equivalent to the shifted\nand scaled negative Tsallis entropy, which tends to negative entropy as p \u2192 1.\nFinetuning deep networks still breaks several assumption of Theorem 1: the initial weights are not i.i.d., the network is not linearized (although potentially close to it), and the data correlations are unknown. Moreover, the cross-entropy loss we use is technically not a loss with a unique finite root. However, the readout layers in the deep networks that we test have fewer units than the number of ImageNet Deng et al. (2009) classes, so the activity in the readout layer is not linearly separable and the weights do not tend to infinity. Yet, they are still large and change significantly during training, so we exclude the readout layer when assessing weight distributions (along with biases and BatchNorm parameters). We use networks pretrained on ImageNet, and finetune them to 100% accuracy on a subset of ImageNet validation set. The pretrained networks have not seen the validation data, but they already perform reasonably well on it, so there\u2019s no distribution shift in the data that would force the networks to change their weights a lot. We used N = D0.5 data points for D weights (N = D0.75 exceeded dataset size) and four architecture types: ShuffleNet v2 Ma et al. (2018) (x0.5/x1), EfficientNet Tan & Le (2019) (b0-b3), ResNet He et al. (2016) (18/34), and CORnet Kubilius et al. (2019) (S/RT) chosen to span a wide range of parameter counts (0.3M to 53.4M). CORnets were also chosen since they mimic the primate ventral stream and its recurrent connections.\nFor all three tested potentials, weight changes are close to a Gaussian distribution (\u2206CDF < 0.2; Fig. 4A). ShuffleNet v2s and ResNets reach consistently more Gaussian solutions than EfficientNets and CORnets, despite ShuffleNet v2s having fewer parameters. This suggests that some trained architectures are more \u201clinear\u201d, which may be due to the architecture itself, or the way the networks were trained. The magnitude of changes in the dual space was typically smaller than 0.2 (see Appendix B). Just like for the toy task in Fig. 3B, if we trained with a specific potential, \u03d5, we only observed Gaussian changes when the hypothesized potential, \u03d5\u2032, was close to \u03d5 (Fig. 4B). The only exceptions were 3-norm-trained EfficientNets 0,1,3 and CORnet-S, for which the 2-norm solution was slightly more Gaussian than the 3-norm solution even when \u03d5 was a 3-norm. However, this difference was small, and the networks had overall worse fit than other architectures. In all cases, using negative entropy as \u03d5\u2032 provided the best fit for negative entropy-trained networks and the worst fit for other potentials. This is important because negative entropy results in multiplicative weights updates, while p-norms result in additive updates, which represent very distinct hypotheses for synaptic plasticity mechanisms. Taken together, these empirical results suggest that Theorem 1 holds more generally and can be used to infer the distribution of weight changes beyond linear regression."
        },
        {
            "heading": "4.4 ESTIMATING SYNAPTIC GEOMETRY EXPERIMENTALLY",
            "text": "Previously we showed that Theorem 1 holds when finetuning pretrained deep networks, despite non-i.i.d. initial weights and an unknown input correlation structure. As such, these finetuning experiments indicate our theory is applicable to neuroscience experiments when we measure the distribution of weights before and after learning, and use the histogram of weight changes to estimate the synaptic geometry. In this section we apply this technique to experimental neuroscience data.\n10 -3 10 -2 10 -1 100 10 -3 10 -2 10 -1 100 10 -3 10 -2 10 -1 100 10 -3 10 -2 10 -1 100 spine volume\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nde ns\nity\ninitial weights for NE\nspine volume\nfinal weights\nspine volume\ninitial weights for 3-norm\nspine volume\nfinal weightsA B init weights final weights init weights final weights data data data data\np=0.77\np=0.5 p=0.23\nFigure 5: Observed vs. modeled synaptic distributions. Pink: spine volume (\u00b5m3) from Dorkenwald et al. (2022); vertical bars: point mass initializations. A. Left: initial weights for negative entropy (NE); right: final weight for a Gaussian change in the dual space. B. Same as A, but for 3-norm.\nSpecifically, we find that if one knows the initial weights, w0, then it is possible to distinguish between different potentials. But if the initial weights are unknown, then multiple potential functions can fit the data. To show this, we use data from a recent experimental study that measured an analogue of synaptic weights (synaptic spine size) using electron microscopy Dorkenwald et al. (2022) (pink histogram in Fig. 5). In the log space, Dorkenwald et al. (2022) found that the distribution was well modelled by a mixture of two Gaussians with approximately the same variance (pink histogram in Fig. 5) \u2013 instead of one Gaussian as has been previously reported Loewenstein et al. (2011). First, we show that the experimental data is consistent with the negative entropy potential (Fig. 5A), when using the following initialization: consider elements of w0 equal to a constant \u00b51 with probability p, and to \u00b52 with probability 1\u2212p. Then, \u2207\u03d5(w\u221e) = \u2207\u03d5(w0)+\u03be will be a mixture of two Gaussians with the same variance but different means, and Fig. 5A shows the \u03be that fits the experimental data (parameters from Dorkenwald et al. (2022) with equalized variance, see Appendix B.4). However, the data is also consistent with the 3-norm potential for a different initialization: if w0 is a mixture of a constant and a log-normal, then adding an appropriate \u03be in the dual space also results in a weight distribution that resemble a mixture of two log-normals and appears to fit the experimental data well (Fig. 5B). Thus, if our goal is to determine the synaptic geometry of the brain, then it is important to estimate the synaptic weight distributions before and after training. Nevertheless, with this data we can rule out a Euclidean synaptic geometry, and if we do have access to w0, then our results show that it is indeed possible to experimentally estimate the potential function."
        },
        {
            "heading": "5 DISCUSSION",
            "text": "We presented a mirror descent-based theory of what synaptic weight distributions can tell us about the underlying synaptic geometry of a network. For a range of loss functions and under mild assumptions on the data, we showed that weight distributions depend on the synaptic geometry, but not on the loss or the training data. Experimentally, we showed that our theory applies to finetuned deep networks across different architectures (including recurrent ones) and network sizes. Thus, our theory would likely apply as well to the hierarchical, recurrent architectures seen in the brain. Our work predicts that if we know synaptic weights before and after learning, we can find the underlying synaptic geometry by finding a transformation in which the weight change is Gaussian.\nIt is important to note that by adopting the mirror descent framework we made the assumption that the brain would seek to achieve the best performance increases for the least amount of synaptic change possible. But, these results could be extended to learning algorithms that are not explicitly derived from that principle, such as three-factor Hebbian learning Fre\u0301maux & Gerstner (2016); Kus\u0301mierz et al. (2017); Pogodin & Latham (2020). For a single layer, three-factor updates between y and x follow \u03f5 y x for some error signal \u03f5. As long as \u03f5 dynamics leads to a solution, the analysis should be similar to the general mirror descent one since weights will span x (as required by the theory). For multi-layered networks, all we would need to assume is that the input to each layer does not change too much over time, similar to the linearization argument made for deep networks.\nMore broadly, our work opens new experimental avenues for understanding synaptic plasticity in the brain. Our approach isolates synaptic geometry from other components of learning, such as losses, error signals, error pathways, etc. Therefore, our findings make it practical to experimentally determine the brain\u2019s synaptic geometry."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported by the following sources. BAR: NSERC (Discovery Grant RGPIN-202005105, RGPIN-2018-04821; Discovery Accelerator Supplement: RGPAS-2020-00031; Arthur B. McDonald Fellowship: 566355-2022), Healthy Brains, Healthy Lives (New Investigator Award: 2b-NISU-8), and CIFAR (Canada AI Chair; Learning in Machine and Brains Fellowship). GL: NSERC (Discovery Grant RGPIN-2018-04821), Canada-CIFAR AI Chair, Canada Research Chair in Neural Computations and Interfacing, and Samsung Electronics Co., Ldt. GG: Canada CIFAR AI Chair. JC: IVADO Postdoctoral Fellowship and the Canada First Research Excellence Fund. AG: Vanier Canada Graduate scholarship.\nThis research was enabled in part by support provided by (Calcul Que\u0301bec) (https://www. calculquebec.ca/en/) and Compute Canada (www.computecanada.ca). The authors acknowledge the material support of NVIDIA in the form of computational resources."
        },
        {
            "heading": "A WEIGHT DISTRIBUTION CONVERGENCE",
            "text": "We start by giving an intuitive outline of our proof of Gaussian convergence of weight changes. As for any Gaussian convergence, we\u2019ll end up using the central limit theorem (although an unusual version of it) w.r.t. the dataset size. With this in mind, the steps are:\nSetup First, we decide how we take the large data and network limits. We have a vector of weight changes whose size grows to infinity. To discuss what it converges to, we consider this vector to be a part of a stochastic process (so, an already infinite-dimensional vector). We then consider the convergence of this stochastic process to some limiting process (in our case, to a Gaussian process). This allows us to approximate a sample of weight changes as a sample from that limiting process.\nLemma 1 Next, we show that the inverse matrix in Eq. (15) used in the weight change expression behaves like an identity matrix. If we just took the limit w.r.t. the network size, this lemma would be trivial since off-diagonal elements are products of independent vectors. But we also increase the dataset size (and hence the matrix itself), so we need to be more careful. This is where we get (most of) the requirements for how the dataset size should scale with network size.\nLemma 2 shows that the deviations from identity resulting from Lemma 1 don\u2019t affect the final weight change expression when we take the limits.\nLemma 3 shows that if you replace that inverse matrix with an identity, you can apply the exchangeable central limit theorem to the weight change expression. At this point, it\u2019s applied for any finite slice of the stochastic process we defined. The exchangeable central limit theorem has much more restrictive moment conditions than the standard CLT, since it doesn\u2019t require independence, so we have to carefully check all of them.\nTheorem 1 Finally, we combine the lemmas and show that any finite slice of that stochastic process converges to a Gaussian random variable. Due to the properties of Gaussian processes, we can conclude that the whole process converges to a Gaussian process.\nSetup We want to show that the weight change looks Gaussian in the large width limit. Since the number of weights goes to infinity, \u201clooks Gaussian\u201d should translate to convergence to a Gaussian process. Our approach will be similar to Matthews et al. (2018), which showed convergence of wide networks to Gaussian processes. However, they worked with finite-dimensional inputs and multiple hidden layers; for our purposes we\u2019ll need to have an infinite-dimensional input and a single layer.\nWe\u2019ll be working with countable stochastic processes defined over Z>0. We need three stochastic processes: the input data X , the initial weights W 0 and the \u201cteacher\u201d weights W \u2217.\nFor width D, the label yD and the model at initialization y\u03020D are defined as\nyD = D\u2211 d=1 1\u221a D Xd W \u2217 d , y\u0302 0 D = D\u2211 d=1 1\u221a D Xd W 0 d . (17)\nFor N sample points of dimension D, we stack them into an N \u00d7 D matrix XD, and denote the D-dimensional samples of weights as w\u2217D, w0D. The weight change we\u2019re interested in is then\n\u03beD = 1\u221a D\n( XD )\u22a4 ( XD HD (XD)\u22a4 )\u22121 XD(w\u2217D \u2212w0D) (18)\n= ( XD )\u22a4 ( XD HD (XD)\u22a4 )\u22121 (yD \u2212 y0D) . (19)\nNote the explicit 1/ \u221a D scaling we omitted in the main text for convenience and N -dimensional vectors of labels yD, y0D. Here H D is a diagonal matrix with hdd = \u22072\u03d5\u22121(W 0d / \u221a D) (we dropped the \u03d5\u22121 subscript used in the main text for readability).\nInvertibility assumption. Throughout the proofs we assume that XD HD (XD)\u22a4 is almost surely invertible. This is a very mild assumption, although it can break if Xid are sampled from a discrete\ndistribution, if Xd have strong correlations along d or if the initial weights are not almost surely non-zero.\n\u03beD is a D-dimensional vector, but we understand it as a subset of the following stochastic process: \u039eD = (X) \u22a4 ( XD HD (XD)\u22a4 )\u22121\n(yD \u2212 y0D) . (20) This infinite-width expansion is similar to the one used in Matthews et al. (2018). It\u2019s a welldefined stochastic process since N is finite, meaning that \u039eD is a weighted sum of N stochastic processes. However, it formalizes convergence of a finite-dimensional vector \u03beD to something infinite-dimensional. That is, we\u2019ll work with convergence of the process \u039eD to a limit process \u039e in distribution.3\nFirst, we show that an appropriately scaled matrix inside the inverse behaves like an identity: Lemma 1. Assume that the input data points Xd have zero mean and unit variance. Additionally assume that uniformly for any d, there exists a constant c such that\n\u221e\u2211 d\u2032=1 (EXd Xd\u2032)2 \u2264 c , \u221e\u2211 d\u2032=1 \u2223\u2223Cov (X2d , X2d\u2032)\u2223\u2223 \u2264 c . (21) Also for i.i.d. initial weights, define moments (assuming they exist) of hdd = \u03d5\u22121(W 0d / \u221a D) as\nh1(D) = E\u22072\u03d5\u22121(W 0d / \u221a D) , h2(D) = Var ( \u22072\u03d5\u22121(W 0d / \u221a D) ) . (22)\nThen for N,D large enough (so the r.h.s. is smaller than 4),\u2225\u2225\u2225\u2225\u2225 ( 1 Dh1(D) XD HD (XD)\u22a4 )\u22121 \u2212 IN \u2225\u2225\u2225\u2225\u2225 2 \u2264 8 \u221a N4 D c+ 1 p ( 1 + h2 h21 ) (23)\nwith probability at least 1\u2212 p.\nProof. Denote\nA \u2261 1 Dh1 XD HD (XD)\u22a4 , Aij = 1 Dh1 D\u2211 d=1 Xid X j d hdd , (24)\ndropping the explicit D-dependence in h1 and h2 for convenience.\nSince all Xd are independent from W 0d , EAij = \u03b4ij . Variance can be bounded. For i = j,\nVar(Aii) = 1\nD2 h21 D\u2211 d=1 Var((Xid)2 hdd) + 1 D2 h21 D\u2211 d,d\u2032 \u0338=d Cov((Xid) 2 hdd, (X i d\u2032) 2 hd\u2032d\u2032) (25)\n= D\u2211 d=1 h2 + (h 2 1 + h2)Var((Xid)2) D2 h21 + D\u2211 d,d\u2032 \u0338=d\nh21 Cov ( (Xid) 2, (Xid\u2032) 2 )\nD2 h21 (26)\n\u2264 h2(1 + Var((X i d) 2))\nDh21 +\nc D \u2264 c D\n( 1 +\nh2 h21\n) +\nh2 Dh21 \u2264 c+ 1 D\n( 1 +\nh2 h21\n) , (27)\nwhere in the last line we used the second part of Eq. (21).\nFor i \u0338= j,\nVar(Aij) = 1\nD2 h21 D\u2211 d=1 Var(XidX j d hdd) + 1 D2 h21 D\u2211 d,d\u2032 \u0338=d Cov(Xid X j d hdd, X i d\u2032 X j d\u2032 hd\u2032d\u2032) (28)\n= D\u2211 d=1\n(h21 + h2) ( Var(Xid) )2 D2 h21 + D\u2211 d,d\u2032 \u0338=d h21 ( Cov(Xid, X i d\u2032) )2 D2 h21 \u2264 c D ( 1 + h2 h21 ) , (29)\n3See Eq. 10 in Matthews et al. (2018) for the discussion on what \u201din distribution\u201d means for stochastic processes.\nnow using the first part of Eq. (21) for the last inequality.\nUsing Chebyshev\u2019s inequality and the union bound, we can bound\nP ( max ij |Aij \u2212 \u03b4ij | \u2265 \u03f5 ) \u2264 N 2 \u03f52 c+ 1 D ( 1 + h2 h21 ) \u2261 p . (30)\nTherefore,\n\u2225A\u2212 IN\u22252 \u2264 \u2225A\u2212 IN\u2225F \u2264 \u03f5N =\n\u221a N4\nD\nc+ 1\np\n( 1 +\nh2 h21\n) (31)\nwith probability at least 1\u2212 p. By Lemma 4.1.5 of Vershynin (2018), if \u2225A \u2212 IN\u22252 \u2264 \u03f5N then all singular values \u03c3i of A lie between (1 \u2212 \u03f5N)2 and (1 + \u03f5N)2 for N large enough so \u03f5N < 1. Therefore, we also have a bound on the singular values of A\u22121. Additionally taking parameters big enough for \u03f5N \u2264 0.5, we have\n\u2225A\u22121 \u2212 IN\u22252 = max i\n( 1 \u03c3i \u2212 1 ) \u2264 1 (1\u2212 \u03f5N)2 \u2212 1 = 2\u03f5N \u2212 \u03f5 2N2 (1\u2212 \u03f5N)2 \u2264 8 \u03f5N , (32)\nwhich completes the proof.\nNow we need to show that the inverse behaves like an identity in the overall expression as well:\nLemma 2. In the assumptions of Lemma 1, additionally assume that W \u2217d and W 0d have finite mean and variance independent of d and D, that h2(D)/h1(D)2 is independent of D, and\u2211\u221e\nd\u2032=1 EXd Xd\u2032 \u2264 c uniformly over d. Then for any scalar weights \u03b1i and a finite index set A \u2282 Z>0, as D, N \u2192 \u221e with N = o(D1/(5+\u03b4)) for any \u03b4 > 0,\n1\u221a N \u2211 p\u2208A \u03b1p (X: p) \u22a4 ( A\u22121 \u2212 IN ) (yD \u2212 y0D) \u2192p 0 . (33)\nRemark. For both cross-entropy and p-norm potentials, \u22072\u03d5\u22121(W/ \u221a D) is a power of W/ \u221a D, and so h2(D)/h1(D)2 depends only on the moments of W , but not on D, therefore satisfying the assumption.\nProof. Again denoting the matrix inside the inverse as A (see Eq. (24)), we first bound the following via Cauchy-Schwarz:\u2223\u2223\u2223\u2223\u2223\u2211\ni\u2208A \u03b1i (X: i)\n\u22a4 ( A\u22121 \u2212 IN ) (yD \u2212 y0D) \u2223\u2223\u2223\u2223\u2223 2 \u2264 \u2225\u2225\u2225\u2225\u2225\u2211 i\u2208A \u03b1i (X: i) \u2225\u2225\u2225\u2225\u2225 2\n2 \u2225\u2225yD \u2212 y0D\u2225\u222522 \u2225\u2225A\u22121 \u2212 IN\u2225\u222522 . (34)\nFirst term. Using Cauchy-Schwarz and that Xp variances are one, and also that \u03b1, A are fixed,\nE \u2225\u2225\u2225\u2225\u2225\u2225 \u2211 p\u2208A \u03b1p (X: p) \u2225\u2225\u2225\u2225\u2225\u2225 2\n2\n= N E \u2211 p\u2208A \u03b1p X 1 p 2 \u2264 N E \u2211 p\u2208A \u03b12p \u2211 p\u2208A (X1p) 2  = O(N) . (35) Second term. Using that weights and inputs are independent, and that the sum of input data covariances is bounded by c by our assumption,\nE \u2225\u2225yD \u2212 y0D\u2225\u222522 = ND E\n( D\u2211\nd=1\nX1d (W \u2217 d \u2212W 0d )\n)2 (36)\n= N\nD D\u2211 d,d\u2032 \u0338=d (E (W \u2217d \u2212W 0d ))2 E (X1d X1d\u2032) (37)\n+ N\nD D\u2211 d=1 ( (E (W \u2217d \u2212W 0d ))2 + Var((W \u2217d \u2212W 0d )) ) E (X1d)2 (38)\n\u2264 N c (E (W \u2217d \u2212W 0d ))2 +N Var((W \u2217d \u2212W 0d )) = O(N) . (39)\nThird term. Here we will use the probabilistic bound from Lemma 1.\nOverall, for any \u03b4 > 0,\nP  1 N \u2223\u2223\u2223\u2223\u2223\u2211 i\u2208A \u03b1i (X: i) \u22a4 ( A\u22121 \u2212 IN ) (yD \u2212 y0D) \u2223\u2223\u2223\u2223\u2223 2 > \u03f5  (40) \u2264 P\n\u2225\u2225\u2225\u2225\u2225\u2211 i\u2208A \u03b1i (X: i) \u2225\u2225\u2225\u2225\u2225 2\n2\n> N1+ \u03b4 2 ; \u2225\u2225yD \u2212 y0D\u2225\u222522 > N1+ \u03b42 ; \u2225\u2225A\u22121 \u2212 IN\u2225\u222522 > \u03f5N1+\u03b4  (41) \u2264 E \u2225\u2225\u2225\u2211p\u2208A \u03b1p (X: p)\u2225\u2225\u22252 2\nN1+ \u03b4 2\n+ E \u2225\u2225yD \u2212 y0D\u2225\u222522 N1+ \u03b4 2 + P (\u2225\u2225A\u22121 \u2212 IN\u2225\u222522 > \u03f5N1+\u03b4 ) (42)\n\u2264 O ( 1\nN\u03b4/2\n) + 64 N5+\u03b4\nD\nc+ 1\n\u03f5\n( 1 +\nh2 h21\n) (43)\nwhere we used Eq. (34) in the first line, the union bound with Markov\u2019s inequality in the second and the bounds on all terms in the third one (for N large enough so \u03f5/N1+\u03b4 < 2).\nAs long as N5+\u03b4 = o(D) and h2/h21 is constant, the probability goes to zero with N,D.\nThe previous result suggests we can replace the inverse matrix in \u039eD (Eq. (20)) with an identity. This will require an additional step, but now we can show that with the identity, for any finite index set A of points in \u039eD and arbitrary weights \u03b1, the A, \u03b1-projection of \u039eD solution converges to a Gaussian.\nLemma 3. Consider a finite index set A and an associated vector of weights \u03b1. If in addition to condition of Lemmas 1 and 2 we assume weights have a finite 8th moment, Xip are uniformly bounded, and that Xip(yD \u2212 y0D)i (for point i and index p) converges in probability to a random variable. Then the A, \u03b1-projection defined as,\n1\u221a N N\u2211 i=1 RNi, RNi = \u2211 p\u2208A \u03b1p X i p(yD \u2212 y0D)i , (44)\nconverges to N (0, \u03c32W \u03b1\u22a4\u03a3A\u03b1) in distribution, where (\u03a3A)pp\u2032 = EXipXip\u2032 and \u03c32W = E (W \u2217d \u2212 W 0d ) 2, as long as D increases monotonically with N such that N/D = o(1).\nProof. We assumed that D is a monotonically increasing function of N . Therefore, we get a triangular array of RNi with infinitely exchangeable rows. That is, for each N we have a process RNi with i = 1, . . . , such that any permutation of indices i doesn\u2019t change the joint distribution. This is due to joint weight dependence among labels. Sums of infinitely exchangeable sequences behave similarly to sums of independent sequences when it comes to limiting distributions. In particular, we will use the classic central limit theorem result of Blum et al. (1958).\nTo apply it, we need to compute several moments of RNi.\nFirst moment. Since E(W \u2217d \u2212W 0d ) = 0, we have ERNi = 0. Second moment. Again using E (W \u2217d \u2212W 0d ) = 0 and the fact that all weights are i.i.d., so E (W \u2217d \u2212 W 0d )(W \u2217 d\u2032 \u2212W 0d\u2032) = \u03b4ij , we get\n\u03c32R = ER2Ni = 1\nD \u2211 pp\u2032dd\u2032 \u03b1p \u03b1p\u2032 E XipXip\u2032XidXid\u2032(W \u2217d \u2212W 0d )(W \u2217d\u2032 \u2212W 0d\u2032) (45)\n= 1\nD \u2211 pp\u2032d \u03b1p \u03b1p\u2032 E XipXip\u2032(Xid)2(W \u2217d \u2212W 0d )2 . (46)\nThis term is O(1) due to the 1/D scaling. Denoting \u03c32W = E (W \u2217d \u2212 W 0d )2 and \u03b3Dpp\u2032 =\u2211 d E XipXip\u2032(Xid)2/D (and the corresponding matrix \u0393D), we can re-write the variance as\n\u03c32R(D) = \u03c3 2 W \u03b1 \u22a4\u0393D\u03b1 . (47)\nAbsolute 3rd and 4th moments. For convenience, we first find the 4th moment. Since the inputs are bounded and independent from the weights, and dropping the summation over all pi and di for readability,\nER4Nj = 1\nD2 \u2211( 4\u220f i=1 \u03b1pi ) E ( 4\u220f i=1 Xjpi X j di ) E ( 4\u220f i=1 (W \u2217di \u2212W 0 di) ) = O(1) . (48)\nThis follows from E (\u220f4\ni=1 X j pi X j di\n) = O(1) due to bounded inputs, and due to centered and i.i.d.\nweights E (\u220f4\ni=1(W \u2217 di \u2212W 0di) ) = 0 if any di differs from the rest (i.e. only O(D2) out of D4 are\nnon-zero; these are the ones with d1 = d2 and d3 = d4 and permutations of those conditions).\nFor the absolute 3rd moment, we can use Cauchy-Schwarz to find its scaling: E |RNj |3 = E |RNj | \u00b7 |RNj |2 \u2264 \u221a E |RNj |2 E |RNj |4 = O(1) , (49)\nsince both terms in the square root are O(1).\nCovariance. For two different points i and j, defining the covariance \u03c3pd = EXipXid,\nERNiRNj = 1\nD \u2211 pp\u2032dd\u2032 \u03b1p \u03b1p\u2032 E XipX j p\u2032X i dX j d\u2032(W \u2217 d \u2212W 0d )(W \u2217d\u2032 \u2212W 0d\u2032) (50)\n= 1\nD \u2211 pp\u2032d \u03b1p \u03b1p\u2032 E XipX j p\u2032X i dX j d(W \u2217 d \u2212W 0d )2 = \u03c32W D \u2211 pp\u2032d \u03b1p\u03b1p\u2032\u03c3pd\u03c3p\u2032d . (51)\nSince we required \u2211\nd \u03c3 2 pd \u2264 c, \u2211 d \u03c3pd\u03c3p\u2032d \u2264 \u221a ( \u2211 d \u03c3 2 pd)( \u2211 d \u03c3 2 p\u2032d) \u2264 c by Cauchy-Schwarz.\nThus, ERNiRNj = O(1/D) = o(1/N).\nCovariance of R2Ni. Again, take i \u0338= j and adapt the calculation of the 4th moment:\nER2NiR2Nj= 1\nD2 \u2211( 4\u220f k=1 \u03b1pk ) E ( 2\u220f k=1 Xipk X i dk )( 4\u220f k=3 Xjpk X j dk )( 4\u220f k=1 (W \u2217dk \u2212W 0 dk ) ) .\n(52)\nDue to i.i.d. weights the whole sum has only O(D2) terms and therefore the whole expression is O(1). However, we require a more precise control over the sum.\nThe summands split up into three cases (with a potential O(D) overlap): (1) d1 = d2 = d3 = d4, (2) d1 = d3, d2 = d4 or d1 = d4, d2 = d3, and (3) d1 = d2, d3 = d4. The first case has only D terms, each O(1), in the sum over dk. The second case is a bit more tricky. Assuming d1 = d3 w.l.o.g., by Cauchy-Schwarz and by the assumptions on correlations (note that the weights are i.i.d.,\nso we can take the expectation w.r.t. the weights out of the sum),\u2211 d1,d2 E ( Xip1X i p2X i d1X i d2 ) ( Xjp3X j p4X j d1 Xjd2 ) E ( (W \u2217d1 \u2212W 0 d1) 2(W \u2217d2 \u2212W 0 d2) 2 )\n(53)\n\u2264 \u2211 d1,d2 \u221a E ( Xip1X i p2 )2\u221aE (Xip3Xip4)2E (Xid1Xid2)2 E ((W \u2217d1 \u2212W 0d1)2(W \u2217d2 \u2212W 0d2)2) (54)\n\u2264 cD \u221a E ( Xip1X i p2 )2\u221aE (Xip3Xip4)2E ((W \u22171 \u2212W 01 )2(W \u22172 \u2212W 02 )2) . (55) Since other terms in the full sum are O(1), the contribution of the second case is O(D).\nThe last case is what we had for covariance. Since d1 = d2 and d3 = d4,\u2211 d1,d3 E ( Xip1X i p2(X i d1) 2 ) ( Xjp3X j p4(Xd3) 2 ) E ( (W \u2217d1 \u2212W 0 d1) 2(W \u2217d3 \u2212W 0 d3) 2 )\n(56)\n= D(D \u2212 1)\u03b3Dp1p2\u03b3 D p3p4\u03c3 4 W +D\u03b3 D p1p2\u03b3 D p3p4E ( W \u22171 \u2212W 01 )4 . (57)\nNow we can combine all three terms, using that the weights have finite second and 4th moments. We will also use that for d1 = d2 and d3 = d4, we can ignore the second term with E ( W \u22171 \u2212W 01 )4 since it\u2019s an O(D) contribution to an O(D2) sum. Therefore,\nER2NiR2Nj = 1\nD2 \u2211 pk\n( 4\u220f\nk=1\n\u03b1pk )( \u03c34WD(D \u2212 1)\u03b3Dp1p2\u03b3 D p3p4 +O(D) ) (58)\n= ( \u03c32W \u03b1 \u22a4\u0393D\u03b1 )2 +O\n( 1\nD\n) = \u03c34R(D) +O ( 1\nD\n) . (59)\nAsymptotic of the second moment. To find the limit of the variances, we need to compute\nE R\u03032Ni = 1 D E \u2211 pp\u2032d \u03b1p\u03b1p\u2032 X i pX i p\u2032(X i d) 2(W \u2217d \u2212W 0d )2 . (60)\nThis variable is uniformly integrable since its 4th moment exists (almost identical computation to R2Ni) and is uniformly bounded for all N (using Billingsley (1999), immediately after Theorem 3.5 for \u03f5 = 2). Therefore, limN E R\u03032Ni = E limN R\u03032Ni (Theorem 3.5 in Billingsley (1999)). The latter is easy to compute, since\nE \u2211 d (Xid) 2(W \u2217d \u2212W 0d )2 = D\u03c32W , (61)\nVar ( E \u2211 d (Xid) 2(W \u2217d \u2212W 0d )2 ) = \u2211 d,d\u2032 Cov ( (Xid) 2(W \u2217d \u2212W 0d )2, (Xid\u2032)2(W \u2217d\u2032 \u2212W 0d\u2032)2 ) (62)\n= O(D)+ \u2211\nd,d\u2032 \u0338=d\nCov ( (Xid) 2(W \u2217d \u2212W 0d )2, (Xid\u2032)2(W \u2217d\u2032 \u2212W 0d\u2032)2 ) (63)\n= O(D)+\u03c32W \u2211\nd,d\u2032 \u0338=d\nCov ( (Xid) 2, (Xid\u2032) 2 ) = O(D) , (64)\nwhere in the last line we again used the assumption on summable covariances of (Xid) 2. Therefore, by Chebyshev\u2019s inequality 1D \u2211 d(X i d)\n2(W \u2217d \u2212W 0d )2 (note the returned 1/D) converges to \u03c32W in probability. Therefore, by Slutsky\u2019s theorem R\u0303 2 Ni converges to \u03c3 2 W \u2211 pp\u2032 \u03b1p\u03b1p\u2032 X i pX i p\u2032 . Since the expectation of XipX i p\u2032 is \u03c3pp\u2032 , we can denote \u03a3A as a matrix with entries \u03c3 2 pp\u2032 , such that\nE R\u03032Ni = \u03c32W \u03b1\u22a4\u0393D\u03b1 \u2192 \u03c32W \u03b1\u22a4\u03a3A\u03b1 . (65)\nSince we assumed that RNi converges to a random variable, and R2Ni is uniformly integrable (since its 2nd moment exists), we can claim that ER2Ni \u2192 \u03c32W \u03b1\u22a4\u03a3A\u03b1 = ER2.\nFor R2NiR 2 Nj , we can repeat the calculation. It\u2019s also uniformly integrable since its 8th moment exists: E \u220f8\nk=1(W \u2217 dk \u2212W 0dk) leaves out D 4 terms out of D8, the weights have a finite 8th moment,\nand the inputs are bounded. Therefore, ER2NiR2Nj \u2192 \u03c34W ( \u03b1\u22a4\u03a3A\u03b1 )2 . (66)\nCLT for exchangeable arrays. Assume that \u03c32R(D) = O(1) (defined in Eq. (47)) has a non-zero limit, and D is large enough for \u03c32R(D) > 0. Then GNi = RNi/\u03c3R(D) is zero mean, unit variance and has a finite third moment. It also satisfies the following:\nEGN1GN2 = O ( 1\nD\n) = o ( 1\nN\n) , (67)\nlim N\nEG2N1G2N2 = 1 , (68)\nE |GN1|3 = O(1) = o( \u221a N) . (69)\nTherefore, GNi satisfies all conditions of Theorem 2 in Blum et al. (1958), and\n1\u221a N N\u2211 i=1 GNi \u2212\u2192d N (0, 1). (70)\nTherefore, we also have 1\u221a N N\u2211 i=1 RNi \u2212\u2192d N (0, \u03c32W \u03b1\u22a4\u03a3A\u03b1). (71)\n(Note that in the case \u03b1\u22a4\u03a3A\u03b1 = 0, we don\u2019t need to apply CLT since the sum converges to zero in probability by Chebyshev\u2019s inequality.)\nNow we\u2019re ready to prove the main result (see the more informal statement in the main text, Theorem 1). Theorem 1. In the assumptions of Lemmas 1 to 3, the scaled sequence of stochastic processes \u039eD = (X) \u22a4 ( XD HD (XD)\u22a4 )\u22121 (yD \u2212 y0D) converges to a Gaussian process:\n\u039e\u0303D = DE\u22072\u03d5\u22121(W 0d / \u221a D)\u221a\nN \u039eD \u2192d GP(0, \u03c32W\u03a3) , (72)\nwhere \u03a3 is the covariance matrix over Xd and \u03c32W = E (W \u2217d \u2212W 0d )2.\nRemark. Lemma 1 used the following correlation assumptions: \u2211\u221e\nd\u2032=1 (EXd Xd\u2032) 2 \u2264 c and\u2211\u221e\nd\u2032=1 \u2223\u2223Cov (X2d , X2d\u2032)\u2223\u2223 \u2264 c. Lemma 2 also used \u2211\u221ed\u2032=1 EXd Xd\u2032 \u2264 c. In terms of Theorem 1 in the main text, the second condition corresponds to c\u2032ij . The first and the third conditions are joined into the cij condition for convenience since it implies both conditions used here.\nProof. First, for an A, \u03b1-projection of the scaled stochastic process, we split the solution into two terms (A is defined in Eq. (24)):\u2211\np\u2208A \u03b1p \u039e\u0303\nD p = 1\u221a N \u2211 p\u2208A \u03b1p (X: p) \u22a4 A\u22121(yD \u2212 y0D) (73)\n= 1\u221a N \u2211 p\u2208A \u03b1p (X: p) \u22a4 (yD \u2212 y0D) + 1\u221a N \u2211 p\u2208A \u03b1p (X: p) \u22a4 ( A\u22121 \u2212 IN ) (yD \u2212 y0D) . (74)\nWe proved convergence of the first term to a Gaussian in Lemma 3. By Lemma 2, the second term converges to zero in probability. By Slutsky\u2019s theorem, since the first term converges to a Gaussian in distribution, and the second one to zero in probability, the whole expression converges to a Gaussian.\nConvergence to a Gaussian process. We\u2019ve shown that for any scalar weights \u03b1 and a finite index set A \u2282 Z>0, the projection of the sequence of the scaled stochastic processes \u039e\u0303D w.r.t. A , \u03b1 converges to a Gaussian in distribution (with parameters found in Lemma 3). Therefore, \u039e\u0303D converges to a Gaussian process in distribution by Lemma 6 of Matthews et al. (2018).\nThe teacher weights setup with yD = \u2211D\nd=1 1\u221a D Xd W \u2217 d simplifies the proofs, but it is not necessary.\nHere we provide a similar result for a generic y without a detailed proof: Theorem 2. In the assumptions of Lemmas 1 to 3, additionally assume that labels yi are zeromean, bounded, and independent of D, Xid and W 0 d , and also that W 0 d are zero-mean. Then the\nscaled sequence of stochastic processes \u039eD = (X)\u22a4 ( XD HD (XD)\u22a4 )\u22121 (y\u2212 y0D) converges to a Gaussian process:\nDE\u22072\u03d5\u22121(W 0d / \u221a D)\u221a\nN \u039eD \u2192d GP(0, (\u03c32y + \u03c32W 0)\u03a3) , (75)\nwhere \u03a3 is the covariance matrix over Xd and \u03c32W 0 = E (W 0 d ) 2, \u03c32y = E y2.\nProof. The label setup doesn\u2019t affect Lemma 1. For Lemmas 2 and 3, due to the independence assumption for the labels and the weights, the proofs of the lemmas split into moment calculations for W 0d and y. The former is identical to the original calculation for W \u2217 d \u2212 W 0d , since the weights are zero-mean. The latter doesn\u2019t involve sums over repeating coordinates, simplifying moment computations. The overlap is similar to the computation for W 0d . With this change in the lemmas, the rest of the proofs is the same as for Theorem 1.\nA.1 RICH AND LAZY REGIMES\nAssuming for simplicity that all initial weights are the same and positive (w0d = w 0 = \u03b1/\n\u221a D) and\nthat \u03c32\u03bb is equal to 1, then for \u03be \u223c N (0, 1), Eq. (13) becomes\n\u2207\u03d5(w\u221e) \u2248 \u2207\u03d5(w0) + \u221a N\nD\u22072\u03d5\u22121(w0) \u03be . (76)\nFor p-norms, \u03d5(w) = 1pw p;\u2207\u03d5(w0) = (w0)p\u22121;\u22072\u03d5\u22121(w0) = 1p\u22121 (w 0)2\u2212p. Therefore,\n\u2207\u03d5(w\u221e) \u2248 \u03b1 p\u22121\nD(p\u22121)/2 +\n(p\u2212 1) \u221a N\u03b1p\u22122\nDp/2 \u03be =\n\u03b1p\u22122\nDp/2\n( \u03b1 \u221a D + (p\u2212 1) \u221a N \u03be ) . (77)\nAs long as \u03b1 \u226b \u221a N/D, the initial weights (first term) will be much larger than the weight change (second term), and we will be in the lazy regime. This additionally sets a limit on the dataset size. Put another way, for \u03b1 = 1 and the standard \u201clazy\u201d initialization with O( \u221a 1/D) weights, the number of data points N has to be much smaller than D for the linearization to make sense (in addition to the more technical assumption on the N scaling in Theorem 1).\nFor negative entropy, \u03d5(w) = w logw;\u2207\u03d5(w0) = 1 + logw0;\u22072\u03d5\u22121(w0) = w0. Therefore,\n\u2207\u03d5(w\u221e) \u2248 1 + log\u03b1\u2212 1 2 logD +\n\u221a N\n\u03b1 \u221a D \u03be . (78)\nUnlike for p-norms, the initial weights\u2019 contribution grows to infinity even as \u03b1 tends to zero. As a result, we require only \u03b1 \u226b \u221a N/(D logD) to keep learning in the lazy regime. However, for \u03b1 = 1/ \u221a D (i.e. O(1/D) weights, the standard rich regime scaling) negative entropy still results in large weight changes since N grows with D.\nOverall, different potentials result in different asymptotic boundaries of the lazy regime. Negative entropy allows smaller initialization schemes while maintaining the lazy regime."
        },
        {
            "heading": "B EXPERIMENTAL DETAILS",
            "text": "B.1 LINEAR REGRESSION\nFor experiments in Section 4.1, we optimized models using mirror descent, where the gradients were computed in the standard SGD way with momentum of 0.9 and no weight decay (with mixed precision). Learning rate was changed during learning according to the cosine annealing schedule. The initial learning rate was chosen on a single seed via grid search over 16 points (log10-space\nfrom 1e-7 to 1e-1) and 100 epochs. If the minimum MSE loss was larger than 1e-3, we increased the number of epochs by 100 and repeated the grid search (up to 500 epochs; all grid searches converged to MSE smaller than 1e-3). The widths were chosen as 10 points in a log10-space from 1e2 to 2e4. The full results are shown in Fig. 6.\nB.2 POTENTIAL ROBUSTNESS\nIn Section 4.2, both figures in Fig. 3B were obtained in the following way. For D = 100 and 30 seeds, we sampled a 1000-dimensional weight sample w0 from either N (0, 1/D) or a log-normal with \u00b5 = \u2212 log \u221a 2D, \u03c32 = log 2 multiplied by a \u00b11 with equal probability (so it has the approximately the same variance as the Gaussian). We then computed the final weights w for the potential \u03d5 as w = \u2207\u03d5\u22121 ( \u2207\u03d5(w0) + 0.1\u2207\u03d5(1/ \u221a D)N (0, 1) ) , so the Gaussian change had a smaller magnitude than a weight with a standard deviation of 1/ \u221a D.\nB.3 FINETUNING\nIn Section 4.3, for N chosen datapoints, the dataset was randomly sampled from the 50k available points for each seed (out of 10). Networks were trained on the cross-entropy loss using stochastic mirror descent; the gradients had momentum of 0.9 but no weight decay, batch size of 256. Learning rate was changed during learning according to the cosine annealing schedule. The initial learning rate was chosen on a log10 grid of 5 points from 1e-5 to 1e-1. The initial number of epochs was 30, but increased by 30 up to 4 times if the accuracy was less than 100%. All resulting accuracies from this search were \u2265 99%. (Note that this is the desired behavior, since we\u2019re testing the models that fully learned the train set.) The dataset was not augmented; all images were center cropped to have a resolution of 224 pixels and normalized by the standard ImageNet mean/variance. We trained networks with mixed precision and FFCV Leclerc et al. (2022); for the latter we had to convert ImageNet to the FFCV format (see the accompanying code).\nThe magnitude of weight changes was usually smaller than 0.2, although for 2-norm for ResNet18/34 and CORnet-S, and 3-norm for EfficientNets b0,1,3 it reached 0.3-0.5. For negative entropy, the magnitude always stayed smaller than 0.02.\nWe have conducted experiments with recurrent neural networks trained on row-wise sequential MNIST LeCun et al. (2010). The networks were trained on the train set, and then finetuned on a subset of the test set (same procedure as for deep networks) for N = D0.5 (the number of weights scales quadratically with the hidden size, so N equals the number of hidden units). The Gaussian fits, and non-Gaussianity of weight changes measured with a wrong potential were remarkably similar to our theoretical predictions and behavior of deep networks (Fig. 7A). Note that for networks trained with p-norms but evaluated with negative entropy, if the weight flips its sign during training we can immediately conclude that negative entropy was not used for training. Because we\u2019re finetuning already trained networks, most weights don\u2019t flip signs. We plot the distribution of changes in the dual space for all weights for simplicity (we can do it in the code since the dual weights are signed).\nWe also evaluated a self-supervised loss from SimSiam Chen & He (2021): for a pre-trained ResNet50 (see the full architecture in Chen & He (2021)), we finetuned it with the same selfsupervised loss l = 0.5 z\u22a41 p2 + 0.5 z \u22a4 2 p1 for positive pairs 1/2 passed through decoders with or without stop-gradient (z and p). We trained the network for 50 epochs and used the learning rate that minimized the loss the best, since we don\u2019t evaluate accuracy. We evaluated weight changes in the encoder (a ResNet50). The results were very similar to networks finetuned with a supervised loss, confirming our results for supervised experiments (Fig. 7B).\nB.4 COMPARISON TO SYNAPTIC DATA\nIn Section 4.4, we used the data from Dorkenwald et al. (2022). We used the the accompanying dataset, specifically the \u201cspine vol um3\u201d data available here. The dataset contained 1961 spine volume recordings.\nFor negative entropy fitting (Fig. 5A), we sampled the initial weights as 10\u22121.42 with probability p = 0.77 and 10\u22120.77 with p = 0.23. In the dual space, we added a centered Gaussian with \u03c3 = 10\u22120.23. The means and probabilities exactly match the ones fitted in Dorkenwald et al. (2022) (Table 2); the standard deviations were fitted as 10\u22120.24 and 10\u22120.22; we used a log-average since the standard deviations were very similar and we expect the Gaussian parameters to be the same across weights.\nFor 3-norm (Fig. 5B), we obtained a qualitative fit. With equal probability, the initial weights were either 1e-2 or log-normal with \u00b5 = \u22122.7 and \u03c3 = 0.9. In the dual space, the Gaussian change was zero-mean with \u03c3 = 0.0012. A small part of the final weights became negative with this sampling procedure, which we treated as synapses that were reduced to zero weight during learning and therefore we excluded them from the plots.\nThe 3-norm approximately fits the data because for a small constant initial weight c, the final weight will be approximately log-normal: log ( \u2207\u03d5\u22121(\u2207\u03d5(c) + \u03be) ) \u2248 log ( c+ \u03be/\u22072\u03d5(c) ) \u2248 \u03be/\u22072\u03d5(c). The second part of the mixture is already log-normal and is barely changed by the Gaussian change, resulting in a mixture of two (approximately) log-normals."
        },
        {
            "heading": "C NOTES ON MIRROR DESCENT",
            "text": "For negative entropy, we assumed that the weights do not change signs. This way, |w| log |w| is strictly convex for either w > 0 or w < 0 and the gradient of potential is invertible. This is implied by the exponentiated gradient update Eq. (7), which indeed preserves the signs of the weights.\nPowerpropagation Schwarz et al. (2021) reparametrized network\u2019s weights w as w = \u03b8 |\u03b8|\u03b1\u22121, such that gradient descent is done over \u03b8. The original weights are therefore updated for a loss L as (assuming all terms remain positive for simplicity):\nwt+1 = ( \u03b8t \u2212 \u03b7 \u2202 L\n\u2202wt \u2202wt \u2202 \u03b8t\n)\u03b1 \u2248 (\u03b8t)\u03b1 \u2212 \u03b1 (\u03b8t)\u03b1\u22121 \u03b7 \u2202 L\n\u2202wt \u2202wt \u2202 \u03b8t (79)\n= (\u03b8t)\u03b1 \u2212 \u03b12 (\u03b8t)2\u03b1\u22122 \u03b7 \u2202 L \u2202wt = wt \u2212 \u03b12 \u03b7 (wt) 2(\u03b1\u22121) \u03b1 \u2202 L \u2202wt , (80)\nwhere we linearized the update in the first line and then used the relation between \u03b8 and w. If we treat (wt) 2(\u03b1\u22121) \u03b1 as the Hessian of \u03d5\u22121 in mirror descent (see Eq. (14)), powerpropagation can be viewed as an approximation to mirror descent with p-norms, where p = 2/\u03b1. For \u03b1 = 2, we can also treat powerpropagation as an approximation to exponentiated gradient (Eq. (7))."
        }
    ],
    "title": "SYNAPTIC WEIGHT DISTRIBUTIONS DEPEND",
    "year": 2024
}