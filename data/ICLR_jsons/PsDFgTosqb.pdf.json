{
    "abstractText": "Bilevel programs (BPs) find a wide range of applications in fields such as energy, transportation, and machine learning. As compared to BPs with continuous (linear/convex) optimization problems in both levels, the BPs with discrete decision variables have received much less attention, largely due to the ensuing computational intractability and the incapability of gradient-based algorithms for handling discrete optimization formulations. In this paper, we develop deep learning techniques to address this challenge. Specifically, we consider a BP with binary tender, wherein the upper and lower levels are linked via binary variables. We train a neural network to approximate the optimal value of the lower-level problem, as a function of the binary tender. Then, we obtain a single-level reformulation of the BP through a mixed-integer representation of the value function. Furthermore, we conduct a comparative analysis between two types of neural networks: general neural networks and the novel input supermodular neural networks, studying their representational capacities. To solve high-dimensional BPs, we introduce an enhanced sampling method to generate higher-quality samples and implement an iterative process to refine solutions. We demonstrate the performance of these approaches through extensive numerical experiments, whose lower-level problems are linear and mixed-integer programs, respectively.",
    "authors": [
        {
            "affiliations": [],
            "name": "BINARY TENDER"
        },
        {
            "affiliations": [],
            "name": "Bo Zhou"
        },
        {
            "affiliations": [],
            "name": "Ruiwei Jiang"
        },
        {
            "affiliations": [],
            "name": "Siqian Shen"
        }
    ],
    "id": "SP:56dfafdbab0f09245ea1c90fca12224167fbd05e",
    "references": [
        {
            "authors": [
                "Brandon Amos",
                "Lei Xu",
                "J Zico Kolter"
            ],
            "title": "Input convex neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Fan Bao",
                "Guoqiang Wu",
                "Chongxuan Li",
                "Jun Zhu",
                "Bo Zhang"
            ],
            "title": "Stability and generalization of bilevel programming in hyperparameter optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "J.F. Bard"
            ],
            "title": "Practical Bilevel Optimization: Algorithms and Applications",
            "year": 1998
        },
        {
            "authors": [
                "Yasmine Beck",
                "Ivana Ljubi\u0107",
                "Martin Schmidt"
            ],
            "title": "A survey on bilevel optimization under uncertainty",
            "venue": "In preparation,",
            "year": 2022
        },
        {
            "authors": [
                "Tanveer Hossain Bhuiyan",
                "Hugh R Medal",
                "Apurba K Nandi",
                "Mahantesh Halappanavar"
            ],
            "title": "Riskaverse bi-level stochastic network interdiction model for cyber-security risk management",
            "venue": "International Journal of Critical Infrastructure Protection,",
            "year": 2021
        },
        {
            "authors": [
                "Felix B\u00fcnning",
                "Adrian Schalbetter",
                "Ahmed Aboudonia",
                "Mathias Hudoba de Badyn",
                "Philipp Heer",
                "John Lygeros"
            ],
            "title": "Input convex neural networks for building MPC",
            "venue": "In Proceedings of the 3rd Conference on Learning for Dynamics and Control,",
            "year": 2021
        },
        {
            "authors": [
                "Can Chen",
                "Xi Chen",
                "Chen Ma",
                "Zixuan Liu",
                "Xue Liu"
            ],
            "title": "Gradient-based bi-level optimization for deep learning: A survey",
            "venue": "arXiv preprint arXiv:2207.11719,",
            "year": 2022
        },
        {
            "authors": [
                "Xin Chen",
                "Daniel Zhuoyu Long",
                "Jin Qi"
            ],
            "title": "Preservation of supermodularity in parametric optimization: Necessary and sufficient conditions on constraint structures",
            "venue": "Operations research,",
            "year": 2021
        },
        {
            "authors": [
                "Yize Chen",
                "Yuanyuan Shi",
                "Baosen Zhang"
            ],
            "title": "Optimal control via neural networks: A convex approach",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "B. Colson",
                "P. Marcotte",
                "G. Savard"
            ],
            "title": "Bilevel programming: A survey",
            "venue": "4OR: A Quarterly Journal of Operations Research,",
            "year": 2005
        },
        {
            "authors": [
                "William Cook",
                "Albertus MH Gerards",
                "Alexander Schrijver",
                "\u00c9va Tardos"
            ],
            "title": "Sensitivity theorems in integer linear programming",
            "venue": "Mathematical Programming,",
            "year": 1986
        },
        {
            "authors": [
                "Adejuyigbe O. Fajemisin",
                "Donato Maragno",
                "Dick den Hertog"
            ],
            "title": "Optimization with constraint learning: A framework and survey",
            "venue": "European Journal of Operational Research,",
            "year": 2023
        },
        {
            "authors": [
                "S. Ferrari",
                "R.F. Stengel"
            ],
            "title": "Smooth function approximation using neural networks",
            "venue": "IEEE Transactions on Neural Networks,",
            "year": 2005
        },
        {
            "authors": [
                "Matteo Fischetti",
                "Jason Jo"
            ],
            "title": "Deep neural networks and mixed integer linear optimization. Constraints",
            "year": 2018
        },
        {
            "authors": [
                "J. Fortuny-Amat",
                "B. McCarl"
            ],
            "title": "A representation and economic interpretation of a two-level programming problem",
            "year": 1981
        },
        {
            "authors": [
                "Scott Fujimoto",
                "Herke van Hoof",
                "David Meger"
            ],
            "title": "Addressing function approximation error in actor-critic methods",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Mingyi Hong",
                "Hoi-To Wai",
                "Zhaoran Wang",
                "Zhuoran Yang"
            ],
            "title": "A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic",
            "venue": "arXiv preprint arXiv:2007.05170,",
            "year": 2020
        },
        {
            "authors": [
                "Milad Kabirifar",
                "Mahmud Fotuhi-Firuzabad",
                "Moein Moeini-Aghtaie",
                "Niloofar Pourghaderi",
                "Payman Dehghanian"
            ],
            "title": "A bi-level framework for expansion planning in active power distribution networks",
            "venue": "IEEE Transactions on Power Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Thomas Kleinert",
                "Martine Labb\u00e9",
                "Ivana Ljubi\u0107",
                "Martin Schmidt"
            ],
            "title": "A survey on mixed-integer programming techniques in bilevel optimization",
            "venue": "EURO Journal on Computational Optimization,",
            "year": 2021
        },
        {
            "authors": [
                "Jiale Li",
                "Zhenbo Liu",
                "Xuefei Wang"
            ],
            "title": "Public charging station localization and route planning of electric vehicles considering the operational strategy: A bi-level optimizing approach",
            "venue": "Sustainable Cities and Society,",
            "year": 2022
        },
        {
            "authors": [
                "Shiyu Liang",
                "R. Srikant"
            ],
            "title": "Why deep neural networks for function approximation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Ivana Ljubi\u0107",
                "Eduardo Moreno"
            ],
            "title": "Outer approximation and submodular cuts for maximum capture facility location problems with random utilities",
            "venue": "European Journal of Operational Research,",
            "year": 2018
        },
        {
            "authors": [
                "Daniel Zhuoyu Long",
                "Jin Qi",
                "Aiqi Zhang"
            ],
            "title": "Supermodularity in two-stage distributionally robust optimization",
            "venue": "MANAGEMENT SCIENCE,",
            "year": 2023
        },
        {
            "authors": [
                "Garth P McCormick"
            ],
            "title": "Computability of global solutions to factorable nonconvex programs: Part iconvex underestimating problems",
            "venue": "Mathematical programming,",
            "year": 1976
        },
        {
            "authors": [
                "Ioana Molan",
                "Martin Schmidt"
            ],
            "title": "Using neural networks to solve linear bilevel problems with unknown lower level",
            "venue": "Technical report, Tech. rep. url: http://www. optimizationonline. org/DB HTML/2022/07/8972",
            "year": 2022
        },
        {
            "authors": [
                "Nima Nasiri",
                "Ahmad Sadeghi Yazdankhah",
                "Mohammad Amin Mirzaei",
                "Abdolah Loni",
                "Behnam Mohammadi-Ivatloo",
                "Kazem Zare",
                "Mousa Marzband"
            ],
            "title": "A bi-level market-clearing for coordinated regional-local multi-carrier systems in presence of energy storage technologies",
            "venue": "Sustainable Cities and Society,",
            "year": 2020
        },
        {
            "authors": [
                "George L Nemhauser",
                "Laurence A Wolsey"
            ],
            "title": "Maximizing submodular set functions: formulations and analysis of algorithms",
            "venue": "Studies on Graphs and Discrete Programming,",
            "year": 1981
        },
        {
            "authors": [
                "Mingyao Qi",
                "Ruiwei Jiang",
                "Siqian Shen"
            ],
            "title": "Sequential competitive facility location: Exact and approximate algorithms",
            "venue": "To appear in Operations Research,",
            "year": 2022
        },
        {
            "authors": [
                "Feng Qiu",
                "Shabbir Ahmed",
                "Santanu S Dey",
                "Laurence A Wolsey"
            ],
            "title": "Covering linear programming with violations",
            "venue": "INFORMS Journal on Computing,",
            "year": 2014
        },
        {
            "authors": [
                "Ted Ralphs",
                "S Tahernajad",
                "S DeNegre",
                "M G\u00fczelsoy",
                "A Hassanzadeh"
            ],
            "title": "Bilevel integer optimization: Theory and algorithms",
            "venue": "In International Symposium on Mathematical Programming,",
            "year": 2015
        },
        {
            "authors": [
                "Maria Jo\u00e3o Santos",
                "Eduardo Curcio",
                "Pedro Amorim",
                "Margarida Carvalho",
                "Alexandra Marques"
            ],
            "title": "A bilevel approach for the collaborative transportation planning problem",
            "venue": "International Journal of Production Economics,",
            "year": 2021
        },
        {
            "authors": [
                "Ryo Sato",
                "Mirai Tanaka",
                "Akiko Takeda"
            ],
            "title": "A gradient method for multilevel optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Thiago Serra",
                "Christian Tjandraatmadja",
                "Srikumar Ramalingam"
            ],
            "title": "Bounding and counting linear regions of deep neural networks",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "David Simchi-Levi",
                "Xin Chen",
                "Julien Bramel"
            ],
            "title": "The logic of logistics. Theory, algorithms, and applications for logistics and supply chain management",
            "year": 2005
        },
        {
            "authors": [
                "J Cole Smith",
                "Yongjia Song"
            ],
            "title": "A survey of network interdiction models and algorithms",
            "venue": "European Journal of Operational Research,",
            "year": 2020
        },
        {
            "authors": [
                "Sahar Tahernejad",
                "Ted K Ralphs",
                "Scott T DeNegre"
            ],
            "title": "A branch-and-cut algorithm for mixed integer bilevel linear optimization problems and its implementation",
            "venue": "Mathematical Programming Computation,",
            "year": 2020
        },
        {
            "authors": [
                "Haoxiang Wang",
                "Han Zhao",
                "Bo Li"
            ],
            "title": "Bridging multi-task learning and meta-learning: Towards efficient training and effective adaptation",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "M Hosein Zare",
                "Juan S Borrero",
                "Bo Zeng",
                "Oleg A Prokopyev"
            ],
            "title": "A note on linearized reformulations for a class of bilevel linear integer problems",
            "venue": "Annals of Operations Research,",
            "year": 2019
        },
        {
            "authors": [
                "Bo Zeng",
                "Yu An"
            ],
            "title": "Solving bilevel mixed integer program by reformulations and decomposition",
            "venue": "Optimization online,",
            "year": 2014
        },
        {
            "authors": [
                "Bo Zhou",
                "Jiakun Fang",
                "Xiaomeng Ai",
                "Shichang Cui",
                "Wei Yao",
                "Zhe Chen",
                "Jinyu Wen"
            ],
            "title": "Storage right-based hybrid discrete-time and continuous-time flexibility trading between energy storage station and renewable power plants",
            "venue": "IEEE Transactions on Sustainable Energy,",
            "year": 2023
        },
        {
            "authors": [
                "Simchi-Levi"
            ],
            "title": "n\u2217. This finishes the proof. A.2 PROOF OF PROPOSITION 3. Consider functions f : R \u2192 R, g : R \u2192 R, and their composite f(g(x)). It can be shown that f(g(x)) is supermodular in x if one of the following is satisfied",
            "year": 2005
        },
        {
            "authors": [
                "Ljubi\u0107",
                "Moreno",
                "Qi"
            ],
            "title": "2022)). Consequently, there is no need to incorporate the exponentially many inequalities (17) up front, and we only need to incorporate the violated ones on-the-fly, e.g., in a branch-and-bound algorithm for solving the BP",
            "year": 2022
        },
        {
            "authors": [
                "C INSTANCE"
            ],
            "title": "GENERATION Following the instance generation rules of (Tahernejad et al., 2020), all instances are generated in the following forms",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Bilevel programs (BPs) are appealing for modeling problems that involve sequential decision interactions from two or multiple players. Their hierarchical decision processes arise in a wide range of real-world applications, including energy (Zhou et al., 2023), security (Bhuiyan et al., 2021), transportation (Santos et al., 2021), and market design (Nasiri et al., 2020). Recently, BPs have also shown strong modeling power in machine learning problems, including meta learning (Wang et al., 2021), actor-critic reinforcement learning (Hong et al., 2020), hyperparameter optimization (Bao et al., 2021), and deep learning (Chen et al., 2022). Next, we provide a brief introduction to BP, its generic formulation, and solution methods.\nBilevel Program. In a BP, a leader and a follower solve their own decision making problems in an interactive way: The leader\u2019s decisions made in the upper level will affect the follower\u2019s problem solved at the lower level (e.g., the leader\u2019s decisions are involved in the objective function and/or constraints of the follower\u2019s problem) and vice versa. This can be described formally as\nmin x\nf(x, y\u2217) (1a)\ns.t. x \u2208 X(y\u2217) (1b) where y\u2217 \u2208 arg max g(y, x) (1c)\ns.t. y \u2208 Y (x), (1d) where x represents the leader\u2019s decision and y represents the follower\u2019s. Here, the leader\u2019s decision x will affect the follower\u2019s objective function g(y, x) and feasible region Y (x) \u2286 Rm. On the other hand, the leader\u2019s objective function f(x, y\u2217) depends on both her own decision x and the follower\u2019s optimal decision y\u2217, which is a function of x defined through (1c)\u2013(1d). Additionally, the leader\u2019s feasible region X(y\u2217) \u2286 Rn can also depend on y\u2217.\nSingle-Level Reformulation. To solve the BP (1), a general approach is to incorporate the lowerlevel decisions and their feasible region into the upper-level problem, giving rise to\nmin x f(x, y) (2a)\ns.t. x \u2208 X(y) (2b) y \u2208 Y (x). (2c)\nFormulation (2) relaxes the optimality of y, and therefore, its solution only provides a lower bound for the BP (1). To retrieve optimality, one can incorporate an optimality condition\ng(y, x) \u2265 \u03c6(x), (3) where \u03c6(x) := maxy\u2208Y (x) g(y, x) represents the optimal value of the lower-level problem as a function of x. Hence, combining (2) and (3) produces a single-level reformulation of the BP (1). In case the lower-level problem is continuous (i.e., y consists of continuous decision variables only), constraint (3) can be made explicit using strong duality or the KKT conditions. However, such luxury is immediately lost if the lower-level problem involves discrete decision variables (e.g., y is binary or mixed-binary). In that case, the closed-form expression of \u03c6(x) is either non-existent or highly intractable, prohibiting solving the BP effectively.\nIn light of this challenge, we study BPs with binary tender. Here, \u201ctender\u201d is defined as the linking variables between the upper- and lower-level problems and \u201cbinary tender\u201d means that all tender variables are binary. Note that the problems in both levels can involve general decisions (e.g., continuous and/or integer variables), and we only assume that the entries of x appearing in the lowerlevel formulation are binary-valued. Such BPs arise in many applications, including energy system expansion planning (Kabirifar et al., 2022), charging station planning (Li et al., 2022), competitive facility location (Qi et al., 2022), and network interdiction (Smith & Song, 2020). In addition, we assume the leader\u2019s feasible region is independent of y\u2217, i.e., X(y\u2217) \u2261 X , which is a special case of (1). In this case, we can obtain an upper bound for the BP (1) from any feasible solution x and the corresponding follower\u2019s optimal solution y\u2217 (e.g., by solving the follower\u2019s problem using x). Our main contributions are three-fold.\n\u2022 We employ neural networks to learn and approximate the value function \u03c6(x). Then, we derive a closed-form representation of the learned value function. This yields a single-level, mixed-integer formulation for the BP (1), which can be readily solved by off-the-shelf optimization solvers.\n\u2022 Motivated by the fact that \u03c6(x) is supermodular for a large class of BPs, we design an input supermodular neural network (ISNN) that ensures a supermodular mapping from input to output. We analyze the representability of both general neural networks (GNN) and ISNN to provide guidance for network architecture selection.\n\u2022 To solve high-dimensional BPs, we propose an enhanced sampling method for generating higher-quality samples and training neural networks to better approximate \u03c6(x) around the optimal solution. Building upon this enhanced sampling method, we execute an iteration process to improve the accuracy of the derived solution.\nThe remainder of the paper is organized as follows. In Section 2, we review related works in the literature. In Section 3, we elaborate our methodology of learning to solve BPs. We conduct numerical experiments in Section 4 and draw conclusions in Section 5."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": "The theories and algorithms for finding optimal solutions to BPs depend on the structure and properties of the lower-level problem, as well as the coupling between the two levels (Kleinert et al., 2021; Beck et al., 2022).\nBilevel linear/convex programs. The earlier studies on BPs focused on linear or convex lowerlevel problems. Consequently, one can use the KKT conditions of the lower-level problem or strong duality to reformulate the BP as a single-level problem with complementarity constraints (FortunyAmat & McCarl, 1981) or bilinear terms (Zare et al., 2019; McCormick, 1976), respectively. In both cases, solving BPs boils down to solving the resulting single-level, nonconvex, and nonlinear reformulation (Colson et al., 2005; Bard, 1998).\nBilevel (mixed-integer) nonconvex programs. Recently, more studies focused on more general BPs having discrete decision variables or nonconvex objectives/constraints. In such problems, we notice that, because of the existence of integer decision variables or nonconvexity, neither KKT conditions nor strong duality approaches may be able to capture the (parametric) global optimal solutions at the lower level. To achieve global optimum, we instead need to exploit special structures of BPs (Qi et al., 2022) or construct the aforementioned relaxation (2) for single-level reformulation. For BPs with nonconvex objectives/constraints, approximation methods were employed to approximate the lower-level problem and reformulate the BP as a single-level formulation, such as polyhedral approximation (Sato et al., 2021) and other gradient-based methods in machine learning (Hong et al., 2020; Chen et al., 2022). However, when discrete decision variables are incorporated, gradient-based methods are no longer applicable. To address this issue, integer programming techniques are employed and various cutting plane-based algorithms, such as the MiBS solver, have been developed (Ralphs et al., 2015; Zeng & An, 2014). However, learning techniques have not been explored in solving such BPs, which is the focus of this paper.\nEmbedding neural networks into optimization. Due to its strong power in data analysis and fitting, machine learning, such as neural networks, has drawn wide interest in function approximation (Fujimoto et al., 2018; Liang & Srikant, 2017; Ferrari & Stengel, 2005). Furthermore, when the adopted activation function is a piecewise linear function, such as ReLU, the output \u03c6\u0303(x) of a neural network can be reformulated as a mixed-integer linear form (Fischetti & Jo, 2018; Serra et al., 2018). Consequently, in case \u03c6(x) is unknown or hard to access, one can use \u03c6\u0303(x) to replace \u03c6(x) and directly incorporate the mixed-integer representation of \u03c6\u0303(x) into optimization formulations. Molan & Schmidt (2022) considered this idea in BPs with unknown lower-level problems. They utilized a neural network to approximate the mapping from x to the optimal y\u2217 in the lower level, and exploited Lipschitz optimization techniques to reformulate the activation function (ReLU). Different from Molan & Schmidt (2022), we propose to learn and approximate \u03c6(x) in BPs.\nInput convex neural network (ICNN). Another related stream of literature proposed ICNNs, which ensure that the output approximation \u03c6\u0303(x) is a convex function. In this case, the target optimization formulation remains a convex program if it was so before incorporating \u03c6\u0303(x). ICNN was first proposed by Amos et al. (2017), who derived sufficient conditions on the neural network architecture and parameter settings for the convexity of \u03c6\u0303(x). ICNNs were then applied to, e.g., optimal control (Chen et al., 2019) and energy optimization (Bu\u0308nning et al., 2021). Different from these works, we study BPs with binary tender, and thus the input x of the neural network is binary-valued. Accordingly, we consider supermodularity, the counterpart of convexity in discrete optimization, and establish ISNN that guarantees to output a supermodular \u03c6\u0303(x). In Section 3, we derive a neural network architecture for ISNN and show its representability."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "We use neural networks (see Fajemisin et al. (2023)) to obtain an approximate value function \u03c6\u0303(x) of \u03c6(x) and solve the BP (1) by incorporating a closed-form expression of \u03c6\u0303(x) into formulation (2)\u2013 (3). Figure 1 illustrates the proposed method, including sampling, training, and solving."
        },
        {
            "heading": "3.1 SAMPLING",
            "text": "We consider a set of the upper-level decision variables x\u0302 and compute \u03c6(x\u0302) = maxy {g(y, x\u0302) : y \u2208 Y (x\u0302)} to obtain sample-label pairs (x\u0302, \u03c6(x\u0302)). When the dimension n of x is small, we can enumerate all possible sample-label pairs. As n increases, the number of all sample-label pairs increases exponentially and enumeration becomes numerically prohibitive. In this case, we can sample a sufficiently large set of x\u0302 instead. However, a na\u0131\u0308ve sampling may frequently produce infeasible solutions, let alone obtaining high-quality ones. To this end, we propose an enhanced sampling method to quickly find feasible samples and improve the quality of samples when the number of samples is limited.\nEnhanced Sampling. A basic idea in enhanced sampling is \u201csampling via optimization.\u201d This involves confining the sample selection within the feasible region of the BP with respect to a random but tractable objective function. This is mathematically specified as\nmin xTQx+ hTx (4a) s.t. x \u2208 X, y \u2208 Y (x), (4b)\nwhere Q and h are randomly generated n\u00d7n positive semi-definite matrix and n\u00d71 vector, respectively. Accordingly, we can efficiently generate samples by solving (4). We note that the reason for solving a quadratic program here is to avoid repeated samples and enhance sampling efficiency.\nAnother basic idea is to sample more often in the vicinity of the optimal solution to the BP. A more accurate approximation \u03c6\u0303(x) to the true \u03c6(x) in this vicinity, even at the cost of inaccuracy for those x far from optimum, enhances the tightness of approximating the BP (1), because most of the effort in solving (1) is spent on comparing the near-optimal x\u2019s. To this end, we let fub represent a known upper bound of the BP (1), which can be obtained from any feasible solution. Then, we strengthen (4) to be\nmin xTQx+ hTx (5a) s.t. f(x, y) \u2264 fub (5b) x \u2208 X, y \u2208 Y (x), (5c)\nwhere constraint (5b) restricts the search space to elevate quality of samples. Here we note that though (5) is a mixed-integer quadratic program, we only need a feasible solution to (5) for sampling, which does not incur too much computational burden and saves the overall computational time. Furthermore, by sampling feasible solutions x\u0302 to the BP, we can compute their objective value f(x\u0302, y\u0302\u2217), y\u0302\u2217 representing the corresponding optimal solution from the lower level, and iteratively strengthen fub if f(x\u0302, y\u0302\u2217) < fub. This allows to continuously refine the search space and obtain higher-quality samples. Algorithm 1 summarizes the above sampling process.\nAlgorithm 1 Enhanced Sampling 1: INPUT: sample size Ns, initial upper bound fub, maximum number of updates Nub. 2: Initialize the set of samples \u2126\u2190 \u2205, count of samples ks \u2190 0, count of updates kub \u2190 0. 3: repeat 4: Randomly generate an n\u00d7 n positive semi-definite matrix Q and an n\u00d7 1 vector h. 5: Solve (5a)\u2013(5c) and store an optimal solution x\u0302. 6: if x\u0302 /\u2208 \u2126 then 7: Solve maxy\u2208Y (x\u0302) g(y, x\u0302), and store the optimal value \u03c6(x\u0302) and an optimal solution y\u0302\u2217. 8: Augment \u2126\u2190 \u2126 \u222a {x\u0302} and ks \u2190 ks + 1. 9: end if 10: if kub < Nub and f(x\u0302, y\u0302\u2217) < fub then 11: Update fub \u2190 f(x\u0302, y\u0302\u2217). 12: kub \u2190 kub + 1. 13: end if 14: until ks = Ns 15: OUTPUT: sample-label pairs (x, \u03c6(x)) for all x \u2208 \u2126."
        },
        {
            "heading": "3.2 TRAINING",
            "text": "Using the sample-label pairs from Algorithm 1, we adopt supervised learning to train a neural network to fit the mapping \u03c6(x). In this step, we consider two types of neural networks, i.e., GNN and\nISNN, which use the same architecture as shown in Figure 2. There areK hidden layers and one output layer in the architecture and we employ passthrough to enhance the representability of the neural network. We note that the input of the architecture, x\u0303, can be different from x (see Sections 3.2.1\u2013 3.2.2 for details). In addition, \u03c6\u0303 denotes the output of the neural network, zk denotes the output of the kth hidden layer, Wk/Dk and bk are the weights and biases of the kth layer, respectively, and \u03c3(\u00b7) denotes the activation function.\nSpecifically, the neural network defines \u03c6\u0303(x\u0303) through z1 = \u03c3(W1x\u0303+ b1) (6a) zk = \u03c3(Wkzk\u22121 + bk +Dkx\u0303), \u2200k = 2, . . . ,K, (6b) \u03c6\u0303 = WK+1zK + bK+1 +DK+1x\u0303. (6c)"
        },
        {
            "heading": "3.2.1 GENERAL NEURAL NETWORK",
            "text": "For a GNN, we define x\u0303 := x. Proposition 1 shows the representability of the GNN. Proposition 1. Consider an arbitrary \u03c6 : {0, 1}n \u2192 R. Then, the following statements hold. i. (Universal Approximation) there exists a GNN with the architecture in Figure 2 such that \u03c6\u0303 fits \u03c6 exactly, i.e., \u03c6\u0303(x) = \u03c6(x) for all x \u2208 {0, 1}n. ii. (Maximum Representability) Suppose that the hidden layers of the GNN consist of Nnr many neurons, which may be distributed arbitrarily among these layers. If we measure the GNN representability by the number of parameters it trains, then the representability is maximized when GNN consists of two hidden layers, i.e., when K = 2. In particular, the first layer consists of Nnr/2 (if Nnr is even) or (Nnr \u2212 1)/2 neurons (if Nnr is odd), and the second layer consists of Nnr/2 (if Nnr is even) or (Nnr + 1)/2 neurons (if Nnr is odd). iii. (Sufficient Fitting) Given Ns sample-label pairs, the GNN is able to output an approximation \u03c6\u0303 that fits these pairs exactly if\nNnr \u2265 \u2308\u221a (2n+ 2)2 + 4Ns + 1\u2212 (2n+ 3) \u2309 . (7)\nWe give a proof of Proposition 1 in Appendix A.1. According to Proposition 1, when we adopt a GNN for training, we incorporate two hidden layers in the GNN architecture, each with Nnr/2 neurons, where Nnr is the smallest even number that satisfies (7)."
        },
        {
            "heading": "3.2.2 INPUT SUPERMODULAR NEURAL NETWORK",
            "text": "In light of the special class of BPs studied in this paper, we are motivated to design an ISNN architecture that guarantees to output a supermodular approximation \u03c6\u0303(x\u0303) of the true value function \u03c6(x), where we set x\u0303 := [x>, (1 \u2212 x)>]> in the case of ISNN. Long et al. (2023) and Chen et al. (2021) provide some sufficient conditions for parametric optimization \u03c6(x) to be supermodular.\nFirst, we recall the definition of supermodularity. Definition 2. Consider a subset D of Rn, a function f : D \u2192 R, and for x, y \u2208 D define x\u2228 y = [max{x1, y1}, . . . ,max{xn, yn}]> and x\u2227 y = [min{x1, y1}, . . . ,min{xn, yn}]>. Then, f is called supermodular if f(x) + f(y) \u2264 f(x \u2228 y) + f(x \u2227 y) for all x, y \u2208 D. In addition, a set S is called a lattice if x \u2228 y \u2208 S and x \u2227 y \u2208 S for all x, y \u2208 S.\nWe remark that this definition of supermodularity applies to a mixed-integer-valued argument and the domain D of a supermodular function f needs not to be {0, 1}n. Then, we are ready to present the main result of this section.\nProposition 3. The function \u03c6\u0303(x\u0303) output from ISNN is supermodular in x\u0303 if W1:(K+1) and D2:K are non-negative, and \u03c3(\u00b7) is convex and non-decreasing (e.g., ReLU).\nThe proof is given in Appendix A.2. By Proposition 3, we simply need to add appropriate sign constraints to some weight parameters when training the ISNN.\nNext, Proposition 4 shows the representability of ISNN. Proposition 4. Consider an arbitrary (not necessarily supermodular) \u03c6 : {0, 1}n \u2192 R. Then, the following statements hold. i. (Universal Approximation) there exists an ISNN with the architecture in Figure 2 such that \u03c6\u0303(x,1\u2212 x) = \u03c6(x) for all x \u2208 {0, 1}n. ii. (Maximum Representability) With a fixed number of neurons Nnr in its hidden layers, the ISNN\u2019s architecture does not affect its representability (measured by the number of parameters the ISNN trains). iii. (Sufficient Fitting) Given Ns sample-label pairs, the ISNN is able to output an approximation \u03c6\u0303 that fits these pairs exactly if\nNnr \u2265 \u2308\nNs 2n+ 1\n\u2212 1 \u2309 . (8)\nWe give a proof of Proposition 4 in Appendix A.3. Since the ISNN\u2019s architecture does not affect its representability, to better compare with GNN, we incorporate two hidden layers in ISNN, each with Nnr/2 neurons, where Nnr is the smallest integer that satisfies (8).\nThanks to the supermodularity of \u03c6\u0303(x\u0303) promised by Proposition 3, we can recast the approximate optimality condition g(y, x) \u2265 \u03c6\u0303(x\u0303) as linear inequalities, in lieu of linearizing \u03c6\u0303(x\u0303) using auxiliary binary variables and big-M coefficients. We place the theoretical analysis for supermodular \u03c6\u0303(x\u0303) in Appendix B."
        },
        {
            "heading": "3.3 SOLVING",
            "text": "3.3.1 MIXED-INTEGER REPRESENTATION OF \u03c6\u0303(x\u0303)\nWe represent the epigraph of \u03c6\u0303(x\u0303) as linear inequalities using auxiliary binary variables, to be embedded into the BP reformulation (2)\u2013(3). To this end, we recall that the ReLU activation function takes the form\n\u03c3(x) = max{x, 0} = { x, x \u2265 0 0, x < 0.\n(9)\nThen, \u03c3(x) can be rewritten as\n0 \u2264 \u03c3(x) \u2264M\u03b4 (10a) x \u2264 \u03c3(x) \u2264 x+M(1\u2212 \u03b4) (10b)\nwhere \u03b4 is an auxiliary binary variable andM is a sufficiently large positive number. We note that, in computation, the value ofM does not have to be arbitrarily big and it can be iteratively strengthened by solving relaxations of (2)\u2013(3); see, e.g., Qiu et al. 2014. Likewise, the defition of \u03c6\u0303(x\u0303) in (6) can be represented layer by layer as follows: z\u20321 = W1x\u0303+ b1\n0 \u2264 z1 \u2264M\u03b41 z\u20321 \u2264 z1 \u2264 z\u20321 +M(1\u2212 \u03b41)\n(11a)\n z\u2032k = Wkzk\u22121 + bk +Dkx\u0303\n0 \u2264 zk \u2264M\u03b4k z\u2032k \u2264 zk \u2264 z\u2032k +M(1\u2212 \u03b4k) \u2200k = 2, ...,K (11b)\n\u03c6\u0303 = WK+1zK + bK+1 +DK+1x\u0303, (11c)\nwhere z\u2032k and \u03b4k are auxiliary continuous and binary variables for all 1 \u2264 k \u2264 K, respectively. Plugging this representation into (3) yields a mixed-integer programming approximation of the BP (1):\nmin x,y f(x, y) (12a)\ns.t. x \u2208 X, y \u2208 Y (x) (12b) g(y, x) \u2265 \u03c6\u0303(x\u0303), (11) (12c)\nx\u0303 =\n{ x if using GNN,\n[x>, (1\u2212 x)>]> if using ISNN. (12d)\nHere we note that existing algorithms and highly-efficient off-the-shelf solvers can be adopted to handle the single-level program (12), which is yet out of the scope of this paper."
        },
        {
            "heading": "3.3.2 NEURAL BILEVEL ALGORITHM",
            "text": "Combining the above sampling, training, and solving processes, we conclude our neural bilevel algorithm in Algorithm 2. By iteratively conducting the enhanced sampling, we can find new and higher-quality samples and improve the accuracy of the found solution.\nAlgorithm 2 Neural Bilevel Algorithm 1: INPUT: maximum number of iterations Niteration, initial upper bound fub := +\u221e. 2: Solve (2) and store an optimal solution x\u2217. 3: Store a y\u2217 \u2208 argmaxy\u2208Y (x\u2217) g(y, x\u2217). 4: Update fub \u2190 min{fub, f(x\u2217, y\u2217)}. 5: for i = 1, ..., Niteration do 6: Conduct Algorithm 1 to obtain the pairs (x\u0302, \u03c6(x\u0302)) and their corresponding f(x\u0302, y\u0302). 7: Train a neural network \u03c6\u0303(x\u0303) (GNN or ISNN) using (x\u0302, \u03c6(x\u0302)). 8: Solve (12) using the trained \u03c6\u0303 and store an optimal solution x\u2217. 9: Store a y\u2217 \u2208 argmaxy\u2208Y (x\u2217) g(y, x\u2217). 10: Update fub \u2190 min{fub, f(x\u2217, y\u2217), f(x\u0302, y\u0302)}. 11: end for 12: OUTPUT: Current (i.e., best) upper bound fub and its corresponding solution (x, y)."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 ILLUSTRATIVE EXAMPLES",
            "text": "We first use an illustrative example to show the effectiveness of our proposed methods. We consider a BP with a nonconvex and nonlinear program in the lower level:\nmin x\n2x1 + x2 \u2212 3y\ns.t. x1, x2 \u2208 {0, 1} (13a) where y\u2217 \u2208 arg max \u2212 (y \u2212 2)2\ns.t. 0 \u2264 y \u2264 1 + 2|x1 \u2212 x2| (13b)\nThis BP admits the following optimality condition for the lower-level problem:\n\u2212 (y \u2212 2)2 \u2265 \u03c6(x1, x2). (14)\nwhere \u03c6(x1, x2) := max{\u2212(y \u2212 2)2 : (13b)}. In this example, \u03c6(x1, x2) admits a closedform expression. Indeed, by the structure of the lower-level objective function, we notice that y\u2217 = min{1 + 2|x1 \u2212 x2|, 2} and so \u03c6(x1, x2) = \u2212(min{1 + 2|x1 \u2212 x2|, 2} \u2212 2)2, which is depicted in Figure 3(a). Incorporation of this expression yields the optimal solution (x\u22171, x \u2217 2) = (0, 1). In Figure 8(a) of Appendix D.1, we illustrate the feasible region of (x1, x2, y) described by constraints (13a)\u2013(13b) and why (14) ensures optimality.\nOn the other hand, following the steps and methods described in Section 3, we use neural networks to approximate the value function \u03c6(x1, x2). Thanks to the simplicity of this example, we find closed-form expressions of these approximations: \u03c6\u0303G(x1, x2) = x1 + x2 \u2212 1 \u2212 2\u03c3(x1 + x2 \u2212 1) for GNN and \u03c6\u0303IS(x1, x2) = x1 + (1 \u2212 x2) \u2212 2 + 2\u03c3((1 \u2212 x1) + x2 \u2212 1)) for ISNN, which are\ndepicted in Figures 3(b) and 3(c), respectively. Figures 8(b) and 8(c) in Appendix D.1 visualize their corresponding optimality cuts. Both approximations \u03c6\u0303G(x1, x2) and \u03c6\u0303IS(x1, x2) lead to the (true) optimal solution x\u2217 = (0, 1)>. From these, we observe that incorporating the approximate \u03c6\u0303(x1, x2) from either GNN or ISNN correctly produces the optimal solution."
        },
        {
            "heading": "4.2 RANDOMLY GENERATED INSTANCES",
            "text": ""
        },
        {
            "heading": "4.2.1 SETUP",
            "text": "We randomly generate 6 classes of instances for numerical experiments, with n = 10, 20, 30, 40, 50, 60. The details of instance generation is reported in Appendix C. We allow the lower-level problem to be a linear program (LP) or a mixed-integer linear program (MILP). By default, we generate 1,000 samples using Algorithm 1 and use them in training neural networks. We adopt ReLU as the activation function and design the architecture of neural networks by Proposition 1 and by Propositions 3\u20134 for GNN and ISNN, respectively. We use the Adam algorithm (Kingma & Ba, 2014) for training for 1000 epochs and set the learning rate as 0.001 with the decay 0.001."
        },
        {
            "heading": "4.2.2 COMPARISON WITH STATE-OF-THE-ART SOLVERS",
            "text": "We compare our method with the state-of-the-art solver for bilevel problems, MiBS (Tahernejad et al., 2020) and use its solution within a 1-hour time limit as the benchmark to calculate the objective differences (i.e., the gap between the objectives from our method and MiBS). The negative objective difference means that our method provides a better solution than the benchmark. For each instance, we replicate our method for 10 times, considering the randomness in sampling and training.\nFor the instances with a LP lower level, we report the objective difference of the best upper bound found by Algorithm 2 in Figure 4 and the average computational time in Figure 5. In all figures, the legend, for example, \u201cGNN\u20132\u201d means that we use GNN to fit samples and Niteration = 2. Figure 4(a) and Figure 4(b) show the average and minimum objective difference in the 10 replications. The distribution of the objective difference of the 10 replications is provided in Figure 9 of Appendix D.2. The computational time reported in Figure 5 is the average time of the 10 replications. The itemized time in sampling, training, and solving is provided in Figure 10 of Appendix D.2.\nFrom the results, we can see that when n = 10, both GNN and ISNN can produce a true optimal solution in all replications. This is because we can enumerate all feasible solutions x in sampling for n = 10. Yet during the sampling process, there exists much repeated sampling, for which the computational time is slightly longer than that of n = 20. When n \u2265 20, the computational time increases as n gets larger, which mainly results from the longer sampling time, and is almost linear with Niteration. However, the computational time is significantly shorter than that of MiBS when n \u2265 40. In addition, the objective difference of instances with n = 30, 50, 60 is larger than 15%. The increase of Niteration results in the decrease of average objective difference for both GNN and ISNN. It means that the influence of sampling gets significant and validates that enhanced sampling helps reduce average objective difference. Yet as n increases, the marginal improvement of enhanced sampling becomes smaller, which is caused by the increasing difficulty in finding highquality samples. Comparing GNN and ISNN with the same Niteration, we observe that ISNN always outperforms GNN in both average and minimum objective difference. Using ISNN can reduce objective difference by more than 10% when n \u2265 40, which validates the effectiveness of ISNN in improving accuracy.\nFor the instances with a MILP lower level, we report the objective difference of the best upper bound found by Algorithm 2 in Figure 7 and the average computational time in Figure 6. More details are provided in Figures 11\u201312 of Appendix D.2. We observe that these results produce similar insights as those of the LP lower-level problems. Notably, our method has excellent accuracy in most instances (achieving objective difference less than 5%, only except when n = 30) and even outperforms MiBS when n = 20, 40, 50. This is because when the lower level is a MILP, it becomes significantly more challenging to find high-quality feasible solutions, and in these instances even MiBS reports incorrect optimal solutions. In contrast, the proposed method is able to produce better feasible solutions in (dramatically) shorter computational time, validating its effectiveness."
        },
        {
            "heading": "5 CONCLUSIONS",
            "text": "We considered machine learning methods for solving mixed-integer, nonconvex BPs with binary tender. We developed an enhanced sampling algorithm to find high-quality samples, designed GNN and ISNN to approximate the lower-level value function (in terms of the upper-level decisions), and incorporated the results as optimality cuts into a single-level reformulation of the BP. We validate the effectiveness of the proposed approaches using an illustrative example and larger-scale, randomly generated instances. Through these experiments, we demonstrated that the enhanced sampling helps reduce average objective difference and ISNN always outperforms GNN. The computational time of using either GNN or ISNN is significantly shorter than that of (a state-of-the-art solver) MiBS when the dimension of the bianry tender exceeds 30. Notably, in most instances with a MILP lower level, the proposed method can produce even better solutions than MiBS in shorter computational time."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The authors would like to thank anonymous reviewers for their detailed and helpful comments. Ruiwei Jiang is supported in part by the U.S. Air Force Office of Scientific Research under the grant FA9550-23-1-0323."
        },
        {
            "heading": "A PROOFS",
            "text": "A.1 PROOF OF PROPOSITION 1\nAny arbitrary \u03c6 : {0, 1}n \u2192 R can be rewritten as\n\u03c6(x) = (1\u2212 x1)(1\u2212 x2)(1\u2212 x3) \u00b7 \u00b7 \u00b7 (1\u2212 xn)\u03c6(0, 0, 0, . . . , 0) + x1(1\u2212 x2)(1\u2212 x3) \u00b7 \u00b7 \u00b7 (1\u2212 xn)\u03c6(1, 0, 0, . . . , 0) + (1\u2212 x1)x2(1\u2212 x3) \u00b7 \u00b7 \u00b7 (1\u2212 xn)\u03c6(0, 1, 0, . . . , 0) + x1x2(1\u2212 x3) \u00b7 \u00b7 \u00b7 (1\u2212 xn)\u03c6(1, 1, 0, . . . , 0) + (1\u2212 x1)(1\u2212 x2)x3 \u00b7 \u00b7 \u00b7 (1\u2212 xn)\u03c6(0, 0, 1, . . . , 0) + \u00b7 \u00b7 \u00b7 + x1x2x3 \u00b7 \u00b7 \u00b7xn\u03c6(1, 1, 1, . . . , 1)\n= \u2211\nz\u2208{0,1}n \u03c6(z) n\u220f i=1 [ (1\u2212 zi) + (2zi \u2212 1)xi ] .\nThis shows that \u03c6 is a polynomial of (at most) degree n. Then, \u03c6 admits the following representation: \u03c6(x) = a \u220f i xi + \u2211 {j}\u2286[n] bj \u220f i 6=j xi + \u2211 {k,j}\u2286[n] ck,j \u220f i 6=j,k xi + \u00b7 \u00b7 \u00b7 ,\nwhere a, b, c, . . . are coefficients of the polynomial. Since x \u2208 {0, 1}n, we can rewrite\u220f i/\u2208S xi = max {\u2211 i/\u2208S xi \u2212 (n\u2212 |S| \u2212 1), 0 } = \u03c3 (\u2211 i/\u2208S xi \u2212 (n\u2212 |S| \u2212 1) )\nfor all subsets S \u2286 [n]. It follows that\n\u03c6(x) = a\u03c3 (\u2211 i xi \u2212 (n\u2212 1) ) + \u2211 {j}\u2286[n] bj\u03c3 \u2211 i 6=j xi \u2212 (n\u2212 2)  +\n\u2211 {k,j}\u2286[n] ck,j\u03c3 \u2211 i 6=j,k xi \u2212 (n\u2212 3) + \u00b7 \u00b7 \u00b7 . There are 2n terms in this expression and the n first-order terms and the zeroth-order term do not need activation. Therefore, \u03c6(x) can be represented using a neural network with an architecture as in Figure 2 with K = 1 and 2n \u2212 n\u2212 1 neurons (activations). This proves claim (i).\nFor a GNN as shown in Figure 2, \u03c6\u0303(x) admits a closed-form expression as in (6). Let zk \u2208 Rnk for all k \u2208 [K], then the number of neurons in the hidden layers is Nnr = \u2211K k=1 nk and the total number of parameters Npm GNN trains is\nNpm = n1(n+ 1) + K\u2211 k=2 nk(nk\u22121 + 1 + n) + (nK + 1 + n)\n= K\u2211 k=2 nknk\u22121 + nK + (n+ 1) ( K\u2211 k=1 nk + 1 )\n= K\u2211 k=2 nknk\u22121 + nK + (n+ 1)(Nnr + 1).\nThen, with fixedNnr, to maximizeNpm by adjusting the GNN architecture boils down to optimizing\u2211K k=2 nknk\u22121 + nK over (integer) variables K and n1, n2, . . . , nK . To this end, we state the following technical lemma, whose proof is relegated to Appendix A.1.1.\nLemma 5. Consider the following (non-convex) quadratic program with continuous decision variables:\nmax n1,...,nK K\u2211 k=2 nknk\u22121 + nK (15a)\ns.t. K\u2211 k=1 nk = Nnr, (15b)\nnk \u2265 0 \u2200n \u2208 [K]. (15c) Then, an optimal solution to this program is nk = 0 for all k \u2208 [K \u2212 2], nK\u22121 = (Nnr \u2212 1)/2, and nK = (Nnr + 1)/2. Additionally, the corresponding optimal value is (Nnr + 1)2/4.\nBy Lemma 5, the optimal solution to formulation (15) remains the same ifNnr is odd and we restrict each nk to take integer values only, because (Nnr \u2212 1)/2, (Nnr + 1)/2 \u2208 N. In case thatNnr is even, consider the (integer) solution nk = 0 for all k \u2208 [K\u22122], nK\u22121 = Nnr/2, and nK = Nnr/2. The objective value of this solution is (N2 + 2N)/4, that is, 1/4 less than the optimal value of (15). Since all coefficients of the objective function (15a) are integer-valued, the objective value of any (integer) solution must be integer-valued, too. It follows that there does not exist any other (integer) solution, whose objective value is strictly larger than (N2 + 2N)/4. Therefore, the solution nk = 0 for all k \u2208 [K \u2212 2], nK\u22121 = Nnr/2, and NK = Nnr/2 is optimal when Nnr is even. This proves claim (ii).\nTo fit the Ns sample-label pairs exactly, we need the number of parameters Npm of the GNN to be at least as large as Ns, i.e., Npm \u2265 Ns. But claim (ii) implies that, when choosing nK\u22121 = nK = Nnr/2,\nNpm = 1\n4 (N2nr + 2Nnr) + (n+ 1)(Nnr + 1) \u2265 Ns.\nIt follows that Nnr \u2265 \u221a (2n+ 2)2 + 4Ns + 1\u2212 (2n+ 3),\nproving claim (iii).\nA.1.1 PROOF OF LEMMA 5\nFor each local optimal solution n to formulation (15), the KKT condition states that there exist Lagrangian multipliers \u00b5 \u2208 RK+ and \u03bb \u2208 R, such that\n\u00b5knk = 0 \u2200k \u2208 [K], (16a) \u00b5k \u2212 \u03bb+ nk\u22121 + nk+1 = 0 \u2200k \u2208 [K], (16b)\nwhere n0 := 0 and nK+1 := 1. For example, for the solution n\u2217 with n\u2217k = 0 for all k \u2208 [K \u2212 2], n\u2217K\u22121 = (Nnr \u2212 1)/2, and n\u2217K = (Nnr + 1)/2, we pair it with the Lagrangian multipliers \u03bb\u2217 = (Nnr + 1)/2 and\n\u00b5\u2217k =  1 2 (Nnr + 1) if k \u2208 [K \u2212 3] 1 if k = K \u2212 2\n0 if k = K \u2212 1,K\n\u2200k \u2208 [K].\nSince (n\u2217, \u00b5\u2217, \u03bb\u2217) satisfies (16a)\u2013(16b) as well as (15b)\u2013(15c), n\u2217 is a local optimal solution to (15) with objective value (Nnr + 1)2/4. In what follows, we shall show that any other local optimal solution to (15) does not achieve a strictly larger objective value, establishing the global optimality of n\u2217.\nTo this end, we multiply both sides of (16b) by nk and then sum them up for all k \u2208 [K] to yield K\u2211 k=1 \u00b5knk \u2212 \u03bb K\u2211 k=1 nk + 2 K\u2211 k=2 nknk\u22121 + nK = 0\n=\u21d2 K\u2211 k=2 nknk\u22121 + nK = 1 2 (Nnr\u03bb+ nK),\nwhich follows from (16a) and (15b). This rewrites the quadratic program (15) as\nmax n\u22650,\u03bb\u22650,\u00b5\u2208R\n1 2 (Nnr\u03bb+ nK)\ns.t. (15b)\u2013(15c), (16a)\u2013(16b).\nFor any local optimal solution n, together with its corresponding Lagrangian multipliers (\u00b5, \u03bb) satisfying (15b)\u2013(15c) and (16a)\u2013(16b), we discuss the following three cases.\n1. If \u00b5K = 0, then (16b) with k = K implies that\n0 = nK\u22121 + 1 + \u00b5K \u2212 \u03bb = nK\u22121 + 1\u2212 \u03bb =\u21d2 \u03bb = nK\u22121 + 1. We denote m := \u2211K\u22122 k=1 nk with 0 \u2264 m \u2264 Nnr. The following two sub-cases show that n cannot be better than n\u2217. (a) If nK\u22121 = 0, then \u03bb = 1 and nK = Nnr\u2212 \u2211K\u22121 k=1 nk = Nnr\u2212m. It follows that the\nobejctive value of n is\n1 2 (Nnr\u03bb+ nK) = Nnr \u2212 1 2 m \u2264 Nnr \u2264\n(Nnr + 1) 2\n4 ,\nthat is, smaller than that of n\u2217. This finishes the proof. (b) If nK\u22121 > 0, then \u00b5K\u22121 = 0 by (16a). It follows from (16b) with k = K \u2212 1 that\n0 = nK\u22122+nK\u2212\u00b5K\u22121\u2212\u03bb = nK\u22122+nK\u2212nK\u22121\u22121 =\u21d2 nK\u22121\u2212nK = nK\u22122\u22121. But nK\u22121 + nK = N \u2212m by definition of m. This implies that\nnK\u22121 = 1\n2 (Nnr \u2212m+ nK\u22122 \u2212 1) and nK =\n1 2 (Nnr \u2212m\u2212 nK\u22122 + 1).\nThen, the objective value of n is\n1 2 (Nnr\u03bb+ nK) = 1 2 Nnr(nK\u22121 + 1) + 1 2 nK\n= 1\n4\n[ N2nr + (nK\u22122 + 2\u2212m)Nnr + (1\u2212 nK\u22122 \u2212m) ] \u2264 1\n4\n[ N2nr + 2Nnr + (1\u2212 2m) ] \u2264 1\n4 (Nnr + 1)\n2,\nwhere the first inequality is because, with fixed m, setting nK\u22122 = m maximizes the objective value. Hence, the objective value of n is smaller than that of n\u2217, finishing the proof.\n2. If \u00b5K > 0 and \u00b5K\u22121 = 0, then nK = 0 by (16a) and so (16b) implies that, with k = K\u22121, 0 = \u00b5K\u22121 \u2212 \u03bb+ nK\u22122 + nK = \u2212\u03bb+ nK\u22122,\nthat is, \u03bb = nK\u22122. The following two sub-cases show that n cannot be better than n\u2217. (a) If \u00b5K\u22122 > 0, then \u03bb = nK\u22122 = 0 by (16a). Hence, the objective value of n is\n(Nnr\u03bb+ nK)/2 = 0, smaller than that of n\u2217. This finishes the proof. (b) If \u00b5K\u22122 = 0, then (16b) with k = K \u2212 2 implies that\n0 = \u00b5K\u22122 + nK\u22123 + nK\u22121 + \u03bb = nK\u22123 + nK\u22121 + \u03bb,\nthat is, \u03bb = nK\u22123 + nK\u22121. But (15b) implies that\nNnr \u2265 nK\u22123 + nK\u22122 + nK\u22121 = 2\u03bb =\u21d2 \u03bb \u2264 Nnr\n2 .\nIt follows that the objective value of n is\n1 2 (Nnr\u03bb+ nK) \u2264 N2nr 4 \u2264 (Nnr + 1) 2 4 ,\nthat is, smaller than that of n\u2217. This finishes the proof.\n3. If \u00b5K > 0 and \u00b5K\u22121 > 0, then nK = nK\u22121 = 0 by (16a). Let j \u2208 [K \u2212 2] denote the largest index such that nj > 0. Then, \u00b5j = 0 by (16a). We discuss the following three sub-cases about j to show that n cannot be better than n\u2217.\n(a) If j = 1, then \u00b51 = 0 and n2 = 0 by definition of j. It follows from (16b) with k = 1 that\n0 = n2 + \u00b51 \u2212 \u03bb = \u2212\u03bb =\u21d2 \u03bb = 0.\nHence, the objective value of n is (Nnr\u03bb+ nK)/2 = 0, smaller than that of n\u2217. This finishes the proof.\n(b) If j \u2265 2 and nj\u22121 > 0, then \u00b5j\u22121 = \u00b5j = 0 by definition of j. It follows from (16b) with k = j \u2212 1 and k = j that\n\u00b5j\u22121 \u2212 \u03bb+ nj\u22122 + nj = 0 =\u21d2 \u03bb = nj\u22122 + nj , \u00b5j \u2212 \u03bb+ nj\u22121 + nj+1 = 0 =\u21d2 \u03bb = nj\u22121 + nj+1.\nBut (15b) implies that\nNnr \u2265 nj\u22122 + nj\u22121 + nj + nj+1 = 2\u03bb =\u21d2 \u03bb \u2264 Nnr\n2 .\nIt follows that the objective value of n is\n1 2 (Nnr\u03bb+ nK) \u2264 N2nr 4 \u2264 (Nnr + 1) 2 4 ,\nthat is, smaller than that of n\u2217. This finishes the proof.\n(c) If j \u2265 2 and nj\u22121 = 0, then (16b) with k = j implies that\n0 = \u00b5j \u2212 \u03bb+ nj\u22121 + nj+1 = \u2212\u03bb =\u21d2 \u03bb = 0,\nwhere the second equality follows from the definition of j. Hence, the objective value of n is (Nnr\u03bb+ nK)/2 = 0, smaller than that of n\u2217. This finishes the proof.\nA.2 PROOF OF PROPOSITION 3.\nConsider functions f : R \u2192 R, g : Rn \u2192 R, and their composite f(g(x)). It can be shown that f(g(x)) is supermodular in x if one of the following is satisfied (see, e.g., Proposition 2.2.5(c) in Simchi-Levi et al. (2005)):\n1. f is increasing and convex, and g is increasing and supermodular;\n2. f is decreasing and convex, and g is increasing and submodular.\nHere, a function g is called increasing if g(x) \u2264 g(x\u2032) whenever x \u2264 x\u2032, decreasing if \u2212g is increasing, and submodular if \u2212g is supermodular. We proceed to prove by induction. For each k \u2264 K, suppose that zk\u22121 is increasing and supermodular in x. Then, Wkzk\u22121 + bk + Dkx is increasing and supermodular in x because Wk \u2265 0 and Dk \u2265 0. It follows that zk is increasing and supermodular in x because \u03c3 is increasing and convex. In particular, zK is increasing and supermodular in x.\nFinally, \u03c6\u0303 is supermodular in x by (6c) because WK+1 \u2265 0 and DK+1x is supermodular (actually linear) in x. This finishes the proof.\nA.3 PROOF OF PROPOSITION 4.\nDefine x\u2032i := 1\u2212 xi for all i \u2208 [n] and m\u03c6 := minz\u2208{0,1}n \u03c6(z). Then, any arbitrary \u03c6 : {0, 1}n \u2192 R can be rewritten as\n\u03c6(x) = \u2211\nz\u2208{0,1}n \u03c6(z) n\u220f i=1 [ zixi + (1\u2212 zi)x\u2032i ]\n= \u2211\nz\u2208{0,1}n \u03c6(z) max { n\u2211 i=1 [ zixi + (1\u2212 zi)x\u2032i ] \u2212 (n\u2212 1), 0 }\n= \u2211\nz\u2208{0,1}n \u03c6(z) \u03c3 ( n\u2211 i=1 [ zixi + (1\u2212 zi)x\u2032i ] \u2212 (n\u2212 1) )\n= m\u03c6 + \u2211\nz\u2208{0,1}n\n( \u03c6(z)\u2212m\u03c6 ) \u03c3 ( n\u2211 i=1 [ zixi + (1\u2212 zi)x\u2032i ] \u2212 (n\u2212 1) ) ,\nwhere the second equality is because all xi and x\u2032i in the product are binary, the third equality is by the definition of the activation function, and the last equality is because \u03c3 (\u2211n\ni=1 [ zixi + (1\u2212 zi)x\u2032i ] \u2212 (n\u2212 1) ) = 1 if and only if z = x. Then, the coefficients of all\nxi and x\u2032i are nonnegative (because zi \u2265 0 and 1 \u2212 zi \u2265 0), and the coefficients of all activation functions are \u03c6(z) \u2212m\u03c6 \u2265 0. It follows that \u03c6(x) can be represented using an ISNN with an architecture as in Figure 2 with K = 1 hidden layer, which consists of 2n neurons and nonnegative coefficients only. This proves claim (i).\nFor an ISNN as shown in Figure 2, \u03c6\u0303(x\u0303) admits a closed form expression as in (6). Since W1:K+1 and D2:K are nonnegative, the expression of zk,i, the ith entry of zk, in ISNN can be rewritten as\nzk,i = \u03c3 \u2211 j Wk,ijzk\u22121,j + bk,i +Dk,ix\u0303  = \u03c3\n\u2211 j Wk,ij\u03c3(Wk\u22121,jzk\u22122 + bk\u22121,j +Dk\u22121,j x\u0303) + bk,i +Dk,ix\u0303  = \u03c3\n\u2211 j \u03c3(Wk,ijWk\u22121,jzk\u22122 +Wk,ijbk\u22121,j +Wk,ijDk\u22121,j x\u0303) + bk,i +Dk,ix\u0303  = \u03c3\n\u2211 j \u03c3(W \u2032k\u22121,jzk\u22122 + b \u2032 k\u22121,j +D \u2032 k\u22121,j x\u0303) + bk,i +Dk,ix\u0303  , where Wk,ij denotes the entry of Wk in row i and column j, Wk\u22121,j denotes the jth row of Wk\u22121, and\nW \u2032k\u22121,j := Wk,ijWk\u22121,j \u2265 0, b\u2032k\u22121,j := Wk,ijbk\u22121,j , D\u2032k\u22121,j := Wk,ijDk\u22121,j \u2265 0.\nThat is, the same zk,i can be represented by an ISNN with Wk,ij = 1 for all i, j, i.e., Wk is an all-one matrix. In light of this, the ISNN admits the following equivalent expression:\nz1 = \u03c3(W1x\u0303+ b1)\nzk = \u03c3(1zk\u22121 + bk +Dkx\u0303), k = 2, ...,K\n\u03c6\u0303 = 1>zK + bK+1 +DK+1x\u0303,\nwhere 1 denotes an all-one vector or all-one matrix. Hence, the total number of parameters Npm to be trained in the ISNN is\nNpm = n1(2n+ 1) + K\u2211 k=2 nk(2n+ 1) + (2n+ 1)\n= ( K\u2211 k=1 nk + 1 ) (2n+ 1)\n= (Nnr + 1)(2n+ 1), where zk \u2208 Rnk for all k \u2208 [K] and so the number of neurons in the ISNN is Nnr = \u2211K k=1 nk. Therefore, for fixed Nnr, Npm is independent of W and K. This proves claim (ii).\nTo fit the Ns sample-label pairs exactly, we need the number of parameters Npm of the ISNN to be at least as large as Ns, i.e.,\nNpm = (Nnr + 1)(2n+ 1) \u2265 Ns. It follows that\nNnr \u2265 Ns\n2n+ 1 \u2212 1,\nproving claim (iii).\nA.4 APPROXIMATION ERROR OF \u03c6\u0303\nWe evaluate the error of using the \u03c6\u0303 obtained from neural networks (GNN or ISNN) to approximate the true (but unknown) value function \u03c6, defined as \u2016\u03c6 \u2212 \u03c6\u0303\u2016\u221e := maxx\u2208X |\u03c6(x) \u2212 \u03c6\u0303(x)|. The following proposition provides an upper bound of the approximation error, as a function of the lower-level problem and the neural network. Proposition 6. Consider a mixed-integer and linear lower-level problem, i.e., g(y, x) := g>y and Y (x) := {y \u2208 Rn\u2212p+ \u00d7 Z p + : Gy = Tx+ h}, where g \u2208 Rm, G \u2208 Rq\u00d7m, T \u2208 Rq\u00d7n, and h \u2208 Rq . Then, it holds that \u2016\u03c6\u2212 \u03c6\u0303\u2016\u221e \u2264 (C\u2016g\u20162 + L)d,\nwhere C is a constant depending only on n and G, L is a constant depending only on the neural network coefficients {Wk}K+1k=1 and {Dk} K+1 k=2 , and d is the Hausdorff distance between the sets of sampled and unsampled points in X , i.e.,\nd := max x\u2208X\\\u2126 min z\u2208\u2126 \u2016x\u2212 z\u20162.\nProof: Pick any x, x\u2032 \u2208 X . First, using the representation (6) of the neural network, we have \u2016z1 \u2212 z\u20321\u20162 \u2264 \u2016W1\u20162(x\u2212 x\u2032), and\n\u2016zk \u2212 z\u2032k\u20162 \u2264 \u2016Wk\u20162 \u2016zk\u22121 \u2212 z\u2032k\u22121\u20162 + \u2016Dk|\u20162 \u2016x\u2212 x\u2032\u20162 for all k = 2, . . . ,K. A mathematical induction on k yields that\n|\u03c6\u0303(x)\u2212 \u03c6\u0303(x\u2032)| \u2264 L\u2016x\u2212 x\u2032\u20162, where L = \u220fK+1 k=1 \u2016Wk\u20162 + \u2211K+1 k=2 \u2016Dk\u20162( \u220fK+1 `=k+1 \u2016W`\u20162).\nSecond, since \u03c6(x) represents the optimal value of a mixed-integer linear program parameterized by x, by Cook et al. (1986) we have\n|\u03c6(x)\u2212 \u03c6(x\u2032)| \u2264 C1\u2016g\u20162 \u2016x\u2212 x\u2032\u20162 + C2\u2016g\u20162, where C1, C2 are constants that depend only on n and G. But since x, x\u2032 \u2208 {0, 1}n, \u03c6(x)\u2212\u03c6(x\u2032) = 0 when x = x\u2032 and \u2016x\u2212 x\u2032\u20162 \u2265 1 when x 6= x\u2032. It follows that\n|\u03c6(x)\u2212 \u03c6(x\u2032)| \u2264 C1\u2016g\u20162 \u2016x\u2212 x\u2032\u20162 + C2\u2016g\u20162 \u2016x\u2212 x\u2032\u20162 = C \u2016g\u20162 \u2016x\u2212 x\u2032\u20162,\nwhere C = C1 + C2.\nThird, pick a z \u2208 argminz\u2208\u2126\u2016x\u2212 z\u20162. Then,\n|\u03c6(x)\u2212 \u03c6\u0303(x)| = |\u03c6(x)\u2212 \u03c6(z) + \u03c6\u0303(z)\u2212 \u03c6\u0303(x)| \u2264 |\u03c6(x)\u2212 \u03c6(z)|+ |\u03c6\u0303(z)\u2212 \u03c6\u0303(x)| \u2264 C \u2016g\u20162 \u2016x\u2212 z\u20162 + L \u2016x\u2212 z\u20162 = (C\u2016g\u20162 + L)\u2016x\u2212 z\u20162,\nwhere the first equality is because z \u2208 \u2126. Since x is arbitrary, taking the maximum over all x \u2208 X of both sides of the inequality finishes the proof."
        },
        {
            "heading": "B EFFICIENT CALCULATION FOR ISNN",
            "text": "Although Section 3 provides a framework to solve general BPs through value function approximation, if there are total Nnr neurons in the neural network, we need to introduce Nnr auxiliary binary variables \u03b4, Nnr auxiliary continuous variables z\u2032, and 4Nnr big-M constraints (see (11)) to represent \u03c6\u0303(x\u0303), which weakens its scalability.\nThanks to the supermodularity of \u03c6\u0303(x\u0303) promised by Proposition 3, we can replace (12c) with linear inequalities without introducing auxiliary binary variables or big-M constraints as in (11). To this end, we define set [2n] := {1, 2, . . . , 2n} and the indicator set of a binary vector x\u0303 as S(x\u0303) := {1 \u2264 k \u2264 2n : x\u0303k = 1}. In addition, we define a set function \u03d5 : 2[2n] \u2192 R such that \u03d5(S(x\u0303)) := \u03c6\u0303(x\u0303) for all x\u0303 \u2208 {0, 1}2n. Then, the following proposition recasts the (nonlinear) constraint g(y, x) \u2265 \u03c6\u0303(x\u0303) as linear inequalities.\nProposition 7. (Adapted from Theorem 6 of Nemhauser & Wolsey (1981)). If \u03c6\u0303(x\u0303) is supermodular in x\u0303, then for all x\u0303 \u2208 {0, 1}2n, g(\u00b7) \u2265 \u03c6\u0303(x\u0303) if and only if\ng(\u00b7) \u2265 \u03d5(S)\u2212 \u2211 k\u2208S \u03c1([2n]\\{k}, k)(1\u2212 x\u0303k)\n+ \u2211\nk\u2208[2n]\\S\n\u03c1(S, k)x\u0303k, \u2200S \u2286 [2n] (17)\nwhere \u03c1(S, k) := \u03d5(S \u222a {k})\u2212 \u03d5(S) for all S \u2286 [2n] and k \u2208 [2n]\\S.\nAs compared to the reformulation (11), (17) does not introduce any new auxiliary variables. Yet, it involves an exponential number of linear inequalities which can nevertheless increase the formulation size. Fortunately, the inequalities (17) can be easily separated, that is, given fixed x\u0302 and other variables, in polynomial time one can certify that g(\u00b7) \u2265 \u03c6\u0303(x\u0302) or find an violated inequality (17) with respect to some S. This is because a worst-case S\u2217 on the right-hand side of (17), i.e.,\nS\u2217 \u2208 argmax S\u2286[2n]\n{ \u03d5(S)\u2212 \u2211 k\u2208S \u03c1([2n]\\{k}, k)(1\u2212 x\u0302k)\n+ \u2211\nk\u2208[2n]\\S\n\u03c1(S, k)x\u0302k } admits a closed-form solution S\u2217 = S(x\u0302) (see Ljubic\u0301 and Moreno (2018) and Qi et al. (2022)). Consequently, there is no need to incorporate the exponentially many inequalities (17) up front, and we only need to incorporate the violated ones on-the-fly, e.g., in a branch-and-bound algorithm for solving the BP.\nC INSTANCE GENERATION\nFollowing the instance generation rules of (Tahernejad et al., 2020), all instances are generated in the following forms:\nmin x\ncTx+ dT1y\ns.t. A1x \u2264 b1 x \u2208 {0, 1}n\ny \u2208 arg max y dT2y\ns.t. A2x+B2y \u2264 b2 0 \u2264 y \u2264 y y \u2208 Rm (or y \u2208 Zm),\nwhere c is a n\u00d7 1 vector, d1 and d2 are m\u00d7 1 vectors. The constraint number is set to be the same as the variable number of each level, and hence, A1 is a n \u00d7 n matrix, b1 is a n \u00d7 1 vector, A2 is a m \u00d7 n matrix, B2 is a m \u00d7 m matrix, and b2 is a m \u00d7 1 vector. For instances with a linear lower-level problem, y is continuous variables; for instances with a mixed-integer linear lower-level problem, y is integer variables. In our numerical experiments, we setm = 20, y = 1, and generate 6 instances with n = 10, 20, 30, 40, 50, 60. The coefficients are randomly generated in a range given in the following table, where \u03b4 = 200/(m+ n) and d1 = d2."
        },
        {
            "heading": "D SUPPLEMENTARY RESULTS",
            "text": "D.1 OPTIMALITY CUT IN ILLUSTRATIVE EXAMPLE\nD.2 DETAILS IN GENERATED INSTANCES\nD.3 COMPARISON OF SAMPLING METHODS\nWe compare the efficiency of three sampling methods, including random sampling, Latin hypercube sampling (LHS), and the proposed enhanced sampling. We set a fixed sampling time of 30 minutes and adopt the generated MILP instances for numerical experiments. We report the number of obtained samples in Figure 13. We note that when n = 10, the maximum possible number of samples is 1024, for which enumeration can be adopted to find all feasible x. Yet when n \u2265 20, enumeration becomes impractical and sampling is required.\nFrom Figure 13, we see that the performance of random sampling is unstable and strongly depends on the lower-level problem. When the lower-level problem has a relatively large feasible region, such as the instance n = 20 or n = 60, it is easy for random sampling to find a feasible x, for which sampling efficiency is high. Yet if the lower-level problem has a relatively small feasible region, such as the instance n = 30 or n = 40, the sampling efficiency of random sampling gets quite low and can not find enough samples for training. In addition, because we do not have probability information, LHS has a similar performance to random sampling in most instances. However, through the proposed enhanced sampling, we guarantee to find a feasible x in each sampling, and from Figure 13, the number of obtained samples is almost linearly decreasing with n. It is reasonable because a larger n causes a longer time consumption for each sampling and thus fewer samples under a fixed sampling time. Therefore, to avoid the unstable performance of naive random sampling or other non-trivial sampling, we adopt the proposed enhanced sampling for all instances.\nD.4 HIGHER DIMENSIONAL INSTANCES\nWe generate some larger instances with n up to 120. We consider a MILP lower level and conduct experiments with a similar setup to Section 4.2.1. Figure 14 shows the numerical results.\nFrom Figure 14(a), the average objective difference tends to increase as n gets larger. When n \u2265 100, the average objective difference gets higher than 5%. The results are reasonable because more samples are required to approximate \u03c6(x) when n gets larger yet our sample number is fixed. From Figure 14(b), the computational time increases as n gets larger, and when n \u2265 100, the computational time is longer than the set time limit of MiBS. Similar to Figure 12, sampling consumes the most computational time, for which more advanced sampling is required in future works."
        }
    ],
    "year": 2024
}