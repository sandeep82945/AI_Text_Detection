{
    "abstractText": "Recent research in document image understanding is impeded by the scarcity of high-quality data. We introduce ADoPD, a large-scale dataset for document page decomposition. ADoPD distinguishes itself by utilizing a novel data-driven document taxonomy discovery method for data collection. Our approach takes advantage of both large-scale pretrained models and human-in-the-loop process, ensuring diversity and balance in our data collection. Leveraging our data-driven document taxonomy, we collected and densely annotated labels for document images, covering four document image understanding tasks: Doc2Mask, Doc2Box, Doc2Tag, and Doc2Seq. Specifically, for each image, the annotations include human-labeled entity masks, text bounding boxes, as well as automatically generated tags and captions. We provide detailed experimental analyses to validate our data-driven document taxonomy method and experimentally analyze the four tasks based on different models. We believe that ADoPD has the potential to become a cornerstone dataset to support future research on document image understanding.",
    "authors": [],
    "id": "SP:eb5df62c121ec397f6c900ac340f0a21a444f9dd",
    "references": [
        {
            "authors": [
                "Yejin Bang",
                "Samuel Cahyawijaya",
                "Nayeon Lee",
                "Wenliang Dai",
                "Dan Su",
                "Bryan Wilie",
                "Holy Lovenia",
                "Ziwei Ji",
                "Tiezheng Yu",
                "Willy Chung"
            ],
            "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
            "venue": "arXiv preprint arXiv:2302.04023,",
            "year": 2023
        },
        {
            "authors": [
                "Zhaowei Cai",
                "Nuno Vasconcelos"
            ],
            "title": "Cascade r-cnn: High quality object detection and instance segmentation",
            "venue": "TPAMI, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Kai Chen",
                "Jiaqi Wang",
                "Jiangmiao Pang",
                "Yuhang Cao",
                "Yu Xiong",
                "Xiaoxiao Li",
                "Shuyang Sun",
                "Wansen Feng",
                "Ziwei Liu",
                "Jiarui Xu",
                "Zheng Zhang",
                "Dazhi Cheng",
                "Chenchen Zhu",
                "Tianheng Cheng",
                "Qijie Zhao",
                "Buyu Li",
                "Xin Lu",
                "Rui Zhu",
                "Yue Wu",
                "Jifeng Dai",
                "Jingdong Wang",
                "Jianping Shi",
                "Wanli Ouyang",
                "Chen Change Loy",
                "Dahua Lin"
            ],
            "title": "MMDetection: Open mmlab detection toolbox and benchmark",
            "venue": "arXiv preprint arXiv:1906.07155,",
            "year": 1906
        },
        {
            "authors": [
                "Zhe Chen",
                "Yuchen Duan",
                "Wenhai Wang",
                "Junjun He",
                "Tong Lu",
                "Jifeng Dai",
                "Yu Qiao"
            ],
            "title": "Vision transformer adapter for dense predictions",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Bowen Cheng",
                "Alexander G. Schwing",
                "Alexander Kirillov"
            ],
            "title": "Per-pixel classification is not all you need for semantic segmentation",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Hiuyi Cheng",
                "Peirong Zhang",
                "Sihang Wu",
                "Jiaxin Zhang",
                "Qiyuan Zhu",
                "Zecheng Xie",
                "Jing Li",
                "Kai Ding",
                "Lianwen Jin"
            ],
            "title": "M6doc: A large-scale multi-format, multi-type, multi-layout, multi-language, multi-annotation category dataset for modern document layout analysis",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Michael Denkowski",
                "Alon Lavie"
            ],
            "title": "Meteor universal: Language specific translation evaluation for any target language",
            "venue": "In Proceedings of the ninth workshop on statistical machine translation,",
            "year": 2014
        },
        {
            "authors": [
                "Yuning Du",
                "Chenxia Li",
                "Ruoyu Guo",
                "Cheng Cui",
                "Weiwei Liu",
                "Jun Zhou",
                "Bin Lu",
                "Yehua Yang",
                "Qiwen Liu",
                "Xiaoguang Hu"
            ],
            "title": "Pp-ocrv2: Bag of tricks for ultra lightweight ocr system",
            "venue": "arXiv preprint arXiv:2109.03144,",
            "year": 2021
        },
        {
            "authors": [
                "Mark Everingham",
                "Luc Van Gool",
                "Christopher KI Williams",
                "John Winn",
                "Andrew Zisserman"
            ],
            "title": "The pascal visual object classes (voc) challenge",
            "year": 2010
        },
        {
            "authors": [
                "Luciano Floridi",
                "Massimo"
            ],
            "title": "Chiriatti. Gpt-3: Its nature, scope, limits, and consequences",
            "venue": "Minds and Machines,",
            "year": 2020
        },
        {
            "authors": [
                "Golnaz Ghiasi",
                "Yin Cui",
                "Aravind Srinivas",
                "Rui Qian",
                "Tsung-Yi Lin",
                "Ekin D. Cubuk",
                "Quoc V. Le",
                "Barret Zoph"
            ],
            "title": "Simple copy-paste is a strong data augmentation method for instance segmentation",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Jiuxiang Gu",
                "Jason Kuen",
                "Vlad I Morariu",
                "Handong Zhao",
                "Rajiv Jain",
                "Nikolaos Barmpalios",
                "Ani Nenkova",
                "Tong Sun"
            ],
            "title": "Unified pretraining framework for document understanding",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Jiuxiang Gu",
                "Yifei Ming",
                "Yi Zhou",
                "Jason Kuen",
                "Vlad I Morariu",
                "Handong Zhao",
                "Ruiyi Zhang",
                "Nikolaos Barmpalios",
                "Anqi Liu",
                "Yixuan Li"
            ],
            "title": "A critical analysis of out-of-distribution detection for document understanding",
            "venue": "In EMNLP,",
            "year": 2023
        },
        {
            "authors": [
                "Adam W Harley",
                "Alex Ufkes",
                "Konstantinos G Derpanis"
            ],
            "title": "Evaluation of deep convolutional nets for document image classification and retrieval",
            "venue": "In ICDAR,",
            "year": 2015
        },
        {
            "authors": [
                "Yupan Huang",
                "Tengchao Lv",
                "Lei Cui",
                "Yutong Lu",
                "Furu Wei"
            ],
            "title": "Layoutlmv3: Pre-training for document ai with unified text and image masking",
            "venue": "In ACMMM,",
            "year": 2022
        },
        {
            "authors": [
                "Wenyu Jiang",
                "Hao Cheng",
                "Mingcai Chen",
                "Chongjun Wang",
                "Hongxin Wei. Dos"
            ],
            "title": "Diverse outlier sampling for out-of-distribution detection",
            "venue": "arXiv preprint arXiv:2306.02031,",
            "year": 2023
        },
        {
            "authors": [
                "Geewook Kim",
                "Teakgyu Hong",
                "Moonbin Yim",
                "JeongYeon Nam",
                "Jinyoung Park",
                "Jinyeong Yim",
                "Wonseok Hwang",
                "Sangdoo Yun",
                "Dongyoon Han",
                "Seunghyun Park"
            ],
            "title": "Ocr-free document understanding transformer",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Jordy Landeghem",
                "Rub\u00e9n Tito",
                "\u0141ukasz Borchmann",
                "Micha\u0142 Pietruszka",
                "Pawe\u0142 J\u00f3ziak",
                "Rafa\u0142 Powalski",
                "Dawid Jurkiewicz",
                "Micka\u00ebl Coustaty",
                "Bertrand Ackaert",
                "Ernest Valveny"
            ],
            "title": "Document understanding dataset and evaluation (dude)",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Joonho Lee",
                "Hideaki Hayashi",
                "Wataru Ohyama",
                "Seiichi Uchida"
            ],
            "title": "Page segmentation using a convolutional neural network with trainable co-occurrence features",
            "venue": "In ICDAR,",
            "year": 2019
        },
        {
            "authors": [
                "David D. Lewis",
                "Gady Agam",
                "Shlomo Engelson Argamon",
                "Ophir Frieder",
                "David A. Grossman",
                "Jefferson Heard"
            ],
            "title": "Building a test collection for complex document information processing",
            "venue": "Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,",
            "year": 2006
        },
        {
            "authors": [
                "Junlong Li",
                "Yiheng Xu",
                "Tengchao Lv",
                "Lei Cui",
                "Cha Zhang",
                "Furu Wei"
            ],
            "title": "Dit: Self-supervised pre-training for document image transformer",
            "venue": "In ACMMM,",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi"
            ],
            "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
            "venue": "In ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi"
            ],
            "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597,",
            "year": 2023
        },
        {
            "authors": [
                "Kai Li",
                "Curtis Wigington",
                "Chris Tensmeyer",
                "Handong Zhao",
                "Nikolaos Barmpalios",
                "Vlad I Morariu",
                "Varun Manjunatha",
                "Tong Sun",
                "Yun Fu"
            ],
            "title": "Cross-domain document object detection: Benchmark suite and method",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Minghao Li",
                "Yiheng Xu",
                "Lei Cui",
                "Shaohan Huang",
                "Furu Wei",
                "Zhoujun Li",
                "Ming Zhou"
            ],
            "title": "Docbank: A benchmark dataset for document layout analysis",
            "venue": "In COLING,",
            "year": 2020
        },
        {
            "authors": [
                "Peizhao Li",
                "Jiuxiang Gu",
                "Jason Kuen",
                "Vlad Morariu",
                "Handong Zhao",
                "Rajiv Jain",
                "Varun Manjunatha",
                "Hongfu Liu"
            ],
            "title": "Selfdoc: Self-supervised document representation learning",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Chin-Yew Lin"
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "In Text summarization branches out,",
            "year": 2004
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Minesh Mathew",
                "Dimosthenis Karatzas",
                "CV Jawahar"
            ],
            "title": "Docvqa: A dataset for vqa on document images",
            "venue": "In WACV,",
            "year": 2021
        },
        {
            "authors": [
                "Puneet Mathur",
                "Rajiv Jain",
                "Jiuxiang Gu",
                "Franck Dernoncourt",
                "Dinesh Manocha",
                "Vlad Morariu"
            ],
            "title": "Docedit: Language-guided document editing",
            "venue": "In AAAI,",
            "year": 2023
        },
        {
            "authors": [
                "Yifei Ming",
                "Ziyang Cai",
                "Jiuxiang Gu",
                "Yiyou Sun",
                "Wei Li",
                "Yixuan Li"
            ],
            "title": "Delving into out-ofdistribution detection with vision-language representations",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Ajoy Mondal",
                "Peter Lipps",
                "CV Jawahar"
            ],
            "title": "Iiit-ar-13k: a new dataset for graphical object detection in documents",
            "venue": "In International Workshop on Document Analysis Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Maxime Oquab",
                "Timoth\u00e9e Darcet",
                "Th\u00e9o Moutakanni",
                "Huy Vo",
                "Marc Szafraniec",
                "Vasil Khalidov",
                "Pierre Fernandez",
                "Daniel Haziza",
                "Francisco Massa",
                "Alaaeldin El-Nouby"
            ],
            "title": "Dinov2: Learning robust visual features without supervision",
            "venue": "arXiv preprint arXiv:2304.07193,",
            "year": 2023
        },
        {
            "authors": [
                "Nazih Ouwayed",
                "Abdel Bela\u00efd"
            ],
            "title": "A general approach for multi-oriented text line extraction of handwritten documents",
            "venue": "IJDAR,",
            "year": 2012
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "Wei-Jing Zhu"
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "In ACL,",
            "year": 2002
        },
        {
            "authors": [
                "Birgit Pfitzmann",
                "Christoph Auer",
                "Michele Dolfi",
                "Ahmed S Nassar",
                "Peter Staar"
            ],
            "title": "Doclaynet: A large human-annotated dataset for document-layout segmentation",
            "venue": "In SIGKDD,",
            "year": 2022
        },
        {
            "authors": [
                "Devashish Prasad",
                "Ayan Gadpal",
                "Kshitij Kapadni",
                "Manish Visave",
                "Kavita Sultanpure"
            ],
            "title": "Cascadetabnet: An approach for end to end table detection and structure recognition from image-based documents",
            "venue": "CVPRW,",
            "year": 2020
        },
        {
            "authors": [
                "Lu Qi",
                "Jason Kuen",
                "Tiancheng Shen",
                "Jiuxiang Gu",
                "Weidong Guo",
                "Jiaya Jia",
                "Zhe Lin",
                "Ming-Hsuan Yang"
            ],
            "title": "High-quality entity segmentation",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Yongming Rao",
                "Wenliang Zhao",
                "Yansong Tang",
                "Jie Zhou",
                "Ser-Lam Lim",
                "Jiwen Lu"
            ],
            "title": "Hornet: Efficient high-order spatial interactions with recursive gated convolutions",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "In NeurIPS,",
            "year": 2015
        },
        {
            "authors": [
                "RSM Saad",
                "RI Elanwar",
                "NSA Kader",
                "S Mashali",
                "M Betke"
            ],
            "title": "Bce-arabic-v1 dataset: towards interpreting arabic document images for people with visual impairments categories and subject descriptors. PETRA \u201816",
            "venue": "Corfu Island,",
            "year": 2016
        },
        {
            "authors": [
                "Brandon Smock",
                "Rohith Pesala",
                "Robin Abraham"
            ],
            "title": "Pubtables-1m: Towards comprehensive table extraction from unstructured documents",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Zineng Tang",
                "Ziyi Yang",
                "Guoxin Wang",
                "Yuwei Fang",
                "Yang Liu",
                "Chenguang Zhu",
                "Michael Zeng",
                "Cha Zhang",
                "Mohit Bansal"
            ],
            "title": "Unifying vision, text, and layout for universal document processing",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Ramakrishna Vedantam",
                "C Lawrence Zitnick",
                "Devi Parikh"
            ],
            "title": "Cider: Consensus-based image description evaluation",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz",
                "Joe Davison",
                "Sam Shleifer",
                "Patrick von Platen",
                "Clara Ma",
                "Yacine Jernite",
                "Julien Plu",
                "Canwen Xu",
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush"
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "In EMNLP,",
            "year": 2020
        },
        {
            "authors": [
                "Peng Zhang",
                "Can Li",
                "Liang Qiao",
                "Zhanzhan Cheng",
                "Shiliang Pu",
                "Yi Niu",
                "Fei Wu"
            ],
            "title": "Vsr: a unified framework for document layout analysis combining vision, semantics and relations",
            "venue": "In ICDAR,",
            "year": 2021
        },
        {
            "authors": [
                "Xu Zhong",
                "Jianbin Tang",
                "Antonio Jimeno Yepes"
            ],
            "title": "Publaynet: largest dataset ever for document layout analysis",
            "venue": "In ICDAR,",
            "year": 2019
        },
        {
            "authors": [
                "Xizhou Zhu",
                "Weijie Su",
                "Lewei Lu",
                "Bin Li",
                "Xiaogang Wang",
                "Jifeng Dai"
            ],
            "title": "Deformable detr: Deformable transformers for end-to-end object detection",
            "venue": "In ICLR,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The emergence of large-scale document data has significantly propelled advancements in document understanding research (Zhong et al., 2019; Mondal et al., 2020; Cheng et al., 2023). While these datasets have contributed to advancements in document-related tasks (Mathew et al., 2021; Mathur et al., 2023), they still fall short when compared to datasets in more established fields like computer vision or natural language processing (NLP) (Caron et al., 2021). For example, advancements in image decomposition, fueled by datasets like MSCOCO (Lin et al., 2014) and Pascal VOC (Everingham et al., 2010), inspired real-world applications. To advance document understanding research, building a document page decomposition dataset of comparable quality is essential.\nWe construct ADOPD by addressing two important questions: (1) How do we gather document data, and what types are necessary? The urgency of this question stems from the deficiency of diversity in existing document datasets. Current datasets (Li et al., 2020b; Pfitzmann et al., 2022) predominantly utilize PDFs with homogeneous content, like scientific papers and business documents. This limitation emphasizes the necessity for more varied data, a driving force behind ADOPD. (2) What should\nbe annotated in a document image to achieve document page decomposition? Documents, with their varied forms, can be interpreted differently based on an individual\u2019s background. Document understanding encompasses intricacies such as visuals, text, and layout. For example, a poster with a form may appear visually as a form, but its text could categorize it as a science or education book. The complex nature of document data poses challenges in hierarchically structuring it, a critical aspect for successful vision datasets like ImageNet and MSCOCO. Meanwhile, accurately describing the content of documents is highly valuable, but it is also more challenging than natural image captioning.\nTo lay the foundation for ADOPD, we explore the fundamental question: Is it possible to establish a comprehensive and effective taxonomy for documents? In practice, pre-defining a fixed taxonomy solely based on human knowledge is challenging. Instead of a pre-defined label set, we assume an open taxonomy, gradually assembling it through large-scale data for an data-driven taxonomy discovery method. Meanwhile, data-driven methods involving large-scale data face challenges in reading and comprehending documents. Relying solely on manual annotation is time-consuming and costly. Therefore, we leverage the powerful\nzero-shot capabilities of large pretrained models such as CLIP (Radford et al., 2021) and Large Language Model (LLM) (Floridi & Chiriatti, 2020) to assist in data selection and analysis. Large models, despite their benefits, pose challenges like biases and illusions. To mitigate these issues, we propose adding safeguards. This involves using out-of-distribution (OOD) detection (Gu et al., 2023) for outlier data selection, complemented by a human-in-the-loop approach to achieve data diversity.\nFig. 1 highlights the diverse nature of ADOPD, encompassing both visually rich and text-rich documents, offering advantages in terms of diversity but presenting challenges for annotation. For visually rich documents, such as posters and diagrams, entity masks are effective in capturing the intricate relationships between visual elements. Conversely, text bounding boxes are more suitable for identifying and comprehending the key textual elements in text-rich documents, such as letters and articles. Fig. 2 showcases the four document page decomposition tasks: entity segmentation (DOC2MASK), text detection (DOC2BOX), tagging (DOC2TAG), and captioning (DOC2SEQ). To gather annotations, we propose a \u201cHybrid Data Annotation\u201d approach, which combines machine learning and human expertise. Specifically, we generate tags and captions from pretrained models, guiding annotators in labeling image elements with a strategy tailored to the document type.\nOverall, ADOPD is a large-scale diverse document page decomposition dataset, expected to pave the way for future research in document domain. Our contributions are summarized as follows:\n\u2022 We present ADOPD, comprehensive dataset for document page decomposition, encompassing four distinct tasks: DOC2MASK, DOC2BOX, DOC2SEQ, and DOC2TAG.\n\u2022 We propose a data-driven approach for constructing document taxonomies during data collection and safeguard the ADOPD through outlier detection and human-in-the-loop.\n\u2022 We conduct extensive experiments and analysis on ADOPD, demonstrating its effectiveness and generalization capabilities for document understanding research."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": "Document Datasets. In recent years, several document image datasets have been introduced (Mondal et al., 2020; Smock et al., 2022; Landeghem et al., 2023; Saad et al., 2016). Table 1 compares ADOPD with recent document layout analysis datasets1. PubLayNet (Zhong et al., 2019) comprises images and annotations generated through the automated alignment of PDFs with XML formats. DocBank (Li et al., 2020b) is created using LaTeX-generated PDF files and employs an efficient weakly supervised approach for annotation. DocLayNet (Pfitzmann et al., 2022) relies on human\n1The symbol indicates automatic annotations, \u00b2 represents human annotations, and \u00c6 signifies LLM assistance. P indicates that the document source is PDF, while \u00ec indicates the presence of images.\nannotation rather than automated methods. This dataset encompasses six distinct document types and encompasses a total of 11 annotation categories. M6Doc (Cheng et al., 2023) is a recently introduced dataset featuring approximately 9k modern document images, divided into seven subsets. It contains detailed annotations spanning multiple distinct categories. Current large-scale document image datasets primarily focus on PDFs, unlike the scanned or photographed images commonly encountered in real-world scenarios. This limited data distribution biases trained models. While some publicly available datasets contain real-world data, they cover a narrow range of document layouts and categories. This poses challenges in developing and evaluating document understanding models.\nDocument Models. The document domain has witnessed the emergence of foundational models (Zhang et al., 2021; Li et al., 2020a; Prasad et al., 2020), etc. fueled by advancements in deep learning. Despite the rapid development of document understanding models, the scarcity of high-quality large-scale document data remains a significant challenge. Earlier document layout analysis methods (Ouwayed & Bela\u00efd, 2012; Lee et al., 2019) relied heavily on rule-based and heuristic algorithms. However, their applicability was limited to simple document types, resulting in poor generalization performance. In\naddition to task-driven models, researchers have proposed a range of document pretraining models (Huang et al., 2022; Li et al., 2021; Gu et al., 2021; Tang et al., 2023; Kim et al., 2022). These models are typically pretrained on the IIT-CDIP (Lewis et al., 2006) dataset and evaluated on various document benchmarks. Despite the remarkable performance of these models on benchmark datasets, it is critical to acknowledge that most current image-based document datasets are predominantly composed of a narrow range of document types, failing to capture the heterogeneity of real-world documents. Moreover, the restricted data diversity in these benchmark datasets constrains the development and evaluation of document models."
        },
        {
            "heading": "3 ADOPD DATASET",
            "text": ""
        },
        {
            "heading": "Pre-Labeled",
            "text": "ADOPD stands out among document datasets as it is constructed using diverse web images. Sec. 3.1 introduces document page decomposition tasks, while Sec. 3.2 presents a data-driven approach to discovering document taxonomy for data collection and analysis. Sec. 3.3 proposes a collaborative approach that integrates human annotators with state-of-the-art models to enhance overall efficiency."
        },
        {
            "heading": "3.1 TASK DEFINITION",
            "text": "Fig. 2 illustrates the document page decomposition task defined in this paper, which encompasses four subtasks: DOC2MASK, DOC2BOX, DOC2SEQ, and DOC2TAG.\n\u2022 The DOC2MASK task entails segmenting visual entities in document images in a class-agnostic manner. An \u201centity\" in this context denotes a thing (instance) mask or a stuff mask. For example, in Fig. 1, an entity represents a meaningful and coherent region (e.g., banner, figure, logo, etc).\n\u2022 The DOC2BOX task focuses on identifying text region-of-Interest (RoI) within a document image, regardless of their specific types. The term \u201cbox\" refers to text RoI (e.g., paragraphs and titles, etc).\n\u2022 The DOC2SEQ task involves generating captions for document images, requiring the model to analyze visual elements and structured text. Given the complexity of document images, the model must effectively comprehend visual, textual, and layout information to produce detailed captions.\n\u2022 The DOC2TAG task is akin to image tagging, specifically multi-label image recognition, where the objective is to assign multiple semantic labels to an image. In DOC2TAG, two levels of tagging are utilized: one based on the overall image content and another on specific local regions."
        },
        {
            "heading": "3.2 DATA-DRIVEN DOCUMENT TAXONOMY DISCOVERY",
            "text": "In standard classification scenario, we deal with a given dataset denoted as Dfull, where X represents the input space, and Y = {1, . . . ,K} is the label space. The classification model, denoted as f := g \u25e6 h, consists of a feature extractor h : X \u2192 Rd and a classifier g : Rd \u2192 RK , which maps the input\u2019s feature embedding to K real-valued numbers called logits. In practice, establishing a guiding taxonomy associated with K is crucial for effective data collection, enabling us to manage and assess the diversity of the collected data. However, determining an appropriate value for K in documents is challenging due to the diversity of documents. We draw inspiration from large-scale pretrained models such as CLIP, GPT-4, etc, which have been trained on large-scale datasets and can serve as knowledgeable \u201cexperts\" for data selection. Despite the benefits of pretrained models, the predictions from such models are not always reliable. E.g., LLMs2 tend to suffer from hallucination problems (Bang et al., 2023). Hence, incorporating safeguards into data collection is essential. Fig. 3 provides an overview of our data collection process, which will be detailed in the subsequent sections.\nCan Large-Scale pretrained Models Facilitate Data Collection? Given a document image x \u223c Dfull, we can extract document information using pre-existing models as follows:\n{z, SOCR, SCaption, SAttribute, SLabel} = {h(x), fOCR(x), fI2T(x), fTag(x), fCLIP(x|Y)} (1) {S\u2217Caption, S\u2217Tag} = LLM( SOCR, SCaption, SAttribute, SLabel|Prompt) (2)\nwhere z \u2208 RD is obtained through an image feature extractor h(\u00b7). The sequence SOCR consists of words and their coordinates, extracted by OCR tool fOCR(\u00b7). The caption SCaption is generated by the captioning model fI2T(\u00b7). Tags SAttribute are produced by the image tagging model fTag(\u00b7). Labels SLabel are generated by the CLIP model fCLIP(\u00b7|Y), constrained by Y . Integrating multimodal information, as expressed in Eq.1, for document reasoning poses a significant challenge. As demonstrated in Eq.2, we harness the power of LLMs and formulate prompts to predict tags (S\u2217Tag) and captions (S\u2217Caption) for document images. The ablation study of these prompts is explored in the Appendix.\nHow to Safeguard Data Collection? Despite the impressive zero-shot capabilities of LLMs for sequence reasoning, prediction errors and uncertainties may still arise. Some failure cases can be addressed with stricter prompts. Even so, fully relying on LLMs for data selection poses heavy risks.\nFig. 4 illustrates our data selection diagram, strengthened by outlier detection. For each batch of sampled web images (Dselected), we define it as a mix of in-distribution (ID) (Dpseudo-in) and out-of-distribution (Dpseudo-out) data. In Dpseudo-in, all samples belong to taxonomies we have already explored, while Dpseudo-out comprises samples from taxonomies we haven\u2019t explored yet. Alg. 1 outlines the process where we integrate outlier detection for data collection and taxonomy discovery. Given the dataset pool denoted as Dtfull and t indicates the time step, we initially select a batch of data, denote as\nDtselected-in, from Dfull. Based on the current taxonomy Yt, we first partition Yt into 100 clusters using the K-means algorithm (Jiang et al., 2023). Afterwards, we sample Dtpseudo-in from Dtselected-in based on the K clusters, corresponding to step 1\u20dd in Fig. 4. Specifically, we randomly select a sub-category ytk from cluster k as the representative category. We then use CLIP to sample n\nt documents classified under ytk. The pseudo input ID data Dtpseudo-in comprises a total of 100 \u00b7 nt document images. This selection process ensures a balanced sampling operation within the current taxonomy Yt.\n2Without specific indication, LLM in this paper refers to GPT-4.\nSampling from ID data can lead to biased distributions because models trained on such data may silently fail when faced with OOD inputs. Therefore, enhancing the diversity of ADOPD by incorporating hard negative examples may result in an overall improvement in diversity. Therefore, we explicitly sample a OOD subset Dtpseudo-out from the current candidate pool Dtselected-in \\ Dtpseudo-in, corresponding to step 3\u20dd in Fig. 4. To obtain Dtpseudo-out, we employ K-means to segregate outliers from Dtselected-in. Specifically, we extract image features Z\u0304t for D 0:t\u22121 selected \u222a Dtselectd-in, where D 0:t\u22121 selected = D0:t\u22121pseudo-in \u222a D 0:t\u22121 pseudo-out. In this context, Z\u0304\nt = z\u0304tk, with k \u2208 [0, 100) representing the set of K-means centroids estimated from D0:t\u22121selected \u222aDtpseudo-in. The outlier score is estimated as the Euclidean distance between it and the nearest centroid:\nst(z) = min k\u2208[0,100) ||z \u2212 z\u0304tk||2, z \u2208 (Dtselected-in \\ Dtpesudo-in) (3)\nwhere Dtpseudo-out contains data points with outlier scores ranked in the top nt across K clusters.\nAlgorithm 1: Data-Driven Taxonomy Discovery Input : Y0,Dfull, \u03f5. Output :Expanded Taxonomy Y while True do\n1\u20dd CollectDtselect-in fromD t full ; 2\u20dd SelectDtpseudo-in fromD t selected-in based on Y\nt\u22121 ; 3\u20dd Generate image embeddings Z forD0:t\u22121selected \u222a D t pesudo-in; 3\u20dd Calculate st(z), \u2200z \u2208 (Dtselected-in \\ D t pesudo-in) ; 3\u20dd Select outlier dataDtpesudo-out; foreach x \u223c Dtpesudo-in \u222a D t pesudo-out do\n4\u20dd Predict new labels using four prompters; 4\u20dd Update Yt with the newly predicted labels;\n4\u20dd Refine Yt with human annotator; if |Yt| > \u03f5 then\nStop; else\nt\u2190 t + 1 ; Given the selected ID and OOD data, we have Dtselected = Dtpseudo-in\u222aDtpseudo-out, which is ready for annotation (step 4\u20dd in Fig. 4). Before annotation, we update Yt\u22121 by using the newly selected data Dtselected. Here, we employ the approach outlined in Eq. 2, leveraging the LLM to predict the presence of new labels and obtain the updated taxonomy Yt. We use promptbased methods to predict document tags by considering four aspects: visual (Pvisual), textual (Ptextual), layout (Playout), and multimodal (Pmultimodal). Each aspect is addressed through unique input combinations in the prompt. Additional details about the prompts can be found in the Appendix. After obtaining outputs, we implement two safeguards to filter out failures. Firstly, we design a prompt-based summarizer (Psummary) using LLM to obtain 10 tags by summarizing the tags predicted through the four prompt strategies. Secondly, after the label generation by LLM, human annotators review and eliminate labels that are confusing or irrelevant to the document."
        },
        {
            "heading": "3.3 MODEL-ASSISTED DATA ANNOTATION",
            "text": "Data Collection. The images in ADOPD are sourced from the Laion-HR (Laion High Resolution), which comprises high-resolution web images, including multilingual document images. Laion-HR provides a foundation for our multi-lingual multi-modal ADOPD. We leverage pretrained models with humans in the loop to collect and filter data. The process includes the following steps:\n\u2022 Model-Assisted Data Selection: We first select images based on Laion-HR\u2019s metadata by applying criteria such as pwatermark < 0.8 and punsafe < 0.5. Then, we construct a document discovery dataset using natural image datasets (e.g., ImageNet, etc) and document datasets (e.g., DocLayNet, etc). We then finetune a DiT-based binary image classifier (Li et al., 2022a) to identify potential documents (probability > 0.8). Subsequently, we apply an OCR tool (Du et al., 2021), and retain those with a word count exceeding 103. Additionally, we train a watermark detection model to filter watermarked images. In managing redundancy within Laion-HR, despite different image URLs, we calculate MD5 hashes and Hamming distances between images to exclude duplicates. \u2022 Human Selection and Sensitive Verification: Based on our taxonomy4 obtained by Alg. 1, we adopt pretrained CLIP model for zero-shot tagging. Human annotators then carefully select safe and valid images for all categories. We do not rigidly specify that images must be print-format documents, but instead suggested the annotators to choose those that resemble documents. Annotators are tasked with filtering the dataset for potentially sensitive information (e.g., race, ethnic origins, sexual orientations, religious beliefs, biometric information, and government identification forms, etc.).\n3Fig. 9 in the Appendix shows the percentage of data selection. 4Note that the taxonomy Y gradually change with the growth of data collection.\nData Annotation. The annotation process of ADOPD strongly adheres to the main principle\u2014understanding the structure and layout of the document. We do not impose rigid strict constraints on entity/word labeling. Annotators can label both \u201cstuff/things\u201d and \u201ctext RoI\u201d with great flexibility.\n\u2022 Model-Assisted Manual Annotation: In the early stage, we utilize a pretrained CropFormer (Qi et al., 2023) to generate pseudo entity masks. Annotators follow guidelines to adjust the masks by adding, modifying, or deleting as needed. After annotating a sufficient amount of data, CropFormer is retrained with the new annotations and serves as the seed model for data preprocessing in the subsequent stage. Through this iterative process, our model progressively reduces annotation costs while simultaneously increasing annotation efficiency5. During the annotation process, we provide document captions (S\u2217Caption) and tags (S \u2217 Tag) to aid annotators in understanding the document. \u2022 Multi-Task and Multi-Lingual Annotation: ADOPD stands out from other document datasets for its multi-task and multi-lingual characteristics. Our primary focus is on English and CJK (Chinese, Japanese, Korean) documents, amounting to 60k document images in English and rest for the other languages. We reserve a private test set for the competition. Each dataset has four tasks introduced in Sec. 3.1. Specifically, for DOC2MASK annotation, we refrain from imposing semantic constraints on labeling entities, therefore encouraging annotators to come up with openended names or descriptions that are accurate (e.g., \u201cdoc-in-doc\u201d, \u201cbanner\u201d, \u201cinfographic\u201d, \u201cnatural image\u201d, etc). Note that we exclude the use of labels for evaluations. For DOC2BOX, we have stricter rules which require annotators to comprehend words and them according to their semantic meaning. The annotation files follow the MSCOCO annotation format. The comprehensive guidelines, along with the complete dataset, will be available for reference."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 IMPLEMENTATION DETAILS",
            "text": "Baseline Models. We experiment on the subset of ADOPD, with training and validation sets comprising 50k and 10k images, respectively. (1) DOC2MASK: we evaluate two frameworks: Mask2Former (Cheng et al., 2021) and CropFormer (Qi et al., 2023)6. We perform ablation studies on these frameworks using different backbones, such as Swin Transformer (Swin) (Liu et al., 2021), Hornet (Rao et al., 2022), and ViT (Parmar et al., 2018). (2) DOC2BOX: we benchmark three models: Faster R-CNN (Ren et al., 2015), Deformable-DETR (Zhu et al., 2021), and Cascade Mask-RCNN (Cai & Vasconcelos, 2019). We also enhance Cascade Mask-RCNN by incorporating pretrained ViT backbones, specifically DINOv1 (Caron et al., 2021) and Dinov2 (Oquab et al., 2023) with ViT-Adapter (Chen et al., 2022). (3) DOC2SEQ: we build an encoder-decoder model using pretrained ViT and GPT-2 (Radford et al., 2019), fine-tuned on 80k image-caption pairs for training and 20k for validation. The captions are generated using prompts specified in Eq. 2. Acknowledging the gap between LLM-generated and human annotations, we collect an extra 5k human-annotated validation set for further comparison. (4) DOC2TAG: we validate our taxonomy discovery using the CLIP ViT-G/14 model and report the OOD performance on RVL-CDIP (Harley et al., 2015).\nWe build DOC2MASK using the Detectron2 (Wu et al., 2019) and DOC2BOX with MMDetection (Chen et al., 2019). All experiments are run on NVIDIA A100-80GB GPUs. Following standard practices(Ghiasi et al., 2021), we employ an input resolution of 1024\u00d71024, achieved by re-scaling and padding the shorter side of the image. DOC2MASK (CropFormer and Mask2Former) and DOC2BOX (Faster R-CNN, Cascade Mask-RCNN) are trained for 15 epochs with a batch size of 32 on 8 GPUs to achieve full convergence. We train Deformable-DETR for 30 epochs due to slow convergence issues. We build other models (DOC2SEQ and DOC2TAG) with Huggingface Transformers framework (Wolf et al., 2020). For DOC2SEQ, we train it for 50 epochs on 8 GPUs with a total batch size of 800. Finetuning CLIP ViT-G/14 on DOC2SEQ data takes 100 epochs on 8x8 GPUs.\nEvaluation Metrics. We evaluate DOC2MASK and DOC2BOX with the mean average recall (mAR) and mean average precision (mAP) metrics. This assessment considers ten overlap thresholds ranging from 0.5 to 0.95 in increments of 0.05 (mAP@0.5-0.95). For OOD evaluation, we use metrics including the Area Under the Receiver Operating Characteristic (AUROC), False Positive Rate at 95% Recall (FPR95), maximum concept matching (MCM) score (Ming et al., 2022), and accuracy\n5Fig. 13 and Fig. 12 in the Appendix illustrate the effectiveness of model-assisted annotation. 6https://huggingface.co/datasets/qqlu1992/Adobe_EntitySeg\n(ACC). For DOC2SEQ, we use the BLEU@n (B@n) (Papineni et al., 2002), CIDEr (C) (Vedantam et al., 2015), METEOR (M) (Denkowski & Lavie, 2014) and ROUGE (R) (Lin, 2004) for evaluation."
        },
        {
            "heading": "4.2 DOCUMENT PAGE DECOMPOSITION TASKS ANALYSIS",
            "text": "This section presents the evaluation of four tasks. We begin by testing the core goal of ADOPD: providing a comprehensive dataset tailored for document page decomposition.\nComparing the Model Architectures. Table 2 presents a comparative analysis of models categorized into Mask2Former and CropFormer on DOC2MASK. Overall, CropFormer outperforms Mask2Former, leveraging similar backbones and pretrained datasets. This superiority is attributed to CropFormer\u2019s integration of image crops, in addition to the full image input used by Mask2Former. These crops enhance mask prediction by providing more detailed and fine-grained image information. This emphasizes the importance of the model\u2019s ability to handle multi-view and local image information, especially in the context of document images.\nTable 3 compares various object detection models, such as Faster R-CNN, Deformable-DETR, and Cascade Mask R-CNN (MR-CNN). It reveals that Deformable-DETR, while showing\nimprovement, does not significantly outperform anchor-based detectors like Faster R-CNN and Cascade Mask-RCNN. Despite achieving a higher mAR, the limited mAP improvement may be attributed to the distinctive data distribution of text boxes, which differs from general objects in natural images with clear classification boundaries. Simultaneously, the Cascade MR-CNN, a fusion of Mask R-CNN and Cascade R-CNN, achieves the highest mAP. It improves instance segmentation performance and benefits text detection, particularly for words where pixel-level feature representation is crucial.\nComparing Backbones and Pretraining. Table 2 explores the effects of the vision backbone. SAM (Kirillov et al., 2023) pretrained on SA1B outperforms the Swin/Hornet models trained on ImageNet or EntitySeg. This can be attributed to two factors: firstly, SA1B is sufficiently large (around 1 Billion), and there is some distribution overlap with ADOPD. Secondly, while Swin/Hornet architectures are well-suited for segmentation, SAM is trained with pixel-level supervised learning, enabling it to acquire improved pixel-level representations crucial for document image understanding.\nTable 3 compares different backbones on DOC2BOX. Dinov2P14+VITAdapter achieves the best mAP with a slightly lower mAR. Fig. 5b visualizes its results. This emphasizes that, with the right pretraining strategy, self-supervised backbones can learn better features compared to supervised pretrained ResNet. This is crucial for document analysis due to the lack of high-quality ImageNet-like pretraining data in the document domain. Comparison of Dinov1P8 and Dinov1P16 reveals the importance of patch\nsize in document tasks and indicates that fine-grained patches enhance document image features.\nEvaluating Generalization Ability. In Table 4 (a), we compare the model trained on ADOPD with those fine-tuned on EntitySeg. Combined with Fig. 5a, it is evident that models fine-tuned on ADOPD can better focus on fine-grained document elements and make more reasonable predictions for document entity masks. Conversely, models pretrained on EntitySeg can predict some masks but tend to excessively detect elements present in natural images (e.g., people, objects), while neglecting the document\u2019s inherent layout. Table 4 (b) validates the cross-dataset generalization pretraining advantage of ADOPD, specifically focusing on the evaluation set of DocLayNet. For a\nR aw\nI m\nag e\nA D\noP D\n-D oc\n2M as k E nt it yS eg V 2\n(a) Mask Prediction Comparison: From top to bottom, we showcase the original image and predictions from the best models trained on ADOPD, EntityV2, and SAM (SA1B), respectively.\nfair comparison, we consider only text detection without categorizing the boxes. Directly applying the model fine-tuned on ADOPD to DocLayNet data yields zero-shot results with high recalls. Furthermore, fine-tuning on DocLayNet with ADOPD pretrained backbones outperforms fine-tuning with ImageNet backbones. Note that DocLayNet\u2019s testing is limited to its included types, without evaluating the generalization capability of ADOPD for other taxonomy types.\nPrompt-Guided Context-Aware Captioning Benefits Vision-Language Modeling Table 4.2 evaluates caption quality. We collect 5K test data to evaluate the effectiveness of DOC2SEQ. The \u00c6 represents the GPT-4 model, and BLIPLarge (Li et al., 2022b) and BLIP2-OPT-2.7b (Li et al., 2023) are obtained from Huggingface model hub. ViTBase-P32-384/ViTBase-P16-384+GPT2 are finetuned on DOC2SEQ. Although GPT-4 captions achieve a commendable CIDEr score which signifies consensus, a noticeable disparity persists between GPT-4-generated captions and human annotations. Models fine-tuned on DOC2SEQ can attain similar performance to GPT-4 on B@n, but they demonstrate significantly lower CIDEr scores. Fig. 6 (left) shows the lengths of captions. We can see that human annotated captions are significantly longer than those generated by machines, which affect the evaluation results. These findings illustrate that document captioning is challenging due to the diverse interpretations and lengths of captions, which complicate the evaluation process. To verify the benefits of prompt-guided captions, we fine-tune CLIP with DOC2SEQ data and conduct\ntwo experiments: zero-shot evaluation on RVL-CDIP test set and supervised training on RVL-CDIP based on finetuned CLIP vision backbone. While finetuned CLIP improves zero-shot capability for specific data (e.g., Budget and Presentation), the overall enhancement is comparable to raw pretrained CLIP. We observe from Fig. 6 (right) and Table 6 that finetuning the CLIP ViT backbone, initially trained on DOC2SEQ, with a classifier layer for separate training on RVL-CDIP results in a noticeable improvement. This underscores the importance of caption rewriting for handling noisy data.\n0 50 100 150 200 250 300 350 Number of Words\n0\n50\n100\n150\n200\n250\n300\n350\nN um\nbe r o\nf C ap\ntio ns 60\n.9 8\n34 .8\n4\nDocument Caption Word Count Distribution\nHuman Annotated Caption LLM Generated Caption\n0 5 10 15 20 25 Epoch\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nA cc\nur ac\ny (%\n)\nRVL-CDIP Classification Performance\n+ADoPD LAION\nFigure 6: Ablation study on captions.\nMethod Test B@1 B@2 B@3 B@4 M R C \u00c6 \u00b2 27.0 16.7 11.7 8.5 12.8 22.4 84.7 BLIPLarge \u00c6 12.4 8.5 6.4 5.0 9.3 22.6 18.3 \u00b2 8.2 5.6 4.0 3.0 8.6 21.6 3.4 BLIP2-OPT-2.7b \u00c6 4.3 3.6 3.0 2.6 10.7 25.1 16.5 \u00b2 12.3 7.6 5.3 3.9 9.0 21.8 18.0 ViTBase-P32-384+GPT2 \u00c6 22.5 9.2 4.4 2.5 7.7 16.8 9.8 \u00b2 16.7 5.8 2.3 1.0 5.8 13.9 4.4 ViTBase-P16-384+GPT2 \u00c6 23.4 9.7 4.7 2.7 8.0 17.2 11.0 \u00b2 17.3 6.0 2.4 1.1 6.0 14.1 5.3\nTable 5: Ablation experiments on DOC2SEQ.\nData-Driven Document Taxonomy Analysis. To verify Alg. 1, we collect the ID dataset from both RVL-CDIP and Laion-HR based on the 16 classes provided in RVL-CDIP. We sample OOD categories such as \u201cMagazine\u201d (M), \u201cComic\u201d (C), \u201cGuidebook\u201d (G), \u201cYearbook\u201d (Y), \u201cWorksheet\u201d (W), and \u201cOpen Book\u201d (OB), etc, from Y and collect the OOD data from Laion-HR. In the Appendix, Table 7 shows OOD detection results for two variants: predicting 16 and 50 centroids separately. The K-means method with 50 centroids excels in detecting outliers across all categories. Fig. 7 (center) displays taxonomy expansion with HITL taxonomy cleaning. Initially, we start with an initial ID set with 10 classes selected from RVL-CDIP. At every step, we sample 10 detected outlier data. As the data increases, our outlier detection method successfully retrieves outliers for the majority of novel categories. Fig. 7 (left, right) illustrates the distribution of \u201cComic\u201d and ID data, where \u201cComic\u201d is detected as an outlier in the first step. Red color indicates the detected outlier samples."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "This paper presents, ADOPD, a large-scale document page decomposition dataset and a systematic process covering data collection, taxonomy analysis, model-assisted data annotation, and HITL processes. ADOPD aims to bridge the data and model gap between the document domain and the natural image, as well as the vision-language domain. We conduct a comprehensive analysis of ADOPD and perform detailed experimental comparisons across four tasks. Our results highlight the value of ADOPD. ADOPD is a small step towards a large-scale high-quality document image dataset. ADOPD opens up many opportunities and directions for future exploration, including the analysis of multi-lingual documents and sensitive data. We release ADOPD to aid future foundational models for document understanding, hoping to catalyze advancements in document analysis."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 ABLATION STUDY OF OUTLIER DETECTION",
            "text": "Table 7 presents OOD detection results obtained using our K-means-based method, with two variants: predicting 16 and 50 centroids separately. The K-means method with 50 centroids excels in OOD detection across all categories, showcasing its effectiveness in identifying outlier data.\nTable 7: Performance of outlier detection methods in dataset exploration.\nID Outlier Score AUROC (%)\u2191 FPR95 (%)\u2193M C G Y W OB M C G Y W OB K-Means (16) 99.99 99.99 99.99 99.99 99.85 99.98 0.01 0.11 0.00 0.00 0.67 0.08 RVL-CDIP K-Means (50) 99.99 99.99 99.99 99.99 99.87 99.97 0.01 0.03 0.00 0.00 0.53 0.16 MCM 80.23 94.50 91.36 92.44 91.92 89.57 80.58 31.27 51.01 42.11 44.06 49.37\nK-Means (16) 86.28 90.86 89.08 87.64 59.16 78.13 33.02 21.40 28.05 39.61 82.66 65.74\nADOPD K-Means (50) 86.89 92.86 89.41 87.46 59.90 78.71 32.35 27.69 29.53 39.85 80.46 65.70 MCM 71.42 91.13 86.69 87.58 87.58 84.16 89.89 45.18 68.23 54.39 56.66 60.76"
        },
        {
            "heading": "A.1.1 ABLATION STUDY OF PROMPTS",
            "text": "Table 8 shows the results of human evaluation for various prompts. In each case, we randomly chose 100 images for comparison and instructed human evaluators to exclude any incorrect samples."
        },
        {
            "heading": "Prompt \u00b2 \u00b2 Prompt \u00b2 \u00b2",
            "text": "As outlined in Sec.3.2, we utilize four prompts to generate annotated document labels, leveraging linguistic, visual, layout, and multi-modal information. Detailed prompts are listed in Table9. Although the format of prompts focused on different aspects is consistent, different prompts include varying input items (indicated by check signals in Table9). The red texts in the table represent inputs or variables related to prompt selection (as shown in Table10).\nConsidering the order of input items may impact the quality of taxonomy generation, we conducted experiments on different prompts to investigate the optimal input order. Given that some sentences are fixed for all prompts, our study focuses on the influence of the order of the prompt sentences containing inputs (Row 3-7 in Table 9). To simplify the study, we specifically consider Rows 6 and 7, which both represent high-level information for the entire document images and should be put together, while the order between them remains undecided. In the study, human annotators assess the output labels of the prompts and vote on them. They consider the relevance between the document image and labels and eliminate orders that result in conflicting or ambiguous labels. We then count the number of votes for each order to determine the most favored order."
        },
        {
            "heading": "1. Order of Visual and Textual Prompt Inputs",
            "text": "i. Visual We shuffle the order of input items of visual prompt (3, 6, and 7) into 4 different\norders while keeping 6 and 7 adjacent: 3-6-7, 3-7-6, 6-7-3, and 7-6-3. The investigation result is shown Fig. 8(a)\nii. Textual Similar to visual prompt, 4 orders are investigated: 5-6-7, 5-7-6, 6-7-5, and 7-6-5. The investigation result is shown Fig. 8(b)\nConsidering the results of the Visual and Textual Prompt, we choose 3-6-7 and 5-6-7 as the input order of our prompt. We can also find that it would be better to input the description (Row 6) before the pre-labels (Row 7) and put it at the end of the list. We will keep this order in the following study.\n2. Order of Layout Prompt Inputs We investigated two different layout orders: 4-5-6-7 and 5-4-6-7. Considering that text space may influence layout comprehension, we classified a document with a text area larger than 0.2 of the size as text-rich. In our study, we present the results separately\nbased on this criterion in Fig. 8(c). The figure illustrates that when the text is rich, the order 5-4-6-7 receives more votes, while 4-5-6-7 is favored otherwise. Consequently, we choose to use the order 5-4-6-7 when the text is rich and 4-5-6-7 when the text is not as text-rich.\n3. Order of Mutimodal Prompt Inputs Multimodal prompts predict the labels using all provided information. Building on previous findings, we standardized the order of Rows 6 and 7, placing them at the bottom of the inputs. For the remaining 3 inputs, we shuffled them to create a total of 6 different orders: 3-4-5-6-7, 4-3-5-6-7, 4-5-3-6-7, 3-5-4-6-7, 5-3-4-6-7, and 5-4-3-6-7. As shown in Fig. 8, the results suggest that the order 3-4-5-6-7 yields the best performance among all orders. After obtaining labels with different information, we summarize them into 10 categories to classify the document. The detailed prompt is shown in Table 11.\nTable 10: Variable settings for prompts.\nModality USED RESOURCE Visual The possible visual elements while considering other mentioned details such as description, and possible categories l Textual The textual content summarization while considering other mentioned details such as description, and possible categories Layout The layout information and design of the document while considering relevant details such as description, visual elements, textual content, and possible categories Multimodal All the information above such as description, visual elements, textual content, and possible categories and layout information\nTable 11: Prompts for label summary.\nPrompt Sentence\n\u00a0 1 You are a languistic expert that can well understand the difference between phrases. \u00a0 2 You will receive 4 lists of labels of the same document image from 4 annotators. They focus on visual, linguistic, layout, and multimodal aspects of the image. \u00a0 3 You should come out 10 most common conceptions from the list with in 4 words for each. \u00a0 4 The labels given by the visual-focused annotator are: VisualLabel1, VisualLabel2, VisualLabel3 ... \u00a0 5 The labels given by the linguistic-focused annotator are: TextLabel1, TextLabel2, TextLabel3 ... \u00a0 6 The labels given by the layout-focused annotator are: LayoutLabel1, LayoutLabel2, LayoutLabel3 ... \u00a0 7 The labels given by the multimdal-focused annotator are: MultimdalLabel1, MultimdalLabel2, MultimdalLabel3 ... \u00a0 8 Here is a example for output: [1. {Lable1}, 2. {Lable1}, ...]."
        },
        {
            "heading": "A.2 DATA ANNOTATION PROCESS ANALYSIS",
            "text": "A.2.1 IMAGE ANALYSIS\nFig. 10 illustrates our analysis of the image data, showcasing the high resolution of our dataset. During the annotation process, despite some images having high resolution, the text appears blurry. We require annotators to skip labeling such images to ensure clear visibility of text in all pictures. During training, we resize each image to 1024 x 1024, but during testing, we use the raw resolution."
        },
        {
            "heading": "A.2.2 MODEL-ENHANCED ANNOTATION COST ANALYSIS",
            "text": "In Fig. 12, we analyze the variation in DOC2MASK annotation costs with the improvement of the model. As the cost of annotating data depends on various factors such as the complexity, type, language, and other aspects of the current batch of images. The displayed cost changes represent a subset of the ADOPD (English) dataset. It is evident that with model enhancement, there is a downward trend in relative costs. Here, the relative cost is calculated based on the cost of modifying and adding masks for each image, with the y-axis representing normalized costs rather than the actual cost for each image. In practice, costs are associated with masks and boxes, so the varying number of masks or boxes per image can lead to different costs.\nFigure 11: Hierarchical structure visualization of the discovered taxonomy. The child nodes represent actual document types, while the intermediate parents are obtained through KNN clustering into 100 classes. We assign specific unique words to each cluster through human input as labels.\n0 2 4 6 8 10 12 14 Batch Number\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nR el\nat iv\ne C\nos t\nModel-Assisted Annotation Cost Curve (Data Growth) Modified Mask Refined Mask\nFigure 12: Data Annotation Cost Over Time Curve. Presented cost is relative and for reference only."
        },
        {
            "heading": "A.3 MULTI-LINGUAL DOC2BOX SAMPLES",
            "text": ""
        },
        {
            "heading": "A.2.3 MODEL-ASSISTED ANNOTATION DATA VISUALIZATION",
            "text": ""
        }
    ],
    "title": "ADOPD: A LARGE-SCALE DOCUMENT PAGE DECOM-",
    "year": 2023
}