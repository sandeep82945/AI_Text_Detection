{
    "abstractText": "We introduce latent intuitive physics, a transfer learning framework for physics simulation that can infer hidden properties of fluids from a single 3D video and simulate the observed fluid in novel scenes. Our key insight is to use latent features drawn from a learnable prior distribution conditioned on the underlying particle states to capture the invisible and complex physical properties. To achieve this, we train a parametrized prior learner given visual observations to approximate the visual posterior of inverse graphics, and both the particle states and the visual posterior are obtained from a learned neural renderer. The converged prior learner is embedded in our probabilistic physics engine, allowing us to perform novel simulations on unseen geometries, boundaries, and dynamics without knowledge of the true physical parameters. We validate our model in three ways: (i) novel scene simulation with the learned visual-world physics, (ii) future prediction of the observed fluid dynamics, and (iii) supervised particle simulation. Our model demonstrates strong performance in all three tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiangming Zhu"
        },
        {
            "affiliations": [],
            "name": "Huayu Deng"
        },
        {
            "affiliations": [],
            "name": "Haochen Yuan"
        },
        {
            "affiliations": [],
            "name": "Yunbo Wang"
        },
        {
            "affiliations": [],
            "name": "Xiaokang Yang"
        }
    ],
    "id": "SP:06be96d6c04ea468ee8bcc3ce5858fd174e1b3a8",
    "references": [
        {
            "authors": [
                "Kelsey R Allen",
                "Tatiana Lopez-Guevara",
                "Kimberly Stachenfeld",
                "Alvaro Sanchez-Gonzalez",
                "Peter Battaglia",
                "Jessica Hamrick",
                "Tobias Pfaff"
            ],
            "title": "Physical design using differentiable learned simulators",
            "venue": "arXiv preprint arXiv:2202.00728,",
            "year": 2022
        },
        {
            "authors": [
                "Christopher Bates",
                "Peter W Battaglia",
                "Ilker Yildirim",
                "Joshua B Tenenbaum"
            ],
            "title": "Humans predict liquid dynamics using probabilistic simulation",
            "venue": "In CogSci,",
            "year": 2015
        },
        {
            "authors": [
                "Peter Battaglia",
                "Razvan Pascanu",
                "Matthew Lai",
                "Danilo Jimenez Rezende"
            ],
            "title": "Interaction networks for learning about objects, relations and physics",
            "venue": "In NeurIPS,",
            "year": 2016
        },
        {
            "authors": [
                "Peter W Battaglia",
                "Jessica B Hamrick",
                "Joshua B Tenenbaum"
            ],
            "title": "Simulation as an engine of physical scene understanding",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2013
        },
        {
            "authors": [
                "Filipe De Avila Belbute-Peres",
                "Thomas Economon",
                "Zico Kolter"
            ],
            "title": "Combining differentiable pde solvers and graph neural networks for fluid flow prediction",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Jan Bender",
                "Dan Koschier"
            ],
            "title": "Divergence-free smoothed particle hydrodynamics",
            "venue": "In SCA, pp",
            "year": 2015
        },
        {
            "authors": [
                "Jan Bender"
            ],
            "title": "SPlisHSPlasH Library, 2022",
            "venue": "URL https://github.com/ InteractiveComputerGraphics/SPlisHSPlasH",
            "year": 2022
        },
        {
            "authors": [
                "Zhenfang Chen",
                "Kexin Yi",
                "Yunzhu Li",
                "Mingyu Ding",
                "Antonio Torralba",
                "Joshua B Tenenbaum",
                "Chuang Gan"
            ],
            "title": "Comphy: Compositional physical reasoning of objects and events from videos",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Danny Driess",
                "Zhiao Huang",
                "Yunzhu Li",
                "Russ Tedrake",
                "Marc Toussaint"
            ],
            "title": "Learning multi-object dynamics with compositional neural radiance fields",
            "venue": "In CoRL,",
            "year": 2023
        },
        {
            "authors": [
                "Sebastien Ehrhardt",
                "Aron Monszpart",
                "Niloy Mitra",
                "Andrea Vedaldi"
            ],
            "title": "Unsupervised intuitive physics from visual observations",
            "venue": "In ACCV,",
            "year": 2019
        },
        {
            "authors": [
                "David L Gilden",
                "Dennis R Proffitt"
            ],
            "title": "Heuristic judgment of mass ratio in two-body collisions",
            "venue": "Perception & Psychophysics,",
            "year": 1994
        },
        {
            "authors": [
                "Shanyan Guan",
                "Huayu Deng",
                "Yunbo Wang",
                "Xiaokang Yang"
            ],
            "title": "Neurofluid: Fluid dynamics grounding with particle-driven neural radiance fields",
            "venue": "In ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Yuan-Chen Guo",
                "Di Kang",
                "Linchao Bao",
                "Yu He",
                "Song-Hai Zhang"
            ],
            "title": "Nerfren: Neural radiance fields with reflections",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Jiaqi Han",
                "Wenbing Huang",
                "Hengbo Ma",
                "Jiachen Li",
                "Josh Tenenbaum",
                "Chuang Gan"
            ],
            "title": "Learning physical dynamics with subequivariant graph neural networks",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Mary Hegarty"
            ],
            "title": "Mechanical reasoning by mental simulation",
            "venue": "Trends in cognitive sciences,",
            "year": 2004
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "Simon Le Cleac\u2019h",
                "Hong-Xing Yu",
                "Michelle Guo",
                "Taylor Howell",
                "Ruohan Gao",
                "Jiajun Wu",
                "Zachary Manchester",
                "Mac Schwager"
            ],
            "title": "Differentiable physics simulation of dynamics-augmented neural objects",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2023
        },
        {
            "authors": [
                "Xuan Li",
                "Yi-Ling Qiao",
                "Peter Yichen Chen",
                "Krishna Murthy Jatavallabhula",
                "Ming Lin",
                "Chenfanfu Jiang",
                "Chuang Gan"
            ],
            "title": "PAC-neRF: Physics augmented continuum neural radiance fields for geometry-agnostic system identification",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Yunzhu Li",
                "Jiajun Wu",
                "Russ Tedrake",
                "Joshua B Tenenbaum",
                "Antonio Torralba"
            ],
            "title": "Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Yunzhu Li",
                "Toru Lin",
                "Kexin Yi",
                "Daniel Bear",
                "Daniel L.K. Yamins",
                "Jiajun Wu",
                "Joshua B. Tenenbaum",
                "Antonio Torralba"
            ],
            "title": "Visual grounding of learned physical models",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Yunzhu Li",
                "Shuang Li",
                "Vincent Sitzmann",
                "Pulkit Agrawal",
                "Antonio Torralba"
            ],
            "title": "3d neural scene representations for visuomotor control",
            "venue": "In CoRL,",
            "year": 2022
        },
        {
            "authors": [
                "Xingyu Lin",
                "Yufei Wang",
                "Zixuan Huang",
                "David Held"
            ],
            "title": "Learning visible connectivity dynamics for cloth smoothing",
            "venue": "In CoRL,",
            "year": 2022
        },
        {
            "authors": [
                "Lingjie Liu",
                "Jiatao Gu",
                "Kyaw Zaw Lin",
                "Tat-Seng Chua",
                "Christian Theobalt"
            ],
            "title": "Neural sparse voxel fields",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Michael McCloskey",
                "Allyson Washburn",
                "Linda Felch"
            ],
            "title": "Intuitive physics: the straight-down belief and its origin",
            "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition,",
            "year": 1983
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P. Srinivasan",
                "Matthew Tancik",
                "Jonathan T. Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Damian Mrowca",
                "Chengxu Zhuang",
                "Elias Wang",
                "Nick Haber",
                "Li F Fei-Fei",
                "Josh Tenenbaum",
                "Daniel L Yamins"
            ],
            "title": "Flexible neural representation for physics prediction",
            "venue": "In NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "Thomas M\u00fcller",
                "Alex Evans",
                "Christoph Schied",
                "Alexander Keller"
            ],
            "title": "Instant neural graphics primitives with a multiresolution hash encoding",
            "venue": "ACM Transactions on Graphics,",
            "year": 2022
        },
        {
            "authors": [
                "Tobias Pfaff",
                "Meire Fortunato",
                "Alvaro Sanchez-Gonzalez",
                "Peter Battaglia"
            ],
            "title": "Learning mesh-based simulation with graph networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Lukas Prantl",
                "Benjamin Ummenhofer",
                "Vladlen Koltun",
                "Nils Thuerey"
            ],
            "title": "Guaranteed conservation of momentum for learning particle-based fluid dynamics",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Adam N Sanborn",
                "Vikash K Mansinghka",
                "Thomas L Griffiths"
            ],
            "title": "Reconciling intuitive physics and newtonian mechanics for colliding objects",
            "venue": "Psychological review,",
            "year": 2013
        },
        {
            "authors": [
                "Alvaro Sanchez-Gonzalez",
                "Jonathan Godwin",
                "Tobias Pfaff",
                "Rex Ying",
                "Jure Leskovec",
                "Peter Battaglia"
            ],
            "title": "Learning to simulate complex physics with graph networks",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Connor Schenck",
                "Dieter Fox"
            ],
            "title": "Spnets: Differentiable fluid dynamics for deep neural networks",
            "venue": "In CoRL,",
            "year": 2018
        },
        {
            "authors": [
                "Yidi Shao",
                "Chen Change Loy",
                "Bo Dai"
            ],
            "title": "Transformer with implicit edges for particle-based physics simulation",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Anthony Simeonov",
                "Yilun Du",
                "Andrea Tagliasacchi",
                "Joshua B Tenenbaum",
                "Alberto Rodriguez",
                "Pulkit Agrawal",
                "Vincent Sitzmann"
            ],
            "title": "Neural descriptor fields: Se (3)-equivariant object representations for manipulation",
            "venue": "In ICRA,",
            "year": 2022
        },
        {
            "authors": [
                "Cheng Sun",
                "Min Sun",
                "Hwann-Tzong Chen"
            ],
            "title": "Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Tomer D Ullman",
                "Elizabeth Spelke",
                "Peter Battaglia",
                "Joshua B Tenenbaum"
            ],
            "title": "Mind games: Game engines as an architecture for intuitive physics",
            "venue": "Trends in cognitive sciences,",
            "year": 2017
        },
        {
            "authors": [
                "Benjamin Ummenhofer",
                "Lukas Prantl",
                "Nils Thuerey",
                "Vladlen Koltun"
            ],
            "title": "Lagrangian fluid simulation with continuous convolutions",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Zhenjia Xu",
                "Jiajun Wu",
                "Andy Zeng",
                "Joshua B Tenenbaum",
                "Shuran Song"
            ],
            "title": "DensePhysNet: Learning dense physical object representations via multi-step dynamic interactions",
            "venue": "In RSS,",
            "year": 2019
        },
        {
            "authors": [
                "PhysNeRF Guan"
            ],
            "title": "As shown in Figure 8, the network is based on fully-connected layers and similar to NeRF (Mildenhall et al., 2020). Unlike original NeRF, PhysNeRF performs volume rendering according to the geometric distribution of neighboring physical particles along a given camera ray. For a sampled point in a ray, a ball query is conducted to identify neighboring fluid particles around the sampled point",
            "year": 2022
        },
        {
            "authors": [
                "Liu et al",
                "Sun"
            ],
            "title": "2022), we apply positional encoding \u0393(\u00b7) to every input. In our experiments, we set the maximum encoded frequency L = 10 for \u0393(ep) and L = 4 for \u0393(ed),\u0393(d). We optimize PhysNeRF in a coarse-to-fine manner (Mildenhall et al., 2020). For the fine MLP network, the search radius of particle encoding of ep and ed is set as 3 times the particle radius and we consider 20 fluid particles within this search radius",
            "venue": "Following (Mildenhall et al.,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Understanding the intricate dynamics of physical systems has been a fundamental pursuit of science and engineering. Recently, deep learning-based methods have shown considerable promise in simulating complex physical systems (Battaglia et al., 2016; Mrowca et al., 2018; Schenck & Fox, 2018; Li et al., 2019; Ummenhofer et al., 2020; Sanchez-Gonzalez et al., 2020; Shao et al., 2022; Prantl et al., 2022; Han et al., 2022; Guan et al., 2022; Li et al., 2023). However, most previous works focus on physics simulation with given accurate physical properties, which requires strong domain knowledge or highly specialized devices. Let us consider a question: Can we predict physical systems with limited knowledge of its physical properties? If not, is it possible to transfer hidden physics present in readily accessible visual observations into learning-based physics simulators (Li et al., 2019; Ummenhofer et al., 2020; Sanchez-Gonzalez et al., 2020; Prantl et al., 2022)?\nInspired by human perception, researchers in the field of AI have proposed a series of intuitive physics methods (McCloskey et al., 1983; Battaglia et al., 2013; Ehrhardt et al., 2019; Xu et al., 2019; Li et al., 2020) to solve this problem. A typical approach is to build the so-called \u201cinverse graphics\u201d models of raw visual observations, which involves training learning-based physical simulators by solving the inverse problem of rendering 3D scenes (Guan et al., 2022; Li et al., 2023). However, NeuroFluid (Guan et al., 2022) needs to finetune the deterministic transition model in response to every physics dynamics associated with new-coming physical properties. PAC-NeRF (Li et al., 2023) adopts heuristic (rather than learnable) simulators and explicitly infers physical properties (such as the viscosity of fluids) given visual observations. It requires an appropriate initial guess of the optimized properties and specifying the fluid type (e.g., Newtonian vs. non-Newtonian fluids).\nIn this paper, we introduce the learning framework of latent intuitive physics, which aims to infer hidden fluid dynamics from a 3D video, allowing for the simulation of the observed fluid in novel scenes without the need for its exact physical properties. The framework arises from our intuition that we humans can imagine how a fluid with a specific physical property will move given its initial state by watching a video showcase of it flowing, even though we do not explicitly estimate the exact\n\u2217Equal contribution. \u2020Corresponding author: Yunbo Wang.\nPublished as a conference paper at ICLR 2024\n<latexit sha1_base64=\"JKjydBx6EuT3mrcMPK3bvYKX1c4=\">AAACFHicbVDLSsNAFJ34rPUVdelmsAgVpSRS0WXRjcsK9gFNCJPJpB06eTAzEWvIR7jxV9y4UMStC3f+jZM2FG09MHDuOfcy9x43ZlRIw/jWFhaXlldWS2vl9Y3NrW19Z7ctooRj0sIRi3jXRYIwGpKWpJKRbswJClxGOu7wKvc7d4QLGoW3chQTO0D9kPoUI6kkRz+Oq1aA5MD10/sMWgH14LR2zOxkWj1kR45eMWrGGHCemAWpgAJNR/+yvAgnAQklZkiInmnE0k4RlxQzkpWtRJAY4SHqk56iIQqIsNPxURk8VIoH/YirF0o4Vn9PpCgQYhS4qjNfUcx6ufif10ukf2GnNIwTSUI8+chPGJQRzBOCHuUESzZSBGFO1a4QDxBHWKocyyoEc/bkedI+rZn12tlNvdK4LOIogX1wAKrABOegAa5BE7QABo/gGbyCN+1Je9HetY9J64JWzOyBP9A+fwCfN56Q</latexit>\n<latexit sha1_base64=\"T64wdCOtEdbSkRWaNrTdkNualNs=\">AAACAXicbZDLSsNAFIZPvNZ6i7oR3AwWoW5KIhVdFt3oroK9QBvKZDpph04uzkyEGuvGV3HjQhG3voU738ZJG0Fbfxj4+M85zDm/G3EmlWV9GXPzC4tLy7mV/Ora+samubVdl2EsCK2RkIei6WJJOQtoTTHFaTMSFPsupw13cJ7WG7dUSBYG12oYUcfHvYB5jGClrY65e1Ns+1j1XS+5G93/4OXosGMWrJI1FpoFO4MCZKp2zM92NySxTwNFOJayZVuRchIsFCOcjvLtWNIIkwHu0ZbGAPtUOsn4ghE60E4XeaHQL1Bo7P6eSLAv5dB3dWe6opyupeZ/tVasvFMnYUEUKxqQyUdezJEKURoH6jJBieJDDZgIpndFpI8FJkqHltch2NMnz0L9qGSXS8dX5ULlLIsjB3uwD0Ww4QQqcAFVqAGBB3iCF3g1Ho1n4814n7TOGdnMDvyR8fEN5yiXMA==</latexit>\n<latexit sha1_base64=\"NbWszBP4MlZi9iJhLvMXbt5U7V8=\">AAACAXicbZDLSsNAFIZP6q3WW9SN4CZYhLopiVR0WXTjsoK9QBvKZDpph04mYWYi1hg3voobF4q49S3c+TZO2wja+sPAx3/OYc75vYhRqWz7y8gtLC4tr+RXC2vrG5tb5vZOQ4axwKSOQxaKlockYZSTuqKKkVYkCAo8Rpre8GJcb94QIWnIr9UoIm6A+pz6FCOlra65F5U6AVIDz0/u0vsfvE2PumbRLtsTWfPgZFCETLWu+dnphTgOCFeYISnbjh0pN0FCUcxIWujEkkQID1GftDVyFBDpJpMLUutQOz3LD4V+XFkT9/dEggIpR4GnO8crytna2Pyv1o6Vf+YmlEexIhxPP/JjZqnQGsdh9aggWLGRBoQF1btaeIAEwkqHVtAhOLMnz0PjuOxUyidXlWL1PIsjD/twACVw4BSqcAk1qAOGB3iCF3g1Ho1n4814n7bmjGxmF/7I+PgGLTaXXg==</latexit>\nvalues of physical properties. The key idea is to represent the hidden physical properties in visual observations, which may be difficult to observe, using probabilistic latent states z shown in Figure 1. The latent space connects the particle space and visual space to infer and transfer hidden physics with probabilistic modeling. Specifically, our approach includes a probabilistic particle transition module p(x\u2032|x, z)1, a physical prior learner, a particle-based posterior estimator, and a neural renderer, all integrated into a differentiable neural network. The latent features are drawn from trainable marginal distributions p(z|x) that are learned to approximate the visual posterior distribution q(z|I) obtained from a learned neural renderer. By employing probabilistic latent features, our model gains flexibility in modeling complex systems and is capable of capturing uncertainty in the data that a deterministic model may not be able to handle. Once p(z|x) is converged, we embed the prior learner in our probabilistic fluid simulator that is pretrained in particle space containing fluids with a wide range of physical properties. In this way, we transfer the hidden physics from visual observations to particle space to enable novel simulations of unseen fluid geometries, boundary conditions, and dynamics. In our experiments, we demonstrate the effectiveness of latent intuitive physics by comparing it to strong baselines of fluid simulation approaches in novel scene simulation, future prediction of the observed dynamics, and supervised particle simulation tasks.\nThe contributions of this paper can be summarized as follows: \u2022 We introduce latent intuitive physics, a learning-based approach for fluid simulation, which infers\nthe hidden properties of fluids from 3D exemplars and transfers this knowledge to a fluid simulator. \u2022 We propose the first probabilistic particle-based fluid simulation network, which outperforms prior\nworks in particle-based simulation with varying physical properties."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Learning-based simulation of particle dynamics. Recent research has introduced deep learningbased methods to accelerate the forward simulation of complex particle systems and address inverse problems. These methods have shown success in simulating rigid bodies (Battaglia et al., 2016; Han et al., 2022), fluids (Belbute-Peres et al., 2020; Sanchez-Gonzalez et al., 2020; Shao et al., 2022; Prantl et al., 2022), and deformable objects (Mrowca et al., 2018; Li et al., 2019; Sanchez-Gonzalez et al., 2020; Lin et al., 2022). In the context of fluid simulation, DPI-Net (Li et al., 2019) proposes dynamic graphs with multi-step spatial propagation, GNS (Sanchez-Gonzalez et al., 2020) uses message-passing networks, and TIE (Shao et al., 2022) uses a Transformer-based model to capture the spatiotemporal correlations within the particle system. Another line of work includes methods like CConv (Ummenhofer et al., 2020) and DMCF (Prantl et al., 2022), which introduces higherdimensional continuous convolution operators to model interactions between particles. Different from the approaches for other simulation scenarios for rigid and deformable objects, these models do not assume strong geometric priors such as object-centric transformations or pre-defined mesh topologies (Pfaff et al., 2021; Allen et al., 2022). However, these models are deterministic, assuming that all physical properties are measurable and focus on learning fluid dynamics under known physical properties. Moreover, the stochastic components that commonly exist in the real physical world are not taken into account by the deterministic models.\nIntuitive physics learning with neural networks. Researchers have explored intuitive physics methods from a range of perspectives. These include heuristic models (Gilden & Proffitt, 1994;\n1Here, we use x to indicate the historical states and x\u2032 to indicate the future states in the physical process.\nSanborn et al., 2013), probabilistic mental simulation models (Hegarty, 2004; Bates et al., 2015), and the cognitive intuitive physics models (Battaglia et al., 2013; Ullman et al., 2017). Recent advances in deep learning typically investigate intuitive physics from different aspects. Some approaches adopt 3D videos to to downstream tasks, such as predicting multi-object dynamics (Driess et al., 2023), fluid dynamic (Guan et al., 2022; Li et al., 2023), system identification (Li et al., 2023), manipulation (Simeonov et al., 2022; Li et al., 2022) or reasoning physics parameters (Li et al., 2020; Chen et al., 2022; Le Cleac\u2019h et al., 2023). However, most works focus on rigid body dynamics (Driess et al., 2023; Li et al., 2020; Le Cleac\u2019h et al., 2023). The most relevant work to our method is NeuroFluid (Guan et al., 2022) and PAC-NeRF (Li et al., 2023), both focusing on fluid dynamics modeling with visual observation. NeuroFluid directly adopts the learning-based simulator from CConv (Ummenhofer et al., 2020). Unlike our approach, it is deterministic and cannot handle the stochastic components or inaccessible physical properties in complex physical scenarios. PAC-NeRF employs specific non-learnable physics simulators tailored to different types of fluids. When using PAC-NeRF to solve inverse problems, users need to make an appropriate initialization for the optimized parameters based on the category of fluid observed."
        },
        {
            "heading": "3 PROBLEM FORMULATION",
            "text": "We study the inverse problem of fluid simulation, which refers to learning inaccessible physical properties by leveraging visual observations. Specifically, we consider a dynamic system where we only have a single sequence of observations represented as multi-view videos {Imt }m=1:Mt=1:T , where Imt represents a visual observation received at time t from view m. We want to predict the future states of the system when it appears in novel scenes. Let xt = (x1t , . . . , x N t ) \u2208 X be a state of the system at time t, where xit = (p i t, v i t) represents the state of the i\nth particle that involves the position pit and velocity vit, with v i t being the time derivative of p i t. The dynamics of particles {x1, . . . ,xT } is jointly governed by a set of physical properties, such as density, viscosity, and pressure. These properties are hidden and need to be inferred in visual observations. To bridge the gap between particle simulators and the visual world with a varying set of physical properties in a unified framework, we introduce a set of latent features zt = (z1t , . . . , z N t ), where z i t is the latent feature attached to each particle. As shown in Figure 2(a), the particle state transition function can thus be represented as xt \u223c p(xt\u22121, zt), where zt \u223c p(x1:t\u22121, zt\u22121). As the explicit physical properties are inaccessible, we can infer latent distribution p(zt | x1:t) from q(z | I1:T ). The final goal is to simulate novel scenes with new initial and boundary conditions based on learned physics (see Figure 2(d))."
        },
        {
            "heading": "4 LATENT INTUITIVE PHYSICS",
            "text": "In this section, we introduce latent intuitive physics for fluid simulation, which enables the transfer of hidden fluid properties from visual observations to novel scenes. As shown in Figure 3, our model consists of four network components: the probabilistic particle transition module (\u03b8), the physical prior learner (\u03c8), the particle-based posterior estimator (\u03be), and the neural renderer (\u03d5). The training pipeline involves three stages shown in Figure 2\u2014pretraining, inference, and transfer:\na) Pretrain the probabilistic fluid simulator on the particle dataset, which involves the particle transition module p\u03b8(xt | xt\u22121, zt), the prior learner p\u03c8(z\u0303t | x1:t\u22121, z\u0303t\u22121), and the posterior q\u03be(zt | x1:t, zt\u22121). The prior module reasons about latent features from historical particle states.\nPublished as a conference paper at ICLR 2024\n<latexit sha1_base64=\"a0/MgcY+YMbrVjoyc5FdON336Kc=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0WPRi8eK9gPaUDbbSbt0swm7G7GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSATXxnW/ncLK6tr6RnGztLW9s7tX3j9o6jhVDBssFrFqB1Sj4BIbhhuB7UQhjQKBrWB0M/Vbj6g0j+WDGSfoR3QgecgZNVa67z7xXrniVt0ZyDLxclKBHPVe+avbj1kaoTRMUK07npsYP6PKcCZwUuqmGhPKRnSAHUsljVD72ezUCTmxSp+EsbIlDZmpvycyGmk9jgLbGVEz1IveVPzP66QmvPIzLpPUoGTzRWEqiInJ9G/S5wqZEWNLKFPc3krYkCrKjE2nZEPwFl9eJs2zqndevbg7r9Su8ziKcATHcAoeXEINbqEODWAwgGd4hTdHOC/Ou/Mxby04+cwh/IHz+QNefY3d</latexit>\n<latexit sha1_base64=\"b6gNODI/AwvTbrgLtsKqOQFRL0I=\">AAAB63icbVBNSwMxEJ2tX7V+VT16CRbBU9kVRY9FLx4r2A9ol5JNs21okg1JVihL/4IXD4p49Q9589+YbfegrQ8GHu/NMDMvUpwZ6/vfXmltfWNzq7xd2dnd2z+oHh61TZJqQlsk4YnuRthQziRtWWY57SpNsYg47USTu9zvPFFtWCIf7VTRUOCRZDEj2OZSXxk2qNb8uj8HWiVBQWpQoDmofvWHCUkFlZZwbEwv8JUNM6wtI5zOKv3UUIXJBI9oz1GJBTVhNr91hs6cMkRxol1Ji+bq74kMC2OmInKdAtuxWfZy8T+vl9r4JsyYVKmlkiwWxSlHNkH542jINCWWTx3BRDN3KyJjrDGxLp6KCyFYfnmVtC/qwWX96uGy1rgt4ijDCZzCOQRwDQ24hya0gMAYnuEV3jzhvXjv3seiteQVM8fwB97nDybXjlI=</latexit>\nb) Infer the visual posterior latent features z\u0302 from consecutive image observations of a specific fluid, which is achieved by optimizing a differentiable neural renderer (\u03d5). c) Train the prior learner p\u03c8(z\u0303t | x1:t\u22121, z\u0303t\u22121) to approximate the converged distribution of z\u0302, which enables the transfer of inaccessible physical properties from the visual world to the simulator."
        },
        {
            "heading": "4.1 STAGE A: PROBABILISTIC FLUID SIMULATOR PRETRAINING",
            "text": "Building a probabilistic model has significant advantages for fluid simulation: First, it allows us to predict fluid dynamics without knowing true physical parameters. Instead, it relies on inferring latent features from consecutive inputs. This is valuable because many complex physical phenomena naturally involve stochastic components. Second, when provided with visual observations, we can seamlessly integrate the probabilistic fluid simulation into our variational inference method via the latent space. Next, we introduce how to infer physical properties from particle data.\nThe architecture of our probabilistic fluid simulator is shown in Figure 3 (Left). Particle states xt itself is not a feature but simply defines the particle\u2019s state in spatial space. We use the continuous convolution (CConv) (Ummenhofer et al., 2020) as a feature encoder to get feature representations of each particle. Inspired by traditional Smoothed Particle Hydrodynamics (SPH) methods, CConv predicts particle features by aggregating its neighbors\u2019 features in a continuous and smooth manner. Static particles such as boundaries, are processed similarly but with the particle positions and normal vectors as input (see the literature by Ummenhofer et al. (2020)). Since the invisible physical properties cannot be inferred from a single state, we use a GRU to gather historical information and infer the distribution of the prior latents. The prior learner is trained along with a separate posterior estimator q\u03be(zt | x1:t, zt\u22121) (not used at test time). The models can be written as Prior: z\u0303t \u223c GRU\u03c8(CConv(z\u0303t\u22121,xt\u22121)); Posterior: zt \u223c GRU\u03be(CConv(zt\u22121,xt)), (1) where zt=1 and z\u0303t=1 are zero-initialized. The posterior estimator takes xt as input, i.e., the target of the prediction. The prior and posterior latents are sampled from distinct Gaussian distributions, with their parameters determined by predicted means and variances by two GRUs. During training, we align their distributions through KL divergence. Notably, our approach assumes time-varying and particle-dependent fluid properties, which aligns with conventional SPH methods (please refer to the work by Bender & Koschier (2015)). This approach empirically achieves better performance than optimizing a global latent variable, as demonstrated in our experiments.\nFor the particle transition module p\u03b8(xt | xt\u22121, zt), we adopt another CConv with additional inputs of zt drawn from the inferred latent distribution. This allows the module to incorporate the previous states xt\u22121 and corresponding latent features for future prediction. As the stochastic physical component has been captured by zt, we employ a deterministic architecture for the particle transition module: x\u0302t \u225c {(p\u0302it, v\u0302it)}i=1:N \u223c T\u03b8(xt\u22121, zt). During training, the latent posteriors zt are used as inputs of T\u03b8. At test time, we use the latent priors z\u0303t instead. The fluid simulator is trained with\nL\u03b8,\u03c8,\u03be = E [ 1 N N\u2211 i=1 wi \u2225\u2225p\u0302it \u2212 pit\u2225\u2225\u03b32 + \u03b2 DKL (q\u03be (zt | x1:t, zt\u22121) \u2225 p\u03c8 (z\u0303t | x1:t\u22121, z\u0303t\u22121)) ]. (2)\nSimilar to the previous work (Li et al., 2019; Prantl et al., 2022; Ummenhofer et al., 2020; SanchezGonzalez et al., 2020), we use the \u2225 \u00b7 \u2225\u03b32 error between the predicted position and the ground-truth positions, and weight it by the neighbor count to form the reconstruction loss. Specifically, we use wi = exp(\u2212 1cN (p\u0302it)), where N (p\u0302it) denotes the number of neighbors for the predicted particle i and c is the average neighbor count."
        },
        {
            "heading": "4.2 STAGE B: VISUAL POSTERIOR INFERENCE",
            "text": "Here we introduce how to solve the inverse problem by inferring scene-specific visual posteriors, where the visual observation governs the physical properties. In this stage, the pretrained particle transition module T\u03b8 infers the input visual posterior, which facilitates the adaptation of the prior learner in the latent space in Stage C. To this end, the particle transition module is combined with a differentiable neural renderer R\u03d5 that provides gradients backpropagated from the photometric error over sequences in observation space. Note that only visual observations are available in the following.\nNeural renderer. To enable joint modeling of the state-to-state function of fluid dynamics and the state-to-graphics mapping function, we integrate the probabilistic particle transition module with the particle-driven neural renderer (PhysNeRF) in NeuroFluid (Guan et al., 2022) in a differentiable framework. PhysNeRF uses view-independent particle encoding ep and view-dependent particle encoding ed to estimate the volume density \u03c3 and the color c of each sampled point along each ray r(t) = o + td, such that (c, \u03c3) = R\u03d5(ep, ed,d). In this way, it establishes correlations between the particle distribution and the neural radiance field. Unlike the original PhysNeRF, we exclude the position of the sampled point from the inputs to the rendering network, which enhances the relationships between the fluid particle encodings and the rendering results. The neural renderer R\u03d5 is pretrained on multiple visual scenes so that it can respond to various particle-based geometries.\nInitial states estimation. NeuroFluid assumes known initial particle states. However, when only visual observations are available, estimating the initial particle states xt=1 becomes necessary. These estimated initial states are used to drive the neural renderer (R\u03d5) for generating visual predictions at the first time step and also to initiate the particle transition module (T\u03b8) for simulating subsequent states. We estimate the initial particle positions using the voxel-based neural rendering technique (Liu et al., 2020; Sun et al., 2022; M\u00fcller et al., 2022) at the first time step. During training, we maintain an occupancy cache to represent empty vs. nonempty space and randomly sample fluid particles within each voxel grid in the visual posterior inference stage (see Appendix D.2.3 for details).\nOptimization. We first finetune the neural renderer R\u03d5 on current visual observation with initial state estimation x\u0302t=1. Then the parameters of T\u03b8 and R\u03d5 are frozen and we initialize a set of visual posterior latents z\u0302, such that x\u0302t = T\u03b8(x\u0302t\u22121, z\u0302). In practice, we attach a particle-dependent Gaussian distribution N (\u00b5\u0302i, \u03c3\u0302i) with trainable parameters to each particle i. At each time step, we sample z\u0302i \u223c N (\u00b5\u0302i, \u03c3\u0302i) to form z\u0302 = (z\u03021, . . . , z\u0302N ). The output of the neural renderer R\u03d5 is denoted as C\u0302(r, t). We optimize the distributions of the visual posterior latent z\u0302 over the entire sequence by backpropagating the photometric error \u2211 r,t \u2225C\u0302(r, t)\u2212C(r, t)\u2225. We summarize the overall training algorithm in the Alg. 1 in the appendix."
        },
        {
            "heading": "4.3 STAGE C: PHYSICAL PRIOR ADAPTATION",
            "text": "The visual posteriors learned in the previous stage are specific to the estimated particles within the observed scene and cannot be directly applied to simulate novel scenes. Therefore, in this stage, we aim to adapt the hidden physical properties encoded in the visual posterior to the physical prior learner p\u03c8 . Instead of finetuning the entire particle transition model as NeuroFluid (Guan et al., 2022) does, we only finetune the prior learner module. Due to the unavailability of the ground truth supervision signal in particle space, tuning all parameters in the transition model in visual scenes might lead to overfitting problems, as the transition model may forget the pre-learned knowledge of feasible dynamics, or learn implausible particle transitions even if it can generate similar fluid geometries that are sufficient to minimize the image rendering loss. Specifically, we perform forward modeling on particle states x\u0302t by applying x\u0302t = T\u03b8(xt\u22121, z\u0303t), where z\u0303t is sampled from the distribution p\u03c8(z\u0303t | x1:t\u22121, z\u0303t\u22121) predicted by the prior learner. To transfer the visual posterior to p\u03c8, we finetune the prior learner by minimizing the distance between its generated distribution and the pre-learned visual posteriors {N (\u00b5\u0302i, \u03c3\u0302i)}i=1:N with T\u03b8 and R\u03d5 fixed. The volume rendering loss is still used for supervision as well. The entire training objective is\nL\u03c8 = \u2211\nr,t \u2225C\u0302(r, t)\u2212C(r, t)\u2225+ \u03b2 DKL (q(z\u0302) \u2225 p\u03c8 (z\u0303t | x1:t\u22121, z\u0303t\u22121)) . (3)\nWith the finetuned physical prior learner p\u03c8, we can embed it into the probabilistic fluid simulator to perform novel simulations on unseen fluid geometries, boundary conditions, and dynamics with identical physical properties, which brings the simulation back to the particle space."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "5.1 EVALUATION OF VISUAL PHYSICAL INFERENCE",
            "text": "Settings. To evaluate latent intuitive physics for inferring and transferring unknown fluid dynamics from visual observations, we generate a single sequence using Cuboid as the fluid body (not seen during pretraining in Stage A). The fluid freely falls in a default container. The sequence contains 60 time steps, where the first 50 time steps are used for visual inference, while the last 10 time steps are reserved for additional validation on future rollouts. We use Blender (Community, 2018) to generate multi-view images with 20 randomly sampled viewpoints. We assess the performance of our fluid simulator after transferring hidden properties from visual scenes through: (1) simulating novel scenes with fluid geometries that were not seen during training (Stanford Bunny, Sphere, Dam Break), (2) simulating novel scenes with fluid dynamics with previously unseen boundaries. We conduct experiments across three distinct sets of physical properties. In each evaluation set, we impose random rotation, scaling, and velocities on fluid bodies. Please refer to Appendix C & D.2 for more information on visual examples, novel scenes for evaluation, and implementation details. Following Ummenhofer et al. (2020), we compute the average Euclidean distance from the ground truth particles to the closest predicted particles as the evaluation metric, where the average prediction error is d\u0304 = 1T\u00d7N \u2211 t \u2211 iminp\u0302t ||pit \u2212 p\u0302t||2. Please see Appendix D.1 for details.\nCompared methods. We use four baseline models. CConv (Ummenhofer et al., 2020) learns particle dynamics without physical parameter inputs. NeuroFluid (Guan et al., 2022) grounds fluid dynamics in observed scenes with a particle-driven neural render. PAC-NeRF (Li et al., 2023) and System Identification (Sys-ID) estimate explicitly physical parameters in the observed scenes. PACNeRF employs an MPM simulator. Sys-ID utilizes a CConv simulator that takes learnable physical parameters as inputs. It also employs the same neural renderer as our approach. All models are trained on Cuboid and tested on novel scenes. Please refer to Appendix D.2.1 for more details.\nNovel scene simulation results. We evaluate the simulation results of our approach given only the true initial particle states xt=1 of the novel scenes. Table 1 shows the average prediction error across all testing sequences. We predict each sequence 10 times with different zt drawn from the same prior learner and calculate the standard deviation of the errors. We can see that the adapted probabilistic fluid simulator from the visual posteriors significantly outperforms the baselines on novel scenes across all physical properties. Though our model is trained on scenes with the default fluid boundary, it shows the ability to generalize to unseen boundary conditions. Figure 4 showcases the qualitative results. Among the baselines, Sys-ID underperforms in novel scene simulation, as it requires accurate\nparameter inference from visual observations. In contrast, our model encodes the hidden properties with higher-dimensional latent variables, providing a stronger feature representation for the unknown properties in a physical system. PAC-NeRF employs an MPM simulator, which inherently produces more accurate and stable simulation results than the learning-based simulators used by other models trained on limited data. Despite this advantage, PAC-NeRF tends to overfit the observed scenes and yields degraded performance when applied to novel scenes. More results are shown in Appendix F.\nFuture prediction results of the observed scenes. We predict the particle dynamics of the observed scene Cuboid for 10 time steps into the future. As shown in Table 2, our model performs best in most cases. NeuroFluid slightly outperforms our model on \u03c1 = 500, \u03bd = 0.2. Since NeuroFluid jointly optimizes the entire transition model and renderer on the observed scene, it is possible to overfit the observed scene and produce plausible future prediction results. However, it fails to generalize to novel scenes as shown in Table 1. Unlike NeuroFluid, our approach adapts the physical prior learner to visual scenes, without training a probabilistic physics engine. By leveraging knowledge from the pretraining stage, the transition model is less prone to overfitting on the observed scene. This not only enhances the generalization ability but also significantly reduces the training burden."
        },
        {
            "heading": "5.2 EVALUATION OF PROBABILISTIC FLUID SIMULATOR",
            "text": "To validate whether our approach can infer hidden physics from particle data in latent space, we evaluate the pretrained probabilistic fluid simulator (\u03b8, \u03c8, \u03be) in a particle dataset generated with DFSPH (Bender & Koschier, 2015), which simulates fluids with various physical parameters (e.g., viscosity \u03bd, density \u03c1) falling in a cubic box. Each scene contains 273-19,682 fluid particles and 200 time steps. To assess the simulation performance under incomplete measurement of physical parameters, the true parameters are invisible to simulators. See the Appendix C.1 for more details.\nWe compare our probabilistic fluid simulator with four representative particle simulation approaches, based on graph neural networks, continuous convolution models, and Transformer, i.e., DPI-Net (Li et al., 2019), CConv (Ummenhofer et al., 2020), DMCF (Prantl et al., 2022), and TIE (Shao et al., 2022). Following Ummenhofer et al. (2020), given two consecutive input states xt\u22121:t, we compute the errors of the predicted particle positions w.r.t. the true particles: dt+\u03c4 = 1N \u2211 i ||pit+\u03c4 \u2212 p\u0302it+\u03c4 ||2, for the next two steps (\u03c4 \u2208 {1, 2}). To assess the long-term prediction ability, we also calculate the average distance d\u0304 from true particle positions to the predicted particles over the entire sequence. The first 10 states are given, and the models predict the following 190 states. From Table 3, our model performs best in both short-term and long-term prediction. The qualitative result of long-term prediction is shown in Figure 16 in Appendix F. These results showcase that our probabilistic fluid simulation method provides an effective avenue for intuitive physics learning."
        },
        {
            "heading": "5.3 GENERALIZATION TO DYNAMICS DISCREPANCIES",
            "text": "To validate the generalization ability of our approach across the discrepancies in fluid dynamics patterns, we consider a more complex scenario that contains a mixture of two different fluids. Specifically, we use the pretrained probabilistic fluid simulator discussed in Sec 5.2, and adapt the model to a visual scene containing two heterogeneous fluids interacting with each other. We use two prior learners with the same initialization and a single particle transition module to learn the hidden physics of different fluid drops separately. Table 4 and Figure 5 present both quantitative and qualitative results. Our approach showcases robust generalization ability when dealing with visual scenes containing fluid dynamics that diverge significantly from the patterns in the pretrained dataset.\nFurthermore, we explore the performance of an alternative method that employs scene-specific, time-invariant latent variables (i.e., Global Latent in Table 4 and Figure 5). From these results, we find that optimizing time-varying latent features individually for each particle is more effective than optimizing two sets of global latents, each designated for a particular fluid type."
        },
        {
            "heading": "5.4 ABLATION STUDY",
            "text": "To verify the effectiveness of transferring the learned visual posterior z\u0302 to the physical prior learner, we experiment with different variants of our method. The result is shown in Table 5. w/o StageB\nrefers to the model without transferring posterior latent distribution z\u0302 to the physical prior learner and directly finetunes it on visual observations. w/o Stage C refers to without adapting the physical prior learner. In this case, since we only have features z\u0302 attached to each particle, we cannot simulate novel scenes of various particle numbers. The results show that the posterior latent distribution can make training more stable by restricting the range of distribution in latent space, and the transfer learning of the prior learner enables the prediction in novel scenes. In addition, we further investigate the performance gap between feeding the ground truth initial state and the estimated initial state to the probabilistic fluid simulator. We find that models utilizing estimated initial states yield comparable performance results when compared to those using ground truth initial states."
        },
        {
            "heading": "6 POSSIBILITIES OF REAL-WORLD EXPERIMENTS",
            "text": "The primary focus of this paper is to explore the feasibility of a new learning scheme for intuitive physics. Using synthetic data can greatly facilitate the evaluation of our model, as the simulation results can be directly quantified using particle states; whereas realworld scenes would necessitate more advanced fluid flow measurement techniques like particle image velocimetry. For a similar reason, earlier attempts like NeuroFluid and PAC-NeRF are also evaluated on synthetic data. Nevertheless, we acknowledge that realworld validation is meaningful and challenging, and so make our best efforts to explore the possibility of implementing latent intuitive physics in real-world scenarios. As shown in Figure 6, we capture RGB images of dyed water in a fluid tank at a resolution of 1,200\u00d7 900. To cope with the complex and noisy visual environment, we adopt NeRFREN (Guo et al., 2022) to remove the\nreflection and refraction and segment the fluid body by SAM (Kirillov et al., 2023). The preprocessed images are then used to estimate fluid positions using our proposed initial state estimation module. We carefully discuss the pipeline in Appendix G and include a video in the supplementary. Another notable challenge of the complete real-world experiments is to acquire high frame-rate images with synchronized cameras across multiple viewpoints. We leave this part for future work."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "In this paper, we presented latent intuitive physics, which learns the hidden physical properties of fluids from a 3D video. The key contributions include: 1) a probabilistic fluid simulator that considers the stochastic nature of complex physical processes, and 2) a variational inference learning method that can transfer the posteriors of the hidden parameters from visual observations to the fluid simulator. Accordingly, we proposed the pretraining-inference-transfer optimization scheme for the model, which allows for easy transfer of visual-world fluid properties to novel scene simulation with various initial states and boundary conditions. We evaluated our model on synthetic datasets (similar to existing literature) and discussed its potential in real-world experiments."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported by the National Natural Science Foundation of China (Grant No. 62250062, 62106144), the Shanghai Municipal Science and Technology Major Project (Grant No. 2021SHZDZX0102), the Fundamental Research Funds for the Central Universities, the Shanghai Sailing Program (Grant No. 21Z510202133), and the CCF-Tencent Rhino-Bird Open Research Fund."
        },
        {
            "heading": "A MODEL DETAILS",
            "text": ""
        },
        {
            "heading": "A.1 OVERALL FRAMEWORK",
            "text": "Figure 7 demonstrates the main architecture of the proposed framework, and Table 6 presents the summary of all model components, including formulation, input, output, training stage, and objective. The entire model includes a probabilistic particle transition module (\u03b8), a physical prior learner (\u03c8), a particle-based posterior module (\u03be), and a neural renderer (\u03d5). The physical prior learner p\u03c8(z\u0303t | x1:t\u22121, z\u0303t\u22121) infers latent features z\u0303t from consecutive particle data, which the transition module p\u03b8(xt | xt\u22121, zt) can use to predict the next state xt. In the pre-training stage, a particle-based posterior q\u03be(zt | x1:t, zt\u22121) generates posterior zt with the prediction target xt as input. It guides the prior learner to generate meaningful latent features that capture hidden properties essential for accurate predictions. During visual inference and transfer in Stage B and C, the neural renderer R\u03d5 generates images according to particle positions and viewing directions, which enables photometric error to be backpropagated through the whole differentiable neural network. In the visual posterior inference stage (Stage B), the visual posteriors z\u0302 are optimized by backpropagating image rendering errors. They serve as adaptation targets for the prior learner, facilitating efficient adaptation to the specific observed fluid.\n! !! !"
        },
        {
            "heading": "A.2 PROBABILISTIC FLUID SIMULATOR",
            "text": "The probabilistic fluid simulator is used to infer hidden physics and simulate various kinds of fluid dynamics. Specifically, as shown in Figure 3, our probabilistic fluid simulator consists of three modules: probabilistic particle transition module (\u03b8), physical prior learner (\u03c8) and particle-based posterior module (\u03be).\nWe use continuous convolution (CConv) (Ummenhofer et al., 2020) as the shared feature encoder for the prior learner (\u03c8) and the particle-based posterior module (\u03be) to extract features from particle states. The CConv module predicts particle features in a smooth and continuous way: (f \u2217 g)(x) =\n1 n(x) \u2211 i\u2208N (x,R) a ( xi, x ) fig ( \u039b ( xi \u2212 x )) , with f the input feature function and g as filter function. The input consists of particle positions and the corresponding feature. The normalization 1n(x) can be turned on with the normalization parameter. The per neighbor value a(xi, x) is a window function to produce a smooth response of the convolution under varying particle neighborhoods. It determines the ith particle feature by aggregating particle features given its neighbors\u2019 features. Thereby, the features of the neighbors are weighted with a kernel function depending on their relative position. The kernel functions themselves are discretized via a regular grid with spherical mapping and contain the learnable parameters. To cope with boundary interactions, we follow the implementation of (Ummenhofer et al., 2020). Specifically, the feature encoder is realized with two separate CConv layers: One is to encode the fluid particles in the neighborhood of each particle location; The other one is to handle the virtual boundary particles in the same neighborhood. The features extracted from these processes are then concatenated to form the inputs for subsequent layers. The following GRUs can summarize the historical information and provide the inferred distribution of latent features about the system. In practice, we use the mean of distributions at the last time step as input of the encoder CConv to avoid excessively noisy inputs."
        },
        {
            "heading": "A.3 NEURAL RENDERER",
            "text": "We present detailed model architecture of PhysNeRF Guan et al. (2022). As shown in Figure 8, the network is based on fully-connected layers and similar to NeRF (Mildenhall et al., 2020). Unlike original NeRF, PhysNeRF performs volume rendering according to the geometric distribution of neighboring physical particles along a given camera ray. For a sampled point in a ray, a ball query is conducted to identify neighboring fluid particles around the sampled point. These neighboring particles are then parameterized to obtain view-independent and view-dependent encodings ep, ed, which are used in the volume rendering subsequently. A multilayer perception (MLP) is trained to map these encodings along with view direction d to volume density \u03c3 and emitted color c of each sampled point along each ray r(t) = o + td, such that (c, \u03c3) = R\u03d5(ep, ed,d). Different from original PhysNeRF, we exclude the position of the sampled point from the inputs to the MLP network, which enhances the relationships between the fluid particle encodings and the rendering results. The\nview-independent encodings ep includes 3 parts: fictitious particle center pc, soft particle density \u03c3p and radial deformation vD, calculated as:\nFictitious particle center: pc = 1\nK \u2211 pi\u2208N (r(t),rs) wip i\nSoft particle density: \u03c3p = \u2211 i wi\nRadial deformation: vD = 1\nK \u2211 i \u2225\u2225\u2225\u2225\u2225\u2225\u2225pi \u2212 r(t)\u2225\u2225\u2212 1K \u2211 i \u2225\u2225pi \u2212 r(t)\u2225\u2225\u2225\u2225\u2225\u2225\u2225 2 ,\n(4)\nwhere N (r(t), rs) is the ball query neighborhood within radius rs of the sampled camera ray point r(t) andwi = max(1\u2212(\u2225p i\u2212r(t)\u22252 rs\n)3, 0). The view-dependent encoding ed includes the normalized view direction to the fictitious particle center, which is an important reference direction for the network to infer the refraction and reflection, calculated as:\ndc = (pc \u2212 o) / \u2225pc \u2212 o\u22252 . (5)\nFinally, we take into account all the above physical quantities and derive the view-independent encoding and the view-dependent encoding as\nep = (pc, \u03c3p,vD) , ed = dc. (6)\nFollowing (Mildenhall et al., 2020; Liu et al., 2020; Sun et al., 2022), we apply positional encoding \u0393(\u00b7) to every input. In our experiments, we set the maximum encoded frequency L = 10 for \u0393(ep) and L = 4 for \u0393(ed),\u0393(d). We optimize PhysNeRF in a coarse-to-fine manner (Mildenhall et al., 2020). For the fine MLP network, the search radius of particle encoding of ep and ed is set as 3 times the particle radius and we consider 20 fluid particles within this search radius. For the coarse MLP network, the search radius scale and the number of encoding neighbors are set as 1.3 times the parameters of the fine network."
        },
        {
            "heading": "A.4 HYPERPARAMETERS",
            "text": "Table 7 shows the hyperparameters used in experiments."
        },
        {
            "heading": "B TRAINING ALGORITHM",
            "text": "Algorithm 1 gives detailed descriptions of the computation flow of the training process."
        },
        {
            "heading": "C DATASETS",
            "text": ""
        },
        {
            "heading": "C.1 PARTICLE DATASETS",
            "text": "This dataset is used for pretraining the probabilistic fluid simulator in Stage A. Following (Ummenhofer et al., 2020; Prantl et al., 2022), we simulate the dataset with DFSPH (Bender & Koschier, 2015) using the SPlisHSPlasH framework2. This simulator generates fluid flows with low-volume compression. The particle dataset contains 600 scenes, with 540 scenes used for training and 60 scenes reserved for the test set. In each scene, the simulator randomly places a fluid body of random shape in a cubic box (see Default Boundary in Figure 9) and the fluid body freely falls under the influence of gravity and undergoes collisions with the boundary. The initial fluid body in each scene is applied with random rotation, scaling, and initial velocity. The simulator randomly samples physical properties viscosity \u03bd and density \u03c1 for fluid bodies from uniform distribution U(0.01, 0.4) and U(500, 2000) respectively. The simulation lasts for 4 seconds, which consists of 200 time steps. In general, there are 273 \u223c 19682 fluid particles in each scene in this dataset."
        },
        {
            "heading": "C.2 GENERATION OF VISUAL OBSERVATIONS",
            "text": "The visual observations are used in Stages B & C. Under each set of physical parameters, we generate a single 3D video of fluid dynamics with Cuboid (unseen during Stage A) geometry. Each example contains 60 time steps, where the most significant dynamic changes are included. We use Blender (Community, 2018) to generate multi-view visual observations. Each fluid dynamic example is captured from 20 randomly sampled viewpoints, with the cameras evenly spaced on the upper hemisphere containing the object. The first 50 time steps are used for training, and the last 10 time steps are used for the evaluation of future prediction."
        },
        {
            "heading": "C.3 EVALUATION SETS OF VISUAL PHYSICAL INFERENCE",
            "text": "Our model and the baselines are evaluated on 3 challenging novel scene simulation tasks. Figure 9 shows boundaries and fluid geometries used for evaluation.\n\u2022 Unseen Fluid Geometries. We use Standford Bunny, Sphere, Dam Break that are unseen during pretraining as fluid bodies. The default boundary used in Stage A is used as a fluid container. We generate 50 sequences of 60 time steps under each set of physical parameters for evaluation. In each evaluation scene, random fluid geometry is chosen and random rotation, scaling, and velocities are imposed on the fluid body.\n\u2022 Unseen Boundary conditions. We use Dam Break fluid and unseen boundary for this task. This evaluation set features an out-of-domain boundary with a slender pillar positioned in the center. In each scene, the Dam Break fluid collapses and strikes the pillar in the container, which was unseen during pretraining and from visual observations. Similarly, random rotation, scaling, and velocities are imposed on Dam Break fluid, and 50 sequences of 60 time steps under each set of physical parameters for evaluation.\nThe horizontal initial velocity of the fluid body is randomly sampled from a uniform distribution of U(\u22122, 2), with a vertical initial velocity of 0, and scale by a factor randomly sampled from another uniform distribution of U(0.8, 1.2). Table 9 shows the particle number of fluid bodies with no scaling applied. Table 8 illustrates the features of the visual observation scene and evaluation sets."
        },
        {
            "heading": "C.4 DYNAMICS DISCREPANCIES",
            "text": "In this task, we simulate two heterogeneous fluids with different physical properties. The generation of visual examples of this task is the same as previous tasks, but with two fluids of different physical properties interacting with each other. The evaluation set contains the same heterogeneous fluids but with unseen fluid geometries as fluid bodies. The evaluation set contains 50 sequences in total. Similarly, random rotation, scaling, and velocities are imposed on the fluid bodies in scenes in the evaluation set.\n2https://github.com/InteractiveComputerGraphics/SPlisHSPlasH"
        },
        {
            "heading": "D EXPERIMENTAL DETAILS",
            "text": ""
        },
        {
            "heading": "D.1 EVALUATION METRICS",
            "text": "We adopt the evaluation metric from the literature of CConv Ummenhofer et al. (2020). For all experiments but the short-term predictions in Table 3, we compute the average Euclidean distance from the true particle positions (pit) to the positions of the closest predicted particles (p\u0302t):\nd = 1 T \u00d7N \u2211 t \u2211 i min p\u0302t ||pit \u2212 p\u0302t||2\nwhere T is the prediction time horizon and N is the number of particles. In particular, for the short-term prediction experiments (n+ 1 and n+ 2) in Table 3, we compute an one-to-one mapping metric, the average error of the predicted particles w.r.t. the corresponding ground truth particles:\ndt = 1\nN \u2211 i ||pit \u2212 p\u0302it||2\nwhere p\u0302it is the predicted position for particle i.\nD.2 VISUAL PHYSICAL INFERENCE"
        },
        {
            "heading": "D.2.1 BASELINES",
            "text": "Our method is compared with the four baseline models below. For NeuroFluid and system identification, we adopt PhysNeRF with the same architecture and hyperparameter in Table 7. All models are trained given the single 3D video of fluid dynamics with the Cuboid geometry and evaluated on novel scenes given ground truth initial states.\n\u2022 CConv (Ummenhofer et al., 2020): Given the initial particle positions and velocities, CConv (Ummenhofer et al., 2020) uses a continuous convolution network that is performed in 3D space to simulate the particle transitions. However, the input to the CConv model only contains particle position and velocity such that it has a limitation that it can only perform simulation on fluid with identical physical parameters. We use this CConv model which has no additional input feature, pretrained on particle dataset in Sec. C.1 and directly evaluate this model on novel scene simulations.\n\u2022 NeuroFluid (Guan et al., 2022): A fully differentiable method for fluid dynamics grounding that links particle-based fluid simulation with particle-driven neural rendering in an end-to-end trainable framework. This approach links particle-based fluid simulation with particle-driven neural rendering in an end-to-end trainable framework, such that the two networks can be jointly optimized to obtain reasonable particle representations between them. We adopt the CConv model pretrained on the particle dataset in Sec. C.1 as the initial transition model and optimize NeuroFluid on Cuboid scene in an end-to-end manner."
        },
        {
            "heading": "Default Boundary Unseen Boundary Cuboid Standford Bunny Sphere Dam Break",
            "text": "\u2022 PAC-NeRF (Li et al., 2023): PAC-NeRF is a method that estimates physical parameters from multi-view videos with Eulerian-Lagrangian representation of neural radiance field and MPM simulator. We feed the physical parameters estimated on Cuboid scene to the MPM simulator and use the given physical parameters to rollout on novel scenes.\n\u2022 System Identification (Sys-ID): For system identification, we train another CConv but with additional features of physical parameters for input including viscosity \u03bd and density \u03c1. Sys-ID optimizes fluid properties by backpropagating the rendering loss through the trained transition network and PhysNeRF. The optimized physical parameters are used as model inputs to simulate novel scenes.\nNote that baselines and our model are optimized over the entire visual sequence.\nD.2.2 IMPLEMENTATION DETAILS\nWe train our model and the baselines with multi-view observations on the fluid sequence of the Cuboid geometry. Before Stage B, The PhysNeRF is finetuned on visual observations for 100k steps with learning rate 3e\u22124 and exponential learning rate decay \u03b3 = 0.1 given the estimated initial state and multi-view observation of the first frame. After that, we freeze R\u03d5 and T\u03b8 (pretrained on Particle Dataset) and infer the visual posterior by backpropagating the rendering loss. Then the physical prior learner p\u03c8 is trained to adapt to the inferred visual posterior. The visual posterior latent and physical prior learner are separately optimized for 100k steps and 50k steps in Stage B and Stage C, with a learning rate of 1e\u22124 and a cosine annealing scheduler.\nD.2.3 INITIAL STATE ESTIMATION\nWe estimate the initial state of fluid particles given multi-view observation on the first frame and then feed the estimated initial state to all simulators and the neural renderer. We estimate the initial particle positions using the voxel-based neural rendering technique (Liu et al., 2020; Sun et al., 2022; M\u00fcller et al., 2022) and maintain an occupancy cache to represent empty vs. nonempty space. At test time, we randomly sample fluid particles within each voxel grid to generate initial particle positions. To maintain spatial consistency between the estimated initial particle positions and the particle density generated by SPlisHSPlasH (Bender et al., 2022), we employ the fluid particle discretization tools provided in SPlisHSPlasH to calculate the fluid particle count per unit volume. When estimating initial particle positions with new fluid scenes, we adopt the voxel-based neural renderer to predict the spatial volume occupied by the fluid, enabling the subsequent sampling of fluid particles according to the prescribed fluid particle density per unit volume. Since we do not apply random initial velocities to the fluid dynamics in visual examples, the initial velocities are set as zero."
        },
        {
            "heading": "D.3 EVALUATION OF PROBABILISTIC FLUID SIMULATOR",
            "text": ""
        },
        {
            "heading": "D.3.1 BASELINES ON PARTICLE DATASET",
            "text": "We compare our probabilistic fluid simulator with four representative particle simulation approaches, based on GNN, continuous convolution, and Transformer, i.e., DPI-Net (Li et al., 2019), CConv (Ummenhofer et al., 2020), DMCF (Prantl et al., 2022), and TIE (Shao et al., 2022).\n\u2022 DPI-Net (Li et al., 2019): DPI-Net is a particle-based simulation method that combines multi-step spatial propagation, a hierarchical particle structure, and dynamic interaction graphs.\n\u2022 CConv (Ummenhofer et al., 2020): The method employs spatial convolutions as the primary differentiable operation to establish connections between particles and their neighbors. It predicts particle features and dynamics in a smooth and continuous way.\n\u2022 DMCF (Prantl et al., 2022): DMCF imposes a hard constraint on momentum conservation by employing antisymmetrical continuous convolutional layers. It utilizes a hierarchical network architecture, a resampling mechanism that ensures temporal coherence.\n\u2022 TIE (Shao et al., 2022): A Transformer-based model that captures the complex semantics of particle interactions in an edge-free manner. The model adopts a decentralized approach to computation, wherein the processing of pairwise particle interactions is replaced with per-particle updates. The original method is conducted on the PyFlex dataset with less number of fluid particles (\u223c hundreds). However, on Particle Dataset (\u223c thousands), the experiment on TIE leads to unacceptable memory cost. Therefore, we downsample the fluid particles in each scene of the Particle Dataset to the ratio of 1/20.\nD.3.2 IMPLEMENTATION DETAILS\nIn Stage A, the probabilistic fluid simulator is trained to predict two future states from two inputs. The ADAM optimizer (Kingma & Ba, 2015) is used with an initial learning rate of 0.001 and a batch size of 16 for 50k iterations. We follow previous works (Ummenhofer et al., 2020; Prantl et al., 2022) to set a scheduled learning rate decay where the learning rate is halved every 5k iterations, beginning at iteration 25k. The latent distribution of each particle is an 8-dimensional Gaussian with parameterized mean and standard deviation. The KL regularizer \u03b2 is set as 0.1, shown in Table 7. The experiments are conducted on 4 NVIDIA RTX 3090 GPUs. To enhance long-term prediction capability, the probabilistic fluid simulator is trained to predict 5 future states from 5 inputs for experiments in visual physical inference. Additionally, we use tanh as an activation function for layers in the shared feature encoder (CConv). This is to ensure the learned posterior of latent distribution from visual observation lies in the space of the learned physical prior learner."
        },
        {
            "heading": "E EXPERIMENTS WITH NON-ZERO INITIAL VELOCITIES OF THE OBSERVED VISUAL SCENE",
            "text": "To assess the performance of our method in a more general case, specifically, learning from visual observations of fluids with non-zero initial velocities, we modify the training scheme in Stage B by randomly initializing {vit=1}i=1:N and treating them as trainable parameters. The optimization of {vit=1}i=1:N is carried out concurrently with the optimization of the visual posteriors z\u0302. We evaluate the results against a model trained with true non-zero initial velocities given on the observed scene. In Table 10, we compare future prediction errors on observed scenes of fluids with non-zero initial velocities. In Table 11, we compare the average simulation errors of the two models on novel scenes with new initial geometries and boundaries. We can observe that both models produce comparable results, showcasing the ability of our method to infer uncertain initial velocities of fluids by treating them as optimized variables."
        },
        {
            "heading": "F ADDITIONAL VISUALIZATION RESULTS",
            "text": "Figure 10, 11, & 12 provide visualizations of the predicted particles of baselines and our model on the novel scenes with unseen fluid geometries. Figure 13, 14, & 15 provide visualizations of the predicted particles of baselines and our model on the novel scenes with unseen boundary conditions. We can see that CConv and NeuroFluid tend to produce noisy predictions on novel scenes. PAC-NeRF produces sporadic predictions that are contrary to the continuous dynamics behavior as ground truth simulations. Our model has more reasonable prediction results and can effectively respond to different physical parameters.\nWe present qualitative results of the predicted particles of the pretrained probabilistic fluid simulator in Figure 16. We can see that our method produces closer prediction results with ground truth. This indicates that our probabilistic fluid simulator shows the capability to capture broad hidden physics within the latent space, leveraging spatiotemporal correlation in particle data."
        },
        {
            "heading": "G FURTHER DISCUSSIONS ON REAL-WORLD EXPERIMENTS",
            "text": "The primary focus of this paper is to explore the feasibility of the proposed inference\u2013transfer learning scheme for physics, we first use synthetic visual data to evaluate the proposed method. In real-world experiments, however, we need to use specialized fluid flow measurement techniques, such as Particle Image Velocimetry (PIV), to measure the performance of the model. Most inverse graphics methods are not applied to real-world scenarios of fluid dynamics that undergo intense change and visual noises, including prior arts like NeuroFluid (Guan et al., 2022) and PAC-NeRF (Li et al., 2023). The visual observations of fluid dynamics in real-world scenarios contain more visual noises, such as reflection and refraction, which makes it harder to cope with.\nNevertheless, we acknowledge that real-world validation is meaningful and challenging. To explore the possibility of applications of latent intuitive physics in real-world scenarios with complex and noisy visual observation, we made our best efforts to conduct real-world experiments. As the pipeline provided in Figure 17, we capture the dynamics of dyed water in the fluid tanks. We collect RGB images at a resolution of 1,200\u00d7 900 on the hemisphere of the scene. As the presence of refraction and reflection phenomena (which are usually absent in other dynamics contexts) will bring extra burden for the reconstruction of fluid geometries and dynamics, we first adopt NeRFREN (Guo et al., 2022) to remove the inherent reflection and refraction of fluids and the container. Then we segment the fluid body and remove the background using SAM (Kirillov et al., 2023). The preprocessed images are then used to estimate fluid positions using the initial state estimation module. Estimated positions are shown in Figure. 17. Please refer to the supplementary video for a vivid illustration of real-world experiments. However, an outstanding challenge of the complete experiment is to acquire high frame-rate images with synchronized cameras across multiple viewpoints. Therefore, we have to leave this part for future work."
        }
    ],
    "year": 2024
}