{
    "abstractText": "Unsupervised methods for dimensionality reduction of neural activity and behavior have provided unprecedented insights into the underpinnings of neural information processing. One popular approach involves the recurrent switching linear dynamical system (rSLDS) model, which describes the latent dynamics of neural spike train data using discrete switches between a finite number of low-dimensional linear dynamical systems. However, a few properties of rSLDS model limit its deployability on trial varying data, such as a fixed number of states over trials, and no latent structure nor organization of states. Here we overcome these limitations by endowing the rSLDS model with a semi-Markov discrete state process, with latent geometry, that captures key properties of stochastic processes over partitions with flexible state cardinality. We leverage partial differential equations (PDE) theory to derive an efficient, semi-parametric formulation for dynamical sufficient statistics to the discrete states. This process, combined with switching dynamics, defines our infinite recurrent switching linear dynamical system (irSLDS) model class. We first validate and demonstrate the capabilities of our model on synthetic data. Next, we turn to the analysis of mice electrophysiological data during decision-making, and uncover strong non-stationary processes underlying both within-trial and trial-averaged neural activity.",
    "authors": [],
    "id": "SP:e5d228ada8e729eb809822b348c7d8b9cf05ab4b",
    "references": [
        {
            "authors": [
                "Charles E. Antoniak"
            ],
            "title": "Mixtures of dirichlet processes with applications to bayesian nonparametric problems",
            "venue": "Ann. Statist.,",
            "year": 1974
        },
        {
            "authors": [
                "Zoe C. Ashwood",
                "Nicholas A. Roy",
                "Iris R. Stone",
                "Anne E. Urai",
                "Anne K. Churchland",
                "Alexandre Pouget",
                "Jonathan W. Pillow"
            ],
            "title": "Mice alternate between discrete strategies during perceptual decision-making",
            "venue": "Nature Neuroscience,",
            "year": 2022
        },
        {
            "authors": [
                "Matthew Beal",
                "Zoubin Ghahramani",
                "Carl Rasmussen"
            ],
            "title": "The Infinite Hidden Markov Model",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2001
        },
        {
            "authors": [
                "D.M. Blei",
                "P.I. Frazier"
            ],
            "title": "Distance Dependent Chinese Restaurant Processes",
            "venue": "Journal of Machine Learning Research,",
            "year": 2011
        },
        {
            "authors": [
                "Guy E. Blelloch"
            ],
            "title": "Prefix sums and their applications",
            "venue": "Technical Report CMU-CS-90-190,",
            "year": 1990
        },
        {
            "authors": [
                "Scott S. Bolkan",
                "Iris R. Stone",
                "Lucas Pinto",
                "Zoe C. Ashwood",
                "Jorge M. Iravedra Garcia",
                "Alison L. Herman",
                "Priyanka Singh",
                "Akhil Bandi",
                "Julia Cox",
                "Christopher A. Zimmerman",
                "Jounhong Ryan Cho",
                "Ben Engelhard",
                "Jonathan W. Pillow",
                "Ilana B. Witten"
            ],
            "title": "Opponent control of behavior by dorsomedial striatal pathways depends on task demands and internal state",
            "venue": "Nat Neurosci,",
            "year": 2022
        },
        {
            "authors": [
                "Adam J. Calhoun",
                "Jonathan W. Pillow",
                "Mala Murthy"
            ],
            "title": "Unsupervised identification of the internal states that shape natural behavior",
            "venue": "Nature Neuroscience,",
            "year": 2019
        },
        {
            "authors": [
                "Julia C Costacurta",
                "Lea Duncker",
                "Blue Sheffer",
                "Winthrop Gillis",
                "Caleb Weinreb",
                "Jeffrey Evan Markowitz",
                "Sandeep R. Datta",
                "Alex H Williams",
                "Scott Linderman"
            ],
            "title": "Distinguishing discrete and continuous behavioral variability using warped autoregressive HMMs",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Benjamin R. Cowley",
                "Adam C. Snyder",
                "Katerina Acar",
                "Ryan C. Williamson",
                "Byron M. Yu",
                "Matthew A. Smith"
            ],
            "title": "Slow drift of neural activity as a signature of impulsivity in macaque visual and prefrontal",
            "venue": "cortex. Neuron,",
            "year": 2020
        },
        {
            "authors": [
                "Pierre Del Moral",
                "Arnaud Doucet",
                "Ajay Jasra"
            ],
            "title": "Sequential monte carlo samplers",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
            "year": 2006
        },
        {
            "authors": [
                "Thomas S. Ferguson"
            ],
            "title": "A bayesian analysis of some nonparametric problems",
            "venue": "Ann. Statist.,",
            "year": 1973
        },
        {
            "authors": [
                "E. Fox",
                "E.B. Sudderth",
                "M.I. Jordan",
                "A.S. Willsky"
            ],
            "title": "Bayesian nonparametric inference of switching dynamic linear models",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2011
        },
        {
            "authors": [
                "Emily B. Fox",
                "Erik B. Sudderth",
                "Michael I. Jordan",
                "Alan S. Willsky"
            ],
            "title": "An hdp-hmm for systems with state persistence",
            "venue": "In Proceedings of the 25th International Conference on Machine Learning,",
            "year": 2008
        },
        {
            "authors": [
                "Emily B. Fox",
                "Erik B. Sudderth",
                "Michael I. Jordan",
                "Alan S. Willsky"
            ],
            "title": "Bayesian nonparametric methods for learning markov switching processes",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2010
        },
        {
            "authors": [
                "Joshua Glaser",
                "Matthew Whiteway",
                "John P Cunningham",
                "Liam Paninski",
                "Scott Linderman"
            ],
            "title": "Recurrent Switching Dynamical Systems Models for Multiple Interacting Neural Populations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Nicholas Kantas",
                "Arnaud Doucet",
                "Sumeetpal Sindhu Singh",
                "Jan Marian Maciejowski"
            ],
            "title": "An overview of sequential monte carlo methods for parameter estimation in general state-space models",
            "venue": "IFAC Proceedings Volumes,",
            "year": 2009
        },
        {
            "authors": [
                "International Brain Laboratory",
                "Valeria Aguillon-Rodriguez",
                "Dora Angelaki",
                "Hannah Bayer",
                "Niccol\u00f2 Bonacchi",
                "Matteo Carandini",
                "Fanny Cazettes",
                "Gaelle Chapuis",
                "Anne K Churchland",
                "Yang Dan"
            ],
            "title": "Standardized and reproducible measurement of decision-making in mice",
            "venue": "Elife, 10:e63711,",
            "year": 2021
        },
        {
            "authors": [
                "Taheri",
                "Anne E Urai",
                "Miles Wells",
                "Steven J West",
                "Matthew R Whiteway",
                "Olivier Winter",
                "Ilana B Witten"
            ],
            "title": "Reproducibility of in-vivo electrophysiological measurements in mice",
            "venue": "URL https://doi.org/10.1101/2022.05.09.491042",
            "year": 2022
        },
        {
            "authors": [
                "S.W. Linderman",
                "M.J. Johnson",
                "R.P. Adams"
            ],
            "title": "Dependent multinomial models made easy: Stick-breaking with the polya-gamma augmentation",
            "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems",
            "year": 2015
        },
        {
            "authors": [
                "S.W. Linderman",
                "A.C. Miller",
                "R.P. Adams",
                "D.M. Blei",
                "L. Paninski",
                "M.J. Johnson"
            ],
            "title": "Recurrent switching linear dynamical systems, 2016",
            "venue": "URL https://arxiv.org/abs/1610.08466",
            "year": 2016
        },
        {
            "authors": [
                "Scott Linderman",
                "Annika Nichols",
                "David Blei",
                "Manuel Zimmer",
                "Liam Paninski"
            ],
            "title": "Hierarchical recurrent state space models reveal discrete and continuous dynamics of neural activity in c",
            "venue": "URL https://doi.org/10.1101/621540",
            "year": 2019
        },
        {
            "authors": [
                "Scott Linderman",
                "Benjamin Antin",
                "David Zoltowski",
                "Joshua Glaser"
            ],
            "title": "Ssm: Bayesian learning and inference for state space models, 2020",
            "venue": "URL https://github.com/lindermanlab/ssm",
            "year": 2020
        },
        {
            "authors": [
                "Samuele M. Marcora",
                "Walter Staiano",
                "Victoria Manning"
            ],
            "title": "Mental fatigue impairs physical performance in humans",
            "venue": "Journal of Applied Physiology,",
            "year": 2009
        },
        {
            "authors": [
                "Josue Nassar",
                "Scott Linderman",
                "Monica Bugallo",
                "Il Memming Park"
            ],
            "title": "Tree-structured recurrent switching linear dynamical systems for multi-scale modeling",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Liam Paninski",
                "Yashar Ahmadian",
                "Daniel Gil Ferreira",
                "Shinsuke Koyama",
                "Kamiar Rahnama Rad",
                "Michael Vidne",
                "Joshua Vogelstein",
                "Wei Wu"
            ],
            "title": "A new look at state-space models for neural data",
            "venue": "Journal of Computational Neuroscience,",
            "year": 2009
        },
        {
            "authors": [
                "Nicholas G. Polson",
                "James G. Scott",
                "Jesse Windle"
            ],
            "title": "Bayesian inference for logistic models using p\u00f3lya\u2013gamma latent variables",
            "venue": "Journal of the American Statistical Association,",
            "year": 2013
        },
        {
            "authors": [
                "Nicholas A. Roy",
                "Ji Hyun Bak",
                "Athena Akrami",
                "Carlos D. Brody",
                "Jonathan W. Pillow"
            ],
            "title": "Extracting the dynamics of behavior in sensory decision-making experiments. Neuron, 109(4):597\u2013610.e6, February 2021",
            "venue": "doi: 10.1016/j.neuron.2020.12.004. URL https://doi.org/10.1016/j",
            "year": 2020
        },
        {
            "authors": [
                "Brian J. Schriver",
                "Svetlana Bagdasarov",
                "Qi Wang"
            ],
            "title": "Pupil-linked arousal modulates behavior in rats performing a whisker deflection direction discrimination task",
            "venue": "Journal of Neurophysiology,",
            "year": 2018
        },
        {
            "authors": [
                "Hideaki Shimazaki",
                "Shigeru Shinomoto"
            ],
            "title": "A method for selecting the bin size of a time histogram",
            "venue": "Neural Computation,",
            "year": 2007
        },
        {
            "authors": [
                "Jimmy Smith",
                "Scott Linderman",
                "David Sussillo"
            ],
            "title": "Reverse engineering recurrent neural networks with Jacobian switching linear dynamical systems",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jimmy T.H. Smith",
                "Andrew Warrington",
                "Scott Linderman"
            ],
            "title": "Simplified state space layers for sequence modeling",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Yee Whye Teh",
                "Michael I Jordan",
                "Matthew J Beal",
                "David M Blei"
            ],
            "title": "Hierarchical dirichlet processes",
            "venue": "Journal of the American Statistical Association,",
            "year": 2006
        },
        {
            "authors": [
                "Alexander B. Wiltschko",
                "Matthew J. Johnson",
                "Giuliano Iurilli",
                "Ralph E. Peterson",
                "Jesse M. Katon",
                "Stan L. Pashkovski",
                "Victoria E. Abraira",
                "Ryan P. Adams",
                "Sandeep Robert Datta"
            ],
            "title": "Mapping sub-second structure in mouse",
            "venue": "behavior. Neuron,",
            "year": 2015
        },
        {
            "authors": [
                "Byron M. Yu",
                "John P. Cunningham",
                "Gopal Santhanam",
                "Stephen I. Ryu",
                "Krishna V. Shenoy",
                "Maneesh Sahani"
            ],
            "title": "Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity",
            "venue": "Journal of Neurophysiology,",
            "year": 2009
        },
        {
            "authors": [
                "D.M. Zoltowski",
                "J.W. Pillow",
                "S.W. Linderman"
            ],
            "title": "Unifying and generalizing models of neural dynamics during decision-making, 2020",
            "venue": "URL https://arxiv.org/abs/2001.04571",
            "year": 2001
        },
        {
            "authors": [
                "Linderman"
            ],
            "title": "2016), we can leverage P\u00f3lya-gamma augmentation (Polson et al., 2013; Linderman et al., 2015) to deal with these non-Gaussian factors. The key is that instead of performing a categorical choice over a pre-determined set of K dynamical modes, we perform an association, a categorical choice, with a previous time-step",
            "year": 2015
        },
        {
            "authors": [
                "Transition module We implement the irSLDS within the SSM package from Linderman"
            ],
            "title": "This can be accomplished by using the same SLDS parent class as the (r)SLDS models, and specifying a new discrete state process",
            "venue": "We implement this discrete state process as a ssm.transitions.Transitions module.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Parsing behavioral and neural dynamics into simpler alternating states is providing unprecedented insights into the nature of computation in the brain (Wiltschko et al., 2015; Calhoun et al., 2019; Ashwood et al., 2022; Bolkan et al., 2022). Such data, conventionally collected over multiple trials and spanning considerable lengths of time, is often related to latent processes that exhibit complex dynamics across multiple timescales (Cowley et al., 2020; Roy et al., 2021; Nassar et al., 2019; Yu et al., 2009). Learning is one important example of such non-stationarity, but other changes may be present, such as fatigue (Marcora et al., 2009) or arousal (Schriver et al., 2018). However, inferring those latent processes from data, either within or across trials, remains challenging.\nThe state-space models that perform this segmentation into discrete states commonly revolve around the hidden Markov model (HMM). This model posits an underlying, hidden, discrete Markov chain, with a different observation model for each state giving rise to the data. Each discrete state can also be extended to capture a different dynamical regime, governing the dynamics of some continuous-space stochastic process. The resulting switching state-space models capture activity that alternate between a discrete set of dynamical regimes, and have proved useful in the modeling of complex nonlinear activity (Fox et al., 2010; Smith et al., 2021). A significant step in capturing online dependencies in such models is through recurrence (Linderman et al., 2016; Zoltowski et al., 2020), where the continuous dynamical variables govern switches between the discrete states. Such models have been providing powerful insights into the time-dependence of those processes (e.g. Glaser et al. (2020)), but suffer from training challenges due to the fixed cardinality of the discrete states and a lack of geometry over states.\nIndeed, traditional HMMs, and models built upon them such as switching state-space models, use a fixed number of discrete states. This fixed cardinality is typically determined by cross-validation, a procedure that can prove computationally expensive. Furthermore, it does not encourage the use of fewer discrete states on any subset of trials or sessions, so that they can be interpreted and determined\nas needed. To overcome these limitations, multiple avenues have considered expending HMMs with flexible state cardinality (Beal et al., 2001; Fox et al., 2011). These models revolve around the Hierarchical Dirichlet Process (HDP), which places a flexible non-parametric prior over the transition probabilities between the (theoretically infinite) states. Unfortunately, while the HDP and its generalizations can capture state dependence and persistence (Teh et al., 2006; Fox et al., 2008), in their standard formulation they do not allow for recurrence and other dependencies between the discrete states allocations.\nFurthermore, HMMs do not have any a priori state geometry. In particular, the discrete states are permutation invariant, such that the model is equivalent under any permutation in the label ordering of the discrete states (see schematic in Appendix Fig. 4B). This induces non-identifiability and limits the interpretability of the inferred latent discrete states. While recent work has focused on variability in discrete dynamical regimes (Nassar et al., 2019; Linderman et al., 2019; Costacurta et al., 2022) yielding impressive results in terms of flexibility, most of these can be cast as \u201clocal\u201d variations ; they consider perturbations or parametrized variations of the parameters associated with each discrete states. To our understanding, none specifically tackle the geometric nature of the discrete states space.\nIn this work, we use partial differential equations (PDE) theory to develop an alternate prior over the discrete state transitions in recurrent switching state-space models. Our formulation allows us to (1) capture important properties of the stochastic processes for flexible state cardinality, while supporting the use of recurrent connections, (2) induce a geometry over the discrete states, and (3) induce semi-Markovian discrete states dynamics to capture more intricate temporal dependencies. After reviewing relevant background in Section \u00a72, we present in Section \u00a73 our model, the infinite recurrent switching linear dynamical system (irSLDS), and provide details on its generative formalism and inference procedure. In Section \u00a74, we first validate the model on synthetic data and showcase its properties, before turning to mice electro-physiology data from the International Brain Laboratory (IBL). The PDE prior is defined by less parameters than the traditional HMM-based transitions model, yet we show that it maintains or even outperform the traditional model while offering an interpretable structure that is amenable to uncovering trial-varying structure."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "We review in this section the key models considered in this work. We consider time-stamped data {(tn,yn)}Tn=1 with time steps tn and data yn \u2208 RM ."
        },
        {
            "heading": "2.1 SWITCHING LINEAR DYNAMICAL SYSTEMS",
            "text": "Hidden Markov Models (HMM) posit an underlying discrete-state Markov chain with states zn \u2208 {1, . . . ,K}, and conditionally independent observations p(xn|zn). Auto-regressive HMMs (ARHMM) extend this framework by endowing the observations xn \u2208 RD with linear dynamics, dependent on the discrete state zn. The generative model reads as\nzn+1 \u223c P (zn+1|zn, n,xn) (1) xn+1 \u223c N ( A(zn)xn + a (zn),\u03a3x ) (2)\nat time n \u2208 {0, . . . , T}, with a different set of dynamics A(zn) \u2208 RD\u00d7D and bias a(zn) \u2208 RD for each discrete state zn, and covariance \u03a3x. The switching process in (eq. 1) is described by a transition matrix, written in full generality, but is usually taken to be time- (n) and observation (xn) independent such that P (zn+1 = j | zn = i) = Aij for some transition matrix A. In this work, we are interested in leveraging those models to study possibly high-dimensional data yn \u2208 RM , M \u2265 D, such as spike train data. We thus turn to Switching Linear Dynamical Systems (SLDS) models, which model the AR-HMM observations xn as a (low-dimensional) representation of the data, and model the data y1:T as conditionally independent linear Gaussian emissions\nyn \u223c N (Cxn + c,\u03a3y) (3)\nfor n \u2208 {0, . . . , T}, with decoding weights C \u2208 RM\u00d7D, c \u2208 RM , and emission covariance \u03a3y. Finally, inputs un, e.g. the time steps tn, may be linearly encoded in the continuous dynamics (eq. 2)\nor directly along with the emissions in (eq. 3). Following Fox et al. (2010), we refer to AR-HMM and SLDS models as instances of Markov switching processes, or generally switching state-space models.\nA vital augmentation of Markov switching processes is to allow observations or internal states to guide the switches in discrete states. Linderman et al. (2016) introduced such a xn \u2192 zn+1 dependency (blue arrows in Fig. 1E), coining it recurrence. Now, in place of the general equation in eq. (1), the continuous states xn guide the switching through\nzn+1 | zn,xn \u221d Rxn + r\nfor recurrent encoding matrix R \u2208 RK\u00d7D and bias r \u2208 RK , which yields the recurrent SLDS (rSLDS). This is the central model we consider in this work, and the graphical model is depicted in Fig. 1E."
        },
        {
            "heading": "2.2 DISTANCE DEPENDENT CHINESE RESTAURANT PROCESS",
            "text": "The switching process in (eq. 1) sets the discrete dynamical modes of the continuous dynamics. Modeling and inferring trajectories under this process amounts to the problem of clustering the dynamics in a discrete set of dynamical modes. As stated in the introduction, we wish to infer the number of states K from the data, contrary to the formulation above. Dirichlet Processes (DP) are a classical tool in this case, as they provide an infinite random measure over cluster partitions (Ferguson, 1973; Antoniak, 1974). They can be described by the Chinese Restaurant Process (CRP, a form of P\u00f3lya urn process, see primer in Appendix \u00a7A), which captures the self-reinforcing and non-parametric structure of the prior. A key limitation is that points under the CRP are exchangeable (Blei & Frazier, 2011) (see schematic in Appendix Fig. 4A)\u2014under any permutation of the ordering, the probability of a configuration is the same. The purpose of recurrence is to actively control the dynamical mode transition, which fundamentally breaks exchangeability.\nWe seek a stochastic process over discrete state partitions that will allow us to capture recurrent (i.e. online) dependencies on the data or latent states. To this end, we turn to the distance dependent CRP (dist-CRP) from Blei & Frazier (2011). This variant of the CRP breaks exchangeability by associating each time step to another, and then perform clustering based on pairwise assignments. Specifically, at time step n, this process assigns time step i \u2208 [n] with ci \u2208 [n] following\np(ci = j|D,\u03b1, \u03b2) \u221d { f(Dij ;\u03b2) if i \u0338= j \u03b1 else\n(4)\nwith distance matrix Dij , decay function f(\u00b7;\u03b2) and decay parameter \u03b2 > 0. As we consider time-stamped data, we consider the time difference Dij = ti \u2212 tj for i \u2265 j, and enforce that no step is assigned with future steps by setting f(Dij) = 0 if i < j. Finally, we use the entire history c:n = {c:n\u22121, cn} of pairwise assignments to perform clustering: if ci = cj , then i and j are in the same cluster zi. Applying this to the history c:n, we get z:n, thereby setting zn. The graphical model for this process is illustrated in Fig. 1A."
        },
        {
            "heading": "3 INFINITE RECURRENT SWITCHING LINEAR DYNAMICAL SYSTEMS",
            "text": "Our technical contribution consists in using key properties of the dist-CRP to guide the switching process in the rSLDS in a manner that supports efficient inference and generation. Section \u00a73.1 presents the fallouts of a naive combination, providing details on how conventional methods relying on P\u00f3lya-gamma augmentation yield intractable Bayesian inference in this case. Section \u00a73.2 motivates our alternative approach to modeling based on sufficient statistics and partial differential equations (PDEs), and highlights key non-exchangeable and geometrical properties of our model. Finally, Section \u00a73.3 provides implementation details for both generation and inference."
        },
        {
            "heading": "3.1 P\u00d3LYA-GAMMA AUGMENTATION YIELDS INTRACTABLE INFERENCE",
            "text": "P\u00f3lya-gamma augmentation (Polson et al., 2013; Linderman et al., 2015) is a powerful augmentation strategy that handles non-Gaussian factors by introducing additional latent variables to obtain joint Gaussian likelihoods. It was used in the original formulation of the rSLDS model by Linderman et al. (2016) to allow tractable inference with message passing. We show in Appendix \u00a7C.1 that\nfor appropriate choices of decay function f , we can also leverage P\u00f3lya-gamma augmentation to similarly handle the non-Gaussian factors emerging from recurrence. However, the resulting Gaussian augmentation grows linearly for each step n, making it computationally inefficient. Luckily as we\u2019ll see below, we can use sufficient statistics and recurrent dynamics to circumvent this problem."
        },
        {
            "heading": "3.2 PDE PRIOR FOR EFFICIENT PARAMETRIZATION AND NON-EXCHANGEABILITY",
            "text": "To remedy the challenges arising out of a naive combination of the original dist-CRP and SLDS models, we rely on the following sufficient statistic to express the cluster allocations\np(zn = j|c:n, \u03b1, \u03b2) = p(zn = j|w(\u00b7, tn), \u03b1, \u03b2) \u221d { w(j, tn;\u03b2) if j in history \u03b1 else\n(5)\nwhere w(j, t) denotes the influence function of state j at time t w : J \u00d7 R+ \u2192 R+, w(j, t) = \u2211\n{i : ti\u2264t,zi=j}\nf(t\u2212 ti;\u03b2)\nwith distance function f(\u00b7;\u03b2) and decay parameter \u03b2 > 0. Using this sufficient statistic makes the entire process Markovian. Setting f(x;\u03b2) = \u03ba exp(\u2212\u03b2x) to be an exponentially decreasing function, notice that we can rewrite w(j, t) above as a solution to the continuous time ODE\n\u2202 \u2202t w(j, t) = \u2212\u03b2w(j, t) + \u03ba1{zn=j}, \u2200j \u2208 J (6)\nwith c\u00e0dl\u00e0g trajectories w(j, t) in time t. Each time tn a discrete state zn = j is chosen, its influence function is bumped by \u03ba > 0, increasing the weight of this state for future time steps (see Fig. 1B). This has a similar effect as the \u201cstickiness\u201d parameter in the sticky HDP-HMM Fox et al. (2008).\nIn practice, we need to evolve in parallel the influence functions w for each state j. For any fixed t, we make the modeling choice of relating these states to one another by using a smoothness prior (see Fig. 1C). Led by the fact that the temporal evolution is given by (eq. 6), we model the sufficient statistic to evolve according to a partial differential equation (PDE), the heat equation (see Fig. 1)\n\u2202 \u2202t w(j, t) = \u03b3\n\u22022\n\u2202j2 w(j, t), \u03b3 > 0 (7)\nwith initial conditions for t \u2265 tn w(\u00b7, tn) = lim\nt\u2191tn w(\u00b7, t) + \u03ba1zn , w(\u00b7, 0) = 0\nover the state space j \u2208 J . While there is a multitude of alternate priors that could offer a form of spatial smoothness with temporal dynamics, we choose the heat-equation for the following reasons:\n1. The temporal dynamics encompass the dist-CRP. Indeed, by standard separation of variables argument, we have that the temporal component of a solution w to (eq. 7) exhibits the exponential decay desired in (eq. 6) for some appropriate \u03b2 > 0.\n2. The spatial dynamics offer an intuitive notion of diffusion of probabilities. Each time a discrete state j is chosen, \u201cnearby\u201d states also become more probable at the next time step (see Fig. 1C top).\n3. The heat-equation as a differential operator on functions possesses numerous linearity qualities, which make general numerical behavior and approximations favorable.\nWe stress the meaning of \u201cnearby\u201d: in models based around an HMM, the states do not have a geometry1. In comparison, here discrete states can be interpreted as points on a continuous state space J , which we can restrain it to discrete number of states. We discuss how to do so in practice in the next subsection."
        },
        {
            "heading": "3.3 IMPLEMENTATION AND INFERENCE IN INFINITE RECURRENT SLDS",
            "text": "We proceed by describing some of the implementation implications of the PDE prior presented in the previous section. We assume from now on regularly-sampled data in time, of difference \u2206t, and drop the inclusion of the time data tn to only consider the emissions data yn. The code for this work builds on the SSM package (Linderman et al., 2020), and present in Appendix C.2 the relevant modules.\nPDE prior implementation with finite difference methods We implement such sufficient statistic w following the heat-equation through finite difference methods. Let [wn+1]j+1 := w(j +\u2206j, (n+ 1)\u2206t) be our discrete approximation. We use a forward difference of the time derivative and central difference approximation to the second order spatial partial derivative to obtain the solution\nwn+1 = Uwn, U = tridiag(\u03b2, 1\u2212 2\u03b2, \u03b2) \u2208 RK\u00d7K\nwhere \u03b2 = \u03b3 \u2206t\u2206j2 . We impose \u2206t \u2264 (\u2206j)2\n4\u03b3 as a general requirement for stability, and let \u2206j be adjusted accordingly given \u03b3 (model parameter) and \u2206t (data parameter). Inputs can be added to drive the system, including (1) the desired \u03ba1zn=j adding self-reinforcement to the system, and (2) the past internal states encoded by a module of weight R \u2208 RK\u00d7D and bias r \u2208 RK for recurrence. The dimension for wn \u2208 RK is fixed to K a hyperparameter, but importantly, the heat equation dynamics presented capture properties of the dist-CRP. With this lense, K rather acts as an upper bound on the number of states used per trajectory. In all, the dynamics of the sufficient statistic wn follow\nwn+1 | {wn,xn, zn = i} = Uwn + \u03ba1i +Rxn + r (8) with parameters of decay \u03b2 > 0 and self-reinforcement \u03ba \u2208 R. The j-th entry of the vector wn+1 in (eq. 8) defines, up to a necessary row-normalization factor, the ij-th entry of our transition matrix [Wn+1]ij . Dynamical modes zn are then sampled according to (eq. 5), with final random transition parameter \u03b1 > 0. With a similar intuition as the Infinite HMM from Beal et al. (2001), now a few scalar parameters {\u03b1, \u03b2, \u03ba} govern the discrete state zn process, contrasting with the K \u00d7K matrix in the rSLDS. In all eqs. (8, 5) with the switching continuous dynamics in (eq. 2) and emissions in (eq. 3) together define the infinite recurrent SLDS (irSLDS, see graphical model in Fig. 1F).\n1One can consider spectral clustering from the graph Laplacian defined from the transition matrix. This however remains a transformation, and the underlying states originally have no geometry.\nInference with variational Laplace-EM The added sufficient statistic wn in (eq. 8) is deterministically determined given zn\u22121 and xn\u22121, thus adds no component to the posterior inference. We follow prior work and perform inference in this model using variational Laplace-EM from Zoltowski et al. (2020). As an overview, we posit the same structured mean-field approximation q(z1:T )q(x1:T ) to the states posterior. For our continuous states posterior, we use a Laplace approximation around the most likely path x\u03021:T (Paninski et al., 2009). Given a continuous states trajectory under this posterior, the transition matrices W1:T can be computed to define our model joint p(y1:T , z1:T ,x1:T ). From there, the discrete state posterior approximation is found by locally maximizing the expected model joint under the continuous state posterior. Generally, conditioned on samples x1:T , the factor graph for our z posterior is equivalent to that of an heterogeneous HMM. Common tools can then be used, such as importantly the Viterbi algorithm to obtain the most likely discrete state sequence z\u03021:T conditioned on x\u03021:T , which we will consider below.\nDiscrete state geometry The support of wn is taken to be the set {\u2206j , 2\u2206j , . . . ,K\u2206j}, which lives on the latent geometry J used in our PDE formalism. In this work we consider one possible way to leverage this geometry: to define intermediate states. Indeed, the continuity of J allows for interpolation between states. From any distribution on our discrete states zn \u2208 {1, . . . ,K}, or continuous interpolation thereof, one can define different statistics of interest such as the mean or the mode. Such statistics can take continuous values in J , and are ill-defined for traditional HMM-based models due to permutation invariance. In particular, given the most likely sequence z\u03021:T and our posterior q(zn+1 | zn,xn) with interpolation function q\u0303 over J , we will consider the interpolated sequence z\u03031:T as the interpolated posterior modes\nz\u0303n = argmax z\u2208J\nq\u0303(zn = z | z\u0302n\u22121, x\u0302n\u22121), n \u2208 {1, . . . , T}, z\u03030 = argmax q\u0303(z0) (9)\nParallelizing transition matrix dynamics with scans In both generation and inference, the core computational difference in terms of complexity between the irSLDS and the (r)SLDS models is that the transition matrix Wn for the discrete states zn now possesses its own recurrent dynamics (from eq. (8), see Appendix eq. (13)). While the (r)SLDS models also have time-varying transition matrices, they do not have such a recurrent term. To illustrate the difference, considering eq. (8) for row i of the transition matrix Wn, the equivalent notation for the rSLDS would be\nwn+1 | {xn, zn} = [w0]zn +Rxn + r. Computing the zn transition matrices in the rSLDS thus amounts to the cost of encoding the continuous state sequence x1:T , which can be distributed over different processors. In comparison, we are forced to compute sequentially the transition matrices statistics Wn, naively taking O(K3T ) operations in addition to the sequential encoding of x1:T . Fortunately, notice that the dynamics (Appendix eq. 13) are linear. In this case, as presented by Blelloch (1990), computing the transition matrices can be cast as a scan operation (see Smith et al. (2023) for a recent application of this concept). With this, we can efficiently parallelize our transition matrix dynamics and reduce the computation to O(K3(T/L+ logL)) operations over L processors, resulting in similar time complexity as the rSLDS model. We refer to Appendix C.2 for code and more details on our use of the parallel scan."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We train models by maximizing the Evidence Lower Bound (ELBO) using variational Laplace-EM from Zoltowski et al. (2020). We then compare model performance using the log marginal likelihood:\nlog p(y\u03031:T ) = \u222b p(y\u03031:T |x1:T , z1:T )p(x1:T , z1:T )dx1:T dz1:T ,\nwhich is the log-probability of held-out test data y\u03031:T under a given model, where test data arises from a 4:1 train-to-test split of the full dataset (see details in Appendix \u00a7B). The required integral is high-dimensional and intractable, and we thus resort to sequential Monte Carlo (SMC), also known as particle filtering, to compute it (Del Moral et al., 2006; Kantas et al., 2009)."
        },
        {
            "heading": "4.1 VALIDATION ON THE SYNTHETIC NASCAR TASK",
            "text": "We consider the synthetic NASCAR task as used by Linderman et al. (2016). In the original task, the true underlying model is a rSLDS with K = 4 states, dissecting the 2-D state space in four\nquadrants with rotational and bias dynamics (see Fig. 2A). The resulting continuous latent states xn resemble car tracks on a NASCAR driving track (see Fig. 2C-left for corresponding mean posterior estimates under variational Laplace-EM), and the data is obtained from high-dimensional (N = 10) Gaussian emissions of the xn dynamics. We compare the irSLDS against the rSLDS and SLDS models, presenting results over 5 random initialization seeds.\nFirst, we found both our model, the irSLDS, and the true generative model, the rSLDS, to be similarly able to generate accurate samples of the dynamics (see Fig. 2D). In terms of performance on held-out time steps of the trajectory (T = 200), both models attained similar accuracy (see Tab. 1). The irSLDS attained the higher mean on the true number of states K = 4, and both models performed similary for the overcomplete K = 8. The SLDS however did not perform as well, as to be expected\u2014this original NASCAR task was introduced to test the inclusion of recurrence in the rSLDS, compared to the standard SLDS. While the SLDS model could attain relatively high test log-likelihood (see Tab. 1), it struggled to accurately depict the true dynamics (see Fig. 2D-left). We refer to this accuracy in the generating process as dynamical accuracy, which we quantify by reporting the mean squared\nerror (MSE) between the learned dynamics and the true, available, dynamics2. Indeed, the SLDS model showed poor dynamical accuracy (see Tab. 1), while the irSLDS matched the accuracy of the true rSLDS. In all, this validates the incorporating of recurrence into our model, and furthermore provides evidence that the PDE prior can mimick the performance and generative capabilities of the fully-trained HMM prior.\nThe original NASCAR task has a periodic temporal structure (a property unchanged by permutation invariance) with a constant number of required states K\u2014both stationary properties. Before turning to our target experimental data, we first aim to assess the handling of non-stationary challenges. To this end, we introduce the extended NASCAR task. In the new task, the dynamics alternate between the original dynamics (Fig. 2A) and a set of new extended dynamics (Fig. 2B), where two additional bias dynamics are introduced. This effectively changes the true number of discrete states fromK = 4 \u2192 6 during this extended dynamics block. We consider B \u2208 {2, 3} blocks of alternating dynamics, each of same expected block lengths T/B. In this new task, we found the irSLDS to outperform the rSLDS, in turn outperform the SLDS (Tab. 1). Looking into the learned solutions, we observe (see Fig. 2F) that the model correctly identifies two new states after the switch. Interestingly, the posterior ends up attributing these two states to be in between previous states, such that the sequence of discrete states exhibits this traversing trajectory. This provides an example on how we can leverage the discrete state geometry of the irSLDS to gain a better understanding at the computation at play."
        },
        {
            "heading": "4.2 THE IRSLDS UNCOVERS TRIAL-VARYING STRUCTURE IN NEURAL DYNAMICS REGIMES",
            "text": "Next, we turn our attention to an electrophysiological dataset from the International Brain Laboratory (IBL), recorded in mice during a sensory-motor decision-making task. In this task, mice reported the location of a sinusoidal grating stimulus by turning a wheel either left or right, with task difficulty controlled by stimulus constrast (Fig. 3A) (Laboratory et al., 2021). We consider Neuropixels probe recordings from the \u201cBrainwide map\u201d data release (Laboratory, 2022). For our analyses, we projected spike train data onto the top principal components to obtain firing rates, making it amenable to analysis by state-space models with Gaussian emissions (Fig. 3B-C) (further methodological and data details can be found in Appendix \u00a7B, including firing-rate and continuous latent dimensions). As the irSLDS directly expands on the rSLDS, we focus on comparing these two models in this section. We picked K = 8 discrete states ( see Appendix Tab. 3 for test marginal LL values for K \u2208 {2, 4, 8}). First, we found that the models uncovered discrete latent states that switched at task-relevant event times. Indeed, we did not provide the models with task event times such as the stimulus onset (\u201cStim On\u201d) or reward, or behavioral measurements such as the movement onset time (\u201cFirst movement\u201d). Given that a task event occurred at time step t, we plot in Fig. 3D the estimated probability of a switch a time-step t+ l for a lag l (p(switcht+l|eventt)), for the irSLDS. We compare this against the baseline probability p(switcht), obtained by trial shuffling. The models learned task switches significantly different from chance, capturing a switch in discrete states either preceding or following a relevant task event. We thus posit that the statistical models capture relevant dynamics for the tasks.\nSecond, we found that the irSLDS uncovered differences in the discrete state distribution over trials. In Fig. 3E we plot the number of active states used in the most likely z\u03021:T trajectories under the variational posterior. We fit a spline function to this data for both models, determined from the minimal p-value of the F -test for a comparison against a constant function (computed over various degrees of smoothness and degree). While both models differed significantly from a constant function (p \u226a 0.001), the irSLDS uncovered larger fluctuations in the number-of-discrete-states-per-trial curve. Fluctuations were much smaller for the rSLDS model, and the spline provided a poorer fit (R2 of 0.16 v.s. 0.46 for irSLDS). This indicates that discrete state distribution over trials fluctuated more dramatically and systematically under the irSLDS model, with substantially more discrete states used in the middle of the session than at the beginning or end. The irSLDS attained a slightly higher log-likelihood on held-out test trials (see Tab. 3), so in all this indicates that those fluctuations are important, and might be missed by traditional HMM-based priors.\nTo further investigate the task-induced neural activity, we turn from trial-level modeling to sessionlevel modeling. Specifically, we focus on the average firing rate yt \u2208 RD, averaging from 200ms before the stimulus onset to the reward, for each trial n \u2208 [Ntrials]. Fig. 3F shows the most likely\n2 We minimize this error over reflections and rotations of the learned dynamics field, to deal with the non-identifiability of the latent dynamics\ndiscrete state sequences z\u03021:Ntrials under each model. Both models achieve similar training ELBO (see Appendix Tab. 3). We observe that the irSLDS showcases a distinctive slow geometric drift in the discrete states occupied, as observed from z\u0302 and further solidified by the trace of the interpolated z\u0303 (defined in eq. 9). Finally, we note how the interpolation also indicates the presence of uncertainty between states 3 and 4 at the end of the session. Such conclusions cannot be drawn in the (r)SLDS, as any permutation of the discrete states yields an equivalent visualization of the discrete state process."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we extend recurrent switching state-space models with an input-driven heat-equation prior over the dynamics of the transition matrix. This results in a semi-Markov discrete state process that capture the behavior of stochastic processes on partitions for a variable number of states per trajectory, as well as inducing a continuous geometry on the discrete states. We show that while this process is defined by less parameters than the traditional HMM models, it still matches or outperforms the original model while providing insights into time-varying processes in data. We showcase the model on a synthetic task, before turning to electrophysiological data from the IBL. The IBL hosts extensive datasets covering multiple repeated- to brain-wide recordings (Laboratory et al., 2022; Laboratory, 2022), and the modeling presented here provides grounds for further investigation into the time-varying processes underlying neural data."
        },
        {
            "heading": "A BACKGROUND",
            "text": ""
        },
        {
            "heading": "A.1 CHINESE RESTAURANT PROCESS",
            "text": "Dirichlet Processes (DP) mixture models provide a powerful random measure over clusterings. We refer to Ferguson (1973); Antoniak (1974) for a measure-theoretic treatment of DPs, and Teh et al. (2006) for a machine-learning overview. They can be alternatively defined through the Chinese Restaurant Process (CRP), a process akin to the P\u00f3lya Urn process. The analogy goes as follows : upon entering a restaurant, a customer i selects to sit at a table k with probability proportional to the number of people already sat at that table. With some fixed rate \u03b1, they may decide to start a new table. Put otherwise, for a new customer i, its table allocation zi follows\np(zi = k|z:i, \u03b1) \u221d { nk if k \u2264 K \u03b1 if k = K + 1\n(10)\nwith nk the size of cluster k \u2208 [K]. In this work, one should think of tables as clusters or dynamical modes, and the customers i as time-steps. It can be easily seen that this process induces a joint probability over cluster assignments that is invariant to the order of customers entering. We call this property exchangeability. This enforces a strong and limited prior on distributions of partitions that can arise from this model."
        },
        {
            "heading": "B TRAINING DETAILS",
            "text": ""
        },
        {
            "heading": "B.1 NASCAR TASK",
            "text": "Data generation We generate NASCAR track trajectories by instantiating the true model as described, and generating a sample trajectory running for T = 1000 steps. We then split the first 800 steps as train data, and keep the last 200 steps as test data (4:1 split). We repeat this procedure for 5 random seeds. For all models, we use K = 4, D = 2 (xt dimension) and M = 10 (observation yt dimension).\nB.2 IBL TASK\nPre-processing Given spike train data from a given period, we first perform the following preprocessing steps to obtain the firing rates over the period\n1. Kernel smoothing against a Gaussian kernel to obtain firing rate graphs. We use a standarddeviation of 100ms if the period is a whole trial, and of 30ms if the period is the Open-Loop period.\n2. The firing rate graphs are evaluated over an evenly space grid over the period interval. The bin size for the grid is determined following Algorithm 1 from Shimazaki & Shinomoto (2007), taking the minimal value, after removing outliers, over trials. We define outliers as values falling outside the [Q1\u2212 1.5 \u2217 IQR, Q3 + 1.5 \u2217 IQR] quartile interval. The chosen bin size is on the order of 40ms for whole-trial period, and 10ms for the Open-loop period.\nAfter the firing rates are obtained, the next step in processing depends on the nature of the data analyzed, and involves using PCA. If we consider trial-level modeling, then perform PCA to project the firing rates onto the top PCs (M = 50 PCs, over 90% variance explained). If we consider session-level modeling, we first average the firing rates from 200ms before to the stimulus onset to the reward, then perform PCA to project the per-trial activity onto the top PCs from the concatenated trials (M = 10 PCs, over 85% variance explained).\nFinally set our internal states xn to be of dimension D = 4 for per-trial neural activity, and D = 2 for per-trial-averaged activity.\nEphys data details We use Neuropixels recordings tied to the Brainwide data release Laboratory (2022). The results in this paper pertain to the first available session, with session id ae8787b1-4229-4d56-b0c2-566b61a25b77. We refer the reader to the data release website to obtain more information on the specific probe: https://viz.internationalbrainlab. org/app.\nData we use a 4:1 train to test split for the per-trial spike train analysis. 20% of the trials are randomly selected as test trials \u2013 these trials span the whole session."
        },
        {
            "heading": "C MODELING",
            "text": ""
        },
        {
            "heading": "C.1 P\u00d3LYA-GAMMA AUGMENTATION FOR GIBBS SAMPLING IN INFINITE RECURRENT SLDS",
            "text": "We provide more details on inference challenges in an infinite recurrent SLDS model naively combining the dist-CRP and the SLDS. Following previous methodologies, we could use Gibbs sampling and leverage message passing to perform inference. It would require us to handle the conditional density\np(x1:N |c1:N , z1:N , {y1:N , t1:N}) \u221d N\u220f\nn=1\n\u03c8(xn\u22121,xn, zn)\u03c8(xn\u22121, cn, t:n)\u03c8(xn,yn)\nwhere \u03c8(xn,xn+1, cn+1) is the potential from the continuous recurrent dynamics, and \u03c8(xn, yn) the evidence potentials. The recurrent connections introduce the dependencies captured in \u03c8(xn, cn+1), which adds significant challenges for inference. Without it, in the standard SLDS, the potentials are all Gaussian, allowing for analytical integration.\nFollowing Linderman et al. (2016), we can leverage P\u00f3lya-gamma augmentation (Polson et al., 2013; Linderman et al., 2015) to deal with these non-Gaussian factors. The key is that instead of performing a categorical choice over a pre-determined set of K dynamical modes, we perform an association, a categorical choice, with a previous time-step j \u2208 {1, . . . , t}. Because of the conceptual categorical similarities in updating, we find similarities in inference methodology with the rSLDS. This is where\nthe choice of decay function f comes in, in enforcing that link. This non-Gaussian factor is\n\u03c8(xn\u22121, cn, t:n) = p(cn|xn\u22121, t:n) \u221d n\u220f\nj=1\nf(tn \u2212 tj ;\u03b2(xn\u22121))I[cn=j]\u03b1I[cn=n]\n= \u03b1I[cn=n] n\u22121\u220f j=1 ( e[\u03bdn]j 1 + e[\u03bdn]j )I[cn=j] for \u03bdn \u2208 Rn\u22121, [\u03bdn]j := \u03b2(xn\u22121) \u00b7 (tn \u2212 tj). We can leverage the following integral quantity\n(e\u03bd)a\n(1 + e\u03bd)b = 2\u2212be\u03ba\u03bd \u222b \u221e 0 e\u2212\u03c9\u03bd 2/2pPG(\u03c9|b, 0)d\u03c9 b > 0, \u03ba = a\u2212 b 2\nto introduce auxiliary variables {\u03c9j}nj=1 such that the conditional density p(cn+1|xn, t:n+1, \u03c9n) becomes Gaussian \u03c8(xn, cn+1, t:n+1, \u03c9n) \u221d \u03b1 N (\u03bdn | \u2126\u22121n \u03ban,\u2126\u22121n ) (11) where \u2126n = diag(\u03c91:t\u22121), and [\u03ban]j = 12 I[cn = j], \u03ban \u2208 R\nn. With this augmentation, the required potentials are Gaussian and the integral can be calculated analytically. We refer the reader to Blei & Frazier (2011) for subsequent details on how one handles the messages mn\u2192n+1(cn+1).\nWhile we do obtain an analytical expression for the augmented potentials in (eq. 11), such potentials are multivariate Gaussians of size n for each time-step n \u2208 {1, . . . , N}. This adds significant computational complexity, both in handling the potentials and in any required marginalization.\nC.2 IMPLEMENTATION OF THE IRSLDS\nIn this appendix subsection, we present code modules and functions relevant to the modeling.\nParallel scan We cast the linear dynamics of the transition matrices Wn as a scan operation to efficiently parallelize the computation. The SSM package relies on Numba and autograd, the latter not having a readily implemented associate scan operation. To this end, we implemented in Numba and Numpy the scan operation, using numba for the parallization (Listing 1). Unfortunately, this scan operation cannot leverage the automatic differentiation from autograd. We found however that including the scan in the computation does not significantly change the learned solutions in the irSLDS parameters. Thus in practice, we use the scan to do faster hyperparameter search or gradient-free computation (e.g. generating samples, evaluating likelihoods), but rely on the full sequential version for the final training.\n1 @numba.jit() 2 def binary_operator(q_i, q_j): 3 r\"\"\" Binary operator for parallel scan of linear recurrence. 4 Args: 5 q_i: tuple containing U_i and b_i at position i (K,K,), (K,K,) 6 q_j: tuple containing U_j and b_j at position j (K,K,), (K,K,) 7 Returns: 8 new element ( U_out, bias_out ) 9 \"\"\"\n10 U_i, b_i = q_i 11 U_j, b_j = q_j 12 return np.matmul(U_i, U_j), np.matmul(b_i, U_j) + b_j 13 14 @numba.jit(parallel=True, forceobj=True) 15 def apply_scan(U, bias_elements): 16 r\"\"\" 17 Apply parallel scan for a length T linear recurrence of the form 18 19 x[0] = bias_elements[0] 20 x[t] = U x[t-1] + bias_elements[t] 21 22 Args: 23 U: Recurrent matrix, np.ndarray (K,K,) 24 bias_elements: Additive bias elements, np.ndarray (T+1,K,) 25 First bias element is the initialization for x. 26 Returns: 27 X: State dynamics, np.ndarray (T,K,) 28 \"\"\" 29 T = len(bias_elements) - 1 30 K = U.shape[0] 31 32 # Initalization 33 X = np.empty((T, K, K), dtype=np.float64) 34 q_i = ( np.eye(K), np.zeros_like(bias_elements[0]) )\n35 36 # Use numba prange for parallization 37 for i in numba.prange(T): 38 q_i = binary_operator(q_i, c_i) 39 X[i] = q_i[1] 40 return X[1:]\nListing 1: Numba implementation of associative scan\nTransition module We implement the irSLDS within the SSM package from Linderman et al. (2020). This can be accomplished by using the same SLDS parent class as the (r)SLDS models, and specifying a new discrete state process. We implement this discrete state process as a ssm.transitions.Transitions module.\n1 class HeatRecurrentTransition(Transitions): 2 \"\"\" 3 Use w_t as a probability dist to sample z_{t+1} from. 4 w_t follows the heat equation: w_t \\in L^1 satisfying 5 D_t w_t = \\gamma D_xx w_t 6 \"\"\" 7 def __init__(self, K, D, M=0, gamma=1.0, kappa=0.4, scan=False): 8 super().__init__(K, D, M) 9\n10 # Parameters linking past state to current state distribution 11 self.beta = beta 12 self.scan = scan 13 self._set_beta() 14 self._set_U() # FDM matrix for heat equation 15 self.kappa = kappa 16 17 # Parameters linking past observations to state distribution 18 self.Vs = npr.randn(K, M) # Inputs encoding, per state 19 self.Rs = npr.randn(K, D) # Previous observations encoding, per state 20 self.r = npr.randn(K) 21 22 def _set_beta(self): 23 delta_t = 1.0 # assume regular emissions. 24 delta_x = np.sqrt((8 * self.gamma * delta_t)) # determine the delta x such that FMD is stable 25 assert delta_t <= (delta_x ** 2)/(4 * self.gamma) # general assertion for stability 26 self.beta = (self.gamma * delta_t) / (delta_x ** 2) 27 28 def _set_U(self): 29 self.U = self.beta*np.diag(np.ones(self.K-1), k=1) +\\ 30 (1-2*self.beta)*np.diag(np.ones(self.K)) +\\ 31 self.beta*np.diag(np.ones(self.K-1), k=-1) 32 if self.mod: 33 self.U[0,-1] = self.beta 34 self.U[-1,0] = self.beta 35 36 def get_sufficient_statistics(self, data, input, mask, tag, w_state=None, SCAN=False): 37 T, _ = data.shape 38 39 # Initialization 40 ws = [] 41 if (w_state is None): # Init 42 wt = 1/self.K * np.ones((self.K, self.K)) 43 44 45 # Dynamics 46 if SCAN: 47 # Use einsum to parallelize over time the biases encoding 48 kappa_U = np.einsum(\u2019ij,jk->ik\u2019, self.U, self.kappa*np.eye(self.K)) 49 bias_elements = np.repeat(kappa_U[np.newaxis, :, :], T, axis=0) + \\ 50 np.repeat(np.einsum(\u2019tm,mk->tk\u2019, input, self.Vs.T)[:, np.newaxis, :], self.K, axis=1) + \\ 51 np.repeat(np.einsum(\u2019td,dk->tk\u2019, data, self.Rs.T)[:, np.newaxis, :], self.K, axis=1) 52 53 # Add initial condition to initial bias 54 bias_elements = np.concatenate([wt[np.newaxis, :, :], bias_elements], axis=0) 55 56 # Use the scan operation to parallelize the linear dynamics over time 57 ws = apply_scan(self.U, bias_elements) 58 return ws 59 else: 60 # Use sequential dynamics computation 61 for t in np.arange(T-1): 62 wt = np.dot(wt, self.U.T) # FDM 63 wt = wt + self.kappa*np.eye(self.K) # self reinforcement. 64 wt = wt + np.dot(input[t], self.Vs.T) + np.dot(data[t], self.Rs.T) # inputs 65 ws.append(wt) 66 ws = np.array(ws) 67 return ws 68 69 def log_transition_matrices(self, data, input, mask, tag, w_state=None): 70 ws = self.get_sufficient_statistics(data, input, mask, tag, w_state=w_state, SCAN=self.scan) 71 normalized_log_Ps = ws - logsumexp(ws, axis=2, keepdims=True) 72 if self.alpha>0.: 73 reweighted_log_Ps = (1-self.alpha)*normalized_log_Ps 74 out = reweighted_log_Ps - logsumexp(reweighted_log_Ps, axis=2, keepdims=True)\n75 else: 76 out = normalized_log_Ps 77 return out\nListing 2: Basic implementation of the transition matrix dynamics with heat-equation prior"
        },
        {
            "heading": "C.3 TRANSITION MATRIX DYNAMICS",
            "text": "We explicit here the dynamics of the transition matrix Wn \u2208 RK\u00d7K . The equation for the sufficient statistic wn dynamics in (8), which we rewrite here for reference,\nwn+1 | {wn,xn, zn = i} = Uwn + \u03ba1i +Rxn + r,\nrefers to row i of the transition matrix Wn+1, so that\n[Wn+1]i,j = [wn+1 | {xn, zn = i}]j . (12)\nWe note that all vectors are columns vectors.\nTo get the dynamics of the transition matrix itself, we must expand out the computation for each row. Specifically, given Wn, we obtain\nWn+1 =  ( U [Wn] \u22a4 1,: )\u22a4 ...(\nU [Wn] \u22a4 K,:\n)\u22a4 + \u03ba1 \u22a4 1\n... \u03ba1\u22a4K\n+ (Rxn + r) \u22a4\n... (Rxn + r) \u22a4\n\n=  [Wn]1,: U \u22a4\n... [Wn]K,: U \u22a4 + \u03baId + (Rxn + r)\u22a4 \u2297 1 =WnU \u22a4 + \u03baId + (Rxn + r) \u22a4 \u2297 1 (13)\nas the dynamics of the transition matrix. One can find these dynamics in our implementation of the transition module in Listing 2, lines 62\u201364. Unless otherwise noted, we use \u03ba = 0.4, and \u03b2 = 1.0 for defining U ."
        },
        {
            "heading": "D RESULTS",
            "text": ""
        },
        {
            "heading": "D.1 ACCURACY METRICS",
            "text": "All models were trained for 100 iterations, for every task. We only report the \u201cmean\u201d test loglikelihood, i.e. the log of the mean of the test likelihoods."
        },
        {
            "heading": "D.2 EMPIRICAL RUNNING TIMES",
            "text": ""
        },
        {
            "heading": "Extended NASCAR",
            "text": ""
        },
        {
            "heading": "NASCAR",
            "text": ""
        }
    ],
    "year": 2023
}