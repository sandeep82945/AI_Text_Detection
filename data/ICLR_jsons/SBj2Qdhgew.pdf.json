{
    "abstractText": "This work presents an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. Existing works often focus on either global fairness (overall disparity of the model across all clients) or local fairness (disparity of the model at each client), without always considering their trade-offs. There is a lack of understanding of the interplay between global and local fairness in FL, particularly under data heterogeneity, and if and when one implies the other. To address this gap, we leverage a body of work in information theory called partial information decomposition (PID), which first identifies three sources of unfairness in FL, namely, Unique Disparity, Redundant Disparity, and Masked Disparity. We demonstrate how these three disparities contribute to global and local fairness using canonical examples. This decomposition helps us derive fundamental limits on the trade-off between global and local fairness, highlighting where they agree or disagree. We introduce the Accuracy & Global-Local Fairness Optimality Problem (AGLFOP), a convex optimization that defines the theoretical limits of accuracy and fairness trade-offs, identifying the best possible performance any FL strategy can attain given a dataset and client distribution. We also present experimental results on synthetic datasets and the ADULT dataset to support our theoretical findings.",
    "authors": [],
    "id": "SP:83c6dd3e9ad28d093bfb966095c2d682f2d00425",
    "references": [
        {
            "authors": [
                "Annie Abay",
                "Yi Zhou",
                "Nathalie Baracaldo",
                "Shashank Rajamoni",
                "Ebube Chuba",
                "Heiko Ludwig"
            ],
            "title": "Mitigating bias in federated learning",
            "venue": "arXiv preprint arXiv:2012.02447,",
            "year": 2020
        },
        {
            "authors": [
                "Alekh Agarwal",
                "Alina Beygelzimer",
                "Miroslav Dud\u0131\u0301k",
                "John Langford",
                "Hanna Wallach"
            ],
            "title": "A reductions approach to fair classification",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Wael Alghamdi",
                "Hsiang Hsu",
                "Haewon Jeong",
                "Hao Wang",
                "Peter Michalak",
                "Shahab Asoodeh",
                "Flavio Calmon"
            ],
            "title": "Beyond adult and compas: Fair multi-class prediction via information projection",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Mohamed Ishmael Belghazi",
                "Aristide Baratin",
                "Sai Rajeswar",
                "Sherjil Ozair",
                "Yoshua Bengio",
                "Aaron Courville",
                "R Devon Hjelm"
            ],
            "title": "Mine: mutual information neural estimation",
            "venue": "arXiv preprint arXiv:1801.04062,",
            "year": 2018
        },
        {
            "authors": [
                "Flavio Calmon",
                "Dennis Wei",
                "Bhanukiran Vinzamuri",
                "Karthikeyan Natesan Ramamurthy",
                "Kush R Varshney"
            ],
            "title": "Optimized pre-processing for discrimination prevention",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Cl\u00e9ment L Canonne"
            ],
            "title": "A short note on an inequality between kl and tv",
            "venue": "arXiv preprint arXiv:2202.07198,",
            "year": 2022
        },
        {
            "authors": [
                "Irene Chen",
                "Fredrik D Johansson",
                "David Sontag"
            ],
            "title": "Why is my classifier discriminatory",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Jaewoong Cho",
                "Gyeongjo Hwang",
                "Changho Suh"
            ],
            "title": "A fair classifier using mutual information",
            "venue": "IEEE international symposium on information theory (ISIT),",
            "year": 2020
        },
        {
            "authors": [
                "Lingyang Chu",
                "Lanjun Wang",
                "Yanjie Dong",
                "Jian Pei",
                "Zirui Zhou",
                "Yong Zhang"
            ],
            "title": "Fedfair: Training fair models in cross-silo federated learning",
            "venue": "arXiv preprint arXiv:2109.05662,",
            "year": 2021
        },
        {
            "authors": [
                "Thomas M Cover"
            ],
            "title": "Elements of information theory",
            "year": 1999
        },
        {
            "authors": [
                "Sen Cui",
                "Weishen Pan",
                "Jian Liang",
                "Changshui Zhang",
                "Fei Wang"
            ],
            "title": "Addressing algorithmic disparity and performance inconsistency in federated learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Wei Du",
                "Depeng Xu",
                "Xintao Wu",
                "Hanghang Tong"
            ],
            "title": "Fairness-aware agnostic federated learning",
            "venue": "In Proceedings of the 2021 SIAM International Conference on Data Mining (SDM),",
            "year": 2021
        },
        {
            "authors": [
                "Dheeru Dua",
                "Casey Graff"
            ],
            "title": "UCI machine learning repository, 2017",
            "venue": "URL http://archive. ics.uci.edu/ml",
            "year": 2017
        },
        {
            "authors": [
                "S. Dutta",
                "D. Wei",
                "H. Yueksel",
                "P.Y. Chen",
                "S. Liu",
                "K.R. Varshney"
            ],
            "title": "Is there a trade-off between fairness and accuracy? a perspective using mismatched hypothesis testing",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Sanghamitra Dutta",
                "Praveen Venkatesh",
                "Piotr Mardziel",
                "Anupam Datta",
                "Pulkit Grover"
            ],
            "title": "Fairness under feature exemptions: Counterfactual and observational measures",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Cynthia Dwork",
                "Moritz Hardt",
                "Toniann Pitassi",
                "Omer Reingold",
                "Richard Zemel"
            ],
            "title": "Fairness through awareness",
            "venue": "In Proceedings of the 3rd innovations in theoretical computer science conference,",
            "year": 2012
        },
        {
            "authors": [
                "David A Ehrlich",
                "Andreas C Schneider",
                "Michael Wibral",
                "Viola Priesemann",
                "Abdullah Makkeh"
            ],
            "title": "Partial information decomposition reveals the structure of neural representations",
            "venue": "arXiv preprint arXiv:2209.10438,",
            "year": 2022
        },
        {
            "authors": [
                "Yahya H Ezzeldin",
                "Shen Yan",
                "Chaoyang He",
                "Emilio Ferrara",
                "A Salman Avestimehr"
            ],
            "title": "Fairfed: Enabling group fairness in federated learning",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Sainyam Galhotra",
                "Karthikeyan Shanmugam",
                "Prasanna Sattigeri",
                "Kush R Varshney"
            ],
            "title": "Causal feature selection for algorithmic fairness",
            "venue": "In Proceedings of the 2022 International Conference on Management of Data,",
            "year": 2022
        },
        {
            "authors": [
                "AmirEmad Ghassami",
                "Sajad Khodadadian",
                "Negar Kiyavash"
            ],
            "title": "Fairness in supervised learning: An information theoretic approach",
            "venue": "IEEE international symposium on information theory (ISIT),",
            "year": 2018
        },
        {
            "authors": [
                "Ziv Goldfeld",
                "Yury Polyanskiy"
            ],
            "title": "The information bottleneck problem and its applications in machine learning",
            "venue": "IEEE Journal on Selected Areas in Information Theory,",
            "year": 2020
        },
        {
            "authors": [
                "Vincent Grari",
                "Boris Ruf",
                "Sylvain Lamprier",
                "Marcin Detyniecki"
            ],
            "title": "Fairness-aware neural reyni minimization for continuous features",
            "venue": "arXiv preprint arXiv:1911.04929,",
            "year": 2019
        },
        {
            "authors": [
                "Moritz Hardt",
                "Eric Price",
                "Nati Srebro"
            ],
            "title": "Equality of opportunity in supervised learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Shengyuan Hu",
                "Zhiwei Steven Wu",
                "Virginia Smith"
            ],
            "title": "Provably fair federated learning via bounded group loss",
            "venue": "arXiv preprint arXiv:2203.10190,",
            "year": 2022
        },
        {
            "authors": [
                "R.G. James",
                "C.J. Ellison",
                "J.P. Crutchfield"
            ],
            "title": "dit: a Python package for discrete information theory",
            "venue": "The Journal of Open Source Software,",
            "year": 2018
        },
        {
            "authors": [
                "Peter Kairouz",
                "Jiachun Liao",
                "Chong Huang",
                "Maunil Vyas",
                "Monica Welfert",
                "Lalitha Sankar"
            ],
            "title": "Generating fair universal representations using adversarial models",
            "year": 1910
        },
        {
            "authors": [
                "Peter Kairouz",
                "H Brendan McMahan",
                "Brendan Avent",
                "Aur\u00e9lien Bellet",
                "Mehdi Bennis",
                "Arjun Nitin Bhagoji",
                "Kallista Bonawitz",
                "Zachary Charles",
                "Graham Cormode",
                "Rachel Cummings"
            ],
            "title": "Advances and open problems in federated learning",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Toshihiro Kamishima",
                "Shotaro Akaho",
                "Jun Sakuma"
            ],
            "title": "Fairness-aware learning through regularization approach",
            "venue": "IEEE 11th International Conference on Data Mining Workshops,",
            "year": 2011
        },
        {
            "authors": [
                "Toshihiro Kamishima",
                "Shotaro Akaho",
                "Hideki Asoh",
                "Jun Sakuma"
            ],
            "title": "Fairness-aware classifier with prejudice remover regularizer",
            "venue": "ECML PKDD",
            "year": 2012
        },
        {
            "authors": [
                "Jian Kang",
                "Tiankai Xie",
                "Xintao Wu",
                "Ross Maciejewski",
                "Hanghang Tong"
            ],
            "title": "Multifair: Multi-group fairness in machine learning",
            "venue": "arXiv preprint arXiv:2105.11069,",
            "year": 2021
        },
        {
            "authors": [
                "Joon Sik Kim",
                "Jiahao Chen",
                "Ameet Talwalkar"
            ],
            "title": "Fact: A diagnostic for group fairness trade-offs",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Michael Kleinman",
                "Alessandro Achille",
                "Stefano Soatto",
                "Jonathan C Kao"
            ],
            "title": "Redundant information neural estimation",
            "year": 2021
        },
        {
            "authors": [
                "Artemy Kolchinsky"
            ],
            "title": "A novel approach to the partial information",
            "venue": "decomposition. Entropy,",
            "year": 2022
        },
        {
            "authors": [
                "Tian Li",
                "Maziar Sanjabi",
                "Ahmad Beirami",
                "Virginia Smith"
            ],
            "title": "Fair resource allocation in federated learning",
            "venue": "arXiv preprint arXiv:1905.10497,",
            "year": 2019
        },
        {
            "authors": [
                "Paul Pu Liang",
                "Yun Cheng",
                "Xiang Fan",
                "Chun Kai Ling",
                "Suzanne Nie",
                "Richard Chen",
                "Zihao Deng",
                "Faisal Mahmood",
                "Ruslan Salakhutdinov",
                "Louis-Philippe Morency"
            ],
            "title": "Quantifying & modeling feature interactions: An information decomposition framework",
            "venue": "arXiv preprint arXiv:2302.12247,",
            "year": 2023
        },
        {
            "authors": [
                "Suyun Liu",
                "Luis Nunes Vicente"
            ],
            "title": "Accuracy and fairness trade-offs in machine learning: A stochastic multi-objective approach",
            "venue": "Computational Management Science,",
            "year": 2022
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
            "venue": "Proceedings of the 20th International Conference on Artificial Intelligence and Statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Ninareh Mehrabi",
                "Fred Morstatter",
                "Nripsuta Saxena",
                "Kristina Lerman",
                "Aram Galstyan"
            ],
            "title": "A survey on bias and fairness in machine learning",
            "venue": "ACM Comput. Surv.,",
            "year": 2021
        },
        {
            "authors": [
                "Salman Mohamadi",
                "Gianfranco Doretto",
                "Donald A Adjeroh"
            ],
            "title": "More synergy, less redundancy: Exploiting joint mutual information for self-supervised learning",
            "venue": "arXiv preprint arXiv:2307.00651,",
            "year": 2023
        },
        {
            "authors": [
                "Ari Pakman",
                "Amin Nejatbakhsh",
                "Dar Gilboa",
                "Abdullah Makkeh",
                "Luca Mazzucato",
                "Michael Wibral",
                "Elad Schneidman"
            ],
            "title": "Estimating the unique information of continuous variables",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Afroditi Papadaki",
                "Natalia Martinez",
                "Martin Bertran",
                "Guillermo Sapiro",
                "Miguel Rodrigues"
            ],
            "title": "Minimax demographic group fairness in federated learning",
            "venue": "arXiv preprint arXiv:2201.08304,",
            "year": 2022
        },
        {
            "authors": [
                "Borja Rodr\u0131\u0301guez-G\u00e1lvez",
                "Filip Granqvist",
                "Rogier van Dalen",
                "Matt Seigel"
            ],
            "title": "Enforcing fairness in private federated learning via the modified method of differential multipliers",
            "venue": "arXiv preprint arXiv:2109.08604,",
            "year": 2021
        },
        {
            "authors": [
                "Mukund Prasad Sah",
                "Amritpal Singh"
            ],
            "title": "Aggregation techniques in federated learning: Comprehensive survey, challenges and opportunities",
            "venue": "2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),",
            "year": 2022
        },
        {
            "authors": [
                "Yuxin Shi",
                "Han Yu",
                "Cyril Leung"
            ],
            "title": "A survey of fairness-aware federated learning",
            "venue": "arXiv preprint arXiv:2111.01872,",
            "year": 2021
        },
        {
            "authors": [
                "Megan Smith",
                "Cecilia Munoz",
                "D.J. Patil"
            ],
            "title": "Big Risks, Big Opportunities: the Intersection of Big Data and Civil Rights, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Tycho MS Tax",
                "Pedro AM Mediano",
                "Murray Shanahan"
            ],
            "title": "The partial information decomposition of generative neural network",
            "venue": "models. Entropy,",
            "year": 2017
        },
        {
            "authors": [
                "Praveen Venkatesh",
                "Gabriel Schamberg"
            ],
            "title": "Partial information decomposition via deficiency for multivariate gaussians",
            "venue": "IEEE International Symposium on Information Theory (ISIT),",
            "year": 2022
        },
        {
            "authors": [
                "Praveen Venkatesh",
                "Sanghamitra Dutta",
                "Neil Mehta",
                "Pulkit Grover"
            ],
            "title": "Can information flows suggest targets for interventions in neural circuits",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hao Wang",
                "Hsiang Hsu",
                "Mario Diaz",
                "Flavio P. Calmon"
            ],
            "title": "The impact of split classifiers on group fairness",
            "venue": "IEEE International Symposium on Information Theory (ISIT),",
            "year": 2021
        },
        {
            "authors": [
                "Lin Wang",
                "Zhichao Wang",
                "Xiaoying Tang"
            ],
            "title": "Fedeba+: Towards fair and effective federated learning via entropy-based model",
            "venue": "arXiv preprint arXiv:2301.12407,",
            "year": 2023
        },
        {
            "authors": [
                "Patricia Wollstadt",
                "Sebastian Schmitt",
                "Michael Wibral"
            ],
            "title": "A rigorous information-theoretic definition of redundancy and relevancy in feature selection based on (partial) information decomposition",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2023
        },
        {
            "authors": [
                "Qiang Yang"
            ],
            "title": "Federated learning",
            "year": 2020
        },
        {
            "authors": [
                "Yuchen Zeng",
                "Hongxu Chen",
                "Kangwook Lee"
            ],
            "title": "Improving fairness via federated learning",
            "venue": "arXiv preprint arXiv:2110.15545,",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Yue Zhang",
                "Ziyi Kou",
                "Dong Wang"
            ],
            "title": "Fairfl: A fair federated learning approach to reducing demographic bias in privacy-sensitive classification models",
            "venue": "IEEE International Conference on Big Data (Big Data),",
            "year": 2020
        },
        {
            "authors": [
                "Han Zhao",
                "Geoffrey J Gordon"
            ],
            "title": "Inherent tradeoffs in learning fair representations",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Kamishima"
            ],
            "title": "Mutual Information is a measure of dependence between two random variables Z and \u0176 (captures correlation as well as all non-linear dependencies). Mutual information is zero if and only if Z and \u0176 are independent. This means that if the model\u2019s predictions are highly correlated with sensitive attributes (like gender or race), that\u2019s a sign of unfairness",
            "year": 2011
        },
        {
            "authors": [
                "Cho"
            ],
            "title": "This means that if the model\u2019s predictions are highly correlated with sensitive attributes (like gender or race), that\u2019s a sign of unfairness. Mutual information has been explored in fairness in the context of centralized machine learning in Kamishima et al",
            "year": 2021
        },
        {
            "authors": [
                "Venkatesh"
            ],
            "title": "2021) provides another interpretation of mutual information",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Federated learning (FL) is a framework where several parties (clients) collectively train machine learning models while retaining the confidentiality of their local data (Yang, 2020; Kairouz et al., 2021). With the growing use of FL in various high-stakes applications, such as finance, healthcare, recommendation systems, etc., it is crucial to ensure that these models do not discriminate against any demographic group based on sensitive features such as race, gender, age, nationality, etc. (Smith et al., 2016). While there are several methods to achieve group fairness in the centralized settings (Mehrabi et al., 2021), these methods do not directly apply to a FL setting since each client only has access to their local dataset, and hence, is restricted to only performing local disparity mitigation.\nSome recent works (Du et al., 2021; Abay et al., 2020; Ezzeldin et al., 2023) focus on developing models that are fair when evaluated on the entire dataset across all clients, a concept known as global fairness. For example, several banks may decide to engage in a FL process to train a model that will determine loan qualifications without exchanging data among them. A globally fair model does not discriminate against any protected group when evaluated on the entire dataset across all the banks. On the other hand, local fairness considers the disparity of the model at each client, i.e., when evaluated on a client\u2019s local dataset. Local fairness is important, as the models are ultimately deployed and used at the local client (Cui et al., 2021).\nOne might notice that global and local fairness can differ, particularly when the local demographics at a client differ from the global demographics across the entire dataset (data heterogeneity, e.g., a bank with customers predominantly from one race). Previous research has mostly focused on either global fairness or local fairness, without always considering their interplay. Global and local fairness are the same when the data is i.i.d. across clients (Ezzeldin et al., 2023; Cui et al., 2021), but their interplay in other situations is not well-understood.\nThis work aims to provide a fundamental understanding of group fairness trade-offs in the FL setting. We first formalize the notions of global and local disparity in FL using information theory. Next, we leverage a body of work within information theory called partial information decomposition (PID) to further identify three sources of disparity in FL that contribute to the global and local disparity, namely, Unique Disparity, Redundant Disparity, and Masked Disparity. This information-theoretic decomposition is significant because it helps us derive fundamental limits on the trade-offs between global and local disparity, particularly under data heterogeneity, and provides insights on when they agree and disagree. We introduce the Accuracy-Global and Local Fairness Optimality Problem (AGFOP), a novel convex optimization that rigorously explores the trade-offs between accuracy and both global and local fairness. This framework establishes the theoretical limits of what any FL technique can achieve in terms of accuracy and fairness given a dataset and client distribution. This work provides a more nuanced understanding of the interplay between these two fairness notions that can better inform disparity mitigation techniques and their convergence and effectiveness in practice.\nOur main contributions can be summarized as follows:\n\u2022 Partial information decomposition of global & local disparity into three sources of unfairness: We first define global disparity as the mutual information I(Z; Y\u0302 ) where Y\u0302 is a model\u2019s prediction and Z is the sensitive attribute (Definition 2). Then, we show that local disparity can in fact be represented as the conditional mutual information I(Z; Y\u0302 |S) where S denotes the client (Definition 3). We also demonstrate relationships between these information-theoretic quantifications and well-known fairness metrics such as statistical parity (see Lemma 1). Using an informationtheoretic quantification then enables us to further decompose the global and local disparity into three non-negative components: Unique Disparity, Redundant Disparity, and Masked Disparity. We provide canonical examples to help understand these disparities in the context of FL (see Section 3.1). The significance of our information-theoretic decomposition lies in separating out the regions of agreement and disagreement of local and global disparity, demystifying their trade-offs.\n\u2022 Fundamental limits on trade-offs between local and global fairness: We show the limitations of achieving global fairness using local fairness due to Redundant Disparity (see Theorem 1) and the limitations of achieving local fairness even if global fairness is achieved due to Masked Disparity (see Theorem 2). We also identify the necessary and sufficient conditions under which one form of fairness (local or global) implies the other (see Theorem 3 and Theorem 4), as well as, discuss other conditions that are sufficient but not necessary.\n\u2022 A convex optimization framework for quantifying accuracy-fairness trade-offs in FL: We present the Accuracy and Global-Local Fairness Optimality Problem (AGLFOP) (see Definition 4), a novel convex optimization framework for systematically exploring the trade-offs between accuracy and both global and local fairness metrics. AGLFOP evaluates all potential joint distributions, thereby setting the theoretical boundaries for the best possible performance achievable for a given dataset and client distribution in FL.\n\u2022 Experimental demonstration: We validate our theoretical findings using synthetic and Adult datasets. We explore trade-offs for accuracy with global and local fairness by examining the Pareto frontiers of the AGLFOP. We also investigate the PID of disparities in the Adult dataset trained within a FL setting with multiple clients under various data heterogeneity scenarios.\nRelated Works: There are various perspectives to fairness in FL (Shi et al., 2021). One definition is client-fairness (Li et al., 2019), which aims to achieve equal performance across all client devices (Wang et al., 2023). In this work, we are instead interested in group fairness, i.e., fairness with respect to demographic groups based on gender, race, etc. Methods for achieving group fairness in a centralized setting (Agarwal et al., 2018; Hardt et al., 2016; Dwork et al., 2012; Kamishima et al., 2011; Pessach & Shmueli, 2022) may not directly apply in a FL setting since each client only has access to their local dataset. Existing works on group fairness in FL generally aim to develop models that achieve global fairness, without much consideration for the local fairness at each client (Ezzeldin et al., 2023). For instance, one approach to achieve global fairness in FL poses a constrained optimization problem to find the best model locally, while also ensuring that disparity at a client does not exceed a threshold and then aggregates those models (Chu et al., 2021; Rodr\u0131\u0301guez-Ga\u0301lvez et al., 2021; Zhang et al., 2020). Other techniques involve bi-level optimization that aims to find the optimal global model (minimum loss) under the worst-case fairness violation (Papadaki et al., 2022; Hu et al., 2022; Zeng et al., 2021), or re-weighting mechanisms (Abay et al., 2020; Du et al., 2021), both of which often require sharing additional parameters with a server. Cui et al. (2021) argues\nfor local fairness, as the model will be deployed at the local client level, and propose constrained multi-objective optimization. While accuracy-fairness tradeoffs have been examined in centralized settings (Chen et al., 2018; Dutta et al., 2020; Kim et al., 2020; Zhao & Gordon, 2022; Liu & Vicente, 2022), such considerations, along with the relationship between local and global fairness, remain largely unexplored in FL. Our work addresses this gap by examining them through the lens of PID.\nInformation-theoretic measures have been used to express and handle group fairness in the centralized setting in Kamishima et al. (2012); Calmon et al. (2017); Ghassami et al. (2018); Dutta et al. (2021); Cho et al. (2020); Baharlouei et al. (2019); Grari et al. (2019); Wang et al. (2021); Galhotra et al. (2022); Alghamdi et al. (2022); Kairouz et al. (2019); Baharlouei et al. (2019); Grari et al. (2019). PID is also generating interest in other problems in ML (Ehrlich et al., 2022; Tax et al., 2017; Liang et al., 2023; Wollstadt et al., 2023; Mohamadi et al., 2023; Pakman et al., 2021). In this work, instead of trying to minimize information-theoretic measures as a regularizer, our goal is to quantify the fundamental trade-offs between local and global fairness in FL and develop insights on their interplay to better understand what is information-theoretically possible using any optimization technique."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "Here, we introduce the FL setup and outline its key components. Let K be the total number of federating clients. A client is represented as S \u2208 {0, 1, . . . ,K\u22121}. A client S = s has a dataset Ds = {(xi, yi, zi)}i=1,...ns where xi denotes the input features, yi \u2208 {0, 1} is the true label, and zi \u2208 {0, 1} is the sensitive attribute (assume binary) with 1 indicating the privileged group and 0 indicating the unprivileged group. The term ns denotes the number of datapoints at client S = s. The collective dataset is given by D = \u222aK\u22121s=0 Ds. When denoting a random variable drawn from this dataset, we let X denote the input features, Z denote the sensitive attribute, and Y denote the true label. We also let Y\u0302 represent the predictions of a model f\u03b8(X) which is parameterized by \u03b8.\nStandard FL aims to minimize the empirical risk: min\u03b8 L(\u03b8) = min\u03b8 1K \u2211K\u22121\ns=0 \u03b1sLs(\u03b8), where Ls(\u03b8) =\n1 ns \u2211 (x,y)\u2208Ds l(f\u03b8(x), y) is the local objective (or loss) at client s, \u03b1s is an importance\ncoefficient (often equal across clients), and l(\u00b7, \u00b7) denotes a predefined loss function. In order to minimize the objective L(\u03b8), a decentralized approach is employed. Each client S trains on their own private dataset Ds and provides their trained local model to a centralized server. The server aggregates the parameters of the local models to create a global model f\u03b8(x) (Sah & Singh, 2022). E.g., the FedAvg algorithm (McMahan et al., 2017) is a popular approach that aggregates the parameters of local models by taking their average, which is then used to update the global model. This process is repeated until the global model achieves a satisfactory performance level.\nBackground on Partial Information Decomposition (PID): PID decomposes the mutual information I(Z;A,B) about a random variable Z contained in the tuple (A,B) into four non-negative terms:\nI(Z;A,B) = Uni(Z:A|B) + Uni(Z:B|A) + Red(Z:A,B) + Syn(Z:A,B) (1) Here, Uni(Z:A|B) denotes the unique information about Z that is present only in A and not in B. E.g., shopping preferences (A) may provide unique information about gender (Z) that is not present in address (B). Red(Z:A,B) denotes the redundant information about Z that is present in both A and B. E.g., zipcode (A) and county (B) may provide redundant information about race.\nSyn(Z:A,B) denotes the synergistic information not present in either A or B individually, but present jointly in (A,B), e.g., each individual digit of the zipcode may not have information about race but together they provide significant information about race.\nNumerical Example. Let Z=(Z1, Z2, Z3) with each Zi\u223c i.i.d. Bern(1/2). Let A = (Z1, Z2, Z3 \u2295 N), B = (Z2, N), N \u223c Bern(1/2) is independent of Z. Here, I(Z;A,B) = 3 bits. The unique information about Z that is contained only in A and not in B is effectively in Z1, and is given by Uni(Z:A|B) = I(Z;Z1) = 1 bit. The redundant information about Z that is contained in both A and B is effectively in Z2 and is given by\nRed(Z:A,B) = I(Z;Z2) = 1 bit. Lastly, the synergistic information about Z that is not contained in either A or B alone, but is contained in both of them together is effectively in the tuple (Z3\u2295N,N), and is given by Syn(Z:A,B)=I(Z; (Z3 \u2295N,N)) = 1 bit. This accounts for the 3 bits in I(Z;A,B). Here, we include a popular definition of Uni(Z:A|B) from Bertschinger et al. (2014).\nDefinition 1 (Unique Information). Let \u2206 be the set of all joint distributions on (Z,A,B) and \u2206p be the set of joint distributions with the same marginals on (Z,A) and (Z,B) as the true distribution, i.e., \u2206p = {Q\u2208\u2206 : PrQ(Z=z,A=a)=Pr(Z=z,A=a) and PrQ(Z=z,B=b) = Pr(Z=z,B=b)}. Then, Uni(Z:A|B) = minQ\u2208\u2206p IQ(Z;A|B), where IQ(Z;A|B) is the conditional mutual information when (Z,A,B) have joint distribution Q and PrQ(\u00b7) denotes the probability under Q.\nDefining any one of the PID terms suffices to get the others. Red(Z:A,B) is the sub-volume between I(Z;A) and I(Z;B) (see Fig. 1). Hence, Red(Z:A,B) = I(Z;A) \u2212 Uni(Z:A|B) and Syn(Z:A,B) = I(Z;A,B)\u2212Uni(Z:A|B)\u2212Uni(Z:B|A)\u2212 Red(Z:A,B) (from equation 1)."
        },
        {
            "heading": "3 MAIN RESULTS",
            "text": "We formalize the notions of global and local disparity in FL using information theory. Definition 2 (Global Disparity). The global disparity of a model f\u03b8 with respect to Z is defined as I(Z; Y\u0302 ), the mutual information between Z and Y\u0302 (where Y\u0302 = f\u03b8(X)).\nThis is related to a widely-used group fairness notion called statistical parity. Existing works (Agarwal et al., 2018) define the global statistical parity as: Pr(Y\u0302 = 1|Z = 1) = Pr(Y\u0302 = 1|Z = 0). Global statistical parity is satisfied when Z is independent of Y\u0302 , which is equivalent to zero mutual information I(Z; Y\u0302 ) = 0. To further justify our choice of I(Z; Y\u0302 ) as a measure of global disparity, we provide a relationship between the absolute statistical parity gap and mutual information when they are non-zero in Lemma 1 (Proof in Appendix B).\nLemma 1 (Relationship between Global Statistical Parity Gap and I(Z; Y\u0302 )). Let Pr(Z=0) = \u03b1. The gap SPglobal = |Pr(Y\u0302 = 1|Z = 1)\u2212 Pr(Y\u0302 = 1|Z = 0)| is bounded by \u221a 0.5 I(Z;Y\u0302 ) 2\u03b1(1\u2212\u03b1) .\nSimilarly, Ezzeldin et al. (2023) defines local statistical parity at a client s as: Pr(Y\u0302=1|Z=1, S=s) = Pr(Y\u0302=1|Z=0, S=s). A critical observation that we make in this work is that: local unfairness can be quantified as I(Z; Y\u0302 |S), the mutual information between Z and Y\u0302 conditioned on S. Definition 3 (Local Disparity). The local disparity is the conditional mutual information I(Z; Y\u0302 |S).\nLemma 2. I(Z; Y\u0302 |S)=0 if and only if Pr(Y\u0302=1|Z=1, S=s)=Pr(Y\u0302=1|Z=0, S=s) at all clients s. The proof (see Appendix B) uses the fact that I(Z; Y\u0302 |S) = \u2211K\ns=1 Pr(S=s)I(Z; Y\u0302 |S = s) where I(Z; Y\u0302 |S = s) is the local disparity at client s, and Pr(S=s) = ns/n, the proportion of data points at client s. Similar to Lemma 1, we can also get a relationship between SPs and I(Z; Y\u0302 |S = s) when they are non-zero as expressed in Corollary 1 in Appendix B. We can also define other fairness metrics similarly. For instance, global equalized odds can be formulated in terms of the conditional mutual information, denoted as I(Z; Y\u0302 |Y ) and local equalized odds as I(Z; Y\u0302 |Y, S)."
        },
        {
            "heading": "3.1 PARTIAL INFORMATION DECOMPOSITION OF GLOBAL AND LOCAL DISPARITY",
            "text": "Here, we introduce a PID approach to understanding fairness in FL. We decompose global and local disparity into three sources of unfairness: Unique, Redundant, and Masked Disparity, and provide examples to illustrate and better understand these disparities in the context of FL. Proposition 1. The global and local disparity in FL can be decomposed into non-negative terms as:\nI(Z; Y\u0302 ) = Uni(Z:Y\u0302 |S) + Red(Z:Y\u0302 , S). (2) I(Z; Y\u0302 |S) = Uni(Z:Y\u0302 |S) + Syn(Z:Y\u0302 , S). (3)\nWe refer to Fig. 2 for an illustration of this result. In the proof, equation 2 follows from the relationship between different PID terms while equation 3 requires the chain rule of mutual information (Cover, 1999). For completeness, we show the non-negativity of PID terms in Appendix C. The term Uni(Z:Y\u0302 |S) quantifies the unique information the sensitive attribute Z provides about the model prediction Y\u0302 that is not provided by client label S. We refer to this as\nthe Unique Disparity. The Unique Disparity contributes to both local and global disparity, highlighting the region where they agree. The term Red(Z:Y\u0302 , S) quantifies the information about sensitive attribute Z that is common between prediction Y\u0302 and client S. We call this the Redundant Disparity. The Unique and Redundant Disparities together make up the global disparity I(Z; Y\u0302 ). The term Syn(Z:Y\u0302 , S) represents the synergistic information about sensitive\nattribute Z that is not present in either Y\u0302 or S individually, but is present jointly in (Y\u0302 , S). We refer to this as the Masked Disparity, as it is only observed when Y\u0302 and S are considered together. Redundant and Masked Disparities cause disagreement between global and local fairness.\nCanonical Examples. For a deeper understanding, we now examine a loan approval scenario featuring binary-sensitive attributes across two clients, i.e., Y\u0302 , Z, S\u2208{0, 1}. Here, I(Z; Y\u0302 , S)=H(Z)\u2212H(Z|Y\u0302 , S)\u2264H(Z)=1, i.e., the maximum disparity is 1 bit for this case.\nExample 1 (Pure Uniqueness). Let Y\u0302 = Z and Z \u22a5\u22a5 S. The\nmen (Z = 1) and women (Z = 0) are identically distributed across the clients. Suppose, the model only approves men but rejects women for a loan across both clients. This model is both locally and globally unfair, I(Z; Y\u0302 ) = I(Z; Y\u0302 |S) = 1. This is a case of purely Unique Disparity since all the information about gender is derived exclusively from the model predictions; the client S has no correlation with gender Z. Both global and local disparities are in agreement. Here, Uni(Z:Y\u0302 |S) = 1, Red(Z:Y\u0302 , S) = 0, and Syn(Z:Y\u0302 , S) = 0. Example 2 (Pure Redundancy). Let Y\u0302 = S and Z = S with probability 0.9. The client S = 0 predominantly caters to women (90%), and the client S = 1 has 90%-men. So, there is a correlation between the clients and gender. Suppose, the model approves everyone from client S = 1 while rejecting everyone in S = 0. Such a model is locally fair because men and women are treated equally within a particular client, and I(Z; Y\u0302 |S) = 0. However, the model is globally unfair (favoring men with I(Z; Y\u0302 ) = 0.53). This is a case of purely Redundant Disparity since information about Z is derived from both Y\u0302 and S. Global and local disparities are in disagreement. Here, Uni(Z:Y\u0302 |S) = 0, Red(Z:Y\u0302 , S) = 0.53, and Syn(Z:Y\u0302 , S) = 0.\nIn general, pure Redundant Disparity is observed when Z \u2212 S \u2212 Y\u0302 form a Markov chain, even if Z and Y\u0302 are correlated, i.e., Y\u0302 = S and S = g(Z) for some function g.\nExample 3 (Pure Synergy). Let Y\u0302 = Z \u2295 S and Z \u22a5\u22a5 S. The model approves men (Z = 1) from client S = 0 and women (Z = 0) from client S = 1, while others are rejected. Such a model is locally unfair, as it singularly prefers one gender within each client. However, it is globally fair since it maintains an equal approval rate for both men and women with I(Z; Y\u0302 |S) = 1 and I(Z; Y\u0302 |S) = 0. This is a case of purely Masked Disparity as information about Z that is not observable in either Y\u0302 or S individually is present jointly. Here, Uni(Z:Y\u0302 |S) = 0, Red(Z:Y\u0302 , S) = 0, and Syn(Z:Y\u0302 , S) = 1. Remark 1 (Merits of PID). These canonical examples demonstrate corner cases with pure uniqueness, redundancy, and synergy. In practice, there is usually a mixture of all of these cases. i.e., non-zero Unique, Redundant, and Masked Disparities. In these scenarios, PID serves as a powerful tool that can disentangle the regions of agreement and disagreement between local and global disparity, particularly when data is distributed non-identically across clients (also see experiments in Section 4). In contrast, traditional fairness metrics lack the granularity to capture these nuanced interactions, making PID an essential asset for a more comprehensive understanding and mitigation of disparities. Using PID, we are able to uncover the fundamental information-theoretic limits and trade-offs between global and local disparities, which we will explore in greater depth next."
        },
        {
            "heading": "3.2 FUNDAMENTAL LIMITS ON TRADEOFFS BETWEEN LOCAL AND GLOBAL DISPARITY",
            "text": "We examine the use of local fairness to achieve global fairness, or scenarios where a model is trained to achieve local fairness and subsequently deployed at the global level. As clients only have access to their own datasets, applying local disparity mitigation methods at each client can be convenient.\nWorks such as Cui et al. (2021) argue that local fairness is important as models are deployed at the local client level. But, what happens to global fairness? In Theorem 1, we formally demonstrate that even if local clients are able to use some optimal local mitigation methods and model aggregation techniques to achieve local fairness, the global disparity may still be greater than zero. Theorem 1 (Impossibility of Using Local Fairness to Attain Global Fairness). As long as Redundant Disparity Red(Z:Y\u0302 , S) > 0, the global disparity I(Z; Y\u0302 ) > 0 even if local disparity goes to 0.\nIn order to achieve local fairness, Unique and Masked Disparities must be reduced to zero. The proof leverages Proposition 1, particularly relying on nonnegativity of Unique and Redundant Disparities (see Appendix D). Recall, Example 2 (Pure Redundancy), where the local disparity was zero, but the global disparity was 0.53 as a result of the Redundant Disparity. This is not uncommon in real-world scenarios. Sensitive attributes like race or ethnicity may be correlated with location. For example, one hospital might predominantly serve White patients, while another might primarily serve Black patients. A model may be trained to achieve local fairness but would fail to be globally fair due to a non-zero Redundant Disparity, highlighting the region of disagreement (see Fig.2).\nWe now consider the scenario where a model is able to achieve global fairness and is subsequently deployed at the local client level. Theorem 2 (Global Fairness Does Not Imply Local Fairness). As long as Masked Disparity Syn(Z:Y\u0302 , S)>0, local fairness will not be attained even if global fairness is attained.\nIn order to achieve global fairness, the Unique and Redundant Disparities must be reduced to zero. Recall Example 3 (Pure Synergy), where the model accepts men from client S = 0 and women from client S = 1, while rejecting all others. While this model is globally fair, it is not locally fair. This demonstrates that while it is possible to train a model to achieve global fairness, it may still exhibit disparity when deployed at the local level due to the canceling of disparities between clients. This effect is captured by the Masked Disparity.\nWe now discuss the necessary and sufficient condition to achieve global fairness using local fairness. Theorem 3 (Necessary and Sufficient Condition to Achieve Global Fairness Using Local Fairness). If local disparity I(Z; Y\u0302 |S) goes to zero, then the global disparity I(Z; Y\u0302 ) also goes to zero, if and only if the Redundant Disparity Red(Z:Y\u0302 , S)=0. A sufficient condition for Red(Z:Y\u0302 , S)=0 is Z\u22a5\u22a5S.\nThe results of Theorem 3 suggest that sensitive attributes is uniformly distributed across clients the Redundant Disparity will decrease to zero (see proof in Appendix D). Hence, when the local disparity goes to zero, the global disparity will also decrease to zero. However, in practice, this proportion is fixed since the dataset at each client cannot be changed, i.e., I(Z;S) is fixed. Therefore, we explore another more controllable condition to eliminate Redundant Disparity even when I(Z;S) > 0.\nOne might think that a potential solution to have no redundant information about Z between Y\u0302 and S is to enforce independence between them, i.e., the model should make predictions at the same rate across all clients. However, interestingly, the PID literature demonstrates counterexamples (Kolchinsky, 2022) where this does not hold, and in fact, an additional condition of Syn(Z:Y\u0302 , S) = 0 is required.\nLemma 3. A sufficient condition for Red(Z:Y\u0302 , S) = 0 is Syn(Z:Y\u0302 , S) = 0 and Y\u0302 \u22a5\u22a5 S. Remark 2. It is worth noting that the independence between Y\u0302 and S can be approximately achieved if the true Y and S are independent, as Y\u0302 is an estimation of Y . However, it is often the case that Y \u22a5\u22a5 S is fixed due to the fixed nature of datasets at each client. The mutual information I(Y ;S) can provide insights into the anticipated value of I(Y\u0302 ;S), as FL typically aims to also achieve a reasonable level of accuracy. It may even be possible to enforce Y\u0302 \u22a5\u22a5 S at the cost of accuracy.\nLastly, we explore conditions to attain local fairness through global fairness. Theorem 4. Local disparity will always be less than global disparity if and only if Masked Disparity Syn(Z:Y\u0302 , S) = 0. A sufficient condition is when Z \u2212 Y\u0302 \u2212 S form a Markov chain. Remark 3 (Extension to Personalized Federated Learning Setting). Interestingly, our results extend to the personalized FL setting, where client s can tailor the final global model Y\u0302 = f(X) into a personalized version to improve local performance, Y\u0302 = fs(X). In this case, we can define the global model as a random variable Y\u0302 = g(X,S) and all of the propositions would hold."
        },
        {
            "heading": "3.3 AN OPTIMIZATION FRAMEWORK FOR EXPLORING THE ACCURACY FAIRNESS TRADE-OFF",
            "text": "Now, we explore the inherent trade-off between model accuracy and fairness in the FL context. We formulate the Accuracy and Global-Local Fairness Optimality Problem (AGLFOP), an optimization to delineate the theoretical boundaries of accuracy and fairness trade-offs, capturing the optimal performance any model or FL technique can achieve for a specified dataset and client distribution.\nLet \u2206 be the set of all joint distributions defined for (Z, S, Y, Y\u0302 ). Let \u2206p be a set of all joint distributions Q \u2208 \u2206 that maintain fixed marginals on (Z, S, Y ) as determined by a given dataset and client distribution, i.e., \u2206p = {Q \u2208 \u2206 : PrQ(Z=z, S=s, Y=y) = Pr(Z=z, S=s, Y=y),\u2200z, s, y}. Definition 4 (Accuracy and Global-Local Fairness Optimality Problem (AGLFOP)). Let c(Q) \u2208 [0, 1] be the classification error and IQ(Z; Y\u0302 ) and IQ(Z; Y\u0302 |S) be global and local fairness for distribution Q. Then, the AGLFOP for a specific dataset and client distribution is an optimization of the form:\nargmin Q\u2208\u2206p c(Q) subject to IQ(Z; Y\u0302 ) \u2264 \u03f5g, IQ(Z; Y\u0302 |S) \u2264 \u03f5l. (4)\nHere c(Q)= \u2211\nz,s,y,y\u0302 PrQ(Z=z, S=s, Y=y, Y\u0302=y\u0302)I(y \u0338= y\u0302), the classification error under distribution Q. It quantifies the proportion of incorrect predictions, calculated as the summation of the probabilities of misclassifying the true labels, where I(\u00b7) denotes the indicator function. The complement of the classification error, 1\u2212 c(Q), quantifies the accuracy. Theorem 5. The AGLFOP is a convex optimization problem.\nThe AGLFOP is a convex optimization problem (see proof in Appendix E) that evaluates all potential joint distributions within the set \u2206p which includes the specific dataset and how they are distributed across clients. The true distribution of this given dataset across clients is Pr(Z = z, S = s, Y = y). This makes it an appropriate framework for investigating the accuracy-fairness trade-off. The Pareto front of this optimization problem facilitates a detailed study of the trade-offs, showcasing the maximum accuracy that can be attained for a given global and local fairness relaxation (\u03f5g, \u03f5l).\nThe set \u2206p can be further restricted for specialized applications, e.g., to constrain to all derived classifiers from an optimal classifier (Hardt et al., 2016). We can restrict our optimization space \u2206p to lie within the convex hull derived by the False Positive Rate (FPR) and True Positive Rate (TPR) of an initially trained classifier. This would characterize the accuracy-fairness for all derived classifiers from the original trained classifier. The convex hall characterizes the distributions that can be achieved with any derived classifier. The constraints of AGLFOP can also be expressed using PID terms, offering intriguing insights that we will explore in the following section. The AGLFOP can be computed in any FL environment. Specifically, their computation necessitates the characterization of the joint distribution Pr(Z=z, S=s, Y=y) = Pr(S=s) Pr(Z=z|S=s) Pr(Y=y|Z=z, S=s), which can be readily acquired by aggregating pertinent statistics across all participating clients.\nRemark 4 (Broader Potential). While the AGLFOP currently focuses on independence between Z and Y , it can be adapted to explore other fairness notions. In fact, it can also be used to study optimality in situations where different clients have varying fairness requirements, e.g., adhering to statistical parity globally while upholding equalized odds at the local level. Moreover, variants of this optimization problem can be developed to penalize only the worst-case client fairness scenarios."
        },
        {
            "heading": "4 EXPERIMENTAL DEMONSTRATION",
            "text": "In this section, we provide experimental evaluations on synthetic and real-world datasets to validate our theoretical findings. We investigate the PID of global and local disparity under various conditions. Furthermore, we examine the trade-offs between these fairness metrics and model accuracy.\nData and Client Distribution. We consider the following: (1) Synthetic dataset: A 2-D feature vector X=(X0, X1) has a distribution, i.e., X|Y=1\u223cN ((2, 2), [ 5 11 5 ]), X|Y=0\u223cN ((\u22122,\u22122), [ 10 11 3 ]). Assume binary sensitive attribute Z=1 if X0>0 and 0 otherwise to encode a dependence; and (2) Adult dataset (Dua & Graff, 2017) with gender as sensitive attribute. We consider three cases for partitioning the datasets across clients: (Scenario 1) sensitive-attribute independently distributed across clients, i.e., Z \u22a5\u22a5 S, (Scenario 2) high sensitive-attribute heterogeneity across clients, i.e.,\nZ = S with probability \u03b1, and (Scenario 3) high sensitive-attribute synergy level across clients, i.e., Y = Z \u2295 S. Further details are described in Appendix F. Experiment A: Accuracy-Global-Local-Fairness Trade-off Pareto Front. To explore the tradeoffs between model accuracy and different fairness constraints, we plot the Pareto frontiers for the AGLFOP. We solve for maximum accuracy (1\u2212\u03b4) while varying global and local fairness relaxations (\u03f5g, \u03f5l). We present results for synthetic and Adult datasets as well as PID terms for various data splitting scenarios across clients1. The three-way trade-off among accuracy, global, and local fairness can be visualized as a contour plot (see Fig. 3).\nInterestingly, PID allows us to quantify the agreement and disagreement between local and global fairness. In scenarios characterized by Unique Disparity, local and global fairness agree, and accuracy trade-offs are balanced between them. In cases characterized by Redundant Disparity, the trade-off is primarily between accuracy and global fairness (the accuracy changes along the horizontal axis (\u03f5l) seemingly nonexistent given (\u03f5g)). In contrast, scenarios with Masked Disparity exhibit a trade-off that is primarily between accuracy and local fairness (the trade-off is across the vertical axis).\nExperiment B: Demonstrating Disparities in Federated Learning Settings. In this experiment, we investigate the PID of disparities in the Adult dataset trained within a FL framework. We employ the FedAvg algorithm (McMahan et al., 2017) for training and analyze:\n1We use Python dit package (James et al., 2018) for PID computation and cvxpy for convex solvers.\n\u2022 PID Across Various Splitting Scenarios. We partition the dataset among clients based on the Scenarios 1-3, utilize FedAvg for model training in each case, and examine the PID of both local and global disparities (see Fig. 4). For each scenario, we also evaluate the effects of using a simple local disparity mitigation technique. This is achieved by incorporating a statistical parity regularizer at each client. The results and implementation details are presented in Table 1 in Appendix F.2).\n\u2022 PID Under Varying Sensitive Attribute Heterogeneity Level. We partition the dataset across two clients with varying levels of sensitive attribute heterogeneity. We use \u03b1 = Pr(Z = 0|S = 0) to control the sensitive attribute heterogeneity level across clients. Our results are summarized in Fig. 4 and extended Table 2 in Appendix F.2.\n\u2022 Observing Levels of Masked Disparity. We partition the dataset with varying sensitive attribute synergy levels across clients to study the impact on the Masked Disparity. The synergy level \u03bb \u2208 [0, 1] measures how closely the true label Y aligns with Z \u2295 S (see Definition 7 in Appendix F.2). Results are in Fig. 4 and extended Table 3 in Appendix F.2.\n\u2022 Experiments Involving Multiple Clients. We experiment with multiple clients K = 5 and K = 10. Our findings are presented in Fig. 6, Fig. 5 and Table 4 in Appendix F.2.\nDiscussion: Our information-theoretic framework provides a nuanced understanding of the sources of disparity in FL, namely, Unique, Redundant, and Masked disparities. Our experiments offer insights into the agreement and disagreement between local and global fairness under various data distributions. Our experiments and theoretical results show that depending on the data distribution achieving one can often come at the cost of the other (disagreement). The way data is distributed across clients significantly impacts the type of disparity that dominates. Our optimization framework establishes the accuracy-fairness tradeoffs for a dataset and client distribution.\nImportantly, our research can: (i) inform the use of local disparity mitigation techniques and their convergence and effectiveness when deployed in practice; and (ii) also serve as a valuable tool for policy decision-making, shedding light on the effects of model bias at both the global and local levels. Future studies could also investigate how this approach could be extended to more sophisticated fairness measures. This estimation of PID terms largely depends on (i) the empirical estimators of the probability distributions; and (ii) the efficiency of the convex optimization algorithm used for calculating the unique information. As the number of clients or sensitive attributes increases, the computational cost may rise accordingly. efficient PID computation techniques. Future work could explore alternative efficient PID computation techniques (Belghazi et al., 2018; Venkatesh & Schamberg, 2022; Pakman et al., 2021; Kleinman et al., 2021)."
        },
        {
            "heading": "A BACKGROUND ON INFORMATION THEORY",
            "text": "Information Theory is a mathematical framework primarily concerned with quantifying information.\nEntropy measures the uncertainty or unpredictability of a random variable Z. It is defined as: H(Z) = \u2212 \u2211 z p(z) log p(z) (5)\nwhere p(z) is the probability of Z taking the value z.\nMutual Information is a measure of dependence between two random variables Z and Y\u0302 (captures correlation as well as all non-linear dependencies). Mutual information is zero if and only if Z and Y\u0302 are independent. This means that if the model\u2019s predictions are highly correlated with sensitive attributes (like gender or race), that\u2019s a sign of unfairness. Mutual information has been explored in fairness in the context of centralized machine learning in Kamishima et al. (2011).\nThe mutual information I(Z; Y\u0302 ) is expressed as:\nI(Z; Y\u0302 ) = \u2211 z,y\u0302 p(z, y\u0302) log p(z, y\u0302) p(z)p(y\u0302) (6)\nwhere p(z, y) is the joint probability distribution of Z and Y\u0302 .\nConditional Mutual Information measures the amount of information one random variable contains about another, given the knowledge of a third variable. I(Z; Y\u0302 |S) is the mutual information (dependence) between Z and Y\u0302 conditioned on S. For random variables Z, Y\u0302 , and S, the conditional mutual information I(Z; Y\u0302 |S) is defined as:\nI(Z; Y\u0302 |S) = \u2211 s,z,y\u0302 p(s, z, y\u0302) log p(z, y\u0302|s) p(z|s)p(y\u0302|s) , (7)\nI(Z; Y\u0302 |S) = \u2211 s p(s)I(Z; Y\u0302 |S = s),\nwhere p(z, y\u0302|s), p(z|s), p(y\u0302|s) are the conditional probabilities.\nMUTUAL INFORMATION AS A MEASURE OF FAIRNESS\nMutual information can be used as a measure of the unfairness or disparity of a model. Mutual Information has been interpreted as the dependence between sensitive attribute Z and model prediction Y\u0302 (captures correlation as well as all non-linear dependencies). Mutual information is zero if and only if Z and Y\u0302 are independent. This means that if the model\u2019s predictions are highly correlated with sensitive attributes (like gender or race), that\u2019s a sign of unfairness. Mutual information has been explored in fairness in the context of centralized machine learning in Kamishima et al. (2011); Cho et al. (2020); Kang et al. (2021).\nIn a recent work, Venkatesh et al. (2021) provides another interpretation of mutual information I(Z; Y\u0302 ) in fairness as the accuracy of predicting Z from Y\u0302 (or the expected probability of error in correctly guessing Z from Y\u0302 ) from Fano\u2019s inequality. Even in information bottleneck literature Goldfeld & Polyanskiy (2020), mutual information has been interpreted as a measure of how well one random variable predicts (or, aligns with) the other.\nFor local fairness, we are interested in the dependence between model prediction Y\u0302 and sensitive attributes Z at each and every client, i.e., the dependence between Y\u0302 and Z conditioned on the client S. For example, the disparity at client S = 1 is I(Z; Y\u0302 |S = 1) (the mutual information (dependence) between model prediction and sensitive attribute conditioned on client S = 1 (considering data at client S = 1). Our measure for local disparity is the conditional mutual information (dependence) between Z and Y\u0302 conditioned on S, denoted as I(Z; Y\u0302 |S). Local disparity\nI(Z; Y\u0302 |S) = \u2211\ns p(s)I(Z; Y\u0302 |S = s), is an average of the disparity at each client weighted by the p(s), the fraction of data at client S = s. The local disparity is only zero if and only if all client has zero disparity in their local dataset.\nWhile other works Kang et al. (2021) have used mutual information measures to enforce statistical parity, we are the first to bring Prinsker\u2019s Inequality [4] in the context of fairness to show that statistical parity gap is actually upper bounded by the square root of mutual information (see Lemma 1). Though this is not the main result of the paper, it further justifies the use of mutual information measures to study global and local fairness in FL."
        },
        {
            "heading": "B ADDITIONAL RESULTS AND PROOFS FOR SECTION 3",
            "text": "Lemma 1 (Relationship between Global Statistical Parity Gap and I(Z; Y\u0302 )). Let Pr(Z=0) = \u03b1. The gap SPglobal = |Pr(Y\u0302 = 1|Z = 1)\u2212 Pr(Y\u0302 = 1|Z = 0)| is bounded by \u221a 0.5 I(Z;Y\u0302 ) 2\u03b1(1\u2212\u03b1) .\nProof. Mutual information can be expressed as KL divergence.\nI(Z; Y\u0302 ) = DKL\n( P (Y\u0302 , Z)||P (Y\u0302 ), P (Z) ) Using Pinsker\u2019s Inequality (Canonne, 2022),\ndtv(P,Q) \u2264 \u221a\n0.5DKL(P ||Q) where, dtv(P,Q) is the total variation between two probability distributions P,Q.\ndtv\n( Pr(Y\u0302 , Z),Pr(Y\u0302 ) Pr(Z) ) = 1\n2 \u2211 y\u0302,z \u2223\u2223\u2223Pr(Y\u0302 = y\u0302, Z = z)\u2212 Pr(Y\u0302 = y\u0302) Pr(Z = z)\u2223\u2223\u2223 = \u2211 z Pr(Z = z) \u2211 y\u0302,z 1 2\n\u2223\u2223\u2223Pr(Y\u0302 = y\u0302|Z = z)\u2212 Pr(Y\u0302 = y\u0302)\u2223\u2223\u2223 = 1\n2 Pr(Z = 1)\n[ |Pr(Y\u0302 = 1|Z = 1)\u2212 Pr(Y\u0302 = 1)|+ |Pr(Y\u0302 = 0|Z = 1)\u2212 Pr(Y\u0302 = 0)| ] + 1\n2 Pr(Z = 0)\n[ |Pr(Y\u0302 = 1|Z = 0)\u2212 Pr(Y\u0302 = 1)|+ |Pr(Y\u0302 = 0|Z = 0)\u2212 Pr(Y\u0302 = 0)| ] = 1\n2 \u03b1(1\u2212 \u03b1)|SP1|+ 1 2 \u03b1(1\u2212 \u03b1)|SP0|+ 1 2 \u03b1(1\u2212 \u03b1)|SP1|+ 1 2 \u03b1(1\u2212 \u03b1)|SP0|\n= \u03b1(1\u2212 \u03b1)|SP1|+ \u03b1(1\u2212 \u03b1)|SP0| (8) where, Pr(Z = 0) = 1\u2212 Pr(Z = 1) = \u03b1 and\nSPi = Pr(Y\u0302 = i|Z = 1)\u2212 Pr(Y\u0302 = i|Z = 0) = Pr(Y\u0302 = i|Z = 1)\u2212 Pr(Y\u0302 = i). To complete the proof, we show that |SP1| = |SP0|\nSP1 =Pr(Y\u0302 = 1|Z = 1)\u2212 Pr(Y\u0302 = 1) =Pr(Y\u0302 = 1|Z = 1)\u2212 ( 1\u2212 Pr(Y\u0302 = 0) ) =\u2212 1 + Pr(Y\u0302 = 1|Z = 1) + Pr(Y\u0302 = 0) =\u2212 Pr(Y\u0302 = 0|Z = 1) + Pr(Y\u0302 = 0) = \u2212SP0\nHence, |SP1| = |SP0| and from equation 8, we get,\n2\u03b1(1\u2212 \u03b1)|SP1| \u2264 \u221a 0.5I(Z; Y\u0302 ).\nRemark 5 (Tightness of Lemma 1). Since our proof exclusively utilizes Pinsker\u2019s inequality, their tightness is equivalent. Given I(Z; Y\u0302 ) \u2264 min{H(Z), H(Y\u0302 )} \u2264 H(Y\u0302 ) and H(Y ) \u2264 1 in binary classification. Hence, I(Z; Y\u0302 ) \u2264 1 which is aligned with the known tight regime of Pinsker\u2019s inequality (i.e., DKL(P ||Q) \u2264 1) Canonne (2022). The inequality gets tighter with smaller mutual information I(Z; Y\u0302 ) values.\nLemma 2. I(Z; Y\u0302 |S)=0 if and only if Pr(Y\u0302=1|Z=1, S=s)=Pr(Y\u0302=1|Z=0, S=s) at all clients s.\nProof. We aim to establish that I(Z; Y\u0302 |S) = 0 if and only if Pr(Y\u0302 = 1|Z = 1, S = s) = Pr(Y\u0302 = 1|Z = 0, S = s) for all clients s. For brevity, we denote Pr(Z = z, S = s, Y = y) = p(z, s, y).\nForward Direction: Let us assume I(Z; Y\u0302 |S) = 0. According to the definition of conditional mutual information, we have\nI(Z; Y\u0302 |S) = \u2211 s\u2208S \u2211 z\u2208Z \u2211 y\u0302\u2208{0,1} p(s, z, y\u0302) log ( p(z, y\u0302|s) p(z|s)p(y\u0302|s) ) = 0.\nThis equation implies that log (\np(z,y\u0302|s) p(z|s)p(y\u0302|s) ) = 0 for all s, z, y\u0302, and consequently p(z,y\u0302|s)p(z|s)p(y\u0302|s) = 1 \u2200s.\nObserving that p(z, y\u0302|s) = p(z|s)p(y\u0302|z, s), we deduce that p(z|s)p(y\u0302|z,s)p(z|s)p(y\u0302|s) = 1.\nFrom this, it directly follows that p(y\u0302|z, s) = p(y\u0302|s), and thus Pr(Y\u0302 = 1|Z = 1, S = s) = Pr(Y\u0302 = 1|Z = 0, S = s).\nReverse Direction: Assume Pr(Y\u0302 = 1|Z = 1, S = s) = Pr(Y\u0302 = 1|Z = 0, S = s) for all s. This implies p(y\u0302|s, z) = p(y\u0302|s) for all s, z, y\u0302. Plugging this into the definition of conditional mutual information, we find I(Z; Y\u0302 |S) = 0. Thus, both directions of the equivalence are proven, concluding the proof.\nCorollary 1. The statistical parity at each client s can be expressed as\n|SPs| \u2264 \u221a 0.5 I(Z; Y\u0302 |S = s) 2\u03b1s(1\u2212 \u03b1s)\nwhere, \u03b1s = Pr(Z = 0|S = s) = 1\u2212 Pr(Z = 1|S = s). Definition 5 (Difference Between Local and Global Disparity). The difference between global and local disparity is: I(Z; Y\u0302 ) \u2212 I(Z; Y\u0302 |S) = I(Z; Y\u0302 ;S). This term is the \u201cinteraction information,\u201d which, unlike other mutual-information-based measures, can be positive or negative.\nInteraction information quantifies the redundancy and synergy present in a system. In FL, positive interaction information indicates a system with high levels of redundancy and global disparity, while negative interaction information indicates a system with high levels of synergy and local disparity. Interaction information can inform the trade-off between local and global disparity."
        },
        {
            "heading": "C PROOF FOR SECTION 3.1",
            "text": "Proposition 1. The global and local disparity in FL can be decomposed into non-negative terms as: I(Z; Y\u0302 ) = Uni(Z:Y\u0302 |S) + Red(Z:Y\u0302 , S). (2) I(Z; Y\u0302 |S) = Uni(Z:Y\u0302 |S) + Syn(Z:Y\u0302 , S). (3)\nProof. Equation 2 follows directly from the PID terms definition.\nUni(Z:Y\u0302 |S) + Red(Z:Y\u0302 , S)= min Q\u2208\u2206p IQ(Z; Y\u0302 |S) + I(Z; Y\u0302 )\u2212 min Q\u2208\u2206p IQ(Z; Y\u0302 |S) = I(Z; Y\u0302 ). (9)\nEquation 3 follows from PID terms definition and the chain rule of mutual information.\nUni(Z:Y\u0302 |S) + Syn(Z:Y\u0302 , S) = min Q\u2208\u2206p IQ(Z; Y\u0302 |S) + I(Z; Y\u0302 , S)\u2212 I(Z;S)\u2212 min Q\u2208\u2206p IQ(Z; Y\u0302 |S)\n(10)\n= I(Z;S) + I(Z; Y\u0302 |S)\u2212 I(Z;S) (11) = I(Z; Y\u0302 |S) (12)\nNow, we prove the non-negativity property of PID decomposition.\nUni(Z:Y\u0302 |S) = minQ\u2208\u2206p IQ(Z; Y\u0302 |S) is non-negative since the conditional mutual information is non-negative by definition.\nSyn(Z:Y\u0302 , S) = I(Z; Y\u0302 |S)\u2212minQ\u2208\u2206p IQ(Z; Y\u0302 |S) \u2265 I(Z; Y\u0302 |S)\u2212 I(Z; Y\u0302 |S) = 0 The Redundant Disparity;\nRed(Z:Y\u0302 , S) = I(Z; Y\u0302 )\u2212 min Q\u2208\u2206p IQ(Z; Y\u0302 |S)\n= max Q\u2208\u2206p\nIQ(Y\u0302 ;Z)\u2212 IQ(Z; Y\u0302 |S)\nFirst equality holds by definition. Second equality holds since marginals on (Y\u0302 , Z) is fixed in \u2206p, hence, maxQ\u2208\u2206p IQ(Y\u0302 ;Z) = I(Y\u0302 ;Z).\nTo prove non-negativity of redundant disparity, we construct a distribution Q0 such that:\nPr Q0\n(Z = z, Y\u0302 = y, S = s) = Pr(Z = z, Y\u0302 = y) Pr(Z = z, S = s)\nPr(Z = z)\nNext, we show Q0 \u2208 \u2206p. Recall the set \u2206p in Definition 1:\n\u2206p = {Q \u2208 \u2206 : Pr Q (Z = z, Y\u0302 = y) = Pr(Z = z, Y\u0302 = y),Pr Q (Z = z, S = s) = Pr(Z = z, S = s)}\nPr Q0 (Z = z, Y\u0302 = y) = \u2211 s Pr Q0 (Z = z, Y\u0302 = y, S = s) = \u2211 s Pr(Z = z, Y\u0302 = y) Pr(Z = z) Pr(Z = z, S = s)\n= Pr(Z = z, Y\u0302 = y)\nPr(Z = z)\n\u2211 s Pr(Z = z, S = s) = Pr(Z = z, Y\u0302 = y).\nPr Q0 (Z = z, S = s) = \u2211 y\u0302 Pr Q0 (Z = z, Y\u0302 = y, S = s) = \u2211 y\u0302 Pr(Z = z, Y\u0302 = y) Pr(Z = z, S = s) P (Z = z)\n= Pr(Z = z, S = s)\nPr(Z = z)\n\u2211 y\u0302 Pr(Z = z, Y\u0302 = y) = Pr(Z = z, S = s).\nMarginals of Q0 satisfy conditions on set \u2206p, hence Q0 \u2208 \u2206p. Also, note that by construction of Q0, Y\u0302 and S are independent conditioned on Z, i.e., IQ0(Y\u0302 ;S|Z) = 0. Hence, we have\nRed(Z:Y\u0302 , S) (a) = max\nQ\u2208\u2206p IQ(Z; Y\u0302 )\u2212 IQ(Z; Y\u0302 |S)\n(b) \u2265 IQ0(Z; Y\u0302 )\u2212 IQ0(Z; Y\u0302 |S) (c) = HQ0(Z) +HQ0(Y\u0302 )\u2212HQ0(Z, Y\u0302 )\u2212HQ0(Z|S)\u2212HQ0(Y\u0302 |S) +HQ0(Z, Y\u0302 |S) (d) = IQ0(Y\u0302 ;S)\u2212 IQ0(Y\u0302 ;S|Z) (e) = IQ0(Y\u0302 ;S) (f) \u2265 0.\nHere, (a) hold from definition of Red(Z:Y\u0302 , S), (b) hold since Q0 \u2208 \u2206p, (c)-(d) holds from expressing mutual information in terms of entropy, (e) hold since IQ0(Y\u0302 ;S|Z) = 0, (f) holds from non-negativity property of mutual information."
        },
        {
            "heading": "D ADDITIONAL RESULTS AND PROOFS FOR SECTION 3.2",
            "text": "Theorem 1 (Impossibility of Using Local Fairness to Attain Global Fairness). As long as Redundant Disparity Red(Z:Y\u0302 , S) > 0, the global disparity I(Z; Y\u0302 ) > 0 even if local disparity goes to 0.\nProof. For the sake of completeness, we have provided a detailed proof that demonstrates the non-negativity property of the terms involved.\nUni(Z:Y\u0302 |S) = minQ\u2208\u2206p IQ(Z; Y\u0302 |S) is non-negative since the conditional mutual information is non-negative by definition.\nSyn(Z:Y\u0302 , S) = I(Z; Y\u0302 |S)\u2212minQ\u2208\u2206p IQ(Z; Y\u0302 |S) \u2265 I(Z; Y\u0302 |S)\u2212 I(Z; Y\u0302 |S) = 0 The Redundant Disparity;\nRed(Z:Y\u0302 , S) = I(Z; Y\u0302 )\u2212 min Q\u2208\u2206p IQ(Z; Y\u0302 |S)\n= max Q\u2208\u2206p\nIQ(Y\u0302 ;Z)\u2212 IQ(Z; Y\u0302 |S)\nFirst equality holds by definition. Second equality holds since marginals on (Y\u0302 , Z) is fixed in \u2206p, hence, maxQ\u2208\u2206p IQ(Y\u0302 ;Z) = I(Y\u0302 ;Z).\nTo prove non-negativity of redundant disparity, we construct a distribution Q0 such that:\nPr Q0\n(Z = z, Y\u0302 = y, S = s) = Pr(Z = z, Y\u0302 = y) Pr(Z = z, S = s)\nPr(Z = z)\nNext, we show Q0 \u2208 \u2206p. Recall the set \u2206p in Definition 1:\n\u2206p = {Q \u2208 \u2206 : Pr Q (Z = z, Y\u0302 = y) = Pr(Z = z, Y\u0302 = y),Pr Q (Z = z, S = s) = Pr(Z = z, S = s)}\nPr Q0 (Z = z, Y\u0302 = y) = \u2211 s Pr Q0 (Z = z, Y\u0302 = y, S = s) = \u2211 s Pr(Z = z, Y\u0302 = y) Pr(Z = z) Pr(Z = z, S = s)\n= Pr(Z = z, Y\u0302 = y)\nPr(Z = z)\n\u2211 s Pr(Z = z, S = s) = Pr(Z = z, Y\u0302 = y).\nPr Q0 (Z = z, S = s) = \u2211 y\u0302 Pr Q0 (Z = z, Y\u0302 = y, S = s) = \u2211 y\u0302 Pr(Z = z, Y\u0302 = y) Pr(Z = z, S = s) P (Z = z)\n= Pr(Z = z, S = s)\nPr(Z = z)\n\u2211 y\u0302 Pr(Z = z, Y\u0302 = y) = Pr(Z = z, S = s).\nMarginals of Q0 satisfy conditions on set \u2206p, hence Q0 \u2208 \u2206p. Also, note that by construction of Q0, Y\u0302 and S are independent conditioned on Z, i.e., IQ0(Y\u0302 ;S|Z) = 0. Hence, we have\nRed(Z:Y\u0302 , S) (a) = max\nQ\u2208\u2206p IQ(Z; Y\u0302 )\u2212 IQ(Z; Y\u0302 |S)\n(b) \u2265 IQ0(Z; Y\u0302 )\u2212 IQ0(Z; Y\u0302 |S) (c) = HQ0(Z) +HQ0(Y\u0302 )\u2212HQ0(Z, Y\u0302 )\u2212HQ0(Z|S)\u2212HQ0(Y\u0302 |S) +HQ0(Z, Y\u0302 |S) (d) = IQ0(Y\u0302 ;S)\u2212 IQ0(Y\u0302 ;S|Z) (e) = IQ0(Y\u0302 ;S) (f) \u2265 0.\nHere, (a) hold from definition of Red(Z:Y\u0302 , S), (b) hold since Q0 \u2208 \u2206p, (c)-(d) holds from expressing mutual information in terms of entropy, (e) hold since IQ0(Y\u0302 ;S|Z) = 0, (f) holds from non-negativity property of mutual information.\nHence, from proposition 1, we prove Theorem 1.\nAs local disparity I(Z; Y\u0302 |S) \u2192 0, then Uni(Z:Y\u0302 |S) \u2192 0 and Syn(Z:Y\u0302 , S) \u2192 0, therefore the global disparity I(Z; Y\u0302 ) \u2192 Red(Z:Y\u0302 , S) \u2265 0.\nTheorem 2 (Global Fairness Does Not Imply Local Fairness). As long as Masked Disparity Syn(Z:Y\u0302 , S)>0, local fairness will not be attained even if global fairness is attained.\nProof. Proof requires the non-negativity property of PID terms (follows similarly from proof of Theorem 1). The argument then goes as follows:\nAs global disparity I(Z; Y\u0302 ) \u2192 0, then Uni(Z:Y\u0302 |S) \u2192 0 and Red(Z:Y\u0302 , S) \u2192 0, therefore the global disparity I(Z; Y\u0302 |S) \u2192 Syn(Z:Y\u0302 , S) \u2265 0.\nTheorem 3 (Necessary and Sufficient Condition to Achieve Global Fairness Using Local Fairness). If local disparity I(Z; Y\u0302 |S) goes to zero, then the global disparity I(Z; Y\u0302 ) also goes to zero, if and only if the Redundant Disparity Red(Z:Y\u0302 , S)=0. A sufficient condition for Red(Z:Y\u0302 , S)=0 is Z\u22a5\u22a5S.\nProof. From the PID of local and global disparity,\nI(Z; Y\u0302 ) = Uni(Z:Y\u0302 |S) + Red(Z:Y\u0302 , S). I(Z; Y\u0302 |S) = Uni(Z:Y\u0302 |S) + Syn(Z:Y\u0302 , S).\nTherefore if, I(Z; Y\u0302 |S) = 0, then Uni(Z:Y\u0302 |S) = 0 Hence,\nI(Z; Y\u0302 ) = Red(Z:Y\u0302 , S)\nI(Z; Y\u0302 ) = 0 \u21d0\u21d2 Red(Z:Y\u0302 , S) = 0\nTo prove the sufficient condition, we leverage the PID of I(Z;S) and the non-negative property of the PID terms.\nI(Z;S) = Uni(Z:S|Y\u0302 ) + Red(Z:Y\u0302 , S) I(Z;S) \u2265 Red(Z:Y\u0302 , S)\nHence, Z \u22a5\u22a5 S =\u21d2 Red(Z:Y\u0302 , S) = 0.\nLemma 3. A sufficient condition for Red(Z:Y\u0302 , S) = 0 is Syn(Z:Y\u0302 , S) = 0 and Y\u0302 \u22a5\u22a5 S.\nProof. Interaction information expressed in PID terms:\nI(Z; Y\u0302 ;S) = I(Z; Y\u0302 )\u2212 I(Z; Y\u0302 |S) = Red(Z:Y\u0302 , S)\u2212 Syn(Z; Y\u0302 , S)\nIf synergistic information Syn(Z; Y\u0302 , S) = 0, we have:\nI(Z; Y\u0302 ;S) = I(Z; Y\u0302 )\u2212 I(Z; Y\u0302 |S) = Red(Z:Y\u0302 , S) \u2265 0\nSince the interaction information is positive and symmetric,\nI(Y\u0302 ;S) \u2265 I(Y\u0302 ;S)\u2212 I(Y\u0302 ;S|Z) = Red(Z:Y\u0302 , S)\nand therefore, Y\u0302 \u22a5\u22a5 S =\u21d2 Red(Z:Y\u0302 , S) = 0.\nTheorem 4. Local disparity will always be less than global disparity if and only if Masked Disparity Syn(Z:Y\u0302 , S) = 0. A sufficient condition is when Z \u2212 Y\u0302 \u2212 S form a Markov chain.\nProof. By leveraging the PID of I(Z;S|Y\u0302 ),\nI(Z;S|Y\u0302 ) = Uni(Z:S|Y\u0302 ) + Syn(Z:Y\u0302 , S)\nMarkov chain Z \u2212 Y\u0302 \u2212 S implies, I(Z;S|Y\u0302 ) = 0. Hence, Syn(Z:Y\u0302 , S) = 0. Rest of proof follows from nonnegative property of PID terms:\nI(Z; Y\u0302 |S) = Uni(Z:Y\u0302 |S) \u2264 Uni(Z:Y\u0302 |S) + Red(Z:Y\u0302 , S) = I(Z; Y\u0302 )."
        },
        {
            "heading": "E PROOFS FOR SECTION 3.3",
            "text": "Definition 6 (Convex Function). A function f : Rn \u2192 R is said to be convex if, for all x, y \u2208 Rn and for all \u03bb \u2208 [0, 1], the following inequality holds:\nf(\u03bbx+ (1\u2212 \u03bb)y) \u2264 \u03bbf(x) + (1\u2212 \u03bb)f(y).\nLemma 4 (Log Sum Inequality). The log-sum inequality states that for any two sequences of non-negative numbers a1, a2, . . . , an and b1, b2, . . . , bn, the following inequality holds:\nn\u2211 i=1 ai log ( ai bi ) \u2265 ( n\u2211 i=1 ai ) log (\u2211n i=1 ai\u2211n i=1 bi ) Theorem 5. The AGLFOP is a convex optimization problem.\nProof. The set \u2206p is a convex set, since for any two points Q1, Q2 \u2208 \u2206p, their convex combination also lies in \u2206p (probability simplex). To prove AGFOP is a convex optimization problem, we show each term is convex in Q using the definition of a convex function (see Definition 6).\nLet Q \u2208 \u2206p denote the joint distribution for (Z, S, Y, Y\u0302 ). For brevity, we denote Pr(Z = z, S = s, Y = y) = p(z, s, y), the fixed marginals on (Z, S, Y ).\nTo prove that c(Q) is convex, we need to show: c(Q\u03bb) \u2264 \u03bbc(Q1) + (1 \u2212 \u03bb)c(Q2), where Q\u03bb = \u03bbQ1 + (1\u2212 \u03bb)Q2, Q1, Q2 \u2208 \u2206p, and \u03bb \u2208 [0, 1]. We first express c(Q\u03bb) as:\nc(Q\u03bb) = \u2211\nz,s,y,y\u0302\nQ\u03bb(z, s, y, y\u0302) \u00b7 I(y \u0338= y\u0302) (13)\n= \u03bb \u2211\nz,s,y,y\u0302\nQ1(z, s, y, y\u0302) \u00b7 I(y \u0338= y\u0302) + (1\u2212 \u03bb) \u2211\nz,s,y,y\u0302\nQ2(z, s, y, y\u0302) \u00b7 I(y \u0338= y\u0302) (14)\n= \u03bbc(Q1) + (1\u2212 \u03bb)c(Q2) (15)\nThus, c(Q) is convex as it satisfies the convexity condition.\nTo prove convexity of IQ(Z; Y\u0302 ), we need to show that for any Q1, Q2 \u2208 \u2206p and for any \u03bb \u2208 [0, 1], the following inequality holds: IQ\u03bb(Z : Y\u0302 ) \u2264 \u03bbIQ1(Z : Y\u0302 ) + (1\u2212 \u03bb)IQ2(Z : Y\u0302 ). Note that the marginals are convex in the joint distribution, i.e., Q\u03bb(z, y\u0302) = \u2211 s,y Q\u03bb(z, s, y, y\u0302).\nThis is not necessarily true for conditionals. However, when some marginals are fixed, convexity holds for some conditionals, i.e., Q\u03bb(y\u0302|z) = \u2211 s,y Q\u03bb(z, s, y, y\u0302)/p(z).\nNote that the conditional Q\u03bb(y\u0302|z) is convex in the joint distribution Q\u03bb(z, s, y, y\u0302). Q\u03bb(y\u0302|z) = \u2211 s,y Q\u03bb(z, s, y, y\u0302)/p(z) = \u2211 s,y Q\u03bb(y\u0302|z, s, y)p(s, y, z)/p(z) (16)\n= \u2211 s,y Q\u03bb(y\u0302|z, s, y)p(s, y|z) (17)\nHence, we can show convexity of IQ(Z; Y\u0302 ) in Q\u03bb(y\u0302|z).\nIQ\u03bb(Z; Y\u0302 ) = \u2211 z,y\u0302 Q\u03bb(z, y\u0302) log ( Q\u03bb(z, y\u0302) Q\u03bb(z)Q\u03bb(y\u0302) ) (18)\n= \u2211 z,y\u0302 Q\u03bb(z)Q\u03bb(y\u0302|z) log ( Q\u03bb(y\u0302|z) Q\u03bb(y\u0302) ) (19)\n(a) = \u2211 z,y\u0302 p(z)(\u03bbQ1(y\u0302|z) + (1\u2212 \u03bb)Q2(y\u0302|z)) log ( \u03bbQ1(y\u0302|z) + (1\u2212 \u03bb)Q2(y\u0302|z) \u03bbQ1(y\u0302) + (1\u2212 \u03bb)Q2(y\u0302) ) (20)\n(b) \u2264 \u03bb \u2211 z,y\u0302 p(z)Q1(y\u0302|z) log ( Q1(y\u0302|z) Q1(y\u0302) ) + (1\u2212 \u03bb) \u2211 z,y\u0302 p(z)Q2(y\u0302|z) log ( Q2(y\u0302|z) Q2(y\u0302) ) (21)\n= \u03bbIQ1(Z; Y\u0302 ) + (1\u2212 \u03bb)IQ2(Z; Y\u0302 ) (22) Here (a) holds from expressing the linear combinations. Also note that, Q\u03bb(y\u0302) = \u2211 z Q\u03bb(y\u0302|z)p(z), which can also be expressed as a linear combination. The inequality (b) holds from the log-sum inequality (see Lemma 4).\nTo prove the convexity of IQ(Z; Y\u0302 |S), we need to show that for any Q1, Q2 \u2208 \u2206p and any \u03bb \u2208 [0, 1], the following inequality holds: IQ\u03bb(Z; Y\u0302 |S) \u2264 \u03bbIQ1(Z; Y\u0302 |S) + (1\u2212 \u03bb)IQ2(Z; Y\u0302 |S) Note that the conditional Q\u03bb(y\u0302|z, s) is convex in the joint distribution Q\u03bb(z, s, y, y\u0302).\nQ\u03bb(y\u0302|z, s) = \u2211 y Q\u03bb(y\u0302, z, s, y)/p(z, s) = \u2211 y Q\u03bb(y\u0302|z, s, y)p(y, z, s)/p(z, s) (23)\n= \u2211 y Q\u03bb(y\u0302|z, s, y)p(y|z, s) (24)\nHence, we can show convexity of IQ(Z; Y\u0302 |S) in Q\u03bb(y\u0302|z, s):\nIQ\u03bb(Z; Y\u0302 |S) = \u2211 z,s,y\u0302 Q\u03bb(z, s, y\u0302) log ( Q\u03bb(y\u0302|z, s) Q\u03bb(y\u0302|s) ) (25)\n(a) = \u2211 z,s,y\u0302 p(z, s) (\u03bbQ1(y\u0302|z, s) + (1\u2212 \u03bb)Q2(y\u0302|z, s)) log ( \u03bbQ1(y\u0302|z, s) + (1\u2212 \u03bb)Q2(y\u0302|z, s) \u03bbQ1(y\u0302|s) + (1\u2212 \u03bb)Q2(y\u0302|s) ) (26)\n(b) \u2264 \u03bb \u2211 z,s,y\u0302 p(z, s)Q1(y\u0302|z, s) log ( Q1(y\u0302|z, s) Q1(y\u0302|s) ) + (1\u2212 \u03bb) \u2211 z,s,y\u0302 p(z, s)Q2(y\u0302|z, s) log ( Q2(y\u0302|z, s) Q2(y\u0302|s) ) (27)\n= \u03bbIQ1(Z; Y\u0302 |S) + (1\u2212 \u03bb)IQ2(Z; Y\u0302 |S) (28) The equality (a) holds from linear combinations of Q\u03bb(y\u0302|s) = \u2211 z Q\u03bb(y\u0302|z, s)p(z|s). The inequality (b) holds due to the application of the log-sum inequality (see Lemma 4)."
        },
        {
            "heading": "F EXPANDED EXPERIMENTAL SECTION",
            "text": "This section includes additional results, expanded tables, figures, and details that provide a more comprehensive understanding of our study.\nData. We consider the following datasets:\n(1) Synthetic dataset: A 2-D feature vector X = (X0, X1) follows a distribution given by X|Y=1 \u223c N ((2, 2), [ 5 11 5 ]), X|Y=0 \u223c N ((\u22122,\u22122), [ 10 11 3 ]). Assume Z is a binary sensitive attribute such that Z = 1 if X0 > 0, else Z = 0, to encode dependence of X0 with Z.\n(2) Adult dataset (Dua & Graff, 2017). The Adult dataset is a publicly available dataset in the UCI repository based on 1994 U.S. census data (Dua & Graff, 2017). The goal of this dataset is to\nsuccessfully predict whether an individual earns more or less than 50,000 per year based on features such as occupation, marital status, and education. We select gender as a sensitive attribute, with men as Z = 1 and women as Z = 0.\nClient Distribution. We strategically partition our datasets across clients to explore scenarios characterized by Unique, Redundant, and Masked Disparities.\nScenario 1: Uniform Distribution of Sensitive Attributes Across Clients. In this case, the sensitive attribute Z is independently distributed across clients, i.e., Z \u22a5\u22a5 S. We randomly distribute the dataset to all clients.\nScenario 2: High Heterogeneity in Sensitive Attributes Across Clients. In this scenario, we split observe significant heterogeneity in the distribution of sensitive attributes across clients, denoted by Z = S with a probability \u03b1. For instance, when \u03b1 = 0.9, the client with S = 0 consists of 90% women, while the client with S = 1 is composed of 90% men. In the context of the Adult dataset, we use \u03b1 = Pr(Z = 0|S = 0) as a parameter to regulate this heterogeneity. Scenario 3: High Synergy Level Across Clients. In this case, the true label Y \u2248 Z \u2295 S. To emulate this scenario, we partition the data such that client S = 0 possesses data of males (Z = 1) with true labels Y = 1 and females (Z = 0) with true labels Y = 0. Conversely, client S = 1 contains the remaining data, i.e., males with Y = 0 and females with Y = 1. We introduce the synergy level measures alignment to Y = Z \u2295 S. Definition 7 (Synergy Level (\u03bb)). The synergy level \u03bb of a given dataset and client distribution is defined as the probability that the model prediction Y\u0302 is aligned with Z \u2295 S\n\u03bb = Pr(Y = Z \u2295 S)\nThe synergy level \u03bb \u2208 [0, 1] \u03bb = 1 where implies perfect alignment between Y and Z \u2295 S,\u03bb = 0 implies that Y and Z \u2295 S are have zero alignment. To set \u03bb when splitting data across clients, we first split with perfect XOR alignment and then shuffle fractions of the dataset between clients.\nAdult Heterogeneous Split: In Fig. 3 (fourth row), we split the Adult dataset to capture various disparities simultaneously. We set the synergy level \u03bb = 0.8 (see Definition 7). Due to the nature of the Adult dataset, this introduces some correlation between the sensitive attribute Z and client S.\nF.1 EXPERIMENT A: ACCURACY-GLOBAL-LOCAL-FAIRNESS TRADE-OFF PARETO FRONT.\nTo explore the trade-offs between model accuracy and different fairness constraints, we plot the Pareto frontiers for the AGLFOP. We solve for maximum accuracy (1\u2212 \u03b4) while varying global and local fairness relaxations (\u03f5g, \u03f5l). We present results for synthetic and Adult datasets as well as PID terms for various data splitting scenarios across clients. The three-way trade-off among accuracy, global, and local fairness can be visualized as a contour plot (see Fig. 3).\nFor the Adult dataset, we restrict our optimization space \u2206p to lie within the convex hull derived by the False Positive Rate (FPR) and True Positive Rate (TPR) of an initially trained classifier (trained using FedAvg). This characterizes the accuracy-fairness for all derived classifiers from the original trained classifier (motivated by the post-processing technique from Hardt et al. (2016)). The convex hall characterizes the distributions that can be achieved with any derived classifier. The convex hull for each protected group is composed of points, including (0, 0), (TPR,FPR), (TPR,FPR) and (1, 1), where TPR and FPR denote the true positive and false positive rates of a predictor that inverts all predictions for a protected group. Future work would also explore alternative constraints for various specialized applications.\nF.2 EXPERIMENT B: DEMONSTRATING DISPARITIES IN FEDERATED LEARNING SETTINGS.\nIn this experiment, we investigate the PID of disparities in the Adult dataset trained within a FL framework. We employ the FedAvg algorithm (McMahan et al., 2017) for training and analyze:\nSetup. Our FL model employs a two-layer architecture with 32 hidden units per layer, using ReLU activation, binary cross-entropy loss, and the Adam optimizer. The server initializes the model weights and distributes them to clients, who train locally on partitioned datasets for 2 epochs with a\nbatch size of 64. Client-trained weights are aggregated server-side via the FedAvg algorithm, and this process iterates until convergence. Evaluation metrics are estimated using the dit package (James et al., 2018), which includes PID functions for decomposing global and local disparities into Unique, Redundant, and Masked categories, following the definition from Bertschinger et al. (2014).\nPID of Disparity Across Various Splitting Scenarios. We partition the dataset across two clients, each time varying the level of sensitive attribute heterogeneity. We use \u03b1 = Pr(Z = 0|S = 0) to control the sensitive attribute heterogeneity level. It is worth noting that the Adult dataset exhibits a gender imbalance with a male-to-female ratio of 0.33 : 0.67. Consequently, P (Z = 0) = 0.33, making \u03b1 = Pr(Z = 0|S = 0) = 0.33 the indicator for an independent and identically distributed (i.i.d.) case.\nIn this first setup, we distribute the sensitive attribute uniformly across clients (splitting scenario 1) and employ FedAvg for training. The FL model achieves an accuracy of 84.45% with a global disparity of 0.0359 bits and a local disparity also of 0.0359 bits. The PID reveals that the Unique Disparity is 0.0359 bits, with both Redundant and Masked Disparities being negligible. This aligns with our centralized baseline, indicating that the disparity originates exclusively from the dependency between the model\u2019s predictions and the sensitive attributes, rather than being influenced by S.\nWhen the data is split to introduce high heterogeneity in sensitive attributes across clients (splitting scenario 2), the resulting FL model exhibits a global disparity of 0.0431 bits and a local disparity of 0.0014 bits. PID reveals a Redundant Disparity of 0.0431 bits and a Masked Disparity of 0.0014 bits, with no Unique Disparity.\nNext we split and train according to splitting scenario 3. The trained model reports a local disparity of 0.1761 bits and a global disparity of 0.0317 bits. The PID decomposition shows a Masked Disparity of 0.1761 bits and a Redundant Disparity of 0.0317 bits, with no Unique Disparity observed. The emergence of non-zero Redundant Disparity is attributable to the data splitting, which consequently leads to I(Z;S) = 0.2409 bits.\nWe summarize the three scenarios in Fig. 4. Additionally, we evaluate the effects of using a naive local disparity mitigation technique on the various disparities present.\nEffects of Naive Local Fairness Mitigation Technique. We evaluate the effects of using a naive local disparity mitigation technique on the various disparities present. This is achieved by incorporating a statistical parity regularizer to the loss function at each individual client:\nclient loss = client cross entropy loss + \u03b2 client fairness loss.\nWe use an implementation from FairTorch package (Akihiko Fukuchi, 2021). The term \u03b2 is a hyperparameter that trades off between accuracy and fairness. We use \u03b2 = 0.1 to maintain similarly accurate models. The results are presented in Table 1.\nAn extended table has been included to provide a more in-depth analysis of the PID of global and local disparity for various sensitive attribute distributions across 10 clients.\nPID of Disparity under Heterogeneous Sensitive Attribute Distribution. We analyze the PID of local and global disparities under different sensitive attribute distributions across clients. We train the model with two clients, each having equal-sized datasets. We use \u03b1 = Pr(Z = 0|S = 0) to represent\nsensitive attribute heterogeneity. Note that for a fixed \u03b1, the proportions of sensitive attributes at the other client are fixed. For example since Pr(Z = 0) = 0.33 for the Adult dataset, \u03b1 = 0.33 results in even distribution of sensitive attributes across the two clients. Our results are summarized in Fig. 4 and Table 2.\nObserving Levels of Masked Disparity. We aim to gain a deeper understanding of the circumstances leading to Masked Disparities. Through scenario 3, we showed how high Masked Disparities can occur. However, the level of synergy portrayed in the example may not always be present in reality. We attempt to quantify this using a metric synergy level. The synergy level (\u03bb) measures how closely the model prediction Y\u0302 aligns with the XOR of Z and S (see Definition 7). To achieve a high synergy level, we apply the method outlined in Scenario 3. To decrease the level, we randomly switch data points between clients until the synergy level reaches 0. The number of data points switched controls the level of synergy, ranging from 0 to 1. We conduct experiments with varying levels of synergy to observe the impact on Masked Disparity. The results are summarized in Fig. 4 and Table 3.\nMultiple Client Case. We examine scenarios involving multiple clients. Observations are similar to the two-client case we previously studied. To observe a high Unique Disparity, sensitive attributes need to be identically distributed across clients. To observe the Redundant Disparity, there must be some dependency between clients and a specific sensitive attribute, meaning certain demographic groups are known to belong to a specific client. The Masked Disparity can be observed when there is a high level of synergy or XOR behavior between variables Z and S. Note that since S is no longer binary, we can convert its decimal value to binary and then take the XOR.\nWe experiment, with K = 5 clients. We examine the three disparities. To observe the Unique Disparity by randomly distributing the data among clients. For Redundant Disparity, we divide the data such that the first two clients have mostly females and the remaining three clients have mostly males. For Masked Disparity, we distribute the data similarly to scenario 3 (see Fig. 5).\nInsights from Experiments: When data is uniformly distributed across clients, unique disparity is dominant and contributes to both global and local unfairness (see Figure 4 Scenario 1: model trained using FedAvg on the adult dataset and distributed uniformly across clients). In the trade-off Pareto Front (Figure 3, row 1), we see that both local and global fairness constraints have balanced tradeoffs with accuracy. The PID decomposition (Figure 4, row 1, column 2,3,4) explains this as we see the disparity is mainly unique disparity, with little redundant or masked disparity. The unique disparity highlights where local and global disparity are in agreement.\nIn the case with sensitive attribute heterogeneity (sensitive attribute imbalance across clients). The disparity observed is mainly redundant disparity (see Figure 4 scenario 2 and middle), this is a globally unfair but locally fair model (recall proposition 1). Observe in the tradeoff plot (see Figure 3, row 2) that the accuracy trade-off is with mainly global fairness (an accurate model could have zero local disparity but be globally unfair).\nIn the cases with sensitive-attribute synergy across clients. For example, in a two-client case (one client is more likely to have qualified women and unqualified men and vice versa at the other client). We observe that the mask disparity is dominant (see Figure 4 Scenario 3). The trade AGLFOP tradeoff plot (see Figure 3, row 3) is characterized by Masked Disparity with trade-offs mainly between local fairness and accuracy (an accurate model could have zero global disparity but be locally unfair).\nThe AGLFOP provides the theoretical boundaries trade-offs, capturing the optimal performance any model or FL technique can achieve for a specified dataset and client distribution. For example, say one wants a perfectly globally fair and locally fair model, i.e., (\u03f5g = 0, \u03f5l = 0). Under high sensitive attribute heterogeneity (see Figure 3 row 2), they cannot have a model that does better than 64%."
        }
    ],
    "title": "DEMYSTIFYING LOCAL & GLOBAL FAIRNESS TRADE-OFFS IN FEDERATED LEARNING USING PARTIAL INFORMATION DECOMPOSITION",
    "year": 2023
}