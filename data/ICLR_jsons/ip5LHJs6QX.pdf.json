{
    "abstractText": "In this work, we present efficient modulation, a novel design for efficient vision networks. We revisit the modulation mechanism, which operates input through convolutional context modeling and feature projection layers, and fuses features via element-wise multiplication and an MLP block. We demonstrate that the modulation mechanism is particularly well suited for efficient networks and further tailor the modulation design by proposing the efficient modulation (EfficientMod) block, which is considered the essential building block for our networks. Benefiting from the prominent representational ability of modulation mechanism and the proposed efficient design, our network can accomplish better trade-offs between accuracy and efficiency and set new state-of-the-art performance in the zoo of efficient networks. When integrating EfficientMod with the vanilla self-attention block, we obtain the hybrid architecture which further improves the performance without loss of efficiency. We carry out comprehensive experiments to verify EfficientMod\u2019s performance. With fewer parameters, our EfficientMod-s performs 0.6 top-1 accuracy better than EfficientFormerV2-s2 and is 25% faster on GPU, and 2.9 better than MobileViTv2-1.0 at the same GPU latency. Additionally, our method presents a notable improvement in downstream tasks, outperforming EfficientFormerV2-s by 3.6 mIoU on the ADE20K benchmark. Code and checkpoints are available in the supplementary material.",
    "authors": [],
    "id": "SP:385e985dc2791bab529195638b403f823266422f",
    "references": [
        {
            "authors": [
                "Chun-Fu Chen",
                "Rameswar Panda",
                "Quanfu Fan"
            ],
            "title": "Regionvit: Regional-to-local attention for vision transformers",
            "venue": "ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Jierun Chen",
                "Shiu-hong Kao",
                "Hao He",
                "Weipeng Zhuo",
                "Song Wen",
                "Chul-Ho Lee",
                "S-H Gary Chan"
            ],
            "title": "Run, don\u2019t walk: Chasing higher flops for faster neural networks",
            "year": 2023
        },
        {
            "authors": [
                "Yinpeng Chen",
                "Xiyang Dai",
                "Dongdong Chen",
                "Mengchen Liu",
                "Xiaoyi Dong",
                "Lu Yuan",
                "Zicheng Liu"
            ],
            "title": "Mobile-former: Bridging mobilenet and transformer",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoliang Dai",
                "Alvin Wan",
                "Peizhao Zhang",
                "Bichen Wu",
                "Zijian He",
                "Zhen Wei",
                "Kan Chen",
                "Yuandong Tian",
                "Matthew Yu",
                "Peter Vajda"
            ],
            "title": "Fbnetv3: Joint architecture-recipe search using predictor pretraining",
            "year": 2021
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Xiaohan Ding",
                "Xiangyu Zhang",
                "Ningning Ma",
                "Jungong Han",
                "Guiguang Ding",
                "Jian Sun"
            ],
            "title": "Repvgg: Making vgg-style convnets great again",
            "year": 2021
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Graham",
                "Alaaeldin El-Nouby",
                "Hugo Touvron",
                "Pierre Stock",
                "Armand Joulin",
                "Herv\u00e9 J\u00e9gou",
                "Matthijs Douze"
            ],
            "title": "Levit: a vision transformer in convnet\u2019s clothing for faster inference",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Jianyuan Guo",
                "Kai Han",
                "Han Wu",
                "Yehui Tang",
                "Xinghao Chen",
                "Yunhe Wang",
                "Chang Xu"
            ],
            "title": "Cmt: Convolutional neural networks meet vision transformers",
            "year": 2022
        },
        {
            "authors": [
                "Meng-Hao Guo",
                "Cheng-Ze Lu",
                "Zheng-Ning Liu",
                "Ming-Ming Cheng",
                "Shi-Min Hu"
            ],
            "title": "Visual attention network",
            "venue": "Computational Visual Media,",
            "year": 2023
        },
        {
            "authors": [
                "Kai Han",
                "Yunhe Wang",
                "Qi Tian",
                "Jianyuan Guo",
                "Chunjing Xu",
                "Chang Xu"
            ],
            "title": "Ghostnet: More features from cheap operations",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Kai Han",
                "Yunhe Wang",
                "Qiulin Zhang",
                "Wei Zhang",
                "Chunjing Xu",
                "Tong Zhang"
            ],
            "title": "Model rubik\u2019s cube: Twisting resolution, depth and width for tinynets",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "NIPS 2014 Deep Learning Workshop,",
            "year": 2015
        },
        {
            "authors": [
                "Qibin Hou",
                "Cheng-Ze Lu",
                "Ming-Ming Cheng",
                "Jiashi Feng"
            ],
            "title": "Conv2former: A simple transformerstyle convnet for visual recognition",
            "venue": "arXiv preprint arXiv:2211.11943,",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Howard",
                "Mark Sandler",
                "Grace Chu",
                "Liang-Chieh Chen",
                "Bo Chen",
                "Mingxing Tan",
                "Weijun Wang",
                "Yukun Zhu",
                "Ruoming Pang",
                "Vijay Vasudevan"
            ],
            "title": "Searching for mobilenetv3",
            "year": 2019
        },
        {
            "authors": [
                "Andrew G Howard",
                "Menglong Zhu",
                "Bo Chen",
                "Dmitry Kalenichenko",
                "Weijun Wang",
                "Tobias Weyand",
                "Marco Andreetto",
                "Hartwig Adam"
            ],
            "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
            "venue": "arXiv preprint arXiv:1704.04861,",
            "year": 2017
        },
        {
            "authors": [
                "Gao Huang",
                "Yu Sun",
                "Zhuang Liu",
                "Daniel Sedra",
                "Kilian Q Weinberger"
            ],
            "title": "Deep networks with stochastic depth",
            "venue": "In ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "Alexander Kirillov",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Panoptic feature pyramid networks",
            "year": 2019
        },
        {
            "authors": [
                "Jiachen Li",
                "Ali Hassani",
                "Steven Walton",
                "Humphrey Shi"
            ],
            "title": "Convmlp: Hierarchical convolutional mlps for vision",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Yanyu Li",
                "Geng Yuan",
                "Yang Wen",
                "Ju Hu",
                "Georgios Evangelidis",
                "Sergey Tulyakov",
                "Yanzhi Wang",
                "Jian Ren"
            ],
            "title": "Efficientformer: Vision transformers at mobilenet",
            "year": 2022
        },
        {
            "authors": [
                "Yanyu Li",
                "Ju Hu",
                "Yang Wen",
                "Georgios Evangelidis",
                "Kamyar Salahi",
                "Yanzhi Wang",
                "Sergey Tulyakov",
                "Jian Ren"
            ],
            "title": "Rethinking vision transformers for mobilenet size and speed. ICCV, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Yawei Li",
                "Kai Zhang",
                "Jiezhang Cao",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Localvit: Bringing locality to vision transformers",
            "venue": "arXiv preprint arXiv:2104.05707,",
            "year": 2021
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "Jihao Liu",
                "Xin Huang",
                "Guanglu Song",
                "Hongsheng Li",
                "Yu Liu"
            ],
            "title": "Uninet: Unified architecture search with convolution, transformer, and mlp",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Zhuang Liu",
                "Hanzi Mao",
                "Chao-Yuan Wu",
                "Christoph Feichtenhofer",
                "Trevor Darrell",
                "Saining Xie"
            ],
            "title": "A convnet for the 2020s",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Sgdr: Stochastic gradient descent with warm restarts",
            "year": 2017
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "year": 2019
        },
        {
            "authors": [
                "Ningning Ma",
                "Xiangyu Zhang",
                "Hai-Tao Zheng",
                "Jian Sun"
            ],
            "title": "Shufflenet v2: Practical guidelines for efficient cnn architecture design",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Muhammad Maaz",
                "Abdelrahman Shaker",
                "Hisham Cholakkal",
                "Salman Khan",
                "Syed Waqas Zamir",
                "Rao Muhammad Anwer",
                "Fahad Shahbaz Khan"
            ],
            "title": "Edgenext: efficiently amalgamated cnntransformer architecture for mobile vision applications",
            "venue": "In ECCV,",
            "year": 2023
        },
        {
            "authors": [
                "Sachin Mehta",
                "Mohammad Rastegari"
            ],
            "title": "Mobilevit: Light-weight, general-purpose, and mobilefriendly vision transformer",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Sachin Mehta",
                "Mohammad Rastegari"
            ],
            "title": "Separable self-attention for mobile vision transformers",
            "venue": "TMLR,",
            "year": 2023
        },
        {
            "authors": [
                "Junting Pan",
                "Adrian Bulat",
                "Fuwen Tan",
                "Xiatian Zhu",
                "Lukasz Dudziak",
                "Hongsheng Li",
                "Georgios Tzimiropoulos",
                "Brais Martinez"
            ],
            "title": "Edgevits: Competing light-weight cnns on mobile devices with vision transformers",
            "year": 2022
        },
        {
            "authors": [
                "Zhiliang Peng",
                "Wei Huang",
                "Shanzhi Gu",
                "Lingxi Xie",
                "Yaowei Wang",
                "Jianbin Jiao",
                "Qixiang Ye"
            ],
            "title": "Conformer: Local features coupling global representations for visual recognition",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Ilija Radosavovic",
                "Raj Prateek Kosaraju",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Designing network design spaces",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Yongming Rao",
                "Wenliang Zhao",
                "Benlin Liu",
                "Jiwen Lu",
                "Jie Zhou",
                "Cho-Jui Hsieh"
            ],
            "title": "Dynamicvit: Efficient vision transformers with dynamic token sparsification",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Mark Sandler",
                "Andrew Howard",
                "Menglong Zhu",
                "Andrey Zhmoginov",
                "Liang-Chieh Chen"
            ],
            "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
            "year": 2018
        },
        {
            "authors": [
                "Mingxing Tan",
                "Quoc Le"
            ],
            "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
            "venue": "In ICML,",
            "year": 2019
        },
        {
            "authors": [
                "Mingxing Tan",
                "Quoc Le"
            ],
            "title": "Efficientnetv2: Smaller models and faster training",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Yehui Tang",
                "Kai Han",
                "Jianyuan Guo",
                "Chang Xu",
                "Chao Xu",
                "Yunhe Wang"
            ],
            "title": "Ghostnetv2: Enhance cheap operation with long-range attention",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Matthijs Douze",
                "Francisco Massa",
                "Alexandre Sablayrolles",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Alexandre Sablayrolles",
                "Gabriel Synnaeve",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Going deeper with image transformers",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Asher Trockman",
                "J Zico Kolter"
            ],
            "title": "Patches are all you need",
            "venue": "arXiv preprint arXiv:2201.09792,",
            "year": 2022
        },
        {
            "authors": [
                "Zhengzhong Tu",
                "Hossein Talebi",
                "Han Zhang",
                "Feng Yang",
                "Peyman Milanfar",
                "Alan Bovik",
                "Yinxiao Li"
            ],
            "title": "Maxvit: Multi-axis vision transformer",
            "year": 2022
        },
        {
            "authors": [
                "Pavan Kumar Anasosalu Vasu",
                "James Gabriel",
                "Jeff Zhu",
                "Oncel Tuzel",
                "Anurag Ranjan"
            ],
            "title": "Mobileone: An improved one millisecond mobile backbone",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Alvin Wan",
                "Xiaoliang Dai",
                "Peizhao Zhang",
                "Zijian He",
                "Yuandong Tian",
                "Saining Xie",
                "Bichen Wu",
                "Matthew Yu",
                "Tao Xu",
                "Kan Chen"
            ],
            "title": "Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions",
            "year": 2020
        },
        {
            "authors": [
                "Wenhai Wang",
                "Enze Xie",
                "Xiang Li",
                "Deng-Ping Fan",
                "Kaitao Song",
                "Ding Liang",
                "Tong Lu",
                "Ping Luo",
                "Ling Shao"
            ],
            "title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Wenhai Wang",
                "Enze Xie",
                "Xiang Li",
                "Deng-Ping Fan",
                "Kaitao Song",
                "Ding Liang",
                "Tong Lu",
                "Ping Luo",
                "Ling Shao"
            ],
            "title": "Pvt v2: Improved baselines with pyramid vision transformer",
            "venue": "Computational Visual Media,",
            "year": 2022
        },
        {
            "authors": [
                "Ross Wightman",
                "Hugo Touvron",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Resnet strikes back: An improved training procedure in timm",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Bichen Wu",
                "Xiaoliang Dai",
                "Peizhao Zhang",
                "Yanghan Wang",
                "Fei Sun",
                "Yiming Wu",
                "Yuandong Tian",
                "Peter Vajda",
                "Yangqing Jia",
                "Kurt Keutzer"
            ],
            "title": "Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Jianwei Yang",
                "Chunyuan Li",
                "Xiyang Dai",
                "Jianfeng Gao"
            ],
            "title": "Focal modulation networks",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Hongxu Yin",
                "Arash Vahdat",
                "Jose M Alvarez",
                "Arun Mallya",
                "Jan Kautz",
                "Pavlo Molchanov"
            ],
            "title": "A-vit: Adaptive tokens for efficient vision transformer",
            "year": 2022
        },
        {
            "authors": [
                "Weihao Yu",
                "Mi Luo",
                "Pan Zhou",
                "Chenyang Si",
                "Yichen Zhou",
                "Xinchao Wang",
                "Jiashi Feng",
                "Shuicheng Yan"
            ],
            "title": "Metaformer is actually what you need for vision",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Weihao Yu",
                "Chenyang Si",
                "Pan Zhou",
                "Mi Luo",
                "Yichen Zhou",
                "Jiashi Feng",
                "Shuicheng Yan",
                "Xinchao Wang"
            ],
            "title": "Metaformer baselines for vision",
            "venue": "arXiv preprint arXiv:2210.13452,",
            "year": 2022
        },
        {
            "authors": [
                "Haokui Zhang",
                "Wenze Hu",
                "Xiaoyu Wang"
            ],
            "title": "Parc-net: Position aware circular convolution with merits from convnets and transformer",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Xiangyu Zhang",
                "Xinyu Zhou",
                "Mengxiao Lin",
                "Jian Sun"
            ],
            "title": "Shufflenet: An extremely efficient convolutional neural network for mobile devices",
            "year": 2018
        },
        {
            "authors": [
                "Bolei Zhou",
                "Aditya Khosla",
                "Agata Lapedriza",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Learning deep features for discriminative localization",
            "year": 2016
        },
        {
            "authors": [
                "Bolei Zhou",
                "Hang Zhao",
                "Xavier Puig",
                "Sanja Fidler",
                "Adela Barriuso",
                "Antonio Torralba"
            ],
            "title": "Scene parsing through ade20k dataset",
            "year": 2017
        },
        {
            "authors": [
                "Yang"
            ],
            "title": "2023) and in Table 5. However, the reason hidden behind is not fully explored. Here, we tentatively explain the superiority of modulation mechanism, and show that modulation mechanism is especially well suited for efficient networks. Recall the abstracted formula of modulation mechanism in Eq. 4 that y = p (ctx (x)\u2299",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Vision Transformers (ViTs) (Dosovitskiy et al., 2021; Liu et al., 2021; Vaswani et al., 2017) have shown impressive accomplishments on a wide range of vision tasks and contributed innovative ideas for vision network design. Credited to the self-attention mechanism, ViTs are distinguished from conventional convolutional networks by their dynamic properties and capability for long-range context modeling. However, due to the quadratic complexity over the number of visual tokens, self-attention is neither parameter- nor computation-efficient. This inhibits ViTs from being deployed on edge or mobile devices and other real-time application scenarios. To this end, some attempts have been made to employ self-attention within local regions (Liu et al., 2021; Chen et al., 2022a) or to selectively compute informative tokens (Rao et al., 2021; Yin et al., 2022) to reduce computations. Meanwhile, some efforts (Mehta & Rastegari, 2022; Chen et al., 2022b; Graham et al., 2021) attempt to combine convolution and self-attention to achieve desirable effectiveness-efficiency trade-offs.\nMost recently, some works (Liu et al., 2022b; Yu et al., 2022a; Trockman & Kolter, 2022) suggest that a pure convolutional network can also attain satisfying results compared with self-attention. Among these, FocalNet (Yang et al., 2022) and VAN (Guo et al., 2023), which are computationally efficient and implementation-friendly, show cutting-edge performance and significantly outperform ViT counterparts. Generally, both approaches consider context modeling using a large-kernel convolutional block and modulate the projected input feature using element-wise multiplication (followed by an MLP block), as shown in Fig. 1b. Without the loss of generality, we refer to this design as Modulation Mechanism, which exhibits promising performance and benefits from the effectiveness of convolution and the dynamics of self-attention. Although the modulation mechanism provides satisfactory performance and is theoretically efficient (in terms of parameters and FLOPs), it suffers unsatisfying inference speed when the computational resource is limited. The reasons are two-fold: i) redundant and isofunctional operations, such as successive depth-wise convolutions and redundant linear projections take up a large portion of operating time; ii) fragmentary operations in the context modeling branch considerably raise the latency and are in contravention of guidance G3 in ShuffleNetv2 (Ma et al., 2018).\nIn this work, we propose Efficient Modulation, a simple yet effective design that can serve as the essential building block for efficient models (see Fig. 1c). In comparison to modulation blocks behind FocalNet (Yang et al., 2022) and VAN (Guo et al., 2023), the efficient modulation block is more simple and inherits all benefits (see Fig. 1b and Fig. 1c). In contrast to the Transformer block, our EfficientMod\u2019s computational complexity is linearly associated with image size, and we emphasize large but local interactions, while Transformer is cubically correlated with the token number and directly computes the global interactions. As opposed to the inverted residual (MBConv) block (Sandler et al., 2018), which is still the de facto fundamental building block for many effective networks, our solution uses fewer channels for depth-wise convolution and incorporates dynamics (see Table 6 for comparison). By analyzing the connections and differences between our and these designs, we offer a deep insight into where our effectiveness and efficiency come from. By examining the similarities and distinctions, we gain valuable insights into the efficiency of our approach.\nWith our Efficient Modulation block, we introduce a new architecture for efficient networks called EfficientMod Network. EfficientMod is a pure convolutional-based network and exhibits promising performance. Meanwhile, our proposed block is orthogonal to the traditional self-attention block and has excellent compatibility with other designs. By integrating attention blocks with our EfficientMod, we get a hybrid architecture, which can yield even better results. Without the use of neural network searching (NAS), our EfficientMod offers encouraging performance across a range of tasks. Compared with the previous state-of-the-art method EfficientFormerV2 (Li et al., 2023b), EfficientMod-s outperforms EfficientFormerV2-S2 by 0.3 top-1 accuracy and is 25% faster on GPU. Furthermore, our method substantially surpasses EfficientFormerV2 on downstream tasks, outperforming it by 3.6 mIoU on the ADE20K semantic segmentation benchmark with comparable model complexity. Results from extensive experiments indicated that the proposed EfficientMod is effective and efficient."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Efficient ConvNets Designs. One of the most profound efficient networks is MobileNet (Howard et al., 2017), which decouples a conventional convolution into a point-wise and a depth-wise convolution. By doing so, the parameter number and FLOPs are radically reduced, and the inference speed is substantially boosted. Subsequently, MobileNetV2 (Sandler et al., 2018) further pushed the field by introducing the inverted bottleneck block (also known as the MBConv block), which is now the de facto fundamental building block for most efficient networks (Tan & Le, 2019; Guo et al., 2022; Li et al., 2021; Peng et al., 2021; Tu et al., 2022). In addition, some other contributions are also noteworthy. Network architecture search (NAS) can provide better network designs like MobileNetv3 (Howard et al., 2019), EfficientNet (Tan & Le, 2019; 2021), and FBNet (Wu et al., 2019; Wan et al., 2020; Dai et al., 2021), etc. ShuffleNet (Zhang et al., 2018) leverages group operation and\nchannel shuffling to save computations. ShuffleNetv2 (Ma et al., 2018), FasterNet (Chen et al., 2023), and GhostNet (Han et al., 2020a) emphasize the effectiveness of feature re-use. Regarding efficient design, our model is similar to the MBConv block, but the inner operations and mechanisms behind it are different. Regarding design philosophy, our model is similar to FocalNet Yang et al. (2022) and VAN Guo et al. (2023), but is considerably more efficient and elegant. We discuss the connections and differences in Sec. 3.5 and Table 6.\nTransformers in Efficient Networks. Transformer has garnered considerable interest from the vision community, which undoubtedly includes effective networks. Some methods, such as MobileFormer (Chen et al., 2022b), contemplate adding self-attention to ConvNets to capture local and global interactions concurrently. Self-attention, however, endures high computational costs due to the quadratic complexity of the number of visual tokens. To eschew prohibitive computations, EfficientFormerV2 and EdgeNeXt (Maaz et al., 2023) consider MBConv blocks in the early stages and employ self-attention in the later stages when the token number (or feature resolution) is small. In contrast to earlier efforts, which combined self-attention and MBConv block to achieve a trade-off between efficiency and effectiveness, we distilled the inherent properties of self-attention, dynamics, and large receptive field and introduced these properties to our EfficientMod. We also explore the hybrid architecture that integrates self-attention and EfficientMod for better performance.\nDiscussion on Efficient Networks. Although parameter number and FLOPs are widely employed metrics to assess the theoretical complexity of a model, they do not reflect the network\u2019s real-time cost, as endorsed in ShuffleNetv2 (Ma et al., 2018). Practical guidelines for efficient network design are critical, like fewer network fragments (Ma et al., 2018) and consistent feature dimension (Li et al., 2022), etc. FasterNet (Chen et al., 2023) also demonstrates that low FLOPs do not necessarily lead to low latency due to inefficient low floating-point operations per second. In this work, we present EfficientMod and incorporate prior observations into our design to achieve practical effectiveness."
        },
        {
            "heading": "3 METHOD",
            "text": ""
        },
        {
            "heading": "3.1 REVISIT MODULATION DESIGN",
            "text": "We first derive the general concept of modulation mechanism from VAN and FocalNet.\nVisual Attention Networks. VAN (Guo et al., 2023) considers a convolutional attention design, which is simple yet effective. Specifically, given input feature x \u2208 Rc\u00d7h\u00d7w, we first project x to a new feature space using a fully-connected (FC) layer (with activation function) f (\u22c5) and then feed it into two branches. The first branch ctx (\u22c5) extracts the context information, and the second branch is an identical mapping. We use element-wise multiplication to fuse the feature from both branches, and a new linear projection p (\u22c5) is added subsequently. In detail, a VAN block can be written as:\nOutput = p (ctx (f (x))\u2299 f (x)) , (1) ctx (x) = g (DWConv7,3 (DWConv5,1 (x))) , (2)\nwhere \u2299 is element-wise multiplication, DWConvk,d means a depth-wise convolution with kernel size k and dilation d, and g (\u22c5) is another FC layer in the context branch. Following the design philosophy of MetaFormer (Yu et al., 2022a), the VAN block is employed as a token-mixer, and a two-layer MLP block (with a depth-wise convolution) is adjacently connected as a channel-mixer.\nFocalNets. FocalNets (Yang et al., 2022) introduced the Focal Modulation that replaces selfattention but enjoys the dynamics and large receptive fields. FocalNet also considers a parallel two branches design, where one context modeling branch ctx (\u22c5) adaptively aggregates different levels of contexts and one linear project branch v (\u22c5) project x to a new space. Similarly, the two branches are fused by element-wise multiplication, and an FC layer p (\u22c5) is employed. Formally, the hierarchical modulation design in FocalNet can be given by (ignoring the global average pooling level for clarity):\nctx (x) = g ( L\n\u2211 l=1\nact (DWConvkl (f (x))\u2299 z (f (x)))) , (3)\nwhere ctx includes L levels of context information that are hierarchically extracted by depth-wise convolutional layer with a kernel size of kl, z (\u22c5) project c-channel feature to a gating value. act (\u22c5) is GELU activation function after each convolutional layer.\nAbstracted Modulation Mechanism. Both VAN and FocalNet demonstrated promising representational ability and exhibited satisfying performance. By revisiting as aforementioned, we reveal that both methods share some indispensable designs, which greatly contribute to their advancements. Firstly, the two parallel branches are operated individually, extracting features from different feature spaces like self-attention mechanism (as shown in Fig. 1a). Secondly, for the context modeling, both considered large receptive fields. VAN stacked two large kernel convolutions with dilation while FocalNet introduced hierarchical context aggregation as well as a global average pooling to achieve a global interaction. Thirdly, both methods fuse the features from two branches via element-wise multiplication, which is computationally efficient. Lastly, a linear projection is employed after feature fusion. We argue that the gratifying performance of the two models can be credited to the above key components. Meanwhile, there are also distinct designs, like the particular implementations of context modeling and the design of feature projection branches (shared or individual projection). Consolidating the aforementioned similarities and overlooking specific differences, we abstract the modulation mechanism as depicted in Fig. 1b and formally define the formulation as:\nOutput = p (ctx (x)\u2299 v (x)) . (4) The abstracted modulation mechanism inherits desirable properties from both convolution and selfattention but operates in a convolutional fashion with satisfying efficiency in theory. Specifically, Eq. 4 enjoys dynamics like self-attention due to the element-wise multiplication. The context branch also introduces local feature modeling, but a large receptive field is also achieved via large kernel size (which is not a bottleneck for efficiency). Following VAN and FocalNet, a two-layer MLP block is constantly introduced after the modulation design, as shown in Fig. 1c. Besides aforementioned strengths that make modulation mechanism suitable for efficient networks, we also tentatively introduce a novel perspective in Appendix Sec. K that modulation has the unique potential to project the input feature to a very high dimensional space."
        },
        {
            "heading": "3.2 EFFICIENT MODULATION",
            "text": "Despite being more efficient than self-attention, the abstracted modulation mechanism still fails to meet the efficiency requirements of mobile networks in terms of theoretical complexity and inference latency. Here, we introduce Efficient Modulation, which is tailored for efficient networks but retains all the desirable properties of the modulation mechanism.\nSliming Modulation Design. A general modulation block has many fragmented operations, as illustrated in Fig. 1b. Four FC layers are introduced without considering the details of the context modeling implementation. As stated in guideline G3 in ShuffleNetv2 (Ma et al., 2018), too many fragmented operations will significantly reduce speed, even if the computational complexity may be low by tweaking the channel number. To this end, we fuse the FC layers from the MLP and modulation blocks as shown in Fig.1c. We consider v (\u22c5) to expand the channel dimension by an expansion factor of r and leverage p (\u22c5) to squeeze the channel number. That is, the MLP block is fused into our modulation design with a flexible expansion factor, resulting in a unified block similar to the MBConv block (we will discuss the differences and show our superiority in Table. 6).\nSimplifying Context Modeling. We next tailor our context modeling branch for efficiency. given the input x, we first project x to a new feature space by a linear projection f (x). Then, a depth-wise convolution with GELU activation is employed to model local spatial information. We set the kernel size to 7 to balance the trade-off between efficiency and a large receptive field. Lastly, a linear projection g (x) is employed for channel communication. Notice that the channel number is kept the same throughout the context modeling branch. In short, our context modeling branch can be given by:\nctx (x) = g (act (DWConv7,1 (f (x)))) . (5)\nThis design is much simpler than the context modeling in VAN and FocalNet. We discard isofunctional depth-wise convolutions by one large-kernel depth-wise convolution. We acknowledge that this may slightly degrade the performance as a compromise to efficiency. Ablation studies demonstrate that each operation in our context branch is indispensable."
        },
        {
            "heading": "3.3 NETWORK ARCHITECTURE",
            "text": "With the modifications as mentioned above, we arrive at our Efficient Modulation block depicted in Fig. 1c. Next, we instantiate our efficient networks. Please see Appendix Sec. B for more details.\nFirst, we introduce a pure convolutional network solely based on the EfficientMod block. Following common practice (Li et al., 2023b; Yu et al., 2022a), we adopt a hierarchical architecture of 4 stages; each stage consists of a series of our EfficientMod blocks with residual connection. For simplicity, we used overlapped patch embedding (implemented with a convolutional layer) to down-size the features by a factor of 4, 2, 2, and 2, respectively. For each block, we normalize the input feature using Layer Normalization (Ba et al., 2016) and feed the normalized feature to our EfficientMod block. We employ Stochastic Depth (Huang et al., 2016) and Layer Scale (Touvron et al., 2021b) to improve the robustness of our model. Notice that our EfficientMod block is orthogonal to the self-attention mechanism. Following recent advances that combine convolution and attention for better performance (Li et al., 2023b; Mehta & Rastegari, 2022; Chen et al., 2022b; Pan et al., 2022), we next combine our EfficientMod with attention block to get a new hybrid design. We consider the vanilla attention block as in ViT (Dosovitskiy et al., 2021) without any modifications. The attention blocks are only introduced in the last two stages, where the feature size is relatively small. We vary the width and depth to match the parameters in the pure convolutional-based EfficientMod counterpart for a fair comparison. We introduce three scales ranging from 4M to 13M parameters, resulting in EfficientMod-xxs, EfficientMod-xs, and EfficientMod-s."
        },
        {
            "heading": "3.4 COMPUTATIONAL COMPLEXITY ANALYSIS",
            "text": "We also examine our design\u2019s theoretical computational complexity and practical guidelines.\nGiven input feature x \u2208 RC \u00d7H \u00d7W , the total parameters number of one EfficientMod block is 2 (r + 1)C2 + k2C, and the computational complexity is O (2(r + 1)HWC2 +HWk2C), where k is kernel size and r is the expansion ratio in v (\u22c5). We ignore the activation function and bias in learnable layers for simplicity. Compared with Attention, our complexity is linear to the input resolution. Compared with MBConv, we reduce the complexity of depth-wise convolution by a factor of r, which is crucial for effectiveness as validated in Table 6.\nBesides the theoretical computational complexity, we also provide some practical guidelines for our design. I) We reduce the FLOPs by moving more parameters to later stages where the feature resolution is small. The reason behind is that our EfficientMod\u2019s FLOPs are basically equal to the input resolution \u00d7 the number of parameters. Following this guideline, we can add more blocks or substantially increase the width in later stages. Note that this guideline is not unique to our EfficientMod and can be applied to all FC and Convolutional layers. II) We only introduce attention blocks to the last two stages, as a common practice in many works (Li et al., 2023b; Mehta & Rastegari, 2022; Yu et al., 2022b; Mehta & Rastegari, 2023) considering self-attention\u2019s computational complexity. III) We use Repeat operation to match channel number to save CPU time with a light overhead on GPU. EfficientFormer observed that the Reshape\nis often a bottleneck for many models. Here, we introduce more details. Reshape is considerably sluggish on the CPU but is GPU-friendly. Meanwhile, Repeat operation is swift on CPU but time-consuming on GPU. As shown in Fig. 2, two solutions (Repeat and Reshape) can be used for interact in EfficientMod, we select Repeat to get the optimal GPU-CPU latency trade-off."
        },
        {
            "heading": "3.5 RELATION TO OTHER MODELS",
            "text": "Lastly, we discuss the connections and differences between our EfficientMod block and other notable designs to emphasize the unique properties of our approach.\nMobileNetV2 ushered in a new era in the field of efficient networks by introducing mobile inverted bottleneck (MBConv in short) block. Compared to the MBConv block that sequentially arranges the FC layer, our EfficientMod block separates the depth-wise convolutional layer and inserts it from the side into the middle of the two-layer FC network via element-wise multiplication. We will show that our design is a more efficient operation (due to the channel number reduction of depth-wise convolution) and achieve better performance (due to modulation operation) in Table 6.\nSENet introduces dynamics to ConvNets by proposing channel-attention mechanism (Hu et al., 2018). An SE block can be given by y = x \u22c5 sig (W2 (act (W1x))). Many recent works (Tan & Le, 2019; Zhang et al., 2022; Liu et al., 2022a) incorporate it to achieve better accuracy while maintaining a low complexity in theory. However, due to the fragmentary operations in SE block, it would significantly reduce the inference latency on GPUs. On the contrary, our EfficientMod block inherently involves channel attention via y = ctx (x) \u22c5 q (x), where q (x) adaptively adjust the channel weights of ctx (x)."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we validate our EfficientMod on four tasks: image classification on ImageNet1K (Deng et al., 2009), object detection and instance segmentation on MS COCO (Lin et al., 2014), and semantic segmentation on ADE20K (Zhou et al., 2017). We implement all networks in PyTorch and convert to ONNX models on two different hardware: \u2022 GPU: We chose the P100 GPU for our latency evaluation since it can imitate the computing power\nof the majority of devices in recent years. Other GPUs may produce different benchmark results, but we observed that the tendency is similar. \u2022 CPU: Some models may operate with unpredictable latency on different types of hardware (mostly caused by memory accesses and fragmented operations). We also provide all models\u2019 measured latency on the Intel(R) Xeon(R) CPU E5-2680 CPU for a full comparison.\nFor the latency benchmark, we set the batch size to 1 for both GPU and CPU to simulate real-world applications. To counteract the variance, we repeat 4000 runs for each model and report the mean inference time. We use four threads following the common practice. For details on more devices (e.g., different GPUs, iPhone, etc.), please check out supplemental material."
        },
        {
            "heading": "4.1 IMAGE CLASSIFICATION ON IMAGENET-1K",
            "text": "We evaluate the classification performance of EfficientMod networks on ImageNet-1K. Our training recipe follows the standard practice in DeiT (Touvron et al., 2021a), details can be found in Appendix Sec. 5. Strong training tricks (e.g., re-parameterization and distillation) were not used to conduct a fair comparison and guarantee that all performance was derived from our EfficientMod design.\nWe compare our EfficientMod with other efficient designs and present the results in Table 1. Distinctly, our method exhibits admirable performance in terms of both classification accuracy and inference latency on different hardware. For instance, our EfficientMod-s performs the same as EdgeViT but runs 15 milliseconds (about 73%) faster on the GPU and 11 milliseconds (about 32%) faster on the CPU. Moreover, our model requires fewer parameters and more minor computational complexity. EfficientMod-s also outperforms EfficientFormerV2-S2 by 0.6 improvements and runs 1.8ms (about 25%) faster on GPU. Our method performs excellently for different scales. Be aware that some efficient designs (like MobileNetV2 and FasterNet) prioritize low latency while other models prioritize performance (like MobileViTv2 and EdgeViT). In contrast, our EfficientMod provides state-of-the-art performance while running consistently fast on both GPU and CPU.\nTo better grasp the enhancements of our method, we use EfficientMod-s as an example and outline the specific improvements of each modification. The results of our EfficientMod, from the pure convolutional-based version to the hybrid model, are presented in Table 2.\nWe note that even the pure convolutional-based version of EfficientMod already produces impressive results at 80.5%, significantly surpassing related convolutionalbased networks. By adapting to hybrid architecture, we further enhance the performance to 81.0%.\nMeanwhile, some methods are trained with strong training strategies, like re-parameterization (Ding et al.,\n2021) in MobileOne and distillation (Hinton et al., 2015) in EfficientFormerV2. When trained with distillation (following the setting in (Li et al., 2023b)), we improve EfficientMod-s from 81.0 to 81.9%, as shown in Table 3. All following results are without distillation unless stated otherwise.\n4.2 ABLATION STUDIES\nCompare to other Modulation models. We compare our EfficientMod-xxs with FocalNet and VAN-B0, that has a similar number of parameters. For a fair comparison, we customize Focal-\nNet_Tiny_lrf by reducing the channel number or the blocks. We tested three variants, selected the best one, and termed it FocalNet@4M. Since Conv2Former (Hou et al., 2022) code has not been fully released, we didn\u2019t consider it in our comparison. From Table 4, we see that EfficientMod outperforms other modulation methods for both accuracy and latency.\nAblation of each component. We start by examining the contributions provided by each component of our design. Experiments are conducted on the convolutional EfficientMod-s without introducing attention and knowledge distillation. Table 5 shows the results of eliminating each component in the context modeling branch. Clearly, all these components are critical to our final results. Introducing all, we arrive at 80.5% top-1 accuracy. Meanwhile, we also conducted an experiment to validate the effectiveness of element-wise multiplication. We substitute it with summation (same computations and same latency) to fuse features from two branches and present the results in the last row of the table. As expected, the performance drops by 1% top-1 accuracy. The considerable performance drop reveals the effectiveness of our modulation operation, especially in efficient networks.\nConnection to MBConv blocks. To verify the superiority of EfficientMod block, we compare our design and the essential MBConv with isotropic and hierarchical architectures, respectively. Please check Appendix Sec. B for detailed settings. With almost the same number of parameters and FLOPs, results in Table 6 indicate that our EfficientMod consistently runs faster than MBConv counterparts by a significant margin on both GPU and CPU. One most probable explanation is that our depth-wise convolution is substantially lighter than MBConv\u2019s (channel numbers are c and rc, respectively, where r is set to 6). Besides the faster inference, our design consistently provides superior empirical results than MBConv block. Please check Appendix Sec. G for more studies on scalability.\nContext Visualization. Inherited from modulation mechanism, our EfficientMod block can distinguish informative context. Following FocalNet, we visualize the forward output of the context layer (computing the mean value along channel dimension) in EfficientMod-Conv-s, as shown in Fig. 4. Clearly, our model consistently captures the informative objects, and the background is restrained, suggesting the effectiveness of the modulation design in efficient networks."
        },
        {
            "heading": "4.3 OBJECT DETECTION AND INSTANCE SEGMENTATION ON MS COCO",
            "text": "To validate the performance of EfficientMod on downstream tasks, we conduct experiments on MS COCO dataset for object detection and instance segmentation. We validate our EfficientMod-s on top of the common-used detector Mask RCNN (He et al., 2017). We follow the implementation of previous work (Yu et al., 2022a; Wang et al., 2021; 2022; Tan & Le, 2021), and train the model using 1\u00d7 scheduler, i.e., 12 epochs. We compare our convolutional and hybrid EfficientMod-s with other methods and report the results in Table. 7. Results suggest that EfficientMod consistently outper-\nforms other methods with similar parameters. Without self-attention, our EfficientMod surpasses PoolFormer by 4.2 mAP for detection and 3.6 mAP on instance segmentation task. When introducing attention and compared with hybrid models, our method still outperforms others on both tasks."
        },
        {
            "heading": "4.4 SEMANTIC SEGMENTATION ON ADE20K",
            "text": "We next conduct experiments on the ADE20K (Zhou et al., 2017) dataset for the semantic segmentation task. We consider Semantic FPN (Kirillov et al., 2019) as the segmentation head due to its simple and efficient design. Following previous work (Yu et al., 2022a; Li et al., 2023b; 2022; Wang et al., 2021), we train our model for 40k iterations with a total batch size of 32 on 8 A100 GPUs. We train our model using AdamW (Loshchilov & Hutter, 2019) optimizer. The Cosine Annealing scheduler (Loshchilov & Hutter, 2017) is used to decay the learning rate from initialized value 2e-4.\nResults in Table 7 demonstrate that EfficientMod outperforms other methods by a substantial margin. Without the aid of attention, our convolutional EfficientMod-s already outperforms PoolFormer by 6.3 mIoU. Furthermore, the pure convolutional EfficientMod even achieves better results than the attention-equipped methods. In this regard, our convolutional EfficientMod-s performs 1.1 mIoU better than the prior SOTA efficient method EfficientFormerV2 (42.4 vs. 43.5). The design of our EfficientMod block is the sole source of these pleasing improvements. When introducing Transformer blocks to get the hybrid design, we further push the performance to 46.0 mIoU, using the same number of parameters and even fewer FLOPs. Hybrid EfficientMod-s performs noticeably better than other hybrid networks, outperforming PvTv2 and EfficientFormerV2 by 3.5 and 3.6 mIoU, respectively. Two conclusions are offered: 1) EfficientMod design makes significant advancements, demonstrating the value and effectiveness of our approach; 2) Large receptive fields are especially helpful for high-resolution input tasks like segmentation, and the vanilla attention block (which achieves global range) can be an off-the-shelf module for efficient networks. Please check Appendix Sec. F for the analysis of the improvement gap between MS COCO and ADE20K."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We present Efficient Modulation (EfficientMod), a unified convolutional-based building block that incorporates favorable properties from both convolution and attention mechanisms. EfficientMod simultaneously extracts the spatial context and projects input features, and then fuses them using a simple element-wise multiplication. EfficientMod\u2019s elegant design gratifies efficiency, while the inherent design philosophy guarantees great representational ability. With EfficientMod, we built a series of efficient models. Extensive experiments examined the efficiency and effectiveness of our method. EfficientMod outperforms previous SOTA methods in terms of both empirical results and practical latency. When applied to dense prediction tasks, EfficientMod delivered impressive results. Comprehensive studies indicate that our method has great promise for efficient applications.\nLimitations and Broader Impacts. The scalability of efficient designs is one intriguing but understudied topic, like the huge latency gap in Table 6. Also, employing large kernel sizes or introducing attention blocks might not be the most efficient way to enlarge the receptive field. We have not yet observed any negative societal impacts from EfficientMod. Instead, we encourage study into reducing computations and simplifying real-world applications with limited computational resources."
        },
        {
            "heading": "A CODES AND MODELS",
            "text": "The codes can be found in the supplemental material. We provide anonymous links to the pre-trained checkpoints and logs. The ReadME.md file contains thorough instructions to conduct experiments. After the submission, we will make our codes and pre-trained checkpoints available."
        },
        {
            "heading": "B DETAILED CONFIGURATIONS",
            "text": "Detailed Framework Configurations We provide a detailed configuration of our EfficientMod in Table 8. Our EfficientMod is a hierarchical architecture that progressively downsizes the input resolution by 4,2,2,2 using the traditional convolutional layer. For the stages that include both EfficientMod and attention blocks, we employ EfficientMod blocks first, then utilize the attention blocks. By varying channel and block numbers, we introduce EfficientModxxs, EfficientMod-xs, EfficientMod-s, and a pure convolutional version of EfficientMod-s.\nDetailed Ablation Configurations For the isotropic designs in Table 6, we patchify the input image using a 14\u00d714 patch size, bringing us a resolution of 16 \u00d7 16. We adjust the depth and width to deliberately match the number of parameters as EfficientMod-xs and EfficientMod-s. We also vary the expansion ratio to match the number of parameters and FLOPs for MBConv and EfficientMod counterparts. We chose 256\nand 196 for the channel number and 13 and 11 for the depth, respectively. Similar to EfficientMod-s and EfficientMod-xs, the generated models will have 12.5M and 6.4M parameters, respectively. By doing so, we guarantee that any performance differences result purely from the design of the MBConv and EfficientMod blocks. For hierarchical networks, we replace the EfficientMod block with the MBConv block and vary the expansion ratio to the match parameter number.\nTraining details The detailed training hyper parameters are presented in Table 5.\nC VISUALIZATION OF MODULATION\nWe provide a complete visualization of our modulation design, as seen in Fig.6. We showcase the project input v, the context modeling output ctx, and the corresponding modulation outcomes ctx \u2217 v.The block\u2019s output is presented last. The visualization implementation is the same as the settings in Section 4.2. We divide the feature into r chunks along the channel dimension when the input feature is expanded by a factor of r (for example, 6 in Fig. 6), and we next visualize each chunk and its accompanying modulation result. Interestingly, as the channel count increases, different thunks show generally similar results v. The difference is substantially accentuated after being modulated by the same context, indicating the success of the modulation mechanism."
        },
        {
            "heading": "D LATENCY COMPARISON",
            "text": "We also demonstrate the accuracy versus GPU and CPU latency for various methods, as seen in Fig. 7. We eliminate specific models that run significantly slower (such as EdgeViT on GPU) or yield\nmuch lower accuracy (such as MobileNetV2) for better visualization. Our approach consistently surpasses related work, especially on GPU, by a clear margin. Our EfficientMod outperforms MobileViTv2 by 2.8 top-1 accuracy on ImageNet with the same GPU latency. We are 25% faster than the previous state-of-the-art approach EffcientFormerv2 to obtain results that are similar or slightly better (0.3%). On the CPU, we also achieve a promising latency-accuracy trade-off, demonstrating that our EfficientMod can be utilized as a general method on different devices."
        },
        {
            "heading": "E DISTILLATION IMPROVEMENTS",
            "text": "We provide detailed experiments for distillation on each model. Our teacher model is the widely used RegNetY-160 (Radosavovic et al., 2020), the same as the teacher in EfficientFormerV2 for fair comparison. Though other models may offer superior improvements, a better teacher model is not the primary objective of this study. The right table shows that knowledge distillation is a potent way to improve our method\u2019s top-1\naccuracy about 1% without introducing any computational overhead during inference."
        },
        {
            "heading": "F ANALYSIS ON IMPROVEMENT GAP BETWEEN OBJECT DETECTION AND SEMANTIC SEGMENTATION",
            "text": "Why does ModelMod improve significantly on ADE20K but only modestly on MS COCO? Besides the differences in datasets and evaluation metrics, we attribute this discrepancy to the number of parameters in detection or segmentation head. Notice that EfficientMod only introduces 12M parameters. When equipped with Mask RCNN, the additional parameters are over 20M ( over 60% in total), dominating the final detection network. Hence, the impact of the backbone is largely inhibited. On the contrary, Semantic FPN only introduces 4-5M parameters for semantic segmentation on ADE20K (about 25% in total). Hence, EfficientMod\u2019s capabilities are fully utilized.\nG SCALABILITY OF EFFICIENTMOD\nLatency against input resolution. We first validate our scalability for the input resolution. We vary the input resolution from 224 to 512 by a step size of 32. We compare our convolutional and hybrid variants of EfficientMod with some strong baselines, including MobileFormer (Chen et al., 2022b), MobileViTv2 (Mehta & Rastegari, 2023), and EfficientFormerV2 (Li et al., 2023b). For a fair comparison, we consider model size in the range of 10-15M parameters for all models. From Fig. 8, we can observe that our method and EfficientFormerV2 show promising scalability to the input resolution when compared with MobileFormer and MobileViTv2. Compared to EfficientFormerV2, our method (both\nconvolutional and hybrid) also exhibits even lower latency when the resolution is small.\nCompare with MBConv. We also investigate the scalability of the width and kernel size for our EfficientMod. We compare EfficientMod and the commonly used MBConv from MobileNetV2 (Sandler et al., 2018) using the same settings described in Sec. B. To match the computational complexity and parameter number, the expansion ratio is set to 6 and 7 for EfficientMod and MBConv, respectively. As shown in Fig. 9, our EfficientMod consistently runs faster than MBConv block regardless of width\nor kernel size. When increasing the width or kernel size, the latency tendency of our EfficientMod is much smoother than MBConv\u2019s, suggesting EfficientMod has great potential to be generalized to larger models."
        },
        {
            "heading": "H BENCHMARK RESULTS ON MORE GPUS",
            "text": "In addition to the results on the P100 GPU presented in Table 1, we also conducted latency benchmarks on several other GPU instances, including the T4, V100-SXM2, and A100-SXM4-40GB. We observed that there might be some variances even when the GPU types are the same. Therefore, we randomly allocated GPUs from our server and conducted three separate benchmark tests. Table 10 reports the mean and standard deviation values.\nAs shown in the table, our EfficientMod consistently performs fast on different GPU devices. An interesting finding is that the results of A100 have a higher latency than V100.\nBatch Size 1 2 4 8 16 32 64 V100-SXM2 3.3 3.9 5.3 8.0 13.7 25.4 48.2 A100-SXM4 3.7 3.9 4.2 5.7 8.9 14.6 27.2 Firstly, we show that this is a common phenomenon for almost all models, as we can see in the table. Secondly, we observed consistently low GPU utilization for A100, consistently below 40%, indicating that A100\u2019s strong performance is not being fully harnessed. Thirdly, we evaluated batch size 1 to simulate real-world scenarios. When scaling up the batch size, GPU utilization increased, resulting in lower latency for A100, as depicted above (we toke EfficientMod-s as an example). Lastly, we highlight that latency could be influenced by intricate factors that are challenging to debug, including GPU architectures, GPU core numbers, CUDA versions, Operating Systems, etc."
        },
        {
            "heading": "I LATENCY BENCHMARK ON DOWNSTREAM TASKS",
            "text": "Besides the study on the scalability of EfficientMod in Sec. G, we also explore the latency on realworld downstream tasks. We directly benchmark methods in Table 7 on one A100 GPU (without converting to ONNX format) and report the latency in Table 11.\nClearly, our EfficientMod also exhibits promising efficiency on these tasks. An intriguing observation is that PoolFormer-S12 exhibits the highest latency. This is particularly interesting, considering that the core operation within the network is the pooling operation. We consider two factors could be contributing to this phenomenon: 1) the PoolFormer network architecture might not be optimized for efficiency. 2) pooling operations might not be as highly optimized in CUDA as convolutions (a phenomenon we\u2019ve also noticed in our backbone design). Additionally, we have observed that as the input resolution increases, the latency gap between Hybrid EfficientMod-s and Conv EfficientMod-s widens. This is attributed to the computational complexity introduced by the Attention Mechanism in our hybrid version. One potential remedy is to reduce computations by downsizing the resolution for the attention block, similar to the approach employed in EfficientFormerV2. However, our Hybrid EfficientMod-s maintains competitive and promising latency results compared to methods like EfficientFormerV2-s2 and other alternatives."
        },
        {
            "heading": "J OPTIMIZATION FOR MOBILE DEVICE",
            "text": "As presented in previous results, our EfficientMod mainly focuses on GPU and CPU devices. Next, we explore the optimization for mobile devices. We convert our PyTorch model to a Core ML model using coremltools1. We then make use of the iOS application 2 from MobileOne (Chen et al., 2022b) and benchmark latency on an iPhone 13 (iOS version 16.6.1).\nWe take a pure convolution-based EfficientMod-xxs (without attention module, which achieves 75.3% top-1 accuracy) and compare it with other networks in Table 12. Based on our observation that permute operation is exceptionally time-consuming in CoreML models, we replace the permute + linear layer with a convolutional layer, which is mathematically equal. Our model is able to achieve 75.3% top-1 accuracy at 1.2ms latency on iPhone 13. Inspired by the analysis of normalization layers in EfficientFormer (Li et al., 2022), we further remove all Layer Normalization and add a Batch Normalization layer after each convolutional layer (which can be automatically fused during inference) and re-train the model. By doing so, we reduce the latency to 0.9 ms and achieve a 74.7% top-1 accuracy, which already achieves a promising result. We further slightly adjust the block and\n1https://github.com/apple/coremltools 2https://github.com/apple/ml-mobileone/tree/main/ModelBench\nchannel numbers and get a 75.2% accuracy at 1.0ms. The strong performance indicates that our proposed building block also performs gratifyingly on mobile devices."
        },
        {
            "heading": "K TENTATIVE EXPLANATION TOWARDS THE SUPERIORITY OF MODULATION MECHANISM FOR EFFICIENT NETWORKS",
            "text": "It has been demonstrated that modulation mechanism can enhance performance with almost no additional overhead in works such as Yang et al. (2022); Guo et al. (2023) and in Table 5. However, the reason hidden behind is not fully explored. Here, we tentatively explain the superiority of modulation mechanism, and show that modulation mechanism is especially well suited for efficient networks.\nRecall the abstracted formula of modulation mechanism in Eq. 4 that y = p (ctx (x)\u2299 v (x)), it can be simply rewritten as y = f(x2), where f(x2) = ctx (x)\u2299 v (x) and we ignore p (\u22c5) since it is a learnable linear projection. Hence, we can recursively give the output of l-th layer modulation block with residual by:\nx1 = x0 + f1(x 2 0), (6)\nx2 = x1 + f2(x 2 1), (7)\n= x0 + f1(x 2 0) + f2(x 2 0) + 2f2(x0 \u2217 f1(x 2 o)) + (f1(x 2 0)) 2, (8)\nxl = a1g1(x 1 0) + a2g2(x 2 0) + a3g3(x 3 0) +\u22ef + algl(x\n2l 0 ), (9)\nwhere l indexes the layer, al is the weight for each item, gl indicates the combined function for l-th item, and we do not place emphasis on the details of gl. With only a few blocks, we can easily project the input to a very high dimensional feature space, even infinite-dimensional space. For instance, with only 10 modulation blocks, we will get a 210-dimensional feature space. Hence, we can conclude that i) modulation mechanism is able to reduce the requirement of channel number since it can naturally project input feature to very high dimension in a distinct way; ii) modulation mechanism does not require a very deep network since several blocks are able to achieve high dimensional space. However, in the case of large models, the substantial width and depth of these models largely offset the benefits of modulation. Hence, we emphasize that the abstracted modulation mechanism is particularly suitable for the design of efficient networks.\nNotice that the tentative explanation presented above does not amount to a highly formalized proof. Our future effort will center on a comprehensive and in-depth investigation."
        },
        {
            "heading": "L DETAILED ANALYSIS OF EACH DESIGN",
            "text": "Efficiency of Slimming Modulation Design We have integrated an additional MLP layer into the EfficientMod block to validate the efficiency of slimming modulation design. This modification was aimed at assessing the impact of slimming on both performance and computational efficiency. Remarkably, this resulted in a notable reduction in both GPU and CPU latency, with a negligible impact on accuracy. This underlines the effectiveness of our slimming approach in enhancing model efficiency without compromising accuracy.\nMethod Param. FLOPs Top-1 GPU Latency CPU Latency EfficientMod-s-Conv (sperate MLP) 12.9M 1.5G 80.6 6.2 ms 26.2 ms EfficientMod-s-Conv 12.9M 1.5G 80.5 5.8 ms 25.0 ms\nEfficiency of simplifying Context Modeling To further validate the efficiency of our approach in simplifying context modeling, we compared our single kernel size (7x7) implementation against multiple convolutional layers with varying kernel sizes, as the implementation of FocalNet. Experiments are conducted based on EfficientMod-s-Conv variant. Our findings reinforce the superiority of using a single, optimized kernel size. This strategy not only simplifies the model but also achieves a better accuracy-latency trade-off, demonstrating the practicality and effectiveness of our design choice.\nKernel Sizes Param. FLOPs Top-1 GPU Latency CPU Latency [3, 3] 12.7M 1.4G 79.7 5.8 ms 28.5 ms [3, 5] 12.8M 1.5G 80.1 6.1 ms 29.0 ms [3, 7] 12.9M 1.5G 80.2 6.4 ms 29.7 ms [5, 5] 12.9M 1.5G 80.2 6.3 ms 29.2 ms [5, 7] 13.0M 1.5G 80.3 6.6 ms 29.8 ms\n[3, 5 ,7] 13.1M 1.5G 80.5 7.2 ms 32.4 ms [7] 12.9M 1.5G 80.5 5.8 ms 25.0 ms\nIntegrating Attention in EfficientMod The introduction of vanilla attention in the last two stages of EfficientMod aimed to improve global representation. We adjusted the block and channel numbers to ensure the parameter count remained comparable between EfficientMod-s-Conv and EfficientMods. The results highlight that EfficientMod-s not only shows improved performance but also reduced latency, thereby validating our approach in integrating attention for enhanced efficiency.\nMethod Param. FLOPs Top-1 GPU Latency CPU Latency EfficientMod-s-Conv 12.9M 1.5G 80.5 5.8 ms 25.0 ms EfficientMod-s 12.9M 1.4G 81.0 5.5 ms 23.5 ms\nAs shown above, the additional experiments and analyses affirm the distinct contributions and efficacy of each design element in our model, suggesting our EfficientMod can achieve a promising latency-accuracy trade-off."
        }
    ],
    "year": 2023
}