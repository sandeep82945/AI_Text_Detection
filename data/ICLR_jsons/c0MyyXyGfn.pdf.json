{
    "abstractText": "Reinforcement learning (RL) for complex tasks remains a challenge, primarily due to the difficulties of engineering scalar reward functions and the inherent inefficiency of training models from scratch. Instead, it would be better to specify complex tasks in terms of elementary subtasks and to reuse subtask solutions whenever possible. In this work, we address continuous space lexicographic multi-objective RL problems, consisting of prioritized subtasks, which are notoriously difficult to solve. We show that these can be scalarized with a subtask transformation and then solved incrementally using value decomposition. Exploiting this insight, we propose prioritized soft Q-decomposition (PSQD), a novel algorithm for learning and adapting subtask solutions under lexicographic priorities in continuous state-action spaces. PSQD offers the ability to reuse previously learned subtask solutions in a zero-shot composition, followed by an adaptation step. Its ability to use retained subtask training data for offline learning eliminates the need for new environment interaction during adaptation. We demonstrate the efficacy of our approach by presenting successful learning, reuse, and adaptation results for both lowand high-dimensional simulated robot control tasks, as well as offline learning results. In contrast to baseline approaches, PSQD does not trade off between conflicting subtasks or priority constraints and satisfies subtask priorities during learning. PSQD provides an intuitive framework for tackling complex RL problems, offering insights into the inner workings of the subtask composition.",
    "authors": [
        {
            "affiliations": [],
            "name": "Finn Rietz"
        }
    ],
    "id": "SP:c0ca8547dc878dd25e665a661988367e832afa2e",
    "references": [
        {
            "authors": [
                "Joshua Achiam",
                "David Held",
                "Aviv Tamar",
                "Pieter Abbeel"
            ],
            "title": "Constrained policy optimization",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Eitan Altman"
            ],
            "title": "Constrained Markov decision processes, volume 7",
            "venue": "CRC press,",
            "year": 1999
        },
        {
            "authors": [
                "Isac Arnekvist",
                "Danica Kragic",
                "Johannes A Stork"
            ],
            "title": "Vpe: Variational policy embedding for transfer reinforcement learning",
            "venue": "In 2019 International Conference on Robotics and Automation (ICRA),",
            "year": 2019
        },
        {
            "authors": [
                "Yinlam Chow",
                "Mohammad Ghavamzadeh",
                "Lucas Janson",
                "Marco Pavone"
            ],
            "title": "Risk-constrained reinforcement learning with percentile risk criteria",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2017
        },
        {
            "authors": [
                "Rodrigo de Lazcano",
                "Kallinteris Andreas",
                "Jun Jet Tai",
                "Seungjae Ryan Lee",
                "Jordan Terry"
            ],
            "title": "URL http://github.com/Farama-Foundation/ Gymnasium-Robotics",
            "venue": "Gymnasium robotics,",
            "year": 2023
        },
        {
            "authors": [
                "Alexander Dietrich",
                "Christian Ott",
                "Alin Albu-Sch\u00e4ffer"
            ],
            "title": "An overview of null space projections for redundant, torque-controlled robots",
            "venue": "The International Journal of Robotics Research,",
            "year": 2015
        },
        {
            "authors": [
                "Simon Duane",
                "A.D. Kennedy",
                "Brian J. Pendleton",
                "Duncan Roweth"
            ],
            "title": "Hybrid monte carlo",
            "venue": "Physics Letters B,",
            "year": 1987
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Zolt\u00e1n G\u00e1bor",
                "Zsolt Kalm\u00e1r",
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Multi-criteria reinforcement learning",
            "venue": "Proceedings of the Fifteenth International Conference on Machine Learning (ICML",
            "year": 1998
        },
        {
            "authors": [
                "Sehoon Ha",
                "Peng Xu",
                "Zhenyu Tan",
                "Sergey Levine",
                "Jie Tan"
            ],
            "title": "Learning to walk in the real world with minimal human effort",
            "venue": "In Conference on Robot Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Haoran Tang",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Reinforcement learning with deep energy-based policies",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Vitchyr Pong",
                "Aurick Zhou",
                "Murtaza Dalal",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Composable deep reinforcement learning for robotic manipulation",
            "venue": "IEEE international conference on robotics and automation (ICRA),",
            "year": 2018
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Kristian Hartikainen",
                "George Tucker",
                "Sehoon Ha",
                "Jie Tan",
                "Vikash Kumar",
                "Henry Zhu",
                "Abhishek Gupta",
                "Pieter Abbeel"
            ],
            "title": "Soft actor-critic algorithms and applications",
            "venue": "arXiv preprint arXiv:1812.05905,",
            "year": 2018
        },
        {
            "authors": [
                "W.K. Hastings"
            ],
            "title": "Monte Carlo sampling methods using Markov chains and their applications",
            "venue": "Biometrika, 57(1):97\u2013109,",
            "year": 1970
        },
        {
            "authors": [
                "Karol Hausman",
                "Jost Tobias Springenberg",
                "Ziyu Wang",
                "Nicolas Heess",
                "Martin A. Riedmiller"
            ],
            "title": "Learning an embedding space for transferable robot skills",
            "venue": "In 6th International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Conor F Hayes",
                "Roxana R\u0103dulescu",
                "Eugenio Bargiacchi",
                "Johan K\u00e4llstr\u00f6m",
                "Matthew Macfarlane",
                "Mathieu Reymond",
                "Timothy Verstraeten",
                "Luisa M Zintgraf",
                "Richard Dazeley",
                "Fredrik Heintz"
            ],
            "title": "A practical guide to multi-objective reinforcement learning and planning",
            "venue": "Autonomous Agents and Multi-Agent Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Hunt",
                "Andre Barreto",
                "Timothy Lillicrap",
                "Nicolas Heess"
            ],
            "title": "Composing entropic policies using divergence correction",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Alex Irpan"
            ],
            "title": "Deep reinforcement learning doesn\u2019t work yet",
            "venue": "https://www.alexirpan.com/ 2018/02/14/rl-hard.html,",
            "year": 2018
        },
        {
            "authors": [
                "Sertac Karaman",
                "Emilio Frazzoli"
            ],
            "title": "Incremental sampling-based algorithms for optimal motion planning",
            "venue": "Robotics Science and Systems VI,",
            "year": 2010
        },
        {
            "authors": [
                "Romain Laroche",
                "Mehdi Fatemi",
                "Joshua Romoff",
                "Harm van Seijen"
            ],
            "title": "Multi-advisor reinforcement learning",
            "venue": "CoRR, abs/1704.00756,",
            "year": 2017
        },
        {
            "authors": [
                "Steven M LaValle",
                "James J Kuffner Jr."
            ],
            "title": "Randomized kinodynamic planning",
            "venue": "The international journal of robotics research,",
            "year": 2001
        },
        {
            "authors": [
                "Changjian Li",
                "Krzysztof Czarnecki"
            ],
            "title": "Urban driving with multi-objective deep reinforcement learning",
            "venue": "Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS \u201919,",
            "year": 2019
        },
        {
            "authors": [
                "Qingkai Liang",
                "Fanyu Que",
                "Eytan Modiano"
            ],
            "title": "Accelerated primal-dual policy optimization for safe reinforcement learning",
            "venue": "arXiv preprint arXiv:1802.06480,",
            "year": 2018
        },
        {
            "authors": [
                "Nicholas Metropolis",
                "Arianna W Rosenbluth",
                "Marshall N Rosenbluth",
                "Augusta H Teller",
                "Edward Teller"
            ],
            "title": "Equation of state calculations by fast computing machines",
            "venue": "The journal of chemical physics,",
            "year": 1953
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Andrei A Rusu",
                "Joel Veness",
                "Marc G Bellemare",
                "Alex Graves",
                "Martin Riedmiller",
                "Andreas K Fidjeland",
                "Georg Ostrovski"
            ],
            "title": "Human-level control through deep reinforcement learning",
            "year": 2015
        },
        {
            "authors": [
                "Thanh Thi Nguyen",
                "Ngoc Duy Nguyen",
                "Peter Vamplew",
                "Saeid Nahavandi",
                "Richard Dazeley",
                "Chee Peng Lim"
            ],
            "title": "A multi-objective deep reinforcement learning framework",
            "venue": "Engineering Applications of Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik M Roijers",
                "Peter Vamplew",
                "Shimon Whiteson",
                "Richard Dazeley"
            ],
            "title": "A survey of multiobjective sequential decision-making",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2013
        },
        {
            "authors": [
                "Gavin A Rummery",
                "Mahesan Niranjan"
            ],
            "title": "On-line Q-learning using connectionist systems, volume 37",
            "year": 1994
        },
        {
            "authors": [
                "Stuart J Russell",
                "Andrew Zimdars"
            ],
            "title": "Q-decomposition for reinforcement learning agents",
            "venue": "In Proceedings of the 20th International Conference on Machine Learning",
            "year": 2003
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Joar Skalse",
                "Lewis Hammond",
                "Charlie Griffin",
                "Alessandro Abate"
            ],
            "title": "Lexicographic multi-objective reinforcement learning",
            "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Krishnan Srinivasan",
                "Benjamin Eysenbach",
                "Sehoon Ha",
                "Jie Tan",
                "Chelsea Finn"
            ],
            "title": "Learning to be safe: Deep rl with a safety critic",
            "venue": "arXiv preprint arXiv:2010.14603,",
            "year": 2020
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Chen Tessler",
                "Daniel J. Mankowitz",
                "Shie Mannor"
            ],
            "title": "Reward constrained policy optimization",
            "venue": "In 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Peter Vamplew",
                "Benjamin J Smith",
                "Johan K\u00e4llstr\u00f6m",
                "Gabriel Ramos",
                "Roxana R\u0103dulescu",
                "Diederik M Roijers",
                "Conor F Hayes",
                "Fredrik Heintz",
                "Patrick Mannion",
                "Pieter JK Libin"
            ],
            "title": "Scalar reward is not enough: A response to silver, singh, precup and sutton (2021)",
            "venue": "Autonomous Agents and Multi-Agent Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Quantao Yang",
                "Johannes A Stork",
                "Todor Stoyanov"
            ],
            "title": "Null space based efficient reinforcement learning with hierarchical safety constraints",
            "venue": "European Conference on Mobile Robots (ECMR),",
            "year": 2021
        },
        {
            "authors": [
                "Hengrui Zhang",
                "Youfang Lin",
                "Sheng Han",
                "Kai Lv"
            ],
            "title": "Lexicographic actor-critic deep reinforcement learning for urban autonomous driving",
            "venue": "IEEE Transactions on Vehicular Technology,",
            "year": 2022
        },
        {
            "authors": [
                "Brian D Ziebart",
                "Andrew L Maas",
                "J Andrew Bagnell",
                "Anind K Dey"
            ],
            "title": "Maximum entropy inverse reinforcement learning",
            "venue": "In Aaai,",
            "year": 2008
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Reinforcement learning (RL) for complex tasks is challenging because as practitioners we must design a scalar-valued reward function that induces the desired behavior (Ha et al., 2021; Tessler et al., 2019; Irpan, 2018) and we must expensively learn each task from scratch. Instead, it might be more suitable to model complex tasks as several simpler subtasks (Ga\u0301bor et al., 1998; Roijers et al., 2013; Nguyen et al., 2020; Hayes et al., 2022), and more efficient to reuse or adapt subtask solutions that were learned previously (Arnekvist et al., 2019; Hausman et al., 2018; Finn et al., 2017). To prevent potentially conflicting subtasks from deteriorating the overall solution, we can impose lexicographic priorities on the subtasks (Skalse et al., 2022; Ga\u0301bor et al., 1998; Zhang et al., 2022). This means ordering all subtasks by priority and forbidding lower-priority subtasks from worsening the performance of higher-priority subtasks, which is similar to null-space projection approaches for complex multi-task control problems (Dietrich et al., 2015). In RL, this setting is referred to as a lexicographic multi-objective reinforcement learning (MORL) problem.\nHowever, state-of-the-art RL algorithms for continuous state and action spaces like Soft Q-Learning (SQL) (Haarnoja et al., 2017), Soft Actor-Critic (SAC) (Haarnoja et al., 2018b), or Proximal Policy Optimization (PPO) (Schulman et al., 2017) do not support lexicographic MORL problems out of the box and obvious extensions still trade off conflicting subtasks, leading to suboptimal solutions, as we demonstrate in our experiments. Current lexicographic MORL algorithms like (Li & Czarnecki, 2019; Zhang et al., 2022) enumerate actions and are therefore limited to discrete problems or rely on constrained optimization methods that trade-off between optimizing subtask and priority objectives (Skalse et al., 2022). All these lexicographic algorithms learn a monolithic policy,\n\u2217Corresponding author: finn.rietz@oru.se\nprecluding any interpretation or reuse of previously learned subtask solutions. Furthermore, it is unknown what overall reward function these approaches actually optimize, which hampers principled understanding of lexicographic MORL algorithms.\nIn this paper, we are the first to show that such complex, lexicographic MORL problems can be scalarized (Roijers et al., 2013) and we propose prioritized soft Q-decomposition (PSQD), a learning algorithm that solves lexicographic MORL tasks in a decomposed fashion (Russell & Zimdars, 2003). For this, we proposed a subtask transformation that recovers the Q-function of the lexicographic MORL problem as the sum of Q-functions of the transformed subtasks and so provides the overall scalar reward function. PSQD computes this sum incrementally by learning and transforming subtask Q-functions, beginning at the highest-priority subtask and step-by-step learning and adding Q-functions for lower-priority subtasks. In every step, an arbiter ensures that the lower-priority subtasks do not worsen already solved higher-priority subtasks and restricts the lower-priority subtask to solutions within the so-called action indifference space of the higher-priority subtasks. PSQD provides a safe learning and exploration framework, which is crucial for many real-world scenarios, and allows reuse and adaptation of separately learned subtask solutions to more complex, lexicographic MORL task, which saves data and computation costs. The decomposable Q-function benefits interpretability of the learned behavior by allowing analysis and inspection of subtask solutions and their action indifference spaces.\nThe contributions of this paper are: (1) We show that lexicographic MORL problems can be scalarized using our proposed subtask transformation. (2) We show how to reuse and adapt separately learned subtask solutions to more complex, lexicographic MORL problems, starting from a priorityrespecting zero-shot composition solution. (3) We provide PSQD, an algorithm for lexicographic MORL problems with continuous state-action spaces, that reuses both subtask solutions and retained offline data."
        },
        {
            "heading": "2 PROBLEM DEFINITION AND PRELIMINARIES",
            "text": "We formalize the complex lexicographic MORL problem (Skalse et al., 2022; Ga\u0301bor et al., 1998) as a Markov decision process (MDP) M \u2261 (S,A, r, ps, \u03b3, ) consisting of state space S, action space A, reward function r : S \u00d7 A \u2192 Rn, and discount factor \u03b3 \u2208 [0, 1]. The subtasks are numbered i = 1, 2, . . . n and ranked with the transitive priority relation 1 2 . . . n with task 1 having the highest and task n having the lowest priority. Each dimension of the vector-valued reward corresponds to one scalar subtask reward function [r(s,a)]i = ri(s,a). The discrete-time dynamics are given by a conditional distribution ps(st+1 | st,at) with st, st+1 \u2208 S and at \u2208 A. We use (st,at) \u223c \u03c1\u03c0 to denote the state-action marginal induced by a policy \u03c0(at | st). Lexicographic subtask priorities can be defined by constraining the policy in each subtask i to stay close to optimal (i.e., \u03b5-optimal) for the next higher-priority task i \u2212 1 (Skalse et al., 2022; Zhang et al., 2022). Formally, such an \u03b5-lexicographic MORL problem implies a set of allowed policies \u03a0i for each subtask i,\n\u03a0i = {\u03c0 \u2208 \u03a0i\u22121 | max \u03c0\u2032\u2208\u03a0i\u22121 Ji\u22121(\u03c0 \u2032)\u2212 Ji\u22121(\u03c0) \u2264 \u03b5i\u22121}, (1)\nwhere Ji are performance criteria like value-functions, and 0 \u2264 \u03b5i \u2208 R are thresholds for each subtask. However, it is not practical to compute the sets \u03a0i explicitly when solving \u03b5-lexicographic MORL problems.\nCurrent approaches solve \u03b5-lexicographic MORL problems with constrained RL or incremental action selection in discrete domains (Skalse et al., 2022; Zhang et al., 2022) and it is unclear what equivalent scalar reward function these methods actually optimize (Skalse et al., 2022; Ga\u0301bor et al., 1998; Vamplew et al., 2022). In contrast, we transform the individual subtasks and their Q-functions to implement the priority constraints in equation 1. This leads to an equivalent scalar reward function for the complex \u03b5-lexicographic MORL problem. Our definition of the performance criteria Ji in Sec. 3.1 is key to our incremental and decomposed learning approach with this reward function."
        },
        {
            "heading": "2.1 Q-DECOMPOSITION",
            "text": "Our goal is to reuse and adapt subtask solutions that were learned previously to new and more complex \u03b5-lexicographic MORL problems. For this, we require a decomposed learning algorithm\nthat models individual subtask solutions and combines them to the solution of the complex \u03b5lexicographic MORL problem.\nQ-decomposition (Russell & Zimdars, 2003) is such an algorithm for MORL problems where the complex task\u2019s reward function is the sum of subtask rewards, r\u03a3(st,at) = \u2211n i=1[r(st,at)]i. Instead of learning the sum Q-function, Q\u03a3, the algorithm learns one Q-function Qi for each subtask and submits them to a central arbiter. The arbiter combines the Q-functions, Q\u03a3(s,a) =\u2211n i=1Qi(s,a), and selects the maximizing action for its policy \u03c0\u03a3. Q-decomposition uses on-policy SARSA-style (Sutton & Barto, 2018; Rummery & Niranjan, 1994) updates with the arbiter policy \u03c0\u03a3 to avoid illusion of control (Russell & Zimdars, 2003) or the attractor phenomenon Laroche et al. (2017); van Seijen et al. (2017) in subtask learning.\nSumming and scaling subtask rewards alone does not result in lexicographic priorities because low-priority subtasks with sufficiently large reward scales can still dictate over high-priority subtasks. Therefore, we cannot directly use the Q-decomposition algorithm for lexicographic MORL. Our algorithm builds on Q-decomposition but we transform subtasks to ensure that reward- and Q-functions can be summed up whilst implementing lexicographic priorities. To emphasize this difference, we denote transformed subtask Q-functions by Q i, which estimate the long-term value of transformed subtask rewards r i, such that the Q-function of the lexicographic MORL problem corresponds to Q1 2 ...n(s,a) = \u2211n i=1Q i(s,a), or Q for short. Qi and ri refer to the untransformed subtask Q-functions and rewards in the lexicographic MORL problem."
        },
        {
            "heading": "2.2 MAXIMUM ENTROPY REINFORCEMENT LEARNING",
            "text": "We aim to solve \u03b5-lexicographic MORL problems with continuous states and actions, but instead of explicitly computing equation 1, we focus on identifying as many near optimal actions as possible, such that lower-priority subtasks have more alternative actions to choose from. For this reason, we use maximum entropy reinforcement learning (MaxEnt RL) (Ziebart et al., 2008; Haarnoja et al., 2018b; 2017) for learning subtask policies \u03c0i and subtask Q-functions Qi. MaxEnt RL maximizes the reward signal augmented by the Shannon entropy of the policy\u2019s action distribution, H(at | st) = Eat\u223c\u03c0(at|st)[\u2212 log \u03c0(at | st)], i.e.,\n\u03c0\u2217MaxEnt = arg max \u03c0 \u221e\u2211 t=1 E(st,at)\u223c\u03c1\u03c0 [ \u03b3t\u22121 ( r(st,at) + \u03b1H(at | st) )] , (2)\nwhere the scaler \u03b1 can be used to determine the relative importance of the entropy term. This improves exploration in multimodal problems but also prevents overly deterministic policies, which is key for identifying many near-optimal actions. Following Haarnoja et al. (2017), we drop \u03b1 because it can be replaced with reward scaling. Importantly, the entropy term in the objective leads to the following energy-based Boltzmann distribution as the optimal MaxEnt policy,\n\u03c0\u2217MaxEnt(at | st) = exp (Q\u2217soft(st,at)\u2212 V \u2217soft(st)), (3) where Q\u2217soft(st,at) acts as the negative energy and V \u2217 soft(st) is the log-partition function. The soft Q-function Q\u2217soft and soft value function V \u2217 soft in equation 3 are given by\nQ\u2217soft(st,at) = r(st,at) + E(st+1,... )\u223c\u03c1\u03c0 [ \u221e\u2211 l=1 \u03b3l ( r(st+l,at+l) +H(at+l | st+l) )] , (4)\nV \u2217soft(st) = log \u222b A exp(Q\u2217soft(st,a \u2032)) da\u2032, (5)\nwhere the entropy in equation 4 is based on the optimal MaxEnt policy \u03c0\u2217MaxEnt. For sampling actions, we can disregard the log-partition function in equation 3 and directly exploit the proportionality relationship,\n\u03c0\u2217MaxEnt(at | st) \u221d exp (Q\u2217soft(st,at)), (6)\nbetween the policy and the soft Q-function, e.g., with Monte Carlo methods (Metropolis et al., 1953; Hastings, 1970; Duane et al., 1987). We drop the MaxEnt and soft subscripts for the remainder of the paper to avoid visual clutter. The soft Q-learning (Haarnoja et al., 2017) algorithm implements a soft Bellman contraction operator and in practice learns a model for sampling the complex Boltzmann distribution in equation 6. Our algorithm, PSQD, builds on soft Q-learning and combines it with \u03b5-lexicographic priorities as well as Q-decomposition from above."
        },
        {
            "heading": "3 PRIORITIZED SOFT Q-DECOMPOSITION",
            "text": "In this section, we explain our approach for solving \u03b5-lexicographic MORL problems. In Sec. 3.1, we explain our subtask transformation for modeling \u03b5-lexicographic priorities and show how it scalarizes \u03b5-lexicographic MORL problems. In Sec. 3.2, we derive our decomposed learning algorithm, PSQD, for \u03b5-lexicographic MORL problems and describe a practical version of this algorithm for continuous spaces in Sec. 3.3."
        },
        {
            "heading": "3.1 SUBTASK TRANSFORMATION FOR LEXICOGRAPHIC PRIORITIES IN MAXENT RL",
            "text": "Our goal is to learn a MaxEnt (arbiter) policy \u03c0 (a | s) \u221d exp(Q (s,a)) for the \u03b5-lexicographic MORL problem that fulfills the constraint in equation 1. However, instead of explicitly representing the sets \u03a0i, we focus on the action space and define a state-based version of the \u03b5-optimality constraint on the policy \u03c0 using the on-arbiter subtask Q-functions Qi\nmax a\u2032\u2208A\nQi(s,a \u2032)\u2212Qi(s,a) \u2264 \u03b5i,\u2200a \u223c \u03c0 , \u2200s \u2208 S,\u2200i \u2208 {1, . . . , n\u2212 1}. (7)\nThis restricts \u03c0 in each state to actions that are close to the optimal for each subtask i < n. As discussed in Sec. 2.1, summing up subtask Q-functions and using the proportionality relationship in equation 6 to directly define an arbiter policy \u03c0\u03a3 will in general not satisfy the \u03b5-lexicographic priority constraints in equation 7. Therefore, we propose a transformation of the subtasks and their Q-functions which provides that the constraints are satisfied when the transformed subtask Q-functions are summed up to Q . Our transformation relies on a constraint indicator function ci : S \u00d7A \u2192 {0, 1} that shows whether an action a in state s is permitted by the priority constraint for subtasks i,\nci(s,a) =\n{ 1, maxa\u2032\u2208AQi(s,a\n\u2032)\u2212Qi(s,a) \u2264 \u03b5i 0, otherwise . (8)\nWe refer to the set of permitted actions by subtask i in state s, A i(s) = {a \u2208 A | ci(s,a) = 1}, as the action indifference space of subtask i. The intuition is that subtask i is indifferent as to which \u03b5optimal action in A i(s) is selected because they are all close enough to optimal. Analogously, the intersection A (s) = \u2229n\u22121i=1 A i(s) is the global action indifference space and A\u0304 (s) = A\\A (s) is the set of forbidden actions. As mentioned above, keeping A i(s) large is crucial because it provides more actions for optimizing the arbiter policy.\nUsing Qi and ci as defined above, we can now write down the Q-function of the \u03b5-lexicographic MORL problem, which shows that constraint-violating actions are forbidden by their Q-values,\nQ (s,a) = n\u22121\u2211 i=1 ln(ci(s,a)) +Qn(s,a), (9)\nwhere we use ln(1) = 0 and limx\u21920+ ln(x) = \u2212\u221e. Here, the ln(ci) terms are the transformed Qfunctions for subtasks i < n in the view of Q-decomposition. Moreover, Q is equal to Qn for all a \u2208 A (s) andQ (s,a) = \u2212\u221e otherwise. Our MaxEnt arbiter policy \u03c0 (a | s) \u221d exp(Q (s,a)) has a product structure,\n\u03c0 (a | s) \u221d exp ( n\u22121\u2211 i=1 ln(ci(s,a)) +Qn(s,a) ) = ( n\u22121\u220f i=1 ci(s,a) ) expQn(s,a), (10)\nconfirming that \u03c0 has likelihood 0 for actions outside the global indifference space whilst selecting actions inside the global indifference spaces proportional to expQn.\nA close inspection of equation 9 shows that the corresponding transformed subtask reward functions are r i(s,a) = ln(ci(s,a)) and r n = rn, further revealing r = \u2211n i=1 r i as the scalar reward function of the \u03b5-lexicographic MORL problem. For a detailed derivation, see supplementary material in Sec. A. The Q-decomposition view suggests a decomposed learning algorithm with the transformed rewards r i. However, because the transformed rewards r i are defined as the transformed Q-functions Q i, our decomposed algorithm learns Qi and obtains Q i by first computing ci for equation 9. Furthermore, Qi are learned on-policy for the arbiter, to avoid illusion of control in the subtasks (Russell & Zimdars, 2003; Laroche et al., 2017; van Seijen et al., 2017)."
        },
        {
            "heading": "3.2 INCREMENTAL AND DECOMPOSED LEARNING WITH LEXICOGRAPHIC PRIORITIES",
            "text": "We want to exploit the structure of the Q-function Q and policy \u03c0 of the \u03b5-lexicographic MORL problem as described in Sec. 3.1 to define an incremental and decomposed learning approach that can reuse and adapt subtask solutions. Our algorithm incrementally includes more subtasks ordered by their priority: In the first step, we learn subtask 1 (with highest priority) without any restrictions. In the following steps, we learn subtask 1 < i \u2264 n restricted to the action indifference spaces of subtasks with higher priority than i and reuse the results of higher-priority subtasks, i.e. Q1 to Qi\u22121 for the components of Q . Subtasks with priority lower than i do not affect the learning of subtask i. In this and the following sections, we describe the process generically for the last subtask i = n because it aligns with the notation in equation 9.\nIn the Q-decomposition view, we are optimizing the arbiter policy \u03c0 with Q-function Q for the MORL problem with (transformed) rewards r 1, . . . , r n\u22121 and we are given on-arbiter-policy estimates Q\u03c0 i for subtasks 1 \u2264 i < n. As seen in equation 9, our means for improving the arbiter policy \u03c0 is maximizing the term Qn for subtask n. For this, we perform a policy evaluation step under the arbiter policy to get a new estimate of the subtask Q-functionQ\u03c0 n , which also changes our estimate of the global arbiter Q-function Q \u2014we call this arbiter policy evaluation. Next, we perform a policy improvement step by softly maximizing Q\u03c0 n , which again also changes Q \u2014we call this arbiter policy improvement. This algorithm closely resembles the Q-decomposition algorithm in Sec. 2.1 because Qn is updated on-policy with the global arbiter policy \u03c0 . We theoretically analyze a subtask view of this algorithm below and provide an additional arbiter view analysis and interpretation of this algorithm in supplementary material Sec. B.\nSubtask view theory. In the subtask view, the algorithm optimizes subtask n with soft Q-learning in an MDPM n that is likeM in Sec. 2 but uses scalar reward rn and has the action space restricted to the global action indifference space, A (s), in every state. Therefore, the action space ofM n satisfies the lexicographic priority constraints by construction. This allows us to perform off-policy soft Q-learning inM n to directly obtain Q\u2217 n, while still respecting all priority constraints. We do this with an off-policy, soft Bellman backup operator T\nT Q(s,a) , r(s,a) + \u03b3Es\u2032\u223cp [ log \u222b A (s\u2032) exp ( Q(s\u2032,a\u2032) ) da\u2032\ufe38 \ufe37\ufe37 \ufe38\nV (s\u2032)\n] , (11)\nwhich is the update step from soft Q-learning (Haarnoja et al., 2017) and where a \u2208 A (s). Theorem 3.1 (Prioritized Soft Q-learning). Consider the soft Bellman backup operator T , and an initial mapping Q0 : S \u00d7 A \u2192 R with |A | < \u221e and define Ql+1 = T Ql, then the sequence of Ql converges to Q\u2217 , the soft Q-value of the optimal arbiter policy \u03c0 \u2217 , as l\u2192\u221e.\nProof. Proof and detailed derivation in supplementary material Sec. F.4.\nWhile the described backup operator T yields the optimal global solution in tabular settings, it is intractable in large and continuous state and action spaces. In the next section, we convert T and Theorem 3.1 into a stochastic optimization problem that can be solved approximately via parametric function approximation."
        },
        {
            "heading": "3.3 PRACTICAL LEARNING AND ADAPTATION ALGORITHM FOR CONTINUOUS SPACES",
            "text": "For PSQD, we implement the subtask view described in Sec. 3.2, building on the soft Q-learning (SQL) algorithm (Haarnoja et al., 2017). Learning subtask n in the MDP M n requires an efficient way for sampling in the global action indifference space A (s), which becomes increasingly difficult in higher dimensions due to the curse of dimensionality. However, we can exploit the incrementally learned subtask policies \u03c0i(a | s) \u221d expQi(a, s) to efficiently sample in subtask action indifference spaces because the high-probability regions of subtask policies are the subtask indifference spaces. For rejecting action samples that are outside of the global action indifference space, a /\u2208 A (s), we evaluate the constraint indicator functions ci in equation 8. The remaining action samples are used in importance sampling proportionally to expQn(a, s).\nTechnically, we learn \u03c0n andQn for the subtask n by integrating the sampling process from above in SQL, which corresponds to performing soft Q-learning inM n. We model the subtask Q-function by a DNN Qn with parameters \u03b8 and minimizes the temporal-difference loss\nJQ(\u03b8) = Est,at\u223cD\n[ 1\n2\n( Q\u03b8n(st,at)\u2212 rn(st,at) + \u03b3Est+1\u223cp [ V \u03b8\u0304n (st+1) ])2] , (12)\nwhere \u03b8\u0304 is a target parameter obtained as exponentially moving average of \u03b8 and V \u03b8\u0304n is the empirical approximation of equation 5. The subtask policy is represented by a state-conditioned, stochastic DNN fn with parameters \u03c6, such that at = f\u03c6n (\u03b6t, st), where \u03b6t represents noise from an arbitrary distribution. The parameter \u03c6 is updated by minimizing the (empirical approximation of the) Kullback-Leibler divergence between f\u03c6n and the optimal MaxEnt arbiter policy in equation 3:\nJ\u03c0(\u03c6) = DKL ( f\u03c6n (\u00b7|st, \u03b6t) \u2223\u2223\u2223\u2223\u2223\u2223 exp (Q\u03b8 (st, \u00b7)\u2212 V \u03b8 (st))). (13) The objectives J\u03c0(\u03c6) and JQ(\u03b8) can be computed offline, given a replay buffer of saved transitions. Thus, PSQD can optimize the n-th subtask offline, for example reusing the transitions collected during independent and isolated training for subtask n in the subsequent adaptation step for subtask n as part of the \u03b5-lexicographic MORL problem, as detailed in supplementary material C. A pictographic overview of our method as well as pseudocode can be found in supplementary material D."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We evaluate PSQD in an instructive 2D navigation environment and a high-dimensional control problem (details in Sec. G in the supplementary material). In Sec. 4.1 and Sec. 4.2, we qualitatively analyze zero-shot and offline adaptation results, respectively. In Sec. 4.3, we empirically compare our method with a number of baseline methods. Finally, in Sec. 4.4, we demonstrate PSQD\u2019s efficacy in a high-dimensional, continuous control environment. For all experiments, we first learn the (untransformed) subtasks individually such that we have access to DNN approximations of Q\u2217i and \u03c0\u2217i for reuse and adaptation. We use the hat symbol, e.g., \u03c0\u0302, to denote DNN approximations."
        },
        {
            "heading": "4.1 ZERO-SHOT REUSE OF PREVIOUSLY LEARNED SUBTASKS SOLUTIONS",
            "text": "In this experiment, we demonstrate that our approach provides solutions that satisfy lexicographic priorities even in the zero-shot setting without interacting with the new environment. We design a 2D navigation environment (see Fig. 1a), where the agent is initialized at random positions and has to reach the goal area at the top while avoiding a \u2229-shaped obstacle. For the zero-shot solution, we simply apply our transformation (Sec. 3.1) to the already learned subtask Q-functions Q\u0302\u22171, Q\u0302 \u2217 2 to obtain the Q-function Q1 2 = ln(c1) + Q\u0302\u22172. The modular structure of Q1 2 allows us to inspect the components of the zero-shot solution: Fig. 1b reveals that Q\u0302\u22171 learns the obstacle avoidance subtask with large negative values inside the obstacle and inspecting the ln(c1) component shows our transformation and how it implements the priority constraint. In Fig. 1c, we plot ln(c1((s,a)), \u2200a \u2208 A, with the agent placed at the red dot in Fig. 1a to show the action indifference space at that state. Our transformation excludes actions that would lead to collisions, allowing us to visually infer the agent\u2019s behavior. This is in stark contrast to prior zero-shot methods, e.g. (Haarnoja et al., 2018a), where the behavior of the zero-shot solution is unpredictable and potentially arbitrary sub-optimal, as noted by Hunt et al. (2019). In Fig. 1d, we visualize the policy \u03c01 2 \u221d Q1 2 with sample trajectories and project ln(c1) onto the image to illustrate the global action indifference space. Our zero-shot solution never collides with the obstacle due to the priority constraint without any training on the lexicographic MORL problem. However, our zero-shot is suboptimal for the lexicographic MORL problem, because it sometimes gets trapped and fails to reach the goal for starting positions in the middle. In Sec. G (supplementary material), we show more detailed zero-shot results."
        },
        {
            "heading": "4.2 OFFLINE ADAPTATION AND DATA REUSE",
            "text": "In this experiment, we demonstrate that PSQD can adapt previously learned subtask solutions to the lexicographic MORL problem using only the training data from the previous, isolated learning process without environment interaction. After adapting Q\u0302\u22172 with the offline data, Q\u0302 \u03c0 2 no longer\nreflects the greedy policy \u03c0\u22172 that gets the zero-shot solution stuck, but instead reflects the arbiter agent \u03c0\u2217 and accounts for the long-term outcome. Comparing Q\u0302\n\u2217 2 in Fig. 2a with Q\u0302 \u03c0 2 in Fig. 2b\nshows that the adaptation process leads to lower values inside the \u2229-shape such that our solution escapes and avoids that region. This optimal behavior is illustrated with sample traces in Fig. 2c. In Fig. 2d we compare the mean return of the zero-shot, offline, and online adapted agents and an RRT (LaValle & Kuffner Jr, 2001; Karaman & Frazzoli, 2010) oracle. This shows that the zero-shot agent, which gets stuck, performs considerably worse than the adapted agents, which instead are on par with the RRT oracle. We continue this analysis for the more complex tasks r1 2 3 and r1 3 2 in supplementary material Sec. G."
        },
        {
            "heading": "4.3 EMPIRICAL COMPARISON AGAINST BASELINES",
            "text": "In this experiment, we compare PSQD to a set of five baseline methods which all try to implement lexicographic priorities in MORL problems. As, to the best of our knowledge, PSQD is the first method for solving lexicographic MORL problems with continuous action spaces, we compare against obvious extensions of established RL algorithms and ablations of PSQD: Naive subtaskpriority implementation for SAC (Haarnoja et al., 2018b) and PPO (Schulman et al., 2017), a modified version of Q-Decomposition (Russell & Zimdars, 2003) for continuous action spaces that we call soft Q-decomposition (SQD), as well as the SQL composition method proposed by Haarnoja et al. (2018a). For the ablation of PSQD, instead of transforming subtasks, we introduce additional negative reward for priority constraint violations based on the constraint indicator functions ci. For SAC and PPO we naively implement subtask-priorities by augmenting the learning objective to additionally include the KL-divergence between the current and the previously learned higher-priority policy. SQD concurrently learns the on-arbiter-policy Q-functions for all subtasks. The SQL compo-\nsition (Haarnoja et al., 2018a) also learns solutions for all subtasks concurrently, however off-policy and thereby with illusion of control. See Sec. G.3 for more details on the baseline methods.\nFor SAC, PPO, and PSQD (incl. ablation), we first train all algorithms on the obstacle avoidance subtask 1, where all methods converge to an approximately optimal solution. Subsequently, we reuse the obtained Q1 and train each method on subtask 2, recording the returns for both subtasks separately in Fig. 3. PSQD is the only method that maintains zero return for subtask 1 (i.e. zero collisions) while learning for subtask 2, outperforming all other baselines. As seen in the right panel of Fig. 3, PSQD obtains less reward for subtask 2 than other baselines. However, this is because the priority constraint prevents PSQD from navigating through the obstacle, while some baselines accept obstacle collisions to reach the goal quicker. In contrast to PSQD, none of the baseline methods converge to a solution that satisfies the lexicographic priorities and maintains the optimal performance in the top-priority collision avoidance subtask that they initially achieved."
        },
        {
            "heading": "4.4 HIGH-DIMENSIONAL CONTROL",
            "text": "In this experiment, we demonstrate that PSQD scales well to high-dimensional action spaces while maintaining subtask priorities, unlike traditionally composed multi-objective agents. We simulated an Franka Emika Panda joint-control task with a 9-dimensional action space (Fig 4, left), where the high-priority subtasks wants to avoid the red cube while the low-priority subtask want to reach the green sphere with the end-effector. We start with the zero-shot Q-function Q1 2 = ln(c1) + Q\u0302\u22172 and sample from \u03c0 as described in Sec. 3.3. We subsequently adapt the zero-shot solution to obtain Q\u22171 2 = ln(c1) + Q\u0302 \u03c0 2 . We also include the SQL-Comp. method (Haarnoja et al., 2018a) as a baseline, by composing Q1+2 = Q\u0302\u22171 + Q\u0302 \u2217 2. Representative trajectories from our adapted solution and the baseline are shown in Fig. 4 (left). Starting from t = 0 in the leftmost panel, our adapted agent moves around the obstacle. In contrast, the baseline violates the priority and moves through the forbidden part of the workspace. More quantitatively, we show mean returns for the different agents in Fig. 4 (right). Our solution avoids the volume even in the zero-shot setting and is improved considerably through the adaptation process, while the baseline induces high costs in r1 because it moves through the forbidden part of the workspace. We note that Haarnoja et al. (2018a) do not claim that their method solves lexicographic MORL problems, we use it to illustrate that priorities are generally difficult to express in traditional multi-objective tasks."
        },
        {
            "heading": "5 RELATED WORK",
            "text": "Task priorities for continuous action-spaces. To the best of our knowledge, PSQD is the first method to solve general lexicographic MORL tasks with continuous action-spaces. The following prior works consider task priorities in RL: Yang et al. (2021) combine null-space control and RL by constraining policy search to the null-space of higher priority constraints. However, Yang et al. (2021)\u2019s method is a special case of PSQD since it requires access to the task-space Jacobian, while PSQD exploits given Q-functions. Skalse et al. (2022); Zhang et al. (2022); Li & Czarnecki (2019) also model lexicographic task priorities, however only in discrete settings and without revealing the reward function of the lexicographic task. PSQD generalizes these methods to continuous action-\nspaces and maximizes a known reward function. Furthermore, none of these works emphasize composition and reuse of subtask solutions, which is central to our method.\nComposition for knowledge transfer. Composition for transfer between tasks in the MaxEnt framework has initially been demonstrated by Haarnoja et al. (2018a), however, only in a zero-shot setting without providing a learning algorithm to improve the composed agent and without priorities. Hunt et al. (2019) build upon Haarnoja et al. (2018a) and propose a learning algorithm that learns the divergence between the subtask Q-functions and obtains the optimal solution to convex task compositions. Thus, similar to our method, these works facilitate knowledge transfer between tasks via composition, however, they offer no way for modeling strict task priorities.\nTask priorities for safety. Task priorities can be used to implement safety. Prior works mostly implement (safety-) constrained RL via constrained MDPs (CMDPs) (Altman, 1999), where policy search is constrained by an unordered set of cost functions. CMDPs can be solved by optimizing Lagrangian objectives and dual gradient descent techniques Boyd et al. (2004). This way, (Ha et al., 2021; Tessler et al., 2019; Chow et al., 2017) focus on CMDPs with one handcrafted cost function, while (Achiam et al., 2017; Liang et al., 2018) provide methods for solving CMPDs with multiple constraints. While these methods yield constraint-satisfying agents, unlike our method, they do not facilitate reuse and adaptation via composition, since they optimize monolithic policies from scratch. An exception w.r.t reuse is safety critic (Srinivasan et al., 2020), which pre-trains a transferable estimator of expected future safety incidents. However, safety-critic can only incorporate one such critic and thus only transfer knowledge from one pre-training task, while our method can exploit any number of pre-trained Q-functions."
        },
        {
            "heading": "6 LIMITATIONS",
            "text": "Our method depends on the manual selection of \u03b51, . . . , \u03b5n\u22121 thresholds. These scalars are on the scale of subtask Q-functions Qi, which might be hard to estimate when function approximators are employed and depend on domain-specific reward functions, user preferences, and task semantics. There are, however, a number of informed ways for finding adequate \u03b5i scalars. In practice, one can analyze the Q-function in key-states and select \u03b5i such that undesired actions are excluded. This works even when the action space is of high dimensionality, since approximate Q-function mean, min, max, and percentiles can be computed via sampling."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "The main contribution of this paper is a principled framework for solving lexicographic MORL problems with continuous action spaces in a decomposed fashion. In this framework, we can zeroshot interpretable agents that respect task priority constraints in low- and high-dimensional settings. Our learning algorithm, PSQD, facilitates reuse of subtask solutions by adapting them to solve the lexicographic MORL problem optimally."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation."
        },
        {
            "heading": "A LEXICOGRAPHIC MULTI-OBJECTIVE TASKS",
            "text": "Here, we describe in detail how we model lexicographic task-priority constraints as scalarizable multi-objective RL tasks. To recap, we have multiple reward functions r1, . . . , rn, and we want to find a global arbiter policy \u03c0 that maximizes rn, while behaving almost optimal w.r.t. all higher priority tasks r1, . . . , rn\u22121. This is expressed with the constraint\nmax a\u2032\u2208A\nQi(s,a \u2032)\u2212Qi(s,a) \u2264 \u03b5i,\u2200a \u223c \u03c0, \u2200s \u2208 S,\u2200i \u2208 {1, . . . , n\u2212 1} (14)\non the performance measure J(\u03c0), which in this case is the Q-value. We note again the indicator functions\nci(s,a) =\n{ 1, maxa\u2032\u2208AQi(s,a\n\u2032)\u2212Qi(s,a) \u2264 \u03b5i 0, otherwise , (15)\nand that \u220fn\u22121 i=1 ci(s,a) = 1 when an action a is allowed in state s according to each of the higher priority constraints (and thus in the global action indifference space A (s)). The key idea now is to transform the higher-priority Q-functions Qi into new reward functions r i by applying the natural logarithm to the constraint indicator functions, such that r i(s,a) = ln(ci(s,a)). Based on these transformed Q-functions and new rewards, we now define a multi-objective, vector-valued reward function that reflects strict task priorities\nr (s,a) =  r 1(s,a) r 2(s,a)\n... r n\u22121(s,a) rn(s,a)\n =  ln(c1(s,a)) ln(c2(s,a))\n... ln(cn\u22121(s,a))\nrn(s,a)  . (16) We define the global reward function that the arbiter maximizes as sum of transformed rewards\nr (s,a) = n\u2211 i=1 [r(s,a)]i = n\u22121\u2211 i=1\nln ci(s,a)\ufe38 \ufe37\ufe37 \ufe38 constraint indication +rn(s,a), (17)\nsuch that we are in the Q-decomposition setting Russell & Zimdars (2003). This allows us to decompose the learning problem into n subtask Q-functions Q1, . . . , Qn and one global policy \u03c0 . The global Q-function Q for the prioritized task r corresponds to the sum of subtask Q-functions, Q = \u2211n\u22121 i=1 Q i + Qn, where the first n \u2212 1 higher-priority Q-functions are for the transformed rewards, while the n-th Q-function is for the ordinary reward function rn. Because of MaxEnt RL the global arbiter policy \u03c0 satisfies the proportional relationship \u03c0 (a|s) \u221d exp(Q (s,a)), as described in Sec. 2.2.\nThe global reward function in equation 17 allows us to infer certain properties ofQ and \u03c0 . Firstly, by noting that ln(1) = 0 we see that the constraint indication summation in equation 17 is zero when action a is allowed according to all higher priority tasks and in the global action indifference space A (s). This means that r (s,a) = rn(s,a),\u2200s \u2208 S,\u2200a \u2208 A (s), i.e. maximizing the global task corresponds to maximizing the lowest priority tasks in the global indifference space, which is a property we exploit extensively.\nNext, by defining limx\u21920+ ln(x) = \u2212\u221e, we see that r (s,a) = \u2212\u221e when any number of higher priority constraints are violated by a in s. Furthermore, by noting that \u2212\u221e \u00b1 R = \u2212\u221e and \u2212\u221e \u00b7 R\u22650 = \u2212\u221e, we see that Q (s,a) = \u2212\u221e,\u2200s \u2208 S,\u2200a \u2208 A\u0304 (s), i.e. the value of any constraint-violating action is \u2212\u221e under the global task r . Importantly, due to the proportional relationship \u03c0 (a|s) \u221d exp(Q (s,a)), it follows that such constraint-violating actions have zero probability under our arbiter policies. This also means that the expected future constraint-violation value of arbiter policies is zero, since the arbiter policy can not select constraint-violating actions in the future. With this in mind, for the global Q-function of arbiter policies, we see that\nQ (s,a) = n\u2211 i=1 Q i(s,a)\n= n\u22121\u2211 i=1 Q i(s,a) +Qn(s,a)\n= n\u22121\u2211 i=1 r i(s,a) + rn(s,a) + \u03b3Est+1\u223cp,at+1\u223c\u03c1\u03c0 [ 0 due to \u03c0 \ufe37 \ufe38\ufe38 \ufe37 Q i(st+1,at+1) +Qn(st+1,at+1)]\n= n\u22121\u2211 i=1 r i(s,a) + rn(s,a) + \u03b3Est+1\u223cp,at+1\u223c\u03c1\u03c0 [ Qn(st+1,at+1)]\n= n\u22121\u2211 i=1 r i(s,a) +Qn(s,a) = n\u22121\u2211 i=1 ln ci(s,a) +Qn(s,a).\n(18)\nThis shows that we only have to learn the n-th subtask Q-function for r (s,a) during the adaptation step, because the transformed Q-values of all n\u2212 1 higher-priority subtask Q-functions are know by construction.\nLastly, since we are using the MaxEnt framework we have the proportional relationship \u03c0 (a | s) \u221d exp ( Q (s,a) ) = exp\n( n\u22121\u2211 i=1 ln(ci(s,a)) +Qn(s,a) )\n= ( n\u22121\u220f i=1 ci(s,a) ) expQn(s,a)\n(19)\nand can see that\n\u03c0 (a | s) \u221d\n{ expQn(s,a) if \u220fn\u22121 i=1 ci(s,a) = 1,\n0 otherwise. (20)\nThis shows that \u03c0 softly maximizes rn(s,a) in the global action indifference space A (s), as the policy is proportional to exp(Qn(s,a)) and has zero probability for constraint-violating actions."
        },
        {
            "heading": "B ARBITER VIEW THEORY",
            "text": "Here we provide an analysis of our algorithm from the perspective of the arbiter agent for the lexicographic MORL task. In this view, the algorithm optimizes the arbiter policy with policy evaluation and policy improvement steps in a transformed MORL problem using on-policy updates for subtask n, like Q-decomposition. Now we show that this learning scheme converges by considering a fixed arbiter policy \u03c0 with the on-policy soft Bellman backup operator T \u03c0 defined as\nT \u03c0 Qn(st,at) , rn(st,at) + \u03b3Est+1\u223cp[V \u03c0 n (st+1)], (21)\nwith V \u03c0 n (st) = Eat\u223c\u03c0 [Qn(st,at)\u2212 log(\u03c0 (at | st))\ufe38 \ufe37\ufe37 \ufe38\nH\n], (22)\nwhere V \u03c0 n (st) is the soft, on-policy value function (Haarnoja et al., 2018b). Theorem B.1 (Arbiter policy evaluation). Consider the soft Bellman backup operator T \u03c0 and an initial mapping Q0n : S \u00d7 A \u2192 R with |A| < \u221e and define Ql+1n = T \u03c0 Qln. The sequence of Qln converges to Q\u03c0 n , the soft Q-value of \u03c0 , as\u2192\u221e.\nProof. See supplementary material Sec. F.1.\nTheorem B.2 (Arbiter policy improvement). Given an arbiter policy \u03c0 , define a new arbiter policy as \u03c0\u2032 (\u00b7|s) \u221d exp(Q \u03c0 (s, \u00b7)), \u2200s \u2208 S. Then Q \u03c0\u2032 (\u00b7, s) \u2265 Q \u03c0 (\u00b7, s), \u2200s,a.\nProof. See supplementary material Sec. F.2.\nCorollary B.3 (Arbiter policy iteration). Let \u03c00 be the initial arbiter policy and define \u03c0 l+1 (\u00b7 | s) \u221d exp(Q\u03c0 l (s, \u00b7)). Assume |A| <\u221e and that rn is bounded. Then Q \u03c0l improves monotonically and \u03c0l converges to \u03c0 \u2217 .\nProof. See supplementary material Sec. F.3."
        },
        {
            "heading": "C OFFLINE ADAPTATION",
            "text": "Our algorithm, PSQD, relies on the off-policy soft Bellman backup operator T in Eq. equation 11 and the objectives JQ(\u03b8) in Eq. equation 12 and J\u03c0(\u03c6) in Eq. equation 13 which can be computed without online policy rollouts. We can therefore adapt previously learned subtask solutions Qi, \u03c0i offline with retained training data for subtask i, (st,at, rt, st+1) \u2208 Di. However, since \u03c0i was unconstrained during pre-training, Di likely contains constraint-violating transitions that are impossible in M i. To account for this, when sampling a transition (st,at, rt, st+1) \u223c Di during adaptation of the pre-trained Qi, \u03c0i, we can check whether at \u2208 A (st) and discard all constraint violating transitions. This is like sampling from a new dataset D i that contains only transitions from M i. Depending on how well D i covers M i, we can learn the optimal solution for the global task entirely offline, without requiring additional environment interaction. This makes our approach maximally sample-efficient for learning complex, lexicographic tasks by re-using data retained from subtask pre-training.\nto solve each subtask, we refer to this as the subtask pre-training step. In the middle box, the subtask agents are combined into the lexicographic arbiter agent. The subtask adaptation loop that we implement in practice, as described in Sec. 3.2 and 3.3, is denoted by , while the arbiter learning perspective, described in App. B, is denoted by ."
        },
        {
            "heading": "D PSQD ALGORITHM DETAILS",
            "text": "Here, we provide additional details on our method. A high-level overview of our framework and learning algorithm is given in Fig. 5. As can be seen, we begin by pre-training agents on the untransformed, original subtasks r1, . . . , rn. During the pre-training step, the agents are learning in isolation and greedily optimize their respective reward signals. Different learning algorithms could be used for pre-training the subtask agents, but in our case, the pre-training step corresponds to running n instance of Soft Q-Learning Haarnoja et al. (2017). The pseudo-code for pre-training is given in Alg. 1.\nAlgorithm 1 Subtask pre-training with SQL Require: Subtask reward functions r1, . . . , rn\nSamplers\u2190 {} Q-functions\u2190 {} Buffers\u2190 {} for i \u2208 {1, . . . , n} do . Iterate (concurrently) over subtasks {1, . . . , n}\nInitialize \u03c6i, \u03b8i, \u03b8\u0304i,Di . Initialize subtask network parameters and empty buffer for t \u2208 {1, . . . , T} do . T total steps\nOnline interaction \u03b6t \u223c P . Random noise for ASVGD sampling network at \u223c f\u03c6ii (\u03b6t, st) . Unconstrained action selection using sampling network st+1 \u223c p(st+1 | st,at) D \u222aD \u2190 {st,at, ri(st,at), st+1} . Store transition and subtask reward Update networks Update \u03b8i . SQL Q-function update Update \u03c6i . SQL sampling network update \u03b8\u0304i \u2190 \u03c4\u03b8i + (1\u2212 \u03c4)\u03b8\u0304i . Polyak target network update\nend for Samplers\u2190 Samplers \u222a{\u03c6i} Q-functions\u2190 Q-functions \u222a{\u03b8i} Buffers\u2190 Buffers \u222a{Di}\nend for return Subtask sampling networks {\u03c61, . . . , \u03c6n}, Q-functions {\u03b81 . . . \u03b8n}, and Buffers {D1, . . . ,Dn}\nAfter the SQL pre-training step in Alg 1, we have access to converged subtask Q-functions, sampling networks for those Q-functions, and populated replay buffers for each subtask ri. Next, we perform our adaption step where we finetune the pre-trained Q-functions and sampling networks under the consideration of the lexicographic constraint on higher-priority subtask performance. This corresponds to the subtask adaptation loop in Figure 5. Importantly, adapting the i-th subtask Qfunction and sampling network requires that all higher-priority subtasks components have already been adapted to the lexicographic setting because otherwise, their indifference spaces might not overlap (see also App. E). We therefore employ an iterative adaptation procedure where we start with the second highest task (the highest-priority task is unconstrained) and adapt it under consideration of the lexicographic constraint for the highest-priority task. Then, the third-highest-priority task can be adapted under consideration of the lexicographic constraint for the highest- and secondhighest-priority tasks, and so on.\nAlgorithm 2 Incremental PSQD subtask adaptation Require: Subtask reward functions r1, . . . , rn Require: Threshold scalars \u03b51, . . . , \u03b5n\u22121 Require: Pre-trained subtask sampling network parameters \u03c61, . . . , \u03c6n Require: Pre-trained subtask Q-function parameters \u03b81, . . . , \u03b8n Require: Populated subtask replay buffers D1, . . . ,Dn\nfor i \u2208 {2, . . . , n} do . Iterate over subtasks, starting with second-highest priority task D = D \u2212 {st,at, ri(st,at), st+1}\u2200at /\u2208 A . Discard constraint-violating transitions for t \u2208 {1, . . . , T} do . T total steps\nOptional online interaction // As described in Sec. 3.3, we perform importance sampling to sample from \u03c0 i, the // lexicographically constrained policy for the current subtask. The already adapted // \u03c61, . . . , \u03c6i\u22121 and the current \u03c6i sampling networks are used as proposal distributions, // with the unnormalized target density being given by Eq. 10. at \u223c \u03c0 i st+1 \u223c p(st+1 | st,at) D \u222aD \u2190 {st,at, ri(st,at), st+1} . Store transition and subtask reward\nUpdate networks Update \u03b8i using Eq. 12 Update \u03c6i using Eq. 13 \u03b8\u0304i \u2190 \u03c4\u03b8i + (1\u2212 \u03c4)\u03b8\u0304i . Polyak target network update\nend for end for return Adapted sampling network parameters \u03c6i and Q-function parameters \u03b8i\nNotice that the online adaptation step in Alg. 2 is optional, it is also possible to compute the network updates entirely offline, using the filtered versions of the subtask replay buffers from pre-training. The filtered versions of the replay buffer are obtained as described in App. C, where we ensure that no constrain-violating transitions are used during offline adaptation. If additional online interaction data should be collected from the environment, this can be done by sampling actions from the constrained policy for the current subtask."
        },
        {
            "heading": "E DISCUSSION ON SEMANTICALLY INCOMPATIBLE SUBTASKS",
            "text": "Here, we discuss an interesting edge case, namely, situations where subtasks are semantically incompatible. Let us consider an example where we wish to jointly optimize three subtasks: Subtask r1 rewards moving to the left, r2 = \u2212r1 rewards moving to the right, and r3 rewards moving to the top. Clearly, r1 and r2 are semantically incompatible, since r2 is the inverse of r1. We should first notice that such MORL tasks with semantically incompatible subtasks are inherently ill-posed, since no agent can, at the same time, behave optimally for both r1 and r2. Intuitively, an agent can either move to the left, or to the right, but it can not tend to both directions at the same time. We emphasize the semantic incompatibility, since, from the MORL algorithm\u2019s perspective, tasks can not be \u201cincompatible\u201d. Even when r2 = \u2212r1, maximizing the summed reward r1 + r2 is still valid, even though the optimal behavior for this task might not be aligned with the RL practitioner hoped to obtain.\nIn the general case, given semantically incompatible subtasks, a non-lexicographic MORL agent that maximizes r1 + r2 + r3 will likely fail to progress on either of the incompatible subtasks, as shown by (Russell & Zimdars, 2003). The behavior depends on subtask reward scale, the discount factor, and function approximation errors, which can be considerable since deep regression for Bellman optimality tends to overestimate state-action values Mnih et al. (2015). Therefore, MORL algorithms without subtask priorities are likely to produce unexpected and undesired behavior when subtasks are semantically incompatible.\nImportantly, if we instead optimize the lexicographic task r1 r2 r3, most of the above issues are alleviated: The lexicographic problem definition states clearly how the agent ought to behave, since, no matter how different the subtasks might be, near-optimal performance of high-priority tasks is required by definition. For the same reason, lexicographic tasks are also insensitive with respect to subtask reward scale and discount factors, since they are not optimizing a weighted combination of subtask rewards. In fact, lexicographic RL was introduced and motivated by (Ga\u0301bor et al., 1998) to precisely resolve situations with semantically incompatible subtasks, e.g. \u201cBuridan\u2019s Ass\u201d dilemma, which is a philosophical paradox that translates directly to conflicting subtasks in MORL.\nWhile lexicographic RL inherently resolves subtask incompatibility via its problem definition, we now clarify how our learning algorithm resolves these situations in practice. First, we notice that greedy subtask solutions obtained through SQL pre-training in Alg. 1 can indeed be \u201cincompatible\u201d. Pre-training independently on our exemplary tasks r1 and r2 = \u2212r1 would result in Q-functions Q\u22171 and Q\u22172 that assign high-value to inverse parts of the action space (moving left and moving right). Thus, the intersection of the indifference spaces given by Q\u22171 and Q \u2217 2 can be empty, which would imply an empty action space for lower-priority subtasks and an ill-posed learning problem.\nOur definition of the lexicographic constraint in Eq. 7, however, formulates the lexicographic constraints on the on-policy Q-functions for the constraint-respecting arbiter agent. Our learning and adaption procedure in Alg. 2 accounts for this since it sequentially adapts subtask solutions, starting with higher-priority subtasks. This means that in the first iteration, we adapt the solution of the subtask with second-highest priority to the lexicographic optimality constraint on the highest-priority subtasks. This changes the greedy subtask agent (i.e. its Q-function) Q\u22172 Q \u2217 2 to an agent that is no longer greedy with respect to its original subtask \u2013 the adapted agent instead solves r2 as well as possible, while respecting the lexicographic constraint. As described in App. A,Q\u2217 2 assigns\u2212\u221e to all actions that violate the lexicographic constraint, therefore, the high-value region and indifference space of Q\u2217 2 has to be inside the high-value region in Q \u2217 1, i.e. inside the indifference space of the higher-priority task r1. Thus, although Q\u22171 and Q \u2217 2 might be \u201cincompatible\u201d, the adapted Q \u2217 2 is indeed compatible with Q\u22171, even when r2 = \u2212r1. Since in our adaption procedure, lower-priority subtasks always use the already adapted subtask solutions for higher-priority tasks, the intersection of all higher-priority indifference spaces can never be empty.\nIn summary, lexicographic MORL tasks resolve incompatible subtasks by definition, since lowerpriority subtasks are constrained to solutions that are also near-optimal for all higher-priority subtasks. Our adaption procedure accounts for this by adapting subtasks sequentially, which ensures that the intersection of all higher-priority subtasks is never empty."
        },
        {
            "heading": "F PROOFS",
            "text": "In Sec. F.1 to Sec. F.3 we prove the \u201con-arbiter-policy\u201d view of our algorithm. We prove the subtask view of our algorithm in Sec. F.4. These proofs are based on and make use of the following observation. Due to the incremental nature of our learning algorithm, the n\u2212 1 higher-priority tasks have already been learned in an on-arbiter-policy fashion and are fixed during learning of the n subtask Q-function. Thus we have access to the transformed subtask Q-functions Q i for all higher priority tasks 1, . . . , n \u2212 1, which are stationary during learning of the n-th subtask Q-function. As a consequence, the arbiter policy \u03c0 , which we want to improve and for which we want to learn n-th subtask Q-function in an on-policy fashion, is already constrained to the global indifference space and can never select constraint-violating actions. Thus, we can rely on existing proofs for MaxEnt RL, since our arbiter policy is just a particular MaxEnt policy.\nF.1 ARBITER POLICY EVALUATION\nLemma F.1. Arbiter policy evaluation. Consider the soft Bellman backup operator T \u03c0 and an initial mapping Q0n : S \u00d7 A \u2192 R with |A| < \u221e and define Ql+1n = T \u03c0Qln. The sequence of Qln will converge to Q\u03c0n, the soft Q-value of \u03c0, as l\u2192\u221e.\nThe proof is straightforward based on the observation that the n-th subtask Q-function is simply a soft Q-function for the task rn, learned under the expectation of the global arbiter policy (Russell & Zimdars, 2003), which has zero-probability for constraint-violating actions. Thus, the proof is analogous to the one by Haarnoja et al. (2018b) except in our case \u03c0 is a constraint-respecting arbiter policy. We repeat the proof here only for completeness:\nProof. We define the on-policy, entropy augmented reward signal for soft Q-functions as r\u03c0n(st,at) , rn(st,at) + Est+1\u223cp[H(\u03c0 (\u00b7|st+1))] and rewrite T \u03c0 as\nQ\u03c0n(st,at)\u2190 r\u03c0n(st,at) + \u03b3Est+1\u223cp,at+1\u223c\u03c0[Q\u03c0n(st+1,at+1)]. (23) The standard convergence result for policy evaluation (Sutton & Barto, 2018) thus holds. The assumption |A| <\u221e is required to guarantee that the entropy augmented reward is bounded.\nF.2 ARBITER POLICY IMPROVEMENT\nWe want to show that we can obtain an arbiter policy \u03c0 that is better or as least as good as any other arbiter policy by softly maximizing the soft global Q-function. Since the arbiter policy maximizes the sum of all (transformed) subtask Q-functions we set r = r = \u2211n\u22121 j=1 r j + rn and Q \u03c0 soft =\nQ\u03c0 = \u2211n\u22121 j=1 Q j +Qn for the remainder of this section.\nSince we are concerned with finding an improved arbiter policy and since all arbiter policies have zero probability for selecting constraint-violating actions, the r j and Q j terms evaluate to zero and we can only improve Q\u03c0soft by increasing rn. Q \u03c0 soft still correctly assigns a value of \u2212\u221e to actions that violate constraints, however, this value is known through the constraint on already optimal higher-priority Q-functions and not backed up while learning task rn. In practice, since arbiter policies can never select constraint-violating actions, the soft policy improvement theorem by Haarnoja et al. (2017) also holds for us since we are softly maximizing the ordinary task rn with a particular soft policy. Arbiter policy improvement for the global tasks thus degrades to maximizing the n-th task while respecting priority constraints. In the following, we use the short-hand rt = r(st,at) for a reward at time t for more compact notation. Theorem F.2. Arbiter policy improvement. Given an arbiter policy \u03c0, define a new arbiter policy as \u03c0\u2032(\u00b7|s) \u221d exp(Q\u03c0 (s, \u00b7)), \u2200s \u2208 S. Then \u03c0\u2032(\u00b7|s) \u2265 \u03c0(\u00b7|s), \u2200s,a.\nProof. The soft policy maximization objective J(\u03c0) , \u2211 t E(st,at)\u223c\u03c1\u03c0 [Q \u03c0 soft(st,at) +H(\u03c0(\u00b7|st))] (24)\ncan be improved either by increasing the soft Q-value or the entropy term:\nE(st,at)\u223c\u03c1\u03c0 [Q \u03c0 soft(st,at)] +H(\u03c0(\u00b7|st)) \u2264 E(st,at)\u223c\u03c1\u03c0\u2032 [Q \u03c0 soft(st,at)] +H(\u03c0\u2032(\u00b7|st)). (25)\nRecalling the definition of the soft Q-function\nQ\u03c0soft(st,at) , r0(st,at) + E(st+1,at+1,... )\u223c\u03c1\u03c0 [ \u221e\u2211 l=1 \u03b3l(r(st+l,at+l) +H(at+l | st+l)) ] , (26)\nthe proof for the (soft) policy improvement theorem now simply expands the right side of equation 25 with one-step look-aheads until one obtains Q\u03c0 \u2032\nsoft: Q\u03c0soft(s0,a0) = r0 + Es1 [ \u03b3 ( H(\u03c0(\u00b7|s1)) + Ea1\u223c\u03c0[Q\u03c0soft(s1,a1)] )] \u2264 r0 + Es1 [ \u03b3 ( H(\u03c0\u2032(\u00b7|s1)) + Ea1\u223c\u03c0\u2032 [Q\u03c0soft(s1,a1)]\n)] = r0 + Es1 [ \u03b3 ( H(\u03c0\u2032(\u00b7|s1)) + r1 )] + \u03b32Es2 [ H(\u03c0(\u00b7|s2)) + Ea2\u223c\u03c0[Q\u03c0soft(s2,a2)]\n] \u2264 r0 + Es1 [ \u03b3 ( H(\u03c0\u2032(\u00b7|s1)) + r1 )] + \u03b32Es2 [ H(\u03c0\u2032(\u00b7|s2)) + Ea2\u223c\u03c0\u2032 [Q\u03c0soft(s2,a2)]\n] = r0 + E(s1,s2)\u223cp,(a1,a2)\u223c\u03c0\u2032 [ \u03b3 ( H(\u03c0\u2032(\u00b7|s1)) + r1 )] + \u03b32H(\u03c0\u2032(\u00b7|s2)) +Q\u03c0soft(s2,a2)\n...\n\u2264 r0 + E(st+1,at+1,... )\u223c\u03c1\u03c0\u2032 [ \u221e\u2211 l=1 \u03b3l(rt+l +H(\u03c0\u2032(\u00b7 | st+l)) ] = Q\u03c0 \u2032\nsoft(s0,a0) (27)\nF.3 ARBITER POLICY ITERATION\nCorollary F.3 (Arbiter policy iteration). Let \u03c00 be the initial arbiter policy and define \u03c0 l+1 (\u00b7 | s) \u221d exp(Q \u03c0l (s, \u00b7)). Assume |A| < \u221e and that rn is bounded. Then Q \u03c0l improves monotonically and \u03c0l converges to \u03c0 \u2217 .\nProof. Let \u03c0l be the arbiter policy at iteration l. According to Theorem B.2, the sequence Q \u03c0l improves monotonically. Since arbiter policies only select actions inside the global action indifference space, the ln(ci) components of Q evaluate to zero and since rn is assumed to be bounded and since the entropy term is bounded by the |A| < \u221e assumption, the sequences converges to some \u03c0\u2217 . \u03c0 \u2217 is optimal, since according to Theorem B.2 we must have Q \u03c0\u2217 > Q \u03c0 ,\u2200\u03c0 6= \u03c0\u2217 .\nF.4 PRIORITIZED SOFT Q-LEARNING\nWe now prove the local view on our algorithm, by showing that we can define an off-policy backup operator that accurately reflects the global arbiter policy, thus connecting the global on-policy and local off-policy views on our algorithm. For this, we first note the soft Bellman optimality equations that have already been proven by Haarnoja et al. (2017)\nQ\u2217soft(st,at) = r(st,at) + \u03b3Est+1\u223cp[V \u2217soft(st+1)], (28)\nwith\nV \u2217soft(st) = log \u222b A\nexp\ufe38 \ufe37\ufe37 \ufe38 softmax\n( 1 \u03b1 Q\u2217soft(st,a \u2032) ) da\u2032, (29)\nwhere the softmax operator represents a policy \u03c0softmax that softly maximizes Q\u2217. Thus, we can also write\nQ\u2217soft(st,at) = r(st,at) + \u03b3Est+1\u223cp,at+1\u223c\u03c0softmax [Q\u2217soft(st+1,at+1)], (30)\nwhich shows that soft Q-learning is equivalent to Q-learning with softmax instead of greedy max action selection.\nWe now show that we can learn the optimal n-th subtask Q-function under the arbiter policy in an off-policy, soft Q-learning fashion. In principal, this simply replaces the softmax policy \u03c0softmax with the the arbiter policy \u03c0 in equation 30. However, the arbiter policy does not softly maximize the n-th subtask Q-function like the locally greedy softmax policy in equation 29 would do. Instead, it softly maximizes the sum of subtask Q-functions Q (s,a) = \u2211n\u22121 j=1 Q j(s,a) + Qn(s,a). To show that we can approximate the global arbiter oplicy in an off-policy fashion, we split the action space of the MDP into two disjoints sub-spaces A = A (s)\u222a\u0307A\u0304 (s). The first sub-space is the action indifference space A (s) that contains all constraint respecting actions in each state, while the second sub-space A\u0304 (s) contains all constraint violating actions. Thus, we can split the integral in equation 29\nV \u2217soft(st) = log \u222b A (st) exp ( Q\u2217soft(st,a ) ) da + log \u222b A\u0304 (st) exp ( Q\u2217soft(st, a\u0304) ) da\u0304. (31)\nWe now insert our optimal global Q-function and obtain\nV \u2217 (st) = log \u222b A (s) exp ( Q\u2217 (st,a ) ) da + log \u222b A\u0304 (s) exp ( Q\u2217 (st, a\u0304) ) da\u0304, (32)\nwhich immediately simplifies to\nV \u2217 (st) = log \u222b A (s) exp ( Q\u2217 (st,a ) ) da\n= log \u222b A (s) exp ( Q\u2217n(st,a ) ) da ,\n(33)\nwhere the second integral over A\u0304 disappears because actions in A\u0304 have zero probability under the arbiter policy whileQ\u2217 becomesQ\n\u2217 n because the n\u22121 subtask Q-function all evaluate to zero. Thus,\nthe optimal value of a state under the arbiter policy corresponds to the softmax of the untransformed, n-th subtask Q-function, in the global indifference space. Thus for the optimal state-action value function of the arbiter policy we have\nQ\u2217 (st,at) = r(st,at) + \u03b3Est+1\u223cp[V \u2217 (st+1)], (34)\nwhich is equivalent to\nQ\u2217 (st,at) = r(st,at) + \u03b3Est+1\u223cp,at+1\u223c\u03c0 [Q\u2217 (st+1,at+1)]. (35)\nWe can learn Q\u2217 with the off-policy, soft Bellman backup operator\nT Q(s,a) , r(s,a) + \u03b3Es\u2032\u223cp [ log \u222b A exp ( Q(st,a ) ) da ] \ufe38 \ufe37\ufe37 \ufe38\nV (s\u2032)\n. (36)\nTheorem F.4 (Prioritized soft Q-learning). Consider the soft Bellman backup operator T , and an initial mapping Q0 : S \u00d7 A \u2192 R with |A | < \u221e and define Ql+1 = T Ql, then the sequence of Ql converges to Q\u2217 , the soft Q-value of the optimal arbiter policy \u03c0 \u2217 , as l\u2192\u221e.\nAs we have shown, V \u2217 is the value function of the global arbiter policy, while Q \u2217 = Q \u2217 n is an ordinary soft Q-function (because we are in the indifference space). Thus, the original convergence proof by Haarnoja et al. (2017) directly applies to T , we repeat it here for completeness with some annotations:\nProof. We want to show that T is a contraction, thus we note that the definition of a contraction on some metric space M with norm d is\nd(f(x), f(y)) < kd(x, y), (37)\nwith 0 \u2264 k \u2264 1. Next we define the supremum norm for soft Q-functions as ||Q1 \u2212 Q2|| , maxs,a |Q1(s,a)\u2212Q2(s,a)| and set = ||Q1 \u2212Q2||. It follows that\nlog \u222b exp(Q1(s \u2032,a\u2032))da\u2032 \u2264 log \u222b exp(Q2(s \u2032,a\u2032) + )da\u2032\n= log ( exp( ) \u222b exp(Q2(s \u2032,a\u2032))da\u2032 )\n= + log \u222b exp(Q2(s \u2032,a\u2032))da\u2032.\n(38)\nFrom the last row and using the subtraction property of inequalities, we get\nlog \u222b exp(Q1(s \u2032,a\u2032))da\u2032 \u2212 log \u222b exp(Q2(s \u2032,a\u2032))da\u2032 \u2264 (39)\nand immediately see that the soft multi-objective Bellman operator is indeed a contraction: ||\u03b3Es\u2032\u223cp [ log \u222b A exp ( Q1(s \u2032,a\u2032) ) da\u2032\u2212 log \u222b A exp ( Q2(s \u2032,a\u2032) ) da\u2032 ] || \u2264 \u03b3 = \u03b3||Q1\u2212Q2|| (40)\nHere, in the left side of the inequality, ||T\u03a3Q1 \u2212 T\u03a3Q2||\u03a3, the r(s,a) terms cancel out and we can collapse the two expectations Es\u2032\u223cp into one. The same is true on the right side of the inequality. Thus T is a contraction with optimal Q-function as fixed point."
        },
        {
            "heading": "G EXPERIMENT DETAILS",
            "text": "G.1 2D NAVIGATION ENVIRONMENT\nThe action space for this environment is in R2 and corresponds to positional changes in the x, y plane, while the observation space is the current (x, y) coordinate. The agent\u2019s position is clipped to the range [\u221210, 10] for both dimensions. We normalize actions to unit length to bound the action space and penalize non-straight actions. The high-priority task r1 corresponds to obstacle avoidance and yields negative rewards in close proximity to the \u2229-shaped obstacle (see Fig. 1a)\nr1(s) =\n{ \u2212\u03c32 \u00b7 exp(\u2212 d 2\n2\u00b7l2 ), if d > 0 \u2212\u03b2 \u2212 \u03c32 \u00b7 exp(\u2212 d 2\n2\u00b7l2 ) otherwise, (41)\nwhere d is obstacle distance (inferred from s), \u03c3 = 1 and l = 1 parameterize a squared exponential kernel, and \u03b2 = 10 is a an additional punishment for colliding with the obstacle. The auxiliary rewards r2 and r3 respectively yield negative rewards everywhere except in small areas at the top and at the right side of the environment\nr2(s) = { 0 if s.y > 7 \u2212\u03b4 otherwise, (42)\nr3(s) = { 0 if s.x > 7 \u2212\u03b4 otherwise, , (43)\nwhere we use \u03b4 = 5 in all our experiments. Thus, the lexicographically prioritized task r1 2 corresponds to reaching the top without colliding with the obstacle. Due to the low dimensionality of the environment we directly exploit the proportionality relationship in equation 10 and rely on importance sampling instead of learning a policy network for the 2D navigation experiments.\nG.2 FRANKA EMIKA PANDA ENVIRONMENT\nThis environment features a simulated Franka Emika Panda arm, shown in Fig. 4, and based on the Gymnasium Robotics package (de Lazcano et al., 2023). The action space A \u2208 R9 corresponds to joint positions while the state space S \u2208 R18 contains all joint positions and velocities. In this environment, the high-priority task r1 corresponds to avoiding a fixed volume (red block) in the robots workspace and returns \u221210 when any number of robot joints are inside the volume. The low-priority task r2 corresponds to reaching a fixed end-effector position (green sphere) and yields rewards proportional to the negative distance between the end-effector and the target plus a bonus of 100 for reaching the target. The prioritized task r1 2 thus corresponds to reaching the target end-effector location while keeping all joints outside of the avoidance volume.\nG.3 BASELINE ALGORITHMS\nHere we provide additional details on the algorithms used in the baseline comparison in Sec. 4.3. Since, to the best of our knowledge, PSQD is the first method that solves lexicographic MORL problems with continuous action spaces, we can not rely on existing lexicographic RL algorithms for discrete problems as baselines. Instead, we implement lexicographic task priority constraints by simplistic means in the following, state-of-the-art DRL algorithms that support continuous action spaces.\nPrioritized Soft Actor-Critic Soft Actor-Critic (SAC) (Haarnoja et al., 2018b;c) learns a univariate Gaussian actor by minimizing the Kullback-Leibler divergence between the policy and the normalized, soft Q-function. This policy improvement step is given by\n\u03c0new = arg min \u03c0\u2032\u2208\u03a0 DKL\n( \u03c0\u2032(\u00b7, st) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223exp ( 1 \u03b1Q \u03c0old(st, \u00b7) )\nZ\u03c0old(st)\n) , (44)\nwhere \u03a0 is a set of tractable policies (e.g. parameterized Gaussians), and Z\u03c0old(st) is the normalizing constant for the unnormalized density given by Q\u03c0old(st, \u00b7). To make for a lexicographic, i.e. a task priority-constrained version of SAC, a simple approach is to add a regularization term to\nequation 44 to penalize lower-priority subtask policies from diverging from higher-priority subtask policies. Based on this intuition, for our prioritized SAC baseline, we augment the objective in equation 44 by adding another KL term that measures the divergence between the current subtask policy and the higher-priority subtask policy\n\u03c0new = arg min \u03c0\u2032\u2208\u03a0\n(1\u2212 \u03b2)DKL ( \u03c0\u2032(\u00b7, st) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223exp ( 1 \u03b1Q \u03c0old(st, \u00b7) )\nZ\u03c0old(st)\n) + \u03b2DKL ( \u03c0\u2032(\u00b7, st) \u2223\u2223\u2223\u2223\u03c0pre(\u00b7, st)), (45) where \u03b2 is a weight for the convex combination of the two KL terms and \u03c0pre is the pre-trained, higher-priority policy. In practice, the parameters of the univariate Gaussian actor for our prioritized SAC are given by a DNN parameterized by \u03c6, that uses the reparametrization trick to minimize the loss\nJ\u03c0(\u03c6) = Est\u223cD [ Eat\u223c\u03c0\u03c6 [(1\u2212 \u03b2)(\u03b1 log(\u03c0\u03c6(at | st))\u2212Q(st,at))\n+\u03b2(log \u03c0\u03c6(at | st)\u2212 log \u03c0pre(at | st))] ] ,\n(46)\nwhich is the original SAC objective with the additional KL regularization.\nPrioritized Proximal Policy Optimization To implement a prioritized version of Proximal Policy Optimization (PPO) Schulman et al. (2017), we apply the same intuition as for the prioritized SAC version, i.e. we regularize the policy to be similar to the pre-trained, higher-priority policy. PPO maximizes a clipped version of the surrogate objective\nJ\u03c0(\u03c6) = Et [ \u03c0\u03c6(at | st) \u03c0\u03c6old(at | st) A\u0302t ] , (47)\nwhere A\u0302t is an estimator of the advantage function for task return at timestep t and \u03c6old is the parameter vector of the policy before doing an update. To encourage learning a policy that respects task priority constraints, we change the maximization to\nJ\u03c0(\u03c6) = Et [ \u03c0\u03c6(at | st) \u03c0\u03c6old(at | st) ( (1\u2212 \u03b2)A\u0302t \u2212 \u03b2DKL ( \u03c0\u2032(\u00b7, st) \u2223\u2223\u2223\u2223\u03c0pre(\u00b7, st)))], (48) such that the update now increases the probability of actions that have positive advantage and similar probability under the current and pre-trained, higher-priority policy.\nPSQD Ablation For the ablation to PSQD, we also pre-train the higher-priority Q-function for the obstacle avoidance task and rely on a the c1(s,a) priority constraint indicator functions. However, instead of using them to project the policy into the indifference space, as PSQD does, we use them as an indicator for a punishment of -100 that is subtracted from the lower-priority task reward whenever the agent selects a constraint-violating action.\nSoft Q-Decomposition With \u201cSoft Q-Decomposition\u201d we refer to a continuous version of Russell & Zimdars (2003)\u2019s discrete Q-Decomposition algorithm that we describe in Sec. 2.1. This algorithm concurrently learns the subtask Q-functionsQi for a MORL task with vectorized reward function, such that the overall Q-function, Q\u03a3, can be recovered from the sum of subtask Q-functions. A key property of this algorithm is that the constituent Q-functions are learned on-policy for a central arbiter agent that maximizes the sum of rewards. To adapt the discrete Q-Decomposition algorithm to continuous action spaces, we combine it with the Soft Q-Learning (SQL) (Haarnoja et al., 2017) algorithm. However, SQL is a soft version of Q-learning (Watkins & Dayan, 1992) and thereby off-policy, thus we can not directly use SQL to learn the constituent Q-functions. This is because the SQL update to the Q-function\nJQ(\u03b8) = Est,at\u223cD\n[ 1\n2\n( Q\u03b8n(st,at)\u2212 rn(st,at) + \u03b3Est+1\u223cp [ V \u03b8\u0304n (st+1) ])2] (49)\nV \u03b8\u0304n (st) = log \u222b A exp(Q\u03b8\u0304n(st,a \u2032)) da\u2032, (50)\ncalculates the value of the next state by integrating over the entire action space, thereby finding the softmax of the next state value. In practice, this integral is not computed exactly, but instead approximated with actions sampled from some proposal distribution qa\u2032\nV \u03b8\u0304n (st) = logEqa\u2032 [ exp 1\u03b1Q \u03b8\u0304(st,at)\nqa\u2032(a\u2032)\n] , (51)\nwhich typically is the current policy. Thus, to fix the illusion of control in SQL update to the subtask Q-functions, it suffices to use the arbiter policy instead of the greedy subtask policy for qa\u2032 . This way, the approximate soft value function in equation 51 assigns state values that correspond to the real softmax (\u201cLogSumExp\u201d expression) of Q-values for actions sampled from the arbiter policy."
        },
        {
            "heading": "H ADDITIONAL RESULTS",
            "text": "Here we provide additional qualitative results from the 2D navigation environment that aim to make for additional intuition on our method and how it solves lexicographic MORL problems.\nH.1 OBSTACLE INDIFFERENCE SPACE\nTo provide further intuition for how our composed agent implements constraints, we provide additional examples for Q . In Fig. 7, we plot Q = ln(c1) + Q\u0302 \u03c0 2 at multiple locations in the environment. The black areas indicate A\u0304 , the areas forbidden by the higher-priority obstacle avoidance task, which remain fixed during subsequent adaptation steps. The non-black areas correspond to Q\u0302\u03c0 2 in A , where the lower-priority tasks can learn and express its preferences.\nH.2 MULTIPLE CONSTRAINTS\nWe illustrate tasks with multiple lexicographic priority constraints in the 2D environment. For this, we consider the prioritized tasks r1 2 3 and r1 3 2. Both of these lexicographic MORL tasks assign the highest priority to the obstacle avoidance subtask r1, but prioritize r2 (moving to the top) and r3 (moving to the right) differently. The prioritized task r1 2 3 assigns higher priority to reaching the top than reaching the side, while r1 3 2 assigns higher priority to reaching the side than reaching the top. This is a concrete example of how task priorities can be used to model different, complex tasks. The composed, adapted agents corresponding to these lexicographic tasks are visualized in Fig. 8a and 8b. Each white arrow corresponds to one action taken by the agent.\nAs can be seen, the behavior of the agents for the different prioritized tasks differs noticeably, which is expected for different task prioritizations. The learned agent for r1 2 3 first drives to the top, then to the side, while the agent for r1 3 2 first drives to the side, then to the top. We can again explain this behavior by inspecting the components of these agents in Fig. 8c and 8b. For this, we place the agent at marker 5 in Fig. 7e and plot the constraint indicator functions for the higher priority tasks as well as the Q-function for the low priority task, for the entire action space. Both agents are constrained by ln(c1) which induces A\u0304 1 and forbids actions that bring the agent into close proximity of the obstacle. ln(c2) in Fig. 8c induces A\u0304 2, which constraints \u03c0 to actions that move up. ln(c3) in Fig. 8d, on the other hand, restricts the \u03c0 to actions that move to the side.\nH.3 \u03b5 ABLATION\nTo illustrate how \u03b5 thresholds affect the learned behavior and performance, we perform an ablation study using different values for \u03b51. We again use the 2D navigation environment from App. G and assign high priority to the obstacle avoidance tasks and low priority to reaching the top goal. We generate values for \u03b51 that are roughly log-spaced between 0 and 100. For this experiment, we pre-train only the high-priority obstacle avoidance task and learn the low-priority goal-navigation agent from scratch. We report the same metrics as for our baseline comparison in Sec. 4.3, meaning during learning of the low-priority tasks r2, we log the episode return of both subtasks, to show the trade-off between the two objectives, depending on the threshold value.\nThe average results from five different random seeds are shown in Fig. 9, with high-priority task returns shown on the left, and low-priority task returns shown on the right. Performance on the low-priority subtask roughly falls into three groups, depending on \u03b51-threshold. With very small values for \u03b51, i.e. \u03b51 = 0.0, 0.1, or 0.25, the algorithm can not improve the low-priority subtasks. This is expected because the small thresholds induce such small indifference spaces that none of the available actions allow the agent to improve the performance of the low-priority subtask. At the same time, these small \u03b51 thresholds result in optimal performance for the high-priority task, since they allow almost no divergence from the optimal behavior. Independently of the value for \u03b51, as can be seen in the left panel of Fig. 9, the episode returns for r1 are roughly constant, since Q\u22171 does not change during learning of r2.\nOn the other end of the range, i.e. with large \u03b51 values of 3.98, 10, or 100, the algorithm achieves optimal performance for the low-priority task. This is because the large \u03b51 values induce indifferences space that are not restrictive enough to prevent the agent from hitting and moving through the obstacle, which is reflected by the high costs that these agents obtain under the high-priority task.\nMost interesting are the remaining \u03b51 values, namely \u03b51 = 0.63 and \u03b51 = 1.58. With these values, the agent achieves the same near-optimal performance with respect to the high-priority obstacle avoidance task while also managing to improve low-priority subtask performance considerably. This is because the resulting indifference spaces prevent obstacle collisions (as can be seen in Fig. 7 with \u03b51 = 1) but still allow for many actions that can be used to optimize the low-priority task.\nIn summary, the \u03b5i scalars have a strong effect on the behavior and lower-priority subtask performance. Too large thresholds are ineffective at forbidding undesired actions, while too small thresholds are too restrictive and prevent the agent from improving subtask performance at all. However, as described in Sec. 6, it is straightforward to infer these adequate threshold values by analysis of the higher-priority Q-function, even when the action space is of high dimensionality."
        },
        {
            "heading": "I REPRODUCIBILITY",
            "text": "A GitHub repository with the implementation of the algorithm, experiment setup with hyperparameters, and documentation is available here: https://github.com/frietz58/psqd/. The repository provides the complete PSQD implementation and can be used to reproduce the results in this paper."
        }
    ],
    "title": "LEXICOGRAPHIC REINFORCEMENT LEARNING",
    "year": 2024
}