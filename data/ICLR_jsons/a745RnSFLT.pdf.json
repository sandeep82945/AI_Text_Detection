{
    "abstractText": "Zero-shot learning in prompted vision-language models, the practice of crafting prompts to build classifiers without an explicit training process, has achieved impressive performance in many settings. This success presents a seemingly surprising observation: these methods suffer relatively little from overfitting, i.e., when a prompt is manually engineered to achieve low error on a given training set (thus rendering the method no longer actually zero-shot), the approach still performs well on held-out test data. In this paper, we show that we can explain such performance well via recourse to classical PAC-Bayes bounds. Specifically, we show that the discrete nature of prompts, combined with a PAC-Bayes prior given by a language model, results in generalization bounds that are remarkably tight by the standards of the literature: for instance, the generalization bound of an ImageNet classifier is often within a few percentage points of the true test error. We demonstrate empirically that this holds for existing handcrafted prompts and prompts generated through simple greedy search. Furthermore, the resulting bound is well-suited for model selection: the models with the best bound typically also have the best test performance. This work thus provides a possible justification for the widespread practice of \u201cprompt engineering,\u201d even if it seems that such methods could potentially overfit the training data.",
    "authors": [
        {
            "affiliations": [],
            "name": "Victor Akinwande"
        },
        {
            "affiliations": [],
            "name": "Yiding Jiang"
        },
        {
            "affiliations": [],
            "name": "Dylan Sam"
        },
        {
            "affiliations": [],
            "name": "J. Zico Kolter"
        }
    ],
    "id": "SP:cea30634fc1681779bedc5f739eb144c7cf0e1a8",
    "references": [
        {
            "authors": [
                "Stephen Bach",
                "Victor Sanh",
                "Zheng Xin Yong",
                "Albert Webson",
                "Colin Raffel",
                "Nihal V Nayak",
                "Abheesht Sharma",
                "Taewoon Kim",
                "M Saiful Bari",
                "Thibault F\u00e9vry"
            ],
            "title": "Promptsource: An integrated development environment and repository for natural language prompts",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,",
            "year": 2022
        },
        {
            "authors": [
                "Peter L. Bartlett",
                "Dylan J. Foster",
                "Matus Telgarsky"
            ],
            "title": "Spectrally-normalized margin bounds for neural networks",
            "venue": "ArXiv, abs/1706.08498,",
            "year": 2017
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 1877
        },
        {
            "authors": [
                "Gordon Christie",
                "Neil Fendley",
                "James Wilson",
                "Ryan Mukherjee"
            ],
            "title": "Functional map of the world",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Amit Daniely",
                "Sivan Sabato",
                "Shai Ben-David",
                "Shai Shalev-Shwartz"
            ],
            "title": "Multiclass learnability and the erm principle",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2015
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Gintare Karolina Dziugaite",
                "Daniel M Roy"
            ],
            "title": "Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data",
            "venue": "arXiv preprint arXiv:1703.11008,",
            "year": 2017
        },
        {
            "authors": [
                "Gintare Karolina Dziugaite",
                "Alexandre Drouin",
                "Brady Neal",
                "Nitarshan Rajkumar",
                "Ethan Caballero",
                "Linbo Wang",
                "Ioannis Mitliagkas",
                "Daniel M Roy"
            ],
            "title": "In search of robust measures of generalization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Gintare Karolina Dziugaite",
                "Kyle Hsu",
                "Waseem Gharbieh",
                "Gabriel Arpino",
                "Daniel Roy"
            ],
            "title": "On the role of data in pac-bayes bounds",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen"
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "In Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig"
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Yiding Jiang",
                "Behnam Neyshabur",
                "Hossein Mobahi",
                "Dilip Krishnan",
                "Samy Bengio"
            ],
            "title": "Fantastic generalization measures and where to find them",
            "venue": "arXiv preprint arXiv:1912.02178,",
            "year": 2019
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "venue": "arXiv preprint arXiv:2001.08361,",
            "year": 2020
        },
        {
            "authors": [
                "John Langford",
                "Rich Caruana"
            ],
            "title": "not) bounding the true error",
            "venue": "In NIPS,",
            "year": 2001
        },
        {
            "authors": [
                "Teven Le Scao",
                "Alexander M Rush"
            ],
            "title": "How many data points is a prompt worth",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant"
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang"
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig"
            ],
            "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Computing Surveys,",
            "year": 2023
        },
        {
            "authors": [
                "Sanae Lotfi",
                "Marc Finzi",
                "Sanyam Kapoor",
                "Andres Potapczynski",
                "Micah Goldblum",
                "Andrew G Wilson"
            ],
            "title": "Pac-bayes compression bounds so tight that they can explain generalization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "David A McAllester"
            ],
            "title": "Pac-bayesian model averaging",
            "venue": "In Proceedings of the twelfth annual conference on Computational learning theory, pp",
            "year": 1999
        },
        {
            "authors": [
                "Daniel McNamara",
                "Maria-Florina Balcan"
            ],
            "title": "Risk bounds for transferring representations with and without fine-tuning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Vaishnavh Nagarajan",
                "J Zico Kolter"
            ],
            "title": "Generalization in deep networks: The role of distance from initialization",
            "venue": "arXiv preprint arXiv:1901.01672,",
            "year": 2019
        },
        {
            "authors": [
                "Vaishnavh Nagarajan",
                "J Zico Kolter"
            ],
            "title": "Uniform convergence may be unable to explain generalization in deep learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Ryota Tomioka",
                "Nathan Srebro"
            ],
            "title": "In search of the real inductive bias: On the role of implicit regularization in deep learning",
            "venue": "arXiv preprint arXiv:1412.6614,",
            "year": 2014
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Ryota Tomioka",
                "Nathan Srebro"
            ],
            "title": "Norm-based capacity control",
            "venue": "in neural networks. ArXiv,",
            "year": 2015
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Srinadh Bhojanapalli",
                "David McAllester",
                "Nati Srebro"
            ],
            "title": "Exploring generalization in deep learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Srinadh Bhojanapalli",
                "David A. McAllester",
                "Nathan Srebro"
            ],
            "title": "A pac-bayesian approach to spectrally-normalized margin bounds for neural networks",
            "venue": "ArXiv, abs/1707.09564,",
            "year": 2017
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Shai Shalev-Shwartz",
                "Shai Ben-David"
            ],
            "title": "Understanding machine learning: From theory to algorithms",
            "venue": "Cambridge university press,",
            "year": 2014
        },
        {
            "authors": [
                "Yoad Tewel",
                "Yoav Shalev",
                "Idan Schwartz",
                "Lior Wolf"
            ],
            "title": "Zero-shot image-to-text generation for visual-semantic arithmetic",
            "venue": "arXiv preprint arXiv:2111.14447,",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar",
                "Aurelien Rodriguez",
                "Armand Joulin",
                "Edouard Grave",
                "Guillaume Lample"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Vladimir Vapnik"
            ],
            "title": "Principles of risk minimization for learning theory",
            "venue": "Advances in neural information processing systems,",
            "year": 1991
        },
        {
            "authors": [
                "Vladimir Naumovich Vapnik"
            ],
            "title": "Chervonenkis: On the uniform convergence of relative frequencies of events to their probabilities",
            "year": 1971
        },
        {
            "authors": [
                "Hemanth Venkateswara",
                "Jose Eusebio",
                "Shayok Chakraborty",
                "Sethuraman Panchanathan"
            ],
            "title": "Deep hashing network for unsupervised domain adaptation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Paul Viallard",
                "Pascal Germain",
                "Amaury Habrard",
                "Emilie Morvant"
            ],
            "title": "A general framework for the disintegration of pac-bayesian bounds",
            "venue": "arXiv preprint arXiv:2102.08649,",
            "year": 2021
        },
        {
            "authors": [
                "Yuxin Wen",
                "Neel Jain",
                "John Kirchenbauer",
                "Micah Goldblum",
                "Jonas Geiping",
                "Tom Goldstein"
            ],
            "title": "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery",
            "venue": "arXiv preprint arXiv:2302.03668,",
            "year": 2023
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Jong Wook Kim",
                "Mike Li",
                "Simon Kornblith",
                "Rebecca Roelofs",
                "Raphael Gontijo Lopes",
                "Hannaneh Hajishirzi",
                "Ali Farhadi",
                "Hongseok Namkoong"
            ],
            "title": "Robust fine-tuning of zero-shot models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Chiyuan Zhang",
                "Samy Bengio",
                "Moritz Hardt",
                "Benjamin Recht",
                "Oriol Vinyals"
            ],
            "title": "Understanding deep learning (still) requires rethinking generalization",
            "venue": "Communications of the ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Jingkang Yang",
                "Chen Change Loy",
                "Ziwei Liu"
            ],
            "title": "Learning to prompt for visionlanguage models",
            "venue": "International Journal of Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Wenda Zhou",
                "Victor Veitch",
                "Morgane Austern",
                "Ryan P Adams",
                "Peter Orbanz"
            ],
            "title": "Non-vacuous generalization bounds at the imagenet scale: A pac-bayesian compression approach",
            "venue": "In 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Andrei Ioan Muresanu",
                "Ziwen Han",
                "Keiran Paster",
                "Silviu Pitis",
                "Harris Chan",
                "Jimmy Ba"
            ],
            "title": "Large language models are human-level prompt engineers",
            "venue": "arXiv preprint arXiv:2211.01910,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Generalization bounds provide statistical guarantees on the average-case performance of a learning algorithm\u2019s output. However, in the case of deep learning models, there is still debate about how useful such bounds can be: Zhang et al. (2021) highlighted that classical approaches for deriving generalization bounds are insufficient for explaining the generalization ability of deep learning, spurring a flurry of new approaches for deriving tighter generalization bounds for deep neural networks (Bartlett et al., 2017; Dziugaite & Roy, 2017; Neyshabur et al., 2017b). In the recent literature on generalization bounds for deep learning, a large focus has been on developing datadependent bounds, or bounds that consider both the data distribution and the hypothesis space. Some of the best data-dependent bounds arise from the PAC-Bayes framework (McAllester, 1999) and are derived by bounding the KL divergence between a prior over the hypothesis space and the posterior yielded by the learning algorithm. However, although PAC-Bayes bounds led to the first non-vacuous generalization bounds for deep learning (Dziugaite & Roy, 2017), they are still too loose to be practically useful (Jiang et al., 2019) in most realistic settings. In fact, as Lotfi et al. (2022) have recently argued, many PAC-Bayes bounds with data-dependent priors, while non-vacuous, can be best described as validation bounds \u2014 i.e., the use of data-dependent priors effectively leverages held-out data in a manner similar to cross-validation, which undermines their ability to explain generalization.\nNotwithstanding the lack of a clear theoretical basis, modern machine learning models are moving towards increasingly large pretrained models (Kaplan et al., 2020; Dosovitskiy et al., 2020). One prevailing paradigm is to use pretrained foundation models such as CLIP (Radford et al., 2021) or ALIGN (Jia et al., 2021) as feature extractors and provide weak supervision for a downstream target task via prompts, which are text descriptions of the desired tasks that are often significantly easier to obtain compared to full model weights or even a generic linear classifier over the last layer. The versatility and performance of prompting pretrained models have led to the rise of prompt engineering, an emergent paradigm in machine learning where practitioners carefully design the task specification in text or even learn the prompts in a data-driven fashion (Lester et al., 2021). For example, to obtain a two-class image classifier, one would write two sentences that describe the classes (e.g., \u201cThis is\na dog\u201d and \u201cThis is a cat\u201d), and the two sentences are turned into text embeddings which can be used to classify image embeddings. Despite its empirical success, little is understood of how and why prompting these pretrained models work and, in particular, why the method seems to suffer little from overfitting: manually tuning or even greedily optimizing prompts on a given training set often performs nearly as well on the corresponding test set.\nIn this paper, we demonstrate that rather simple analysis tools capture this behavior surprisingly well (under some assumptions). In particular, we show that classical PAC-Bayes bounds (McAllester, 1999), when applied to the discrete hypothesis class defined by prompts (and specifically with a prior given by a large language model), are often remarkably tight, even for large domains: for example, we achieve a generalization bound of 32% error for a full ImageNet classifier, which is within 6% of the actual test error. This represents a vast improvement over existing bounds for deep learning, where achieving any non-vacuous bound on domains like ImageNet typically requires a great deal of effort; see, for instance, Table 1 for a comparison with other approaches. Perhaps more interestingly, our bounds do not depend on the training data as the prior approaches do 1 but instead depend on the pretraining data of pretrained model (e.g., CLIP) through the image encoder.\nTo summarize, we find that, unlike conventional deep learning models, prompting pretrained models does not suffer from vacuous generalization bounds, and one can readily derive a strong theoretical guarantee for using prompts via well-studied techniques. Overall, these findings suggest that, despite a large amount of automatic or manual tuning, prompt engineering is a principled approach for using these pretrained models that do not suffer the same lack of theoretical grounding as conventional deep learning models. On the other hand, it does introduce its own set of considerations, which we will discuss in the experiments section and conclusion."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": "Prompt Engineering. With the advent of large pretrained models, prompting developed as a different yet effective method to harness the abilities of these large models with limited labeled data (Brown et al., 2020; Le Scao & Rush, 2021; Liu et al., 2023). The flexibility of prompting has enabled a wide range of new capabilities unavailable to previous machine learning models, leading to a significant effort to document successful prompting methods (Bach et al., 2022) in both classification and text-to-image generation. One downside of prompting is that the performance varies greatly depending on how the prompt is phrased. To address this issue, several methods have been proposed to learn \u201coptimal\u201d prompts given labeled data, which empirically performs well and is parameter efficient (Lester et al., 2021; Li & Liang, 2021; Gao et al., 2021; Zhou et al., 2022a;b). A limitation of data-driven methods is their tendency to learn \u201csoft\u201d prompts or embedding vectors that do not correspond to specific tokens. Moreover, from a learning theoretic perspective, the continuous nature of soft prompts, combined with transformations by non-linear models, results in a complex hypothesis space, making it less amenable to theoretical analysis. In contrast, another line of work uses gradient-based methods to learn prompts that consist of discrete tokens that can be mapped to natural language (Wen et al., 2023). This work studies the theoretical guarantees of the latter\n1Data-dependent priors primarily refer to the setting where a portion of training data is used to obtain a prior that is closer to the final posterior. We note that in our setting, the training of pretrained models uses data from different distributions and does not use any training data from the task of interest.\napproach, that is, why these discrete prompting methods seem to work without any overfitting, and our analysis extends to the methods proposed in Wen et al. (2023).\nPrompt engineering has been extended to computer vision through CLIP (Contrastive LanguageImage Pretraining) (Radford et al., 2021). CLIP combines an image and language encoder trained jointly to minimize a contrastive loss, enabling it to perform classification tasks based on natural language instructions. Examples include object recognition, image caption generation (Tewel et al., 2021), and zero-shot image classification using textual descriptions even for unseen labels.\nGeneralization bounds. Generalization bounds are upper bounds on the test error of a model. Deriving such bounds for deep learning has been difficult, and most are usually vacuous (Zhang et al., 2021; Jiang et al., 2019; Dziugaite et al., 2020). Many well-studied tools in statistical learning theory are fundamentally limited when it comes to the analysis of deep neural networks (Nagarajan & Kolter, 2019b). The core component of a generalization bound is a complexity measure, a quantity that relates to some aspect of generalization. A complexity measure may depend on the properties of the trained model, optimizer, and possibly training data, as long as it does not have access to a validation set. The most classic bounds, such as VC-dimension (Vapnik, 1971), are often related to some form of parameter counting which is often too pessimistic for deep neural networks. Norm-based bounds usually rely on the margin and some norms of the model weights (Langford & Caruana, 2001; Bartlett et al., 2017; Neyshabur et al., 2015; 2017b), but these bounds have been ineffective at studying generalization of deep learning (Nagarajan & Kolter, 2019a). Another main class is the PAC-Bayes bounds (McAllester, 1999) which have been much more successful in deep learning due to the flexibility of prior (Neyshabur et al., 2017a; Dziugaite & Roy, 2017; Zhou et al., 2019; Lotfi et al., 2022), although these bounds are still much looser than the actual generalization error. Our approach also belongs to the PAC-Bayes family, but we apply the PAC-Bayes bounds to the distribution of discrete tokens (with a language model as the prior) rather than to a distribution over the parameters of a neural network. This allows us to derive significantly tighter bounds compared to applying the PAC-Bayes bounds with less informative priors."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "Notations. Let X \u2208 Rd be a set of inputs and Y = [K] be a label set, and there exists a probability distribution D on (X\u00d7Y) which is unknown. Let our data (X1, Y1), . . . , (Xn, Yn) be drawn i.i.d from D, and consider a predictor f : X \u2192 Y and a fixed set of predictors indexed by the parameter set \u0398. We use f\u03b8 to denote the classifier indexed by \u03b8. We consider the 0\u20131 loss given by \u2113(y\u2032, y) = 1{y \u0338= y\u2032}. The generalization error (risk) of a predictor is defined as R(\u03b8) = E(X,Y )\u223cP [\u2113(f\u03b8(X), Y )] and the empirical risk r(\u03b8) = 1n \u2211n i=1 \u2113(f\u03b8(Xi), Yi) satisfies ES [r(\u03b8)] = R(\u03b8) for a sample S =\n[(X1, Y1) , . . . , (Xn, Yn)]. An estimator is a function \u03b8\u0302 : \u22c3\u221e n=1(X \u00d7 Y)n \u2192 \u0398.\nVision-language models. CLIP consists of two encoders encimg and enctxt. Given an image X \u2208 X , the image encoder encimg : X \u2192 Rd maps an image X to a d-dimension real-valued embedding. Let T be the space of texts and T \u2208 T a single piece of text, the image encoder enctxt : T \u2192 Rd maps T to a d-dimension real-valued embedding. Given a batch of images {Xi}Bi=1 and their corresponding texts {Ti}Bi=1, the training objective maximizes the cosine similarity of the embeddings of the matching image and text pair and minimize the cosine similarity of image and text pairs that do not correspond to each other. The primary task we consider in this work is image classification via pretrained vision-language models. The goal is to find a class prompt, \u03b8k \u2208 T , for each class that achieves good accuracy. For a K-class classification problem with \u03b8 = (\u03b81, \u03b82, . . . , \u03b8K) \u2208 \u0398 = T K , the zero-shot classifier is f\u03b8(X) = argmaxk\u2208[K] \u2329 enctxt(\u03b8k), encimg(X) \u232a .\nGeneralization bounds. Deriving generalization bounds is closely related to assigning hypotheses prior probabilities of being good (Shalev-Shwartz & Ben-David, 2014). One of the simplest approaches uses uniform convergence over the entire discrete hypothesis space (where |\u0398| denotes the number of functions in the class) to derive the well-known generalization bound,\nTheorem 3.1 (Shalev-Shwartz & Ben-David (2014)). For every \u03b4 > 0, with probability 1\u2212\u03b4 over the training set of size n, for any hypothesis \u03b8 \u2208 \u0398, the following holds R(\u03b8) \u2264 r(\u03b8) + \u221a log |\u0398|+log( 1\u03b4 )\n2n .\nThis result does not consider the implicit bias of the learning algorithm (Neyshabur et al., 2014), the training data S, or the data-generating distribution D. In contrast, the PAC-Bayes framework offers a flexible approach for leveraging this information by defining a hierarchy over hypotheses in the hypothesis class \u0398 that takes the form of a prior distribution P over \u0398. That is, we assign a probability P (\u03b8) \u2265 0 for each \u03b8 \u2208 \u0398 and refer to P (\u03b8) as the prior score of \u03b8. The learning process defines a posterior probability over \u0398, which we denote by Q. In the context of supervised learning, we can think of Q as defining the following prediction rule: given an instance X , we randomly pick a hypothesis \u03b8 according to Q and predict f\u03b8(X). Remarkably, it was shown that the expected generalization gap can be upper bounded by the KL-divergence between P and Q: Theorem 3.2 (McAllester (1999)). For every \u03b4 > 0, prior P over \u0398, with probability 1\u2212 \u03b4 over the training set of size n, for any posterior Q over \u0398, the following holds\nE\u03b8\u223cQ[R(\u03b8)] \u2264 E\u03b8\u223cQ[r(\u03b8)] +\n\u221a DKL(Q \u2225P ) + log(n\u03b4 ) + 2\n2n\u2212 1 . (1)"
        },
        {
            "heading": "4 METHODOLOGY",
            "text": "Designing a prompt is analogous to finding a set of weights in typical machine learning models, where the hypothesis space is the space of texts/tokens. The goal is to find class prompts that maximize training accuracy without finetuning the model\u2019s parameters. This process, which is often referred to as prompt engineering, can be formulated as discrete optimization over the space of tokens, V ."
        },
        {
            "heading": "4.1 PROMPT SEARCH",
            "text": "To study the generalization capabilities of discrete prompts, we consider a simple greedy search algorithm that mimics an overeager prompt engineer who exhaustively tries adjusting prompts with every possible word, although the analysis extends to other techniques that produce discrete prompts. To find class prompts of length L, we will search for K \u00b7 L tokens over the space, VK\u00b7L. Naively, this search is exponential in the length of the prompt so to circumvent this problem, the prompts are generated successively; that is, we increment the prompts by selecting the token that maximizes a search criterion, J , on the training dataset from a set of candidate tokens, V\u0302 \u2286 V . With a slight abuse of notation, we will use V\u0302(\u03b8) to denote a candidate set that can be conditioned on the current \u03b8. The search criterion is the objective being optimized (e.g., the empirical loss), and candidate tokens are permissible tokens that can be used to extend the current class prompts. At every step of the search, we keep the class prompts fixed except for all but one class. The prompt for each class k is a sequence of l tokens \u03b8kl \u2208 V , \u03b8k\u2264l = (\u03b8k1 , \u03b8k2 , . . . , \u03b8kl ) where l < L, and we use \u03b8\u00ack to denote the class prompts for all classes that are not the kth class. The next token for \u03b8k is obtained via:\n\u03b8kl+1 = argmax v\u2208V\u0302(\u03b8)\nJ ( v, \u03b8k\u2264l, \u03b8 \u00ack). (2) The pseudocode for this sequential search is outlined in detail in Algorithm 1.\nEmpirical risk minimization. Using \u2295 to denote concatenation, we consider a simple form of search, greedy search, where we use:\nV\u0302greedy(\u03b8) = V, Jgreedy ( v, \u03b8k\u2264l, \u03b8 \u00ack) = \u2212r((. . . , \u03b8k\u22121, \u03b8k\u2264l \u2295 v, \u03b8k+1, . . . )), (3) where r is the empirical risk in terms of the 0\u20131 loss (see Section 3). In other words, we always search over all possible tokens (line 6) to maximize the training accuracy. This greedy search is an empirical risk minimization (Vapnik, 1991, ERM) learner since its only objective is to minimize the training error. There are several drawbacks to this simple algorithm, the chief of which is that we need to search over V exhaustively at each step, which can be expensive since it consists of all the tokens of the vision-language model (e.g., CLIP has about 50000 tokens). Instead, we could search over only a subset of V . To reduce this search space, we use a language model (LM) to induce a distribution over the next tokens conditioned on \u03b8k and only evaluate the tokens with high probabilities:\npnext(\u03b8 k l+1 | \u03b8k\u2264l) = pLM ( \u03b8kl+1 | \u03b8k\u2264l = (\u03b8k1 , \u03b8k2 , . . . , \u03b8kl ) ) . (4)\nGiven that CLIP is trained with natural language supervision, autoregressive LMs that are also trained on natural language can likely predict suitable next tokens. We then take the top N candidates and only evaluate the accuracy of these candidates. Conveniently, this can be seen as constraining the complexity of the prompt as the language model provides a structured prior. We observe that this pruning incurs minimal performance loss, suggesting that LMs indeed are good prior in searching for class prompts on image classification tasks. Furthermore, we may use predefined strings to further constrain the hypothesis space by starting with an initial prompt such as \u201cThis is an image of [...]\u201d, instead of an empty string. These initial prompts can provide additional structure to the generated prompts by constraining the output distribution, similar to the role of inductive bias. We refer to this method as Greedy.\nStructural risk minimization via PAC-Bayes. This procedure can be further augmented to optimize the PAC-Bayes bound via structural risk minimization (Vapnik & Chervonenkis, 1974, SRM) similar to the approach of Dziugaite & Roy (2017), namely, we will take the hypothesis complexity (e.g., KL-divergence) into account as we search for the next token for each prompt. We use the KL-divergence directly in the objective optimization without sacrificing the quality of the solution. Once again, we do this optimization in a sequential manner via Algorithm 1:\nV\u0302LM(\u03b8) = { v \u2208 V \u2223\u2223\u2223 max v\u2032 pnext(v \u2032 | \u03b8k\u2264l)\u2212 pnext(v | \u03b8k\u2264l) \u2264 \u2206 } , (5)\nJLM(v, \u03b8k\u2264l, \u03b8\u00ack) = \u2212r ( (. . . , \u03b8k\u22121, \u03b8k\u2264l \u2295 v, \u03b8k+1, . . . ) ) + \u03b2 log pnext(v | \u03b8k\u2264l), (6)\nwhere \u2206 controls the size of the search space (adjusted according to computational constraints) and \u03b2 is a hyperparameter that controls the strength of the regularization. This set of permissible tokens could also be pruned and fixed beforehand by discarding tokens with low marginal probability. We refer to this version of search as regularized greedy."
        },
        {
            "heading": "4.2 GENERALIZATION GUARANTEES FOR PROMPTS",
            "text": "Since the space of all prompts is discrete and the total number of possible prompts is |\u0398| = |V|LK , for a single hypothesis \u03b8\u0302, we have the following uniform convergence bound for prompts that depends on prompt length, the number of classes, and the number of tokens in the vocabulary by assigning uniform probability to each hypothesis (from Theorem 3.1):\nR(\u03b8\u0302) \u2264 r(\u03b8\u0302) +\n\u221a LK log |V|+ log( 1\u03b4 )\n2n . (7)\nHowever, not all prompts are equally likely to be good. To obtain a tighter generalization guarantee on the learned \u03b8\u0302, we will leverage a classical PAC-Bayes bound to derive an upper bound on the generalization error of the learned prompts.\nIn conventional application of PAC-Bayes to deep learning, P and Q are often chosen to be isotropic Gaussian on the parameters (Langford & Caruana, 2001) so the KL-divergence between the prior and posterior can be easily computed. We instead use a language model as the prior over K independent prompts, P (\u03b8) = \u220fK i=1 \u220fL j=1 pLM(\u03b8 i j | \u03b8i\u2264j). Further, we treat the prompts \u03b8\u0302 found through search or through prompt engineering as a point mass posterior, Q(\u03b8) = 1{\u03b8 = \u03b8\u0302}. In this case, the KL-divergence is conveniently equal to the negative log-likelihood of \u03b8\u0302 under the LM because the posterior is zero everywhere except for at \u03b8\u0302:\nDKL(Q \u2225P ) = \u2211 \u03b8\u2208\u0398 Q(\u03b8) log Q(\u03b8) P (\u03b8) = log 1 P (\u03b8\u0302) = \u2212 K\u2211 i=1 L\u2211 j=1 log pLM ( \u03b8\u0302ij | \u03b8\u0302i\u2264j ) . (8)\nThis bound has an intuitive interpretation, which is that the generalizing prompts are the ones that achieve good training performance and are likely under the language model. Having a pointmass posterior over discrete space also means that we can derandomize the PAC-Bayes bound for free (Viallard et al., 2021). Combining these observations, we have the following deterministic upper bound on the generalization error (from Theorem 3.2):\nR(\u03b8\u0302) \u2264 r(\u03b8\u0302) + \u221a\u221a\u221a\u221a\u2212\u2211Ki=1 \u2211Lj=1 log pLM (\u03b8\u0302ij | \u03b8\u0302i\u2264j)+ log(n\u03b4 ) + 2 2n\u2212 1 . (9)\nWe note that these techniques are not novel from a theoretical perspective and there are more sophisticated PAC-Bayes variants that may yield tighter results. Nonetheless, in the next section, we will observe that this simple bound is surprisingly tight even for complex datasets such as ImageNet.\nData leakage and contamination. One strong assumption of these bounds, which we make explicitly and which could indeed be violated in practice, is that the image encoder is trained without access to the training set used for prompt engineering. If it is trained on this data, even from the training set, then the functional complexity of the hypothesis class depends not just on the prompt, but also implicitly on the complexity of the image encoder. We emphasize that this fact does not change the nature of the bounds above, but it does change whether or not any given bound in the experiments can be formally considered a valid bound, or could be violated.\nIn practice, this is difficult to verify for the e.g. CLIP encoder, since the data it was trained on is not publicly disclosed. Nonetheless, the CLIP paper includes a sensitivity analysis that shows a relatively small effect of including any of the evaluation datasets they consider (Radford et al., 2021). Thus, while we fully acknowledge that data contamination may apply to the experiments below, we believe this to be similar to many current evaluations of foundation models, where it is difficult to assess the extent to which any performance is truly zero-shot."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we evaluate the generalization of discrete prompts generated by Greedy on CIFAR10, CIFAR-100, ImageNet as well as domain generalization datasets fMoW (Christie et al., 2018) and OfficeHome (Venkateswara et al., 2017), which is much less studied in the context of numerical\ngeneralization bounds. We also evaluate existing well-performing handcrafted prompts taken from CLIP and Wise-FT (Wortsman et al., 2022). Given these prompts, we compute generalization bounds via PAC-Bayes (PAC-Bayes) and via uniform convergence (UC). The PAC-Bayes bounds are computed using LLaMA-7B (Touvron et al., 2023) as the prior. Within Greedy, we search using the CLIP vocabulary of 49 408 tokens and measure the generalization bounds for 100 realizations of Greedy with each corresponding to a fixed prompt length l \u2208 {1, . . . , 10} and split portion of the dataset s \u2208 {0.1, . . . , 1.0}. More details on the experimental procedure are in Appendix C.\nBaselines We compare our generalization bounds against existing generalization bounds on CIFAR10, CIFAR-100, and ImageNet. In particular, we compare against the works of Lotfi et al. (2022) and Zhou et al. (2019), which represent the latest progress in PAC-Bayes bounds for deep learning.\nAs shown in Table 1, discrete prompts achieve much tighter bounds than the stateof-the-art across all 3 datasets. We remark that our approach is also data-independent, while still achieving a tighter bound than the data-dependent approach in the work of Lotfi et al. (2022). An added benefit of this result is that we make little modification to the existing learning paradigm \u2013 indeed prior bounds often need to make strict assumptions about the neural network such as Gaussian posterior or the weights lying in a low dimensional manifold (Lotfi et al., 2022) which may hurt the performance.\nWe observe that even simple UC bounds over discrete prompts generated by Greedy lead to tight, non-vacuous\nbounds across a variety of datasets, and PAC-Bayes bounds with an LLM prior further improve these bounds (Figure 1). These also apply to handcrafted prompts (Figure 2) from the existing literature (Radford et al., 2021; Wortsman et al., 2022) (other datasets\u2019 result in Appendix B).\nStructural risk minimization with the PAC-Bayes bound PAC-Bayes is related to SRM (Vapnik & Chervonenkis, 1974), where one tries to optimize both the goodness of fit and complexity of the model. When we compare test error against train error or the generalization bound (Figure 3), we observe that the generalization bound can serve as a useful criterion for model selection. We consider using SRM, where our complexity term is exactly the KL divergence term in Equation 8. Regularized Greedy now jointly maximizes train accuracy and minimizes this KL divergence term when adding new tokens to each class prompt. We observe that this naturally leads to tighter bounds for prompts yielded by Greedy on CIFAR-10 (Figure 4) while maintaining comparable accuracy. Interestingly, using LLaMA-7B as the prior does not significantly improve the linguistic coherence of prompts obtained through regularized search, which leaves room for more sophisticated search techniques to address this in future work.\nPruning the hypothesis space In addition to regularizing the search objective with the KL term directly, another method to improve our generalization bounds is to prune the vocabulary using a large language model. We experiment with conditioning the language model on the class names and then selecting tokens from the language model\u2019s vocabulary with the highest probability under the language model. In Figure 4, we report the performance and generalization of Greedy when the tokens considered in search are restricted to within k standard-deviations (see Appendix B.1 for details) away from the maximum logit token. While the vocabulary size of LLaMA-7b is 32 000 tokens, the number of tokens within 3, 2, 1 standard deviations from the maximum token are 6894, 1361, 185 respectively. We observe this implicitly prunes the hypotheses to contain those with smaller generalization error at a small cost to the train and test error. Restricting the vocabulary also encodes prior knowledge about the data or domain. For example, further results using a vocabulary of English words in Appendix B.1 (instead of CLIP\u2019s vocabulary of tokens) show that we can learn slightly more interpretable prompts.\nEffects of prompt length Another key quantity of prompt engineering is the prompt length which directly controls the size of the hypothesis space. We analyze how the length of class prompts impacts the performance of Greedy (Figure 5). We note that at a certain length, the train accuracy plateaus, which means that a relatively small\nprompt length suffices for good classification performance.\nFitting random labels Motivated by our new observations about prompt engineering, we hypothesize that the learned prompts are less prone to overfitting the noise in the data. Zhang et al. (2021) showed that conventional deep neural networks can fit both random labels, arguing that these models have much higher capacity than what traditional statistical learning theory can deal with. To demonstrate that prompt engineering is robust to label noise, we experiment with running Greedy\non training data with a certain proportion of randomly flipped labels. We observe that both training and test accuracy drop monotonically in tandem as we flip these training labels (Figure 6), which suggests that the prompts cannot overfit the random labels. For a baseline comparison, we also compare the performance of a linear probe on random labels. We observe that this achieves roughly random performance (13.60% accuracy) with 100% flipped labels. This supports that Greedy is not too simple of a search approach to fit the random labels as other more complex methods also cannot.\nLearning with small data When the number of data points is small (e.g., n = 20), the use of PAC-Bayes is especially attractive since we can use all the data points to estimate the posterior and bound its risk. Furthermore, prompt engineering is frequently used with limited labeled data; thus, further progress in understanding its generalization properties must provide bounds in this regime. In Figure 6, we report the train and test accuracy of Greedy as we vary the amount of training data (between 1%\u201310% of the full data) we use in computing the search objective. We observe less than 2% increase in error with 2% of the training set of CIFAR-10. This highlights that Greedy can be remarkably data efficient. We then compute both the uniform convergence and PAC-Bayes bounds with 20 samples per class (Table 3). The results underscore the importance of an informative prior in the form of the LLM. The bounds obtained with the LLM prior are, albeit loose but still non-vacuous. To the best of our knowledge, this is not possible with prior approaches unless it is data-dependent. One could ask since we assume the representation from CLIP is not learned from the training data, can we simply use an SVM-like bound on the learned features (McNamara & Balcan, 2017)? As a case in point, we present a standard linear probe (on top of CLIP\u2019s features), which achieves slightly better accuracy but a vacuous generalization bound. The implementation details are described in Appendix C. The discrete nature of prompts and the fact that the corresponding hypothesis space of CLIP is so small is crucial to the success of our approach. We believe that exploring avenues to obtain tighter PAC-Bayes bounds in the small data regime is an opportunity for future work and the use of data-dependent priors may be fruitful in this regard."
        },
        {
            "heading": "6 CONCLUSION AND LIMITATIONS",
            "text": "In this paper, we study the generalization properties of engineered prompts on image recognition tasks. We observe the surprising fact: prompt engineering does not seem to overfit, and also performs well on the test distribution. We provide a principled approach to analyze this generalization behavior by framing discrete prompts as a relatively small hypothesis class, onto which we can naturally apply classical PAC-Bayes bounds using an LLM prior. This results in the tightest bounds yet observed across multiple complex datasets, including CIFAR-10, CIFAR-100, and ImageNet. As a whole, this supports the use of prompt-engineering or simple greedy searches over potential class prompts as a high-performing and well-generalizing classifier.\nDespite the ability to produce highly non-vacuous bounds, the bounds rely on the fact that pretrained vision-language models readily contain some hypothesis class that will perform well on the training set (for whatever the desired task is). This, in turn, naturally relies on the generalization performance of the underlying model itself, which our analysis evidently does not, and cannot, address (as they are only aware of the language model, which does not observe the data).\nNonetheless, what our bounds do address is the fact that when given these performant models, manual prompt engineering (even when \u201coverfitting\u201d to a training set) often exhibits surprisingly strong generalization behavior. Given the prevalence of prompt engineering in modern ML, we believe that this work provides an important perspective on this widespread practice."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "We thank Nina Balcan for valuable discussions during this project. Victor Akinwande, and Dylan Sam were supported by funding from Bosch Center for AI. Dylan Sam was also supported by a National Science Foundation Graduate Research Fellowship under Grant No. DGE2140739 and the ARCS Foundation. Yiding Jiang is supported by the Google PhD Fellowship."
        },
        {
            "heading": "A PESUDOCODE",
            "text": "Algorithm 1 Sequential Prompt Search 1: \u03b8 \u2190 [initial_prompt]\u00d7K 2: for l = 0 to L\u2212 1 do 3: class_order\u2190 randomly sampled order of class indices 4: for k in class_order do 5: criteria\u2190 \u2212\u221e 6: for v in V\u0302(\u03b8) do \u25b7 This step is vectorized in practice. 7: score\u2190J (v, \u03b8k\u2264l, \u03b8\u00ack) \u25b7 Evaluate the score of v. 8: if score > criteria then \u25b7 Keep the prompt with best performance. 9: criteria\u2190 score \u25b7 Update the current best score. 10: \u03b8kl+1 \u2190 v \u25b7 Update \u03b8k with the better token. 11: Return \u03b8"
        },
        {
            "heading": "B ADDITIONAL RESULTS",
            "text": "Other datasets We report the results of discrete prompts both generated by Greedy and handcrafted on FMoW in Figure 7.\nCreating the pruned vocabulary To create the pruned vocabulary, we take the class name of each class (e.g., \u201cdog\u201d) and feed each one into the LLM and compute the logits over the next token. We then compute the standard deviation, \u03c3, over each class\u2019s logits and take the tokens that are k\u03c3 from the maximum logit. Then, we use the union of the top tokens of all classes as the pruned vocabulary.\nVocabulary subsampling In addition to the result on CIFAR-10 in Figure 6, we report the performance of discrete prompts generated by greedy on CIFAR-100, when a random subset of the CLIP vocabulary is used in Figure 8. We observe less than 2% increase in error with 1% of the vocabulary. This provides further evidence of the robustness of Greedy to the vocabulary size. Random sampling, while easy to implement, prunes hypotheses that may have desirable properties. As such, we report the performance of greedy on CIFAR-100 when the vocabulary is pruned using the language model, and observe that Greedy can recover prompts with better generalization (See Figure 9).\nFitting random labels In addition to the result on CIFAR-10 in Figure 6, we report results on fitting to randomly labeled data for CIFAR-100, FMoW, and OfficeHome in in Figure 10, and observe consistently that Greedy does not fit random labels. This provides evidence that contrasts the current literature on the ability of neural networks to easily fit random labels.\nFitting with small data In Figure 11 we report results on fitting to small sample sizes on both CIFAR-10 and CIFAR-100. We consider random subsets between 1% \u2013 10% of the data and between\n0.1% \u2013 1%. We observe that the discrete prompts that Greedy can learn, even with small sample sizes, observe good generalization that degrades as the sizes of the training set decrease.\nLearning with a different vocabulary The Greedy algorithm is agnostic to the set of tokens used in the search procedure. In practice, one may use a vocabulary that encodes prior knowledge about the data or domain. Additionally, certain properties like interpretability may be desired. We report results on searching with the language model\u2019s vocabulary (See Figure 12). We do not observe significant degradation in performance. We also report results on penalizing the search criteria using the bound (i.e. SRM) with different \u03b2 values (See Figure 13). We observe that Greedy is able to recover prompts with better generalization as the penalty increases at a small cost to accuracy. We run Greedy on a vocabulary of English words obtained from the english-words2 package. We show the prompts learned on CIFAR-10 in Figure 15.\n2https://github.com/mwiens91/english-words-py\nB.1 PROMPTING THE LANGUAGE MODEL BEFORE SEARCH\nPrompt prefixes We prefix Greedy with:\n1. a photo of a\n2. a blurry photo of a\n3. a black and white photo of a\n4. a low contrast photo of a\n5. a high contrast photo of a\n6. a bad photo of a\n7. a good photo of a\n8. a photo of a small\n9. a photo of a big\n10. a photo of the\nWe show the performance on CIFAR-10 in Figure 14. We observe that Greedy consistently converges to a prompt with roughly the same test-error regardless of the prefix."
        },
        {
            "heading": "C EXPERIMENTAL DETAILS",
            "text": "Hyperparameters We report the hyperparameters used in CLIP, LLaMA-7b, and the Greedy algorithm in Table 4.\nLinear Probe Baseline A linear probe baseline was trained with a batch size of 64 and a learning rate of 0.01 for 10 epochs. We compute the generalization bound using McAllester\u2019s bound with a prior distribution over linear model weights of N (w(0), \u03c32I) and a posterior of N (w, \u03c32I), where I is an identity matrix of dimension (768 \u00d7 the number of classes), w is our learned weights, and w(0) is our random initialization. We then optimize over a grid of 20000 values for \u03c3 \u2208 [0.1, ..., 1]. This mirrors the procedure from the work of Jiang et al. (2019). We also note that computing a standard UC bound is challenging as we cannot specify a meaningful prior over an infinite space. Other approaches are challenging due to difficulties in computing multiclass analogues of the VC dimension such as the Natarajan dimension (Daniely et al., 2015)."
        }
    ],
    "title": "UNDERSTANDING PROMPT ENGINEERING MAY NOT REQUIRE RETHINKING GENERALIZATION",
    "year": 2024
}