{
    "abstractText": "We introduce Lemur and Lemur-Chat, openly accessible language models optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents. The evolution from language chat models to functional language agents demands that models not only master human interaction, reasoning, and planning but also ensure grounding in the relevant environments. This calls for a harmonious blend of language and coding capabilities in the models. Lemur and Lemur-Chat are proposed to address this necessity, demonstrating balanced proficiencies in both domains, unlike existing open-source models that tend to specialize in either. Through meticulous pretraining using a code-intensive corpus and instruction fine-tuning on text and code data, our models achieve state-of-the-art averaged performance across diverse text and coding benchmarks. Comprehensive experiments demonstrate Lemur\u2019s superiority over existing open-source models and its proficiency across various agent tasks involving human communication, tool usage, and interaction under fullyand partiallyobservable environments. The harmonization between natural and programming languages enables Lemur-Chat to significantly narrow the gap with proprietary models on agent abilities, providing key insights into developing advanced open-source agents adept at reasoning, planning, and operating seamlessly across environments. Our model and code have been open-sourced at https://github.com/OpenLemur/Lemur.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yiheng Xu"
        },
        {
            "affiliations": [],
            "name": "Hongjin Su"
        },
        {
            "affiliations": [],
            "name": "Chen Xing"
        },
        {
            "affiliations": [],
            "name": "Boyu Mi"
        },
        {
            "affiliations": [],
            "name": "Qian Liu"
        },
        {
            "affiliations": [],
            "name": "Weijia Shi"
        },
        {
            "affiliations": [],
            "name": "Binyuan Hui"
        },
        {
            "affiliations": [],
            "name": "Fan Zhou"
        },
        {
            "affiliations": [],
            "name": "Yitao Liu"
        },
        {
            "affiliations": [],
            "name": "Tianbao Xie"
        },
        {
            "affiliations": [],
            "name": "Zhoujun Cheng"
        },
        {
            "affiliations": [],
            "name": "Siheng Zhao"
        },
        {
            "affiliations": [],
            "name": "Lingpeng Kong"
        },
        {
            "affiliations": [],
            "name": "Bailin Wang"
        },
        {
            "affiliations": [],
            "name": "Caiming Xiong"
        },
        {
            "affiliations": [],
            "name": "Tao Yu"
        }
    ],
    "id": "SP:f156ac064c1ca51c9fd284a43b6b5375bbcce02c",
    "references": [
        {
            "authors": [
                "Armen Aghajanyan",
                "Lili Yu",
                "Alexis Conneau",
                "Wei-Ning Hsu",
                "Karen Hambardzumyan",
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Omer Levy",
                "Luke Zettlemoyer"
            ],
            "title": "Scaling laws for generative mixed-modal language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Austin",
                "Augustus Odena",
                "Maxwell Nye",
                "Maarten Bosma",
                "Henryk Michalewski",
                "David Dohan",
                "Ellen Jiang",
                "Carrie Cai",
                "Michael Terry",
                "Quoc Le"
            ],
            "title": "Program synthesis with large language models",
            "venue": "arXiv preprint arXiv:2108.07732,",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Federico Cassano",
                "John Gouwar",
                "Daniel Nguyen",
                "Sydney Nguyen",
                "Luna Phipps-Costin",
                "Donald Pinckney",
                "Ming-Ho Yee",
                "Yangtian Zi",
                "Carolyn Jane Anderson",
                "Molly Q Feldman",
                "Arjun Guha",
                "Michael Greenberg",
                "Abhinav"
            ],
            "title": "Jangda. Multipl-e: A scalable and polyglot approach to benchmarking neural code generation",
            "venue": "IEEE Transactions on Software Engineering,",
            "year": 2023
        },
        {
            "authors": [
                "Harrison Chase"
            ],
            "title": "LangChain, 2022. URL https://github.com/langchain-ai/ langchain",
            "year": 2022
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harri Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "venue": "arXiv preprint arXiv:2107.03374,",
            "year": 2021
        },
        {
            "authors": [
                "Wenhu Chen",
                "Xueguang Ma",
                "Xinyi Wang",
                "William W Cohen"
            ],
            "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "venue": "arXiv preprint arXiv:2211.12588,",
            "year": 2022
        },
        {
            "authors": [
                "Wenhu Chen",
                "Ming Yin",
                "Max Ku",
                "Elaine Wan",
                "Xueguang Ma",
                "Jianyu Xu",
                "Tony Xia",
                "Xinyi Wang",
                "Pan Lu"
            ],
            "title": "Theoremqa: A theorem-driven question answering dataset",
            "venue": "arXiv preprint arXiv:2305.12524,",
            "year": 2023
        },
        {
            "authors": [
                "Xinyun Chen",
                "Maxwell Lin",
                "Nathanael Sch\u00e4rli",
                "Denny Zhou"
            ],
            "title": "Teaching large language models to self-debug",
            "venue": "arXiv preprint arXiv:2304.05128,",
            "year": 2023
        },
        {
            "authors": [
                "Zhoujun Cheng",
                "Tianbao Xie",
                "Peng Shi",
                "Chengzu Li",
                "Rahul Nadkarni",
                "Yushi Hu",
                "Caiming Xiong",
                "Dragomir Radev",
                "Mari Ostendorf",
                "Luke Zettlemoyer",
                "Noah A. Smith",
                "Tao Yu"
            ],
            "title": "Binding language models in symbolic languages",
            "venue": "ICLR, abs/2210.02875,",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Mark Chen",
                "Heewoo Jun",
                "Lukasz Kaiser",
                "Matthias Plappert",
                "Jerry Tworek",
                "Jacob Hilton",
                "Reiichiro Nakano"
            ],
            "title": "Training verifiers to solve math word problems",
            "venue": "arXiv preprint arXiv:2110.14168,",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Deng",
                "Yu Gu",
                "Boyuan Zheng",
                "Shijie Chen",
                "Samuel Stevens",
                "Boshi Wang",
                "Huan Sun",
                "Yu Su"
            ],
            "title": "Mind2web: Towards a generalist agent for the web, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, May 2019",
            "year": 2019
        },
        {
            "authors": [
                "Linxi Fan",
                "Guanzhi Wang",
                "Yunfan Jiang",
                "Ajay Mandlekar",
                "Yuncong Yang",
                "Haoyi Zhu",
                "Andrew Tang",
                "De-An Huang",
                "Yuke Zhu",
                "Anima Anandkumar"
            ],
            "title": "Minedojo: Building open-ended embodied agents with internet-scale knowledge",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Zhangyin Feng",
                "Daya Guo",
                "Duyu Tang",
                "Nan Duan",
                "Xiaocheng Feng",
                "Ming Gong",
                "Linjun Shou",
                "Bing Qin",
                "Ting Liu",
                "Daxin Jiang",
                "Ming Zhou"
            ],
            "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages, September 2020",
            "year": 2020
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima",
                "Shawn Presser",
                "Connor Leahy"
            ],
            "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling, December 2020",
            "year": 2020
        },
        {
            "authors": [
                "Luyu Gao",
                "Aman Madaan",
                "Shuyan Zhou",
                "Uri Alon",
                "Pengfei Liu",
                "Yiming Yang",
                "Jamie Callan",
                "Graham Neubig"
            ],
            "title": "Pal: Program-aided language models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Xinyang Geng"
            ],
            "title": "Easylm: A simple and scalable training framework for large language models, March 2023",
            "venue": "URL https://github.com/young-geng/EasyLM",
            "year": 2023
        },
        {
            "authors": [
                "Ran Gong",
                "Qiuyuan Huang",
                "Xiaojian Ma",
                "Hoi Vo",
                "Zane Durante",
                "Yusuke Noda",
                "Zilong Zheng",
                "SongChun Zhu",
                "Demetri Terzopoulos",
                "Li Fei-Fei"
            ],
            "title": "Mindagent: Emergent gaming interaction",
            "venue": "arXiv preprint arXiv:2309.09971,",
            "year": 2023
        },
        {
            "authors": [
                "Izzeddin Gur",
                "Hiroki Furuta",
                "Austin Huang",
                "Mustafa Safdari",
                "Yutaka Matsuo",
                "Douglas Eck",
                "Aleksandra Faust"
            ],
            "title": "A real-world webagent with planning, long context understanding, and program synthesis",
            "venue": "arXiv preprint arXiv:2307.12856,",
            "year": 2023
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Ana Marasovi\u0107",
                "Swabha Swayamdipta",
                "Kyle Lo",
                "Iz Beltagy",
                "Doug Downey",
                "Noah A. Smith"
            ],
            "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks",
            "year": 2020
        },
        {
            "authors": [
                "Shibo Hao",
                "Tianyang Liu",
                "Zhen Wang",
                "Zhiting Hu"
            ],
            "title": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings",
            "year": 2023
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Steven Basart",
                "Andy Zou",
                "Mantas Mazeika",
                "Dawn Song",
                "Jacob Steinhardt"
            ],
            "title": "Measuring massive multitask language understanding",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Saurav Kadavath",
                "Akul Arora",
                "Steven Basart",
                "Eric Tang",
                "Dawn Song",
                "Jacob Steinhardt"
            ],
            "title": "Measuring mathematical problem solving with the MATH dataset",
            "venue": "Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks",
            "year": 2021
        },
        {
            "authors": [
                "Jordan Hoffmann",
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Elena Buchatskaya",
                "Trevor Cai",
                "Eliza Rutherford",
                "Diego de Las Casas",
                "Lisa Anne Hendricks",
                "Johannes Welbl",
                "Aidan Clark",
                "Tom Hennigan",
                "Eric Noland",
                "Katie Millican",
                "George van den Driessche",
                "Bogdan Damoc",
                "Aurelia Guy",
                "Simon Osindero",
                "Karen Simonyan",
                "Erich Elsen",
                "Jack W. Rae",
                "Oriol Vinyals",
                "Laurent Sifre"
            ],
            "title": "Training Compute-Optimal Large Language Models, March 2022",
            "year": 2022
        },
        {
            "authors": [
                "Yushi Hu",
                "Chia-Hsuan Lee",
                "Tianbao Xie",
                "Tao Yu",
                "Noah A Smith",
                "Mari Ostendorf"
            ],
            "title": "In-context learning for few-shot dialogue state tracking",
            "venue": "arXiv preprint arXiv:2203.08568,",
            "year": 2022
        },
        {
            "authors": [
                "Wenlong Huang",
                "Pieter Abbeel",
                "Deepak Pathak",
                "Igor Mordatch"
            ],
            "title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Wenlong Huang",
                "Fei Xia",
                "Ted Xiao",
                "Harris Chan",
                "Jacky Liang",
                "Pete Florence",
                "Andy Zeng",
                "Jonathan Tompson",
                "Igor Mordatch",
                "Yevgen Chebotar",
                "Pierre Sermanet",
                "Noah Brown",
                "Tomas Jackson",
                "Linda Luu",
                "Sergey Levine",
                "Karol Hausman",
                "Brian Ichter"
            ],
            "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models, July 2022b",
            "year": 2022
        },
        {
            "authors": [
                "Pastor",
                "Linda Luu",
                "Kuang-Huei Lee",
                "Yuheng Kuang",
                "Sally Jesmonth",
                "Nikhil J. Joshi",
                "Kyle Jeffrey",
                "Rosario Jauregui Ruano",
                "Jasmine Hsu",
                "Keerthana Gopalakrishnan",
                "Byron David",
                "Andy Zeng",
                "Chuyuan Kelly Fu"
            ],
            "title": "Do as I can, not as I say: Grounding language in robotic affordances",
            "venue": "Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Anushrut Jignasu",
                "Kelly Marshall",
                "Baskar Ganapathysubramanian",
                "Aditya Balu",
                "Chinmay Hegde",
                "Adarsh Krishnamurthy"
            ],
            "title": "Towards foundational ai models for additive manufacturing: Language models for g-code debugging, manipulation, and comprehension",
            "venue": "arXiv preprint arXiv:2309.02465,",
            "year": 2023
        },
        {
            "authors": [
                "Denis Kocetkov",
                "Raymond Li",
                "Loubna Ben Allal",
                "Jia Li",
                "Chenghao Mou",
                "Carlos Mu\u00f1oz Ferrandis",
                "Yacine Jernite",
                "Margaret Mitchell",
                "Sean Hughes",
                "Thomas Wolf",
                "Dzmitry Bahdanau",
                "Leandro von Werra",
                "Harm de Vries"
            ],
            "title": "The Stack: 3 TB of permissively licensed source code, November 2022",
            "year": 2022
        },
        {
            "authors": [
                "Andreas K\u00f6pf",
                "Yannic Kilcher",
                "Dimitri von R\u00fctte",
                "Sotiris Anagnostidis",
                "Zhi-Rui Tam",
                "Keith Stevens",
                "Abdullah Barhoum",
                "Nguyen Minh Duc",
                "Oliver Stanley",
                "Rich\u00e1rd Nagyfi",
                "Shahul ES",
                "Sameer Suri",
                "David Glushkov",
                "Arnav Dantuluri",
                "Andrew Maguire",
                "Christoph Schuhmann",
                "Huu Nguyen",
                "Alexander Mattick"
            ],
            "title": "OpenAssistant Conversations \u2013 Democratizing Large Language Model Alignment",
            "year": 2023
        },
        {
            "authors": [
                "Hanna Kurniawati"
            ],
            "title": "Partially observable markov decision processes (pomdps) and robotics",
            "venue": "arXiv preprint arXiv:2107.07599,",
            "year": 2021
        },
        {
            "authors": [
                "Yuhang Lai",
                "Chengxi Li",
                "Yiming Wang",
                "Tianyi Zhang",
                "Ruiqi Zhong",
                "Luke Zettlemoyer",
                "Wen-tau Yih",
                "Daniel Fried",
                "Sida Wang",
                "Tao Yu"
            ],
            "title": "Ds-1000: A natural and reliable benchmark for data science code generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Thomas Wolf",
                "Arjun Guha",
                "Leandro von Werra",
                "Harm de Vries"
            ],
            "title": "StarCoder: May the source be with you",
            "year": 2023
        },
        {
            "authors": [
                "Jacky Liang",
                "Wenlong Huang",
                "Fei Xia",
                "Peng Xu",
                "Karol Hausman",
                "Brian Ichter",
                "Pete Florence",
                "Andy Zeng"
            ],
            "title": "Code as policies: Language model programs for embodied control",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2023
        },
        {
            "authors": [
                "Bill Yuchen Lin",
                "Yicheng Fu",
                "Karina Yang",
                "Prithviraj Ammanabrolu",
                "Faeze Brahman",
                "Shiyu Huang",
                "Chandra Bhagavatula",
                "Yejin Choi",
                "Xiang Ren"
            ],
            "title": "Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks",
            "venue": "arXiv preprint arXiv:2305.17390,",
            "year": 2023
        },
        {
            "authors": [
                "Xiao Liu",
                "Hao Yu",
                "Hanchen Zhang",
                "Yifan Xu",
                "Xuanyu Lei",
                "Hanyu Lai",
                "Yu Gu",
                "Hangliang Ding",
                "Kaiwen Men",
                "Kejuan Yang",
                "Shudan Zhang",
                "Xiang Deng",
                "Aohan Zeng",
                "Zhengxiao Du",
                "Chenhui Zhang",
                "Sheng Shen",
                "Tianjun Zhang",
                "Yu Su",
                "Huan Sun",
                "Minlie Huang",
                "Yuxiao Dong",
                "Jie Tang"
            ],
            "title": "AgentBench: Evaluating LLMs",
            "year": 2023
        },
        {
            "authors": [
                "Ziyang Luo",
                "Can Xu",
                "Pu Zhao",
                "Qingfeng Sun",
                "Xiubo Geng",
                "Wenxiang Hu",
                "Chongyang Tao",
                "Jing Ma",
                "Qingwei Lin",
                "Daxin Jiang"
            ],
            "title": "Wizardcoder: Empowering code large language models with evol-instruct, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Gr\u00e9goire Mialon",
                "Roberto Dess\u0131",
                "Maria Lomeli",
                "Christoforos Nalmpantis",
                "Ram Pasunuru",
                "Roberta Raileanu",
                "Baptiste Rozi\u00e8re",
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Asli Celikyilmaz",
                "Edouard Grave",
                "Yann LeCun",
                "Thomas Scialom"
            ],
            "title": "Augmented Language Models: A Survey, February 2023",
            "year": 2023
        },
        {
            "authors": [
                "Subhabrata Mukherjee",
                "Arindam Mitra",
                "Ganesh Jawahar",
                "Sahaj Agarwal",
                "Hamid Palangi",
                "Ahmed Awadallah"
            ],
            "title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4",
            "year": 2023
        },
        {
            "authors": [
                "Theo X Olausson",
                "Jeevana Priya Inala",
                "Chenglong Wang",
                "Jianfeng Gao",
                "Armando SolarLezama"
            ],
            "title": "Demystifying gpt self-repair for code generation",
            "venue": "arXiv preprint arXiv:2306.09896,",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Guilherme Penedo",
                "Quentin Malartic",
                "Daniel Hesslow",
                "Ruxandra Cojocaru",
                "Alessandro Cappelli",
                "Hamza Alobeidli",
                "Baptiste Pannier",
                "Ebtesam Almazrouei",
                "Julien Launay"
            ],
            "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",
            "year": 2023
        },
        {
            "authors": [
                "Xavier Puig",
                "Kevin Ra",
                "Marko Boben",
                "Jiaman Li",
                "Tingwu Wang",
                "Sanja Fidler",
                "Antonio Torralba"
            ],
            "title": "Virtualhome: Simulating household activities via programs",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language Models are Unsupervised Multitask Learners",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-toText Transformer",
            "year": 2019
        },
        {
            "authors": [
                "Baptiste Rozi\u00e8re",
                "Jonas Gehring",
                "Fabian Gloeckle",
                "Sten Sootla",
                "Itai Gat",
                "Xiaoqing Ellen Tan",
                "Yossi Adi",
                "Jingyu Liu",
                "Tal Remez",
                "J\u00e9r\u00e9my Rapin",
                "Artyom Kozhevnikov",
                "Ivan Evtimov",
                "Joanna Bitton",
                "Manish Bhatt",
                "Cristian Canton Ferrer",
                "Aaron Grattafiori",
                "Wenhan Xiong",
                "Alexandre D\u00e9fossez",
                "Jade Copet",
                "Faisal Azhar",
                "Hugo Touvron",
                "Louis Martin",
                "Nicolas Usunier",
                "Thomas Scialom",
                "Gabriel Synnaeve"
            ],
            "title": "Code Llama: Open Foundation Models for Code, August 2023",
            "year": 2023
        },
        {
            "authors": [
                "Baptiste Roziere",
                "Jonas Gehring",
                "Fabian Gloeckle",
                "Sten Sootla",
                "Itai Gat",
                "Xiaoqing Ellen Tan",
                "Yossi Adi",
                "Jingyu Liu",
                "Tal Remez",
                "J\u00e9r\u00e9my Rapin"
            ],
            "title": "Code llama: Open foundation models for code",
            "venue": "arXiv preprint arXiv:2308.12950,",
            "year": 2023
        },
        {
            "authors": [
                "Stuart J Russell"
            ],
            "title": "Artificial intelligence a modern approach",
            "venue": "Pearson Education, Inc.,",
            "year": 2010
        },
        {
            "authors": [
                "David Saxton",
                "Edward Grefenstette",
                "Felix Hill",
                "Pushmeet Kohli"
            ],
            "title": "Analysing Mathematical Reasoning Abilities of Neural Models, April 2019",
            "year": 2019
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u0131",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom"
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "venue": "arXiv preprint arXiv:2302.04761,",
            "year": 2023
        },
        {
            "authors": [
                "Yongliang Shen",
                "Kaitao Song",
                "Xu Tan",
                "Dongsheng Li",
                "Weiming Lu",
                "Yueting Zhuang"
            ],
            "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Tianlin Shi",
                "Andrej Karpathy",
                "Linxi Fan",
                "Jonathan Hernandez",
                "Percy Liang"
            ],
            "title": "World of bits: An open-domain platform for web-based agents",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Noah Shinn",
                "Federico Cassano",
                "Beck Labash",
                "Ashwin Gopinath",
                "Karthik Narasimhan",
                "Shunyu Yao"
            ],
            "title": "Reflexion: Language agents with verbal reinforcement learning",
            "venue": "arXiv preprint arXiv:2303.11366,",
            "year": 2023
        },
        {
            "authors": [
                "Mohit Shridhar",
                "Jesse Thomason",
                "Daniel Gordon",
                "Yonatan Bisk",
                "Winson Han",
                "Roozbeh Mottaghi",
                "Luke Zettlemoyer",
                "Dieter Fox"
            ],
            "title": "Alfred: A benchmark for interpreting grounded instructions for everyday tasks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Mohit Shridhar",
                "Xingdi Yuan",
                "Marc-Alexandre Cote",
                "Yonatan Bisk",
                "Adam Trischler",
                "Matthew Hausknecht"
            ],
            "title": "Alfworld: Aligning text and embodied environments for interactive learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Shaden Smith",
                "Mostofa Patwary",
                "Brandon Norick",
                "Patrick LeGresley",
                "Samyam Rajbhandari",
                "Jared Casper",
                "Zhun Liu",
                "Shrimai Prabhumoye",
                "George Zerveas",
                "Vijay Korthikanti",
                "Elton Zhang",
                "Rewon Child",
                "Reza Yazdani Aminabadi",
                "Julie Bernauer",
                "Xia Song",
                "Mohammad Shoeybi",
                "Yuxiong He",
                "Michael Houston",
                "Saurabh Tiwary",
                "Bryan Catanzaro"
            ],
            "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model, February 2022",
            "year": 2022
        },
        {
            "authors": [
                "D\u0131\u0301dac Sur\u0131\u0301s",
                "Sachit Menon",
                "Carl Vondrick"
            ],
            "title": "Vipergpt: Visual inference via python execution for reasoning",
            "venue": "CoRR, abs/2303.08128,",
            "year": 2023
        },
        {
            "authors": [
                "Richard S. Sutton",
                "Andrew G. Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "IEEE Transactions on Neural Networks,",
            "year": 2005
        },
        {
            "authors": [
                "Mirac Suzgun",
                "Nathan Scales",
                "Nathanael Sch\u00e4rli",
                "Sebastian Gehrmann",
                "Yi Tay",
                "Hyung Won Chung",
                "Aakanksha Chowdhery",
                "Quoc V Le",
                "Ed H Chi",
                "Denny Zhou"
            ],
            "title": "Challenging big-bench tasks and whether chain-of-thought can solve them",
            "venue": "arXiv preprint arXiv:2210.09261,",
            "year": 2022
        },
        {
            "authors": [
                "Zhang",
                "Angela Fan",
                "Melanie Kambadur",
                "Sharan Narang",
                "Aurelien Rodriguez",
                "Robert Stojnic",
                "Sergey Edunov",
                "Thomas Scialom"
            ],
            "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models, July 2023a",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "arXiv preprint arXiv:2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Guanzhi Wang",
                "Yuqi Xie",
                "Yunfan Jiang",
                "Ajay Mandlekar",
                "Chaowei Xiao",
                "Yuke Zhu",
                "Linxi Fan",
                "Anima Anandkumar"
            ],
            "title": "Voyager: An open-ended embodied agent with large language models",
            "venue": "arXiv preprint arXiv:2305.16291,",
            "year": 2023
        },
        {
            "authors": [
                "Xingyao Wang",
                "Zihan Wang",
                "Jiateng Liu",
                "Yangyi Chen",
                "Lifan Yuan",
                "Hao Peng",
                "Heng Ji"
            ],
            "title": "Mint: Evaluating llms in multi-turn interaction with tools and language feedback",
            "venue": "arXiv preprint arXiv:2309.10691,",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560,",
            "year": 2022
        },
        {
            "authors": [
                "Rushang Karia",
                "Shailaja Keyur Sampat",
                "Savan Doshi",
                "Siddhartha Mishra",
                "Sujan Reddy",
                "Sumanta Patro",
                "Tanay Dixit",
                "Xudong Shen",
                "Chitta Baral",
                "Yejin Choi",
                "Noah A. Smith",
                "Hannaneh Hajishirzi",
                "Daniel Khashabi"
            ],
            "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks, October 2022b",
            "year": 2022
        },
        {
            "authors": [
                "Yue Wang",
                "Weishi Wang",
                "Shafiq Joty",
                "Steven C.H. Hoi"
            ],
            "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le"
            ],
            "title": "Finetuned Language Models Are Zero-Shot Learners",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "David E Wilkins"
            ],
            "title": "Practical planning: extending the classical AI planning paradigm",
            "year": 2014
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz",
                "Jamie Brew"
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "venue": "URL https://api.semanticscholar.org/CorpusID:204509627",
            "year": 1910
        },
        {
            "authors": [
                "Tianbao Xie",
                "Siheng Zhao",
                "Chen Henry Wu",
                "Yitao Liu",
                "Qian Luo",
                "Victor Zhong",
                "Yanchao Yang",
                "Tao Yu"
            ],
            "title": "Text2reward: Automated dense reward function generation for reinforcement learning",
            "venue": "arXiv preprint arXiv:2309.11489,",
            "year": 2023
        },
        {
            "authors": [
                "Binfeng Xu",
                "Zhiyuan Peng",
                "Bowen Lei",
                "Subhabrata Mukherjee",
                "Yuchen Liu",
                "Dongkuan Xu"
            ],
            "title": "Rewoo: Decoupling reasoning from observations for efficient augmented language models",
            "venue": "arXiv preprint arXiv:2305.18323,",
            "year": 2023
        },
        {
            "authors": [
                "Can Xu",
                "Qingfeng Sun",
                "Kai Zheng",
                "Xiubo Geng",
                "Pu Zhao",
                "Jiazhan Feng",
                "Chongyang Tao",
                "Daxin Jiang"
            ],
            "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions, June 2023b",
            "year": 2023
        },
        {
            "authors": [
                "John Yang",
                "Akshara Prabhakar",
                "Karthik Narasimhan",
                "Shunyu Yao"
            ],
            "title": "Intercode: Standardizing and benchmarking interactive coding with execution feedback",
            "venue": "arXiv preprint arXiv:2306.14898,",
            "year": 2023
        },
        {
            "authors": [
                "Zhilin Yang",
                "Peng Qi",
                "Saizheng Zhang",
                "Yoshua Bengio",
                "William W Cohen",
                "Ruslan Salakhutdinov",
                "Christopher D Manning"
            ],
            "title": "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
            "venue": "arXiv preprint arXiv:1809.09600,",
            "year": 2018
        },
        {
            "authors": [
                "Shunyu Yao",
                "Howard Chen",
                "John Yang",
                "Karthik Narasimhan"
            ],
            "title": "Webshop: Towards scalable real-world web interaction with grounded language agents",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik Narasimhan",
                "Yuan Cao"
            ],
            "title": "React: Synergizing reasoning and acting in language models",
            "venue": "arXiv preprint arXiv:2210.03629,",
            "year": 2022
        },
        {
            "authors": [
                "Weiran Yao",
                "Shelby Heinecke",
                "Juan Carlos Niebles",
                "Zhiwei Liu",
                "Yihao Feng",
                "Le Xue",
                "Rithesh Murthy",
                "Zeyuan Chen",
                "Jianguo Zhang",
                "Devansh Arpit"
            ],
            "title": "Retroformer: Retrospective large language agents with policy gradient optimization",
            "venue": "arXiv preprint arXiv:2308.02151,",
            "year": 2023
        },
        {
            "authors": [
                "Tao Yu",
                "Rui Zhang",
                "Kai Yang",
                "Michihiro Yasunaga",
                "Dongxu Wang",
                "Zifan Li",
                "James Ma",
                "Irene Li",
                "Qingning Yao",
                "Shanelle Roman"
            ],
            "title": "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql",
            "venue": "task. arXiv preprint arXiv:1809.08887,",
            "year": 2018
        },
        {
            "authors": [
                "Kechi Zhang",
                "Zhuo Li",
                "Jia Li",
                "Ge Li",
                "Zhi Jin"
            ],
            "title": "Self-edit: Fault-aware code editor for code generation",
            "venue": "arXiv preprint arXiv:2305.04087,",
            "year": 2023
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Wei-Lin Chiang",
                "Ying Sheng",
                "Siyuan Zhuang",
                "Zhanghao Wu",
                "Yonghao Zhuang",
                "Zi Lin",
                "Zhuohan Li",
                "Dacheng Li",
                "Eric P. Xing",
                "Hao Zhang",
                "Joseph E. Gonzalez",
                "Ion Stoica"
            ],
            "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot",
            "year": 2023
        },
        {
            "authors": [
                "Shuyan Zhou",
                "Frank F Xu",
                "Hao Zhu",
                "Xuhui Zhou",
                "Robert Lo",
                "Abishek Sridhar",
                "Xianyi Cheng",
                "Yonatan Bisk",
                "Daniel Fried",
                "Uri Alon"
            ],
            "title": "Webarena: A realistic web environment for building autonomous agents",
            "venue": "arXiv preprint arXiv:2307.13854,",
            "year": 2023
        },
        {
            "authors": [
                "Hoffmann"
            ],
            "title": "predict the optimal continue-pre-training data mixture ratio for a pair of domains to maximize their performance would be very meaningful and interesting, and it is still an open research question for the current large language model Hernandez et al",
            "venue": "Chowdhery et al",
            "year": 2023
        },
        {
            "authors": [
                "RoboCodeGen (Liang"
            ],
            "title": "2023) serves as a specialized evaluation framework focused on robotics-related tasks. It comprises three main types of questions that target spatial reasoning (e.g., identifying the closest point among a set of points), geometric reasoning (e.g., verifying if one bounding box is contained within another), and controls (e.g., PD control)",
            "year": 2023
        },
        {
            "authors": [
                "WebArena (Zhou"
            ],
            "title": "2023) creates self-hostable websites of four popular categories by simulating real-world equivalent functionalities and data. To simulate human problem-solving abilities, WebArena also embeds tools and knowledge resources as standalone websites",
            "year": 2023
        },
        {
            "authors": [
                "Li"
            ],
            "title": "2023) is a model with 15.5B parameters and 8k context length. It is first trained on 1 trillion tokens sourced from The Stack Kocetkov et al. (2022), a large collection of permissively licensed GitHub repositories, and then finetuned on 35B Python tokens",
            "year": 2022
        },
        {
            "authors": [
                "Li"
            ],
            "title": "2023) is similar to StarCoder-15B, except that it is finetuned in RefinedWeb",
            "venue": "Penedo et al",
            "year": 2023
        },
        {
            "authors": [
                "Llama-2 Touvron"
            ],
            "title": "2023b) is a model with 70B parameters trained on 2 trillion tokens from public sources. Its training corpus is mostly natural language texts, which enables its strong abilities in textual understanding",
            "year": 2023
        },
        {
            "authors": [
                "Llama-2-70B-Chat Touvron"
            ],
            "title": "2023b) finetunes Llama-2-70B and is optimized for dialogue use cases",
            "year": 2023
        },
        {
            "authors": [
                "CodeLlama-34B Roziere"
            ],
            "title": "2023) finetunes Llama-2-34B on code-intensive corpus with 500B tokens",
            "year": 2023
        },
        {
            "authors": [
                "Roziere"
            ],
            "title": "Code Llama - Instruct is based on Code Llama and fine-tuned with an additional approx. 5B tokens to better follow human instructions",
            "year": 2023
        },
        {
            "authors": [
                "Luo"
            ],
            "title": "2023) was trained with instrucions on code-intensive datasets. It achieved superior performance on popular code generation benchmarks including HumanEval, HumanEval+, MBPP, and DS1000",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Intelligent agents are broadly conceptualized as autonomous problem solvers with the abilities to sense their environment, decide, and act upon that enviorment (Sutton & Barto, 2005; Russell, 2010; Wilkins, 2014). Recent implementations of this concept in creating language agents (Yao et al., 2022b; Gravitas, 2023; Wang et al., 2023a) capable of utilizing natural language for varied and intricate tasks in diverse environments have demonstrated potential, particularly when built upon large language models (LLMs) (Brown et al., 2020; Chen et al., 2021; Chowdhery et al., 2022; OpenAI, 2023; Touvron et al., 2023a). Such agents harness the human knowledge in LLMs and can think and communicate in human terms. This equips them to employ varied tools, operate in complex environments, engage in language reasoning, and create spontaneous multi-agent systems.\nTo effectively form the foundation of language agents, LLMs should not only master human interaction, reasoning, and planning but also ensure grounding in the relevant environments (Wei et al., 2022; Huang et al., 2022a; Ichter et al., 2022). Human interaction, reasoning, and planning can be largely realized through the natural language capabilities of LLMs. On the other hand, the grounded execution in the environment is usually achieved by using general-purpose code or domain-specific APIs, such as controlling web browsers (Shi et al., 2017; Yao et al., 2022a; Deng et al., 2023; Zhou\n\u2217Equal contribution.\net al., 2023), interacting with OS CLI terminals (Yang et al., 2023), and manipulating robotic arms via primitive APIs (Ichter et al., 2022; Huang et al., 2022b; Liang et al., 2023). Therefore, we posit that for the construction of language agents, it is imperative for language models to possess harmonized capabilities in both natural language and programming languages. This balance ensures that models do not specialize exclusively in certain areas but can seamlessly integrate with environment contexts and generate controllable and valid actions. Presently, closed-source models like GPT-4 (OpenAI, 2023) demonstrate such capabilities, which empower them to function as language agents. However, current open-source LLMs such as Llama 2 (Touvron et al., 2023a) and CodeLlama (Rozie\u0300re et al., 2023) have traditionally been tailored for either textual or code-related tasks, with limited ability to effectively balance both.\nTo address this need, we introduce Lemur and Lemur-Chat, cutting-edge, openly accessible models pre-trained and fine-tuned to harmonize text and code capabilities. We enhanced the base Llama2-70B through thoughtfully designed pre-training and instruction fine-tuning stages. Specifically, we built a code-centric corpus based on The Stack (Kocetkov et al., 2022), comprising 90B tokens with a 10:1 code-to-text ratio, ensuring improved capabilities in coding ability while maintaining performance in natural language ability. We refer to this model as Lemur. After pretraining, we conducted instruction fine-tuning using about 300K examples from both text and code to build an instruction-following model, which we refer to as Lemur-Chat. Thorough assessments across 8 textual and coding benchmarks validate the superior performance of both Lemur and Lemur-Chat in multiple text and code evaluations, establishing them as the most well-rounded open-source models.\nMoreover, this work embarks on assessing the vital capabilities of language agents across various scenarios, which we refer to as agent benchmarks. We place a particular emphasis on their toolusage abilities, and abilities in grounding in the environment feedback and human feedback. We also explore the challenges posed by real and partially observable environments where the agent has to take actions based on limited knowledge and take additional actions to gather more information. Experimental results indicate that Lemur-Chat outperforms other open-sourced models in 12 of the 13 agent benchmarks. This underscores how the integration of natural and coding capabilities allows Lemur-Chat to exceed the current open-source models for language agents, markedly bridging the performance disparity between open-source and commercial alternatives. Our experiments show an important insight that in agent scenarios, there is a need for synergy between natural language and coding abilities. Specifically, for models with strong natural language capabilities but weak coding abilities like Llama-2-70B-Chat, they can effectively use simple tools to assist reasoning (\u00a74.2) because the action space is small and the difficulty of using tools is low. However, when facing complex decision-making scenarios such as web browsing and house navigation, the action space is usually large, and models with strong coding abilities have an advantage in generating complex executable action sequences (\u00a74.5). Moreover, we did error analysis on several environments covering multiple agent skills to reflect the key challenges for future language agent development, including tool and action executability \u00a7D.1, problem difficulty \u00a7D.2, and domain knowledge \u00a7D.3. Overall, Lemur has both strong natural language and coding abilities, enabling it to achieve better performance in both scenarios. This research provides insights into optimizing the synergy between natural and programming languages, laying a solid foundation for developing advanced language agents capable of operating efficiently in various environments."
        },
        {
            "heading": "2 PRE-TRAINING AND INSTRUCTION TUNING OF LEMUR",
            "text": "This section will introduce the method to build the Lemur and Lemur-Chat models and their performance on commonly-used benchmarks for pre-trained language model evaluation. To build a more balanced model, the pipeline includes two stages: pre-training (\u00a7 2.1) and instruction finetuning (\u00a7 2.2). The training pipeline is shown in Figure 1."
        },
        {
            "heading": "2.1 PRE-TRAINING",
            "text": "In the pre-training stage, we choose the Llama-2-70B base model as the starting point, which is the cutting-edge open-sourced base model in most scenarios except the coding scenario. Our goal is to improve the coding ability while maintaining the reasoning ability of Llama-2. To this end, we built a corpus with a code-to-text ratio of 10:1. We discuss how we decided this ratio in \u00a7 A.1.2. For the code part, we base it on The Stack (Kocetkov et al., 2022), a collection of source\ncodes from GitHub with permissive licenses. Among all languages, we focus on scripting or interpreted languages (Python, SQL, Bash, Perl, etc.) because agent models are often executed in interactive scenarios. Unlike compiled languages like C++, the interpreted nature of scripting languages allows for immediate execution and ease of modification, which is essential for dynamic interaction in language agent scenarios. As for the text aspect, we use RefinedWeb (Penedo et al., 2023), Redpajama (Computer, 2023), as well as CommonCrawl, Wikipedia, Books, ArXiv, StackExchange and DM Mathematics (Saxton et al., 2019) to build the textual data corpus. Following previous works (Gao et al., 2020; Smith et al., 2022; Computer, 2023; Li et al., 2023), we do extensive deduplication after aggregating all data sources. The composition of the data is shown in Appendix \u00a7 A.1.1. We train the Lemur-70B model initialized with Llama-2-70B using a TPUv4-512 pod. We train the model with sequence packing (Raffel et al., 2019; Chung et al., 2022) to improve the training efficiency. Please refer to Appendix \u00a7 A.1.3 for more details."
        },
        {
            "heading": "2.2 INSTRUCTION FINE-TUNING",
            "text": "During the instruction fine-tuning phase, we include four data sources to construct Lemur-Chat, including the Open Assistant crowdsourced annotated dialogue corpus (Ko\u0308pf et al., 2023), Orca data with chain of thought reasoning for human-written tasks (Lian et al., 2023; Mukherjee et al., 2023), ShareGPT & Chatlogs containing real user and ChatGPT history records (ShareGPT data), as well as Evol-CodeAlpaca data (Luo et al., 2023) consisting of complex coding tasks generated by ChatGPT along with their solutions. The statistics of these instruction datasets are shown in Table 1. After we collect these data, we additionally clean and deduplicate these instruction fine-tuning data. We conduct training on these data for 2 epochs. Please refer to \u00a7 A.2 for more details."
        },
        {
            "heading": "3 FROM LANGUAGE MODEL TO LANGUAGE AGENT",
            "text": "This section introduces how we measure the language and coding abilities of a language model to guide the process of harmonization. We further discuss the new challenges faced when connecting LLM to the environment and describe how we examine the necessary agent capabilities."
        },
        {
            "heading": "3.1 FUNDAMENTAL LANGAUGE AND CODING CAPABILITIES",
            "text": "Previous work typically uses a variety of benchmarks to comprehensively reflect the performance of models on different types of tasks. Therefore, we evaluate the performance of various models across text and code benchmarks as follows.\n\u2022 Text benchmarks: MMLU (Hendrycks et al., 2021a) to determine factuality, BBH (Suzgun et al., 2022) to check reasoning abilities, GSM8K (Cobbe et al., 2021) to gauge math reasoning.\n\u2022 Code benchmarks: HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) to test Python writing abilities, Spider (Yu et al., 2018) to assess database query capabilities, MultiPL-E (Cassano et al., 2023) to measure the multi-lingual coding capabilities, DS-1000 (Lai et al., 2023) for evaluation in data science scenarios"
        },
        {
            "heading": "3.2 CONNECTING LLM AGENTS TO ENVIRONMENT",
            "text": "While the measurements in \u00a73.1 provide valuable insights on each domain, they may not fully encapsulate the models\u2019 capabilities in language agent scenarios due to their focus on single-turn interactions in fully observable settings. In order to address this discrepancy, we scrutinize models in the context of multi-turn interactive scenarios to gauge their adaptability in practical situations. Our assessment is centered on various capabilities of language agents, which are encapsulated in the factors outlined in Figure 2 and Table 2. For a more comprehensive assessment of these capabilities, we reorganize existing multiple datasets into four sets to examine the diverse skills demanded by the agent scenario. Please refer to Appendix B for details of each dataset.\nTable 2: Multi-turn agent evaluation under different settings. We evaluate crucial capabilities for LLM as an agent, spanning tool usage, feedback adherence and environment exploration.\nCapabilities Environments Observability Datasets\nAugment with Tools (\u00a74.2) Python Calculator Fully M-{GSM8K, MATH, TheoremQA}WikiSearch M-{HotpotQA, MMLU}\nSelf-debug with Environment Feedback (\u00a74.3)\nPython\nFully\nM-{HumanEval, MBPP} OS Terminal InterCode-Bash\nDatabase InterCode-SQL Robotics API RoboCodeGen\nFollow Natural Language Feedback (\u00a74.4)\nUser/Agent Fully M-Reasoning w/ GPT-4 feedbackUser/Agent M-Code w/ GPT-4 feedback\nExplore in Partially Observable Environment (\u00a74.5)\nOS Terminal Partially InterCode-CTF Web Browser WebArena\nEmbodied Simulator M-ALFWorld\nAugment with Tools Tool usage (Schick et al., 2023; Hao et al., 2023; Mialon et al., 2023) is an important capability of language agents. Tasks like calculations or information retrieval can be offloaded to external modules (e.g. Python interpreters or search engines) using tools (Gao et al., 2023; Chen et al., 2022), improving reliability and interpretability. Tools involve calling operators in symbolic languages or invoking APIs (Shen et al., 2023). This requires decomposing a task, grounding intention to tools, and using the results for further actions, relying on natural language reasoning and programming abilities (Cheng et al., 2023; Sur\u0131\u0301s et al., 2023). To assess the ability of language agents to solve complex multi-turn problems using tools, we introduce the part of the MINT\ndataset (Wang et al., 2023b) which focuses on tool-utilization for reasoning. This part includes several adapted datasets for testing: MINT-{GSM8K, MATH} (Cobbe et al., 2021; Hendrycks et al., 2021b) is used to test the model\u2019s ability to solve mathematical problems using a Python interpreter, and MINT-{HotpotQA, MMLU} (Yang et al., 2018; Hendrycks et al., 2021a) assesses the model\u2019s capability to solve knowledge-based questions using Wikipedia searches. At the same time, for the MINT-TheoremQA (Chen et al., 2023a), the model needs to perform knowledge searches on Wikipedia and use a Python interpreter to calculate and draw conclusions.\nSelf-debug with Environment Feedback Self-debug is an important way to test whether a model can incorporate environmental feedback (Jignasu et al., 2023; Olausson et al., 2023; Chen et al., 2023b). In the self-debug scenario, the model usually needs to complete a complex operation sequence, such as Python functions, database queries/modifications, robot action sequences, etc (Gur et al., 2023; Yao et al., 2023). These complex operations sometimes cannot be executed successfully and will return error messages. This requires the model to comprehend this kind of environmental feedback and correct errors, which tests the joint effect of natural language reasoning ability and programming languages. We use rich datasets from multiple benchmarks to evaluate this performance, including multi-turn MINT-MBPP and MINT-HumanEval in MINT (Wang et al., 2023b), SQL and Bash in InterCode (Yang et al., 2023), as well as RoboCodeGen that calls robot APIs through code (Liang et al., 2023). These environments require the model to complete complex tasks and will provide execution errors. In these environments, model performance will vary according to its self-debugging ability, reflecting the ability to incorporate feedback.\nFollow Natural Language Feedback Following natural language feedback is an important mechanism for agents to receive information from humans or other agents (Wang et al., 2022a; Ouyang et al., 2022; Gong et al., 2023). In scenarios where complex problems are solved through multi-turn interactions, the model not only needs to incorporate environmental feedback but also feedback from humans or other agents in order to improve. This mechanism requires the model to understand new instructions based on a context that combines natural language and code, and ground them into new action sequences. To evaluate the model\u2019s ability to accept natural language feedback, we follow the approach of the MINT benchmarks: using a GPT-4 simulated user as a teacher to guide the model in problem-solving. This setup includes a series of MINT datasets (Wang et al., 2023b) to comprehensively evaluate performance after adding natural language feedback in various scenarios.\nExplore in Partially Observable Environments Exploring partially observable environments is a unique and challenging factor in agent scenarios. All the settings mentioned earlier can be considered as fully observable environments, which means that agents can observe all the information of the environment to plan, reason, and make decisions. However, in partially observable environments (also known as Partially Observable Markov Decision Process) (Kurniawati, 2021), agents can only partially observe the environmental information to solve problems. This requires agents to collect information through exploration and continue making decisions. This process places high demands on various abilities of agents, such as natural language planning and reasoning, environmental interaction, etc. To measure this ability, we use three datasets: InterCode-CTF (Yang et al., 2023) and WebArena (Zhou et al., 2023) in digital environments, as well as ALFWorld (Shridhar et al., 2020b) in physical environments. InterCode-CTF provides an OS terminal for models to solve Catch the Flag (CTF) problems where agents need multiple rounds of exploration to obtain the flag. WebArena evaluates agents\u2019 ability to control browsers for task completion through exploration. ALFWorld is a simulated home environment where agents need to explore navigation and complete specific tasks."
        },
        {
            "heading": "4 EXPERIMENTAL RESULTS",
            "text": ""
        },
        {
            "heading": "4.1 LANGUAGE AND CODE CAPABILITIES",
            "text": "The comprehensive evaluations of text and code benchmarks in Table 3 demonstrate the impressive capabilities of Lemur-70B and Lemur-70B-Chat models. Deviating from Llama-2-70B and Llama-2-70B-Chat, which are mostly pre-trained and fine-tuned on text data, the Lemur models augment coding abilities and thereby enhance the overall performance by 4.3% and 14.8% respectively. Alternatively, models like StarCoder-15B, WizardCoder-15B, CodeLlama-34B and CodeLlama-34B-INST, which are primarily trained on code datasets, exhibit solid performance in code benchmarks. On average, Lemur-70B outperforms StarCoder-15B, StarCoderPlus-15B and CodeLlama-34B by 14.3%, 16.8% and 1.9% respectively, and Lemur-70B-Chat surpasses\nWizardCoder-15B and CodeLlama-34B-INST by 18.0% and 9.4% respectively. This commendable increase highlights the virtue of harmonizing textual and coding skills.\nThe synergic text and code abilities enable them to function as language agents. However, a disparity exists between natural language and coding capabilities in current open-source models. Such limitations obstruct these models\u2019 abilities to act as language agents, leading to performance degradation in agent benchmarks. In subsequent sections, we meticulously evaluate various critical capabilities of agents, revealing the importance of synergic text and code abilities to language models."
        },
        {
            "heading": "4.2 AUGMENT WITH TOOLS",
            "text": "In the realm of problem-solving, agents, akin to humans, employ various tools to augment their capabilities. This is exemplified by (Chen et al., 2022), who showcase that the mathematical reasoning prowess of LLM can be significantly enhanced with the aid of Python. As per the data presented in Table 4, it is evident that Lemur-70B-Chat outperforms both Llama-2-70B-Chat and CodeLlama-34B-INST, indicating its superior ability to effectively leverage tools. In addition, we found performance differences between open-source models and closed-source models in the challenging math benchmark tests M-MATH and M-TheoremQA. We further analyzed the errors to better understand this phenomenon, please refer to the content in Appendix D.1.\nTable 4: The tool-augmented reasoning tasks evaluate the model\u2019s capabilities to use tools in the reasoning process. Across five tasks with Python and WikiSearch API as tools, Lemur-70B-Chat outperforms both Llama-2-70B-Chat and CodeLlama-34B-INST by large margins.\nModel Math Reasoning with Python Question Answering with WikiSearch API\nMicroAvg\nM-GSM8K M-MATH M-TheoremQA M-HotpotQA M-MMLU\nLlama-2-70B-Chat 33.33 3.00 2.04 27.91 42.11 20.25 CodeLlama-34B-INST 25.00 4.00 2.04 16.28 30.26 14.87 Lemur-70B-Chat 58.33 6.00 8.16 44.19 56.58 31.65 gpt-3.5-turbo 43.75 26.00 28.57 27.91 56.58 36.71 gpt-4 93.75 57.00 57.14 46.51 80.26 66.77"
        },
        {
            "heading": "4.3 SELF-DEBUG WITH ENVIRONMENT FEEDBACK",
            "text": "The technique of self-debug gains considerable traction in the realm of code generation (Olausson et al., 2023; Zhang et al., 2023). This method consists of models using feedback informa-\ntion, such as interpreter error tracebacks and database observations, to rectify any existing errors. This adaptive capacity is essential for agents because they have to constantly receive and react to feedback from the environment during interaction. As demonstrated in Table 5, the performance of the Lemur-70B-Chat significantly surpasses that of the Llama-2-70B-Chat and CodeLlama-34B-INST in interactive coding benchmarks. This underscores the importance of having balanced capabilities for interactive agents in such environments.\nWe further analyze the results of InterCode-SQL to understand how models incorporate environment feedback. In this setting, agents are provided with database schema and guidelines for SQL game interactions. Acting as agents, the models are tasked with querying databases and responding to questions in a multi-turn interaction environment. Figure 3 shows the Growth in Success Rate across models with each interaction turn. Lemur demonstrates robust performance in the first round, showing an initial performance that is comparable to that of text-bison-001 and slightly lower than gpt-3.5-turbo. Nevertheless, Lemur improves consistently across ten interactive rounds, surpassing the performance of gpt-3.5-turbo eventu-\nally. In contrast, text-bison-001, which exhibits comparable initial performance with Lemur, does not show significant improvement. Llama-2-70B-Chat, while displaying consistent adaptability to feedback throughout the process, has a significant gap in initial performance due to inferior coding ability, hence its success rate remains relatively lower. CodeLlama-34B-INST hardly answers the questions correctly in the first round. We observe that this is because it blindly follows the advice in the game guide and stubbornly executes the show tables command first, instead of trying to understand the provided database structure. After an improvement in the second round, its performance returns to normal. However, the growth remained limited even after ten rounds of interaction, settling at par with Llama-2-70B-Chat and reflecting its relative weakness in adapting to environmental feedback.Additionally, we analyzed the probelm difficulty influence in \u00a7D.2."
        },
        {
            "heading": "4.4 FOLLOW NATURAL LANGUAGE FEEDBACK",
            "text": "Following natural language feedback from users or other agents is an important ability for language agents. It requires agents to understand complex natural language instructions and convert them into symbolic executable sequences based on the current contexts. To evaluate the models\u2019 ability to follow natural language feedback, we follow the evaluation settings of MINT (Wang et al., 2023b), which measures language agents\u2019 ability to leverage natural language feedback using performance improvement. To provide natural language feedback in multi-turn settings, MINT uses GPT-4 to simulate a user providing helpful feedback on the solutions from evaluated language agents.\nTable 7: Performance comparison of different models in partially observable environments InterCode-CTF, WebArena and ALFWorld.\nModel Digital Env. Physical Env.\nIC-CTF WebAreana ALFWorld\nLlama-2-70B-Chat 9.00 1.72 21.64 CodeLlama-34B-INST 16.00 4.06 37.31 Lemur-70B-Chat 22.00 5.30 59.70\ngpt-3.5-turbo 11.00 7.38 41.79 gpt-4 37.00 10.59 84.33\nLlama CodeLlama Lemur\n2\n4\n6\n1.72\n4.06\n5.3\n2.09\n4.68\n5.67\nSc or\ne\nSymbolic Programming\nFigure 4: Performance Comparison of symbolic and programming representation for WebArena.\nWe evaluate models on MINT-Reasoning and MINT-Code with and without GPT-4 feedback and the experimental results are in Table 6. MINT-Reasoning includes five modified benchmarks: GSM8k, MATH, TheoremQA, HotpotQA, and MMLU. MINT-Coding includes modified HumanEval and MBPP. We find that all models can benefit from GPT-4, which means powerful GPT-4 as a teacher can provide helpful feedback even without ground truth. We also calculate \u2206 feedback, which indicates the absolute improvement thanks to GPT-4 feedback. According to the results from Table 6, Lemur-70B-Chat model obtain 8.19 in \u2206feedback, which is significantly better than Llama-2-70B-Chat and CodeLlama-34B-INST."
        },
        {
            "heading": "4.5 EXPLORE IN PARTIALLY OBSERVABLE ENVIRONMENTS",
            "text": "We evaluate language agents in varied environments and find that Lemur-70B-Chat exhibits balanced and commendable performance across all tested tasks. As shown in Table 7, it scored 22.00 in InterCode-CTF, showcasing its proficiency in multifaceted terminal skills and strategic adaptability. Moreover, we conducted further error analysis on CTF tasks in \u00a7D.3 and found that besides general skills of OS terminal, CTF tasks also require cybersecurity domain knowledge. This indicates the challenging requirements for agents to perform well in different environments. In WebArena, it achieved a score of 5.79, reflecting its adeptness in interpreting and executing advanced natural language commands in intricate web environments. In ALFWorld, it demonstrated superior planning and commonsense reasoning with a score of 59.70, successfully navigating and performing in simulated physical environments. While gpt-4 overall exhibits higher scores, Lemur-70B-Chat\u2019s consistent performance across diverse and partially observable environments underscores its versatility and potential in handling real-world, multifarious applications.\nWe also conduct experiments to explore different output formats for actions in the WebArena environment. Instead of merely prompting the language model to directly produce predefined actions\n(e.g. type [id] [content] [press enter after]), we deterministically map the action space to Python representations (e.g. type(id:int, content:str, press enter after:bool)). We then prompt the language model to predict this representation and then parse the model\u2019s Python prediction into the predefined executable actions. Our findings, as shown in Figure 4 indicate that mapping to a Python representation leads to better performance than directly predicting actions. Such results suggest that carefully and rationally selecting intermediate representation, based on the pre-training corpus, can effectively enhance the model\u2019s performance as a language agent, aligning with prior findings (Hu et al., 2022)."
        },
        {
            "heading": "5 RELATED WORK",
            "text": "Transfer Learning on Code The advancement in expansive language models (Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2019; Brown et al., 2020) has catalyzed progress in transfer learning for code-related tasks, further enriched by the continuous pre-training paradigm (Gururangan et al., 2020). Hernandez et al. (2021) offered insights into the interplay between model size, training data, and performance in transferring language ability to code tasks. Several models have been introduced, exhibiting enhanced program synthesis and infilling/completion performance, by undergoing continual pre-training on extensive code data (Feng et al., 2020; Wang et al., 2021; Chen et al., 2021; Rozie\u0300re et al., 2023). However, their intense focus on code often results in a compromise on natural language capabilities. Lemur addresses this by moderately augmenting a large model with a balanced mixture of code and natural language data, maintaining proficiency in both domains.\nInstruction Fine-tuning The process of aligning LLMs to follow instructions, often referred to as instruction tuning, has been primarily directed towards NLP tasks (Wei et al., 2021; Wang et al., 2022b). Recent studies have sought to broaden the use cases of instruction tuning to involve a wider variety of general tasks (Ouyang et al., 2022). Self-instruct method generates instructions using seed instructions, aiding the understanding of how to adapt language models by fine-tuning them on instructions garnered from ChatGPT (Wang et al., 2022a; Zheng et al., 2023; Xu et al., 2023b; Mukherjee et al., 2023). We adopt a similar approach in tuning our model to follow instructions.\nLanguage Agents Language agents are adept at following user instructions and engaging with environments to execute tasks. Recent trends in research and open-source communities have employed Large Language Models (LLMs) (Brown et al., 2020; Chen et al., 2021; Chowdhery et al., 2022; OpenAI, 2023) as the principal controllers for these agents (Yao et al., 2022b; Chase, 2022; Gravitas, 2023; Shinn et al., 2023; Wang et al., 2023a; Xu et al., 2023a; Lin et al., 2023; Yao et al., 2023). This is driven by the LLMs\u2019 demonstrated abilities in reasoning, planning, grounding, and code generation (Wei et al., 2022; Huang et al., 2022a; Ichter et al., 2022; Xie et al., 2023), crucial for comprehending user instructions, grasping the environmental context, and generating executable actions. Lemur seamlessly integrates capabilities in both text and code, enabling the generation of environment-grounded, executable actions essential for constructing language agents.\nAgent Evaluation Benchmarks The rapid evolution of language agents requires an accurate and comprehensive assessment of agents. Recent works pose new dimensions that put language agents in web environments (Deng et al., 2023; Yao et al., 2022a; Zhou et al., 2023), interactive code environments (Yang et al., 2023), digital game (Fan et al., 2022), and household (Puig et al., 2018; Shridhar et al., 2020a;b) to finish certain task under natural language instruction. Apart from collecting new tasks, these agent benchmarks can be established by re-purposing and transforming existing datasets to agent datasets (Wang et al., 2023b; Yang et al., 2023; Liu et al., 2023). Our research combines these tasks, conducts extensive evaluation, and evaluates the model\u2019s capability to construct a language agent from multiple dimensions."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In conclusion, this research underscores the pivotal role of harmonizing natural and programming language proficiencies in the evolution of language models to sophisticated language agents. By developing Lemur and Lemur-Chat, we demonstrated that the meticulous amalgamation of these competencies allows for elevated performance in diverse environments and applications, narrowing the existent capability divide between open-source and proprietary models. We open-sourced both models, intending to foster further research in the field of language models for agents."
        },
        {
            "heading": "A LEMUR AND LEMUR-CHAT",
            "text": "A.1 PRE-TRAINING\nA.1.1 PRE-TRAINING DATA\nWe performed pre-training on the top of Llama-2. The detailed statistics of pre-training data corpus is presented at below in Table 8.\nA.1.2 DATA MIXTURE RATIO DISCUSSION\nThe data mixture for continual pre-training is a complex trade-off between the model size, the vanilla performance of the model, data quality, learning efficiency, catastrophic forgetting etc Hernandez et al. (2021). To roughly estimate a good ratio of composing the training corpora, we conducted a continue pre-training study with Llama models on a set of different code-text ratios and found that the ratio of 10:1 is an efficient ratio for the Llama model to transfer from text to text-code balance.\nDue to computational limits, it is difficult for us to conduct comprehensive experiments and perform systematic studies on this scale. However, we hope that our open-source effective settings and training checkpoints can benefit the community for continual exploration. We believe that a method to predict the optimal continue-pre-training data mixture ratio for a pair of domains to maximize their performance would be very meaningful and interesting, and it is still an open research question for the current large language model Hernandez et al. (2021); Chowdhery et al. (2022); Hoffmann et al. (2022); Aghajanyan et al. (2023).\nWhen generalizing this approach to different models, the data mixture ratio and training steps need further adjustment. For example, smaller models have less capacity for harmonizing both text and code capabilities. Therefore, they may suffer from relatively obvious forgetting of natural language knowledge.. The strategy of data mixture for large language model pretraining is an open and valuable research question. We believe we will have a more efficient and predictable data mixture strategy in future studies.\nA.1.3 TRAINING DETAILS\nWe train our model on a TPUv4-512 pod. Our codebase is based on Jax and EasyLM (Geng, 2023). Following the pretraining methodology of Llama 2 (Touvron et al., 2023a), we used a batch size of\n4M tokens. To improve training efficiency, we packed multiple shorter sequences into each batch entry when possible, an approach known as sequential packing (Raffel et al., 2019).\nOptimization was performed with Adam using a peak learning rate of 4e-5 along with \u03b21 = 0.9 and \u03b22 = 0.95. Gradients were clipped at 1.0 to prevent exploding gradients. A cosine decay schedule was used for the learning rate, with a linear warmup of 2000 steps followed by decaying the learning rate to 10.% of its peak value at the end of training.\nA.2 INSTRUCTION FINE-TUNING\nA.2.1 INSTRUCTION FINE-TUNING DATA\nOpenORCA The OpenOrca dataset is a collection of augmented FLAN Collection data. Currently 1M GPT-4 completions, and 3.2M GPT-3.5 completions. It is tabularized in alignment with the distributions presented in the ORCA paper (Lian et al., 2023) and currently represents a partial completion of the full intended dataset, with ongoing generation to expand its scope. The data is primarily used for training and evaluation in the field of natural language processing.\nOpenAssistant 1 OpenAssistant(OASST1) is a crowdsourced human-annotated assistant-style conversation corpus comprising 161,443 messages, enriched with 461,292 quality ratings. (Ko\u0308pf et al., 2023). The data results from the collaboration of over 13,500 volunteers worldwide.\nShareGPT and Chatlogs We curated English human instructions from ShareGPT and Chatlogs for instruction-tuning. To filter English instructions, we utilized the langdetect package in Python and eliminated any instructions with non-English detection results. Additionally, considering that non-English instructions often contain consecutive non-English characters, we implemented a second filtering step, removing all sentences with three or more consecutive non-English characters. To ensure semantic diversity, we employed instructor(Su et al., 2023) to encode the filtered instructions, calculate cosine similarity, and remove instructions with a similarity score greater than 0.95. After deduplication, we obtained nearly 80K instances, with an average of about 6 rounds of high-quality data per instance.\nEvol-CodeAlpaca We use two open-sourced Evolution-Instruct (Luo et al., 2023) datasets, i.e., Evol-Instruct-Code-80k-v1 and evol-codealpaca-v1, and an execution-verified Python dataset constructed by us. After applying the same deduplication method as the Text data, we obtained \u223c 46K examples.\nA.2.2 INSTRUCTION FINE-TUNING DETAILS\nWe use Huggingface Transformers (Wolf et al., 2019) with Accelerate Library to fine-tune the Lemur model to obtain the Lemur-Chat model. We train on our data collection for two epochs. We use Adam optimizer with a learning rate of 2e-5 and a batch size of 128."
        },
        {
            "heading": "B AGENT EVALUATION",
            "text": "MINT (Wang et al., 2023b) is a well-rounded evaluation that covers a range of tasks repurposed for multi-turn evaluation. It consists of three types of tasks, namely reasoning (MMLU (Hendrycks et al., 2021a), GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021a), TheoremQA (Chen et al., 2023a), HotpotQA (Yang et al., 2018)), code generation (HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021)), and decision-making (ALFWorld (Shridhar et al., 2020b)). To assess the proficiency of language models in employing tools, their reasoning process is reoriented to incorporate tool use. For instance, language models are prompted to utilize the Python calculator to work out mathematical problems, rather than supplying the answer outright. In code generation, the LLM is encouraged to incorporate Python interpreter messages to check generated code. To prevent any misunderstanding, we use the prefix \u201cM-\u201d to differentiate the original dataset from the MINT version in our paper.\nInterCode-Bash/SQL (Yang et al., 2023) are two tasks that serve as an experimental platform that assesses the capacity of extensive language models to integrate feedback from the environment dur-\ning interactive coding. This benchmark evaluates models in a way of generating a series of actions under user instruction, and regards elements such as execution outcomes, and error backtracking, amongst others as environment observations.\nRoboCodeGen (Liang et al., 2023) serves as a specialized evaluation framework focused on robotics-related tasks. It comprises three main types of questions that target spatial reasoning (e.g., identifying the closest point among a set of points), geometric reasoning (e.g., verifying if one bounding box is contained within another), and controls (e.g., PD control).\nInterCode-CTF is a task in the InterCode evaluation suite. Capture the Flag (CTF) is a competitive cybersecurity game that requires LLMs to discover encrypted \u201cflag\u201d hidden within code snippets or file systems. Compared with Bash and SQL generation, CTF is much more complex, requiring agents to have knowledge of multiple coding languages, modularize higher-order objectives into sub-problems, create multi-step plans for solving each problem, and adjust strategies when a plan fails to provide any useful insights.\nWebArena (Zhou et al., 2023) creates self-hostable websites of four popular categories by simulating real-world equivalent functionalities and data. To simulate human problem-solving abilities, WebArena also embeds tools and knowledge resources as standalone websites.\nALFWorld (Shridhar et al., 2020b) is a synthetic environment benchmark adapted from Alfred (Shridhar et al., 2020a) in the text-based interface where agents need to navigate in simulated households (e.g., go to coffee table 1, pick up paper 2, use desk lamp 1) and achieve high-level goals (e.g., check the paper under the desk lamp). Task instances may involve over 50 locations and 50 steps to solve, thus challenging the agent\u2019s ability to plan, track sub-goals, and conduct systematic exploration."
        },
        {
            "heading": "C BASELINE MODELS",
            "text": "StarCoder-15B Li et al. (2023) is a model with 15.5B parameters and 8k context length. It is first trained on 1 trillion tokens sourced from The Stack Kocetkov et al. (2022), a large collection of permissively licensed GitHub repositories, and then finetuned on 35B Python tokens.\nStarCoderPlus-15B Li et al. (2023) is similar to StarCoder-15B, except that it is finetuned in RefinedWeb Penedo et al. (2023), The Stack Kocetkov et al. (2022) and Wikipedia dataset. It is expected to have more balanced text and code capabilities.\nLlama-2 Touvron et al. (2023b) is a model with 70B parameters trained on 2 trillion tokens from public sources. Its training corpus is mostly natural language texts, which enables its strong abilities in textual understanding.\nLlama-2-70B-Chat Touvron et al. (2023b) finetunes Llama-2-70B and is optimized for dialogue use cases.\nCodeLlama-34B Roziere et al. (2023) finetunes Llama-2-34B on code-intensive corpus with 500B tokens.\nCodeLlama-34B-INST Roziere et al. (2023), Code Llama - Instruct is based on Code Llama and fine-tuned with an additional approx. 5B tokens to better follow human instructions.\nWizardCoder-15B Luo et al. (2023) was trained with instrucions on code-intensive datasets. It achieved superior performance on popular code generation benchmarks including HumanEval, HumanEval+, MBPP, and DS1000."
        },
        {
            "heading": "D ERROR ANALYSIS ON LANGUAGE AGENT TASKS",
            "text": "D.1 AUGMENT WITH TOOLS: A CASE STUDY ON MINT-MATH\nIn Table 9, we report the percentage of examples in MINT-MATH dataset with execution errors and invalid actions. From the results, we can see that Llama-2-70B-Chat produces the most execution error, followed by CodeLlama-34B-INST, Lemur-70B-Chat, gpt-3.5-turbo and gpt-4 has the fewest error. This is aligned with the performance of task success rates reported\nin Table 4, which indicates that execution error is an important factor in explaining the performance difference between models.\nOn the other hand, the rate of invalid actions among different models also follows similar trends, except that Llama-2-70B-Chat achieves extremely low error rate. With further investigation into the example output, we find out that Llama-2-70B-Chat usually writes trivial and ineffective actions, which does not contribute to solving the problem.\nD.2 SELF-DEBUG WITH ENVIRONMENT FEEDBACK: A CASE STUDY ON INTERCODE-SQL\ngpt-4 95.16 82.96 86.21 68.67 84.14 In Table 10, We report detailed scores of different models in various spectrums of the Spider evaluation set divided by difficulty level. According to Table 10, as the difficulty level increases, the performance gap between gpt-4 and Lemur-Chat gradually widens. When evaluated with easy questions, the model usually solves the problem in the first round; while in the difficult split, the model needs to iteratively incorporate the environment feedback. The larger gap between LemurChat and gpt-4 indicates better abilities of gpt-4 to perform multi-turn interaction with the environment.\nD.3 EXPLORE IN PARTIALLY OBSERVABLE ENVIRONMENT : A CASE STUDY ON CTF\nWe manually researched the CTF (Capture The Flag) tasks, which is a popular competition program originating from cybersecurity. We manually labeled the problems in 100 CTF tasks and divided each problem into 6 categories. Figure 5 shows the performance comparison between Lemur-70B-Chat and gpt-4. We found that gpt-4 and Lemur-70B-Chat perform significantly better in the \u201dgeneral skills\u201d category than in domain-specific fields such as \u201dcryptography\u201d and \u201dreverse engineering\u201d. This means current language agents methods easily fail on domainspecific scenarios, which guide the future research of language agents."
        }
    ],
    "title": "LEMUR: HARMONIZING NATURAL LANGUAGE AND CODE FOR LANGUAGE AGENTS",
    "year": 2024
}