{
    "abstractText": "Due to privacy or patent concerns, a growing number of large models are released without granting access to their training data, making transferring their knowledge inefficient and problematic. In response, Data-Free Knowledge Distillation (DFKD) methods have emerged as direct solutions. However, simply adopting models derived from DFKD for real-world applications suffers significant performance degradation, due to the discrepancy between teachers\u2019 training data and real-world scenarios (student domain). The degradation stems from the portions of teachers\u2019 knowledge that are not applicable to the student domain. They are specific to the teacher domain and would undermine students\u2019 performance. Hence, selectively transferring teachers\u2019 appropriate knowledge becomes the primary challenge in DFKD. In this work, we propose a simple but effective method AuG-KD. It utilizes an uncertainty-guided and sample-specific anchor to align student-domain data with the teacher domain and leverages a generative method to progressively trade off the learning process between OOD knowledge distillation and domain-specific information learning via mixup learning. Extensive experiments in 3 datasets and 8 settings demonstrate the stability and superiority of our approach. Code available at https://github.com/IshiKura-a/AuG-KD",
    "authors": [
        {
            "affiliations": [],
            "name": "Zihao Tang"
        },
        {
            "affiliations": [],
            "name": "Zheqi Lv"
        },
        {
            "affiliations": [],
            "name": "Shengyu Zhang"
        },
        {
            "affiliations": [],
            "name": "Yifan Zhou"
        },
        {
            "affiliations": [],
            "name": "Xinyu Duan"
        },
        {
            "affiliations": [],
            "name": "Kun Kuang"
        }
    ],
    "id": "SP:fa204cdb37b0d68279eaf8c61a0a08cf0b30b952",
    "references": [
        {
            "authors": [
                "Mart\u0131\u0301n Arjovsky",
                "L\u00e9on Bottou",
                "Ishaan Gulrajani",
                "David Lopez-Paz"
            ],
            "title": "Invariant risk minimization",
            "venue": "CoRR, abs/1907.02893,",
            "year": 2019
        },
        {
            "authors": [
                "Shuanghao Bai",
                "Min Zhang",
                "Wanqi Zhou",
                "Siteng Huang",
                "Zhirong Luan",
                "Donglin Wang",
                "Badong Chen"
            ],
            "title": "Prompt-based distribution alignment for unsupervised domain adaptation",
            "venue": "In Proceedings of the 38th AAAI Conference on Artificial Intelligence (AAAI",
            "year": 2024
        },
        {
            "authors": [
                "Kuluhan Binici",
                "Shivam Aggarwal",
                "Nam Trung Pham",
                "Karianto Leman",
                "Tulika Mitra"
            ],
            "title": "Robust and resource-efficient data-free knowledge distillation by generative pseudo replay",
            "venue": "In ThirtySixth AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Nitay Calderon",
                "Subhabrata Mukherjee",
                "Roi Reichart",
                "Amir Kantor"
            ],
            "title": "A systematic study of knowledge distillation for natural language generation with pseudo-target training",
            "venue": "ACL 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Hanting Chen",
                "Tianyu Guo",
                "Chang Xu",
                "Wenshuo Li",
                "Chunjing Xu",
                "Chao Xu",
                "Yunhe Wang"
            ],
            "title": "Learning student networks in the wild",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Yoojin Choi",
                "Jihwan P. Choi",
                "Mostafa El-Khamy",
                "Jungwon Lee"
            ],
            "title": "Data-free network quantization with adversarial knowledge distillation",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR Workshops",
            "year": 2020
        },
        {
            "authors": [
                "Elliot Creager",
                "J\u00f6rn-Henrik Jacobsen",
                "Richard S. Zemel"
            ],
            "title": "Environment inference for invariant learning",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.-J. Li",
                "K. Li",
                "L. Fei-Fei"
            ],
            "title": "ImageNet: A Large-Scale Hierarchical Image Database",
            "venue": "In CVPR09,",
            "year": 2009
        },
        {
            "authors": [
                "Ning Ding",
                "Yixing Xu",
                "Yehui Tang",
                "Chao Xu",
                "Yunhe Wang",
                "Dacheng Tao"
            ],
            "title": "Source-free domain adaptation via distribution estimation",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Kien Do",
                "Hung Le",
                "Dung Nguyen",
                "Dang Nguyen",
                "Haripriya Harikumar",
                "Truyen Tran",
                "Santu Rana",
                "Svetha Venkatesh"
            ],
            "title": "Momentum adversarial distillation: Handling large distribution shifts in data-free knowledge distillation",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Gongfan Fang",
                "Yifan Bao",
                "Jie Song",
                "Xinchao Wang",
                "Donglin Xie",
                "Chengchao Shen",
                "Mingli Song"
            ],
            "title": "Mosaicking to distill: Knowledge distillation from out-of-domain data",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Gongfan Fang",
                "Jie Song",
                "Xinchao Wang",
                "Chengchao Shen",
                "Xingen Wang",
                "Mingli Song"
            ],
            "title": "Contrastive model invertion for data-free knolwedge distillation",
            "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Geoffrey E. Hinton",
                "Oriol Vinyals",
                "Jeffrey Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "CoRR, abs/1503.02531,",
            "year": 2015
        },
        {
            "authors": [
                "Andrew Howard",
                "Ruoming Pang",
                "Hartwig Adam",
                "Quoc V. Le",
                "Mark Sandler",
                "Bo Chen",
                "Weijun Wang",
                "Liang-Chieh Chen",
                "Mingxing Tan",
                "Grace Chu",
                "Vijay Vasudevan",
                "Yukun Zhu"
            ],
            "title": "Searching for mobilenetv3",
            "venue": "IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Jiaxing Huang",
                "Dayan Guan",
                "Aoran Xiao",
                "Shijian Lu"
            ],
            "title": "Model adaptation: Historical contrastive learning for unsupervised domain adaptation without source data",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Nazmul Karim",
                "Niluthpol Chowdhury Mithun",
                "Abhinav Rajvanshi",
                "Han-pang Chiu",
                "Supun Samarasekera",
                "Nazanin Rahnavard"
            ],
            "title": "C-sfda: A curriculum learning aided self-training framework for efficient source free domain adaptation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Sunok Kim",
                "Seungryong Kim",
                "Dongbo Min",
                "Pascal Frossard",
                "Kwanghoon Sohn"
            ],
            "title": "Stereo confidence estimation via locally adaptive fusion and knowledge distillation",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,",
            "year": 2023
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "pp. 32\u201333,",
            "year": 2009
        },
        {
            "authors": [
                "Kun Kuang",
                "Peng Cui",
                "Susan Athey",
                "Ruoxuan Xiong",
                "Bo Li"
            ],
            "title": "Stable prediction across unknown",
            "year": 2024
        },
        {
            "authors": [
                "Zheqi Lv",
                "Wenqiao Zhang",
                "Shengyu Zhang",
                "Kun Kuang",
                "Feng Wang",
                "Yongwei Wang",
                "Zhengyu Chen",
                "Tao Shen",
                "Hongxia Yang",
                "Beng Chin Ooi"
            ],
            "title": "Duet: A tuning-free device-cloud collaborative parameters generation framework for efficient device model generalization",
            "venue": "In Proceedings of the ACM Web Conference",
            "year": 2023
        },
        {
            "authors": [
                "Zheqi Lv",
                "Wenqiao Zhang",
                "Zhengyu Chen",
                "Shengyu Zhang",
                "Kun Kuang"
            ],
            "title": "Intelligent model update strategy for sequential recommendation",
            "venue": "In Proceedings of the ACM Web Conference",
            "year": 2024
        },
        {
            "authors": [
                "Ningning Ma",
                "Xiangyu Zhang",
                "Hai-Tao Zheng",
                "Jian Sun"
            ],
            "title": "Shufflenet V2: practical guidelines for efficient CNN architecture design",
            "venue": "Computer Vision - ECCV 2018 - 15th European Conference,",
            "year": 2018
        },
        {
            "authors": [
                "Paul Micaelli",
                "Amos J. Storkey"
            ],
            "title": "Zero-shot knowledge transfer via adversarial belief matching",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Yulei Niu",
                "Long Chen",
                "Chang Zhou",
                "Hanwang Zhang"
            ],
            "title": "Respecting transfer gap in knowledge distillation",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Wonpyo Park",
                "Dongju Kim",
                "Yan Lu",
                "Minsu Cho"
            ],
            "title": "Relational knowledge distillation",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Gaurav Patel",
                "Konda Reddy Mopuri",
                "Qiang Qiu"
            ],
            "title": "Learning to retain while acquiring: Combating distribution-shift in adversarial data-free knowledge distillation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Jiangbo Pei",
                "Zhuqing Jiang",
                "Aidong Men",
                "Liang Chen",
                "Yang Liu",
                "Qingchao Chen"
            ],
            "title": "Uncertaintyinduced transferability representation for source-free unsupervised domain adaptation",
            "venue": "IEEE Trans. Image Process.,",
            "year": 2023
        },
        {
            "authors": [
                "Xingchao Peng",
                "Ben Usman",
                "Neela Kaushik",
                "Judy Hoffman",
                "Dequan Wang",
                "Kate Saenko"
            ],
            "title": "Visda: The visual domain adaptation challenge",
            "venue": "CoRR, abs/1710.06924,",
            "year": 2017
        },
        {
            "authors": [
                "Xufeng Qian",
                "Yue Xu",
                "Fuyu Lv",
                "Shengyu Zhang",
                "Ziwen Jiang",
                "Qingwen Liu",
                "Xiaoyi Zeng",
                "TatSeng Chua",
                "Fei Wu"
            ],
            "title": "Intelligent request strategy design in recommender system",
            "venue": "In KDD \u201922: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Adriana Romero",
                "Nicolas Ballas",
                "Samira Ebrahimi Kahou",
                "Antoine Chassang",
                "Carlo Gatta",
                "Yoshua Bengio"
            ],
            "title": "Fitnets: Hints for thin deep nets",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Subhankar Roy",
                "Martin Trapp",
                "Andrea Pilzer",
                "Juho Kannala",
                "Nicu Sebe",
                "Elisa Ricci",
                "Arno Solin"
            ],
            "title": "Uncertainty-guided source-free domain adaptation",
            "venue": "Computer Vision - ECCV 2022 - 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Kate Saenko",
                "Brian Kulis",
                "Mario Fritz",
                "Trevor Darrell"
            ],
            "title": "Adapting visual category models to new domains",
            "venue": "Computer Vision - ECCV 2010, 11th European Conference on Computer Vision,",
            "year": 2010
        },
        {
            "authors": [
                "Shiori Sagawa",
                "Pang Wei Koh",
                "Tatsunori B. Hashimoto",
                "Percy Liang"
            ],
            "title": "Distributionally robust neural networks",
            "venue": "In 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Mingxing Tan",
                "Quoc V. Le"
            ],
            "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
            "venue": "In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Zihao Tang",
                "Zheqi Lv",
                "Shengyu Zhang",
                "Fei Wu",
                "Kun Kuang"
            ],
            "title": "Modelgpt: Unleashing llm\u2019s capabilities for tailored model generation, 2024",
            "year": 2024
        },
        {
            "authors": [
                "Jean-Baptiste Truong",
                "Pratyush Maini",
                "Robert J. Walls",
                "Nicolas Papernot"
            ],
            "title": "Datafree model extraction",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Laurens van der Maaten",
                "Geoffrey Hinton"
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of Machine Learning Research,",
            "year": 2008
        },
        {
            "authors": [
                "Hemanth Venkateswara",
                "Jose Eusebio",
                "Shayok Chakraborty",
                "Sethuraman Panchanathan"
            ],
            "title": "Deep hashing network for unsupervised domain adaptation",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Xiao Wang",
                "Peng Cui",
                "Jing Wang",
                "Jian Pei",
                "Wenwu Zhu",
                "Shiqiang Yang"
            ],
            "title": "Community preserving network embedding",
            "venue": "Proceedings of the ThirtyFirst AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Yiru Wang",
                "Weihao Gan",
                "Jie Yang",
                "Wei Wu",
                "Junjie Yan"
            ],
            "title": "Dynamic curriculum learning for imbalanced data classification",
            "venue": "IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Yuzheng Wang",
                "Zhaoyu Chen",
                "Dingkang Yang",
                "Pinxue Guo",
                "Kaixun Jiang",
                "Wenqiang Zhang",
                "Lizhe Qi"
            ],
            "title": "Model robustness meets data privacy: Adversarial robustness distillation without original data",
            "venue": "CoRR, abs/2303.11611,",
            "year": 2023
        },
        {
            "authors": [
                "Yuzheng Wang",
                "Zhaoyu Chen",
                "Jie Zhang",
                "Dingkang Yang",
                "Zuhao Ge",
                "Yang Liu",
                "Siao Liu",
                "Yunquan Sun",
                "Wenqiang Zhang",
                "Lizhe Qi"
            ],
            "title": "Sampling to distill: Knowledge transfer from open-world data",
            "venue": "CoRR, abs/2307.16601,",
            "year": 2023
        },
        {
            "authors": [
                "Lijin Yang",
                "Yifei Huang",
                "Yusuke Sugano",
                "Yoichi Sato"
            ],
            "title": "Interact before align: Leveraging cross-modal knowledge for domain adaptive action recognition",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Shiqi Yang",
                "Yaxing Wang",
                "Joost van de Weijer",
                "Luis Herranz",
                "Shangling Jui"
            ],
            "title": "Exploiting the intrinsic neighborhood structure for source-free domain adaptation",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Yingguang Yang",
                "Renyu Yang",
                "Hao Peng",
                "Yangyang Li",
                "Tong Li",
                "Yong Liao",
                "Pengyuan Zhou"
            ],
            "title": "Fedack: Federated adversarial contrastive knowledge distillation for cross-lingual and cross-model social bot detection",
            "venue": "Proceedings of the ACM Web Conference 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Hongxu Yin",
                "Pavlo Molchanov",
                "Jos\u00e9 M. \u00c1lvarez",
                "Zhizhong Li",
                "Arun Mallya",
                "Derek Hoiem",
                "Niraj K. Jha",
                "Jan Kautz"
            ],
            "title": "Dreaming to distill: Data-free knowledge transfer via deepinversion",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Min Zhang",
                "Siteng Huang",
                "Wenbin Li",
                "Donglin Wang"
            ],
            "title": "Tree structure-aware few-shot image classification via hierarchical aggregation",
            "venue": "In Proceeding of the 17th European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Min Zhang",
                "Junkun Yuan",
                "Yue He",
                "Wenbin Li",
                "Zhengyu Chen",
                "Kun Kuang"
            ],
            "title": "Map: Towards balanced generalization of iid and ood through model-agnostic adapters",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 11921\u201311931,",
            "year": 2023
        },
        {
            "authors": [
                "Min Zhang",
                "Haoxuan Li",
                "Fei Wu",
                "Kun Kuang"
            ],
            "title": "Metacoco: A new few-shot classification benchmark with spurious correlation",
            "venue": "In Proceedings of the Twelfth International Conference on Learning Representations (ICLR 2024).,",
            "year": 2024
        },
        {
            "authors": [
                "Shengyu Zhang",
                "Tan Jiang",
                "Tan Wang",
                "Kun Kuang",
                "Zhou Zhao",
                "Jianke Zhu",
                "Jin Yu",
                "Hongxia Yang",
                "Fei Wu"
            ],
            "title": "Devlbert: Learning deconfounded visio-linguistic representations",
            "venue": "In MM \u201920: The 28th ACM International Conference on Multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "Shengyu Zhang",
                "Fuli Feng",
                "Kun Kuang",
                "Wenqiao Zhang",
                "Zhou Zhao",
                "Hongxia Yang",
                "Tat-Seng Chua",
                "Fei Wu"
            ],
            "title": "Personalized latent structure learning for recommendation",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Shengyu Zhang",
                "Qiaowei Miao",
                "Ping Nie",
                "Mengze Li",
                "Zhengyu Chen",
                "Fuli Feng",
                "Kun Kuang",
                "Fei Wu"
            ],
            "title": "Transferring causal mechanism over meta-representations for target-unknown crossdomain recommendation",
            "venue": "ACM Trans. Inf. Syst., 2024b. doi: 10.1145/3643807. URL https: //doi.org/10.1145/3643807. Just Accepted",
            "year": 2024
        },
        {
            "authors": [
                "Wenqiao Zhang",
                "Zheqi Lv",
                "Hao Zhou",
                "Jia-Wei Liu",
                "Juncheng Li",
                "Mengze Li",
                "Siliang Tang",
                "Yueting Zhuang"
            ],
            "title": "Revisiting the domain shift and sample uncertainty in multi-source active domain transfer",
            "venue": "arXiv preprint arXiv:2311.12905,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "With the surge of interest in deploying neural networks on resource-constrained edge devices, lightweight machine learning models have arisen. Prominent solutions include MobileNet (Howard et al., 2019), EfficientNet (Tan & Le, 2019), ShuffleNet (Ma et al., 2018), etc. Although these models have shown promising potential for edge devices, their performance still falls short of expectations. In contrast, larger models like ResNet (He et al., 2016) and CLIP (Radford et al., 2021), have achieved gratifying results in their respective fields (Wang et al., 2017; Tang et al., 2024). To further refine lightweight models\u2019 performance, it is natural to ask: can they inherit knowledge from larger models? The answer lies in Knowledge Distillation (Hinton et al., 2015) (KD).\nVanilla KD (Kim et al., 2023; Calderon et al., 2023) leverages massive training data to transfer knowledge from teacher models T to students S, guiding S in emulating T \u2019s prediction distribution. Although these methods have shown remarkable results in datasets like ImageNet (Deng et al., 2009) and CIFAR10 (Krizhevsky, 2009), when training data is unavailable due to privacy concerns (Truong et al., 2021) or patent restrictions, these methods might become inapplicable.\nTo transfer T \u2019s knowledge without its training data, a natural solution is to use synthesized data samples for compensation, which forms the core idea of Data-Free Knowledge Distillation (DFKD) (Binici et al., 2022; Li et al., 2023; Patel et al., 2023; Do et al., 2022; Wang et al., 2023a). These methods typically leverage T \u2019s information, such as output logits, activation maps, intermediate outputs, etc., to train a generator to provide synthetic data from a normally distributed latent variable.\n\u2217Shengyu Zhang and Kun Kuang are corresponding authors.\nThe distillation process is executed with these synthesized data samples. However, DFKD methods follow the Independent and Identically Distributed Hypothesis (IID Hypothesis). They suppose that T \u2019s training data (teacher domain Dt) and the real application (student domain Ds) share the same distribution (Fang et al., 2021b). In case the disparity between these two distributions cannot be neglected, these methods would suffer great performance degradation. Namely, the disparity is denoted as Domain Shift while the distillation without T \u2019s training data under domain shift is denoted as Out-of-Domain Knowledge Distillation (OOD-KD). In Figure 1, we demonstrated the difference among KD, DFKD, and OOD-KD problems, where KD can access both Dt and Ds, while DFKD can access neither Dt or Ds. OOD-KD can access Ds, but has no prior knowledge of Ds. Moreover, KD and DFKD problems require the IID assumption between Dt and Ds, which can hardly satisfied in real applications. Here, OOD-KD problem is designed for address the distribution shift between Dt and Ds. Although domain shift has garnered widespread attention in other fields (Lv et al., 2023; 2024; Zhang et al., 2023c; Huang et al., 2021; Lv et al., 2022), there\u2019s no handy solution in OOD-KD (Fang et al., 2021a). MosiacKD (Fang et al., 2021a) is the state-or-the-art method for addressing OOD-KD problem, but it mainly focuses on the improvement of performance in Dt, ignoring the importance of Ds (i.e. out-of-domain performance). Recently, some studies propose cross-domain distillation (Li et al., 2022; Yang et al., 2022) for OOD-KD, but these methods require grant access to Dt, which is impractical in real applications.\nIn this paper, we focus on the problem of OOD-KD, and to address this problem we are still facing the following challenges: (i) How to selectively transfer teachers\u2019 knowledge. In OOD-KD problem, the difference of the joint distribution P (X,Y ) between teacher domain Dt and student domain Ds creates a significant barrier. Since T is optimized for Dt, faced with data in Ds, T is likely to give inaccurate predictions or fail to reflect the precise relationships between classes in Ds, impeding S\u2019s performance unavoidably. (ii) The absence of T \u2019s training data makes OOD-KD extremely challenging. As T \u2019s training data act as the carrier of knowledge in vanilla KD, without it, knowledge transferring becomes troublesome. In contrast, data in the application scenes are easy to obtain. It is important to notice that their domain-specific information is applicable to Ds, if utilized properly, it is able to benefit S\u2019s training.\nTo tackle these challenges, we propose a simple but effective method: Anchor-Based Mixup Generative Knowledge Distillation (AuG-KD). Our method utilizes an uncertainty-driven and samplespecific anchor to align student-domain data with Dt and leverage a generative method to progressively evolve the learning process from OOD knowledge distillation to domain-specific information learning. Particularly, AuG-KD consists of 3 modules: Data-Free Learning Module, Anchor Learning Module, and Mixup Learning Module. Data-Free Learning Module bears semblance to vanilla DFKD, tackling the absence of Dt. Anchor Learning Module designs an uncertainty-aware AnchorNet to map student-domain samples to \u201canchor\u201d samples in Dt, enabling T to provide proper knowledge for distillation. Mixup Learning module utilizes the \u201canchor\u201d samples to generate a series of images that evolve from Dt to Ds, treating them as additional data for training. As the module progresses, T becomes less certain about them while the domain-specific information gradually becomes important, balancing OOD knowledge distillation and domain-specific information learning ultimately. Extensive experiments attest to the excellent performance of our proposed method. In essence, our contributions can be briefly summarized as follows:\n\u2022 We aim at an important and practical problem OOD-KD. To the best of our knowledge, we are the first to provide a practical solution to it. \u2022 We propose a simple but effective method AuG-KD. AuG-KD devises a lightweight AnchorNet to discover a data-driven anchor that maps student-domain data to Dt. AuGKD then adopts a novel uncertainty-aware learning strategy by mixup learning, which pro-\ngressively loosens uncertainty constraints for a better tradeoff between OOD knowledge distillation and domain-specific information learning. \u2022 Comprehensive experiments in 3 datasets and 8 settings are conducted to substantiate the stability and superiority of our method."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Since OOD-KD is a novel problem, we focus on the concept of Knowledge Distillation first. KD is a technique that aims to transfer knowledge from a large teacher model to an arbitrary student model, first proposed by Hinton et al. (2015). The vanilla KD methods either guide the student model to resemble the teacher\u2019s behavior on training data (Bucila et al., 2006) or utilize some intermediate representations of the teacher (Binici et al., 2022; Romero et al., 2015; Park et al., 2019). In recent years, knowledge distillation has witnessed the development of various branches, such as Adversarial Knowledge Distillation (Binici et al., 2022; Yang et al., 2023), Cross-Modal Knowledge Distillation (Li et al., 2022; Yang et al., 2022), and Data-Free Knowledge Distillation (Li et al., 2023; Patel et al., 2023; Do et al., 2022; Wang et al., 2023a).\nRecently, data-free methods (DKFD) have garnered significant attention. DFKD typically relies on teacher models\u2019 information such as output logits and activation maps to train a generator for compensation from a normally distributed latent variable. Besides, there are also some sampling-based methods utilizing unlabeled data (Chen et al., 2021; Wang et al., 2023b). However, the effectiveness of DFKD methods is based on the assumption of the IID Hypothesis, which assumes that student-domain data is distributed identically to that in Dt. This assumption does not hold in many real-world applications (Arjovsky et al., 2019; Zhang et al., 2020; Liu et al., 2023), leading to significant performance degradation. The violation of the IID Hypothesis, also known as out-of-domain or domain shift, has been extensively discussed in various fields (Huang et al., 2021; Liang et al., 2022; Sagawa et al., 2020; Zhang et al., 2024b; 2023b; Qian et al., 2022). However, little attention has been paid to it within the context of knowledge distillation (Fang et al., 2021a). MosiacKD (Fang et al., 2021a) first proposes Out-of-Domain Knowledge Distillation but their objective is fundamentally different from ours. They use OOD data to assist source-data-free knowledge distillation and focus on in-domain performance. In contrast, we use OOD data for better out-of-domain performance. IPWD (Niu et al., 2022) also focuses on the gap between Dt and Ds. However, different from OOD-KD, they mainly solve the imbalance in teachers\u2019 knowledge Some studies discuss the domain shift problem in cross-time object detection (Li et al., 2022; Yang et al., 2022), but grant access to Dt, which is impractical in real-world scenarios. These studies try to figure out the problems in the context of knowledge distillation. However, they either discuss a preliminary version of the problem or lack rigor in their analysis. In summary, it is crucial to recognize that there is a growing demand for solutions to OOD-KD, while the research in this area is still in its early stage."
        },
        {
            "heading": "3 PROBLEM FORMULATION",
            "text": "To illustrate the concept of Out-of-domain Knowledge Distillation, we focus on its application in image classification. In this work, the term \u201cdomain\u201d refers to a set of input-label pairs denoted as D = {(xi, yi)}Ni=1. Here, the input xi \u2208 X \u2282 RC\u00d7H\u00d7W represents an image with H \u00d7 W dimensions and C channels, while the corresponding label is denoted as yi \u2208 Y \u2282 {0, 1, \u00b7 \u00b7 \u00b7 ,K \u2212 1} := [K], where K represents the number of classes. Vanilla KD methods guide the student model S(\u00b7; \u03b8s) to imitate the teacher model T (\u00b7; \u03b8t) and learn from the ground truth label, formatted as:\n\u03b8\u0302s = argmin \u03b8s\nE(x,y)\u223cPs [ DKL(T (x; \u03b8t) \u2225 S(x; \u03b8s)) + CE(S(x; \u03b8s), y) ] (1)\nwhere CE refers Cross Entropy, Ps is the joint distribution in Ds. In the context of OOD-KD, the teacher domain Dt and the student domain Ds differ in terms of the joint distribution P (X,Y ). For instance, inDt, the majority of the images labeled as \u201ccow\u201d might depict cows on grassy landscapes. On the other hand, the ones in the student domainDs could show cows on beaches or other locations. Unavoidably, T not only learns the class concept but also utilizes some spurious correlations (e.g., associating the background \u201cgrass\u201d with the cow) to enhance its training performance.\nHowever, as the occurrence of spurious correlations cannot be guaranteed in the target application, blindly mimicking the behavior of T is unwise. Hence, the key challenge lies in leveraging the\nteacher\u2019s knowledge effectively, accounting for the domain shift between Ds and Dt (Zhang et al., 2022; 2023a; 2024a; Bai et al., 2024). Vanilla methods bridge this domain shift with the assistance of T \u2019s training data. However, in OOD-KD, due to various reasons (privacy concerns, patent protection, computational resources, etc.), quite a number of models are released without granting access to their training data and even some of the models are hard to adapt. This situation further amplifies the difficulty of the problem. Hence, we present the definition of OOD-KD herein.\nProblem Definition: Given an immutable teacher model T with its parameter \u03b8t and labeled student-domain data Ds = {(xi, yi)}Nsi=1 whose joint distribution P (X,Y ) differs from that of Dt but the label space is the same (Yt = Ys), the objective of OOD-KD is to train a student model S(\u00b7; \u03b8s) only with access toDs and T , leaving the teacher model T unchanged in the overall process."
        },
        {
            "heading": "4 METHOLOGY",
            "text": "To address OOD-KD, we propose a simple but effective method AuG-KD. Generally, AuG-KD is composed of three modules: Data-Free Learning Module, Anchor Learning Module, and Miuxp Learning Module, as is vividly shown in Figure 2. In a certain module, the green blocks inside are trained with the help of fixed red blocks. The overall algorithm utilizes these 3 modules sequentially. For space issues, we leave the pseudo-code of our overall method in Appendix A. In the following sections, we provide detailed descriptions of each module."
        },
        {
            "heading": "4.1 MODULE 1: DATA-FREE LEARNING",
            "text": "To leverage T \u2019s knowledge without access to its training data, DFKD methods are indispensable. This module follows the vanilla DFKD methods, training a Generator G(\u00b7; \u03b8g) : Z 7\u2192 X from a normally-distributed latent variable z0 \u223c N (0, 1) under the instructions of the teacher model T . The generated image is denoted as x = G(z0; \u03b8g), while the normalized version of it is x\u0303 = N(x). y = argmaxT (x\u0303; \u03b8t) means the labels predicted by T . The dimension of Z is denoted as Nz . H refers to the Information Entropy. AM stands for Activation Map, which observes the mean and variance of the outputs from BatchNorm2d layers.\nLKL(z0) = DKL(S(x\u0303; \u03b8s) \u2225 T (x\u0303; \u03b8t)) (2) LCE(z0) = CE ( T (x\u0303; \u03b8t), y ) (3)\nLgenerator = Ez0\u223cN (0,1) [ \u2212LKL + LCE + \u03b1g \u00b7H(T (x\u0303; \u03b8t)) + AM(T (x\u0303; \u03b8t)) ] (4)\n\u03b8\u0302g = argmin\u03b8g Lgenerator (5)\nMeanwhile, an additional encoder E(\u00b7; \u03b8e) : X,Y 7\u2192 Z is trained, keeping \u03b8g fixed. It absorbs the generated image x = G(z0; \u03b8g) and the label y = argmaxT (x\u0303; \u03b8t) as input and outputs the related latent variable z = E(x, y; \u03b8e) with Eq. 6, where MSE represents the mean squared error.\n\u03b8\u0302e = argmin \u03b8e Lencoder = argmin \u03b8e\nEz0\u223cN (0,1) [ MSE(z0, z) + \u03b1e \u00b7DKL(z \u2225 z0) ] (6)\nWhen training the encoder, the student model S is trained simultaneously with Eq. 7.\n\u03b8\u0302s = argmin \u03b8s Lstudent = argmin \u03b8s Ez0\u223cN (0,1)[LKL] (7)"
        },
        {
            "heading": "4.2 MODULE 2: ANCHOR LEARNING",
            "text": "Anchor Learning Module trains an AnchorNet (m,\u03c8; \u03b8a) to map student-domain data to the teacher domain. It consists of a class-specific mask m(\u00b7; \u03b8a) : Y 7\u2192 {0, 1}Nz and a mapping function \u03c8(\u00b7; \u03b8a) : Z 7\u2192 Z, which are trained concurrently in this module. m and \u03c8 are integrated into a lightweight neural network AnchorNet as shown in Figure 2. Detailed implementations of them are provided in Appendix A. This idea draws inspiration from invariant learning (Creager et al., 2021; Kuang et al., 2018), which is proposed especially for the problem of domain shift. IRM (Arjovsky et al., 2019) assumes the partial invariance either in input space or latent space, implying the presence of some invariant factors across domains despite the domain shift. In this work, we assume that a portion of the latent variable z exhibits such invariance:\nAssumption 1 Given any image pair ((x1, y1), (x2, y2)) that is identical except for the domainspecific information, there exists a class-specific binary mask operator m(\u00b7; \u03b8a) : Y 7\u2192 {0, 1}Nz that satisfies the partial invariance properties in the latent space under the EncoderE(\u00b7; \u03b8e) : X,Y 7\u2192 Z, as shown in Eq. 8. The mask masks certain dimensions in the latent space to zero if the corresponding component in it is set to 0 or preserves them if set to 1.\n(1\u2212m(y1; \u03b8a))\u2299 E(x1, y1; \u03b8e) \u2261 (1\u2212m(y2; \u03b8a))\u2299 E(x2, y2; \u03b8e) (8) \u2299 in Eq 8 is the element-wise multiplication operator. Assumption 1 sheds light on the method of effectively transferring T \u2019s knowledge: just to change the domain-specific information. With it, we can obtain the invariant part in the latent space of an arbitrary data sample. If we change the variant part, we can change the domain-specific information and thus can change the domain of the data sample to Dt. As a result, T can provide more useful information for distillation. To direct \u03c8 to change the domain-specific information and map the samples to Dt, we introduce the uncertainty metric U(x;T ) which draws inspiration from Energy Score (Liu et al., 2020), formulated as:\nU(x;T ) = \u2212t \u00b7 log K\u2211 i exp Ti(x) t (9)\nwhere t is the temperature and Ti(x) denotes the ith logits of image x output by the teacher model T . U(x;T ) measures T \u2019s uncertainty of an arbitrary image x. The lower the value of U(x;T ) is, the more confident T is in its prediction.\nTo preserve more semantic information during the mapping, we include the cross-entropy between T \u2019s prediction on the mapped image and the ground truth label in the loss function of AnchorNet, as shown in Eq. 10-11, where x\u2032 = G(z\u2032; \u03b8g) and z\u2032 = m(y; \u03b8a) \u2299 \u03c8(z; \u03b8a) + (1 \u2212m(y; \u03b8a)) \u2299 z represent the resultant images and latent variables after mapping individually. We denote x\u2032 as \u201canchor\u201d. These anchors are in the teacher domain Dt. T is hence more confident about its prediction on them and can thus provide more useful information for distillation.\nFor simplicity, the portion of invariant dimensions in z is preset by \u03b1a. Linv regulates it based on the absolute error between the l1-norm and the desired number of ones in the mask m.\nLinv(y) = |(1\u2212 \u03b1a) \u00b7Nz \u2212 \u2225m(y; \u03b8a)\u22251| (10)\n\u03b8\u0302a = argmin \u03b8a Lanchor = argmin \u03b8a\nE(x,y)\u223cPs [ U(x\u2032;T ) + Linv(y) + \u03b2a \u00b7 CE(T (x\u2032; \u03b8t), y) ] (11)"
        },
        {
            "heading": "4.3 MODULE 3: MIXUP LEARNING",
            "text": "This module is a process of knowledge distillation usingDs and mixup data provided by AnchorNet. To be specific, for an arbitrary image x in Ds, Encoder E(\u00b7; \u03b8e) encodes it to z and AnchorNet (m,\u03c8; \u03b8a) maps it to z\u2032. Mixup Learning utilizes the mapping from z\u2032 to z to generate a series of images that evolves during the training process. The evolution is governed by a stage factor f , which is given by a monotonically non-decreasing scheduler function F (\u00b7; a, b) : N 7\u2192 [0, 1]. Parameter a controls the rate of the change of mixup images, while b determines their starting point. These parameters adhere to the property F (a \u00b7 \u266fEpoch; a, b) = 1 and F (0; a, b) = b, where \u266fEpoch represents the total number of training epochs. The mixup samples are formulated as shown in Eq. 12. Figure 3 vividly illustrates the mixup samples provided. xm = (1\u2212 f) \u00b7G((1\u2212 f) \u00b7 z\u2032 + f \u00b7 z; \u03b8g) + f \u00b7 x (12) As the latent variable evolves from z\u2032 to z, the mixup samples evolve from Dt to Ds. Consequently, at the beginning of training, the teacher model T exhibits more certainty regarding the samples and can provide more valuable knowledge. As training progresses, T becomes less certain about its predictions, which thus encourages the student model to learn more from the student-domain data."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "5.1 EXPERIMENTAL SETTINGS",
            "text": "The proposed method is evaluated on 3 datasets Office-31 (Saenko et al., 2010), Office-Home (Venkateswara et al., 2017), and VisDA-2017 (Peng et al., 2017). These datasets consist of multiple domains and are hence appropriate to our study.\nOffice-31 This dataset contains 31 object categories in three domains: Amazon, DSLR, and Webcam with 2817, 498, and 795 images respectively, different in background, viewpoint, color, etc.\nOffice-Home Office-Home is a 65-class dataset with 4 domains: Art, Clipart, Product, and RealWorld. Office-Home comprises 15500 images, with 70 images per class on average.\nVisDA-2017 VisDA-2017 is a 12-class dataset with over 280000 images divided into 3 domains: train, validation, and test. The training images are simulated images from 3D objects, while the validation images are real images collected from MSCOCO (Lin et al., 2014).\nMain experiments adopt ResNet34 (He et al., 2016) as the teacher model and MobileNet-V3-Small (Howard et al., 2019) as the student model. Usually, teacher models are trained with more data samples (maybe from multiple sources) than student models. To better align with real-world scenarios, all domains are utilized for training the teacher model T , except for one domain that is reserved specifically for adapting the student model S. Since Office-31 and Office-Home do not have official train-test splits released, for evaluation purposes, the student domainDs of these two datasets is divided into training, validation, and testing sets using a seed, with proportions set at 8:1:1 respectively. As to VisDA-2017, we split the validation domain into 80% training and 20% validation and directly use the test domain for test. The performance of our methods is compared with baselines using top 1, 3, and 5 accuracy metrics.\nGiven that OOD-KD is a relatively novel problem, there are no readily available baselines. Instead, we adopt state-of-the-art DFKD methods, including DFQ (Choi et al., 2020), CMI (Fang et al., 2021b), DeepInv (Yin et al., 2020), ZSKT (Micaelli & Storkey, 2019), and PRE-DFKD (Binici et al., 2022), and fine-tune them on the student domain. One more baseline \u201cw/o KD\u201d is to train the student model S without the assistance of T , starting with weights pre-trained on ImageNet (Deng et al., 2009). To ensure stability, each experiment is conducted five times using different seeds, and the results are reported as mean \u00b1 standard variance. Due to limited space, we leave hyperparameter settings, full ablation results, and combination with other baselines (like Domain Adaptation methods) to Appendix B and C."
        },
        {
            "heading": "5.2 RESULTS AND OBSERVATIONS",
            "text": "Our main results are summarized in Table 1, 2, and 3. Extensive experiments solidly substantiate the stability and superiority of our methods. In this section, we will discuss the details of our results.\nLarger domain shift incurs larger performance degradation. It is evident that all the teacher models experience significant performance degradation when subjected to domain shift. The extent of degradation is directly proportional to the dissimilarity between the student domain Ds and the teacher domain Dt. For instance, in Office-Home, Art is the most distinctive domain, it is significantly different from other domains. As a result, in the CPR\u2192A setting, the performance of the teacher model T exhibits the largest decline, with an approximate 70% drop absolutely. The same\nphenomenon can be observed in VisDA-2017, where the train domain is 3D simulated images, but the others are real-world photographs. Moreover, the problem of performance degradation can be amplified by the imbalance amount of training data between T and S. Usually, we assume that teacher models are trained with more data samples. When the assumption violates, like DW \u2192 A in Office-31, where the Amazon domain is larger than the sum of other domains, the issue of performance degradation becomes more prominent.\nDFKD methods are unstable but can be cured with more data samples. It is worth noting that the standard variance of each method, in most settings, is slightly high. This observation can be attributed to both the inherent characteristics of DFKD methods and the limited amount of data for adaptation. As DFKD methods train a generator from scratch solely based on information provided by the teacher model, their stability is not fully guaranteed. Although the remedy to it goes beyond our discussion, it is worth noting that as the amount of data increases (Office-31 (5000) to VisDA2017 (28000)), these methods exhibit improved stability (Office-31 (7.6) to VisDA-2017 (0.3)).\nAnchorNet DOES change the domain of data samples. To make sure AnchorNet does enable T to provide more useful information, we observe the mixup data samples in Mixup Learning Module under the setting Amazon, Webcam\u2192 DSLR (AW\u2192 D) in Office-31, as shown in Figure 3. These domains exhibit variations in background, viewpoint, noise, and color. Figure 1 gives a few examples in Amazon (the right up) and DSLR (the right bottom). Obviously, Amazon differs from other domains \u2013 the background of the samples in it is white. In AW\u2192 D, when f = 0 the images are closer to Dt, with white backgrounds (Amazon has more samples than Webcam). As f goes larger, the mixup samples get closer to Ds, depicting more features of DSLR."
        },
        {
            "heading": "5.3 ABLATION STUDY",
            "text": "To further validate the effectiveness of our methods, we perform ablation experiments from three perspectives: Framework, Hyperparameter, and Setting. In line with our main experiments, each experiment is conducted five times with different seeds to ensure the reliability of the results. For simplicity, we focus on Amazon, Webcam\u2192 DSLR setting in Office-31.\n(a) Ablation study on the framework of our method. Framework Ablation: Amazon, Webcam \u2192 DSLR\nMethod Acc Acc@3 Acc@5\nM1 33.7\u00b15.1 56.1\u00b18.1 68.6\u00b13.1\nM1+M2+M3 80.1\u00b15.7 92.3\u00b14.1 96.4\u00b12.1 (w/o Mixup) 46.4\u2191 \u00b10.6\u2191 36.2\u2191 \u00b14.0\u2193 27.8\u2191 \u00b11.0\u2193\nM1+M3 83.5\u00b14.1 94.5\u00b12.9 96.1\u00b12.03.4\u2191 \u00b11.6\u2193 3.2\u2191 \u00b11.2\u2193 0.3\u2193 \u00b10.1\u2193\nM1+M2+M3 84.3\u00b13.1 94.9\u00b12.6 97.6\u00b10.80.8\u2191 \u00b11.0\u2193 0.4\u2191 \u00b10.3\u2193 1.6\u2191 \u00b11.1\u2193\n(b) Ablation study (Acc) on different T \u2192 S pairs. Setting Ablation: Amazon, Webcam \u2192 DSLR\nT \u2192 S pair r34\u2192 mb r50 \u2192 r18 r34 \u2192 sf r34 \u2192 ef\nDFQ+ 80.4\u00b15.7 87.5\u00b15.7 86.3\u00b12.4 90.2\u00b14.6 w/o KD 63.5\u00b17.9 84.3\u00b14.8 79.6\u00b11.8 85.1\u00b14.7 PRE-DFKD+ 68.3\u00b119.5 79.2\u00b15.6 87.9\u00b15.1 83.9\u00b14.9\nOurs 84.3\u00b13.1 88.2\u00b14.3 88.6\u00b15.1 91.8\u00b13.5"
        },
        {
            "heading": "5.3.1 FRAMEWORK ABLATION",
            "text": "Here, we evaluate the effectiveness of our modules. Framework ablation studies traditionally involve masking parts of the proposed modules for experimental purposes. Yet, it is essential to recognize: 1. Module 1 is fundamental to our method and is non-removable; 2. Module 2 serves to support Module 3. There is no need to test the results only with Module 1&2. Hence, our investigation focuses on the outcomes absent Module 2, and absent both Module 2 and 3, denoted as M1+M3 and M1, respectively. Additionally, our analysis dives into Module 3, where we omit the mixup samples to evaluate their critical role, denoted as M1+M2+M3 (w/o Mixup). It\u2019s worth noting that there is no need to add one more setting w/o M2 & Mixup here since it makes no difference to M1+M2+M3 (w/o Mixup). Consequently, we get three distinct ablation scenarios: M1, M1+M3, and M1+M2+M3 (w/o Mixup). To be specific, in M1, we directly choose S\u2019s best checkpoint in Module 1 and test it onDs. In M1+M2+M3 (w/o Mixup), the model trains solely onDs. In M1+M3, we mask AnchorNet by equating its output z\u2032 with its input z and then proceed with the method.\nThe results are presented in Table 4a. The performance improvement between M1 and M1+M2+M3 (w/o Mixup) mainly stems from the supervision of Ds. As M1 is a simple DFKD, the striking performance gap underscores the urgent need for solutions to OOD-KD. The considerable enhancement\nevidences the efficacy of Module 2 and 3 in remedying domain shifts. The rise in average accuracy coupled with reduced variance firmly attests to the significance of each component in our method."
        },
        {
            "heading": "5.3.2 SETTING ABLATION",
            "text": "In this study, we change the T \u2212 S pair in our experiments. We additionally employ ResNet50\u2192 ResNet18 (r50\u2192 r18), ResNet34\u2192 ShuffleNet-V2-X0-5 (r34\u2192 sf) and ResNet34\u2192 EfficientNetB0 (r34 \u2192 ef) in this study. The ResNet50 \u2192 ResNet18 pair is a commonly used evaluation pair in traditional distillation methods, while ShuffleNet and EfficientNet are well-known lightweight neural networks suitable for edge devices. These pairs are compared with several effective baselines in our main experiments. The results of this study are displayed in Table 4b, which confirm the effectiveness of our methods across different teacher-student distillation pairs."
        },
        {
            "heading": "5.3.3 HYPERPARAMETER ABLATION",
            "text": "In this ablation study, our primary focus is on two hyperparameters, namely a and b in Module 3, which govern the speed and starting point of the mixup data samples. We perform a grid study on the values of a and b within their domain [0, 1], with a step size of 0.05. Since a = 0 is useless but causes the division-by-zero problem, we set the minimum value of a to step size 0.05.\nDetailed results are depicted in Figure 4. Due to the limited space, we present only a portion of a \u2212 b assignments here, with more results included in Appendix C. The red line in the figures represents the baseline, wherein no mixup data but only raw images are provided. Notably, the blue line consistently surpasses the red line over the majority of the range, testifying to the effectiveness of our method. Both Figure 4a and 4c demonstrate a slight decrease in performance as b increases, suggesting that an excessively large assignment of b is not preferred."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this work, we dive into the problem of Out-of-Domain Knowledge Distillation to selectively transfer teachers\u2019 proper knowledge to students. Further, we propose a simple but effective method AuG-KD. It utilizes a data-driven anchor to align student-domain data with the teacher domain and leverages a generative method to progressively evolve the learning process from OOD knowledge distillation to domain-specific information learning. Extensive experiments validate the stability and superiority of our approach. However, it is worth emphasizing that the research on OOD-KD is still in its early stages and considered preliminary. Therefore, we encourage further attention and exploration in this emerging and practical field."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported by National Science and Technology Major Project (2022ZD0119100), the National Natural Science Foundation of China (62376243, 62037001, U20A20387), Scientific Research Fund of Zhejiang Provincial Education Department (Y202353679), and the StarryNight Science Fund of Zhejiang University Shanghai Institute for Advanced Study (SN-ZJU-SIAS-0010)."
        },
        {
            "heading": "A METHOD DETAILS",
            "text": "Our proposed method consists of three main modules. Data-Free Learning Module serves as the cornerstone of the entire approach. It trains a teacher domain Dt\u2019s generator G(\u00b7; \u03b8g) : Z 7\u2192 X and encoder E(\u00b7; \u03b8e) : X,Y 7\u2192 Z and warmups the student model S in advance. In Anchor Learning Module, a mapping is established within the latent space Z that aligns the distribution of Ds to that of Dt, taking into account the uncertainty metric provided by T . Finally, in Mixup Learning Module, the mapping obtained in Anchor Learning Module is employed to generate synthetic data of Dt and mixuped with Ds. The pseudo-code of our proposed method is displayed in Algorithm 1.\nAlgorithm 1: Pseudo-code of our proposed method Input: Student domain data Ds, Batch size b, Latent size Nz Output: Optimized S(\u00b7; \u03b8s) /* Data-Free Learning Module, trains G(\u00b7; \u03b8g), E(\u00b7; \u03b8e), S(\u00b7; \u03b8s) */ Sample zval from normal distribution, with size (10b,Nz); for i\u2190 1 to \u266fepoch do\nSample z0 from normal distribution with size (b,Nz); Compute Lgenerator and update \u03b8g; for \u2190 1 to 5 do\nCompute Lencoder, Lstudent and update \u03b8e, \u03b8t; end Evaluate G(\u00b7; \u03b8g), E(\u00b7; \u03b8e), S(\u00b7; \u03b8s) with zval, save the best parameter;\nend /* Anchor Learning Module, trains (m,\u03c8; \u03b8a) */ for i\u2190 1 to \u266fepoch do\nfor (x, y) \u2208 Ds\u2019s training set do z \u2190 E(x, y; \u03b8e); z\u2032 \u2190 m(y; \u03b8a)\u2299 \u03c8(z; \u03b8a) + (1\u2212m(y; \u03b8a))\u2299 z; x\u2032 \u2190 G(z\u2032; \u03b8g); Compute Lanchor and update \u03b8g end Evaluate (m,\u03c8; \u03b8a) with Ds\u2019s validation set, save the best parameter;\nend /* Mixup Learning Module, trains S(\u00b7; \u03b8s) */ for i\u2190 1 to \u266fepoch do\nfor (x, y) \u2208 Ds\u2019s training set do f \u2190 F (i\u2212 1; a, b); xm \u2190 (1\u2212 f) \u00b7G(f \u00b7 z\u2032 + (1\u2212 f) \u00b7 z; \u03b8g) + f \u00b7 x; (x, y)\u2190 (x||xm, y||y)/* Concatenate two batches */ Compute Lstudent with newly get (x, y) and update \u03b8s end Evaluate S(\u00b7; \u03b8s) with Ds\u2019s validation set, save the best parameter;\nend\nIn Anchor Learning Module, we integrate the mask operator m and the mapping function \u03c8 into a lightweight neural network AnchorNet. Concretely, the network is implemented with Pytorch as shown in Code A. In a forward pass, we first embed the class label to get the class-specific mask and then map the latent variable back to Dt. To retrain domain-invariant information during mapping, we combine the mapped latent variable with the original one with the help of the class-specific mask.\nclass AnchorNet(nn.Module): \"\"\"AnchorNet\nArgs: latent_size (int): Latent dimensionality num_classes (int): Number of classes\nThe AnchorNet module takes an input tensor and a label tensor as input.\nIt embeds the class labels, generates a mask based on the embedding, masks the input, and passes it through a CNN module.\nThe CNN module consists of 1D convolutional and linear layers.\nThe weights are initialized from a uniform distribution in __init__.\nThe forward pass: 1. Embeds class labels 2. Generates mask from label embedding 3. Masks input tensor 4. Passes masked input through CNN module 5. Returns masked output and mask tensor\n\"\"\" def __init__(self, latent_size: int, num_classes: int):\nsuper().__init__()\nself.num_classes = num_classes self.embed_class = nn.Linear(num_classes, latent_size)\nself.mask = nn.Sequential( nn.Linear(latent_size, latent_size), nn.Linear(latent_size, latent_size), nn.Linear(latent_size, latent_size), nn.BatchNorm1d(latent_size), nn.Sigmoid(), Lambda(lambda x: x - 0.5), nn.Softsign(), nn.ReLU()\n)\nself.module = nn.Sequential( View(1, -1), nn.Conv1d(1, 4, 3, 1, 1), nn.BatchNorm1d(4), nn.LeakyReLU(), nn.Conv1d(4, 8, 3, 1, 1), nn.BatchNorm1d(8), nn.LeakyReLU(), nn.Conv1d(8, 4, 3, 1, 1), nn.BatchNorm1d(4), nn.LeakyReLU(), View(-1), nn.Linear(4 * latent_size, latent_size), )\n... (initializations)\ndef forward(self, inputs: Tensor, **kwargs) -> Tuple[Tensor, Tensor]: y = self.embed_class(one_hot(kwargs[\u2019labels\u2019], self.num_classes)) mask = self.mask(y)\nmasked_inputs = inputs * mask z = self.module(masked_inputs) return masked_inputs * z + (1 - masked_inputs) * inputs, mask\nIn Mixup Learning Module, the generation of mixup samples is controlled by a monotonically nondecreasing scheduler function F (\u00b7; a, b) : N 7\u2192 [0, 1], which is parameterized by a and b. Parameter a controls the rate of the change of mixup images, while b determines their starting point. These parameters adhere to the property F (a \u00b7 \u266fEpoch; a, b) = 1 and F (0; a, b) = b. The idea of scheduler function draws inspiration from Curriculum Learning (Wang et al., 2019). All of our experiments directly adopt the simplest linear scheduler function:\nF (x; a, b) = 1\u2212 b\na \u00b7 \u266fEpoch \u00b7min(max(x, 0), a \u00b7 \u266fEpoch) + b"
        },
        {
            "heading": "B EXPERIMENT DETAILS",
            "text": "Each experiment is conducted using a single NVIDIA GeForce RTX 3090 and takes approximately 1 day to complete.\nB.1 HYPERPARAMETERS AND TRAINING SCHEDULES\nWe summarize the hyperparameters and training schedules of AuG-KD on the three datasets in Table 5.\nNotably, the temperature of the KL-divergence in Module 3 is set to 10. As to baselines, we adopt their own hyperparameter settings. During the fine-tuning stage of each baseline, the standard setting involves 200 epochs, with a learning rate of 1e-3 and weight decay of 1e-4. Slight adjustments for optimal results are granted.\nModule 3 is determined by two significant hyperparameters a and b, which control mixup data\u2019s evolution speed and starting point. In the section of the ablation study, we have demonstrated that most a \u2212 b settings are effective. For reproducibility, we provide detailed a \u2212 b assignments in our main experiments in Table 6."
        },
        {
            "heading": "C ADDITIONAL RESULTS",
            "text": "C.1 VISUALIZATION ON MASK PROVIDED BY ANCHORNET\nIn Module 2, AnchorNet integrates the mask operator m and the mapping function \u03c8 into a lightweight neural network. The mask is class-specific and plays a crucial role in retaining domain-invariant knowledge in the latent space. To vividly demonstrate the effectiveness of the mask, we conduct t-SNE (van der Maaten & Hinton, 2008) on the latent variables and the masked version of them. To be specific, we use AnchorNet and Encoder trained under the setting AW\u2192D in Office-31 and select 32 images for each class (31 classes in total). For each image (x, y), we encode it to get the latent variable z = E(x; \u03b8e), and obtain the class-specific mask m(y; \u03b8a). Next, we obtain the masked latent variable z\u2032 = (1 \u2212m(y; \u03b8a)) \u2299 z. The t-SNE results on z and z\u2032 are displayed in Figure 5 and 6 Each displays a distribution of data points plotted against two t-SNE components. The points are colored and shaped differently to represent different classes within the latent space.\nIn Figure 5, the distribution is quite mixed, with no distinct clusters or separation between the different classes. In contrast, in Figure 6, after applying mask operation on the latent variables, there appears to be a more distinct separation between different classes. Clusters of the same shapes and colors are more evident, indicating that the mask operation has enhanced the class-specific knowledge within the latent space.\nC.2 FULL ABLATION STUDY RESULTS\nIn previous sections, we thoroughly examined the impact of various assignments of a and b on the overall performance. For the sake of limited space, we only demonstrate part of the results previously. Full results are provided in Figure 7-12. The red line in the figures represents the baseline, wherein no mixup data but only raw images are provided. These results are in alignment with the observations before. Notably, the blue line consistently surpasses the red line over the\nmajority of the range. Most a \u2212 b assignments provide effective mixup samples that better transfer the knowledge of the teacher model.\nC.3 COMBINATIONS WITH MORE METHODS\nAlthough the mainstream of Data Free Knowledge Distillation lies in the generation methods, i.e., they rely on a generator to generate teachers\u2019 training data for compensation (Li et al., 2023), there exist some sampling methods relying on the tremendous unlabeled data samples in the wild. For example, DFND (Chen et al., 2021) identifies images most relevant to the given teacher and tasks from a large unlabeled dataset, selects useful data samples and finally uses them to conduct supervised learning to the student network with the labels the teacher give. ODSD (Wang et al., 2023b) sample open-world data close to the original data\u2019s distribution by an adaptive sampling module, introduces a low-noise representation to alleviate the domain shifts and builds a structured relationship of multiple data examples to exploit data knowledge.\nWhen discussing the problem of OOD-KD, some readers might come up with Source-Free Domain Adaptation (Huang et al., 2021; Pei et al., 2023; Ding et al., 2022), a specific setting in Domain Adaptation. Although it falls beyond the scope of our current work, for the sake of rigor, we now highlight the differences between OOD-KD and SFDA.\nSFDA assumes the absence of training data for the source models, which is akin to the scenario of the teacher model in OOD-KD. However, SFDA does its adaptation on the source model and assumes that the target model shares the same framework as the source model. The difference between source model (teacher model) and target model (student model) in the framework makes integrating teachers\u2019 knowledge into the SFDA framework remains an open problem. Moreover, some SFDA methods involve specific modifications to the backbone model, which violates the immutability of the teacher model in OOD-KD. For example, approaches like SHOT (Liang et al., 2020) and SHOT++ (Liang et al., 2022) divide the backbone model into feature extractor and classifier, sharing the classifier across domains. SFDA methods like C-SFDA (Karim et al., 2023) utilize confident examples for better performance. Their performance is limited when deploying to resource-constrained edge devices. What\u2019s worse, some SFDA methods base their methodology only on ResNet series models (Yang et al., 2021; Kundu et al., 2022), which is inapplicable to most lightweight neural networks.\nWe demonstrate the results of some splendid SFDA methods (C-SFDA (Karim et al., 2023), SFDADE (Ding et al., 2022)), Uncertainty-SFDA (U-SFDA) (Roy et al., 2022) under the setting Office-31 Amazon, Webcam \u2192 DSLR in Table 7. Since in OOD-KD, T remains immutable, adaptation is adopted to S directly. As SFDA methods do not make use of ground truth labels or teachers\u2019 knowledge, in order to align with our methods, we apply additional distillation progress after employing them. However, S suffers great performance degradation confronted with domain shift, similar to that observation of T in main experiments. Consequently, they cannot be fully exploited, resulting in inferior performance compared to DFKD methods."
        }
    ],
    "title": "OUT-OF-DOMAIN KNOWLEDGE DISTILLATION",
    "year": 2024
}