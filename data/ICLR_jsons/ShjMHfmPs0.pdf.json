{
    "abstractText": "Seismic advances in generative AI algorithms for imagery, text, and other data types have led to the temptation to use AI-synthesized data to train next-generation models. Repeating this process creates an autophagous (\u201cself-consuming\u201d) loop whose properties are poorly understood. We conduct a thorough analytical and empirical analysis using state-of-the-art generative image models of three families of autophagous loops that differ in how fixed or fresh real training data is available through the generations of training and whether the samples from previous-generation models have been biased to trade off data quality versus diversity. Our primary conclusion across all scenarios is that without enough fresh real data in each generation of an autophagous loop, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease. We term this condition Model Autophagy Disorder (MAD), by analogy to mad cow disease, and show that appreciable MADness arises in just a few generations. Generation t = 1 t = 3 t = 5 t = 7 t = 9 Figure 1: Training generative artificial intelligence (AI) models on synthetic data progressively amplifies artifacts. As AI-synthesized data proliferates in standard datasets and the Internet, future AI models will train on both real and synthetic data, forming autophagous (\u201cself-consuming\u201d) loops. Here we highlight a potential unintended consequence of autophagous training. We trained a sequence of StyleGAN2 (Karras et al., 2019a) models wherein the model at generation t \u2265 2 trains only on data synthesized by the model at generation t\u2212 1. This setup is a fully synthetic loop (Figure 3) without sampling bias (\u03bb = 1). Note how the cross-hatched artifacts (possibly an architectural fingerprint (Karras et al., 2021)) are progressively amplified at each generation. Appendix D has more samples.",
    "authors": [],
    "id": "SP:98074615b0395440419e1c389f394fa6250a482d",
    "references": [
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Samuli Laine",
                "Erik H\u00e4rk\u00f6nen",
                "Janne Hellsten",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Alias-free generative adversarial networks",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Christoph Schuhmann"
            ],
            "title": "LAION-5B: An open large-scale dataset for training next generation image-text models",
            "venue": "In NeurIPS Datasets and Benchmarks Track,",
            "year": 2022
        },
        {
            "authors": [
                "Matthew Gault"
            ],
            "title": "AI spam is already flooding the internet and it has an obvious tell",
            "venue": "VICE,",
            "year": 2023
        },
        {
            "authors": [
                "Matthew Cantor"
            ],
            "title": "Nearly 50 news websites are \u2018AI-generated\u2019, a study says",
            "venue": "Would I be able to tell? The Guardian,",
            "year": 2023
        },
        {
            "authors": [
                "Veniamin Veselovsky",
                "Manoel Horta Ribeiro",
                "Robert West"
            ],
            "title": "Artificial artificial artificial intelligence: Crowd workers widely use large language models for text production",
            "venue": "tasks. arXiv preprint arXiv:2306.07899,",
            "year": 2023
        },
        {
            "authors": [
                "Jon Christian"
            ],
            "title": "CNET secretly used AI on articles that didn\u2019t disclose that fact, staff say",
            "venue": "Futurusm,",
            "year": 2023
        },
        {
            "authors": [
                "Walter H.L. Pinaya",
                "Petru-Daniel Tudosiu",
                "Jessica Dafflon",
                "Pedro F. Da Costa",
                "Virginia Fernandez",
                "Parashkev Nachev",
                "Sebastien Ourselin",
                "M. Jorge Cardoso"
            ],
            "title": "Brain imaging generation with latent diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Chengyuan Deng",
                "Shihang Feng",
                "Hanchen Wang",
                "Xitong Zhang",
                "Peng Jin",
                "Yinan Feng",
                "Qili Zeng",
                "Yinpeng Chen",
                "Youzuo Lin"
            ],
            "title": "OpenFWI: Large-scale multi-structural benchmark datasets for full waveform inversion",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Lorenzo Luzi",
                "Ali Siahkoohi",
                "Paul M Mayer",
                "Josue Casco-Rodriguez",
                "Richard Baraniuk"
            ],
            "title": "Boomerang: Local sampling on image manifolds using diffusion models",
            "venue": "arXiv preprint arXiv:2210.12100,",
            "year": 2022
        },
        {
            "authors": [
                "Marvin Klemp",
                "Kevin R\u00f6sch",
                "Royden Wagner",
                "Jannik Quehl",
                "Martin Lauer"
            ],
            "title": "LDFA: Latent diffusion face anonymization for self-driving applications",
            "venue": "arXiv preprint arXiv:2302.08931,",
            "year": 2023
        },
        {
            "authors": [
                "Kai Packh\u00e4user",
                "Lukas Folle",
                "Florian Thamm",
                "Andreas Maier"
            ],
            "title": "Generation of anonymous chest radiographs using latent diffusion models for training thoracic abnormality classification systems",
            "venue": "arXiv preprint arXiv:2211.01323,",
            "year": 2022
        },
        {
            "authors": [
                "August DuMont Sch\u00fctte",
                "J\u00fcrgen Hetzel",
                "Sergios Gatidis",
                "Tobias Hepp",
                "Benedikt Dietz",
                "Stefan Bauer",
                "Patrick Schwab"
            ],
            "title": "Overcoming barriers to data sharing with medical image generation: a comprehensive evaluation",
            "venue": "NPJ Digital Medicine,",
            "year": 2021
        },
        {
            "authors": [
                "Shekoofeh Azizi",
                "Simon Kornblith",
                "Chitwan Saharia",
                "Mohammad Norouzi",
                "David J Fleet"
            ],
            "title": "Synthetic data from diffusion models improves imagenet classification",
            "venue": "arXiv preprint arXiv:2304.08466,",
            "year": 2023
        },
        {
            "authors": [
                "Max F Burg",
                "Florian Wenzel",
                "Dominik Zietlow",
                "Max Horn",
                "Osama Makansi",
                "Francesco Locatello",
                "Chris Russell"
            ],
            "title": "A data augmentation perspective on diffusion models and retrieval",
            "venue": "arXiv preprint arXiv:2304.10253,",
            "year": 2023
        },
        {
            "authors": [
                "Pablo Villalobos",
                "Jaime Sevilla",
                "Lennart Heim",
                "Tamay Besiroglu",
                "Marius Hobbhahn",
                "Anson Ho"
            ],
            "title": "Will we run out of data? an analysis of the limits of scaling datasets in machine learning",
            "venue": "arXiv preprint arXiv:2211.04325,",
            "year": 2022
        },
        {
            "authors": [
                "Ahmed Elgammal",
                "Bingchen Liu",
                "Mohamed Elhoseiny",
                "Marian Mazzone"
            ],
            "title": "CAN: Creative adversarial networks, generating \"art\" by learning about styles and deviating from style norms",
            "venue": "arXiv preprint arXiv:1706.07068,",
            "year": 2017
        },
        {
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei A Efros"
            ],
            "title": "Image-to-image translation with conditional adversarial networks",
            "year": 2017
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Tuomas Kynk\u00e4\u00e4nniemi",
                "Tero Karras",
                "Samuli Laine",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Improved precision and recall metric for assessing generative models",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Natalie Jomini Stroud"
            ],
            "title": "Niche News: The Politics of News Choice",
            "year": 2011
        },
        {
            "authors": [
                "Ivan Dylko",
                "Igor Dolgov",
                "William Hoffman",
                "Nicholas Eckhart",
                "Maria Molina",
                "Omar Aaziz"
            ],
            "title": "The dark side of technology: An experimental investigation of the influence of customizability technology on online political selective exposure",
            "venue": "Computers in Human Behavior,",
            "year": 2017
        },
        {
            "authors": [
                "Michael A Beam"
            ],
            "title": "Automating the news: How personalized news recommender system design choices impact news reception",
            "venue": "Communication Research,",
            "year": 2014
        },
        {
            "authors": [
                "Eytan Bakshy",
                "Solomon Messing",
                "Lada A Adamic"
            ],
            "title": "Exposure to ideologically diverse news and opinion on",
            "year": 2015
        },
        {
            "authors": [
                "Derek O\u2019Callaghan",
                "Derek Greene",
                "Maura Conway",
                "Joe Carthy",
                "P\u00e1draig Cunningham"
            ],
            "title": "Down the (white) rabbit hole: The extreme right and online recommender systems",
            "venue": "Social Science Computer Review,",
            "year": 2015
        },
        {
            "authors": [
                "Neal Nathanson",
                "John Wilesmith",
                "Christian Griot"
            ],
            "title": "Bovine Spongiform Encephalopathy (BSE): Causes and Consequences of a Common Source Epidemic",
            "venue": "American Journal of Epidemiology, 145(11):959\u2013969,",
            "year": 1997
        },
        {
            "authors": [
                "Jiaxin Huang",
                "Shixiang Shane Gu",
                "Le Hou",
                "Yuexin Wu",
                "Xuezhi Wang",
                "Hongkun Yu",
                "Jiawei Han"
            ],
            "title": "Large language models can self-improve",
            "venue": "arXiv preprint arXiv:2210.11610,",
            "year": 2022
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Miika Aittala",
                "Janne Hellsten",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Analyzing and improving the image quality of StyleGAN",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Ishaan Gulrajani",
                "Faruk Ahmed",
                "Martin Arjovsky",
                "Vincent Dumoulin",
                "Aaron C Courville"
            ],
            "title": "Improved training of Wasserstein GANs",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Ivan Kobyzev",
                "Simon JD Prince",
                "Marcus A Brubaker"
            ],
            "title": "Normalizing flows: An introduction and review of current methods",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Li Deng"
            ],
            "title": "The MNIST database of handwritten digit images for machine learning research [best of the web",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2012
        },
        {
            "authors": [
                "Ilia Shumailov",
                "Zakhar Shumaylov",
                "Yiren Zhao",
                "Yarin Gal",
                "Nicolas Papernot",
                "Ross Anderson"
            ],
            "title": "The curse of recursion: Training on generated data makes models forget",
            "venue": "arXiv preprint arXiv:2305.17493,",
            "year": 2023
        },
        {
            "authors": [
                "Gonzalo Mart\u00ednez",
                "Lauren Watson",
                "Pedro Reviriego",
                "Jos\u00e9 Alberto Hern\u00e1ndez",
                "Marc Juarez",
                "Rik Sarkar"
            ],
            "title": "Combining generative artificial intelligence (AI) and the Internet: Heading towards evolution or degradation",
            "venue": "arXiv preprint arXiv:2303.01255,",
            "year": 2023
        },
        {
            "authors": [
                "Gonzalo Mart\u00ednez",
                "Lauren Watson",
                "Pedro Reviriego",
                "Jos\u00e9 Alberto Hern\u00e1ndez",
                "Marc Juarez",
                "Rik Sarkar"
            ],
            "title": "Towards understanding the interplay of generative artificial intelligence and the Internet",
            "venue": "arXiv preprint arXiv:2306.06130,",
            "year": 2023
        },
        {
            "authors": [
                "Ryuichiro Hataya",
                "Han Bao",
                "Hiromi Arai"
            ],
            "title": "Will large-scale generative models corrupt future datasets",
            "venue": "arXiv preprint arXiv:2211.08095,",
            "year": 2022
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "GANs trained by a two time-scale update rule converge to a local Nash equilibrium",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Lecun",
                "L. Bottou",
                "Y. Bengio",
                "P. Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE,",
            "year": 1998
        },
        {
            "authors": [
                "Andrew Brock",
                "Jeff Donahue",
                "Karen Simonyan"
            ],
            "title": "Large scale GAN training for high fidelity natural image synthesis",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Ahmed Imtiaz Humayun",
                "Randall Balestriero",
                "Richard Baraniuk"
            ],
            "title": "Polarity sampling: Quality and diversity control of pre-trained generative networks via singular values",
            "year": 2022
        },
        {
            "authors": [
                "David Williams"
            ],
            "title": "Probability With Martingales",
            "year": 1991
        },
        {
            "authors": [
                "Leland McInnes",
                "John Healy",
                "Nathaniel Saul",
                "Lukas Grossberger"
            ],
            "title": "UMAP: Uniform manifold approximation and projection",
            "venue": "The Journal of Open Source Software,",
            "year": 2018
        },
        {
            "authors": [
                "Luca Guarnera",
                "Oliver Giudice",
                "Sebastiano Battiato"
            ],
            "title": "Deepfake detection by analyzing convolutional traces",
            "venue": "In CVPR workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Eric Mitchell",
                "Yoonho Lee",
                "Alexander Khazatsky",
                "Christopher D Manning",
                "Chelsea Finn"
            ],
            "title": "DetectGPT: Zero-shot machine-generated text detection using probability curvature",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Ruixiang Tang",
                "Yu-Neng Chuang",
                "Xia Hu"
            ],
            "title": "The science of detecting LLM-generated texts",
            "venue": "arXiv preprint arXiv:2303.07205,",
            "year": 2023
        },
        {
            "authors": [
                "John Kirchenbauer",
                "Jonas Geiping",
                "Yuxin Wen",
                "Manli Shu",
                "Khalid Saifullah",
                "Kezhi Kong",
                "Kasun Fernando",
                "Aniruddha Saha",
                "Micah Goldblum",
                "Tom Goldstein"
            ],
            "title": "On the reliability of watermarks for large language models",
            "venue": "arXiv preprint arXiv:2306.04634,",
            "year": 2023
        },
        {
            "authors": [
                "John Kirchenbauer",
                "Jonas Geiping",
                "Yuxin Wen",
                "Jonathan Katz",
                "Ian Miers",
                "Tom Goldstein"
            ],
            "title": "A watermark for large language models",
            "venue": "arXiv preprint arXiv:2301.10226,",
            "year": 2023
        },
        {
            "authors": [
                "Yunqing Zhao",
                "Tianyu Pang",
                "Chao Du",
                "Xiao Yang",
                "Ngai-Man Cheung",
                "Min Lin"
            ],
            "title": "A recipe for watermarking diffusion models",
            "venue": "arXiv preprint arXiv:2303.10137,",
            "year": 2023
        },
        {
            "authors": [
                "Sen Peng",
                "Yufei Chen",
                "Cong Wang",
                "Xiaohua Jia"
            ],
            "title": "Protecting the intellectual property of diffusion models by the watermark diffusion process",
            "venue": "arXiv preprint arXiv:2306.03436,",
            "year": 2023
        },
        {
            "authors": [
                "Yuxin Wen",
                "John Kirchenbauer",
                "Jonas Geiping",
                "Tom Goldstein"
            ],
            "title": "Tree-ring watermarks: Fingerprints for diffusion images that are invisible and robust",
            "venue": "arXiv preprint arXiv:2305.20030,",
            "year": 2023
        },
        {
            "authors": [
                "Pierre Fernandez",
                "Guillaume Couairon",
                "Herv\u00e9 J\u00e9gou",
                "Matthijs Douze",
                "Teddy Furon"
            ],
            "title": "The stable signature: Rooting watermarks in latent diffusion models",
            "venue": "arXiv preprint arXiv:2303.15435,",
            "year": 2023
        },
        {
            "authors": [
                "Jianwei Fei",
                "Zhihua Xia",
                "Benedetta Tondi",
                "Mauro Barni"
            ],
            "title": "Supervised GAN watermarking for intellectual property protection",
            "venue": "In Workshop on Information Forensics and Security (WIFS),",
            "year": 2022
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560,",
            "year": 2022
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B Hashimoto"
            ],
            "title": "Alpaca: A strong, replicable instruction-following model, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Leonid V Kantorovich"
            ],
            "title": "Mathematical methods of organizing and planning production",
            "venue": "Management Science,",
            "year": 1960
        },
        {
            "authors": [
                "Tong Che",
                "Ruixiang Zhang",
                "Jascha Sohl-Dickstein",
                "Hugo Larochelle",
                "Liam Paull",
                "Yuan Cao",
                "Yoshua Bengio"
            ],
            "title": "Your GAN is secretly an energy-based model and you should use discriminator driven latent sampling",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Laurent Dinh",
                "Jascha Sohl-Dickstein",
                "Samy Bengio"
            ],
            "title": "Density estimation using Real NVP",
            "venue": "In International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "Filippo Pagani",
                "Martin Wiegand",
                "Saralees Nadarajah"
            ],
            "title": "An n-dimensional Rosenbrock distribution for Markov chain Monte Carlo testing",
            "venue": "Scandinavian Journal of Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "Ishaan Gulrajani",
                "Faruk Ahmed",
                "Martin Arjovsky",
                "Vincent Dumoulin",
                "Aaron C Courville"
            ],
            "title": "Improved training of Wasserstein GANs",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Dhariwal"
            ],
            "title": "2018) and a hidden dimension of 64. The training is carried out for 20 epochs with a batch size of 256 for each generation, ensuring convergence as determined by monitoring the model\u2019s likelihood over a validation",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Synthetic data1 from generative artificial intelligence (AI) models like Stable Diffusion (Rombach et al., 2022) and ChatGPT (OpenAI, 2023) is rapidly proliferating on the Internet. Indeed, there will soon be more synthetic data than real data on the Internet.\nSince the training datasets for generative AI models tend to be sourced from the Internet, today\u2019s AI models are unwittingly being trained on increasing amounts of AI-synthesized data. Figure 2\n1By \u201csynthetic\u201d we mean AI-synthesized data, as opposed to data synthesized via physics-based simulations.\nconfirms that the popular LAION-5B dataset (Schuhmann et al., 2022), used to train state-of-the-art models like Stable Diffusion, contains synthetic images from several earlier generations of generative models. AI is generating formerly human-sourced data, like reviews (Gault, 2023), websites (Cantor, 2023), and data annotations (Veselovsky et al., 2023), often with no indication of its origin (Christian, 2023). As the use of generative models continues to grow rapidly, this situation will only accelerate.\nMoreover, throwing caution to the wind, AI-synthesized data is increasingly used by choice for training because it is convenient, especially in data-scarce applications like medicine (Pinaya et al., 2022) and geophysics (Deng et al., 2022), and it can protect privacy (Luzi et al., 2022; Klemp et al., 2023) in sensitive data applications like medicine (Packh\u00e4user et al., 2022; DuMont Sch\u00fctte et al., 2021). Most importantly, as deep learning models become increasingly enormous (Azizi et al., 2023; Burg et al., 2023), we are simply running out of real data on which to train them (Economist, 2023a;b; Villalobos et al., 2022).\nThe witting or unwitting use of synthetic data to train generative models departs from standard AI training practice in one important respect: repeating this process for generation after generation of models forms an autophagous (\u201cself-consuming\u201d) loop (Figure 3). Different autophagous loop variations arise depending on how existing real and synthetic data are combined into future training sets. Additional variations arise depending on the model sampling biases used to trade off perceptual quality (fidelity or coherence) versus diversity (variety or heterogeneity).2\nThe potential ramifications of autophagous loops on the properties and performance of generative models is poorly understood. In one direction, autophagy might progressively amplify the biases and artifacts present in any generative model as fingerprints. In Figure 1, autophagy progressively amplifies cross-hatching artifacts (reminiscent of aliasing (Karras et al., 2021)) in StyleGAN2 models. In another direction, autophagous loops featuring generative models tuned to produce high quality syntheses at the expense of diversity (such as Karras et al. (2019a); Ho and Salimans (2021)) might progressively dilute the diversity of the data on the Internet.3 By analogy to mad cow disease (Nathanson et al., 1997), we term these and other symptoms of autophagy as Model Autophagy Disorder (MAD).\nContributions. We conduct a careful theoretical and empirical study of AI autophagy with generative image models. The concepts developed herein apply to any data type, including text. We also unify the results of contemporaneous work. Our three key contributions establish that, without enough fresh real data each generation, future generative models are doomed to go MAD. Moreover, we demonstrate that appreciable MADness can occur in only a handful of generations.\n1. Realistic models for autophagous loops. We propose three families of self-consuming loops that realistically model how real and synthetic data are used to train generative models (recall Figure 3):\n\u2022 The fully synthetic loop (Section 3), where the training dataset for each generation\u2019s model consists solely of synthetic data sampled from previous generations\u2019 models, such as by training\n2We quantify quality and diversity via precision and recall, respectively (Kynk\u00e4\u00e4nniemi et al., 2019). 3Similar to diversity exposure in recommender systems, where maximizing click rates discourages exposure to diverse ideas (Stroud, 2011; Dylko et al., 2017; Beam, 2014; Bakshy et al., 2015; O\u2019Callaghan et al., 2015).\nTraining Dataset\nGenerative Model(s)\nFixed Real Data Fresh Real Data\nTrain\nSynthesize w/ bia s \u03bb\n2. Sampling bias plays a key r\u00f4le in autophagous loops. Practitioners often favor high-quality syntheses, whether through curation or automatic quality-diversity tradeoffs (Ho and Salimans, 2021; Karras et al., 2020). We show that, without these sampling biases, MADness degrades quality and diversity, while with them, quality can be maintained but diversity degrades even faster.\n3. Autophagous loop behaviors hold across a wide range of generative models and datasets, including Gaussian, Gaussian mixture, diffusion (DDPM, Ho et al., 2020), StyleGAN2 (Karras et al., 2020), Wasserstein GAN (WGAN, Gulrajani et al., 2017a), and Normalizing Flow (Kobyzev et al., 2020) models trained on datasets like FFHQ (Karras et al., 2019b) and MNIST (Deng, 2012).\nRelated work. We define a cohesive autophagy framework, supported empirically by state-of-the-art models, that unifies and significantly extends contemporaneous results that consider fragmented aspects of MADness. Shumailov et al. (2023) study autophagous loops without sampling bias and show that MADness ensues from variational autoencoders and Gaussian mixture models in fully synthetic and language models in synthetic augmentation loops. However, the absence of quality-diversity tradeoffs (sampling biases) in their models limits the applicability of their findings to real-world scenarios. Furthermore, each generation they only fine-tune their language models, while we train our models from scratch. Mart\u00ednez et al. (2023a) also consider unbiased synthetic augmentation loops, but only show qualitative evidence of MADness on a small dataset. Mart\u00ednez et al. (2023b) focus only on fully synthetic loops and report that sampling bias can prevent degradation of image quality in small datasets. Finally, Huang et al. (2022); Hataya et al. (2022) and others have considered synthetic data augmentation, but not in the context of autophagous loops."
        },
        {
            "heading": "2 SELF-CONSUMING GENERATIVE MODELS",
            "text": "Consider a sequence of generative models (Gt)t\u2208N, where each model approximates a reference probability distribution Pr. At each generation t \u2208 N, the model Gt trains from scratch on the dataset Dt = (Dtr,Dts) containing both ntr real samples Dtr from Pr and nts synthetic samples Dts from trained generative model(s). The first-generation model G1 trains only on real data: n1s = 0, D1s = \u2205. Definition. An autophagous generative process is a sequence of distributions (Gt)t\u2208N where each generative model Gt is trained on data that includes samples from previous models (G\u03c4 )t\u22121\u03c4=1.\nDefinition. Let dist(\u00b7, \u00b7) denote a distance metric on distributions. A MAD generative process is a sequence of distributions (Gt)t\u2208N such that E[dist(Gt,Pr)] grows with t.\nClaim. Under mild conditions, an autophagous generative process is a MAD generative process.\nTwo critical aspects affect whether a sequence of generative models goes MAD: the balance of real and synthetic training data and how the generative models synthesize data. We study three realistic autophagous mechanisms, each of which includes synthetic data and potentially real data in a feedback loop (recall Section 1 and Figure 3):\n\u2022 The fully synthetic loop: Each model Gt for t \u2265 2 trains exclusively on synthetic data sampled from models (G\u03c4 )t\u22121\u03c4=1 from previous generations, i.e., Dt = Dts.\n\u2022 The synthetic augmentation loop: Each model Gt for t \u2265 2 trains on a dataset Dt = (Dr,Dts): a fixed set of real data Dr from Pr, plus synthetic data Dts from previous generations\u2019 models.\n\u2022 The fresh data loop: Each model Gt for t \u2265 2 trains on a dataset Dt = (Dtr,Dts): a fresh set of real data Dtr drawn from Pr, plus synthetic data Dts from previous generations\u2019 models.\nMetrics for MADness. Throughout this paper we measure the distance between the synthetic data and the real data (reference distribution) using the Fr\u00e9chet inception distance (FID) (Heusel et al., 2017),4 the quality of the synthetic data using precision, and the diversity of the synthetic data using recall (Kynk\u00e4\u00e4nniemi et al., 2019). See Appendix A.4 for more details."
        },
        {
            "heading": "2.1 BIASED SAMPLING IN AUTOPHAGOUS LOOPS",
            "text": "While the above three autophagous loops realistically mimic real-world generative model training scenarios that involve synthetic data, it is also critical to consider how each generation\u2019s synthetic data is produced in practice. In particular, most syntheses are to some degree biased to maximize perceptual quality, whether through manual curation (\u201ccherry-picking\u201d) or commonplace techniques that automatically boost quality and sacrifice diversity by sampling closer to the modes of the synthetic distribution of generative models (OpenAI, 2023; Ho and Salimans, 2021; Karras et al., 2020; Brock et al., 2019; Humayun et al., 2022). We refer to this common practice as sampling bias. We employ a number of generative models in our experiments below; each has a unique controllable parameter to increase sample quality. We unify these parameters in the universal sampling bias parameter \u03bb \u2208 [0, 1], where \u03bb = 1 corresponds to unbiased sampling and \u03bb = 0 corresponds to sampling from the modes of the generative distribution Gt with zero variance. The exact interpretation of \u03bb differs across various models, but in general synthetic sample quality will increase and diversity will decrease as \u03bb is decreased from 1. Below we provide specific definitions for \u03bb for the various generative models we consider in this paper:\nGaussian model: To implement biased sampling from an estimated distribution N (\u00b5,\u03a3), we sample from N (\u00b5, \u03bb\u03a3). As \u03bb decreases, we draw samples closer to the mean \u00b5t. Generative adversarial network: In our StyleGAN2 experiments, we decrease the truncation parameter \u03a8 \u2208 [0, 1] to increase sample quality. (Karras et al., 2020) Therefore, \u03bb = \u03a8. Diffusion model: For DDPMs, we use a classifier-free diffusion guidance factor w (with 10% conditioning dropout) (Ho and Salimans, 2021) and define \u03bb = 11+w ."
        },
        {
            "heading": "3 THE FULLY SYNTHETIC LOOP: TRAINING EXCLUSIVELY ON SYNTHETIC DATA LEADS TO MADNESS",
            "text": "First, we analyze the fully synthetic loop, where each model trains on synthetic data from previous generations. We focus on the inter-generational propagation of non-idealities resulting from estimation errors and sampling biases and characterize the convergence of the autophagous loop. The simplicity of the fully synthetic loop primarily reflects niche examples like training generative models on their own high-quality outputs (followfox.ai, 2023). Nevertheless, this loop represents a worse-case scenario and so offers valuable insights for more practical autophagous loops discussed in subsequent sections. Our analysis and experiments support our main conclusion for the fully synthetic loop: either the quality (precision) or the diversity (recall) of synthetic data deteriorates over generations.\n4We calculate MNIST FIDs via LeNet (Lecun et al., 1998) features instead of Inception features.\nGaussian fully synthetic loop: random walks and variance collapse. We first show that Gaussian fully synthetic loops\u2019 martingale nature incurs MADness. Consider a reference distribution Pr = N (\u00b50,\u03a30), where \u00b50 \u2208 Rd and \u03a30 \u2208 Rd\u00d7d, and a Gaussian generative process Gt = N (\u00b5t,\u03a3t). At each time t \u2208 N, we sample ns vectors from Gt\u22121 with bias \u03bb \u2264 1; i.e., we draw x1t , . . . ,x ns t\niid\u223c N (\u00b5t\u22121, \u03bb\u03a3t\u22121). From these vectors we construct the unbiased parameters of the next model Gt:\n\u00b5t = 1\nns ns\u2211 i=1 xit, \u03a3t = 1 ns \u2212 1 ns\u2211 i=1 (xit \u2212 \u00b5t)(xit \u2212 \u00b5t)\u22a4. (1)\nIt is straightforward to see that \u00b5t and \u03a3t are (super)martingale processes (Williams, 1991), forming random walks. For \u03a3t, we also have the following result, proved in Appendix B.\nProposition. For the random process defined in Equation (1), for any \u03bb \u2264 1, we have \u03a3t a.s.\u2212\u2212\u2192 0.\nThat is, when fitting a distribution to data sampled from that distribution repeatedly, we should not only expect some modal drift because of the random walk in \u00b5t (reduction in quality) but also inevitably a collapse of the variance \u03a3t (vanishing of diversity).\nThe key takeaway is that these effects\u2014the random walk and the variance collapse\u2014are solely due to the estimation error of fitting the model parameters using random data. Importantly, this result holds true even when there is no sampling bias (\u03bb = 1). The magnitudes of the steps of the random walk in \u00b5t are determined by two main factors: the number of samples ns and the covariance \u03a3t. Unsurprisingly, the larger the ns, the smaller the steps of the random walk, since there will be less estimation error. This will also slow the convergence of \u03a3t to 0. Meanwhile, \u03a3t can be controlled using a sampling bias factor \u03bb < 1. The smaller the choice of \u03bb, the more rapidly \u03a3t will converge to 0, stopping the random walk of \u00b5t (as illustrated in Figure 15). Thus, the sampling bias factor \u03bb provides a trade-off to preserve quality at the expense of diversity. Furthermore, Shumailov et al. (2023) recently showed that the expected Wasserstein-2 distributional distance E[dist(Gt,Pr)] increases in this process, supporting our conclusion that Gt is a MAD generative process. We now empirically study the fully synthetic loop using FFHQ StyleGAN2 and MNIST DDPM models; see Appendix A.1 for experiment details.\nUnbiased sampling degrades synthetic data quality and diversity. Figure 4 plots the FID, precision, and recall for FFHQ StyleGAN2 and MNIST DDPM models in fully synthetic loops with (\u03bb < 1) and without (\u03bb = 1) sampling bias. In the latter case, synthetic data distributions undergo random walks that deviate from the reference distribution because each generation\u2019s training data is finite. Consequently, the models go MAD: FID increases, while precision and recall steadily decrease.\nBiased sampling can boost synthetic data quality, but at the expense of diversity. As for the biased FFHQ StyleGAN2 and MNIST DDPM models (\u03bb = 0.7 and 0.5) in fully synthetic loops\n(see Figure 4), sampling bias increases precision, but also accelerates losses in recall (shown clearly in Figure 5) compared to unbiased models. Moreover, the FID still increases, indicating a MAD generative process. See Figure 14 in Appendix C.3 for results with different MNIST DDPM sampling bias values, which show the same trend.\nSynthetic mode behavior depends on sampling bias. To visualize MAD generative processes, in Figure 6 we reduced the dimensionality of the real and synthetic MNIST-DDPM fully synthetic loop samples from Figure 4 via Uniform Manifold Approximation and Projection (UMAP) (McInnes et al., 2018). In unbiased sampling, the ten modes of the synthetic distribution (one for each digit) progressively drift away from the real distribution modes, despite originating from a conditional model, eventually merging into one large cluster. By generation t = 10, the synthetic digits are illegible (Figure 24 in Appendix F). In sharp contrast to the unbiased case, UMAP reveals that biased sampling successfully keeps syntheses on the real data manifold (high precision), but contracts the synthetic support around a single set of ten digits (zero recall). Appendix C confirms these trends for Gaussian mixtures Appendix C.1, WGANs Appendix C.2, and Normalizing Flows Appendix C.4."
        },
        {
            "heading": "4 THE SYNTHETIC AUGMENTATION LOOP: FIXED REAL TRAINING DATA CAN DELAY BUT NOT PREVENT MADNESS",
            "text": "While analysis of the fully synthetic loop is straightforward, practitioners will use real data when it is available. We now explore the synthetic augmentation loop, where a fixed real dataset is augmented with autophagous synthetic data. Synthetic data augmentation can improve classification (Luzi et al., 2022; Burg et al., 2023), but the impact of autophagous data augmentation is unclear\u2014does increasing training data volume enhance synthesis, even if the added samples may stray from reality? We find that, in the synthetic augmentation loop, fixed real training data only delays the inevitable degradation of the quality or diversity of synthetic data over generations.\nKeeping the original real dataset in the synthetic augmentation loop only slows MADness. See Appendix A.2 for the experimental details. Figure 7 shows how keeping the full FFHQ dataset in a StyleGAN25 synthetic augmentation loop still produces the same symptoms (albeit more slowly) as the fully synthetic loop: the distance from the real dataset (FID) increases, while the quality (precision) and diversity (recall) of synthetic samples still decrease without sampling bias. In fact, in Appendix G we see the same artifacts appear as in Figure 1 and Appendix D. Additionally, sampling bias \u03bb impacts MNIST DDPM synthetic augmentation loops (Figure 8) in the same way it impacts fully synthetic loops: FID still increases, but \u03bb < 1 can increase quality (precision) in exchange for diversity (recall). Additional synthetic augmentation loop experiments can be found in Appendix H."
        },
        {
            "heading": "5 THE FRESH DATA LOOP: FRESH REAL DATA CAN PREVENT MADNESS",
            "text": "Our most elaborate autophagous loop model gets training data from two sources: unseen (fresh) real data and synthetic data from previously trained models. A clear instance of this is the LAION-5B dataset (Schuhmann et al., 2022) where the dataset contains both real and AI-synthesized images from the Internet (Figure 2). We seek to understand how generative models evolve in the fresh data loop, which alters the synthetic augmentation loop by incorporating fresh (instead of fixed) real\n5Unique to our StyleGAN2 synthetic augmentation loop, we linearly grow a pool of synthetic data to assess whether access to all previous generations\u2019 synthetic data could help future generations learn (see Appendix A.2).\nsamples at each iteration. We imagine that a fraction p \u2208 (0, 1) of a corpus of data (e.g., the Internet) is real, and the remainder 1\u2212 p is synthetic. Independently sampling nt data points from this corpus yields ntr = pn\nt real and nts = (1\u2212 p)nt synthetic data points to train the t-th generation model. The fresh data loop reveals two intriguing phenomena. Firstly, asymptotic performance converges independently of initial performance, depending only on the ratio of real-to-synthetic training data. Second, limited amounts of synthetic data can actually improve performance in the fresh data loop\u2014 since synthetic data propagates information from previously seen real data and thus increases the effective dataset size\u2014but too much synthetic data can still invoke MADness. Overall, our fresh data loop analyses and experiments establish that, with enough fresh real data, the quality and diversity of synthetic data do not degrade over generations.\nInitial models will eventually be forgotten in the fresh data loop. First we show that the initial model does not affect the fresh data loop. We train the first model on nini real samples, and all others with nr new real and ns synthetic (with bias \u03bb) samples from the previous model; see Appendix A.3 for details. Interestingly, for both MNIST DDPM and Gaussian models, the Wasserstein distance and FID converged independently of nini after a few iterations (Figure 9). In other words, we observe that models in any given fresh data loop converge to a limit point dependent on nr, ns, and \u03bb, not on the initial model G1 or its dataset size nini:\nlim t\u2192\u221e\nE[dist(Gt,Pr)] =: WD(nr, ns, \u03bb). (2)\nFor autophagy, this brings some hope: with fresh real data at each generation, E[dist(Gt,Pr)] does not necessarily increase with t. Thus, the fresh data loop can prevent a MAD generative process.\nThe fresh data loop exhibits a phase transition. Since fresh real data mitigates MADness, one may suspect that synthetic data promotes fresh data loop MADness. However, the truth is that modest amounts of synthetic data in fresh data loops can actually boost performance; only when\nsynthetic data exceeds some critical threshold do models suffer. We formalize this observation through Monte-Carlo simulation of the fresh data loop limit point (Equation (2)) in Gaussian models. For comparison, we compute the effective sample size ne that an alternative model would need to reach the same performance as the asymptote from scratch:\nFind ne s.t. E[dist(G(ne),Pr)] = WD(nr, ns, \u03bb). (3)\nThat is, ne captures the asymptotic sample efficiency of the fresh data loop.\nFigure 10 depicts how the ratio ne/nr changes with nr, ns, and \u03bb. When ne/nr \u2265 1, we say the amount of synthetic data ns is admissible because it effectively increases the number of real samples. For ne/nr < 1, synthetic data effectively reduces the number of real samples.\nFirst, we confirm that, given nr and \u03bb < 1, there exists a phase transition in ns. If ns exceeds the admissibility threshold, the effective sample size ne drops below the fresh sample size nr, meaning that synthetic data does not asymptotically improve performance. However, the synthetic-to-real ratio ns/nr needed to achieve ne/nr \u2265 1 is not constant. In fact, in Figure 10 we see that the admissible amount of synthetic data ns (such that ne/nr \u2265 1) can be quite high for low values of nr, but as nr grows, the admissible ratio of synthetic-to-real data ns/nr shrinks.\nSecond, we find that the admissible threshold value for ns depends strongly on sampling bias \u03bb. Perhaps surprisingly, stronger bias (smaller \u03bb) actually reduces the number of synthetic samples that can be used without harming performance. Taking the limit \u03bb \u2192 1 for unbiased sampling seems to ensure that the effective number of samples is always increased (ne/nr is always greater than 1), but whether this limiting behavior extends beyond Gaussian models is unclear. As discussed in ??, it is unlikely that generative models in practice synthesize without bias, so it is better to draw conclusions from the \u03bb < 1 case. See Appendix I for more fresh data loop experiments."
        },
        {
            "heading": "6 DISCUSSION",
            "text": "Our theoretical and empirical analyses enable us to extrapolate what might happen as generative models become ubiquitous and train future models in autophagous (self-consuming) loops. Using state-of-the-art generative image models and datasets, we have studied three families of autophagous loops and identified the key r\u00f4le of sampling biases. Some results are clear: without enough fresh real data, future models are doomed to Model Autophagy Disorder (MAD), progressively losing quality (precision) or diversity (recall), and amplifying generative artifacts. If left uncontrolled, MAD could poison the entire Internet\u2019s data quality and diversity. After all, our autophagous loops went appreciably MAD after just 5 generations (Figure 1). It seems inevitable that AI autophagy\u2019s unintended consequences could arise in the near future.\nPractitioners who deliberately use synthetic training data should heed our warning. For those in truly data-scarce applications, our results suggest how much real data can prevent MADness. For example, future training of a medical image generator on inter-institutional anonymous syntheses (DuMont Sch\u00fctte et al., 2021) should ensure that all synthetic images are artifact-free and diverse (see Section 3), and that real (preferably new) data is maximally present in training (see Sections 4 and 5).\nPractitioners who unknowingly train on synthetic data could try controlling the ratio of real-tosynthetic training data by identifying and rejecting synthetic data. Some identifiers find telltale patterns of AI synthesis (Guarnera et al., 2020; Mitchell et al., 2023; Tang et al., 2023). Others make synthetic data steganographically idenfiable via watermarking (Kirchenbauer et al., 2023a;b; Zhao et al., 2023; Peng et al., 2023; Wen et al., 2023; Fernandez et al., 2023; Fei et al., 2022). However, watermarking deliberately introduces hidden artifacts that could be uncontrollably or harmfully amplified by autophagy. In the fresh data loop, modest amounts of synthetic data can boost performance (ne/nr > 1 in Figure 10). Future research could develop autophagy-aware watermarking that helps identify synthetic data while avoiding the amplification of its own artifacts.\nFuture research could combine or alternate autophagous loops (e.g., loops with synthetic, fixed real, or fresh real data all together), examine how MADness affects downstream tasks (e.g., classification), and use other data types. We have focused on imagery, but autophagy and MADness can occur in any data type. For example, autophagous language models (Huang et al., 2022; Wang et al., 2022; Taori et al., 2023) can also go MAD, losing quality (coherence or correctness) or diversity (variety). Shumailov et al. (2023) have reached similar conclusions, but there is much work to do in this vein."
        },
        {
            "heading": "A EXPERIMENT SETUPS",
            "text": "Here are detailed descriptions of our experiments."
        },
        {
            "heading": "A.1 THE FULLY SYNTHETIC LOOP",
            "text": "We empirically study the fully synthetic loop using two representative deep generative models and two practical training datasets. After training an initial model G1 with a fully real dataset containing n1r samples from the (unknown) reference distribution, subsequent models (Gt)\u221et=2 are trained using nts synthetic samples from the immediately preceding model Gt\u22121, where each synthetic sample is produced with sampling bias \u03bb. Our primary experiments are organized as follows:\n\u2022 Generative adversarial network: We use an unconditional StyleGAN2 model (Karras et al., 2020) and initially train it on n1r = 70k samples from the FFHQ dataset (Karras et al., 2019b). We downsized the FFHQ images to 128\u00d7 128 (using Lanczos PyTorch anti-aliasing filtering as in Karras et al. (2020)) to reduce the computational cost. We set nts = 70k for t \u2265 2. \u2022 Diffusion model: We use a conditional DDPM (Ho et al., 2020) with T = 500 diffusion time steps and initially train it on n1r = 60k real samples from the MNIST dataset. We set n t s = 60k for\nt \u2265 2. To calculate FIDs, we use the features extracted by a LeNet (Lecun et al., 1998) rather than an Inception network, because numerical digits are not exactly natural images. For consistency, we continue to use the term \u201cFID\u201d in this case."
        },
        {
            "heading": "A.2 THE SYNTHETIC AUGMENTATION LOOP",
            "text": "We simulate the synthetic augmentation loop using the same deep generative models and experimental conditions as in Appendix A.1. Recall that we first require training an initial model G1 with a fully real dataset of n1r samples. All subsequent models (Gt)\u221et=2 are trained using nts synthetic samples from the previous model(s) and all of the original n1r samples used to train G1. Note that each synthetic sample is always produced with sampling bias \u03bb. Our experiments are organized as follows:\n\u2022 Generative adversarial network: We use an unconditional StyleGAN2 architecture Karras et al. (2020) trained on the FFHQ-128\u00d7128 dataset Karras et al. (2019b). Like the StyleGAN experiment in Appendix A.1, at each generation t \u2265 2 we sample 70k images with no sampling bias (\u03bb = 1) from the immediately preceding model Gt\u22121. However, now the synthetic dataset Dts includes samples from all the previously models (G\u03c4 )t\u22121\u03c4=1, producing a synthetic data pool of size nts = (t\u2212 1)70k that grows linearly with respect to t. The real FFHQ dataset is always present at every generation: D1r = Dtr and n1r = ntr = 70k for every generation t. \u2022 Diffusion model: We use a conditional MNIST DDPM Ho et al. (2020) with T = 500 diffusion time steps. In this experiment the synthetic dataset Dts is only sampled from the previous generation Gt\u22121 with sampling bias \u03bb, and n1r = nts = 60k for all t \u2265 2. The original real MNIST dataset is also available at every generation: D1r = Dtr and n1r = ntr = 60k for all t."
        },
        {
            "heading": "A.3 THE FRESH DATA LOOP",
            "text": "As in previous autophagous loop variants, we assume that all models are initially trained solely on real samples, with the number of real samples denoted here as n1r = nini. In subsequent generations (i.e., for t \u2265 2) the generative models are trained with a fixed number of real samples, denoted as ntr = nr, and a fixed number of synthetic samples, denoted by n t s = ns. In the fresh data loop, the dataset Dtr is independently sampled from the reference probability distribution Pr, while the dataset Dts is sampled exclusively from the previous generation Gt\u22121, with a sampling bias represented as \u03bb. We simulate the fresh data loop using different values for nini, nr, ns, and \u03bb. The Gaussian example enables examination of the fresh data loop in greater detail, especially in the asymptotic regime. Meanwhile, our MNIST DDPM example demonstrates the impact of fresh data loop on more realistic dataset and model.\n\u2022 Gaussian model: We consider a normal reference distribution Pr = N (0d, Id) with a dimension of d = 100. For modeling the Gaussian distribution, we utilize an unbiased moment estimation approach, as described in Equation (1).\n\u2022 Diffusion model: We use a conditional DDPM Ho et al. (2020) with T = 500 diffusion time steps. We consider the MNIST dataset as our reference distribution."
        },
        {
            "heading": "A.4 METRICS FOR MADNESS",
            "text": "Ascertaining whether an autophagous loop has gone MAD or not (recall Definition 2) requires that we measure how far the synthesized data distribution Gt has drifted from the true data distribution Pr over the generations t. We use the notion of the Wasserstein distance as implemented by the Fr\u00e9chet Inception Distance (FID) for this purpose. We will also find the standard concepts of precision and recall useful for making rigorous the notions of quality and diversity, respectively.\nWasserstein distance, or earth mover\u2019s or optimal transport distance (Kantorovich, 1960), measures the minimum work required to move the probability mass of one distribution to another. Computing the Wasserstein distance between two datasets (e.g., real and synthetic images) is prohibitively expensive. As such, standard practice employs the FID (Heusel et al., 2017) as an approximation, which calculates the Wasserstein-2 distance between Inception feature distributions of real and synthetic images. For our MNIST experiments we calculate FIDs using the features from a LeNet (Lecun et al., 1998) rather than an Inception network, because numerical digits are not exactly natural images.\nPrecision quantifies the portion of synthesized samples that are deemed high quality or visually appealing. We use precision as an indicator of sample quality. We compute precision by calculating the fraction of synthetic samples that are closer to a real data example than to their k-th nearest neighbor (Kynk\u00e4\u00e4nniemi et al., 2019). We use the default k = 5 in all experiments.\nRecall estimates the fraction of samples in a reference distribution that are inside the support of the distribution learned by a generative model. High recall scores suggest that the generative model captures a large portion of diverse samples from the reference distribution. We compute recall in a manner similar to precision (Kynk\u00e4\u00e4nniemi et al., 2019). Given a set of synthetic samples from the generative model, we calculate the fraction of real data samples that are closer to any synthetic sample than its k-th nearest neighbor. In Appendix C.1 we demonstrate how recall captures synthetic diversity in an autophagous loop more accurately than variance."
        },
        {
            "heading": "B PROOF OF SYNTHETIC GAUSSIAN MARTINGALE VARIANCE COLLAPSE",
            "text": "We now prove that for the process described in Equation (1), \u03a3t a.s.\u2212\u2212\u2192 0.\nProof. First write xit = \u221a \u03bb\u03a3 1/2 t\u22121z i t + \u00b5t\u22121 for z i t \u223c N (0d, Id). Then consider the process tr[\u03a3t], which is a lower bounded submartingale:\ntr[\u03a3t] = \u03bbtr\n[ \u03a3\n1/2 t\u22121\n( 1\nN \u2212 1 N\u2211 i=1 (zit \u2212 \u00b5zt )(zit \u2212 \u00b5zt )\u22a4 ) \u03a3 1/2 t\u22121 ] , (4)\nwhere \u00b5zt = 1 N \u2211N i=1 z i t. By Doob\u2019s martingale convergence theorem (Williams, 1991, Ch. 11), there exists a random variable w such that tr[\u03a3t] a.s.\u2212\u2212\u2192 w, and we now show that we must have w = 0. Without loss of generality, we can assume that \u03a3t\u22121 is diagonal, in which case it becomes clear that tr[\u03a3t] is a generalized \u03c72 random variable, being a linear combination of d independent \u03c72 random variables with N \u2212 1 degrees of freedom, mixed with weights \u03bbdiag(\u03a3t\u22121). Therefore, we can write tr[\u03a3t] = \u03bbyttr[\u03a3t\u22121], where yt is a generalized \u03c72 random variable with the same degrees of freedom but with mixing weights diag(\u03a3t\u22121)/tr[\u03a3t\u22121], and E[yt|\u03a3t\u22121] = 1. This implies that at least one mixing weight is greater than 1/D for each t, which means that for any 0 < \u03f5 < 1, there exists c > 0 such that Pr(|yt \u2212 1| > \u03f5) > c. Now consider the case \u03bb = 1. Since |yt \u2212 1| > \u03f5 infinitely often with probability one, the only w that can satisfy limt\u2192\u221e tr[\u03a30] \u220ft s=1 ys = w is w = 0. For general \u03bb \u2264 1, tr[\u03a3t] is simply the product of the process for \u03bb = 1 and the sequence \u03bbt\u22121, and so the product must also converge to zero almost surely. Finally, since tr[\u03a3t]\na.s.\u2212\u2212\u2192 0, we also must have \u03a3t a.s.\u2212\u2212\u2192 0, where convergence is defined with any matrix norm."
        },
        {
            "heading": "C ADDITIONAL EXPERIMENTS FOR THE FULLY SYNTHETIC LOOP",
            "text": "Here we present additional experiments for the fully synthetic loop."
        },
        {
            "heading": "C.1 RECALL VERSUS VARIANCE: GMMS IN AN UNBIASED FULLY SYNTHETIC LOOP",
            "text": "We also trained 2D GMMs in an unbiased fully synthetic loop using the same 25-mode distribution as (Che et al., 2020). In Figure 11 we see that the fully synthetic loop gradually reduces the number of modes covered by the synthetic distribution. Various metrics could measure this loss in diversity, so in Figure 12 we explore how well each metric reflects the dynamics of the fully synthetic loop, finding that recall is best-equipped to measure diversity in multimodal datasets."
        },
        {
            "heading": "C.2 WASSERSTEIN GANS IN AN UNBIASED FULLY SYNTHETIC LOOP",
            "text": "In this experiment we trained Wasserstein GANs (with gradient penalty) (Gulrajani et al., 2017a) on the MNIST dataset in a fully synthetic loop for 100 generations. As shown in Figure 13, the FID monotonically increases, while quality (precision) and diversity (recall) monotonically decrease.\n7For multidimensional datasets, we calculate variance as the trace of covariance."
        },
        {
            "heading": "C.3 ADDITIONAL MNIST DDPM FULLY SYNTHETIC LOOP RESULTS",
            "text": "In Figure 4 we showcased the results of training MNIST DDPMs in a fully synthetic loop with various sampling bias factors \u03bb. In Figure 14 we have the results (FID, precision, and recall) more generations t and different sampling biases \u03bb."
        },
        {
            "heading": "C.4 NORMALIZING FLOW FULLY SYNTHETIC LOOP",
            "text": "We implemented the fully synthetic loop using normalizing flows (Dinh et al., 2016; Kingma and Dhariwal, 2018) for generative modeling of the two-dimensional Rosenbrock reference distribution (Pagani et al., 2022) in order to visualize the outcome of this particular scenario in a controlled setting. Normalizing flows are unique in that they enable exact evaluation of the likelihood of the estimated distribution due to their invertibility (Dinh et al., 2016). This leads to a relatively straightforward training procedure compared to GANs, which often require careful balancing between the generator and discriminator networks to avoid mode collapse (Gulrajani et al., 2017b). Therefore, by using a low-dimensional reference distribution, this setup allows us to demonstrate the fully synthetic loop while eliminating potential training imperfections.\nAccording to the fully synthetic loop setup, we start with a training dataset of 104 samples from the 2D Rosenbrock distribution with the density function Pr(x1, x2) \u221d exp ( \u2212 12x 2 1 \u2212 ( x2 \u2212 x21 )2) (Pagani et al., 2022), which is plotted on the left-hand side of Figure 15. The subsequent generations of normalizing flow models are trained using synthetic data generated by the previous pre-trained normalizing flow for 16 generations, both with and without sampling bias. We employ the GLOW normalizing flow architecture (Kingma and Dhariwal, 2018) with eight coupling layers (Kingma and Dhariwal, 2018) and a hidden dimension of 64. The training is carried out for 20 epochs with a batch size of 256 for each generation, ensuring convergence as determined by monitoring the model\u2019s likelihood over a validation set. Figure 15 summarizes the results of this fully synthetic\nloop setup. To incorporate sampling bias, we sample from N (0d, \u03bbId) from the latent space of the model, where d = 2. As shown, regardless of the presence of sampling bias, the resulting distribution after 16 generations loses the tails of the reference distribution, indicating a loss of diversity. This phenomenon becomes more pronounced when sampling bias is present (\u03bb < 1).\nt = 1 t = 16"
        },
        {
            "heading": "D FFHQ UNBIASED FULLY SYNTHETIC LOOP IMAGES",
            "text": "We show additional randomly chosen synthetic samples produced by the same FFHQ StyleGAN2 unbiased fully synthetic loop as in Figure 1."
        },
        {
            "heading": "E FFHQ BIASED FULLY SYNTHETIC LOOP IMAGES",
            "text": "We show additional randomly chosen synthetic samples produced by the same FFHQ StyleGAN2 for biased (\u03bb = 0.7) fully synthetic loop as in Figure 5."
        },
        {
            "heading": "F MNIST DDPM FULLY SYNTHETIC LOOP IMAGES",
            "text": "Here we show randomly chosen samples from each generation of an MNIST DDPM in a fully synthetic loop for different sampling biases."
        },
        {
            "heading": "G FFHQ UNBIASED SYNTHETIC AUGMENTATION LOOP IMAGES",
            "text": ""
        },
        {
            "heading": "H ADDITIONAL RESULTS FOR THE SYNTHETIC AUGMENTATION LOOP",
            "text": ""
        },
        {
            "heading": "H.1 THE MNIST DDPM SYNTHETIC AUGMENTATION LOOP",
            "text": "In this section, we repeat the experiment in Fig 8 and described in A.2 using all synthetic data from previous generations, i.e, at each iteration t we used nts = (t\u2212 1)60k synthetic samples from (G\u03c4 )t\u22121\u03c4=1, combined with the initial real data we had. The results are shown in Figure 28. We see the same trend as in Figure 8. For a better comparison between using synthetic samples from all previous generations vs only using from the previous generation is shown in Figure 29. Using synthetic data from all previous generations only slows down the process the degradation in models with respect to only using data from previous generations."
        },
        {
            "heading": "H.2 THE GAUSSIAN SYNTHETIC AUGMENTATION LOOP",
            "text": "In this section, we replicate the Gaussian experiment in Section 5 for synthetic augmentation loops.\nIn particular, we sample nr real data from the reference distribution Pr = N (0d, Id) with a dimension of d = 100 to train the first model G1. For the next generations, we sample ns synthetic data from model Gt\u22121 with sampling bias \u03bb, and combine it with the same nr real samples we used to train G1. We report nenr with ne defined in Equation 3.\nWe report the results for this experiment in Figure 30 and 31. We observe that for any values of nr > d with d = 100, the presence of synthetic samples reduces effective number of samples progressively. However, when the problem is ill-posed, i.e. nr < d, ne can be increased with respect to nr if some sampling bias \u03bb is present in the system. However, in our experiments we observe that ne cannot surpass nr for any values of \u03bb or ne, as it will always corresponds to an ill-posed problem ne < d."
        },
        {
            "heading": "I ADDITIONAL RESULTS FOR THE FRESH DATA LOOP",
            "text": "Here we provide three additional Gaussian experiments investigating the convergence of the fresh data loop.\nExperiment 1: In Figure 10 we showed how Gaussian fresh data loop convergence depends on ns and nr for a few different values of \u03bb. Now we depict how convergence depends on ns and \u03bb for a few different values of nr.\nExperiment 2: In Section 5 we assumed that we only sample from the previous generation Gt\u22121 for creating the synthetic dataset Dts. In this experiment we sample randomly from K previous models (G\u03c4 )t\u22121\u03c4=t\u22121\u2212K . Here nr = 103, ns = 104, and \u03bb = 1. In Figure 33 we see how ne nr\nvaries with respect to K. Increasing the memory K in sampling from previous generations can boost performance, however the rate of improvement becomes slower as K increases. However the rate of improvement on ne is sublinear with respect to K.\nExperiment 3: Here we assume that we are sampling from an environment where p percent of data is real, and the rest is synthetic data from the previous generation Gt\u22121 with sampling bias \u03bb. We change the total number of data in the dataset n = |Dt|, with nr = p\u00d7 n and ns = (1\u2212 n)\u00d7 p. We show the Wasserstein distance for different p and \u03bb in Figure 34.\nLet us first examine the dynamics of the Gaussian fresh data loop without sampling bias (\u03bb = 1). We observe in Figure 34 (left) that the Wasserstein distance (WD) decreases with respect to dataset size n. However, the presence of synthetic data (p < 100%) decreases the rate at which the WD decreases, and increases the overall WD each generation in the fresh data loop. This means that with presence of synthetic data in the Internet, the progress of generative models will become slower\nIn the presence of sampling bias (\u03bb < 1, Figure 34 right), we see that even for close values of \u03bb to 1, the Wasserstein distance follows a sub-linear trend, meaning that eventually the rate of progress in generative models will effectively stop, no matter how much (realistically) the total dataset size is increased."
        }
    ],
    "title": "SELF-CONSUMING GENERATIVE MODELS GO MAD",
    "year": 2023
}