{
    "abstractText": "This paper reveals that large language models (LLMs), despite being trained solely on text data, are surprisingly strong encoders for purely visual tasks in the absence of language. Even more intriguingly, this can be achieved by a simple yet previously overlooked strategy \u2013 employing a frozen transformer block from pretrained LLMs as a constituent encoder layer to directly process visual tokens. Our work pushes the boundaries of leveraging LLMs for computer vision tasks, significantly departing from conventional practices that typically necessitate a multimodal vision-language setup with associated language prompts, inputs, or outputs. We demonstrate that our approach consistently enhances performance across a diverse range of tasks, encompassing pure 2D and 3D visual recognition tasks (e.g., image and point cloud classification), temporal modeling tasks (e.g., action recognition), non-semantic tasks (e.g., motion forecasting), and multi-modal tasks (e.g., 2D/3D visual question answering and image-text retrieval). Such improvements are a general phenomenon, applicable to various types of LLMs (e.g., LLaMA and OPT) and different LLM transformer blocks. We additionally propose the information filtering hypothesis to explain the effectiveness of pre-trained LLMs in visual encoding \u2013 the pre-trained LLM transformer blocks discern informative visual tokens and further amplify their effect. This hypothesis is empirically supported by the observation that the feature activation, after training with LLM transformer blocks, exhibits a stronger focus on relevant regions. We hope that our work inspires new perspectives on utilizing LLMs and deepening our understanding of their underlying mechanisms.",
    "authors": [
        {
            "affiliations": [],
            "name": "FROZEN TRANSFORMERS"
        }
    ],
    "id": "SP:ded8341928556cf280c8fae72d2e8a01ce82ac18",
    "references": [
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Jeff Donahue",
                "Pauline Luc",
                "Antoine Miech",
                "Iain Barr",
                "Yana Hasson",
                "Karel Lenc",
                "Arthur Mensch",
                "Katherine Millican",
                "Malcolm Reynolds"
            ],
            "title": "Flamingo: a visual language model for few-shot learning",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Daichi Azuma",
                "Taiki Miyanishi",
                "Shuhei Kurita",
                "Motoaki Kawanabe"
            ],
            "title": "ScanQA: 3D question answering for spatial scene understanding",
            "year": 2022
        },
        {
            "authors": [
                "David Bau",
                "Bolei Zhou",
                "Aditya Khosla",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Network dissection: Quantifying interpretability of deep visual representations",
            "year": 2017
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Angel X Chang",
                "Thomas Funkhouser",
                "Leonidas Guibas",
                "Pat Hanrahan",
                "Qixing Huang",
                "Zimo Li",
                "Silvio Savarese",
                "Manolis Savva",
                "Shuran Song",
                "Hao Su"
            ],
            "title": "Shapenet: An information-rich 3d model repository",
            "venue": "arXiv preprint arXiv:1512.03012,",
            "year": 2015
        },
        {
            "authors": [
                "Ming-Fang Chang",
                "John Lambert",
                "Patsorn Sangkloy",
                "Jagjeet Singh",
                "Slawomir Bak",
                "Andrew Hartnett",
                "De Wang",
                "Peter Carr",
                "Simon Lucey",
                "Deva Ramanan"
            ],
            "title": "Argoverse: 3d tracking and forecasting with rich maps",
            "year": 2019
        },
        {
            "authors": [
                "Damai Dai",
                "Li Dong",
                "Yaru Hao",
                "Zhifang Sui",
                "Baobao Chang",
                "Furu Wei"
            ],
            "title": "Knowledge neurons in pretrained transformers",
            "venue": "In ACL,",
            "year": 2022
        },
        {
            "authors": [
                "Alexandros Delitzas",
                "Maria Parelli",
                "Nikolas Hars",
                "Georgios Vlassis",
                "Sotirios Anagnostidis",
                "Gregor Bachmann",
                "Thomas Hofmann"
            ],
            "title": "Multi-CLIP: Contrastive Vision-Language Pre-training for Question Answering tasks in 3D Scenes",
            "venue": "In BMVC,",
            "year": 2023
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "year": 2021
        },
        {
            "authors": [
                "Zi-Yi Dou",
                "Yichong Xu",
                "Zhe Gan",
                "Jianfeng Wang",
                "Shuohang Wang",
                "Lijuan Wang",
                "Chenguang Zhu",
                "Pengchuan Zhang",
                "Lu Yuan",
                "Nanyun Peng"
            ],
            "title": "An empirical study of training end-to-end vision-and-language transformers",
            "year": 2022
        },
        {
            "authors": [
                "Dumitru Erhan",
                "Yoshua Bengio",
                "Aaron Courville",
                "Pascal Vincent"
            ],
            "title": "Visualizing higher-layer features of a deep network",
            "venue": "University of Montreal,",
            "year": 2009
        },
        {
            "authors": [
                "Jiyang Gao",
                "Chen Sun",
                "Hang Zhao",
                "Yi Shen",
                "Dragomir Anguelov",
                "Congcong Li",
                "Cordelia Schmid"
            ],
            "title": "Vectornet: Encoding hd maps and agent dynamics from vectorized representation",
            "year": 2020
        },
        {
            "authors": [
                "Shanghua Gao",
                "Zhong-Yu Li",
                "Ming-Hsuan Yang",
                "Ming-Ming Cheng",
                "Junwei Han",
                "Philip Torr"
            ],
            "title": "Large-scale unsupervised semantic segmentation",
            "year": 2022
        },
        {
            "authors": [
                "Mor Geva",
                "Roei Schuster",
                "Jonathan Berant",
                "Omer Levy"
            ],
            "title": "Transformer feed-forward layers are key-value memories",
            "venue": "arXiv preprint arXiv:2012.14913,",
            "year": 2020
        },
        {
            "authors": [
                "Ankit Goyal",
                "Hei Law",
                "Bowei Liu",
                "Alejandro Newell",
                "Jia Deng"
            ],
            "title": "Revisiting point cloud shape classification with a simple and effective baseline",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Priya Goyal",
                "Piotr Doll\u00e1r",
                "Ross Girshick",
                "Pieter Noordhuis",
                "Lukasz Wesolowski",
                "Aapo Kyrola",
                "Andrew Tulloch",
                "Yangqing Jia",
                "Kaiming He"
            ],
            "title": "Accurate, large minibatch SGD: Training imagenet in 1 hour",
            "venue": "arXiv preprint arXiv:1706.02677,",
            "year": 2017
        },
        {
            "authors": [
                "Raghav Goyal",
                "Samira Ebrahimi Kahou",
                "Vincent Michalski",
                "Joanna Materzynska",
                "Susanne Westphal",
                "Heuna Kim",
                "Valentin Haenel",
                "Ingo Fruend",
                "Peter Yianilos",
                "Moritz Mueller-Freitag"
            ],
            "title": "The\u201d something something\u201d video database for learning and evaluating visual common sense",
            "venue": "In ICCV,",
            "year": 2017
        },
        {
            "authors": [
                "Yash Goyal",
                "Tejas Khot",
                "Douglas Summers-Stay",
                "Dhruv Batra",
                "Devi Parikh"
            ],
            "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Jiaxian Guo",
                "Junnan Li",
                "Dongxu Li",
                "Anthony Meng Huat Tiong",
                "Boyang Li",
                "Dacheng Tao",
                "Steven Hoi"
            ],
            "title": "From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models",
            "year": 2023
        },
        {
            "authors": [
                "Tanmay Gupta",
                "Aniruddha Kembhavi"
            ],
            "title": "Visual programming: Compositional visual reasoning without training",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "year": 2022
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Thomas Dietterich"
            ],
            "title": "Benchmarking neural network robustness to common corruptions and perturbations",
            "venue": "In ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "Gaussian error linear units (gelus)",
            "venue": "arXiv preprint arXiv:1606.08415,",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Steven Basart",
                "Norman Mu",
                "Saurav Kadavath",
                "Frank Wang",
                "Evan Dorundo",
                "Rahul Desai",
                "Tyler Zhu",
                "Samyak Parajuli",
                "Mike Guo"
            ],
            "title": "The many faces of robustness: A critical analysis of out-of-distribution generalization",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Zhao",
                "Steven Basart",
                "Jacob Steinhardt",
                "Dawn Song"
            ],
            "title": "Natural adversarial examples",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation,",
            "year": 1997
        },
        {
            "authors": [
                "Yining Hong",
                "Haoyu Zhen",
                "Peihao Chen",
                "Shuhong Zheng",
                "Yilun Du",
                "Zhenfang Chen",
                "Chuang Gan"
            ],
            "title": "3d-llm: Injecting the 3d world into large language models",
            "venue": "NeurIPS,",
            "year": 2023
        },
        {
            "authors": [
                "Andrew Jaegle",
                "Felix Gimeno",
                "Andy Brock",
                "Oriol Vinyals",
                "Andrew Zisserman",
                "Joao Carreira"
            ],
            "title": "Perceiver: General perception with iterative attention",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "year": 2001
        },
        {
            "authors": [
                "Lee Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "In NAACL,",
            "year": 2019
        },
        {
            "authors": [
                "Wonjae Kim",
                "Bokyung Son",
                "Ildoo Kim"
            ],
            "title": "Vilt: Vision-and-language transformer without convolution or region supervision",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Jing Yu Koh",
                "Ruslan Salakhutdinov",
                "Daniel Fried"
            ],
            "title": "Grounding language models to images for multimodal inputs and outputs",
            "year": 2023
        },
        {
            "authors": [
                "Ranjay Krishna",
                "Yuke Zhu",
                "Oliver Groth",
                "Justin Johnson",
                "Kenji Hata",
                "Joshua Kravitz",
                "Stephanie Chen",
                "Yannis Kalantidis",
                "Li-Jia Li",
                "David A Shamma"
            ],
            "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
            "year": 2017
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi"
            ],
            "title": "Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi"
            ],
            "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
            "year": 2023
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "Xudong Lin",
                "Simran Tiwari",
                "Shiyuan Huang",
                "Manling Li",
                "Mike Zheng Shou",
                "Heng Ji",
                "ShihFu Chang"
            ],
            "title": "Towards fast adaptation of pretrained contrastive models for multi-channel videolanguage retrieval",
            "year": 2023
        },
        {
            "authors": [
                "Yicheng Liu",
                "Jinghuai Zhang",
                "Liangji Fang",
                "Qinhong Jiang",
                "Bolei Zhou"
            ],
            "title": "Multimodal motion prediction with stacked transformers",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 1907
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Sgdr: Stochastic gradient descent with warm restarts",
            "venue": "arXiv preprint arXiv:1608.03983,",
            "year": 2016
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Xiaojian Ma",
                "Silong Yong",
                "Zilong Zheng",
                "Qing Li",
                "Yitao Liang",
                "Song-Chun Zhu",
                "Siyuan Huang"
            ],
            "title": "SQA3D: Situated question answering in 3d scenes",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Xu Ma",
                "Can Qin",
                "Haoxuan You",
                "Haoxi Ran",
                "Yun Fu"
            ],
            "title": "Rethinking network design and local geometry in point cloud: A simple residual mlp framework",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Meng",
                "David Bau",
                "Alex Andonian",
                "Yonatan Belinkov"
            ],
            "title": "Locating and editing factual associations in gpt",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Meng",
                "Arnab Sen Sharma",
                "Alex Andonian",
                "Yonatan Belinkov",
                "David Bau"
            ],
            "title": "Mass editing memory in a transformer",
            "venue": "arXiv preprint arXiv:2210.07229,",
            "year": 2022
        },
        {
            "authors": [
                "Jack Merullo",
                "Louis Castricato",
                "Carsten Eickhoff",
                "Ellie Pavlick"
            ],
            "title": "Linearly mapping from image to text",
            "venue": "space. ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Vicente Ordonez",
                "Girish Kulkarni",
                "Tamara Berg"
            ],
            "title": "Im2text: Describing images using 1 million captioned photographs",
            "venue": "NeurIPS,",
            "year": 2011
        },
        {
            "authors": [
                "Bryan A Plummer",
                "Liwei Wang",
                "Chris M Cervantes",
                "Juan C Caicedo",
                "Julia Hockenmaier",
                "Svetlana Lazebnik"
            ],
            "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models",
            "venue": "In ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "Charles R Qi",
                "Or Litany",
                "Kaiming He",
                "Leonidas J Guibas"
            ],
            "title": "Deep hough voting for 3d object detection in point clouds",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Sarah Schwettmann",
                "Neil Chowdhury",
                "Antonio Torralba"
            ],
            "title": "Multimodal neurons in pretrained text-only transformers",
            "venue": "arXiv preprint arXiv:2308.01544,",
            "year": 2023
        },
        {
            "authors": [
                "Ramprasaath R Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "year": 2017
        },
        {
            "authors": [
                "Piyush Sharma",
                "Nan Ding",
                "Sebastian Goodman",
                "Radu Soricut"
            ],
            "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "year": 2018
        },
        {
            "authors": [
                "Baifeng Shi",
                "Trevor Darrell",
                "Xin Wang"
            ],
            "title": "Top-down visual attention from analysis by synthesis",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Jianlin Su",
                "Yu Lu",
                "Shengfeng Pan",
                "Ahmed Murtadha",
                "Bo Wen",
                "Yunfeng Liu"
            ],
            "title": "Roformer: Enhanced transformer with rotary position embedding",
            "venue": "arXiv preprint arXiv:2104.09864,",
            "year": 2021
        },
        {
            "authors": [
                "Zhan Tong",
                "Yibing Song",
                "Jue Wang",
                "Limin Wang"
            ],
            "title": "Videomae: Masked autoencoders are dataefficient learners for self-supervised video pre-training",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Matthijs Douze",
                "Francisco Massa",
                "Alexandre Sablayrolles",
                "Herve Jegou"
            ],
            "title": "Training data-efficient image transformers and distillation through attention",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Mikaela Angelina Uy",
                "Quang-Hieu Pham",
                "Binh-Son Hua",
                "Duc Thanh Nguyen",
                "Sai-Kit Yeung"
            ],
            "title": "Revisiting point cloud classification: A new benchmark dataset and classification model on realworld data",
            "year": 2019
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Haohan Wang",
                "Songwei Ge",
                "Zachary Lipton",
                "Eric P Xing"
            ],
            "title": "Learning robust global representations by penalizing local predictive power",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Wenhai Wang",
                "Zhe Chen",
                "Xiaokang Chen",
                "Jiannan Wu",
                "Xizhou Zhu",
                "Gang Zeng",
                "Ping Luo",
                "Tong Lu",
                "Jie Zhou",
                "Yu Qiao"
            ],
            "title": "Visionllm: Large language model is also an open-ended decoder for vision-centric tasks",
            "venue": "arXiv preprint arXiv:2305.11175,",
            "year": 2023
        },
        {
            "authors": [
                "Yilun Xu",
                "Shengjia Zhao",
                "Jiaming Song",
                "Russell Stewart",
                "Stefano Ermon"
            ],
            "title": "A theory of usable information under computational constraints",
            "year": 2020
        },
        {
            "authors": [
                "Xumin Yu",
                "Lulu Tang",
                "Yongming Rao",
                "Tiejun Huang",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "title": "Point-bert: Pretraining 3d point cloud transformers with masked point modeling",
            "year": 2021
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models",
            "venue": "arXiv preprint arXiv:2205.01068,",
            "year": 2022
        },
        {
            "authors": [
                "Bolei Zhou",
                "Yiyou Sun",
                "David Bau",
                "Antonio Torralba"
            ],
            "title": "Interpretable basis decomposition for visual explanation",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Goyal"
            ],
            "title": "2017a). Concretely, VideoMAE adopts the batch size of 384 video",
            "year": 2017
        },
        {
            "authors": [
                "Loshchilov",
                "Hutter"
            ],
            "title": "2017) with a learning rate of 5e-4 and batch size equal to 32 samples. For mmTransformer, we train it with the same learning rate, batch size, and optimizer as VectorNet. The training lasts 32 epochs, where we drop the learning rate by 1/4 on epochs 20, 24, and 28. The training time for both models is around 2 days on one A100",
            "year": 2017
        },
        {
            "authors": [
                "H FD Figure"
            ],
            "title": "Intuitive illustration of METER (Dou et al., 2022) framework and how to insert the frozen language transformer (pink",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Large language models (LLMs), trained on massive amounts of text data, have recently demonstrated remarkable potential across various tasks, extending beyond their original linguistic domain. For example, in the field of computer vision, LLMs exhibit the ability to interact with visual tokens and decode them into tokenized output. This is commonly achieved in a multi-modal visionlanguage framework that incorporates the language modality, as exemplified by either projecting visual tokens to LLMs via linear layers (Koh et al., 2023; Lin et al., 2023; Merullo et al., 2023; Schwettmann et al., 2023) or employing cross-attention mechanisms between visual and language tokens (Alayrac et al., 2022; Li et al., 2022; 2023; Wang et al., 2023). As we explore the limits of utilizing LLMs for computer vision tasks, an interesting question arises: can LLMs effectively handle tasks that are exclusively visual, without any reliance on language?\nThis paper provides a positive demonstration of feasibility in addressing this question, by introducing a straightforward yet previously overlooked approach: incorporating a frozen transformer block from a pre-trained LLM as a general-purpose visual encoder layer, directly processing the visual tokens. Specifically, as illustrated in Fig. 1a and Fig. 1b, our design involves the following steps: (1) extract a frozen LLM transformer block and append it on top of the original visual encoder; (2) insert trainable linear layers before and after the added LLM block to align the feature dimensions; and (3) freeze the LLM transformer while optimizing the other modules as usual during training.\nSurprisingly, this simple design enhances performance across a wide spectrum of tasks, including 2D and 3D recognition (image and point cloud classification), video understanding (action recognition), and non-semantic (motion forecasting) tasks. In addition to these purely visual tasks, our approach is also effective in multi-modal tasks (2D/3D visual question answering and image-text retrieval).\nNotably, such improvements are general across various types of LLMs like LLaMA (Touvron et al., 2023) and OPT (Zhang et al., 2022), as well as different LLM transformer blocks.\nOur discovery of using a pre-trained LLM transformer block as a visual encoder layer is intriguing, because it significantly deviates from the conventional designs of vision-language models (VLMs). In particular, our treatment of LLM transformers as encoders (1) operates independently of language prompts, inputs, or outputs; (2) allows for training from scratch without the need for pre-trained backbones like CLIP (Radford et al., 2021); and (3) decouples and simplifies the usage of LLMs into separate transformer blocks.\nHowever, one crucial question remains: why are LLMs effective in visual encoding, given that they have been exclusively trained on text and have never encountered visual input? To this end, we propose the information filtering hypothesis: the pre-trained LLM transformer blocks discern informative visual tokens and further amplify their contribution to the latent representation. This hypothesis stems from our observation across multiple tasks, where the feature activation consistently exhibits a stronger focus on relevant regions, after integrating the frozen LLM transformer blocks.\nIn summary, we have made the following contributions:\n\u2022 We discover that using a frozen transformer block from pre-trained LLMs as a visual encoder layer facilitates a diverse range of tasks, by introducing a simple yet under-explored approach. \u2022 We demonstrate that the benefits of frozen LLM transformers in visual encoding are a general phenomenon, through our investigation on various LLMs and transformer blocks. \u2022 We propose the information filtering hypothesis to explain the effectiveness of frozen LLM transformers in processing visual tokens: the incorporated LLM blocks distinguish the informative tokens and amplify their effect.\nWe hope that our work will drawn attention to the intriguing application of employing LLM transformers as versatile encoders, not only for visual inputs but potentially also for other modalities. Additionally, we hope to inspire new perspectives on understanding LLMs/VLMs."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Large language models. Pretraining transformers (Vaswani et al., 2017) with masked token prediction facilitates the generalizability of language models (LMs) to various tasks, represented by BERT (Kenton & Toutanova, 2019). Later on, larger models at scale are proposed guided by the scaling law (Kaplan et al., 2020), such as GPT (Brown et al., 2020), LLaMA (Touvron et al., 2023), OPT (Zhang et al., 2022), etc. These large models with tens of billions of parameters unlock the intriguing ability of in-context learning and excellent zero-shot performance on various tasks. Our work highlights the interesting discovery that the transformer blocks in such large language models (LLMs) are able to interact with visual data and enhance a wide spectrum of computer vision tasks.\nLanguage models for visual tasks. LMs are mostly used as text encoders for vision-language models (VLMs) (Dou et al., 2022; Kim et al., 2021) or image-text pretraining (Radford et al., 2021) before the emergence of LLMs. After the creation of LLMs, their code generation ability encourages\nflexibly combining the vision algorithms for user queries, represented by visual programming (Gupta & Kembhavi, 2023). In addition, the strong ability of LLMs has also elicited using them as generalizable decoders, i.e., translating the latent feature into output tokens. These frameworks commonly project visual features to the input layer of LLMs directly (Guo et al., 2023; Koh et al., 2023; Lin et al., 2023; Merullo et al., 2023) or use the structures of latent bottleneck (Jaegle et al., 2021) to further encode visual tokens (Alayrac et al., 2022; Hong et al., 2023; Li et al., 2022; 2023; Wang et al., 2023). Our exploration reveals the potential of considering the transformer blocks in LLMs as general-purpose encoders for visual data, as opposed to the previous usages of either pure encoders for text embeddings or decoders for tokenized outputs.\nInterpreting neural networks. Understanding neural networks begins by visualizing the convolutional patterns in low-level layers (Erhan et al., 2009). With a deeper interest in semantics, attribution-based methods like Grad-CAM (Selvaraju et al., 2017) further analyze the contribution of neurons for a certain class. The network dissection studies (Bau et al., 2017; Zhou et al., 2018) also discover that neural network units correspond to semantic concepts. For LLMs, researchers discover that the knowledge is mainly located at the MLPs in feedforward networks (FFN) (Dai et al., 2022; Geva et al., 2020; Meng et al., 2022a;b), and sometimes correspond to visual concepts (Schwettmann et al., 2023). Compared to them, we study a new scenario of why a pre-trained LLM transformer can benefit visual encoding and propose the information filtering hypothesis."
        },
        {
            "heading": "3 METHOD: FROZEN LLM TRANSFORMER FOR VISUAL ENCODING",
            "text": "Framework design. We formally introduce using a pre-trained LLM transformer as a visual encoder layer shown in Fig. 1a. Without loss of generality, we consider a neural network that maps a input x to latent representation z and predicts labels y with an encoder FE and decoder FD,\nFE(x)\u2212\u2192 z, FD(z)\u2212\u2192 y. (1) Then a single pre-trained transformer block from an LLM like LLaMA (Touvron et al., 2023), denoted as FLM , is inserted between the encoder FE and decoder FD. As the feature dimensions are different between the encoder FE and the language transformer FLM , we employ two linear layers F1L and F2L before and after FLM to align the dimensionality. These modify the neural network into FE(x)\u2212\u2192 z, F2L \u00b7FLM \u00b7F1L(z)\u2212\u2192 z\u2032, FD(z\u2032)\u2212\u2192 y. (2) In the training stage, the pretrained transformer FLM remains frozen, as in the pseudo-code of Fig. 1b, while all the other modules are trained normally, including F1L and F2L.\nComparison with vision-language models. Our approach appears similar to recent visionlanguage models (VLMs) at the first glance, such as Lin et al. (2023), FROMAGe (Koh et al., 2023), and LiMBeR (Merullo et al., 2023), where linear layers directly project visual features to the input space of LLMs. However, our approach is different, because the linear layer F1L does not necessarily align the visual representation z into the language space. Concretely, this is reflected in three aspects: (1) Independence of visual pre-training. Our paradigm supports training-fromscratch without relying on pre-trained visual encoders like CLIP (Radford et al., 2021). (2) Independence of language. Our framework can function without language-based input or prompts, and it is applicable for general visual representation learning instead of only vision-language tasks. (3) Independence of transformer blocks. Previous VLMs treat an LLM as a coherent module, while our framework separates each transformer block as an independent layer for visual encoding.\nComparison with LLMs. We substantially change the behaviors of LLM transformers, due to the distinct formats between visual and text data. (1) Attention mask. LLMs commonly utilize autoregressive masks to mimic the order of text generation. However, the tokens in visual data come all at once, such as the image tokens of the cat (Fig. 1a). So we abandon auto-regressive attention masks and only use attention masks to indicate the padded tokens. (2) Positional embedding. The positional embedding in LLMs, e.g., rotary positional embedding (Su et al., 2021) in LLaMA, is not a common option for visual encoders. Therefore, we remove the positional embeddings of LLMs for simplicity and consistency with the original visual backbones. Considering the importance of attention masks and positional embeddings in transformers, it is surprising in hindsight that our framework has a positive influence on visual tasks."
        },
        {
            "heading": "4 APPLICABILITY OF LLM TRANSFORMERS FOR VISUAL TASKS",
            "text": "We instantiate our framework to various tasks and observe the wide applicability of pretrained LLM transformers. Our experiments cover 2D (image classification) and 3D (point cloud classification),\nsingle-frame and multi-frame (action recognition), semantics and motion (motion forecasting), and involving linguistic input or not (2D and 3D vision-language tasks). By default, we adopt the last transformer block from LLaMA-7B (Touvron et al., 2023). Our framework achieves consistent and significant improvements across these tasks following the standards of prior work. More analysis on our designs is in Sec. 5."
        },
        {
            "heading": "4.1 IMAGE CLASSIFICATION",
            "text": "Image classification is the most common challenge for representation learning. We conduct experiments on ImageNet1k (Deng et al., 2009), and additionally evaluate on robustness benchmarks: corrupted images from ImageNet-C (Hendrycks & Dietterich, 2018), natural adversarial images from ImageNet-A (Hendrycks et al., 2021b), and out-of-distribution images from ImageNet-SK (Wang et al., 2019) and ImageNet-R (Hendrycks et al., 2021a).\nWithout loss of generality, we select ViT (Dosovitskiy et al., 2021) due to its wide use and native support for transformers. Following the notation in Eqn. 2, the encoder FE is the set of self-attention transformer blocks and the decoder FD denotes the linear classifier, respectively. An intuitive illustration is in Fig. 1a. We train both the baseline ViT models and ViT-LLaMA from scratch following the same configuration of DeiT (Touvron et al., 2021). More details are in Section C.1.\nThe accuracy of ViT models consistently improves after incorporating the frozen LLaMA transformer block as in Table 1, including both the accuracy on clean ImageNet images and robustness on corrupted or adversarial images. Our further experiments validate that the improvement is closely related to the LLM transformer instead of the sole consequence of an increased model capacity. Please refer to Sec. 5.1 for details.\n4.2 POINT CLOUD RECOGNITION\nPoint cloud classification handles a fundamentally different modality compared to images. The models predict the labels by processing unordered 3D points and understanding the geometry. Our experiments cover two common datasets: ScanObjectNN (Uy et al., 2019) and ModelNet40 (Goyal et al., 2021). ScanObjectNN contains three splits: background points (BG), foreground (OBJ), and clipped (T50) points. For ModelNet40, we experiment with different densities (1k, 4k, 8k) of points.\nWe adopt Point-BERT (Yu et al., 2021) and load its pretrained parameters on ShapeNet (Chang et al., 2015). Then we append the LLaMA transformer after its final attention block before fine-\ntuning on point cloud classification datasets. Details are in Sec. C.2.\nAs shown in Table 2, our approach improves the accuracy for point cloud classification, further supporting the applicability of using a frozen LLM transformer as a visual encoding layer. Note that the accuracy slightly drops on ModelNet40 with 1k and 4k points, due to the saturation and \u223c0.2% variance of performance on ModelNet40 which is also analyzed in Ma et al. (2022). However, with an increased number of points (8k), the improvement of the LLaMA transformer is noticeable on ModelNet40. More importantly, on the more challenging ScanObjectNN, our approach improves the accuracy consistently and significantly. This experiment also shows that our framework is compatible with finetuning setups, in addition to training-from-scratch scenarios in Sec. 4.1."
        },
        {
            "heading": "4.3 ACTION RECOGNITION",
            "text": "For the video modality, we apply the pretrained LLM transformer block to action recognition, where the algorithm predicts the action labels of video clips. We choose the benchmark of \u201cSomethingsomething-v2\u201d (SSv2) (Goyal et al., 2017b) for evaluation, because it highlights the challenge of understanding cross-frame movement, instead of relying on single-frame semantics.\nWe follow VideoMAE (Tong et al., 2022) and adopt the simple yet effective ViT backbones. Different from patches of tokens in 2D images, the video tokens are cubes spanning both spatially and temporally. Identical to Fig. 1a, we place the LLaMA transformer behind the last self-attention block in ViT. Our training setup also adopts the two-step practice in VideoMAE: (1) initialize ViT transformers from MAE (He et al., 2022) pre-training; (2) add the LLM transformer and then finetune on the SSv2 dataset using the same configuration of VideoMAE. More details are in Sec. C.3.\nThe LLaMA transformer enhances the accuracy for both ViT-S and ViT-B in Table 3, supporting the applicability of our framework for videos. To clarify, our baseline accuracy is lower than VideoMAE\u2019s original paper, because VideoMAE used 32/64 GPUs to enable a larger batch size than our computational resources. Nonetheless, we control the settings identical between ViT and ViT-LLaMA for a fair comparison and indicate the positive effects of using LLM transformers.\n4.4 MOTION FORECASTING\nWe select motion forecasting as an example of a nonsemantic task. It is safety-critical for autonomous driving and capitalizes on the understanding of dynamics, agentagent interaction, and agent-lane relationship. The input commonly includes the historical trajectories of agents and way-points of lane segments, which are both represented in polylines on the bird\u2019s-eye view (BEV). The desired output is a set of K most possible future trajectories.\nWe conduct experiments on Argoverse (Chang et al., 2019). The evaluation metrics are minimum average displacement (ADE), minimum final displacement (FDE), and miss rate (MR), which calculate the errors of predictions from different aspects and are better at lower values. We apply the frozen LLM transformer to VectorNet (Gao et al., 2020) and mmTransformer (Liu et al., 2021). They first convert the agents and lanes into features, and then our LLaMA transformer block processes these agent and lane tokens. Demonstration and details are in Sec. C.4.\nAccording to Table 4, the models with LLaMA forecast better trajectories. However, we notice that the improvement is less significant compared with semantic tasks, which reflects the preference of LLM transformers for encoding rich semantics over object movements."
        },
        {
            "heading": "4.5 VISION-LANGUAGE TASKS",
            "text": "2D Vision-Language tasks. The benefits of frozen LLM transformers for visual encoding are not limited to purely visual tasks. We experiment with 2D vision-language (VL) tasks, including visual question answering (VQA) on VQAv2 (Goyal et al., 2017c) and zero-shot image retrieval on Flickr30k (Plummer et al., 2015). We adopt the widely-used METER (Dou et al., 2022) as our baseline. It extracts uni-modal features for images and texts, fuses cross-modal features, and decodes the output from the cross-modal features. We insert the LLM transformer block after the cross-modal fusion, which is the end of METER\u2019s encoder. An intuitive illustration is in Fig. H. During training, our setup follows Shi et al. (2023): initialize the image encoder with CLIP-B/32 (Radford et al., 2021) and text encoder with RoBERTa (Liu et al., 2019), and then finetune on VQAv2 or Flickr30k. Details are in Sec. C.5. As in Table 5a, both of the 2D VL tasks are significantly enhanced with the LLaMA transformer. This evidence supports the potential of a frozen LLM transformer for multi-modal tasks. 3D Visual Question Answering. We extend our proposed idea into 3D VQA, which requires comprehending an input 3D scene represented by point clouds or multi-view images and then answering questions. 3D VQA challenges the ability to ground language in 3D contexts. We conduct our experiments on the SQA3D (Ma et al., 2023) dataset, comparing with baseline methods and state of the arts (Ma et al., 2023; Azuma et al., 2022) on the exact match (EM) metric. We follow the baseline SQA3D to process the textual input with LSTM (Hochreiter & Schmidhuber, 1997) and 3D\npoint clouds with VoteNet (Qi et al., 2019). Here, we add the LLM block after the VL fusion, which is consistent with our 2D VL design (Sec. 4.5). More details are in Sec. C.6. According to Table 5b, adding a frozen LLM transformer effectively enhances the QA ability of models. Full comparison with detailed breakdown metrics is in Table C."
        },
        {
            "heading": "5 ANALYSIS ON LLM TRANSFORMERS FOR VISUAL TASKS",
            "text": "We justify our design choices (Sec. 5.1) and illustrate the wide applicability of our framework to various LLMs and transformer layers (Sec. 5.2). Our investigation also discovers that sufficiently large LLMs are the premise of benefiting visual encoding with a frozen transformer (Sec. B.2).\n5.1 ABLATION STUDY ON DESIGN CHOICES\nModel Capacity. Regarding the wide applicability of frozen LLM transformers, we question if the improvement mainly comes from the increased capacity of the linear layers F1L and F2L, instead of the pre-trained weights in LLM transformers FLM . To analyze model capacity, we create ViT-S-MLP, which has identical trainable parameters compared with ViT-S-LLaMA. Concretely, ViTS-MLP removes the LLM block FLM , and then inserts a GeLU activation (Hendrycks & Gimpel, 2016) and layer normalization (Ba\net al., 2016) between F1L and F2L. It also adopts the identical training procedure as ViT and ViTLLaMA in Sec. 4.1 for a fair comparison. The results are summarized in Table 6: the ViT-S-MLP has better performance than ViT-S due to its increased capacity, but the improvement is only about half of ViT-S-LLaMA. Therefore, the LLM transformer weights are crucial for the improvement and the observed benefits are not mere consequences of an increased model capacity.\nFinetuning. We further verify whether finetuning the language transformer (ViT-S-LLaMA-FT) is better than freezing it. As in Table 6, finetuning decreases the performance compared with ViT-S-LLaMA. We analyze this phenomenon by visualizing the loss curves in Fig. F and training under an additional setting of 100 epochs. Although finetuning improves performance under short training (100 epochs in Table A), it hurts the accuracy when trained sufficiently: in Fig. F, ViT-S-LLaMA-FT shows lower training loss but relatively larger validation loss, which indicates overfitting. Thus, our observation demonstrates the challenges of training large transformers, and we accordingly freeze the LLM transformers in our design because of its simplicity and effectiveness."
        },
        {
            "heading": "5.2 VARYING LLM TRANSFORMER LAYERS",
            "text": "We discover that LLM transformers influence visual representation learning significantly under our framework, even though they have identical capacities. Specifically, we use transformer blocks from diverse depths of LLaMA-7B (Touvron et al., 2023) and OPT (Zhang et al., 2022) onto ViT-S. The models are trained on the ablation study setting of 100 epochs. (More details in Sec. C.7.) As shown in Fig. 2, the type of layer significantly changes the performance. These experiments also validate that our framework is applicable to various LLMs and transformer layers, and highlight the importance of selecting proper transformer layers. We additionally observe that the last LLM layers consistently improve the performance although they might not be optimal."
        },
        {
            "heading": "6 INFORMATION FILTERING HYPOTHESIS",
            "text": "This section aims to explain how a frozen and pre-trained LLM transformer benefits visual tasks. Intuitively, our hypothesis can be stated as:\nInformation filtering hypothesis. The pretrained LLM transformer functions as a \u201cfilter\u201d that distinguishes the informative tokens and amplifies their contribution for the prediction, in the form of enlarged magnitudes or frequencies in the feature activation.\nWe first derive this hypothesis in the context of image classification (Sec. 6.1), then provide quantitative investigation (Sec. 6.2) and discuss the observation on other tasks (Sec. 6.3). Due to space limits, we include more details in Sec. A and discuss limitations in Sec. A.4."
        },
        {
            "heading": "6.1 QUALITATIVE DERIVATION OF INFORMATION FILTERING HYPOTHESIS",
            "text": "Emergent concentration on informative tokens. Our hypothesis originates from the emergent behavior that the feature activation highlights the informative tokens after adding a pre-trained LLM transformer. In the analysis, we extract the activation of features after each layer as Fig. 3a, including the original ViT FE , the attention layer FALM and feedforward network FFLM in the LLM transformer, and the linear layers F1L and F2L. Notably, the feature activation is calculated regarding both magnitudes (L2-norm after centering) and frequencies (L2-norm of angles after Fourier transformation)1. The different layers in Fig. 3a indeed show diverse preferences over magnitudes or frequencies.\nAs clearly demonstrated in Fig. 3a, the token activation better captures the regions of target objects after adding the LLM transformer, especially the magnitudes of F2L and frequencies of FALM . Their tendency of segmentation is a surprising discovery because emergent segmentation only exists in self-supervised learned (Caron et al., 2021) or specially-designed (Shi et al., 2023) ViTs. More importantly, the activation\u2019s concentration on the target object directly supports our hypothesis as evidence of selecting the informative tokens.\nNoisy attention scores. In contrast to the feature activation, attention scores struggle to capture the relevant visual tokens for prediction. We investigate the attention scores between the CLS token and visual tokens in the last transformer block, which are the last self-attention block in FE for ViT and the transformer of FLM for ViT-LLaMA, respectively. Ideal attention scores that distinguish the target object should exhibit object segmentation patterns like DINO (Caron et al., 2021). However, supervised ViT models commonly have noisy attention scores (left part in Fig. 3b). Although ViT-LLaMA illustrates the ability of emergent segmentation in a few attention heads, most of the attention scores also suffer from scattering and noisiness. These observations contrast the feature activation and indicate that the benefits of LLM transformers cannot be simply attributed to attention scores, since attention scores fail to reliably contribute correct visual tokens.\nDeriving the amplification of informative tokens. According to our visualization in Fig. 3a, the frozen LLM transformer distinguishes the informative tokens. Intuitively, such tokens naturally\n1Details of calculation are in Sec. A.5.\nbenefit the downstream decoding, but this is straightforward only when the decoder directly takes the visual tokens as input. As a counterexample, ViT utilizes a CLS token for classification, and the visual tokens output by F2L is not the input to the decoder and always receives zero gradients during training. To bridge the gap in CLS token scenarios, the second half of our hypothesis is necessary: the frozen LLM transformer amplifies the contribution of informative tokens.\nConcretely, the calculation of CLS token is,\nz2L[CLS] = F 2 L \u00b7FFLM \u00b7FALM ( \u2211 v\u2208V wv z1L[v] ) , (3)\nwhere V denotes visual tokens, wv denotes weight of visual token v. Eqn. 3 describes the process of (1) aggregating visual tokens z1L[v] guided by the attention scores wv; and (2) sequentially flowing through the remaining layers, including the MLPs in LLM transformer\u2019s attention head FALM , feedforward network FFLM , and the second linear layer F2L. To concentrate on explaining visual tokens, Eqn. 3 also slightly simplifies self-attention by removing CLS token from the right-hand side.\nIn Eqn. 3, the useful visual tokens are not reliably attributed to CLS tokens because the attention scores wv are observed to be noisy. Therefore, our objective is to bridge the visual tokens in Eqn. 4 below, which are informative (as in Fig. 3a), to the final feature representation z2L[CLS].\nz2L[v] = F 2 L \u00b7FFLM \u00b7FALM(z1L[v]), where v \u2208V. (4)\nThis pursuit leads to our hypothesis that the frozen LLM blocks amplify the fraction of informative token features in addition to distinguishing them. We express the hypothesis below in a formal way, which is a simple change to Eqn. 3:\nz2L[CLS] \u221d \u2211 v\u2208V\nwv ( F2L \u00b7FFLM \u00b7FALM(z1L[v]) )\ufe38 \ufe37\ufe37 \ufe38 z2L[v] . [Hypothesis] (5)\nEqn. 5 holds equal under the special case of F2L \u00b7FFLM \u00b7FALM being linear transformation. This equation explains how the informative tokens in Fig. 3a are implicitly reflected and supervised in the CLS token. It further clarifies how the informative tokens can benefit the final prediction though the attention scores wi are noisy.\nAs a brief remark, our derivation builds upon two observed pieces of evidence: (1) visual tokens concentrating on informative regions; and (2) noisy attention scores. These lead to the first half of our hypothesis and explain the benefits when visual tokens are direct input to decoders. By further bridging visual tokens to CLS tokens, we propose that the inserted LLM transformer amplifies the effects of informative tokens. A more thorough version of the derivation is in Sec. A.1.\n6.2 QUANTITATIVE EVIDENCE\nThe qualitative observation in Sec. 6 is further supported with quantitative evidence. Specifically, we use the ImageNet-S (Gao et al., 2022) dataset to provide the ground truth of \u201cinformative regions\u201d from its annotation of semantic segmentation masks. To assess the fidelity of feature activation and attention scores, we first generate pseudo-masks highlighting their concentrating regions, i.e., the tokens with larger activation or attention scores than the others on the same image. Then the quality of features and attention scores are reflected by the mIoU (mean intersection-over-union) between their pseudo-masks and ground truth segmentation masks. Implementation details are in Sec. A.3.\nFinally, we summarize the mIoU statistics for feature activation and attention scores in Fig. 4. As demonstrated, both ViT-S-LLaMA and ViT-B-LLaMA have better mIoU of pseudo-masks than attention scores. This directly supports our hypothesis that the features {z2L[v])|v\u2208\nV} contribute more reliably than attention scores {wv|v\u2208V}. We additionally notice that the pseudomasks from ViT-LLaMA generally have larger mIoU compared with ViT, which reflects the benefits\nof training ViTs with a frozen LLM transformer. The advantage of the feature in the first linear layer F1L also reveals that training with our framework is beneficial to even earlier stages of features. However, we would like to clarify that the pseudo-masks from either magnitude or frequency activation are intuitive but lossy measures to quantify feature quality because neural networks can encode information into other formats. Therefore, developing better measurements to analyze individual network layers will be meaningful for future work."
        },
        {
            "heading": "6.3 INFORMATION FILTERING HYPOTHESIS ON OTHER TASKS",
            "text": "The previous sections mainly discuss our information filtering hypothesis in terms of image classification. Meanwhile, we also discover supportive evidence of our hypothesis on various other tasks. Due to space limits, this section investigates action recognition as an example, and Sec. A.2 covers additional tasks including point cloud classification, 2D VQA, and 3D VQA.\nWe mainly analyze the information filtering hypothesis in action recognition qualitatively because the ground truth of \u201crelevant regions\u201d is difficult to quantify for this task. In practice, we follow a similar procedure in Sec. 6.1 and visualize the activation of video tokens in Fig. 5. At a low activation threshold, we notice that the video tokens from ViT-S-LLaMA better capture the foreground areas of hands and manipulated objects than ViT-S. With the video tokens in VideoMAE activated both spatially and temporally, we further increase the threshold to demonstrate its ability to select informative frames. As in the \u201chigh threshold\u201d row of Fig. 5, ViT-S-LLaMA more accurately focuses on the middle frames with actual human-object interaction. Therefore, we conclude that the informative video tokens are indeed distinguished and augmented in action recognition, which aligns with the information filtering hypothesis."
        },
        {
            "heading": "7 CONCLUSIONS",
            "text": "Discussion and limitations. We have validated the capability of pre-trained, frozen language transformers across a wide spectrum of visual tasks. It is important to note that our goal is to methodically explore this under-investigated problem. Therefore, our experiments are designed to maximize the diversity of tasks under fair comparisons with well-established or competitive baselines, rather than striving for state-of-the-art performance for all tasks, which is also constrained by our computational resources. We leave scaling up the experiments to state-of-the-art levels for all tasks as interesting future work. Meanwhile, we also notice that our information filtering hypothesis has not covered several intriguing questions, e.g., how to quantify the functions of different layers and analyze how the training process facilitates the visual tokens features to cooperate with the language transformer, which are also meaningful directions.\nConclusions. In this work, we explore the unexpected capability of large language models (LLMs) as encoders for visual tasks, a significant departure from their conventional text-based applications. By seamlessly integrating a frozen transformer block from pre-trained LLMs into visual encoders, we observe consistent performance enhancements across diverse visual challenges from 2D image and video classification, 3D point cloud recognition, non-semantic motion forecasting, as well as 2D and 3D vision-language tasks. This phenomenon, underpinned by our proposed information filtering hypothesis, highlights the inherent adaptability and versatility of LLMs for more general representation learning. We hope that our insights will catalyze further exploration into the uncharted fields of LLM applications and foster innovative strategies to harness their potential in novel ways."
        },
        {
            "heading": "A APPENDIX FOR THE INFORMATION FILTERING HYPOTHESIS",
            "text": ""
        },
        {
            "heading": "A.1 DETAILED DERIVATION OF THE HYPOTHESIS",
            "text": "We expand the details of several steps in our derivation (Sec. 6.1) for better clarity. Beginning from the formula of the CLS token below,\nz2L[CLS] = F 2 L \u00b7FFLM \u00b7FALM ( \u2211 v\u2208V wv z1L[v] ) , (A)\nwhich is identical to the main paper (Eqn. 3), we further separate the visual tokens into the subset of informative tokens Vi and uninformative tokens Vu. This changes Eqn. A into,\nz2L[CLS] = F 2 L \u00b7FFLM \u00b7FALM ( \u2211 i\u2208Vi wi z1L[i]+ \u2211 u\u2208Vu wu z1L[u] ) . (B)\nUsing the same notation, our observation on the feature activation can be stated as: the attention weights for informative tokens {wi, i\u2208Vi} are still noisy after incorporating the frozen LLM transformer, while the final visual tokens shown as below have emergent concentration on target regions.\nz2L[i] = F 2 L \u00b7FFLM \u00b7FALM(z1L[i]), where i \u2208Vi (C)\nCombining Eqn. B and Eqn. C inspires us to express our hypothesis in terms of the connection between visual and CLS token with Eqn. D below, where the added modules F2L \u00b7FFLM \u00b7FALM augment the informative tokens Vi and lead to better prediction:\nz2L[CLS] \u221d \u2211 i\u2208Vi\nwi ( F2L \u00b7FFLM \u00b7FALM(z1L[i]) )\ufe38 \ufe37\ufe37 \ufe38 z2L[i] + \u2211 u\u2208Vu wu ( F2L \u00b7FFLM \u00b7FALM(z1L[u]) )\ufe38 \ufe37\ufe37 \ufe38 z2L[u] . [Hypothesis] (D)\nThis is a more thorough expression of our hypothesis in the main paper (Eqn. 5) to better differentiate the role of informative tokens in our hypothesis.\nA.2 INFORMATION FILTERING HYPOTHESIS ON OTHER TASKS\nThis section provides the evidence for our information filtering hypothesis in other tasks, supplementing the discussion on image classification (Sec. 6.1) and action recognition (Sec. 6.3). Specifically, we observe that the frozen LLM transformer selects and amplifies information tokens in point cloud recognition, 2D visual question answering (VQA), and 3D VQA. The tasks of motion forecasting and image-text retrieval are not illustrated because it is more abstract to intuitively define their \u201cinformative\u201d tokens. For the investigated tasks, we mainly analyze qualitatively because the ground truth for relevant regions is ambiguous on such tasks, unlike the segmentation masks for image classification (Sec. 6.2).\nPoint cloud recognition. We visualize the activation of the point tokens in point cloud classification before and after adding the frozen LLM transformer in Fig. A. In the examples of chairs and desks, we observe that \u201cPointBERT+LLaMA\u201d concentrates less on the background (chairs, indicated with red arrows) and more on the actual object surfaces (desks, indicated with red arrows). This demonstrates that the frozen LLM transformer learns to focus on the informative points, which is consistent with our hypothesis.\n2D VQA. We investigate the activation of visual tokens in the 2D VQA task in Fig. B. With the METER (Dou et al., 2022) framework initializing the visual backbone from CLIP (Radford et al., 2021) weights, the quality of feature activation is reasonable and mostly concentrates on the target regions for both the baseline METER and our \u201cMETER+LLaMA.\u201d However, we can still witness that feature activation better aligns with the images and questions than the noisy attention heads. Furthermore, the activation of \u201cMETER+LLaMA\u201d is also slightly advantageous over METER with less scattering, such as better concentrating on the regions of light and leaves at high thresholds.\n3D VQA. We analyze the effect of a frozen LLM transformer for 3D VQA and further confirm our hypothesis. As in Fig. C, we compare the activation of visual tokens, which are seed points in\n(a) Chair (b) Desk\nPointBERT PointBERT PointBERT+LLaMAPointBERT+LLaMA\nFigure A: Visualization of feature activation for point cloud classification. Brighter colors indicate higher activation values. To highlight the most salient activation, we apply a threshold to filter out points with low activation values. This visualization demonstrates that the model learns to focus on the most discriminative foreground object for classifying the point cloud after adding the frozen LLM transformer.\nIs the light on?\nLow\nMETER METER +LLaMAThreshold\nHigh\nHow many leaves?\nLow\nThreshold\nHigh\nMETER METER +LLaMA Attn Scores Attn Scores\nFigure B: Visualization of attention scores and feature activation for 2D VQA. We are able to visualize attention scores because METER uses CLS token. Both low and high thresholds for feature activation are displayed to illustrate the concentration on relevant regions.\nVoteNet (Qi et al., 2019), before and after incorporating the LLaMA transformer. To provide the context, the scenes in SQA3D (Ma et al., 2023) are projected onto the bird\u2019s-eye-view (BEV). From the visualizations, we clearly observe that the feature activation exhibits sharper concentration to the directions guided by language after adding LLaMA, such as the \u201ctable behind me\u201d areas in the left figure and \u201cto my left side\u201d areas in the right figure. Therefore, these indicate that the added LLM transformer filters the informative points and augments them for downstream question answering."
        },
        {
            "heading": "A.3 QUANTITATIVE EVIDENCE",
            "text": "In Sec. 6.2, we generate the pseudo-masks from feature activation or attention scores and then evaluate their quality with mIoU. This section describes the details.\nSituation: I am standing in front of the table and facing trash can.\nQuestion: How many chairs are at the table behind me?\nSituation: I am standing between the toilet on my right and sink on my left.\nQuestion: Is the door open or closed to my left side?"
        },
        {
            "heading": "SQA3D SQA3D+LLaMA SQA3D+LLaMASQA3D",
            "text": "Figure C: Analysis of feature activation for 3D VQA. The scenes are viewed from BEV. The green arrow marks the location and facing direction. The colors of points indicate their activation: a lighter color (yellow) represents a larger magnitude than a dark color (blue and green). We observe that \u201cSQA3D+LLaMA\u201d has sharper activation that is better related to the questions. Thus, it supports our information filtering hypothesis. (Best viewed in color and zooming in.)\nDataset. We leverage ImageNet-S (Gao et al., 2022) because it provides semantic segmentation masks for ImageNet (Deng et al., 2009) images. Specifically, we adopt the ImageNet-S version with 50 categories and run our evaluation on its validation set, to avoid data leakage from the training set.\nDefinition of IoU. Our IoU calculates the alignment between the highly activated tokens and the ground truth mask. As tokens are sparse and in low resolution, we slightly change the calculation of IoU for our purpose. Specifically, we first project the ground truth mask to the resolution of tokens to acquire a binary mask of tokens, indicating whether they are related to the target object (with value 1) or not (with value 0), denoted as Mg. Then we compute the IoU between the pseudo-mask Mp generated from feature activation with Mg as follows:\nTP = sum(MgMp),FP = sum((1\u2212Mg)Mp),FN = sum(Mg(1\u2212Mp)), (E) I\u0303oU(Mg,Mp) = TP/(TP+FP+FN). (F)\nPseudo-mask generation. To generate pseudo masks from feature activation, we are motivated by Fig. 3a and treat the highly-activated regions as the final result. To generate pseudo masks with attention scores, we first sum the scores from all the attention heads and follow a similar procedure of treating highly-scored regions as pseudo masks. Concretely, given a feature activation z, generating a pseudo-mask is as straightforward as Mp=(z>t), where t is a threshold between 0 and 1. Although the process is natural, we notice that selecting the thresholds for activation or score significantly affects the quality of pseudo masks. Therefore, we always automatically choose the threshold maximizing the mIoU between the pseudo and ground truth masks to avoid threshold tuning and enable a fair comparison. The algorithm of choosing the best threshold for the activation or attention scores for each image is as below,\nIoU(Mg,z) = max t\n( I\u0303oU(Mg,Mp) ) ,where Mp = z > t and t \u2208 {0.1,0.2,0.3, ...,0.9}. (G)\nFinally, our mIoU in Sec. 6.2 is the mean IoU on all the images.\nFull results with both magnitude and frequency activation. Our comparison of mIoU in Fig. 4 only visualizes the larger mIoU from the activation of magnitude or frequency for clarity. To supplement the comprehension, we display the complete statistics in Fig. D. As illustrated, both ViT-SLLaMA and ViT-B-LLaMA have better pseudo mask quality from feature activation than attention scores and directly support our hypothesis. With the new statics from both activation types, we additionally notice that the neural network layers have varied preferences over magnitudes or frequencies. However, the ViT-LLaMA features still have better fidelity compared to attention scores and features from ViT. As stated in Sec. 6.2, either magnitude or frequency is an intuitive but lossy way to understand feature representation. Thus, future work is needed to further investigate the advantages and properties of individual layers.\nAttn \ud835\udc6d\ud835\udc3f 1 \ud835\udc6d\ud835\udc3f\ud835\udc40 \ud835\udc34 \ud835\udc6d\ud835\udc3f\ud835\udc40 \ud835\udc39 \ud835\udc6d\ud835\udc3f 2 ViT-S Attn \ud835\udc6d\ud835\udc3f 1 \ud835\udc6d\ud835\udc3f\ud835\udc40 \ud835\udc34 \ud835\udc6d\ud835\udc3f\ud835\udc40 \ud835\udc39 \ud835\udc6d\ud835\udc3f 2 ViT-B\nViT-S-LLaMA ViT-B-LLaMA\nMagnitude Frequency\n7.5\n31.0\n49.5\n30.2\n46.9\n29.7\n47.1\n54.1\n46.4\n34.1\n44.0\n0\n10\n20\n30\n40\n50\n60\nm Io\nU (\n% )\n21.7\n32.5 30.5 32.6\n41.8\n33.2\n41.3\n28.4\n41.6 38.1 38.4\n0\n10\n20\n30\n40\n50\n60\nm Io\nU (\n% )\nFigure D: Visualization of mIoU between the ground truth masks and pseudo masks generated from attention scores/feature activation. This figure supplements Fig. 4 by providing the mIoU for both magnitude and frequency activation, where Fig. 4 selects the better one for the clarity of illustration."
        },
        {
            "heading": "A.4 DISCUSSION AND LIMITATIONS OF THE HYPOTHESIS",
            "text": "Perspective of usable information. We supplement the opinions from Xu et al. (2020) that greatly inspire our investigation of the information filtering hypothesis. Xu et al. (2020) proposes that a\nwell-trained neural network layer can be considered as a decipher adding usable information into the features and enabling subsequent modules to better infer the latent information. In our information filtering hypothesis, the incorporated modules are indeed acting as deciphers that enlarge the contribution of usable tokens and benefit downstream predictions.\nLimitations. Though our information filtering hypothesis explains how the performance improves with frozen LLM transformers, we notice that several intriguing phenomena are not yet covered. First, the current hypothesis is unable to analyze the utilities of different layers separately. Second, the hypothesis does not explain how the training dynamics facilitate the visual token features to cooperate with the frozen language transformer, which is interesting future work.\nA.5 IMPLEMENTATION DETAILS IN DERIVING HYPOTHESIS\nMagnitude and frequency activation Our visualization in Sec. 6 and Fig. 3a calculates the feature activation on magnitude or frequency domains to reflect their concentration of target objects. We illustrate the Pytorch-style pseudo-code for our operations in Fig. E. For magnitude, we simply compute the L2 norm of features after centering them. Similarly, the activation of the frequency domain is the norm of angle vectors after the 1D Fourier transformation and centering. Finally, the activation values are normalized to 0 and 1 for visualization and quantitative analysis.\ndef activations(visual_tokens): # visual_tokens: tensor with shape [H, W, C] # magnitude calculation avg_token_feature = visual_tokens.mean(dim=0, keepdim=True) activation = (visual_tokens \u2013 avg_token_feature).norm(dim=-1) mag_min, mag_max = activation.min(), activation.max() mag_activation = (activation \u2013 mag_min) / (mag_max \u2013 mag_min)\n# frequency calculation freq_token = torch.fft.fft(feat).angle() avg_freq_token = freq_token.mean(dim=0, keepdim=True) activation = (freq_token \u2013 avg_freq_token).norm(dim=-1) freq_min, freq_max = activation.min(), activation.max() freq_activation = (activation \u2013 freq_min) / (freq_max \u2013 freq_min) return mag_activation, freq_activation\nFigure E: Pytorch-style pseudo-code for calculating the activation of features on magnitude and frequency domains."
        },
        {
            "heading": "B ADDITIONAL ANALYTICAL RESULTS",
            "text": ""
        },
        {
            "heading": "B.1 ABLATION STUDY ON DESIGN CHOICES",
            "text": "This section provides supplemental results for the analysis on design choices in Sec. 5.1.\nModel Acc\n100 Epochs\nViT-S 75.3 ViT-S-LLaMA 75.8 ViT-S-MLP 75.5 ViT-S-LLaMA-FT 76.8 300 Epochs\nViT-S 80.1 ViT-S-LLaMA 80.7 ViT-S-MLP 80.4 ViT-S-LLaMA-FT 78.9\nTable A: Ablation studies on model capacity and finetuning.\nResults with 100 epochs of training. We conduct the ablation studies mostly on 100 epochs to balance the computation and fidelity of conclusions. In addition to the experiments lasting 300 epochs in Sec. 5.1, we supplement it with experiments lasting 100 epochs, summarized in Table A. According to the numbers, adding a frozen LLM transformer as a visual encoder is still effective, improving the accuracy of baseline ViT. In addition, we highlight that finetuning is beneficial under insufficient training (100 epochs) but it hurts the accuracy when trained longer due to overfitting, which is analyzed in the next paragraph. In conclusion, both of the experiments validate our design choices and the effectiveness of using pre-trained LLM transformer blocks as encoder layers.\nLoss curves for finetuning. We analyze the design choice of finetuning the LLM transformer in Sec. 5.1. Fig. F below shows the loss curves for both training and validation sets during the training process. The training loss is much larger than the validation loss because their functions are different: the training loss is label-smoothing cross-entropy while the validation loss is the regular cross-entropy loss. With the trend in Fig. F, we conclude that jointly finetuning the pretrained LLM transformer might not benefit the performance while making\nthe training process more complicated. Therefore, our experiments adopt the simple solution of freezing the LLM transformer.\nEpochs\nL o\nss V\nal u\nes\nFigure F: Loss curves on the training and validation set for finetuning the LLaMA transformer (ViTS-LLaMA-FT) or not (ViT-S-LLaMA). We use solid lines to denote training losses and dashed lines to denote validation losses. Though finetuning shows advantages in the beginning, it finally hurts the performance due to overfitting (a larger loss on the validation set compared to not finetuning).\nB.2 VARYING LLM TRANSFORMER SCALES\nModel Acc\nViT-S 75.25\n+ OPT-125M 71.63 + OPT-350M 71.56 + OPT-1.3B 75.62 + OPT-2.7B 75.74 + OPT-6.7B 76.29\nTable B: Accuracy improves with larger transformer. This section analyzes the influence of the scales of language transformers with OPT (Zhang et al., 2022). Our experiments incorporate the final transformer layers from OPT-{125M, 350M, 1.3B, 2.7B, 6.7B}, into ViT-S for image classification. Our experiment setting builds upon DeiT (Touvron et al., 2021) and trains for 100 epochs. Additionally, our experiments with small-scale OPT (OPT{125M,350M}) even yield the loss values of nan when trained with the original DeiT learning rate, so we decrease their learning rate by 1/5 for stable training. This reflects the importance of the LLMs\u2019 scales indirectly.\nAs indicated by the results in Table B, the benefits of frozen language transformer grow with an increasing capacity of OPT transformers. The added transformers enhance the performance only with sufficient sizes (1.3B, 2.7B, 6.7B) and hurt the accuracy when the sizes are small (125M, 350M). Therefore, the phenomena of LLM transformers enhancing visual tasks only \u201cemerges\u201d at sufficient scales."
        },
        {
            "heading": "B.3 BREAKDOWN METRICS FOR 3D VQA",
            "text": "We additionally provide the full metrics on SQA3D in Table C, supplementing Table 5b. As shown in the table, adding a frozen transformer improves the performance on the main metric and most of the analytical metrics.\nMethods EM@1 EM@10 What Is How Can Which Others\nScanQA (Azuma et al., 2022) 46.58 85.97 31.64 63.80 46.02 69.53 43.87 45.34 Multi-CLIP (Delitzas et al., 2023) 48.02 - - - - - - - SQA3D (Ma et al., 2023) 47.20 86.82 33.48 66.10 42.37 69.53 43.02 46.40 SQA3D + LLaMA 48.09 89.03 34.27 67.05 48.17 68.34 43.87 45.64\nTable C: Performance of SQA3D dataset and adding language transformers on 3D Question Answering (QA). Adding LLaMA achieves best performance. EM@1 and EM@10 means Top-1 and Top-10 Exact Match (Accuracy) metric. \u201cWhat\u201d, \u201cIs\u201d, \u201cHow\u201d, \u201cCan\u201d, \u201cWhich\u201d, \u201cOthers\u201d are detailed breakdown of question types reported in EM@1."
        },
        {
            "heading": "C DETAILS OF IMPLEMENTATIONS",
            "text": "C.1 IMAGE CLASSIFICATION\nWe follow the DeiT (Touvron et al., 2021) in training the models of ViT-{T,S,B} and ViT-{T,S,B}LLaMA in Sec. 4.1. Each visual token is a 16\u00d716 patch on 224\u00d7224 images. Although DeiT has not released their training scripts for ViT-B (see this link), we adopt the identical procedure for ViT-T and ViT-S for a fair comparison. The most important configurations include a total of 300 epochs, a base learning rate of 5e-4, a cosine annealing learning rate schedule (Loshchilov & Hutter, 2016), and an AdamW optimizer (Kingma & Ba, 2014; Loshchilov & Hutter, 2017). The total time for training lasts 4-6 days on 4xA100 GPUs. The only change we adopt is the warm-up length of 20 epochs, compared to the original warm-up of 10 epochs in DeiT. A longer warm-up stabilizes the training of ViT models and also enables us to slightly outperform the original ViT-T and ViT-S performance in Table 1."
        },
        {
            "heading": "C.2 POINT CLOUD CLASSIFICATION",
            "text": "In this section, we describe the implementation details of the pointcloud classification method presented in Sec. 4.2. For optimization, we use AdamW (Kingma & Ba, 2014; Loshchilov & Hutter, 2017) optimizer and a cosine annealing learning rate schedule (Loshchilov & Hutter, 2016). To map the dimension between the PointBERT and LLaMa transformer tokens, we add two linear layers with a learning rate of 5e-5. The PointBERT backbone has a learning rate of 5e-4, consistent with the original setting in (Yu et al., 2021).. We finetune our model for 300 epochs on both the ScanObjectNN (Uy et al., 2019) and ModelNet40 (Goyal et al., 2021) datasets. The training takes around 6-10 hours on 4\u00d7A200 GPUs"
        },
        {
            "heading": "C.3 ACTION RECOGNITION",
            "text": "We investigate action recognition in Sec. 4.3 and provide more details of implementation here. Our setup strictly follows VideoMAE (Tong et al., 2022), where a ViT model is (1) pretrained by masked auto-encoding (He et al., 2022); then (2) finetuned for additional epochs. As stated in Sec. 4.3, we directly begin from the second step and inherit the parameters of self-attention blocks in ViT from pretrained VideoMAE models. During the training process, VideoMAE trains ViT-S for 40 epochs (5 epochs of warm-up) and ViT-B for 30 epochs (5 epochs of warm-up), where we adopt the same length of training. The optimizer is AdamW (Kingma & Ba, 2014; Loshchilov & Hutter, 2017) with the cosine annealing learning rate schedule (Loshchilov & Hutter, 2016).\nIn Sec. 4.3 and Table 3, our ViT-S and ViT-B performance is lower than the reported numbers in VideoMAE. This is because VideoMAE uses 32\u223c64 GPUs during the finetuning stage and supports a much larger batch size compared to us, though we scale the learning rate according to the batch size as Goyal et al. (2017a). Concretely, VideoMAE adopts the batch size of 384 video clips, while our computational resource only supports a batch size of 24 clips and 12 clips for ViT-S and ViT-B, respectively. However, we control the setup between ViT and ViT-LLaMA for a fair comparison. Finally, the ViT-S-LLaMA and ViT-B-LLaMA experiments take around 3-4 days on 4\u00d7A100 GPUs.\nC.4 MOTION FORECASTING\nModel ADE\u2193(k=1) FDE\u2193(k=1) Paper 1.66 3.67 Ours 1.60 3.60\nTable D: Our implementation of VectorNet is better than their paper. Please note that this table uses the same single-modal setting (k=1), as VectorNet for comparison. We evaluate the effects of frozen LLM transformers in Sec. 4.4 with motion forecasting. Since motion forecasting is a less well-known task in computer vision, we intuitively demonstrate its problem setting and modular architecture in Fig. G, where VectorNet (Gao et al., 2020) and mmTransformer (Liu et al., 2021) encode each lane or agent trajectory into a token embedding, then our LLM blocks process these tokens and feed them into the regression decoder.\nSince VectorNet and mmTransformer have not released their training code, we reproduce their results on our own and achieve better or similar results as reported in the paper. As in Table D and Ta-\nble E, the baseline used in our paper (Table 4) have comparable or even better performance compared to the original performance in the papers, which is critical for a fair and meaningful comparison.\nModel ADE\u2193 FDE\u2193 MR\u2193 Paper 0.71 1.15 10.6 Ours 0.72 1.10 10.7\nTable E: Our implementation of mmTransformer is comparable to the performance in their paper, with large advantages on the main metric of FDE. During the training time, we separately train VectorNet or mmTransformer. VectorNet is a relatively simple architecture, so its training lasts 60 epochs, with cosine annealing learning rate schedule (Loshchilov & Hutter, 2016). We use the AdamW optimizer (Kingma & Ba, 2014; Loshchilov & Hutter, 2017) with a learning rate of 5e-4 and batch size equal to 32 samples. For mmTransformer, we train it with the same learning rate, batch size, and optimizer as VectorNet. The training lasts 32 epochs, where we drop the learning rate by 1/4 on epochs 20, 24, and 28. The training time for both models is around 2 days on one A100 GPU.\nLanes\nAgents VectorNet\nOr mmTransformer\n\ud835\udc6d\ud835\udc3f\ud835\udc40\ud835\udc6d\ud835\udc3f 1 \ud835\udc6d\ud835\udc3f 2\nLane Tokens\nAgent Tokens\nRegression Decoder\nFigure G: Demonstration of typical motion forecasting design and our implementation. Motion forecasting models the trajectories of agents and lanes as polylines. Exiting models (Gao et al., 2020; Liu et al., 2021) using MLPs or transformers to convert the lanes and agent trajectories into token embeddings, then employ a decoder to regress the future trajectories. In our design, we treat either VectorNet (Gao et al., 2020) or mmTransformer (Liu et al., 2021) as general encoder, then insert the frozen LLM blocks to process their embeddings.\nC.5 2D VISION LANGUAGE\nThis section provides more details on implementations and designs to supplement our discussion on 2D vision-language models in Sec. 4.5. Our experiments adopt the widely used METER (Dou et al., 2022) as the baseline and incorporate pretrained LLM transformers after its vision-language fusion module. In Fig. H, we intuitively illustrate the modular design of METER and the special place to insert our frozen LLM transformer and linear layers.\nConventionally, METER follows a two-stage training strategy: (1) pretrains the whole visionlanguage model (VLM) on a large combination of vision-language datasets, including COCO Lin et al. (2014), Conceptual Captions (Sharma et al., 2018), SBU Captions (Ordonez et al., 2011), and Visual Genome (Krishna et al., 2017); (2) finetuning on downstream tasks like visual question answering (VQA) or image-text retrieval. However, the first step of pretraining is computationally extensive, so we adopt the setup in Shi et al. (2023) by skipping the pretraining step and directly training on the target task. Specifically, we initialize the image encoder from CLIP-B/32 (Radford et al., 2021) and text encoder from RoBERTa (Liu et al., 2019), then finetune all the modules jointly expect for the LLM transformer FLM . Because of the initialization from CLIP and RoBERTa, our model is capable of predicting reasonably.\nDuring the training stage, we strictly follow the same hyper-parameters and configurations on VQAv2 (Goyal et al., 2017c) and Flickr30k (Plummer et al., 2015) provided by METER. The most critical detail is that METER assigns different learning rates for each module. For example, the cross-modal fusion module and the decoder have larger learning rates compared to the pretrained image and text encoders. Similarly, our experiments set the learning rates of linear layers (F1L and F2L) 10\u00d7 the learning rate of the image encoder because they are randomly initialized. Finally, each training on VQAv2 and Flickr30k lasts for 10 epochs and around 1 day on 4\u00d7A100 GPUs.\nImage Encoder\nText Encoder What is the woman doing in the photo?\nCrossmodal Fusion \ud835\udc6d\ud835\udc3f\ud835\udc40\ud835\udc6d\ud835\udc3f 1 \ud835\udc6d\ud835\udc3f 2 Decoder\n\ud835\udc6d\ud835\udc38 \ud835\udc6d\ud835\udc37\nFigure H: Intuitive illustration of METER (Dou et al., 2022) framework and how to insert the frozen language transformer (pink FLM) and linear layers (green F1L and F2L) to process the visual tokens after vision-language fusion.\nC.6 3D VISION LANGUAGE\nThis section provides more details of the dataset and training configurations of 3D vision language task. We conduct our experiments on SQA3D dataset (Ma et al., 2023), which contains 33.4k questions in 650 unique ScanNet scenes. In addition to question answering (QA), the benchmark also requires the model to understand its situation (position, orientation, etc.) in the 3D scene as described by text. Hence it is called situated question answering (SQA). We use batch size 32 during our model training, and AdamW as our optimizer. The hidden size for each embedding token is 768. We train all parameters from scratch for 30 epochs, and decrease the learning rate by 10 times at 10, 15, and 20-th epoch. The model is trained on a single A100 GPU."
        },
        {
            "heading": "C.7 DEPTHS OF LLM LAYERS",
            "text": "When varying the depth of transformer blocks in Sec. 5.2 and Fig. 2, we adopt the ablation setup of training for 100 epochs, compared to the full training of 300 epochs. The experiments are based on ViT-S/16 (Dosovitskiy et al., 2021) in DeiT (Touvron et al., 2021) with batch size 1024, which are also used in other ablation experiments in our project. The whole training process involves 20 epochs of warm-up and the remaining 80 epochs adopt the cosine annealing learning rate schedule (Loshchilov & Hutter, 2016). Each experiment takes around 2 days on 4\u00d7A100 GPUs."
        }
    ],
    "year": 2023
}