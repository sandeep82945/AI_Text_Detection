{
    "abstractText": "Given that Transformers are ubiquitous in wide tasks, interpreting their internals is a pivotal issue. Still, their particular components, feed-forward (FF) blocks, have typically been less analyzed despite their substantial parameter amounts. We analyze the input contextualization effects of FF blocks by rendering them in the attention maps as a human-friendly visualization scheme. Our experiments with both maskedand causal-language models reveal that FF networks modify the input contextualization to emphasize specific types of linguistic compositions. In addition, FF and its surrounding components tend to cancel out each other\u2019s effects, suggesting potential redundancy in the processing of the Transformer layer.1",
    "authors": [],
    "id": "SP:755aae2091f20925391b5701c669056a10d63ebc",
    "references": [
        {
            "authors": [
                "Samira Abnar",
                "Willem Zuidema"
            ],
            "title": "Quantifying Attention Flow in Transformers",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL),",
            "year": 2020
        },
        {
            "authors": [
                "Adrian M.P. Bra\u015foveanu",
                "R\u0103zvan Andonie"
            ],
            "title": "Visualizing Transformers for NLP: A Brief Survey",
            "venue": "In 24th International Conference Information Visualisation (IV),",
            "year": 2020
        },
        {
            "authors": [
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language Models are Few-Shot Learners",
            "venue": "In Advances in Neural Information Processing Systems 33 (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Gino Brunner",
                "Yang Liu",
                "Dami\u00e1n Pascual",
                "Oliver Richter",
                "Massimiliano Ciaramita",
                "Roger Wattenhofer"
            ],
            "title": "On Identifiability in Transformers",
            "venue": "In 8th International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Nadia Burkart",
                "Marco F Huber"
            ],
            "title": "A Survey on the Explainability of Supervised Machine Learning",
            "venue": "Artificial Intelligence Research,",
            "year": 2021
        },
        {
            "authors": [
                "Diogo V Carvalho",
                "Eduardo M Pereira",
                "Jaime S Cardoso"
            ],
            "title": "Machine Learning Interpretability: A Survey on",
            "venue": "Methods and Metrics. Electronics,",
            "year": 2019
        },
        {
            "authors": [
                "Kevin Clark",
                "Urvashi Khandelwal",
                "Omer Levy",
                "Christopher D Manning"
            ],
            "title": "What Does BERT Look At? An Analysis of BERT\u2019s Attention",
            "venue": "In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,",
            "year": 2019
        },
        {
            "authors": [
                "Damai Dai",
                "Li Dong",
                "Yaru Hao",
                "Zhifang Sui",
                "Baobao Chang",
                "Furu Wei"
            ],
            "title": "Knowledge Neurons in Pretrained Transformers",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL),",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "venue": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),",
            "year": 2019
        },
        {
            "authors": [
                "Stefan Elfwing",
                "Eiji Uchibe",
                "Kenji Doya"
            ],
            "title": "Sigmoid-weighted linear units for neural network function approximation in reinforcement learning",
            "venue": "Neural Networks,",
            "year": 2018
        },
        {
            "authors": [
                "Mor Geva",
                "Roei Schuster",
                "Jonathan Berant",
                "Omer Levy"
            ],
            "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "Gaussian Error Linear Units (GELUs)",
            "venue": "arXiv preprint,",
            "year": 2016
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameter-Efficient transfer learning for NLP",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Goro Kobayashi",
                "Tatsuki Kuribayashi",
                "Sho Yokoi",
                "Kentaro Inui"
            ],
            "title": "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Goro Kobayashi",
                "Tatsuki Kuribayashi",
                "Sho Yokoi",
                "Kentaro Inui"
            ],
            "title": "Incorporating Residual and Normalization Layers into Analysis of Masked Language Models",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2021
        },
        {
            "authors": [
                "Olga Kovaleva",
                "Alexey Romanov",
                "Anna Rogers",
                "Anna Rumshisky"
            ],
            "title": "Revealing the Dark Secrets of BERT",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP),",
            "year": 2019
        },
        {
            "authors": [
                "Nikolaus Kriegeskorte",
                "Marieke Mur",
                "Peter Bandettini"
            ],
            "title": "Representational similarity analysis connecting the branches of systems neuroscience",
            "venue": "Frontiers in Systems Neuroscience,",
            "year": 2008
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "venue": "arXiv preprint,",
            "year": 1907
        },
        {
            "authors": [
                "S. Shapley Lloyd"
            ],
            "title": "A Value for n-person Games. Contributions to the Theory of Games (AM-28)",
            "venue": "Volume II, pp",
            "year": 1953
        },
        {
            "authors": [
                "Ali Modarressi",
                "Mohsen Fayyaz",
                "Yadollah Yaghoobzadeh",
                "Mohammad Taher Pilehvar"
            ],
            "title": "GlobEnc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),",
            "year": 2022
        },
        {
            "authors": [
                "Ali Modarressi",
                "Mohsen Fayyaz",
                "Ehsan Aghazadeh",
                "Yadollah Yaghoobzadeh",
                "Mohammad Taher Pilehvar"
            ],
            "title": "DecompX: Explaining Transformers Decisions by Propagating Token Decomposition",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL),",
            "year": 2023
        },
        {
            "authors": [
                "Vinod Nair",
                "Geoffrey E. Hinton"
            ],
            "title": "Rectified Linear Units Improve Restricted Boltzmann Machines",
            "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML),",
            "year": 2010
        },
        {
            "authors": [
                "Daisuke Oba",
                "Naoki Yoshinaga",
                "Masashi Toyoda"
            ],
            "title": "Exploratory Model Analysis Using DataDriven Neuron Representations",
            "venue": "In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language Models are Unsupervised Multitask Learners",
            "venue": "Technical report, OpenAI,",
            "year": 2019
        },
        {
            "authors": [
                "Anna Rogers",
                "Olga Kovaleva",
                "Anna Rumshisky"
            ],
            "title": "A Primer in BERTology",
            "venue": "What We Know About How BERT Works. Transactions of the Association for Computational Linguistics (TACL),",
            "year": 2020
        },
        {
            "authors": [
                "Thibault Sellam",
                "Steve Yadlowsky",
                "Ian Tenney",
                "Jason Wei",
                "Naomi Saphra",
                "Alexander D\u2019Amour",
                "Tal Linzen",
                "Jasmijn Bastings",
                "Iulia Raluca Turc",
                "Jacob Eisenstein",
                "Dipanjan Das",
                "Ellie Pavlick"
            ],
            "title": "The MultiBERTs: BERT Reproductions for Robustness Analysis",
            "venue": "In 10th International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts"
            ],
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
            "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2013
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan"
            ],
            "title": "Axiomatic Attribution for Deep Networks",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning (ICML), PMLR,",
            "year": 2017
        },
        {
            "authors": [
                "Ian Tenney",
                "Dipanjan Das",
                "Ellie Pavlick"
            ],
            "title": "BERT Rediscovers the Classical NLP Pipeline",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL),",
            "year": 2019
        },
        {
            "authors": [
                "Ilya O Tolstikhin",
                "Neil Houlsby",
                "Alexander Kolesnikov",
                "Lucas Beyer",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Jessica Yung",
                "Andreas Steiner",
                "Daniel Keysers",
                "Jakob Uszkoreit"
            ],
            "title": "Mlp-mixer: An all-mlp architecture for vision",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar",
                "Aurelien Rodriguez",
                "Armand Joulin",
                "Edouard Grave",
                "Guillaume Lample"
            ],
            "title": "Llama: Open and efficient foundation language models, 2023a. URL https://arxiv.org/abs/2302.13971v1",
            "year": 2023
        },
        {
            "authors": [
                "Sergey Edunov",
                "Thomas Scialom"
            ],
            "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models, 2023b. URL https://arxiv.org/abs/2307.09288v2",
            "year": 2023
        },
        {
            "authors": [
                "Iulia Turc",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Well-Read Students Learn Better: On the Importance of Pre-training",
            "venue": "Compact Models. arXiv preprint,",
            "year": 2019
        },
        {
            "authors": [
                "Jesse Vig"
            ],
            "title": "Visualizing Attention in Transformer-Based Language Representation Models",
            "venue": "arXiv preprint arXiv:1904.02679,",
            "year": 2019
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "Remi Louf",
                "Morgan Funtowicz",
                "Joe Davison",
                "Sam Shleifer",
                "Patrick von Platen",
                "Clara Ma",
                "Yacine Jernite",
                "Julien Plu",
                "Canwen Xu",
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush"
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Ruibin Xiong",
                "Yunchang Yang",
                "Di He",
                "Kai Zheng",
                "Shuxin Zheng",
                "Chen Xing",
                "Huishuai Zhang",
                "Yanyan Lan",
                "Liwei Wang",
                "Tieyan Liu"
            ],
            "title": "On Layer Normalization in the Transformer Architecture",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning (ICML), PMLR,",
            "year": 2020
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin",
                "Todor Mihaylov",
                "Myle Ott",
                "Sam Shleifer",
                "Kurt Shuster",
                "Daniel Simig",
                "Punit Singh Koura",
                "Anjali Sridhar",
                "Tianlu Wang",
                "Luke Zettlemoyer"
            ],
            "title": "OPT: Open Pre-trained Transformer Language Models",
            "venue": "arXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Kobayashi"
            ],
            "title": "2021) rewrite the operation of the attention block (ATB",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "1 INTRODUCTION\nTransformers have been ubiquitous in a wide range of natural language processing tasks (Vaswani et al., 2017; Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020); interpreting their inner workings is challenging and important to rationalize their outputs due to their black-box nature (Carvalho et al., 2019; Rogers et al., 2020; Bras\u0327oveanu & Andonie, 2020). In particular, tracking their component-by-component internal processing can provide rich information about their intermediate processing, given the stacked architecture of Transformer. With this motivation, existing studies have typically analyzed the role of each Transformer component or its specific subpart (Kobayashi et al., 2020; 2021; Modarressi et al., 2022). Taking the analyses of feed-forward (FF) networks as examples, Oba et al. (2021) explored the relationship between FF\u2019s neuron activation and specific phrases, and Geva et al. (2021) and Dai et al. (2022) examined the knowledge stored in FF\u2019s parameters through viewing FF as key-value memories.\nAmong the various approaches to interpreting the model\u2019s internal processing, rendering n\u00d7n attention maps through a target subpart (e.g., attention mechanism), where the (i, j) cell represents how strongly a particular input xj \u2208 {x1, . . . ,xn} contributed to a particular output yi, has gained certain popularity as a human-friendly interface (Bras\u0327oveanu & Andonie, 2020; Vig, 2019). It allows us to understand how each input word internally interacts with surrounding words (input contextualization). Such maps are typically estimated with attention weights in each attention head (Clark et al., 2019; Kovaleva et al., 2019). Needless to say, this has an obvious limitation of only reflecting a specific process (Query-Key attention computation) of the Transformer layer; then, existing studies refined the attention map computation to reflect other components\u2019 processing, such as residual and normalization layers (Brunner et al., 2020; Abnar & Zuidema, 2020; Kobayashi et al., 2020; 2021; Modarressi et al., 2022), enabling us to look into other components through the rendered attention map. Notably, visualizing and refining attention maps have several advantages: (i) Attention mechanisms are spread out in the entire Transformer architecture; thus, if one can view their surrounding component\u2019s processing through this \u201csemi-transparent window,\u201d these pictures can complete the Transformer\u2019s component-by-component internal processing, (ii) token-to-token relationships are relatively more familiar to humans than directly observing high-dimensional, continuous intermediate representations/parameters, and input attribution is of major interest in explaining the model prediction, and (iii) it has been reported that such attention map refinement approaches tend to estimate better attributions than other, e.g., gradient-based methods (Modarressi et al., 2022; 2023).\nStill, in this line of analyses towards input contextualization via attention map, FF networks have been overlooked from the analysis scope, although there are several motivations to consider FFs.\n1We will make our analysis tool and experimental codes publicly available upon acceptance.\nUnder review as a conference paper at ICLR 2024\nFor example, FFs account for about two-thirds of the layer parameters in typical Transformer-based models, such as BERT and GPT series; this implies that FF has the expressive power to dominate the model\u2019s inner workings, and understanding such a large component\u2019s processing is also related to pruning large models. In addition, there is a growing general interest in FFs with the rise of FFfocused methods such as adapters (Houlsby et al., 2019) and MLP-Mixer (Tolstikhin et al., 2021), although analyzing these specially designed models is beyond this paper\u2019s focus. Furthermore, it has been reported that FFs indeed perform some linguistic operations, while existing studies have not explicitly focused on input contextualization(Geva et al., 2021; Dai et al., 2022).\nIn this study, we analyze the FF blocks in the Transformer layer, i.e., FF networks and their surrounding residual and normalization layers, with respect to their impact on input contextualization through the lens of attention map. Notably, although the FFs are applied to each input representation independently, their transformation can inherently affect the input contextualization (\u00a7 3), and our experiments show that this indeed occurs (\u00a7 5.1). Technically, we propose a method to compute attention maps reflecting the FF blocks\u2019 processing by extending a norm-based analysis (Kobayashi et al., 2021), which has several advantages: the impact of input (\u2225x\u2225) is considered unlike the vanilla gradient, and that only the forward computation is required. Although the original norm-based approach can not be simply applied to the non-linear part in the FF, this study handles this limitation by partially applying an integrated gradient (Sundararajan et al., 2017) and enables us to track the input contextualization from the information geometric perspectives.\nOur experiments with both masked- and causal-language models (LMs) disclosed the contextualization effects of the FF blocks. Specifically, we first reveal that FF and layer normalization in specific layers tend to largely control contextualization. We also observe typical FF effect patterns, independent of the LM types, such as amplifying a specific type of lexical composition, e.g., subwords-to-word and words-to-multi-word-expression constructions (\u00a7 5.2). Furthermore, we also first observe that the FF\u2019s effects are weakened by surrounding residual and normalization layers (\u00a7 6), suggesting redundancy in the Transformer\u2019s internal processing.\n2 BACKGROUND\nNotation: Boldface letters such as x denote row vectors.\nTransformer layer: The Transformer architecture (Vaswani et al., 2017) consists of a series of layers, each of which updates each token representation xi \u2208 Rd in input sequence X := [x\u22a41 , . . . ,x \u22a4 n ]\n\u22a4 \u2208 Rn\u00d7d to a new representation yi \u2208 Rd. That is, the information of context X is added to xi, and xi is updated to yi. We call this process contextualization of xi. Each layer is composed of four parts: multi-head attention (ATTN), feed-forward network (FF), residual connection (RES), and layer normalization (LN) (see Fig. 1). Note that we use the Post-LN architecture (Vaswani et al., 2017) for the following explanations in this paper, but the described methods can simply be extended to the Pre-LN variant (Xiong et al., 2020). A single layer can be written as a composite function:\nyi=Layer(xi;X)=(FFB \u25e6ATB)(xi;X)=(LN2 \u25e6 RES2 \u25e6 FF\u25e6 LN1 \u25e6 RES1 \u25e6ATTN)(xi;X). We call (LN1 \u25e6RES1 \u25e6ATTN)(\u00b7) attention block (ATB), and (LN2 \u25e6RES2 \u25e6FF)(\u00b7) feed-forward block (FFB). Each component updates the representation as follows:\nATTN(xi;X) = ( \u2211\nh \u2211 j \u03b1 h i,jxjWV + bV )WO + bO \u2208 Rd (1)\nwhere \u03b1hi,j := (xiWQ + bQ)(xiWK + bK) \u22a4 \u2208 R (2)\nFF(zi) = g(ziW1 + b1)W2 + b2 \u2208 Rd (3) (RES \u25e6 f)(zi) = f(zi) + zi \u2208 Rd (4)\nLN(zi) = zi \u2212m(zi)\ns(zi) \u2299 \u03b3 + \u03b2 \u2208 Rd, (5)\nwhere W ,\u03b3 denote weight parameters, and b,\u03b2 denote bias parameters corresponding to query (Q), key (K), value (V), etc. f : Rd \u2192 Rd, g : Rd\u2032 \u2192 Rd\u2032 , m : Rd \u2192 R, and s : Rd \u2192 R denote an arbitrary vector-valued function, activation function in FF (e.g., GELU, Hendrycks & Gimpel, 2016), element-wise mean, and element-wise standard deviation, respectively. h denotes the head number in the multi-head attention. See the original paper (Vaswani et al., 2017) for more details.\nUnder review as a conference paper at ICLR 2024\n(Ours)\n(Ours)\n(Ours)\n!!!\"!#!$!%\nLayer Normalization (LN1)\nAttention (ATTN)\nLayer Normalization (LN2)\nFeed-Forward Network (FF)\nResidual Connection (RES1)\nResidual Connection (RES2)\nToken-to-token attention map\n!!!\"!#!$!%\nMulti-head Attention (ATTN)\n\"!\"\"\"#\"$\"%\nAttention weights\nInput vectors\nOutput vectors\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nATB\nATBFF\nATBFFRES\nATBFFRESLN\n14\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is diff rentiable in practice, nd covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nATB\nATBFF\nATBFFRES\nATBFFRES N\n14\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nATB\nATBFF\nATBFFRES\nATBFFRESLN\n14\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nA B\nATBFF\nATBFFRES\nATBFFRESLN\n14\n(Kobayashi+\u201921)\nPost-LN Transformer layer\nAttention Block (ATB)\nFeed-Forward Block (FFB)\n!!!\"!#!$!%\nLayer Normalization (LN1)\nResidual Connection (RES1)\nResidual Connection (RES2)\nAttention (ATTN) Multi-head Attention (ATTN)\n\"!\"\"\"#\"$\"%\nAttention weights\nInput vectors\nOutput vectors\nU der review as a conference paper at ICLR 2024\nThe above decomposition is applicable to a y activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nATB\nATBFF\nATBFFRES\nATBFFRESLN\n14\n(Kobayashi+\u201921)\nP -LN Transformer layer\nAttention Block (ATB)\nFeed-Forward Block (FFB)\n(Ours)\n(Ours)\n(Ours)\nUnder review as a conference paper at ICLR 2024 Decomposition of broadcasted GELU In a practical neural network implementation, the GELU layer is passed a vector x1 + \u00b7 \u00b7 \u00b7 + xn 2 Rd 0 instead of a scalar x1 + \u00b7 \u00b7 \u00b7 + xn 2 R and the GELU function is applied (broadcasted) element-wise. Let GELU : Rd0 ! Rd0 be defined as the function that broadcasts the element-level activation GELU: R ! R. The contribution of each input vector x0j = [x 0 j [1], . . . , x 0 j [d 0]] to the output vector GELU(x01 + \u00b7 \u00b7 \u00b7 + x0n) is as follows: GELU(x01 + \u00b7 \u00b7 \u00b7 + x0n) = 2 64 GELU(x01[1] + \u00b7 \u00b7 \u00b7 + x0n[1]) ...\nGELU(x01[d 0] + \u00b7 \u00b7 \u00b7 + x0n[d0])\n3 75 >\n(18)\n= 2 664 Pn j=1 IGj(x 0 1[1], . . . , x 0 n[1]; G\u0302ELU,0)\n...Pn j=1 IGj(x 0 1[d 0], . . . , x0n[d 0]; G\u0302ELU,0)\n3 775 >\n(19)\n= nX\nj=1\n2 664 IGj(x 0 1[1], . . . , x 0 n[1]; G\u0302ELU,0)\n... IGj(x1[d 0], . . . , xn[d0]; G\u0302ELU,0)\n3 775 > . (20)\nThe above decomposition is applicable to a y activation function g that passes through the origin and is differentiabl in practice, and cover al activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nATB\nATBLN\nATBLNFF\nATBLNRES\n15\nUnder review as a conference paper at ICLR 2024 Decomposition of broadcasted GELU In a practical neural network implementation, the GELU layer is passed a vector x1 + \u00b7 \u00b7 \u00b7 + xn 2 Rd 0 instead of a scalar x1 + \u00b7 \u00b7 \u00b7 + xn 2 R and the GELU function is applied (broadcasted) element-wise. Let GELU : Rd0 ! Rd0 be defined as the function that broadcasts the element-level activation GELU: R ! R. The contribution of each input vector x0j = [x 0 j [1], . . . , x 0 j [d 0]] to the output vector GELU(x01 + \u00b7 \u00b7 \u00b7 + x0n) is as follows: GELU(x01 + \u00b7 \u00b7 \u00b7 + x0n) = 2 64 GELU(x01[1] + \u00b7 \u00b7 \u00b7 + x0n[1]) ... GELU(x01[d 0] + \u00b7 \u00b7 \u00b7 + x0n[d0]) 3 75 > (18)\n= 2 664 Pn j=1 IGj(x 0 1[1], . . . , x 0 n[1]; G\u0302ELU,0)\n...Pn j=1 IGj(x 0 1[d 0], . . . , x0n[d 0]; GELU,0)\n3 775 >\n(19)\n= nX\nj=1\n2 664 IGj(x 0 1[1], . . . , x 0 n[1]; G\u0302ELU,0) ..\nIGj(x1[d 0], . . . , xn[d0]; GELU,0)\n3 775 > . (20)\nThe above decom o ition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al , 2018).\nATB\nATBLN\nATBLNFF\nATBLNRES\n15\nUnder review as a conference paper at ICLR 2024 Decomposition of broadcasted GELU In a practical neural network implementation, the GELU layer is passed a vector x1 + \u00b7 \u00b7 \u00b7 + xn 2 Rd 0 instead of a scalar x1 + \u00b7 \u00b7 \u00b7 + xn 2 R and the GELU function is applied (broadcasted) lement-wise. Let GELU : Rd0 ! Rd0 be defined as the function that broadcasts the element-level activation GELU: R ! R. The contribution of each input vector x0j = [x 0 j [1], . . . , x 0 j [d 0]] to the output vector GELU(x01 + \u00b7 \u00b7 \u00b7 + x0n) is as follows: GELU(x01 + \u00b7 \u00b7 \u00b7 + x0n) = 2 64 GELU(x01[1] + \u00b7 \u00b7 \u00b7 + x0n[1]) ... GELU(x01[d 0] + \u00b7 \u00b7 \u00b7 + x0n[d0]) 3 75 > (18)\n= 2 664 Pn j=1 IGj(x 0 1[1], . . . , x 0 n[1]; G\u0302ELU,0)\n...Pn j=1 IGj(x 0 1[d 0], . . . , x0n[d 0]; G\u0302ELU,0)\n3 775 >\n(19)\n= nX\nj=1\n2 664 IGj(x 0 1[1], . . . , x 0 n[1]; G\u0302ELU,0)\n... IGj(x1[d 0], . . . , xn[d0]; G\u0302ELU,0)\n3 775 > . (20)\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions curre tly employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nATBLNFFRES\nTable 2: Word pairs for which FF amplified the interaction the most in BERT.\nLayer Top amplified token-pairs\n1 (##our, det), (##iques, ##mun), (##cend, trans), (outer, space), (##ili, res),(##ific, honor), (##nate, ##imi), (opera, soap), (deco, art), (##night, week)\n2 (##roy, con), (guard, national), (america, latin), (easily, could), (##oy, f),(marshall, islands), (##ite, rec), (channel, english), (finance, finance), (##ert, rev)\n3 (\u2019, t), (##mel, ##o s), (hut, ##chin), (water, ##man), (paso, ##k),(avoid, ##ant), (toys, ##hop), (competitive, ##ness), (##la, ##p), (##tree, ##t)\n4 (##l, ds), (##l, ##tera), (##cz, ##ave), (##r, ##lea), (##e, beth),(##ent, ##ili), (##er, ##burn), (##et, mn), (##ve, wai), (##rana, ke)\n5 (##ent, ##ili), (##ious, ##car), (##on, ##ath), (##l, ds), (##r, ##ense),(##ence, ##ili), (res, ##ili), (##rative, ##jo), (##ci, ##pres), (##able, ##vor)\n6 (on, clay), (with, charged), (##ci, ##pres), (b , consid rs), (on, behalf),(fleming, colin), (##r, ##lea), (-, clock), (##ur, nam), (by, denoted)\n7 (##ons, ##mel), (vi, saint), (vi, st), (##l, ##ife), (##ti, jon),(##i, wii), (##en, ##chel), (##son, bis), (vessels, among), (##her, ##rn)\n8 (##ano, ##lz), (bo, ##lz), (nathan, or), (arabia, against), (sant, ##ini),(previous, unlike), (saudi, against), (##ia, ##uring), (he, gave), (tnt, equivalent)\n9 (no, situation), (##ek, czech), (according, situation), (decided, year), (v, classification),(eventually, year), (but, difficulty), (##ference, track), (she, teacher), (might, concerned)\n10 (jan, ##nen), (hee, ##e), (primary, stellar), (crete, crete), (nuclear, 1991),(inspector, police), (##tu, ##nen), (quote, quote), (f, stellar), (v, stellar)\n11 (tiny, tiny), (##water, ##water), (hem, hem), (suddenly, suddenly), (fine, singer),(henley, henley), (highway, highway), (moving, moving), (dug, dug), (farmers, agricultural)\n12 (luis, luis), (tong, ), (board, judicial), (##i, index), (\u2014,\u2014), (four, fifty), (cloud, thunder), (located, transmitter), (##ota, ##T), (##ss, analysis)\n15\nLayer Normalization (LN2)\nFeed-Forward Network\n(FF)\n! % ! $ ! # ! \" ! !\nFigure 1: Overview of the Transformer layer for Post-LN and Pre-LN architectures. The bl ck diagram to the left of each represents the analysis scope adopted in this study. The right part of this figure illustrates the component-by-component changes of the attention maps.\nAttention map: Our interest lies in how strongly each input xj contributes to the computation of a particular output yi. This is typically visualized as an n \u00d7 n attention map, where the (i, j) cell represents how strongly xj contributed to compute yi. A typical approximation of such a map is facilitated with the attention weights (\u03b1hi,j or \u2211 h \u03b1 h i,j ; henceforth denoted \u03b1i,j) (Clark et al., 2019; Kovaleva et al., 2019); however, this only reflects a specific process (QK attention computation) of the Transformer layer (Brunner et al., 2020; Kobayashi et al., 2020; Abnar & Zuidema, 2020).\nLooking into Transformer layer through attention map: Nevertheless, an attention map is not only for visualizing the attention weights; existing studies (Kobayashi et al., 2020; 2021) have analyzed other components through the lens of refined attention map. Specifically, Kobayashi et al. (2020) pointed out that the processing in the multi-head QKV attention mechanism can be written as a sum of transformed vectors:\nyATTNi = \u2211 j F ATTN i (xj ;X) + b ATTN, (6)\nwhere FATTNi (xj ;X) := \u03b1 h i,j(xjW h V )W h O, b ATTN := bV WO + bO. (7)\nHere, input representation xi is updated to yATTNi by aggregating transformed inputs FATTNi (xj ;X). Then, \u2225FATTNi (xj ;X)\u2225 instead of \u03b1i,j alone is regarded as refined attention weight with the simple intuition that larger input contributes to output more in summation. This refined weight reflects not only the original attention weight \u03b1i,j , but also the effect of surrounding processing involved in F , such as value vector transformation. We call this strategy of decomposing the process into the sum of transformed vectors and measuring their norms norm-based analysis, and the obtained n\u00d7 n attribution matrix is generally called attention map in this study. This norm-based analysis has been further generalized to a broader component of the Transformer layer. Specifically, Kobayashi et al. (2021) showed that the operation of attention block (ATB) could also be rewritten into the sum of transformed inputs and a bias term:\nyATBi = ATB(xi;X) = \u2211 j F ATB i (xj ;X) + b ATB. (8)\nThen, the norm \u2225FATBi (xj ;X)\u2225 was analyzed to quantify how much the input xj impacted the computation of the output yATBi through the ATBs. See Appendix A for details of F ATB i and b ATB.\n3 PROPOSAL: ANALYZING FFBS THROUGH REFINED ATTENTION MAP\nThe transformer layer is not only an ATB; it consists of an ATB and FFB (Figs. 1 and 2). Thus, this study broadens the analysis scope to include the entire FF block consisting of feed-\nUnder review as a conference paper at ICLR 2024\nforward, residual, and normalization layers, in addition to the ATB. Note that FFBs do not involve token-wise interaction among the inputs X; thus, the layer output yLayeri can be written as transformed yATBi . Then, our aim is to decompose the entire layer processing as follows:\nyLayeri = FFB (\u2211 j F ATB i (xj ;X) + b ATB ) (9)\n= \u2211\nj F Layer i (xj ;X) + b Layer. (10)\nHere, the norm \u2225F Layeri (xj ;X)\u2225 can render the updated attention map reflecting the processing in the entire layer. This attention map computation can also be performed component-by-component; we can track each FFB component\u2019s effect.\nWhy FFBs can control contextualization patterns: While FFBs are applied independently to each input representation, FFB\u2019s each input already contains mixed information from multiple token representations due to the ATB\u2019s process, and the FFBs can freely modify these weights through a nonlinear transformation (see Fig. 2).\nDifficulty to incorporate FF: While FFBs have the potential to significantly impact contextualization patterns, incorporating the FF component into norm-based analysis is challenging due to its nonlinear activation function, g (Eq. 3), which cannot be decomposed additively by the distributive law:\ng (\u2211 j F (xj) ) \u0338=\u2211j(g \u25e6 F )(xj). (11)\nThis inequality matters in the transformation from Eq. 9 to Eq. 10. That is, simply measuring the norm \u2225(g \u25e6F )(xj)\u2225 is not mathematically valid. Indeed, the FF component has been excluded from previous norm-based analyses. Note that the other components in FFBs, residual connection and layer normalization, can be analyzed in the same way proposed in Kobayashi et al. (2021).\nIntegrated Gradients (IG): The IG (Sundararajan et al., 2017) is a technique for interpreting deep learning models by using integral and gradient calculations. It measures the contribution of each input feature to the output of a neural model. Given a function f : Rn \u2192 R and certain input x\u2032 = (x\u20321, . . . , x \u2032 n) \u2208 Rn, IG calculates the contribution (attribution) score (\u2208 R) of each input feature x\u2032i \u2208 R to the output f(x\u2032) \u2208 R:\nf(x\u2032) = n\u2211\nj=1\nIGj(x \u2032; f, b) + f(b), IGi(x\u2032; f, b) := (x\u2032i \u2212 bi)\n\u222b 1\n\u03b1=0\n\u2202f\n\u2202xi \u2223\u2223\u2223\u2223 x= b+\u03b1(x\u2032 \u2212 b) d\u03b1. (12)\nHere, b \u2208 Rn denotes a baseline vector used to estimate the contribution. At least in this study, it is set to a zero vector, which makes zero output when given into the activation function (g(0) = 0), to satisfy desirable property for the decomposition (see Appendix B.2 for details).\nExpansion to FF: We explain how to use IG for the decomposition of FF output. As aforementioned, the problem lies in the nonlinear part, thus we focus on the decomposition around the nonlinear activation. Let us define F Pre gi (xj) := F ATB i (xj ;X)W1 \u2208 Rd \u2032 as the decomposed vector prior to the nonlinear activation g : Rd\u2032 \u2192 Rd\u2032 . The activated token representation y\u2032 \u2208 Rd\u2032 is written as follows:\ny\u2032i = g\n( \u2211n\nj=1 F Pre g i (xj)\n) = \u2211n j=1 hj ( F Pre gi (x1), . . . ,F Pre g i (xn); g\u0303,0 ) , (13)\nhj ( F Pre gi (x1), . . . ,F Pre g i (xn); g\u0303,0 ) =   IGj ( F Pre gi (x1)[1], . . . ,F Pre g i (xn)[1]; g\u0303,0 )\n... IGj ( F Pre gi (x1)[d \u2032], . . . ,F Pre gi (xn)[d \u2032]; g\u0303,0 )\n  \u22a4 , (14)\nUnder review as a conference paper at ICLR 2024\nwhere the transformation g\u0303 : Rn \u2192 R is defined as g\u0303(x1, . . . , xn) := g(x1 + \u00b7 \u00b7 \u00b7 + xn), which adds up the inputs into a single scalar and then applies the element-level activation g : R \u2192 R. The function hj : Rn\u00d7d\n\u2032 \u2192 Rd\u2032 yields how strongly a particular input F Pre gi (xj) contributed to the output y\u2032i. Each element hj [k] indicates the contribution of the k-th element of input F Pre g i (xj)[k] to the k-th element of output y\u2032i[k]. Note that contribution can be calculated element-level because the activation g is applied independently to each element.\nNotably, the sum of contributions across inputs matches the output (Eq. 13), achieved by the desirable property of IG\u2014completeness Sundararajan et al. (2017). This should be satisfied in the norm-based analysis and ensures that the output vector and the sum of decomposed vectors are the same. The norm \u2225hj(F Pre gi (x1), . . . ,F Pre gi (xn); g\u0303,0)\u2225 is interpreted as the contribution of input xj to the output y\u2032i of the activation in FF.\nExpansion to entire layer: Expanding on this, the contribution of a layer input xj to the i-th FF output is exactly calculated as \u2225hj(F Pre gi (x1), . . . ,F Pre gi (xn); g\u0303,0)W2\u2225. Then, combined with the exact decomposition of ATTN, RES, and LN shown by Kobayashi et al. (2020; 2021), the entire Transformer layer can be written as the sum of vector-valued functions with each input vector xj \u2208 X as an argument as written in Eq. 10. This allows us to render the updated attention map reflecting the processing in the entire layer by measuring the norm of each decomposed vector.\n4 GENERAL EXPERIMENTAL SETTINGS\nEstimating refined attention map: To elucidate the input contextualization effect of each component in FFBs, we computed attention maps by each of the following four scopes (Fig. 1):\n\u2022 ATB (Kobayashi et al., 2021): Analyzing the attention block (i.e., ATTN, RES1, and LN1) using vector norms as introduced in Eq. 8. \u2022 ATBFF (proposed): Analyzing components up to FF using vector norms and IG. \u2022 ATBFFRES (proposed): Analyzing components up to RES2 using vector norms and IG. \u2022 ATBFFRESLN (proposed): Analyzing the whole layer (all components) using vector\nnorms and IG. We will compare the attention maps from different scopes to separately analyze the contextualization effect. Note that if the model adopts the Pre-LN architecture, the scopes will be expanded from ATB to ATBLN, ATBLNFF, and ATBLNFFRES (see the Pre-LN part in Fig. 1).\nModels: We analyzed 11 variants of the masked language models: six BERTs (uncased) with different sizes (large, base, medium, small, mini, and tiny) (Devlin et al., 2019; Turc et al., 2019), three BERTs-base with different seeds (Sellam et al., 2022), plus two RoBERTas with different sizes (large and base) (Liu et al., 2019). We also analyzed two causal language models: GPT-2 with 117M parameters and OPT with 125M parameters. Note that the masked language models adopt the Post-LN architecture, and the causal language models adopts the Pre-LN architecture.\nData: We used two datasets with different domains: Wikipedia excerpts (992 sequences) (Clark et al., 2019)2 and the Stanford Sentiment Treebank v2 dataset (872 sequences from validation set) (SST-2, Socher et al., 2013). The input was segmented by each model\u2019s pre-trained tokenizers; analysis was conducted at the subword level.3\n5 EXPERIMENT 1: CONTEXTUALIZATION CHANGE\nDoes each component in FFBs indeed modify the token-to-token contextualization? We analyze the contextualization change through each component in FFBs.\n5.1 MACRO CONTEXTUALIZATION CHANGE\nCalculating contextualization change: Given two analysis scopes (e.g., before and after FF; ATB \u2194 ATBFF), their contextualization pattern change was quantified following some procedures of\n2https://github.com/clarkkev/attention-analysis 3For masked language models, each sequence was fed into the models with masking 12% tokens as done in\nthe case of the BERT training procedure.\nUnder review as a conference paper at ICLR 2024\nrepresentational similarity analysis (Kriegeskorte et al., 2008). Formally, given an input sequence of length n, two different attention maps from the two scopes are obtained. Then, each attention map (Rn\u00d7n) was flattened into a vector (Rn2 ), and the Spearman\u2019s rank correlation coefficient \u03c1 between the two vectors were calculated. We report the average contextualization change 1\u2212 \u03c1 across input sequences. We will report the results of the BERT-base and GPT-2 on the Wikipedia dataset in this section; other models/datasets also yielded similar results (see Appendix C.1).\nThe contextualization changes through each component in FFBs are shown in Fig. 3. A higher score indicates that the targeted component more drastically updates the contextualization patterns. Note that we explicitly distinguish the pre- and post-layer normalization (PRELN and POSTLN) in this section, and the component order in Fig. 3 is the same as the corresponding layer architecture.\nWe generally observed that each component did modify the input contextualization, especially in particular layers. For example, in the BERT-base, the FF in 3rd and 9th\u201311th layers and normalization in 10th\u201311th layers incurred relatively large contextualization change. Comparing the BERTbase and GPT-2, the depth of such active layers and the account of RES differed, but by-layer average of contextualization change by FF and LN was similar across these LMs: 0.21 and 0.15 for FF in BERT and GPT-2, respectively, and 0.27 and 0.38 for LN in BERT and GPT-2, respectively. Note that Figures 9 to 13 in Appendix C show the results for other model variants, yielding somewhat consistent trends that FFs and LNs in particular layers especially incur contextualization changes.\n5.2 LINGUISTIC PATTERNS IN FF\u2019S CONTEXTUALIZATION EFFECTS\nThe FF network is a completely new scope in the norm-based analysis, while residual and normalization layers have been analyzed in Kobayashi et al. (2021); Modarressi et al. (2022). Thus, we further investigate how FF modified contextualization, using BERT-base and GPT-2 on the Wikipedia dataset as a case study.\nMicro contextualization change: We compared the attention maps before and after FF. Specifically, we subtract a pre-FF attention map from a post-FF map (Fig. 4); we call the resulting diff-map FFamp matrix and the values in each cell FF-amp score.4 A larger FF-amp score of the (i, j) cell\n4Before the subtraction, the two maps were normalized so that the sum of the values of each column was 1; this normalization facilitates the inter-method comparison.\nUnder review as a conference paper at ICLR 2024\nFF-amp matrix\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nATB\nATBFF\nATBFFRES\nATBFFRESLN\n14\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nATB\nATBFF\nATBFFRES\nATBFFRESLN\n14\nFigure 4: FF-amp matrix is computed by subtracting the attention map before FF (ATB) from that after FF (ATBFF).\npresents that the contribution of input xj to output yi is more amplified by FF. Our question is which kind of token pairs gain a higher FF-amp score.5\nFFs amplify particular linguistic compositions: Based on the preliminary observations of high FF-amp token pairs, we set 7 linguistic categories typically amplified by FFBs. This includes, for example, token pairs consisting of the same words but different positions in the input. Table 1 shows the top token pairs w.r.t. the amp score in\nsome layers and their token pair category. Fig. 5 summarizes the ratio of each category in the top 50 token pairs with the highest FF-amp score.6 Compared to the categories for randomly sampled token pairs (FR; the second rightmost bar in Fig. 5) and adjacent token pairs (AR; the rightmost bar in Fig. 5), the amplified pairs in each layer have unique characteristics; for example, in the former layers, subword pairs consisting the same token are highly contextualized by FFs. Note that this phenomenon of processing somewhat shallow morphological information in the former layers can be consistent with the view of BERT as a pipeline perspective, with gradually more advanced processing from the former to the latter layers Tenney et al. (2019).\nSimple word co-occurrence does not explain the FF\u2019s amplification: Do FFs simply amplify the interactions between frequently co-occurring words? We additionally investigated the relationship between the FF\u2019s amplification and word co-occurrence. Specifically, we calculated PMI for each subword pair on the Wikipedia dump and then calculated the Spearman\u2019s rank correlation coefficient between FF-amp scores of each pair (wi, wj) from BERT-base and PMI values.7 We observed that the correlation scores were fairly low in any layer (coefficient values were 0.06\u20130.14). Thus, we found that the FF does not simply modify the contextualization based on the word co-occurrence.\n5We aggregated the average FF-amp score for each subword type pair (wi, wj). Pairs consisting of the same position\u2019s token (wi, wi) and pairs occurring only once in the dataset were excluded.\n6One of the authors of this paper has conducted the annotation. 7We defined three types of co-occurrences in calculating PMI: the simultaneous occurrence of two subwords (i) in an article, (ii) in a sentence, and (iii) in a chunk of 512 tokens. No correlation was observed for either type of co-occurrence. Pairs including special tokens and pairs consisting of the same subword were excluded.\nUnder review as a conference paper at ICLR 2024\nTo sum up, these findings indicate that FF did amplify the contextualization aligned with particular types of linguistic compositions in various granularity (i.e., word and NE phrase levels).\n6 EXPERIMENT 2: DYNAMICS OF CONTEXTUALIZATION CHANGE\nWe analyze the relationship between the contextualization performed by FF and other components, such as RES, given the previous observation that contextualization changes in a particular component are sometimes overwritten by other components (Kobayashi et al., 2021). We will report the results of the BERT-base and GPT-2 on the Wikipedia dataset in this section; other models/datasets also yielded similar results (see Appendix D).\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such a ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, nd covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinto , 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and cov rs all a tiva ion functions currently e ployed in Tr sformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is appl cable to a y activation fu ction g that passes through the origin and is differentiable in practice, a d covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation fu ctions currentl employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference pap r at ICLR 2024\nThe above decomposition is pplicable to any activation fun i n g that passes through he origin and is differentiable in practice, a d covers all activation fu cti ns currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwi g et al., 2018).\nLNATTN\nLNATTNRES\n16\nUnder review as a conference paper at ICLR 2024\nThe above decompos tion is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practic , nd covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinto , 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\n16\nUnder review as a conferenc paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is diffe entiable i pra tic , and covers all activation functio s currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\n16\nUnder review as a conference paper at ICLR 2024\nThe above d co sition is applicable to any activation fu ctio g that passes through the origin and is differ nti b e in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activati n function g that passes through the origin and is differentiable in practice, and cov rs al activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference pap r at ICLR 2024\nThe above decompos tion is pplicable t any ac vat n fun tion g that p sses hrough the origin and is differentiable in practice, a d cov rs all activ tion fu ction currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwi g et al., 2018).\nLNATTN\nLNATTNRES\n16\nFig. 6 shows the contextualization change scores (1 \u2212 \u03c1 described in \u00a7 5.1) by FF and subsequent components: FF (+FF), FF and RES (FF+RES), and FF, RES and LN (FF+RES+LN). Note that we analyzed the next-layer\u2019s LN1 in the case of GPT2 (Pre-LN architecture) to analyze the BERTs and GPTs from the same perspective\u2014whether the other components overwrite the contextualization performed in the FF network. If a score is zero, the resulting contextualization map is the same as that computed before FF (after ATBs in BERT or after LN2 in GPT-2). The notable point is that through the FF and subsequent RES and LN, the score once becomes large but finally converges to be small; that is, the contextualization by FFs tends to be canceled by the following components. We look into this cancellation phenomenon with a specific focus on each component.\n6.1 FF AND RES\nThe residual connection bypasses the feed-forward layer as Eq. 4. Here, the interest lies in how dominant the bypassed representation x\u2032i is relative to FF(x \u2032 i). For example, if the representation x \u2032 i has a much larger L2 norm than FF(x\u2032i), the final output through the RES2 \u25e6 FF will be similar to the original input x\u2032i; that is, the contextualization change performed in the FF would be diminished.\nRES2 adds a large vector: We observe that the vectors bypassed via RES2 are more than twice as large as output vectors from FF in the L2 norm in most layers (Fig. 7). That is, the representation (contextualization) updated by the FF tends to be overwritten/canceled by the original one. This\nUnder review as a conference paper at ICLR 2024\nobservation is consistent with that of RES1 (Kobayashi et al., 2021). Notably, this cancellation is weakened in the 10th\u201311th layers in BERT and former layers in GPT2s, where FFs\u2019 contextualization was relatively large (Figs. 3a and 3e).\n6.2 FF AND LN\nWe also analyzed the relationship between the contextualization performed in FF and LN. Note that the layer normalization first normalizes the input representation, then multiplies a weight vector \u03b3 element-wise, and adds a bias vector \u03b2 (Eq. 8).\nCancellation mechanism: Again, as shown in Fig. 6, the contextualization change after the LN (+FF+RES+LN) is much lower than in preceding scopes (+FF and +FF+RES). That is, the LNs substantially canceled out the contextualization performed in the preceding components of FF and RES. Then, we specifically tackle the question, how did LN cancel the FF\u2019s effect?\nWe first found that the FF output representation has outliers in some specific dimensions (green lines in Fig. 8), and that the weight \u03b3 of LN tends to shrink these special dimensions (red lines in Fig. 8). In the layers where FF incurs a relatively large impact on contextualization,8 the Pearson correlation coefficient between LN\u2019s \u03b3 and mean absolute value of FF output by dimension was from \u22120.45 to \u22120.78 in BERTs and from \u22120.22 to \u22120.59 in GPT-2 across layers. Thus, we suspect that such specific outlier dimensions in the FF outputs encoded \u201cflags\u201d for potential contextualization change, and LN typically declines such FF\u2019s suggestions by erasing the outliers.\nIndeed, we observed that ignoring such special outlier dimensions (bottom 1% with the lowest value of \u03b3) in calculating FF\u2019s contextualization makes the change score quite small; contextualization changes by FF went from 0.21 to 0.09 in BERT and from 0.15 to 0.02 in GPT-2 on a layer average. Thus, FF\u2019s contextualization effect is realized using very specific dimensions, and LN2 cancels the effect by shrinking these special dimensions. Note that a related phenomenon was discovered by Modarressi et al. (2022): LN2 and LN1 cancel each other out by outliers in their weights in BERT. These imply, at least based only on existing observations, that the Transformer layer seems to perform redundant contextualization processing. Further investigation is an important future work.\n7 CONCLUSIONS AND FUTURE WORK\nWe have analyzed the FF blocks w.r.t. input contextualization through the lens of a refined attention map by leveraging the existing norm-based analysis and the integrated gradient method having an ideal property\u2014completeness. Our experiments using masked- and causal-language models have shown that FFs indeed modify the input contextualization by amplifying specific types of linguistic compositions (e.g., subword pairs forming one word). We have also found that FF and its surrounding components tend to cancel out each other\u2019s contextualization effects and clarified their mechanism, implying the redundancy of the processing within the Transformer layer. Applying our analysis to other model variants, such as larger OPT (Zhang et al., 2022) models and LLaMA (Touvron et al., 2023a;b) families or LMs with FF-based adapters, will be our future work. In addition, focusing on inter-layer contextualization dynamics could also be fascinating future directions.\n8FFs in BERT\u2019s 3rd and 10th\u201311th layers and GPT-2\u2019s 1st\u201311th layers\nUnder review as a conference paper at ICLR 2024\nETHICS STATEMENT\nWe recognize that this study has not used any special data or methods with potential ethical issues. One recent issue in the whole ML community is that neural-network-based models make predictions containing non-intended biases (e.g., gender bias). This paper gives a method for interpreting the inner workings of real-world machine learning models, which may help us understand such biased behaviors of the models in the future.\nREPRODUCIBILITY\nWe have reported the used data and pre-trained models in the manuscript as much as possible, and no private or unofficial data people can not access are used. In addition, we used several variants of models, including different types of masked- and causa-LMs; this hopefully has contributed to enhancing our results\u2019 generality (reproducibility). All the models can be publicly available through Huggingface\u2019s transformers framework (Wolf et al., 2020). Additionally, we will make our analysis tool and experimental codes publicly available upon acceptance.\nREFERENCES\nSamira Abnar and Willem Zuidema. Quantifying Attention Flow in Transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 4190\u20134197, 2020. doi: 10.18653/v1/2020.acl-main.385. URL https://www.aclweb.org/ anthology/2020.acl-main.385.\nAdrian M. P. Bras\u0327oveanu and Ra\u0306zvan Andonie. Visualizing Transformers for NLP: A Brief Survey. In 24th International Conference Information Visualisation (IV), pp. 270\u2013279, 2020. doi: 10.1109/IV51561.2020.00051. URL https://ieeexplore.ieee.org/ abstract/document/9373074.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems 33 (NeurIPS), volume 33, pp. 1877\u20131901, 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nGino Brunner, Yang Liu, Damia\u0301n Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger Wattenhofer. On Identifiability in Transformers. In 8th International Conference on Learning Representations (ICLR), 2020. URL https://openreview.net/forum?id=BJg1f6EFDB.\nNadia Burkart and Marco F Huber. A Survey on the Explainability of Supervised Machine Learning. Artificial Intelligence Research, 70:245\u2013317, 2021. doi: 10.1613/jair.1.12228.\nDiogo V Carvalho, Eduardo M Pereira, and Jaime S Cardoso. Machine Learning Interpretability: A Survey on Methods and Metrics. Electronics, 8(8), 832, 2019. ISSN 2079-9292. doi: 10.3390/ electronics8080832. URL https://www.mdpi.com/2079-9292/8/8/832.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What Does BERT Look At? An Analysis of BERT\u2019s Attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 276\u2013286, 2019. doi: 10.18653/v1/ W19-4828. URL https://www.aclweb.org/anthology/W19-4828.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge Neurons in Pretrained Transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 8493\u20138502, 2022. doi: 10.18653/v1/2022.acl-long.581. URL https://aclanthology.org/2022.acl-long.581.\nUnder review as a conference paper at ICLR 2024\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pp. 4171\u20134186, 2019. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423/.\nStefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 107:3\u201311, 2018. ISSN 0893-6080. doi: 10.1016/j.neunet.2017.12.012. URL https://www.sciencedirect. com/science/article/pii/S0893608017302976. Special issue on deep reinforcement learning.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer Feed-Forward Layers Are Key-Value Memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 5484\u20135495, 2021. doi: 10.18653/v1/2021.emnlp-main.446. URL https://aclanthology.org/2021.emnlp-main.446.\nDan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). arXiv preprint, cs.LG/1606.08415v4, 2016. doi: 10.48550/arXiv.1606.08415.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efficient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 2790\u20132799. PMLR, 2019.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. Attention is Not Only a Weight: Analyzing Transformers with Vector Norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 7057\u20137075, 2020. doi: 10.18653/v1/2020.emnlp-main.574. URL https://www.aclweb.org/anthology/ 2020.emnlp-main.574.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. Incorporating Residual and Normalization Layers into Analysis of Masked Language Models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4547\u2013 4568, 2021. doi: 10.18653/v1/2021.emnlp-main.373. URL https://aclanthology.org/ 2021.emnlp-main.373.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the Dark Secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pp. 4364\u20134373, 2019. doi: 10.18653/v1/D19-1445. URL https://www.aclweb. org/anthology/D19-1445.\nNikolaus Kriegeskorte, Marieke Mur, and Peter Bandettini. Representational similarity analysis - connecting the branches of systems neuroscience. Frontiers in Systems Neuroscience, 2:4, 2008. doi: 10.3389/neuro.06.004.2008. URL https://www.frontiersin.org/article/ 10.3389/neuro.06.004.2008.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint, cs.CL/1907.11692v1, 2019. doi: 10.48550/arXiv.1907.11692. URL http://arxiv.org/abs/1907.11692v1.\nShapley Lloyd S. A Value for n-person Games. Contributions to the Theory of Games (AM-28), Volume II, pp. 307\u2013318, 1953. doi: 10.1515/9781400881970-018. URL https://www.jstor. org/stable/j.ctt1b9x1zv.24.\nAli Modarressi, Mohsen Fayyaz, Yadollah Yaghoobzadeh, and Mohammad Taher Pilehvar. GlobEnc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pp.\nUnder review as a conference paper at ICLR 2024\n258\u2013271, 2022. doi: 10.18653/v1/2022.naacl-main.19. URL https://aclanthology. org/2022.naacl-main.19.\nAli Modarressi, Mohsen Fayyaz, Ehsan Aghazadeh, Yadollah Yaghoobzadeh, and Mohammad Taher Pilehvar. DecompX: Explaining Transformers Decisions by Propagating Token Decomposition. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), pp. 2649\u20132664, 2023. doi: 10.18653/v1/2023.acl-long.149. URL https://aclanthology.org/2023.acl-long.149.\nVinod Nair and Geoffrey E. Hinton. Rectified Linear Units Improve Restricted Boltzmann Machines. In Proceedings of the 27th International Conference on Machine Learning (ICML), pp. 807\u2013814, 2010. URL https://icml.cc/Conferences/2010/papers/432.pdf.\nDaisuke Oba, Naoki Yoshinaga, and Masashi Toyoda. Exploratory Model Analysis Using DataDriven Neuron Representations. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pp. 518\u2013528, 2021. doi: 10.18653/v1/2021. blackboxnlp-1.41. URL https://aclanthology.org/2021.blackboxnlp-1.41.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. Technical report, OpenAI, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/ language-models.pdf.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. A Primer in BERTology: What We Know About How BERT Works. Transactions of the Association for Computational Linguistics (TACL), 8:842\u2013866, 2020. doi: 10.1162/tacl a 00349. URL https://www.aclweb.org/ anthology/2020.tacl-1.54.\nThibault Sellam, Steve Yadlowsky, Ian Tenney, Jason Wei, Naomi Saphra, Alexander D\u2019Amour, Tal Linzen, Jasmijn Bastings, Iulia Raluca Turc, Jacob Eisenstein, Dipanjan Das, and Ellie Pavlick. The MultiBERTs: BERT Reproductions for Robustness Analysis. In 10th International Conference on Learning Representations (ICLR), 2022. URL https://openreview.net/ forum?id=K0E_F0gFDgA.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1631\u20131642, 2013. URL https://www.aclweb.org/anthology/ D13-1170.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic Attribution for Deep Networks. In Proceedings of the 34th International Conference on Machine Learning (ICML), PMLR, volume 70, pp. 3319\u20133328, 2017. URL https://proceedings.mlr.press/v70/ sundararajan17a.html.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. BERT Rediscovers the Classical NLP Pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 4593\u20134601, 2019. doi: 10.18653/v1/P19-1452. URL https://www.aclweb.org/ anthology/P19-1452/.\nIlya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Advances in neural information processing systems, 34:24261\u2013 24272, 2021.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe\u0301e Lacroix, Baptiste Rozie\u0300re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a. URL https://arxiv.org/abs/2302.13971v1.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\nUnder review as a conference paper at ICLR 2024\nCristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, 2023b. URL https://arxiv.org/abs/2307.09288v2.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-Read Students Learn Better: On the Importance of Pre-training Compact Models. arXiv preprint, cs.CL/1908.08962v2, 2019. doi: 10.48550/arXiv.1908.08962. URL https://arxiv.org/abs/1908.08962v2.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in Neural Information Processing Systems 30 (NIPS), pp. 5998\u20136008, 2017. URL http://papers.nips. cc/paper/7181-attention-is-all-you-need.\nJesse Vig. Visualizing Attention in Transformer-Based Language Representation Models. arXiv preprint arXiv:1904.02679, 2019. URL https://arxiv.org/abs/1904.02679v2.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP): System Demonstrations, pp. 38\u201345, 2020. URL https:// aclanthology.org/2020.emnlp-demos.6.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On Layer Normalization in the Transformer Architecture. In Proceedings of the 37th International Conference on Machine Learning (ICML), PMLR, volume 119, pp. 10524\u201310533, 2020. URL https://proceedings.mlr.press/v119/ xiong20b.html.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: Open Pre-trained Transformer Language Models. arXiv preprint, cs.CL/2205.01068v4, 2022. URL https://arxiv.org/abs/2205.01068v4.\nA DETAILED FORMULAS FOR EACH ANALYSIS METHOD\nWe describe the mathematical details of the norm-based analysis methods adopted in this paper.\nA.1 ATB\nAs described in \u00a7 2, Kobayashi et al. (2021) rewrite the operation of the attention block (ATB) into the sum of transformed inputs and a bias term:\nyATBi = ATB(xi;X) (15) = LN1 \u25e6 RES1 \u25e6ATTN (16)\n=\nn\u2211\nj=1\nFATBi (xj ;X) + b ATB. (17)\nUnder review as a conference paper at ICLR 2024\nFirst, the multi-head attention mechanism (ATTN; Eq. 1) can be decomposed into a sum of transformed vectors as Eq. 6 and Eq. 7:\nATTN(xi;X) = \u2211\nj\n\u2211\nh\n\u03b1hi,j(xjW h V )W h O + bV WO + bO (18)\n= \u2211\nj\nFATTNi (xj ;X) + b ATTN (19)\nFATTNi (xj ;X) = \u03b1 h i,j(xjW h V )W h O (20)\nbATTN = bV WO + bO. (21) Second, the residual connection (RES; Eq. 4) just performs the vector addition. So, ATTN and RES also can be decomposed into a sum of transformed vectors collectively:\n(RES1 \u25e6ATTN)(xi;X) = ATTN(xi;X) + xi (22) = \u2211\nj \u0338=i FATTNi (xj ;X) + xi + b ATTN (23)\n= \u2211\nj\nFATTN+RESi (xj ;X) + b ATTN (24)\nFATTN+RESi (xj ;X) = { FATTNi (xj ;X) (j \u0338= j) FATTNi (xi;X) + xi (j = i) . (25)\nThird, the layer normalization (LN; Eq. 5) performs a normalization and element-wise affine transformation. Suppose the input to LN is a sum of vectors z = \u2211 j zj , LN\u2019s operation is decomposed as follows:\nLN(z) = z \u2212m(z)\ns(z) \u2299 \u03b3 + \u03b2 (26)\n= z \u2212 1d\n\u2211d k=1 z (k) s(z) \u2299 \u03b3 + \u03b2 (27)\n=\n\u2211 j zj \u2212 1d \u2211d k=1 (\u2211 j zj )(k)\ns(z) \u2299 \u03b3 + \u03b2 (28)\n= \u2211\nj\nzj \u2212 1d \u2211d k=1 z (k) j\ns(z) \u2299 \u03b3 + \u03b2 (29)\n= \u2211\nj\nzj \u2212m(zj) s(z) \u2299 \u03b3 + \u03b2 (30)\n= \u2211\nj\nF LN(zj) + \u03b2 (31)\nF LN(zj) = zj \u2212m(zj)\ns(z) \u2299 \u03b3, (32)\nwhere z(k) denotes the k-th element of vector z. By exploiting this decomposition, attention block (ATB; ATTN, RES, and LN) can be decomposed into a sum of transformer vectors:\n(LN1 \u25e6 RES1 \u25e6ATTN)(xi;X) = LN (\u2211\nj\nFATTN+RESi (xj ;X) + b ATTN\n) (33)\n= \u2211\nj\nF LN ( FATTN+RESi (xj ;X) ) + F LN ( bATTN ) + \u03b2 (34)\n= \u2211\nj\nFATBi (xj ;X) + b ATB (35)\nFATBi (xj ;X) = F LN ( FATTN+RESi (xj ;X) ) (36)\nbATB = F LN ( bATTN ) + \u03b2. (37)\nUnder review as a conference paper at ICLR 2024\nThen, the ATB method quantifies how much the input xj impacted the computation of the output yATBi by the norm \u2225FATBi (xj ;X)\u2225.\nA.2 ATBFF\nThe feed-forward network (FF) via a two-layered fully connected network (Eq. 3). By exploiting the decomposition of an activation function g (\u00a7 3), ATB and FF can be decomposed into a sum of transformed vectors collectively:\n(FF \u25e6ATB)(xi;X) = g ( ATB(xi;X)W1 + b1 ) W2 + b2 (38)\n= g\n(( n\u2211\nj=1\nFATBi (xj ;X) + b ATB ) W1 + b1 ) W2 + b2 (39)\n= g\n( n\u2211\nj=1\nFATBi (xj ;X)W1 + b ATBW1 + b1 ) W2 + b2 (40)\n= g\n( n\u2211\nj=1\nF Pre gi (xj) + b Pre g ) W2 + b2 (41)\n=\n( n\u2211\nj=1\nhj ( F Pre gi (x1), . . . ,F Pre g i (xn), b Pre g; g\u0303,0 )\n+ hb ( F Pre gi (x1), . . . ,F Pre g i (xn), b Pre g; g\u0303,0 )) W2 + b2 (42)\n=\nn\u2211\nj=1\nhj ( F Pre gi (x1), . . . ,F Pre g i (xn), b Pre g; g\u0303,0 ) W2 (43)\n+ hb ( F Pre gi (x1), . . . ,F Pre g i (xn), b Pre g; g\u0303,0 ) W2 + b2 (44)\n= \u2211\nj\nFATB+FFi (xj ;X) + b ATB+FF (45)\nF Pre gi (xj ;X) = F ATB i (xj ;X)W1 (46) bPre g = bATBW1 + b1 (47)\nFATB+FFi (xj ;X) = hj ( F Pre gi (x1), . . . ,F Pre g i (xn), b Pre g; g\u0303,0 ) W2 (48)\nbATB+FF = hb ( F Pre gi (x1), . . . ,F Pre g i (xn), b Pre g; g\u0303,0 ) W2 + b2. (49)\nThen, the ATBFF method quantifies how much the input xj impacted the computation of the output yATB+FFi by the norm \u2225FATB+FFi (xj ;X)\u2225. Detailed decomposition of the activation function g is described in Appendix B.\nUnder review as a conference paper at ICLR 2024\nA.3 ATBFFRES\nThe residual connection (RES; Eq. 4) just performs the vector addition. So, ATB, FF, and RES2 also can be decomposed into a sum of transformed vectors collectively:\n(RES2 \u25e6 FF \u25e6ATB)(xi;X) = (FF \u25e6ATB)(xi;X) + ATB(xi;X) (50) = \u2211\nj \u0338=i FATB+FFi (xj ;X) + F ATB i (xi;X) + b ATB+FF (51)\n= \u2211\nj\nFATB+FF+RESi (xj ;X) + b ATB+FF (52)\nFATB+FF+RESi (xj ;X) = { FATB+FFi (xj ;X) (j \u0338= j) FATB+FFi (xi;X) + F ATB i (xi;X) (j = i) .\n(53) Then, the ATBFFRES method quantifies how much the input xj impacted the computation of the output yATB+FF+RESi by the norm \u2225FATB+FF+RESi (xj ;X)\u2225.\nA.4 ATBFFRESLN\nBy exploiting the decomposition of LN (Eq. 31 and Eq. 32), entire Transformer layer (ATTN, RES1, LN1, FF, RES2, and LN2) can be decomposed into a sum of transformer vectors:\nLayer(xi;X) = LN (\u2211\nj\nFATB+FF+RESi (xj ;X) + b ATB+FF\n) (54)\n= \u2211\nj\nF LN ( FATB+FF+RESi (xj ;X) ) + F LN ( bATB+FF ) + \u03b2 (55)\n= \u2211\nj\nF Layeri (xj ;X) + b Layer (56)\nF Layeri (xj ;X) = F LN ( FATB+FF+RESi (xj ;X) ) (57)\nbLayer = F LN ( bATB+FF ) + \u03b2. (58)\nThen, the ATBFFRESLN method quantifies how much the input xj impacted the computation of the layer output yi by the norm \u2225F Layeri (xj ;X)\u2225.\nB DETAILS OF DECOMPOSITION IN \u00a7 3\nB.1 FEATURE ATTRIBUTION METHODS\nOne of the major ways to interpret black-box deep learning models is measuring how much each input feature contributes to the output. Given a model f : Rn \u2192 R;x 7\u2192 f(x) and a certain input x\u2032 = (x\u20321, . . . , x \u2032 n), this approach decomposes the output f(x\n\u2032) into the sum of the contributions ci corresponding to each input feature x\u2032i:\nf(x\u2032) = c1(x \u2032; f) + \u00b7 \u00b7 \u00b7+ cn(x\u2032; f). (59)\nInterpretation methods with this approach are called feature attribution methods (Carvalho et al., 2019; Burkart & Huber, 2021), and typical methods include Integrated Gradients (Sundararajan et al., 2017) and Shapley values (Lloyd S., 1953).\nB.2 INTEGRATED GRADIENTS (IG)\nIG is an excellent feature attribution method in that it has several desirable properties such as completeness (\u00a7 3). Specifically, IG calculates the contribution of each input feature x\u2032i by attributing the output at the input x\u2032 \u2208 Rn relative to a baseline input b \u2208 Rn:\nIGi(x \u2032; f, b) := (x\u2032i \u2212 bi)\n\u222b 1\n\u03b1=0\n\u2202f\n\u2202xi \u2223\u2223\u2223\u2223 x= b+\u03b1(x\u2032 \u2212 b) d\u03b1. (60)\nUnder review as a conference paper at ICLR 2024\nThis contribution IGi(x\u2032; f, b) satisfies equation 59:\nf(x\u2032) = f(b) + \u2211n\nj=1 IGj(x \u2032; f, b). (61)\nThe first term f(b) can be eliminated by selecting a baseline b for which f(b) = 0 (see B.3).\nB.3 DECOMPOSITION OF GELU WITH IG\nThis paper aims to expand the norm-based analysis (Kobayashi et al., 2020), the interpretation method for Transformer, into the entire Transformer layer. However, a simple approach to decompose the network in closed form as in Kobayashi et al. (2020; 2021) cannot incorporate the activation function (GELU, Hendrycks & Gimpel, 2016) contained in FF9. This paper solves this problem by decomposing GELU with IG (Sundararajan et al., 2017).\nGELU: R \u2192 R is defined as follows:\nGELU(x) = x\n2\n( 1 +\n2\u221a \u03c0\n\u222b x\u221a 2\n0\ne\u2212t 2 dt ) . (62)\nConsidering that in the Transformer layer, the input x \u2208 R of GELU can be decomposed into a sum of xj terms that rely on each token representation of layer inputs (x = x1 + \u00b7 \u00b7 \u00b7 + xn), GELU can be viewed as a multivariable function G\u0303ELU:\nG\u0303ELU(x1, . . . , xn) := GELU( \u2211n j=1 xj). (63)\nGiven a certain input x\u2032 = x\u20321 + \u00b7 \u00b7 \u00b7 + x\u2032n, the contribution of each input feature x\u2032j to the output GELU(x\u2032) is calculated by decomposing G\u0303ELU(x\u20321, . . . , x \u2032 n) with IG (Eq. 60 and 61):\nGELU(x\u2032) = G\u0303ELU(x\u20321, . . . , x \u2032 n)\n= \u2211n\nj=1 IGj(x \u2032 1, . . . , x \u2032 n; G\u0303ELU,0), (64)\nwhere b = 0 was chosen. Note that G\u0303ELU(b) = 0 and the last term in equation 61 is eliminated.\nDecomposition of broadcasted GELU In a practical neural network implementation, the GELU layer is passed a vector x1 + \u00b7 \u00b7 \u00b7+ xn \u2208 Rd \u2032 instead of a scalar x1 + \u00b7 \u00b7 \u00b7+ xn \u2208 R and the GELU function is applied (broadcasted) element-wise. Let GELU : Rd\u2032 \u2192 Rd\u2032 be defined as the function that broadcasts the element-level activation GELU: R \u2192 R. The contribution of each input vector x\u2032j = [x \u2032 j [1], . . . , x \u2032 j [d \u2032]] to the output vector GELU(x\u20321 + \u00b7 \u00b7 \u00b7+ x\u2032n) is as follows:\nGELU(x\u20321 + \u00b7 \u00b7 \u00b7+ x\u2032n) =   GELU(x\u20321[1] + \u00b7 \u00b7 \u00b7+ x\u2032n[1])\n... GELU(x\u20321[d \u2032] + \u00b7 \u00b7 \u00b7+ x\u2032n[d\u2032])\n  \u22a4\n(65)\n=   \u2211n j=1 IGj(x \u2032 1[1], . . . ,x \u2032 n[1]; G\u0303ELU,0)\n...\u2211n j=1 IGj(x \u2032 1[d \u2032], . . . ,x\u2032n[d \u2032]; G\u0303ELU,0)\n  \u22a4\n(66)\n=\nn\u2211\nj=1\n  IGj(x \u2032 1[1], . . . ,x \u2032 n[1]; G\u0303ELU,0)\n... IGj(x1[d \u2032], . . . ,xn[d\u2032]; G\u0303ELU,0)\n  \u22a4 . (67)\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\n9Many recent Transformers, including BERT and RoBERTa, employ GELU as their activation function.\nUnder review as a conference paper at ICLR 2024\nC SUPPLEMENTAL RESULTS OF CONTEXTUALIZATION CHANGE\nWe reported the contextualization change through each component in FFBs of BERT-base and GPT2 on the Wikipedia dataset in \u00a7 5. We will report the results of the other models/datasets in this section. In addition, we provide full results of linguistic patterns in FF\u2019s contextualization effects that could not be included in the main body.\nC.1 MACRO CONTEXTUALIZATION CHANGE\nThe contextualization changes through each component in FFBs of six variants of BERT models with different sizes are shown in Fig. 9. The results for four variants of BERT-base models trained with different seeds are shown in Fig. 10. The results for two variants of RoBERTa models with different sizes are shown in Fig. 11. The results for OPT 125M model are shown in Fig. 12. The results for BERT-base and GPT-2 on the SST-2 dataset are shown in Fig. 13. These different settings with other models/datasets also yield similar results reported in \u00a7 5.1: each component did modify the input contextualization, especially in particular layers. The mask language models showed a consistent trend of larger changes by FF and LN in the middle to late layers. On the other hand, the causal language models showed a trend of larger changes by FF in the early layers.\nC.2 LINGUISTIC PATTERNS IN FF\u2019S CONTEXTUALIZATION EFFECTS\nThe full lists of top 10 pairs with respect to the FF-amp score in each layer of BERT-base and GPT-2 and their token pair categories are shown in Tables 2 and 3.\nD SUPPLEMENTAL RESULTS OF DYNAMICS OF CONTEXTUALIZATION CHANGE\nWe reported the dynamics of contextualization change of BERT-base and GPT-2 on the Wikipedia dataset in \u00a7 6. We will report the results of the other models/datasets in this section.\nD.1 CONTEXTUALIZATION CHANGE THROUGH FF AND SUBSEQUENT COMPONENTS\nThe contextualization changes through by FF and subsequent components, RES and LN, in six variants of BERT models with different sizes are shown in Fig. 14. The results for four variants of BERT-base models trained with different seeds are shown in Fig. 15. The results for two variants of RoBERTa models with different sizes are shown in Fig. 16. The results for OPT 125M model are shown in Fig. 17. The results for BERT-base and GPT-2 on the SST-2 dataset are shown in Fig. 18. These different settings with other models/datasets also yield similar results reported in \u00a7 6: through the FF and subsequent RES and LN, the contextualization change score once becomes large but finally converges to be small; that is, the contextualization by FFs tends to be canceled by the following components.\nD.2 FF AND RES\nThe L2 norm of the output vectors from FF and the vectors bypassed via RES2, in six variants of BERT models with different sizes are shown in Fig. 19. The results for four variants of BERT-base models trained with different seeds are shown in Fig. 20. The results for two variants of RoBERTa models with different sizes are shown in Fig. 21. The results for BERT-base and GPT-2 on the SST-2 dataset are shown in Fig. 22. These different settings with other models/datasets also yield similar results reported in \u00a7 6.1: the vectors bypassed via RES2 are more than twice as large as output vectors from FF in the L2 norm in more than half of the layers. That is, the representation (contextualization) updated by the FF tends to be overwritten/canceled by the original one.\nD.3 FF AND LN\nMean absolute value in each dimension of the input/output vectors of FF and the weight parameter \u03b3 of LN at the layer where FF\u2019s contextualization effects are strongly cancelled by LN, in six variants\nUnder review as a conference paper at ICLR 2024\nof BERT models with different sizes are shown in Fig. 23. The results for four variants of BERT-base models trained with different seeds are shown in Fig. 24. The results for two variants of RoBERTa models with different sizes are shown in Fig. 25. The results for BERT-base and GPT-2 on the SST-2 dataset are shown in Fig. 26. These different settings with other models/datasets also yield similar results reported in \u00a7 6.2: the FF output representation has outliers in some specific dimensions (green lines in the figures), and the weight \u03b3 of LN tends to shrink these special dimensions (red lines in the figures). In the layers where FF incurs a relatively large impact on contextualization, the Pearson correlation coefficient between LN\u2019s \u03b3 and mean absolute value of FF output by dimension was from \u22120.38 to \u22120.93 in BERT-large, from \u22120.32 to \u22120.47 in BERT-medium, from \u22120.56 to \u22120.76 in BERT-small, from \u22120.55 to \u22120.73 in BERT-mini, \u22120.71 in BERT-tiny, from \u22120.69 to \u22120.74 in BERT-base (seed 0), from \u22120.48 to \u22120.74 in BERT-base (seed 10), from \u22120.50 to \u22120.68 in BERT-base (seed 20). In these layers of RoBERTa models, the Pearson\u2019s r was small: from \u22120.02 to \u22120.28 in RoBERTa-large and from 0.01 to \u22120.14 in RoBERT-base. However, the Spearman\u2019s \u03c1 was large: from \u22120.46 to \u22120.56 in RoBERTa-large and from 0.80 to \u22120.94 in RoBERT-base. In \u00a7 6.2, we also observed that ignoring such special outlier dimensions (bottom 1% with the lowest value of \u03b3) in calculating FF\u2019s contextualization makes the change score quite small. The contextualization changes by FF when ignoring the dimensions are shown in Fig. 27.\nUnder review as a conference paper at ICLR 2024\nUnder review as a conference paper at ICLR 2024\nUnder review as a conference paper at ICLR 2024\nUnder review as a conference paper at ICLR 2024\nUnder review as a conference paper at ICLR 2024\nUnder review as a conference paper at ICLR 2024\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, nd covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinto , 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation fu ctio g that passes through the origin and is differenti ble in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\n16\nUnder review as a conference pap r at ICLR 2024\nThe above decomposition is pplicable to any ac vat on fun tion g that p sses hrough the origin and is differentiable in practice, a d covers all activ tion fu ctions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwi g et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nThe above decompositio is appl cable to any activation function g that passes through the origin and is diffe iable in pract ce, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is appl cable to any activation function g that passes through the origin and is diffe iable in p act ce, nd covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinto , 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is appl cable to any activation function g that passes through the origin and is diffe iable in p act ce, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & H nton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nab ve decomposition is appl cable to any activation fu ctio g that passes through the origin and is diffe i ble in pract ce, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is appl cable to any activation function g that passes through the origin and is diffe iable in pract ce, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\n16\nUnder review as a conference pap r at ICLR 2024\nThe ab ve de omp sitio is ppl cable to any ac vat on fun tion g that p sses hrough the origin and is diffe iable in pract ce, a d covers all activ tion fu cti ns currently employed in Transformers, such as Re (Nair & Hinton, 2010) and SiLU (Elfwi g et al., 2018).\nLNATTN\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n25\nUnder review as a conference paper at ICLR 2024\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n16\n+\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, nd covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinto , 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomp sition is ap licable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation fu ctio g that passes through the origin and is differenti ble in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n16\n+ + +\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n16\nUnder review as a conference pap r at ICLR 2024\nThe above decomposition is pplicable to any ac vat on fun tion g that p sses hrough the origin and is differentiable in practice, a d covers all activ tion fu ctions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwi g et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n16\n+ + (Same layer\u2019s PostLN)\n(a) BERTa-base (original).\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differen iable in p actice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n16\n+\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, nd covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinto , 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n16\nUnder review as a conference paper at ICLR 2024\nThe above ecomposition is applicable to any activation fu ctio g that passes through the origin and is differe ti ble in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n16\n+ + +\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n16\nUnder review as a conference pap r at ICLR 2024\nThe above decomp sition is pplic ble to any ac vat on fun tion g that p sses hrough the origin and is diff rentiable in practice, a d covers all activ tion fu ctions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwi g et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n16\n+ + (Same layer\u2019s PostLN)\n(b) BERTa-base (seed 0).\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n+\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, nd covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinto , 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\nUnder review as a conference pap r at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation fu ctio g that passes through the origin and is differenti ble in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n+ + +\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\nUnder review as a conference pap r at ICLR 2024\nThe above decomposition is pplicable to any ac vat on fun tion g that p sses hrough the origin and is differentiable in practice, a d covers all activ tion fu ctions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwi g et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n+ + (Same layer\u2019s PostLN)\n(c) BERTa-base (seed 10).\nUnder review as a conference paper at ICLR 2024\nThe above deco sition is applicabl to any activation function g that passes through the origin and is diffe entiable in practice, and c vers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n+\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is diffe entiable in practice, nd c vers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinto , 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\nUnder review as a conference p p r at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and s diffe entiable in practice, and c vers ll activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation fu ctio g that passes through the origin and is diffe enti ble in practice, and c vers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n+ + +\nUnder review as a conference paper at ICLR 2024\nThe above deco position is applicable to any activation function g that passes through the origin and is diffe entiable in practice, and c vers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\nUnder review as a conference pap r at ICLR 2024\nThe above deco sition is pplicabl to any ac vat on fun t on g that p sses hrough the origin and is diffe entiable in practice, a d c vers all activ tio fu ctions currently employed in Transformers, such s ReLU (Nair & Hinton 0 and SiLU (Elfwi g et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n+ + (Same layer\u2019s PostLN)\n(d) BERTa-base (seed 20).\nFigure 15: Contextualization changes through processing each component (FF, RES, and LN) from just before FF (ATB) in four variants of BERT-base models trained with different seeds (original, 0, 10, and 20). The higher the bar, the more the contextualization (attention map) changes from just before FF.\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n+\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and\nis differentiable in practice, nd covers all activation functions currently employed in Transformers,\nsuch as ReLU (Nair & Hinto , 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activati function g that passes thr ugh the origin and\nis differentiable in practice, and covers all activation functions currently employed in Transformers,\nsuch as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation fu ctio g that passes through the origin and\nis differenti ble in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n+ + +\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\nUnder review as a conference pap r at ICLR 2024\nThe above decomposition is pplicable to any ac vat on fun tion g that p sses hrough the origin and\nis differentiable in practice, a d covers all activ tion fu ctions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwi g et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n+ + (Same layer\u2019s PostLN)\n(a) RoBERTa-large.\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and\nis differen iable in p actice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n+\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and\nis differentiable in practice, nd covers all activation functions currently employed in Transformers,\nsuch as ReLU (Nair & Hinto , 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and\nis differentiable in practice, and covers all activation functions currently employed in Transformers,\nsuch as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\nUnd r review as a conference paper at ICLR 2024\nThe above ecomposition is applicable to any activation fu ctio g that passes through the origin and\nis differe ti ble in practice, and covers all activation functions currently employed in Transformers,\nsuch as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n+ + +\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\nUnder review as a confer nce pap r at ICLR 2024\nThe above decomp sition is pplic ble to any ac vat on fun tion g that p sses hrough the origin and\nis diff rentiable in practice, a d covers all activ tion fu ctions currently employed in Transformers,\nsuch as ReLU (Nair & Hinton, 2010) and SiLU (Elfwi g et al., 2018).\nLNATTN\nLNATTNRES\nLNATTNRESLN\nLNATTNRESLNFF\nLNATTNRESLNFFRES\n+ + (Same layer\u2019s PostLN)\n(b) RoBERTa-base.\nUnder review as a conference paper at ICLR 2024 Under review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, nd covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinto , 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation fu ctio g that passes through the origin and is differenti ble in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\n16\nUnder review as a conference pap r at ICLR 2024\nThe above decomposition is pplicable to any ac vat on fun tion g that p sses hrough the origin and is differentiable in practice, a d covers all activ tion fu ctions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwi g et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, nd covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinto , 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation fu ctio g that passes through the origin and is differenti ble in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is applicable to any activation function g that passes through the origin and is differentiable in practice, and covers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nUnder review as a conference pap r at ICLR 2024\nThe above decomposition is pplicable to any ac vat on fun tion g that p sses hrough the origin and is differentiable in practice, a d covers all activ tion fu ctions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwi g et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomp sition is appl cabl to any activation function g that passes through the origin and is diffe iable in pract ce, and c vers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is appl cable to any activation function g that passes through the origin and is diffe iable in pract ce, nd c vers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinto , 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is appl cable to any activation function g that passes through the origin and is diffe iable in pract ce, and c vers ll ac ivation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nT above decomposition is applicable to any activation fu ctio g that passes through the origin and is differ ti ble in practice, and c vers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\nLNATTN\n16\nUnder review as a conference paper at ICLR 2024\nThe above decomposition is appl cable to any activation function g that passes through the origin and is diffe iable in pract ce, and c vers all activation functions currently employed in Transformers, such as ReLU (Nair & Hinton, 2010) and SiLU (Elfwing et al., 2018).\n16\nUnder review as a conference pap r at ICLR 2024\nThe ab ve decomp sition is ppl cabl to any ac vat on fun t on g that p sses hrough the origin and is diffe iable n pract ce, a d c vers all activ tio fu ctions currently employed in Transformers, such s ReLU (Nair & Hinton, 0) and SiLU (Elfwi g et al., 2018).\nLNATTN\n16\n27\nUnder review as a conference paper at ICLR 2024\nUnder review as a conference paper at ICLR 2024\nUnder review as a conference paper at ICLR 2024\nUnder review as a conference paper at ICLR 2024"
        }
    ],
    "year": 2023
}