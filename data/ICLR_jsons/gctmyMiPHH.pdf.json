{
    "abstractText": "We formalize and study a phenomenon called feature collapse that makes precise the intuitive idea that entities playing a similar role in a learning task receive similar representations. As feature collapse requires a notion of task, we leverage a synthetic task in which a learner must classify \u2018sentences\u2019 constituted of L tokens. We start by showing experimentally that feature collapse goes hand in hand with generalization. We then prove that, in the large sample limit, distinct tokens that play identical roles in the task receive identical local feature in the first layer of the network. This analysis shows that a neural network trained on this task provably learns interpretable and meaningful representations in its first layer.",
    "authors": [
        {
            "affiliations": [],
            "name": "FEATURE COLLAPSE"
        },
        {
            "affiliations": [],
            "name": "Thomas Laurent"
        },
        {
            "affiliations": [],
            "name": "James H. von Brecht"
        },
        {
            "affiliations": [],
            "name": "Xavier Bresson"
        }
    ],
    "id": "SP:24778afdd1a7be80d1d5d5350b7dec2f63eb442f",
    "references": [
        {
            "authors": [
                "Emmanuel Abbe",
                "Enric Boix Adsera",
                "Theodor Misiakiewicz"
            ],
            "title": "The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks",
            "venue": "In Conference on Learning Theory,",
            "year": 2022
        },
        {
            "authors": [
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li"
            ],
            "title": "Physics of language models: Part 1, context-free grammar",
            "venue": "arXiv preprint arXiv:2305.13673,",
            "year": 2023
        },
        {
            "authors": [
                "Jimmy Ba",
                "Murat A Erdogdu",
                "Taiji Suzuki",
                "Zhichao Wang",
                "Denny Wu",
                "Greg Yang"
            ],
            "title": "Highdimensional asymptotics of feature learning: How one gradient step improves the representation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Alberto Bietti",
                "Joan Bruna",
                "Clayton Sanford",
                "Min Jae Song"
            ],
            "title": "Learning single-index models with shallow neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Noam Chomsky"
            ],
            "title": "Three models for the description of language",
            "venue": "IRE Transactions on information theory,",
            "year": 1956
        },
        {
            "authors": [
                "Alexandru Damian",
                "Jason Lee",
                "Mahdi Soltanolkotabi"
            ],
            "title": "Neural networks can learn representations with gradient descent",
            "venue": "In Conference on Learning Theory,",
            "year": 2022
        },
        {
            "authors": [
                "Cong Fang",
                "Hangfeng He",
                "Qi Long",
                "Weijie J Su"
            ],
            "title": "Exploring deep neural networks via layerpeeled model: Minority collapse in imbalanced training",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2021
        },
        {
            "authors": [
                "Vitaly Feldman"
            ],
            "title": "Does learning require memorization? a short tale about a long tail",
            "venue": "In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing,",
            "year": 2020
        },
        {
            "authors": [
                "Vitaly Feldman",
                "Chiyuan Zhang"
            ],
            "title": "What neural networks memorize and why: Discovering the long tail via influence estimation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Wenlong Ji",
                "Yiping Lu",
                "Yiliang Zhang",
                "Zhun Deng",
                "Weijie J Su"
            ],
            "title": "An unconstrained layer-peeled perspective on neural collapse",
            "venue": "arXiv preprint arXiv:2110.02796,",
            "year": 2021
        },
        {
            "authors": [
                "Yoon Kim",
                "Chris Dyer",
                "Alexander M Rush"
            ],
            "title": "Compound probabilistic context-free grammars for grammar induction",
            "venue": "arXiv preprint arXiv:1906.10225,",
            "year": 2019
        },
        {
            "authors": [
                "Thomas Laurent",
                "James Brecht"
            ],
            "title": "Deep linear networks with arbitrary loss: All local minima are global",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Hong Liu",
                "Sang Michael Xie",
                "Zhiyuan Li",
                "Tengyu Ma"
            ],
            "title": "Same pre-training loss, better downstream: Implicit bias matters for language models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Ziwei Liu",
                "Zhongqi Miao",
                "Xiaohang Zhan",
                "Jiayun Wang",
                "Boqing Gong",
                "Stella X Yu"
            ],
            "title": "Largescale long-tailed recognition in an open world",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Jianfeng Lu",
                "Stefan Steinerberger"
            ],
            "title": "Neural collapse with cross-entropy loss",
            "venue": "arXiv preprint arXiv:2012.08465,",
            "year": 2020
        },
        {
            "authors": [
                "Dustin G Mixon",
                "Hans Parshall",
                "Jianzong Pi"
            ],
            "title": "Neural collapse with unconstrained features",
            "venue": "arXiv preprint arXiv:2011.11619,",
            "year": 2020
        },
        {
            "authors": [
                "Alireza Mousavi-Hosseini",
                "Sejun Park",
                "Manuela Girotti",
                "Ioannis Mitliagkas",
                "Murat A Erdogdu"
            ],
            "title": "Neural networks efficiently learn low-dimensional representations with sgd",
            "venue": "arXiv preprint arXiv:2209.14863,",
            "year": 2022
        },
        {
            "authors": [
                "Vardan Papyan",
                "XY Han",
                "David L Donoho"
            ],
            "title": "Prevalence of neural collapse during the terminal phase of deep learning training",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2020
        },
        {
            "authors": [
                "Suzanna Parkinson",
                "Greg Ongie",
                "Rebecca Willett"
            ],
            "title": "Linear neural network layers promote learning single-and multiple-index models",
            "venue": "arXiv preprint arXiv:2305.15598,",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Ruslan Salakhutdinov",
                "Antonio Torralba",
                "Josh Tenenbaum"
            ],
            "title": "Learning to share visual appearance for multiclass object detection",
            "year": 2011
        },
        {
            "authors": [
                "Tom Tirer",
                "Joan Bruna"
            ],
            "title": "Extended unconstrained features model for exploring deep neural collapse",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Stephan Wojtowytsch"
            ],
            "title": "On the emergence of simplex symmetry in the final and penultimate layers of neural network classifiers",
            "venue": "arXiv preprint arXiv:2012.05420,",
            "year": 2020
        },
        {
            "authors": [
                "Jinxin Zhou",
                "Xiao Li",
                "Tianyu Ding",
                "Chong You",
                "Qing Qu",
                "Zhihui Zhu"
            ],
            "title": "On the optimization landscape of neural collapse under mse loss: Global optimality with unconstrained features",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Xiangxin Zhu",
                "Dragomir Anguelov",
                "Deva Ramanan"
            ],
            "title": "Capturing long-tail distributions of object subcategories",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Zhihui Zhu",
                "Tianyu Ding",
                "Jinxin Zhou",
                "Xiao Li",
                "Chong You",
                "Jeremias Sulam",
                "Qing Qu"
            ],
            "title": "A geometric analysis of neural collapse with unconstrained features",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "e.g. Kim"
            ],
            "title": "In Figure 9 we provide an illustration of a simple depth 3 context free grammar. We ran experiments with a context free grammar of depth 4, meaning that we have words, concepts, meta-concepts and meta-meta-concepts. We used a deep neural network with ReLU nonlinearities",
            "year": 2024
        },
        {
            "authors": [
                "Zhu"
            ],
            "title": "Let (A,B) be a critical point of \u03c6. Since AA = BB , we must have that ker(A ) = ker(AA ) = ker(BB ) = ker(B ). According to (ii) these kernels are non trivial and we may choose a unit vector z \u2208 R that belongs to them",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Many machine learning practices implicitly rely on the belief that good generalization and transfer learning requires good features. Despite this, the notion of \u2018good features\u2019 remains vague and carries many potential meanings. One definition is that features/representations should only encode the information necessary to do the task at hand, and discard any unnecessary information as noise. For example, two distinct patches of grass should map to essentially identical representations even if these patches differ in pixel space. Intuitively, a network that gives the same representation to many distinct patches of grass has learned the \u2018grass\u2019 concept. We call this phenomenon feature collapse, meaning that a learner gives the same feature to entities that play similar roles for the task at hand. This phenomenon captures the common intuition, often confirmed by practice, that the ability to learn good representations in early layers is essential for the empirical success of neural networks.\nBroadly speaking, we conduct a theoretical investigation into how such representations are learned in the early layers of a neural network. To make progress we adopt a common approach in theoretical deep-learning community. One starts with a synthetic data model exhibiting a clear latent structure, and then prove that a specific neural network architecture successfully uncovers this latent structure during training. For example, recent work in representation learning (Damian et al., 2022; MousaviHosseini et al., 2022) leverages a data model (the multiple-index model) with latent structure defined by a low dimensional subspace. In this specific setting, the first layer of a fully connected neural network provably learns this low-dimensional subspace.\nOur work follows along the same lines. Specifically, we start with a data model that asks a learner to classify \u2018sentences\u2019 comprised of L tokens. Some of these tokens play identical roles in the sense that replacing one with another does not change the label of the sentence, and this equivalence of tokens defines the latent structure in the data. We then consider a neural network containing a shared embedding module, optionally followed by a LayerNorm module, and then a final linear classification head. We show that this network, when equipped with the LayerNorm module, provably learn the equivalence of these tokens in its first layer. To do so, we consider the large sample limit and derive analytical formulas for the weights of the network trained on the data model. Under some symmetry assumptions on the task, these analytical formulas reveal:\n(i) If the network includes LayerNorm then feature collapse takes place. The neural network learns to give the same embedding to tokens that play identical roles.\n(ii) If the network does not include LayerNorm then feature collapse fails. The network does not give the same embedding to tokens that play identical roles. Moreover, this failure stems from the fact that common tokens receive large embeddings and rare tokens receive small embeddings.\nFinally, we conduct experiments that show feature collapse and generalization go hand in hand. These experiments demonstrate that for the network to generalize well it is essential for tokens playing identical roles to receive the same embeddings.\nIn summary, our main contributions are as follow:\n\u2022 We study how a network learns representations in its first layer. To do so, we make the notion of \u2018good features\u2019 mathematically rigorous via a synthetic data model with latent structure.\n\u2022 We derive analytical formulas for the weights of a two-layer network trained on this data model. These analytical formulas show that, when equipped with a LayerNorm module, the network provably learns interpretable and meaningful representations in its first layer.\nThe remainder of the paper proceeds as follows: In subsection 1.1 we discuss related works; In section 2 we describe the data model and present a set of visual experiments to illustrate the main ideas used in the paper; In section 3 we present the three theorems that constitute the main results of the paper; Finally, section 4 outlines a set of additional experiments performed in the appendix."
        },
        {
            "heading": "1.1 RELATED WORKS",
            "text": "Our work most closely resembles recent work on theoretical representation learning and, to some extent, the recent literature on neural collapse. The works by Damian et al. (2022) and MousaviHosseini et al. (2022) consider a synthetic data model, the multiple-index model, to investigate representation learning. In this model, the learner must solve a regression task with normally distributed inputs x \u2208 Rd and targets y = g(\u27e8u1,x\u27e9, . . . , \u27e8ur,x\u27e9) for some function g : Rr \u2192 R, some vectors u1, . . . ,ur \u2208 Rd, and with r \u226a d. The target y therefore solely depends on the projection of x on the low-dimensional subspace spanned by u1, . . . ,ur. These works prove that a fully connected, two-layer neural network learns, in its first layer, the low dimensional subspace. The behavior of fully connected networks trained on the multiple-index model, or on related data models, has also been studied in various other works, including Ba et al. (2022), Bietti et al. (2022), Abbe et al. (2022) and Parkinson et al. (2023). Both this line of investigation and our work prove, in the appropriate sense, that a network uncovers latent structure. Nevertheless, our data model differs quite substantially from the multi-index model. Moreover, we do not study a fully connected network. Our network has shared embedding module, and this allows us to tie good features to a notion of semantic equivalence.\nThe phenomenon we study in this work, feature collapse, has superficial similarity to neural collapse but in detail is quite different. In a pioneering work, Papyan et al. (2020) conducted a series of experiments that revealed that a well-trained network gives identical representations, in its last layer, to training points that belong to the same class. In a K-class classification task we therefore see the emergence, in the last layer, of K vectors coding for the K classes. Additionally, these K vectors \u2018point\u2019 in \u2018maximally opposed\u2019 directions. This phenomenon, coined neural collapse, has been studied extensively since its discovery (e.g. Mixon et al. (2020); Lu & Steinerberger (2020); Wojtowytsch et al. (2020); Fang et al. (2021); Zhu et al. (2021); Ji et al. (2021); Tirer & Bruna (2022); Zhou et al. (2022)). To emphasize the difference with feature collapse, note that neural collapse refers to a phenomenon where all training points from the same class receive the same representation at the end of the network. Unlike feature collapse, this does not provide any indication that the network has learned good representations in its early layers, or that the neural network has uncovered some latent structure beyond the one encoded in the training labels."
        },
        {
            "heading": "2 A TALE OF FEATURE COLLAPSE",
            "text": "We begin by more fully telling the empirical tale that motivates our theoretical investigation. To make progress we adopt a common approach in theoretical deep-learning and leverage a synthetic data model exhibiting a clear latent structure. The model generates sequences of length L from some underlying set of latent variables that encode the K classes of a classification task. To make it concrete we describe this generative process using NLP terminology like \u2018sentences\u2019 and \u2018words\u2019, but of course do not intend it as a realistic language model. Figure 1 illustrates the basic idea. The left side of the figure depicts a vocabulary of nw = 12 word tokens and nc = 3 concept tokens\nV = {potato, cheese, carrots, chicken, . . .} and C = {vegetable, dairy, meat}\nwith the 12 words partitioned into the 3 equally sized concepts. A sentence x \u2208 VL is a sequence of L words (L = 5 on the figure), and a latent variable z \u2208 CL is a sequence of L concepts. The latent variables generate sentences. For example\nz = [ dairy, veggie, meat, veggie, dairy ] generates\u2212\u2192 x = [ cheese, carrot, pork, potato, butter ]\nwith the sentence on the right obtained by sampling each word at random from the corresponding concept. The first word represents a random sample from the dairy concept (butter, cheese, cream, yogurt) according to the dairy distribution (square box at left), the second word represents a random sample from the vegetable concept (potato, carrot, leek, lettuce) according to the vegetable distribution, and so forth. At right, figure 1 depicts a classification task with K = 3 categories prescribed by the three latent variables z1, z2, z3 \u2208 CL. Sentences generated by the latent variable zk share the same label k, yielding a classification problem that requires a learner to classify sentences among K categories.\nWe use two similar networks to empirically study if and when the feature collapse phenomenon occurs. The first network x 7\u2192 hW,U (x), depicted on the top panel of figure 2, starts by embedding each word in a sentence by applying a d\u00d7nw matrixW to the one-hot representation of each word. It then concatenates these d-dimensional embeddings of each word into a single vector. Finally, it applies a linear transformation U to produce a Kdimensional score vector y = hW,U (x) with one entry for each of the K classes. The d \u00d7 nw embedding matrix W and the K \u00d7 Ld matrix U of linear weights are the only learnable parameters, and the network has no nonlinearities. The second network x 7\u2192 h\u2217W,U (x), depicted at bottom, differs only by the application of a LayerNorm module to the word embeddings prior to the concatenation. For simplicity we use a LayerNorm module which does not contain any learnable parameters; the module simply removes the mean and divides by the standard deviation of its input vector. As for the first network, the only learnable weights are W and U .\nThe task depicted on figure 1, and the networks depicted on figure 2, provide a clear way of studying how interpretable and meaningful representa-\ntions are learned in the first layer of a network. For example, the four words butter, cheese, cream and yogurt clearly play identical role for the task at hand (replacing one with another does not change the label of the sentence). As a consequence we would expect the embedding layer of the network to map them to the same representation. Similarly, the words belonging to the vegetable concepts should receive same representation, and the words belonging to the meat concept should receive same representation. If this takes place, we say that feature collapse has occurred.\nIf feature collapse occurs, this will also be reflected in the second layer of the network. To see this, partition the linear transformation\nU =  \u2014u1,1\u2014 \u2014u1,2\u2014 \u00b7 \u00b7 \u00b7 \u2014u1,L\u2014 \u2014u2,1\u2014 \u2014u2,2\u2014 \u00b7 \u00b7 \u00b7 \u2014u2,L\u2014\n... ... ... \u2014uK,1\u2014 \u2014uK,2\u2014 \u00b7 \u00b7 \u00b7 \u2014uK,L\u2014\n (1)\ninto its components uk,\u2113 \u2208 Rd. Suppose for example that zk,\u2113 = veggie, meaning that the latent variable zk contains the veggie concept in the \u2113th position. If W properly encodes concepts then we expect the vector uk,\u2113 to give a strong response when presented with the embedding of a word that belongs to the veggie concept. So we would expect uk,\u2113 to align with the embeddings of the words that belong to the veggie concept, and so feature collapse would occur in this manner as well.\nIn the remainder of this section we conduct experiments that visually illustrate the feature collapse phenomenon and the formation of interpretable representations in networks h and h\u2217. These experiments also show how layer normalization plays a key role in the feature collapse phenomenon. In our experiments we use the standard cross entropy loss \u2113(y, k), with y \u2208 RK and 1 \u2264 k \u2264 K, and then minimize the corresponding regularized empirical risks\nRemp(W,U) = 1\nK\n1\nnspl K\u2211 k=1 nspl\u2211 i=1 \u2113 ( hW,U (xk,i) , k ) + \u03bb 2 \u2225U\u22252F + \u03bb 2 \u2225W\u22252F (2)\nR\u2217emp(W,U) = 1\nK\n1\nnspl K\u2211 k=1 nspl\u2211 i=1 \u2113 ( h\u2217W,U (xk,i) , k ) + \u03bb 2 \u2225U\u22252F (3)\nof each network via stochastic gradient descent. The xk, i denote the i-th sentence of the kth category in the training set, and so each of the K categories has nspl representatives. For the parameters of the architecture, loss, and training procedure, we use an embedding dimension of d = 100, a weight decay of \u03bb = 0.001, a mini-batch size of 100 and a constant learning rate 0.1, respectively, for all experiments. The LayerNorm module implicitly regularizes the matrix W so we do not penalize it in equation (3). The codes for our experiments are available at https://github.com/xbresson/feature_collapse.\nRemark: Without weight decay (i.e. \u03bb = 0), the above objectives typically do not have global minima. We therefore focus our theoretical investigation on the case \u03bb > 0 which is analytically more tractable. In appendix A.1 we provide an empirical investigation of the case without weight decay to show that both cases (i.e. \u03bb > 0 and \u03bb = 0) exhibit the same behavior in practice."
        },
        {
            "heading": "2.1 THE UNIFORM CASE",
            "text": "We start with an instance of the task from figure 1 with parameters nc = 3, nw = 1200, L = 15, K = 1000, and with uniform word distributions. So each of the 3 concepts (say veggie, dairy, and meat) contain 400 words and the corresponding distributions (the veggie distribution, the dairy distribution, and the meat distribution) are uniform. We form K = 1000 latent variables z1, . . . , z1000 by selecting them uniformly at random from the set CL, which simply means that any concept sequence z = [z1, . . . , zL] has an equal probability of occurrence. We then construct a training set by generating nspl = 5 data points from each latent variable. We then train both networks h, h\u2217 and evaluate their generalization performance; both achieve 100% accuracy on test points.\nSince both networks generalize perfectly, we expect them to have learned good representations. To confirm this, we start by visualizing in figure 3 the learnable parameters W,U of the network hW,U after training. The embedding matrix W contains nw = 1200 columns. Each column is a vector in R100 and corresponds to a word embedding. The top panel of figure 3 depicts these 1200 word embeddings after dimensionality reduction via PCA. The top singular values \u03c31 = 34.9, \u03c32 = 34.7 and \u03c33 = 0.001 associated with the PCA indicate that the word embeddings essentially live in a 2 dimensional subspace of R100, and so the PCA paints an accurate picture of the distribution of word embeddings. We then color code each word embedding accorded to its concept, so that all embeddings of words within a concept receive the same color (say all veggie words in green, all dairy words in blue, and so forth). As the figure illustrates, words from the same concept receive nearly identical embeddings, and\nthese embeddings form an equilateral triangle or two-dimensional simplex. We therefore observe collapse of features into a set of nc = 3 equi-angular vectors at the level of word embeddings. The bottom panel of figure 3 illustrates collapse for the parameters U of the linear layer. We partition the matrix U into vectors uk,\u2113 \u2208 R100 via (1) and visualize them once again with PCA. As for the word embeddings, the singular values of the PCA (\u03c31 = 34.9, \u03c32 = 34.6 and \u03c33 = 0.0003) reveal\nthat the vectors uk,\u2113 essentially live in a two dimensional subspace of R100. We color code each uk,\u2113 according to the concepts contained in the corresponding latent variable (say uk,\u2113 is green if zk,\u2113 = veggie, and so forth). The figure indicates that vectors uk,\u2113 that correspond to a same concept collapse around a single vector. A similar analysis applied to the weights of the network h\u2217W,U tells the same story, provided we examine the actual word features (i.e. the embeddings after the LayerNorm) rather than the weights W themselves.\nIn theorem 1 and 3 (see section 3) we prove the correctness of this empirical picture. We show that the weights of h and h\u2217 collapse into the configurations illustrated on figure 3 in the large sample limit. Moreover, this limit captures the empirical solution very well. For example, the word embeddings in figure 3 have a norm equal to 1.41 \u00b1 0.13, while we predict a norm of 1.42214 theoretically."
        },
        {
            "heading": "2.2 THE LONG-TAILED CASE",
            "text": "At a superficial glance it appears as if the LayerNorm module plays no essential role, as both networks h and h\u2217, in the previous experiment, exhibit feature collapse and generalize perfectly. To probe this issue further, we continue our investigation by conducting a similar experiment (keeping nc = 3, nw = 1200, L = 15, and K = 1000) but with non-uniform, long-tailed word distributions within each of the nc = 3 concepts. For concreteness, say the veggie concept contains the 400 words\npotato, lettuce, . . . . . . , arugula, parsnip, . . . . . . , achojcha\nwhere achojcha is a rare vegetable that grows in the Andes mountains. We form the veggie distribution by sampling potato with probability C/1, sampling lettuce with probability C/2, and so forth down to achojcha that has probability C/400 of being sampled (C is chosen so that all the probabilities sum to 1). This \u201c1/i\u201d power law distribution has a long-tail, meaning that relatively infrequent words such as arugula or parsnip collectively capture a significant portion of the mass. Natural data in the form of text or images typically exhibit long-tailed distributions (Salakhutdinov et al., 2011; Zhu et al., 2014; Liu et al., 2019; Feldman, 2020; Feldman & Zhang, 2020). For instance, the frequencies of words in natural text approximately conform to the \u201c1/i\u201d power law distribution, also known as Zipf\u2019s law (Zipf, 1935), which motivates the specific choice made in this experiment. Many datasets of interest display some form of long-tail behavior, whether at the level of object occurrences in computer vision or the frequency of words or topics in NLP, and effectively addressing these long-tail behaviors is frequently a challenge for the learner.\nTo investigate the impact of a long-tailed word distributions, we first randomly select the latent variables z1, . . . , z1000 uniformly at random as before. We then use them to build two distinct training sets. We build a large training set by generating nspl = 500 training points per latent variable and a small training set by generating nspl = 5 training points per latent variable. We use the \u201c1/i\u201d power law distribution when sampling words from concepts in both cases. We then train h and h\u2217 on both training sets and evaluate their generalization performance. When trained on the large training set, both are 100% accurate at test time (as they should be \u2014 the large training set has 500, 000 total samples). A significant difference emerges between h and h\u2217 when trained on the small training set. The network h achieves a test accuracy of 45% while h\u2217 remains 100% accurate.\nWe once again visualize the weights of each network to study the relationship between generalization and collapse. Figure 4(a) depicts the weights of hW,U (via dimensionality reduction and color coding) after training on the large training set. The word embeddings are on the left sub-panel and the linear weights uk, \u2113 on the right sub-panel. Words that belong to the same concept still receive\nembeddings that are aligned, however, the magnitude of these embeddings depends upon word frequency. The most frequent words in a concept (e.g. potato) have the largest embeddings while the least frequent words (e.g. achojcha) have the smallest embeddings. In other words, we observe \u2018directional collapse\u2019 of the embeddings, but the magnitudes do not collapse. In contrast, the linear weights uk,\u2113 mostly concentrate around three well-defined, equi-angular locations; they collapse in both direction and magnitude. A major contribution of our work (c.f. theorem 2 in the next section) is a theoretical insight that explains the configurations observed in figure 4(a), and in particular, explains why the magnitudes of word embeddings depend on their frequencies.\nFigure 4(b) illustrates the weights of hW,U after training on the small training set. While the word embeddings exhibit a similar pattern as in figure 4(a), the linear weights uk,\u2113 remain dispersed and fail to collapse. This leads to poor generalization performance (45% accuracy at test time).\nTo summarize, when the training set is large, the linear weights uk,\u2113 collapse correctly and the network hW,U generalizes well. When the training set is small the linear weights fail to collapse, and the network fails to generalize. This phenomenon can be attributed to the long-tailed nature of the word distribution. To see this, say that zk = [ veggie, dairy, veggie, . . . , meat, dairy ] represents the kth latent variable for the sake of concreteness. With only nspl = 5 samples for this latent variable, we might end up in a situation where the 5 words selected to represent the first occurrence of the veggie concept have very different frequencies than the five words selected to represent the third occurrence of the veggie concept. Since word embeddings have magnitudes that depend on their frequencies, this will result in a serious imbalance between the vectors uk,1 and uk,3 that code for the first and third occurrence of the veggie concept. This leads to two vectors uk,1, uk,3 that code for the same concept but have different magnitudes (as seen on figure 4(b)), so features do not properly collapse. This imbalance results from the \u2018noise\u2019 introduced by sampling only 5 training points per latent variable. Indeed, if nspl = 500 then each occurrence of the veggie concept will exhibit a similar mix of frequent and rare words, uk, 1 and uk, 3 will have roughly same magnitude, and full collapse will take place (c.f. figure 4(a)). Finally, the poor generalization ability of hW,U when the training set is small really stems from the long-tailed nature of the word distribution. The failure mechanism occurs due to the relatively balanced mix of rare and frequent words that occurs with long-tailed data. If the data were dominated by a few very frequent words, then all rare words combined would just contribute small perturbations and would not adversely affect performance.\nWe conclude this section by examining the weights of the network h\u2217W,U after training on the small training set. The left panel of figure 4(c) provides a visualization of the word embeddings after the LayerNorm module. These word representations collapse both in direction and magnitude; they do not depend on word frequency since the LayerNorm forces vectors to have identical magnitude. The right panel of figure 4(c) depicts the linear weights uk,\u2113 and shows that they properly collapse. As a consequence, h\u2217W,U generalizes perfectly (100% accurate) even with only nspl = 5 sample per class. Normalization plays a crucial role by ensuring that word representations do not depend upon word frequency. In turn, this prevents the undesired mechanism that causes hW,U to have uncollapsed linear weights uk,\u2113 when trained on the small training set. Theorem 3 in the next section proves the correctness of this picture. The weights of the network h\u2217 collapse to the \u2018frequency independent\u2019 configuration of figure 4(c) in the large sample limit."
        },
        {
            "heading": "3 THEORY",
            "text": "Our main contributions consist in three theorems. In theorem 1 we prove that the weights of the network hW,U collapse into the configurations depicted on figure 3 when words have identical frequencies. In theorem 2 we provide theoretical justification of the fact that, when words have distinct frequencies, the word embeddings of hW,U must depend on frequency in the manner that figure 4(a) illustrates. Finally, in theorem 3 we show that the weights of the network h\u2217W,U exhibit full collapse even when words have distinct frequencies. Each of these theorems hold in the large nspl limit and under some symmetry assumptions on the latent variables. All proofs are in the appendix.\nNotation. The set of concepts, which up to now was C = {veggie, dairy, meat}, will be represented in this section by the more abstract C = {1, . . . , nc}. We let sc := nw/nc denote the number of words per concept, and represent the vocabulary by\nV = { (\u03b1, \u03b2) \u2208 N2 : 1 \u2264 \u03b1 \u2264 nc and 1 \u2264 \u03b2 \u2264 sc }\nSo elements of V are tuples of the form (\u03b1, \u03b2) with 1 \u2264 \u03b1 \u2264 nc and 1 \u2264 \u03b2 \u2264 sc, and we think of the tuple (\u03b1, \u03b2) as representing the \u03b2th word of the \u03b1th concept. Each concept \u03b1 \u2208 C comes equipped with a probability distribution p\u03b1 : {1, . . . , sc} \u2192 [0, 1] over the words within it, so that p\u03b1(\u03b2) is the probability of selecting the \u03b2th word when sampling out of the \u03b1th concept. For simplicity we assume that the word distributions within each concept follow identical laws, so that\np\u03b1(\u03b2) = \u00b5\u03b2 for all (\u03b1, \u03b2) \u2208 V for some positive scalars \u00b5\u03b2 > 0 that sum to 1. We think of \u00b5\u03b2 as being the \u2018frequency\u2019 of word (\u03b1, \u03b2) in the vocabulary. For example, choosing \u00b5\u03b2 = 1/sc gives uniform word distributions while \u00b5\u03b2 \u221d 1/\u03b2 corresponds to Zipf\u2019s law. We use X := VL to denote the data space and Z := CL to denote the latent space. The elements of the data space X correspond to sequences x = [(\u03b11, \u03b21), . . . , (\u03b1L, \u03b2L)] of L words, while elements of the latent space Z correspond to sequences z = [\u03b11, . . . , \u03b1L] of L concepts. For a given latent variable z we write x \u223c Dz to indicate that the data point x was generated by z (formally Dz : X \u2192 [0, 1] is a probability distribution). Word embeddings, LayerNorm, and word representations. We use w(\u03b1,\u03b2) \u2208 Rd to denote the embedding of word (\u03b1, \u03b2) \u2208 V . The collection of all w(\u03b1,\u03b2) determines the columns of the matrix W \u2208 Rd\u00d7nw . These embeddings feed into a LayerNorm module without learnable parameters:\n\u03c6(v) = v\u2212mean(v)1d\u03c3(v) where mean(v) = 1 d \u2211d i=1 vi and \u03c3 2(v) = 1d \u2211d i=1 ( vi\u2212mean(v) )2 .\nSo the LayerNorm module converts a word embedding w(\u03b1, \u03b2) \u2208 Rd into a vector \u03c6(w(\u03b1, \u03b2)) \u2208 Rd, and we call this vector a word representation.\nEquiangular vectors. We call a collection of nc vectors f1, . . . , fnc \u2208 Rd equiangular if nc\u2211 \u03b1=1 f\u03b1 = 0 and \u27e8f\u03b1, f\u03b1\u2032\u27e9 = { 1 if \u03b1 = \u03b1\u2032 \u22121/(nc \u2212 1) otherwise (4)\nhold for all possible pairs \u03b1, \u03b1\u2032 \u2208 [nc] of concepts. For example, three vectors f1, f2, f3 \u2208 R100 are equiangular exactly when they have unit norms, live in a two dimensional subspace of R100, and form the vertices of an equilateral triangle in this subspace. This example exactly corresponds to the configurations in figure 3 and 4 (up to a scaling factor). Similarly, four vectors f1, f2, f3, f4 \u2208 R100 are equiangular when they have unit norms and form the vertices of a regular tetrahedron. We will sometimes require f1, . . . , fnc \u2208 Rd to also satisfy \u27e8f\u03b1,1d\u27e9 = 0 for all \u03b1 \u2208 [nc], in which case we say f1, . . . , fnc \u2208 Rd form a collection of mean-zero equiangular vectors. Collapse configurations. Our empirical investigations reveal two distinct candidate solutions for the weights (W, U) of the network hW,U and h\u2217W,U . We therefore isolate each of these possible candidates as a definition before turning to the statements of our main theorems. We begin by defining the type of collapse observed when training the network hW,U with uniform word distributions (see figure 3 for a visual illustration of this type of collapse). Definition 1 (Type-I Collapse). The weights (W,U) of the network hW,U form a type-I collapse configuration if and only if the conditions\ni) There exists c \u2265 0 so that w(\u03b1,\u03b2) = c f\u03b1 for all (\u03b1, \u03b2) \u2208 V .\nii) There exists c\u2032 \u2265 0 so that uk,\u2113 = c\u2032 f\u03b1 for all (k, \u2113) satisfying zk,\u2113 = \u03b1 and all \u03b1 \u2208 C.\nhold for some collection f1, . . . , fnc \u2208 Rd of equiangular vectors.\nRecall that the network h\u2217W,U exhibits collapse as well, up to the fact that the word representations \u03c6(w\u03b1, \u03b2) collapse rather than the word embeddings themselves. Additionally, the LayerNorm also fixes the magnitude of the word representations. We isolate these differences in the next definition. Definition 2 (Type-II Collapse). The weights (W,U) of the network h\u2217W,U form a type-II collapse configuration if and only if the conditions\ni) \u03c6(w(\u03b1,\u03b2)) = \u221a d f\u03b1 for all (\u03b1, \u03b2) \u2208 V .\nii) There exists c \u2265 0 so that uk,\u2113 = c f\u03b1 for all (k, \u2113) satisfying zk,\u2113 = \u03b1 and all \u03b1 \u2208 C.\nhold for some collection f1, . . . , fnc \u2208 Rd of mean-zero equiangular vectors.\nFinally, when training the network hW,U with non-uniform word distributions (c.f. figure 4(a)) we observe collapse in the direction of the word embeddings w(\u03b1, \u03b2) but their magnitudes depend upon word frequency. We therefore isolate this final observation as Definition 3 (Type-III Collapse). The weights (W,U) of the network hW,U form a type-III collapse configuration if and only if\ni) There exists positive scalars r\u03b2 \u2265 0 so that w(\u03b1, \u03b2) = r\u03b2 f\u03b1 for all (\u03b1, \u03b2) \u2208 V .\nii) There exists c \u2265 0 so that uk,\u2113 = c f\u03b1 for all (k, \u2113) satisfying zk,\u2113 = \u03b1 and all \u03b1 \u2208 C.\nhold for some collection f1, . . . , fnc \u2208 Rd of equiangular vectors.\nIn a type-III collapse we allow the word embedding w(\u03b1, \u03b2) to have a frequency-dependent magnitude r\u03b2 while in type-I collapse we force all embeddings to have the same magnitude; this makes type-I collapse a special case of type-III collapse, but not vice-versa."
        },
        {
            "heading": "3.1 PROVING COLLAPSE",
            "text": "Our first result proves that the words embeddings w(\u03b1,\u03b2) and linear weights uk,\u2113 exhibit type-I collapse in an appropriate large-sample limit. When turning from experiment (c.f. figure 3) to theory we study the true risk\nR(W,U) = 1 K K\u2211 k=1 E x\u223cDzk [ \u2113(hW,U (x), k) ] + \u03bb 2 ( \u2225W\u22252F + \u2225U\u22252F ) (5)\nrather than the empirical risk Remp(W,U) and place a symmetry assumption on the latent variables. Assumption 1 (Latent Symmetry). For every k \u2208 [K], r \u2208 [L], \u2113 \u2208 [L], and \u03b1 \u2208 [nc] the identities\u2223\u2223\u2223{k\u2032 \u2208 [K] : dist(zk, zk\u2032) = r and zk\u2032,\u2113 = \u03b1}\u2223\u2223\u2223 = { K|Z|(L\u22121r )(nc \u2212 1)r if zk,\u2113 = \u03b1K\n|Z| ( L\u22121 r\u22121 ) (nc \u2212 1)r\u22121 if zk,\u2113 \u0338= \u03b1\n(6)\nhold, with dist(zk, zk\u2032) denoting the Hamming distance between a pair (zk, zk\u2032 ) of latent variables.\nWith this assumption in hand we may state our first main result Theorem 1 (Full Collapse of h). Assume uniform sampling \u00b5\u03b2 = 1/sc for each word distribution. Let \u03c4 \u2265 0 denote the unique minimizer of the strictly convex function H(t) :=\nlog ( 1\u2212 K\nnLc + K nLc\n( 1 + (nc \u2212 1)e\u2212\u03b7t )L) + \u03bbt where \u03b7 = ncnc\u22121 1\u221a nwKL . Assume z1, . . . , zK\nare mutually distinct and satisfy the symmetry assumption 1. Then any (W,U) in a type-I collapse configuration with constants c = \u221a \u03c4/nw and c\u2032 = \u221a \u03c4/(KL) is a global minimizer of (5).\nWe also prove two strengthenings of this theorem in the appendix. First, under an additional technical assumption on the latent variables z1, . . . , zK we prove its converse; any (W,U) that minimizes (5) must be in a type-I collapse configuration (with the same constants c, c\u2032). This additional assumption is mild but technical, so we state it in appendix C. We also prove that if d > nw then R(W,U) does not have spurious local minimizers; all local minimizers are global (see appendix H). The symmetry assumption, while odd at a first glance, is both needed and natural. Indeed, a type-I collapse configuration is highly symmetric and perfectly homogeneous. We therefore expect that such configurations could only solve an analogously \u2018symmetric\u2019 and \u2018homogeneous\u2019 optimization problem. In our case this means using the true risk (5) rather than the empirical risk (2), and imposing that the latent variables satisfy the symmetry assumption. This assumption means that all latent variables play interchangeable roles, or at an intuitive level, that there is no \u2018preferred\u2019 latent variable. To understand this better, consider the extreme case K = nLc and {z1, . . . , zK} = Z , meaning that all latent variables in Z are involved in the task. The identity (6) then holds by simple combinatorics. We may therefore think of (6) as an equality that holds in the large K limit, so it is neither impossible nor unnatural. We refer to appendix C for more discussion about assumption 1.\nWhile theorem 1 proves global optimality of type-I collapse configurations in the limit of large nspl and large K, these solutions still provide valuable predictions when K and nspl have small to\nmoderate values. For example, in the setting of figure 3 (nspl = 5 and K = 1000) the theorem predicts that word embeddings should have a norm c = \u221a \u03c4/nw = 1.42214 (with \u03c4 obtained by minimizing H(t) numerically). By experiment we find that, on average, word embeddings have norm 1.41 with standard deviation 0.13. To take another example, when K = 50 and nspl = 100 (and keeping nc = 3, nw = 1200, L = 15) the theorem predicts that words embeddings should have norm 0.61602. This compares well against the values 0.61 \u00b1 0.06 observed in experiments. The idealized solutions of the theorem capture their empirical counterparts very well.\nFor non-uniform \u00b5\u03b2 we expect hW,U to exhibit type-III collapse rather than type-I collapse. Additionally, in our long-tail experiments, we observe that frequent words (i.e. large \u00b5\u03b2) receive large embeddings. We now prove that this is the case in our next theorem. To state it, consider the following system of sc + 1 equations\n\u03bb L r\u03b2 c ( nc \u2212 1 + exp ( nc nc\u22121c r\u03b2 )) = \u00b5\u03b2 for all 1 \u2264 \u03b2 \u2264 sc (7)\u2211sc\n\u03b2=1 ( r\u03b2 c )2 = LnL\u22121c (8)\nfor the unknowns (c, r1, . . . , rsc). If the regularization parameter \u03bb is small enough, namely \u03bb 2 <\nL nL+1c\n\u2211sc \u03b2=1 \u00b5 2 \u03b2 , then (7)\u2013(8) has a unique solution. This solution defines the magnitudes of the word\nembeddings. The left hand side of (7) is an increasing function of r\u03b2 , so \u00b5\u03b2 < \u00b5\u03b2\u2032 implies r\u03b2 < r\u03b2\u2032 and more frequent words receive larger embeddings. Theorem 2 (Directional Collapse of h). Assume \u03bb2 < (L/nL+1c ) \u2211sc \u03b2=1 \u00b5 2 \u03b2 , K = n L c and {z1, . . . , zK} = Z . Suppose (W, U) is in a type-III collapse configuration for some constants (c, r1, . . . , rsc). Then (W,U) is a critical point of the true risk (5) if and only if (c, r1, . . . , rsc) solve the system (7)\u2013(8).\nEssentially this theorem shows that word embeddings must depend on word frequency and so feature collapse fails. Even in the fully-sampled case K = nLc and {z1, . . . , zK} = Z a network exhibiting type-I collapse is never critical if the word distributions are non-uniform. While we conjecture global optimality of the solutions in theorem 2 under appropriate symmetry assumptions, we have no proof of this yet. The bound on \u03bb is the natural one for theorem 2, for if \u03bb is too large the trivial solution (W,U) = (0, 0) is the only one. In our experiments, \u03bb satisfies this bound.\nOur final theorem completes the picture; it shows that normalization restores global optimality of fully-collapsed configurations. For the network h\u2217W,U with LayerNorm, we use the appropriate limit\nR\u2217(W,U) = 1 K K\u2211 k=1 E x\u223cDzk [ \u2113(h\u2217W,U (x), k) ] + \u03bb 2 \u2225U\u22252F (9)\nof the associated empirical risk and place no assumptions on the sampling distribution. Theorem 3 (Full Collapse of h\u2217). Assume the non-degenerate condition \u00b5\u03b2 > 0 holds. Let \u03c4 \u2265 0 denote the unique minimizer of the strictly convex function H\u2217(t) =\nlog ( 1\u2212 K\nnLc + K nLc\n( 1 + (nc \u2212 1)e\u2212\u03b7 \u2217t )L)\n+ \u03bb2 t 2 where \u03b7\u2217 = ncnc\u22121 1\u221a KL/d . Assume z1, . . . , zK\nare mutually distinct and satisfy assumption 1. Then any (W, U) in a type-II collapse configuration with constant c = \u03c4/ \u221a KL is a global minimizer of (9).\nAs for theorem 1, we prove the converse under an additional technical assumption on the latent variables. Any (W,U) that minimizes (9) must be in a type-II collapse configuration with c = \u03c4/ \u221a KL. The proof and exact statement can be found in section F of the appendix."
        },
        {
            "heading": "4 ADDITIONAL EXPERIMENTS",
            "text": "Our theoretical investigation of feature collapse uses a simple synthetic data model and a basic network. These simplifications allow us to rigorously prove that feature collapse occurs in this setting. The first section of the appendix provides preliminary evidence that the feature collapse phenomenon also occurs in more complex settings, which are beyond the reach of our current analytical tools. In particular, we experimentally observe feature collapse in more complex data models that involve a deeper hierarchy of latent structures. We also investigate the feature collapse phenomenon in transformer architectures in both a classification setup and the usual next-word-prediction setup."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "Xavier Bresson is supported by NUS Grant ID R-252-000-B97-133. The authors would like to express their gratitude to the reviewers for their feedback, which has improved the clarity and contribution of the paper."
        },
        {
            "heading": "Appendix",
            "text": "In section A we conduct an empirical investigation of the feature collapse phenomenon in settings beyond the reach of our current analytical tools. The remaining sections are all devoted to the proofs of the three theorems that constitute the main results of the paper.\nSection B provides formulas for the networks hW,U and h\u2217W,U depicted on figure 2 of the main paper, and formula for the distribution Dzk : X \u2192 [0, 1] underlying the data model depicted on figure 1 of the main paper. We also use this section to introduce various notations that our proofs will rely on.\nSection C is devoted to the symmetry assumptions that we impose on the latent variables. We start with an in depth discussion of assumption 1. from the main paper. This assumption is required for theorem 1 and 3 to hold. We then present and discuss an additional technical assumption on the latent variables (c.f. assumption B) that we will use to prove the converse of theorems 1 and 3.\nWhereas sections B and C are essentially devoted to notations and discussions, most of the analysis occurs in section D, E, F and G. We start by deriving a sharp lower bound for the unregularized risk in section D. Theorem 1 from the main paper, as well as its converse, are proven in section E. Theorem 3 and its converse are proven in section F. Finally we prove theorem 2 in section G.\nWe conclude this appendix by proving in section H that if d > min(nw,KL), then the risk associated to the network hW,U does not have spurious local minimizers; all local minimizers are global. This proof follows the same strategy that was used in Zhu et al. (2021)."
        },
        {
            "heading": "A FURTHER EMPIRICAL INVESTIGATIONS",
            "text": "In this section, show empirically that the feature collapse phenomenon is not limited to the simple controlled setting where we were able to prove it. In particular, we show that feature collapse occurs in the absence of weight decay and when the LayerNorm has learnable parameters. We also show that feature collapse occurs in transformer architectures, and also when the classification task is replaced with a language modeling task. Finally, we show that feature collapse occurs in data models involving a deeper hierarchy of latent structures, such as a Context Free Grammar (CFG).\nA.1 EXPERIMENTS WITHOUT WEIGHT DECAY\nWe start by reproducing the experiments depicted in Figure 3 and 4 but without weight decay. Specifically, we set \u03bb = 0 in equations (2) and (3). All other parameters defining the networks and data model remain the same. To train the networks, we perform 5 million iterations of stochastic gradient descent with a batch size of 100 and a learning rate of 0.1. After training, the empirical losses for all networks are below 10\u22124. The outcomes of these experiments are depicted in Figures 5 and 6. These figures are virtually identical to Figures 3 and 4 in the main paper. In other words, the absence or presence of weight decay does not affect our main qualitative findings."
        },
        {
            "heading": "A.2 LAYERNORM WITH LEARNABLE PARAMETERS",
            "text": "When the LayerNorm module has no learnable weights, the word embeddings must lie on a sphere of constant radius. This constraint aids collapse. A natural question is whether collapse still occurs when the LayerNorm has learnable parameters. To answer this question, we reproduce the experiment that corresponds to Figure 4(c) but allow the LayerNorm module to have learnable weights. The result of this experiment is depicted on Figure 7, and one can clearly observe that feature collapse does occur in this setting as well."
        },
        {
            "heading": "A.3 CLASSIFICATION EXPERIMENTS WITH TRANSFORMERS",
            "text": "In this set of experiments, we train a transformer on the classification task depicted on figure 1. The transformer has 2 layers, 8 heads, 512 dimensions, and we use absolute positional embeddings (as in GPT-2). A classification token is appended to each input sentence, and this classification token is used in the last layer to predict the category. The network is trained with AdamW (constant learning rate of 10\u22124, weight decay of 0.1, \u03b21 = 0.9 and \u03b22 = 0.95) during 3 epochs on a training sets containing 0.5 million sentences. For the data model, we use nc = 3, nw = 1200, L = 15, K = 1000 as in the main paper. In figure 8(a) and 8 (b) we display the word embeddings via dimensionality reduction and color coding. These are the word embeddings obtained before addition of the positional embeddings, and before going through the first transformer layer. Figure 8(a) corresponds to the case in which the words are uniformly distributed, and Figure 8(b) corresponds to the long-tail case. We observe that the word embeddings, in both the uniform and long-tail case, are properly collapsed."
        },
        {
            "heading": "A.4 LANGUAGE MODELING EXPERIMENTS WITH TRANSFORMERS",
            "text": "In this set of experiments, we train a transformer to predict the next token on sentences generated by the data model depicted on Figure 1. We use the GPT-2 architecture (Radford et al., 2019) with 2 layers, 8 heads, and 512 dimensions. The training set contains 1 million sentences generated by our data model with parameters nc = 3, nw = 1200, L = 15, K = 1000, and with uniform word distributions. We perform a single epoch through the training set and use AdamW with same parameters as above. On figure 8 (c) we display the word embeddings via dimensionality reduction and color coding, and we observe that they are they are properly collapsed."
        },
        {
            "heading": "A.5 EXPERIMENTS WITH CONTEXT FREE GRAMMAR",
            "text": "The data model presented in the paper extends to one with a deeper hierarchy of latent structures. Recall that words are partitioned into concepts and that the latent variables are sequences of concepts. We can further partitioned the latent variables into \u2018meta-concepts\u2019 and create deeper latent variables that are sequences of \u2018meta-concepts\u2019. We can iterate this process to obtain a hierarchy of any depth. Such a data model is a particular instance of a Context Free Grammar (Chomsky, 1956), which generates sentences with probabilistic trees and are widely used to understand natural\nlanguage models (e.g. Kim et al. (2019); Allen-Zhu & Li (2023); Liu et al. (2023)). In Figure 9 we provide an illustration of a simple depth 3 context free grammar.\nWe ran experiments with a context free grammar of depth 4, meaning that we have words, concepts, meta-concepts and meta-meta-concepts. We used a deep neural network with ReLU nonlinearities and LayerNorm module at each layer. The architecture of the neural network was chosen to match that of the context free grammar, see Figure 10. In Figure 11 we plot the activations after each of the three hidden layers and readily observe the expected feature collapse phenomenon. All segments of the input sentence that correspond to same concept, meta-concept, or meta-meta-concept receive the same representations in the appropriate layer of the network (layer 1 for concepts, layer 2 for metaconcepts, and layer 3 for meta-meta-concepts). This shows that the feature collapse phenomenon is a general one.\nDetails of the Context Free Grammar: We used the following context free grammar for our experiment. We chooseK = 100 categories. Each category generates a sequence of \u2018meta-meta-concepts\u2019 of length 8 by choosing uniformly at random among 5 possible sequences of meta-meta-concepts. Each meta-meta-concept then generates a sequence of meta-concepts of length 8, again by choosing uniformly at random among 5 possible sequences of meta-concepts. Each meta-concept then generates a sequence of concepts of length 8 by choosing among 5 possible sequences of concepts. Finally, each concept generates a sequence of words of length 8 by choosing among 5 possible sequences of words. At level 0 the sequences of words have an overall length of 84 = 4096."
        },
        {
            "heading": "B PRELIMINARIES AND NOTATIONS",
            "text": ""
        },
        {
            "heading": "B.1 FORMULA FOR THE NEURAL NETWORKS",
            "text": "Recall that the vocabulary is the set\nV = {(\u03b1, \u03b2) \u2208 N2 : 1 \u2264 \u03b1 \u2264 nc and 1 \u2264 \u03b2 \u2264 sc},\nand that we think of the tuple (\u03b1, \u03b2) \u2208 V as representing the \u03b2th word of the \u03b1th concept. The data space is X = VL, and a sentence x \u2208 X is a sequence of L words:\nx = [(\u03b11, \u03b21), . . . , (\u03b1L, \u03b2L)] 1 \u2264 \u03b1\u2113 \u2264 nc and 1 \u2264 \u03b2\u2113 \u2264 sc.\nThe two neural networks h, h\u2217 studied in this work process such a sentence x \u2208 X in multiple steps:\n1. Each word (\u03b1\u2113, \u03b2\u2113) of the sentence is encoded into a one-hot vector.\n2. These one-hot vectors are multiplied by a matrix W to produce word embeddings that live in a d-dimensional space.\n3. Optionally (i.e. in the case of the network h\u2217), these word embeddings go through a LayerNorm module without learnable parameters.\n4. The word embeddings are concatenated and then goes through a linear transformation U .\nWe now formalize these 4 steps, and in the process, we set the notations on which we will rely in all our proofs.\nStep 1: One-hot encoding. Without loss of generality, we choose the following one-hot encoding scheme: word (\u03b1, \u03b2) \u2208 V receives the one-hot vector which has a 1 in entry (\u03b1 \u2212 1)sc + \u03b2 and 0 everywhere else. To formalize this, we define the one-hot encoding function\n\u03b6(\u03b1, \u03b2) = e(\u03b1\u22121)sc+\u03b2 (10)\nwhere ei denotes the ith basis vector of Rnw . The one-hot encoding function \u03b6 can also be applied to a sequence of words. Given a sentence x = [(\u03b11, \u03b21), . . . , (\u03b1L, \u03b2L)] \u2208 X we let\n\u03b6(x) := [ | | | \u03b6(\u03b11, \u03b21) \u03b6(\u03b12, \u03b22) . . . \u03b6(\u03b1L, \u03b2L)\n| | |\n] \u2208 Rnw\u00d7L (11)\nand so \u03b6 maps sentences to nw \u00d7 L matrices.\nStep 2: Embedding. The embedding matrix W has nw columns and each of these columns belongs to Rd. Since \u03b6(\u03b1, \u03b2) denote the one-hot vector associated to word (\u03b1, \u03b2) \u2208 V , we define the embedding of word (\u03b1, \u03b2) by\nw(\u03b1,\u03b2) :=W \u03b6(\u03b1, \u03b2) \u2208 Rd. (12)\nDue to (10), this means that w(\u03b1,\u03b2) is the jth column of W , where j = (\u03b1 \u2212 1)sc + \u03b2. The embedding matrix W can therefore be visualized as follow (for concreteness we choose nc = 3 and nw = 12 as in figure 1 of the main paper):\nW =  | | | | | | | | | | | |w(1,1) w(1,2) w(1,3) w(1,4) w(2,1) w(2,2) w(2,3) w(2,4) w(3,1) w(3,2) w(3,3) w(3,4) | | | | | | | | | | | |  \ufe38 \ufe37\ufe37 \ufe38\nEmbeddings of the words in the 1st concept.\n\ufe38 \ufe37\ufe37 \ufe38 Embeddings of the words in the 2nd concept.\n\ufe38 \ufe37\ufe37 \ufe38 Embeddings of the words in the 3rd concept.\nGiven a sentence x = [(\u03b11, \u03b21), . . . , (\u03b1L, \u03b2L)] \u2208 X , appealing to (11) and (12), we find that\nW\u03b6(x) =  | | |w(\u03b11,\u03b21) w(\u03b12,\u03b22) \u00b7 \u00b7 \u00b7 w(\u03b1L,\u03b2L) | | |  \u2208 Rd\u00d7L (13) and therefore W\u03b6(x) is the matrix that contains the d-dimensional embeddings of the words that constitute the sentence x \u2208 X .\nStep 3: LayerNorm. Recall from the main paper that the LayerNorm function \u03c6 : Rd \u2192 Rd is defined by\n\u03c6(v) = v \u2212mean(v)1d\n\u03c3(v) where mean(v) =\n1\nd d\u2211 i=1 vi and \u03c32(v) = 1 d d\u2211 i=1 ( vi\u2212mean(v) )2 ,\nWe will often apply this function column-wise to a matrix. For example if V is the d\u00d7m matrix\nV = [ | | | v1 v2 \u00b7 \u00b7 \u00b7 vm | | | ] , then \u03c6(V ) = [ | | | \u03c6(v1) \u03c6(v2) \u00b7 \u00b7 \u00b7 \u03c6(vm) | | | ] Applying \u03c6 to (13) gives\n\u03c6 ( W\u03b6(x) ) =  | | | \u03c6 ( w(\u03b11,\u03b21) ) \u03c6 ( w(\u03b12,\u03b22) ) \u00b7 \u00b7 \u00b7 \u03c6 ( w(\u03b1L,\u03b2L)\n) | | |  \u2208 Rd\u00d7L (14) and so \u03c6 (W\u03b6(x)) contains the word representations of the words from the input sentence (recall that by word representations we mean the word embeddings after the LayerNorm).\nStep 4: Linear Transformation. Recall from the main paper that\nU =  \u2014u1,1\u2014 \u2014u1,2\u2014 \u00b7 \u00b7 \u00b7 \u2014u1,L\u2014 \u2014u2,1\u2014 \u2014u2,2\u2014 \u00b7 \u00b7 \u00b7 \u2014u2,L\u2014\n... ... ... \u2014uK,1\u2014 \u2014uK,2\u2014 \u00b7 \u00b7 \u00b7 \u2014uK,L\u2014  \u2208 RK\u00d7Ld (15) where each vector uk,\u2113 belongs to Rd. The neural networks hW,U and h\u2217W,U are then given by the formula\nhW,U (x) = U Vec [W\u03b6(x)] (16) h\u2217W,U (x) = U Vec [ \u03c6 ( W\u03b6(x) )] (17)\nwhere Vec : Rd\u00d7L \u2192 RdL is the function that takes as input a d \u00d7 L matrix and flatten it out into a vector with dL entries (with the first column filling the first d entries of the vector, the second column filling the next d entries, and so forth). It will prove convenient to gather the L vectors uk,\u2113 that constitute the kth row of U into the matrix\nU\u0302k = [ | | | uk,1 uk,2 \u00b7 \u00b7 \u00b7 uk,L | | | ] \u2208 Rd\u00d7L (18)\nWith this notation, we have the following alternative expressions for the networks hW,U and h\u2217W,U\nhW,U (x) =  \u2329 U\u03021 , W \u03b6(x) \u232a F\u2329 U\u03022 , W \u03b6(x) \u232a F\n...\u2329 U\u0302K , W\u03b6(x) \u232a F\n and h\u2217W,U (x) =  \u2329 U\u03021 , \u03c6 ( W \u03b6(x) )\u232a F\u2329 U\u03022 , \u03c6 ( W \u03b6(x) )\u232a F\n...\u2329 U\u0302K , \u03c6 ( W\u03b6(x) )\u232a F\n (19)\nwhere \u27e8\u00b7, \u00b7\u27e9F denote the Frobenius inner product between matrices (see next subsection for a definition).\nFinally, we use U\u0302 to denote the matrix obtained by concatenating the matrices U\u03021, . . . , U\u0302K , that is U\u0302 := [ U\u03021 U\u03022 \u00b7 \u00b7 \u00b7 U\u0302K ] \u2208 Rd\u00d7KL (20)\nThe matrix U\u0302 , which is nothing but a reshaped version of the original weight matrix U \u2208 RK\u00d7Ld, will play a crucial role in our analysis."
        },
        {
            "heading": "B.2 BASIC PROPERTIES OF THE FROBENIUS INNER PRODUCT",
            "text": "We recall that the Frobenius inner product between two matrices A,B \u2208 Rm\u00d7n is defined by\n\u27e8A,B\u27e9F = m\u2211 i=1 n\u2211 j=1 AijBij\nand that the Frobenius norm of a matrix A \u2208 Rm\u00d7n is given by \u2225A\u2225F = \u221a\n\u27e8A,A\u27e9F . In the course of our proofs, we will constantly appeal to the following property of the Frobenius inner product, so we state it in a lemma once and for all. Lemma A. Suppose A \u2208 Rm\u00d7n, B \u2208 Rm\u00d7r and C \u2208 Rr\u00d7n. Then\n\u27e8A,BC\u27e9F = \u2329 BTA,C \u232a F\nand \u27e8A,BC\u27e9F = \u2329 ACT , B \u232a F\nProof. The Frobenius inner product can be expressed as \u27e8A,B\u27e9F = Tr(ATB), and so we have\n\u27e8A,BC\u27e9F = Tr(A TBC) = Tr\n(( BTA )T C ) = \u2329 BTA,C \u232a F .\nUsing the cyclic property of the trace, we also get\n\u27e8A,BC\u27e9F = Tr(A TBC) = Tr(CATB) = Tr\n(( ACT )T B ) = \u2329 ACT , B \u232a F\nB.3 THE TASK, THE DATA MODEL, AND THE DISTRIBUTION Dzk\nRecall that C = {1, . . . , nc} represents the set of concepts, and that Z = CL is the latent space. We aim to study a classification task in which the K classes are defined by K latent variables\nz1, . . . , zk \u2208 Z\nWe write x \u223c Dzk to indicate that the sentence x \u2208 X is generated by the latent variable zk \u2208 Z (see figure 1 of the main paper for a visual illustration). Formally, Dzk is a probability distribution on the data space X , and we now give the formula for its p.d.f. First, recall that \u00b5\u03b2 > 0 stands for the probability of sampling the \u03b2th word of the \u03b1th concept. Let us denote the kth latent variable by\nzk = [ zk,1 , zk,2 , . . . , zk,L ] \u2208 Z where 1 \u2264 zk,\u2113 \u2264 nc. The probability of sampling the sentence x = [ (\u03b11, \u03b21) , (\u03b12, \u03b22) . . . , (\u03b1L, \u03b2L) ] \u2208 X according to Dzk is then given by the formula\nDzk ({x}) = L\u220f \u2113=1 1{\u03b1\u2113=zk,\u2113} \u00b5\u03b2\u2113\nNote that Dzk ({x}) > 0 if and only if [zk,1, . . . , zk,L] = [\u03b11, . . . , \u03b1L]. So a sentence x has a nonzero probability of being generated by the latent variable zk only if its words match the concepts in zk. If this is the case, then the probability of sampling x according to Dzk is simply given by the product of the frequencies of the words contained in x.\nWe use Xk to denote the support of the distribution Dzk , that is Xk := {x \u2208 X : Dzk(x) > 0} and we note that if the latent variables z1, . . . , zK are mutually distinct, then Xj \u2229 Xk = \u2205 for all j \u0338= k. Since the K latent variables define the K classes of our classification problem, we may alternatively define Xk by\nXk = {x \u2208 X : x belongs to the kth category}\nTo each latent variable zk = [ zk,1 , zk,2 , . . . , zk,L ] we associate a matrix\nZk = [ | | | ezk,1 ezk,2 \u00b7 \u00b7 \u00b7 ezk,L | | | ] \u2208 Rnc\u00d7L (21)\nIn other words, the matrix Zk provides a one-hot representation of the concepts contained in the latent variable zk. Concatenating the matrices Z1, . . . , ZK gives the matrix Z = [Z1 Z2 \u00b7 \u00b7 \u00b7 ZK ] \u2208 Rnc\u00d7KL (22) which is reminiscent of the matrix U\u0302 defined by (20).\nWe encode the way words are partitioned into concepts into a \u2018partition matrix\u2019 P \u2208 Rnc\u00d7nw . For example, if we have 12 words and 3 concepts, then the partition matrix is\nP = [ 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 ] \u2208 Rnc\u00d7nw , (23)\nindicating that the first 4 words belong to concept 1, the next 4 words belongs to concept 2, and so forth. Formally, recalling that \u03b6(\u03b1, \u03b2) is the the one-hot encoding of word (\u03b1, \u03b2) \u2208 V , the matrix P is defined the relationship\nP \u03b6(\u03b1, \u03b2) = e\u03b1 for all (\u03b1, \u03b2) \u2208 V. (24) Importantly, note that the matrix P maps datapoints to their associated latent variables. Indeed, if x = [(\u03b11, \u03b21), . . . , (\u03b1L, \u03b2L)] is generated by the latent variable zk (meaning that x \u2208 Xk), then we have that\nP \u03b6(x) = P [ | | | \u03b6(\u03b11, \u03b21) \u03b6(\u03b12, \u03b22) . . . \u03b6(\u03b1L, \u03b2L)\n| | |\n] = [ | | | e\u03b11 e\u03b12 . . . e\u03b1L | | | ] = Zk (25)\nwhere the last equality is due to definition (21) of the matrix Zk.\nAnother important matrix for our analysis will be the matrix Q \u2208 Rnc\u00d7nw . In the concrete case where we have 12 words and 3 concepts, this matrix takes the form\nQ = [ \u00b51 \u00b52 \u00b53 \u00b54 0 0 0 0 0 0 0 0 0 0 0 0 \u00b51 \u00b52 \u00b53 \u00b54 0 0 0 0 0 0 0 0 0 0 0 0 \u00b51 \u00b52 \u00b53 \u00b54 ] \u2208 Rnc\u00d7nw (26)\nand, in general, it is defined by the relationship Q \u03b6(\u03b1, \u03b2) = \u00b5\u03b2 e\u03b1 for all (\u03b1, \u03b2) \u2208 V. (27)"
        },
        {
            "heading": "C SYMMETRY ASSUMPTIONS ON THE LATENT VARIABLES",
            "text": "In subsection C.1 we provide an in depth discussion of the symmetry assumption required for theorems 1 and 3 to hold. In subsection C.2 we present and discuss the assumption that will be needed to prove the converse of theorems 1 and 3."
        },
        {
            "heading": "C.1 SYMMETRY ASSUMPTION NEEDED FOR THEOREM 1 AND 3",
            "text": "To better understand the symmetry assumption 1 from the main paper, let us start by considering the extreme case K = nLc and {z1, z2, . . . , zK} = Z, (28) meaning that z1, . . . , zK are mutually distinct and represent all possible latent variables in Z . In this case, we easily obtain the formula\u2223\u2223\u2223{j \u2208 [K] : dist(zj , z1) = r and zj,L = z1,L}\u2223\u2223\u2223 = (L\u2212 1\nr\n) (nc \u2212 1)r (29)\nwhere dist(zj , z1) is the Hamming distance between the latent variables zj and z1. To see this, note that the left side of (29) counts the number of latent variables zj that differs from z1 at r locations and agrees with z1 at the last location \u2113 = L. This number is clearly equal to the right side of (29) since we need to choose r positions out of the first L \u2212 1 positions, and then, for each chosen position \u2113, we need to choose a concept out of the nc \u2212 1 concepts that differs from z1,\u2113. A similar reasoning shows that, if z1,L \u0338= \u03b1, then\u2223\u2223\u2223{j \u2208 [K] : dist(zj , z1) = r and zj,L = \u03b1}\u2223\u2223\u2223 = (L\u2212 1\nr \u2212 1\n) (nc \u2212 1)r\u22121 (30)\nwhere the term ( L\u22121 r\u22121 )\narises from the fact that we only need to choose r \u2212 1 positions, since z1 and zj differ in their last position \u2113 = L. Suppose now that the random variables z1, . . . , zK are selected uniformly at random from Z , and say, for the sake of concreteness, that\nK = 1\n5 nLc\nso that z1, . . . , zK represent 20% of all possible latent variables (note that |Z| = nLc ). Then (29) \u2013 (30) should be replaced by\u2223\u2223\u2223{j \u2208 [K] : dist(zj , z1) = r and zj,L = z1,L}\u2223\u2223\u2223 \u2248 1\n5 ( L\u2212 1 r ) (nc \u2212 1)r (31)\u2223\u2223\u2223{j \u2208 [K] : dist(zj , z1) = r and zj,L = \u03b1}\u2223\u2223\u2223 \u2248 1\n5 ( L\u2212 1 r \u2212 1 ) (nc \u2212 1)r\u22121 for \u03b1 \u0338= z1,L (32)\nwhere the equality only holds approximatively due to the random choice of the latent variables. In the above example, we chose z1 as our \u2018reference\u2019 latent variables and we \u2018froze\u2019 the concept appearing in position \u2113 = L. These choices were clearly arbitrary. In general, when K is large, we have\n\u2223\u2223\u2223{j \u2208 [K] : dist(zj , zk) = r and zj,\u2113 = \u03b1}\u2223\u2223\u2223 \u2248  K nLc ( L\u22121 r ) (nc \u2212 1)r if zk,\u2113 = \u03b1\nK nLc ( L\u22121 r\u22121 ) (nc \u2212 1)r\u22121 if zk,\u2113 \u0338= \u03b1\n(33)\nand this approximate equality hold for most k \u2208 [K], r \u2208 [L], \u2113 \u2208 [L], and \u03b1 \u2208 [nc]. The symmetry assumption 1 from the main paper requires (33) to hold not approximatively, but exactly. For convenience we restate below this symmetry assumption: Assumption A (Latent Symmetry). For every k \u2208 [K], r \u2208 [L], \u2113 \u2208 [L], and \u03b1 \u2208 [nc] the identities\n\u2223\u2223\u2223{j \u2208 [K] : dist(zj , zk) = r and zj,\u2113 = \u03b1}\u2223\u2223\u2223 =  K nLc ( L\u22121 r ) (nc \u2212 1)r if zk,\u2113 = \u03b1\nK nLc ( L\u22121 r\u22121 ) (nc \u2212 1)r\u22121 if zk,\u2113 \u0338= \u03b1\n(34)\nhold.\nTo be clear, if the latent variables z1, . . . , zK are selected uniformly at random from Z , then they will only approximatively satisfy assumption A. Our analysis, however, is conducted in the idealized case where the latent variables exactly satisfy the symmetry assumption. Specifically, we show that, in the idealized case where assumption A is exactly satisfied, then the weightsW and U of the network are given by some explicit analytical formula. Importantly, as it is explained in the main paper, our experiments demonstrate that these idealized analytical formula provide very good approximations for the weights observed in experiments when the latent variables are selected uniformly at random.\nIn the next lemma, we isolate three properties which hold for any latent variables satisfying assumption A. Importantly, when proving collapse, we will only rely on these three properties \u2014 we will never explicitly need assumption A. We will see shortly that these three properties, in essence, amount to saying that all position \u2113 \u2208 [L] and all concepts \u03b1 \u2208 [nc] plays interchangeable roles for the latent variables. There are no \u2018preferred\u2019 \u2113 or \u03b1, and this is exactly what will allow us to derive symmetric analytical solutions.\nBefore stating our lemma, let us define the \u2018sphere\u2019 of radius r centered around the kth latent variable Sr(k) := { j \u2208 [K] : dist(zj , zk) = r } for r, k \u2208 [L] (35)\nWith this notation in hand we may now state Lemma B. Suppose the latent variables z1, . . . , zK satisfy the symmetry assumption A. Then z1, . . . , zK satisfies the following properties:\n(i) |Sr(j)| = |Sr(k)| for all r \u2208 [L] and all j, k \u2208 [K].\n(ii) The equalities K\u2211 k=1 Zk = K nc 1nc1 T L and ZZ T = KL nc Inc\nhold, with Inc denoting the nc \u00d7 nc identity matrix.\n(iii) There exists \u03b81, . . . , \u03b8L > 0 and matrices A1, . . . , AL \u2208 Rnc\u00d7L such that\nZk \u2212 1 |Sr(k)| \u2211\nj\u2208Sr(k) Zj = \u03b8rZk +Ar\nholds for all r \u2208 [L], all j \u2208 [K], and all k \u2208 [K].\nWe will prove this lemma shortly, but for now let us start by getting some intuition about properties (i), (ii) and (iii). Property (i) is transparent: it states that all latent variables have the same number of \u2018distance-r neighbors\u2019. Recalling how matrix Zk was defined (c.f. (21)), we see that the first identity of (ii) is equivalent to\n|{k \u2208 [K] : zk,\u2113 = \u03b1}| = K\nnc for all \u2113 \u2208 [L] and all \u03b1 \u2208 [nc]. (36)\nThis means that the number of latent variables that have concept \u03b1 in position \u2113 is equal to K/nc. In other words, each concept is equally represented at each position \u2113. We now turn to the second identity of statement (ii). Recalling the definition (22) of matrix Z, we see that ZZT \u2208 Rnc\u00d7nc is a diagonal matrix since each column of Z contains a single nonzero entry. One can also easily see that the \u03b1th entry of the diagonal is[\nZZT ] \u03b1,\u03b1\n= |{(k, \u2113) \u2208 [K]\u00d7 [L] : zk,\u2113 = \u03b1}|, which is the total number of times concept \u03b1 appears in the latent variables. Overall, the identity ZZT = KLnc Inc is therefore equivalent to the statement\n|{(k, \u2113) \u2208 [K]\u00d7 [L] : zk,\u2113 = \u03b1}| = KL\nnc for all \u03b1 \u2208 [nc]\nand it is therefore a direct consequence of (36).\nProperty (iii) is harder to interpret. Essentially it is a type of mean value property that states that summing over the latent variables which are at distance r of zk gives back zk. We will see that this mean value property plays a key role in our analysis.\nTo conclude this subsection, we prove lemma B.\nProof of lemma B. We start by proving statement (i). Since Sr(k) = {j \u2208 [K] : dist(zj , zk) = r}, we clearly have that\n|Sr(k)| = nc\u2211 \u03b1=1 \u2223\u2223\u2223{j \u2208 [K] : dist(zj , zk) = r and zj,\u2113 = \u03b1}\u2223\u2223\u2223 (37) We then use identity (34) and Pascal\u2019s rule to find\n|Sr(k)| = (nc \u2212 1) ( K\n|Z| ( L\u2212 1 r \u2212 1 ) (nc \u2212 1)r\u22121 ) + K |Z| ( L\u2212 1 r ) (nc \u2212 1)r\n= K\n|Z| (nc \u2212 1)r (( L\u2212 1 r \u2212 1 ) + ( L\u2212 1 r )) = K\n|Z|\n( L\nr\n) (nc \u2212 1)r (38)\nwhich clearly implies that |Sr(k)| = |Sr(j)| for all j, k \u2208 [K] and all r \u2208 [L]. We now turn to the first identity of t (ii). As previously mentioned, this identity is equivalent to (36). Choose k such that zk,\u2113 \u0338= \u03b1. Then any any latent variable zj with zj,\u2113 = \u03b1 is at least at a distance 1 of zk and we may write\n|{j \u2208 [K] : zj,\u2113 = \u03b1}| = L\u2211 r=1 \u2223\u2223\u2223{j \u2208 [K] : dist(zj , zk) = r and zj,\u2113 = \u03b1}\u2223\u2223\u2223 (39) =\nL\u2211 r=1 K nLc ( L\u2212 1 r \u2212 1 ) (nc \u2212 1)r\u22121 (40)\nwhich is equal toK/nc according to the binomial theorem. The second identity of (ii), as mentioned earlier, is a direct consequence of the first identity. We finally turn to statement (iii). Appealing to (38), we find that,\u2223\u2223\u2223{j \u2208 [K] : dist(zj , zk) = r and zj,\u2113 = \u03b1}\u2223\u2223\u2223 |Sr(k)| = K |Z| ( L\u22121 r ) (nc \u2212 1)r K |Z| ( L r ) (nc \u2212 1)r = ( L\u22121 r )( L r ) = L\u2212 r L if zk,\u2113 = \u03b1. On the other hand, if zk,\u2113 \u0338= \u03b1, we obtain\u2223\u2223\u2223{j \u2208 [K] : dist(zj , zk) = r and zj,\u2113 = \u03b1}\u2223\u2223\u2223 |Sr(k)| = K |Z| ( L\u22121 r\u22121 ) (nc \u2212 1)r\u22121 K |Z| ( L r ) (nc \u2212 1)r = 1 nc \u2212 1 ( L\u22121 r\u22121 )( L r ) = 1 nc \u2212 1 r L\nFix \u2113 \u2208 [L] and assume that zk,\u2113 = \u03b1\u22c6. We then have\n1 |Sr(k)| \u2211\nj\u2208Sr(k) ezj,\u2113 =\n1\n|Sr(k)| nc\u2211 \u03b1=1 \u2223\u2223\u2223{j \u2208 Sr(k) : zj,\u2113 = e\u03b1}\u2223\u2223\u2223 e\u03b1 =\nnc\u2211 \u03b1=1 \u2223\u2223\u2223{j \u2208 [K] : dist(zj , zk) = r and zj,\u2113 = \u03b1}\u2223\u2223\u2223 |Sr(k)| e\u03b1\n= L\u2212 r L e\u03b1\u22c6 + 1 nc \u2212 1 r L \u2211 \u03b1 \u0338=\u03b1\u22c6 e\u03b1\n= L\u2212 r L e\u03b1\u22c6 \u2212 1 nc \u2212 1 r L e\u03b1\u22c6 + 1 nc \u2212 1 r L nc\u2211 \u03b1=1 e\u03b1\n= ( 1\u2212 nc\nnc \u2212 1 r L\n) e\u03b1\u22c6 + 1\nnc \u2212 1 r L 1nc\nRecalling that zk,\u2113 = \u03b1\u22c6, the above implies that\nezk,\u2113 \u2212 1 |Sr(k)| \u2211\nj\u2208Sr(k) ezj,\u2113 =\nnc nc \u2212 1 r L ezk,\u2113 \u2212 1 nc \u2212 1 r L 1nc (41)\nFinally, recalling that\nZk = [ | | | ezk,1 ezk,2 \u00b7 \u00b7 \u00b7 ezk,L | | | ] \u2208 Rnc\u00d7L\nwe see that (41) can be written in matrix format as\nZk \u2212 1 |Sr(k)| \u2211\nj\u2208Sr(k) Zj =\nnc nc \u2212 1 r L Zk \u2212 1 nc \u2212 1 r L 1nc1 T L\nand therefore the scalars \u03b8r and the matricesAr appearing in statement (iii) are given by the formula \u03b8r =\nnc nc\u22121 r L and Ar = \u2212 1 nc\u22121 r L1nc1 T L ."
        },
        {
            "heading": "C.2 SYMMETRY ASSUMPTION NEEDED FOR THE CONVERSE OF THEOREM 1 AND 3",
            "text": "In this subsection we present the symmetry assumption that will be needed to prove the converse of theorem 1 and 3. This assumption, as we will shortly see, is quite mild and is typically satisfied even for small values of K.\nFor each pair of latent variables (zj , zk) we define the matrix\n\u0393(j,k) := Zj(Zj \u2212 Zk)T \u2208 Rnc\u00d7nc .\nWe also define A := { A \u2208 Rnc\u00d7nc : There exists a, b \u2208 R s.t. A = aInc + b1nc1Tnc } (42)\nwhich is the set of matrices whose diagonal entries are equal to some constant and whose offdiagonal entries are equal to some possibly different constant. We may now state our symmetry assumption. Assumption B. Any positive semi-definite matrix A \u2208 Rnc\u00d7nc that satisfies\u2329\nA , \u0393(j,k) \u2212 \u0393(j \u2032,k\u2032) \u232a F = 0 \u2200j, k, j\u2032, k\u2032 \u2208 [K] s.t. dist(zj , zk) = dist(zj\u2032 , zk\u2032) (43)\nmust belongs to A.\nNote that (43) can be viewed as a linear system of equations for the unknown A \u2208 Rnc\u00d7nc , with one equation for each quadruplet (j, k, j\u2032, k\u2032) satisfying dist(zj , zk) = dist(zj\u2032 , zk\u2032). To put it differently, each quadruplet (j, k, j\u2032, k\u2032) satisfying dist(zj , zk) = dist(zj\u2032 , zk\u2032) adds one equation to the system, and our assumption requires that we have enough of these equations so that all positive semi-definite solutions are constrained to live in the set A. Since a symmetric matrix has (nc + 1)nc/2 distinct entries, we would expect that (nc + 1)nc/2 quadruplets should be enough to fully determine the matrix. This number of quadruplets is easily achieved even for small values of K. So assumption B is quite mild.\nThe next lemma states that assumption B is satisfied when K = nLc . In light of the above discussion this is not surprising, since the choice K = nLc leads to a system with a number of equations much larger than (nc + 1)nc/2. The proof, however, is instructive: it simply handpicks (nc + 1)nc/2 \u2212 2 quadruplets to determine the entries of the matrix A. The \u2018\u22122\u2019 arises from the fact A is a 2 dimensional subspace, and therefore (nc + 1)nc/2\u2212 2 equations are \u2018enough\u2019 to constrain A to be in A. Lemma C. Suppose K = nLc and {z1, . . . , zK} = Z . Then z1, . . . , zK satisfy the symmetry assumption B.\nProof. Let A = CTC be a positive semi-definite matrix that solve satisfies (43). We use c\u03b1 to denote the \u03b1th column of C. Since {z1, . . . , zK} = Z , we can find i, j, k \u2208 [K] such that\nzi = [2, 1, 1, . . . , 1] \u2208 Z zj = [3, 1, 1, . . . , 1] \u2208 Z zk = [4, 1, 1, . . . , 1] \u2208 Z\nUsing lemma A and recalling the definition (21) of the matrix Zk, we get\u2329 A , \u0393(i,j) \u232a F = \u2329 CTC,Zi(Zi \u2212 Zj)T \u232a F\n= \u27e8C(Zi \u2212 Zj), CZi\u27e9F = \u27e8CZi, CZi\u27e9F \u2212 \u27e8CZj , CZi\u27e9F = ( \u27e8c2, c2\u27e9+ (L\u2212 1)\u27e8c1, c1\u27e9 ) \u2212 ( \u27e8c2, c3\u27e9+ (L\u2212 1)\u27e8c1, c1\u27e9\n) = \u27e8c2, c2\u27e9 \u2212 \u27e8c2, c3\u27e9\nSimilarly we obtain that \u2329 A , \u0393(i,k) \u232a F = \u27e8c2, c2\u27e9 \u2212 \u27e8c2, c4\u27e9 Since dist(zi, zj) = dist(zi, zk) = 1, and since A satisfies (43), we must have\u2329 A , \u0393(i,j) \u232a F = \u2329 A , \u0393(i,k) \u232a F\nwhich in turn implies that A2,3 = \u27e8c2, c3\u27e9 = \u27e8c2, c4\u27e9 = A2,4\nThis argument easily generalizes to show that all off-diagonal entries of the matrix A must be equal to some constant b \u2208 R. We now take care of the diagonal entries. Since {z1, . . . , zK} = Z , we can find i\u2032, j\u2032, k\u2032 \u2208 [K] such that\nzi\u2032 = [1, 1, . . . , 1] \u2208 Z zj\u2032 = [2, 2, . . . , 2] \u2208 Z zk\u2032 = [3, 3, . . . , 3] \u2208 Z\nAs before, we compute\u2329 A , \u0393(i \u2032,j\u2032) \u232a F = \u27e8CZi\u2032 , CZi\u2032\u27e9F \u2212 \u27e8CZj\u2032 , CZi\u2032\u27e9F = L\u27e8c1, c1\u27e9 \u2212 L\u27e8c1, c2\u27e9 = L\u27e8c1, c1\u27e9 \u2212 Lb\nwhere we have used the fact that the off diagonal entries are all equal to b. Similarly we obtain\u2329 A , \u0393(j \u2032,k\u2032) \u232a F = L\u27e8c2, c2\u27e9 \u2212 Lb\nSince dist(zi\u2032 , zj\u2032) = dist(z\u2032j , zk\u2032) = L, we must have \u2329 A,\u0393(i \u2032,j\u2032) \u232a F = \u2329 A,\u0393(j \u2032,k\u2032) \u232a F\nwhich implies that A1,1 = A2,2. This argument generalizes to show that all diagonal entries of A are equal."
        },
        {
            "heading": "D SHARP LOWER BOUND ON THE UNREGULARIZED RISK",
            "text": "In this section we derive a sharp lower bound for the unregularized risk associated with the network hW,U ,\nR0(W,U) := 1\nK K\u2211 k=1 E x\u223cDzk [ \u2113(hW,U (x), k) ] , (44)\nwhere \u2113 : RK \u2192 R is the cross entropy loss\n\u2113(y, k) = \u2212 log ( exp (yk)\u2211K j=1 exp (yj) ) for y \u2208 RK\nThe kth entry of the output y = hW,U (x) of the neural network, according to formula (19), is given by yk = \u2329 U\u0302k , W \u03b6(x) \u232a F\nRecalling that Xk is the support of the distribution Dzk : X \u2192 [0, 1], we find that the unregularized risk can be expressed as\nR0(W,U) = 1\nK K\u2211 k=1 \u2211 x\u2208Xk \u2113(hW,U (x), k) Dzk(x)\n= 1\nK K\u2211 k=1 \u2211 x\u2208Xk \u2212 log  e\u27e8U\u0302k,W\u03b6(x)\u27e9F\u2211K j=1 e \u27e8U\u0302j ,W\u03b6(x)\u27e9 F  Dzk(x) = 1\nK K\u2211 k=1 \u2211 x\u2208Xk log 1 +\u2211 j \u0338=k e\u2212\u27e8U\u0302k\u2212U\u0302j ,W\u03b6(x)\u27e9F  Dzk(x) where we did the slight abuse of notation of writing Dzk(x) instead of Dzk({x}). Note that a data points x that belongs to class k is correctly classified by the the network hW,U if and only if\u2329\nU\u0302k , W \u03b6(x) \u232a F > \u2329 U\u0302j , W \u03b6(x) \u232a F for all j \u0338= k\nWith this in mind, we introduce the following definition: Definition A (Margin). Suppose x \u2208 Xk. Then the margin between data point x and class j is\nMW,U (x, j) := \u2329 U\u0302k \u2212 U\u0302j ,W\u03b6(x) \u232a F\nWith this definition in hand, the unregularized risk can conveniently be expressed as\nR0(W,U) = 1\nK K\u2211 k=1 \u2211 x\u2208Xk log 1 +\u2211 j \u0338=k e\u2212MW,U (x,j)  Dzk(x) (45) and a data point x \u2208 Xk is correctly classified by the network if and only if the margins MW,U (x, j) are all strictly positive (for j \u0338= k). We then introduce a definition that will play crucial role in our analysis. Definition B (Equimargin Property). If dist(zk, zj) = dist(zk\u2032 , zj\u2032) =\u21d2 MW,U (x, j) = MW,U (x\u2032, j\u2032) \u2200x \u2208 Xk and \u2200x\u2032 \u2208 Xk\u2032\nthen we say that (W,U) satisfies the equimargin property.\nTo put it simply, (W,U) satisfies the equimargin property if the margin between data point x \u2208 Xk and class j only depends on dist(zk, zj). We denote by E the set of all the weights that satisfy the equimargin property\nE = {(W,U) : (W,U) satisfies the equimargin property} (46)\nand by N the set of weights for which the submatrices U\u0302k defined by (18) sum to 0,\nN = { (W,U) :\nK\u2211 k=1 U\u0302k = 0\n} (47)\nWe will work under the assumption that the latent variables z1, . . . , zK satisfy the symmetry assumption A. According to lemma B, |Sr(k)| then doesn\u2019t depend on k, and so we will simply use |Sr| to denote the size of the set Sr(k). Lemma B also states that\nZk \u2212 1 |Sr(k)| \u2211\nj\u2208Sr(k) Zj = \u03b8rZk +Ar\nfor some matrices A1, . . . , AL and some scalars \u03b81, . . . , \u03b8L > 0. We use these scalars to define\ng(x) := log ( 1 +\nL\u2211 r=1\n|Sr| e\u03b8rx/K )\n(48)\nand we note that g : R \u2192 R is a strictly increasing function. With these definitions in hand we may state the main theorem of this section.\nTheorem D. If the latent variables satisfy the symmetry assumption A, then R0(W,U) = g ( \u2212 \u2329 U\u0302 ,WQTZ \u232a F ) for all (W,U) \u2208 N \u2229 E (49)\nR0(W,U) > g ( \u2212 \u2329 U\u0302 ,WQTZ \u232a F ) for all (W,U) \u2208 N \u2229 Ec (50)\nWe recall that the matrices U\u0302 , Q, and Z where defined in section B (c.f. (20), (26) and (22)). The remainder of this section is devoted to the proof of the above theorem."
        },
        {
            "heading": "D.1 PROOF OF THE THEOREM",
            "text": "We will use two lemmas to prove the theorem. The first one (lemma D below) simply leverages the strict convexity of the various components defining the unregularized risk R0. Recall that if f : Rd \u2192 R is strictly convex, and if the strictly positive scalars p1, . . . , pn > 0 sum to 1, then\nf ( n\u2211 i=1 pivi ) \u2264 n\u2211 i=1 pif(vi) (51)\nand that equality holds if and only if v1 = v2 = . . . = vn. For this first lemma, the only property we need on the latent variables is that |Sr(k)| = |Sr(j)| = |Sr| for all j, k \u2208 [K] and all r \u2208 [L]. Define the quantity\nNW,U (r) = 1\nK\n1\n|Sr| K\u2211 k=1 \u2211 j\u2208Sr(k) \u2211 x\u2208Xk MW,U (x, j) Dzk(x) (52)\nwhich should be viewed as the averaged margin between data points and classes which are at a distance r of one another. We then have the following lemma:\nLemma D. If |Sr(k)| = |Sr(j)| for all j, k \u2208 [K] and all r \u2208 [L], then\nR0(W,U) = log ( 1 +\nL\u2211 r=1\n|Sr|e\u2212NW,U (r) )\nfor all (W,U) \u2208 E (53)\nR0(W,U) > log ( 1 +\nL\u2211 r=1\n|Sr|e\u2212NW,U (r) )\nfor all (W,U) /\u2208 E (54)\nProof. Using the strict convexity of the function f : RK\u22121 \u2192 R defined by f(v1, . . . , vk\u22121, vk+1, . . . , vK) = log ( 1 + \u2211 j \u0338=k evj )\nwe obtain\nR0(W,U) = 1\nK K\u2211 k=1 \u2211 x\u2208Xk log 1 +\u2211 j \u0338=k e\u2212M(x,j)  Dzk(x) \u2265 1 K K\u2211 k=1 log 1 +\u2211 j \u0338=k e \u2212\u2211x\u2208Xk M(x,j)Dzk (x)\n and equality holds if and only if, for all k \u2208 [K], we have that\nM(x, j) = M(y, j) for all x,y \u2208 Xk and all j \u0338= k (55)\nWe then let M(k, j) = \u2211 x\u2208Xk M(x, j)Dzk(x)\nand use the strict convexity of the exponential function to obtain\n1\nK K\u2211 k=1 log 1 +\u2211 j \u0338=k e\u2212M(k,j)  = 1 K K\u2211 k=1 log 1 + L\u2211 r=1 \u2211 j\u2208Sr(k) e\u2212M(k,j)  = 1\nK K\u2211 k=1 log 1 + L\u2211 r=1 |Sr| 1 |Sr| \u2211 j\u2208Sr(k) e\u2212M(k,j)  \u2265 1 K K\u2211 k=1 log ( 1 + L\u2211 r=1 |Sr|e\u2212 1 |Sr| \u2211 j\u2208Sr(k) M(k,j)\n) Moreover, equality holds if and only if, for all k \u2208 [K] and all r \u2208 [L], we have that\nM(k, i) = M(k, j) for all i, j \u2208 Sr(k) (56) We finally set\nM(k, r) = 1 |Sr| \u2211\nj\u2208Sr(k) M(k, j)\nand use the strict convexity of the function f(v1, . . . , vL) = log ( 1 + \u2211L r=1 |Sr|evr ) to get\n1\nK K\u2211 k=1 log\n( 1 +\nL\u2211 r=1\n|Sr|e\u2212M(k,r) ) \u2265 log ( 1 +\nL\u2211 r=1 |Sr|e\u2212 1 K \u2211K k=1 M(k,r) ) Moreover equality holds if and only if, for all k \u2208 [K] and all r \u2208 [L], we have that\nM(k, r) = M(k\u2032, r) for all k, k\u2032 \u2208 [K] and all r \u2208 [L] (57)\nImportantly, note that\n1\nK K\u2211 k=1 M(k, r) = 1 K 1 |Sr| K\u2211 k=1 \u2211 j\u2208Sr(k) \u2211 x\u2208Xk MW,U (x, j) Dzk(x)\nwhich is precisely how NW,U (r) was defined (c.f. (52)). To conclude the proof, we remark that conditions (55), (56) and (57) are all satisfied if and only if (W,U) satisfies the equi-margin property.\nWe now show that, if assumption A holds, NW,U (r) can be expressed in a simple way. Lemma E. Assume that the latent variables satisfy the symmetry assumption A. Then\nNW,U (r) = \u03b8r K\n\u2329 U\u0302 ,WQTZ \u232a F\nfor all (W,U) \u2208 N (58)\nProof. We let Xk = \u2211 x\u2208Xk \u03b6(x)Dzk(x) and note that the averaged margin can be expressed as\nNW,U (r) = 1\nK\n1\n|Sr| K\u2211 k=1 \u2211 j\u2208Sr(k) \u2211 x\u2208Xk MW,U (x, j) Dzk(x)\n= 1\nK\n1\n|Sr| K\u2211 k=1 \u2211 j\u2208Sr(k) \u2211 x\u2208Xk \u2329 U\u0302k \u2212 U\u0302j ,W\u03b6(x) \u232a F Dzk(x)\n= 1\nK\n1\n|Sr| K\u2211 k=1 \u2211 j\u2208Sr(k) \u2329 U\u0302k \u2212 U\u0302j , Xk \u232a F\n= 1\nK K\u2211 k=1 \u2329 U\u0302k,WXk \u232a F \u2212 1 K 1 |Sr| K\u2211 k=1 \u2211 j\u2208Sr(k) \u2329 U\u0302j ,WXk \u232a F\n(59)\nLet\na (r) k,j = { 1 if dist(zk, zj) = r 0 otherwise\nand rewrite the second term in (59) as\n1\nK\n1\n|Sr| K\u2211 k=1 \u2211 j\u2208Sr(k) \u2329 U\u0302j ,WXk \u232a F = 1 K 1 |Sr| K\u2211 k=1 K\u2211 j=1 a (r) k,j \u2329 Uj ,WXk \u232a F\n= 1\nK\n1\n|Sr| K\u2211 j=1 K\u2211 k=1 a (r) j,k \u2329 U\u0302k,WXj \u232a F\n= 1\nK\n1\n|Sr| K\u2211 k=1 \u2211 j\u2208Sr(k) \u2329 U\u0302k,WXj \u232a F\n= 1\nK K\u2211 k=1\n\u2329 U\u0302k , W 1 |Sr| \u2211\nj\u2208Sr(k) Xj \u232a F\nCombining this with (59) we obtain\nNW,U (r) = 1\nK K\u2211 k=1\n\u2329 U\u0302k , W ( Xk \u2212 1 |Sr| \u2211\nj\u2208Sr(k) Xj ) \u232a F\n(60)\nFrom formula (26), we see that row \u03b1 of the matrix Q is given by the formula\nQTe\u03b1 = sc\u2211 \u03b2=1 \u03b6(\u03b1, \u03b2) \u00b5\u03b2 . (61)\nWe then write zk = [\u03b11, . . . , \u03b1L] and note that the \u2113th column of Xk can be expressed as\n[ Xk ] :,\u2113 = nc\u2211 \u03b2=1 \u03b6(\u03b1\u2113, \u03b2)\u00b5\u03b2 = Q Te\u03b1\u2113 . (62)\nFrom this we obtain that Xk = Q TZk\nand therefore (60) becomes\nNW,U (r) = 1\nK K\u2211 k=1\n\u2329 U\u0302k , WQ T ( Zk \u2212 1 |Sr| \u2211\nj\u2208Sr(k) Zj ) \u232a F\n= 1\nK K\u2211 k=1 \u2329 U\u0302k , WQ T ( \u03b8rZk +Ar ) \u232a F\nwhere we have used the identity Zk\u2212 1|Sr| \u2211 j\u2208Sr(k) Zj = \u03b8rZk+Ar to obtain the second equality.\nFinally, we use the fact that \u2211 k U\u0302k = 0 to obtain\nNW,U (r) = \u03b8r K K\u2211 k=1 \u2329 U\u0302k , WQ TZk \u232a F = \u03b8r K \u2329 U\u0302 ,WQTZ \u232a F\nCombining lemma D and E concludes the proof of theorem D."
        },
        {
            "heading": "E PROOF OF THEOREM 1 AND ITS CONVERSE",
            "text": "In this section we prove theorem 1 under assumption A, and its converse under assumptions A and B. We start by recalling the definition of a type-I collapse configuration. Definition C (Type-I Collapse). The weights (W,U) of the network hW,U form a type-I collapse configuration if and only if the conditions\ni) There exists c \u2265 0 so that w(\u03b1,\u03b2) = c f\u03b1 for all (\u03b1, \u03b2) \u2208 V .\nii) There exists c\u2032 \u2265 0 so that uk,\u2113 = c\u2032 f\u03b1 for all (k, \u2113) satisfying zk,\u2113 = \u03b1 and all \u03b1 \u2208 C.\nhold for some collection f1, . . . , fnc \u2208 Rd of equiangular vectors.\nIt will prove convenient to reformulate this definition using matrix notations. Toward this goal, we define equiangular matrices as follow: Definition D. (Equiangular Matrices) A matrix F \u2208 Rd\u00d7nc is said to be equiangular if and only if the relations\nF1nc = 0 and F TF = nc nc \u2212 1 Inc \u2212 1 nc \u2212 1 1nc1 T nc\nhold.\nComparing the above definition with the definition of equiangular vectors provided in the main paper, we easily see that a matrix\nF = [ | | | f1 f2 \u00b7 \u00b7 \u00b7 fnc | | | ] \u2208 Rd\u00d7nc\nis equiangular if and only if its columns f1, . . . , fnc \u2208 Rd are equiangular. Relations (i) and (ii) defining a type-I collapse configuration can now be expressed in matrix format as\nW = c F P and U\u0302 = c\u2032 F Z for some equiangular matrix F where the matrices Z and P are given by formula (22) and (23). We then let\n\u2126Ic := { (W,U) : There exist an equiangular matrix F such that\nW = c F P and U\u0302 = c \u221a nw KL F Z } (63)\nand note that \u2126Ic is simply the set of weights (W,U) which are in a type-I collapse configuration with constant c and c\u2032 = c \u221a nw/(KL). We now state the main theorem of this section. Theorem E. Assume uniform sampling \u00b5\u03b2 = 1/sc for each word distribution. Let \u03c4 \u2265 0 denote the unique minimizer of the strictly convex function\nH(t) := log ( 1\u2212 K\nnLc + K nLc\n( 1 + (nc \u2212 1)e\u2212\u03b7t )L) + \u03bbt where \u03b7 =\nnc nc \u2212 1 1\u221a nwKL\nand let c = \u221a \u03c4/nw. Then we have the following:\n(i) If the latent variables z1, . . . , zK are mutually distinct and satisfy assumption A, then \u2126Ic \u2282 argminR\n(ii) If the latent variables z1, . . . , zK are mutually distinct and satisfy assumptions A and B, then\n\u2126Ic = argminR\nNote that (i) states that any (W,U) \u2208 \u2126Ic is a minimizer of the regularized risk \u2014 this corresponds to theorem 1 from the main paper. Statement (ii) assert that any minimizer of the regularized risk must belong to \u2126Ic \u2014 this is the converse of theorem 1. The remainder of this section is devoted to the proof of theorem E. We will assume uniform sampling\n\u00b5\u03b2 = 1/sc for all \u03b2 \u2208 [sc] everywhere in this section \u2014 all lemmas and propositions are proven under this assumption, even when not explicitly stated."
        },
        {
            "heading": "E.1 THE BILINEAR OPTIMIZATION PROBLEM",
            "text": "From theorem D, it is clear that the quantity\u2329 U\u0302 ,WQTZ \u232a F plays an important role in our analysis. In this subsection we consider the bilinear optimization problem\nmaximize \u2329 U\u0302 ,WQTZ \u232a F\n(64)\nsubject to 1\n2\n( \u2225W\u22252F + \u2225U\u0302\u22252F ) = c2 nw (65)\nwhere c \u2208 R is some constant. The following lemma identifies all solutions of this optimization problem. Lemma F. Assume the latent variables satisfy assumption A. Then (W,U) is a solution of the optimization problem (64) \u2013 (65) if and only if it belongs to the set\nBIc = { (W,U) : There exist a matrix F \u2208 Rd\u00d7nc with \u2225F\u22252F = nc\nsuch that W = c FP and U\u0302 = c \u221a nw KL FZ } (66)\nNote that the set BIc is very similar to the set \u2126Ic that defines type-I collapse configuration (c.f. (92)). In particular, since an equiangular matrix has nc columns of norm 1, it always satisfies \u2225F\u22252F = nc, and therefore we have the inclusion \u2126Ic \u2282 BIc . (67) The remainder of this subsection is devoted to the proof of the lemma.\nFirst note that the lemma is trivially true if c = 0, so we may assume c \u0338= 0 for the remainder of the proof. Second, we note that since \u00b5\u03b2 = 1/sc, then the matrices P and Q defined by (23) and (26) are scalar multiple of one another. We may therefore replace the matrix Q appearing in (64) by P , wich leads to\nmaximize \u2329 U\u0302 ,WPTZ \u232a F\n(68)\nsubject to 1\n2\n( \u2225W\u22252F + \u2225U\u0302\u22252F ) = c2 nw (69)\nWe now show that any (W, U\u0302) \u2208 BIc satisfies the constraint (69) and have objective value equal to sc c 2 \u221a KLnw.\nClaim A. If (W, U\u0302) \u2208 BIc , then 1\n2\n( \u2225W\u22252F + \u2225U\u0302\u22252F ) = c2 nw and\n\u2329 U\u0302 ,WPTZ \u232a F = c2 sc \u221a KLnw\nProof. Assume (W,U) \u2208 BIc . From definition (23) of the matrix P , we have PPT = scInc , and therefore\n\u2225W\u22252F = c2\u2225FP\u22252F = c2 \u27e8FP, FP \u27e9F = c 2 \u2329 FPPT , F \u232a F = c2 sc \u2225F\u22252F = c2 sc nc = c2 nw\nwhere we have used the fact that sc = nw/nc. Using ZZT = KLnc I from lemma B, we obtain\n\u2225FZ\u22252F = \u27e8FZ,FZ\u27e9F = \u2329 FZZT , F \u232a F =\n( KL\nnc\n) \u2225F\u22252F = KL\nAs a consequence we have\n\u2225U\u0302\u22252F = c2 nw KL \u2225FZ\u22252F = c2 nw\nand, using PPT = scInc one more time,\u2329 U\u0302 ,WPTZ \u232a F = c2 \u221a nw KL \u2329 FZ,FPPTZ \u232a F = c2 sc \u221a nw KL \u27e8FZ,FZ\u27e9F = c 2 sc \u221a KLnw\nWe then prove thatW and U\u0302 must have same Frobenius norm if they solve the optimization problem. Claim B. If (W,U) is a solution of (68) \u2013 (69), then\n\u2225W\u22252F = \u2225U\u0302\u22252F = c2 nw (70)\nProof. We prove it by contradiction. Suppose (W, U\u0302) is a solution of (64)\u2013(65) with \u2225W\u22252F \u0338= \u2225U\u0302\u22252F . Since the average of \u2225W\u22252F and \u2225U\u0302\u22252F is equal to c2nw > 0 according to the constaint, there must then exists \u03f5 \u0338= 0 such that\n\u2225W\u22252F = c2nw + \u03f5 and \u2225U\u0302\u22252F = c2nw \u2212 \u03f5 Let\nW0 =\n\u221a c2nw\nc2nw + \u03f5 W and U\u03020 =\n\u221a c2nw\nc2nw \u2212 \u03f5 U\u0302\nand note that \u2225W0\u22252F = \u2225U\u03020\u22252F = c2 nw\nand therefore (W0, U\u03020) clearly satisfies the constraint. We also have\u2329 U\u03020,W0P TZ \u232a F = \u221a c4n2w c4n2w \u2212 \u03f52 \u2329 U\u0302 ,WPTZ \u232a F > \u2329 U\u0302 ,WPTZ \u232a F\nsince \u03f5 \u0338= 0 and therefore (W, U\u0302) can not be a maximizer, which is a contradiction.\nAs a consequence of the above claim, the optimization problem (68) \u2013 (69) is equivalent to maximize \u2329 U\u0302 ,WPTZ \u232a F\n(71)\nsubject to \u2225W\u22252F = c2 nw and \u2225U\u0302\u22252F = c2 nw (72) We then have Claim C. If (W, U\u0302) is a solution of (71) \u2013 (72), then (W, U\u0302) \u2208 BIc .\nNote that according to the first claim, all (W, U\u0302) \u2208 BIc have same objective value, and therefore, according to the above claim, they must all be maximizer. As a consequence, proving the above claim will conclude the proof of lemma F.\nProof of the claim. Maximizing (71) over U\u0302 first gives\nU\u0302 = c \u221a nw WPTZ\n\u2225WPTZ\u2225F (73)\nand therefore the optimization problem (71) \u2013 (72) reduces to\nmaximize \u2225WPTZ\u22252F subject to \u2225W\u22252F = c2 nw\nUsing ZZT = KLnc I from lemma B we then get\n\u2225WPTZ\u22252F = \u2329 WPTZ,WPTZ \u232a F = \u2329 WPTZZT ,WPT \u232a F = KL\nnc \u2225WPT \u22252F\nand therefore the problem further reduces to\nmaximize \u2225WPT \u22252F subject to \u2225W\u22252F = c2 nw\nThe KKT conditions for this optimization problem are\nWPTP = \u03bdW (74)\n\u2225W\u22252F = c2 nw (75)\nwhere \u03bd \u2208 R is the Lagrange multiplier.\nAssume that (W, U\u0302) is a solution of the original optimization problem (71) \u2013 (72). Then, according to the above discussion, W must satisfy (74) \u2013 (75). Right multiplying (74) by PT , and using PPT = scInc , gives\nscWP T = \u03bdWPT\nSo either \u03bd = sc or WPT = 0. The latter is not possible since the choice WPT = 0 leads to an objective value equal to zero in the original optimization problem (71) \u2013 (72). We must therefore have \u03bd = sc, and equation (74) becomes\nW = 1\nsc WPTP (76)\nwhich can obviously be written as W = c FP\nby setting F := 1c scWP T . Since W satisfies (75) we must have\nc2 nw = \u2225W\u22252F = c2\u2225FP\u22252F = c2 \u27e8FP, FP \u27e9F = c 2 \u2329 FPPT , F \u232a F = c2 sc\u2225F\u22252F , (77)\nand so \u2225F\u22252F = nw/sc = nc.\nAccording to (73), U\u0302 bust be a scalar multiple of the matrix\nWPTZ = (cFP )PTZ = c sc FZ\nUsing the fact that ZZT = KLnc I and \u2225F\u2225 2 F = nc we then obtain that\n\u2225FZ\u22252F = \u27e8FZ,FZ\u27e9F = \u2329 FZZT , F \u232a F = KL\nnc \u2225F\u22252F = KL (78)\nand so equation (73) becomes\nU\u0302 = c \u221a nw WPTZ\n\u2225WPTZ\u2225F = c\n\u221a nw FZ\u221a KL\n(79)\nwhich concludes the proof."
        },
        {
            "heading": "E.2 PROOF OF COLLAPSE",
            "text": "Recall that the regularized risk associated with the network hW,U is defined by\nR(W,U) = R0(W,U) + \u03bb\n2\n( \u2225W\u22252F + \u2225U\u22252F ) (80)\nand recall that the set of weights in type-I collapse configuration is\n\u2126Ic = { (W,U) : There exist an equiangular matrix F such that\nW = c F P and U\u0302 = c \u221a nw KL F Z } (81)\nThis subsection is devoted to the proof of the following proposition.\nProposition A. We have the following:\n(i) If the latent variables z1, . . . , zK are mutually distinct and satisfy assumption A, then there exists c \u2208 R such that\n\u2126Ic \u2282 argminR\n(ii) If the latent variables z1, . . . , zK are mutually distinct and satisfy assumptions A and B, then any (W,U) that minimizes R must belong to \u2126Ic for some c \u2208 R.\nThis proposition states that, under appropriate symmetry assumption, the weights of the network hW,U do collapse into a type-I configuration. This proposition however does not provide the value of the constant c involved in the collapse. Determining this constant will be done in the subsection E.3.\nWe start with a simple lemma. Lemma G. Any global minimizer of (80) must belong to N . Proof. Let (W \u22c6, U\u22c6) be a global minimizer. Define B = 1K \u2211K k=1 U \u22c6 k and\nU0 = [U \u22c6 1 \u2212B U\u22c62 \u2212B \u00b7 \u00b7 \u00b7 U\u22c6K \u2212B]\nFrom the definition of the unregularized risk we have R0(W \u22c6;U0) = R0(W \u22c6;U\u22c6) and therefore\n1\nK (R(W \u22c6;U0)\u2212R(W \u22c6;U\u22c6)) =\n\u03bb\n2\n1\nK K\u2211 k=1 ( \u2225U\u22c6k \u2212B\u22252F \u2212 \u2225U\u22c6k\u22252F ) = \u03bb\n2\n1\nK K\u2211 k=1 ( \u2225B\u22252F \u2212 2 \u27e8B,U\u22c6k \u27e9F ) = \u03bb\n2\n( \u2225B\u22252F \u2212 2 \u2329 B, 1\nK K\u2211 k=1 U\u22c6k \u232a F )\n= \u2212\u03bb 2 \u2225B\u22252F\nSo B must be equal to zero, otherwise we would have R(W \u22c6, U0) < R(W \u22c6, U\u22c6).\nThe next lemma bring together the bilinear optimization problem from subsection E.1 and the sharp lower bound on the unregularized risk that we derived in section D. Lemma H. Assume the latent variables satisfy assumption A. Assume also that (W \u22c6, U\u22c6) is a global minimizer of (80) and let c \u2208 R be such that\n1\n2\n( \u2225W \u22c6\u22252F + \u2225U\u22c6\u22252F ) = c2 nw."
        },
        {
            "heading": "Then the following hold:",
            "text": "(i) Any (W,U) that belongs to N \u2229 E \u2229 BIc is also a global minimizer of (80).\n(ii) If N \u2229 E \u2229 BIc \u0338= \u2205, then (W \u22c6, U\u22c6) must belong to N \u2229 E \u2229 BIc .\nProof. Recall from theorem D that R0(W,U) = g ( \u2212 \u2329 U\u0302 ,WQTZ \u232a F ) for all (W,U) \u2208 N \u2229 E (82)\nR0(W,U) > g ( \u2212 \u2329 U\u0302 ,WQTZ \u232a F ) for all (W,U) \u2208 N \u2229 Ec (83)\nWe start by proving (i). If (W,U) \u2208 N \u2229 E \u2229 BIc , then we have R0(W \u22c6, U\u22c6) \u2265 g ( \u2212 \u2329 U\u0302\u22c6,W \u22c6QTZ \u232a F ) [because (W \u22c6, U\u22c6) \u2208 N due to lemma G ]\n\u2265 g ( \u2212 \u2329 U\u0302 , WQTZ \u232a F ) [because (W,U) \u2208 BIc and g is increasing]\n= R0(W,U) [because (W,U) \u2208 N \u2229 E ] Since (W,U) \u2208 BIc we must have 12 ( \u2225W\u22252F + \u2225U\u22252F ) = c2 nc = 1 2 ( \u2225W \u22c6\u22252F + \u2225U\u22c6\u22252F ) . Therefore R(W,U) \u2264 R(W \u22c6, U\u22c6) and (W,U) is a minimizer. We now prove (ii) by contradiction. Suppose that (W \u22c6, U\u22c6) /\u2208 N \u2229 E \u2229 BIc . This must mean that\n(W \u22c6, U\u22c6) /\u2208 E \u2229 BIc since it clearly belongs to N . If (W \u22c6, U\u22c6) /\u2208 E then the first inequality in the above computation is strict according to (83). If (W \u22c6, U\u22c6) /\u2208 BIc then the second inequality is strict because g is strictly increasing.\nThe above lemma establishes connections between the set of minimizers of the risk and the set E \u2229 N \u2229 BIc . The next two lemmas shows that the set E \u2229 N \u2229 BIc is closely related to the set of collapsed configurations \u2126Ic . In other words we use the set E \u2229 N \u2229 BIc as a bridge between the set of minimizers and the set of type-I collapse configurations.\nLemma I. If the latent variables satisfy the symmetry assumption A, then\n\u2126Ic \u2282 E \u2229 N \u2229 BIc\nProof. We already know from (67) that \u2126Ic \u2282 BIc . We now show that \u2126Ic \u2282 E . Suppose (W,U) \u2208 \u2126Ic . Then there exists an equiangular matrix F \u2208 Rd\u00d7nc such that\nW = c F P and U\u0302 = c\u2032 F Z where c\u2032 = c \u221a nw/(KL). Recall from (25) that\nP\u03b6(x) = Zk for all x \u2208 Xk.\nConsider two latent variables\nzk = [\u03b11, . . . , \u03b1L] and zj = [\u03b1\u20321, . . . , \u03b1 \u2032 L]\nand assume x is generated by zk, meaning that x \u2208 Xk. We then have MW,U (x, j) = \u2329 U\u0302k \u2212 U\u0302j ,W\u03b6(x) \u232a F\n= c c\u2032 \u27e8F Zk \u2212 F Zj ,F P\u03b6(x)\u27e9F = c c\u2032 \u27e8F Zk \u2212 F Zj ,F Zk\u27e9F\n= c c\u2032 L\u2211 \u2113=1 \u2329 f\u03b1\u2113 \u2212 f\u03b1\u2032\u2113 , f\u03b1\u2113 \u232a F\n= c c\u2032 ( L\u2212\nL\u2211 \u2113=1 \u2329 f\u03b1\u2032\u2113 , f\u03b1\u2113 \u232a F ) Since f1, . . . , fnc are equiangular, we have\nL\u2211 \u2113=1 \u2329 f\u03b1\u2032\u2113 , f\u03b1\u2113 \u232a F = ( L\u2212 dist(zj , zk) ) \u2212 1 nc \u2212 1 dist(zj , zk) = L\u2212 nc nc \u2212 1 dist(zj , zk).\nTherefore MW,U (x, j) = cc\n\u2032 nc nc \u2212 1 dist(zj , zk)\nand it is clear that the margin only depends on dist(zj , zk), and therefore (W,U) satisfies the equimargin property.\nFinally we show that \u2126Ic \u2282 N . Suppose (W,U) \u2208 \u2126Ic . From property (ii) of lemma B we have K\u2211 k=1 Zk = K nc 1nc1 T L\nTherefore,\nK\u2211 k=1 U\u0302k = c \u2032 K\u2211 k=1 F Zk = c \u2032 K nc F 1nc1 T L = 0\nwhere we have used the fact that F 1nc = 0.\nLemma J. If the latent variables satisfy assumptions A and B, then\n\u2126Ic = E \u2229 N \u2229 BIc\nProof. From the previous lemma we know that \u2126Ic \u2282 E \u2229 N \u2229 BIc so we need to show that\nE \u2229 N \u2229 BIc \u2282 \u2126Ic . Let (W,U) \u2208 E \u2229 N \u2229 BIc . Since (W,U) belongs to BIc , there exists a matrix F \u2208 Rd\u00d7nc with \u2225F\u22252F = nc such that\nW = c F P and U = c\u2032 F Z (84) where c\u2032 = c \u221a nw/(KL). Our goal is to show that F is equiangular, meaning that it satisfies the two relations\nF 1nc = 0 and F TF = nc nc \u2212 1 Inc \u2212 1 nc \u2212 1 1nc1 T nc . (85)\nThe first relation is easily obtained. Indeed, using the fact that (W,U) \u2208 N together with the identity\u2211K k=1 Zk = K nc 1nc1 T L (which hold due to lemma B), we obtain\n0 = K\u2211 k=0 Uk = c \u2032 K\u2211 k=0 FZk = c \u2032K nc F1nc1 T L.\nWe then note that the matrix F1nc1 T L is the zero matrix if and only if F1nc = 0.\nWe now prove the second equality of (85). Assume that x \u2208 Xk. Using the fact that P\u03b6(x) = Zk together with (84), we obtain\nMW,U (x, j) = \u2329 U\u0302k \u2212 U\u0302j ,W\u03b6(x) \u232a F\n= c c\u2032 \u27e8F Zk \u2212 F Zj , F P\u03b6(x)\u27e9F = c c\u2032 \u27e8F Zk \u2212 F Zj , F Zk\u27e9F = c c\u2032 \u2329 FTF (Zk \u2212 Zj), Zk \u232a F\n= c c\u2032 \u2329 FTF , \u0393(k,j) \u232a F\n(86)\nWe recall that the matrices \u0393(k,j) = Zk(Zk \u2212 Zj)T \u2208 Rnc\u00d7nc .\nare precisely the ones involved in the statement of assumption B. Since (W,U) \u2208 E , the margins must only depend on the distance between the latent variables. Due to (86), we can be express this as\u2329 FTF , \u0393(j,k) \u232a F = \u2329 FTF , \u0393(j \u2032,k\u2032) \u232a F \u2200j, k, j\u2032, k\u2032 \u2208 [K] s.t. dist(zj , zk) = dist(zj\u2032 , zk\u2032)\nSince the FTF is clearly positive semi-definite, we may then use assumption B to conclude that FTF \u2208 A. Recalling definition (42) of the set A, we therefore have\nFTF = a Inc + b 1nc1 T nc (87)\nfor some a, b \u2208 R. To conclude our proof, we need to show that\na = nc nc \u2212 1 and b = \u2212 1 nc \u2212 1 . (88)\nCombining (87) with the first equality of (85), we obtain\n0 = FTF 1nc = a 1nc + b 1nc1 T nc1nc = (a+ bnc)1nc (89)\nCombining (87) with the fact that \u2225F\u22252F = nc, we obtain\nnc = \u2225F\u22252F = Tr(FTF ) = nc(a+ b) (90) The constants a, b \u2208 R, according to (89) and (90) must therefore solve the system{\na+ bnc = 0\na+ b = 1\nand one can easily check that the solution of this system is precisely given by (88).\nWe conlude this subsection by proving proposition A.\nProof of Proposition A. Let (W \u22c6, U\u22c6) be a global minimizer of R and let c \u2208 R be such that 1\n2\n( \u2225W \u22c6\u22252F + \u2225U\u22c6\u22252F ) = c2 nw\nIf the latent variables satisfies assumption A, we can use lemma I together with the first statement of lemma H to obtain \u2126Ic \u2282 E \u2229 N \u2229 BIc \u2282 argminR, which is precisely statement (i) of the proposition.\nWe now prove statement (ii) of the proposition. If the latent variables satisfies assumption A and B then lemma J asserts that \u2126Ic = E \u2229 N \u2229 BIc The set \u2126Ic is clearly not empty (because the set of equiangular matrices is not empty), and we may therefore use the second statement of lemma H to obtain that\n(W \u22c6, U\u22c6) \u2208 E \u2229 N \u2229 BIc = \u2126Ic\nE.3 DETERMINING THE CONSTANT c\nThe next lemma provides an explicit formula for the regularized risk of a network whose weights are in type-I collapse configuration with constant c. Lemma K. Assume the latent variables satisfy assumption A. If the pair of weights (W,U) belongs to \u2126Ic , then\nR(W,U) = log ( 1\u2212 K\nnLc + K nLc\n( 1 + (nc \u2212 1)e\u2212\u03b7 nwc 2 )L) + \u03bbnwc 2 (91)\nwhere \u03b7 = ncnc\u22121 \u221a 1 nwKL .\nFrom the above lemma it is clear that if the pair (W,U) \u2208 \u2126Ic minimizes R, then the constant cmust minimize the right hand side of (91). Therefore combining lemma K with proposition A concludes the proof of theorem E.\nRemark In the previous subsections, we only relied on relations (i), (ii) and (iii) of lemma B to prove collapse. Assumption A was never fully needed. In this section however, in order to determine the specific values of the constant involved in the collapse, we will need the actual combinatorial values provided by assumption A.\nThe remainder of this section is devoted to the proof of lemma K.\nProof of lemma K. Recall from (45) that the unregularized risk can be expressed as\nR0(W,U) = 1\nK K\u2211 k=1 \u2211 x\u2208Xk log 1 +\u2211 j \u0338=k e\u2212MW,U (x,j)  Dzk(x) We also recall that the set \u2126Ic is given by\n\u2126Ic = { (W,U) : There exist an equiangular matrix F such that\nW = c F P and U\u0302 = c \u221a nw KL F Z } (92)\nand that P\u03b6(x) = Zk for all x \u2208 Xk (see equation (25) from section B). Consider two latent variables\nzk = [\u03b11, . . . , \u03b1L] and zj = [\u03b1\u20321, . . . , \u03b1 \u2032 L]\nand assume x is generated by zk, meaning that x \u2208 Xk. MW,U (x, j) = \u2329 U\u0302k \u2212 U\u0302j ,W\u03b6(x) \u232a F\n= c2 \u221a nw KL \u27e8F Zk \u2212 F Zj ,F P\u03b6(x)\u27e9F\n= c2 \u221a nw KL \u27e8F Zk \u2212 F Zj ,F Zk\u27e9F\n= c2 \u221a nw KL L\u2211 \u2113=1 \u2329 f\u03b1\u2113 \u2212 f\u03b1\u2032\u2113 , f\u03b1\u2113 \u232a F\n= c2 \u221a nw KL ( L\u2212 L\u2211 \u2113=1 \u2329 f\u03b1\u2032\u2113 , f\u03b1\u2113 \u232a F ) Since f1, . . . , fnc are equiangular, we have\nL\u2211 \u2113=1 \u2329 f\u03b1\u2032\u2113 , f\u03b1\u2113 \u232a F = ( L\u2212 dist(zj , zk) ) \u2212 1 nc \u2212 1 dist(zj , zk) = L\u2212 nc nc \u2212 1 dist(zj , zk).\nTherefore\nMW,U (x, j) = c 2 \u221a nw KL nc nc \u2212 1 dist(zj , zk)\nLetting \u03c9 = \u221a\nnw KL nc nc\u22121 we therefore obtain\nR0(W,U) = 1\nK K\u2211 k=1 \u2211 x\u2208Xk log 1 +\u2211 j \u0338=k e\u2212\u03c9c 2dist(zj ,zk)  Dzk(x) = 1\nK K\u2211 k=1 log 1 +\u2211 j \u0338=k e\u2212\u03c9c 2dist(zj ,zk)  (93) where we have used the quantity inside the log does not depends on x. We proved in section C (see equation (38)) that if the latent variables satisfy assumption A, then\n|Sr| = K\nnLc\n( L\nr\n) (nc \u2212 1)r\nUsing this identity we obtain\u2211 j \u0338=k e\u2212\u03c9c 2dist(zj ,zk) = L\u2211 r=1 |{j : dist(zj , zk) = r}| e\u2212\u03c9c 2r\n= K\nnLc L\u2211 r=1 ( L r ) (nc \u2212 1)r e\u2212\u03c9c 2r\n= \u2212 K nLc + K nLc L\u2211 r=0 ( L r ) (nc \u2212 1)r e\u2212\u03c9c 2r\n= \u2212 K nLc + K nLc\n( 1 + (nc \u2212 1)e\u2212\u03c9c 2 )L\nwhere we have used the binomial theorem to obtain the last equality. The above quantity does not depends on k, therefore (93) can be expressed as\nR0(W,U) = log ( 1\u2212 K\nnLc + K nLc\n( 1 + (nc \u2212 1)e\u2212\u03c9 c 2 )L)\nWe then remark that the matrix F P has nw columns, and that each of these columns has norm 1. Similarly, the F Z has KL columns of length 1. We therefore have\n1\n2\n( \u2225W\u22252F + \u2225U\u0302\u22252F ) = 1\n2\n( c2\u2225F P\u22252F + c2\nnw KL\n\u2225F Z\u22252F ) = c2nw.\nTo conclude the proof we simply remark that \u03c9 = nw\u03b7."
        },
        {
            "heading": "F PROOF OF THEOREM 3 AND ITS CONVERSE",
            "text": "In this section we prove theorem 3 under assumption A, and its converse under assumptions A and B. We start by recalling the definition of a type-II collapse configuration. Definition E (Type-II Collapse). The weights (W,U) of the network h\u2217W,U form a type-II collapse configuration if and only if the conditions\ni) \u03c6(w(\u03b1,\u03b2)) = \u221a d f\u03b1 for all (\u03b1, \u03b2) \u2208 V .\nii) There exists c \u2265 0 so that uk,\u2113 = c f\u03b1 for all (k, \u2113) satisfying zk,\u2113 = \u03b1 and all \u03b1 \u2208 C.\nhold for some collection f1, . . . , fnc \u2208 Rd of mean-zero equiangular vectors.\nAs in the previous section we will reformulate the above definition using matrix notations. Toward this aim we make the following definition: Definition F. (Mean-Zero Equiangular Matrices) A matrix F \u2208 Rd\u00d7nc is said to be a mean-zero equiangular matrix if and only if the relations\n1Td F = 0, F1nc = 0 and F TF = nc nc \u2212 1 Inc \u2212 1 nc \u2212 1 1nc1 T nc\nhold.\nComparing the above definition with the definition of equiangular vectors provided in the main paper, we easily see that F is a mean-zero equiangular matrix if and only if its columns are meanzero equiangular vectors. Relations (i) and (ii) of definition F can be conveniently expressed as\n\u03c6(W ) = \u221a d F P and U\u0302 = c F Z\nfor some equiangular matrix F. We then set \u2126IIc = { (W,U) : There exist a mean-zero equiangular matrix F such that\n\u03c6(W ) = \u221a d F P and U\u0302 = c F Z } (94)\nand note that \u2126IIc is simply the set of weights (W,U) which are in a type-II collapse configuration. We now state the main theorem of this section. Theorem F. Assume the non-degenerate condition \u00b5\u03b2 > 0 holds. Let \u03c4 \u2265 0 denote the unique minimizer of the strictly convex function\nH\u2217(t) = log ( 1\u2212 K\nnLc + K nLc\n( 1 + (nc \u2212 1)e\u2212\u03b7 \u2217t )L) + \u03bb\n2 t2 where \u03b7\u2217 = nc nc \u2212 1 1\u221a KL/d\nand let c = \u03c4/ \u221a KL. Then we have the following:\n(i) If the latent variables z1, . . . , zK are mutually distinct and satisfy assumption A, then\n\u2126IIc \u2282 argminR\u2217\n(ii) If the latent variables z1, . . . , zK are mutually distinct and satisfy assumptions A and B, then\n\u2126IIc = argminR\u2217\nNote that statement (i) corresponds to theorem 3 of the main paper, whereas statement (ii) can be viewed as its converse. To prove F we will follow the same steps than in the previous section. The main difference occurs in the study of the bilinear problem, as we will see in the next subsection. We will assume\n\u00b5\u03b2 > 0\neverywhere in this section \u2014 all lemmas and propositions are proven under this assumption, even when not explicitly stated.\nBefore to go deeper in our study let us state a very simple lemma that expresses the regularized risk R\u2217 associated with network h\u2217 in term of the function R0 defined by equation (44).\nLemma L. Given a pair of weights (W,U), we have R\u2217(W,U) = R0 ( \u03c6(W ) , U ) + \u03bb\n2 \u2225U\u22252F (95)\nProof. Recall from section B that\nhW,U (x) = U Vec [W\u03b6(x)] h\u2217W,U (x) = U Vec [ \u03c6 ( W\u03b6(x) )] Note that since \u03b6(\u03b1, \u03b2) is a one hot vector, we obviously have that \u03c6 (W\u03b6(\u03b1, \u03b2)) = \u03c6 (W ) \u03b6(\u03b1, \u03b2). Therefore the the network h\u2217 and h are related as follow:\nh\u2217W,U (x) = U Vec [ \u03c6 ( W\u03b6(x) )] = U Vec [ \u03c6(W ) \u03b6(x) ] = h\u03c6(W ), U (x)\nAs a consequence, the regularized risk associated with the network h\u2217W,U can be expressed as\nR\u2217(W,U) = 1 K K\u2211 k=1 Ex\u223cDzk [ \u2113(h\u2217W,U (x), k) ] + \u03bb 2 \u2225U\u22252F\n= 1\nK K\u2211 k=1 Ex\u223cDzk [ \u2113(h\u03c6(W ),U (x), k) ] + \u03bb 2 \u2225U\u22252F\n= R0(\u03c6(W ) , U) + \u03bb\n2 \u2225U\u22252F\nwhere R0 is the unregularized risk defined in (44)."
        },
        {
            "heading": "F.1 THE BILINEAR OPTIMIZATION PROBLEM",
            "text": "Let Range(\u03c6) = {V \u2208 Rd\u00d7nw : There exist W \u2208 Rd\u00d7nw such that V = \u03c6(W ) }\nand consider the optimization problem maximize \u2329 U\u0302 , V QTZ \u232a F\n(96)\nsubject to V \u2208 Range(\u03c6) and \u2225U\u0302\u22252F = KLc2 (97)\nwhere the optimization variables are the matrix V \u2208 Rd\u00d7nw and the matrix U\u0302 \u2208 Rd\u00d7KL. Lemma M. Assume the latent variables satisfy assumption A. Then (V,U) is a solution of the optimization problem (96) \u2013 (97) if and only if it belongs to the set\nBIIc = { (V,U) : There exist a matrix F \u2208 F such that V = \u221a dFP and U\u0302 = c FZ } (98)\nwhere F denotes the set of matrices whose columns have unit length and mean zero, that is\nF = {F \u2208 Rd\u00d7nc : 1Td F = 0 and the columns of F have unit length}.\nThe remainder of this subsection is devoted to the proof of the above lemma.\nWe start by showing that all (V,U) \u2208 BIIc have same objective values and satisfy the constraints. Claim D. If (V,U) \u2208 BIIc , then\nV \u2208 Range(\u03c6) , \u2225U\u0302\u22252F = KLc2, and \u2329 U\u0302 , V QTZ \u232a F = c \u221a dKL\nProof. Assume (V,U) \u2208 BIIc . Since the columns of P are one hot vectors in Rnc , the columns of FP have unit length and mean zero. Therefore the columns of V have norm equal to \u221a d and mean zero. Therefore V \u2208 Range(\u03c6).\nUsing ZZT = KLnc I from lemma B, together with the fact that \u2225F\u2225 2 F = nc since its columns have unit length, we obtain\n\u2225FZ\u22252F = \u27e8FZ,FZ\u27e9F = \u2329 FZZT , F \u232a F =\n( KL\nnc\n) \u2225F\u22252F = KL (99)\nAs a consequence we have \u2225U\u0302\u22252F = c2KL. Finally, note that\nPQT = Inc\nas can clearly be seen from formulas (23) and (26). We therefore have\u2329 U\u0302 , V QTZ \u232a F = c \u221a d \u2329 FZ,FPQTZ \u232a F = c \u221a d \u27e8FZ,FZ\u27e9F = c \u221a dKL\nWe then prove that\nClaim E. If (V, U\u0302) is a solution of (96) \u2013 (97), then (V, U\u0302) \u2208 BIIc .\nNote that according to the first claim, all (V, U\u0302) \u2208 BIIc have same objective value, and therefore, according to the above claim, they must all be maximizer. As a consequence, proving the above claim will conclude the proof of lemma M.\nProof of the claim. Maximizing (96) \u2013 (97) over U\u0302 first gives\nU\u0302 = c \u221a KL V QTZ\n\u2225V QTZ\u2225F (100)\nand therefore the optimization problem reduces to\nmaximize \u2225V QTZ\u22252F (101) subject to V \u2208 Range(\u03c6) (102)\nUsing the fact that ZZT = KLnc I we then get\n\u2225V QTZ\u22252F = \u2329 V QTZ, V QTZ \u232a F = \u2329 V QTZZT , V QT \u232a F = KL\nnc \u2225V QT \u22252F (103)\nand so the problem further reduces to\nmaximize \u2225V QT \u22252F (104) subject to V \u2208 Range(\u03c6) (105)\nLet us define v(\u03b1,\u03b2) := V \u03b6(\u03b1, \u03b2)\nIn other words v(\u03b1,\u03b2) is the jth column of V , where j = (\u03b1 \u2212 1)sc + \u03b2. The KKT conditions for the optimization problem (104) \u2013 (105) then amount to solving the system\nV QTQ = V D\u03bd + 1d \u03bb T (106)\n\u27e8v(\u03b1,\u03b2),1d\u27e9 = 0 for all (\u03b1, \u03b2) \u2208 V (107) \u2225v(\u03b1,\u03b2)\u22252 = d for all (\u03b1, \u03b2) \u2208 V (108)\nforD\u03bd some nw\u00d7nw diagonal matrix of Lagrange multipliers for the constraint (108) and \u03bb \u2208 Rnw a vector of Lagrange multipliers for the mean zero constraints. Left multiplying the first equation by 1Td and using the second shows \u03bb = 0nw , and so it proves equivalent to find solutions of the reduced system\nV QTQ = V D\u03bd (109) \u27e8v(\u03b1,\u03b2),1d\u27e9 = 0 for all (\u03b1, \u03b2) \u2208 V (110) \u2225v(\u03b1,\u03b2)\u22252 = d for all (\u03b1, \u03b2) \u2208 V (111)\ninstead. Recalling the identity Q\u03b6(\u03b1, \u03b2) = \u00b5\u03b2e\u03b1 (see (27) in section B) we obtain\nQTQ\u03b6(\u03b1, \u03b2) = \u00b5\u03b2 Q T e\u03b1\nand so right multiplying (109) by \u03b6(\u03b1, \u03b2) gives\nV QT e\u03b1 = \u03bd(\u03b1, \u03b2)\n\u00b5\u03b2 v(\u03b1, \u03b2) for all (\u03b1, \u03b2) \u2208 V\nwhere we have denoted by \u03bd(\u03b1, \u03b2) the Lagrange multiplier corresponding to the constraint (111). Define the support sets\n\u039e\u03b1 := {\u03b2 \u2208 [sc] : \u03bd(\u03b1, \u03b2) \u0338= 0} and \u039e := {\u03b1 : \u039e\u03b1 \u0338= \u2205} of the Lagrange multipliers. If \u03b1 \u2208 \u039e then imposing the norm constraint (111) gives\n\u2225V QT e\u03b1\u2225 = \u03bd(\u03b1, \u03b2)\n\u00b5\u03b2\n\u221a d,\nand so \u2225V QT e\u03b1\u2225 > 0 if \u03b1 \u2208 \u039e since \u03bd(\u03b1, \u03b2) > 0 for some \u03b2 \u2208 [sc] by definition. This implies that the relation\nv(\u03b1, \u03b2) = \u221a d V QT e\u03b1 \u2225V QT e\u03b1\u2225\nfor all (\u03b1, \u03b2) \u2208 \u039e\u00d7 [sc]\nmust hold. As a consequence there exist mean-zero, unit length vectors f1, . . . , fnc (namely the normalized V QT e\u03b1) so that\nv(\u03b1, \u03b2) = \u221a d f\u03b1\nholds for all pairs (\u03b1, \u03b2) with \u03b1 \u2208 \u039e. Taking a look at (26), we easily see that its \u03b1th row of the matrix Q can be written as QT e\u03b1 = \u2211 \u03b2 \u00b5\u03b2\u03b6(\u03b1, \u03b2), and therefore\nV QT e\u03b1 = \u2211 \u03b2\u2208[sc] \u00b5\u03b2V \u03b6(\u03b1, \u03b2) = \u2211 \u03b2\u2208[sc] \u00b5\u03b2v(\u03b1, \u03b2) = \u221a d f\u03b1  \u2211 \u03b2\u2208[sc] \u00b5\u03b2  = \u221ad f\u03b1 holds as well. If \u03b1 /\u2208 \u039e then V QT e\u03b1 = 0 since the corresponding Lagrange multiplier vanishes. It therefore follows that\n\u2225V QT \u22252F = \u2211 \u03b1\u2208[nc] \u2225V QTe\u03b1\u22252 = d \u2211 \u03b1\u2208\u039e \u2225f\u03b1\u22252 = d |\u039e|\nand so global maximizers of (104) \u2013 (105) must have full support. In other words, there exist meanzero, unit-length vectors f1, . . . , fnc so that\nv(\u03b1, \u03b2) = \u221a d f\u03b1 (112)\nholds. Equivalently V = \u221a dFP for some F \u2208 F . We then recover U\u0302 using (100).\nU\u0302 = c \u221a KL V QTZ\n\u2225V QTZ\u2225F = c\n\u221a KL FPQTZ\n\u2225FPQTZ\u2225F = c\n\u221a KL FZ\n\u2225FZ\u2225F (113)\nwhere we have used the fact that PQT = Inc . To conclude the proof, we use the fact \u2225FZ\u2225F =\u221a KL, as was shown in (99)."
        },
        {
            "heading": "F.2 PROOF OF COLLAPSE",
            "text": "Recall from lemma L that the regularized risk associated with the network h\u2217W,U can be expressed as\nR\u2217(W,U) = R0 ( \u03c6(W ) , U ) + \u03bb\n2 \u2225U\u22252F (114)\nand recall that the set of weights in type-II collapse configuration is \u2126IIc = { (W,U) : There exist a mean-zero equiangular matrix F such that\n\u03c6(W ) = \u221a d F P and U\u0302 = c F Z } (115)\nThis subsection is devoted to the proof of the following proposition.\nProposition B. We have the following:\n(i) If the latent variables z1, . . . , zK are mutually distinct and satisfy assumption A, then there exists c \u2208 R such that\n\u2126IIc \u2282 argminR\u2217\n(ii) If the latent variables z1, . . . , zK are mutually distinct and satisfy assumptions A and B, then any (W,U) that minimizes R\u2217 must belong to \u2126IIc for some c \u2208 R.\nAs in the previous section, we have the following lemma. Lemma N. Any global minimizer of (114) must belong to N .\nThe proof is identical to the proof of lemma G. The next lemma bring together the bilinear optimization problem from subsection F.1 and the sharp lower bound on the unregularized risk that we derived in section D. Lemma O. Assume the latent variables satisfy assumption A. Assume also that (W \u22c6, U\u22c6) is a global minimizer of (114) and let c \u2208 R be such that\n\u2225U\u22c6\u22252F = KLc2"
        },
        {
            "heading": "The the following hold:",
            "text": "(i) Any (W,U) that satisfies (\u03c6(W ), U) \u2208 N \u2229 E \u2229 BIIc\nis also a global minimizer of R\u2217.\n(ii) If N \u2229 E \u2229 BIIc \u0338= \u2205, then (\u03c6(W \u22c6), U\u22c6) \u2208 N \u2229 E \u2229 BIIc\nProof. Recall from theorem D that R0(V,U) = g ( \u2212 \u2329 U\u0302 , V QTZ \u232a F ) for all (V,U) \u2208 N \u2229 E (116)\nR0(V,U) > g ( \u2212 \u2329 U\u0302 , V QTZ \u232a F ) for all (V,U) \u2208 N \u2229 Ec (117)\nWe start by proving (i). Define V \u22c6 = \u03c6(W \u22c6), and assume that U, V,W are such that \u03c6(W ) = V and (V,U) \u2208 N \u2229 E \u2229 Bc. Then we have\nR0(\u03c6(W \u22c6), U\u22c6) = R0(V \u22c6, U\u22c6) \u2265 g (\u2212\u27e8U\u22c6, V \u22c6QZ\u27e9F ) [because (V\n\u22c6, U\u22c6) \u2208 N ] \u2265 g (\u2212\u27e8U, V QZ\u27e9F ) [because (V,U) \u2208 B II c ] = R0(V,U) [because (V,U) \u2208 N \u2229 E ] = R0(\u03c6(W ), U)\nSince \u2225U\u22252F = KLc2 = \u2225U\u22c6\u22252F , we have R\u2217(W,U) \u2264 R\u2217(W \u22c6, U\u22c6) and therefore (W,U) is a minimizer.\nWe now prove (ii) by contradiction. Suppose that (\u03c6(W \u22c6), U\u22c6) /\u2208 N \u2229 E \u2229 BIIc . This must mean that (\u03c6(W \u22c6), U\u22c6) /\u2208 E \u2229 BIIc since it clearly belongs to N . If (\u03c6(W \u22c6), U\u22c6) /\u2208 E then the first inequality in the above computation is strict according to (117). If (\u03c6(W \u22c6), U\u22c6) /\u2208 BIIc then the second inequality is strict because g is strictly increasing.\nThe next two lemmas shows that the set E \u2229 N \u2229 BIIc is closely related to the set of collapsed configurations \u2126IIc . In order to states these lemmas, the following definition will prove convenient\n\u2126 II c = { (V,U) : There exist a mean-zero equiangular matrix F such that\nV = \u221a d F P and U\u0302 = c F Z } (118)\nNote that (W,U) \u2208 \u2126IIc if and only if (\u03c6(W ), U) \u2208 \u2126 II c . Also, in light of (98), the inclusion\n\u2126 II\nc \u2282 BIIc is obvious. We now prove the following lemma. Lemma P. If the latent variables satisfy the symmetry assumption A, then\n\u2126 II\nc \u2282 E \u2229 N \u2229 BIIc\nProof. The proof is almost identical to the one of lemma I. We repeat it for completeness. We already know that \u2126 II\nc \u2282 BIIc . We the show that \u2126 II c \u2282 E . Suppose (V,U) \u2208 \u2126 II\nc . Then there exists a mean-zero equiangular matrix F \u2208 Rd\u00d7nc such that\nV = \u221a d F P and U\u0302 = c F Z\nRecall from (25) that P\u03b6(x) = Zk for all x \u2208 Xk. Consider two latent variables\nzk = [\u03b11, . . . , \u03b1L] and zj = [\u03b1\u20321, . . . , \u03b1 \u2032 L]\nand assume x is generated by zk, meaning that x \u2208 Xk. We then have MV,U (x, j) = \u2329 U\u0302k \u2212 U\u0302j , V \u03b6(x) \u232a F\n= c \u221a d \u27e8F Zk \u2212 F Zj ,F P\u03b6(x)\u27e9F = c \u221a d \u27e8F Zk \u2212 F Zj ,F Zk\u27e9F\n= c \u221a d L\u2211 \u2113=1 \u2329 f\u03b1\u2113 \u2212 f\u03b1\u2032\u2113 , f\u03b1\u2113 \u232a F = c \u221a d dist(zj , zk)\nFrom the above computation it is clear that the margin only depends on dist(zj , zk), and therefore (V,U) satisfies the equimargin property.\nFinally we show that \u2126 II c \u2282 N . Suppose (V,U) \u2208 \u2126 II c . Using the identity \u2211K k=1 Zk = K nc 1nc1 T L we obtain K\u2211 k=1 U\u0302k = c K\u2211 k=1 F Zk = c K nc F 1nc1 T L = 0\nwhere we have used the fact that F 1nc = 0.\nFinally, we have the following lemma. Lemma Q. If the latent variables satisfy assumptions A and B, then\n\u2126 II\nc = E \u2229 N \u2229 BIIc\nProof. The proof, again, is very similar to the one of lemma J. From the previous lemma we know that \u2126 II\nc \u2282 E \u2229 N \u2229 BIIc so we need to show that\nE \u2229 N \u2229 BIIc \u2282 \u2126 II c .\nLet (V,U) \u2208 E \u2229 N \u2229 BIIc . Since (V,U) belongs to BIIc , there exists a matrix F \u2208 Rd\u00d7nc whose columns have unit length and mean 0 such that\nV = \u221a d F P and U = c F Z\nOur goal is to show that F is a mean-zero equiangular matrix, meaning that it satisfies the three relations\n1Tnc F = 0, F 1nc = 0 and F TF = nc nc \u2212 1 Inc \u2212 1 nc \u2212 1 1nc1 T nc . (119)\nWe already know that the first relation is satisfied since the columns of F have mean 0. The second relation is easily obtained. Indeed, using the fact that (V,U) \u2208 N together with the identity\u2211K k=1 Zk = K nc 1nc1 T L (which hold due to lemma B), we obtain\n0 = K\u2211 k=0 Uk = c \u2032 K\u2211 k=0 FZk = c K nc F1nc1 T L.\nwhich implies F1nc = 0.\nWe now prove the third equality of (119). Assume that x \u2208 Xk. Using the fact that P\u03b6(x) = Zk together with (84), we obtain\nMV,U (x, j) = \u2329 U\u0302k \u2212 U\u0302j , V \u03b6(x) \u232a F\n= c \u221a d \u27e8F Zk \u2212 F Zj , F P\u03b6(x)\u27e9F = c \u221a d \u27e8F Zk \u2212 F Zj , F Zk\u27e9F\n= c \u221a d \u2329 FTF (Zk \u2212 Zj), Zk \u232a F\n= c \u221a d \u2329 FTF , \u0393(k,j) \u232a F\n(120)\nSince (V,U) \u2208 E , the margins must only depend on the distance between the latent variables. Due to (120), we can be express this as\u2329 FTF , \u0393(j,k) \u232a F = \u2329 FTF , \u0393(j \u2032,k\u2032) \u232a F \u2200j, k, j\u2032, k\u2032 \u2208 [K] s.t. dist(zj , zk) = dist(zj\u2032 , zk\u2032)\nSince the FTF is clearly positive semi-definite, we may then use assumption B to conclude that FTF \u2208 A. Recalling definition (42) of the set A, we therefore have\nFTF = a Inc + b 1nc1 T nc (121)\nfor some a, b \u2208 R. To conclude our proof, we need to show that\na = nc nc \u2212 1 and b = \u2212 1 nc \u2212 1 . (122)\nCombining (121) with the first equality of (119), we obtain\n0 = FTF 1nc = a 1nc + b 1nc1 T nc1nc = (a+ bnc)1nc\nSince the columns of F have unit length, the diagonal entries of FTF must all be equal to 1, and therefore (121) implies that a + b = 1. The constants a, b \u2208 R, according must therefore solve the system {\na+ bnc = 0\na+ b = 1\nand one can easily check that the solution of this system is precisely given by (122).\nWe conlude this subsection by proving proposition B.\nProof of Proposition B. Let (W \u22c6, U\u22c6) be a global minimizer of R and let c \u2208 R be such that\n\u2225U\u22c6\u22252F = KLc2\nWe first prove statement (i) of the proposition. If the latent variables satisfies assumption A then lemma P asserts that\n\u2126 II\nc \u2282 E \u2229 N \u2229 BIIc Assume (W,U) \u2208 \u2126IIc . This implies that (\u03c6(W ), U) \u2208 \u2126 II\nc , and and therefore (\u03c6(W ), U) \u2208 E \u2229 N \u2229 BIIc . We can then use lemma O to conclude that (W,U) is a global minimizer of R\u2217.\nWe now prove statement (ii) of the proposition. If the latent variables satisfies assumption A and B then lemma Q asserts that\n\u2126 II\nc = E \u2229 N \u2229 BIIc The set \u2126 II\nc is clearly not empty (because the set of mean-zero equiangular matrices is not empty), and we may therefore use the second statement of lemma O to obtain that\n(\u03c6(W \u22c6), U\u22c6) \u2208 E \u2229 N \u2229 BIIc = \u2126 II c\nwhich in turn implies (W \u22c6, U\u22c6) \u2208 \u2126IIc .\nF.3 DETERMINING THE CONSTANT c\nThe next lemma provides an explicit formula for the regularized risk of a network h\u2217W,U whose weights are in type-II collapse configuration with constant c. Lemma R. Assume the latent variables satisfy assumption A. If the pair of weights (W,U) belongs to \u2126IIc , then\nR\u2217(W,U) = log ( 1\u2212 K\nnLc + K nLc\n( 1 + (nc \u2212 1)e\u2212\u03b7 \u2217\u221aKLc )L) + \u03bb\n2\n(\u221a KLc )2 (123)\nwhere \u03b7\u2217 = ncnc\u22121 \u221a d KL .\nCombining lemma R with proposition B concludes the proof of theorem F.\nProof of lemma R. We recall that\nR0(W,U) = 1\nK K\u2211 k=1 \u2211 x\u2208Xk log 1 +\u2211 j \u0338=k e\u2212MW,U (x,j)  Dzk(x) and\n\u2126IIc = { (W,U) : There exist a mean-zero equiangular matrix F such that\n\u03c6(W ) = \u221a d F P and U\u0302 = c F Z } (124)\nConsider two latent variables zk = [\u03b11, . . . , \u03b1L] and zj = [\u03b1\u20321, . . . , \u03b1 \u2032 L] and assume x \u2208 Xk. Using the identity P\u03b6(x) = Zk we then obtain\nM\u03c6(W ),U (x, j) = \u2329 U\u0302k \u2212 U\u0302j , \u03c6(W )\u03b6(x) \u232a F\n= c \u221a d \u27e8F Zk \u2212 F Zj ,F P\u03b6(x)\u27e9F = c \u221a d \u27e8F Zk \u2212 F Zj ,F Zk\u27e9F\n= c \u221a d L\u2211 \u2113=1 \u2329 f\u03b1\u2113 \u2212 f\u03b1\u2032\u2113 , f\u03b1\u2113 \u232a F\n= c \u221a d ( L\u2212\nL\u2211 \u2113=1 \u2329 f\u03b1\u2032\u2113 , f\u03b1\u2113 \u232a F ) = c \u221a d\nnc nc \u2212 1 dist(zj , zk)\nLetting \u03c9\u2217 = \u221a d ncnc\u22121 we therefore obtain\nR0(W,U) = 1\nK K\u2211 k=1 \u2211 x\u2208Xk log 1 +\u2211 j \u0338=k e\u2212c \u03c9 \u2217 dist(zj ,zk)  Dzk(x) = 1\nK K\u2211 k=1 log 1 +\u2211 j \u0338=k e\u2212c \u03c9 \u2217 dist(zj ,zk)  (125)\nwhere we have used the quantity inside the log does not depends on x. Using the identity |Sr| = K nLc ( L r ) (nc \u2212 1)r we then obtain obtain\u2211\nj \u0338=k e\u2212c \u03c9 dist(zj ,zk) = L\u2211 r=1 |{j : dist(zj , zk) = r}| e\u2212c \u03c9 \u2217 r\n= K\nnLc L\u2211 r=1 ( L r ) (nc \u2212 1)r e\u2212c \u03c9 \u2217 r\n= \u2212 K nLc + K nLc L\u2211 r=0 ( L r ) (nc \u2212 1)r e\u2212c \u03c9 \u2217 r\n= \u2212 K nLc + K nLc\n( 1 + (nc \u2212 1)e\u2212c \u03c9 \u2217 )L\nwhere we have used the binomial theorem to obtain the last equality. The above quantity does not depends on k, therefore (125) can be expressed as\nR0(W,U) = log ( 1\u2212 K\nnLc + K nLc\n( 1 + (nc \u2212 1)e\u2212c \u03c9 \u2217 )L)\nWe then remark that the matrix F Z has KL columns, and that each of these columns has norm 1. We therefore have\n\u2225U\u0302\u22252F = \u2225cFZ\u22252F = c2KL for all (W,U) \u2208 \u2126IIc To conclude the proof we simply note that \u03c9\u2217 = \u221a KL\u03b7\u2217."
        },
        {
            "heading": "G PROOF OF THEOREM 2",
            "text": "This section is devoted to the proof of theorem 2 from the main paper, which we recall below for convenience. Theorem 2 (Directional Collapse of h). Assume K = nLc and {z1, . . . , zK} = Z . Assume also that the regularization parameter \u03bb satisfies\n\u03bb2 < L\nnL+1c sc\u2211 \u03b2=1 \u00b52\u03b2 (126)\nFinally, assume that (W, U) is in a type-III collapse configuration for some constants c, r1, . . . , rsc \u2265 0. Then (W,U) is a critical point of R if and only if (c, r1, . . . , rsc) solve the system\n\u03bb\nL r\u03b2 c\n( nc \u2212 1 + exp ( nc\nnc \u2212 1 c r\u03b2\n)) = \u00b5\u03b2 for all 1 \u2264 \u03b2 \u2264 sc (127)\nsc\u2211 \u03b2=1 (r\u03b2 c )2 = LnL\u22121c . (128)\nAt the end of this section, we also show that if (149) holds, then the system (150) \u2013 (151) has a unique solution (see proposition D in subsection G.2).\nThe strategy to prove theorem 2 is straightforward: we simply need to evaluate the gradient of the risk on weights (W,U) which are in a type-III collapse configuration. Setting this gradient to zero will then lead to a system for the constants c, r1, . . . , rsc defining the configuration. While conceptually simple, the gradient computation is quite lengthy.\nWe start by deriving formulas for the partial derivatives of R0 with respect to the linear weights uk,\u2113 and the word embeddings w(\u03b1,\u03b2). As we will see, \u2202R0/\u2202uk,\u2113 and \u2202R0/\u2202w(\u03b1,\u03b2) plays symmetric roles. In order to observe this symmetry, the following notation will prove convenient:\n\u03a6(\u03b1,\u03b2),(k,\u2113)(W,U) := 1\nK K\u2211 j=1 \u2211 x\u2208Xj 1{x\u2113=(\u03b1,\u03b2)} ( 1{j=k} \u2212 qk,W,U (x) ) Dzj (x) (129)\nwhere\nqk,W,U (x) := e\u27e8U\u0302k,W\u03b6(x)\u27e9F\u2211K k\u2032=1 e \u27e8Uk\u2032 ,W\u03b6(x)\u27e9F\nWe may now state the first lemma of this section: Lemma S. The partial derivatives of R0 with respect to uk,\u2113 and w(\u03b1,\u03b2) are given by\n\u2212 \u2202R0 \u2202uk,\u2113 (W,U) = nc\u2211 \u03b1=1 sc\u2211 \u03b2=1 \u03a6(\u03b1,\u03b2),(k,\u2113)(W,U) w(\u03b1,\u03b2)\n\u2212 \u2202R0 \u2202w(\u03b1,\u03b2) (W,U) = K\u2211 k=1 L\u2211 \u2113=1 \u03a6(\u03b1,\u03b2),(k,\u2113)(W,U) uk,\u2113\nProof. Given K matrices V1, . . . , VK \u2208 Rnw\u00d7KL, we define\nf(V1, . . . , VK) := 1\nK K\u2211 k=1 \u2211 x\u2208Xk \u2113 ( \u27e8V1, \u03b6(x)\u27e9F , . . . , \u27e8VK , \u03b6(x)\u27e9F ; k ) Dzk(x)\nwhere \u2113(y1, . . . , yK ; k) is the cross entropy loss\n\u2113(y1, . . . , yK ; k) = \u2212 log ( exp (yk)\u2211K\nk\u2032=1 exp (yk\u2032) ) The partial derivative of f with respect to the matrix Vj can easily be found to be\n\u2212 \u2202f \u2202Vj (V1, . . . , VK) = 1 K K\u2211 k=1 \u2211 x\u2208Xk\n( 1{j=k} \u2212\ne\u27e8Vj ,\u03b6(x)\u27e9F\u2211K k\u2032=1 e \u27e8Vk\u2032 ,\u03b6(x)\u27e9F\n) \u03b6(x) Dzk(x) (130)\nWe then recall from (19) that the kth entry of the vector y = hW,U (x) is yk = \u2329 U\u0302k , W \u03b6(x) \u232a F = \u2329 WT U\u0302k , \u03b6(x) \u232a F\nand so the unregularized risk can be expressed in term of the function f :\nR0(W,U) = 1\nK K\u2211 k=1 \u2211 x\u2208Xk \u2113 ( \u27e8WT U\u03021, \u03b6(x)\u27e9F , . . . , \u27e8WT U\u0302K , \u03b6(x)\u27e9F ; k ) Dzk(x)\n= f(WT U\u03021, . . . ,W T U\u0302K)\nThe chain rule then gives\n\u2202R0 \u2202W (W,U) = K\u2211 j=1 U\u0302j [ \u2202f \u2202Vj (WT U\u03021, . . . ,W T U\u0302K) ]T (131)\n\u2202R0 \u2202U\u0302j (W,U) =W\n[ \u2202f\n\u2202Vj (WT U\u03021, . . . ,W T U\u0302K)\n] (132)\nUsing formula (130) for \u2202f/\u2202Vj and the notation\nqj,W,U (x) := e\u27e8W T U\u0302j ,\u03b6(x)\u27e9 F\u2211K\nk\u2032=1 e \u27e8WTUk\u2032 ,\u03b6(x)\u27e9F\nwe can express (131) and (132) as follow\n\u2212\u2202R0 \u2202W (W,U) = K\u2211 j=1 U\u0302j\n[ 1\nK K\u2211 k=1 \u2211 x\u2208Xk ( 1{j=k} \u2212 qj,W,U (x) ) \u03b6(x) Dzk(x)\n]T\n\u2212\u2202R0 \u2202U\u0302j (W,U) =W\n[ 1\nK K\u2211 k=1 \u2211 x\u2208Xk ( 1{j=k} \u2212 qj,W,U (x) ) \u03b6(x) Dzk(x)\n]\nWe now compute the partial derivative of R0 with respect to uj,\u2113. Let e\u2113 \u2208 RL be the \u2113th basis vector. We then have\n\u2212 \u2202R0 \u2202uj,\u2113 (W,U) = \u2212 [ \u2202R0 \u2202U\u0302j (W,U) ] e\u2113\n= 1\nK K\u2211 k=1 \u2211 x\u2208Xk ( 1{j=k} \u2212 qj,W,U (x) ) (W\u03b6(x) e\u2113) Dzk(x)\nRecall from (13) that W\u03b6(x) is the matrix that contains the d-dimensional embeddings of the words that constitute the sentence x \u2208 X . So W\u03b6(x) e\u2113 is simply the embedding of the \u2113th word of the sentence x, and we can write it as\nW\u03b6(x) e\u2113 = nc\u2211 \u03b1=1 sc\u2211 \u03b2=1 1{x\u2113=(\u03b1,\u03b2)}w(\u03b1,\u03b2)\nWe therefore have\n\u2212 \u2202R0 \u2202uj,\u2113 (W,U) = 1 K K\u2211 k=1 \u2211 x\u2208Xk ( 1{j=k} \u2212 qj,W,U (x) )  nc\u2211 \u03b1=1 sc\u2211 \u03b2=1 1{x\u2113=(\u03b1,\u03b2)}w(\u03b1,\u03b2)  Dzk(x) =\nnc\u2211 \u03b1=1 sc\u2211 \u03b2=1\n( 1\nK K\u2211 k=1 \u2211 x\u2208Xk ( 1{j=k} \u2212 qj,W,U (x) ) 1{x\u2113=(\u03b1,\u03b2)}Dzk(x)\n) w(\u03b1,\u03b2)\n= nc\u2211 \u03b1=1 sc\u2211 \u03b2=1 \u03a6(\u03b1,\u03b2),(j,\u2113)(W,U) w(\u03b1,\u03b2)\nwhich is the desired formula.\nWe now compute the gradient with respect w(\u03b1,\u03b2). Recalling that \u03b6(\u03b1, \u03b2) is the one hot vector associate with word (\u03b1, \u03b2), we have\n\u2212 \u2202R0 \u2202w(\u03b1,\u03b2) (W,U) = \u2212 [ \u2202R0 \u2202W (W,U) ] \u03b6(\u03b1, \u03b2)\n= 1\nK K\u2211 j=1 K\u2211 k=1 \u2211 x\u2208Xk ( 1{j=k} \u2212 qj,W,U (x) ) ( U\u0302j \u03b6(x) T \u03b6(\u03b1, \u03b2) ) Dzk(x)\nRecall that the \u2113th column of \u03b6(x) is the one-hot encoding of the \u2113th word in the sentence x. Therefore, the \u2113th entry of the vector \u03b6(x)T \u03b6(\u03b1, \u03b2) \u2208 RL is given by the formula[\n\u03b6(x)T \u03b6(\u03b1, \u03b2) ] \u2113 = { 1 if x\u2113 = (\u03b1, \u03b2) 0 otherwise\nAs a consequence\nU\u0302j \u03b6(x) T \u03b6(\u03b1, \u03b2) = L\u2211 \u2113=1 1{x\u2113=(\u03b1,\u03b2)}uj,\u2113\nwhich leads to\n\u2212 \u2202R0 \u2202w(\u03b1,\u03b2) (W,U) = 1 K K\u2211 j=1 K\u2211 k=1 \u2211 x\u2208Xk ( 1{j=k} \u2212 qj,W,U (x) ) ( L\u2211 \u2113=1 1{x\u2113=(\u03b1,\u03b2)}uj,\u2113 ) Dzk(x)\n= L\u2211 \u2113=1 K\u2211 j=1\n( 1\nK K\u2211 k=1 \u2211 x\u2208Xk ( 1{j=k} \u2212 qj,W,U (x) ) 1{x\u2113=(\u03b1,\u03b2)} ) Dzk(x) uj,\u2113\n= L\u2211 \u2113=1 K\u2211 j=1 \u03a6(\u03b1,\u03b2),(j,\u2113)(W,U) uj,\u2113\nwhich is the desired formula."
        },
        {
            "heading": "G.1 GRADIENT OF THE RISK FOR WEIGHTS IN TYPE-III COLLAPSE CONFIGURATION",
            "text": "In lemma S we computed the gradient of the risk for any possible weights (W,U) and for any possible latent variables z1, . . . , zK . In this section we will derive a formula for the gradient when the weights are in type-III collapse configuration and when the latent variables satisfy {z1, . . . , zK} = Z . We start by recalling the definition of a type-III collapse configuration. Definition G (Type-III Collapse). The weights (W,U) of the network hW,U form a type-III collapse configuration if and only if\ni) There exists positive scalars r\u03b2 \u2265 0 so that w(\u03b1, \u03b2) = r\u03b2 f\u03b1 for all (\u03b1, \u03b2) \u2208 V .\nii) There exists c \u2265 0 so that uk,\u2113 = c f\u03b1 for all (k, \u2113) satisfying zk,\u2113 = \u03b1 and all \u03b1 \u2208 C.\nhold for some collection f1, . . . , fnc \u2208 Rd of equiangular vectors.\nWe also define the constant \u03b3 \u2208 R and the sigmoid \u03c3 : R \u2192 R as follow:\n\u03b3 := 1\nnc \u2212 1 and \u03c3(x) :=\n1\n1 + \u03b3e(1+\u03b3)x (133)\nThe goal of this subsection is to prove the following proposition. Proposition C. Suppose K = nLc and {z1, . . . , zK} = Z . If the weights (W,U) are in a type-III collapse configuration with constants c, r1, . . . , rsc \u2265 0, then\n\u2212 \u2202R0 \u2202uk,\u2113 (W,U) = 1 c 1 + \u03b3 nLc  sc\u2211 \u03b2=1 \u00b5\u03b2 \u03c3(c r\u03b2) r\u03b2  uk,\u2113 \u2212 \u2202R0 \u2202w(\u03b1,\u03b2) (W,U) = c L(1 + \u03b3) nc \u00b5\u03b2 \u03c3(c r\u03b2) r\u03b2 w(\u03b1,\u03b2)\nImportantly, note that the above proposition states that \u2202R0/\u2202uk,\u2113 and uk,\u2113 are aligned, and that \u2202R0/\u2202w(\u03b1,\u03b2) and w(\u03b1,\u03b2) are aligned. We start by introducing some notations which will make these gradient computations easier. The latent variables z1, . . . , zK will be written as\nzk = [ zk,1 , zk,2 , . . . , zk,L ] \u2208 Z\nwhere 1 \u2264 zk,\u2113 \u2264 nc. We remark that any sentence x generated by the latent variable zk must be of the form\nx = [(zk,1, \u03b21), . . . , (zk,L, \u03b2L)]\nfor some (\u03b21, . . . , \u03b2L) \u2208 [nc]L, and that this sentence has a probability \u00b5\u03b21\u00b5\u03b22 \u00b7 \u00b7 \u00b7\u00b5\u03b2L of being sampled. In light of this, we make the following definitions. For every \u03b2 = (\u03b21, . . . , \u03b2L) \u2208 [nLc ] we let\nxk,\u03b2 := [(zk,1, \u03b21), . . . , (zk,L, \u03b2L)] \u2208 X (134) \u00b5[\u03b2] := \u00b5[\u03b21]\u00b5[\u03b22] \u00b7 \u00b7 \u00b7 \u00b5[\u03b2L] \u2208 [0, 1] (135)\nwhere we have used \u00b5[\u03b2\u2113] instead of \u00b5\u03b2\u2113 in order to avoid the double subscript. With these definitions at hand we have that Dzj (xk,\u03b2) = { \u00b5[\u03b2] if k = j 0 otherwise\nWe are now ready to prove proposition C. We break the computation into four lemmas. The first one simply uses the notations that we just introduced in order to express \u03a6(\u03b1,\u03b2),(k,\u2113) in a more convenient format. Lemma T. The quantity \u03a6(\u03b1\u22c6,\u03b2\u22c6),(k,\u2113)(W,U) can be expressed as\n\u03a6(\u03b1\u22c6,\u03b2\u22c6),(k,\u2113)(W,U) = 1\nK \u2211 \u03b2\u2208[nLc ] 1{\u03b2\u2113=\u03b2\u22c6} 1{zk,\u2113=\u03b1\u22c6} \u2212 K\u2211 j=1 1{zj,\u2113=\u03b1\u22c6} qk,W,U (xj,\u03b2)  \u00b5[\u03b2].\nProof. Using the above notations, we rewrite \u03a6(\u03b1,\u03b2),(k,\u2113)(W,U) as follow:\n\u03a6(\u03b1\u22c6,\u03b2\u22c6),(k,\u2113)(W,U) = 1\nK K\u2211 j=1 \u2211 x\u2208Xj 1{x\u2113=(\u03b1\u22c6,\u03b2\u22c6)} ( 1{j=k} \u2212 qk,W,U (x) ) Dzj (x)\n= 1\nK K\u2211 j=1 \u2211 \u03b2\u2208[nLc ] 1{(zj,\u2113,\u03b2\u2113)=(\u03b1\u22c6,\u03b2\u22c6)} ( 1{j=k} \u2212 qk,W,U (xj,\u03b2) ) Dzj (xj,\u03b2)\n= 1\nK K\u2211 j=1 \u2211 \u03b2\u2208[nLc ] 1{zj,\u2113=\u03b1\u22c6}1{\u03b2\u2113=\u03b2\u22c6} ( 1{j=k} \u2212 qk,W,U (xj,\u03b2) ) \u00b5[\u03b2]\n= 1\nK \u2211 \u03b2\u2208[nLc ] 1{\u03b2\u2113=\u03b2\u22c6}  K\u2211 j=1 1{zj,\u2113=\u03b1\u22c6} ( 1{j=k} \u2212 qk,W,U (xj,\u03b2) ) \u00b5[\u03b2] To conclude the proof we simply remark that \u2211 j 1{zj,\u2113=\u03b1\u22c6}1{j=k} = 1{zk,\u2113=\u03b1\u22c6}.\nThe following notation will be needed in our next lemma:\n\u03b4(\u03b1, \u03b1\u2032) =\n{ 1 if \u03b1 = \u03b1\u2032\n\u2212\u03b3 if \u03b1 \u0338= \u03b1\u2032 for all \u03b1, \u03b1 \u2032 \u2208 [nc] (136)\nwhere we recall that \u03b3 = 1/(nc \u2212 1). We think of \u03b4(\u03b1, \u03b1\u2032) as a \u2018biased Kroecker delta\u2019 on the concepts. Importantly, note that if f1, . . . , fnc are equiangular, then \u27e8f\u03b1, f\u03b1\u2032\u27e9 = \u03b4(\u03b1, \u03b1\u2032) which is the motivation behind this definition. We may now state our second lemma. Lemma U. Assume K = nLc and {z1, . . . , zK} = Z . Assume also that the weights (W,U) are in a type-III collapse configuration with constants c, r1, . . . , rsc \u2265 0. Then\nqk,W,U (xj,\u03b2) =\n\u220fL \u2113=1 exp ( c r\u03b2\u2113 \u03b4(zj,\u2113, zk,\u2113) ) \u220fL \u2113=1 \u03c8(c r\u03b2\u2113)\nwhere \u03c8(x) = ex + 1 \u03b3 e\u2212\u03b3x.\nfor all j, k \u2208 [K] and all \u03b2 = (\u03b21, . . . , \u03b2L) \u2208 [nc]L.\nProof. Recalling that xj,\u03b2 := [(zj,1, \u03b21), . . . , (zj,L, \u03b2L)], we obtain\u2329 U\u0302k,W\u03b6(xj,\u03b2) \u232a F = L\u2211 \u2113=1 \u27e8uk,\u2113,w(zj,\u2113,\u03b2\u2113)\u27e9 = L\u2211 \u2113=1 \u27e8 c fzk,\u2113 , r\u03b2\u2113 fzj,\u2113 \u27e9 = c L\u2211 \u2113=1 r\u03b2\u2113 \u03b4(zk,\u2113 , zj,\u2113) We then have\nqk,W,U (xj,\u03b2) = e\u27e8U\u0302k,W\u03b6(xj,\u03b2)\u27e9F\u2211K\nk\u2032=1 e \u27e8U\u0302k\u2032 ,W\u03b6(xj,\u03b2)\u27e9F\n= exp\n( c \u2211L \u2113=1 r\u03b2\u2113 \u03b4(zk,\u2113 , zj,\u2113) ) \u2211K k\u2032=1 exp ( c \u2211L \u2113=1 r\u03b2\u2113 \u03b4(zk\u2032,\u2113 , zj,\u2113)\n) = \u220fL \u2113=1 exp ( c r\u03b2\u2113 \u03b4(zk,\u2113 , zj,\u2113)\n) \u2211K k\u2032=1 \u220fL \u2113=1 exp ( c r\u03b2\u2113 \u03b4(zk\u2032,\u2113 , zj,\u2113)\n) Since {z1, . . . , zK} = Z , the latent variables zk\u2032 = [zk\u2032,1, . . . , zk\u2032,L] achieve all possible tuples [\u03b1\u20321, \u00b7 \u00b7 \u00b7 , \u03b1\u2032L] \u2208 [nc]L. The bottom term can therefore be expressed as K\u2211 k\u2032=1 L\u220f \u2113=1 exp ( c r\u03b2\u2113 \u03b4(zk\u2032,\u2113 , zj,\u2113)\n) =\nnc\u2211 \u03b1\u20321=1 nc\u2211 \u03b1\u20322=1 \u00b7 \u00b7 \u00b7 nc\u2211 \u03b1\u2032L=1 exp ( c r\u03b21\u03b4(\u03b1 \u2032 1, zj,1) ) exp ( c r\u03b22\u03b4(\u03b1 \u2032 2, zj,2) ) \u00b7 \u00b7 \u00b7 exp ( c r\u03b2L\u03b4(\u03b1 \u2032 L, zj,L) )\n= L\u220f \u2113=1  nc\u2211 \u03b1\u2032\u2113=1 exp ( c r\u03b2\u2113\u03b4(\u03b1 \u2032 \u2113, zk,\u2113) )\nRecalling the definition of \u03b4(\u03b1, \u03b1\u2032), we find that\nnc\u2211 \u03b1\u2032\u2113=1 exp ( c r\u03b2\u2113\u03b4(\u03b1 \u2032 \u2113, zk,\u2113) ) = exp(c r\u03b2\u2113) + \u2211 \u03b1\u2032\u2113 \u0338=zk,\u2113 exp ( \u2212 c r\u03b2\u2113 nc \u2212 1 )\n= exp(c r\u03b2\u2113) + (nc \u2212 1) exp ( \u2212 c r\u03b2\u2113 nc \u2212 1 ) = \u03c8(c r\u03b2\u2113) (137)\nWe now find a convenient expression for the term appearing between parenthesis in the statement of lemma T. Lemma V. Assume K = nLc and {z1, . . . , zK} = Z . Assume also that the weights (W,U) are in a type-III collapse configuration with constants c, r1, . . . , rsc \u2265 0. Then\n1{zk,\u2113=\u03b1\u22c6} \u2212 K\u2211 j=1 1{zj,\u2113=\u03b1\u22c6} qk,W,U (xj,\u03b2) = \u03b4(zk,\u2113, \u03b1 \u22c6) \u03c3(c r\u03b2\u2113) (138)\nfor all k \u2208 [K], \u2113 \u2208 [L], \u03b1\u22c6 \u2208 [nc] and all \u03b2 = (\u03b21, . . . , \u03b2L) \u2208 [nc]L.\nProof. For simplicity we are going to prove equation (138) in the case \u2113 = 1. Using the previous lemma we obtain\nK\u2211 j=1 1{zj,1=\u03b1\u22c6}qk,W,U (xj,\u03b2) = K\u2211 j=1 1{zj,1=\u03b1\u22c6}\n\u220fL \u2113=1 exp ( c r\u03b2\u2113\u03b4(zj,\u2113, zk,\u2113) ) \u220fL \u2113=1 \u03c8(c r\u03b2\u2113)\nSince the latent variables zj = [zj,1, . . . , zj,L] achieve all possible tuples [\u03b11, \u00b7 \u00b7 \u00b7 , \u03b1L] \u2208 [nc]L, we can rewrite the above as\nnc\u2211 \u03b11=1 nc\u2211 \u03b12=1 \u00b7 \u00b7 \u00b7 nc\u2211 \u03b1L=1 1{\u03b11=\u03b1\u22c6}\n\u220fL \u2113=1 exp ( c r\u03b2\u2113\u03b4(\u03b1\u2113, zk,\u2113) ) \u220fL \u2113=1 \u03c8(c r\u03b2\u2113)\n= nc\u2211 \u03b12=1 \u00b7 \u00b7 \u00b7 nc\u2211 \u03b1L=1\nexp ( c r\u03b21\u03b4(\u03b1 \u22c6, zk,1) ) \u220fL \u2113=2 exp ( c r\u03b2\u2113\u03b4(\u03b1\u2113, zk,\u2113) ) \u220fL \u2113=1 \u03c8(c r\u03b2\u2113)\n= exp\n( c r\u03b21\u03b4(\u03b1 \u22c6, zk,1) )\n\u220fL \u2113=1 \u03c8(c r\u03b2\u2113)\nnc\u2211 \u03b12=1 \u00b7 \u00b7 \u00b7 nc\u2211 \u03b1L=1 L\u220f \u2113=2 exp ( c r\u03b2\u2113\u03b4(\u03b1\u2113, zk,\u2113) ) (139)\nWe then note that nc\u2211 \u03b12=1 \u00b7 \u00b7 \u00b7 nc\u2211 \u03b1L=1 L\u220f \u2113=2 exp ( c r\u03b2\u2113\u03b4(\u03b1\u2113, zk,\u2113) ) = L\u220f \u2113=2  nc\u2211 \u03b1\u2032\u2113=1 exp ( c r\u03b2\u2113\u03b4(\u03b1\u2113, zk,\u2113) ) and, repeating computation (137), we find that\nnc\u2211 \u03b1\u2113=1 exp (c r\u03b2\u2113\u03b4(\u03b1\u2113, zk,\u2113)) = \u03c8(c r\u03b2\u2113)\nGoing back to (139) we therefore have\nK\u2211 j=1 1{zj,1=\u03b1\u22c6}qk,W,U (xj,\u03b2) = exp\n( c r\u03b21\u03b4(\u03b1 \u22c6, zk,1) )\n\u220fL \u2113=1 \u03c8(c r\u03b2\u2113)\nL\u220f \u2113=2 \u03c8(c r\u03b2\u2113) = exp\n( c r\u03b21\u03b4(\u03b1 \u22c6, zk,1) )\n\u03c8(c r\u03b21)\nand so\n1{zk,1=\u03b1\u22c6} \u2212 K\u2211 j=1 1{zj,1=\u03b1\u22c6} qk,W,U (xj,\u03b2) = 1\u2212 exp(c r\u03b21) \u03c8(c r\u03b21 ) if zk,1 = \u03b1\u22c6\n\u2212 exp(\u2212\u03b3 c r\u03b21)\u03c8(c r\u03b21 ) if zk,1 \u0338= \u03b1 \u22c6\n(140)\nWe now manipulate the above formula. Recalling that \u03b3 = 1/(nc \u2212 1), and recalling the definition of \u03c8(x), we get\n1\u2212 e x \u03c8(x) = 1\u2212 e x\nex + 1\u03b3 e \u2212\u03b3x =\n1\n1 + \u03b3e(1+\u03b3)x = \u03c3(x) (141)\nand\n\u2212e \u2212\u03b3x\n\u03c8(x) = \u2212 e\n\u2212\u03b3x\nex + 1\u03b3 e \u2212\u03b3x = \u2212\u03b3\n( 1\n1 + \u03b3e(1+\u03b3)x\n) = \u2212\u03b3\u03c3(x)\nwhich concludes the proof.\nOur last lemma provides a formula for the quantity \u03a6(\u03b1\u22c6,\u03b2\u22c6),(k,\u2113)(W,U) when the weights are in a type-III collapse configuration. Lemma W. Assume K = nLc and {z1, . . . , zK} = Z . Assume also that the weights (W,U) are in a type-III collapse configuration with constants c, r1, . . . , rsc \u2265 0. Then\n\u03a6(\u03b1,\u03b2),(k,\u2113)(W,U) = \u00b5\u03b2 nLc \u03c3(c r\u03b2) \u03b4(zk,\u2113, \u03b1) (142)\nfor all k \u2208 [K], \u2113 \u2208 [L], \u03b1 \u2208 [nc] and \u03b2 \u2208 [sc].\nProof. Combining lemmas T and V, and recalling that K = nLc , we obtain\n\u03a6(\u03b1\u22c6,\u03b2\u22c6),(k,\u2113)(W,U) = 1\nnLc \u2211 \u03b2\u2208[nLc ] 1{\u03b2\u2113=\u03b2\u22c6} 1{zk,\u2113=\u03b1\u22c6} \u2212 K\u2211 j=1 1{zj,\u2113=\u03b1\u22c6} qk,W,U (xj,\u03b2)  \u00b5[\u03b2] = 1\nnLc \u2211 \u03b2\u2208[nLc ] 1{\u03b2\u2113=\u03b2\u22c6} ( \u03b4(zk,\u2113, \u03b1 \u22c6) \u03c3(c r\u03b2\u2113) ) \u00b5[\u03b2]\n= \u03b4(zk,\u2113, \u03b1\n\u22c6)\nnLc\n\u2211 \u03b2\u2208[nLc ] 1{\u03b2\u2113=\u03b2\u22c6} \u03c3(c r\u03b2\u2113)\u00b5[\u03b2]\nChoosing \u2113 = 1 for simplicity we get\u2211 \u03b2\u2208[nLc ] 1{\u03b21=\u03b2\u22c6} \u03c3(c r\u03b21)\u00b5[\u03b2] = sc\u2211 \u03b21=1 sc\u2211 \u03b22=1 \u00b7 \u00b7 \u00b7 sc\u2211 \u03b2L=1 1{\u03b21=\u03b2\u22c6} \u03c3(c r\u03b21) \u00b5[\u03b21]\u00b5[\u03b22] \u00b7 \u00b7 \u00b7\u00b5[\u03b2L]\n= sc\u2211 \u03b22=1 \u00b7 \u00b7 \u00b7 sc\u2211 \u03b2L=1 \u03c3(c r\u03b2\u22c6) \u00b5[\u03b2 \u22c6]\u00b5[\u03b22] \u00b7 \u00b7 \u00b7\u00b5[\u03b2L] = \u00b5[\u03b2\u22c6] \u03c3(c r\u03b2\u22c6)\nwhich concludes the proof.\nWe now prove the proposition.\nProof of proposition C. Combining lemmas S and W, and using the fact that w(\u03b1,\u03b2) = r\u03b2f\u03b1, we obtain\n\u2212 \u2202R0 \u2202uk,\u2113 (W,U) = nc\u2211 \u03b1=1 sc\u2211 \u03b2=1 \u03a6(\u03b1,\u03b2),(k,\u2113)(W,U) w(\u03b1,\u03b2)\n= nc\u2211 \u03b1=1 sc\u2211 \u03b2=1 ( \u00b5\u03b2 nLc \u03c3(c r\u03b2) \u03b4(zk,\u2113, \u03b1) ) r\u03b2f\u03b1\n= 1\nnLc  sc\u2211 \u03b2=1 \u00b5\u03b2 \u03c3(c r\u03b2) r\u03b2 ( nc\u2211 \u03b1=1 \u03b4(zk,\u2113, \u03b1) f\u03b1 )\nUsing the fact that \u2211nc \u03b1=1 f\u03b1 = 0 we get\nnc\u2211 \u03b1=1 \u03b4(zk,\u2113, \u03b1) f\u03b1 = fzk,\u2113 \u2212 \u03b3 \u2211 \u03b1 \u0338=zk,\u2113 f\u03b1 = fzk,\u2113 + \u03b3 fzk,\u2113 \u2212 \u03b3 nc\u2211 \u03b1=1 f\u03b1 = (1 + \u03b3) fzk,\u2113 (143)\nUsing the fact that uk,\u2113 = c fzk,\u2113 we then get\n\u2212 \u2202R0 \u2202uk,\u2113 (W,U) = 1 nLc  sc\u2211 \u03b2=1 \u00b5\u03b2 \u03c3(c r\u03b2) r\u03b2  (1 + \u03b3) fzk,\u2113 = 1 + \u03b3\nnLc  sc\u2211 \u03b2=1 \u00b5\u03b2 \u03c3(c r\u03b2) r\u03b2  uk,\u2113 c\nwhich is the desired formula.\nMoving to the other gradient we get\n\u2212 \u2202R0 \u2202w(\u03b1,\u03b2) (W,U) = K\u2211 k=1 L\u2211 \u2113=1 \u03a6(\u03b1,\u03b2),(k,\u2113)(W,U) uk,\u2113\n= K\u2211 k=1 L\u2211 \u2113=1 ( \u00b5\u03b2 nLc \u03c3(c r\u03b2) \u03b4(zk,\u2113, \u03b1) ) c fzk,\u2113\n= \u00b5\u03b2 nLc \u03c3(c r\u03b2) c L\u2211 \u2113=1 ( K\u2211 k=1 \u03b4(zk,\u2113, \u03b1) fzk,\u2113 ) Since the latent variables zk = [zk,1, . . . , zk,L] achieve all possible tuples [\u03b1\u20321, \u00b7 \u00b7 \u00b7 , \u03b1\u2032L] \u2208 [nc]L, we have, fixing \u2113 = 1 for simplicity,\nK\u2211 k=1 \u03b4(zk,1, \u03b1) fzk,1 = nc\u2211 \u03b1\u20321=1 nc\u2211 \u03b1\u20322=1 \u00b7 \u00b7 \u00b7 L\u2211 \u03b1\u2032L=1 \u03b4(\u03b1\u20321, \u03b1) f\u03b1\u20321 = n L\u22121 c L\u2211 \u03b1\u20321=1 \u03b4(\u03b1\u20321, \u03b1) f\u03b1\u20321 (144)\nRepeating computation (143) shows that the above is equal to nL\u22121c (1+ \u03b3) f\u03b1. We then use the fact that w(\u03b1,\u03b2) = r\u03b2f\u03b1 to obtain\n\u2212 \u2202R0 \u2202w(\u03b1,\u03b2) (W,U) = \u00b5\u03b2 nLc\n\u03c3(c r\u03b2) cL ( nL\u22121c (1 + \u03b3) f\u03b1 ) = \u00b5\u03b2 nLc \u03c3(c r\u03b2) cL ( nL\u22121c (1 + \u03b3) w(\u03b1,\u03b2) r\u03b2\n) = \u00b5\u03b2 nc \u03c3(c r\u03b2) cL ( (1 + \u03b3) w(\u03b1,\u03b2) r\u03b2 )\nwhich is the desired formula."
        },
        {
            "heading": "G.2 PROOF OF THE THEOREM AND STUDY OF THE NON-LINEAR SYSTEM",
            "text": "In this subsection we start by proving theorem 2, and then we show that the system (150) \u2013 (151) has a unique solution if the regularization parameter \u03bb is small enough.\nProof of theorem 2. Recall that the regularized risk associated with the network hW,U is defined by\nR(W,U) = R0(W,U) + \u03bb\n2\n( \u2225W\u22252F + \u2225U\u22252F ) (145)\n= R0(W,U) + \u03bb\n2  nc\u2211 \u03b1=1 sc\u2211 \u03b2=1 \u2225w(\u03b1,\u03b2)\u22252 + K\u2211 k=1 L\u2211 \u2113=1 \u2225uk,\u2113\u22252  (146)\nand therefore (W,U) is a critical points if and only if\n\u2212 \u2202R0 \u2202uk,\u2113 (W,U) = \u03bb uk,\u2113 and \u2212 \u2202R0 \u2202w(\u03b1,\u03b2) (W,U) = \u03bb w(\u03b1,\u03b2)\nAccording to proposition C, if (W,U) is in a type-III collapse configuration, then the above equations becomes\n1\nc\n1 + \u03b3\nnLc  sc\u2211 \u03b2=1 \u00b5\u03b2 \u03c3(c r\u03b2) r\u03b2  uk,\u2113 = \u03bb uk,\u2113 and c L(1 + \u03b3) nc \u00b5\u03b2 \u03c3(c r\u03b2) r\u03b2 w(\u03b1,\u03b2) = \u03bb w(\u03b1,\u03b2)\nSo (W,U) is critical if and only if the constants r1, . . . , rsc and c satisfy the sc + 1 equations\n1\nc\n1 + \u03b3\nnLc sc\u2211 \u03b2=1 \u00b5\u03b2 \u03c3(c r\u03b2)r\u03b2 = \u03bb (147)\nc L(1 + \u03b3)\nnc\n\u00b5\u03b2 \u03c3(c r\u03b2)\nr\u03b2 = \u03bb for all \u03b2 \u2208 [sc] (148)\nFrom the second equation we have that\n(1 + \u03b3) \u00b5\u03b2 \u03c3(c r\u03b2) r\u03b2 = nc \u03bb r\n2 \u03b2\nLc\nUsing this we can rewrite the first equation as\n1\nc\n1\nnLc sc\u2211 \u03b2=1 nc \u03bb r 2 \u03b2 Lc = \u03bb which simplifies to\nsc\u2211 \u03b2=1 (r\u03b2 c )2 = LnL\u22121c .\nwhich is the desired equation (see (151)).\nWe now rewrite the second equation as\n\u03bb\nL r\u03b2 c nc (1 + \u03b3)\u03c3(c r\u03b2) = \u00b5\u03b2\nWe then recall that \u03c3(x) := 1 1+\u03b3e(1+\u03b3)x and therefore\nnc (1 + \u03b3) \u03c3(cr\u03b2) = nc 1 + \u03b3 (1 + \u03b3e(1+\u03b3)cr\u03b2 ) = nc \u2212 1 + exp\n( nc\nnc \u2212 1 cr\u03b2 ) and therefore the second equation can be written as\n\u03bb\nL r\u03b2 c\n( nc \u2212 1 + exp ( nc\nnc \u2212 1 c r\u03b2\n)) = \u00b5\u03b2 .\nWe now prove that if the regularization parameter \u03bb is small enough then the system has a unique solution. Proposition D. Assume \u00b51 \u2265 \u00b52 \u2265 . . . \u2265 \u00b5sc > 0 and\n\u03bb2 < L\nnL+1c sc\u2211 \u03b2=1 \u00b52\u03b2 . (149)\nThen the system sc + 1 equations\n\u03bb\nL r\u03b2 c\n( nc \u2212 1 + exp ( nc\nnc \u2212 1 c r\u03b2\n)) = \u00b5\u03b2 for all 1 \u2264 \u03b2 \u2264 sc (150)\nsc\u2211 \u03b2=1 (r\u03b2 c )2 = LnL\u22121c (151)\nhas a unique solution (c, r1, . . . , rsc) \u2208 R sc+1 + . Moreover this solution satisfies r1 \u2265 r2 \u2265 . . . \u2265 rsc > 0.\nProof. Letting \u03c1\u03b2 := r\u03b2/c, the system is equivalent to\ng(c, \u03c1\u03b2) = L\n\u03bbnc \u00b5\u03b2 for all \u03b2 \u2208 [sc] (152)\nsc\u2211 \u03b2=1 \u03c12\u03b2 = Ln L\u22121 c (153)\nfor the unknowns (c, \u03c11, \u03c12, . . . , \u03c1sc) where g(c, x) = x ( 1 + \u03b3e(1+\u03b3)c 2x ) /(1 + \u03b3) and \u03b3 = 1/(nc \u2212 1)\nNote that \u2202g\n\u2202x (c, x) \u2265\n( 1 + \u03b3e(1+\u03b3)c 2x ) /(1 + \u03b3) \u2265 1 \u2200(c, x) \u2208 R\u00d7 [0,+\u221e)\nand therefore x 7\u2192 g(c, x) is strictly increasing on [0,+\u221e). Also note that we have g(c, 0) = 0, lim\nx\u2192+\u221e g(c, x) = +\u221e\nSo x 7\u2192 g(c, x) is a bijection from [0,+\u221e) to [0,+\u221e) as well as a bijection from (0,+\u221e) to (0,+\u221e). Recall that \u00b5\u03b2 \u2208 (0,+\u221e) for all \u03b2 \u2208 [sc]. Therefore given c \u2208 R and \u03b2 \u2208 [sc], the equation\ng(c, x) = L\n\u03bbnc \u00b5\u03b2\nhas a unique solution in (0,+\u221e) that we denote by \u03d5\u03b2(c). In other words, the function \u03d5\u03b2(c) is implicitly defined by\ng(c, \u03d5\u03b2(c)) = L\n\u03bbnc \u00b5\u03b2 . (154)\nAlso, since g(0, x) = x, we have\n\u03d5\u03b2(0) = L\n\u03bbnc \u00b5\u03b2\nClaim F. The function \u03d5\u03b2 : [0,+\u221e) \u2192 (0,+\u221e) is continuous, strictly decreasing, and satisfies limc\u2192+\u221e \u03d5\u03b2(c) = 0.\nProof. We first show that c 7\u2192 \u03d5\u03b2(c) is continuous. Since \u2202g\u2202x (c, x) \u2265 1 for all x \u2265 0, we have g(c, x2)\u2212 g(c, x1) = \u222b x2 x1 \u2202g \u2202x (c, x)dx \u2265 \u222b x2 x1 1dx = x2 \u2212 x1 for all c and all x2 \u2265 x1 \u2265 0.\nAs a consequence, for all c1, c2, we have\n|\u03d5\u03b2(c2)\u2212 \u03d5\u03b2(c1)| \u2264 |g(c1, \u03d5\u03b2(c2))\u2212 g(c1, \u03d5\u03b2(c1))| = |g(c1, \u03d5\u03b2(c2))\u2212 g(c2, \u03d5\u03b2(c2))| (155)\nwhere we have used the fact that g(c1, \u03d5\u03b2(c1)) = L\u03bbnc\u00b5\u03b2 = g(c2, \u03d5\u03b2(c2)). From (155) it is clear that the continuity of c 7\u2192 g(c, x) implies the continuity of c 7\u2192 \u03d5\u03b2(c). We now prove that \u03d5\u03b2 is strictly decreasing on [0,+\u221e). Let 0 \u2264 c1 < c2. Note that for any x > 0, the function c 7\u2192 g(c, x) is strictly increasing on [0,+\u221e). Since \u03d5\u03b2(c) > 0 we therefore have\ng(c2, \u03d5\u03b2(c2)) = L\n\u03bbnc \u00b5\u03b2 = g(c1, \u03d5\u03b2(c1)) < g(c2, \u03d5\u03b2(c1))\nSince x 7\u2192 g(c, x) is strictly increasing for all c, the above implies that \u03d5\u03b2(c2) < \u03d5\u03b2(c1). Finally we show that limc\u2192+\u221e \u03d5\u03b2(c) = 0. Since \u03d5\u03b2 is decreasing and non-negative on [0,+\u221e), the limc\u2192+\u221e \u03d5\u03b2(c) = A is well defined. We obviously have \u03d5\u03b2(c) \u2265 A for all c \u2265 0. Since x 7\u2192 g(c, x) is increasing we have\nL\n\u03bbnc \u00b5\u03b2 = g(c, \u03d5\u03b2(c)) \u2265 g(c, A)\nBut the function g(c, A) is unbounded for all A > 0. Therefore we must have A = 0.\nSystem (152)\u2013(153) is equivalent to\n\u03c1\u03b2 = \u03d5\u03b2(c) for all \u03b2 \u2208 [sc] (156) sc\u2211 \u03b2=1 (\u03d5\u03b2(c)) 2 = LnL\u22121c (157)\nDefine the function\n\u03a6(c) := sc\u2211 \u03b2=1 (\u03d5\u03b2(c)) 2\nThen \u03a6 clearly inherits the properties of the \u03d5\u03b2\u2019s: it is continuous, strictly decreasing, and satisfies\n\u03a6(0) = sc\u2211 \u03b2=1 ( L \u03bbnc \u00b5\u03b2 )2 and lim c\u2192+\u221e \u03a6(c) = 0\nTherefore, if\nLnL\u22121c \u2264 L\u2211 \u03b2=1 ( L \u03bbnc \u00b5\u03b2 )2 then there is a unique c \u2265 0 satisfying (157). Since x 7\u2192 g(c, x) is increasing, equation (152) implies that the corresponding \u03c1\u03b2\u2019s satisfy \u03c11 \u2265 \u03c12 \u2265 . . . \u2265 \u03c1sc > 0.\nH NO SPURIOUS LOCAL MINIMIZER FOR R(W,U).\nIn this section we prove that if d > min(nw,KL), then R(W,U) does not have spurious local minimizers; all local minimizers are global. To do this, we introduce the function\nf : Rd\u00d7KL \u2192 R\ndefine as follow. Any matrix V \u2208 Rd\u00d7KL can be partition intoK submatrices Vk \u2208 Rd\u00d7L according\nV = [V1 V2 \u00b7 \u00b7 \u00b7 VK ] where Vk \u2208 Rd\u00d7L (158)\nThe function f is then defined by the formula\nf(V ) := 1\nK K\u2211 k=1 \u2211 x\u2208Xk \u2113 (\u2329 V1, \u03b6(x) \u232a F , . . . , \u2329 VK , \u03b6(x) \u232a F ; k ) Dzk(x)\nwhere \u2113(y1, . . . , yK ; k) denotes the cross entropy loss\n\u2113(y1, . . . , yK ; k) = \u2212 log ( exp (yk)\u2211K\nk\u2032=1 exp (yk\u2032)\n)\nWe remark that f is clearly convex and differentiable. We then recall from (19) that the kth entry of the vector y = hW,U (x) is\nyk = \u2329 U\u0302k , W \u03b6(x) \u232a F = \u2329 WT U\u0302k , \u03b6(x) \u232a F\nRecalling that U\u0302 = [ U\u03021 \u00b7 \u00b7 \u00b7 U\u0302K ] , we then see that the risk can be expressed as\nR(W,U) = f(WT U\u0302) + \u03bb 2\n( \u2225W\u22252F + \u2225U\u0302\u22252F ) (159)\nThe fact that R(W,U) does not have spurious local minimizers come from the following general theorem.\nTheorem G. Let g : Rm\u00d7n \u2192 R be a convex and differentiable function. Define\n\u03c6(A,B) := g(ATB) + \u03bb\n2\n( \u2225A\u22252F + \u2225B\u22252F ) where A \u2208 Rd\u00d7m and B \u2208 Rd\u00d7n\nand assume \u03bb > 0 and d > min(m,n). Then any local minimizer (A,B) of the function \u03c6 : Rd\u00d7m \u00d7 Rd\u00d7n \u2192 R is also a global minimizer.\nThe above theorem directly apply to (159) and shows that the risk R(W,U) does not have spurious local minimizers when \u03bb > 0 and d > min(nw,KL).\nThe remainder of the section is devoted to the proof of theorem G. We will follow the exact same steps as in Zhu et al. (2021), and provide the proof mostly for completeness (and also to show how the techniques from Zhu et al. (2021) apply to our case). Finally, we refer to Laurent & Brecht (2018) for a proof of theorem G in the case \u03bb = 0.\nProof of theorem G. To prove the theorem it suffices to assume that d > m without loss of generality. To see this, note that the function g\u0303(D) = g(DT ) is also convex and differentiable and note that (A,B) is a local minimum of\ng(ATB) + \u03bb\n2\n( \u2225A\u22252F + \u2225B\u22252F ) if and only if it is a local minimum of\ng\u0303(BTA) + \u03bb\n2\n( \u2225A\u22252F + \u2225B\u22252F ) So the theorem for the case d > n follows by appealing to the case d > m with the function g\u0303.\nSo we may assume d > m. Following Zhu et al. (2021), we define the function \u03c8 : Rm\u00d7n \u2192 R by\n\u03c8(D) := g(D) + \u2225D\u2225\u2217\nwhere \u2225D\u2225\u2217 denote the nuclear norm of D. We then have:\nClaim G. For all A \u2208 Rd\u00d7m and B \u2208 Rd\u00d7n, we have that \u03c8(ATB) \u2264 \u03c6(A,B).\nProof. This is a direct consequence of the inequality\n\u2225ATB\u2225\u2217 \u2264 1\n2\n( \u2225A\u22252F + \u2225B\u22252F ) that we reprove here for completeness. Let ATB = U\u03a3V T be the compact SVD of ATB. That is \u03a3 \u2208 Rr\u00d7r, U \u2208 Rm\u00d7r, V \u2208 Rn\u00d7r, and r is the rank of ATB. We then have\n\u2225ATB\u2225\u2217 = Tr(\u03a3) = Tr(UTATBV ) = \u27e8AU,BV \u27e9F \u2264 1\n2\n( \u2225AU\u22252F + \u2225BV \u22252F ) \u2264 1\n2\n( \u2225A\u22252F + \u2225B\u22252F )\nComputing the derivatives of \u03c6 gives\n\u2202\u03c6 \u2202A (A,B) = B\n[ \u2207g(ATB) ]T + \u03bbA and\n\u2202\u03c6 \u2202B (A,B) = A \u2207g(ATB) + \u03bbB (160)\nSo (A,B) is a critical point of \u03c6 if and only if\n\u03bbA = \u2212B [ \u2207g(ATB) ]T (161)\n\u03bbB = \u2212A \u2207g(ATB) (162)\nImportantly, from the above we get\nAAT = BBT \u2208 Rd\u00d7d (163)\nwhich implies thatA andB have same singular values and same left singular vectors. Let U \u2208 Rd\u00d7d be the orthonormal matrix containing the eigenvectors of AAT = BBT . From this matrix we can construct an SVD for both A and B:\nA = U\u03a3AV T A and B = U\u03a3BV T B\nwhere \u03a3A \u2208 Rd\u00d7m and \u03a3B \u2208 Rd\u00d7n have the same singular values. From this we get the SVD of ATB,\nATB = VA\u03a3 T A\u03a3BV T B (164)\nand it is transparent that,\n\u2225ATB\u2225\u2217 = \u2225A\u22252F = \u2225B\u22252F . (165)\nIn particular this implies that if (A,B) is a critical point of \u03c6, then we must have \u03c6(A,B) = \u03c8(ATB). This also implies that\u2329\n\u2207g(ATB), ATB \u232a F = \u2329 A\u2207g(ATB), B \u232a F = \u2212\u03bb\u2225B\u22252F = \u2212\u03bb\u2225ATB\u2225\u2217 (166)\nUsing this together with the fact that the nuclear norm is the dual of the operator norm, that is \u2225C\u2225\u2217 = sup\u2225G\u2225op\u22641 \u27e8G,C\u27e9F , we easily obtain:\nClaim H. Suppose (A,B) is a critical point of \u03c6 which satisfies \u2225\u2225\u2207g(ATB)\u2225\u2225\nop \u2264 \u03bb, then (A,B)\nis a global minimizer of \u03c6.\nProof. For any matrix C \u2208 Rm\u00d7n we have \u2225ATB\u2225\u2217 + \u2329 \u2212 1 \u03bb \u2207g(ATB), C \u2212ATB \u232a F = \u2329 \u2212 1 \u03bb \u2207g(ATB), C \u232a F \u2264 sup \u2225G\u2225op\u22641 \u27e8G,C\u27e9F = \u2225C\u2225\u2217\nand therefore \u2212 1\u03bb\u2207g(A TB) \u2208 \u2202\u2225ATB\u2225\u2217. This implies that ATB is a global min of \u03c8. The fact that \u03c6(A,B) = \u03c8(ATB) (because (A,B) is a critical point \u03c6) together with Claim G, then implies that (A,B) is a global minimizer of \u03c6. We now show that all local min (A,B) of \u03c6 with ker(AT ) \u0338= \u2205 must satisfy \u2225\u2225\u2207g(ATB)\u2225\u2225\nop \u2264 \u03bb.\nClaim I. Suppose (A,B) is a critical point of \u03c6 which satisfies\n(i) ker(AT ) \u0338= \u2205 (ii) \u2225\u2225\u2207g(ATB)\u2225\u2225\nop > \u03bb\nThen (A,B) is not local min.\nProof. We follow the computation from Zhu et al. (2021). Let (A,B) be a critical point of \u03c6. Since AAT = BBT , we must have that ker(AT ) = ker(AAT ) = ker(BBT ) = ker(BT ). According to (ii) these kernels are non trivial and we may choose a unit vector z \u2208 Rd that belongs to them. We then consider the perturbations\ndA = zaT dB = zbT\nwhere a \u2208 Rm and b \u2208 Rn are unit vectors to be chosen later. Note that since z,a and b are unit vectors we have \u2225dA\u22252F = \u2225dB\u22252F = 1. Moreover, the columns of dA and dB are clearly in the kernel of AT and BT , therefore AT dA = AT dB = BT dA = BT dB = 0. This implies that all the \u2018cross terms\u2019 disappear when expanding the expression:\n(A+ \u03b5dA)T (B + \u03b5dB) = ATB + \u03b52dAT dB = ATB + \u03b52abT\nWe also have \u2225A+ \u03b5dA\u22252F = \u2225A\u22252F + \u2225\u03b5dA\u22252F = \u2225A\u22252F + \u03b52\nand similarly, \u2225B + \u03b5dB\u22252F = \u2225B\u22252F + \u03b52. We then get \u03c6(A+ \u03b5dA,B + \u03b5dB) = g ( (A+ \u03b5dA)T (B + \u03b5dB) ) + \u03bb\n2\n( \u2225A+ \u03b5dA\u22252F + \u2225B + \u03b5dB\u22252F ) = g(ATB + \u03b52abT ) + \u03bb\n2\n( \u2225A\u22252F + \u2225B\u22252F ) + \u03bb\u03b52\n= [ g(ATB) + \u2329 \u2207f(ATB), \u03b52abT \u232a F +O(\u03b54) ] + \u03bb\n2\n( \u2225A\u22252F + \u2225B\u22252F ) + \u03bb\u03b52\n= \u03c6(A,B) + \u03b52 ( \u2329 \u2207g(ATB),abT \u232a F + \u03bb ) +O(\u03b54)\nLet G = \u2207f(ATB) \u2208 Rm\u00d7n. We want to choose the unit vectors a and b that makes \u2329 G,abT \u232a F as negative as possible. The best choice is to choose \u2212a and b to be the first left and right singular vectors of G since this give the negative of the best rank-one approximation of G. So we choose a \u2208 Rm and b \u2208 Rn such that Gb = \u2212\u03c31a, and therefore\u2329\nabT , G \u232a F = Tr(baTG) = Tr(aTGb) = \u2212\u03c31\nwhich gives \u03c6(A+ \u03b5dA,B + \u03b5dB) = \u03c6(A,B) + \u03b52 ( \u2212 \u2225\u2225\u2207g(ATB)\u2225\u2225\nop + \u03bb\n) +O(\u03b54)\nand (ii) implies that (A,B) is not a local min.\nCombining the previous two claims we can easily prove the theorem. Indeed, if d > m, then the kernel of AT is nontrivial and (i) is always satisfied. As a consequence, if (A,B) is a local min of \u03c6, then \u2225\u2225\u2207g(ATB)\u2225\u2225 op \u2264 \u03bb, and therefore (A,B) must be a global min of \u03c6."
        }
    ],
    "year": 2024
}