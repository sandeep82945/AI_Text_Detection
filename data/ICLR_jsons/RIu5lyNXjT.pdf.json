{
    "abstractText": "As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FORMATSPREAD, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights1. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.",
    "authors": [],
    "id": "SP:493a4e3931a198a97012253f5520a440a281cda8",
    "references": [
        {
            "authors": [
                "Armen Aghajanyan"
            ],
            "title": "Tweet: Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM\u2019s are broken. Evaluating a task requires marginalizing across all prompts",
            "year": 2023
        },
        {
            "authors": [
                "Ebtesam Almazrouei",
                "Hamza Alobeidli",
                "Abdulaziz Alshamsi",
                "Alessandro Cappelli",
                "Ruxandra Cojocaru",
                "Merouane Debbah",
                "Etienne Goffinet",
                "Daniel Heslow",
                "Julien Launay",
                "Quentin Malartic",
                "Badreddine Noune",
                "Baptiste Pannier",
                "Guilherme Penedo"
            ],
            "title": "Falcon-40B: an open large language model with state-of-the-art performance",
            "year": 2023
        },
        {
            "authors": [
                "Olivier Chapelle",
                "Lihong Li"
            ],
            "title": "An empirical evaluation of thompson sampling",
            "venue": "Advances in neural information processing systems,",
            "year": 2011
        },
        {
            "authors": [
                "Tianqi Chen",
                "Carlos Guestrin"
            ],
            "title": "XGBoost: A scalable tree boosting system",
            "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD",
            "year": 2016
        },
        {
            "authors": [
                "Jonathan H Clark",
                "Chris Dyer",
                "Alon Lavie",
                "Noah A Smith"
            ],
            "title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability",
            "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2011
        },
        {
            "authors": [
                "Mingkai Deng",
                "Jianyu Wang",
                "Cheng-Ping Hsieh",
                "Yihan Wang",
                "Han Guo",
                "Tianmin Shu",
                "Meng Song",
                "Eric Xing",
                "Zhiting Hu"
            ],
            "title": "RLPrompt: Optimizing discrete text prompts with reinforcement learning",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Tim Dettmers",
                "Mike Lewis",
                "Younes Belkada",
                "Luke Zettlemoyer"
            ],
            "title": "GPT3.int8(): 8-bit matrix multiplication for transformers at scale",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ning Ding",
                "Shengding Hu",
                "Weilin Zhao",
                "Yulin Chen",
                "Zhiyuan Liu",
                "Haitao Zheng",
                "Maosong Sun"
            ],
            "title": "Openprompt: An open-source framework for prompt-learning",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,",
            "year": 2022
        },
        {
            "authors": [
                "Yanai Elazar",
                "Nora Kassner",
                "Shauli Ravfogel",
                "Abhilasha Ravichander",
                "Eduard Hovy",
                "Hinrich Sch\u00fctze",
                "Yoav Goldberg"
            ],
            "title": "Measuring and improving consistency in pretrained language models",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen"
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "doi: 10.18653/v1/2021.acl-long.295. URL https: //aclanthology.org/2021.acl-long.295",
            "year": 2021
        },
        {
            "authors": [
                "Hila Gonen",
                "Srini Iyer",
                "Terra Blevins",
                "Noah A Smith",
                "Luke Zettlemoyer"
            ],
            "title": "Demystifying prompts in language models via perplexity estimation",
            "venue": "arXiv preprint arXiv:2212.04037,",
            "year": 2022
        },
        {
            "authors": [
                "Peter Henderson",
                "Riashat Islam",
                "Philip Bachman",
                "Joelle Pineau",
                "Doina Precup",
                "David Meger"
            ],
            "title": "Deep reinforcement learning that matters",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Or Honovich",
                "Uri Shaham",
                "Samuel R. Bowman",
                "Omer Levy"
            ],
            "title": "Instruction induction: From few examples to natural language task descriptions",
            "venue": "doi: 10.18653/v1/ 2023.acl-long.108. URL https://aclanthology.org/2023.acl-long.108",
            "year": 2023
        },
        {
            "authors": [
                "Gabriel Ilharco",
                "Marco Tulio Ribeiro",
                "Mitchell Wortsman",
                "Suchin Gururangan",
                "Ludwig Schmidt",
                "Hannaneh Hajishirzi",
                "Ali Farhadi"
            ],
            "title": "Editing models with task arithmetic",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Riashat Islam",
                "Peter Henderson",
                "Maziar Gomrokchi",
                "Doina Precup"
            ],
            "title": "Reproducibility of benchmarked deep reinforcement learning tasks for continuous control",
            "venue": "arXiv preprint arXiv:1708.04133,",
            "year": 2017
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Frank F Xu",
                "Jun Araki",
                "Graham Neubig"
            ],
            "title": "How can we know what language models know",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Xinxi Lyu",
                "Sewon Min",
                "Lianhui Qin",
                "Kyle Richardson",
                "Sean Welleck",
                "Hannaneh Hajishirzi",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Sameer Singh",
                "Yejin Choi"
            ],
            "title": "Prompt waywardness: The curious case of discretized interpretation of continuous prompts",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Tze Leung Lai",
                "Herbert Robbins"
            ],
            "title": "Asymptotically efficient adaptive allocation rules",
            "venue": "Advances in applied mathematics,",
            "year": 1985
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant"
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Chin-Yew Lin"
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "In Text summarization branches out,",
            "year": 2004
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp"
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
            "venue": "doi: 10.18653/v1/2022.acl-long.556. URL https://aclanthology",
            "year": 2022
        },
        {
            "authors": [
                "Moin Nadeem",
                "Anna Bethke",
                "Siva Reddy"
            ],
            "title": "Stereoset: Measuring stereotypical bias in pretrained language models",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Archiki Prasad",
                "Peter Hase",
                "Xiang Zhou",
                "Mohit Bansal"
            ],
            "title": "GrIPS: Gradient-free, edit-based instruction search for prompting large language models",
            "year": 2023
        },
        {
            "authors": [
                "Reid Pryzant",
                "Dan Iter",
                "Jerry Li",
                "Yin Tat Lee",
                "Chenguang Zhu",
                "Michael Zeng"
            ],
            "title": "Automatic prompt optimization with\u201d gradient descent\u201d and beam search",
            "venue": "arXiv preprint arXiv:2305.03495,",
            "year": 2023
        },
        {
            "authors": [
                "Guanghui Qin",
                "Jason Eisner"
            ],
            "title": "Learning how to ask: Querying lms with mixtures of soft prompts",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),",
            "year": 2021
        },
        {
            "authors": [
                "Timo Schick",
                "Sahana Udupa",
                "Hinrich Sch\u00fctze"
            ],
            "title": "Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "John Schulman",
                "Barret Zoph",
                "Christina Kim",
                "Jacob Hilton",
                "Jacob Menick",
                "Jiayi Weng",
                "Juan Felipe Ceron Uribe",
                "Liam Fedus",
                "Luke Metz",
                "Michael Pokorny"
            ],
            "title": "Chatgpt: Optimizing language models for dialogue",
            "venue": "OpenAI blog,",
            "year": 2022
        },
        {
            "authors": [
                "Weijia Shi",
                "Xiaochuang Han",
                "Hila Gonen",
                "Ari Holtzman",
                "Yulia Tsvetkov",
                "Luke Zettlemoyer"
            ],
            "title": "Toward human readable prompt tuning: Kubrick\u2019s the shining is a good movie, and a good prompt too",
            "venue": "arXiv preprint arXiv:2212.10539,",
            "year": 2022
        },
        {
            "authors": [
                "Taylor Shin",
                "Yasaman Razeghi",
                "Robert L. Logan IV",
                "Eric Wallace",
                "Sameer Singh"
            ],
            "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
            "venue": "doi: 10.18653/v1/2020.emnlp-main.346. URL https: //aclanthology.org/2020.emnlp-main.346",
            "year": 2020
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "arXiv preprint arXiv:2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Karia",
                "Savan Doshi",
                "Shailaja Keyur Sampat",
                "Siddhartha Mishra",
                "Sujan Reddy A",
                "Sumanta Patro",
                "Tanay Dixit",
                "Xudong Shen"
            ],
            "title": "Super-NaturalInstructions: Generalization via declarative instructions on 1600+",
            "venue": "NLP tasks. pp",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Wei",
                "Nika Haghtalab",
                "Jacob Steinhardt"
            ],
            "title": "Jailbroken: How does llm safety training fail",
            "venue": "arXiv preprint arXiv:2307.02483,",
            "year": 2023
        },
        {
            "authors": [
                "Jules White",
                "Quchen Fu",
                "Sam Hays",
                "Michael Sandborn",
                "Carlos Olea",
                "Henry Gilbert",
                "Ashraf Elnashar",
                "Jesse Spencer-Smith",
                "Douglas C Schmidt"
            ],
            "title": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
            "venue": "arXiv preprint arXiv:2302.11382,",
            "year": 2023
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi"
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Andrei Ioan Muresanu",
                "Ziwen Han",
                "Keiran Paster",
                "Silviu Pitis",
                "Harris Chan",
                "Jimmy Ba"
            ],
            "title": "Large language models are human-level prompt engineers",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Andy Zou",
                "Zifan Wang",
                "J Zico Kolter",
                "Matt Fredrikson"
            ],
            "title": "Universal and transferable adversarial attacks on aligned language models",
            "venue": "arXiv preprint arXiv:2307.15043,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "As the capabilities of LLMs have rapidly improved, their sensitivity to input prompt features has been used to optimize performance via prompt engineering (White et al., 2023). However, there has been little work in characterizing this sensitivity, especially to seemingly innocuous feature choices that preserve prompt meaning and intent. In this work, we analyze the sensitivity of widely used, open-source LLMs to a class of features that should not influence a prompt\u2019s interpretation: formatting choices. We find that pre-trained LLMs are sensitive to these choices in unpredictable ways, with accuracy varying in up to 76 points for LLaMA-2-13B between equivalent formats, and \u223c10 accuracy points on average across 50+ tasks and several models. We also show that this variance is not eliminated by adding few-shot examples, increasing model size, or instruction tuning.\nDesigning prompt templates is a critical part of effectively using a pre-trained language model. This design process includes making choices about wording, choosing few-shot examples for in-context learning, and making decisions about seemingly trivial features like formatting. This process, and often even the resulting templates, is rarely reported or discussed in research papers, under the assumption that performance variance across these choices is insignificant compared to variance across data points or models. However, some anecdotal evidence points to formatting choices actually having a significant influence on model behavior (Aghajanyan, 2023). In some cases, researchers report a limited number of manually generated formats to show that scaling trends hold despite performance being significantly different (Schick et al., 2021). The assumption that formatting does not\n1We will release FORMATSPREAD\u2019s code at upon acceptance.\ninfluence overall model performance may become problematic when improvements over existing approaches are attributed to the amount and source of training data, number of parameters, or model architecture, without also accounting for changes in prompt format. Ignoring variance across formats may also negatively affect user experience, e.g. if users inadvertently choose formats the LLM does not perform well on.\nOur proposed tool, FORMATSPREAD, enables a systematic analysis of these variances across a wide set of semantically equivalent prompt formats within a user-specified computational budget. We find that choices in formatting few-shot examples during in-context learning introduce spurious biases that may lead to significantly different conclusions in model performance. The sensitivity to formatting choices that we discover across widely-used, open-source models suggests that future research would benefit from reporting a performance spread over a sufficient sample of plausible formats, instead of simply reporting the formatting used and its performance, as is currently standard. Moreover, we argue that this reporting is crucial when comparing the performance of different models, as we show the influence of formatting choices only weakly correlates between models, thus making and fixing a formatting choice could introduce a significant confounding factor.\nFully exploring the space of prompt formats is intractable, as computation costs scale linearly with the number of formats considered. FORMATSPREAD efficiently explores the space of prompt formats under a user-specified computational budget using Bayesian optimization. FORMATSPREAD does not require access to the model weights, allowing its use on API-gated models: we find a spread up to 56 accuracy points with a median spread of 6.4 accuracy points with GPT3.5 across 320 formats and 53 tasks at a cost of under 10USD on average per task. Beyond facilitating evaluation, we also propose a suite of analyses to further characterize model sensitivity to formatting. Among other results, we show that the separability of continuous prompt embeddings correlates with the spread observed in task performance."
        },
        {
            "heading": "2 OVERVIEW",
            "text": "We evaluate LLM performance over the space of prompt formats that may plausibly be chosen by a non-adversarial user when designing a prompt for a target task, where the space of formats is defined by a grammar (\u00a73.1). Our grammar\u2019s definition naturally induces a definition of semantic equivalence among formats. We quantify model sensitivity in terms of performance range in a target task across the space of equivalent prompt formats to the original choice (\u00a74.2). We cast the problem of searching across this space as a bandit problem, and propose FORMATSPREAD (\u00a73), which consists of a grammar (\u00a73.1) and a procedure to estimate the minimum and maximum performance across a set of semantically equivalent formats given a pre-defined metric (\u00a73.2). FORMATSPREAD uses Bayesian optimization to identify the expected performance range with low additional computational cost (\u00a74.5) all without requiring access to model weights, which enables use on API-gated\nLLMs. Furthermore, we perform in-depth analysis of this observed sensitivity, including by quantifying the contribution of individual feature choices to the final performance (\u00a74.3) and measuring the identifiability of a format based solely on a model\u2019s internal, continuous representation of any prompt via correlation with model performance (\u00a74.4)."
        },
        {
            "heading": "3 MEASURING SENSITIVITY WITH FORMATSPREAD",
            "text": ""
        },
        {
            "heading": "3.1 GRAMMAR OF PLAUSIBLE PROMPT FORMATS",
            "text": "We construct a grammar that defines both the space of plausible prompt formats and semantic equivalence between formats. The grammar is manually constructed, as opposed to automatically induced from data, to guarantee a higher level of precision when defining the set of equivalent formats. Our grammar is directly tested by verifying that it can generate the formatting associated with 100+ Super-NaturalInstructions tasks (Wang et al., 2022).\nOur grammar consists of fields that are composed to create a prompt format. For example, the format \u2018Passage: <text> || Answer: <text>\u2019, has basic fields \u2018Passage: <text>\u2019, and \u2018Answer: <text>\u2019, denoted a1, and a2. Each basic field consists of a descriptor (e.g. \u2018Passage\u2019), a separator (e.g. \u2018: \u2019), and a text placeholder to replace with each data point. We define basic fields as B1(d, s, f) := f(d)s<text> using Backus-Naur notation, where d is a descriptor string, s\u2208S1 a separator, and f \u2208Fcasing a function that alters d while preserving meaning. Thus, in our example, a1=B1(Passage, \u2019: \u2019, id) and a2=B1(Answer, \u2019: \u2019, id), with id the identity function. We define joining several fields as B(n)2 (X1, . . ., Xn,c) := X1cX2c . . . cXn, with c\u2208C being a space. Our example\u2019s prompt format may be written as B(2)2 (a1, a2, \u2019 || \u2019). The grammar also supports enumeration, which is defined as joining several basic fields, each representing a different list item. For example, the enumeration \u2018Option (A): <text>, Option (B): <text>, Option (C): <text>\u2019may be written as B(3)2 (a1, a2, a3, \u2019 || \u2019), where ai = B1(ei, \u2019: \u2019, id). In our example, e1 represents \u2018Option (A)\u2019, and may in turn be written as the concatenation ei := ds2fitem(i) with d = \u2018Option\u2019, s2 = \u2019 \u2019 (single space), and fitem(1) = \u2018(A)\u2019. Each fitem transforms an item i using a number format (e.g. letters or Roman numerals, denoted as Fitem2) and an item wrapper (e.g. (A) or [A], denoted as Fitem1). In summary, we define valid prompt formats as those accepted by the following grammar:\nB0() := <text>\nB\u20320(d, s) := f(d)s with s \u2208 S1, f \u2208 Fcasing B1(d, s, f) := f(d)s<text> with s \u2208 S1, f \u2208 Fcasing\nB (n) 2 (X1, . . . , Xn, c) := X1c . . . cXn with c \u2208 C, Xi \u2208 {B0, B \u2032 0, B1, B2, B3} \u2200i\nB (n) 3 (d, j1, . . . , jn, s1, s2, c, f1, f2) := B (n) 2 (B1(e1, s1, f2)), . . . , B1(en, s1, f2), c)\nwhere ei := f2(d) s2 f1(ji), ji \u2208 N0 \u2200i, s1 \u2208 S1, s2 \u2208 S2, f1 \u2208 Fitem, f2 \u2208 Fcasing\nOur grammar defines valid formats as finite compositions of B0, B\u20320, B1, B2, B3. The sets S1,S2, C, Fcasing, Fitem (two sets of separators, spaces, casing functions, and itemizing functions respectively) are pre-defined by the user. Throughout this work, we instantiate all sets with values typically observed in human-written prompt formats. We intentionally only modify the casing of descriptors (via Fcasing) to guarantee semantic equivalence; one may also define a set of functions that paraphrases the descriptor, e.g., via synonym replacement. Appendix A.2 contains the full list of values we use for the constant sets, as well as a visualization of a prompt template generated from the grammar.\nPrompt Format Equivalence. Two prompt formats p1, p2 are equivalent if they represent the same rule application Bi, the descriptors (if any) are the same, and the sub-elements (if any) are equivalent. Appendix A.1 contains the formal definition of equivalence. The grammar\u2019s strict definition allows us to assume that sets of equivalent formats share equivalent meanings. When measuring sensitivity (\u00a73.2), we explore only the space of formats equivalent to a task\u2019s original format.\nContextual Restrictions. We define restrictions to the combinations of spaces and separators to further ensure naturalness. For example, if B2(X1,. . . ,Xn,c) where c does not contain a newline, then each Xi\u2019s separators and any subcomponents\u2019 separators should not contain a newline. This\navoids unnatural formats like Input:\\n <text> Output:\\n <text>. We also allow for adding conditions that force constants (separators, spaces, etc.) in different applications of Bi to be equal. When measuring sensitivity to format perturbations, if two separators or spaces are equal in an original format, they are forced to jointly change to be considered equivalent. Appendix A.3 contains all contextual restrictions.\nFinal Prompt Construction. Given a valid format p accepted by the grammar, the final prompt is constructed by concatenating with space c an instruction string inst, n fewshot data points D1, . . . , Dn exemplifying the task, and a data point Dn+1 to be solved. All few-shot examples Di are formatted using p. Thus, the final prompt template is: inst c p(D1) c p(D2) c . . . c p(Dn) c p(Dn+1). Since Dn+1\u2019s output will be generated by the model, an empty string is added in place of the answer in the last field in the template. Prompt construction will modify inst to match specific choices encoded in p: concretely, if p enumerates valid multiple-choice options as characters x1 . . . xn, we ensure inst refers to these choices as x1 . . . xn."
        },
        {
            "heading": "3.2 MEASURING SENSITIVITY",
            "text": "We measure how plausible choices in prompt formatting influence quantifiable metrics of generated outputs. Given a set of plausible formats {p1, . . . , pn}, a dataset D, and a scalar metric m, let the performance interval be [mini m(pi,D),maxi m(pi,D)]. We define the performance spread or simply spread as maxi m(pi,D) \u2212 mini m(pi,D). Higher spread indicates more sensitivity to variance within the space of plausible, semantically-equivalent formats. While our method is agnostic to the scalar metric m used, and one could consider a number of metrics including text length, formality, or toxicity, throughout this work we focus our analysis on estimated task accuracy acc. Due to ease in automatic evaluation, here we evaluate on classification tasks.\nOur goal is to compute spread for a given model and task. A comprehensive approach would be to fully evaluate each plausible format pi on the entire evaluation dataset D. This increases the cost of reporting a model\u2019s performance linearly with n, which becomes computationally infeasible for large values of n. Following prior gradient-free prompt engineering work (Zhou et al., 2023; Pryzant et al., 2023), we model our problem as a multi-arm bandit. Given a random sample of n formats (arms) p1, . . . , pn for a task, an arm pi\u2019s hidden value is the actual performance m(pi,D) when evaluated on the full dataset D, and the reward for pulling the arm is an estimate m(pi, D\u0303) where D\u0303 \u2282 D, |D\u0303| = B (mini-batch size) and no element of D\u0303 has yet been evaluated with pi. We assume a budget of E total data point evaluations. We first search for the highest performing format with budget E/2, and then for the lowest performing format with budget E/2. Evaluations done for the first exploration are readily available for the second exploration, which yields a more informative prior for many formats. We consider two well-known regret minimization bandit algorithms: Thompson sampling (used in FORMATSPREAD) and Upper Confidence Bound (UCB).\nThompson Sampling. This simple, high-performing Bayesian inference heuristic randomly draws each arm according to its probability of being optimal (Chapelle & Li, 2011). Each m(pi,D) is modeled as a random variable, and since with our target metric each data point evaluation is a Bernoulli trial, it is natural to model m(pi,D) as a Beta distribution. In each round, Thompson sampling draws from each m(pi, D\u0303) and chooses the best arm i\u0302 (Algorithm 1). It then updates i\u0302 according to the number of observed successes r, and the corresponding B \u2212 r failures, within D\u0303.\nAlgorithm 1 Thompson Sampling for Bernoulli Bandits\nS (1) i \u2190 0, N (1) i \u2190 0 (success counters and total times armed was drawn counter) for t\u2190 1, . . . E/B do for i\u2190 1, . . . ,K do\nTake \u03b8(t)i from Beta(\u03b1i + S (t) i , \u03b2i + (N (t) i \u2212 S (t) i ))\nDraw arm i\u0302 = argmaxi \u03b8 (t) i (or argmin in minimization problems) and observe reward r S (t+1)\ni\u0302 \u2190 S(t) i\u0302 + r, N (t+1) i\u0302 \u2190 N (t) i\u0302 +B\nThompson sampling allows for setting informative priors (\u03b1i, \u03b2i) based on domain knowledge to accelerate runtime. Appendix A.4 details the exact priors we use. To our knowledge, we are the first to consider a Bayesian sampling method for prompt optimization.\nUpper Confidence Bound (UCB) Sampling. UCB (Lai et al., 1985) computes an upper confidence bound to each arm\u2019s performance, derived from Chernoff\u2019s bound. The key difference with Thompson sampling is in how \u03b8(t)i is defined. In UCB\u2019s frequentist approach, \u03b8 (t) i is assigned the estimated\naccuracy plus the upper confidence bound: \u03b8(t)i \u2190Si/Ni + c \u221a log(t)/Ni. We use c = 2 following Pryzant et al. (2023), who find UCB with c = 2 to be most effective for prompt optimization.\nNaive Sampling. Each prompt format is evaluated on E/n points (with appropriate rounding)."
        },
        {
            "heading": "4 CHARACTERIZING PROMPT FORMAT VARIANCE WITH FORMATSPREAD",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUP",
            "text": "Data. We use a subset of 53 tasks from Super-NaturalInstructions (Wang et al., 2022) with diverse human-written formats and instructions, comprising 19 multiple-choice tasks and 34 classification tasks with {2, 3, 4} basic fields. Appendix B.1 details the exact task selection procedure. To construct the final prompt template, we concatenate each task\u2019s instruction and n formatted few-shot examples using \\n\\n as spacing. While selection and ordering of few-shot examples is a component of prompt design influencing features of model output (Lu et al., 2022), our work focuses on prompt formatting. To remove this confounder, we fix the exact choice and ordering of examples for each task and for a given number of shots n. Few-shot examples for each task are chosen randomly within each dataset and are not used for evaluation. We evaluate task data samples on an arbitrary order fixed across settings. Datasets are assumed to be of size 1,000 for fair evaluation across tasks.\nModels. We evaluate LLaMA-2-{7B,13B,70B} (Touvron et al., 2023), Falcon-7B and Falcon-7BInstruct (Almazrouei et al., 2023), GPT-3.5-Turbo (Schulman et al., 2022), all autoregressive LMs.\nTask Evaluation Metrics. We use two popular measures for computing accuracy: exact prefix matching and probability ranking. In exact prefix matching, we check if the output\u2019s prefix matches the expected answer after normalization (casing, spacing, newlines). Ranking accuracy computes the rate that the expected answer is the highest-ranked valid option (in multiple choice and classification tasks) according to the model\u2019s output distribution. Results are reported using ranking accuracy unless specified otherwise. Appendix B.2 shows additional analysis of exact prefix matching, with spreads even higher than those shown in Section 4.2, and including how formatting choice affects task degeneration (i.e., not answering any valid option)."
        },
        {
            "heading": "4.2 PROMPT FORMATS HAVE A LARGE PERFORMANCE SPREAD, NOT ELIMINATED BY INCREASING FEW-SHOT EXAMPLES OR MODEL SIZE, NOR WITH INSTRUCTION TUNING",
            "text": "For each evaluation task we randomly sample 10 plausible prompt formats and use FORMATSPREAD to compute performance spread for each modeling and n-shot choice (Figure 3). We find significant performance spread across tasks, with a median spread of 7.5 accuracy points across choices in the model and the number of few-shot examples. 20% of tasks consistently result in a spread of at least 15 accuracy points for all LLaMA-2 settings, and at least 9 points for all Falcon settings. We observe several tasks with performance spread over 70 accuracy points. Because this analysis uses only 10 randomly sampled formats, it represents a lower bound of the true spreads for each task. Furthermore, there exists significant performance spread regardless of increased model size (Figure 2a and Figure 11 for Llama-2-70B), instruction tuning (Figure 2b), or number of few-shot examples (Figure 2c; Figure 2a and 2b plot 1- and 5-shot jointly). Appendix B.2 demonstrates similar results on a selection of non-classification tasks, and expands the spread discussion to plotting the entire accuracy distribution, along with a dispersion metric.\nComparison trends between models are often reversed just by choosing different formats. Assuming model M is better than M \u2032 by at least d accuracy using prompt p, we compute how often M \u2032 achieves at least d higher accuracy than M under a different format p\u2032. Figure 4 shows these trends are often reversed: LLaMA-2-13B and -70B reverse trend by at least d = 0.02 with probability 0.141; LLaMA-2-7B and Falcon-7B reverse trend by at least d = 0.02 with probability 0.140. Strikingly, often both experiments (first using p, and then p\u2032) were statistically significant (p-value < 0.05) on 1000 samples2: 76% and 47% respectively for the two model comparisons\n2We use one-sided McNemar tests, also known as paired \u03c72 tests, since we evaluate models on the same set of samples. We test the significance of M being better than M \u2032 under p, and M being worse than M \u2032 under p\u2032.\nmentioned above. We find that formats yielding high performance for model M may not yield high performance for M \u2032, implying that formats may not be inherently good or bad (Appendix B.2)."
        },
        {
            "heading": "4.3 HOW DO INDIVIDUAL FEATURES CONTRIBUTE TO PERFORMANCE?",
            "text": "We analyze how choices in particular constants (i.e. S1,S2, C,Fcasing,Fitem) independently influence task performance across different formats. Figure 5 shows the distribution of accuracy for 500 sampled prompts conditioned on the choice of S1 (the separator between a descriptor and the text placeholder) for one task in Super-NaturalInstructions. When comparing the individual influence of two feature choices, we measure both weak and strong notions of dissimilarity between distributions of accuracy across prompts conditioned on a chosen feature. We say two constant choices yield weakly different accuracy distributions if the values between the first quartile (Q1) and third quartile (Q3) do not intersect. This is equivalent to the boxes in a boxplot not overlapping. We say two constant choices yield strongly different accuracy distributions if the ranges [2.5Q1\u22121.5Q3, 2.5Q3+1.5Q1] do not overlap (adjusted to end in a data point). This is equivalent to two boxplots with their whiskers not overlapping. In Figure 5, \u2019 \\n\\t\u2019 and \u2019: \u2019 (fourth and sixth) are only weakly different.\nWe compute accuracy for 500 random formats with 250 samples each on 31 tasks for 1-shot Llama2-7B. Table 1 shows that choices in S2, Fitem1, Fcasing do not independently predict performance differences (weakly or strongly): although these features can have a large performance variance and thus should be explored with FORMATSPREAD, they cannot be used to independently predict accuracy changes. Other constant sets have varying degrees of differences, with S1 (separators) and Fitem2 (number format changes in enumerations) having the most individual impact. All tasks with strong dissimilarities are shown in Appendix B.4.\nSmall prompt variations often yield large performance differences. Table 2 shows a selection of tasks where changing a single constant on a format (e.g., casing in task322) results in large accuracy differences. Figure 6 shows that regardless of the scoring criterion used, a significant ratio of these atomic changes are associated with large accuracy changes. For example, 24% of atomic changes have an associated accuracy change of at least 5 points when using exact prefix matching as scoring criteria (11% when using probability ranking).\nThe space of prompt format accuracy is highly non-monotonic, which makes local search algorithms over the space less effective. Let (p1, p2, p3) be a prompt format triple such that pi+1 is obtained by making an atomic change to pi. We argue that if the prompt format space is smooth, we should often see a triples\u2019 accuracy to be strictly monotonic over i. We choose 24 tasks (13 multiple choice,\n11 non-multiple choice), sample 300 (p1, p2, p3) triples for each, and the compute accuracy (using exact prefix matching) of each pi on 250 samples. 32.4 and 33.6% of triples were monotonic for multiple-choice and non-multiple-choice tasks respectively. Given that random shuffling within a triple will result in monotonicity 33.3% of the time, this suggests that local search mechanisms like simulated annealing may not be effective as they require a locally smooth search space."
        },
        {
            "heading": "4.4 PROMPT FORMATS ARE IDENTIFIABLE TRANSFORMATIONS OF PROMPT EMBEDDINGS",
            "text": "Prompt format choices represent a deterministic transformation of the input, even if its impact on the resulting performance is hard to predict. We represent prompt embeddings as the last hidden layer obtained when processing the whole input prompt (immediately before generating the first token). We demonstrate that format choice yields a highly identifiable transformation over this embedding, which suggests that formats can be seen as transformations of the output probability distribution.\nFor each task, and for both {1, 5}-shot settings, we collect prompt embeddings from LLaMA-2-7B corresponding to 10 randomly sampled valid formats for 1000 evaluation examples. We train an XGBoost (Chen & Guestrin, 2016) classifier that maps from the top n principal components of a prompt embedding to the prompt format.3 We find that although the original prompt embeddings are of size 4,0964, using just the top 100 principal components can result in a classifier with \u22650.98 accuracy in format identification for all 31 tasks analyzed. Figure 7 shows the accuracy of format classification given a fixed number of principal components.5 We find that classifier accuracy given just the top two components correlates moderately with the spread of performance in the prompts they represent (0.424, p = 8.04 \u00b7 10\u22126; 0.555 for the 5-shot setting; using exact prefix matching)."
        },
        {
            "heading": "4.5 FAST EXPLORATION OF THE PROMPT FORMATTING SPACE: FORMATSPREAD",
            "text": "In Section 4.2, we demonstrate that even when sampling just 10 formats from the space of plausible formats, we still observe significant performance spread on many tasks. However, this is only a lower\n3We train with 800 vectors from each of the 10 formats (8000 vectors) and evaluate on the remaining 200. 4Equivalent to the dimension of hidden representations for LLaMA-2-7B. 5Figure 21 in the Appendix visualizes examples of the top two principal components for ten prompt formats.\nbound of the spread a task may exhibit when increasing the number of formats: for example, about 17% of tasks are expected to increase their spread by at least 5 accuracy points when increasing from 10 to 20 sampled formats. Figure 8 quantifies the expected increase in spread when increasing the number of formats by evaluating 500 formats on 250 samples each and computing expected gains.\nFigure 9 compares the efficiency of Thompson sampling, UCB, and naive sampling for estimating spread with respect to a budget E (Section 3.2). To ensure accurate reports, we compute and show the true spread of the highest- and lowest-performing formats chosen by each method using all data. With a budget of 51,200 evaluations, Thompson sampling results in a spread within 1 accuracy point of the true spread, while naive sampling finds a spread within 4 points, and UCB within 11.\nFinally, we use FORMATSPREAD to measure sensitivity of several models where inference is expensive. With a budget of 40,000 evaluations and 320 prompt formats, we find that 1-shot LLaMA-2-70B\u2013ran using 4-bit quantization (Dettmers et al., 2022)\u2013yields a median spread of 0.171 (mean=0.221, std=0.200, using probability ranking across 53 tasks; 25% of tasks had a spread of 0.292 or higher, with a maximum spread of 0.876), and GPT-3.5 yields a median spread of 0.064 (mean=0.110, std=0.115, across 53 tasks using exact prefix matching given that we do not have access to the full logits; 25% of tasks had a spread of 0.148 or higher, with a maximum spread of 0.562), showing sensitivity to formatting is still present even on larger models. 5-shot LLaMA-270B still shows high spreads, with 25% of tasks having a spread of 0.310 and a maximum of 0.841. See spread visualization in Figure 25, and a list of best and worst formats found in Table 6."
        },
        {
            "heading": "5 RELATED WORK",
            "text": "The task of automatically finding the best-performing prompt for a given task without changing model parameters has recently gained attention, given the constantly improving yet somewhat unpredictable performance of LLMs. Prior work has often focused on discovering optimal prompts with gradient-based methods, which are effective, but often lead to disfluent or unnatural prompts (Shin\net al., 2020), which can be mitigated with a Langevin dynamics-based method (Shi et al., 2022). Another approach is to learn, optimize, and insert continuous representations of prompts and tasks as input to models (Qin & Eisner, 2021; Lester et al., 2021; Ding et al., 2022; Ilharco et al., 2023). These methods also require access to the LLM\u2019s parameters, thus cannot be applied to models behind an API. In contrast, FORMATSPREAD does not assume access to any model internals. Prior gradientfree work has focused on edit-based enumeration over human-written prompts (Prasad et al., 2023), reinforcement learning (Deng et al., 2022), and by using LLMs themselves (Zhou et al., 2023; Gao et al., 2021). These works aim to achieve competitive task performance, even if the meaning of the prompt or instruction is modified. To our knowledge, we are the first to focus specifically on prompt formatting variance, a quintessential example of semantic equivalence.\nJailbreaking refers to the behavior of intentionally manipulating prompts to elicit inappropriate or sensitive responses, or otherwise reveal parts of the prompt that were intentionally not revealed. While the objective differs from our work, jailbreaking works (Wei et al., 2023; Zou et al., 2023) share the underlying technical question of finding the lowest-performing prompt. Our methods differ, since Wei et al. (2023) evaluate human-generated attacks to guide adversarial prompt design, and Zou et al. (2023) uses gradient-based search methods simultaneously across multiple models.\nSome existing work has explored the influence of certain prompt design choices on model performance, for example the prompt\u2019s language (Gonen et al., 2022) and the ordering of few-shot examples (Lu et al., 2022). Other work has focused on providing textual interpretations of continuous prompt representations (Khashabi et al., 2022). Beyond autoregressive LLMs, existing work has focused on performance variance in masked language models (Elazar et al., 2021; Jiang et al., 2020). Our work follows efforts in other domains that explore the influence of spurious features on research evaluations, e.g., in deep reinforcement learning (Islam et al., 2017; Henderson et al., 2018) and statistical machine translation (Clark et al., 2011)."
        },
        {
            "heading": "6 DISCUSSION",
            "text": "We introduce FORMATSPREAD, an algorithm that estimates the performance spread across prompt formatting choices.6 We use FORMATSPREAD to evaluate the spread of several widely-used opensource LLMs for classification tasks in few-shot learning settings. We find that spread is large regardless of model choice, even when increasing model size, number of few-shots, or when using instruction tuning. FORMATSPREAD is designed to efficiently search the space of plausible prompt formats under a user-specified computational budget. For example, with a computational budget of exploring only 5% of the entire search space for task with 2,500 test examples and 320 plausible formats, we are able to estimate spread within 2 accuracy points of the true spread.\nWe also characterize the space of prompt formats, finding that it is largely non-monotonic and that few atomic features can be predictors of performance alone, although the separability of format embeddings is highly correlated with observed performance spread. These findings informed the design of our search procedure, where local search methods are not advantageous.\nOur findings suggest that performance spread caused by arbitrary prompt formatting choices may influence conclusions made about model performance, especially when comparing models on benchmark tasks. Thus, we recommend that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible formats. However, we want to emphasize that single-format evaluation may still be sufficient for many use cases. For example, for researchers or practitioners who build systems on top of LLMs, choosing a single prompt format that works sufficiently well for use in this larger system is a valid methodological choice. However, we encourage future research to compute FORMATSPREAD when comparing their systems to outof-the-box models, to ensure fair baseline representation. Furthermore, FORMATSPREAD can be used to identify lower-bound performance of a model or system. For example, when using a model for socially impactful tasks, such as stereotype classification in Figure 1, it is important to report the range of accuracy a non-adversarial user might encounter. Likewise, it is crucial to consider robustness to spurious features when claiming that models possess general abilities, such as theory of mind; and beneficial to report when e.g. exploring model biases. We leave it to future research to develop regularization procedures either during training or with an already-trained model to make models robust to diverse formatting choices.\n6We thoroughly describe the limitations of our method in Appendix C."
        },
        {
            "heading": "A GRAMMAR DEFINITION AND INSTANTIATION DETAILS",
            "text": ""
        },
        {
            "heading": "A.1 EQUIVALENCE RELATION DEFINITION",
            "text": "Precisely, p1 \u223c p2 if and only if at least one of the following hold: p1 = p2 = B0; or pi = B\u20320(di, si) with d1 = d2; or pi = B1(di, si, fi) with d1 = d2; or pi = B (n) 2 (X1,i, . . . , Xn,i, ci) with Xj,1 \u223c Xj,2 \u22001 \u2264 j \u2264 n; or pi = B(n)3 (di, j1,i, . . . , jn,i, s1, s2, c, f) where d1 = d2 and jk,1 = jk,2 \u22001 \u2264 k \u2264 n. It is possible that generated formats equivalent in their string representation are not equivalent according to this equivalence relation.\nA.1.1 VISUALIZATION OF PROMPT FORMAT\u2019S PARSING AND FULL FORMAT GENERATION\nFigure 10 shows a visualization of how a complex format is parsed using our defined grammar. A full prompt consists of an instruction, n few-shots and a data point to solve. For example, if the instruction was Given a sentence and two words that appear in it, answer which one of the two (A or B) appeared first in the sentence., a full prompt may look as follow. Note that we always use \\n\\n as space character between instruction and few-shots. The example below shows a 1-shot prompt. It is simply illustrative and does not correspond to any of the tasks considered.\nGiven a sentence and two words that appear in it, answer which one of the two (A or B) appeared first in the sentence.\nThe quick brown fox jumps OPTIONS: CHOICE (A): fox ; CHOICE (B): brown ANSWER: B\nOver the lazy dog OPTIONS: CHOICE (A): lazy ; CHOICE (B): dog ANSWER:\nFORMATSPREAD forces all instantiations of a multiple choice variable to change jointly to maintain coherence, and this includes text in the instruction. Therefore, when changing the option items from A and B to I and II, the prompt will be generated as follows.\nGiven a sentence and two words that appear in it, answer which one of the two (I or II) appeared first in the sentence.\nThe quick brown fox jumps OPTIONS: CHOICE (I): fox ; CHOICE (II): brown ANSWER: II\nOver the lazy dog OPTIONS: CHOICE (I): lazy ; CHOICE (II): dog ANSWER:\nA.2 ALLOWED VALUES FOR EACH SET S1,S2 , C , FCASING , FITEM\nS1 = {\u2032\u2032,\u2032 \u2032,\u2032 \\n\u2032,\u2032 \\n\u2032,\u2032 \u2212\u2212 \u2032,\u2032 \u2032,\u2032 ; \\n\u2032,\u2032 || \u2032,\u2032 < sep > \u2032,\u2032 \u2212\u2212 \u2032,\u2032 , \u2032,\u2032 \\n \u2032,\u2032 , \u2032,\u2032 \\n \u2032,\u2032 . \u2032,\u2032 , \u2032} S2 = {\u2032\u2032,\u2032 \u2032,\u2032 \u2032, \u2018\\t\u2032} (no space, single space, double space, tab) C = {\u2032\u2032,\u2032 ::: \u2032,\u2032 :: \u2032,\u2032 : \u2032,\u2032 \\n\\t\u2032,\u2032 \\n \u2032,\u2032 : \u2032,\u2032 \u2212 \u2032,\u2032 \u2032,\u2032 \\n \u2032,\u2032 \\n\\t\u2032,\u2032 :\u2032,\u2032 ::\u2032,\u2032\u2212 \u2032,\u2032 \\t\u2032}\nFcasing = {f(x) = x, f(x) = x.title(), f(x) = x.upper(), f(x) = x.lower()} Fitem = {x 7\u2192 f(g(x)) | such that f \u2208 Fitem1 \u2227 g \u2208 Fitem2} Fitem1 = {x 7\u2192 (x), x 7\u2192 x., x 7\u2192 x), x 7\u2192 x ), x 7\u2192 [x], x 7\u2192< x >} Fitem2 = {x\u2192 x+ 1, x\u2192\u2032 A\u2032 + x, x\u2192\u2032 a\u2032 + x,\nx\u2192 0x215F+ x+ 1, x\u2192 ROMAN[x].lower(), x\u2192 ROMAN[x].upper()}\nEnumerations are indexed from (i.e., \u201c1, 2, 3\u201d rather than \u201c0, 1, 2\u201d). ROMAN[x] represents the Roman numerals written in regular ASCII characters. \u20180x215F\u2019+x represent the series of Unicode characters for Roman numerals. denotes a spacing character for clarity."
        },
        {
            "heading": "A.3 RESTRICTIONS TO PROMPT FORMATS SPACES AND SEPARATORS\u2019 COMBINATIONS",
            "text": "We define several restrictions to ensure format naturalness. Users can additionally customize FORMATSPREAD by defining their own rules and restrictions between values. Our rules are as follows:\n\u2022 If B2(X1,. . . ,Xn,c) where c does not contain a newline, then each Xi\u2019s separators and any subcomponents\u2019 separators should not contain a newline.\n\u2022 Similar to the rule above, if B(n)3 (d, j1, . . . , jn, s1, s2, c, f1, f2) such that some separator contains a newline (i.e. s1 contains a newline and/or s2 contains a newline) then the space c must also contain a newline.\n\u2022 For B1(d, s, f) := f(d)s< text >, s must not be the empty string (i.e., there has to be some separation between descriptor and text).\n\u2022 Having c be an empty string space in B(n)2 is only allowed if the first n \u2212 1 components are B1 fields with an empty <text>. Similarly, the newline restrictions mentioned above only apply if the <text> is not empty. This rarely happens in prompt formats, but there are formats such as Question: <text> Options: A. <text> B. <text> where the Options: do not have a corresponding field."
        },
        {
            "heading": "A.4 THOMPSON SAMPLING PRIORS",
            "text": "For the first exploration (i.e., finding the best-performing prompt format), we set an informative prior Beta(\u03b1, \u03b2) := Beta ( max ( \u03b2\u00b7x 1\u2212x , 1.1 ) , 5 )\nfor all arms pi, where x is the original format\u2019s accuracy. Our goal is to set an informative prior where the expected value of the prior distribution is the original format accuracy x, since a priori it is the only information we have about performance.\nThis restricts the parameters as follows:\nE[Beta(\u03b1, \u03b2)] = \u03b1\n\u03b1+ \u03b2 = x\n\u03b1 = \u03b1 \u00b7 x+ \u03b2 \u00b7 x\n\u03b1 = \u03b2 \u00b7 x 1\u2212 x\nSince \u03b2 will modulate how confident is the prior, and we want to avoid the model being overconfident, we fix \u03b2 = 5. Because we want to have an informative prior Beta(\u03b1, \u03b2) with a Gaussian-like PDF, we force \u03b1 > 1 and \u03b2 > 1. In extreme cases, forcing \u03b1 > 1 might alter the expected value. The first exploration\u2019s priors are thus exactly Beta(\u03b1, \u03b2) with \u03b1 = max ( \u03b2\u00b7x 1\u2212x , 1.1 ) and \u03b2 = 5 for all arms pi.\nFor the second exploration (i.e., finding the worst-performing prompt format), the model has access to the first explorations\u2019 counters S(E/B)i and N (E/B) i . Therefore, we set the second exploration\u2019s\npriors to be Beta ( \u03b1+ S\n(E/B) i , \u03b2 +\n( N\n(E/B) i \u2212 S (E/B) i\n)) ."
        },
        {
            "heading": "B ADDITIONAL EXPERIMENTS\u2019 INFORMATION AND PLOTS",
            "text": ""
        },
        {
            "heading": "B.1 TASK SELECTION",
            "text": "We use a number of heuristics to filter Super-NaturalInstructions tasks to our set of 53 evaluation tasks. Datasets should have at least 1000 samples to be considered. We also remove tasks whose instructions are too long (over 3,000 characters) and datasets with inputs longer than 2,000 characters, given that this makes performing inference at scale intractable. We also filter datasets whose valid outputs include more than 20 different strings, given that we focus on classification tasks.\nWe also removed tasks where we found a priori performance on the task was 0% accuracy using LLaMA-2-7B 1-shot. Some Super-NaturalInstructions tasks are derived from the same original dataset, but ask different questions. We did not include more than 4 tasks from the same original dataset.\nFinally, we also searched for having socially impactful tasks. Those tasks were the only SuperNaturalInstructions tasks where we included a format if one was not provided by the dataset.\nThe selected tasks were the following 53: task050, task065, task069, task070, task114, task133, task155, task158, task161, task162, task163, task190, task213, task214, task220, task279, task280, task286, task296, task297, task316, task317, task319, task320, task322, task323, task325, task326, task327, task328, task335, task337, task385, task580, task607, task608, task609, task904, task905, task1186, task1283, task1284, task1297, task1347, task1387, task1419, task1420, task1421, task1423, task1502, task1612, task1678, task1724."
        },
        {
            "heading": "B.2 ADDITIONAL RESULTS FOR SECTION 4.2",
            "text": "Formats are not inherently good or bad. Table 3 shows that if format p1 has lower performance than format p2 under model M , there is < 0.62 probability that this trend would hold under another model M \u2032 (random chance is 0.5). This weak relative order preservation suggests that prompt format performance in a model may not be extrapolated to a different model, or in other words, that there are no inherently good or bad formats. This finding is further supported by Figure 12, which shows that findings of a format being better or worse than another are often inconsistent across models.\nExperiments with exact prefix matching accuracy. Here we show results with using exact prefix matching to compute accuracy. Often, failures in prefix matching are associated with degeneration, i.e., cases where the model does not answer any of the valid options, motivating the use of ranking accuracy. Degeneration makes models (specially smaller models) more unlikely to have high accuracy out of the box. As seen in Figure 6, prefix matching is linked to having higher changes when performing atomic changes. Moreover, exact prefix matching can lead to lower performance as generation is less constrained (see Figure 16). Table 4 shows examples of atomic changes yielding large accuracy changes with exact prefix matching metric.\nFigure 13c shows spread remains regardless of model size increase, architecture change, or number of few-shot examples also when using exact prefix matching as accuracy metric. In line with the results shown for probability ranking in Section 4.2, Figure 15 shows that the probability of reversing performance trends between two models just by changing prompt remains high when using exact prefix matching as metric. Strikingly, spread is significantly higher than in the probability ranking setting (see Figure 14), with median spread ranging from 12 to 28 accuracy points depending on the model used. This further motivates the need for running FORMATSPREAD when benchmarking models with this accuracy metric. This increased spread may be partly due to degeneration, as we will detail next.\nDegeneration. Sometimes when a model does not generate the correct answer with exact prefix matching, it also does not generate a valid response, i.e. it degenerates. We will now quantify this phenomenon using 53 SuperNaturalInstructions classification and multiple choice tasks.\nGiven a model, a task, and a format, let the centered mass be the ratio of examples where the model\u2019s output matched with any valid option (regardless of correctness). Table 5 shows that the correlation between accuracy and centered mass is moderate or high depending on the model. This suggests that very often when a model does not return a valid answer, it does not return any valid answer\nat all. This is especially true for Falcon models, where we observe an almost perfect correlation between accuracy and centered mass. In conclusion, prompt format chosen often do not solely affect accuracy, but they also affect the frequency in which a model is actually able to perform a task. This will especially affect tasks for which there are no alternative metrics. Further research may focus specifically on targeting features that cause degeneration.\nExperiments with Instruction Induction tasks. All experiments thus far focused solely on classification tasks. We will now focus on tasks that require generating (short) text, and cannot be framed as classification tasks. We selected 10 tasks from Instruction Induction (Honovich et al., 2023) that require generating a unique, valid string to be considered a correct response. Examples include identifying the second letter of a word, adding numbers, or answering a synonym to a given word. Instruction Induction tasks also show a wide range of difficulty, resulting in varied settings to be\nanalyzed (see Figure 18b). Given that the collection does not contain human-generated formats, we applied a simple \u2018Input: {}\\n Output: {}\u2019 format. Results for 1-shot and 5-shot settings show spread is still high across models and n-shot choices (see Figure 17).\nExperiments with continuous metrics in open-ended text generation tasks. Throughout the paper we focus on tasks with a single valid output, whether in classification tasks or in short-text generation tasks. This decision is intentional, since it guarantees that a variation in the metric truly\nrepresents a variation in model performance. We have shown that spread remains high when considering option ranking or exact prefix matching as accuracy metric.\nSince LLMs are often used in more open-ended generation contexts, we will now explore the performance variance across prompt formats when considering sentence-length generation tasks (e.g. generate the next sentence of a story, given the four initial sentences of a story, generate a question whose answer is the sentence given). To analyze the automatic generations, we use two widely used metrics: ROUGE-L (Lin, 2004), and BERTScore (Zhang et al., 2019). The first is an n-gram-based metric, and the latter is a model-based metric, and both are [0, 1] metrics where higher is better. Figure 19 shows that variance remains high for LLaMA-2-7B regardless of the metric and the number of n-shots considered, with LLaMA-2-7B 5-shot having 25% of tasks with a ROUGE-L spread of 0.098 or higher, and a BERTScore spread of 0.09 or higher.\nWe observe that the median spread is sometimes smaller than in the accuracy tasks. This may be because although ROUGE, BERTScore, and accuracy are all [0, 1] metrics, typical metric values may be different, which may in turn affect the final spread (an absolute difference). We leave it to future work to quantify the differences in style or content that each format may be inducing.\nFinally, it is worth noting that text generation metrics are known to be noisier, and thus not all metric decreases necessarily correspond to a true performance loss, as is the case for accuracy in single-valid-output tasks. We used 17 SuperNatural Instructions tasks: task037, task038,\ntask040, task067, task071, task072, task105, task216, task223, task240, task348, task389, task443, task845, task1326, task1401, task1613. We selected the 17 open-ended text generation tasks among those with at least 1000 samples, with some formatting present in the original task (e.g. \u2018Passage:\u2019 <text>). We only considered tasks whose instructions were under 1,000 characters and that contained inputs no longer than 5,000 characters.\nWe limit generations to 50 tokens. To parse model outputs more faithfully, and given that none of our expected generations include a newline, we only consider a model\u2019s generation up to the\nfirst newline (excluding leading spaces and newlines in a given generation). This consideration is important given that often models start to generate a new data sample from scratch, immediately after generating the requested answer.\nCharacterizing a model\u2019s accuracy distribution beyond spread. Spread gives a quantitative jump in information with respect to informing a single point in the performance distribution since it measures the distribution range (maximum minus minimum). However, distributions that may share the same range, may yield a widely different probability of obtaining each value in the distribution. Figure 20 plots the accuracy distribution of 30 tasks, sorted in decreasing order by standard deviation. Tasks with high standard deviation reflect a higher likelihood of obtaining dissimilar values when making a formatting selection; Figure 20 shows that the median standard distribution is \u03c3 \u2248 0.04, which can be considered high in our context.\n0.00\n0.25\n0.50 = 0.31 task280 = 0.14 task1297 = 0.10 task190 = 0.09 task327 = 0.08 task319\n0.0 0.5 1.0 0.00\n0.25\n0.50 = 0.07\ntask328\n0.0 0.5 1.0\n= 0.07\ntask904\n0.0 0.5 1.0\n= 0.07\ntask1612\n0.0 0.5 1.0\n= 0.06\ntask296\n0.0 0.5 1.0\n= 0.05\ntask1284\nPerformance Spread\nFr ac\ntio n\nof fo\nrm at\ns\n0.0\n0.5\n1.0 = 0.05 task325 = 0.05 task323 = 0.05 task213 = 0.04 task158 = 0.04 task220\n0.0 0.4 0.8 0.0\n0.5\n1.0 = 0.04\ntask1380\n0.0 0.4 0.8\n= 0.04\ntask1283\n0.0 0.4 0.8\n= 0.04\ntask580\n0.0 0.4 0.8\n= 0.04\ntask1431\n0.0 0.4 0.8\n= 0.04\ntask309\nPerformance Spread\nFr ac\ntio n\nof fo\nrm at\ns\n0.0\n0.5\n1.0 = 0.04 task903 = 0.03 task1186 = 0.03 task069 = 0.03 task905 = 0.03 task337\n0.1 0.4 0.7 0.0\n0.5\n1.0 = 0.02\ntask729\n0.1 0.4 0.7\n= 0.02\ntask065\n0.1 0.4 0.7\n= 0.02\ntask1678\n0.1 0.4 0.7\n= 0.02\ntask1419\n0.1 0.4 0.7\n= 0.02\ntask1387\nPerformance Spread\nFr ac\ntio n\nof fo\nrm at\ns\nFigure 20: Accuracy distribution across 500 formats for 30 tasks evaluated on 250 samples each, sorted by standard deviation in decreasing order. LLaMA-2-7B 1-shot, option ranking metric.\nOn factors influencing spread besides prompt formatting. We believe many factors beyond formatting may be influencing performance variance, but were unable to find a feature that reliably predicts spread. We found that the average prompt length in a task has a negligible correlation with its performance spread: r = 0.228 (p = 1.4 \u00d7 10\u22127) for exact prefix matching metric, and r = \u22120.022 (p = 0.615) for option ranking metric, when jointly considering all models and nshots. Similarly, the standard deviation of the prompt length had negligible correlation with spread: r = 0.125 (p = 0.004) for exact prefix matching, and r = \u22120.099 (p = 0.024) for option ranking metric. When considering each model individually, only LLaMA-2-7B with exact prefix matching showed a correlation |r| > 0.5, with the average prompt length having a correlation r = 0.559 p = 6.86\u00d7 10\u221210. All other settings had |r| < 0.36."
        },
        {
            "heading": "B.3 PCA EXAMPLES",
            "text": "Section 4.4 systematically analyzes whether we can predict the prompt format that generated a given pre-softmax activation layer (i.e., prompt embeddings) by using solely its top-n principal components. Figure 21 shows the top two principal components for two different tasks where all 10 formats considered are easily identifiable solely with a prompt embedding\u2019s top two principal compoenents."
        },
        {
            "heading": "B.4 NOTABLE FEATURES",
            "text": "As discussed in Section 4.3, sometimes the choice of a constant may lead to significantly different accuracy ranges. Figures 22,23, and 24 show all strongly dissimilar choices of constants found on any given task, across 53 Super Natural-Instructions tasks, and on both accuracy metrics considered throughout the work. As can be appreciated, choices of constants do not consistently predict performance in isolation."
        },
        {
            "heading": "B.5 THOMPSON SAMPLING RESULTS",
            "text": "Task Model Best Format Worst Format Best Acc\nWorst Acc\ntask050 Llama-70B sentence {} || question {} || answer {}\nSentence\\t{}\\n Question\\t{}\\n Answer\\t{}\n0.62 0.58\ntask065* Llama-70B Sentence<1>:: {} Sentence<3>:: {} Sentence<4>:: {} Sentence<5>:: {} \\nOption A.: {} Option B.: {} \\nAnswer\\n {}\nSentence I)\\t{} \\n Sentence III)\\t{} \\n Sentence IV)\\t{} \\n Sentence V)\\t{} Option\\t(I)::{}, Option\\t(II)::{} Answer {} 0.88 0.50\ntask069 Llama-70B BEGINNING::: {} MIDDLE A): {} , MIDDLE B): {} ENDING::: {} ANSWER::: {}\nBEGINNING\\n {}\\n MIDDLE <I>:: {} MIDDLE <II>:: {}\\n ENDING\\n {}\\n ANSWER\\n {}\n0.85 0.58\ntask070 Llama-70B Beginning::: {}; \\nMiddle\\t1)\\t{}; \\nMiddle\\t2)\\t{}; \\nEnding::: {}; \\nAnswer::: {}\nBeginning {} || Middle 1.: {} || Middle 2.: {} || Ending {} || Answer {}\n0.80 0.27\ntask114 Llama-70B sentence: {} <sep>answer: {} Sentence: {}\\nAnswer: {} 0.54 0.51\ntask1186 Llama-70B SYSTEM REFERENCE : {}. ORIGINAL REFERENCE : {}. ANSWER : {} System Reference: {}\\nOriginal Reference: {}\\nAnswer: {}\n0.56 0.51\ntask1283 Llama-70B system reference: {} \\noriginal reference: {} \\nanswer: {} SYSTEM REFERENCE\\t{}\\n ORIGINAL REFERENCE\\t{}\\n ANSWER\\t{}\n0.62 0.50\ntask1284 Llama-70B SYSTEM REFERENCE\\t{} || ORIGINAL REFERENCE\\t{} || ANSWER\\t{} System Reference {} , Original Reference {} , Answer {}\n0.74 0.43\ntask1297 Llama-70B Fact\\tI.\\t{} , Fact\\tII.\\t{} <sep>Question: {}; \\n1 ) - {}; \\n2 ) - {}; \\n3 ) - {}; \\n4 ) - {}; \\n5 ) - {}; \\n6 ) - {}; \\n7 ) - {}; \\n8 ) - {} <sep>Answer: {}\nfact a )- {}\\nfact b )- {} \\nquestion: {} || i) : {}ii) : {}iii) : {}iv) : {}v) : {}vi) : {}vii) : {}viii) : {} \\nanswer: {}\n0.88 0.40\ntask133 Llama-70B Sentence:{} \\nReason:{} \\nQuestion:{} \\nAnswer:{}\nSentence:: {}\\n Reason:: {}\\n Question:: {}\\n Answer:: {}\n0.71 0.59\ntask1347 Llama-70B SENTENCE A): {} , SENTENCE B): {} ANSWER::: {}\nSentence i){} Sentence ii){}; \\nAnswer\\n {}\n0.42 0.31\ntask1387 Llama-70B Premise::{} \\nHypothesis::{} \\nAnswer::{}\npremise {}\\n hypothesis {}\\n answer {}\n0.52 0.41\ntask1419 Llama-70B problem- {} \\noptions- \\n <1>:: {}, <2>:: {}, <3>:: {}, <4>:: {}, <5>:: {} \\nanswer- {}\nProblem: {} || Options: <A>{} <sep><B>{} <sep><C>{} <sep><D>{} <sep><E>{} || Answer: {}\n0.24 0.23\ntask1420 Llama-70B Problem\\t{}\\nOptions\\t i):{} -- ii):{} -- iii):{} -- iv):{} -- v):{}\\nAnswer\\t{}\nPROBLEM- {}\\nOPTIONS- \\n[a]{}\\n [b]{}\\n [c]{}\\n [d]{}\\n [e]{}\\nANSWER- {}\n0.26 0.22\ntask1421 Llama-70B PROBLEM:{}, OPTIONS: \\na ){} -- b ){} -- c ){} -- d ){} -- e ){}, ANSWER:{}\nProblem::: {} Options::: \\nI )::{} \\nII )::{} \\nIII )::{} \\nIV )::{} \\nV )::{} Answer::: {}\n0.28 0.21\ntask1423 Llama-70B problem: {} -- options: 1):: {}. 2):: {}. 3):: {}. 4):: {}. 5):: {} -- answer: {}\nPROBLEM:: {}. OPTIONS:: I.{}\\n II.- {}\\n III.- {}\\n IV.{}\\n V.- {}. ANSWER:: {}\n0.25 0.20\ntask1502 Llama-70B Input:{} Output:{} input {} output {} 0.59 0.45\ntask155 Llama-70B SENTENCE - {}, ANSWER - {} Sentence:: {} \\nAnswer:: {} 0.36 0.29\ntask158 Llama-70B Sentence: {} || Answer: {} sentence::{} -- answer::{} 0.55 0.49\ntask161 Llama-70B Sentence\\t{}\\n Answer\\t{} Sentence\\n {} \\nAnswer\\n {} 0.45 0.43 task1612* Llama-70B sentenceI ) - {}sentenceII ) - {} \\n answer::{} Sentence (a):{}. Sentence (b):{}\\nAnswer {} 0.64 0.35\ntask162 Llama-70B Sentence\\n {}; \\nAnswer\\n {} SENTENCE \\n\\t{}; \\nANSWER \\n\\t{}\n0.45 0.40\ntask163 Llama-70B SENTENCE:{}, ANSWER:{} SENTENCE\\n {}\\nANSWER\\n {} 0.47 0.35\ntask1678 Llama-70B PROBLEM: {} , OPTIONS: \\n[a] {}. [b] {}. [c] {}. [d] {}. [e] {} , ANSWER: {} PROBLEM \\n\\t{} \\n OPTIONS \\n\\t [1] {}[2] {}[3] {}[4] {}[5] {} \\n ANSWER \\n\\t{}\n0.21 0.21\ntask1724 Llama-70B input: {} -- output: {} INPUT::{} , OUTPUT::{} 0.48 0.48\ntask190 Llama-70B Sentence a ) - {} -- Sentence b ) - {}\\n Answer\\n\\t{}\nSentence 1.{}\\nSentence 2.{}\\n Answer {}\n0.66 0.35\ntask213* Llama-70B TITLE::: {} \\n Sentence I)::: {} , Sentence II)::: {} , Sentence III)::: {} , Sentence IV)::: {} \\n CHOICES::: \\ni){} \\n ii){} \\n ANSWER::: {} TITLE\\t{}, sentence (i)\\t{} -- sentence (ii)\\t{} -- sentence (iii)\\t{} -- sentence (iv)\\t{}, CHOICES\\t \\n (I):: {} (II):: {}, ANSWER\\t{}\n0.99 0.50\ntask214 Llama-70B Title: {}\\n Sentence I.: {} Sentence II.: {} Sentence III.: {} Sentence IV.: {}\\n Choices: \\ni.::: {} \\nii.::: {}\\n Answer: {}\ntitle {} || sentence [1] {} sentence [2] {} sentence [3] {} sentence [4] {} || choices \\n I.{}; \\nII.{} || answer {} 0.92 0.04\ntask220 Llama-70B SentenceI): {}. SentenceII): {}. SentenceIII): {}. SentenceIV): {}. SentenceV): {} , Choices: \\n<a>- {}\\n <b>{} , Answer: {} Sentence <1>: {} -- Sentence <2>: {} -- Sentence <3>: {} -- Sentence <4>: {} -- Sentence <5>: {} , Choices: \\n I ){} -- II ){} , Answer: {}\n0.99 0.83\ntask279 Llama-70B Passage:: {} \\n Answer:: {} Passage- {} Answer- {} 0.64 0.49\ntask280 Llama-70B Passage:: {} , Answer:: {} PASSAGE\\t{}\\n ANSWER\\t{} 0.84 0.04\ntask286 Llama-70B INPUT- {} , OUTPUT- {} Input\\t{} Output\\t{} 0.69 0.51 task296* Llama-70B SentenceI ):: {}, SentenceII ):: {}, SentenceIII ):: {}, SentenceIV ):: {}, SentenceV ):: {}, SentenceVI ):: {}, SentenceVII ):: {}, SentenceVIII ):: {}, SentenceIX ):: {}, SentenceX ):: {}, A)\\t{} , B)\\t{}, Answer : {} Sentence I.: {}; \\nSentence II.: {}; \\nSentence III.: {}; \\nSentence IV.: {}; \\nSentence V.: {}; \\nSentence VI.: {}; \\nSentence VII.: {}; \\nSentence VIII.: {}; \\nSentence IX.: {}; \\nSentence X.: {} \\n <i>::{} || <ii>::{} \\n Answer\\n {} 0.98 0.53\ntask297 Llama-70B Sentence [a]:: {}; \\nSentence [b]:: {}; \\nSentence [c]:: {}; \\nSentence [d]:: {}; \\nSentence [e]:: {}; \\nSentence [f]:: {}; \\nSentence [g]:: {}; \\nSentence [h]:: {}; \\nSentence [i]:: {}; \\nSentence [j]:: {}\\n (a):: {} (b):: {}\\n Answer {} Sentence1: {} Sentence2: {} Sentence3: {} Sentence4: {} Sentence5: {} Sentence6: {} Sentence7: {} Sentence8: {} Sentence9: {} Sentence10: {} \\n (A) {} (B) {} \\n Answer: {}\n0.51 0.03\ntask316 Llama-70B passage - {} \\nanswer - {} PASSAGE::{}\\nANSWER::{} 0.51 0.49\ntask317 Llama-70B Passage\\n {}\\n Answer\\n {} PASSAGE\\t{}\\n ANSWER\\t{} 0.83 0.07\ntask319 Llama-70B target\\n\\t{}; \\n{}; \\nanswer\\n\\t{}\nTarget: {} {} Answer: {} 0.77 0.58\ntask320 Llama-70B Target: {}; \\n{}; \\nAnswer: {}\nTarget\\n {} \\n {} \\n Answer\\n {}\n0.77 0.58\ntask322 Llama-70B COMMENT:{}\\n ANSWER:{} Comment {} -- Answer {} 0.77 0.48\ntask323 Llama-70B comment \\n\\t{}\\nanswer \\n\\t{} Comment {}. Answer {} 0.84 0.58\ntask325 Llama-70B COMMENT\\t{}; \\nANSWER\\t{} Comment: {}, Answer: {} 0.84 0.48\ntask326 Llama-70B Comment:: {} Answer:: {} Comment: {}, Answer: {} 0.58 0.51\ntask327 Llama-70B Comment {} <sep>Answer {} Comment : {} Answer : {} 0.88 0.69\ntask328 Llama-70B comment:{} -- answer:{} Comment: {} Answer: {} 0.81 0.52\ntask335 Llama-70B post:: {} , answer:: {} Post: {}\\nAnswer: {} 0.52 0.50\ntask337 Llama-70B post {} \\nanswer {} post {} answer {} 0.85 0.63\ntask385 Llama-70B CONTEXT {} -- QUESTION {} -- OPTIONS \\n1.- {}, 2.- {}, 3.- {} -- ANSWER {}\ncontext \\n\\t{} \\nquestion \\n\\t{} \\noptions \\n\\t 1 )::{}. 2 )::{}. 3 )::{} \\nanswer \\n\\t{}\n0.38 0.11\ntask580 Llama-70B Context- {}\\n Question- {}\\n Options- \\na) {} -- b) {} -- c) {}\\n Answer- {}\nContext {}, Question {}, Options \\n I ) - {} \\nII ) - {} \\nIII ) - {}, Answer {} 0.78 0.40\ntask607 Llama-70B INPUT \\n\\t{} \\n OUTPUT \\n\\t{} Input : {}. Output : {} 0.68 0.51\ntask608 Llama-70B input: {}. output: {} INPUT \\n\\t{}\\nOUTPUT \\n\\t{} 0.71 0.48\ntask609 Llama-70B Input \\n\\t{} \\n Output \\n\\t{} Input : {}, Output : {} 0.71 0.51\ntask904 Llama-70B input::: {} , output::: {} INPUT::{}\\n OUTPUT::{} 0.71 0.55\ntask905 Llama-70B Tweet::{} || Label::{} || Answer::{}\nTWEET: {} \\n LABEL: {} \\n ANSWER: {}\n0.68 0.50\ntask050 GPT3.5 Sentence\\n\\t{} \\n Question\\n\\t{} \\n Answer\\n\\t{}\nsentence: {} , question: {} , answer: {}\n0.67 0.61\ntask065 GPT3.5 SENTENCEi.::: {} \\nSENTENCEiii.::: {} \\nSENTENCEiv.::: {} \\nSENTENCEv.::: {} \\nOPTION [1]\\t{}OPTION [2]\\t{} \\nANSWER\\t{}\nSentence 1 ) : {} , Sentence 3 ) : {} , Sentence 4 ) : {} , Sentence 5 ) : {} <sep>Option (i): {} <sep>Option (ii): {} <sep>Answer- {}\n0.82 0.33\ntask069 GPT3.5 BEGINNING: {}\\n MIDDLE\\t(I)::: {} \\n MIDDLE\\t(II)::: {}\\n ENDING: {}\\n ANSWER: {}\nBeginning {} || Middle [a]- {} \\n Middle [b]- {} || Ending {} || Answer {}\n0.83 0.65\ntask070* GPT3.5 Beginning : {}\\nMiddle (I): {} -- Middle (II): {}\\nEnding : {}\\nAnswer : {}\nBeginning\\t{} <sep>Middle i){} || Middle ii){} <sep>Ending\\t{} <sep>Answer\\t{}\n0.70 0.49\ntask114 GPT3.5 sentence \\n\\t{}\\nanswer \\n\\t{} SENTENCE: {}. ANSWER: {} 0.67 0.63\ntask1186 GPT3.5 system reference : {} , original reference : {} , answer : {}\nSystem Reference: {}\\nOriginal Reference: {}\\nAnswer: {}\n0.53 0.51\ntask1283 GPT3.5 SYSTEM REFERENCE : {} , ORIGINAL REFERENCE : {} , ANSWER : {} System Reference\\n {} \\nOriginal Reference\\n {} \\nAnswer\\n {} 0.57 0.50\ntask1284 GPT3.5 System Reference \\n\\t{}\\n Original Reference \\n\\t{}\\n Answer \\n\\t{}\nsystem reference::: {} -- original reference::: {} -- answer::: {}\n0.63 0.57\ntask1297* GPT3.5 Fact<1>: {} -- Fact<2>: {}; \\nQuestion::{}\\nI):: {} II):: {} III):: {} IV):: {} V):: {} VI):: {} VII):: {} VIII):: {}; \\nAnswer::{} fact\\ta. : {} || fact\\tb. : {}\\n question : {} \\na. - {}b. - {}c. - {}d. - {}e. - {}f. - {}g. - {}h. - {}\\n answer : {}\n0.84 0.72\ntask133 GPT3.5 Sentence::{} -- Reason::{} -- Question::{} -- Answer::{}\nSentence - {}; \\nReason - {}; \\nQuestion - {}; \\nAnswer - {}\n0.69 0.64\ntask1347* GPT3.5 SentenceI.:: {}\\n SentenceII.:: {} Answer\\t{} Sentence (I){} Sentence (II){} \\nAnswer::{}\n0.46 0.42\ntask1387 GPT3.5 Premise:{} , Hypothesis:{} , Answer:{}\nPREMISE:: {}; \\nHYPOTHESIS:: {}; \\nANSWER:: {}\n0.47 0.44\ntask1419 GPT3.5 Problem - {} || Options - \\n[A]\\t{}. [B]\\t{}. [C]\\t{}. [D]\\t{}. [E]\\t{} || Answer - {}\nPROBLEM- {}\\nOPTIONS- 1 ) - {} -- 2 ) - {} -- 3 ) - {} -- 4 ) - {} -- 5 ) - {}\\nANSWER- {} 0.24 0.20\ntask1420* GPT3.5 problem- {} \\n options- (I) - {}(II) - {}(III) - {}(IV) - {}(V) - {} \\n answer- {}\nProblem:: {} -- Options:: \\n[i]: {}; \\n[ii]: {}; \\n[iii]: {}; \\n[iv]: {}; \\n[v]: {} -- Answer:: {}\n0.24 0.09\ntask1421 GPT3.5 Problem \\n\\t{}\\n Options \\n\\t \\nA)::{} -- B)::{} -- C)::{} -- D)::{} -- E)::{}\\n Answer \\n\\t{}\nPROBLEM::{}, OPTIONS:: \\n[a]: {} \\n[b]: {} \\n[c]: {} \\n[d]: {} \\n[e]: {}, ANSWER::{} 0.30 0.01\ntask1423 GPT3.5 PROBLEM: {}; \\nOPTIONS: \\n a ) {} \\n b ) {} \\n c ) {} \\n d ) {} \\n e ) {}; \\nANSWER: {}\nProblem:: {} -- Options:: \\n[i]: {}; \\n[ii]: {}; \\n[iii]: {}; \\n[iv]: {}; \\n[v]: {} -- Answer:: {}\n0.30 0.10\ntask1502 GPT3.5 input\\n {}\\noutput\\n {} input::{} -- output::{} 0.54 0.48\ntask155 GPT3.5 SENTENCE::{} -- ANSWER::{} SENTENCE {} ANSWER {} 0.48 0.40\ntask158 GPT3.5 Sentence:{} Answer:{} SENTENCE\\t{} , ANSWER\\t{} 0.63 0.60\ntask161 GPT3.5 sentence\\t{} answer\\t{} Sentence- {}\\nAnswer- {} 0.40 0.31\ntask1612 GPT3.5 sentence 1 ): {} || sentence 2 ): {}, answer::{}\nSentence I ) : {} \\nSentence II ) : {} , Answer {}\n0.66 0.54\ntask162 GPT3.5 Sentence- {} -- Answer- {} SENTENCE\\t{}; \\nANSWER\\t{} 0.37 0.31\ntask163 GPT3.5 Sentence- {}; \\nAnswer- {} sentence: {}\\nanswer: {} 0.43 0.39 task1678* GPT3.5 Problem - {} Options - \\n (I) {} <sep>(II) {} <sep>(III) {} <sep>(IV) {} <sep>(V) {} Answer - {} PROBLEM::{} \\n OPTIONS:: \\n[I] {} , [II] {} , [III] {} , [IV] {} , [V] {} \\n ANSWER::{} 0.24 0.09\ntask1724 GPT3.5 INPUT::{} || OUTPUT::{} INPUT: {} OUTPUT: {} 0.63 0.57\ntask190 GPT3.5 Sentence (I) - {}; \\nSentence (II) - {}; \\nAnswer {}\nSentence<1>: {}\\nSentence<2>: {}, Answer : {}\n0.29 0.16\ntask213 GPT3.5 Title: {} <sep>Sentence <1>: {} || Sentence <2>: {} || Sentence <3>: {} || Sentence <4>: {} <sep>Choices: \\n<i>- {} <sep><ii>- {} <sep>Answer: {}\nTITLE:{}, sentence\\t[a]:{} -- sentence\\t[b]:{} -- sentence\\t[c]:{} -- sentence\\t[d]:{}, CHOICES: \\n[i]:{} [ii]:{}, ANSWER:{}\n0.98 0.81\ntask214* GPT3.5 title: {} \\nSentencei): {}, Sentenceii): {}, Sentenceiii): {}, Sentenceiv): {} \\nchoices: \\n<I>- {} , <II>- {} \\nanswer: {}\nTitle::{} , Sentence\\ta )::{} , Sentence\\tb )::{} , Sentence\\tc )::{} , Sentence\\td )::{} , Choices:: [i]. {}[ii]. {} , Answer::{}\n0.97 0.41\ntask220 GPT3.5 Sentence [1]: {} \\n Sentence [2]: {} \\n Sentence [3]: {} \\n Sentence [4]: {} \\n Sentence [5]: {} -- Choices: \\n <A>::{}\\n <B>::{} -- Answer: {}\nSentence\\ti.: {} || Sentence\\tii.: {} || Sentence\\tiii.: {} || Sentence\\tiv.: {} || Sentence\\tv.: {} Choices: \\n[i]:{} || [ii]:{} Answer: {}\n0.98 0.79\ntask279 GPT3.5 passage {} , answer {} Passage:{}, Answer:{} 0.58 0.56\ntask280 GPT3.5 Passage - {} <sep>Answer - {} Passage {} -- Answer {} 0.85 0.80\ntask286 GPT3.5 Input- {}\\n Output- {} INPUT:: {} OUTPUT:: {} 0.72 0.69\ntask296 GPT3.5 Sentence a ) - {} \\nSentence b ) - {} \\nSentence c ) - {} \\nSentence d ) - {} \\nSentence e ) - {} \\nSentence f ) - {} \\nSentence g ) - {} \\nSentence h ) - {} \\nSentence i ) - {} \\nSentence j ) - {} \\n<A>- {}, <B>- {} \\nAnswer::: {} Sentence\\t(I)- {} -- Sentence\\t(II)- {} -- Sentence\\t(III)- {} -- Sentence\\t(IV)- {} -- Sentence\\t(V)- {} -- Sentence\\t(VI)- {} -- Sentence\\t(VII)- {} -- Sentence\\t(VIII)- {} -- Sentence\\t(IX)- {} -- Sentence\\t(X)- {}, I )::{}, II )::{}, Answer:{}\n0.95 0.68\ntask297 GPT3.5 Sentence (A): {}; \\nSentence (B): {}; \\nSentence (C): {}; \\nSentence (D): {}; \\nSentence (E): {}; \\nSentence (F): {}; \\nSentence (G): {}; \\nSentence (H): {}; \\nSentence (I): {}; \\nSentence (J): {}; \\n<I>::{} <sep><II>::{}; \\nAnswer {}\nSENTENCE\\t(A) : {}; \\nSENTENCE\\t(B) : {}; \\nSENTENCE\\t(C) : {}; \\nSENTENCE\\t(D) : {}; \\nSENTENCE\\t(E) : {}; \\nSENTENCE\\t(F) : {}; \\nSENTENCE\\t(G) : {}; \\nSENTENCE\\t(H) : {}; \\nSENTENCE\\t(I) : {}; \\nSENTENCE\\t(J) : {} -- I) : {} II) : {} -- ANSWER: {}\n0.36 0.06\ntask316 GPT3.5 passage\\n\\t{} \\nanswer\\n\\t{} Passage- {} Answer- {} 0.49 0.48\ntask317 GPT3.5 passage: {} answer: {} passage {} answer {} 0.75 0.70\ntask319 GPT3.5 TARGET:: {} \\n{} \\nANSWER:: {}\nTarget: {}\\n{}\\nAnswer: {} 0.66 0.62\ntask320 GPT3.5 Target::: {} \\n{} \\nAnswer::: {}\ntarget {} -- {} -- answer {} 0.73 0.68\ntask322 GPT3.5 Comment: {} -- Answer: {} comment::{} answer::{} 0.84 0.83"
        },
        {
            "heading": "C LIMITATIONS",
            "text": "As defined by our grammar, all equivalent formats are semantically equivalent to human readers. However, some of them are more likely to be used by humans than others. Spaces and separators are inspired from naturally-occurring formats, but some values are more unusual, such as the spacing <sep> or the separator ::. Contextual restrictions enable disallowing undesired combinations of e.g. spaces and separators. However, formats may have multiple valid parses, and some may be more prone than others to unnatural character combinations. For example, let a data sample be \u2018Passage: Lorem ipsum dolor sit amet. Answer: Yes\u2019. Depending on if we consider the full stop . to be part of the passage or the format, we may parse it as B(2)2 (B1(Passage, \u2019: \u2019, id), B1(Answer, \u2019: \u2019, id), \u2019 \u2019) or B\n(2) 2 (B1(Passage, \u2019: \u2019, id), B1(Answer, \u2019: \u2019, id), \u2019. \u2019). In this work, we choose the former parsing throughout tasks to ensure full sentences. This sometimes7 leads equivalent formats to have a less usual, yet trivially semantically equivalent resulting character combinations, e.g. B(2)2 (B1(Passage, \u2019: \u2019, id), B1(Answer, \u2019: \u2019, id), \u2019; \u2019). This last format would have the following string form on the example above: \u2018Passage: Lorem ipsum dolor sit amet.; Answer: Yes\u2019. We observe high performance spread both in these cases and beyond them. Contextual relations may also restrict these cases if desired by the end user.\nAdditionally, we focus our evaluation on tasks that have reasonably short input instructions and input field length (see task selection details in B.1). Future work may investigate on how input length affects final performance.\n7Less than 20% of cases, based on a manual inspection of 10 formats across 20 tasks."
        }
    ],
    "title": "SPURIOUS FEATURES IN PROMPT DESIGN or: How I learned to start worrying about prompt formatting",
    "year": 2023
}