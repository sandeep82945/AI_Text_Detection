{
    "abstractText": "This paper proposes a new framework of algorithms that is extended from the celebrated extragradient algorithm. The min-max problem has attracted increasing attention because of its applications in machine learning tasks such as generative adversarial networks (GANs) training. While there has been exhaustive research on convex-concave setting, problem of nonconvex-nonconcave setting faces many challenges, such as convergence to limit cycles. Given that general min-max optimization has been found to be intractable, recent research efforts have shifted towards tackling structured problems. One of these follows the weak Minty variational inequality (weak MVI), which is motivated by relaxing Minty variational inequality (MVI) without compromising convergence guarantee of extragradient algorithm. Existing extragradient-type algorithms involve one exploration step and one update step per iteration. We analyze the algorithms with multiple exploration steps and show that current assumption can be further relaxed when more exploration is introduced. Furthermore, we design an adaptive algorithm that explores until the optimal improvement is achieved. This process exploits information from the whole trajectory and effectively tackles cyclic behaviors.",
    "authors": [
        {
            "affiliations": [],
            "name": "ODS WITH"
        },
        {
            "affiliations": [],
            "name": "MULTI-STEP EXPLORATION"
        },
        {
            "affiliations": [],
            "name": "Yifeng Fan"
        },
        {
            "affiliations": [],
            "name": "Yongqiang Li"
        },
        {
            "affiliations": [],
            "name": "Bo Chen"
        }
    ],
    "id": "SP:77f9edbc0b765c182ce13220d8eef147d83025cb",
    "references": [
        {
            "authors": [
                "James K Alcala",
                "Yat Tin Chow",
                "Mahesh Sunkula"
            ],
            "title": "Moving anchor extragradient methods for smooth structured minimax problems",
            "venue": "arXiv preprint arXiv:2308.12359,",
            "year": 2023
        },
        {
            "authors": [
                "Heinz H Bauschke",
                "Walaa M Moursi",
                "Xianfu Wang"
            ],
            "title": "Generalized monotone operators and their averaged resolvents",
            "venue": "Mathematical Programming,",
            "year": 2021
        },
        {
            "authors": [
                "Axel B\u00f6hm"
            ],
            "title": "Solving nonconvex-nonconcave min-max problems exhibiting weak minty solutions",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Radu Ioan Bo\u0163",
                "Axel B\u00f6hm"
            ],
            "title": "Alternating proximal-gradient steps for (stochastic) nonconvexconcave minimax problems",
            "venue": "SIAM Journal on Optimization,",
            "year": 1884
        },
        {
            "authors": [
                "Sayantan Choudhury",
                "Eduard Gorbunov",
                "Nicolas Loizou"
            ],
            "title": "Single-call stochastic extragradient methods for structured non-monotone variational inequalities: Improved analysis under weaker conditions",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2023
        },
        {
            "authors": [
                "Constantinos Daskalakis",
                "Andrew Ilyas",
                "Vasilis Syrgkanis",
                "Haoyang Zeng"
            ],
            "title": "Training gans with optimism",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Constantinos Daskalakis",
                "Dylan J Foster",
                "Noah Golowich"
            ],
            "title": "Independent policy gradient methods for competitive reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Constantinos Daskalakis",
                "Stratis Skoulakis",
                "Manolis Zampetakis"
            ],
            "title": "The complexity of constrained min-max optimization",
            "venue": "In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing,",
            "year": 2021
        },
        {
            "authors": [
                "Jelena Diakonikolas",
                "Constantinos Daskalakis",
                "Michael I Jordan"
            ],
            "title": "Efficient methods for structured nonconvex-nonconcave min-max optimization",
            "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2021
        },
        {
            "authors": [
                "Pierre Foret",
                "Ariel Kleiner",
                "Hossein Mobahi",
                "Behnam Neyshabur"
            ],
            "title": "Sharpness-aware minimization for efficiently improving generalization",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Gauthier Gidel",
                "Hugo Berard",
                "Ga\u00ebtan Vignoud",
                "Pascal Vincent",
                "Simon Lacoste-Julien"
            ],
            "title": "A variational inequality perspective on generative adversarial networks",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2014
        },
        {
            "authors": [
                "Eduard Gorbunov",
                "Adrien Taylor",
                "Samuel Horv\u00e1th",
                "Gauthier Gidel"
            ],
            "title": "Convergence of proximal point and extragradient-based methods beyond monotonicity: the case of negative comonotonicity",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Ya-Ping Hsieh",
                "Panayotis Mertikopoulos",
                "Volkan Cevher"
            ],
            "title": "The limits of min-max optimization algorithms: Convergence to spurious non-critical sets",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "Yu-Guan Hsieh",
                "Franck Iutzeler",
                "J\u00e9r\u00f4me Malick",
                "Panayotis Mertikopoulos"
            ],
            "title": "Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "M Galina"
            ],
            "title": "Korpelevich. The extragradient method for finding saddle points and other problems",
            "venue": "Matecon, 12:747\u2013756,",
            "year": 1976
        },
        {
            "authors": [
                "Sucheol Lee",
                "Donghwan Kim"
            ],
            "title": "Fast extra gradient methods for smooth structured nonconvexnonconcave minimax problems",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Levy",
                "Yair Carmon",
                "John C Duchi",
                "Aaron Sidford"
            ],
            "title": "Large-scale methods for distributionally robust optimization",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Tianyi Lin",
                "Chi Jin",
                "Michael Jordan"
            ],
            "title": "On gradient descent ascent for nonconvex-concave minimax problems",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Panayotis Mertikopoulos",
                "Bruno Lecouat",
                "Houssam Zenati",
                "Chuan-Sheng Foo",
                "Vijay Chandrasekhar",
                "Georgios Piliouras"
            ],
            "title": "Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Arkadi Nemirovski"
            ],
            "title": "Prox-method with rate of convergence o (1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems",
            "venue": "SIAM Journal on Optimization,",
            "year": 2004
        },
        {
            "authors": [
                "Yurii Nesterov"
            ],
            "title": "Dual extrapolation and its applications to solving variational inequalities and related problems",
            "venue": "Mathematical Programming,",
            "year": 2007
        },
        {
            "authors": [
                "Thomas Pethick",
                "Panagiotis Patrinos",
                "Olivier Fercoq",
                "Volkan Cevher\u00e5"
            ],
            "title": "Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Pethick",
                "Olivier Fercoq",
                "Puya Latafat",
                "Panagiotis Patrinos",
                "Volkan Cevher"
            ],
            "title": "Solving stochastic weak minty variational inequalities without increasing batch size",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Leonid Denisovich Popov"
            ],
            "title": "A modification of the arrow-hurwicz method for search of saddle points",
            "venue": "Mathematical notes of the Academy of Sciences of the USSR,",
            "year": 1980
        },
        {
            "authors": [
                "Michael V Solodov",
                "Paul Tseng"
            ],
            "title": "Modified projection-type methods for monotone variational inequalities",
            "venue": "SIAM Journal on Control and Optimization,",
            "year": 1996
        },
        {
            "authors": [
                "Mikhail V Solodov",
                "Benar F Svaiter"
            ],
            "title": "A hybrid projection-proximal point algorithm",
            "venue": "Journal of convex analysis,",
            "year": 1999
        },
        {
            "authors": [
                "Quoc Tran-Dinh"
            ],
            "title": "Sublinear convergence rates of extragradient-type methods: A survey on classical and recent developments",
            "venue": "arXiv preprint arXiv:2303.17192,",
            "year": 2023
        },
        {
            "authors": [
                "Paul Tseng"
            ],
            "title": "On accelerated proximal gradient methods for convex-concave optimization",
            "venue": "SIAM Journal on Optimization,",
            "year": 2008
        },
        {
            "authors": [
                "Zi Xu",
                "Huiling Zhang",
                "Yang Xu",
                "Guanghui Lan"
            ],
            "title": "A unified single-loop alternating gradient projection algorithm for nonconvex\u2013concave and convex\u2013nonconcave minimax problems",
            "venue": "Mathematical Programming,",
            "year": 2023
        },
        {
            "authors": [
                "Junchi Yang",
                "Negar Kiyavash",
                "Niao He"
            ],
            "title": "Global convergence and variance reduction for a class of nonconvex-nonconcave minimax problems",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Min-max optimization has aroused recent interest due to its significant applications in machine learning tasks, such as generative adversarial networks, adversarial training, robust learning and sharpness-aware minimization (Goodfellow et al., 2014; Madry et al., 2018; Levy et al., 2020; Foret et al., 2020).\nMin-max optimization aims to find saddle point of a objective function. Theories and methods have been extensively studied for decades, usually through the prospective of variational inequalities. There has been a plethora of literature focusing on convex-concave (Tseng, 2008; Nesterov, 2007; Nemirovski, 2004) or nonconvex-concave (Xu et al., 2023; Bot\u0327 & Bo\u0308hm, 2023; Lin et al., 2020) objectives. However, problem with nonconvex-nonconcave objective remains relatively obscure. Finding local solution in general nonconvex-nonconcave problems has been proved intractable (Daskalakis et al., 2021).\nOne powerful tool that has been revisited is the celebrated extragradient (Korpelevich, 1976), which has been found effective in solving a class of nonconvex-nonconcave problems (Daskalakis et al., 2020). Diakonikolas et al. (2021) first introduce this new structure as weak Minty variational inequality, featuring a weaker assumption than monotonicity. They make a slight modification on extragradient (named EG+) and establish convergence guarantee on this structure. Pethick et al. (2022) elucidate the mechanism of EG+ and prove a tight parameter range of \u03c1 > \u22121/2L, where \u03c1 represents the weak Minty parameter that describes the degree of non-monotonicity and L is the Lipschitz constant, which is the known best range to have convergence guarantee for this class of problems.\nOur contributions Building on the works of Pethick et al. (2022) and Diakonikolas et al. (2021), we generalize the extragradient algorithm to multi-step cases which adapt to a larger range of problems. Furthermore, we propose a new algorithm that exploits more than local information by\nintroducing adaptive exploration.\n1. We analyze the algorithms with two and more exploration steps. We discover that the range of \u03c1 in weak MVI assumption can be relaxed when more exploration steps are introduced. Especially, when the sum of exploration stepsizes in one iteration is still bounded by 1/L, a similar type of convergence guarantee can be established. Under this restriction, the assumption parameter can be relaxed to \u03c1 > \u2212(1\u22121/e)/L. Additionally, for n = 2, we establish the range of \u03c1 where the stepsizes are unrestricted by these constraints.\n2. Building on the established convergence results, we introduce a novel algorithm that adaptively increases the number of exploration steps. This is inspired by the idea of interpreting each update as a projection onto a certain hyperplane, defined by the weak Minty inequality. Every exploration point provides information and helps narrow down the target range. This algorithm effectively tackles cyclic behaviors.\nRelated work Recent application in GAN training has motivated research on min-max optimization and revisit to classic algorithms. A line of pioneering works introduces the traditional perspective of variational inequality and resorts to optimistic methods (Daskalakis et al., 2018; Gidel et al., 2018; Mertikopoulos et al., 2018). Simple algorithm such as gradient descent ascent is also examined under the Polyak-\u0141ojasiewicz condition (Yang et al., 2020). Hsieh et al. (2021) revisit a list of algorithms including extragradient and demonstrate that current methods still have trouble avoiding limit cycles.\nWeak Minty variational inequality, also known as star-negative comonotonicity, has received recent attention from community and has become a common structure setting. Diakonikolas et al. (2021) first introduced weak MVI as an important assumption, under which their EG+ algorithm enjoys O(1/ \u221a k) convergence rate. Pethick et al. (2022) provide a full picture of the algorithm with an adaptive stepsize selection technique. They also propose an algorithm with backtracking line search incorporated, which effectively escape limit cycles and outperforms existing methods. Bo\u0308hm (2022) propose another variant of EG+ with adaptive stepsize, not requiring knowledge of problem parameter \u03c1 and L. While most literature provide best-iterate convergence results, last-iterate convergence of EG+ under weak MVI has also been established (Gorbunov et al., 2023; Tran-Dinh, 2023), but under more restrictive parameter \u03c1 > \u22121/8L. Another line of work resorts to anchoring techniques. Lee & Kim (2021) combine anchored extragradient with separate stepsizes and obtain a fast O(1/k) rate on gradient norm. Alcala et al. (2023) introduce new moving anchor technique generalizing current algorithms and attain optimal convergence rate. These works are under a more restrictive (unstarred) negative comonotonicity assumption.\nWhen it comes to stochastic methods, similar thought to EG+ can be found in Hsieh et al. (2020), where they show that doubled exploration stepsize in stochastic extragradient is effective on avoiding cycles. Diakonikolas et al. (2021) extend their deterministic result to stochastic setting with unbiased oracle of gradient and bounded variance by increasing oracle queries. Pethick et al. (2023) establish almost sure convergence with single query per iteration, involving one fixed and one diminishing stepsize. Their method works under the same assumption of \u03c1 > \u22121/2L. Aside from separate stepsizes, other variants such as stochastic past extragradient (SPEG) have been shown effective under weak MVI assumption, along with a new expected residual condition (Choudhury et al., 2023)."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "We consider two real vectors, x \u2208 Rdx ,y \u2208 Rdy and minimax problems of the form:\nmin x\u2208Rdx max y\u2208Rdy f(x,y) (2.1)\nwhere f : Rdx \u00d7 Rdy \u2192 R is a smooth (possibly nonconvex-nonconcave) function and dx + dy = d.\nTo study the stationary point, we consider the saddle gradient operator F : Rd \u2192 Rd defined via\nFz = [ \u2207xf(x,y) \u2212\u2207yf(x,y) ] , where z = [ x y ] .\nWe want to find zeros of this operator, the set of which denoted by zerF := {z \u2208 Rd | Fz = 0}. This is a first-order necessary condition of z being the saddle point satisfying (2.1).\nIn this paper, we study problems in which operator F satisfies the following assumptions. Assumption 1. (L-Lipschitz continuity). Operator F is L-Lipschitz continuous. For any u,v \u2208 Rd,\n\u2016Fu\u2212 Fv\u2016 \u2264 L\u2016u\u2212 v\u2016 (2.2) Assumption 2. (Weak MVI). There exists z\u2217 \u2208 zerF such that for any z \u2208 Rd,\n\u3008Fz, z \u2212 z\u2217\u3009 \u2265 \u03c1\u2016Fz\u20162 (2.3) for some \u03c1 \u2208 (\u03c10,\u221e).\nIn this paper, the proposed methods provide convergence guarantee for problems where \u03c1 \u2208 (\u2212(1\u22121/e)/L,\u221e), where e is Euler\u2019s number."
        },
        {
            "heading": "2.1 PRELIMINARY ALGORITHMS",
            "text": "EG (Korpelevich, 1976):\nz\u0304k = zk \u2212 \u03b1kFzk\nzk+1 = zk \u2212 \u03b1kF z\u0304k (EG)\nwhere \u03b3k is the stepsize.\nExtragradient is a classical algorithm for saddle point problems. Often regarded as an explicit approximation of the proximal point method, EG employs the same stepsize for both extrapolation and update phases.\nEG+ (Diakonikolas et al., 2021):\nz\u0304k = zk \u2212 \u03b3kFzk\nzk+1 = zk \u2212 \u03b1kF z\u0304k (EG+)\nwhere \u03b3k = 1/L and \u03b1k = 1/2L are the stepsizes.\nEG+ is a generalization of EG, allowing an aggressive extrapolation step. This slight modification makes it effective for weak MVI problems under \u03c1 > \u22121/8L. AdaptiveEG+ (Pethick et al., 2022):\nz\u0304k = zk \u2212 \u03b3kFzk\n\u03b1k = \u03c3k \u2212 \u3008F z\u0304k, z\u0304k \u2212 zk\u3009 \u2016F z\u0304k\u20162\nzk+1 = zk \u2212 \u03bbk\u03b1kF z\u0304k, \u03bbk \u2208 (0, 2)\n(AdaptiveEG+)\nwhere \u03b3k \u2208 (b\u22122\u03c1c+, 1/L], \u03b1k are the stepsizes and \u03c3k \u2208 (\u2212\u03b3k/2, \u03c1], \u03bbk \u2208 (0, 2) are relaxation parameters.\nThis algorithm provides a tight range of \u03b1k for convergence of (EG+). Diving into the conception of projection, (AdaptiveEG+) broadens the problem range to \u03c1 > \u22121/2L, which has been the best known result for weak MVI problems."
        },
        {
            "heading": "2.2 PRELIMINARY DEFINITIONS",
            "text": "Definition 2.1. (Weak MVI halfspace). Given u \u2208 Rd. Define weak MVI halfspace at u as the set restricted by (2.3) at u,\nD(u) := {w \u2208 Rd | \u3008Fu,u\u2212w\u3009 \u2265 \u03c1\u2016Fu\u20162} (2.4) The boundary of D(u) is a hyperplane with Fu as normal vector,\n\u2202D(u) := {w \u2208 Rd | \u3008Fu,u\u2212w\u3009 = \u03c1\u2016Fu\u20162} (2.5)\nUnder Assumption 2, we have \u2200u \u2208 Rd, z\u2217 \u2208 D(u). As we access gradient at any point, we get the information that zero z\u2217 must be inside the corresponding halfspace. Definition 2.2. (Signed distance). Given u \u2208 Rd and convex set S \u2282 Rd. The signed distance from u to S is defined by\nd(u,S) := { d(u, \u2202S), if u \u2208 Sc \u2212d(u, \u2202S), if u \u2208 S (2.6)\nwhere \u2202S denotes the boundary of S and Sc denotes the complement of S. d(u, \u2202S) is defined by d(u, \u2202S) := infv\u2208\u2202S d(u,v).\nThe signed distance d(u,S) is positive when u is out of S and negative when u is inside S. In this paper, we focus on the signed distance between a point and a weak MVI halfspace, which is linear and therefore convex. More specifically, the iteration point zk and weak MVI halfspace at exploration point zki . Lemma 2.3. Given u,v \u2208 Rd, the signed distance from v to D(u) is\nd(v,D(u)) = \u03c1\u2016Fu\u2016 2 \u2212 \u3008Fu,u\u2212 v\u3009 \u2016Fu\u2016\n(2.7)"
        },
        {
            "heading": "3 GENERALIZED FRAMEWORK OF PROJECTION ALGORITHMS",
            "text": "Projection-type algorithms have been extensively utilized in variational inequality problems (Solodov & Tseng, 1996; Solodov & Svaiter, 1999). Recent work of Pethick et al. (2022) employs projection technique in generalizing extragradient algorithm.\nNotice that in (AdaptiveEG+), the update stepsize \u03b1k = \u03c3k \u2212 \u3008F z\u0304 k,z\u0304k\u2212zk\u3009 \u2016F z\u0304k\u20162 \u2264 d(zk,D(z\u0304k)) \u2016F z\u0304k\u2016 .\nThe main idea lies in perceiving each update as a projection onto the hyperplane \u2202D(z\u0304k). Due to Assumption 2, z\u2217 \u2208 D(z\u0304k), while zk /\u2208 D(z\u0304k) since the algorithm is designed to make \u03b1k > 0. As a result, the hyperplane naturally separates zk and z\u2217. The parameters of \u03c3k \u2264 \u03c1 and \u03bb \u2208 (0, 2) bear no effect on convergence, given the fact that scaling the projection distance no more than twice still takes you closer to the hyperplane.\nIt is interesting to notice that in this process, there is no restriction on the selection of z\u0304k. Literally any z\u0304k generates a halfspace that z\u2217 \u2208 D(z\u0304k). Therefore the same mechanism still applies as long as z\u0304k is chosen such that zk /\u2208 D(z\u0304k), or equivalently d(zk,D(z\u0304k)) > 0. In other words, as long as at every iteration point, we can find another point such that the iteration point is out of its weak MVI halfspace. The border hyperplane consequently separates the iteration point zk and the desired zero z\u2217. Therefore intuitively, a projection onto the hyperplane get closer to z\u2217. This point does not necessarily have to be attained by a single forward operator evaluation z\u0304k = zk \u2212 \u03b3kFzk. Given the foregoing discussion, it is straightforward to propose the following framework for solving the weak MVI type saddle point problems.\nz\u0304k = Gkz k\n\u03b1k = \u03c3k \u2212 \u3008F z\u0304k, z\u0304k \u2212 zk\u3009 \u2016F z\u0304k\u20162 > 0, \u03c3k \u2264 \u03c1\nzk+1 = z\u0304k \u2212 \u03bbk\u03b1kF z\u0304k, \u03bbk \u2208 (0, 2)\n(3.1)\nwhere for all k, Gk : Rd \u2192 Rd is a map such that \u2200z \u2208 Rd, d(z,D(Gkz)) > 0 and \u03c3k \u2264 \u03c1 is selected such that \u03b1k > 0.\nIn other words, if for every zk we are able to find z\u0304k so that zk /\u2208 D(z\u0304k), then the gradient F z\u0304k can be used for extragradient update. Our objective is to find such maps that guarantee this property. One example is Gk = id \u2212 \u03b3kF with b\u2212 \u03c11+\u03c1Lc+ < \u03b3k \u2264 1/L used in EG+, where id denotes identity operator and bxc+ = max{0, x}. Such algorithms enjoy similar convergence guarantee to (AdaptiveEG+)((Pethick et al., 2022), Thm. 3.1), with an O(1/\u221ak) best-iterate convergence rate.\nTheorem 3.1. Let F be L-Lipschitz and satisfy weak Minty condition with \u03c1. Let \u03bbk \u2208 (0, 2), \u03c3k \u2264 \u03c1. Assume that lim infk\u2192\u221e \u03bbk(2\u2212 \u03bbk) > 0 and lim infk\u2192\u221e \u03b1k > 0. Assume that for all k, d(zk,D(z\u0304k)) > 0. Assume that \u03c3k is selected such that \u03b1k > 0. Consider the sequences (zk)k\u2208N and (z\u0304k)k\u2208N generated by (3.1). Then,\nmin k=0,1,...,m \u2016F z\u0304k\u20162 \u2264 1 \u03ba(m+ 1) \u2016z0 \u2212 z\u2217\u20162 (3.2)\nwhere \u03ba = lim infk\u2192\u221e \u03bbk(2\u2212 \u03bbk)\u03b12k. Moreover, (z\u0304k)k\u2208N converges to z\u2217.\nNote that no limitation on \u03c1 is mentioned in this framework. Actually, the range of manageable \u03c1 depends on the selection of \u03c3k and the map Gk, which will be covered in the next section. More specifically, \u03c1 > supz\u0304k \u3008F z\u0304k,z\u0304k\u2212zk\u3009 \u2016F z\u0304k\u20162 , where the right-hand side is related to the settings of Gk.\nFrom another perspective, every operator evaluation Fz provides new information about z\u2217. z\u2217 \u2208 D(z) rules out the possibility that z\u2217 is in another halfspace. This explains why increasing exploration plays a crucial role. Gradient descent ascent fails in \u03c1 < 0 weak MVI problems since both zk and z\u2217 is in D(zk) and the hyperplane cannot separate them. (AdaptiveEG+) manages to find suitable z\u0304k with a larger extrapolation stepsize to separate them and update (project) with a smaller stepsize.\nIt is then natural to increase exploration by taking further steps in the subroutine Gk, pursuing a larger projection distance. See Fig. 1 for intuition. This naturally leads to a question: what kind of convergence guarantee can be provided for aforementioned multi-step algorithms? We address this problem in Section 4 and introduce the \u201cmax distance\u201d algorithm in Section 5."
        },
        {
            "heading": "4 MULTI-STEP EXTRAGRADIENT",
            "text": "We start from the (AdaptiveEG+) algorithm and generalize it to multi-step cases.\nPethick et al. (2022) obtained a range of \u03c1 \u2208 (\u2212 12L ,\u221e) for the algorithm to converge and demonstrated its tightness. Moreover, they restated the condition as \u03c1 > \u2212\u03b3k/2, where the stepsize satisfies \u03b3k \u2264 1/L. We reexamine the claim and articulate our understanding. The key is to assure a positive projection distance. Taking \u03c3k = \u03c1,\n\u03b1k = \u03c1\u2212 \u3008F z\u0304k, z\u0304k \u2212 zk\u3009 \u2016F z\u0304k\u20162 = \u03c1+ \u03b3k\u3008F z\u0304k, Fzk\u3009 \u2016F z\u0304k\u20162 \u2265 \u03c1+ \u03b3k 1 + \u03b3kL\n(4.1)\nThe inequality follows from rearranging \u2016F z\u0304k \u2212 Fzk\u2016 \u2264 \u03b3kL\u2016Fzk\u2016 into \u2016Fzk \u2212 11\u2212\u03b32kL2F z\u0304 k\u2016 \u2264\n\u03b3kL 1\u2212\u03b32kL2 \u2016F z\u0304k\u2016 and applying Cauchy-Schwarz inequality on it.\nRather than the stated \u03c1 > \u2212\u03b3k/2 which only applies when \u03b3k \u2264 1/L, a more precise condition should be \u03c1 > \u2212 \u03b3k1+\u03b3kL . Increasing extrapolation stepsize \u03b3k does not extend the lower bound of \u03c1 infinitely,\nbut push it closer to \u22121/L. Actually, objective functions with \u03c1 \u2264 \u22121/L are knotty for first order methods. For example, both Fz = Lz and Fz = \u2212Lz falls into the structure. Particularly, when \u03b2 \u2264 \u22121, \u03b2-cohypomonotone operator F may fail to have an at most single-valued resolvent JF , making it problematic in finding fixed points of JF (Table 1, (Bauschke et al., 2021))."
        },
        {
            "heading": "4.1 2-STEP EXTRAGRADIENT",
            "text": "Consider the n-step extragradient.\n\u2200i \u2208 [n], zki = zki\u22121 \u2212 \u03b3k,iFzki\u22121 z\u0304k = zkn\n\u03b1k = \u03c3k \u2212 \u3008F z\u0304k, z\u0304k \u2212 zk\u3009 \u2016F z\u0304k\u20162 > 0, \u03c3k \u2264 \u03c1\nzk+1 = z\u0304k \u2212 \u03bbk\u03b1kF z\u0304k, \u03bbk \u2208 (0, 2)\n(n-step EG)\nwhere zk0 = z k is the current point, zki , i \u2208 [n] are intermediate steps, and zk+1 is the adopted update. The extrapolation point z\u0304k is attained by n steps of gradient descent.\nIn this subsection we focus on the case where n = 2. Recall that selection of stepsizes and \u03c3k plays a crucial rule in the algorithm and determines the range of problem parameter \u03c1 that the algorithm can address. We elaborate in the subsequent theorem how to choose parameters that ensures convergence. Theorem 4.1. Let F be L-Lipschitz and satisfy weak Minty condition with \u03c1. Let n = 2. Assume that for all k, \u03b3k,1 = \u03b41/L, \u03b3k,2 = \u03b42/L, and \u03b41, \u03b42 \u2208 (0, 1). Assume that lim infk\u2192\u221e \u03bbk(2\u2212\u03bbk) > 0 and lim infk\u2192\u221e \u03b1k > 0. If for all k, \u03c3k \u2264 \u03c1 and\n\u03c3k > \u2212 1 L [ 1\u2212 1(1+\u03b41)(1+\u03b42) ] if \u03b41 + \u03b42 \u2264 1 \u2212 1L [ \u03b41(1\u2212\u03b421\u2212\u03b4 2 2) 2(1\u2212\u03b421)(1\u2212\u03b422) + \u03b421+\u03b42 ] if \u03b41 + \u03b42 > 1\n(4.2)\nThen the sequence (z\u0304k)k\u2208N generated by (n-step EG) satisfies mink=0,1,...,m\u2016F z\u0304k\u20162 \u2264 1\n\u03ba(m+1)\u2016z 0 \u2212 z\u2217\u20162, where \u03ba = lim infk\u2192\u221e \u03bbk(2\u2212 \u03bbk)\u03b12k.\nSelecting the parameters according to (4.2) guarantees best-iterate convergence. In the following theorem, we present the specific parameter selection that maximize the range of \u03c1 in both cases. Theorem 4.2. Let F be L-Lipschitz and satisfy weak Minty condition with \u03c1. Let n = 2. Assume that for all k, \u03b3k,1 = \u03b41/L, \u03b3k,2 = \u03b42/L, and \u03b41, \u03b42 \u2208 (0, 1). Assume that lim infk\u2192\u221e \u03bbk(2\u2212\u03bbk) > 0 and lim infk\u2192\u221e \u03b1k > 0. Let \u03ba = lim infk\u2192\u221e \u03bbk(2\u2212 \u03bbk)\u03b12k.\n(i) If for all k, \u03b41 = 12 , \u03b42 = 1 2 and \u03c3k = \u03c1 > \u2212 5 9L , then the sequence (z\u0304 k)k\u2208N generated by (n-step EG) satisfies mink=0,1,...,m\u2016F z\u0304k\u20162 \u2264 1\u03ba(m+1)\u2016z 0 \u2212 z\u2217\u20162.\n(ii) If for all k, \u03b41 = \u03b4\u03021, \u03b42 = \u03b4\u03022 and \u03c3k = \u03c1 > \u2212 \u03b6L , where \u03b4\u03021 \u2248 0.52212, \u03b4\u03022 \u2248 0.644793 is the unique solution of following equations, \u03b422(1 + \u03b4 2 1) = (1\u2212 \u03b421)2 \u03b461 = (1 + \u03b4 2 1)(1\u2212 \u03b42)4\n\u03b41 + \u03b42 > 1, \u03b41, \u03b42 < 1\n(4.3)\nand \u03b6 = \u03b4\u03021(1\u2212\u03b4\u0302 2 1\u2212\u03b4\u0302 2 2)\n2(1\u2212\u03b4\u030221)(1\u2212\u03b4\u030222) + \u03b4\u03022 1+\u03b4\u03022 \u2248 0.5834, then the sequence (z\u0304k)k\u2208N generated by (n-step\nEG) satisfies mink=0,1,...,m\u2016F z\u0304k\u20162 \u2264 1\u03ba(m+1)\u2016z 0 \u2212 z\u2217\u20162.\nThe theorem presents quite interesting results. When the sum of sub-iteration stepsizes is bounded by 1/L, a familiar stepsize choice in preliminary algorithms, the optimal range is attained under a succinct invariant stepsize setting. However in otherwise situation, suggested parameters are highly complicated varying stepsizes. Note that invariant and varying both refer to sub-iteration stepsizes \u03b3k,1, . . . , \u03b3k,n here and in the subsequent discussion. See Fig. 6 in Appendix C.1 for a contour of (4.2) that incorporates the results in Theorem 4.1 and 4.2.\n4.2 n-STEP EXTRAGRADIENT While the case of \u2211n i=1 \u03b3k,iL > 1 is convoluted even when n = 2, we are able to generalize the\nresults when \u2211n i=1 \u03b3k,iL \u2264 1 to n \u2265 3 cases: Theorem 4.3. Let F be L-Lipschitz and satisfy weak Minty condition with \u03c1. Assume that for all k and i \u2208 [n], \u03b3k,i = \u03b4i/L, \u03b4i \u2208 (0, 1) and \u2211n i=1 \u03b4i \u2264 1. Assume that lim infk\u2192\u221e \u03bbk(2 \u2212 \u03bbk) > 0 and lim infk\u2192\u221e \u03b1k > 0. If for all k, \u03c3k \u2264 \u03c1 and\n\u03c3k > \u2212 1\nL\n( 1\u2212\nn\u220f i=1 1 1 + \u03b4i\n) (4.4)\nThen the sequence (z\u0304k)k\u2208N generated by (n-step EG) satisfies mink=0,1,...,m\u2016F z\u0304k\u20162 \u2264 1\n\u03ba(m+1)\u2016z 0 \u2212 z\u2217\u20162, where \u03ba = lim infk\u2192\u221e \u03bbk(2\u2212 \u03bbk)\u03b12k.\nGiven the positive result extended, this next theorem expands our understanding of parameter selection from point to line, thus broadening the algorithm\u2019s adaptability. Theorem 4.4. Let F be L-Lipschitz and satisfy weak Minty condition with \u03c1. Assume that for all k and i \u2208 [n], \u03b3k,i = \u03b3 = \u03b4/L. Assume that lim infk\u2192\u221e \u03bbk(2\u2212 \u03bbk) > 0 and lim infk\u2192\u221e \u03b1k > 0.\n(i) If \u03b4 \u2208 ( b 1n\u221a1+\u03c1L \u2212 1c+, 1 n ] , \u03c3k \u2208 ( \u2212n [ 1\u2212 1\n(1+ 1n ) n\n] \u03b3, \u03c1 ] and \u03c1 > \u2212 1L [ 1\u2212 1\n(1+ 1n ) n\n] ,\nwhere bxc+ := max(0, x), then the sequence (z\u0304k)k\u2208N generated by (n-step EG) satisfies mink=0,1,...,m\u2016F z\u0304k\u20162 \u2264 1\u03ba(m+1)\u2016z 0 \u2212 z\u2217\u20162, where \u03ba = lim infk\u2192\u221e \u03bbk(2\u2212 \u03bbk)\u03b12k.\n(ii) Given any \u03c1 > \u2212 1L (1\u2212 1 e ), let n \u2265 d 1 2+2 log (1+\u03c1L)e, then \u03c1 > \u2212 1 L\n[ 1\u2212 1\n(1+ 1n ) n\n] .\nTheorem 4.4 (i) provides the ranges for \u03b3k and \u03c3k under invariant stepsize. When n = 1, the algorithm recovers the parameter range \u03b3 \u2208 ( b\u2212 \u03c11+\u03c1Lc+, 1/L ] and \u03c3k \u2208 (\u2212\u03b3/2, \u03c1] from (AdaptiveEG+). Selecting \u03c3k near its lower bound is the common practice when \u03c1 is unknown. Theorem 4.4 (ii) establishes the global range of \u03c1 \u2208 (\u2212(1\u22121/e)/L,\u221e) in this paper. We remark that the range of \u03c1 in Theorem 4.4 (i) is not the global optimal result. It can be improved if there are no restriction on \u2211n i=1 \u03b3k,iL, just as in the case of n = 2. Yet it is interesting to figure out whether the global optimum of \u2212\u03c10L converges to 1 \u2212 1/e as n \u2192 \u221e. Our answer is no. In numerical experiments, 3-step EG with \u03b41 \u2248 0.272899, \u03b42 \u2248 0.512753, \u03b43 \u2248 0.515522 demonstrates a range of \u2212\u03c10L \u2248 0.632242, which already exceeds 1\u2212 1/e, and 4-step EG improves it to at least \u2212\u03c10L \u2248 0.657724. Refer to Appendix C.4 for more details. This leaves room for further enhancements."
        },
        {
            "heading": "5 ADAPTIVE EXPLORATION BY PURSUING MAX DISTANCE",
            "text": "It is discussed in Section 4 that increasing extrapolation stepsize push the lower bound of \u03c1 towards the threshold of \u22121/L. Inspired by this we propose an algorithm that explores aggressively. In the following scheme, extrapolation process will not stop until projection distance stops increasing.\nThe algorithm aims to find a projection distance as large as possible. Thanks to the convergence results in Section 4.2, choosing early stepsizes in accordance with (n-step EG) guarantees a positive distance for problems with \u03c1 > \u2212 1L ( 1\u2212 1e ) and subsequent explorations will only increase it.\nNote that the distances may converge and thus be monotonic, when the GDA sub-iteration of zki directly converges. Therefore a very small tolerance \u03b51 > 0 is introduced to preclude the subiterations from endless loop. The algorithm may have worse complexity in such scenarios when F are more structured than monotone, reflecting a trade-off between complexity in easier problems and convergence in a broader class. \u03b52 recovers GDA when the algorithm potentially stagnates due to intractable local environment, thereby circumvents thorny areas (see Example 3 and Appendix D.3).\nParameter choice and knowledge of \u03c1 By setting \u03c3k = \u03c1, Algorithm 1 exploit the information of \u03c1 to the largest extent. A smaller \u03c3k \u2264 \u03c1 still guarantees convergence as long as \u03b1k > 0, since\nAlgorithm 1 Max Distance Extragradient initialize:\nz0 \u2208 Rn, \u03c1, \u03bbk \u2208 (0, 2), \u03b3k \u2208 (0, 1/L], \u03c3k \u2264 \u03c1 and tolerance \u03b51, \u03b52 > 0 repeat for k = 0, 1, . . .\nLet zk0 = z k, dk0 = \u2212\u221e repeat for i = 1, 2, . . . Let zki = z k i\u22121 \u2212 \u03b3kFzki\u22121\nCompute estimated distance\ndki = \u03c3k\u2016Fzki \u20162 \u2212 \u3008Fzki , zki \u2212 zk\u3009\n\u2016Fzki \u2016 until dki \u2212 dki\u22121 < \u03b51\u2016Fzki \u2016 Let z\u0304k = zki\u22121, d\u0304\nk = dki\u22121 Compute stepsize\n\u03b1k = d\u0304k\n\u2016F z\u0304k\u2016 = \u03c3k \u2212 \u3008F z\u0304k, z\u0304k \u2212 zk\u3009 \u2016F z\u0304k\u20162\nUpdate the vector: if \u03b1k \u2265 \u03b52 then zk+1 = zk \u2212 \u03bbk\u03b1kF z\u0304k else zk+1 = z\u0304k\nuntil convergence return zk+1\nprojection onto a larger halfspace still proceed toward the goal. A safe parameter range can be found in Theorem 4.4 (i). More aggressive candidates such as \u03c3k = \u22121/L may apply to harder problems."
        },
        {
            "heading": "6 EXAMPLES AND EXPERIMENTS",
            "text": "We consider three classic examples corresponding to \u03c1 > \u2212 1\u2212 1 e\nL , \u03c1 \u2208 (\u2212 1 L ,\u2212 1\u2212 1e L ), \u03c1 < \u2212 1 L\nrespectively. Example 1. (bilinear)\nmin x\u2208R max y\u2208R\nf(x, y) := axy + b\n2 (x2 \u2212 y2) (Bilinear)\nwhere a > 0, b < 0. Example 2. ((Pethick et al., 2022), Example 3)\nFz = (\u03c8(x, y)\u2212 y, \u03c8(y, x)\u2212 x) (PolarGame) where \u03c8(x, y) = 116ax(\u22121 + x\n2 + y2)(\u22129 + 16x2 + y2) and a = 1. Example 3. ((Hsieh et al., 2021), Example 5.2)\nmin x\u2208R max y\u2208R f(x, y) := x(y \u2212 0.45) + \u03c6(x)\u2212 \u03c6(y) (Forsaken)\nwhere \u03c6(z) = 14z 2 \u2212 12z 4 + 16z 6.\nTested algorithms include (n-step EG), (MDEG) in this paper, (AdaptiveEG+), (CurvatureEG+) from (Pethick et al., 2022) and (EG+ Adaptive) from (Bo\u0308hm, 2022). All experiments are implemented without the knowledge of \u03c1. In Example 1 we choose the parameters to make \u03c1L \u2208 (\u22120.6,\u22120.5) and verify the convergence result of (n-step EG) in Theorem 4.4(i). Example 2 exhibits two limit cycles, one attracting and one repellent. (Algorithm 1) excels at handling such cyclic problems and evades the limit cycle in the first iteration. Example 3 further exceeds the manageable threshold of \u03c1 > \u22121/L, posing challenges for the algorithms. The basic version of (Algorithm 1) erroneously stagnates in the problematic area, echoing the discussion on intractability. Introducing the tolerance \u03b52 prevents the algorithms from incorrect convergence and helps to recover GDA when in a predicament. The modified algorithm circumvents the thorny area, as shown in Fig. 4. Among all tested algorithms, (Algorithm 1) and (CurvatureEG+) converge in Example 2 and 3, where our method relies solely on global information."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "This paper opens up a new dimension for extragradient-type algorithms and demonstrates how expanding extrapolation could help address more problems. We provide bound analysis on our framework of multi-step extrapolation EG+ algorithms, relax the condition \u03c1 > \u22121/2L to \u03c1 > \u2212(1\u22121/e)/L and capture past algorithms as special cases. Furthermore, the adaptive method we propose effectively resolves problems with limit cycles. While our method utilizes repeated GDA steps in its subroutine, investigating alternative subroutines that offer better approximation of the proximal point operator could represent a valuable research direction."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work has been supported by National Natural Science Foundation of China (NSFC) Grant No. 62073294, No. U2341216. We would like to thank the anonymous reviewers for spending time and efforts and bringing in many insightful comments and suggestions, which greatly contributed to the improvement of this work."
        },
        {
            "heading": "A TECHNICAL LEMMAS",
            "text": "Lemma A.1. Let F be L-Lipschitz and satisfy weak Minty condition with \u03c1. Assume that for any zk \u2208 Rd, there is z\u0304k \u2208 Rd such that d(zk,D(z\u0304k)) > 0. Let \u03c3k \u2264 \u03c1, \u03bbk > 0. Let \u03b1k = \u03c3k\u2212 \u3008F z\u0304 k,z\u0304k\u2212zk\u3009 \u2016F z\u0304k\u20162 , zk+1 = zk \u2212 \u03bbk\u03b1kF z\u0304k. Assume that for all k, \u03b1k > 0. Then,\n\u2016zk+1 \u2212 z\u2217\u20162 \u2264 \u2016zk \u2212 z\u2217\u20162 \u2212 \u03bbk(2\u2212 \u03bbk)\u03b12k\u2016F z\u0304k\u20162 (A.1)\nProof. From the expression of \u03b1k we know that\n\u3008F z\u0304k, z\u0304k \u2212 zk\u3009 = (\u03c3k \u2212 \u03b1k)\u2016F z\u0304k\u20162 (A.2) \u2264 (\u03c1\u2212 \u03b1k)\u2016F z\u0304k\u20162 (A.3)\nTogether with Assumption 2\n\u3008F z\u0304k, zk \u2212 z\u2217\u3009 \u2265 \u03b1k\u2016F z\u0304k\u20162 (A.4)\nTherefore,\n\u2016zk+1 \u2212 z\u2217\u20162 = \u2016zk \u2212 z\u2217 \u2212 \u03bbk\u03b1kF z\u0304k\u20162 (A.5) = \u2016zk \u2212 z\u2217\u20162 \u2212 2\u03bbk\u03b1k\u3008F z\u0304k, zk \u2212 z\u2217\u3009+ \u03bb2k\u03b12k\u2016F z\u0304k\u20162 (A.6) \u2264 \u2016zk \u2212 z\u2217\u20162 \u2212 2\u03bbk\u03b12k\u2016F z\u0304k\u20162 + \u03bb2k\u03b12k\u2016F z\u0304k\u20162 (A.7) = \u2016zk \u2212 z\u2217\u20162 \u2212 \u03bbk(2\u2212 \u03bbk)\u03b12k\u2016F z\u0304k\u20162 (A.8)\nRemark 1. It is worth mentioning that Lemma A.1 is highly related to (Solodov & Svaiter, 1999), Lemma 2.1.\nNotice that when \u03bbk = 1,\n\u3008F z\u0304k, zk \u2212 z\u0304k + \u03c3kF z\u0304k\u3009 = \u03b1k\u2016F z\u0304k\u20162 > 0 (A.9) \u3008F z\u0304k, z\u2217 \u2212 z\u0304k + \u03c3kF z\u0304k\u3009 = (\u03c3k \u2212 \u03c1)\u2016F z\u0304k\u20162 \u2264 0 (A.10)\nand zk+1 = PH[zk] = zk \u2212 \u03b1kF z\u0304k where PH[zk] is the projection of zk onto the hyperplane H = {w \u2208 Rd | \u3008F z\u0304k,w \u2212 z\u0304k + \u03c3kF z\u0304k\u3009 = 0}, which is evident from (A.9). According to (Solodov & Svaiter, 1999), Lemma 2.1,\n\u2016zk \u2212 z\u2217\u20162 \u2265 \u2016zk+1 \u2212 z\u2217\u20162 + \u2016zk+1 \u2212 zk\u20162 (A.11) = \u2016zk+1 \u2212 z\u2217\u20162 + \u03b12k\u2016F z\u0304k\u20162 (A.12)\nwhich is in accordance with Lemma A.1.\nLemma A.2. Let F be L-Lipschitz and satisfy weak Minty condition with \u03c1. Let n = 2. Assume that for all k, \u03b3k,1 = \u03b41/L, \u03b3k,2 = \u03b42/L, and \u03b41, \u03b42 \u2208 (0, 1). Define function g : (0, 1)\u00d7 (0, 1)\u2192 R as\ng(\u03b41, \u03b42) :=\n{ 1\n(1+\u03b41)(1+\u03b42) if \u03b41 + \u03b42 \u2264 1\n1\u2212\u03b421\u2212\u03b4 2 2\n2(1\u2212\u03b421)(1\u2212\u03b422) if \u03b41 + \u03b42 > 1\n(A.13)\nInitialize zk and generate the sequence (zki )i\u2208[n] by (n-step EG). The following inequality holds and is tight:\n\u3008Fzk2 , Fzk\u3009 \u2016Fzk2\u20162 \u2265 g(\u03b41, \u03b42) (A.14)\nProof. In this proof we treat Fzk2 as an axis. Define following as coordinates with respect to Fz k 2 ,\nx := \u3008Fzk, Fzk2 \u3009 \u2016Fz2\u20162\n(A.15)\nx1 := \u3008Fzk1 , Fzk2 \u3009 \u2016Fz2\u20162\n(A.16)\nFrom (n-step EG) and Lipschitz continuity (Assumption 1), it is clear that\n\u2016Fzk \u2212 Fzk1\u2016 \u2264 \u03b3k,1L\u2016Fzk\u2016 = \u03b41\u2016Fzk\u2016 (A.17) \u2016Fzk1 \u2212 Fzk2\u2016 \u2264 \u03b3k,2L\u2016Fzk1\u2016 = \u03b42\u2016Fzk1\u2016 (A.18)\nChanging the perspective to fixed Fzk1 and Fz k 2 we expand and recomplete the square\n\u2016Fzk \u2212 1 1\u2212 \u03b421 Fzk1\u2016 \u2264 \u03b41 1\u2212 \u03b421 \u2016Fzk1\u2016 (A.19)\n\u2016Fzk1 \u2212 1\n1\u2212 \u03b422 Fzk2\u2016 \u2264 \u03b42 1\u2212 \u03b422 \u2016Fzk2\u2016 (A.20)\nwhere Fzk1 is in a sphere whose center and radius is proportional to Fz k 2 , and Fz k is in a sphere whose center and radius is proportional to Fzk1 . The boundary of such area is called a Cartesian oval in two dimension, or a Cartesian surface in three or more dimensions.\nApply Cauchy-Schwarz inequality on (A.20)\n\u2212 \u03b42 1\u2212 \u03b422 \u2016Fzk2\u20162 \u2264 \u3008Fzk1 \u2212 1 1\u2212 \u03b422 Fzk2 , Fz k 2 \u3009 \u2264 \u03b42 1\u2212 \u03b422 \u2016Fzk2\u20162 (A.21)\n1\n1 + \u03b42 \u2016Fzk2\u20162 \u2264 \u3008Fzk1 , Fzk2 \u3009 \u2264\n1\n1\u2212 \u03b42 \u2016Fzk2\u20162 (A.22)\n1\n1 + \u03b42 \u2264 x1 \u2264\n1\n1\u2212 \u03b42 (A.23)\nApply Cauchy-Schwarz inequality on (A.19)\n\u3008Fzk \u2212 1 1\u2212 \u03b421 Fzk1 , Fz k 2 \u3009 \u2265 \u2212\u2016Fzk \u2212 1 1\u2212 \u03b421 Fzk1\u2016\u2016Fzk2\u2016 (A.24)\n\u2265 \u2212 \u03b41 1\u2212 \u03b421 \u2016Fzk1\u2016\u2016Fzk2\u2016 (A.25)\n\u3008Fzk, Fzk2 \u3009 \u2265 1\n1\u2212 \u03b421 \u3008Fzk1 , Fzk2 \u3009 \u2212 \u03b41 1\u2212 \u03b421 \u2016Fzk1\u2016\u2016Fzk2\u2016 (A.26)\nDivide both sides with \u2016Fzk2\u20162 we have\nx \u2265 1 1\u2212 \u03b421 x1 \u2212 \u03b41 1\u2212 \u03b421 \u2016Fzk1\u2016 \u2016Fzk2\u2016\n(A.27)\nFrom \u2016Fzk1 \u2212 Fzk2\u2016 \u2264 \u03b42\u2016Fzk1\u2016 it is straightforward to see \u2016Fzk1\u20162 \u2264 21\u2212\u03b422 \u3008Fz k 1 , Fz k 2 \u3009 \u2212\n1 1\u2212\u03b422 \u2016Fzk2\u20162, deriving that \u2016Fzk1 \u2016 \u2016Fzk2 \u2016\n\u2264 \u221a\n2x1\u22121 1\u2212\u03b422 . Therefore,\nx \u2265 1 1\u2212 \u03b421\n( x1 \u2212 \u03b41 \u221a 2x1 \u2212 1 1\u2212 \u03b422 ) (A.28)\nDefine function p : [ 11+\u03b42 , 1 1\u2212\u03b42 ]\u2192 R as p(x1) := 1\n1\u2212\u03b421\n( x1 \u2212 \u03b41 \u221a 2x1\u22121 1\u2212\u03b422 )\np\u2032(x1) = 1\n1\u2212 \u03b421\n( 1\u2212 \u03b41\u221a\n(1\u2212 \u03b422)(2x1 \u2212 1)\n) (A.29)\nwhich is monotonically increasing. Let p\u2032(x1) = 0, the solution is x1 = 1+\u03b421\u2212\u03b4 2 2\n2(1\u2212\u03b422) . Whether this\nextremum point fall into the domain hinge upon the relation between 1+\u03b4 2 1\u2212\u03b4 2 2\n2(1\u2212\u03b422) and 11+\u03b42 , since it is\neasy to examine that 1+\u03b4 2 1\u2212\u03b4 2 2\n2(1\u2212\u03b422) < 11\u2212\u03b42 .\nIf 1+\u03b4 2 1\u2212\u03b4 2 2\n2(1\u2212\u03b422) \u2264 11+\u03b42 , equivalently \u03b4 2 1 \u2264 (1\u2212 \u03b42)2, \u03b41 + \u03b42 \u2264 1\nx \u2265 p( 1 1 + \u03b42 ) = 1 (1 + \u03b41)(1 + \u03b42) (A.30)\nIf 1+\u03b4 2 1\u2212\u03b4 2 2\n2(1\u2212\u03b422) > 11+\u03b42 , equivalently \u03b4 2 1 > (1\u2212 \u03b42)2, \u03b41 + \u03b42 > 1\nx \u2265 p(1 + \u03b4 2 1 \u2212 \u03b422\n2(1\u2212 \u03b422) ) = 1\u2212 \u03b421 \u2212 \u03b422 2(1\u2212 \u03b421)(1\u2212 \u03b422)\n(A.31)\nestablishing the lemma.\nLemma A.3. Let F be L-Lipschitz and satisfy weak Minty condition with \u03c1. Assume that for all k, \u03b3k,i = \u03b4i/L, \u03b4i \u2208 (0, 1), i=1,. . . ,n and \u2211n i=1 \u03b4i \u2264 1. Initialize zk and generate the sequence (zki )i\u2208[n] by (n-step EG). The following inequality holds and is tight:\n\u3008Fzkn, Fzk\u3009 \u2016Fzkn\u20162 \u2265 n\u220f i=1 1 1 + \u03b4i (A.32)\nProof. Similar to Lemma A.2, what we want to prove is that the minimum value is attained when Fzk is scalar multiple of Fzkn. We make the following key proposition and prove it using mathematical induction:\n\u2016Fzk \u2212 1 (1\u2212 n\u2211 i=1 \u03b4i) n\u220f i=1 (1 + \u03b4i) Fzkn\u2016 \u2264\nn\u2211 i=1 \u03b4i\n(1\u2212 n\u2211 i=1 \u03b4i) n\u220f i=1 (1 + \u03b4i) \u2016Fzkn\u2016 (A.33)\nFor n = 1, it is mentioned in (A.19) that \u2016Fzk \u2212 1 1\u2212\u03b421 Fzk1\u2016 \u2264 \u03b411\u2212\u03b421 \u2016Fz k 1\u2016.\nAssume that the proposition holds for n = m\u2212 1. Apply the result on the m\u2212 1 extrapolations from zk1 to z k m,\n\u2016Fzk1 \u2212 1 (1\u2212 m\u2211 i=2 \u03b4i) m\u220f i=2 (1 + \u03b4i) Fzkm\u2016 \u2264\nm\u2211 i=2 \u03b4i\n(1\u2212 m\u2211 i=2 \u03b4i) m\u220f i=2 (1 + \u03b4i) \u2016Fzkm\u2016 (A.34)\nSquare both sides and rearrange the equation,\n\u3008Fzk1 , Fzkm\u3009 \u2265 (1\u2212\nm\u2211 i=2 \u03b4i) m\u220f i=2 (1 + \u03b4i)\n2 \u2016Fzk1\u20162 +\n1 + m\u2211 i=2 \u03b4i\n2 m\u220f i=2 (1 + \u03b4i) \u2016Fzkm\u20162 (A.35)\nTo prove the n = m occasion, we examine the correctness of following inequality.\n\u2016 1 1\u2212 \u03b421 Fzk1 \u2212 1 (1\u2212 m\u2211 i=1 \u03b4i) m\u220f i=1 (1 + \u03b4i) Fzkm\u2016 \u2264\nm\u2211 i=1 \u03b4i\n(1\u2212 m\u2211 i=1 \u03b4i) m\u220f i=1 (1 + \u03b4i) \u2016Fzkm\u2016 \u2212 \u03b41 1\u2212 \u03b421 \u2016Fzk1\u2016\n(A.36)\n\u2016 1 1\u2212 \u03b421 Fzk1 \u2212 1 (1\u2212 m\u2211 i=1 \u03b4i) m\u220f i=1 (1 + \u03b4i) Fzkm\u20162 \u2212\n m\u2211 i=1 \u03b4i\n(1\u2212 m\u2211 i=1 \u03b4i) m\u220f i=1 (1 + \u03b4i) \u2016Fzkm\u2016 \u2212 \u03b41 1\u2212 \u03b421 \u2016Fzk1\u2016\n 2\n= 1\n1\u2212 \u03b421 \u2016Fzk1\u20162 +\n1 + m\u2211 i=1 \u03b4i\n(1\u2212 m\u2211 i=1 \u03b4i) m\u220f i=1 (1 + \u03b4i)2 \u2016Fzkm\u20162 +\n2\u03b41 m\u2211 i=1 \u03b4i\n(1\u2212 \u03b421)(1\u2212 m\u2211 i=1 \u03b4i) m\u220f i=1 (1 + \u03b4i) \u2016Fzk1\u2016\u2016Fzkm\u2016\n\u2212 2 (1\u2212 \u03b421)(1\u2212 m\u2211 i=1 \u03b4i) m\u220f i=1 (1 + \u03b4i) \u3008Fzk1 , Fzkm\u3009\n\u2264 1 1\u2212 \u03b421\n\u2016Fzk1\u20162 + 1 +\nm\u2211 i=1 \u03b4i\n(1\u2212 m\u2211 i=1 \u03b4i) m\u220f i=1 (1 + \u03b4i)2 \u2016Fzkm\u20162 +\n2\u03b41 m\u2211 i=1 \u03b4i\n(1\u2212 \u03b421)(1\u2212 m\u2211 i=1 \u03b4i) m\u220f i=1 (1 + \u03b4i) \u2016Fzk1\u2016\u2016Fzkm\u2016\n\u2212 1\u2212\nm\u2211 i=2 \u03b4i\n(1\u2212 \u03b421)(1\u2212 m\u2211 i=1 \u03b4i)(1 + \u03b41) \u2016Fzk1\u20162 \u2212\n1 + m\u2211 i=2 \u03b4i\n(1\u2212 \u03b421)(1\u2212 m\u2211 i=1 \u03b4i) m\u220f i=1 (1 + \u03b4i) m\u220f i=2 (1 + \u03b4i) \u2016Fzkm\u20162\n=\u2212 \u03b41\nm\u2211 i=1 \u03b4i\n(1\u2212 \u03b421)(1\u2212 m\u2211 i=1 \u03b4i)(1 + \u03b41) \u2016Fzk1\u20162 \u2212\n\u03b41(1 + \u03b41) m\u2211 i=1 \u03b4i\n(1\u2212 \u03b421)(1\u2212 m\u2211 i=1 \u03b4i) m\u220f i=1 (1 + \u03b4i)2 \u2016Fzkm\u20162\n+\n2\u03b41 m\u2211 i=1 \u03b4i\n(1\u2212 \u03b421)(1\u2212 m\u2211 i=1 \u03b4i) m\u220f i=1 (1 + \u03b4i) \u2016Fzk1\u2016\u2016Fzkm\u2016\n=\u2212 \u03b41\nm\u2211 i=1 \u03b4i\n(1\u2212 \u03b421)(1\u2212 m\u2211 i=1 \u03b4i)(1 + \u03b41) \u2016Fzk1\u20162 + 1m\u220f i=2 (1 + \u03b4i)2 \u2016Fzkm\u20162 \u2212 2 m\u220f i=2 (1 + \u03b4i) \u2016Fzk1\u2016\u2016Fzkm\u2016 \n=\u2212 \u03b41\nm\u2211 i=1 \u03b4i\n(1\u2212 \u03b421)(1\u2212 m\u2211 i=1 \u03b4i)(1 + \u03b41) \u2016Fzk1\u2016 \u2212 1m\u220f i=2 (1 + \u03b4i) \u2016Fzkm\u2016  2 \u2264 0\n(A.37)\nIt is rather easy to examine the positiveness of the right-hand side m\u2211 i=1 \u03b4i\n(1\u2212 m\u2211 i=1 \u03b4i) m\u220f i=1 (1 + \u03b4i) \u2016Fzkm\u2016 \u2265\nm\u2211 i=1 \u03b4i\n(1\u2212 m\u2211 i=1 \u03b4i) m\u22121\u220f i=1 (1 + \u03b4i) \u2016Fzkm\u22121\u2016\n...\n\u2265\nm\u2211 i=1 \u03b4i\n(1\u2212 m\u2211 i=1 \u03b4i)(1 + \u03b41) \u2016Fzk1\u2016\n\u2265 \u03b41 (1\u2212 \u03b41)(1 + \u03b41) \u2016Fzk1\u2016 = \u03b41 1\u2212 \u03b421 \u2016Fzk1\u2016\n(A.38)\nThe above two formulae complete the proof of (A.36), and the n = m case follows from triangle inequality and \u2016Fzk \u2212 1\n1\u2212\u03b421 Fzk1\u2016 \u2264 \u03b411\u2212\u03b421 \u2016Fz k 1\u2016. The proposition (A.33) is then proved by\ninduction. Using Cauchy-Schwarz inequality we obtain\n\u3008Fzkn, Fzk \u2212 1 (1\u2212 n\u2211 i=1 \u03b4i) n\u220f i=1 (1 + \u03b4i) Fzkn\u3009 \u2265 \u2212\nn\u2211 i=1 \u03b4i\n(1\u2212 n\u2211 i=1 \u03b4i) n\u220f i=1 (1 + \u03b4i) \u2016Fzkn\u20162 (A.39)\n\u3008Fzkn, Fzk\u3009 \u2265 1 n\u220f i=1 (1 + \u03b4i) \u2016Fzkn\u20162 (A.40)\nestablishing the lemma."
        },
        {
            "heading": "B PROOFS",
            "text": "Proof of Lemma 2.3. Let w be any point on the hyperplane \u2202D(u). According to the definition,\n\u3008Fu,u\u2212w\u3009 = \u03c1\u2016Fu\u20162 (B.1)\nFu is perpendicular to the hyperplane, and the distance from v to \u2202D(u) is equal to the length of the orthogonal projection of v \u2212w on Fu.\nd(v, \u2202D(u)) = \u2016PFu(v \u2212w)\u2016 (B.2)\n= |\u3008Fu,v \u2212w\u3009| \u2016Fu\u2016\n(B.3)\n= |\u3008Fu,u\u2212w\u3009 \u2212 \u3008Fu,u\u2212 v\u3009|\n\u2016Fu\u2016 (B.4)\n= |\u03c1\u2016Fu\u20162 \u2212 \u3008Fu,u\u2212 v\u3009|\n\u2016Fu\u2016 (B.5)\nTo convert this into a signed distance, we remove the absolute value according to Definition 2. If u \u2208 S, d(v, \u2202D(u)) \u2264 0; If u \u2208 Sc, d(v, \u2202D(u)) > 0. Thus,\nd(v,D(u)) = \u03c1\u2016Fu\u2016 2 \u2212 \u3008Fu,u\u2212 v\u3009 \u2016Fu\u2016\n(B.6)\nProof of Theorem 3.1. Telescoping (A.1) from k = 0 to k = m,\n\u2016z0 \u2212 z\u2217\u20162 \u2212 \u2016zm+1 \u2212 z\u2217\u20162 \u2265 m\u2211 k=1 \u03bbk(2\u2212 \u03bbk)\u03b12k\u2016F z\u0304k\u20162 (B.7)\nLet \u03b5k := \u03bbk(2\u2212 \u03bbk)\u03b12k and \u03ba = lim infk\u2192\u221e \u03b5k\n\u2016z0 \u2212 z\u2217\u20162 \u2265 m\u2211 k=1 \u03b5k\u2016F z\u0304k\u20162 \u2265 \u03ba m\u2211 k=1 \u2016F z\u0304k\u20162 (B.8)\nTherefore,\nmin k=0,1,...,m \u2016F z\u0304k\u20162 \u2264 1 m+ 1 m\u2211 k=1 \u2016F z\u0304k\u20162 \u2264 1 \u03ba(m+ 1) \u2016z0 \u2212 z\u2217\u20162 (B.9)\nSince \u03ba = lim infk\u2192\u221e \u03b5k > 0, {\u2016F z\u0304k\u20162}k\u2208N converges to zero. Combined with Lipschitzness, {\u2016z\u0304k \u2212 z\u2217\u2016}k\u2208N converges to zero and {z\u0304k}k\u2208N converges to z\u2217.\nProof of Theorem 4.1. Expand the formula for calculating the stepsize \u03b1k,\n\u03b1k = \u03c3k \u2212 \u3008Fzk2 , zk2 \u2212 zk\u3009 \u2016Fzk2\u20162 = \u03c3k + \u03b3k,1\u3008Fzk2 , Fzk\u3009 \u2016Fzk2\u20162 + \u03b3k,2\u3008Fzk2 , Fzk1 \u3009 \u2016Fzk2\u20162\n(B.10)\nThe lower bounds of the last two terms are established in Lemma A.2 and (A.22),\n\u03b1k \u2265 \u03c3k + \u03b3k,1g(\u03b41, \u03b42) + \u03b3k,2\n1 + \u03b42\n= \u03c3k + 1\nL\n[ \u03b41g(\u03b41, \u03b42) +\n\u03b42 1 + \u03b42 ] (B.11) Hence, if (4.2) holds, \u03b1k > 0. It is also straightforward that d(zk,D(z\u0304k)) > 0 since \u03c1 \u2265 \u03c3k. With the assumptions met, we can refer to Theorem 3.1 for the convergence result.\nProof of Theorem 4.2. (i) When \u03b41 + \u03b42 \u2264 1,\n\u03c1 \u2265 \u03c3k > \u2212 1\nL\n[ 1\u2212 1\n(1 + \u03b41)(1 + \u03b42)\n] (B.12)\n\u2265 \u2212 1 L\n[ 1\u2212 ( 2 2 + \u03b41 + \u03b42 )2 ]\n(B.13)\n= \u2212 5 9L\n(B.14)\nThe equality in (B.13) holds when \u03b41 = \u03b42 = 1/2.\n(ii) Define function q : {(x, y) | x+ y > 1, x < 1, y < 1} \u2192 R as\nq(\u03b41, \u03b42) := \u03b41(1\u2212 \u03b421 \u2212 \u03b422)\n2(1\u2212 \u03b421)(1\u2212 \u03b422) + \u03b42 1 + \u03b42\n(B.15)\nTry to find its critical point,\n\u2202q\n\u2202\u03b41 = (1\u2212 \u03b421)2 \u2212 \u03b422(1 + \u03b421) 2(1\u2212 \u03b421)2(1\u2212 \u03b422) = 0 (B.16)\n\u2202q\n\u2202\u03b42 =\n1 (1 + \u03b42)2 \u2212 \u03b4\n3 1\u03b42\n(1\u2212 \u03b421)(1\u2212 \u03b422)2 = 0 (B.17)\nEquivalently,\n\u03b422(1 + \u03b4 2 1) = (1\u2212 \u03b421)2 (B.18)\n\u03b431\u03b42(1 + \u03b42) 2 = (1\u2212 \u03b421)(1\u2212 \u03b422)2 (B.19)\n(4.3) follows by rearranging the equations,\n\u03b461\u03b4 2 2(1 + \u03b42) 4 = (1\u2212 \u03b421)2(1\u2212 \u03b422)4 (B.20) = \u03b422(1 + \u03b4 2 1)(1\u2212 \u03b422)4 (B.21)\n\u03b461(1 + \u03b42) 4 = (1 + \u03b421)(1\u2212 \u03b42)4 (B.22)\nWe compute the solution in Mathematica, \u03b42 = ( z; 1\u2212 13z + 24z2 \u2212 20z3 + 16z4 )\u22121 2\n(B.23)\n\u03b41 =\n\u221a \u2212540\u03b432 + 432\u03b422 \u2212 351\u03b42 + 243\n189 (B.24)\nwhere \u03b42 = ( z; 1\u2212 13z + 24z2 \u2212 20z3 + 16z4 )\u22121 2\nis the adopted notation for the second root of the polynomial 1\u2212 13z + 24z2 \u2212 20z3 + 16z4 in Mathematica\u2019s ordering, which is the larger one of its 2 real roots.\nThe closed form solution of \u03b42 can be solved from the quadratic equation:\n\u03b42 = 5\n16 +\n1\n16\n\u221a \u221239 + 8\n3 2 3\n(576 + 7 \u221a 6771) 1 3 \u2212 8\n3(576 + 7 \u221a 6771) 1 3\n+ 1\n2\n[ \u221239\n32 \u2212 (576 + 7\n\u221a 6771) 1 3\n8 \u00b7 3 23 +\n1\n8(3(576 + 7 \u221a 6771)) 1 3\n+ 61\n32 \u221a \u221239 + 8\n3 2 3\n(576 + 7 \u221a 6771) 1 3 \u2212 8\n3(576+7 \u221a 6771) 1 3\n] 1 2\n(B.25)\nProof of Theorem 4.3. Notice that Lemma A.3 can be applied on parts of the steps,\n\u3008Fzkn, Fzkn\u22121\u3009 \u2016Fzkn\u20162 \u2265 1 1 + \u03b4n\n(B.26)\n\u3008Fzkn, Fzkn\u22122\u3009 \u2016Fzkn\u20162 \u2265 1 (1 + \u03b4n\u22121)(1 + \u03b4n)\n(B.27)\n... (B.28)\n\u3008Fzkn, Fzk\u3009 \u2016Fzkn\u20162 \u2265 1 (1 + \u03b41) . . . (1 + \u03b4i)\n(B.29)\nExpand \u03b1k,\n\u03b1k = \u03c3k \u2212 \u3008Fzkn, zkn \u2212 zk\u3009 \u2016Fzkn\u20162\n(B.30)\n= \u03c3k + \u03b31\u3008Fzkn, Fzk\u3009 \u2016Fzkn\u20162 + \u03b32\u3008Fzkn, Fzk1 \u3009 \u2016Fzkn\u20162 + \u00b7 \u00b7 \u00b7+ \u03b3n\u3008Fzkn, Fzkn\u22121\u3009 \u2016Fzkn\u20162\n(B.31)\n\u2265 \u03c3k + 1\nL\n[ \u03b41\n(1 + \u03b41) . . . (1 + \u03b4n) + \u00b7 \u00b7 \u00b7+ \u03b4n\u22121 (1 + \u03b4n\u22121)(1 + \u03b4n) + \u03b4n 1 + \u03b4n\n] (B.32)\n= \u03c3k + 1\nL\n[ 1\u2212 1\n(1 + \u03b41) . . . (1 + \u03b4n)\n] (B.33)\nThe inequality is tight since equality holds when Fzk = 11+\u03b41Fz k 1 = \u00b7 \u00b7 \u00b7 = 1(1+\u03b41)...(1+\u03b4n)Fz k n. Thus, the sufficient and necessary condition of \u03b1k > 0 is:\n\u03c3k > \u2212 1\nL\n[ 1\u2212 1\n(1 + \u03b41) . . . (1 + \u03b4n)\n] (B.34)\nSimilarly, the convergence result follows from Theorem 3.1.\nProof of Theorem 4.4. (i) According to AM-GM inequality,\n(1 + \u03b41) . . . (1 + \u03b4n) \u2264 (1 + 1\nn n\u2211 k=1 \u03b4k) n \u2264 (1 + 1 n )n (B.35)\nThe equality holds when \u03b41 = \u00b7 \u00b7 \u00b7 = \u03b4n = 1n . Therefore,\n\u03c1 > \u2212 1 L\n[ 1\u2212 1\n(1 + \u03b41) . . . (1 + \u03b4n)\n] \u2265 \u2212 1\nL\n[ 1\u2212 1\n(1 + 1n ) n\n] (B.36)\n\u03b4 \u2264 1n and \u03c3k \u2264 \u03c1 directly come from the theorem assumption.\n\u03b4 > b 1n\u221a1+\u03c1L \u2212 1c+ follows from \u03c1 \u2265 \u03c3k > \u2212 1 L [ 1\u2212 1(1+\u03b4)n ] .\nMoreover,\n\u03c3k > \u2212 1\nL\n[ 1\u2212 1\n(1 + \u03b4)n\n] \u2265 \u2212 1\nL\n[ 1\u2212 1\n(1 + 1n ) n\n] \u2265 \u2212n [ 1\u2212 1\n(1 + 1n ) n\n] \u03b3 (B.37)\nestablishing the parameter ranges in Theorem 4.4 (i).\n(ii) Given \u03c1 > \u2212 1L (1\u2212 1 e ), let n = d 1 2+2 log (1+\u03c1L)e\nn log (1 + 1\nn ) > n(\n1 n \u2212 1 2n2 ) = 1\u2212 1 2n (B.38)\n> 1\u2212 [1 + log (1 + \u03c1L)] = \u2212 log (1 + \u03c1L) (B.39)\n(1 + 1\nn )n >\n1\n1 + \u03c1L (B.40)\n\u03c1 > \u2212 1 L [1\u2212 1 (1 + 1n ) n ] (B.41)\nestablishing Theorem 4.4 (ii)."
        },
        {
            "heading": "C FIGURES AND INTUITIONS",
            "text": "C.1 ADDITIONAL FIGURES\nThe presented problem settings in Fig. 5 appear in various classic algorithms. \u03c1 = 0 recovers Minty variational inequality, also known as star-monotonicity. The MVI ensures that the negative gradient does not point outward from the solution z\u2217. (EG+) extends from (EG) and allows for a slight extent of non-monotinicity with \u03c1 > \u22121/8L. (AdaptiveEG+) further relaxes the problem parameter to \u03c1 > \u22121/2L. These efforts make it permissible for the direction of gradient descent to move away from the solution to a limited extent.\nOur methods introduced in the main paper provide new convergence guarantee for problems under \u03c1 > \u2212(1\u22121/e)/L. Furthermore, \u03c1 > \u22121/L marks the boundary of tractability. As discussed in Section 4, increasing the exploration stepsize could potentially solve more problems within this range.\nFig. 6 visually encapsulates the findings regarding 2-step EG in Section 4.1. The plot is a contour of the following function, in the box [0, 0.8]\u00d7 [0, 0.8].\nh(\u03b41, \u03b42) := { 1\u2212 1(1+\u03b41)(1+\u03b42) if \u03b41 + \u03b42 \u2264 1 \u03b41(1\u2212\u03b421\u2212\u03b4 2 2)\n2(1\u2212\u03b421)(1\u2212\u03b422) + \u03b421+\u03b42 if \u03b41 + \u03b42 > 1\n(C.1)\nThe function is continuous on the dashed line \u03b41 + \u03b42 = 1. However, its properties change noticeably across the dashed line. Notably, on the lower left side of the dashed line, the contour lines exhibit symmetry, which vanishes on the upper right side. Moreover, when either \u03b41 or \u03b42 approaches 1, the function value tends to plummet towards negative infinity.\nC.2 WEAK MVI HALFSPACE AND (MDEG)\nIn Fig. 1 we give intuitive explanations for projection algorithms involved in this paper. The blue regions represent the weak MVI halfspace generated from the latest iteration point. Fig. 1a explains why GDA fails at star-negative conomonotonic problems, as the hyperplane cannot separate zk and z\u2217. Fig. 1b shows the principle of (AdaptiveEG+). A larger extrapolation stepsize helps separating zk and z\u2217, and a smaller update stepsize complete the projection. Fig. 1c demonstrates how (MDEG) works. Instead of increasing stepsize, consecutive extrapolations exploit the structure efficiently.\nThe example used in the figures is Example 1 with a = 5, b = \u22121, and z0 = (0,\u22121), \u03b3 = \u22121/2L, \u03c3k = 1.2 \u00b7 \u03c1.\nWe mention another perspective that the max distance projection in Algorithm 1 can be considered as an approximation of the projection onto a convex hull. Consider \u03b4k = \u03c1 for convenience. Instead of computing dki = d(z k,D(zki )), construct a convex hull \u22c2 iD(zki ) and compute the distance\ndki = d(z k, \u22c2i j=1D(zkj )). On obtaining the maximum distance, similarly project onto this convex\nhull, \u03b1k = P\u22c2 iD(zki ))(z\nk). This approach, while computationally more expensive, utilizes all the information available throughout the entire exploration process.\nAs shown in Fig. 7, hyperplane projection performs a good approximation of the convex hull scheme. This idea could be helpful in exploring other subroutines. For example, it is feasible to choose zki using Monte Carlo method. When employing such desultory and inconsecutive approach, the convex\nhull scheme brings considerable improvement over hyperplane projection. See Fig. 8 for an example where the exploration points are randomly selected.\nC.3 RELATION TO PAST EXTRAGRADIENT\nPast Extragradient ((Popov, 1980)) is recently proved to converge for weak MVI problems with \u03c1 > \u22121/2L ((Choudhury et al., 2023; Gorbunov et al., 2023; Bo\u0308hm, 2022)). Past Extragradient has the following form:\nz\u0304k = zk \u2212 \u03b3kF z\u0304k\u22121\nzk+1 = zk \u2212 \u03c9kF z\u0304k (C.2)\nSince the three points zk\u22121, zk, z\u0304k are collinear, it can be viewed as PEG derives z\u0304k directly from zk\u22121 and retraces a step back to get zk. Despite the differences in extrapolation, both AdaptiveEG+ and PEG perform updates in the form of zk+1 = zk \u2212 \u03c9kF z\u0304k, suggesting a projection from zk onto a hyperplane perpendicular to F z\u0304k.\nMoreover, the underlying principles for convergence are similar:\nIn AdaptiveEG+, perform a extrapolation step from zk: z\u0304k = zk \u2212 \u03b3kFzk, and the weak MVI hyperplane \u2202D(z\u0304k) separates zk and z\u2217. In PEG, perform an extrapolation step from zk\u22121: z\u0304k = zk\u22121 \u2212 (\u03b3k + \u03c9k\u22121)F z\u0304k\u22121, and the weak MVI hyperplane \u2202D(z\u0304k) separates zk\u22121 and z\u2217.\nNote that in AdaptiveEG+ the extrapolation step follows the form of gradient descent, while it is not the case with PEG.\nC.4 n-STEP EG AND CARTESIAN OVALS\nWhen we consider n-step extrapolation, we often perceive the last gradient Fzkn as benchmark and measure anterior extrapolations with it. This perspective naturally results from the update rule of \u03b1k.\nWhen there is 1 extrapolation step, Fzk is distributed over a circle (sphere).\n\u2016Fzk \u2212 1 1\u2212 \u03b421 Fzk1\u2016 \u2264 \u03b41 1\u2212 \u03b421 \u2016Fzk1\u2016 (C.3)\nWhen there are 2 extrapolation steps, Fzk is distributed inside a Cartesian oval (surface).\n\u2016Fzk \u2212 1 1\u2212 \u03b421 Fzk1\u2016 \u2264 \u03b41 1\u2212 \u03b421 \u2016Fzk1\u2016 (C.4)\n\u2016Fzk1 \u2212 1\n1\u2212 \u03b422 Fzk2\u2016 \u2264 \u03b42 1\u2212 \u03b422 \u2016Fzk2\u2016 (C.5)\nFz k\n2 Fz k\n1 Fz k\n(a) \u03b41 = \u03b42 = 0.3 (b) \u03b41 = \u03b42 = 0.5 (c) \u03b41 = \u03b42 = 0.65\nFigure 10: Cartesian ovals. \u221a\n(x\u2212 1 1\u2212\u03b421 )2 + y2 = \u03b41 1\u2212\u03b421\n+ \u03b42 \u221a x2 + y2, where Fzk2 = (1, 0).\nWhen there are 3 extrapolation steps, Fzk is distributed inside the envelope of circles (spheres) whose center is inside a Cartesian oval (surface).\n\u2016Fzk \u2212 1 1\u2212 \u03b421 Fzk1\u2016 \u2264 \u03b41 1\u2212 \u03b421 \u2016Fzk1\u2016 (C.6)\n\u2016Fzk1 \u2212 1\n1\u2212 \u03b422 Fzk2\u2016 \u2264 \u03b42 1\u2212 \u03b422 \u2016Fzk2\u2016 (C.7)\n\u2016Fzk2 \u2212 1\n1\u2212 \u03b423 Fzk3\u2016 \u2264\n\u03b43\n1\u2212 \u03b423 \u2016Fzk3\u2016 (C.8)\nWe have conducted numerical calculations on 3-step and 4-step cases. The results are presented below along with n = 1, 2 cases.\nn \u03b41 \u03b42 \u03b43 \u03b44 \u2212\u03c10L 1 1 0.5 2 0.52212 0.644793 0.583456 3 0.272899 0.512753 0.515522 0.632242 4 0.22 0.31 0.39 0.44 0.657724\nThe n = 3 result is (nearly) tight, obtained from numerical optimization method in Mathematica. The n = 4 result is achieved by manual tests and serves as a lower bound. We anticipate that a larger n will continue to yield an improved range for \u03c1."
        },
        {
            "heading": "D ADDITIONAL STATEMENTS",
            "text": "D.1 EXPERIMENT PARAMETERS\nFor Example 1, two experiments are conducted. In the first experiment, a = 3 \u221a 2, b = \u2212 \u221a 7. \u03b3k,i = 12L , \u03c3k = \u2212 10 9 \u03b3i \u00b7 0.99 is used in 2-step EG.\nIn the second experiment, a = \u221a\n2, b = \u22121. \u03b3k,i = 14L , \u03c3k = \u2212 1476 625 \u03b3i \u00b7 0.99 is used in 4-step EG.\nFor Example 2 and Example 3, in both experiment \u03b3k,i = 1L and \u03c3k = \u2212 1 2L is used in (MDEG).\nRecommended tolerance for (MDEG) is \u03b51 = 10\u22123, \u03b52 = 10\u22123.\nD.2 PROPERTIES OF EXAMPLE 1\nLemma D.1. The saddle gradient operator F of f(x, y) := axy + b2 (x 2 \u2212 y2), where a > 0, b < 0,\nsatisfies Assumption 1 with L = \u221a a2 + b2 and Assumption 2 with \u03c3 = ba2+b2 . Proof. The operator Fz = [ \u2207xf(x, y) \u2212\u2207yf(x, y) ] = [ ay + bx by \u2212 ax ] = Az, where the matrix A := [ b a \u2212a b ] .\n\u2016Fu\u2212 Fv\u2016 = \u2016Au\u2212Av\u2016 \u2265 \u2016A\u20162\u2016u\u2212 v\u2016 (D.1)\nTherefore, the Lipschitz constant L = \u2016A\u20162 = \u221a a2 + b2.\nRecall the weak Minty condition, where f(x, y) has the only stationary point z\u2217 = (0, 0).\n\u3008Fz, z \u2212 z\u2217\u3009 \u2265 \u03c1\u2016Fz\u20162 (D.2) (Az)T (z \u2212 z\u2217) \u2265 \u03c1(Az)TAz (D.3)\nzTATz \u2265 \u03c1zTATAz (D.4) b(x2 + y2) \u2265 \u03c1(a2 + b2)(x2 + y2) (D.5)\nTherefore, the weak Minty parameter \u03c1 = ba2+b2 .\nWe provide an interesting result that (Algorithm 1) converges on Example 1 for \u03c1 > \u2212 1L , if the stepsizes are selected infinitely small. Theorem D.2. Consider Example 1, f(x, y) := axy + b2 (x\n2 \u2212 y2), where a > 0, b < 0. Apply (Algorithm 1) on Example 1 with infinitely small stepsize \u03b3k \u2192 0. Assume that \u03c3k = \u03c1, \u03bbk = 1, z0 = (x0, y0), then the algorithm converges to the the stationary point after one iteration, i.e. z1 = z\u2217 = (0, 0). Proof. Recall that Fz = [ \u2207xf(x, y) \u2212\u2207yf(x, y) ] = [ ay + bx by \u2212 ax ] = Az, where the matrix A := [ b a \u2212a b ] . Try to normalize A:\nA = 1\u221a\na2 + b2\n[ b\u221a\na2+b2 a\u221a a2+b2\n\u2212 a\u221a a2+b2 b\u221a a2+b2\n] (D.6)\n= 1\nL\n[ \u2212 cos\u03d5 sin\u03d5 \u2212 sin\u03d5 \u2212c cos\u03d5 ] (D.7)\nwhere \u03d5 := arctan\u2212ab . cos\u03d5 = \u2212 b\u221a a2+b2 , sin\u03d5 = a\u221a a2+b2 .\nWhen \u03b3k \u2192 0, the discrete process of gradient descent zki = zki\u22121 \u2212 \u03b3kFzki\u22121 evolves into a continuous gradient flow\nz\u0307 = dz\ndt = \u2212Fz (D.8)\nwhich corresponds to a linear system z\u0307 = \u2212Az. The solution to the ODEs is\nz(t) = e\u2212Atz(0) (D.9) = e\u2212bt [ cos at \u2212 sin at sin at cos at ] z(0) (D.10)\nwhere z(0) = z0 = (x0, y0) = r0(cos \u03b80, sin \u03b80).\nCalculate the gradient vector Fz(t),\nFz(t) = Ae\u2212Atz(0) (D.11) = e\u2212bt [ b cos at+ a sin at a cos at\u2212 b sin at \u2212a cos at+ b sin at b cos at+ a sin at ] z(0) (D.12)\n= Le\u2212bt [ \u2212 cos (at+ \u03d5) sin (at+ \u03d5) \u2212 sin (at+ \u03d5) \u2212 cos (at+ \u03d5) ] z(0) (D.13)\nSince the matrix is a rotation matrix,\n\u2016Fz(t)\u2016 = Le\u2212bt\u2016z(0)\u2016 = Lr0e\u2212bt (D.14)\nFurthermore,\n\u3008Fz(t), z(t)\u3009 = z(t)TFz(t) (D.15) =\u2212 Le\u22122btz(0)T [\ncos at sin at \u2212 sin at cos at ] [ cos (at+ \u03d5) \u2212 sin (at+ \u03d5) sin (at+ \u03d5) cos (at+ \u03d5) ] z(0) (D.16)\n=\u2212 Le\u22122btz(0)T [ cos\u03d5 \u2212 sin\u03d5 sin\u03d5 cos\u03d5 ] z(0) (D.17)\n=\u2212 Le\u22122bt(x20 + y20) cos\u03d5 (D.18) =\u2212 Lr20e\u22122bt cos\u03d5 (D.19)\nThe matrix multiplication is straightforward since they are rotation matrices.\n\u3008Fz(t), z(0)\u3009 = z(0)TFz(t) (D.20) =\u2212 Le\u2212btz(0)T [ cos (at+ \u03d5) \u2212 sin (at+ \u03d5) sin (at+ \u03d5) cos (at+ \u03d5) ] z(0) (D.21)\n=\u2212 Le\u2212bt(x20 + y20) cos (at+ \u03d5) (D.22) =\u2212 Lr20e\u2212bt cos (at+ \u03d5) (D.23)\nNote that \u03c1L = b\u221a a2+b2 = \u2212 cos\u03d5. Now we can calculate the projection distance,\nd(z(0),D(z(t))) (D.24)\n=\u03c1\u2016Fz(t)\u2016 \u2212 \u3008Fz(t), z(t)\u2212 z(0)\u3009 \u2016Fz(t)\u2016\n(D.25)\n=\u03c1Lr0e \u2212bt + r0e \u2212bt cos\u03d5\u2212 r0 cos (at+ \u03d5) (D.26) =\u2212 r0e\u2212bt cos\u03d5+ r0e\u2212bt cos\u03d5\u2212 r0 cos (at+ \u03d5) (D.27) =\u2212 r0 cos (at+ \u03d5) (D.28)\nLet us simplify the notation with d(t) := d(z(0),D(z(t))). The derivative d\u2032(t) = ar0 sin (at+ \u03d5). At the starting point, d(0) = \u2212r0 cos\u03d5 < 0, d\u2032(0) = ar0 sin\u03d5 > 0. The signed distance is initially negative and within a increasing interval.\nAlgorithm 1 adopts the projection when the distance function reaches its first local maximum value. Given the nature of the cosine function, it becomes apparent that the final distance will be d\u03040 = d(\u03c0\u2212\u03d5a ) = r0. Consequently,\nF z\u03040 = Fz( \u03c0 \u2212 \u03d5 a ) (D.29)\n= e\u2212 b(\u03c0\u2212\u03d5) a L [ 1 0 0 1 ] z(0) = e\u2212 b(\u03c0\u2212\u03d5) a Lz0 (D.30)\nThe stepsize \u03b10 = d\u0304 0 \u2016F z\u03040\u2016 = e b(\u03c0\u2212\u03d5) a L\u22121. Therefore,\nz1 = z0 \u2212 \u03b10F z\u03040 = z0 \u2212 z0 = 0 = z\u2217 (D.31)\nestablishing the theorem.\nD.3 PROPERTIES OF EXAMPLE 3\nIt is introduced by Pethick et al. (2022) that in the box \u2016z\u2016\u221e \u2264 3/2, the object function has Lipschitz constant L = 180 \u221a 1 2 (1089 \u221a 801761 + 993841) \u2248 12.4026. The critical point is z\u2217 = (0.0780267, 0.411934). According to our calculation, global weak MVI parameter records \u22121.52057 at the point z\u2032 = (\u22120.258079, 0.791652). Recall that \u2212 1L \u2248 \u22120.0806285. However, this astonishing value of \u03c1L \u2248 \u221218.8589 does not reflect much of its nature. Looking into the local value of \u03c1 and L in Fig. 12 we find that most areas remain tractable parameter of \u03c1 > \u2212 1L , while a minimum value of \u03c1L \u2248 \u22123.04076 is recorded at the point (1.03889, 1.35309). Furthermore in the outer areas colored in white, \u03c1L rises dramatically to very large positive value. It is plausible that these anomalies prevent convergence, and convergent algorithms actually circumvent such thorny area and make progress in tractable area.\nAs shown in Fig. 13, a wide range of choice of \u03c3k leads to convergence. From \u03c3k = 0 to \u03c3k = \u2212 10L , (MDEG) withstands the limit cycles. Convergence to limit cycle is observed under the parameter of \u03c3k = \u2212 15L . While the global L \u2248 12.4026 reflects a spike of local Lipschitz constant near the border, even considering Ll \u2248 1.97703 in a smaller box of \u2016z\u2016\u221e \u2264 1, the algorithm makes a good coverage of \u03c3k from \u2212 1Ll to 0."
        }
    ],
    "title": "WEAKER MVI CONDITION: EXTRAGRADIENT METH-",
    "year": 2024
}