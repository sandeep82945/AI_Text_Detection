{
    "abstractText": "We consider cross-silo federated linear contextual bandit (LCB) problem under differential privacy, where multiple silos interact with their respective local users and communicate via a central server to realize collaboration without sacrificing each user\u2019s privacy. We identify three issues in the state-of-the-art (Dubey & Pentland, 2020): (i) failure of claimed privacy protection, (ii) incorrect regret bound due to noise miscalculation and (iii) ungrounded communication cost. To resolve these issues, we take a two-step approach. First, we design an algorithmic framework consisting of a generic federated LCB algorithm and flexible privacy protocols. Then, leveraging the proposed framework, we study federated LCBs under two different privacy constraints. We first establish privacy and regret guarantees under silo-level local differential privacy, which fix the issues present in state-of-the-art algorithm. To further improve the regret performance, we next consider shuffle model of differential privacy, under which we show that our algorithm can achieve nearly \u201coptimal\u201d regret without a trusted server. We accomplish this via two different schemes \u2013 one relies on a new result on privacy amplification via shuffling for DP mechanisms and another one leverages the integration of a shuffle protocol for vector sum into the tree-based mechanism, both of which might be of independent interest. Finally, we support our theoretical results with numerical evaluations over contextual bandit instances generated from both synthetic and real-life data.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xingyu Zhou"
        },
        {
            "affiliations": [],
            "name": "Sayak Ray Chowdhury"
        }
    ],
    "id": "SP:67032229d3dbee887bf26f43823e08ac75b9cc70",
    "references": [
        {
            "authors": [
                "Yasin Abbasi-Yadkori",
                "D\u00e1vid P\u00e1l",
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Improved algorithms for linear stochastic bandits",
            "venue": "Advances in neural information processing systems,",
            "year": 2011
        },
        {
            "authors": [
                "Achraf Azize",
                "Debabrota Basu"
            ],
            "title": "When privacy meets partial information: A refined analysis of differentially private bandits",
            "venue": "arXiv preprint arXiv:2209.02570,",
            "year": 2022
        },
        {
            "authors": [
                "Borja Balle",
                "James Bell",
                "Adri\u00e0 Gasc\u00f3n",
                "Kobbi Nissim"
            ],
            "title": "The privacy blanket of the shuffle model",
            "venue": "In Annual International Cryptology Conference,",
            "year": 2019
        },
        {
            "authors": [
                "Mark Bun",
                "Thomas Steinke"
            ],
            "title": "Concentrated differential privacy: Simplifications, extensions, and lower bounds",
            "venue": "In Theory of Cryptography Conference,",
            "year": 2016
        },
        {
            "authors": [
                "T-H Hubert Chan",
                "Elaine Shi",
                "Dawn Song"
            ],
            "title": "Private and continual release of statistics",
            "venue": "ACM Transactions on Information and System Security (TISSEC),",
            "year": 2011
        },
        {
            "authors": [
                "Albert Cheu",
                "Adam Smith",
                "Jonathan Ullman",
                "David Zeber",
                "Maxim Zhilyaev"
            ],
            "title": "Distributed differential privacy via shuffling",
            "venue": "In Annual International Conference on the Theory and Applications of Cryptographic Techniques,",
            "year": 2019
        },
        {
            "authors": [
                "Albert Cheu",
                "Matthew Joseph",
                "Jieming Mao",
                "Binghui Peng"
            ],
            "title": "Shuffle private stochastic convex optimization",
            "venue": "arXiv preprint arXiv:2106.09805,",
            "year": 2021
        },
        {
            "authors": [
                "Sayak Ray Chowdhury",
                "Xingyu Zhou"
            ],
            "title": "Distributed differential privacy in multi-armed bandits",
            "venue": "arXiv preprint arXiv:2206.05772,",
            "year": 2022
        },
        {
            "authors": [
                "Sayak Ray Chowdhury",
                "Xingyu Zhou"
            ],
            "title": "Shuffle private linear contextual bandits",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Roel Dobbe",
                "Ye Pu",
                "Jingge Zhu",
                "Kannan Ramchandran",
                "Claire Tomlin"
            ],
            "title": "Customized local differential privacy for multi-agent distributed optimization",
            "venue": "arXiv preprint arXiv:1806.06035,",
            "year": 2018
        },
        {
            "authors": [
                "Abhimanyu Dubey"
            ],
            "title": "No-regret algorithms for private gaussian process bandit optimization",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Abhimanyu Dubey",
                "AlexSandy\u2019 Pentland"
            ],
            "title": "Differentially-private federated linear bandits",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "John C Duchi",
                "Michael I Jordan",
                "Martin J Wainwright"
            ],
            "title": "Local privacy and statistical minimax rates",
            "venue": "IEEE 54th Annual Symposium on Foundations of Computer Science,",
            "year": 2013
        },
        {
            "authors": [
                "Cynthia Dwork",
                "Aaron Roth"
            ],
            "title": "The algorithmic foundations of differential privacy",
            "venue": "Found. Trends Theor. Comput. Sci.,",
            "year": 2014
        },
        {
            "authors": [
                "Cynthia Dwork",
                "Moni Naor",
                "Toniann Pitassi",
                "Guy N Rothblum"
            ],
            "title": "Differential privacy under continual observation",
            "venue": "In Proceedings of the forty-second ACM symposium on Theory of computing,",
            "year": 2010
        },
        {
            "authors": [
                "\u00dalfar Erlingsson",
                "Vitaly Feldman",
                "Ilya Mironov",
                "Ananth Raghunathan",
                "Kunal Talwar",
                "Abhradeep Thakurta"
            ],
            "title": "Amplification by shuffling: From local to central differential privacy via anonymity",
            "venue": "In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms,",
            "year": 2019
        },
        {
            "authors": [
                "Vitaly Feldman",
                "Audra McMillan",
                "Kunal Talwar"
            ],
            "title": "Hiding among the clones: A simple and nearly optimal analysis of privacy amplification by shuffling",
            "venue": "IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS),",
            "year": 2021
        },
        {
            "authors": [
                "Evrard Garcelon",
                "Kamalika Chaudhuri",
                "Vianney Perchet",
                "Matteo Pirotta"
            ],
            "title": "Privacy amplification via shuffling for linear contextual bandits",
            "venue": "In International Conference on Algorithmic Learning Theory,",
            "year": 2022
        },
        {
            "authors": [
                "Antonious Girgis",
                "Deepesh Data",
                "Suhas Diggavi",
                "Peter Kairouz",
                "Ananda Theertha Suresh"
            ],
            "title": "Shuffled model of differential privacy in federated learning",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Osama A Hanna",
                "Antonious M Girgis",
                "Christina Fragouli",
                "Suhas Diggavi"
            ],
            "title": "Differentially private stochastic linear bandits:(almost) for free",
            "venue": "arXiv preprint arXiv:2207.03445,",
            "year": 2022
        },
        {
            "authors": [
                "Jiafan He",
                "Tianhao Wang",
                "Yifei Min",
                "Quanquan Gu"
            ],
            "title": "A simple and provably efficient algorithm for asynchronous federated contextual linear bandits",
            "venue": "arXiv preprint arXiv:2207.03106,",
            "year": 2022
        },
        {
            "authors": [
                "Jiahao He",
                "Jiheng Zhang",
                "Rachel Zhang"
            ],
            "title": "A reduction from linear contextual bandit lower bounds to estimation lower bounds",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Ruiquan Huang",
                "Weiqiang Wu",
                "Jing Yang",
                "Cong Shen"
            ],
            "title": "Federated linear contextual bandits",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ruiquan Huang",
                "Huanyu Zhang",
                "Luca Melis",
                "Milan Shen",
                "Meisam Hejazinia",
                "Jing Yang"
            ],
            "title": "Federated linear contextual bandits with user-level differential privacy",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Peter Kairouz",
                "H Brendan McMahan",
                "Brendan Avent",
                "Aur\u00e9lien Bellet",
                "Mehdi Bennis",
                "Arjun Nitin Bhagoji",
                "Kallista Bonawitz",
                "Zachary Charles",
                "Graham Cormode",
                "Rachel Cummings"
            ],
            "title": "Advances and open problems in federated learning",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Michael Kearns",
                "Mallesh Pai",
                "Aaron Roth",
                "Jonathan Ullman"
            ],
            "title": "Mechanism design in large games: Incentives and privacy",
            "venue": "In Proceedings of the 5th conference on Innovations in theoretical computer science,",
            "year": 2014
        },
        {
            "authors": [
                "Fengjiao Li",
                "Xingyu Zhou",
                "Bo Ji"
            ],
            "title": "Differentially private linear bandits with partial distributed feedback",
            "venue": "arXiv preprint arXiv:2207.05827,",
            "year": 2022
        },
        {
            "authors": [
                "Fengjiao Li",
                "Xingyu Zhou",
                "Bo Ji"
            ],
            "title": "private) kernelized bandits with distributed biased feedback",
            "venue": "Proceedings of the ACM on Measurement and Analysis of Computing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Ziyu Liu",
                "Shengyuan Hu",
                "Zhiwei Steven Wu",
                "Virginia Smith"
            ],
            "title": "On privacy and personalization in cross-silo federated learning",
            "venue": "arXiv preprint arXiv:2206.07902,",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Lowy",
                "Meisam Razaviyayn"
            ],
            "title": "Private federated learning without a trusted server: Optimal algorithms for convex losses",
            "venue": "arXiv preprint arXiv:2106.09779,",
            "year": 2021
        },
        {
            "authors": [
                "Andrew Lowy",
                "Ali Ghafelebashi",
                "Meisam Razaviyayn"
            ],
            "title": "Private non-convex federated learning without a trusted server",
            "venue": "arXiv preprint arXiv:2203.06735,",
            "year": 2022
        },
        {
            "authors": [
                "Nikita Mishra",
                "Abhradeep Thakurta"
            ],
            "title": "nearly) optimal differentially private stochastic multi-arm bandits",
            "venue": "In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Tao Qin",
                "Tie-Yan Liu"
            ],
            "title": "Introducing LETOR 4.0",
            "venue": "datasets. CoRR,",
            "year": 2013
        },
        {
            "authors": [
                "Wenbo Ren",
                "Xingyu Zhou",
                "Jia Liu",
                "Ness B Shroff"
            ],
            "title": "Multi-armed bandits with local differential privacy",
            "venue": "arXiv preprint arXiv:2007.03121,",
            "year": 2020
        },
        {
            "authors": [
                "Touqir Sajed",
                "Or Sheffet"
            ],
            "title": "An optimal private stochastic-mab algorithm based on optimal private stopping rule",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Roshan Shariff",
                "Or Sheffet"
            ],
            "title": "Differentially private contextual linear bandits",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Thomas Steinke"
            ],
            "title": "Composition of differential privacy & privacy amplification by subsampling",
            "venue": "arXiv preprint arXiv:2210.00597,",
            "year": 2022
        },
        {
            "authors": [
                "Jay Tenenbaum",
                "Haim Kaplan",
                "Yishay Mansour",
                "Uri Stemmer"
            ],
            "title": "Differentially private multi-armed bandits in the shuffle model",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jay Tenenbaum",
                "Haim Kaplan",
                "Yishay Mansour",
                "Uri Stemmer"
            ],
            "title": "Concurrent shuffle differential privacy under continual observation",
            "venue": "arXiv preprint arXiv:2301.12535,",
            "year": 2023
        },
        {
            "authors": [
                "Sharan Vaswani",
                "Abbas Mehrabian",
                "Audrey Durand",
                "Branislav Kveton"
            ],
            "title": "Old dog learns new tricks: Randomized ucb for bandit problems",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Roman Vershynin"
            ],
            "title": "High-dimensional probability: An introduction with applications in data science, volume 47",
            "year": 2018
        },
        {
            "authors": [
                "Yuanhao Wang",
                "Jiachen Hu",
                "Xiaoyu Chen",
                "Liwei Wang"
            ],
            "title": "Distributed bandit learning: How much communication is needed to achieve (near) optimal regret",
            "year": 2020
        },
        {
            "authors": [
                "Kai Zheng",
                "Tianle Cai",
                "Weiran Huang",
                "Zhenguo Li",
                "Liwei Wang"
            ],
            "title": "Locally differentially private (contextual) bandits learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Xingyu Zhou",
                "Jian Tan"
            ],
            "title": "Local differential privacy for bayesian optimization",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Huang"
            ],
            "title": "bandits while only relying on discrete privacy noise, hence avoiding the privacy leakage of continuous privacy noise on finite computers due to floating point arithmetic",
            "year": 2023
        },
        {
            "authors": [
                "Huang"
            ],
            "title": "2021) that some assumption on the context distribution is necessary for a sublinear regret under user-level DP. Second, due to this stochastic context and some coverage conditions on contexts, an exponentially growing batch schedule can be applied in their case. In contrast, under the adversary context case, it is unclear to us how to apply the same technique to derive a sublinear regret",
            "year": 2021
        },
        {
            "authors": [
                "Dubey",
                "Pentland"
            ],
            "title": "2020) does not satisfy silo-level LDP. To give a more concrete illustration of privacy leakage, we now specify the form of f 3, local data Xi and synchronized data Z in (1) according to Dubey & Pentland (2020)",
            "venue": "In particular,",
            "year": 2020
        },
        {
            "authors": [
                "Remark B"
            ],
            "title": "The above result has two implications: (i) the current proof strategy for Fed-DP guarantee in Dubey & Pentland (2020) does not hold since it essentially relies on the post-processing of DP through silo-level LDP; (ii) Fed-DP could fail to handle reasonable adversary model in crosssilo federated LCBs. That is, even if Algorithm 1 in Dubey ",
            "year": 2020
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "MORE ON COMMUNICATION COST ANALYSIS The current analysis",
            "year": 2020
        },
        {
            "authors": [
                "Abbasi-Yadkori"
            ],
            "title": "2011), that is, for two positive definite matrices A,B \u2208 Rd\u00d7d satisfying A \u2ab0 B, then for any x \u2208 R, \u2225x\u2225A \u2264 \u2225x\u2225B",
            "year": 2011
        },
        {
            "authors": [
                "Abbasi-Yadkori"
            ],
            "title": "per-step regret bound in Step 2 and the boundedness of reward; (b) follows from the fact that Vt,i \u2ab0 Gt,i + \u03bbminI under event E ; (c) holds by (5) when t is in good epochs; (d) is true since \u03b2T \u2265 1; (e) holds by the elliptical potential lemma (cf",
            "year": 2011
        },
        {
            "authors": [
                "Dubey",
                "Pentland"
            ],
            "title": "2020) is that its adaptive communication schedule leads to privacy leakage",
            "year": 2020
        },
        {
            "authors": [],
            "title": "This would eventually lead to a K factor blow up in regret due to privacy. Similarly, if we apply PVec to the data points in the prefix sum, then again a single data point can participate in at most K shuffled outputs",
            "year": 2014
        },
        {
            "authors": [
                "Erlingsson"
            ],
            "title": "We first formally introduce our new amplification lemma, which is the key to our analysis, as mentioned in the main paper. The motivation for our new amplification result is two-fold: (i) Existing results on privacy amplification via shuffling",
            "venue": "Cheu et al",
            "year": 2019
        },
        {
            "authors": [
                "Feldman"
            ],
            "title": "2021), we follow the nice idea of hiding among the clones",
            "venue": "Lowy & Razaviyayn",
            "year": 2022
        },
        {
            "authors": [],
            "title": "amplification result requires the local privacy budget to be close to one; the other one comes from the fact that now the local dataset could be n = T , which further reduces the range of valid \u03b5. In this section, we give the vector sum protocol",
            "year": 2021
        },
        {
            "authors": [
                "Cheu"
            ],
            "title": "2022b)). Then, by the binary tree structure, each single data point (bias vector or covariance matrix) only participates",
            "venue": "PVec",
            "year": 2022
        },
        {
            "authors": [
                "Feldman"
            ],
            "title": "Now, we provide proof of amplification Lemma F.2 for completeness",
            "venue": "Lowy & Razaviyayn",
            "year": 2021
        },
        {
            "authors": [
                "Hence",
                "Lowy",
                "Razaviyayn"
            ],
            "title": "2021, Lemma D.10) (with p := e\u2212\u03b5\u03030 = e\u2212n\u03b5",
            "year": 2021
        },
        {
            "authors": [
                "Feldman"
            ],
            "title": "Instead of using Lowy & Razaviyayn (2021, Lemma D.14), we can directly follow the proof of Lemma",
            "venue": "Lowy & Razaviyayn",
            "year": 2021
        },
        {
            "authors": [
                "Feldman"
            ],
            "title": "2022) by applying amplification via sub-sampling to the \u03b4 term as well. In particular, the key step is to rewrite",
            "year": 2022
        },
        {
            "authors": [
                "Feldman"
            ],
            "title": "by the convexity of the hockey-stick divergence and Lemma",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "We consider the classic cross-silo Federated Learning (FL) paradigm (Kairouz et al., 2021) applied to linear contextual bandits (LCB). In this setting, a set of M local silos or agents (e.g., hospitals) communicate with a central server to learn about the unknown bandit parameter (e.g., hidden vector representing values of the user for different medicines). In particular, at each round t \u2208 [T ], each local agent i \u2208 [M ] receives a new user (e.g., patient) with context information ct,i \u2208 Ci (e.g., age, gender, medical history), recommends an action at,i \u2208 Ki (e.g., a choice of medicine), and then it observes a real-valued reward yt,i (e.g., effectiveness of the prescribed medicine). In linear contextual bandits, the reward yt,i is a linear function of the unknown bandit parameter \u03b8\u2217 \u2208 Rd corrupted by i.i.d mean-zero observation noise \u03b7t,i, i.e., yt,i = \u27e8xt,i, \u03b8\u2217\u27e9 + \u03b7t,i, where xt,i = \u03d5i(ct,i, at,i) and \u03d5i : Ci\u00d7Ki \u2192 Rd is a known function that maps a context-action pair to a d-dimensional real-valued feature vector. The goal of federated LCB is to minimize the cumulative group pseudo-regret defined\nRM (T ) = \u2211M\ni=1 \u2211T t=1 [ max a\u2208Ki \u27e8\u03d5i(ct,i, a), \u03b8\u2217\u27e9 \u2212 \u27e8xt,i, \u03b8\u2217\u27e9 ] .\nTo achieve the goal, as in standard cross-silo FL, the agents are allowed to communicate with the central server following a star-shaped communication, i.e., each agent can communicate with the server by uploading and downloading data, but agents cannot communicate with each other directly. However, the communication process (i.e., both data and schedule) could also possibly incur privacy leakage for each user t at each silo i, e.g., the sensitive context information ct,i and reward yt,i.\nTo address this privacy risk, we resort to differential privacy (Dwork & Roth, 2014), a principled way to prove privacy guarantee against adversaries with arbitrary auxiliary information. In standard cross-device FL, the notion of privacy is often the client-level DP, which protects the identity of each participating client or device. However, it has limitations in cross-silo FL, where the protection\ntargets are users (e.g., patients) rather than participating silos or agents (e.g., hospitals). Also, in order to adopt client-level DP to cross-silo FL, one needs the server and other silos to be trustworthy, which is often not the case. Hence, recent studies (Lowy & Razaviyayn, 2021; Lowy et al., 2022; Liu et al., 2022; Dobbe et al., 2018) on cross-silo federated supervised learning have converged to a new privacy notion, which requires that for each silo, all of its communication during the entire process is private (\u201cindistinguishable\u201d) with respect to change of one local user of its own. This allows one to protect each user within each silo without trustworthy server and other silos. In this paper, we adapt it to the setting of cross-silo federated contextual bandits and call it silo-level LDP1.\nDubey & Pentland (2020) adopt a similar but somewhat weaker notion of privacy called Federated DP and takes the first step to tackle this important problem of private and federated linear contextual bandits (LCBs). In fact, the performance guarantees presented by the authors are currently the state-of-the-art for this problem. The proposed algorithm claims to protect the privacy of each user at each silo. Furthermore, given a privacy budget \u03b5 > 0, the claimed regret bound is O\u0303( \u221a MT/\u03b5) with only O(M log T ) communication rounds, which matches the regret of a super-single agent that plays for total MT rounds. Unfortunately, in spite of being the state-of-the-art, the aforementioned privacy, regret and communication cost guarantees have fundamental gaps, as discussed below.\nOur contributions: identify privacy, regret, communication gaps in state-of-the-art (Dubey & Pentland, 2020). In Section 3, we first show that the algorithm in (Dubey & Pentland, 2020) could leak privacy from the side channel of adaptive communication schedule, which depends on users\u2019 nonprivate local data. Next, we identify a mistake in total injected privacy noise in their regret analysis. Accounting for this miscalculation, the correct regret bound would amount to O\u0303(M3/4 \u221a T/\u03b5), which\nis M1/4 factor higher than the claimed one, and doesn\u2019t match regret performance of the super agent. Finally, we observe that due to the presence of privacy noise, its current analysis for O(M log T ) communications no longer holds. To resolve these issues, we take the following two-step approach:\n(i) design a generic algorithmic and analytical framework. In Section 4, we propose a generic federated LCB algorithm along with a flexible privacy protocol. Our algorithm adopts a fixed-batch schedule (rather than an adaptive one in Dubey & Pentland (2020)) that helps avoid privacy leakage from the side channel, as well as subtleties in communication analysis. Our privacy protocol builds on a distributed version of the celebrated tree-based algorithm (Chan et al., 2011; Dwork et al., 2010), enabling us to provide different privacy guarantees in a unified way. We further show that our algorithm enjoys a simple and generic analytical regret bound that only depends on the total amount of injected privacy noise under the required privacy constraints.\n(ii) prove regret guarantees under different privacy notions. We build upon the above framework to study federated LCBs under two different privacy constraints. In Section 5.1, we consider silo-level LDP (a stronger notion of privacy than Federated DP of Dubey & Pentland (2020)) and establish privacy guarantee with a correct regret bound O\u0303(M3/4 \u221a T/\u03b5) and communication cost O( \u221a MT ), hence fixing the gaps in Dubey & Pentland (2020). Next, to match the regret of a super single agent, we consider shuffle DP (SDP) (Cheu et al., 2019) in Section 5.2 and establish a regret bound of O\u0303( \u221a MT/\u03b5). We provide two different techniques to achieve this \u2013 one that relies on a new result on privacy amplification via shuffling for DP mechanisms and the other that integrates a shuffle protocol for vector sums (Cheu et al., 2021) into the tree-based mechanism. See Table 1 for a summary.\nRelated work. In standard multi-armed bandits, where rewards are only sensitive data, different DP models including central (Mishra & Thakurta, 2015; Azize & Basu, 2022; Sajed & Sheffet, 2019),\n1It appears under different names in prior work, e.g., silo-specific sample-level DP (Liu et al., 2022), inter-silo record-level DP (Lowy & Razaviyayn, 2021).\nlocal (Ren et al., 2020) and distributed (Chowdhury & Zhou, 2022a; Tenenbaum et al., 2021), have been studied. In linear contextual bandits, where both contexts and rewards are sensitive, there is a line of work under central (Shariff & Sheffet, 2018), local (Zheng et al., 2020) and shuffle (Chowdhury & Zhou, 2022b; Garcelon et al., 2022; Tenenbaum et al., 2023) models of DP. Li et al. (2022); Hanna et al. (2022) study linear bandits without contexts protection. Dubey & Pentland (2020) is the first to consider federated LCBs under item-level privacy while Huang et al. (2023) study user-level privacy under some distributional assumptions; see Appendix A. Federated or distributed LCBs without privacy have also been studied (Wang et al., 2020; He et al., 2022a; Huang et al., 2021), where a common goal is to achieve the regret of a super single agent that plays MT rounds while keeping communication cost minimal. Lowy & Razaviyayn (2021); Liu et al. (2022) study private cross-silo federated learning under supervised setting, whereas we focus on the sequential learning setting."
        },
        {
            "heading": "2 DIFFERENTIAL PRIVACY IN FEDERATED LCBS",
            "text": "We now formally introduce differential privacy in cross-silo federated contextual bandits. Let a dataset Di at each silo i be given by a sequence of T unique users U1,i, . . . , UT,i. Each user Ut,i is identified by her context information ct,i as well as reward responses she would give to all possible actions recommended to her. We say two datasets Di and D\u2032i at silo i are adjacent if they differ exactly in one participating user, i.e., U\u03c4,i \u0338= U \u2032\u03c4,i for some \u03c4 \u2208 [T ] and Us,i = U \u2032s,i for all s \u0338= \u03c4 . Silo-level local differential privacy (LDP). Consider a multi-round, cross-silo federated learning algorithm Q. At each round t, each silo i communicates a randomized message Zti of its data Di to the server, which may depend (due to collaboration) on previous randomized messages Z1j , . . . , Z t\u22121 j from all other silos j \u0338= i. We allow Zti to be empty if there is no communication at round t. Let Zi = (Z 1 i , . . . , Z T i ) denote the full transcript of silo i\u2019s communications with the server over T rounds andQi the induced local mechanism in this process. Note that Zi is a realization of random messages generated according to the local mechanismQi. We denote by Z\u2212i = (Z1, . . . , Zi\u22121, Zi+1, . . . , ZM ) the full transcripts of all but silo i. We assume that Zi is conditionally independent of Dj for all j \u0338= i given Di and Z\u2212i. With this notation, we have the following definition of silo-level LDP. Definition 2.1 (Silo-level LDP). A cross-silo federated learning algorithm Q with M silos is said to be (\u03b5i, \u03b4i)i\u2208M silo-level LDP if for each silo i\u2208 [M ], it holds that\nP [ Qi(Zi\u2208Ei|Di,Z\u2212i) ] \u2264e\u03b5iP [ Qi(Zi\u2208Ei|D\u2032i,Z\u2212i) ] +\u03b4i ,\nfor all adjacent datasets Di and D\u2032i, and for all events Ei in the range of Qi. If \u03b5i = \u03b5 and \u03b4i = \u03b4 for all i \u2208 [M ], we simply say Q is (\u03b5, \u03b4)-silo-level LDP.\nRoughly speaking, a silo-level LDP algorithm protects the privacy of each individual user (e.g., patient) within each silo in the sense that an adversary (which could either be the central server or other silos) cannot infer too much about any individual\u2019s sensitive information (e.g., context and reward) or determine whether an individual participated in the learning process.2\nRemark 2.2 (Federated DP vs. Silo-level LDP). Dubey & Pentland (2020) consider a privacy notion called Federated DP (Fed-DP in short). As summarized in Dubey & Pentland (2020), Fed-DP requires \u201cthe action chosen by any agent must be sufficiently impervious (in probability) to any single pair (x, y) from any other agent\u201d. Both silo-level LDP and Fed-DP are item-level DP as the neighboring relationship is defined by differing in one participating user. The key here is to note that silo-level DP implies Fed-DP by the post-processing property of DP, and thus it is a stronger notion of privacy. In fact, Dubey & Pentland (2020) claim to achieve Fed-DP by relying on privatizing the communicated data from each silo. However, as we shall see in Section 3, its proposed algorithm fails to privatize the adaptive synchronization schedule, which is the key reason behind privacy leakage in their algorithm.\nShuffle differential privacy (SDP). Another common DP notion for FL is SDP (Cheu et al., 2019), which has been widely studied in supervised learning (Lowy & Razaviyayn, 2021; Girgis et al., 2021; Lowy et al., 2022) to match the centralized utility performance. Motivated by this, we adapt it to FL-LCBs. Specifically, each silo i \u2208 [M ] first applies a local randomizerR to its raw local data and sends the randomized output to a shuffler S. The shuffler S permutes all the messages from all M silos uniformly at random and sends those to the central server. Roughly speaking, SDP requires\n2This is a notion of item-level DP. A comparison with standard local DP, central (joint)-DP and shuffle DP for single-agent LCBs is presented in Appendix G.2.\nall the messages sent by the shuffler to be private (\u201cindistinguishable\u201d) with respect to a single user change among all MT users. This item-level DP is defined formally as follows. Definition 2.3 (SDP). Consider a cross-silo federated learning algorithm Q that induces a (randomized) mechanismM whose output is the collection of all messages sent by the shuffler during the entire learning process. Then, the algorithm Q is said to be (\u03b5, \u03b4)-SDP if\nP [ M(D) \u2208 E ] \u2264 e\u03b5 P [ M(D\u2032) \u2208 E ] + \u03b4 ,\nfor all E in the range ofM and for all adjacent datasets D = (D1, . . . , DM ) and D\u2032 = (D\u20321, . . . , D\u2032M ) such that \u2211M i=1 \u2211T t=1 1{Ut,i \u0338=U \u2032t,i} = 1."
        },
        {
            "heading": "3 PRIVACY, REGRET AND COMMUNICATION GAPS IN STATE-OF-THE-ART",
            "text": "Gap in privacy analysis. We take a two-step approach to demonstrate the privacy issue in Dubey & Pentland (2020). To start with, we argue that Algorithm 1 in Dubey & Pentland (2020) fails to achieve silo-level LDP due to privacy leakage through the side channel of communication schedule (i.e., when agents communicate with the server). The key issue is that the adaptive communication schedule in the algorithm depends on users\u2019 non-private data. This fact can be utilized by an adversary or malicious silo j to infer another silo i\u2019s users\u2019 sensitive information, which violates the requirement of silo-level LDP. In that algorithm, all silos synchronously communicate with the server if\n\u2203 some silo i \u2208 [M ] : f(Xi, Z) > 0 , (1) where f is some function, Xi is non-private local data of silo i since the last synchronization and Z is all previously synchronized data. Crucially, the form of f and the rule (1) are public information, known to all silos even before the algorithm starts. This local and non-private data-dependent communication rule in (1) causes privacy leakage, as illustrated below with a toy example. Example 3.1 (Privacy leakage). Consider two silos i and j following Algorithm 1 in Dubey & Pentland (2020). After the first round, Xi is the data of the first user in silo i (say Alice), Xj is the data of the first user in silo j (say Bob) and Z is zero. Let communication is triggered at the end of first round and assume f(Xj , 0) \u2264 0. Since the rule (1) is public, silo j can infer that f(Xi, 0) > 0, i.e. the communication is triggered by silo i. Since f is also public knowledge, silo j can utilize this to infer some property of Xi. Hence, by observing only the communication signal (even without looking at the data), silo j can infer sensitive data of Alice. In fact, the specific form of f in Dubey & Pentland (2020) allows silo j to infer context information of Alice (details in Appendix B).\nThis example shows that Algorithm 1 in Dubey & Pentland (2020) does not satisfy silo-level LDP, implying their proof for Fed-DP guarantee via post-processing of silo-level LDP does not hold. However, it does not imply that this algorithm fails to satisfy Fed-DP, which is a weaker notion than silo-level LDP. Nevertheless, by leveraging Example 3.1, one can show that this algorithm indeed fails to guarantee Fed-DP. To see this, recall the definition of Fed-DP from Remark 2.2. In the context of Example 3.1, it translates to silo j selecting similar actions for its users when a single user in silo i changes. Specifically, if the first user in silo i changes from Alice to say, Tracy, Fed-DP mandates that all T actions suggested by silo j to its local T users remain \u201cindistinguishable\u201d. This, in turn, implies that the communicated data from silo i must remain \u201cindistinguishable\u201d at silo j for each t\u2208 [T ]. This is because the actions at silo j are chosen deterministically based on its local data as well as on the communicated data from silo i, and the local data at silo j remains unchanged. However, in Algorithm 1 of Dubey & Pentland (2020), the communicated data from silo i is not guaranteed to remain \u201cindistinguishable\u201d as synchronization depends on non-private local data (Xi in (1)). In other words, without additional privacy noise added to Xi in (1), the change from Alice to Tracy could affect the existence of synchronization at round t \u2265 1. Consequently, under these two neighboring situations (e.g. Alice vs. Tracy), the communicated data from silo i could differ significantly at round t+ 1. As a result, the action chosen at round t+ 1 in silo j can be totally different violating Fed-DP. This holds true even if silo i injects noise while communicating its data (as done in Dubey & Pentland (2020)) due to a large change of non-private communicated data (see Appendix B for details).\nGaps in regret and communication analysis. We now turn to regret and communication analysis of Dubey & Pentland (2020), which has fundamental gaps that lead to incorrect conclusions in the end. First, the reported privacy cost in regret bound is O\u0303( \u221a MT/\u03b5) (ignoring dependence on dimension d), which leads to the conclusion that federated LCBs across M silos under silo-level LDP can achieve the same order of regret as in the centralized setting (i.e., when a super single agent\nAlgorithm 1 Private-FedLinUCB 1: Parameters: Batch size B \u2208 N, regularization \u03bb > 0, confidence radii {\u03b2t,i}t\u2208[T ],i\u2208[M ], feature\nmap \u03d5i : Ci \u00d7Ki \u2192 Rd, privacy protocol P = (R,S,A) 2: Initialize: Wi = 0, Ui = 0 for all agents i \u2208 [M ], W\u0303syn = 0, U\u0303syn = 0 3: for t=1, . . . , T do 4: for each agent i = 1, . . . ,M do 5: Receive context ct,i; compute Vt,i = \u03bbI + W\u0303syn +Wi and \u03b8\u0302t,i = V \u22121t,i (U\u0303syn + Ui) 6: Play action at,i=argmaxa\u2208Ki\u27e8\u03d5i(ct,i,a), \u03b8\u0302t,i\u27e9+\u03b2t,i\u2225\u03d5i(ct,i,a)\u2225V \u22121t,i ; observe reward yt,i 7: Set xt,i=\u03d5i(ct,i, at,i), Ui = Ui + xt,iyt,i and Wi = Wi + xt,ix\u22a4t,i 8: end for 9: if tmod B = 0 then\n10: // Local randomizer R at all agents i \u2208 [M ] 11: Send randomized messages Rbiast,i = Rbias(Ui) and Rcovt,i = Rcov(Wi) to S 12: // Third party S 13: Shuffle (or, not) all messages Sbiast = S({Rbiast,i }i\u2208[M ]) and Scovt = S({Rcovt,i }i\u2208[M ]) 14: // Analyzer A at the server 15: Compute private synchronized statistics U\u0303syn = Abias(Sbiast ) and W\u0303syn = Acov(Scovt ) 16: // All agents i \u2208 [M ] 17: Receive W\u0303syn and U\u0303syn from the server and reset Wi = 0, Ui = 0 18: end if 19: end for\nplays MT rounds). However, in the proposed analysis, the total amount of injected privacy noise is miscalculated. In particular, variance of total noise needs to be M\u03c32 rather than the proposed value of \u03c32. This is due to the fact that each silo injects Gaussian noise with variance \u03c32 when sending out local data which amounts to total M\u03c32 noise at the server. Accounting for this correction, the cost of privacy becomes O\u0303(M3/4 \u221a T/\u03b5), which is O(M1/4) factor worse than the claimed one. Hence, we conclude that Algorithm 1 in Dubey & Pentland (2020) cannot achieve the same order of regret as in centralized setting. Second, the proposed analysis to show O(log T ) communication rounds for the data-adaptive schedule (1) under privacy constraint essentially follows from the non-private one of Wang et al. (2020). Unfortunately, due to privacy noise, this direct approach no longer holds, and hence the reported logarithmic cost stands ungrounded (details in Appendix B)."
        },
        {
            "heading": "4 OUR APPROACH",
            "text": "To address the issues in Dubey & Pentland (2020), we introduce a generic algorithm for private, federated linear contextual bandits (Algorithm 1) and a flexible privacy protocol (Algorithm 2). This helps us (a) derive correct privacy, regret, and communication results under silo-level LDP (and hence under Fed-DP) (Section 5.1), and (b) achieve the same order of regret as in centralized setting under SDP (Section 5.2). Throughout the paper, we make the following standard assumptions in LCBs. Assumption 4.1 (Boundedness (Shariff & Sheffet, 2018)). The rewards are bounded, i.e., yt,i \u2208 [0, 1] for all t \u2208 [T ] and i \u2208 [M ]. Moreover, \u2225\u03b8\u2217\u22252 \u2264 1 and supc,a \u2225\u03d5i(c, a)\u22252 \u2264 1 for all i \u2208 [M ]."
        },
        {
            "heading": "4.1 ALGORITHM: PRIVATE FEDERATED LINUCB",
            "text": "We build upon the celebrated LinUCB algorithm (Abbasi-Yadkori et al., 2011) by adopting a fixedbatch schedule for synchronization among agents and designing a privacy protocol P (Algorithm 2) for both silo-level LDP and SDP . At each round t, each agent i recommends an action at,i to each local user following optimism in the face of uncertainty principle. First, the agent computes a local estimate \u03b8\u0302t,i based on all available data to her, which includes previously synchronized data from all agents as well as her own new local data (line 5 of Algorithm 1). Then, the action at,i is selected based on the LinUCB decision rule (line 6), where a proper radius \u03b2t,i is chosen to balance between exploration and exploitation. After observing the reward yt,i, each agent accumulates her own local data (bias vector xt,iyt,i and covariance matrix xt,ix\u22a4t,i) and stores them in Ui and Wi, respectively\nAlgorithm 2 P , a privacy protocol used in Algorithm 1 1: Procedure: Local RandomizerR at each agent 2: //Input: stream data (\u03b31, . . . , \u03b3K), \u03b5>0, \u03b4\u2208(0, 1] 3: for k=1, . . . ,K do 4: Express k in binary form: k = \u2211 j Binj(k) \u00b7 2j\n5: Find index of first one ik=min{j : Binj(k)=1} 6: Compute p-sum \u03b1ik = \u2211 j<ik\n\u03b1j+\u03b3k 7: Output \u03b1\u0302k=\u03b1ik+N (0,\u03c320I) 8: end for 9: Procedure: Analyzer A at server\n10: //Input : data from S : (\u03b1\u0302k,1, . . . , \u03b1\u0302k,M ), k\u2208 [K] 11: for k=1, . . . ,K do 12: Express k in binary and find index of first one ik 13: Add noisy p-sums of all agents: \u03b1\u0303ik = \u2211M i=1 \u03b1\u0302k,i\n14: Output: s\u0303k = \u2211\nj:Binj(k)=1 \u03b1\u0303j 15: end for\n(line 7). A communication is triggered between agents and central server whenever a batch ends \u2013 we assume w.l.o.g. total rounds T is divisible by batch size B (line 9). During this process, a protocolP = (R,S,A) assists in aggregating local data among all agents while guaranteeing privacy properties (to be discussed in detail soon). After communication, each agent receives latest synchronized data W\u0303syn, U\u0303syn from the server (line 17). Here, for any t=kB, k \u2208 [T/B], W\u0303syn represents noisy version of all covariance matrices up to round t from all agents (i.e., \u2211M i=1 \u2211t s=1 xs,ix \u22a4 s,i) and similarly, U\u0303syn\nrepresents noisy version of all bias vectors \u2211M\ni=1 \u2211t s=1 xs,iys,i. Finally, each agent resets Wi and Ui\nso that they can be used to accumulate new local data for the next batch. Note that Algorithm 1 uses a fixed-batch (data-independent) communication schedule rather than the adaptive, data-dependent one in Dubey & Pentland (2020), which allows us to resolve privacy and communication issues."
        },
        {
            "heading": "4.2 PRIVACY PROTOCOL",
            "text": "We now turn to our privacy protocol P (Algorithm 2), which helps to aggregate data among all agents under privacy constraints. The key component of P is a distributed version of the classic tree-based algorithm, which was originally designed for continual release of private sum statistics (Chan et al., 2011; Dwork et al., 2010). That is, given a stream of (multivariate) data \u03b3=(\u03b31, . . . , \u03b3K), one aims to release sk= \u2211k l=1 \u03b3l privately for all k\u2208 [K]. The tree-based mechanism constructs a complete binary tree T in online manner. The leaf nodes contain data \u03b31 to \u03b3K , and internal nodes contain the sum of all leaf nodes in its sub-tree, see Fig. 1 for an illustration. For any new arrival data \u03b3k, it only releases a tree node privately, which corresponds to a noisy partial sum (p-sum) between two time indices. As an example, take k = 6, and hence the new arrival is \u03b36. The tree-based mechanism first computes the p-sum \u2211 [5, 6] = \u03b35 + \u03b36 (line 6 in Algorithm 2). Then, it adds a\nGaussian noise with appropriate variance \u03c320 to \u2211\n[5, 6] and releases the noisy p-sum (line 7). Finally, to compute the prefix sum statistic \u2211 [1, 6] privately, it simply adds noisy p-sums for \u2211 [1, 4] and\u2211\n[5, 6], respectively. Reasons behind releasing and aggregating p-sums are that (i) each data point \u03b3k only affects at most 1 + logK p-sums (useful for privacy) and (ii) each sum statistic \u2211 [1, k] only involves at most 1 + log k p-sums (useful for utility).\nOur privacy protocol P = (R,S,A) breaks down the above classic mechanism of releasing and aggregating p-sums into a local randomizerR at each agent and an analyzerA at the server, separately, while allowing for a possible shuffler in between to amplify privacy. For each k, the local randomizer R at each agent computes and releases the noisy p-sum to a third-party S (lines 4-7). S can either be a shuffler that permutes the data uniformly at random (for SDP) or can simply be an identity mapping (for silo-level LDP). It receives a total of M noisy p-sums, one from each agent, and sends them to the central server. The analyzer A at the server first adds these M new noisy p-sums to synchronize them (line 13). It then privately releases the synchronized prefix sum by adding up all relevant synchronized p-sums as discussed in above paragraph (line 14). Finally, we employ P to Algorithm 1 by observing that local data \u03b3k,i for batch k and agent i consists of bias vectors \u03b3biask,i = \u2211kB t=(k\u22121)B+1 xt,iyt,i and covariance matrices \u03b3 cov k,i = \u2211kB t=(k\u22121)B+1 xt,ix \u22a4 t,i, which are stored\nin Ui and Wi respectively. We denote the randomizer and analyzer for bias vectors asRbias and Abias, and for covariance matrices asRcov and Acov in Algorithm 1."
        },
        {
            "heading": "5 THEORETICAL RESULTS",
            "text": ""
        },
        {
            "heading": "5.1 FEDERATED LCBS UNDER SILO-LEVEL LDP",
            "text": "We first present the performance of Algorithm 1 under silo-level LDP, hence fixing the privacy, regret and communication issues of the state-of-the-art algorithm in Dubey & Pentland (2020). The key idea is to inject Gaussian noise with proper variance (\u03c320 in Algorithm 2) when releasing a p-sum such that all the released p-sums up to any batch k\u2208 [K] is (\u03b5, \u03b4)-DP for any agent i\u2208 [M ]. Then, by Definition 2.1, it achieves silo-level LDP. Note that in this case, there is no shuffler, which is equivalent to the fact that the third party S in P is simply an identity mapping, denoted by I. The following result states this formally, with proof deferred to Appendix E. Theorem 5.1 (Performance under silo-level LDP). Fix batch size B, privacy budgets \u03b5>0, \u03b4\u2208(0, 1). Let P=(R, I,A) be a protocol given by Algorithm 2 with parameters \u03c320=8\u03ba \u00b7 (log(2/\u03b4)+\u03b5) \u03b52 , where \u03ba= 1+log(T/B). Then, under Assumption 4.1, Algorithm 1 instantiated with P satisfies (\u03b5, \u03b4)silo-level LDP. Moreover, for any \u03b1 \u2208 (0, 1], there exist choices of \u03bb and {\u03b2t,i}t,i such that, with probability at least 1\u2212 \u03b1, it enjoys a group regret\nRM (T )=O ( dMB log T+d \u221a MT log(MT/\u03b1) ) +O\u0303 (\u221a T (Md)3/4 log1/4(1/\u03b4)\u221a\n\u03b5 log1/4\n( T/B\u03b1 )) .\nThe first term in the above regret bound doesn\u2019t depend on privacy budgets \u03b5, \u03b4, and serves as a representative regret bound for federated LCBs without privacy constraint. The second term is the dominant one which depends on \u03b5, \u03b4 and denotes the cost of privacy due to injected noise.\nCorollary 5.2. Setting B = \u221a T/M , Algorithm 1 achieves O\u0303 ( d \u221a MT + \u221a T (Md)\n3/4 log1/4(1/\u03b4)\u221a \u03b5 ) group regret, with total \u221a MT synchronizations under (\u03b5, \u03b4)-silo-level LDP.\nComparison with Dubey & Pentland (2020). First, we avoid the privacy leakage by adopting data-independent synchronization. However, this leads to an O( \u221a T ) communication cost. It remains open to design a (correct) data-adaptive schedule with logarithmic cost; details in Appendix D. We also show that privacy cost scales as O(M3/4) with number of agents M , correcting the reported\u221a M scaling. Next, we compare our result with that of a super single agent running for MT rounds under the central model of DP (i.e., where the central server is trusted), which serves as a benchmark for our results. As shown in Shariff & Sheffet (2018), the total regret for such a single agent is O\u0303 ( d \u221a MT + \u221a MT d\n3/4 log1/4(1/\u03b4)\u221a \u03b5\n) . Comparing this with Corollary 5.2, we observe that the privacy\ncost of federated LCBs under silo-level LDP is a multiplicative M1/4 factor higher than a super agent under the central model. This motivates us to consider SDP in next section."
        },
        {
            "heading": "5.2 FEDERATED LCBS UNDER SDP",
            "text": "We now aim to close the above M1/4 gap in the privacy cost under silo-level LDP compared to that achieved by a super single agent (with a truseted central server). To do so, we consider federated LCBs under SDP, which still enjoys the nice feature of silo-level LDP that the central server is not trusted. Thanks to our flexible protocol P , the only change needed compared to silo-level LDP is the introduction of a shuffler S to amplify privacy and adjustment of the privacy noise \u03c320 accordingly. Theorem 5.3 (Performance under SDP via amplification). Fix batch size B and let \u03ba=1+log(T/B). Let P = (R,S,A) be a protocol given by Algorithm 2. Then, under Assumption 4.1, there exist constants C1, C2 > 0 such that for any \u03b5\u2264 \u221a \u03ba\nC1T \u221a M , \u03b4\u2264 \u03baC2T , Algorithm 1 instantiated with P and \u03c320 =O ( 2\u03ba log(1/\u03b4) log(\u03ba/(\u03b4T )) log(M\u03ba/\u03b4) \u03b52M ) , satisfies (\u03b5, \u03b4)-SDP. Moreover, for any \u03b1 \u2208 (0, 1], there exist choices of \u03bb and {\u03b2t,i}t,i such that, with a probability at least 1\u2212 \u03b1, it enjoys a group regret\nRM (T )=O ( dMB log T+d \u221a MT log(MT/\u03b1) ) +O\u0303 ( d3/4 \u221a MT\nlog3/4(M\u03ba/\u03b4)\u221a \u03b5\nlog1/4 ( T/B\u03b1 )) .\nCorollary 5.4. Setting B = \u221a T/M , Algorithm 1 achieves O\u0303 ( d \u221a MT +d3/4 \u221a MT log\n3/4(M\u03ba/\u03b4)\u221a \u03b5 ) group regret, with total \u221a MT synchronizations under (\u03b5, \u03b4)-SDP.\nCorollary 5.4 asserts that privacy cost of federated LCBs under SDP matches that of a super single agent under central DP (up to a log factor in T,M, \u03b4).\nComparison with existing SDP analysis. Note that the above result doesn\u2019t directly follow from prior amplification results (Feldman et al., 2022; Erlingsson et al., 2019; Cheu et al., 2019; Balle et al., 2019), which show that shuffling outputs of M (\u03b5, \u03b4)-LDP algorithms achieve roughly 1/ \u221a M factor amplification in privacy for small \u03b5 \u2013 the key to close the aforementioned gap in privacy cost. However, these amplification results apply only when each mechanism is LDP in the standard sense, i.e., they operate on a dataset of size n = 1. This doesn\u2019t hold in our case since the dataset at each silo is a stream of T points. Lowy & Razaviyayn (2021) adopt group privacy to handle the case of n > 1, which can amplify any general DP mechanism but comes at the expense of a large increase in \u03b4. To avoid this, we prove a new amplification lemma specific to Gaussian DP mechanisms operating on datasets with size n>1. This helps us achieve the required 1/ \u221a M amplification in \u03b5 while keeping the increase in \u03b4 in check. The key idea behind our new lemma is to directly analyze the sensitivity when creating \u201cclones\u201d as in Feldman et al. (2022), but now by accounting for the fact that all n>1 points can be different (see Appendix F for formal statement and proof of the lemma)."
        },
        {
            "heading": "5.2.1 SDP GUARANTEE FOR A WIDE RANGE OF PRIVACY PARAMETERS",
            "text": "One limitation of attaining SDP via amplification is that the privacy guarantee holds only for small values of \u03b5, \u03b4 (see Theorem 5.3). In this subection, we propose an alternative privacy protocol to resolute this limitation. This new protocol leverages the same binary tree structure as in Algorithm 2 for releasing and aggregating p-sums, but it employs different local randomizers and analyzers for computing (noisy) synchronized p-sums of bias vectors and covariance matrices (\u03b1\u0303ik in Algorithm 2). Specifically, it applies the vector sum mechanism PVec of Cheu et al. (2021), which essentially take n vectors as inputs and outputs their noisy sum. Here privacy is ensured by injecting suitable binomial noise to a fixed-point encoding of each vector entry, which depends on \u03b5, \u03b4 and n.\nIn our case, one cannot directly aggregate M p-sums using PVec with n = M . This is because each p-sum would then have a large norm (O(T ) at the worst case), which would introduce a large amount of privacy noise (cf. Theorem 3.2 in Cheu et al. (2021)), resulting in worse utility (regret). Instead, we first expand each p-sum resulting in a set of points such that each with O(1) norm. Then, we aggregate all of those data points using PVec mechanism (one each for bias vectors and covariance matrices). For example, consider summing bias vectors during batch k = 6 and refer back to Fig. 1 for illustration. Here, the p-sum for each agent is given by \u2211 [5, 6] = \u03b35 + \u03b36 (see line 6 in Algorithm 2), the expansion of which results in 2B bias vectors (B each for batch 5 and 6). A noisy sum of n = 2BM bias vectors is then computed using PVec. We denote the entire mechanism as PTVec \u2013 see Algorithm 5 in Appendix F.2 for pseudo-code and a complete description. Now, the key intuition behind using PVec as a building block is that it allows us to compute private vector sums under the shuffle model using nearly the same amount of noise as in the central model. In other words, it \u201csimulates\u201d the privacy noise introduced in vector summation under the central model using a shuffler. This, in turn, helps us match the regret of the centralized setting while guaranteeing (strictly stronger) SDP. Specifically, we have the same order of regret as in Theorem 5.3, but now it holds for a wide range of privacy budgets \u03b5, \u03b4 as presented below. Proof is deferred to Appendix F. Theorem 5.5 (Performance under SDP via vector sum). Fix batch size B and let \u03ba=1+log(T/B). Let PTVec be a privacy protocol given by Algorithm 5. Then, under Assumption 4.1, there exist parameter choices of PTVec such that for any \u03b5\u226460 \u221a 2\u03ba log(2/\u03b4) and \u03b4\u22641, Algorithm 1 instantiated with PTVec satisfies (\u03b5, \u03b4)-SDP. Moreover, for any \u03b1 \u2208 (0, 1], there exist choices of \u03bb and {\u03b2t,i}t,i such that, with a probability at least 1\u2212 \u03b1, it enjoys a group regret\nRM (T )=O ( dMB log T+d \u221a MT log(MT/\u03b1) ) +O\u0303 ( d3/4 \u221a MT\nlog3/4(\u03bad2/\u03b4)\u221a \u03b5\nlog1/4 ( T/B\u03b1 )) .\nRemark 5.6 (Importance of communicating P-sums). A key technique behind closing the regret gap under SDP is to communicate and shuffle only the p-sums rather than prefix sums. With this we can ensure that each data point (bias vector/covariance matrix) participates only in O(logK) shuffle mechanisms (rather than in O(K) mechanisms if we communicate and shuffle prefix-sums). This helps us keep the final privacy cost in check after adaptive composition. In other words, one cannot simply use shuffling to amplify privacy of Algorithm 1 in Dubey & Pentland (2020) to close the regret gap (even ignoring its privacy and communication issues), since it communicates prefix sums at each\nsynchronization. This again highlights the algorithmic novelty of our privacy protocols (Algorithms 2 and 5), which could be of independent interest. See Appendix F for further details."
        },
        {
            "heading": "6 SIMULATION RESULTS AND CONCLUSIONS",
            "text": "We evaluate regret performance of Algorithm 1 under silo-level LDP and SDP, which we abbreviate as LDP-FedLinUCB and SDP-FedLinUCB, respectively. We fix confidence level \u03b1=0.01, batchsize B = 25 and study comparative performances under varying privacy budgets \u03b5, \u03b4. We plot timeaveraged group regret RegM (T )/T in Figure 2 by averaging results over 25 parallel runs.\nSynthetic bandit instance. We simulate a LCB instance with a parameter \u03b8\u2217 of dimension d = 10 and |Ki| = 100 actions for each of the M agents. Similar to Vaswani et al. (2020), we generate \u03b8\u2217 and feature vectors by sampling a (d\u22121)-dimensional vectors of norm 1/ \u221a 2 uniformly at random, and append it with a 1/ \u221a 2 entry. Rewards are corrupted with Gaussian N (0, 0.25) noise.\nReal-data bandit instance. We generate bandit instances from Microsoft Learning to Rank dataset (Qin & Liu, 2013). Queries form contexts c and actions a are the available documents. The dataset contains 10K queries, each with up to 908 judged documents on scale of rel(c, a) \u2208 {0, 1, 2}. Each pair (c, a) has a feature vector \u03d5(c, a), which is partitioned into title and body features of dimensions 57 and 78, respectively. We first train a lasso regression model on title features to predict relevances from \u03d5, and take this model as the parameter \u03b8\u2217 with d = 57. Next, we divide the queries equally into M =10 agents and assign corresponding feature vectors to the agents. This way, we obtain a federated LCB instance with 10 agents, each with number of actions |Ki| \u2264 908. Observations. In Fig. 2(a), we compare performance of LDP-FedLinUCB and SDP-FedLinUCB (with amplification based privacy protocol P) on synthetic bandit instance with M=100 agents under privacy budget \u03b4=0.0001 and \u03b5=0.001 or 0.0001. We observe that regret of SDP-FedLinUCB is less than LDP-FedLinUCB for both values of \u03b5, which is consistent with theoretical results. Here, we only work with small privacy budgets since the privacy guarantee of Theorem 5.3 holds for \u03b5, \u03b4\u226a1. Instead, in Fig. 2(b), we consider higher privacy budgets as suggested in Theorem 5.5 (e.g. \u03b5=0.2, \u03b4=0.1) and compare the regret performance of LDP-FedLinUCB and SDP-FedLinUCB (with vecorsum based privacy protocol PTvec). As expected, we observe that regret of SDP-FedLinUCB decreases faster than that of LDP-FedLinUCB. Next, we benchmark the performance of Algorithm 1 under silo-level LDP (i.e. LDP-FedLinUCB) against a non-private Federated LCB algorithm with fixed communication schedule, building on Abbasi-Yadkori et al. (2011) and refer as FedLinUCB. In Fig. 2(c), we demonstrate the cost of privacy under silo-level LDP on real-data bandit instance by varying \u03b5 in the set {0.2, 1, 5} while keeping \u03b4 fixed to 0.1. We observe that regret of LDP-FedLinUCB decreases and comes closer to that of FedLinUCB as \u03b5 increases (i.e., level of privacy protection decreases). A similar regret behavior is noticed under SDP also (postponed to Appendix H).\nConcluding Remarks. We conclude with some discussions. First, our adversary model behind silo-level LDP excludes malicious users within the same silo. If one is also interested in protecting against adversary users within the same silo, a simple tweak of Algorithm 1 would suffice (see Appendix G.3). With this, one can not only protect against colluding silos (as in silo-level LDP), but also against colluding users within the same silo (as in central JDP). Next, we assume that all MT users are unique in our algorithms. In practice, a user can participate in multiple rounds within the same silo or across different silos; see Appendix G.4. Finally, for future work, a challenging task is to achieve O(log T ) communication cost with correct privacy and regret guarantees; see Appendix D for further discussions."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "XZ is supported in part by NSF CNS-2153220 and CNS-2312835. XZ would like to thank Abhimanyu Dubey for discussions on the work of Dubey & Pentland (2020). XZ would also like to thank Andrew Lowy and Ziyu Liu for insightful discussions on the privacy notion for cross-silo federated learning. XZ would thank Vitaly Feldman and Audra McMillan for the discussion on some subtleties behind \u201chiding among the clones\u201d in Feldman et al. (2022)."
        },
        {
            "heading": "A MORE DETAILS ON RELATED WORK",
            "text": "Private bandit learning has also been studied beyond linear settings, such as kernel bandits (Zhou & Tan, 2021; Dubey, 2021; Li et al., 2023). It is worth noting that Chowdhury & Zhou (2022a) also presents optimal private regret bounds under all three DP models (i.e., central, local, and distributed) in bandits while only relying on discrete privacy noise, hence avoiding the privacy leakage of continuous privacy noise on finite computers due to floating point arithmetic.\nRecently, Huang et al. (2023) took the pioneering step to study user-level privacy for federated LCBs, establishing both regret upper bounds and lower bounds. In contrast to our item-level DP (e.g., silo-level LDP), user-level DP in Huang et al. (2023) roughly requires that even replacing the whole local history at any agent, the central server\u2019s broadcast message should be close across the whole learning period. This notion is more likely to be preferred in cross-device FL settings where the protection target is the device (agent). In addition to this, there are several other key differences compared to our work. First, they deal with linear bandits with stochastic contexts under additional distribution coverage assumptions (rather than the arbitrary adversary contexts in our case). In fact, it has been shown in Huang et al. (2021) that some assumption on the context distribution is necessary for a sublinear regret under user-level DP. Second, due to this stochastic context and some coverage conditions on contexts, an exponentially growing batch schedule can be applied in their case. In contrast, under the adversary context case, it is unclear to us how to apply the same technique to derive a sublinear regret."
        },
        {
            "heading": "B MORE DISCUSSIONS ON GAPS IN SOTA",
            "text": "In this section, we provide more details on the current gaps in Dubey & Pentland (2020), especially on privacy violation and communication cost. It turns out that both gaps come from the fact that an adaptive communication schedule is employed in Dubey & Pentland (2020)."
        },
        {
            "heading": "B.1 MORE ON VIOLATION OF SILO-LEVEL LDP",
            "text": "As shown in the main paper, Algorithm 1 in Dubey & Pentland (2020) does not satisfy silo-level LDP. To give a more concrete illustration of privacy leakage, we now specify the form of f 3, local data Xi and synchronized data Z in (1) according to Dubey & Pentland (2020). In particular, a communication is triggered at round t if for any silo i, it holds that\n(t\u2212t\u2032) log\ndet ( Z+ \u2211t s=t\u2032+1 xs,ix \u22a4 s,i+\u03bbminI ) det (Z+\u03bbminI) >D, (2) where t\u2032 is the latest synchronization time before t, Z is all synchronized (private) covariance matrices up to time t\u2032, \u03bbmin > 0 is some regularization constant (which depends on privacy budgets \u03b5, \u03b4) and D > 0 is some suitable threshold (which depends on number of silos M ).\nWith the above explicit form in hand, we can give a more concrete discussion of Example 3.1. A communication is triggered at round t = 1 if det ( x1,mx \u22a4 1,m+\u03bbminI ) > det (\u03bbminI) e\nD holds for any silo m. This implies that (\u03bbmin + \u2225x1,m\u22252)\u03bbd\u22121min > eD\u03bbdmin, which, in turn, yields \u2225x1,m\u2225 2 > \u03bbmin(e D \u2212 1) =: C. Now, if \u2225x1,j\u22252 \u2264 C, then silo j immediately knows that \u2225x1,i\u22252 > C, where C is a known constant. Since x1,i contains the context information of the user (Alice), this norm condition could immediately reveal that some specific features in the context vector are active (e.g., Alice has both diabetes and heart disease), thus leaking Alice\u2019s private and sensitive information to silo j. Remark B.1. The above result has two implications: (i) the current proof strategy for Fed-DP guarantee in Dubey & Pentland (2020) does not hold since it essentially relies on the post-processing of DP through silo-level LDP; (ii) Fed-DP could fail to handle reasonable adversary model in crosssilo federated LCBs. That is, even if Algorithm 1 in Dubey & Pentland (2020) satisfies Fed-DP, it still cannot protect Alice\u2019s information from being inferred by a malicious silo (which is a typical\n3There is some minor issue in the form of f in Dubey & Pentland (2020). The correct one is given by our restatement of their Algorithm 1, see line 9 in Algorithm 3.\nadversary model in cross-silo FL). Thus, we believe that silo-level LDP is a more proper privacy notion for cross-silo federated LCBs."
        },
        {
            "heading": "B.2 MORE ON VIOLATION OF FED-DP",
            "text": "As shown in the main paper, Algorithm 1 in Dubey & Pentland (2020) also does not satisfy its weaker notion of Fed-DP. To give a more concrete illustration, recall Example 3.1 and let us define mi,j as the message/data sent from silo i to silo j after round t = 1. Suppose in the case of Alice, there is no synchronization and hence mi,j = 0. On the other hand, in the case of Tracy (i.e., the first user at silo i changes from Alice to Tracy), suppose synchronization is triggered by silo i via rule (1) due to Tracy\u2019s data. Then, according to Dubey & Pentland (2020), mi,j = x1,iy1,i +N (only consider bias vector here), where N is the injected noise when silo i sends out its data. Now, based on the requirement of Fed-DP, the recommended action at silo j in round t = 2 needs to be \u201csimilar\u201d or \u201cindistinguishable\u201d in probability under the change from Alice to Tracy. Note that silo j chooses its action at round t = 2 based on its local data (which is unchanged) and mi,j , via deterministic selection rule (i.e., LinUCB) in Algorithm 1 of Dubey & Pentland (2020). Thus, Fed-DP essentially requires mi,j to be close in probability when Alice changes to Tracy, which is definitely not the case (i.e., 0 vs. x1,iy1,i +N ). Thus, Algorithm 1 in Dubey & Pentland (2020) also fails Fed-DP. Remark B.2. One can also think from the following perspective: the non-private data-dependent sync rule (i.e., (2)) in Dubey & Pentland (2020) impacts the communicated messages/data as well, which cannot be made private by injecting noise when sending out data. To rescue, a possible approach is to use private (noisy) data in rule (2) when determining synchronization (while still injecting noise when sending out data). As a result, whether there exists a synchronization would be \u201cindistinguishable\u201d under Alice or Tracy and hence mi,j now would be similar. However, this approach still suffers the gap in communication cost analysis (see below) and moreover it will incur new challenges in regret analysis, see Appendix D for a detailed discussion on this approach."
        },
        {
            "heading": "B.3 MORE ON COMMUNICATION COST ANALYSIS",
            "text": "The current analysis in Dubey & Pentland (2020) (cf. Proposition 5) for communication cost (i.e., how many rounds of communication within T ) essentially follows the approach in the non-private work (Wang et al., 2020) (cf. proof of Theorem 4). However, due to additional privacy noise injected into the communicated data, one key step of the approach in Wang et al. (2020) fails in the private case. In the following, we first point out the issue using notations in Dubey & Pentland (2020).\nThe key issue in its current proof of Proposition 5 in Dubey & Pentland (2020) is that\nlog det(Si,t+n\u2032)\ndet(Si,t) >\nD n\u2032 (3)\nwhich appears right above Eq. 4 in Dubey & Pentland (2020) does not hold. More specifically, [t, t+ n\u2032] is the i-th interval between two communication steps and Si,t,Si,t+n\u2032 are corresponding synchronized private matrices. At the time t+n\u2032, we know (2) is satisfied by some silo (say j \u2208 [M ]), since there is a new synchronization. In the non-private case, Si,t+n\u2032 simply includes some additional local covariance matrices from silos other than j, which are positive semi-definite (PSD). As a result, (3) holds. However, in the private case, Si,t+n\u2032 includes the private messages from silos other than j, which may not be positive semi-definite (PSD), since there are some new covariance matrices as well as new Gaussian privacy noise (which could be negative definite). Thus, (3) may not hold anymore."
        },
        {
            "heading": "C A GENERIC REGRET ANALYSIS FOR ALGORITHM 1",
            "text": "In this section, we first establish a generic regret bound for Algorithm 1 under sub-Gaussian noise condition, i.e., Lemma C.4. To this end, let us first give the following notations. Fix B, T \u2208 N, we let K = T/B be the total number of communication steps. For all i \u2208 [M ] and all t = kB, k \u2208 [K], we let Nt,i = W\u0303t,i \u2212 \u2211t s=1 xs,ix \u22a4 s,i and nt,i = U\u0303t,i \u2212 \u2211t s=1 xs,iys,i be the cumulative\ninjected noise up to the k-th communication by agent i. We further let Ht := \u03bbId + \u2211\ni\u2208[M ] Nt,i and ht := \u2211 i\u2208[M ] nt,i.\nAssumption C.1 (Regularity). Fix any \u03b1 \u2208 (0, 1], with probability at least 1 \u2212 \u03b1, we have Ht is positive definite and there exist constants \u03bbmax, \u03bbmin and \u03bd depending on \u03b1 such that for all t = kB, k \u2208 [K]\n\u2225Ht\u2225 \u2264 \u03bbmax, \u2225\u2225H\u22121t \u2225\u2225 \u2264 1/\u03bbmin, \u2225ht\u2225H\u22121t \u2264 \u03bd.\nWith the above regularity assumption and the boundedness in Assumption 4.1, we fist establish the following general regret bound of Algorithm 1, which can be viewed as a direct generalization of the results in Shariff & Sheffet (2018); Chowdhury & Zhou (2022b) to the federated case.\nLemma C.2. Let Assumptions C.1 and 4.1 hold. Fix any \u03b1 \u2208 (0, 1], there exist choices of \u03bb and {\u03b2t,i}t\u2208[T ],i\u2208[M ] such that, with probability at least 1\u2212 \u03b1, the group regret of Algorithm 1 satisfies\nRegM (T ) = O ( \u03b2T \u221a dMT log ( 1 + MT\nd\u03bbmin\n)) +O ( M \u00b7B \u00b7 d log ( 1 + MT\nd\u03bbmin\n)) ,\nwhere \u03b2T := \u221a 2 log ( 2 \u03b1 ) + d log ( 1 + MTd\u03bbmin ) + \u221a \u03bbmax + \u03bd.\nLemma C.4 is a corollary of the above result, which holds by bounding \u03bbmax, \u03bbmin, \u03bd under subGaussian privacy noise.\nAssumption C.3 (sub-Gaussian private noise). There exist constants \u03c3\u03031 and \u03c3\u03032 such that for all t = kB, k \u2208 [K]: (i) \u2211M i=1 nt,i is a random vector whose entries are independent, mean zero,\nsub-Gaussian with variance at most \u03c3\u030321 , and (ii) \u2211M\ni=1 Nt,i is a random symmetric matrix whose entries on and above the diagonal are independent sub-Gaussian random variables with variance at most \u03c3\u030322 . Let \u03c3 2=max{\u03c3\u030321 , \u03c3\u030322}.\nNow, we are ready to state Lemma C.4 as follows.\nLemma C.4 (A generic regret bound of Algorithm 1). Let Assumptions C.3 and 4.1 hold. Fix time horizon T \u2208 N, batch size B \u2208 [T ], confidence level \u03b1 \u2208 (0, 1]. Set \u03bb = \u0398(max{1, \u03c3( \u221a d +\u221a log(T/(B\u03b1))}) and \u03b2t,i = \u221a 2 log ( 2 \u03b1 ) + d log ( 1 + Mtd\u03bb ) + \u221a \u03bb for all i \u2208 [M ]. Then, Algorithm 1 achieves group regret\nRegM (T ) = O ( dMB log T + d \u221a MT log(MT/\u03b1) ) +O (\u221a \u03c3MT log(MT )d3/4 log1/4(T/(B\u03b1)) ) with probability at least 1\u2212 \u03b1."
        },
        {
            "heading": "C.1 PROOFS",
            "text": "Proof of Lemma C.2. We divide the proof into the following six steps. Let E be the event given in Assumption C.1, which holds with probability at least 1\u2212 \u03b1 under Assumption C.1. In the following, we condition on the event E .\nStep 1: Concentration. In this step, we will show that with high probability, \u2225\u2225\u2225\u03b8\u2217 \u2212 \u03b8\u0302t,i\u2225\u2225\u2225\nVt,i \u2264 \u03b2t,i\nfor all i \u2208 [M ]. Fix an agent i \u2208 [M ] and t \u2208 [T ], let tlast be the latest communication round of all agents before t. By the update rule, we have\n\u03b8\u0302t,i = V \u22121 t,i (U\u0303syn + Ui)\n= V \u22121t,i  M\u2211 j=1 tlast\u2211 s=1 xs,jys,j + M\u2211 j=1 ntlast,j + t\u22121\u2211 s=tlast+1 xs,iys,i  =\n\u03bbI + M\u2211 j=1 tlast\u2211 s=1 xs,jx \u22a4 s,j + M\u2211 j=1 Ntlast,j + t\u22121\u2211 s=tlast+1 xs,ix \u22a4 s,i \u22121 M\u2211 j=1 tlast\u2211 s=1 xs,jys,j + M\u2211 j=1 ntlast,j + t\u22121\u2211 s=tlast+1 xs,iys,i  .\nBy the linear reward function ys,j = \u27e8xs,j , \u03b8\u2217\u27e9 + \u03b7s,j for all j \u2208 [M ] and elementary algebra, we have\n\u03b8\u2217 \u2212 \u03b8\u0302t,i = V \u22121t,i Htlast\u03b8\u2217 \u2212 M\u2211 j=1 tlast\u2211 s=1 xs,j\u03b7s,j \u2212 t\u22121\u2211 s=tlast+1 xs,i\u03b7s,i \u2212 htlast  , where we recall that Htlast = \u03bbI + \u2211M j=1 Ntlast,j and htlast = \u2211M j=1 ntlast,j .\nThus, multiplying both sides by V 1/2t,i , yields\u2225\u2225\u2225\u03b8\u2217 \u2212 \u03b8\u0302t,i\u2225\u2225\u2225 Vt,i \u2264 \u2225\u2225\u2225\u2225\u2225\u2225 M\u2211 j=1 tlast\u2211 s=1 xs,j\u03b7s,j + t\u22121\u2211 s=tlast+1 xs,i\u03b7s,i \u2225\u2225\u2225\u2225\u2225\u2225 V \u22121t,i + \u2225Htlast\u03b8\u2217\u2225V \u22121t,i + \u2225htlast\u2225V \u22121t,i\n(a) \u2264 \u2225\u2225\u2225\u2225\u2225\u2225 M\u2211 j=1 tlast\u2211 s=1 xs,j\u03b7s,j + t\u22121\u2211 s=tlast+1 xs,i\u03b7s,i \u2225\u2225\u2225\u2225\u2225\u2225 (Gt,i+\u03bbminI)\u22121 + \u2225\u03b8\u2217\u2225Htlast + \u2225htlast\u2225H\u22121tlast\n(b) \u2264 \u2225\u2225\u2225\u2225\u2225\u2225 M\u2211 j=1 tlast\u2211 s=1 xs,j\u03b7s,j + t\u22121\u2211 s=tlast+1 xs,i\u03b7s,i \u2225\u2225\u2225\u2225\u2225\u2225 (Gt,i+\u03bbminI)\u22121 + \u221a \u03bbmax + \u03bd\nwhere (a) holds by Vt,i \u2ab0 Htlast and Vt,i \u2ab0 Gt,i + \u03bbminI with Gt,i := \u2211M j=1 \u2211tlast s=1 xs,jx\n\u22a4 s,j +\u2211t\u22121\ns=tlast+1 xs,ix \u22a4 s,i (i.e., non-private Gram matrix) under event E ; (b) holds by the boundedness of \u03b8\u2217 and event E . For the remaining first term, we can use self-normalized inequality (cf. Theorem 1 in Abbasi-Yadkori et al. (2011)) with a proper filtration4. In particular, we have for any \u03b1 \u2208 (0, 1], with probability at least 1\u2212 \u03b1, for all t \u2208 [T ]\u2225\u2225\u2225\u2225\u2225\u2225 M\u2211 j=1 tlast\u2211 s=1 xs,j\u03b7s,j + t\u22121\u2211 s=tlast+1 xs,i\u03b7s,i \u2225\u2225\u2225\u2225\u2225\u2225 (Gt,i+\u03bbminI)\u22121 \u2264 \u221a 2 log ( 1 \u03b1 ) + log ( det(Gt,i + \u03bbminI) det(\u03bbminI) ) .\nNow, using the trace-determinant lemma (cf. Lemma 10 in Abbasi-Yadkori et al. (2011)) and the boundedness condition on \u2225xs,j\u2225 for all s \u2208 [T ] and j \u2208 [M ], we have\ndet(Gt,i + \u03bbminI) \u2264 ( \u03bbmin + Mt\nd\n)d .\nPutting everything together, we have with probability at least 1\u2212 2\u03b1, for all i \u2208 [M ] and all t \u2208 [T ],\u2225\u2225\u2225\u03b8\u2217 \u2212 \u03b8\u0302m\u2225\u2225\u2225 Vt,i \u2264 \u03b2t,i = \u03b2t, where\n\u03b2t :=\n\u221a 2 log ( 1\n\u03b1\n) + d log ( 1 + Mt\nd\u03bbmin\n) + \u221a \u03bbmax + \u03bd. (4)\nStep 2: Per-step regret. With the above concentration result, based on our UCB policy for choosing the action, we have the classic bound on the per-step regret rt,i, that is, with probability at least 1\u2212 2\u03b1\nrt,i = \u27e8\u03b8\u2217, x\u2217t,i\u27e9 \u2212 \u27e8\u03b8\u2217, xt,i\u27e9 (a) = \u27e8\u03b8\u2217, x\u2217t,i\u27e9 \u2212 UCBt,i(x\u2217t,i) + UCBt,i(x\u2217t,i)\u2212 UCBt,i(xt,i) + UCBt,i(xt,i)\u2212 \u27e8\u03b8\u2217, xt,i\u27e9 (b)\n\u2264 0 + 0 + 2\u03b2t,i \u2225xt,i\u2225V \u22121t,i \u2264 2\u03b2T \u2225xt,i\u2225V \u22121t,i where in (a), we let UCBt,i(x) := \u27e8\u03b8\u0302t,i, x\u27e9+ \u03b2t,i \u2225x\u2225V \u22121t,i ; (b) holds by the optimistic fact of UCB (from the concentration), greedy action selection, and the concentration result again.\n4In particular, by the i.i.d noise assumption across time and agents, one can simply construct the filtration sequentially across agents and rounds, which enlarges the single-agent filtration by a factor of M .\nStep 3: Regret decomposition by good and bad epochs. In Algorithm 1, at the end of each synchronization time t = kB for k \u2208 [K], all the agents will communicate with the server by uploading private statistics and downloading the aggregated ones from the server. We then divide time horizon T into epochs by the communication (sync) rounds. In particular, the k-th epoch contains rounds between (tk\u22121, tk], where tk = kB is the k-th sync round. We define Vk := \u03bbminI + \u2211M i=1 \u2211tk t=1 xt,ix \u22a4 t,i, i.e., all the data at the end of the k-th communication plus a regularizer. Then, we say that the k-th epoch is a \u201cgood\u201d epoch if det(Vk)det(Vk\u22121) \u2264 2; otherwise it is a \u201cbad\u201d epoch. Thus, we can divide the group regret into two terms:\nRegM (T ) = \u2211 i\u2208[M ] \u2211 t\u2208good epochs rt,i + \u2211 i\u2208[M ] \u2211 t\u2208bad epochs rt,i.\nStep 4: Bound the regret in good epochs. To this end, we introduce an imaginary single agent that pulls all the MT actions in the following order: x1,1,, x1,2, . . . , x1,M , x2,1, . . . , x2,M , . . . , xT,1, . . . , xT,M . We define a corresponding imaginary design matrix V\u0304t,i = \u03bbminI + \u2211 p<t,q\u2208[M ] xp,qx \u22a4 p,q + \u2211 p=t,q<i xp,qx \u22a4 p,q , i.e., the design matrix right before xt,i. The key reason behind this construction is that one can now use the standard result (i.e., the elliptical potential lemma (cf. Lemma 11 in Abbasi-Yadkori et al. (2011))) to bound the summation of bonus terms, i.e., \u2211 t,i \u2225xt,i\u2225V\u0304 \u22121t,i .\nSuppose that t \u2208 [T ] is within the k-th epoch. One key property we will use is that for all i, Vk \u2ab0 V\u0304t,i and Gt,i + \u03bbminI \u2ab0 Vk\u22121, which simply holds by their definitions. This property enables us to see that for any t \u2208 good epochs, det(V\u0304t,i)/ det(Gt,i + \u03bbminI) \u2264 2. This is important since by the standard \u201cdeterminant trick\u201d, we have\n\u2225xt,i\u2225(Gt,i+\u03bbminI)\u22121 \u2264 \u221a 2 \u2225xt,i\u2225V\u0304 \u22121t,i . (5)\nIn particular, this follows from Lemma 12 in Abbasi-Yadkori et al. (2011), that is, for two positive definite matrices A,B \u2208 Rd\u00d7d satisfying A \u2ab0 B, then for any x \u2208 Rd, \u2225x\u2225A \u2264 \u2225x\u2225B \u00b7 \u221a det(A)/ det(B). Note that here we also use det(A) = 1/ det(A\u22121). Hence, we can bound the regret in good epochs as follows.\u2211 i\u2208[M ] \u2211 t\u2208good epochs rt,i (a) \u2264 \u2211 i\u2208[M ] \u2211 t\u2208good epochs min{2\u03b2T \u2225xt,i\u2225V \u22121t,i , 1}\n(b) \u2264 \u2211 i\u2208[M ] \u2211 t\u2208good epochs min{2\u03b2T \u2225xt,i\u2225(Gt,i+\u03bbminI)\u22121 , 1}\n(c) \u2264 \u2211 i\u2208[M ] \u2211 t\u2208good epochs min{2 \u221a 2\u03b2T \u2225xt,i\u2225V\u0304 \u22121t,i , 1}\n(d) \u2264 \u2211 i\u2208[M ] \u2211 t\u2208good epochs 2 \u221a 2\u03b2T min{\u2225xt,i\u2225V\u0304 \u22121t,i , 1}\n\u2264 \u2211 i\u2208[M ] \u2211 t\u2208[T ] 2 \u221a 2\u03b2T min{\u2225xt,i\u2225V\u0304 \u22121t,i , 1}\n(e) \u2264 O ( \u03b2T \u221a dMT log ( 1 + MT\nd\u03bbmin\n)) , (6)\nwhere (a) holds by the per-step regret bound in Step 2 and the boundedness of reward; (b) follows from the fact that Vt,i \u2ab0 Gt,i + \u03bbminI under event E ; (c) holds by (5) when t is in good epochs; (d) is true since \u03b2T \u2265 1; (e) holds by the elliptical potential lemma (cf. Lemma 11 in Abbasi-Yadkori et al. (2011)).\nStep 5: Bound the regret in bad epochs. Let Tbad be the total number of rounds in all bad epochs. Thus, the total number of bad rounds across all agents are M \u00b7 Tbad. As a result, the cumulative group regret in all these bad rounds are upper bounded by M \u00b7 Tbad due to the to the boundedness of reward.\nWe are left to bound Tbad. All we need is to bound the Nbad \u2013 total number of bad epochs. Then, we have Tbad = Nbad \u00b7B, where B is the fixed batch size. To this end, recall that K = T/B and define \u03a8 := {k \u2208 [K] : log det(Vk)\u2212 log det(Vk\u22121) > log 2}, i.e., Nbad = |\u03a8|. Thus, we have\nlog 2 \u00b7 |\u03a8| \u2264 \u2211 k\u2208\u03a8 log det(Vk)\u2212 log det(Vk\u22121) \u2264 \u2211 k\u2208[K] log det(Vk)\u2212 log det(Vk\u22121)\n\u2264 d log ( 1 + MT\nd\u03bbmin ) Hence, we have Nbad = |\u03a8| \u2264 dlog 2 log ( 1 + MTd\u03bbmin ) . Thus we can bound the regret in bad epochs as follows.\u2211 i\u2208[M ] \u2211 t\u2208bad epochs rt,i \u2264M \u00b7 Tbad = M \u00b7B \u00b7Nbad \u2264M \u00b7B \u00b7 d log 2 log ( 1 + MT d\u03bbmin ) . (7)\nStep 6: Putting everything together. Now, we substitute the total regret in good epochs given by (6) and total regret in bad epochs given by (7) into the total regret decomposition in Step 3, yields the final cumulative group regret\nRegM (T ) = O ( \u03b2T \u221a dMT log ( 1 + MT\nd\u03bbmin\n)) +O ( M \u00b7B \u00b7 d log ( 1 + MT\nd\u03bbmin\n)) ,\nwhere \u03b2T := \u221a 2 log ( 1 \u03b1 ) + d log ( 1 + MTd\u03bbmin ) + \u221a \u03bbmax + \u03bd. Finally, taking a union bound, we\nhave the required result.\nNow, we turn to the proof of Lemma C.4, which is an application of Lemma C.2 we just proved.\nProof of Lemma C.4. To prove the result, thanks to Lemma C.2, we only need to determine the three constants \u03bbmax, \u03bbmin and \u03bd under the sub-Gaussian private noise assumption in Assumption C.3. To this end, we resort to concentration bounds for sub-Gaussian random vector and random matrix.\nTo start with, under (i) in Assumption C.3, by the concentration bound for the norm of a vector containing sub-Gaussian entries (cf. Theorem 3.1.1 in Vershynin (2018)) and a union bound over all communication rounds, we have for all t = kB where k = [T/B] and any \u03b1 \u2208 (0, 1], with probability at least 1\u2212 \u03b1/2, for some absolute constant c1,\u2225\u2225\u2225\u2225\u2225 M\u2211 i=1 nt,i\n\u2225\u2225\u2225\u2225\u2225 = \u2225ht\u2225 \u2264 \u03a3n := c1 \u00b7 \u03c3\u03031 \u00b7 (\u221ad+\u221alog(T/(\u03b1B)). By (ii) in Assumption C.3, the concentration bound for the norm of a sub-Gaussian symmetric random matrix (cf. Corollary 4.4.8 in Vershynin (2018)) and a union bound over all communication rounds, we have for all t = kB where k = [T/B] and any \u03b1 \u2208 (0, 1], with probability at least 1\u2212 \u03b1/2,\u2225\u2225\u2225\u2225\u2225 M\u2211 i=1 Nt,i\n\u2225\u2225\u2225\u2225\u2225 \u2264 \u03a3N := c2 \u00b7 \u03c3\u03032 \u00b7 (\u221ad+\u221alog(T/(\u03b1B)) for some absolute constant c2. Thus, if we choose \u03bb = 2\u03a3N , we have \u2225Ht\u2225 =\n\u2225\u2225\u2225\u03bbId +\u2211Mi=1 Nt,i\u2225\u2225\u2225 \u2264 3\u03a3N , i.e., \u03bbmax = 3\u03a3N , and \u03bbmin = \u03a3N . Finally, to determine \u03bd, we note that\n\u2225ht\u2225H\u22121t \u2264 1\u221a \u03bbmin\n\u2225ht\u2225 \u2264 c \u00b7 ( \u03c3 \u00b7 ( \u221a d+ \u221a log(T/(\u03b1B)) )1/2 := \u03bd,\nwhere \u03c3 = max{\u03c3\u03031, \u03c3\u03032}. The final regret bound is obtained by plugging the three values into the result given by Lemma C.2."
        },
        {
            "heading": "D DISCUSSION ON PRIVATE ADAPTIVE COMMUNICATION",
            "text": "In the main paper and Appendix B, we have pointed out that the gap in privacy guarantee of Algorithm 1 in Dubey & Pentland (2020) is that its adaptive communication schedule leads to privacy leakage\nAlgorithm 3 Restatement of Algorithm 1 in (Dubey & Pentland, 2020) 1: Parameters: Adaptive communication parameter D, regularization \u03bb > 0, confidence radii {\u03b2t,i}t\u2208[T ],i\u2208[M ], feature map \u03d5i : Ci \u00d7Ki \u2192 Rd, privacy budgets \u03b5 > 0, \u03b4 \u2208 [0, 1].\n2: Initialize: For all i \u2208 [M ], Wi = 0, Ui = 0, PRIVATIZER with \u03b5, \u03b4, W\u0303syn = 0, U\u0303syn = 0, 3: for t=1, . . . , T do 4: for each agent i = 1, . . . ,M do 5: Vt,i = \u03bbI + W\u0303syn +Wi, \u03b8\u0302t,i = V \u22121t,i (U\u0303syn + Ui) 6: Play arm at,i = argmaxa\u2208Ki\u27e8\u03d5i(ct,i, a), \u03b8\u0302t,i\u27e9 + \u03b2t,i \u2225\u03d5i(ct,i, a)\u2225V \u22121t,i and set xt,i = \u03d5i(ct,i, at,i) 7: Observe reward yt,i 8: Update Wi = Wi + xt,ix\u22a4t,i, Ui = Ui + xt,iyt,i 9: if log det(Vt,i + xt,ix\u22a4t,i)\u2212 log det(Vlast) > Dt\u2212tlast then\n10: Send a signal to the server to start a synchronization round. 11: end if 12: if a synchronization is started then 13: Send Wi and Ui to PRIVATIZER 14: PRIVATIZER sends private cumulative statistics W\u0303t,i, U\u0303t,i to server 15: Server aggregates W\u0303syn = W\u0303syn + \u2211M j=1 W\u0303t,j and U\u0303syn = U\u0303syn + \u2211M j=1 U\u0303t,j\n16: Receive W\u0303syn and U\u0303syn from the server 17: Reset Wi = 0, Ui = 0, tlast = t and Vlast = \u03bbI + W\u0303syn 18: end if 19: end for 20: end for\ndue to its dependence on non-private data. As mentioned in Remark B.1, one possible approach is to use private data to determine the sync in (2). This will resolve the privacy issue. However, the same issue in communication cost still remains (due to privacy noise), and hence O(log T ) communication does not hold. Moreover, this new approach will also lead to new challenges in regret analysis, when compared with its current one in Dubey & Pentland (2020) and the standard one in Wang et al. (2020).\nTo better illustrate the new challenges, let us restate Algorithm 1 in Dubey & Pentland (2020) using our notations and first focus on how to establish the regret based on its current adaptive schedule (which has the issue of privacy leakage). After we have a better understanding of the idea, we will see how new challenges come up when one uses private data for an adaptive schedule.\nAs shown in Algorithm 3, the key difference compared with our fixed-batch schedule is highlighted in color. Note that we only focus on silo-level LDP and use PRIVATIZER to represent a general protocol that can privatize the communicated data (e.g., P or the standard tree-based algorithm in Dubey & Pentland (2020))."
        },
        {
            "heading": "D.1 REGRET ANALYSIS UNDER NON-PRIVATE ADAPTIVE SCHEDULE",
            "text": "In this section, we demonstrate the key step in establishing the regret with the non-private adaptive communication schedule in Algorithm 3 (i.e., line 9). It turns out that the regret analysis is very similar to our proof for Lemma C.2 for the fixed batch case, in that the only key difference lies in Step 5 when bounding the regret in bad epochs5. The main idea behind adaptive communication is: whenever the accumulated local regret at any agent exceeds a threshold, then synchronization is required to keep the data homogeneous among agents. This idea is directly reflected in the following analysis.\n5There is another subtle but important difference, which lies in the construction of filtration that is required to apply the standard self-normalized inequality to establish the concentration result. We believe that one cannot directly use the standard filtration (e.g., Abbasi-Yadkori et al. (2011)) in the adaptive case, and hence more care is indeed required.\nBound the regret in bad epochs (adaptive communication case). Let\u2019s consider an arbitrary bad epoch k, i.e., (tk\u22121, tk], where tk is the round for the k-th communication. For all i, we want to bound the total regret between (tk\u22121, tk], denoted by Rki . That is, the local regret between any two communications (in the bad epoch) will not be too large. For now, suppose we already have such a bound U (which will be achieved by adaptive communication later), i.e., Rki \u2264 U for all i, k, we can easily bound the total regret in bad epochs. To see this, recall that \u03a8 := {k \u2208 [K] : log det(Vk)\u2212 log det(Vk\u22121) > log 2}, i.e., Nbad = |\u03a8|, we have\u2211\ni \u2211 t\u2208bad epochs rt,i = \u2211 i \u2211 k\u2208\u03a8 Rki = O (|\u03a8|MU) .\nPlugging in Nbad = |\u03a8| \u2264 dlog 2 log ( 1 + MTd\u03bb ) , we have the total regret for bad epochs. Now, we are only left to find U . Here is the place where the adaptive schedule in the algorithm comes in. First, note that \u2211\ntk\u22121<t<tk\nrt,i (a) \u2264 \u2211\ntk\u22121<t<tk\nmin{2\u03b2T \u2225xt,i\u2225V \u22121t,i , 1} (8)\n(b) \u2264 O ( \u03b2T \u221a (tk \u2212 tk\u22121) log\ndetVtk,i detVlast\n) (9)\n(c) \u2264 O ( \u03b2T \u221a D ) ,\nwhere (a) holds by boundedness of reward; (b) follows from the elliptical potential lemma, i.e., Vlast is PSD under event E and Vt,i = Vt\u22121,i + xt\u22121,ix\u22a4t\u22121,i for all t \u2208 (tk\u22121, tk); (c) holds by the adaptive schedule in line 9 of Algorithm 3. As a result, we have Rki \u2264 O ( \u03b2T \u221a D ) + 1, where the regret at\nround tk is at most 1 by the boundedness of reward. With a proper choice of D, one can obtain a final regret bound."
        },
        {
            "heading": "D.2 CHALLENGES IN REGRET ANALYSIS UNDER PRIVATE ADAPTIVE SCHEDULE",
            "text": "Now, we will discuss new challenges when one uses private data for an adaptive communication schedule. In this case, one needs to first privatize the new local gram matrices (e.g., \u2211t s=tlast+1 xs,ix \u22a4 s,i) before being used in the determinant condition. This can be done by using standard tree-based algorithm with each data point as xs,ix\u22a4s,i. With this additional step, now the determinant condition becomes\nlog det(V\u0303t,i)\u2212 log det(Vlast) > D\nt\u2212 tlast , (10) where V\u0303t,i := Vlast + \u2211t\ns=tlast+1 xs,ix \u22a4 s,i +N loc t,i and N loc t,i is the new local injected noise for private\nschedule up to time t. Now suppose one uses (10) to determine tk. Then, it does not imply that (9) is upper bounded by \u03b2T \u221a D. That is, det(V\u0303t,i)det(Vlast) \u2264 D\n\u2032 does not necessarily mean that det(Vlast+ \u2211t s=tlast+1 xs,ix \u22a4 s,i)\ndet(Vlast) \u2264 D\u2032.\nOne may try to work around (8) by first using Gt,i + \u03bbminI to lower bound Vt,i. Then, (9) becomes\nO ( \u03b2T \u221a (tk \u2212 tk\u22121) log det(Gtk,i+\u03bbminI)\ndet(Gtk\u22121,i+\u03bbminI)\n) , which again cannot be bouded based on the rule given\nby (10). To see this, note that det(V\u0303tk\u22121,i)det(Vlast) \u2264 D \u2032 only implies that det(Gtk,i+\u03bbminI)det(Gtk\u22121,i+\u03bbmaxI) \u2264 D\u2032."
        },
        {
            "heading": "E ADDITIONAL DETAILS ON FEDERATED LCBS UNDER SILO-LEVEL LDP",
            "text": "In this section, we provide details for Section 5.1. In particular, we present the proof for Theorem 5.1 and the alternative privacy protocol for silo-level LDP."
        },
        {
            "heading": "E.1 PROOF OF THEOREM 5.1",
            "text": "Proof of Theorem 5.1. Privacy. We only need to show that P in Algorithm 2 with a proper choice of \u03c30 satisfies (\u03b5, \u03b4)-DP for all k \u2208 [K], which implies that the full transcript of the communication is private in Algorithm 1 for any local agent i.\nFirst, we recall that the (multi-variate) Gaussian mechanism satisfies zero-concentrated differential privacy (zCDP) (Bun & Steinke, 2016). In particular, by Bun & Steinke (2016, Lemma 2.5), we have that computation of each node (p-sum) in the tree is \u03c1-zCDP with \u03c1 = L 2\n2\u03c320 . Then, from the\nconstruction of the binary tree in P , one can easily see that one single data point \u03b3i (for all i \u2208 [K]) only impacts at most 1 + log(K) nodes. Thus, by adaptive composition of zCDP (cf. Lemma 2.3 in Bun & Steinke (2016)), we have that the entire releasing of all p-sums is (1 + logK)\u03c1-zCDP. Finally, we will use the conversion lemma from zCDP to approximated DP (cf. Proposition 1.3 in Bun & Steinke (2016)). In particular, we have that \u03c10-zCDP implies (\u03b5 = \u03c10 + 2 \u221a \u03c10 \u00b7 log(1/\u03b4), \u03b4)-DP for all \u03b4 > 0. In other words, to achieve a given (\u03b5, \u03b4)-DP, it suffices to achieve \u03c10-zCDP with \u03c10 = f(\u03b5, \u03b4) := ( \u221a log(1/\u03b4) + \u03b5 \u2212 \u221a log(1/\u03b4))2. In our case, we have \u03c10 = (1 + log(K))\u03c1 = (1 + log(K)) L 2\n2\u03c320 . Thus, we have \u03c320 = (1 + log(K))\nL2 2\u03c10 = (1 + log(K)) L 2 2f(\u03b5,\u03b4) . To simply it, one\ncan lower bound f(\u03b5, \u03b4) by \u03b5 2\n4 log(1/\u03b4)+4\u03b5 (cf. Remark 15 in Steinke (2022)). Therefore, to obtain\n(\u03b5, \u03b4)-DP, it suffices to set \u03c320 = 2 \u00b7 L2 \u00b7 (1+log(K))(log(1/\u03b4)+\u03b5)\n\u03b52 . Note that there are two streams of data in Algorithm 1, and hence it suffices to ensure that each of them is (\u03b5/2, \u03b4/2)-DP. This gives us the final noise level \u03c320 = 8 (1+log(K))(log(2/\u03b4)+\u03b5) \u03b52 (note that by boundedness assumption L = 1 in our case).\nRegret. In order to establish the regret bound, thanks to Lemma C.4, we only need to determine the maximum noise level in the learning process. Recall that \u03c320 = 8 \u00b7 (1+log(K))(log(2/\u03b4)+\u03b5) \u03b52 is the noise level for both streams (i.e., \u03b3bias and \u03b3cov). Now, by the construction of binary tree in P , one can see that each prefix sum \u2211 [1, k] only involves at most 1 + log(k) tree nodes. Thus, we have that the noise level in nt,i and Nt,i are upper bounded by (1 + log(K))\u03c320 . As a result, the overall noise level across all M silos is upper bounded by \u03c32total = M(1 + log(K))\u03c3 2 0 . Finally, setting \u03c3\n2 in Lemma C.4 to be the noise level \u03c32total , yields the required result."
        },
        {
            "heading": "E.2 ALTERNATIVE PRIVACY PROTOCOL FOR SILO-LEVEL LDP",
            "text": "For silo-level LDP, each local randomizer can simply be the standard tree-based algorithm, i.e., releasing the prefix sum at each communication step k (rather than p-sum in Algorithm 2). The analyzer now becomes a simple aggregation. As before, no shuffler is required in this case. This alternative protocol is given by Algorithm 4, which is essentially the main protocol used in Dubey & Pentland (2020).\nIt can be seen that both privacy and regret guarantees under this Palt are the same as Theorem 5.1. To see this, for privacy, the prefix sum is a post-processing of the p-sums. Thus, since we have already shown that the entire releasing of p-sums is private in the proof of Theorem 5.1, hence the same as the prefix sum. Meanwhile, the total noise level at the server is the same as before. Thus, by Lemma C.4, we have the same regret bound."
        },
        {
            "heading": "F ADDITIONAL DETAILS ON FEDERATED LCBS UNDER SDP",
            "text": "In this section, we provide more detailed discussions on SDP and present the proof for Theorem 5.3 (SDP via amplification lemma) and Theorem 5.5 (SDP via vector sum).\nFirst, let us start with some general discussions.\nImportance of communicating P-sums. For SDP, it is important to communicate P-sums rather than prefix sum. Note that communicating noisy p-sums in our privacy protocol P rather than the noisy prefix sum (i.e., the sum from beginning as done in Dubey & Pentland (2020)) plays a key role in achieving optimal regret with shuffling. To see this, both approaches can guarantee silo-level LDP. By our new amplification lemma, privacy guarantee can be amplified by 1/ \u221a M in \u03b5 for each\nAlgorithm 4 Palt, an alternative privacy protocol for silo-level LDP 1: Procedure: Local RandomizerR 2: // Input: stream data \u03b3 = (\u03b3i)i\u2208[K]; privacy parameters \u03b5, \u03b4; Output: private prefix\nsum 3: for k=1, . . . ,K do 4: Express k in binary form: k = \u2211 j Binj(k) \u00b7 2j 5: Find the index of first one ik := min{j : Binj(k) = 1} 6: Compute p-sum \u03b1ik = \u2211 j<i \u03b1j + \u03b3k. 7: Add noise to p-sum \u03b1\u0302ik = \u03b1ik +N (0, \u03c320I) 8: Output private prefix sum s\u0303k = \u2211 j:Binj(k)=1 \u03b1\u0302j\n9: end for 10: end procedure 11: Procedure: Analyzer A 12: // Input: a collection of M data points, y = {yi}i\u2208[M ]; Output: Aggregated sum 13: Output y\u0303 = \u2211 i\u2208[M ] yi 14: end procedure\nof the K shuffled outputs, where K = T/B is total communication rounds. Now, if the prefix sum is released to the shuffler, then any single data point participates in at most K shuffle mechanisms, which would blow up \u03b5 by a factor of O( \u221a K) (by advanced composition (Dwork & Roth, 2014)). This would eventually lead to a K1/4 factor blow up in regret due to privacy. Similarly, if we apply PVec to the data points in the prefix sum, then again a single data point can participate in at most K shuffled outputs.\nOn the other hand, if only noisy p-sums are released for shuffling at each communication round k \u2208 [K] (as in our protocol P) or only the data points in each p-sum are used in PVec (as in our protocol in PTVec), then due to the binary-tree structure, each data point only participates in at most logK shuffled mechanisms, which only leads to O( \u221a logK) blow-up of \u03b5; hence allowing us to achieve the desired O\u0303( \u221a MT ) regret scaling, and close the gap present under silo-level LDP.\nRemark F.1 (Shuffled tree-based mechanism). Both the protocol P in Algorithm 2 along with our new amplification lemma and protocol PTVec in Algorithm 5 can be treated as a black-box method, which integrates shuffling into the tree-based mechanism while providing formal guarantees for continual release of sum statistics. Hence, it can be applied to other federated online learning problems beyond contextual bandits."
        },
        {
            "heading": "F.1 AMPLIFICATION LEMMA FOR SDP",
            "text": "We first formally introduce our new amplification lemma, which is the key to our analysis, as mentioned in the main paper.\nThe motivation for our new amplification result is two-fold: (i) Existing results on privacy amplification via shuffling (e.g., Feldman et al. (2022); Erlingsson et al. (2019); Cheu et al. (2019); Balle et al. (2019)) are only limited to the standard LDP case, i.e., each local dataset has size n = 1, which is not applicable in our case where each silo runs a DP (rather than LDP) mechanism over a dataset of size n = T ; (ii) Although a recent work (Lowy & Razaviyayn, 2021) establishes a general amplification result for the case of n > 1, it introduces a very large value for the final \u03b4 that scales linearly with n due to group privacy.\nWe first present the key intuition behind our new lemma. Essentially, as in Lowy & Razaviyayn (2021), we follow the nice idea of hiding among the clones introduced in Feldman et al. (2022). That is, the output from silo 2 to n can be similar to that of silo 1 by the property of DP (i.e., creating clones). The key difference between n = 1 and n > 1 is that in the latter case, the similarity distance between the output of silo 1 and j (j > 1) will be larger as in this case all n > 1 data points among two silos could be different. To capture this, Lowy & Razaviyayn (2021) resorts to group privacy for\ngeneral DP local randomizers.6 However, group privacy for approximate DP will introduce a large value for \u03b4. Thus, since we know that each local randomizer in our case is the Gaussian mechanism, we can capture the similarity of outputs between silo 1 and j (j > 1) by directly bounding the sensitivity. This helps to avoid the large value for the final \u03b4. Specifically, we have the following result, which can be viewed as a refinement of Theorem D.5 in Lowy & Razaviyayn (2021) when specified to the Gaussian mechanism. We follow the notations in Lowy & Razaviyayn (2021) for easy comparison.\nLemma F.2 (Amplification lemma for Gaussian mechanism). Let X = (X1, \u00b7 \u00b7 \u00b7 , XN ) \u2208 XN\u00d7n be a distributed data set, i.e., N silos each with n data points. Let r \u2208 N and let R(i)r (Z, \u00b7) : Xn \u2192 Z := Rd be a Gaussian mechanism with (\u03b5r0, \u03b4r0)DP, \u03b5r0 \u2208 (0, 1)7, for all Z = Z (1:N) (1:r\u22121) \u2208 Z (r\u22121)\u00d7N and i \u2208 [N ], where X is\nan arbitrary set. Suppose for all i, maxany pair(X,X\u2032) \u2225\u2225\u2225R(i)r (Z, X)\u2212R(i)r (Z, X \u2032)\u2225\u2225\u2225 \u2264 n \u00b7\nmaxadjacent pair(X,X\u2032) \u2225\u2225\u2225R(i)r (Z, X)\u2212R(i)r (Z, X \u2032)\u2225\u2225\u2225.8 Given Z = Z(1:N)(1:r\u22121), consider the shuffled algorithm Ars : Xn\u00d7N \u00d7 Z(r\u22121)\u00d7N \u2192 ZN that first samples a random permutation \u03c0 of [N ] and then computes Zr = (Z (1) r , \u00b7 \u00b7 \u00b7 , Z(N)r ), where Z(i)r := R(i)r (Z, X\u03c0(i)). Then, for any \u03b4 \u2208 [0, 1] such\nthat \u03b5r0 \u2264 1n ln ( N 16 log(2/\u03b4) ) , Ars is (\u03b5r, \u03b4r)-DP, where\n\u03b5r := ln [ 1 + ( e\u03b5 r 0 \u2212 1\ne\u03b5 r 0 + 1\n)( 8 \u221a en\u03b5\nr 0 log(4/\u03b4)\u221a N + 8en\u03b5 r 0 N\n)]\n\u03b4r :=\n( e\u03b5 r 0 \u2212 1\ne\u03b5 r 0 + 1\n) \u03b4 +N(e\u03b5 r\n+ 1)(1 + e\u2212\u03b5 r 0/2)\u03b4r0.\nIf \u03b5r0 \u2264 1/n, choosing \u03b4 = Nn\u03b4r0 yields \u03b5r = O ( \u03b5r0 \u221a log(1/(nN\u03b4r0))\u221a N ) and \u03b4r = O(N\u03b4r0), where\n\u03b4r0 \u2264 1/(Nn).\nF.2 VECTOR SUM PROTOCOL FOR SDP\nOne limitation of our first scheme for SDP is that the privacy guarantee holds only for very small values of \u03b5. This comes from two factors: one is due to the fact that standard 1/ \u221a M amplification result requires the local privacy budget to be close to one; the other one comes from the fact that now the local dataset could be n = T , which further reduces the range of valid \u03b5.\nIn this section, we give the vector sum protocol in Cheu et al. (2021) for easy reference. Let\u2019s also give a concrete example to illustrate how to combine Algorithm 6 with Algorithm 5. Consider a fixed k = 6. Then, for each agent, we have \u03b1i6 = \u03b35 + \u03b36. That is, consider the case of summing bias vectors, for agent i \u2208 [M ], \u03b35 = \u22115B t=4B+1 xt,iyt,i and \u03b36 = \u22116B t=5B+1 xt,iyt,i. Then, D6 consists of 2B data points, each of which is a single bias vector. Now, Rvec and Avec (as well the shuffler) work together to compute the noisy sum of 2B \u00b7M data points. In particular, denote by Pvec the whole process, then we have \u03b1\u0303i6 = Pvec(DM6 ), where DM6 is the data set that consists of n = 2B \u00b7M data points, each of them is a single bias vector.\nNext, we present more details on the implementations, i.e., the parameter choices of g, b, p. Let\u2019s consider k = 6 again as an example. In this case, the total number of data points that participate in Pvec is n = 2B \u00b7M . Then, according to the proof of Theorem C.1 in Chowdhury & Zhou (2022b),\n6This is because it mainly focuses on the lower bound, where one needs to be general to handle any mechanisms.\n7Note that standard Gaussian mechanism only applies to the regime when \u03b5 < 1. In our case, \u03b5r0 is often less than 1. Gaussian mechanism also works for the regime \u03b5 > 1, in this case, \u03c32 \u2248 1/\u03b5 rather than 1/\u03b52. With minor adjustment of the final \u03b5r , our proof can be extended.\n8This is w.l.o.g; one can easily generalize it to any upper bound that is a function of n. 9In our application, each data point means a bias vector or a covariance matrix. See Appendix F.2 for a\nconcrete example.\nAlgorithm 5 PTVec, another privacy protocol used in Algorithm 1 1: Procedure: Local RandomizerR at each agent 2: // Input: stream data (\u03b31, . . . , \u03b3K), privacy budgets \u03b5 > 0, \u03b4 \u2208 (0, 1] 3: for k=1, . . . ,K do 4: Express k in binary form: k = \u2211 j Binj(k) \u00b7 2j\n5: Find index of first one ik=min{j : Binj(k)=1} 6: Let Dk be the set of all data points9that contribute to \u03b1ik = \u2211 j<ik\n\u03b1j + \u03b3k 7: Output yk = RVec(Dk) // apply RVec in Algorithm 6 to each data point 8: end for 9: Procedure: Analyzer A at server\n10: // Input: stream data from S: {y\u0304k = (y\u0304k,1, . . . , y\u0304k,M )}k\u2208[K] 11: for k=1, . . . ,K do 12: Express k in binary and find index of first one ik 13: Add all messages from M agents: \u03b1\u0303ik = AVec(y\u0304k) // apply AVec in Algorithm 6 14: Output: s\u0303k = \u2211 j:Binj(k)=1 \u03b1\u0303j 15: end for\nwe have\ng = max{2 \u221a n, d, 4}, b =\n24 \u00b7 104 \u00b7 g2 \u00b7 ( log (\n4\u00b7(d2+1) \u03b4 ))2 \u03b52n , p = 1/4.\nAlgorithm 6 Pvec, a shuffle protocol for vector summation (Cheu et al., 2021) 1: Input: Database of d-dimensional vectors X = (x1, \u00b7 \u00b7 \u00b7 ,xn); privacy parameters \u03b5, \u03b4; L. 2: procedure: Local Randomizer Rvec(xi) 3: for j \u2208 [d] do 4: Shift component to enforce non-negativity: wi,j \u2190 xi,j + L 5: mj \u2190 R1D(wi,j) 6: end for 7: Output labeled messages {(j,mj)}j\u2208[d] 8: end procedure 9: procedure: Analyzer Avec(y) 10: for j \u2208 [d] do 11: Run analyzer on coordinate j\u2019s messages zj \u2190 A1D(yj) 12: Re-center: oj \u2190 zj \u2212 n \u00b7 L 13: end for 14: Output the vector of estimates o = (o1, \u00b7 \u00b7 \u00b7 od) 15: end procedure\nAlgorithm 7 P1D, a shuffle protocol for summing scalars (Cheu et al., 2021) 1: Input: Scalar database X = (x1, \u00b7 \u00b7 \u00b7xn) \u2208 [0, L]n; g, b \u2208 N; p \u2208 (0, 12 ). 2: procedure: Local RandomizerR1D(xi) 3: x\u0304i \u2190 \u230axig/L\u230b. 4: Sample rounding value \u03b71 \u223c Ber(xig/L\u2212 x\u0304i). 5: Set x\u0302i \u2190 x\u0304i + \u03b71. 6: Sample privacy noise value \u03b72 \u223c Bin(b, p). 7: Report yi \u2208 {0, 1}g+b containing x\u0302i + \u03b72 copies of 1 and g + b\u2212 (x\u0302i + \u03b72) copies of 0. 8: end procedure 9: procedure: Analyzer A1D(S(y1, . . . ,yn))\n10: Output estimator Lg (( \u2211n i=1 \u2211b+g j=1(yi)j)\u2212 pbn). 11: end procedure"
        },
        {
            "heading": "F.3 PROOFS",
            "text": "First, we present proof of Theorem 5.3.\nProof of Theorem 5.3. Privacy. In this proof, we directly work on approximate DP. By the boundedness assumption and Gaussian mechanism, we have that with \u03c320 =\n2L2 log(1.25/\u03b4\u03020) \u03b5\u030220\n, R in P is (\u03b5\u03020, \u03b4\u03020)-DP for each communication round k \u2208 [K] (provided \u03b5\u03020 \u2264 1) . Now, by our amplification\nlemma (Lemma F.2), we have that the shuffled output is (\u03b5\u0302, \u03b4\u0302)-DP with \u03b5\u0302 = O ( \u03b5\u03020 \u221a\nlog(1/(TM\u03b4\u03020))\u221a M ) and \u03b4\u0302 = O(M\u03b4\u03020) (provided \u03b5\u03020 \u2264 1/T and \u03b4\u03020 \u2264 1/(MT )). Here we note that in our case, N = M and n = T , where n = T follows from the fact that there exists \u03b1i in the tree that corresponds to the sum of T data points. Moreover, since the same mechanism is run at all silos, shuffling-thenprivatizing is the same as first privatizing-then-shuffling the outputs. Next, we apply the advanced composition theorem (cf. Theorem 3.20 in Dwork & Roth (2014)). In particular, by the binary tree structure, each data point involves only \u03ba := 1 + log(K) times in the output ofR. Thus, to achieve (\u03b5, \u03b4)-DP, it suffices to have \u03b5\u0302 = \u03b5\n2 \u221a 2\u03ba log(2/\u03b4) and \u03b4\u0302 = \u03b42\u03ba . Using all these equations, we can solve\nfor \u03b5\u03020 = C1 \u00b7 \u03b5 \u221a M\u221a\n\u03ba log(1/\u03b4) log(\u03ba/(\u03b4T )) and \u03b4\u03020 = C2 \u00b7 \u03b4M\u03ba , for some constants C1 > 0 and C2 > 0. To\nsatisfy the conditions on \u03b5\u03020 and \u03b4\u03020, we have \u03b5 \u2264 \u221a \u03ba\nC1T \u221a M and \u03b4 \u2264 \u03baC2T . With the choice of \u03b5\u03020 and \u03b4\u03020, we have the noise variance \u03c320 = O ( 2L2\u03b2 log(1/\u03b4) log(\u03ba/(\u03b4T )) log(M\u03ba/\u03b4) \u03b52M ) . Thus, we can apply P to the bias and covariance terms (with L = 1), respectively.\nRegret. Again, we simply resort to our Lemma C.4 for the regret analysis. In particular, we only need to determine the maximum noise level in the learning process. Note that \u03c320 = O (\n2L2\u03ba log(1/\u03b4) log(\u03ba/(\u03b4T )) log(M\u03ba/\u03b4) \u03b52M\n) is the noise level injected for both bias and covariance terms.\nNow, by the construction of the binary tree in P , one can see that each prefix sum only involves at most 1 + log(k) tree nodes. As a result, the overall noise level across all M silos is upper bounded by \u03c32total = M\u03ba\u03c3 2 0 . Finally, setting \u03c3\n2 in Lemma C.4 to be the noise level \u03c32total , yields the required result.\nNow, we prove Theorem 5.5.\nProof of Theorem 5.5. Privacy. For each calculation of the noisy synchronized p-sum, there exist parameters for PVec such that it satisfies (\u03b50, \u03b40)-SDP where \u03b50 \u2208 (0, 15] and \u03b40 \u2208 (0, 1/2) (see Lemma 3.1 in Cheu et al. (2021) or Theorem 3.5 in Chowdhury & Zhou (2022b)). Then, by the binary tree structure, each single data point (bias vector or covariance matrix) only participates in at most \u03ba := 1+ log(K) runs of PVec. Thus, to achieve (\u03b5, \u03b4)-DP, it suffices to have \u03b50 = \u03b5\n2 \u221a 2\u03ba log(2/\u03b4) and \u03b40 = \u03b42\u03ba by advanced composition theorem. Thus, for any \u03b5 \u2208 (0, 30 \u221a 2\u03ba log(2/\u03b4)) and \u03b4 \u2208 (0, 1), there exist parameters for PVec such that the entire calculations of noisy p-sums are (\u03b5, \u03b4)-SDP. Since we have two streams of data (bias and covariance), we finally have that for any \u03b5 \u2208 (0, 60 \u221a 2\u03ba log(2/\u03b4)) and \u03b4 \u2208 (0, 1), there exist parameters for PVec such that Algorithm 1 with PTVec satisfies (\u03b5, \u03b4)-SDP. Regret. By the same analysis in the proof of Theorem 3.5 in Chowdhury & Zhou (2022b), the injected noise for each calculation of the noisy synchronized p-sum is sub-Gaussian with the variance being at most \u03c3\u03022 = O ( log2(d2/\u03b40)\n\u03b520\n) = O ( \u03ba log(1/\u03b4) log2(d2\u03ba/\u03b4)\n\u03b52\n) . Now, by the binary tree structure,\neach prefix sum only involves at most \u03ba p-sums. Hence, the overall noise level is upper bounded by \u03c32total = \u03ba\u03c3\u0302\n2. Finally, setting \u03c32 in Lemma C.4 to be the noise level \u03c32total , yields the required result.\nNow, we provide proof of amplification Lemma F.2 for completeness. We follow the same idea as in Feldman et al. (2022) and Lowy & Razaviyayn (2021). For easy comparison, we use the same notations as in Lowy & Razaviyayn (2021) and highlighted the key difference using color text.\nProof of Lemma F.2. Let X0,X1 \u2208 Xn\u00d7N be adjacent distributed data sets (i.e.\u2211N i=1 \u2211n j=1 1{xi,j \u0338=xi,j} = 1). Assume WLOG that X0 = (X 0 1 , X2, \u00b7 \u00b7 \u00b7 , XN ) and X1 = (X 1 1 , X2, \u00b7 \u00b7 \u00b7 , XN ), where X01 = (x1,0, x1,2, \u00b7 \u00b7 \u00b7 , x1,n) \u0338= (x1,1, x1,2, \u00b7 \u00b7 \u00b7 , x1,n). We can also assume WLOG that Xj /\u2208 {X01 , X11} for all j \u2208 {2, \u00b7 \u00b7 \u00b7 , N} by re-defining X andR (i) r if necessary.\nFix i \u2208 [N ], r \u2208 [R],Z = Z1:r\u22121 = Z(1:N)(1:r\u22121) \u2208 Z (r\u22121)\u00d7N , denote R(X) := R(i)r (Z, X) for X \u2208 Xn, and As(X) := Ars(Z1:r\u22121,X). Draw \u03c0 uniformly from the set of permutations of [N ]. Now, sinceR is (\u03b5r0, \u03b4r0)-DP,R(X11 ) \u2243\n(\u03b5r0,\u03b4 r 0) R(X01 ), so by Lowy & Razaviyayn (2021, Lemma D.12),\nthere exists a local randomizerR\u2032 such thatR\u2032(X11 ) \u2243 (\u03b5r0,0) R(X01 ) and TV (R\u2032(X11 ),R(X11 )) \u2a7d \u03b4r0.\nHence, by Lowy & Razaviyayn (2021, Lemma D.8), there exist distributions U(X01 ) and U(X 1 1 ) such that\nR(X01 ) = e\u03b5\nr 0\ne\u03b5 r 0 + 1\nU(X01 ) + 1\ne\u03b5 r 0 + 1\nU(X11 ) (11)\nand\nR\u2032(X11 ) = 1\ne\u03b5 r 0 + 1\nU(X01 ) + e\u03b5\nr 0\ne\u03b5 r 0 + 1\nU(X11 ). (12)\nHere, we diverge from the proof in (Lowy & Razaviyayn, 2021). We denote \u03b5\u03030 := n\u03b5r0 and \u03b4\u03030 := \u03b4 r 0. Then, by the assumption of R(X), for any X , we have R(X) \u2243\n(\u03b5\u03030,\u03b4\u03030) R(X01 )) and\nR(X) \u2243 (\u03b5\u03030,\u03b4\u03030) R(X11 )). This is because by the assumption, when the dataset changes from any X to X01 (or X 1 1 ), the total change in terms of l2 norm can be n times that under an adjacent pair. Thus, one has to scale the \u03b5r0 by n while keeping the same \u03b4 r 0 .\nNow, we resume the same idea as in (Lowy & Razaviyayn, 2021). By convexity of hockey-stick divergence and the above result, we haveR(X) \u2243\n(\u03b5\u03030,\u03b4\u03030)\n1 2 (R(X 0 1 ) +R(X11 )) := \u03c1 for all X \u2208 Xn.\nThat is, R is (\u03b5\u03030, \u03b4\u03030) deletion group DP for groups of size n with reference distribution \u03c1. Thus, by Lowy & Razaviyayn (2021, Lemma D.11), we have that there exists a local randomizerR\u2032\u2032 such thatR\u2032\u2032(X) and \u03c1 are (\u03b5\u03030, 0) indistinguishable and TV (R\u2032\u2032(X),R(X)) \u2a7d \u03b4\u03030 for all X. Then by the definition of (\u03b5\u03030, 0) indistinguishability, for all X there exists a \u201cleft-over\u201d distribution LO(X) such thatR\u2032\u2032(X) = 1\ne\u03b5\u03030 \u03c1+ (1\u2212 1/e\u03b5\u03030)LO(X) = 1 2e\u03b5\u03030 (R(X01 ) +R(X11 )) + (1\u2212 1/e\u03b5\u03030)LO(X).\nNow, define a randomizer L by L(X01 ) := R(X01 ), L(X11 ) := R\u2032(X11 ), and\nL(X) := 1 2e\u03b5\u03030 R(X01 ) + 1 2e\u03b5\u03030 R\u2032(X11 ) + (1\u2212 1/e\u03b5\u03030)LO(X)\n= 1\n2e\u03b5\u03030 U(X01 ) +\n1\n2e\u03b5\u03030 U(X11 ) + (1\u2212 1/e\u03b5\u03030)LO(X) (13)\nfor all X \u2208 Xn \\ {X01 , X11}. (The equality follows from (11) and (12).) Note that TV (R(X01 ),L(X01 )) = 0, TV (R(X11 ),L(X11 )) \u2a7d \u03b4r0, and for all X \u2208 Xn \\ {X01 , X11}, TV (R(X),L(X)) \u2a7d TV (R(X),R\u2032\u2032(X)) + TV (R\u2032\u2032(X),L(X)) \u2a7d \u03b4\u03030 + 1\n2e\u03b5\u03030 TV (R\u2032(X11 ),R(X11 )) = (1 + 12en\u03b5r0 )\u03b4 r 0 .\nKeeping r fixed (omitting r scripts everywhere), for any i \u2208 [N ] and Z := Z1:r\u22121 \u2208 Z(r\u22121)\u00d7N , let L(i)(Z, \u00b7), U (i)(Z, \u00b7), and LO(i)(Z, \u00b7) denote the randomizers resulting from the process described above. Let AL : Xn\u00d7N \u2192 ZN be defined exactly the same way as Ars := As (same \u03c0) but with the randomizers R(i) replaced by L(i). Since As applies each randomizer R(i) exactly once and R(1)(Z, X\u03c0(1), \u00b7 \u00b7 \u00b7R(N)(Z, X\u03c0(N)) are independent (conditional on Z = Z1:r\u22121) 10, we have TV (As(X0),AL(X0)) \u2a7d N(1+ 1\n2en\u03b5 r 0 )\u03b4r0 and TV (As(X1),AL(X1) \u2a7d N(1+ 12en\u03b5r0 )\u03b4 r 0 . Now we\n10This follows from the assumption that R(i)(Z1:r\u22121, X) is conditionally independent of X \u2032 given Z1:r\u22121 for all Z1:r\u22121 and X \u0338= X \u2032.\nclaim that AL(X0) and AL(X1) are (\u03b5r, \u03b4) indistinguishable for any \u03b4 \u2a7e 2e\u2212Ne \u2212n\u03b5r0/16. Observe that this claim implies that As(X0) and As(X1) are (\u03b5r, \u03b4r) indistinguishable by Lowy & Razaviyayn (2021, Lemma D.13) (with P \u2032 := AL(X0), Q\u2032 := AL(X1), P := As(X0), Q := As(X1).) Therefore, it only remains to prove the claim, i.e. to show that De\u03b5r (AL(X0),AL(X1) \u2a7d \u03b4 for any \u03b4 \u2a7e 2e\u2212Ne \u2212n\u03b5r0/16.\nNow, define L(i)U (Z, X) :=  U (i)(Z, X01 ) if X = X 0 1 U (i)(Z, X11 ) if X = X 1 1\nL(i)(Z, X) otherwise. . For any inputs Z,X, let AU (Z,X) be\ndefined exactly the same asAs(Z,X) (same \u03c0) but with the randomizersR(i) replaced by L(i)U . Then by (11) and (12),\nAL(X0) = e\u03b5\nr 0\ne\u03b5 r 0 + 1\nAU (X0)+ 1\ne\u03b5 r 0 + 1\nAU (X1) andAL(X1) = 1\ne\u03b5 r 0 + 1\nAU (X0)+ e\u03b5\nr 0\ne\u03b5 r 0 + 1 AU (X1). (14)\nThen by (13), for any X \u2208 Xn\\{X01 , X11} and any Z = Z1:r\u22121 \u2208 Z(r\u22121)\u00d7N , we haveL (i) U (Z, X) =\n1 2e\u03b5\u03030 L(i)U (Z, X01 ) + 1 2e\u03b5\u03030 L(i)U (Z, X11 ) + (1\u2212 e\u2212\u03b5\u03030)LO(i)(Z, X). Hence, Lowy & Razaviyayn (2021, Lemma D.10) (with p := e\u2212\u03b5\u03030 = e\u2212n\u03b5 r 0 ) implies that AU (X0) and AU (X1)) are(\nlog ( 1 + 8 \u221a\ne\u03b5\u03030 ln(4/\u03b4)\u221a N + 8e\u03b5\u03030 N\n) , \u03b4 ) indistinguishable for any \u03b4 \u2a7e 2e\u2212Ne \u2212n\u03b5r0/16.\nHere, we also slightly diverge from Lowy & Razaviyayn (2021). Instead of using Lowy & Razaviyayn (2021, Lemma D.14), we can directly follow the proof of Lemma 3.5 in Feldman et al. (2022) and Lemma 2.3 in Feldman et al. (2022) to establish our claim that AL(X0) and AL(X1) are indistinguishable (hence the final result). Here, we also slightly improve the \u03b4 term compared to Feldman et al. (2022) by applying amplification via sub-sampling to the \u03b4 term as well. In particular, the key step is to rewrite (14) as follows (with T := 12 (AU (X0) +AU (X1))\nAL(X0) = 2\ne\u03b5 r 0 + 1\nT + e\u03b5\nr 0 \u2212 1 e\u03b5 r 0 + 1 AU (X0) and AL(X1) = 2 e\u03b5 r 0 + 1 T + e\u03b5 r 0 \u2212 1 e\u03b5 r 0 + 1 AU (X1). (15)\nThus, by the convexity of the hockey-stick divergence and Lemma 2.3 in Feldman et al. (2022), we have AL(X0) and AL(X1) are(\nlog ( 1 +\n\u03b5r0 \u2212 1 \u03b5r0 + 1\n( 8 \u221a\ne\u03b5\u03030 ln(4/\u03b4r)\u221a N\n) + 8e\u03b5\u03030\nN ) , \u03b5r0 \u2212 1 \u03b5r0 + 1 \u03b4 ) indistinguishable for any \u03b4 \u2a7e 2e\u2212Ne\n\u2212n\u03b5r0/16. As decribed before, this leads to the result that As(X0) and As(X1) are (\u03b5r, \u03b4r) indistinguishable by Lowy & Razaviyayn (2021, Lemma D.13) (original result in Lemma 3.17 of Dwork & Roth (2014)) with (noting that \u03b5\u03030 = n\u03b5r0)\n\u03b5r := ln [ 1 + ( e\u03b5 r 0 \u2212 1\ne\u03b5 r 0 + 1\n)( 8 \u221a en\u03b5\nr 0 ln(4/\u03b4)\u221a N + 8en\u03b5 r 0 N\n)] ,\n\u03b4r :=\n( e\u03b5 r 0 \u2212 1\ne\u03b5 r 0 + 1\n) \u03b4 +N(e\u03b5 r\n+ 1)(1 + e\u2212\u03b5 r 0/2)\u03b4r0."
        },
        {
            "heading": "G FURTHER DISCUSSIONS",
            "text": "In this section, we provide more details on our upper bounds, privacy notion and algorithm design."
        },
        {
            "heading": "G.1 DISCUSSION ON TIGHTNESS OF UPPER BOUNDS",
            "text": "In the paper, we have established regrets of O(M3/4 \u221a T/\u03b5) under silo-level LDP and O( \u221a\nMT/\u03b5) under SDP. An essential open question is regarding tightness of these upper bounds. It turns out that\nthe key to obtain both lower bounds is to first establish a tight characterization for single-agent LCB under central JDP (which is still open to the best of our knowledge), as elaborated below:\nSDP: The current upper bound aligns with the state-of-the-art result achieved by a super single agent under central JDP. However, the tightness of this bound is still uncertain, as it even remains open whether the upper bound under the centralized setting is tight. To our best knowledge, the only existing lower bound for LCBs under central JDP is \u2126( \u221a T + 1/\u03b5) (He et al., 2022b), implying a lower bound of \u2126( \u221a MT + 1/\u03b5) for the super single agent case. That is, the privacy cost in the lower bound is only additive rather than multiplicative cost of 1/ \u221a \u03b5 present in the upper bound. It is unclear to us which one of the upper bound or lower bound is loose.\nSilo-level LDP: A lower bound can potentially be established using a similar reduction as in Lowy & Razaviyayn (2021), where the authors derive a lower bound for silo-level LDP supervised learning via the central model. To be more specific, for any silo-level LDP algorithmA with privacy guarantee \u03b50, one can first \u201dvirtually\u201d shuffle all the MT user sequences and then apply A, leading to a shuffled version As. As shown in Lowy & Razaviyayn (2021),, the shuffled version algorithm As enjoys an SDP privacy guarantee of roughly \u03b5 := \u03b50/ \u221a M (here again, one cannot directly use standard amplification lemma). Since SDP implies central JDP in our linear contextual bandit case, then one can conclude that As has a lower bound of Lc(\u03b5), where Lc(\u03b5) denotes the lower bound for LCB under central JDP with privacy guarantee \u03b5. Finally, one can note that A and As have the same regret performance, hence establishing the regret lower bound Lc(\u03b50/ \u221a M) for A under silo-level LDP.\nImplication: If one can establish a lower bound Lc(\u03b5) = \u2126( \u221a\nT/\u03b5) for standard LCB under central JDP (i.e., \u2126( \u221a MT/\u03b5) for the super single agent case), then by the above argument, it directly implies that our SDP upper bound is tight and moreover, the upper bound under silo-level LDP is also tight.\nIt is worth noting that our new findings in this paper (e.g., identifying existing gaps and establishing new upper bounds) motivate the above interesting questions, which we believe will promote advances in the field."
        },
        {
            "heading": "G.2 SILO-LEVEL LDP/SDP VS. OTHER PRIVACY NOTIONS",
            "text": "In this section, we compare our silo-level LDP and SDP with standard privacy notions for single-agent LCBs, including local, central, and shuffle model for DP, respectively.\nSilo-level LDP vs. single-agent local DP. Under standard LDP for single-agent LCBs (Zheng et al., 2020; Duchi et al., 2013; Zhou & Tan, 2021), each user only trusts herself and hence privatizes her response before sending it to the agent. In contrast, under silo-level LDP, each local user trusts the local silo (agent), which aligns with the pratical situations of cross-silo FL, e.g., patients often trust the local hospitals. In such cases, standard LDP becomes unnecessarily stringent, hindering performance/regret and making it less appealing to cross-silo federated LCBs.\nSilo-level LDP vs. single-agent central DP. The comparison with standard central DP (in particular central JDP)11 for single-agent LCB (e.g., Shariff & Sheffet (2018)) is delicate. We first note that under both notions, users trust the agent and the privacy burden lies at the agent. Under standard central DP, the agent uses private statistics until round t to choose action for each round t, which ensures that any other users t\u2032 \u0338= t cannot infer too much about user t\u2019s information by observing the actions on rounds t\u2032 \u0338= t (i.e., joint differential privacy (JDP) (Kearns et al., 2014)). On the other hand, silo-level LDP does not necessarily require each agent (silo) to use private statistics to recommend actions to users within the silo. Instead, it only requires the agent to privatize its sent messages (both schedule and content). Thus, silo-level LDP may not protect a user t from the colluding of all other users within the same silo. In other words, the adversary model for silo-level LDP is that the adversary could be any other silos or the central server rather than other users within the same silo. Note that the same adversary model is assumed in a similar notion for federated supervised learning (e.g., inter-silo record-level differential privacy (ISRL-DP) in Lowy & Razaviyayn (2021)). In fact, with a minor tweak of our Algorithm 1, one can achieve a slightly stronger notion of privacy than silo-level LDP in that it now can protect against both other silos/server and users within the same silo.\n11As shown in Shariff & Sheffet (2018), JDP relaxation is necessary for achieving sub-linear regret for LCBs under the central model. Otherwise, a linear regret lower bound exists for central standard DP.\nThe key idea is exactly that now each agent will only use private statistics to recommend actions, see Appendix G.3.\nSilo-level LDP vs. Federated DP in Dubey & Pentland (2020). In Dubey & Pentland (2020), the authors define the so-called notion of federated DP for federated LCBs, which essentially means that \u201cthe action chosen by any agent must be sufficiently impervious (in probability) to any single data from any other agent\u201d. This privacy guarantee is directly implied by our silo-level LDP. In fact, in order to show such a privacy guarantee, Dubey & Pentland (2020) basically tried to show that the outgoing communication is private, which is the idea of silo-level LDP. However, as mentioned in the main paper, Dubey & Pentland (2020) only privatizes the communicated data and fails to privatize the communication schedule, which leads to privacy leakage. Moreover, as already mentioned in Remark B.1, Fed-DP fails to protect a user\u2019s privacy even under a reasonable adversary model. Thus, we believe that silo-level LDP is a better option for federated LCBs.\nSDP vs. single-agent shuffle DP. Under the single-agent shuffle DP (Chowdhury & Zhou, 2022b; Tenenbaum et al., 2023), the shuffler takes as input a batch of users\u2019 data (i.e., from t1 to t2), which enables to achieve a regret of O\u0303(T 3/5) (vs. O\u0303(T 3/4) regret under local model and O\u0303( \u221a T ) regret under central model). In contrast, under our SDP, the shuffler takes as input the DP outputs from all M agents. Roughly speaking, single-agent shuffle DP aims to amplify the privacy dependence on T while our SDP amplifies privacy over M . Due to this, single-agent shuffle DP can directly apply a standard amplification lemma (e.g., Feldman et al. (2022)) or shuffle protocol (e.g., Cheu et al. (2021)) that works well with LDP mechanism at each user (i.e., the size of dataset is n = 1). In contrast, in order to realize amplification over M agents\u2019 DP outputs, we have to carefully modify the standard amplification lemma to handle the fact that now each local mechanism operates on n > 1 data points, which is one of the key motivations for our new amplification lemma.\nSublinear regret under SDP vs. linear regret lower bound under central standard DP (not JDP). One may wonder why an even better regret under the shuffle model is possible given that there is a linear regret bound under central model for LCBs. This is not contradicting as the lower bound of linear regret is established under standard central DP while SDP in LCBs only implies central JDP. More specifically, in contrast to standard private data analysis and supervised learning, the shuffle model is NOT an intermediate trust model between central standard DP and local DP for LCBs. That is, even if an LCB algorithm satisfies shuffle DP, it can still fail to satisfy central standard DP. Rather, it is only an intermediate trust model between the central joint DP (JDP) and the local model. That is, if an LCB algorithm satisfies shuffle DP, it satisfies central JDP (via Billboard lemma)."
        },
        {
            "heading": "G.3 A SIMPLE TWEAK OF ALGORITHM 1 FOR A STRONGER PRIVACY GUARANTEE",
            "text": "As discussed in the last subsection, the adversary model behind silo-level LDP only includes other silos and the central server, i.e., excluding adversary users within the same silo. Thus, for silo-level LDP, Algorithm 1 can use non-private data to recommend actions within a batch (e.g., Vt,i includes non-private recent local bias vectors and covariance matrices). If one is also interested in protecting against adversary users within the same silo, a simple tweak of Algorithm 1 suffices.\nAs shown in Algorithm 8, the only difference is a lazy update of \u03b8\u0302t,i is adopted (line 5), i.e., it is only computed using private data without any dependence on new non-private local data. In fact, same regret bound as in Theorem 5.1 can be achieved for this new algorithm (though empirical performance could be worse due to the lazy update). In the following, we highlight the key changes in the regret analysis. It basically follows the six steps in the proof of Lemma C.2. One can now define a mapping \u03ba(t) that maps any t \u2208 [T ] to the most recent communication round. That is, for any t \u2208 [tk\u22121, tk] where tk = kB is the k-th communication round, we have \u03ba(t) = tk\u22121. Then, one can replace all t in Vt,i and Gt,i by \u03ba(t). The main difference that needs a check is Step 4 when bounding the regret in good epochs. The key is again to establish a similar form as (5). To this end, note that for all t \u2208 [tk\u22121, tk] Vk \u2ab0 V\u0304t,i and G\u03ba(t),i + \u03bbminI = Vk\u22121, which enables us to obtain \u2225xt,i\u2225(G\u03ba(t),i+\u03bbminI)\u22121 \u2264 \u221a 2 \u2225xt,i\u2225V\u0304 \u22121t,i . Following the same analysis yields the desired regret bound.\nAlgorithm 8 Priv-FedLinUCB-Lazy 1: Parameters: Batch size B \u2208 N, regularization \u03bb > 0, confidence radii {\u03b2t,i}t\u2208[T ],i\u2208[M ], feature\nmap \u03d5i : Ci \u00d7Ki \u2192 Rd, privacy protocol P = (R,S,A) 2: Initialize: For all i \u2208 [M ], Wi = 0, Ui = 0, W\u0303syn = 0, U\u0303syn = 0 3: for t=1, . . . , T do 4: for each agent i = 1, . . . ,M do 5: Vt,i = \u03bbI + W\u0303syn, \u03b8\u0302t,i = V \u22121t,i U\u0303syn 6: Play arm at,i = argmaxa\u2208Ki\u27e8\u03d5i(ct,i, a), \u03b8\u0302t,i\u27e9 + \u03b2t,i \u2225\u03d5i(ct,i, a)\u2225V \u22121t,i and set xt,i = \u03d5i(ct,i, at,i) 7: Observe reward yt,i 8: Update Ui = Ui + xt,iyt,i and Wi = Wi + xt,ix\u22a4t,i 9: end for\n10: if tmod B = 0 then 11: // Local randomizer R at all agents i \u2208 [M ] 12: Send randomized messages Rbiast,i = Rbias(Ui) and Rcovt,j = Rcov(Wi) to the shuffler 13: // Third party S 14: Sbiast = S({Rbiast,i }i\u2208[M ]) and Scovt = S({Rcovt,i }i\u2208[M ]) 15: // Analyzer A at the server 16: Construct private cumulative statistics U\u0303syn = Abias(Sbiast ) and W\u0303syn = Acov(Scovt ) 17: // All agents i \u2208 [M ] 18: Receive W\u0303syn and U\u0303syn from the server 19: Reset Wi = 0, Ui = 0 20: end if 21: end for"
        },
        {
            "heading": "G.4 NON-UNIQUE USERS",
            "text": "In the main paper, we assume all users across all silos and T rounds are unique. Here, we briefly discuss how to handle the case of non-unique users.\n\u2022 The same user appears multiple times in the same silo. One example of this could be one patient visiting the same hospital multiple times. In such cases, one needs to carefully apply group privacy or other technique (e.g., Chowdhury & Zhou (2022b)) to characterize the privacy loss of these returning users.\n\u2022 The same user appears multiple times across different silos. One example of this could be one patient who has multiple records across different hospitals. Then, one needs to use adaptive advanced composition to characterize the privacy loss of these returning users."
        },
        {
            "heading": "H ADDITIONAL DETAILS ON SIMULATION RESULTS",
            "text": "In Figure 3, we compare regret performance of LDP-FedLinUCB with FedLinUCB under varying privacy budgets.12 In sub-figure (a), we plot results for \u03b4 = 0.1 and varying level of \u03b5 \u2208 {0.2, 1, 5} on synthetic Gaussian bandit instance, wherein sub-figure (b), we plot results for \u03b5 = 5 and varying level of \u03b4 \u2208 {0.1, 0.01, 0.001}. In sub-figure (c), we plot results for \u03b4 = 0.1 and varying level of \u03b5 \u2208 {0.2, 1, 5} on bandit instance generated from MSLR-WEB10K data by training a lasso model on bodyfeatures (d = 78). In all these plots, we observe that regret of LDP-FedLinUCB decreases and, comes closer to that of FedLinUCB as \u03b5, \u03b4 increases (i.e., level of privacy protection decreases), which support our theoretical results. Here, we don\u2019t compare SDP-LinUCB (with privacy amplification) since its privacy guarantee holds for \u03b5, \u03b4 \u226a 1. Instead, we do so in sub-figure (d) with \u03b5 = \u03b4 = 0.0001. Here also, we observe a drop in regret of SDP-FedLinUCB compared to that of LDP-FedLinUCB.\n12All existing non-private federated LCB algorithms (e.g., Wang et al. (2020)) adopts adaptive communication. We refrain from comparing with those to maintain consistency in presentation."
        }
    ],
    "title": "ON DIFFERENTIALLY PRIVATE FEDERATED LINEAR CONTEXTUAL BANDITS",
    "year": 2024
}