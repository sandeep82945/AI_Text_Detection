{
    "abstractText": "In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD target in Q-learning and acts as an additional incentive for desirable transitions. We provide theoretical support for the proposed incentive and demonstrate the effectiveness of EMU compared to conventional episodic control. The proposed method is evaluated in StarCraft II and Google Research Football, and empirical results indicate further performance improvement over state-of-the-art methods. Our code is available at: https://github.com/HyunghoNa/EMU.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hyungho Na"
        },
        {
            "affiliations": [],
            "name": "Yunkyeong Seo"
        },
        {
            "affiliations": [],
            "name": "Il-Chul Moon"
        }
    ],
    "id": "SP:46a0525a48158d84c1fccf798fc2516f88abf3d8",
    "references": [
        {
            "authors": [
                "Marc Bellemare",
                "Sriram Srinivasan",
                "Georg Ostrovski",
                "Tom Schaul",
                "David Saxton",
                "Remi Munos"
            ],
            "title": "Unifying count-based exploration and intrinsic motivation",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Marc G Bellemare",
                "Yavar Naddaf",
                "Joel Veness",
                "Michael Bowling"
            ],
            "title": "The arcade learning environment: An evaluation platform for general agents",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2013
        },
        {
            "authors": [
                "Charles Blundell",
                "Benigno Uria",
                "Alexander Pritzel",
                "Yazhe Li",
                "Avraham Ruderman",
                "Joel Z Leibo",
                "Jack Rae",
                "Daan Wierstra",
                "Demis Hassabis"
            ],
            "title": "Model-free episodic control",
            "venue": "arXiv preprint arXiv:1606.04460,",
            "year": 2016
        },
        {
            "authors": [
                "Yuri Burda",
                "Harrison Edwards",
                "Amos Storkey",
                "Oleg Klimov"
            ],
            "title": "Exploration by random network distillation",
            "venue": "arXiv preprint arXiv:1810.12894,",
            "year": 2018
        },
        {
            "authors": [
                "Li Chenghao",
                "Tonghan Wang",
                "Chengjie Wu",
                "Qianchuan Zhao",
                "Jun Yang",
                "Chongjie Zhang"
            ],
            "title": "Celebrating diversity in shared multi-agent reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Nikolay Dandanov",
                "Hussein Al-Shatri",
                "Anja Klein",
                "Vladimir Poulkov"
            ],
            "title": "Dynamic self-optimization of the antenna tilt for best trade-off between coverage and capacity in mobile networks",
            "venue": "Wireless Personal Communications,",
            "year": 2017
        },
        {
            "authors": [
                "Sanjoy Dasgupta",
                "Anupam Gupta"
            ],
            "title": "An elementary proof of a theorem of johnson and lindenstrauss",
            "venue": "Random Structures & Algorithms,",
            "year": 2003
        },
        {
            "authors": [
                "Marc-Andr\u00e9 Dittrich",
                "Silas Fohlmeister"
            ],
            "title": "Cooperative multi-agent system for production control using reinforcement learning",
            "venue": "CIRP Annals,",
            "year": 2020
        },
        {
            "authors": [
                "Yali Du",
                "Lei Han",
                "Meng Fang",
                "Ji Liu",
                "Tianhong Dai",
                "Dacheng Tao"
            ],
            "title": "Liir: Learning individual intrinsic reward in multi-agent reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Scott Fujimoto",
                "Herke Hoof",
                "David Meger"
            ],
            "title": "Addressing function approximation error in actorcritic methods",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Jayesh K Gupta",
                "Maxim Egorov",
                "Mykel Kochenderfer"
            ],
            "title": "Cooperative multi-agent control using deep reinforcement learning",
            "venue": "In International conference on autonomous agents and multiagent systems,",
            "year": 2017
        },
        {
            "authors": [
                "Matthew Hausknecht",
                "Peter Stone"
            ],
            "title": "Deep recurrent q-learning for partially observable mdps",
            "venue": "In 2015 aaai fall symposium series,",
            "year": 2015
        },
        {
            "authors": [
                "Mikael Henaff",
                "Roberta Raileanu",
                "Minqi Jiang",
                "Tim Rockt\u00e4schel"
            ],
            "title": "Exploration via elliptical episodic bonuses",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Rein Houthooft",
                "Xi Chen",
                "Yan Duan",
                "John Schulman",
                "Filip De Turck",
                "Pieter Abbeel"
            ],
            "title": "Vime: Variational information maximizing exploration",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Hao Hu",
                "Jianing Ye",
                "Guangxiang Zhu",
                "Zhizhou Ren",
                "Chongjie Zhang"
            ],
            "title": "Generalizable episodic memory for deep reinforcement learning",
            "venue": "International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Natasha Jaques",
                "Angeliki Lazaridou",
                "Edward Hughes",
                "Caglar Gulcehre",
                "Pedro Ortega",
                "DJ Strouse",
                "Joel Z Leibo",
                "Nando De Freitas"
            ],
            "title": "Social influence as intrinsic motivation for multi-agent deep reinforcement learning",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Hyoungseok Kim",
                "Jaekyeom Kim",
                "Yeonwoo Jeong",
                "Sergey Levine",
                "Hyun Oh Song"
            ],
            "title": "Emi: Exploration with mutual information",
            "venue": "arXiv preprint arXiv:1810.01176,",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Karol Kurach",
                "Anton Raichuk",
                "Piotr Stanczyk",
                "Michal Zajkc",
                "Olivier Bachem",
                "Lasse Espeholt",
                "Carlos Riquelme",
                "Damien Vincent",
                "Marcin Michalski",
                "Olivier Bousquet"
            ],
            "title": "Google research football: A novel reinforcement learning environment",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Lei Le",
                "Andrew Patterson",
                "Martha White"
            ],
            "title": "Supervised autoencoders: Improving generalization performance with unsupervised regularizers",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "M\u00e1t\u00e9 Lengyel",
                "Peter Dayan"
            ],
            "title": "Hippocampal contributions to control: the third way",
            "venue": "Advances in neural information processing systems,",
            "year": 2007
        },
        {
            "authors": [
                "Zichuan Lin",
                "Tianqi Zhao",
                "Guangwen Yang",
                "Lintao Zhang"
            ],
            "title": "Episodic memory deep q-networks",
            "venue": "arXiv preprint arXiv:1805.07603,",
            "year": 2018
        },
        {
            "authors": [
                "Iou-Jen Liu",
                "Unnat Jain",
                "Raymond A Yeh",
                "Alexander Schwing"
            ],
            "title": "Cooperative exploration for multi-agent deep reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Anuj Mahajan",
                "Tabish Rashid",
                "Mikayel Samvelyan",
                "Shimon Whiteson"
            ],
            "title": "Maven: Multi-agent variational exploration",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "David Henry Mguni",
                "Taher Jafferjee",
                "Jianhong Wang",
                "Nicolas Perez-Nieves",
                "Oliver Slumbers",
                "Feifei Tong",
                "Yang Li",
                "Jiangcheng Zhu",
                "Yaodong Yang",
                "Jun Wang"
            ],
            "title": "Ligs: Learnable intrinsic-reward generation selection for multi-agent learning",
            "venue": "arXiv preprint arXiv:2112.02618,",
            "year": 2021
        },
        {
            "authors": [
                "Shakir Mohamed",
                "Danilo Jimenez Rezende"
            ],
            "title": "Variational information maximisation for intrinsically motivated reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Frans A Oliehoek",
                "Christopher Amato"
            ],
            "title": "A concise introduction to decentralized POMDPs",
            "year": 2016
        },
        {
            "authors": [
                "Frans A Oliehoek",
                "Matthijs TJ Spaan",
                "Nikos Vlassis"
            ],
            "title": "Optimal and approximate q-value functions for decentralized pomdps",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2008
        },
        {
            "authors": [
                "Georg Ostrovski",
                "Marc G Bellemare",
                "A\u00e4ron Oord",
                "R\u00e9mi Munos"
            ],
            "title": "Count-based exploration with neural density models",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Deepak Pathak",
                "Pulkit Agrawal",
                "Alexei A Efros",
                "Trevor Darrell"
            ],
            "title": "Curiosity-driven exploration by self-supervised prediction",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Alexander Pritzel",
                "Benigno Uria",
                "Sriram Srinivasan",
                "Adria Puigdomenech Badia",
                "Oriol Vinyals",
                "Demis Hassabis",
                "Daan Wierstra",
                "Charles Blundell"
            ],
            "title": "Neural episodic control",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Santhosh K Ramakrishnan",
                "Aaron Gokaslan",
                "Erik Wijmans",
                "Oleksandr Maksymets",
                "Alex Clegg",
                "John Turner",
                "Eric Undersander",
                "Wojciech Galuba",
                "Andrew Westbury",
                "Angel X Chang"
            ],
            "title": "Habitatmatterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai",
            "venue": "arXiv preprint arXiv:2109.08238,",
            "year": 2021
        },
        {
            "authors": [
                "Tabish Rashid",
                "Mikayel Samvelyan",
                "Christian Schroeder",
                "Gregory Farquhar",
                "Jakob Foerster",
                "Shimon Whiteson"
            ],
            "title": "Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Tabish Rashid",
                "Gregory Farquhar",
                "Bei Peng",
                "Shimon Whiteson"
            ],
            "title": "Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Mikayel Samvelyan",
                "Tabish Rashid",
                "Christian Schroeder De Witt",
                "Gregory Farquhar",
                "Nantas Nardelli",
                "Tim GJ Rudner",
                "Chia-Man Hung",
                "Philip HS Torr",
                "Jakob Foerster",
                "Shimon Whiteson"
            ],
            "title": "The starcraft multi-agent challenge",
            "year": 1902
        },
        {
            "authors": [
                "Tom Schaul",
                "John Quan",
                "Ioannis Antonoglou",
                "David Silver"
            ],
            "title": "Prioritized experience replay",
            "venue": "arXiv preprint arXiv:1511.05952,",
            "year": 2015
        },
        {
            "authors": [
                "Kihyuk Sohn",
                "Honglak Lee",
                "Xinchen Yan"
            ],
            "title": "Learning structured output representation using deep conditional generative models",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Kyunghwan Son",
                "Daewoo Kim",
                "Wan Ju Kang",
                "David Earl Hostallero",
                "Yung Yi"
            ],
            "title": "Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Bradly C Stadie",
                "Sergey Levine",
                "Pieter Abbeel"
            ],
            "title": "Incentivizing exploration in reinforcement learning with deep predictive models",
            "venue": "arXiv preprint arXiv:1507.00814,",
            "year": 2015
        },
        {
            "authors": [
                "Peter Sunehag",
                "Guy Lever",
                "Audrunas Gruslys",
                "Wojciech Marian Czarnecki",
                "Vinicius Zambaldi",
                "Max Jaderberg",
                "Marc Lanctot",
                "Nicolas Sonnerat",
                "Joel Z Leibo",
                "Karl Tuyls"
            ],
            "title": "Value-decomposition networks for cooperative multi-agent learning",
            "venue": "arXiv preprint arXiv:1706.05296,",
            "year": 2017
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Haoran Tang",
                "Rein Houthooft",
                "Davis Foote",
                "Adam Stooke",
                "OpenAI Xi Chen",
                "Yan Duan",
                "John Schulman",
                "Filip DeTurck",
                "Pieter Abbeel"
            ],
            "title": "exploration: A study of count-based exploration for deep reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Emanuel Todorov",
                "Tom Erez",
                "Yuval Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,",
            "year": 2012
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton"
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research,",
            "year": 2008
        },
        {
            "authors": [
                "Binyu Wang",
                "Zhe Liu",
                "Qingbiao Li",
                "Amanda Prorok"
            ],
            "title": "Mobile robot path planning in dynamic environments through globally guided reinforcement learning",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Jianhao Wang",
                "Zhizhou Ren",
                "Terry Liu",
                "Yang Yu",
                "Chongjie Zhang"
            ],
            "title": "Qplex: Duplex dueling multi-agent q-learning",
            "venue": "arXiv preprint arXiv:2008.01062,",
            "year": 2020
        },
        {
            "authors": [
                "Tonghan Wang",
                "Jianhao Wang",
                "Yi Wu",
                "Chongjie Zhang"
            ],
            "title": "Influence-based multi-agent exploration",
            "venue": "arXiv preprint arXiv:1910.05512,",
            "year": 2019
        },
        {
            "authors": [
                "Tonghan Wang",
                "Tarun Gupta",
                "Anuj Mahajan",
                "Bei Peng",
                "Shimon Whiteson",
                "Chongjie Zhang. Rode"
            ],
            "title": "Learning roles to decompose multi-agent tasks",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Marco A Wiering"
            ],
            "title": "Multi-agent reinforcement learning for traffic light control",
            "venue": "In Machine Learning: Proceedings of the Seventeenth International Conference (ICML\u20192000),",
            "year": 2000
        },
        {
            "authors": [
                "Jiachen Yang",
                "Igor Borovikov",
                "Hongyuan Zha"
            ],
            "title": "Hierarchical cooperative multi-agent reinforcement learning with skill discovery",
            "venue": "arXiv preprint arXiv:1912.03558,",
            "year": 2019
        },
        {
            "authors": [
                "Yaodong Yang",
                "Jianye Hao",
                "Ben Liao",
                "Kun Shao",
                "Guangyong Chen",
                "Wulong Liu",
                "Hongyao Tang"
            ],
            "title": "Qatten: A general framework for cooperative multiagent reinforcement learning",
            "year": 2002
        },
        {
            "authors": [
                "Chao Yu",
                "Akash Velu",
                "Eugene Vinitsky",
                "Jiaxuan Gao",
                "Yu Wang",
                "Alexandre Bayen",
                "Yi Wu"
            ],
            "title": "The surprising effectiveness of ppo in cooperative multi-agent games",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Lulu Zheng",
                "Jiarui Chen",
                "Jianhao Wang",
                "Jiamin He",
                "Yujing Hu",
                "Yingfeng Chen",
                "Changjie Fan",
                "Yang Gao",
                "Chongjie Zhang"
            ],
            "title": "Episodic multi-agent reinforcement learning with curiosity-driven exploration",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Guangxiang Zhu",
                "Zichuan Lin",
                "Guangwen Yang",
                "Chongjie Zhang"
            ],
            "title": "Episodic reinforcement learning with associative memory",
            "venue": "International conference on learning representations,",
            "year": 2020
        },
        {
            "authors": [
                "Chenghao"
            ],
            "title": "2021) led to a decrease in performance on both easy and hard SMAC maps. Thus, we decided to exclude the use of r for EMU (QPLEX) on the super hard SMAC task and for EMU (CDS) on the easy/hard SMAC maps. EMU set task-dependent \u03b4 values as presented in Table 1",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Recently, cooperative MARL has been adopted to many applications, including traffic control (Wiering et al., 2000), resource allocation (Dandanov et al., 2017), robot path planning (Wang et al., 2020a), and production systems (Dittrich & Fohlmeister, 2020), etc. In spite of these successful applications, cooperative MARL still faces challenges in learning proper coordination among multiple agents because of the partial observability and the interaction between agents during training.\nTo address these challenges, the framework of centralized training and decentralized execution (CTDE) (Oliehoek et al., 2008; Oliehoek & Amato, 2016; Gupta et al., 2017) has been proposed. CTDE enables a decentralized execution while fully utilizing global information during centralized training, so CTDE improves policy learning by accessing to global states at the training phase. Especially, value factorization approaches (Sunehag et al., 2017; Rashid et al., 2018; Son et al., 2019; Yang et al., 2020; Rashid et al., 2020; Wang et al., 2020b) maintain the consistency between individual and joint action selection, achieving the state-of-the-art performance on difficult multi-agent tasks, such as StarCraft II Multi-agent Challenge (SMAC) (Samvelyan et al., 2019). However, learning optimal policy in MARL still requires a long convergence time due to the interaction between agents, and the trained models often fall into local optima, particularly when agents perform complex tasks (Mahajan et al., 2019). Hence, researchers present a committed exploration mechanism under this CTDE training practice (Mahajan et al., 2019; Yang et al., 2019; Wang et al., 2019; Liu et al., 2021) with the expectation to find episodes escaping from the local optima.\nDespite the required exploration in MARL with CTDE, recent works on episodic control emphasize the exploitation of episodic memory to expedite reinforcement learning. Episodic control (Lengyel & Dayan, 2007; Blundell et al., 2016; Lin et al., 2018; Pritzel et al., 2017) memorizes explored states and their best returns from experience in the episodic memory, to converge on the best policy. Recently, this episodic control has been adopted to MARL (Zheng et al., 2021), and this episodic\ncontrol case shows faster convergence than the learning without such memory. Whereas there are merits from episodic memory and control from its utilization, there exists a problem of determining which memories to recall and how to use them, to efficiently explore from the memory. According to Blundell et al. (2016); Lin et al. (2018); Zheng et al. (2021), the previous episodic control generally utilizes a random projection to embed global states, but this random projection hardly makes the semantically similar states close to one another in the embedding space. In this case, exploration will be limited to a narrow distance threshold. However, this small threshold leads to inefficient memory utilization because the recall of episodic memory under such small thresholds retrieves only the same state without consideration of semantic similarity from the perspective of goal achievement. Additionally, the naive utilization of episodic control on complex tasks involves the risk of converging to local optima by repeatedly revisiting previously explored states, favoring exploitation over exploration.\nContribution. This paper presents an Efficient episodic Memory Utilization for multi-agent reinforcement learning (EMU), a framework to selectively encourage desirable transitions with semantic memory embeddings.\n\u2022 Efficient memory embedding: When generating features of a global state for episodic memory (Figure 1(b)), we adopt an encoder/decoder structure where 1) an encoder embeds a global state conditioned on timestep into a low-dimensional feature and 2) a decoder takes this feature as an input conditioned on the timestep to predict the return of the global state. In addition, to ensure smoother embedding space, we also consider the reconstruction of the global state when training the decoder to predict its return. To this end, we develop deterministic Conditional AutoEncoder (dCAE) (Figure 1(c)). With this structure, important features for overall return can be captured in the embedding space. The proposed embedding contains semantic meaning and thus guarantees a gradual change of feature space, making the further exploration on memory space near the given state, i.e., efficient memory utilization.\n\u2022 Episodic incentive generation: While the semantic embedding provides a space to explore, we still need to identify promising state transitions to explore. Therefore, we define a desirable trajectory representing the highest return path, such as destroying all enemies in SMAC or scoring a goal in Google Research Football (GRF) (Kurach et al., 2020). States on this trajectory are marked as desirable in episodic memory, so we could incentivize the exploration on such states according to their desirability. We name this incentive structure as an episodic incentive (Figure 1(d)), encouraging desirable transitions and preventing convergence to unsatisfactory local optima. We provide theoretical analyses demonstrating that this episodic incentive yields a better gradient signal compared to conventional episodic control.\nWe evaluate EMU on SMAC and GRF, and empirical results demonstrate that the proposed method achieves further performance improvement compared to the state-of-art baseline methods. Ablation studies and qualitative analyses validate the propositions made by this paper."
        },
        {
            "heading": "2 PRELIMINARY",
            "text": ""
        },
        {
            "heading": "2.1 DECENTRALIZED POMDP",
            "text": "A fully cooperative multi-agent task can be formalized by following the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Oliehoek & Amato, 2016), G = \u27e8I, S,A, P,R,\u2126, O, n, \u03b3\u27e9, where I is the finite set of n agents; s \u2208 S is the true state of the environment; ai \u2208 A is the i-th agent\u2019s action forming the joint action a \u2208 An; P (s\u2032|s,a) is the state transition function; R is a reward function r = R(s,a, s\u2032) \u2208 R; \u2126 is the observation space; O is the observation function generating an observation for each agent oi \u2208 \u2126; and finally, \u03b3 \u2208 [0, 1) is a discount factor. At each timestep, an agent has its own local observation oi, and the agent selects an action ai \u2208 A. The current state s and the joint action of all agents a lead to a next state s\u2032 according to P (s\u2032|s,a). The joint variable of s, a, and s\u2032 will determine the identical reward r across the multi-agent group. In addition, similar to Hausknecht & Stone (2015); Rashid et al. (2018), each agent utilizes a local action-observation history \u03c4i \u2208 T \u2261 (\u2126 \u00d7 A) for its policy \u03c0i(a|\u03c4i), where \u03c0 : T \u00d7A\u2192 [0, 1]."
        },
        {
            "heading": "2.2 DESIRABILITY AND DESIRABLE TRAJECTORY",
            "text": "Definition 1. (Desirability and Desirable Trajectory) For a given threshold returnRthr and a trajectory T := {s0,a0, r0, s1,a1, r1, ..., sT }, T is considered as a desirable trajectory, denoted as T\u03be, when an episodic return is Rt=0 = \u03a3T\u22121t\u2032=t rt\u2032 \u2265 Rthr. A binary indicator \u03be(\u00b7) denotes the desirability of state st as \u03be(st) = 1 when st \u2208 \u2200T\u03be.\nIn cooperative MARL tasks, such as SMAC and GRF, the total amount of rewards from the environment within an episode is often limited as Rmax, which is only given when cooperative agents achieve a common goal. In such a case, we can set Rthr = Rmax. For further description of cooperative MARL, please see Appendix A."
        },
        {
            "heading": "2.3 EPISODIC CONTROL IN MARL",
            "text": "Episodic control was introduced from the analogy of a brain\u2019s hippocampus for memory utilization (Lengyel & Dayan, 2007). After the introduction of deep Q-network, Blundell et al. (2016) adopted this idea of episodic control to the model-free setting by storing the highest return of a given state, to efficiently estimate the Q-values of the state. This recalling of the high-reward experiences helps to increase sample efficiency and thus expedites the overall learning process (Blundell et al., 2016; Pritzel et al., 2017; Lin et al., 2018). Please see Appendix A for related works and further discussions.\nAt timestep t, let us define a global state as st. When utilizing episodic control, instead of directly using st, researchers adopt a state embedding function f\u03d5(s) : S \u2192 Rk to project states toward a k-dimensional vector space. With this projection, a representation of global state st becomes xt = f\u03d5(st). The episodic control memorizes H(f\u03d5(st)), i.e., the highest return of a given global state st, in episodic buffer DE (Pritzel et al., 2017; Lin et al., 2018; Zheng et al., 2021). Here, xt is used as a key to the highest return, H(xt); as a key-value pair in DE . The episodic control in Lin et al. (2018) updates H(xt) with the following rules.\nH(xt) =\n{ max{H(x\u0302t), Rt(st,at)}, if ||x\u0302t \u2212 xt||2 < \u03b4\nRt(st,at), otherwise , (1)\nwhereRt(st,at) is the return of a given (st,at); \u03b4 is a threshold value of state-embedding difference; and x\u0302t = f\u03d5(s\u0302t) is xt = f\u03d5(st)\u2019s nearest neighbor inDE . If there is no similar projected state x\u0302t such that ||x\u0302t\u2212xt||2 < \u03b4 in the memory, thenH(xt) keeps the currentRt(st,at). Leveraging the episodic memory, EMC (Zheng et al., 2021) presents the one-step TD memory target QEC(f\u03d5(st),at) as\nQEC(f\u03d5(st),at) = rt(st,at) + \u03b3H(f\u03d5(st+1)). (2)\nThen, the loss function LEC\u03b8 for training can be expressed as the weighted sum of one-step TD error and one-step TD memory error, i.e., Monte Carlo (MC) inference error, based on QEC(f\u03d5(st),at).\nLEC\u03b8 = (y(s,a)\u2212Qtot(s,a; \u03b8))2 + \u03bb(QEC(f\u03d5(s),a)\u2212Qtot(s,a; \u03b8))2, (3) where y(s,a) is one-step TD target; Qtot is the joint Q-value function parameterized by \u03b8; and \u03bb is a scale factor.\nProblem of the conventional episodic control with random projection Random projection is useful for dimensionality reduction as it preserves distance relationships, as demonstrated by the Johnson-Lindenstrauss lemma (Dasgupta & Gupta, 2003). However, a random projection adopted for f\u03d5(s) hardly has a semantic meaning in its embedding xt, as it puts random weights on the state features without considering the patterns of determining the state returns. Additionally, when recalling the memory from DE , the projected state xt can abruptly change even with a small change of st because the embedding is not being regulated by the return. This results in a sparse selection of semantically similar memories, i.e. similar states with similar or better rewards. As a result, conventional episodic control using random projection only recalls identical states and relies on its own Monte-Carlo (MC) return to regulate the one-step TD target inference, limiting exploration of nearby states on the embedding space.\nThe problem intensifies when the high-return states in the early training phase are indeed local optima. In such cases, the naive utilization of episodic control is prone to converge on local minima. As a result, for the super hard tasks of SMAC, EMC (Zheng et al., 2021) had to decrease the magnitude of this regularization to almost zero, i.e., not considering episodic memories anymore."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "This section introduces Efficient episodic Memory Utilization (EMU) (Figure 1). We begin by explaining how to construct (1) semantic memory embeddings to better utilize the episodic memory, which enables memory recall of similar, more promising states. To further improve memory utilization, as an alternative to the conventional episodic control, we propose (2) episodic incentive that selectively encourages desirable transitions while preventing local convergence towards undesirable trajectories."
        },
        {
            "heading": "3.1 SEMANTIC MEMORY EMBEDDING",
            "text": "Episodic Memory Construction To address the problems of a random projection adopted in episodic control, we propose a trainable embedding function f\u03d5(s) to learn the state embedding patterns affected by the highest return. The problem of a learnable embedding network f\u03d5 is that the match between H(f\u03d5(st)) and st breaks whenever f\u03d5 is updated. Hence, we save the global state st as well as a pair of Ht and xt in DE , so that we can update x = f\u03d5(s) whenever f\u03d5 is updated. In addition, we store the desirability \u03be of st according to Definition 1. Appendix E.1 illustrates the details of memory construction proposed by this paper.\nLearning framework for State Embedding When training f\u03d5(st), it is critical to extract important features of a global state that affect its value, i.e., the highest return. Thus, we additionally adopt a decoder structure H\u0304t = f\u03c8(xt) to predict the highest return Ht of st. We call this embedding function as EmbNet, and its learning objective of f\u03d5 and f\u03c8 can be written as\nL(\u03d5,\u03c8) = (Ht \u2212 f\u03c8(f\u03d5(st)))2 . (4) When constructing the embedding space, we found that an additional consideration of reconstruction of state s conditioned on timestep t improves the quality of feature extraction and constitutes a smoother embedding space. To this end, we develop the deterministic conditional autoencoder (dCAE), and the corresponding loss function can be expressed as\nL(\u03d5,\u03c8) = ( Ht \u2212 fH\u03c8 (f\u03d5(st|t)|t) )2 + \u03bbrcon||st \u2212 fs\u03c8(f\u03d5(st|t)|t)||22, (5)\nwhere fH\u03c8 predicts the highest return; f s \u03c8 reconstructs st; \u03bbrcon is a scale factor. Here, f H \u03c8 and f s \u03c8 share the lower part of networks as illustrated in Figure 1(c). Appendix C.1 presents the details of network structure of f\u03d5 and f\u03c8 , and Algorithm 1 in Appendix C.1 presents the learning framework for f\u03d5 and f\u03c8 . This training is conducted periodically in parallel to the RL policy learning on Qtot(\u00b7; \u03b8). Figure 2 illustrates the result of t-SNE (Van der Maaten & Hinton, 2008) of 50K samples of x \u2208 DE out of 1M memory data in training for 3s_vs_5z task of SMAC. Unlike supervised learning with label data, there is no label for each xt. Thus, we mark xt with its pair of the highest return Ht. Compared to a random projection in Figure 2(a), xt via f\u03d5 is well-clustered, according to the similarity of the embedded state and its return. This clustering of xt enables us to safely select\nepisodic memories around the key state st, which constitutes efficient memory utilization. This memory utilization expedites learning speed as well as encourages exploration to a more promising state s\u0302t near st. Appendix F illustrates how to determine \u03b4 of Eq. 1 in a memory-efficient way."
        },
        {
            "heading": "3.2 EPISODIC INCENTIVE",
            "text": "With the learnable memory embedding for an efficient memory recall, how to use the selected memories still remains a challenge because a naive utilization of episodic memory is prone to converge on local minima. To solve this issue, we propose a new reward structure called episodic incentive rp by leveraging the desirability \u03be of states in DE . Before deriving the episodic incentive rp, we first need to understand the characteristics of episodic control. In this section, we denote the joint Q-function Qtot(\u00b7; \u03b8) simply as Q\u03b8 for conciseness. Theorem 1. Given a transition (s,a, r, s\u2032) and H(x\u2032), let L\u03b8 be the Q-learning loss with additional transition reward, i.e., L\u03b8 := (y(s,a) + rEC(s,a, s\u2032)\u2212Qtot(s,a; \u03b8))2 where rEC(s,a, s\u2032) := \u03bb(r(s,a) + \u03b3H(x\u2032)\u2212Q\u03b8(s,a)), then \u2207\u03b8L\u03b8 = \u2207\u03b8LEC\u03b8 . (Proof in Appendix B.1)\nAs Theorem 1 suggests, we can generate the same gradient signal as the episodic control by leveraging the additional transition reward rEC(s,a, s\u2032). However, rEC(s,a, s\u2032) accompanies a risk of local convergence as discussed in Section 2.3. Therefore, instead of applying rEC(s,a, s\u2032), we propose the episodic incentive rp := \u03b3\u03b7\u0302(s\u2032) that provides an additional reward for the desirable transition (s,a, r, s\u2032), such that \u03be(s\u2032) = 1. Here, \u03b7\u0302(s\u2032) estimates \u03b7\u2217(s\u2032), which represents the difference between the true value V \u2217(s\u2032) of s\u2032 and the predicted value via target network maxa\u2032Q\u03b8\u2212(s\u2032,a\u2032), defined as\n\u03b7\u2217(s\u2032) := V \u2217(s\u2032)\u2212max a\u2032 Q\u03b8\u2212(s \u2032,a\u2032). (6)\nNote that we do not know V \u2217(s\u2032) and subsequently \u03b7\u2217(s\u2032). To accurately estimate \u03b7\u2217(s\u2032) with \u03b7\u0302(s\u2032), we use the expected value considering the current policy \u03c0\u03b8 as \u03b7\u0302(s\u2032) := E\u03c0\u03b8 [\u03b7(s\u2032)] where \u03b7 \u2208 [0, \u03b7max(s\u2032)] for s\u2032 \u223c P (s\u2032|s,a \u223c \u03c0\u03b8). Here, \u03b7max(s\u2032) can be reasonably approximated by using H(f\u03d5(s\u2032)) in DE . Then, with the count-based estimation \u03b7\u0302(s\u2032), episodic incentive rp can be expressed as\nrp = \u03b3\u03b7\u0302(s\u2032) = \u03b3E\u03c0\u03b8 [\u03b7(s\u2032)] \u2243 \u03b3 N\u03be(s\n\u2032)\nNcall(s\u2032) \u03b7max(s\n\u2032) = \u03b3 N\u03be(s\n\u2032)\nNcall(s\u2032) (H(f\u03d5(s \u2032))\u2212max a\u2032 Q\u03b8\u2212(s \u2032, a\u2032)),\n(7) where Ncall(s\u2032) is the number of visits on x\u0302\u2032 = NN(f\u03d5(s\u2032)) \u2208 DE ; and N\u03be is the number of desirable transition from x\u0302\u2032. Here, NN(\u00b7) represents a function for selecting the nearest neighbor. From Theorem 1, the loss function adopting episodic control with an alternative transition reward rp instead of rEC can be expressed as\nLp\u03b8 = (r(s,a) + r p + \u03b3max\na\u2032 Q\u03b8\u2212(s\n\u2032,a\u2032)\u2212Q\u03b8(s,a))2. (8)\nThen, the gradient signal of the one-step TD inference loss \u2207\u03b8Lp\u03b8 with the episodic reward rp = \u03b3\u03b7\u0302(s\u2032) can be written as\n\u2207\u03b8Lp\u03b8 = \u22122\u2207\u03b8Q\u03b8(s, a)(\u2206\u03b5TD + r p) = \u22122\u2207\u03b8Q\u03b8(s, a)(\u2206\u03b5TD + \u03b3\nN\u03be(s \u2032)\nNcall(s\u2032) \u03b7max(s\n\u2032)), (9)\nwhere \u2206\u03b5TD = r(s, a) + \u03b3maxa\u2032Q\u03b8\u2212(s\u2032, a\u2032)\u2212Q\u03b8(s, a) is one-step inference TD error. Here, the gradient signal \u2207\u03b8Lp\u03b8 with the proposed episodic reward rp can accurately estimate the optimal gradient signal as follows. Theorem 2. Let\u2207\u03b8L\u2217\u03b8 = \u22122\u2207\u03b8Q\u03b8(s, a)(\u2206\u03b5\u2217TD) be the optimal gradient signal with the true one step TD error \u2206\u03b5\u2217TD = r(s, a) + \u03b3V\n\u2217(s\u2032) \u2212 Q\u03b8(s, a). Then, the gradient signal \u2207\u03b8Lp\u03b8 with the episodic incentive rp converges to the optimal gradient signal as the policy converges to the optimal policy \u03c0\u2217\u03b8 , i.e., \u2207\u03b8L p \u03b8 \u2192 \u2207\u03b8L\u2217\u03b8 as \u03c0\u03b8 \u2192 \u03c0\u2217\u03b8 . (Proof in Appendix B.2)\nTheorem 2 also implies that there exists a certain bias in \u2207\u03b8LEC\u03b8 as described in Appendix B.2. Besides the property of convergence to the optimal gradient signal presented in Theorem 2, the episodic incentive has the following additional characteristics. (1) The episodic incentive is only applied to the desirable transition. We can simply see that rp = \u03b3\u03b7\u0302 = \u03b3E\u03c0\u03b8 [\u03b7] \u2243 \u03b3\u03b7maxN\u03be/Ncall and if \u03be(s\u2032) = 0 then N\u03be = 0, yielding rp \u2192 0. Subsequently, (2) there is no need to adjust a scale factor by the task complexity. (3) The episodic incentive can reduce the risk of overestimation by considering the\nexpected value of E\u03c0\u03b8 [\u03b7]. Instead of considering the optimistic \u03b7max, the count-based estimation rp = \u03b3\u03b7\u0302 = \u03b3E\u03c0\u03b8 [\u03b7] can consider the randomness of the policy \u03c0\u03b8. Figure 3 illustrates how the episodic incentive works with the desirability stored in DE constructed by Algorithm 2 presented in Appendix E.1. In Figure 3 as we intended, high-value states (at small timesteps) are clustered close to the purple zone, while low-value states (at large timesteps) are located in the red zone."
        },
        {
            "heading": "3.3 OVERALL LEARNING OBJECTIVE",
            "text": "To construct the joint Q-function Qtot from individual Qi of the agent i, any form of mixer can be used. In this paper, we mainly adopt the mixer presented in QPLEX (Wang et al., 2020b) similar to Zheng et al. (2021), which guarantees the complete Individual-Global-Max (IGM) condition (Son et al., 2019; Wang et al., 2020b). Considering any intrinsic reward rc encouraging an exploration (Zheng et al., 2021) or diversity (Chenghao et al., 2021), the final loss function for the action policy learning from Eq. 8 can be extended as\nLp\u03b8 = ( r(s,a) + rp + \u03b2cr c + \u03b3maxa\u2032Qtot(s \u2032,a\u2032; \u03b8\u2212)\u2212Qtot(s,a; \u03b8) )2 , (10)\nwhere \u03b2c is a scale factor. Note that the episodic incentive rp can be used in conjunction with any form of intrinsic reward rc being properly annealed throughout the training. Again, \u03b8 denotes the parameters of networks related to action policy Qi and the corresponding mixer network to generate Qtot. For the action selection via Q, we adopt a GRU to encode a local action-observation history \u03c4 presented in 2.1 similar to Sunehag et al. (2017); Rashid et al. (2018); Wang et al. (2020b); but in Eq. 10, we denote equations with s instead of \u03c4 for the coherence with derivation in the previous section. Appendix E.2 presents the overall training algorithm."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this part, we have formulated our experiments with the intention of addressing the following inquiries denoted as Q1-3.\n\u2022 Q1. How does EMU compare to the state-of-the-art MARL frameworks? \u2022 Q2. How does the proposed state embedding change the embedding space and improve the\nperformance? \u2022 Q3. How does the episodic incentive improve performance?\nWe conduct experiments on complex multi-agent tasks such as SMAC (Samvelyan et al., 2019) and GRF (Kurach et al., 2020). The experiments compare EMU against EMC adopting episodic control (Zheng et al., 2021). Also, we include notable baselines, such as value-based MARL methods QMIX (Rashid et al., 2018), QPLEX (Wang et al., 2020b), CDS encouraging individual diversity (Chenghao et al., 2021). Particularly, we emphasize that EMU can be combined with any MARL framework, so we present two versions of EMU implemented on original QPLEX and CDS, denoted as EMU (QPLEX) and EMU (CDS), respectively. Appendix C provides further details of experiment settings and implementations, and Appendix D.12 provides the applicability of EMU to single-agent tasks, including pixel-based high-dimensional tasks."
        },
        {
            "heading": "4.1 Q1. COMPARATIVE EVALUATION ON STARCRAFT II (SMAC)",
            "text": "Figure 4 illustrates the overall performance of EMU on various SMAC maps. The map categorization regarding the level of difficulty follows the practice of Samvelyan et al. (2019). Thanks to the efficient memory utilization and episodic incentive, both EMU (QPLEX) and EMU (CDS) show significant performance improvement compared to their original methodologies. Especially, in super hard SMAC maps, the proposed method markedly expedites convergence on optimal policy."
        },
        {
            "heading": "4.2 Q1. COMPARATIVE EVALUATION ON GOOGLE RESEARCH FOOTBALL (GRF)",
            "text": "Here, we conduct experiments on GRF to further compare the performance of EMU with other baseline algorithms. In our GRF task, CDS and EMU (CDS) do not utilize the agent\u2019s index on observation as they contain the prediction networks while other baselines (QMIX, EMC, QPLEX) use information of the agent\u2019s identity in observations. In addition, we do not utilize any additional algorithm, such as prioritized experience replay (Schaul et al., 2015), for all baselines and our method, to expedite learning efficiency. From the experiments, adopting EMU achieves significant performance improvement, and EMU quickly finds the winning or scoring policy at the early learning phase by utilizing semantically similar memory."
        },
        {
            "heading": "4.3 Q2. PARAMETRIC AND ABLATION STUDY",
            "text": "In this section, we examine how the key hyperparameter \u03b4 and the choice of design for f\u03d5 affect the performance. To compare the learning quality and performance more quantitatively, we propose a new performance index called overall win-rate, \u00b5\u0304w. The purpose of \u00b5\u0304w is to consider both training efficiency (speed) and quality (win-rate) for different seed cases (see Appendix D.1 for details). We conduct experiments on selected SMAC maps to measure \u00b5\u0304w according to \u03b4 and design choice for f\u03d5 such as (1) random projection, (2) EmbNet with Eq. 4 and (3) dCAE with Eq. 5.\n(a) 3s_vs_5z (b) 5m_vs_6m\nFigure 6: \u00b5\u0304w according to \u03b4 and various design choices for f\u03d5 on SMAC maps. (a) 3s_vs_5z (b) 5m_vs_6m\nFigure 7: Final win-rate according to \u03b4 and various design choices for f\u03d5 on SMAC maps.\nFigure 6 and Figure 7 show \u00b5\u0304w values and test win-rate at the end of training time according to different \u03b4, presented in log-scale. To see the effect of design choice for f\u03d5 distinctly, we conduct experiments with the conventional episodic control. More data of \u00b5\u0304w is presented in Tables 4 and 5 in Appendix D.2. Figure 6 illustrates that dCAE structure shows the best training efficiency throughout various \u03b4 while achieving the optimal policy as other design choices as presented in Figure 7.\nInterestingly, dCAE structure works well with a wider range of \u03b4 than EmbNet. We conjecture that EmbNet can select very different states as exploration if those states have similar return H during training. This excessive memory recall adversely affects learning and fails to find an optimal policy as a result. See Appendix D.2 for detailed analysis and Appendix D.8 for an ablation study on the loss function of dCAE.\nEven though a wide range of \u03b4 works well as in Figures 6 and 7, choosing a proper value of \u03b4 in\nmore difficult MARL tasks significantly improves the overall learning performance. Figure 8 shows the learning curve of EMU according to \u03b41 = 1.3e\u22127, \u03b42 = 1.3e\u22125, \u03b43 = 1.3e\u22123, and \u03b44 = 1.3e\u22122. In super hard MARL tasks such as 6h_vs_8z in SMAC and CA_hard in GRF, \u03b43 shows the best performance compared to other \u03b4 values. This is consistent with the value suggested in Appendix F, where \u03b4 is determined in a memory-efficient way. Further parametric study on \u03b4 and \u03bbrcon are presented in Appendix D.5 and D.6, respectively."
        },
        {
            "heading": "4.4 Q3. FURTHER ABLATION STUDY",
            "text": "In this section, we carry out further ablation studies to see the effect of episodic incentive rp presented in Section 3.2. From EMU (QPLEX) and EMU (CDS), we ablate the episodic incentive and denote them with (No-EI). We additionally ablate embedding network f\u03d5 from EMU and denote them with (No-SE). In addition, we ablate both parts, yielding EMC (QPLEX-original) and CDS (QPLEXoriginal). We evaluate the performance of each model on super hard SMAC maps. Additional ablation studies on GRF maps are presented in Appendix D.7. Note that EMC (QPLEX-original) utilizes the conventional episodic control presented in Zheng et al. (2021).\nFigure 9 illustrates that the episodic incentive largely affects learning performance. Especially, EMU (QPLEX-No-EI) and EMU (CDS-No-EI) utilizing the conventional episodic control show a large performance variation according to different seeds. This demonstrates that a naive utilization of episodic control could be detrimental to learning an optimal policy. On the other hand, the episodic incentive selectively encourages transition considering desirability and thus prevents such a local convergence. Appendix D.9 and D.10 present an additional ablation study on semantic embedding\nand rc, respectively. In addition, Appendix D.11 presents a comparison with an alternative incentive (Henaff et al., 2022) presented in a single-agent setting."
        },
        {
            "heading": "4.5 QUALITATIVE ANALYSIS AND VISUALIZATION",
            "text": "In this section, we conduct analysis with visualization to check how the desirability \u03be is memorized in DE and whether it conveys correct information. Figure 10 illustrates two test scenarios with different seeds, and each snapshot is denoted with a corresponding timestep. In Figure 11, the trajectory of each episode is projected onto the embedded space of DE .\nIn Figure 10, case (a) successfully defeated all enemies, whereas case (b) lost the engagement. Both cases went through a similar, desirable trajectory at the beginning. For example, until t = 10 agents in both cases focused on killing one enemy and kept all ally agents alive at the same time. However, at t = 12, case (b) lost one agent, and two trajectories of case (a) and (b) in embedded space began to bifurcate. Case (b) still had a chance to win around t = 14 \u223c 16. However,\nthe states became undesirable (denoted without star marker) after losing three ally agents around t = 20, and case (b) lost the battle as a result. These sequences and characteristics of trajectories are well captured by desirability \u03be in DE as illustrated in Figure 11.\nFurthermore, the desirable state denoted with \u03be = 1 encourages exploration around it though it is not directly retrieved during batch sampling. This occurs through the propagation of its desirability to states currently distinguished as undesirable during memory construction, using Algorithm 2 in Appendix E.1. Consequently, when the state\u2019s desirability is precisely memorized in DE , it can encourage desirable transitions through the episodic incentive rp."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "This paper presents EMU, a new framework to efficiently utilize episodic memory for cooperative MARL. EMU introduces two major components: 1) a trainable semantic embedding and 2) an episodic incentive utilizing desirability of state. Semantic memory embedding allows us to safely utilize similar memory in a wide area, expediting learning via exploratory memory recall. The proposed episodic incentive selectively encourages desirable transitions and reduces the risk of local convergence by leveraging the desirability of the state. As a result, there is no need for manual hyperparameter tuning according to the complexity of tasks, unlike conventional episodic control. Experiments and ablation studies validate the effectiveness of each component of EMU."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "This research was supported by AI Technology Development for Commonsense Extraction, Reasoning, and Inference from Heterogeneous Data(IITP) funded by the Ministry of Science and ICT(2022-0-00077)."
        },
        {
            "heading": "A RELATED WORKS",
            "text": "This section presents the related works regarding incentive generation for exploration, episodic control, and the characteristics of cooperative MARL.\nA.1 INCENTIVE FOR MULTI-AGENT EXPLORATION\nBalancing between exploration and exploitation in policy learning is a paramount issue in reinforcement learning. To encourage exploration, modified count-based methods (Bellemare et al., 2016; Ostrovski et al., 2017; Tang et al., 2017), prediction error-based methods (Stadie et al., 2015; Pathak et al., 2017; Burda et al., 2018; Kim et al., 2018), and information gain-based methods (Mohamed & Jimenez Rezende, 2015; Houthooft et al., 2016) have been proposed for a single agent reinforcement learning. In most cases, an incentive for exploration is introduced as an additional reward to a TD target in Q-learning; or such an incentive is added as a regularizer for overall loss functions. Recently, various aforementioned methods to encourage exploration have been adopted to the multi-agent setting (Mahajan et al., 2019; Wang et al., 2019; Jaques et al., 2019; Mguni et al., 2021) and have shown their effectiveness. MAVEN (Mahajan et al., 2019) introduces a regularizer maximizing the mutual information between trajectories and latent variables to learn a diverse set of behaviors. LIIR (Du et al., 2019) learns a parameterized individual intrinsic reward function by maximizing a centralized critic. CDS (Chenghao et al., 2021) proposes a novel information-theoretical objective to maximize the mutual information between agents\u2019 identities and trajectories to encourage diverse individualized behaviors. EMC (Zheng et al., 2021) proposes a curiosity-driven exploration by predicting individual Q-values. This individual-based Q-value prediction can capture the influence among agents as well as the novelty of states."
        },
        {
            "heading": "A.2 EPISODIC CONTROL",
            "text": "Episodic control (Lengyel & Dayan, 2007) was well adopted on model-free setting (Blundell et al., 2016) by storing the highest return of a given state, to efficiently estimate its values or Q-values. Given that the sample generation is often limited by simulation executions or real-world observations, its sample efficiency helps to find an accurate estimation of Q-value (Blundell et al., 2016; Pritzel et al., 2017; Lin et al., 2018). NEC (Pritzel et al., 2017) uses a differentiable neural dictionary as an episodic memory to estimate the action value by the weighted sum of the values in the memory. EMDQN (Lin et al., 2018) utilizes a fixed random matrix to generate a state representation, which is used as a key to link between the state representation and the highest return of the state in the episodic memory. ERLAM (Zhu et al., 2020) learns associative memories by building a graphical representation of states in memory, and GEM (Hu et al., 2021) develops state-action values of episodic memory in a generalizable manner. Recently, EMC (Zheng et al., 2021) extended the approach of EMDQN to a deep MARL with curiosity-driven exploration incentives. EMC utilizes episodic memory to regularize policy learning and shows performance improvement in cooperative MARL tasks. However, EMC requires a hyperparameter tuning to determine the level of importance of the one-step TD memory-based target during training, according to the difficulties of tasks. In this paper, we interpret this regularization as an additional transition reward. Then, we present a novel form of reward, called episodic incentive, to selectively encourage the transition toward desired states, i.e., states toward a common goal in cooperative multi-agent tasks."
        },
        {
            "heading": "A.3 COOPERATIVE MULTI-AGENT REINFORCEMENT LEARNING (MARL) TASK",
            "text": "In general, there is a common goal in cooperative MARL tasks, which guarantees the maximum return that can be obtained from the environment. Thus, there could be many local optima with high returns but not the maximum, which means the agents failed to achieve the common goal in the end. In other words, there is a distinct difference between the objective of cooperative MARL tasks and that of a single-agent task, which aims to maximize the return as much as possible without any boundary determining success or failure. Our desirability definition presented in Definition 1 in MARL setting becomes well justified from this view. Under this characteristic of MARL tasks, learning optimal policy often takes a long time and even fails, yielding a local convergence. EMU was designed to alleviate these issues in MARL."
        },
        {
            "heading": "B MATHEMATICAL PROOF",
            "text": "In this section, we present the omitted proofs of Theorem 1 and Theorem 2 as follows."
        },
        {
            "heading": "B.1 PROOF OF THEOREM 1",
            "text": "Proof. The loss function of a conventional episodic control, LEC\u03b8 , can be expressed as the weighted sum of one-step inference TD error \u2206\u03b5TD = r(s, a) + \u03b3maxa\u2032Q\u03b8\u2212(s\u2032, a\u2032) \u2212 Q\u03b8(s, a) and MC inference error \u2206\u03b5EC = QEC(s, a)\u2212Q\u03b8(s, a).\nLEC\u03b8 = (r(s,a) + \u03b3max a\u2032 Q\u03b8\u2212(s \u2032,a\u2032)\u2212Q\u03b8(s,a))2 + \u03bb(QEC(s,a)\u2212Q\u03b8(s,a))2, (11)\nwhere QEC(s,a) = r(s,a) + \u03b3H(s\u2032) and Q\u03b8\u2212 is the target network parameterized by \u03b8\u2212. Then, the gradient of LEC\u03b8 can be derived as \u2207\u03b8LEC\u03b8 = \u22122\u2207\u03b8Q\u03b8(s,a)[(r(s,a) + \u03b3max\na\u2032 Q\u03b8\u2212(s\n\u2032,a\u2032)\u2212Q\u03b8(s,a)) + \u03bb(QEC(s,a)\u2212Q\u03b8(s,a))]\n= \u22122\u2207\u03b8Q\u03b8(s,a)(\u2206\u03b5TD + \u03bb\u2206\u03b5EC). (12)\nNow, we consider an additional reward rEC for the transition to a conventional Q-learning objective, the modified loss function L\u03b8 can be expressed as\nL\u03b8 = (r(s,a) + r EC(s,a, s\u2032) + \u03b3max\na\u2032 Q\u03b8\u2212(s\n\u2032,a\u2032)\u2212Q\u03b8(s,a))2. (13)\nThen, the gradient of L\u03b8 presented in Eq. 13 is computed as \u2207\u03b8L\u03b8 = \u22122\u2207\u03b8Q\u03b8(s,a)(\u2206\u03b5TD + rEC). (14) Comparing Eq. 12 and Eq. 14, if we set the additional transition reward as rEC(s,a, s\u2032) = \u03bb(r(s,a) + \u03b3H(s\u2032)\u2212Q\u03b8(s,a)), then \u2207\u03b8L\u03b8 = \u2207\u03b8LEC\u03b8 holds."
        },
        {
            "heading": "B.2 PROOF OF THEOREM 2",
            "text": "Proof. From Eq. 7, the value of \u03b7\u0302(s\u2032) can be expressed as\n\u03b7\u0302(s\u2032) = E\u03c0\u03b8 [\u03b7(s\u2032)] \u2243 N\u03be(s\n\u2032)\nNcall(s\u2032)\n( H(f\u03d5(s\n\u2032))\u2212max a\u2032 Q\u03b8\u2212(s\n\u2032, a\u2032) ) ]. (15)\nWhen the joint actions from the current time follow the optimal policy, a \u223c \u03c0\u2217\u03b8 , the cumulative reward from s\u2032 converges to V \u2217(s\u2032), i.e., H(f\u03d5(s\u2032)) \u2192 V \u2217(s\u2032). Then, every recall of x\u0302\u2032 = NN(f\u03d5(s\u2032)) \u2208 DE guarantees the desirable transition, i.e., \u03be(s\u2032) = 1, where NN(\u00b7) represents a function for selecting the nearest neighbor. As a result, as Ncall(s\u2032) \u2192 \u221e, N\u03be(s \u2032) Ncall(s\u2032)\n\u2192 1, yielding \u03b7\u0302(s\u2032) \u2243 N\u03be(s\n\u2032) Ncall(s\u2032) ( H(f\u03d5(s \u2032))\u2212maxa\u2032Q\u03b8\u2212(s\u2032, a\u2032) ) \u2192 V \u2217(s\u2032)\u2212maxa\u2032Q\u03b8\u2212(s\u2032,a\u2032). Then, the gradient signal with the episodic incentive\u2207\u03b8Lp\u03b8 becomes \u2207\u03b8Lp\u03b8 = \u22122\u2207\u03b8Q\u03b8(s,a)[\u2206\u03b5TD + r p]\n= \u22122\u2207\u03b8Q\u03b8(s,a)[\u2206\u03b5TD + \u03b3\u03b7\u0302(s\u2032)] \u2243 \u22122\u2207\u03b8Q\u03b8(s,a)[\u2206\u03b5TD + \u03b3(V \u2217(s\u2032)\u2212max\na\u2032 Q\u03b8\u2212(s\n\u2032,a\u2032))]\n= \u22122\u2207\u03b8Q\u03b8(s,a)[r(s,a) + \u03b3max a\u2032 Q\u03b8\u2212(s \u2032,a\u2032)\u2212Q\u03b8(s,a) + \u03b3(V \u2217(s\u2032)\u2212max a\u2032 Q\u03b8\u2212(s \u2032,a\u2032))] = \u22122\u2207\u03b8Q\u03b8(s,a)[r(s,a) + \u03b3V \u2217(s\u2032)\u2212Q\u03b8(s,a)] = \u2207\u03b8L\u2217\u03b8,\n(16) which completes the proof.\nIn addition, when maxa\u2032Q\u03b8\u2212(s\u2032,a\u2032) accurately estimates V \u2217(s\u2032), the original TD-target is preserved as the episodic incentive becomes zero, i.e., rp \u2192 0. Then with the properly annealed intrinsic reward rc, the learning objective presented in Eq. 10 degenerates to the original Bellman optimality equation (Sutton & Barto, 2018). On the other hand, even though the assumption of H(s\u2032)\u2192 V \u2217(s\u2032) yields \u2206\u03b5EC \u2192 \u2206\u03b5\u2217TD, \u2207\u03b8LEC\u03b8 has an additional bias \u2206\u03b5TD due to weighted sum structure presented in Eq. 3. Thus,\u2207\u03b8LEC\u03b8 can converge to \u2207\u03b8L\u2217\u03b8 only when maxa\u2032Q\u03b8\u2212(s\u2032,a\u2032)\u2192 V \u2217(s\u2032) and \u03bb\u2192 0 at the same time.\nC IMPLEMENTATION AND EXPERIMENT DETAILS"
        },
        {
            "heading": "C.1 DETAILS OF IMPLEMENTATION",
            "text": ""
        },
        {
            "heading": "Encoder and Decoder Structure",
            "text": "As illustrated in Figure 1(c), we have an encoder and decoder structure. For an encoder f\u03d5, we evaluate two types of structure, EmbNet and dCAE. For EmbNet with the learning objective presented in Eq. 4, two fully connected layers with 64-dimensional hidden state are used with ReLU activation function between them, followed by layer normalization block at the head. On the other hand, for dCAE with the learning objective presented in Eq. 5, we utilize a deeper encoder structure which contains three fully connected layers with ReLU activation function. In addition, dCAE takes a timestep t as an input as well as a global state st. We set episodic latent dimension dim(x) = 4 as Zheng et al. (2021).\nFor a decoder f\u03c8 , both EmbNet and dCAE utilize a three-fully connected layer with ReLU activation functions. Differences are that EmbNet takes only xt as input and utilizes the 128-dimensional hidden state while dCAE takes xt and t as inputs and adopts the 64-dimensional hidden state. As illustrated in Figure 1(c), to reconstruct global state st, dCAE has two separate heads while sharing lower parts of networks; fs\u03d5 to reconstruct st and f H \u03d5 to predict the return of st, denoted as Ht. Figure 12 illustrates network structures of EmbNet and dCAE. The concept of supervised VAE similar to EMU can be found in (Le et al., 2018).\nThe reason behind avoiding probabilistic autoencoders such as variational autoencoder (VAE) (Kingma & Welling, 2013; Sohn et al., 2015) is that the stochastic embedding and the prior distribution could have an adverse impact on preserving a pair of xt and Ht for given a st. In particular, with stochastic embedding, a fixed st can generate diverse xt. As a result, it breaks the pair of xt and Ht for given st, which makes it difficult to select a valid memory from DE . For training, we periodically update f\u03d5 and f\u03c8 with an update interval of temb in parallel to MARL training. At each training phase, we use Memb samples out of the current capacity of DE , whose maximum capacity is 1 million (1M), and batch size of memb is used for each training step. After updating f\u03d5, every x \u2208 DE needs to be updated with updated f\u03d5. Algorithm 1 shows the details of learning framework for f\u03d5 and f\u03c8 . Details of the training procedure for f\u03d5 and f\u03c8 along with MARL training are presented in Appendix E.2."
        },
        {
            "heading": "Other Network Structure and Hyperparameters",
            "text": "For a mixer structure, we adopt QPLEX (Wang et al., 2020b) in both EMU (QPLEX) and EMU (CDS) and follow the same hyperparameter settings used in their source codes. Common hyperparameters related to individual Q-network and MARL training are adopted by the default settings of PyMARL (Samvelyan et al., 2019).\nAlgorithm 1 Training Algorithm for State Embedding 1: Parameter: learning rate \u03b1, number of training dataset N , batch size B 2: Sample Training dataset (s(i), H(i), t(i)) N\ni=1 \u223c DE , 3: Initialize weights \u03d5,\u03c8 \u2190 0 4: for i = 1 to \u230aN/B\u230b do 5: Compute (x(j) = f\u03d5(s(j)|t(j))iBj=(i\u22121)B+1 6: Predict return (H\u0304(j) = fH\u03c8 (x\n(j)|t(i)))iBj=(i\u22121)B+1 7: Reconstruct state (s\u0304(j) = fs\u03c8(x\n(j))|t(i))iBj=(i\u22121)B+1 8: Compute Loss L(\u03d5,\u03c8) via Eq. 5 9: Update \u03d5\u2190 \u03d5\u2212 \u03b1\u2202L\u2202\u03d5 , \u03c8 \u2190 \u03c8 \u2212 \u03b1 \u2202L \u2202\u03c8\n10: end for"
        },
        {
            "heading": "C.2 EXPERIMENT DETAILS",
            "text": "We utilize PyMARL (Samvelyan et al., 2019) to execute all of the baseline algorithms with their open-source codes, and the same hyperparameters are used for experiments if they are presented either in uploaded codes or in their manuscripts.\nFor a general performance evaluation, we test our methods on various maps, which require a different level of coordination according to the map\u2019s difficulties. Win-rate is computed with 160 samples: 32 episodes for each training random seed, and 5 different random seeds unless denoted otherwise.\nBoth the mean and the variance of the performance are presented for all the figures to show their overall performance according to different seeds. Especially for a fair comparison, we set ncircle, the number of training per a sampled batch of 32 episodes during training, as 1 for all baselines since some of the baselines increase ncircle = 2 as a default setting in their codes.\nFor performance comparison with baseline methods, we use their codes with fine-tuned algorithm configuration for hyperparameter settings according to their codes and original paper if available. For experiments on SMAC, we use the version of starcraft.py presented in RODE (Wang et al., 2021) adopting some modification for compatibility with QPLEX (Wang et al., 2020b). All SMAC experiments were conducted on StarCraft II version 4.10.0 in a Linux environment.\nFor Google research football task, we use the environmental code provided by (Kurach et al., 2020). In the experiments, we consider three official scenarios such as academy_3_vs_1_with_keeper (3_vs_1WK), academy_counterattack_easy (CA_easy), and academy_counterattack_hard (CA_hard).\nIn addition, for controlling rc in Eq. 10, the same hyperparameters related to curiosity-based (Zheng et al., 2021) or diversity-based exploration Chenghao et al. (2021) are adopted for EMU (QPLEX) and EMU (CDS) as well as for baselines EMC and CDS. After further experiments, we found that the curiosity-based rc from (Zheng et al., 2021) adversely influenced super hard SMAC task, with the exception of corridor scenario. Furthermore, the diversity-based exploration from Chenghao et al. (2021) led to a decrease in performance on both easy and hard SMAC maps. Thus, we decided to exclude the use of rc for EMU (QPLEX) on the super hard SMAC task and for EMU (CDS) on the easy/hard SMAC maps. EMU set task-dependent \u03b4 values as presented in Table 1. For other hyperparameters introduced by EMU, the same values presented in Table 8 are used throughout all tasks. For EMU (QPLEX) in corridor, \u03b4 = 1.3e\u2212 5 is used instead of \u03b4 = 1.3e\u2212 3."
        },
        {
            "heading": "C.3 DETAILS OF MARL TASKS",
            "text": "In this section, we specify the dimension of the state space, the action space, the episodic length, and the reward of SMAC (Samvelyan et al., 2019) and GRF (Kurach et al., 2020).\nIn SMAC, the global state of each task of SMAC includes the information of the coordinates of all agents, and features of both allied and enemy units. The action space of each agent consists of the moving actions and attacking enemies, and thus it increases according to the number of enemies. The dimensions of the state and action space and the episodic length vary according to the tasks as shown in Table 2. For reward structure, we used the shaped reward, i.e., the default reward settings of SMAC, for all scenarios. The reward is given when dealing damage to the enemies and get bonuses for winning the scenario. The reward is scaled so that the maximum cumulative reward, Rmax, that can be obtained from the episode, becomes around 20 (Samvelyan et al., 2019).\nIn GRF, the state of each task includes information on coordinates, ball possession, and the direction of players, etc. The dimension of the state space differs among the tasks as in Table 3. The action of each agent consists of moving directions, different ways to kick the ball, sprinting, intercepting the ball and dribbling. The dimensions of the action spaces for each task are the same. Table 3 summarizes the dimension of the action space and the episodic length. In GRF, there can be two reward modes: one is \"sparse reward\" and the other is \"dense reward.\" In sparse reward mode, agents get a positive reward +1 when scoring a goal and get -1 when conceding one to the opponents. In dense reward mode, agents can get positive rewards when they approach to opponent\u2019s goal, but the maximum cumulative reward is up to +1. In our experiments, we adopt sparse reward mode, and thus the maximum reward, Rmax is +1 for GRF.\nExperiments for SMAC (Samvelyan et al., 2019) are mainly carried out on NVIDIA GeForce RTX 3090 GPU, and training for the longest experiment such as corridor via EMU (CDS) took less than 18 hours. Note that when training is conducted with ncircle = 2, it takes more than one and a half times longer. Training encoder/decoder structure and updating DE with updated f\u03d5 together only took less than 2 seconds at most in corridor task. As we update f\u03d5 and f\u03c8 periodically with temb, the additional time required for a trainable embedder is certainly negligible compared to MARL training."
        },
        {
            "heading": "D FURTHER EXPERIMENT RESULTS",
            "text": ""
        },
        {
            "heading": "D.1 NEW PERFORMANCE INDEX",
            "text": "In this section, we present the details of a new performance index called overall win-rate, \u00b5\u0304w. For example, let f iw(s) be the test win-rate at training time s of ith seed run and \u00b5 i w(t) represents the time integration of f iw(s) until t. Then, a normalized overall win-rate, \u00b5\u0304w, can be expressed as\n\u00b5\u0304w(t) = 1\n\u00b5max\n1\nn \u2211n i=1 \u00b5iw(t) = 1\n\u00b5max\n1\nn \u2211n i=1 \u222b t 0 f iw(s)ds, (17)\nwhere \u00b5max = t and \u00b5\u0304w \u2208 [0, 1].\nFigure 13 illustrates the concept of time integration of win-rate, i.e., \u00b5iw(t), to construct the overall win-rate, \u00b5\u0304w. By considering the integration of win-rate of each seed case, the performance variance can be considered, and thus \u00b5\u0304w shows the training efficiency (speed) as well as the training quality (win-rate)."
        },
        {
            "heading": "D.2 ADDITIONAL EXPERIMENT RESULTS",
            "text": "In Section 4.3, we present the summary of parametric studies on \u03b4 with respect to various choices of f\u03d5. To see the training efficiency and performance at the same time, Table 4 and 5 present the overall win-rate \u00b5\u0304w according to training time. We conduct the experiments for 5 different seed cases and at each test phase 32 samples were used to evaluate the win-rate [%]. Note that we discard the component of episodic incentive rp to see the performance variations according to \u03b4 and types of f\u03d5 more clearly.\nAs Table 4 and 5 illustrate that dCAE structure for f\u03d5, which considers the reconstruction loss of global state s as in Eq. 5, shows the best training efficiency in most cases. For 5m_vs_6m task with \u03b4 = 1.3e\u22123, EmbNet achieves the highest value among f\u03d5 choices in terms of \u00b5\u0304w but fails to find optimal policy at \u03b4 = 1.3e\u22122 unlike other design choices. This implies that the reconstruction loss of dCAE facilitates the construction of a smoother embedding space for DE , enabling the retrieval of memories within a broader range of \u03b4 values from the key state. Figure 15 and 16 show the corresponding learning curves of each encoder structure for different \u03b4 values. A large \u03b4 value results in a higher performance variance than the cases with smaller \u03b4, according to different seed cases.\nThis is because a high value of \u03b4 encourages exploratory memory recall. In other words, by adjusting \u03b4, we can control the level of exploration since it controls whether to recall its own MC return or the highest value of other similar states within \u03b4. Thus, without constructing smoother embedding space as in dCAE, learning with exploratory memory recall within large \u03b4 can converge to sub-optimality as illustrated by the case of EmbNet in Figure 16(d).\nIn Figure 14 which shows the averaged number of memory recall (N\u0304call) of all memories inDE , N\u0304call of EmbNet significantly increases as training proceeds. On the other hand, dCAE was able to prevent this problem and recalled the proper memories in the early learning phase, achieving good training efficiency. Thus, embedding with dCAE can leverage a wide area of memory in DE and becomes robust to hyperparameter \u03b4."
        },
        {
            "heading": "D.3 COMPARATIVE EVALUATION ON ADDITIONAL STARCRAFT II MAPS",
            "text": "Figure 17 presents a comparative evaluation of EMU with baseline algorithms on additional SMAC maps. Adopting EMU shows performance gain in various tasks."
        },
        {
            "heading": "D.4 COMPARISON OF EMU WITH MAPPO ON SMAC",
            "text": "In this subsection, we compare the EMU with MAPPO (Yu et al., 2022) on selected SMAC maps. Figure 18 shows the performance in six SMAC maps: 1c3s5z, 3s_vs_5z, 5m_vs_6m, MMM2, 6h_vs_8z and 3s5z_vs_3s6z. Similar to the previous performance evaluation in Figure 4, Win-rate is computed with 160 samples: 32 episodes for each training random seed and 5 different random seeds. Also, for MAPPO, scenario-dependent hyperparameters are adopted from their original settings in the uploaded source code.\nFrom Figure 18, we can see that EMU performs better than MAPPO with an evident gap. Although after extensive training MAPPO showed a comparable performance against off-policy algorithm in its original paper (Yu et al., 2022), within the same training timestep used for our experiments, we found that MAPPO suffers from local convergence in super hard SMAC tasks such as MMM2 and 3s5z_vs_3s6z as shown in Figure 18. Only in 6h_vs_8z, MAPPO shows comparable performance to EMU (QPLEX) with higher performance variance across different seeds."
        },
        {
            "heading": "D.5 ADDITIONAL PARAMETRIC STUDY",
            "text": "In this subsection, we conduct an additional parametric study to see the effect of key hyperparameter \u03b4. Unlike the previous parametric study on Appendix D.2, we adopt both dCAE embedding network for f\u03d5 and episodic reward. For evaluation, we consider three GRF tasks such as academy_3_vs_1_with_keeper (3_vs_1WK), academy_counterattack_easy (CA-easy), and academy_counterattack_hard (CA-hard); and one super hard SMAC map such as 6h_vs_8z. For each task to evaluate EMU, four \u03b4 values, such as \u03b41 = 1.3e\u22127, \u03b42 = 1.3e\n\u22125, \u03b43 = 1.3e\u22123, and \u03b44 = 1.3e\u22122, are considred. Here, to compute the win-rate, 160 samples (32 episodes for each training random seed and 5 different random seeds) are used for 3_vs_1WK and 6h_vs_8z while 100 samples (20 episodes for each training random seed and 5 different random seeds) are used for CA-easy and CA-hard. Note that CDS and EMU (CDS) utilize the same hyperparameters, and EMC and EMU (QPLEX) use the same hyperparameters without a curiosity incentive presented in Zheng et al. (2021) as the model without it showed the better performance when utilizing episodic control.\nIn all cases, EMU with \u03b43 = 1.3e\u22123 shows the best performance. The tasks considered here are all complex multi-agent tasks, and thus adopting a proper value of \u03b4 benefits the overall performance and achieves the balance between exploration and exploitation by recalling the semantically similar memories from episodic memory. The optimal value of \u03b43 is consistent with the determination logic on \u03b4 in a memory efficient way presented in Appendix F.\nD.6 ADDITIONAL PARAMETRIC STUDY ON \u03bbrcon\nAdditionally, we conduct a parametric study for \u03bbrcon in Eq. 5. For each task, EMU with five \u03bbrcon values, such as \u03bbrcon,0 = 0.01, \u03bbrcon,1 = 0.1, \u03bbrcon,2 = 0.5, \u03bbrcon,3 = 1.0 and \u03bbrcon,4 = 10, are evaluated. Here, to compute the win-rate of each case, 160 samples (32 episodes for each training random seed and 5 different random seeds) are used. From Figure 20, we can see that broad range of\n(a) 3s5z (SMAC) (b) 3s_vs_5z (SMAC)\nFigure 20: Parametric study for \u03bbrcon.\n\u03bbrcon \u2208 { 0.1, 0.5, 1.0 }\nwork well in general. However, with large \u03bbrcon as \u03bbrcon,4 = 10, we can observe that some performance degradation at the early learning phase in 3s5z task. This result is in line with the learning trends of Case 1 and Case 2 of 3s5z in Figure 23, which do not consider prediction loss and only take into account the reconstruction loss. Thus, considering both prediction loss and reconstruction loss as Case 4 in Eq. 5 with proper \u03bbrcon is essential to optimize the overall learning performance."
        },
        {
            "heading": "D.7 ADDITIONAL ABLATION STUDY IN GRF",
            "text": "In this subsection, we conduct additional ablation studies via GRF tasks to see the effect of episodic incentive. Again, EMU (CDS-No-EI) ablates episodic incentive from EMU (CDS) and utilizes the conventional episodic control presented in Eq. 3 instead. Again, EMU (CDS-No-SE) ablates semantic embedding by dCAE and adopts random projection with episodic incentive rp. In both tasks, utilizing episodic memory with the proposed embedding function improves the overall performance compared to the original CDS algorithm. By adopting episodic incentives instead of conventional episodic control, EMU (CDS) achieves better learning efficiency and rapidly converges to optimal policies compared to EMU (CDS-No-EI)."
        },
        {
            "heading": "D.8 ADDITIONAL ABLATION STUDY ON EMBEDDING LOSS",
            "text": "In our case, the autoencoder uses the reconstruction loss to enforce the embedded representation x to contain the full information of the original feature, s. We are adding (Ht \u2212 fH\u03c8 (f\u03d5(st|t)|t))2 to guide the embedded representation to be consistent to Ht, as well, which works as a regularizer to the autoencoder. Therefore, fH\u03c8 is used in Eq. 5 to predict the observed Ht from DE as a part of the semantic regularization effort.\nBecauseHt is different from fH\u03c8 (xt), the effort of minimizing their difference becomes the regularizer creating a gradient signal to learn \u03c8 and \u03d5. The update of \u03d5 results in the updated x influenced by the regularization. Note that we update \u03d5 through the backpropagation of \u03c8.\nThe case of L(\u03d5, \u03c8) = ||st \u2212 fs\u03c8(f\u03d5(st|t)|t)||22 occurs when \u03bbrcon becomes relatively much higher than 1, which makes (Ht \u2212 fH\u03c8 (f\u03d5(st|t)|t))2 becomes ineffective. In other words, when \u03bbrcon in Eq. 5 becomes relatively much higher than 1, (Ht \u2212 fH\u03c8 (f\u03d5(st|t)|t))2 becomes ineffective.\nThe case of L(\u03d5, \u03c8) = (Ht\u2212fH\u03c8 (f\u03d5(st|t)|t))2 occurs when the scale factor \u03bbrcon becomes relatively much smaller than 1, which makes (Ht \u2212 fH\u03c8 (f\u03d5(st|t)|t))2 become a dominant factor. We conduct ablation studies considering four cases as follows:\n\u2022 Case 1: L(\u03d5, \u03c8) = ||st \u2212 fs\u03c8(f\u03d5(st))||22, presented in Figure 22(a)\n\u2022 Case 2: L(\u03d5, \u03c8) = ||st \u2212 fs\u03c8(f\u03d5(st|t)|t)||22, presented in Figure 22(b)\n\u2022 Case 3: L(\u03d5, \u03c8) = (Ht \u2212 fH\u03c8 (f\u03d5(st|t)|t))2, presented in Figure 22(c)\n\u2022 Case 4: L(\u03d5, \u03c8) = (Ht \u2212 fH\u03c8 (f\u03d5(st|t)|t))2 + \u03bbrcon||st \u2212 fs\u03c8(f\u03d5(st|t)|t)||22, i.e., Eq. 5, presented in Figure 22(d)\nWe visualize the result of t-SNE of 50K samples x \u2208 DE out of 1M memory data trained by various loss functions: The task was 3s_vs_5z of SMAC as in Figure 2 and the training for all models proceeds for 1.5mil training steps. Case 1 and Case 2 showed irregular return distribution across the embedding space. In those two cases, there was no consistent pattern in the reward distribution. Case 3 with only return prediction in the loss showed better patterns compared to Case 1 and 2 but some features are not clustered well. We suspect that the consistent state representation also contributes to the return prediction. Case 4 of our suggested loss showed the most regular pattern in the return distribution arranging the low-return states as a cluster and the states with desirable returns as another\ncluster. In Figure 23, Case 4 shows the best performance in terms of both learning efficiency and terminal win-rate."
        },
        {
            "heading": "D.9 ADDITIONAL ABLATION STUDY ON SEMANTIC EMBEDDING",
            "text": "To further understand the role of semantic embedding, we conduct additional ablation studies and present them with the general performance of other baseline methods. Again, EMU (CDS-No-SE) ablates semantic embedding by dCAE and adopts random projection instead, along with episodic incentive rp.\nFor relatively easy tasks, EMU (QPLEX-No-SE) and EMU (CDS-No-SE) show comparable performance at first but they converge on sub-optimal policy in most tasks. Especially, this characteristic is well observed in the case of EMU (CDS-No-SE). As large size of memories are stored in an episodic buffer as training goes on, the probability of recalling similar memories increases. However, with random projection, semantically incoherent memories can be recalled and thus it can adversely affect the value estimation. We deem this is the reason for the convergence on suboptimal policy in the case of EMU (No-SE). Thus we can conclude that recalling semantically coherent memory is an essential component of EMU.\nD.10 ADDITIONAL ABLATION ON rc\nIn Eq.10, we introduce rc as an additional reward which may encourage exploratory behavior or coordination. The reason we introduce rc is to show that EMU can be used in conjunction with any form of incentive encouraging further exploration. Our method may not be strongly effective until some desired states are found, although it has exploratory behavior via the proposed semantic embeddings, controlled by \u03b4. Until then, such incentives could be beneficial to find desired or goal states. Figures 26-27 show the ablation study of with and without rc, and the contribution of rc is limited compared to rp."
        },
        {
            "heading": "D.11 COMPARISON OF EPISODIC INCENTIVE WITH EXPLORATORY INCENTIVE",
            "text": "In this subsection, we replace the episodic incentive with another exploratory incentive, introduced by (Henaff et al., 2022). In (Henaff et al., 2022), the authors extend the count-based episodic bonuses to continuous spaces by introducing episodic elliptical bonuses for exploration. In this concept, a high reward is given when the state projected in the embedding space is different from the previous states within the same episode. In detail, with a given feature encoder \u03d5, the elliptical bonus bt at timestep t is computed as follows:\nbt = \u03d5(st) TC\u22121t \u03d5(st) (18)\nwhere C\u22121t is an inverse covariance matrix with an initial value of C \u22121 t=0 = 1/\u03bbe3bI . Here, \u03bbe3b is a covariance regularizer. For update inverse covariance, the authors suggested a computationally efficient update as\nC\u22121t+1 = C \u22121 t \u2212\n1\n1 + bt+1 uuT (19)\nwhere u = C\u22121t \u03d5(st+1). Then, the final reward r\u0304t with episodic elliptical bonuses bt is expressed as\nr\u0304t = rt + \u03b2e3bbt (20)\nwhere \u03b2e3b and rt are a corresponding scale factor and external reward given by the environment, respectively.\nFor this comparison, we utilize the dCAE structure as a state embedding function \u03d5. For a mixer, QPLEX (Wang et al., 2020b) is adopted for all cases, and we denote the case with an elliptical incentive instead of the proposed episodic incentive as QPLEX (SE+E3B). Figure 28 illustrates\nthe performance of adopting an elliptical incentive for exploration instead of the proposed episodic incentive. QPLEX (SE+E3B) uses the same hyperparameters with EMU (SE+EI) and we set \u03bbe3b = 0.1 according to Henaff et al. (2022).\nAs illustrated by Figure 28, adopting an elliptical incentive presented by (Henaff et al., 2022) instead of an episodic incentive does not give any performance gain and even adversely influences the performance compared to QPLEX. It seems that adding excessive surprise-based incentives can be a disturbance in MARL tasks since finding a new state itself does not guarantee better coordination among agents. In MARL, agents need to find the proper combination of joint action in a given similar observations when finding an optimal policy. On the other hand, in high-dimensional pixelbased single-agent tasks such as Habitat (Ramakrishnan et al., 2021), finding a new state itself can be beneficial in policy optimization. From this, we can note that adopting a certain algorithm from a single-agent RL case to MARL case may require a modification or adjustment with domain knowledge.\nAs a simple tuning, we conduct parametric study for \u03b2e3b = {0.01, 0.1} to adjust magnitude of incentive of E3B. Figure 29 illustrates the results. In Figure 29, QPLEX (SE+E3B) with \u03b2e3b = 0.01 shows a better performance than the case with \u03b2e3b = 0.1 and comparable performance to EMC in 5m_vs_6m. However, EMU with the proposed episodic incentive shows the best performance. From this comparison, we can see that incentives proposed by previous work need to be adjusted\naccording to the type of tasks, as it was done in EMC (Zheng et al., 2021). On the other hand, with the proposed episodic incentive we do not need such hyperparameter-scaling, allowing much more flexible application across various tasks."
        },
        {
            "heading": "D.12 ADDITIONAL TOY EXPERIMENT AND APPLICABILITY TESTS",
            "text": "In this section, we conduct additional experiments on the didactic example presented by (Zheng et al., 2021) to see how the proposed method would behave in a simple but complex coordination task. Additionally, by defining Rthr to define the desirability presented in Definition 1, we can extend EMU to a single-agent RL task, where a strict goal is not defined in general.\nDidactic experiment on Gridworld We adopt the didactic example such as gridworld environment from (Zheng et al., 2021) to demonstrate the motivation and how the proposed method can overcome the existing limitations of the conventional episodic control. In this task, two agents in gridworld (see Figure 30(a)) need to reach their goal states at the same time to get a reward r = 10 and if only one arrives first, they get a penalty with the amount of \u2212p. Please refer to (Zheng et al., 2021) for further details.\nTo see the sole effect of the episodic control, we discard the curiosity incentive part of EMC, and for a fair comparison, we set the same exploration rate of \u03f5-greedy with T\u03f5 = 200K for all algorithms. We evaluate the win-rate with 180 samples (30 episodes for each training random seed and 6 different random seeds) at each training time. Notably, adopting episodic control with a naive utilization suffers from local convergence (see QPLEX and EMC (QPLEX) in Figure 30(b)), even though it expedites learning efficiency at the early training phase. On the other hand, EMU shows more robust performance under different seed cases and achieves the best performance by an efficient and discreet utilization of episodic memories.\nApplicability test to single agent RL task We first need to define Rthr value to effectively apply EMU to a single-agent task where a goal of an episode is generally not strictly defined, unlike cooperative multi-agent tasks with a shared common goal.\nIn a single-agent task where the action space is continuous such as MuJoCo (Todorov et al., 2012), the actor-critic method is often adopted. Efficient memory utilization of EMU can be used to train the critic network and thus indirectly influence policy learning, unlike general cooperative MARL tasks where value-based RL is often considered.\nWe implement EMU on top of TD3 and use the open-source code presented in (Fujimoto et al., 2018). We begin to train the model after sufficient data is stored in the replay buffer and conduct 6 times of training per episode with 256 mini-batches. Note that this is different from the default settings of RL training, which conducts training at each timestep. Our modified setting aims to see the effect on the sample efficiency of the proposed model. The performance of the trained model is evaluated at every 50k timesteps.\nWe use the same hyperparameter settings as in MARL task presented in Table 8 except for the update interval, temb = 100K according to large episodic timestep in single-RL compared to MARL tasks. It is worth mentioning that additional customized parameter settings for single-agent tasks may further improve the performance. In our evaluation, three single-agent tasks such as Hopper-v4, Walker2D-v4 and Humanoid-v4 are considered, and Figure 32 illustrates each task. Here, \u03b42 = 1.3e \u2212 5 is used for Hopper-v4 and Walker2D-v4, and \u03b43 = 1.3e \u2212 3 is used for Humanoid-v4 as Humanoid-v4 task contains much higher state dimension space as 376-dimension. Please refer to Todorov et al. (2012) for a detailed description of tasks.\nIn Figure 32, EMU (TD3) shows the performance improvement compared to the original TD3. Thanks to semantically similar memory recall and episodic incentive, states deemed desirable could have high values, and trained policy is encouraged to visit them more frequently. As a result, EMU (TD3) shows the better performance. Interestingly, under state dimension as Humanoid-v4 task, TD3 and EMU (TD3) show a distinct performance gap in the early training phase. This is because, in a task with a high-dimensional state space, it is hard for a critic network to capture important features determining the value of a given state. Thus, it takes longer to estimate state value accurately. However, with the help of semantically similar memory recall and error compensation through episodic incentive, a critic network in EMU (TD3) can accurately estimate the value of the state much faster than the original TD3, leading to faster policy optimization.\nUnlike cooperative MARL tasks, single-RL tasks normally do not have a desirability threshold. Thus, one may need to determine Rthr based on domain knowledge or a preference for the level of return to be deemed successful. Figure 33 presents a performance variation according to Rthr.\nWhen we set Rthr = 1000 in Walker2d task, desirability signal is rarely obtained compared to the case with Rthr = 500 in the early training phase. Thus, EMU with Rthr = 500 shows the better performance. However, both cases of EMU show better performance compared to the original TD3. In Hopper task, both cases of Rthr = 500 and Rthr = 1000 show the similar performance. Thus,\nwhen determining Rthr, it can be beneficial to set a small value rather than a large one that can be hardly obtained.\nAlthough setting a small Rthr does not require much domain knowledge, a possible option to detour this is a periodic update of desirability based on the average return value H(s) in all s \u2208 DE . In this way, a certain state with low return which was originally deemed as desirable can be reevaluated as undesirable as training proceeds. The episodic incentive is not further given to those undesirable states.\nScalability to image-based single-agent RL task Although MARL tasks already contain highdimension state space such as 322-dimension in MMM2 and 282-dimension in corridor, imagebased single RL tasks, such as Atari Bellemare et al. (2013) game, often accompany higher state spaces such as [210x160x3] for \"RGB\" and [210x160] for \"grayscale\". We use the \"grayscale\" type for the following experiments. For the details of the state space in MARL task, please see Appendix C.3.\nIn an image-based task, storing all state values to update all the key values inDE as f\u03d5 updates can be memory-inefficient, and a semantic embedding from original states may become overhead compared to the case without it. In such case, one may resort to a pre-trained feature extraction model such as ResNet model provided by torch-vision in a certain amount for dimension reduction only, before passing through the proposed semantic embedding. The feature extraction model above is not an object of training.\nAs an example, we implement EMU on the top of DQN model and compare it with the original DQN on Atari task. For the EMU (DQN), we adopt some part of pre-trained ResNet18 presented by torch-vision for dimensionality reduction, before passing an input image to semantic embedding. At each epoch, 320 random samples are used for training in Breakout task, and 640 random samples are used in Alien task. The same mini-batch size of 32 is used for both cases. For f\u03d5 training, the same parameters presented in Table 8 are adopted except for the temb = 10K considering the timestep of single RL task. We also use the same \u03b42 = 1.3e\u2212 5 and set Rthr = 50 for Breakout and Rthr = 40 for Alien, respectively. Please refer to Bellemare et al. (2013) and https://gymnasium.farama.org/environments/atari for task details. As in Figure 34, we found a performance gain by adopting EMU on high-dimensional image-based tasks."
        },
        {
            "heading": "E TRAINING ALGORITHM",
            "text": ""
        },
        {
            "heading": "E.1 MEMORY CONSTRUCTION",
            "text": "During the centralized training, we can access the information on whether the episodic return reaches the highest return Rmax or threshold Rthr, i.e., defeating all enemies in SMAC or scoring a goal in GRF. When storing information to DE , by the definition presented Definition. 1, we set \u03be(s) = 1 for \u2200s \u2208 T\u03be. For efficient memory construction, we propagate the desirability of the state to a similar state within the threshold \u03b4. With this desirability propagation, similar states have an incentive for a visit. In addition, once a memory is saved inDE , the memory is preserved until it becomes obsolete (the oldest memory to be recalled). When a desirable state is found near the existing suboptimal memory within \u03b4, we replace the suboptimal memory with the desirable one, which gives the effect of a memory shift to the desirable state. Algorithm 2 presents the memory construction with the desirability propagation and memory shift.\nAlgorithm 2 Episodic memory construction 1: \u03beT : Optimality of trajectory 2: T = {s0,a0, r0, s1, ..., sT }: Episodic trajectory 3: Initialize Rt = 0 4: for t = T to 0 do 5: Compute xt = f\u03d5(st) and yt = (xt \u2212 \u00b5\u0302x)/\u03c3\u0302x 6: pick the nearest neighbor x\u0302t \u2208 DE and get y\u0302t. 7: if ||y\u0302t \u2212 yt||2 < \u03b4 then 8: Ncall(x\u0302t)\u2190 Ncall(x\u0302t) + 1 9: if \u03beT == 1 then 10: N\u03be(x\u0302t)\u2190 N\u03be(x\u0302t) + 1 11: end if 12: if \u03bet == 0 and \u03beT == 1 then 13: \u03bet \u2190 \u03beT \u25b7 desirability propagation 14: x\u0302t \u2190 xt, y\u0302t \u2190 yt, s\u0302t \u2190 st \u25b7 memory shift 15: H\u0302t \u2190 Rt 16: else 17: if H\u0302t < Rt then H\u0302t \u2190 Rt 18: end if 19: end if 20: else 21: Add memory DE \u2190 (xt, yt, Rt, st, \u03bet) 22: end if 23: end for\nFor memory capacity and latent dimension, we used the same values as Zheng et al. (2021), and Table 6 shows the summary of hyperparameter related to episodic memory.\nThe memory construction for EMU seems to require a significantly large memory space, especially for saving global states s. However, DE uses CPU memory instead of GPU memory, and the memory required for the proposed embedder structure is minimal compared to the memory usage of original\nRL training (<1%). Thus, a memory burden due to a trainable embedding structure is negligible. Table 7 presents examples of CPU memory usage to save global states s \u2208 DE ."
        },
        {
            "heading": "E.2 OVERALL TRAINING ALGORITHM",
            "text": "In this section, we present details of the overall MARL training algorithm including training of f\u03d5. Additional hyperparameters related to Algorithm 1 to update encoder f\u03d5 and decoder f\u03c8 are presented in Table 8. Note that variables N and B are consistent with Algorithm 1.\nAlgorithm 3 presents the pseudo-code of overall training for EMU. In Algorithm 3, network parameters related to a mixer and individual Q-network are denoted as \u03b8, and double Q-learning with target network is adopted as other baseline methods (Rashid et al., 2018; 2020; Wang et al., 2020b; Zheng et al., 2021; Chenghao et al., 2021).\nAlgorithm 3 EMU: Efficient episodic Memory Utilization for MARL 1: D: Replay buffer 2: DE : Episodic buffer 3: Qi\u03b8: Individual Q-network of n agents 4: M : Batch size of RL training 5: Initialize network parameters \u03b8, \u03d5, \u03c8 6: while tenv \u2264 tmax do 7: Interact with the environment via \u03f5-greedy policy based on [Qi\u03b8] n i=1 and get a trajectory T .\n8: Run Algorithm 2 to update DE with T 9: Append T to D\n10: for k = 1 to ncircle do 11: Get M sample trajectories [T ]Mi=1 \u223c D 12: Run MARL training algorithm using [T ]Mi=1 and DE , to update \u03b8 with Eq.10 13: end for 14: if tenv mod temb == 0 then 15: Run Algorithm 1 to update \u03d5, \u03c8 16: Update all x \u2208 DE with updated f\u03d5 17: end if 18: end while\nHere, any CTDE training algorithm can be adopted for MARL training algorithm in line 12 in Algorithm 3. As we mentioned in Section C.4, training of f\u03d5 and f\u03c8 and updating all x \u2208 DE only\ntakes less than two seconds at most under the task with largest state dimension such as corridor. Thus, the computation burden for trainable embedder is negligible compared to the original MARL training."
        },
        {
            "heading": "F MEMORY UTILIZATION",
            "text": "A remaining issue in utilizing episodic memory is how to determine a proper threshold value \u03b4 in Eq. 1. Note that this \u03b4 is used for both updating the memory and recalling the memory. One simple option is determining \u03b4 based on prior knowledge or experience, such as hyperparameter tuning. Instead, in this section, we present a more memory-efficient way for \u03b4 selection. When computing ||x\u0302 \u2212 x||2 < \u03b4, the similarity is compared elementwisely. However, this similarity measure puts a different weight on each dimension of x since each dimension of x could have a different range of distribution. Thus, instead of x, we utilize the normalized value. Let us define a normalized embedding y with the statistical mean (\u00b5x) and variance (\u03c3x) of x as\ny = (x\u2212 \u00b5x)/\u03c3x. (21)\nHere, the normalization is conducted for each dimension of x. Then, the similarity measure via ||y\u0302 \u2212 y||2 < \u03b4 with Eq. 21 puts an equal weight to each dimension, as y has a similar range of distribution in each dimension. In addition, an affine projection of Eq. 21 maintains the closeness of original x-distribution, and thus we can safely utilize y-distribution instead of x-distribution to measure the similarity.\nIn addition, y defined in Eq. 21 nearly follows the normal distribution, although it does not strictly follow it. This is due to the fact that the memorized samples x in DE do not originate from the same distribution, nor are they uncorrelated, as they can stem from the same episode. However, we can achieve an approximate coverage of the majority of the distribution, specifically 3\u03c3y in both positive and negative directions of y, by setting \u03b4 as\n\u03b4 \u2264 (2\u00d7 3\u03c3y) dim(y)\nM . (22)\nFor example, when M = 1e6 and dim(y) = 4, if \u03c3y \u2248 1 then \u03b4 \u2264 0.0013. This is the reason we select \u03b4 = 0.0013 for the exploratory memory recall."
        }
    ],
    "year": 2024
}