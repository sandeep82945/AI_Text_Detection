{
    "abstractText": "Graph-based semi-supervised learning, particularly in the context of extremely sparse labeled data, often suffers from degenerate solutions where label functions tend to be nearly constant across unlabeled data. In this paper, we introduce Variance-enlarged Poisson Learning (VPL), a simple yet powerful framework tailored to alleviate the issues arising from the presence of degenerate solutions. VPL incorporates a variance-enlarged regularization term, which induces a Poisson equation specifically for unlabeled data. This intuitive approach increases the dispersion of labels from their average mean, effectively reducing the likelihood of degenerate solutions characterized by nearly constant label functions. We subsequently introduce two streamlined algorithms, V-Laplace and V-Poisson, each intricately designed to enhance Laplace and Poisson learning, respectively. Furthermore, we broaden the scope of VPL to encompass graph neural networks, introducing Variance-enlarged Graph Poisson Networks (V-GPN) to facilitate improved label propagation. To achieve a deeper understanding of VPL\u2019s behavior, we conduct a comprehensive theoretical exploration in both discrete and variational cases. Our findings elucidate that VPL inherently amplifies the importance of connections within the same class while concurrently tempering those between different classes. We support our claims with extensive experiments, demonstrating the effectiveness of VPL and showcasing its superiority over existing methods. The code is available at https://github.com/hitcszx/VPL.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiong Zhou"
        },
        {
            "affiliations": [],
            "name": "Xianming Liu"
        },
        {
            "affiliations": [],
            "name": "Hao Yu"
        },
        {
            "affiliations": [],
            "name": "Jialiang Wang"
        },
        {
            "affiliations": [],
            "name": "Zeke Xie"
        },
        {
            "affiliations": [],
            "name": "Junjun Jiang"
        },
        {
            "affiliations": [],
            "name": "Xiangyang Ji"
        }
    ],
    "id": "SP:7a79f96e28baa9c4f212c73c5ade81091a4c879e",
    "references": [
        {
            "authors": [
                "Morteza Alamgir",
                "Ulrike Luxburg"
            ],
            "title": "Phase transition in the family of p-resistances",
            "venue": "Advances in neural information processing systems,",
            "year": 2011
        },
        {
            "authors": [
                "Aamir Anis",
                "Aly El Gamal",
                "A Salman Avestimehr",
                "Antonio Ortega"
            ],
            "title": "A sampling theory perspective of graph-based semi-supervised learning",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2018
        },
        {
            "authors": [
                "Gunnar Aronsson",
                "Michael Crandall",
                "Petri Juutinen"
            ],
            "title": "A tour of the theory of absolutely minimizing functions",
            "venue": "Bulletin of the American mathematical society,",
            "year": 2004
        },
        {
            "authors": [
                "Mikhail Belkin",
                "Irina Matveeva",
                "Partha Niyogi"
            ],
            "title": "Regularization and semi-supervised learning on large graphs",
            "venue": "In Learning Theory: 17th Annual Conference on Learning Theory, COLT",
            "year": 2004
        },
        {
            "authors": [
                "Mikhail Belkin",
                "Irina Matveeva",
                "Partha Niyogi"
            ],
            "title": "Tikhonov regularization and semi-supervised learning on large graphs",
            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing,",
            "year": 2004
        },
        {
            "authors": [
                "David Berthelot",
                "Nicholas Carlini",
                "Ian Goodfellow",
                "Nicolas Papernot",
                "Avital Oliver",
                "Colin A Raffel"
            ],
            "title": "Mixmatch: A holistic approach to semi-supervised learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Avrim Blum",
                "Tom Mitchell"
            ],
            "title": "Combining labeled and unlabeled data with co-training",
            "venue": "In Proceedings of the eleventh annual conference on Computational learning theory,",
            "year": 1998
        },
        {
            "authors": [
                "Nick Bridle",
                "Xiaojin Zhu"
            ],
            "title": "p-voltages: Laplacian regularization for semi-supervised learning on high-dimensional data",
            "venue": "In Eleventh Workshop on Mining and Learning with Graphs (MLG2013),",
            "year": 2013
        },
        {
            "authors": [
                "Thomas B\u00fchler",
                "Matthias Hein"
            ],
            "title": "Spectral clustering based on the graph p-laplacian",
            "venue": "In Proceedings of the 26th annual international conference on machine learning,",
            "year": 2009
        },
        {
            "authors": [
                "Jeff Calder",
                "Dejan"
            ],
            "title": "Slep\u010dev. Properly-weighted graph laplacian for semi-supervised learning",
            "venue": "Applied mathematics & optimization,",
            "year": 2020
        },
        {
            "authors": [
                "Jeff Calder",
                "Brendan Cook",
                "Matthew Thorpe",
                "Dejan Slepcev"
            ],
            "title": "Poisson learning: Graph based semi-supervised learning at very low label rates",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Kaiming He"
            ],
            "title": "Exploring simple siamese representation learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Alan Egger",
                "Robert Huotari"
            ],
            "title": "Rate of convergence of the discrete p\u00f3lya algorithm",
            "venue": "Journal of approximation theory,",
            "year": 1990
        },
        {
            "authors": [
                "Ahmed El Alaoui",
                "Xiang Cheng",
                "Aaditya Ramdas",
                "Martin J Wainwright",
                "Michael I Jordan"
            ],
            "title": "Asymptotic behavior of lp-based laplacian regularization in semi-supervised learning",
            "venue": "In Conference on Learning Theory,",
            "year": 2016
        },
        {
            "authors": [
                "S Fralick"
            ],
            "title": "Learning to recognize patterns without a teacher",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 1967
        },
        {
            "authors": [
                "Cristina Garcia-Cardona",
                "Ekaterina Merkurjev",
                "Andrea L Bertozzi",
                "Arjuna Flenner",
                "Allon G Percus"
            ],
            "title": "Multiclass data segmentation using diffuse interface methods on graphs",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2014
        },
        {
            "authors": [
                "Chen Gong",
                "Dacheng Tao",
                "Wei Liu",
                "Liu Liu",
                "Jie Yang"
            ],
            "title": "Label propagation via teaching-tolearn and learning-to-teach",
            "venue": "IEEE transactions on neural networks and learning systems,",
            "year": 2016
        },
        {
            "authors": [
                "Yves Grandvalet",
                "Yoshua Bengio"
            ],
            "title": "Semi-supervised learning by entropy minimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2004
        },
        {
            "authors": [
                "Alexander Jung",
                "Alfred O Hero III",
                "Alexandru Mara",
                "Saeed Jahromi"
            ],
            "title": "Semi-supervised learning via sparse label propagation",
            "venue": "arXiv preprint arXiv:1612.01414,",
            "year": 2016
        },
        {
            "authors": [
                "Feng Kang",
                "Rong Jin",
                "Rahul Sukthankar"
            ],
            "title": "Correlated label propagation with application to multi-label learning",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906),",
            "year": 2006
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "A. Krizhevsky",
                "G. Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Computer Science Department, University of Toronto, Tech. Rep,",
            "year": 2009
        },
        {
            "authors": [
                "Rasmus Kyng",
                "Anup Rao",
                "Sushant Sachdeva",
                "Daniel A Spielman"
            ],
            "title": "Algorithms for lipschitz learning on graphs",
            "venue": "In Conference on Learning Theory,",
            "year": 2015
        },
        {
            "authors": [
                "Yann LeCun",
                "L\u00e9on Bottou",
                "Yoshua Bengio",
                "Patrick Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE,",
            "year": 1998
        },
        {
            "authors": [
                "L\u00e1szl\u00f3 Lov\u00e1sz",
                "Mikl\u00f3s Simonovits"
            ],
            "title": "The mixing rate of markov chains, an isoperimetric inequality, and computing the volume",
            "venue": "In Proceedings [1990] 31st annual symposium on foundations of computer science,",
            "year": 1990
        },
        {
            "authors": [
                "Xiaoyi Mai",
                "Romain Couillet"
            ],
            "title": "Random matrix-inspired improved semi-supervised learning on graphs",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Boaz Nadler",
                "Stephane Lafon",
                "Ioannis Kevrekidis",
                "Ronald Coifman"
            ],
            "title": "Diffusion maps, spectral clustering and eigenfunctions of fokker-planck operators",
            "venue": "Advances in neural information processing systems,",
            "year": 2005
        },
        {
            "authors": [
                "Boaz Nadler",
                "Nathan Srebro",
                "Xueyuan Zhou"
            ],
            "title": "Semi-supervised learning with the graph laplacian: The limit of infinite unlabelled data",
            "venue": "Advances in neural information processing systems,",
            "year": 2009
        },
        {
            "authors": [
                "Adam M Oberman"
            ],
            "title": "Finite difference methods for the infinity laplace and p-laplace equations",
            "venue": "Journal of Computational and Applied Mathematics,",
            "year": 2013
        },
        {
            "authors": [
                "Mauricio Flores Rios",
                "Jeff Calder",
                "Gilad Lerman"
            ],
            "title": "Algorithms for lp-based semi-supervised learning on graphs",
            "venue": "arXiv preprint arXiv:1901.05031,",
            "year": 1901
        },
        {
            "authors": [
                "Prithviraj Sen",
                "Galileo Namata",
                "Mustafa Bilgic",
                "Lise Getoor",
                "Brian Galligher",
                "Tina Eliassi-Rad"
            ],
            "title": "Collective classification in network data",
            "venue": "AI magazine,",
            "year": 2008
        },
        {
            "authors": [
                "Zuoqiang Shi",
                "Stanley Osher",
                "Wei Zhu"
            ],
            "title": "Weighted nonlocal laplacian on interpolation from sparse data",
            "venue": "Journal of Scientific Computing,",
            "year": 2017
        },
        {
            "authors": [
                "Kihyuk Sohn",
                "David Berthelot",
                "Nicholas Carlini",
                "Zizhao Zhang",
                "Han Zhang",
                "Colin A Raffel",
                "Ekin Dogus Cubuk",
                "Alexey Kurakin",
                "Chun-Liang Li"
            ],
            "title": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Zixing Song",
                "Xiangli Yang",
                "Zenglin Xu",
                "Irwin King"
            ],
            "title": "Graph-based semi-supervised learning: A comprehensive review",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2022
        },
        {
            "authors": [
                "V. Vapnik",
                "A. Chervonenkis"
            ],
            "title": "Theory of Pattern Recognition [in Russian",
            "venue": "Nauka, Moscow,",
            "year": 1974
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio"
            ],
            "title": "Graph attention networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Sheng Wan",
                "Yibing Zhan",
                "Liu Liu",
                "Baosheng Yu",
                "Shirui Pan",
                "Chen Gong"
            ],
            "title": "Contrastive graph poisson networks: Semi-supervised learning with extremely limited labels",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Bo Wang",
                "Zhuowen Tu",
                "John K Tsotsos"
            ],
            "title": "Dynamic label propagation for semi-supervised multiclass multi-label classification",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2013
        },
        {
            "authors": [
                "Han Xiao",
                "Kashif Rasul",
                "Roland Vollgraf"
            ],
            "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
            "venue": "arXiv preprint arXiv:1708.07747,",
            "year": 2017
        },
        {
            "authors": [
                "Dengyong Zhou",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Learning from labeled and unlabeled data using random walks",
            "venue": "In Joint Pattern Recognition Symposium,",
            "year": 2004
        },
        {
            "authors": [
                "Dengyong Zhou",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Regularization on discrete spaces",
            "venue": "In Joint Pattern Recognition Symposium,",
            "year": 2005
        },
        {
            "authors": [
                "Dengyong Zhou",
                "Olivier Bousquet",
                "Thomas Lal",
                "Jason Weston",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Learning with local and global consistency",
            "venue": "Advances in neural information processing systems,",
            "year": 2003
        },
        {
            "authors": [
                "Xueyuan Zhou",
                "Mikhail Belkin"
            ],
            "title": "Semi-supervised learning by higher order regularization",
            "venue": "In Proceedings of the fourteenth international conference on artificial intelligence and statistics,",
            "year": 2011
        },
        {
            "authors": [
                "Xiaojin Zhu"
            ],
            "title": "Semi-supervised learning literature survey. Semi-Supervised Learning Literature Survey",
            "venue": "Technical report, Computer Sciences, University of Wisconsin-Madisoa,",
            "year": 2006
        },
        {
            "authors": [
                "Xiaojin Zhu",
                "Zoubin Ghahramani",
                "John D Lafferty"
            ],
            "title": "Semi-supervised learning using gaussian fields and harmonic functions",
            "venue": "In Proceedings of the 20th International conference on Machine learning",
            "year": 2003
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Semi-supervised learning (SSL) is a classical machine learning paradigm, which is driven by the desire to harness the vast amount of unlabeled data that exists in the real world, alongside the limited availability of labeled data (Fralick, 1967; Vapnik & Chervonenkis, 1974; Blum & Mitchell, 1998; Berthelot et al., 2019). Traditional supervised learning methods heavily rely on labeled examples for training, which can be costly and time-consuming to obtain, especially for complex tasks in domains like computer vision, natural language processing, and healthcare. Semi-supervised learning seeks to bridge this gap by leveraging both labeled and unlabeled data to improve model performance and generalization. By incorporating the vast pool of unlabeled data, semi-supervised learning algorithms aim to extract more meaningful and robust representations, thereby enhancing the scalability, efficiency, and accuracy of machine learning models (Sohn et al., 2020). The emphasis of this paper is on scenarios where the labeled data is exceptionally scarce. This situation is common in many real-world applications, such as medical diagnosis, where obtaining labeled data can be expensive or time-consuming. The challenge lies in effectively utilizing this limited labeled information to improve model performance.\nGraph-based methods provide a powerful framework for leveraging the underlying structure or relationships within the data (Zhu, 2006; Anis et al., 2018; Song et al., 2022). By constructing a graph where nodes represent data points and edges denote relationships or similarities, information can be propagated across the graph to make predictions on unlabeled data. This approach is particularly advantageous in scenarios where data points are not independent, but rather have dependencies or\n\u2217Correspondence to: Xianming Liu <csxm@hit.edu.cn>\nconnections that can be exploited for learning. In the realm of graph-based SSL, the following optimization problem is considered:\nmin u J (u) \u225c \u2211 ij wijL (u(xi), u(xj)) , (1.1)\nwith some specific constraints applied to labeled points. Herein, wij denotes the edge weight between vertices xi and xj , and L is the measure of distance between two labels. When L takes on the form of \u2113p distance and the labeled points adhere the boundary conditions u(xi) = yi for i \u2208 [l], the problem above is referred to as p-Laplace learning (Zhou & Sch\u00f6lkopf, 2005; B\u00fchler & Hein, 2009). This appellation originates from that at p = 2 it is initially known as Laplace learning or label propagation (Zhu et al., 2003). When L(u(xi), u(xi)) = w p\u22121 ij \u2225u(xi)\u2212 u(xj)\u2225pp, the problem in Eq. 1.1 under certain regularity assumptions follows that the limit of the (unique) minimizers of J (as p grows to infinity) is the lex-minimizer for Lipschitz learning (Egger & Huotari, 1990; Aronsson et al., 2004; Kyng et al., 2015). Moreover, many variants of Laplace learning have been proposed with various types of soft label constraints in place of hard constraints (Zhou et al., 2003; Belkin et al., 2004b;a; Kang et al., 2006; Wang et al., 2013; Gong et al., 2016).\nHowever, it is found that, in the context of large graphs or scenarios with extremely sparse labeled data, distances based on graph Laplacians do not conform to the intuition that nodes within the same clusters can be interconnected via many paths of high probability (Nadler et al., 2005). Instead, they tend to converge to non-informative functions, which results in degenerate solutions that take the form of an almost everywhere constant label function, thereby significantly deteriorating the accuracy performance (Nadler et al., 2009; El Alaoui et al., 2016; Calder et al., 2020). This undesirable property is usually attributed to the fact that long random walks tend to forget their starting position, leading to the mixing of random walks (Lov\u00e1sz & Simonovits, 1990). Several alternative methods have been proposed to address this issue, including higher order graph Laplacian distances (Alamgir & Luxburg, 2011; Zhou & Belkin, 2011; Bridle & Zhu, 2013; Kyng et al., 2015; Rios et al., 2019). More recently, Calder et al. (2020) proposed Poisson learning as a different approach to handling labeled data. This method replaces the original boundary conditions in the Laplace equation with the source term in a graph Poisson equation. However, Poisson learning still treats unlabeled data in the form of Laplace equations, which retains the characteristic of a smooth combination primarily influenced by the unlabeled portion, potentially risking convergence towards non-informative predictions.\nIn this work, we propose a simple yet effective framework, variance-enlarged Poisson learning (VPL), to address the presence of degenerate solutions that tend to be almost constant for unlabeled points. Specifically, we introduce a variance-enlarged regularization term that increases the dispersion of labels away from the mean or average label by enlarging the variance of labels. This leads to the Poisson equation for unlabeled data. Based on the variance-enlarged framework, we propose two specific algorithms, V-Laplace and V-Poisson, to enhance Laplace learning and Poisson learning, respectively. These algorithms are efficient and simple to implement (see Sec. 3.2). We also incorporate VPL into the propagation in graph neural networks and propose variance-enlarged graph Poisson networks (V-GPN), which can improve label propagation at very low label rates. Furthermore, we provide theoretical analysis in both discrete (finite data) and variational (infinite data) cases. In the discrete case, we demonstrate that VPL is equivalent to reducing edge weights and prove that under mild conditions, it amplifies the importance of edge weights connecting vertices within the same class while diminishing the importance of those connecting vertices from different classes. In the variational case, we provide a variational characterization of the performance of VPL with a more general objective incorporating the \u2113p distance (p \u2265 2). The theoretical analysis is formulated in the asymptotic limit as the number of unlabeled data grows to infinity while the number of labeled data stays constant. We derive the condition under which the inclusion of the variance-enlarged regularization in VPL guarantees the convexity of the variational objective. We prove that the associated optimality conditions result in a partial differential equation for the estimate of labeling functions, referred to as the weighted p-Poisson equation. We conduct extensive experiments to demonstrate the effectiveness of VPL, showcasing its superior performance over the state-of-the-arts.\nThe main contributions of our work are summarized as follows:\n\u2022 We present variance-enlarged Poisson learning, a simple yet effective framework for tackling the challenges associated with extremely sparse labeled data in graph-based semi-supervised learning. Our extensive experimental results demonstrate the effectiveness and superior performance of VPL over existing methods.\n\u2022 We propose two specific algorithms, V-Laplace and V-Poisson, which are efficient and simple to implement and achieve better performance than the original Laplace learning and Poisson learning with extremely sparse labeled data. We also consider the problem of classifying nodes in graph-structured data and propose variance-enlarged graph Poisson networks to enlarge the variance of labels in the propagation.\n\u2022 We provide theoretical analysis in both discrete (finite data) and variational (infinite data) cases. In the discrete case, we show that VPL is equivalent to reducing edge weights, which implicitly amplifies the importance of edge weights within the same class. In the variational case, we characterize the performance of VPL in the asymptotic limit as the number of data grows to infinity. The associated optimality conditions lead to the weighted p-Poisson equation."
        },
        {
            "heading": "2 PRELIMINARY",
            "text": "Let \u00b5(x) be an underlying distribution on a compact space \u2126 \u2286 Rd with a smooth boundary. Let u : \u2126\u2192 Y be the unknown function we wish to estimate. For classification problems, we let Y = {e1, ..., ek} (k \u2265 2)1, where ei denotes the one-hot vector whose i-th element is 1. For the standard (transductive) semi-supervised learning problem, we have l labeled examples {(x1, y1), ..., (xl, yl)} and n\u2212 l unlabeled examples {xl+1, ..., xn} with all examples xi sampled i.i.d from \u00b5(x). We let X = {x1, ..., xn} denote the vertices of a symmetric graph with edge weights wij \u2265 0 that indicate the similarity between xi and xj . The degree of vertex xi is denoted by di = \u2211n j=1 wij . The goal of graph-based semi-supervised learning is to construct an estimate of labels for all unlabeled examples by leveraging pairwise similarities in the graph.\nLaplace Learning and Poisson Learning. The main difference between Laplace learning and Poisson learning emerges from their distinct approaches to handling labeled data. In Laplace learning, labeled data is incorporated as boundary conditions, taking the form u(xi) = yi for 1 \u2264 i \u2264 l, while Poisson learning replaces the given labels with the assignment of sources and sinks like flow in the graph, thus resulting in the Poisson equation Lu(xi) = yi \u2212 y (with y = 1l \u2211l j=1 yj) for 1 \u2264 i \u2264 l. On the other hand, both methods treat unlabeled data in the same way as follows:\nLu(xi) = 0, for l + 1 \u2264 i \u2264 n, (2.1) where L is the unnormalized graph Laplacian given by Lu(xi) = \u2211n j=1 wij(u(xi)\u2212 u(xj))."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "In this section, we present in detail the variance-enlarged framework and two specific algorithms from this framework, namely V-Laplace and V-Poisson. We also propose the variance-enlarged graph Poisson network (V-GPN) for semi-supervised node classification with graph-structured data."
        },
        {
            "heading": "3.1 VARIANCE-ENLARGED POISSON LEARNING",
            "text": "We explore the concept of enlarging the variance on labels, aiming to increase the dispersion of labels from their mean or average labels. This approach intuitively alleviates the presence of degenerate solutions that tend to be almost constant for unlabeled points. More precisely, we focus on the optimization objective as follows:\nn\u2211 i=1 n\u2211 j=1 wijL(u(xi), u(xj))\u2212 \u03bb \u00b7Var[u], (3.1)\nwhere Var[u] denotes the variance of these labels. Here, for brevity, we choose not to introduce constraints on the labeled data. In the subsequent sections, we will specifically incorporate the constraints associated with Laplace learning and Poisson learning with respect to labeled data into our framework, while the inclusion of other constraints is also a viable alternative.\n1For binary classification, we usually set Y = {+1,\u22121}.\nFor the specific form of variance, there are two approaches available: a direct calculation of variance across all data or a weighted formulation based on certain criteria. In this paper, we adopt the latter one that takes into account the degrees of each point, i.e., Var[u] = \u2211n i=1 qi\u2225u(xi)\u2212 u\u22252, where we\ndefine u = \u2211n i=1 qiu(xi) and qi = di\u2211n\nj=1 dj .\nWhen L(u(xi), u(xj)) = 12\u2225u(xi)\u2212u(xj)\u2225 2, compared with Laplace learning and Poisson learning, the variance-enlarged objective induces the Poisson equation for unlabeled data as follows:\nLu(xi) = \u03bbqi (u(xi)\u2212 u) , for l + 1 \u2264 i \u2264 n, (3.2)\nFormally, our proposed approach remains aligned with the principles of Poisson learning, leading us to designate it as Variance-Enlarged Poisson Learning.\nRemark. Variance-enlarged regularization is somewhat similar to entropy minimization (Grandvalet & Bengio, 2004), as entropy minimization assumes a prior that prefers minimal class overlap and helps enlarge the variance of labels. However, variance-enlarged Poisson learning can be better suited for conducting an optimization algorithm than entropy minimization due to its quadratic nature, making it a practical choice for certain scenarios with more concise iterations (cf. Sec. 3.2)."
        },
        {
            "heading": "3.2 V-LAPLACE AND V-POISSON",
            "text": "V-Laplace Learning. We consider the boundary conditions of labeled points in Laplace Learning, which is equivalent to solving the problem:{\nLu(xi) = \u03bbqi(u(xi)\u2212 u) if l + 1 \u2264 i \u2264 n u(xi) = yi, if 1 \u2264 i \u2264 l.\n(3.3)\nIn Algorithm 1, we provide an algorithm to solve the V-Laplace problem in Eq. 3.3, where we clamp the labels for labeled data and only update predictions for unlabeled data.\nAlgorithm 1 Variance-enlarged Laplace Learning (V-Laplace) 1: Input: The label matrix Yl \u2208 Rl\u00d7k, the weight matrix W, and timesteps T , and the trade-off\nparameter \u03bb. 2: Output: U \u2208 Ru\u00d7k (where Ul denotes the predicted matrix restricted to labeled rows). 3: D\u2190 diag(W1) 4: L\u2190 D\u2212W 5: U\u2190 zeros(n, k) 6: for i = 1 to T do 7: U\u2190 D ( U\u2212 11\n\u22a4DU 1\u22a4D1 ) 8: U\u2190 U+D\u22121(\u03bbU\u2212 LU) 9: Ul = Yl.\n10: end for\nV-Poisson Learning. We consider the Poisson equation for the labeled points in Poisson Learning, which is equivalent to solving the problem:{\nLu(xi) = \u03bbqi(u(xi)\u2212 u) if l + 1 \u2264 i \u2264 n Lu(xi) = yi \u2212 y + \u03bbqi(u(xi)\u2212 u), if 1 \u2264 i \u2264 l,\n(3.4)\nsatisfying \u2211n\ni=1 diu(xi) = 0, where y = 1 l \u2211l i=1 yi is the average label vector.\nUnlike V-Laplace, which directly adopts constraints on labeled data in Laplace learning, V-Poisson in Algorithm 2 takes a slightly different way to handle labeled data from standard Poisson learning. Here, we also introduce modifications to the labeled data, since it is a crucial step to ensure that the identity \u2211n i=1 Lu(xi) = 0 always holds. In the following, we gives the foundation for convergence of variance-enlarged Poisson learning through the random walk perspective. Theorem 3.1. If \u03bb \u2264 minij wijqiqj , the graph G with the weight matrix W\n\u2032 = W \u2212 \u03bbqq\u22a4 (where q = (q1, ..., qn) \u22a4) is connected and the Markov chain induced by the random walk is aperiodic, we\nhave: (1) For the iterations uT+1(xi) = uT (xi)+ d\u22121i (\u03bbqi(uT (xi)\u2212uT )\u2212LuT (xi)) of V-Laplace learning in Algorithm 1, uT converges to the solution of the problem in Eq. 3.3; (2) For the iterations uT+1(xi) = uT (xi) + d \u22121 i ( \u2211l j=1(yj \u2212 y)I(i = j) + \u03bbqi(uT (xi)\u2212 uT )\u2212 LuT (xi)) of V-Poisson learning in Algorithm 2, then uT \u2192 u as T \u2192\u221e, where u is the solution of Poisson equation 3.4 satisfying \u2211n i=1 diu(xi) = 0.\nAlgorithm 2 Variance-enlarged Poisson Learning (V-Poisson) 1: Input: The label matrix Yl \u2208 Rl\u00d7k, the weight matrix W, and timesteps T , and the trade-off\nparameter \u03bb. 2: Output: The predicted matrix U \u2208 Rn\u00d7k 3: D\u2190 diag(W1) 4: L\u2190 D\u2212W 5: y\u2190 1mYl1 6: B\u2190 [Yl \u2212 y, zeros(u, k)] 7: U\u2190 zeros(n, k) 8: for i = 1 to T do 9: U\u2190 D ( U\u2212 11\n\u22a4DU 1\u22a4D1 ) 10: U\u2190 U+D\u22121 ( B\u22a4 \u2212 LU+ \u03bbU\n) 11: end for"
        },
        {
            "heading": "3.3 VARIANCE-ENLARGED GRAPH POISSON NETWORKS",
            "text": "In this subsection, we consider the problem of classifying nodes in graph-structured data (Kipf & Welling, 2017; Velic\u030ckovic\u0301 et al., 2018), where the graph structure is encoded with a neural network model and trained on a supervised loss for an extremely small set of labels.\nInspired by graph attention networks (GAT) (Velic\u030ckovic\u0301 et al., 2018), we propose variance-enlarged graph Poisson networks (V-GPN) to adaptively capture the importance of the neighbors exerting to the target node via attention mechanism. In this way, the graph information can be gradually refined via network training. To be specific, we calculate the attention coefficients aij between nodes xi and xj as aij = h[Hxi;Hxj ], where h constitutes a trainable weight vector and H is a trainable weight matrix. Subsequently, these attention coefficient aij are normalized using a softmax function to compute the edge weight matrix W that is further optimized via network training, that is, wij =\nexp(aij)\u2211 k\u2208Ni exp(aik)\n, where Ni is some neighborhood of node i. To enlarge the variance of labels in the propagation, we obtain the output of our proposed V-GPN by the following iterations:\nU(t) \u2190 U(t\u22121) +D\u22121 ( \u03bbU (t\u22121) \u2212 LU(t\u22121) ) , (3.5)\nwhere U (t\u22121)\nis defined as in Algorithms 1 and 2, D and L represent the diagonal matrix of degrees and Laplacian matrix based on the attention-based weight matrix W, respectively. Owing to the specifics of the attention calculation, D simplifies to an identity matrix. Consequently, we can streamline the iteration process more succinctly as U(t) \u2190WU(t\u22121) + \u03bbU(t\u22121). To incorporate the structural information arising from node features, we follow the approach in (Wan et al., 2021), employing a feature transformation module fFT composed of a single perception layer to predict labels based on node features. This module modifies the output at (T \u2212 3)-th iteration, giving rise to UT\u22123 \u2190 UT\u22123 + fFT (X), where T denotes the number of iterations."
        },
        {
            "heading": "4 THEORETICAL UNDERSTANDING AND INSIGHTS",
            "text": "In this section, we theoretically investigate variance-enlarged Poisson learning, covering both the discrete (finite data) and variational (infinite data) cases. All proofs can be found in Appendix A."
        },
        {
            "heading": "4.1 THE DISCRETE CASE",
            "text": "For the discrete case with finite data, let us recall the Laplace equation: Lu(xi) = 0, which yields the expression u(xi) = 1di \u2211m j=1 wiju(xj) = 1 di \u2211l j=1 wiju(xj) + 1 di \u2211n j=l+1 wiju(xj). This reveals that u(xi) results from a smooth combination involving all labels and predictions. Consequently, when labeled data is extremely sparse, the result of u(xi) is primarily shaped by the unlabeled portion 1 di \u2211n j=l+1 wiju(xj), potentially leading to a convergence toward non-informative predictions. To mitigate the mixing of label propagation, several studies have focused on re-weighting the graph more heavily near labels (Shi et al., 2017; Calder & Slepc\u030cev, 2020), thus increasing the influence of labeled data and implicitly down-weighting the importance of unlabeled data.\nWe establish that variance-enlarged Poisson learning, under mild conditions, implicitly amplifies the importance of edge weights connecting vertices within the same class, while simultaneously diminishing the importance of those connecting vertices from different classes. We demonstrate that the inclusion of \u2212Var[u] in Eq. 1.1 is equivalent to explicitly reducing all edge weights:\nLemma 4.1. The solution of the Poisson equation in Eq. 3.2 is equivalent to the minimizer of the objective 12 \u2211n i=1 \u2211n j=1(wij \u2212 \u03bbqiqj)\u2225u(xi)\u2212 u(xj)\u222522, which indicates the optimal condition:\nu(xi) = 1\ndi\u2212\u03bbqi [\u2211m j=1 wiju(xj)\u2212 \u03bbqiu ] = 1di\u2212\u03bbqi \u2211m j=1 (wij \u2212 \u03bbqiqj)u(xj). (4.1)\nThis theorem offers a clear and straightforward understanding of the significance of the varianceenlarged regularization term. Moreover, it implies that defining the parameter range \u03bb \u2264 minij wijqiqj is essential for preserving the convexity of the optimization objective.\nTo better illustrate the implication of reducing edge weights as indicated in Eq. 4.1, we introduce a reasonable and intuitively meaningful assumption as follows: Assumption 4.2. For each vertex xi with its label yi, let smi (k) = \u2211m j=1 I(yj=k)wij\u2211m j=1 I(yj=k)\n(where I is the identity function), we assume that sni (yi) > maxk \u0338=yi s n i (k) and s l i(yi) > maxk \u0338=yi s l i(k).\nAssumption 4.2 consists of two statements for each vertex x: (1) The average of edge weights between x and vertices within the same class should exceed the average of edge weights between x and vertices of any other individual class; (2) The average of edge weights between x and labeled vertices within the same class should exceed the average of edge weights between x and labeled vertices of any other individual class. This assumption resembles the commonly-used manifold assumption in which samples located near each other on a low-dimensional manifold are expected to share similar labels, but it differs in its emphasis on a description with a more global notion of class.\nTogether with Theorem 4.1 and Assumption 4.2, we can easily derive the following proposition: Proposition 4.3. Under Assumption 4.2, let bmi (k) = \u2211m j=1 I(yj=k)qiqj\u2211m j=1 I(yj=k) , if for m \u2208 {l, n} and k \u2208 Y , we have \u03bb \u2264 minij wijqiqj , b m i (yi) \u2264 bmi (k), then smi (yi)\u2212\u03bbbmi (yi) smi (k)\u2212\u03bbbmi (k) > smi (yi) smi (k) .\nRemark. This proposition provide the insight that a reduction in edge weights can help give wider influence of vertices within the same class, thus forcing the solution of predictions with larger within-class similarities, i.e., s\nn i (yi)\u2212\u03bbbni (yi) sni (k)\u2212\u03bbbni (k) > sni (yi) sni (k)\n, \u2200k \u0338= yi. On the other hand, the inequality sli(yi)\u2212\u03bbbli(yi) sli(k)\u2212\u03bbbli(k) > sli(yi) sli(k) , \u2200k \u0338= yi indicates that the reduction also amplifies the importance of edge weights connecting labeled vertices within the same class while diminishing the importance of those labeled vertices from different classes. To better illustrate the role of variance-enlarged regularization, consider the edge weights that satisfy wij = 1.0 if yi = yj and wij = 0.1 if yi \u0338= yj , with setting \u03bbqiqj = 0.1. In this scenario, we have smi (yi) \u2212 \u03bbbmi (yi) = 0.9 and smi (yi) \u2212 \u03bbbmi (yi) = 0, \u2200m \u2208 {n, l}, k \u0338= yi. This suggests that the solutions will be a weighted combination of vertices within the same class and eliminate the negative contributions from other classes, especially in the presence of a large number of unlabeled data."
        },
        {
            "heading": "4.2 THE VARIATIONAL CASE",
            "text": "We further use techniques and ideas from PDE theory to yield deeper understanding of the behavior of variance-enlarged Poisson learning in the context of a more general objective incorporating the \u2113p distance (p \u2265 2) (Oberman, 2013). Our analysis focused on the formulations in the asymptotic limit as the graph size goes to infinity (i.e., n\u2192\u221e) and the bandwidth of the edge weight wij = \u03c6 ( \u2225xi\u2212xi\u22252\nh\n) goes to zero (i.e., h\u2192 0)\n(Nadler et al., 2009; El Alaoui et al., 2016). In particular, we consider the problem lim h\u21920 lim n\u2192\u221e 1 n2hp+dCp Jp(u)\u2212 \u03bb \u00b7Var[u], (4.2)\nwhere Jp(u) = \u2211 ij w p ij\u2225u(xi) \u2212 u(xj)\u2225pp, Var[u] = \u2211n i=1 qi\u2225u(xi) \u2212 \u2211n j=1 qju(xj)\u22252, and Cp = 1\ndp/2\n\u222b \u2225z\u2225p2\u03c6(\u2225z\u22252)pdz is a constant.\nWhen n\u2192\u221e and h\u2192 0, follow the asymptotic behavior of the objective Jp(u) in (Nadler et al., 2009; El Alaoui et al., 2016), we have limh\u21920 limn\u2192\u221e 1n2hp+dCpJp(u) = \u222b \u2225\u2207u(x)\u2225p2,p\u00b52(x)dx for even p (please see Theorem 1 in (El Alaoui et al., 2016)). Thus, the functional with respect to u(x) is formulated as\nIp(u) = \u222b \u2225\u2207u(x)\u2225p2,p\u00b52(x)dx\u2212 \u03bb\n2\n[\u222b \u2225u(x)\u222522q(x)dx\u2212 \u2225\u2225\u2225\u2225\u222b u(x)q(x)dx\u2225\u2225\u2225\u22252 2 ] , (4.3)\nwhere the variance term is described as limh\u21920 limn\u2192\u221e Var[u] = \u222b \u2225u(x)\u222522q(x)dx \u2212\n\u2225 \u222b u(x)q(x)dx\u222522, and q(x) is the weight assigned to x for calculating the variance.\nAdditionally, q(x) is somewhat restricted to ensure that q(x) together with a sufficiently small \u03bb can guarantee the convexity of Eq. 4.3. Our first result characterizes the range of \u03bb as follows: Theorem 4.4. For p \u2265 2, there exists a constant C depending on \u2126, q(x) and d, such that, when \u03bb \u2264 infx\u2208\u2126,u\u2208F p\u00b7\u2225\u2207u(x)\u2225p\u221222,p \u00b52(x) C2q(x) , the functional in Eq. 4.3 shows convexity with respect to u.\nAs can be seen, when p = 2, the range can be simplified as \u03bb \u2264 infx\u2208\u2126 2\u00b5 2(x)\nC2q(x) , which depends on q(x) and \u00b5(x). Our next main result characterizes the solutions of the optimization problem infu I(u) in terms of a PDE referred to as the (weighted) p-Poisson equation. Theorem 4.5. Suppose the density \u00b5 is bounded and continuously differentiable. If \u03bb \u2264 infx\u2208\u2126,u\u2208F p\u00b7\u2225\u2207u(x)\u2225p\u221222,p \u00b52(x) C2q(x) , then any twice-differentiable minimizer u : R\nd \u2192 Rk of the functional in Eq. 4.3 must satisfy the Euler-Lagrange equation\ndiv ( \u00b52(x)Dp(\u2207u)\u2207u(x) ) = \u03bbq(x)\np\n[\u222b q(x)u(x)dx\u2212 u(x) ] , (4.4)\nTable 1: Classification accuracy (%) of different methods on MNIST with several labels per class ({1, 2, 3, 4, 5}). The results described as \u201cmean (std)\" are run over 100 trials. Results achieved by V-Laplace and V-Poisson that outperform their original ones are boldfaced and the the best results are underlined.\n# LABELS PER CLASS 1 2 3 4 5\nLAPLACE/LP (ZHU ET AL., 2003) 16.7 (8.0) 30.8 (12.8) 47.0 (15.0) 64.9 (13.3) 76.2 (10.3) NEAREST NEIGHBOR 56.2 (4.9) 66.3 (3.8) 70.2 (2.6) 73.4 (2.7) 75.5 (2.2) RANDOM WALK (ZHOU & SCH\u00d6LKOPF, 2004) 85.0 (4.6) 91.0 (2.5) 92.7 (1.6) 93.9 (1.1) 94.5 (0.8) MBO (GARCIA-CARDONA ET AL., 2014) 19.4 (6.2) 29.3 (6.9) 40.2 (7.4) 50.7 (6.0) 59.2 (6.0) WNLL (SHI ET AL., 2017) 59.2 (14.3) 87.8 (5.8) 93.5 (2.6) 95.4 (1.2) 95.8 (0.7) CENTERED KERNEL (MAI & COUILLET, 2018) 84.3 (5.0) 90.8 (2.5) 92.5 (1.8) 93.9 (1.2) 94.7 (0.9) SPARSE LP (JUNG ET AL., 2016) 14.0 (5.5) 14.0 (4.0) 14.5 (4.0) 18.0 (5.9) 16.2 (4.2) P-LAPLACE (RIOS ET AL., 2019) 72.7 (4.8) 82.7 (3.4) 86.1 (2.3) 88.3 (1.7) 89.7 (1.3) POISSON (CALDER ET AL., 2020) 91.3 (4.2) 94.3 (1.7) 95.0 (1.1) 95.5 (0.7) 95.8 (0.6)\n# LABELS PER CLASS 1 2 3 4 5\nLAPLACE/LP (ZHU ET AL., 2003) 17.0 (6.6) 31.7 (10.0) 43.3 (8.4) 52.8 (6.9) 59.3 (5.7) NEAREST NEIGHBOR 43.9 (4.3) 49.6 (3.3) 52.7 (3.0) 55.0 (2.4) 56.9 (2.7) RANDOM WALK (ZHOU & SCH\u00d6LKOPF, 2004) 57.1 (4.8) 63.1 (4.0) 66.3 (2.8) 68.5 (2.5) 70.1 (2.2) MBO (GARCIA-CARDONA ET AL., 2014) 15.7 (4.1) 20.1 (4.6) 25.7 (4.9) 30.7 (4.9) 34.8 (4.3) WNLL (SHI ET AL., 2017) 43.0 (7.6) 58.6 (5.1) 64.0 (3.4) 67.1 (3.4) 69.6 (2.7) CENTERED KERNEL (MAI & COUILLET, 2018) 36.6 (4.2) 47.2 (4.4) 53.5 (3.9) 58.4 (3.3) 61.6 (3.4) SPARSE LP (JUNG ET AL., 2016) 14.0 (5.5) 14.0 (4.0) 14.5 (4.0) 18.0 (5.9) 16.2 (4.2) P-LAPLACE (RIOS ET AL., 2019) 52.1 (4.8) 58.4 (3.7) 62.0 (3.0) 64.3 (2.5) 66.0 (2.5) POISSON (CALDER ET AL., 2020) 60.4 (4.7) 66.3 (4.0) 68.9 (2.7) 70.7 (2.4) 72.2 (2.2)\nV-LAPLACE 60.6 (5.0) 66.3 (4.2) 69.2 (2.8) 71.0 (2.8) 72.6 (2.3) V-POISSON 61.3 (4.9) 67.1 (4.2) 69.7 (2.8) 71.3 (2.7) 72.9 (2.3)\nwhere div(\u2207f) = ( \u2211d\ni=1 \u2202 2 xif1, \u2211d i=1 \u2202 2 xif2, ..., \u2211d i=1 \u2202 2 xifk)\n\u22a4 for f : Rd \u2192 Rk and Dp(\u2207u) = diag ( \u2225\u2207u1(x)\u2225p\u221222 , \u2225\u2207u2(x)\u2225 p\u22122 2 , ..., \u2225\u2207uk(x)\u2225 p\u22122 2 ) . Furthermore, if the distribution \u00b5 has full\nsupport, then this equation can be solved equivalently by addressing\ndiv(\u2207u(x)) + 2\u2207u(x)\u2207 log\u00b5(x) + (p\u2212 2)\u2206\u221eu(x) + \u03bbq(x) p\u00b52(x) \u00b7Dp(\u2207u)\u22121u(x) = 0, (4.5)\nand \u222b u(x)q(x)dx = 0, where \u2206\u221eu = ( \u2207u\u22a41 \u22072u1\u2207u1\n\u2225\u2207u1\u222522 , \u2207u\u22a42 \u22072u2\u2207u2 \u2225\u2207u2\u222522 , ..., \u2207u\u22a4k \u22072uk\u2207uk \u2225\u2207uk\u222522\n) .\nRemark. Here, we specialize to the case d = k = 1 and p = 2 to better show the behavior of enlarging variance, then \u2206\u221eu = div(\u2207u) = d 2u dx2 , and Eq. 4.5 reduces to the second order ODE: 2\u00b52(x)u\u2032\u2032(x) + 4\u00b5(x)\u00b5\u2032(x)u\u2032(x) + \u03bbq(x)u(x) = 0. If we further let \u00b5 be the uniform distribution and q(x) = \u00b5(x), we have u\u2032\u2032(x) + \u03bbu(x) = 0 2. As illustrated in Fig. 1(a), the solution of Laplace equation (\u03bb\u2032 = 0) is a straight line which is the shortest distance between two labeled points at boundary. This behavior arises from the fact that the solutions of Laplace equation are harmonic functions that have no local maxima or minima. The existence of variance-enlarged regularization change this point, while it makes the confidence of prediction becomes higher as \u03bb increases. Particularly noteworthy in Fig. 1(b), when \u00b5 is a mixture of two Gaussian, VPL mitigates the occurrence of spikes of labeled points and non-informative predictions for unlabeled points. This partially explains the effectiveness of enlarging variance for low label rate problems."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we provide extensive experiments to validate the effectiveness of variance-enlarged Poisson learning with extremely sparse labeled data (where the number of label per class is taken in\n2We can easily derive that u(x) = C1 cos \u221a \u03bb\u2032x+ C2 sin \u221a \u03bb\u2032x.\nLAPLACE/LP (ZHU ET AL., 2003) 10.3 (1.2) 10.8 (1.7) 11.8 (2.7) 13.0 (4.0) 13.1 (3.4) NEAREST NEIGHBOR 29.4 (3.9) 33.4 (3.3) 35.1 (2.9) 36.4 (2.3) 37.4 (2.4) RANDOM WALK (ZHOU & SCH\u00d6LKOPF, 2004) 37.5 (5.1) 44.6 (3.5) 48.4 (3.7) 51.1 (3.0) 52.8 (2.8) MBO (GARCIA-CARDONA ET AL., 2014) 14.2 (4.1) 19.3 (5.2) 24.3 (5.6) 28.5 (5.6) 33.5 (5.7) WNLL (SHI ET AL., 2017) 14.9 (4.8) 24.9 (6.9) 33.2 (6.7) 38.4 (7.0) 42.4 (5.5) CENTERED KERNEL (MAI & COUILLET, 2018) 35.6 (5.4) 42.7 (5.4) 46.0 (3.6) 48.6 (3.2) 50.1 (2.7) SPARSE LP (JUNG ET AL., 2016) 11.8 (2.4) 12.3 (2.4) 11.1 (3.3) 14.4 (3.5) 11.0 (2.9) P-LAPLACE (RIOS ET AL., 2019) 34.7 (4.7) 41.3 (3.5) 44.6 (3.6) 47.2 (2.8) 48.8 (2.8) POISSON (CALDER ET AL., 2020) 39.1 (5.4) 45.4 (3.9) 48.5 (3.6) 51.2 (3.0) 52.9 (2.8)\nV-LAPLACE 33.9 (5.6) 40.5 (4.3) 44.0 (4.9) 46.6 (4.8) 47.8 (4.3) V-POISSON 41.4 (5.4) 48.5 (4.1) 51.7 (3.7) 54.7 (3.1) 56.3 (2.8)\n{1, 2, 3, 4, 5}) on two tasks: graph-based semi-supervised learning on MNIST (LeCun et al., 1998), FashionMNIST (Xiao et al., 2017), and CIFAR-10 (Krizhevsky & Hinton, 2009), and semi-supervised node classification with GNNs on the citation network datasets\u2014Cora and CiteSeer (Sen et al., 2008). More experimental results and details can be found in the Appendix B.\nResults on Graph-based Semi-supervised Learning. Tables 1, 2 and 3 report the average accuracy and deviation while randomly selecting extremely sparse labeled data points on MNIST, FashionMNIST, and CIFAR-10, respectively. These results show that V-Laplace learning and V-Poisson learning have achieved very prominent results compared with Laplace learning and Poisson learning, which demonstrates the effectiveness of VPL. Notably, V-Poisson consistently outperforms other methods by an obvious margin across all cases. We also provide results on SimSiam (Chen & He, 2021) pretrained features on CIFAR-10, where the details can be found in Appendix B.1.\nResults on Semi-supervised Node Classification with GNNs. The experimental results on Cora and CiteSeer are shown in Table 4. As can be seen, our proposed V-GPN achieves substantial performance gains at different label rates when compared with the baselines GCN (Kipf & Welling, 2017), GAT (Velic\u030ckovic\u0301 et al., 2018) and GPN (Wan et al., 2021). In particular, the margin between our proposed framework and the best baseline method can exceed 5% on Cora and CiteSeer datasets given one labeled node per class, which demonstrates that V-GPNs could effectively enhance the learning performance of GNNs with extremely sparse labeled data."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this study, to address the challenges associated with sparse labeled data in graph-based semisupervised learning, we introduced Variance-enlarged Poisson Learning (VPL) to enlarge the variance of labels, which intuitively increases the dispersion of labels away from their average value. Based on this variance-enlarged framework, we present two efficient algorithms, namely V-Laplace and V-Poisson, which are tailored for improving Laplace learning and Poisson learning, respectively. Additionally, we extend VPL to enhance semi-supervised node classification with graph neural networks, introducing Variation-enlarged Graph Poisson Networks. Furthermore, we conduct a theoretical exploration of VPL in both finite and infinite data cases, yielding deeper insights into understanding the behavior of VPL. Through extensive experiments, we demonstrate the effectiveness and superior performance of VPL in scenarios characterized by extremely sparse labeled data."
        },
        {
            "heading": "ACKNOWLEDGE",
            "text": "This work was supported in part by National Natural Science Foundation of China under Grants 92270116 and 62071155, and in part by the Fundamental Research Funds for the Central Universities (Grant No.HIT.OCEF.2023043 and HIT.DZJJ.2023075)."
        },
        {
            "heading": "A PROOFS OF LEMMAS AND THEOREMS",
            "text": ""
        },
        {
            "heading": "A.1 PROOF OF THEOREM 3.1",
            "text": "Theorem 3.1 If \u03bb \u2264 minij wijqiqj , the graph G with the weight matrix W \u2032 = W \u2212 \u03bbqq\u22a4 (where q = (q1, ..., qn) \u22a4) is connected and the Markov chain induced by the random walk is aperiodic, we have: (1) For the iterations uT+1(xi) = uT (xi)+ d\u22121i (\u03bbqi(uT (xi)\u2212uT )\u2212LuT (xi)) of V-Laplace learning in Algorithm 1, uT converges to the solution of the problem in Eq. 3.3; (2) For the iterations uT+1(xi) = uT (xi) + d \u22121 i ( \u2211l j=1(yj \u2212 y)I(i = j) + \u03bbqi(uT (xi)\u2212 uT )\u2212 LuT (xi)) of V-Poisson learning in Algorithm 2, then uT \u2192 u as T \u2192\u221e, where u is the solution of Poisson equation 3.4 satisfying \u2211n i=1 diu(xi) = 0.\nProof. In this proof, we first perform an analysis of the quadratic property of V-Laplace and then show the convergence in Algorithm 1 that conforms to the gradient descent algorithm. For the convergence of V-poisson, our proof depends on the characterization of the iterations, such as the identity \u2211n i=1 diuT (xi) = 0. (1) For the V-Laplace learning, the optimization problem can be formulated as minimizing\u2211n i=1 \u2211n j=1(wij \u2212 \u03bbqiqj)\u2225u(xi) \u2212 u(xj)\u222522 in accordance with Lemma 4.1. Since the iterations in Algorithm 1 follows the GD sequence with the learning rate \u03b7 = 1di < 1\ndi\u2212\u03bbqi , we know that Algorithm 1 will converge to the condition Lu(xi) = \u03bbqi(u(xi)\u2212 u) for l + 1 \u2264 i \u2264 n. (2) For the V-Poisson learning, the iteration in Algorithm 2 shows that\ndi(uT+1(xi)\u2212 uT (xi)) + LuT (xi) = \u03bbqi(uT (xi)\u2212 uT ) + m\u2211 j=1 (yj \u2212 y)I(i = j). (A.1)\nSumming both sides over i = 1, ..., n, we have\nn\u2211 i=1 diuT+1(xi) = n\u2211 i=1 diuT (xi) = ... = n\u2211 i=1 diu0(xi) = 0, (A.2)\nbecause u0(xi) is initialized as zero in Algorithm 2.\nLet u be the solution of the Poisson equation\nLu(xi) = l\u2211\nj=1\n(uj \u2212 y)I(i = j) + \u03bbqi(u(xi)\u2212 u) (A.3)\nsatisfying \u2211n\ni=1 diu(xi) = 0. Define vT (xi) = (d\u2212 \u03bbqi)(uT (xi)\u2212 u(xi)), combining Eq. A.1 and Eq. A.3, we obtain\nvT (xi) = n\u2211 j=1 wij \u2212 \u03bbqiqj dj \u2212 \u03bbqj vT\u22121(xj), (A.4)\nand \u2211n\nj=1 vT (xj) = 0. Since the graph G with the weight matrix W \u2032 = W \u2212\u03bbqq\u22a4 is connected and\nthe Markov chain induced by the corresponding random walk is aperiodic, we have limT\u2192 vT (xi) = di\u2212\u03bbqi\u2211n j=1 dj\u2212\u03bb \u2211n j=1 v0(xj) = 0, i.e., uT \u2192 u as T \u2192\u221e, which completes the proof.d"
        },
        {
            "heading": "A.2 PROOF OF LEMMA 4.1",
            "text": "Lemma 4.1. The solution of the Poisson equation in Eq. 3.2 is equivalent to the minimizer of the objective \u2211n i=1 \u2211n j=1(wij \u2212 \u03bbqiqj)\u2225u(xi)\u2212 u(xj)\u222522.\nProof. When L(u, v) = \u2225u\u2212 v\u222522, we have\n1\n2 n\u2211 i=1 n\u2211 j=1 wijL(u(xi), u(xj))\u2212 \u03bb \u00b7Var[u]\n= 1\n2 n\u2211 i=1 n\u2211 j=1 wij\u2225u(xi)\u2212 u(xj)\u222522 \u2212 \u03bb n\u2211 i=1 qi\u2225u(xi)\u2212 u\u222522\n= 1\n2 n\u2211 i=1 n\u2211 j=1 wij\u2225u(xi)\u2212 u(xj)\u222522 \u2212 \u03bb n\u2211 i=1 qi \u2225\u2225\u2225\u2225\u2225\u2225u(xi)\u2212 n\u2211 j=1 qju(xj) \u2225\u2225\u2225\u2225\u2225\u2225 2\n2\n= 1\n2 n\u2211 i=1 n\u2211 j=1 wij\u2225u(xi)\u2212 u(xj)\u222522 \u2212 \u03bb  n\u2211 i=1 qi\u2225u(xi)\u222522 \u2212 n\u2211 i=1 n\u2211 j=1 qiqj\u27e8u(xi), u(xj)\u27e9  = 1\n2 n\u2211 i=1 n\u2211 j=1 (wij \u2212 \u03bbqiqj)\u2225u(xi)\u2212 u(xj)\u222522\n(A.5)\nwhich completes the proof."
        },
        {
            "heading": "A.3 PROOF OF THEOREM 4.4",
            "text": "Theorem 4.4 For p \u2265 2, there exists a constant C, depending on \u2126 and d, such that, when \u03bb \u2264 infx\u2208\u2126,u\u2208F p\u00b7\u2225\u2207u(x)\u2225p\u221222,p \u00b52(x) C2q(x) , the functional in Eq. 4.3 shows convexity with respect to u.\nProof. Without any loss of generality, one can assume in Eq. 4.3 u is a scalar function. It is well known that a function f on a finite or infinite dimensional space F is convex if and only if for any u, v \u2208 F , f(u+ tv) is a one dimensional convex function in t (Boyd & Vandenberghe, 2004). So given any v \u2208 F and small \u03f5 > 0, define that\nIp(u+ \u03f5v) = \u222b \u2225\u2207(u+ \u03f5v)(x)\u2225p2\u00b52(x)dx\n\u2212 \u03bb 2\n[\u222b \u2225(u+ \u03f5v)(x)\u222522q(x)dx\u2212 \u2225\u2225\u2225\u2225\u222b (u+ \u03f5v)(x)q(x)dx\u2225\u2225\u2225\u22252 2 ] .\n(A.6)\nWe can derive the second-order deviate of I with respect to \u03f5 as follows I \u2032\u2032p (u+ \u03f5v) =p(p\u2212 2) \u222b \u2225\u2207(u+ \u03f5v)(x)\u2225p\u221242 ( \u27e8\u2207u,\u2207v\u27e9(x) + \u03f5\u2225\u2207v(x)\u222522 )2 \u00b52(x)dx\n+ p \u222b \u2225\u2207(u+ \u03f5v)(x)\u2225p\u221222 \u2225\u2207v(x)\u222522\u00b52(x)dx\n\u2212 \u03bb \u222b v2(x)q(x)dx+ (\u03bb \u222b v(x)q(x)dx)2\n(A.7)\nAs aforementioned, I is a convex function on u if and only if I \u2032\u2032(u+ \u03f5v) \u2265 0 for any v, leading us to investigate\nI \u2032\u2032(u+ \u03f5v) \u2265p \u222b \u2225\u2207(u+ \u03f5v)(x)\u2225p\u221222 \u2225\u2207v(x)\u222522\u00b52(x)dx\n\u2212 \u03bb \u222b v2(x)q(x)dx+ (\u03bb \u222b v(x)q(x)dx)2\n(A.8)\nOn one hand, by Poincar\u00e9\u2013Wirtinger inequality on the weighted Sobolev space W 1,p0 (\u2126) satisfying(\u222b \u2225\u2207f(x)\u2225p2q(x)dx )1/p <\u221e, there exists a constant C > 0 depending on \u2126, d and q, such that[\u222b (\nv(x)\u2212 \u222b v(x)q(x)dx )2 q(x)dx ]1/2 \u2264 C (\u222b \u2225\u2207v(x)\u222522q(x)dx )1/2 . (A.9)\nOn the other hand, by mean value theorem, there exists \u03be, such that\u222b \u2225\u2207(u+ \u03f5v)(x)\u2225p\u221222 \u2225\u2207v(x)\u222522\u00b52(x)dx = \u2225\u2207(u+ \u03f5v)(\u03be)\u2225p\u221222 \u00b52(\u03be) q(\u03be) \u222b \u2225\u2207v(x)\u222522q(x)dx. (A.10) Together with Eq. A.9 and Eq. A.10, it immediately implies that when \u03bb \u2264 infx\u2208\u2126,u\u2208F p\u00b7\u2225\u2207u(x)\u2225p\u221222 \u00b52(x) C2q(x) \u2264 p\u00b7\u2225\u2207(u+\u03f5v)(\u03be)\u2225p\u221222 \u00b52(\u03be) C2q(\u03be) , J is a convex function on u."
        },
        {
            "heading": "A.4 PROOF OF THEOREM 4.5",
            "text": "Theorem 4.5. Suppose the density \u00b5 is bounded and continuously differentiable. If \u03bb \u2264 infx\u2208\u2126,u\u2208F p\u00b7\u2225\u2207u(x)\u2225p\u221222,p \u00b52(x) C2q(x) , then any twice-differentiable minimizer u : R\nd \u2192 Rk of the functional in Eq. 4.3 must satisfy the Euler-Lagrange equation\np \u00b7 div ( \u00b52(x)Dp(\u2207u)\u2207u(x) ) + \u03bbu(x)q(x)\u2212 \u03bbq(x) \u222b q(x)u(x)dx = 0. (A.11)\nwhere div(\u2207f) = ( \u2211d\ni=1 \u2202 2 xif1, \u2211d i=1 \u2202 2 xif2, ..., \u2211d i=1 \u2202 2 xifk)\n\u22a4 for f : Rd \u2192 Rk and Dp(\u2207u) = diag ( \u2225\u2207u1(x)\u2225p\u221222 , \u2225\u2207u2(x)\u2225 p\u22122 2 , ..., \u2225\u2207uk(x)\u2225 p\u22122 2 ) . If moreover, the distribution \u00b5 has full\nsupport, then this equation is equivalent to\ndiv(\u2207u(x)) + 2\u2207u(x)\u2207 log\u00b5(x) + (p\u2212 2)\u2206\u221eu(x) + \u03bbq(x) p\u00b52(x) \u00b7Dp(\u2207u)\u22121u(x) = 0, (A.12)\nand \u222b u(x)q(x)dx = 0, where \u2206\u221eu = ( \u2207u\u22a41 \u22072u1\u2207u1\n\u2225\u2207u1\u222522 , \u2207u\u22a42 \u22072u2\u2207u2 \u2225\u2207u2\u222522 , ..., \u2207u\u22a4k \u22072uk\u2207uk \u2225\u2207uk\u222522\n) .\nProof. Without any loss of generality, one can assume in Eq. 4.3 u is a scalar function, since we can analyze each component of u independently. Let Ip(u) = \u222b \u2225\u2207u(x)\u2225p2\u00b52(x)dx \u2212 \u03bb 2 [\u222b (u(x))2q(x)dx\u2212 (\u222b u(x)q(x)dx )2 2 ] . The function u is a minimizer of the functional Ip if for all test functions h and all-sufficient small real numbers \u03f5 > 0, we have Ip(u + \u03f5h) \u2265 Ip(u). Moreover, by a Taylor series expansion, we have\nIp(u+ \u03f5h) =Ip(u) + p\u03f5 \u222b \u27e8\u2207u(x),\u2207h(x)\u27e9 \u00b7 \u2225\u2207u(x)\u2225p\u221222 \u00b52(x)dx\n\u2212 \u03bb\u03f5 (\u222b u(x)h(x)q(x)dx\u2212 \u222b u(x)q(x)dx \u222b h(x)q(x)dx ) +O(\u03f52), (A.13)\nwhere the O(\u03f52) term is non-negative by convexity of Ip. Hence, the function u is a minimizer if and only if\n0 =p \u222b \u27e8\u2207u(x),\u2207h(x)\u27e9 \u00b7 \u2225\u2207u(x)\u2225p\u221222 \u00b52(x)dx\u2212 \u03bb \u222b u(x)h(x)q(x)dx\n+ \u03bb \u222b u(x)q(x)dx \u222b h(x)q(x)dx,\n(A.14)\nfor all testing functions h. By integrating by parts and choosing h to vanish on the boundary of \u2126, the integration by parts formula shows that the above quantity is equal to\u222b\n\u27e8\u2207u(x),\u2207h(x)\u27e9 \u00b7 \u2225\u2207u(x)\u2225p\u221222 \u00b52(x)dx = \u2212 \u222b div ( \u00b52(x)\u2225\u2207u(x)\u2225p\u221222 \u2207u(x) ) h(x)dx,\n(A.15) where div(\u2207f) = \u2211d i=1 \u2202 2 xif for f : R\nd \u2192 R. This expression has to vanish for all test functions h (that vanish on the boundary), which implies the Euler-Lagrange equation\np \u00b7 div(\u00b52(x)\u2225\u2207u(x)\u2225p\u221222 \u2207u(x)) + \u03bbu(x)q(x)\u2212 \u03bbq(x) \u222b u(x)q(x)dx = 0. (A.16)\nWe now further manipulate this equation so as to obtain the p-Poisson equation. In particular, some straightforward computations yield\n\u2202xi(\u00b5 2\u2225\u2207f\u2225p\u221222 \u2202xif)(x) = \u2202xi(\u00b52(x)\u2225\u2207u(x)\u2225 p\u22122 2 )\u2202xiu(x) + \u00b5 2(x)\u2225\u2207u(x)\u2225p\u221222 \u22022xiu(x), (A.17)\nand\n\u2202xi(\u00b5 2\u2225\u2207f\u2225p\u221222 )(x) =2\u2202xi\u00b5(x) \u00b7 \u00b5(x)\u2225\u2207u(x)\u2225p\u22122\n+ \u00b52(x)(p\u2212 2)  d\u2211 j=1 \u2202xi,xjf\u2202xjf  \u00b7 \u2225\u2207u(x)\u2225p\u221242 . (A.18) Now summing these terms yield\ndiv ( \u00b52(x)\u2225\u2207u(x)\u2225p\u221222 \u2207u(x) ) =2\u00b5(x)\u2225\u2207u(x)\u2225p\u221222 \u27e8\u2207\u00b5(x),\u2207u(x)\u27e9+ \u00b52(x)\u2225\u2207u(x)\u2225 p\u22122 2 \u22062u(x)\n+ (p\u2212 2)\u00b52(x)\u2225\u2207u(x)\u2225p\u221242  d\u2211 i,j=1 \u2202xif \u00b7 \u2202xi,xjf \u00b7 \u2202xjf  (x) =\u00b52(x)\u2225\u2207u(x)\u2225p\u221222 \u00b7 ( \u22062u(x) + 2\n\u00b5(x) \u27e8\u2207\u00b5(x),\u2207u(x)\u27e9+ (p\u2212 2)\u2206\u221eu(x) ) where \u22062f := Tr(\u22072f) is the usual 2-Laplacian operator, while \u2206\u221ef := \u27e8\u2207f,\u2207 2f\u2207f\u27e9 \u27e8\u2207f,\u2207f\u27e9 is the \u221e-Laplacian operator, which is defined to be zero when\u2207f = 0. From the derivation above, let v(x) = u(x) \u2212 \u222b u(x)q(x)dx, we have\ndiv ( \u00b52(x)\u2225\u2207v(x)\u2225p\u221222 \u2207v(x) ) = div ( \u00b52(x)\u2225\u2207u(x)\u2225p\u221222 \u2207u(x) ) , then the Euler-Lagrange\nequation in Eq. A.16 is equivalent to p \u00b7 \u00b52(x)\u2225\u2207v(x)\u2225p\u221222 \u00b7 ( \u22062v(x) +\n2\u27e8\u2207\u00b5(x),\u2207v(x)\u27e9 \u00b5(x)\n+ (p\u2212 2)\u2206\u221ev(x) ) + \u03bbq(x)v(x) = 0,\nthat is,\n\u22062v(x) + 2\u27e8\u2207 log\u00b5(x),\u2207v(x)\u27e9+ (p\u2212 2)\u2206\u221ev(x) + \u03bb p \u00b7 q(x)v(x) \u00b52(x)\u2225\u2207v(x)\u2225p\u221222 = 0, (A.19)\nand \u222b q(x)v(x)dx = 0, which completes the proof."
        },
        {
            "heading": "B MORE EXPERIMENTAL DETAILS AND RESULTS",
            "text": ""
        },
        {
            "heading": "B.1 EXPERIMENTS FOR V-LAPLACE AND V-POISSON",
            "text": "Datasets. We assess V-Laplace and V-Poisson on three datasets: MNIST (LeCun et al., 1998), FashionMNIST (Xiao et al., 2017), and CIFAR-10 (Krizhevsky & Hinton, 2009). We follow the setting in Calder et al. (2020), where variational autoencoders with 3 fully connected layers of sizes (784,400,20) and (784, 400, 30) are used, respectively, followed by a symmetrically defined decoder. The autoencoder is trained for 100 epochs on each dataset. The autoencoder architecture, loss, and training, are similar to (Kingma & Welling, 2013).\nFor each dataset, we create a graph in the latent feature space using all available data, resulting in n = 70, 000 nodes for MNIST and FashionMNIST, and n = 60, 000 nodes for CIFAR-10. The graph is constructed as a K-nearest neighbor graph with Gaussian-like weights, calculated as wij = exp(\u22124\u2225xi \u2212 xj\u22252/dK(xi)2) + \u03f5, where xi represents the latent variables for example i, dK(xi) is the distance in the latent space between xi and its K-th nearest neighbor, and \u03f5 \u2265 0 . We set K = 10 and \u03f5 = 1 across all experiments. The weight matrices are symmetrized by replacing W with W +W\u22a4. For simplicity, we directly utilize the off-the-self distance matrices available at\nTable 5: MNIST: Average accuracy scores over 100 trials with standard deviation in brackets.\n# LABELS PER CLASS 1 2 3 4 5\nLAPLACE/LP (ZHU ET AL., 2003) 16.7 (8.0) 30.8 (12.8) 47.0 (15.0) 64.9 (13.3) 76.2 (10.3) NEAREST NEIGHBOR 56.2 (4.9) 66.3 (3.8) 70.2 (2.6) 73.4 (2.7) 75.5 (2.2) RANDOM WALK (ZHOU & SCH\u00d6LKOPF, 2004) 85.0 (4.6) 91.0 (2.5) 92.7 (1.6) 93.9 (1.1) 94.5 (0.8) MBO (GARCIA-CARDONA ET AL., 2014) 19.4 (6.2) 29.3 (6.9) 40.2 (7.4) 50.7 (6.0) 59.2 (6.0) WNLL (SHI ET AL., 2017) 59.2 (14.3) 87.8 (5.8) 93.5 (2.6) 95.4 (1.2) 95.8 (0.7) CENTERED KERNEL (MAI & COUILLET, 2018) 84.3 (5.0) 90.8 (2.5) 92.5 (1.8) 93.9 (1.2) 94.7 (0.9) SPARSE LP (JUNG ET AL., 2016) 14.0 (5.5) 14.0 (4.0) 14.5 (4.0) 18.0 (5.9) 16.2 (4.2) P-LAPLACE (RIOS ET AL., 2019) 72.7 (4.8) 82.7 (3.4) 86.1 (2.3) 88.3 (1.7) 89.7 (1.3) POISSON (CALDER ET AL., 2020) 91.3 (4.2) 94.3 (1.7) 95.0 (1.1) 95.5 (0.7) 95.8 (0.6)\nV-LAPLACE (\u03bb = 0.001) 88.9 (4.7) 93.8 (1.8) 94.7 (1.1) 95.5 (0.7) 95.7 (0.6) V-LAPLACE (\u03bb = 0.002) 89.1 (4.6) 93.9 (1.8) 94.8 (1.1) 95.5 (0.7) 95.8 (0.6) V-LAPLACE (\u03bb = 0.005) 89.7 (4.5) 94.1 (1.7) 94.9 (1.1) 95.6 (0.6) 95.8 (0.6) V-LAPLACE (\u03bb = 0.01) 90.5 (4.4) 94.5 (1.6) 95.1 (1.0) 95.7 (0.6) 95.9 (0.6) V-LAPLACE (\u03bb = 0.02) 91.3 (4.2) 94.9 (1.5) 95.4 (0.9) 95.9 (0.5) 96.1 (0.5) V-LAPLACE (\u03bb = 0.05) 91.6 (4.5) 95.2 (1.8) 95.8 (0.9) 96.2 (0.5) 96.3 (0.9) V-LAPLACE (\u03bb = 0.1) 91.2 (4.9) 95.1 (2.3) 95.8 (1.1) 96.2 (0.7) 96.3 (1.0) V-LAPLACE (\u03bb = 0.2) 91.4 (4.7) 95.2 (2.0) 95.8 (1.0) 96.3 (0.5) 96.3 (1.0) V-LAPLACE (\u03bb = 0.5) 92.0 (4.3) 95.3 (1.5) 95.9 (0.8) 96.2 (0.5) 96.3 (0.5) V-LAPLACE (\u03bb = 1.0) 91.7 (4.0) 95.0 (1.4) 95.6 (0.9) 96.0 (0.6) 96.2 (0.5)\nV-POISSON (\u03bb = 0.001) 91.4 (4.1) 94.4 (1.6) 95.0 (1.1) 95.6 (0.7) 95.8 (0.6) V-POISSON (\u03bb = 0.002) 91.5 (4.1) 94.4 (1.6) 95.0 (1.0) 95.6 (0.7) 95.9 (0.5) V-POISSON (\u03bb = 0.005) 91.8 (4.1) 94.6 (1.6) 95.1 (1.0) 95.7 (0.6) 95.9 (0.5) V-POISSON (\u03bb = 0.01) 92.2 (4.1) 94.8 (1.5) 95.3 (1.0) 95.8 (0.6) 96.0 (0.5) V-POISSON (\u03bb = 0.02) 92.8 (4.0) 95.2 (1.5) 95.6 (0.9) 96.0 (0.6) 96.2 (0.5) V-POISSON (\u03bb = 0.05) 93.4 (3.9) 95.6 (1.4) 96.0 (0.7) 96.3 (0.5) 96.4 (0.4) V-POISSON (\u03bb = 0.1) 93.5 (3.9) 95.7 (1.5) 96.1 (0.7) 96.4 (0.4) 96.4 (0.5) V-POISSON (\u03bb = 0.2) 93.6 (3.9) 95.7 (1.5) 96.1 (0.6) 96.4 (0.4) 96.4 (0.4) V-POISSON (\u03bb = 0.5) 93.4 (3.9) 95.6 (1.4) 96.0 (0.7) 96.3 (0.5) 96.4 (0.4) V-POISSON (\u03bb = 1.0) 92.6 (3.8) 95.0 (1.4) 95.6 (0.9) 96.0 (0.7) 96.2 (0.6)\nhttps://github.com/jwcalder/GraphLearning/tree/master. In term of the total number of iterations, we employ the trick by Calder et al. (2020). Specifically, we add the iteration step pt+1 = WD\u22121pt in the algorithms of V-Laplace and V-Poisson, where p0 is initialized as a vector with ones at the positions of all labeled vertices and zeros elsewhere. The iterations for V-Laplace and V-Poisson are terminated once \u2225pt \u2212 p\u221e\u2225\u221e \u2264 1n is achieved, just before reaching the mixing time. Herein, p\u221e = W1/(1\u22a4W1) represents the invariant distribution.\nComparisons. We conduct a comparative evaluation of V-Laplace and V-Poisson against various graph-based semi-supervised learning methodologies, including Laplace learning (Zhu et al., 2003), lazy random walks (Zhou et al., 2003; Zhou & Sch\u00f6lkopf, 2004), multiclass MBO (Garcia-Cardona et al., 2014), weighted nonlocal Laplacian (Shi et al., 2017), Centered Kernel Method (Mai & Couillet, 2018), sparse label propagation (Jung et al., 2016), p-Laplace learning (Rios et al., 2019), and Poisson learning (Calder et al., 2020). For the volume-constrained MBO method, we set the temperature T = 0.1. In the case of the Centered Kernel Method, we selected \u03b1 to be 5% larger than the spectral norm of the centered weight matrix. Throughout our experiments, we ran 100 trials, varying the random seed from 0 to 99. It\u2019s worth noting that the results for MBO and Sparse Laplace Propagation are adopted from Calder et al. (2020).\nHyper-Parameters. Our proposed V-Laplace and V-Poisson have only one hyper-parameter \u03bb to control the strength of enlarging variance. In order to better illustrate its sensitivity, we provide more results in Table 5, Table 6, and Table 7. For the results of V-Laplace in Table 1, Table 2 and Table 3, we set \u03bb = 0.5, \u03bb = 1.0 and \u03bb = 0.05, respectively. For the results of V-Poisson in Table 1 and Table 2, we set \u03bb = 0.2, \u03bb = 1.0, respectively. For V-Poisson learning on CIFAR-10, we directly use Poisson learning on the weight matrix which we subtract the elements greater than 0 from their minimum value.\nExperiments on Self-Supervised Learned Representations on CIFAR-10. Self-supervised models do currently exhibit promising properties in learning representations, and achieving relatively high linear probes. However, when the number of labels is particularly scarce, for instance, with only one label per class, self-supervised models possess good representations but struggle to learn effective classifiers. To illustrate this point, we refer to the open-source code and model parameters associated\nTable 6: FashionMNIST: Average accuracy scores over 100 trials with standard deviation in brackets.\n# LABELS PER CLASS 1 2 3 4 5\nLAPLACE/LP (ZHU ET AL., 2003) 17.0 (6.6) 31.7 (10.0) 43.3 (8.4) 52.8 (6.9) 59.3 (5.7) NEAREST NEIGHBOR 43.9 (4.3) 49.6 (3.3) 52.7 (3.0) 55.0 (2.4) 56.9 (2.7) RANDOM WALK (ZHOU & SCH\u00d6LKOPF, 2004) 57.1 (4.8) 63.1 (4.0) 66.3 (2.8) 68.5 (2.5) 70.1 (2.2) MBO (GARCIA-CARDONA ET AL., 2014) 15.7 (4.1) 20.1 (4.6) 25.7 (4.9) 30.7 (4.9) 34.8 (4.3) WNLL (SHI ET AL., 2017) 43.0 (7.6) 58.6 (5.1) 64.0 (3.4) 67.1 (3.4) 69.6 (2.7) CENTERED KERNEL (MAI & COUILLET, 2018) 36.6 (4.2) 47.2 (4.4) 53.5 (3.9) 58.4 (3.3) 61.6 (3.4) SPARSE LP (JUNG ET AL., 2016) 14.0 (5.5) 14.0 (4.0) 14.5 (4.0) 18.0 (5.9) 16.2 (4.2) P-LAPLACE (RIOS ET AL., 2019) 52.1 (4.8) 58.4 (3.7) 62.0 (3.0) 64.3 (2.5) 66.0 (2.5) POISSON (CALDER ET AL., 2020) 60.4 (4.7) 66.3 (4.0) 68.9 (2.7) 70.7 (2.4) 72.2 (2.2)\nV-LAPLACE (\u03bb = 0.001) 58.8 (5.1) 65.1 (4.2) 68.3 (2.7) 70.2 (2.6) 71.8 (2.2) V-LAPLACE (\u03bb = 0.002) 59.0 (5.0) 65.2 (4.2) 68.4 (2.7) 70.2 (2.6) 71.9 (2.2) V-LAPLACE (\u03bb = 0.005) 59.3 (5.0) 65.4 (4.2) 68.5 (2.7) 70.4 (2.6) 72.0 (2.2) V-LAPLACE (\u03bb = 0.01) 59.7 (5.0) 65.7 (4.2) 68.7 (2.7) 70.5 (2.6) 72.1 (2.3) V-LAPLACE (\u03bb = 0.02) 59.7 (5.4) 65.9 (4.3) 68.8 (3.0) 70.6 (2.7) 72.3 (2.4) V-LAPLACE (\u03bb = 0.05) 59.2 (5.9) 65.6 (4.7) 68.3 (3.9) 70.1 (3.4) 72.1 (2.9) V-LAPLACE (\u03bb = 0.1) 58.9 (6.0) 65.2 (4.7) 67.9 (4.0) 69.6 (3.5) 71.7 (3.1) V-LAPLACE (\u03bb = 0.2) 59.2 (5.8) 65.3 (4.7) 68.0 (3.8) 69.7 (3.4) 71.7 (3.0) V-LAPLACE (\u03bb = 0.5) 60.1 (5.3) 65.9 (4.6) 68.7 (3.3) 70.4 (3.1) 72.2 (2.6) V-LAPLACE (\u03bb = 1.0) 60.6 (5.0) 66.3 (4.2) 69.2 (2.8) 71.0 (2.8) 72.6 (2.3)\nV-POISSON (\u03bb = 0.001) 60.5 (4.7) 66.3 (4.0) 68.9 (2.7) 70.8 (2.4) 72.2 (2.2) V-POISSON (\u03bb = 0.002) 60.6 (4.7) 66.4 (4.0) 68.9 (2.7) 70.8 (2.4) 72.3 (2.2) V-POISSON (\u03bb = 0.005) 60.7 (4.7) 66.5 (4.1) 69.0 (2.7) 70.9 (2.4) 72.4 (2.2) V-POISSON (\u03bb = 0.01) 60.9 (4.7) 66.7 (4.1) 69.2 (2.7) 71.0 (2.4) 72.5 (2.2) V-POISSON (\u03bb = 0.02) 60.9 (5.0) 67.0 (4.2) 69.4 (2.8) 71.1 (2.6) 72.7 (2.3) V-POISSON (\u03bb = 0.05) 60.6 (5.5) 66.7 (4.5) 69.2 (3.4) 70.7 (3.4) 72.6 (2.8) V-POISSON (\u03bb = 0.1) 60.5 (5.5) 66.4 (4.5) 69.0 (3.6) 70.4 (3.5) 72.2 (2.9) V-POISSON (\u03bb = 0.2) 60.7 (5.4) 66.5 (4.4) 69.1 (3.4) 70.5 (3.3) 72.2 (2.7) V-POISSON (\u03bb = 0.5) 61.2 (5.1) 67.0 (4.3) 69.5 (3.0) 71.0 (2.9) 72.7 (2.5) V-POISSON (\u03bb = 1.0) 61.3 (4.9) 67.1 (4.2) 69.7 (2.8) 71.3 (2.7) 72.9 (2.3)\nwith an exemplary linear probe achievement of 91.9% on CIFAR-10. The source code and model parameters are available at https://github.com/Reza-Safdari/SimSiam-91.9-top1-acc-on-CIFAR10, which utilizes the SimSiam framework (Chen & He, 2021). Building upon the pretrained model, we conduct two experiments to underscore the effectiveness of graph-based semi-supervised learning in scenarios of label scarcity: (i) We utilize the SimSiam pretrained model to extract features from the entire CIFAR-10 dataset and construct a 10-nearest-neighbors graph on these representations. We then randomly choosing which data points are labeled, and predict unlabeled data for all data points using different graph-based semi-supervised learning approaches; (ii) For comparison, we directly use several randomly chosen labeled data to obtain a linear classifier, and finally use this classifier to evaluate the classification accuracy of all samples.\nThe experimental results are summarized in Table 8. It is evident that the linear classifier, trained solely on labeled data, outperforms Laplace in scenarios with extremely sparse data. However, it is crucial to note that linear classification exhibits significantly lower performance compared to other methods, particularly when compared to our proposed V-Laplace and V-Poisson methods."
        },
        {
            "heading": "B.2 EXPERIMENTS FOR V-GPN ON SEMI-SUPERVISED NODE CLASSIFICATION",
            "text": "The experiments are conducted on two commonly-used citation network datasets (Sen et al., 2008). In the citation networks, the nodes represent documents and their links refer to citations between documents, where each node is associated with a bag-of-words feature vector and a label. Moreover, we use the CE loss with a scale parameter s, i.e., \u2212 log exp(s\u00b7f(x)y)\u2211k\nc=1 exp(s\u00b7f(x)i for the labeled node (x, y).\nThe hyper-parameters, such as learning rate \u03b7, weight decay wd, the scale parameter s and the variance-enlarged parameter \u03bb are determined by grid search. Concretely, \u03b7 \u2208 {0.1, 0.05, 0.01}, wd \u2208 {1e\u2212 2, 5e\u2212 3, 1e\u2212 3, 5e\u2212 4}, s \u2208 {1, 5, 10} , and \u03bb \u2208 {1.0, 0.5}, which can also be found in the code of supplement materials.\nTable 7: Classification accuracy (%) of different methods on CIFAR-10 with several labels per class ({1, 2, 3, 4, 5}). The results described as \u201cmean (std)\" are run over 100 trials. Results achieved by V-Laplace and V-Poisson that outperform their original ones are boldfaced and the the best results are underlined.\n# LABELS PER CLASS 1 2 3 4 5\nLAPLACE/LP (ZHU ET AL., 2003) 10.3 (1.2) 10.8 (1.7) 11.8 (2.7) 13.0 (4.0) 13.1 (3.4) NEAREST NEIGHBOR 29.4 (3.9) 33.4 (3.3) 35.1 (2.9) 36.4 (2.3) 37.4 (2.4) RANDOM WALK (ZHOU & SCH\u00d6LKOPF, 2004) 37.5 (5.1) 44.6 (3.5) 48.4 (3.7) 51.1 (3.0) 52.8 (2.8) MBO (GARCIA-CARDONA ET AL., 2014) 14.2 (4.1) 19.3 (5.2) 24.3 (5.6) 28.5 (5.6) 33.5 (5.7) WNLL (SHI ET AL., 2017) 14.9 (4.8) 24.9 (6.9) 33.2 (6.7) 38.4 (7.0) 42.4 (5.5) CENTERED KERNEL (MAI & COUILLET, 2018) 35.6 (5.4) 42.7 (5.4) 46.0 (3.6) 48.6 (3.2) 50.1 (2.7) SPARSE LP (JUNG ET AL., 2016) 11.8 (2.4) 12.3 (2.4) 11.1 (3.3) 14.4 (3.5) 11.0 (2.9) P-LAPLACE (RIOS ET AL., 2019) 34.7 (4.7) 41.3 (3.5) 44.6 (3.6) 47.2 (2.8) 48.8 (2.8) POISSON (CALDER ET AL., 2020) 39.1 (5.4) 45.4 (3.9) 48.5 (3.6) 51.2 (3.0) 52.9 (2.8)\n# LABELS PER CLASS 1 2 3 4 5\nLAPLACE/LP (ZHU ET AL., 2003) 17.2 (7.0) 33.4 (10.2) 49.2 (10.3) 60.9 (8.9) 68.8 (7.0) RANDOM WALK (ZHOU & SCH\u00d6LKOPF, 2004) 69.6 (6.1) 76.7 (3.5) 79.4 (3.4) 81.2 (2.1) 82.2 (1.9) WNLL (SHI ET AL., 2017) 52.6 (9.6) 73.0 (5.5) 78.7 (4.2) 81.5 (2.9) 83.0 (1.9) CENTERED KERNEL (MAI & COUILLET, 2018) 44.2 (5.2) 56.1 (4.9) 63.1 (4.9) 67.6 (4.3) 70.5 (3.1) POISSON (CALDER ET AL., 2020) 73.9 (6.0) 80.4 (3.0) 82.7 (2.7) 84.0 (1.8) 84.8 (1.6)\nLINEAR CLASSIFIER 42.8 (4.3) 53.9 (4.0) 60.8 (3.2) 65.4 (3.3) 68.8 (2.7)\nV-LAPLACE 74.6 (5.7) 80.9 (2.7) 83.1 (3.0) 84.3 (2.0) 84.8 (1.8) V-POISSON 75.6 (5.8) 81.7 (2.8) 83.8 (2.4) 84.8 (1.8) 85.5 (1.6)"
        }
    ],
    "title": "EXTREMELY SPARSE LABELED DATA",
    "year": 2024
}