{
    "abstractText": "We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on many real-world nonEuclidean datasets, and we demonstrate tractable training on general geometries, including triangular meshes with highly non-trivial curvature and boundaries.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ricky T. Q. Chen"
        },
        {
            "affiliations": [],
            "name": "Yaron Lipman"
        }
    ],
    "id": "SP:7a44bdb7acdcaed053710c263bf239d4016d513f",
    "references": [
        {
            "authors": [
                "Michael S Albergo",
                "Eric Vanden-Eijnden"
            ],
            "title": "Building normalizing flows with stochastic interpolants",
            "venue": "International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Brian DO Anderson"
            ],
            "title": "Reverse-time diffusion equation models",
            "venue": "Stochastic Processes and their Applications,",
            "year": 1982
        },
        {
            "authors": [
                "Alexandre Barachant",
                "St\u00e9phane Bonnet",
                "Marco Congedo",
                "Christian Jutten"
            ],
            "title": "Classification of covariance matrices using a riemannian-based kernel for bci applications",
            "year": 2013
        },
        {
            "authors": [
                "Mikhail Belkin",
                "Partha Niyogi"
            ],
            "title": "Laplacian eigenmaps for dimensionality reduction and data representation",
            "venue": "Neural computation,",
            "year": 2003
        },
        {
            "authors": [
                "Heli Ben-Hamu",
                "Samuel Cohen",
                "Joey Bose",
                "Brandon Amos",
                "Aditya Grover",
                "Maximilian Nickel",
                "Ricky T.Q. Chen",
                "Yaron Lipman"
            ],
            "title": "Matching normalizing flows and probability paths on manifolds",
            "venue": "International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Benjamin Blankertz",
                "Guido Dornhege",
                "Matthias Krauledat",
                "Klaus-Robert M\u00fcller",
                "Gabriel Curio"
            ],
            "title": "The non-invasive berlin brain\u2013computer interface: fast acquisition of effective performance in untrained subjects",
            "year": 2007
        },
        {
            "authors": [
                "Joey Bose",
                "Ariella Smofsky",
                "Renjie Liao",
                "Prakash Panangaden",
                "Will Hamilton"
            ],
            "title": "Latent variable modelling with hyperbolic normalizing flows",
            "venue": "Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "G.R. Brakenridge"
            ],
            "title": "Global active archive of large flood events. http://floodobservatory. colorado.edu/Archives/index.html, 2017",
            "venue": "Dartmouth Flood Observatory,",
            "year": 2017
        },
        {
            "authors": [
                "Clemens Brunner",
                "Robert Leeb",
                "Gernot M\u00fcller-Putz",
                "Alois Schl\u00f6gl",
                "Gert Pfurtscheller"
            ],
            "title": "Bci competition 2008\u2013graz data set a. Institute for Knowledge Discovery (Laboratory of BrainComputer Interfaces)",
            "venue": "Graz University of Technology,",
            "year": 2008
        },
        {
            "authors": [
                "Ricky T.Q. Chen"
            ],
            "title": "torchdiffeq, 2018. URL https://github.com/rtqichen/ torchdiffeq",
            "year": 2018
        },
        {
            "authors": [
                "Ricky T.Q. Chen",
                "Yulia Rubanova",
                "Jesse Bettencourt",
                "David K Duvenaud"
            ],
            "title": "Neural ordinary differential equations",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Keenan Crane",
                "Ulrich Pinkall",
                "Peter Schr\u00f6der"
            ],
            "title": "Robust fairing via conformal curvature flow",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2013
        },
        {
            "authors": [
                "Valentin De Bortoli",
                "Emile Mathieu",
                "Michael Hutchinson",
                "James Thornton",
                "Yee Whye Teh",
                "Arnaud Doucet"
            ],
            "title": "Riemannian score-based generative modeling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Luca Falorsi"
            ],
            "title": "Continuous normalizing flows on manifolds",
            "venue": "PhD thesis, University of Amsterdam,",
            "year": 2020
        },
        {
            "authors": [
                "Mevlana C Gemici",
                "Danilo Rezende",
                "Shakir Mohamed"
            ],
            "title": "Normalizing flows on riemannian manifolds",
            "venue": "arXiv preprint arXiv:1611.02304,",
            "year": 2016
        },
        {
            "authors": [
                "Will Grathwohl",
                "Ricky T.Q. Chen",
                "Jesse Bettencourt",
                "Ilya Sutskever",
                "David Duvenaud"
            ],
            "title": "FFJORD: Free-form continuous dynamics for scalable reversible generative models",
            "venue": "International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Ernst Hairer"
            ],
            "title": "Solving differential equations on manifolds",
            "venue": "Lecture Notes, Universite\u0301 de Geneve,",
            "year": 2011
        },
        {
            "authors": [
                "Ernst Hairer",
                "Marlis Hochbruck",
                "Arieh Iserles",
                "Christian Lubich"
            ],
            "title": "Geometric numerical integration",
            "venue": "Oberwolfach Reports,",
            "year": 2006
        },
        {
            "authors": [
                "Paulo Herrera"
            ],
            "title": "Pyevtk, 2019. URL https://github.com/paulo-herrera/PyEVTK",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Wei Huang",
                "Milad Aghajohari",
                "Avishek Joey Bose",
                "Prakash Panangaden",
                "Aaron Courville"
            ],
            "title": "Riemannian diffusion models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "John D Hunter"
            ],
            "title": "Matplotlib: A 2d graphics environment",
            "venue": "Computing in science & engineering,",
            "year": 2007
        },
        {
            "authors": [
                "M.F. Hutchinson"
            ],
            "title": "A stochastic estimator of the trace of the influence matrix for Laplacian smoothing",
            "venue": "splines. 18:1059\u20131076,",
            "year": 1989
        },
        {
            "authors": [
                "Aapo Hyv\u00e4rinen",
                "Peter Dayan"
            ],
            "title": "Estimation of non-normalized statistical models by score matching",
            "venue": "Journal of Machine Learning Research,",
            "year": 2005
        },
        {
            "authors": [
                "Peter W Jones",
                "Mauro Maggioni",
                "Raanan Schul"
            ],
            "title": "Manifold parametrizations by eigenfunctions of the laplacian and heat kernels",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2008
        },
        {
            "authors": [
                "Ron Kimmel",
                "James A Sethian"
            ],
            "title": "Computing geodesic paths on manifolds",
            "venue": "Proceedings of the national academy of Sciences,",
            "year": 1998
        },
        {
            "authors": [
                "Thomas Kluyver",
                "Benjamin Ragan-Kelley",
                "Fernando P\u00e9rez",
                "Brian E Granger",
                "Matthias Bussonnier",
                "Jonathan Frederic",
                "Kyle Kelley",
                "Jessica B Hamrick",
                "Jason Grout",
                "Sylvain Corlay"
            ],
            "title": "Jupyter notebooks-a publishing format for reproducible computational workflows",
            "venue": "In ELPUB,",
            "year": 2016
        },
        {
            "authors": [
                "R Leeb",
                "C Brunner",
                "G M\u00fcller-Putz",
                "A Schl\u00f6gl",
                "GJGUOT Pfurtscheller"
            ],
            "title": "Bci competition 2008\u2013graz data set b",
            "venue": "Graz University of Technology,",
            "year": 2008
        },
        {
            "authors": [
                "Xuechen Li",
                "Ting-Kam Leonard Wong",
                "Ricky T.Q. Chen",
                "David Duvenaud"
            ],
            "title": "Scalable gradients for stochastic differential equations",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Yaron Lipman",
                "Ricky T.Q. Chen",
                "Heli Ben-Hamu",
                "Maximilian Nickel",
                "Matt Le"
            ],
            "title": "Flow matching for generative modeling",
            "venue": "International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Xingchao Liu",
                "Chengyue Gong",
                "Qiang Liu"
            ],
            "title": "Flow straight and fast: Learning to generate and transfer data with rectified flow",
            "venue": "International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Aaron Lou",
                "Derek Lim",
                "Isay Katsman",
                "Leo Huang",
                "Qingxuan Jiang",
                "Ser Nam Lim",
                "Christopher M De Sa"
            ],
            "title": "Neural manifold ordinary differential equations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Simon C Lovell",
                "Ian W Davis",
                "W Bryan Arendall III",
                "Paul IW De Bakker",
                "J Michael Word",
                "Michael G Prisant",
                "Jane S Richardson",
                "David C Richardson"
            ],
            "title": "Structure validation by c\u03b1 geometry: \u03c6, \u03c8 and c\u03b2 deviation",
            "year": 2003
        },
        {
            "authors": [
                "Emile Mathieu",
                "Maximilian Nickel"
            ],
            "title": "Riemannian continuous normalizing flows",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Robert J McCann"
            ],
            "title": "Polar factorization of maps on riemannian manifolds",
            "venue": "Geometric & Functional Analysis GAFA,",
            "year": 2001
        },
        {
            "authors": [
                "Wes McKinney"
            ],
            "title": "Python for data analysis: Data wrangling with Pandas, NumPy, and IPython",
            "year": 2012
        },
        {
            "authors": [
                "Maher Moakher",
                "Philipp G Batchelor"
            ],
            "title": "Symmetric positive-definite matrices: From geometry to applications and visualization",
            "venue": "Visualization and processing of tensor fields,",
            "year": 2006
        },
        {
            "authors": [
                "Francis J Murray",
                "Kenneth S Miller"
            ],
            "title": "Existence theorems for ordinary differential equations",
            "venue": "Courier Corporation,",
            "year": 2013
        },
        {
            "authors": [
                "Laura JW Murray",
                "W Bryan Arendall III",
                "David C Richardson",
                "Jane S Richardson"
            ],
            "title": "Rna backbone is rotameric",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2003
        },
        {
            "authors": [
                "Kirill Neklyudov",
                "Daniel Severo",
                "Alireza Makhzani"
            ],
            "title": "Action matching: A variational method for learning stochastic dynamics from samples",
            "venue": "arXiv preprint arXiv:2210.06662,",
            "year": 2022
        },
        {
            "authors": [
                "Travis E Oliphant"
            ],
            "title": "A guide to NumPy, volume 1",
            "venue": "Trelgol Publishing USA,",
            "year": 2006
        },
        {
            "authors": [
                "Travis E Oliphant"
            ],
            "title": "Python for scientific computing",
            "venue": "Computing in Science & Engineering,",
            "year": 2007
        },
        {
            "authors": [
                "Daniele Panozzo",
                "Alec Jacobson"
            ],
            "title": "Libigl: A c++ library for geometry processing without a mesh data",
            "year": 2014
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "In Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "David Pfau",
                "Stig Petersen",
                "Ashish Agarwal",
                "David GT Barrett",
                "Kimberly L Stachenfeld"
            ],
            "title": "Spectral inference networks: Unifying deep and spectral learning",
            "venue": "arXiv preprint arXiv:1806.02215,",
            "year": 2018
        },
        {
            "authors": [
                "Boris T. Polyak",
                "Anatoli Juditsky"
            ],
            "title": "Acceleration of stochastic approximation by averaging",
            "year": 1992
        },
        {
            "authors": [
                "Prajit Ramachandran",
                "Barret Zoph",
                "Quoc V Le"
            ],
            "title": "Searching for activation functions",
            "venue": "arXiv preprint arXiv:1710.05941,",
            "year": 2017
        },
        {
            "authors": [
                "Danilo Jimenez Rezende",
                "George Papamakarios",
                "S\u00e9bastien Racaniere",
                "Michael Albergo",
                "Gurtej Kanwar",
                "Phiala Shanahan",
                "Kyle Cranmer"
            ],
            "title": "Normalizing flows on tori and spheres",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Noam Rozen",
                "Aditya Grover",
                "Maximilian Nickel",
                "Yaron Lipman. Moser"
            ],
            "title": "flow: Divergence-based generative modeling on manifolds",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "John Skilling"
            ],
            "title": "The eigenvalues of mega-dimensional matrices",
            "venue": "In Maximum Entropy and Bayesian Methods,",
            "year": 1989
        },
        {
            "authors": [
                "Yang Song",
                "Sahaj Garg",
                "Jiaxin Shi",
                "Stefano Ermon"
            ],
            "title": "Sliced score matching: A scalable approach to density and score estimation",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "arXiv preprint arXiv:2011.13456,",
            "year": 2020
        },
        {
            "authors": [
                "Vitaly Surazhsky",
                "Tatiana Surazhsky",
                "Danil Kirsanov",
                "Steven J Gortler",
                "Hugues Hoppe"
            ],
            "title": "Fast exact and approximate geodesics on meshes",
            "venue": "ACM transactions on graphics (TOG),",
            "year": 2005
        },
        {
            "authors": [
                "Greg Turk",
                "Marc Levoy"
            ],
            "title": "Zippered polygon meshes from range images",
            "venue": "In Proceedings of the 21st annual conference on Computer graphics and interactive techniques,",
            "year": 1994
        },
        {
            "authors": [
                "Stefan Van Der Walt",
                "S Chris Colbert",
                "Gael Varoquaux"
            ],
            "title": "The numpy array: a structure for efficient numerical computation",
            "venue": "Computing in Science & Engineering,",
            "year": 2011
        },
        {
            "authors": [
                "Guido Van Rossum",
                "Fred L Drake Jr."
            ],
            "title": "Python reference manual",
            "venue": "Centrum voor Wiskunde en Informatica Amsterdam,",
            "year": 1995
        },
        {
            "authors": [
                "C\u00e9dric Villani"
            ],
            "title": "Optimal transport: old and new, volume 338",
            "year": 2009
        },
        {
            "authors": [
                "Pascal Vincent"
            ],
            "title": "A connection between score matching and denoising autoencoders",
            "venue": "Neural computation,",
            "year": 2011
        },
        {
            "authors": [
                "Omry Yadan"
            ],
            "title": "Hydra - a framework for elegantly configuring complex applications. Github, 2019",
            "venue": "URL https://github.com/facebookresearch/hydra",
            "year": 2019
        },
        {
            "authors": [
                "De Bortoli"
            ],
            "title": "hidden units and tuned the number of layers for each type of experiment, ranging from 6 to 12 layers. We used the Swish activation function (Ramachandran et al., 2017) with a learnable parameter. We used Adam with a learning rate of 1e-4 and an exponential moving averaging on the weights (Polyak & Juditsky, 1992) with a decay of 0.999 for all of our experiments. High-dimensional tori",
            "year": 2022
        },
        {
            "authors": [
                "Lipman"
            ],
            "title": "training on general geometries, we solve for the path xt using 300 Euler steps with projection after every step. In order to avoid division by zero during the computation of the conditional vector field in equation 13, we solve xt from t = 0 to t = 1 \u2212 \u03b5, where \u03b5 is taken to be 1e-5; this effectively flows the base distribution to a non-degenerate distribution around",
            "year": 2023
        },
        {
            "authors": [
                "Brunner"
            ],
            "title": "We construct datasets using electroencephalography (EEG) data collected by Blankertz et al",
            "venue": "(Barachant et al.,",
            "year": 2006
        }
    ],
    "sections": [
        {
            "text": "We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on many real-world nonEuclidean datasets, and we demonstrate tractable training on general geometries, including triangular meshes with highly non-trivial curvature and boundaries.\n1 INTRODUCTION\nWhile generative models have recently made great advances in fitting data distributions in Euclidean spaces, there are still challenges in dealing with data residing in non-Euclidean spaces, specifically on general manifolds. These challenges include scalability to high dimensions (e.g., (Rozen et al., 2021)), the requirement for simulation or iterative sampling during training even for simple geometries like hyperspheres (e.g., (Mathieu & Nickel, 2020; De Bortoli et al., 2022)), and difficulties in constructing simple and scalable training objectives.\nIn this work, we introduce Riemannian Flow Matching (RFM), a simple yet powerful methodology for learning continuous normalizing flows (CNFs; (Chen et al., 2018)) on general Riemannian manifolds M. RFM builds upon the Flow Matching framework (Lipman et al., 2023; Albergo & Vanden-Eijnden, 2023; Liu et al., 2023) and learns a CNF by regressing an implicitly defined target vector field ut(x) that pushes a base distribution p towards a target distribution q defined by the training examples. To address the intractability of ut(x), we employ a similar approach to Conditional Flow Matching (Lipman et al., 2023), where we regress onto conditional vector fields ut(x|x1) that push p towards individual training examples x1.\nA key observation underlying our Riemannian generalization is that the conditional vector field necessary for training the CNF can be explicitly expressed in terms of a \u201cpremetric\u201d d(x, y), which distinguishes pairs of points x and y on the manifold. A natural choice for such a premetric is the geodesic distance function, which coincides with the straight trajectories previously used in Euclidean space by prior approaches.\nOn simple geometries, where geodesics are known in closed form (e.g., Euclidean space, hypersphere, hyperbolic space, torus, or any of their product spaces), Riemannian Flow Matching remains completely simulation-free. Even on general geometries, it only requires forward simulation of a relatively simple ordinary differential equation (ODE), without differentiation through the solver, stochastic iterative sampling, or divergence estimation.\nOn all types of geometries, Riemannian Flow Matching offers several advantages over recently proposed Riemannian diffusion models (De Bortoli et al., 2022; Huang et al., 2022). These advantages include avoiding iterative simulation of a noising process during training for geometries with analytic geodesic formulas; not relying on approximations of score functions or divergences of the parameteric vec-\ntor field; and not needing to solve stochastic differential equations (SDE) on manifolds, which is generally more challenging to approximate than ODE solutions (Kloeden et al., 2002; Hairer et al., 2006; Hairer, 2011). Table 1 summarizes the key differences with relevant prior methods, which we expand on further in Section 4 (Related Work).\nEmpirically, we find that Riemannian Flow Matching achieves state-of-the-art performance on manifold datasets across various settings, being on par or outperforming competitive baselines. We also demonstrate that our approach scales to higher dimensions without sacrificing performance, thanks to our scalable closed-form training objective. Moreover, we present the first successful training of continuous-time deep generative models on non-trivial geometries, including those imposed by discrete triangular meshes and manifolds with non-trivial boundaries that represent challenging constraints on maze-shaped manifolds."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "Riemannian manifolds. This paper considers complete connected, smooth Riemannian manifolds M with metric g as basic domain over which the generative model is learned. Tangent space to M at x \u2208 M is denoted TxM, and g defines an inner product over TxM denoted \u27e8u, v\u27e9g, u, v \u2208 TxM. TM = \u222ax\u2208M {x} \u00d7 TxM is the tangent bundle that collects all the tangent planes of the manifold. U = {ut} denotes the space of time dependent smooth vector fields (VFs) ut : [0, 1]\u00d7M \u2192 TM, where ut(x) \u2208 TxM for all x \u2208 M; divg(ut) is the Riemannian divergence w.r.t. the spatial (x) argument. We will denote by dvolx the volume element over M, and integration of a function f : M \u2192 R over M is denoted \u222b f(x)dvolx. For readers who are looking for a more comprehensive background on Riemannian manifolds, we recommend Gallot et al. (1990).\nProbability paths and flows on manifolds. Probability densities over M are continuous nonnegative functions p : M \u2192 R+ such that \u222b p(x)dvolx = 1. The space of probability densities over M is marked P . A probability path pt is a curve in probability space pt : [0, 1] \u2192 P; such paths will be used as supervision signal for training our generative models. A flow is a diffeomorphism \u03a8 : M \u2192 M defined by integrating instantaneous deformations represented by a time-dependent vector field ut \u2208 U . Specifically, a time-dependent flow, \u03c8t : M \u2192 M, is defined by solving the following ordinary differential equation (ODE) on M over t \u2208 [0, 1],\nd dt \u03c8t(x) = ut(\u03c8t(x)), \u03c80(x) = x, (1)\nand the final diffeomorphism is defined by setting \u03a8(x) = \u03c81(x). Given a probability density path pt, it is said to be generated by ut from p if \u03c8t pushes p0 = p to pt for all t \u2208 [0, 1]. More formally,\nlog pt(x) = log([\u03c8t]\u266fp)(x) = log p(\u03c8 \u22121 t (x))\u2212 \u222b t 0 divg(ut)(xs)ds (2)\nwhere the \u266f symbol denotes the standard push-forward operation and xs = \u03c8s(\u03c8\u22121t (x)). This formula can be derived from the Riemannian version of the instantaneous change of variables Formula (see\nequation 22 in (Ben-Hamu et al., 2022)). Previously, Chen et al. (2018) suggested modeling the flow \u03c8t implicitly by considering parameterizing the vector field ut. This results in a deep generative model of the flow \u03c8t, called a Continuous Normalizing Flow (CNF) which models a probability path pt through a continuous-time deformation of a base distribution p. A number of works have formulated manifold variants (Mathieu & Nickel, 2020; Lou et al., 2020; Falorsi, 2020) that require simulation in order to enable training, while some simulation-free variants (Rozen et al., 2021; Ben-Hamu et al., 2022) scale poorly to high dimensions and do not readily adapt to general geometries."
        },
        {
            "heading": "3 METHOD",
            "text": "We aim to train a generative model that lies on a complete, connected smooth Riemannian manifold M endowed with a metric g. Concretely, we are given a set of training samples x1 \u2208 M from some unknown data distribution q(x1), q \u2208 P . Our goal is to learn a parametric map \u03a6 : M \u2192 M that pushes a simple base distribution p \u2208 P to q."
        },
        {
            "heading": "3.1 FLOW MATCHING ON MANIFOLDS",
            "text": "Flow Matching Lipman et al. (2023) is a method to train Continuous Normalizing Flow (CNF) on Euclidean space that sidesteps likelihood computation during training and scales extremely well, similar to diffusion models (Ho et al., 2020; Song et al., 2020b), while allowing the design of more general noise processes which enables this work. We provide a brief summary and make the necessary adaptation to formulate Flow Matching on Riemannian manifolds. Derivations of the manifold case with full technical details are in Appendix A.\nRiemannian Flow Matching. Flow Matching (FM) trains a CNF by fitting a vector field v \u2208 U , i.e., vt(x) \u2208 TxM, with parameters \u03b8 \u2208 Rp, to an a priori defined target vector field u \u2208 U that is known to generate a probability density path pt \u2208 P over M satisfying p0 = p and p1 = q. On a manifold endowed with a Riemannian metric g, the Flow Matching objective compares the tangent vectors vt(x), ut(x) \u2208 TxM using the Riemannian metric g at that tangent space:\nLRFM(\u03b8) = Et,pt(x) \u2225vt(x)\u2212 ut(x)\u2225 2 g (3)\nwhere t \u223c U [0, 1], the uniform distribution over [0, 1]. Probability path construction. Riemannian Flow Matching therefore requires coming up with a probability density path pt \u2208 P , t \u2208 [0, 1] that satisfies the boundary conditions\np0 = p, p1 = q (4) and a corresponding vector field (VF) ut(x) which generates pt(x) from p in the sense of equation 2. One way to construct such a pair is to create per-sample conditional probability paths pt(x|x1) satisfying p0(x|x1) = p(x), p1(x|x1) \u2248 \u03b4x1(x), (5) where \u03b4x1(x) is the Dirac distribution over M centered at x1. One can then define pt(x) as the marginalization of these conditional probability paths over q(x1).\npt(x) = \u222b M pt(x|x1)q(x1)dvolx1 , (6)\nwhich satisfies equation 4 by construction. It was then proposed by Lipman et al. (2023)\u2014which we verify for the manifold setting\u2014to define ut(x) as the \u201cmarginalization\u201d of conditional vector fields ut(x|x1) that generates pt(x|x1) (in the sense detailed in Section 2),\nut(x) = \u222b M ut(x|x1) pt(x|x1)q(x1) pt(x) dvolx1 , (7)\nwhich provably generates pt(x). However, directly plugging ut(x) into equation 3 is intractable as computing ut(x) is intractable.\nRiemmanian Conditional Flow Matching. A key insight from Lipman et al. (2023) is that when the targets pt and ut are defined as in equations 6 and 7, the FM objective is equivalent to the following Conditional Flow Matching objective,\nLRCFM(\u03b8) = Et,q(x1),pt(x|x1) \u2225vt(x)\u2212 ut(x|x1)\u2225 2 g (8)\nas long as ut(x|x1) is a vector field that generates pt(x|x1) from p.\nAlgorithm 1 Riemannian CFM Require: base p, target q, scheduler \u03ba\nInitialize parameters \u03b8 of vt while not converged do\nsample time t \u223c U(0, 1) sample training example x1 \u223c q sample noise x0 \u223c p if simple geometry then xt = expx1(\u03ba(t)logx1(x0)) else if general geometry then xt = solve_ODE([0, t], x0, ut(x|x1)) end if \u2113(\u03b8) = \u2225vt(xt; \u03b8)\u2212 x\u0307t\u22252g \u03b8 = optimizer_step(\u2113(\u03b8))\nend while\nTo simplify this loss, consider the conditional flow, which we denote via the shorthand,\nxt = \u03c8t(x0|x1), (9)\ndefined as the solution to the ODE in equation 1 with the VF ut(x|x1) and the initial condition \u03c80(x0|x1) = x0. Furthermore, since sampling from pt(x|x1) can be done with \u03c8t(x0|x1), where x0 \u223c p(x0), we can reparametrize equation 8 as\nLRCFM(\u03b8) = Et,q(x1),p(x0) \u2225vt(xt)\u2212 x\u0307t\u2225 2 g (10)\nwhere x\u0307t = d/dt xt = ut(xt|x1). Riemannian Conditional Flow Matching (RCFM) has three requirements: a parametric vector field vt that outputs vectors on the tangent planes, the use of the appropriate Riemannian metric \u2225\u00b7\u2225g, and the design of a (computationally tractable) conditional flow \u03c8t(x|x1) whose probability path satisfies the boundaries conditions in equation 5. We discuss this last point in the next section. Generally, compared to existing methods for training generative models on manifolds, RCFM is both simple and highly scalable; the training procedure is summarized in Algorithm 1 and a detailed comparison can be found in Appendix D."
        },
        {
            "heading": "3.2 CONSTRUCTING CONDITIONAL FLOWS THROUGH PREMETRICS",
            "text": "We discuss the construction of conditional flows \u03c8t(x|x1) on M that concentrate all mass at x1 at time t = 1; Figure 2 provides an illustration. This ensures that equation 5 will hold (regardless of the choice of p) since all points are mapped to x1 at time t = 1, namely\n\u03c81(x|x1) = x1, for all x \u2208 M. (11)\nOn general manifolds, directly constructing \u03c8t that satisfies equation 11 can be overly cumbersome. Alternatively, we propose an approach based on designing a premetric instead, which has simple properties that, when satisfied, characterize conditional flows which satisfy equation 11. Specifically, we define a premetric as d : M\u00d7M \u2192 R satisfying:\n1. Non-negative: d(x, y) \u2265 0 for all x, y \u2208 M. 2. Positive: d(x, y) = 0 iff x = y. 3. Non-degenerate: \u2207d(x, y) \u0338= 0 iff x \u0338= y.\nWe use as convention \u2207d(x, y) = \u2207xd(x, y). Such a premetric denotes the closeness of a point x to x1, and we aim to design a conditional flow \u03c8t(x|x1) that monotonically decreases this\npremetric. That is, given a monotonically decreasing differentiable function \u03ba(t) satisfying \u03ba(0) = 1 and \u03ba(1) = 0, we want to find a \u03c8t that decreases d(\u00b7, x1) according to\nd(\u03c8t(x0|x1), x1) = \u03ba(t)d(x0, x1), (12)\nHere \u03ba(t) acts as a scheduler that determines the rate at which d(\u00b7|x1) decreases. Note that at t = 1, we necessarily satisfy equation 11 since x1 is the unique solution to d(\u00b7|x1) = 0 due to the \u201cpositive\u201d property of the premetric. Our next theorem shows that \u03c8t(x|x1) satisfying equation 12 results in the following vector field,\nut(x|x1) = d log \u03ba(t)\ndt d(x, x1) \u2207d(x, x1) \u2225\u2207d(x, x1)\u22252g , (13)\nThe \u201cnon-degenerate\u201d property guarantees this conditional vector field is defined everywhere x \u0338= x1.\nTheorem 3.1. The flow \u03c8t(x|x1) defined by the vector field ut(x|x1) in equation 13 satisfies equation 12, and therefore also equation 11. Conversely, out of all conditional vector fields that satisfy equation 12, this ut(x|x1) is the minimal norm solution.\nA more concise statement and full proof of this result can be found in Appendix B. Here we provide proof for the first part: Consider the scalar function a(t) = d(xt, x1), where xt = \u03c8t(x|x1) is the flow defined with the VF in equation 13. Differentiation w.r.t. time gives\nd dt a(t) = \u27e8\u2207d(xt, x1), x\u0307t\u27e9g = \u27e8\u2207d(xt, x1), u(xt|x1)\u27e9g = d log \u03ba(t) dt a(t),\nThe solution of this ODE is a(t) = \u03ba(t)d(x, x1), which can be verified through substitution, and hence proves d(xt, x1) = \u03ba(t)d(x, x1). Intuitively, ut(x|x1) is the minimal norm solution since it does not contain orthogonal directions that do not decrease the premetric.\nA simple choice we make in this paper for the scheduler is \u03ba(t) = 1\u2212 t, resulting in a conditional flow that linearly decreases the premetric between xt and x1. Using this, we arrive at a more explicit form of the RCFM objective,\nLRCFM(\u03b8) = Et,q(x1),p(x0) \u2225\u2225\u2225\u2225\u2225vt(xt) + d(x0, x1) \u2207d(xt, x1)\u2225\u2207d(xt, x1)\u22252g \u2225\u2225\u2225\u2225\u2225 2\ng\n. (14)\nFor general manifolds M and premetrics d, training with Riemmanian CFM will require simulation in order to solve for xt, though it does not need to differentiate through xt. However, on simple geometries RCFM can become completely simulation-free by choosing the premetric to be the geodesic distance, as we discuss next.\nGeodesic distance. A natural choice for the premetric d(x, y) over a Riemannian manifold M is the geodesic distance dg(x, y). Firstly, we note that when using geodesic distance as our choice of premetric, the flow \u03c8t(x0|x1)\u2014since it is the minimal norm solution\u2014is equivalent to the geodesic path, i.e., shortest path, connecting x0 and x1. Proposition 3.2. Consider a complete, connected smooth Riemannian manifold (M, g) with geodesic distance dg(x, y). In case d(x, y) = dg(x, y) then xt = \u03c8t(x0|x1) defined by the conditional VF in equation 13 with the scheduler \u03ba(t) = 1\u2212 t is a constant speed geodesic connecting x0 to x1.\nThis makes it easy to compute xt on simple manifolds, which we define as manifolds with closed-form geodesics, e.g., Euclidean space, the hypersphere, the hyperbolic space, the high-dimensional torus, and some matrix Lie Groups. In particular, the geodesic connecting x0 and x1 can be expressed in terms of the exponential and logarithm maps,\nxt = expx1(\u03ba(t) logx1(x0)), t \u2208 [0, 1]. (15)\nThis formula can simply be plugged into equation 10, resulting in a highly scalable training objective. A list of simple manifolds that we consider can be found in Table 5.\nEuclidean geometry. With Euclidean geometry M = Rn, and with standard Euclidean norm d(x, y) = \u2225x\u2212 y\u22252, the conditional VF (equation 13) with scheduler \u03ba(t) = 1\u2212 t reduces to the VF used by Lipman et al. (2023), ut(x|x1) = x1\u2212x1\u2212t , and the RCFM objective takes the form\nLRCFM(\u03b8) = Et,q(x1),p(x0) \u2225vt(xt) + x0 \u2212 x1\u2225 2 2 ,\nwhich coincides with the Euclidean case of Flow Matching presented in prior works (Lipman et al., 2023; Liu et al., 2023)."
        },
        {
            "heading": "3.3 SPECTRAL DISTANCES ON GENERAL GEOMETRIES",
            "text": "Geodesics can be difficult to compute efficiently for general geometries, especially since it needs to be computed for any possible pair of points. Hence, we propose using premetrics that can be computed quickly for any pair of points on M contingent on a one-time upfront cost. In particular, for general Riemannian manifolds, we consider the use of approximate spectral distances as an alternative to the geodesic distance. Spectral distances actually offer some benefits over the geodesic distance such as\nrobustness to topological noise, smoothness, and are globally geometry-aware (Lipman et al., 2010). Note however, that spectral distances do not define minimizing (geodesic) paths, and will require simulation of ut(x|x1) in order to compute conditional flows xt. Let \u03c6i : M \u2192 R be the eigenfunctions of the Laplace-Beltrami operator \u2206g over M with corresponding eigenvalues \u03bbi, i.e., they satisfy \u2206g\u03c6i = \u03bbi\u03c6i, for i = 1, 2, . . . , then spectral distances are of the form\ndw(x, y)2 = \u221e\u2211 i=1 w(\u03bbi) (\u03c6i(x)\u2212 \u03c6i(y))2 , (16)\nwhere w : R \u2192 R+ is some monotonically decreasing weighting function. Popular instances of spectral distances include:\n1. Diffusion Distance Coifman & Lafon (2006): w(\u03bb) = exp(\u22122\u03c4\u03bb), with a parameter \u03c4 . 2. Biharmonic Distance Lipman et al. (2010): w(\u03bb) = \u03bb\u22122.\nIn practice, we truncate the infinite series in equation 16 to the smallest k eigenvalues. These k eigenfunctions can be numerically solved as a one-time preprocessing cost prior to training. Furthermore, we note that using an approximation of the spectral distance with finite k is sufficient for satisfying the properties of the premetric, leading to no bias in the training procedure. Lastly, we consider manifolds with boundaries and show that solving eigenfunctions using the natural, or Neumann, boundary conditions ensures that the resulting ut(x|x1) does not leave the interior of the manifold. Detailed discussions on these points above can be found in Appendix G. Figure 3 visualizes contour plots of these spectral distances for manifolds with non-trivial curvatures."
        },
        {
            "heading": "4 RELATED WORK",
            "text": "Deep generative models on Riemannian manifolds. Some initial works suggested constructing normalizing flows that map between manifolds and Euclidean spaces of the same intrinsic dimension (Gemici et al., 2016; Rezende et al., 2020; Bose et al., 2020), often relying on the tangent space at some pre-specified origin. However, this approach is problematic when the manifold is not homeomorphic to Euclidean space, resulting in both theoretical and numerical issues. On the other hand, continuous-time models such as continuous normalizing flows bypass such topogical constraints and flow directly on the manifold itself. To this end, a number of works have formulated continuous normalizing flows on simple manifolds (Mathieu & Nickel, 2020; Lou et al., 2020; Falorsi, 2020), but these rely on maximum likelihood for training, a costly simulation-based procedure. More recently, simulation-free training methods for continuous normalizing flows on manifolds have been proposed (Rozen et al., 2021; Ben-Hamu et al., 2022); however, these scale poorly to high dimensions and do not adapt to general geometries.\nRiemannian diffusion models. With the influx of diffusion models that allow efficient simulationfree training on Euclidean space (Ho et al., 2020; Song et al., 2020b), multiple works have attempted to adopt diffusion models to manifolds (Mathieu & Nickel, 2020; Huang et al., 2022). However, due to the reliance on stochastic differential equations (SDE) and denoising score matching (Vincent, 2011), these approaches necessitate in-training simulation and approximations when applied to non-Euclidean manifolds.\nFirst and foremost, they lose the simulation-free sampling of xt \u223c pt(x|x1) that is offered in the Euclidean regime; this is because the manifold analog of the Ornstein\u2013Uhlenbeck SDE does not have closed-form solutions. Hence, diffusion-based methods have to resort to simulated random walks as a noising process even on simple manifolds (De Bortoli et al., 2022; Huang et al., 2022).\nFurthermore, even on simple manifolds, the conditional score function is not known analytically, so De Bortoli et al. (2022) proposed approximating the conditional score function with either an eigenfunction expansion or Varhadan\u2019s heat-kernel approximation. These approximations lead to biased gradients in the denoising score matching framework. We find that the heat-kernel approximations can potentially be extremely biased, even with hundreds with eigenfunctions (see Figure 10). In contrast, we show in Figure 11 that our framework can satisfy all premetric requirements even with a small number of eigenfunctions for the spectral distance approximation\u2014hence guaranteeing that the optimal model distribution is the data distribution. See detailed discussions in Appendix G.1.\nA way to bypass the conditional score function is to use implicit score matching (Hyv\u00e4rinen & Dayan, 2005), which Huang et al. (2022) adopts for the manifold case, but this instead requires divergence computation of the large neural nets during training. Using the Hutchinson estimator (Hutchinson, 1989; Skilling, 1989; Grathwohl et al., 2019; Song et al., 2020a) for divergence estimation results in a more scalable algorithm, but the variance of the Hutchinson estimator scales poorly with dimension (Hutchinson, 1989) and is further exacerbated on non-Euclidean manifolds (Mathieu & Nickel, 2020).\nFinally, the use of SDEs as a noising process requires carefully constructing suitable reverse-time processes that approximate either just the probability path (Anderson, 1982) or the actual sample trajectories (Li et al., 2020), whereas ODE solutions are generally well-defined in both forward and reverse directions (Murray & Miller, 2013).\nIn contrast to these methods, Riemannian Flow Matching is simulation-free on simple geometries, has exact conditional vector fields, and does not require divergence computation during training. These properties are summarized in Table 1, and a detailed comparison of algorithmic differences to diffusion-based approaches is presented in Appendix D. Lastly, for general Riemannian manifolds we show that the design of a relatively simple premetric is sufficient, allowing the use of general distance functions that don\u2019t satisfy all axioms of a metric\u2014such as approximate spectral distances with finite truncation\u2014going beyond what is currently possible with existing Riemannian diffusion methods.\nEuclidean Flow Matching. Riemannian Flow Matching is built on top of recent simulation-free methods that work with ODEs instead of SDEs, regressing directly onto generating vector fields instead of score functions (Lipman et al., 2023; Albergo & Vanden-Eijnden, 2023; Liu et al., 2023; Neklyudov et al., 2022), resulting in an arguably simpler approach to continuous-time generative modeling without the intricacies of dealing with stochastic differential equations. In particular, Lipman et al. (2023) shows that this approach encompasses and broadens the probability paths used\nby diffusion models while remaining simulation-free; Albergo & Vanden-Eijnden (2023) discusses an interpretation based on the use of interpolants\u2014equivalent to our conditional flows \u03c8t(x|x1), except we also make explicit the construction of the marginal probability path pt(x) and vector field ut(x); Liu et al. (2023) shows that repeatedly fitting to a model\u2019s own samples leads to straighter trajectories; and Neklyudov et al. (2022) formulates an implicit objective when ut(x) is a gradient field."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We consider data from earth and climate science, protein structures, high-dimensional tori, complicated synthetic distributions on general closed manifolds, and distributions on maze-shaped manifolds that require navigation across non-trivial boundaries. Details regarding training setup is discussed in Appendix H. Due to space constraints, additional experiments on hyperbolic manifold and a manifold over matrices can be found in Appendix J, which are endowed with nontrivial Riemannian metrics. Table 5 provides all details of the simple manifolds and their geometries. Details regarding the more complex mesh manifolds can be found in the open source code, which we release for reproducibility1.\nEarth and climate science datasets on the sphere. We make use of the publicly sourced datasets (NOAA, 2020a;b; Brakenridge, 2017; EOSDIS, 2020) compiled by Mathieu & Nickel (2020). These data points lie on the 2-D sphere, a simple manifold with closed form exponential and logarithm maps. We therefore stick to the geodesic distance and compute geodesics in closed form as in equation 15. Table 2 shows the results alongside prior methods. We achieve a sizable improvement over prior works on the volcano and fire datasets which have highly concentrated regions that require a high fidelity. Figure 8 shows the density of our trained models.\nProtein datasets on the torus. We make use of the preprocessed protein (Lovell et al., 2003) and RNA (Murray et al., 2003) datasets compiled by Huang et al. (2022). These datasets represent torsion angles and can be represented on the 2D and 7D torus. We represent the data on a flat torus, which is isometric to the product of 1-D spheres used by prior works (Huang et al., 2022; De Bortoli et al., 2022) and result in densities that are directly comparable due to this isometry. Results are displayed in Table 3, and we show learned densities of the protein datasets in Figure 9. Compared to Huang et al. (2022), we see a significant gain in performance particularly on the higher dimensional 7D torus, due to the higher complexity of the dataset.\nScaling to high dimensions. We next consider the scalability of our method in the case of high-dimensional tori, following the exact setup in De Bortoli et al. (2022). We com-\n1https://github.com/facebookresearch/riemannian-fm\npare to Moser Flow (Rozen et al., 2021), which does not scale well into high dimensions, and Riemannian Score-based (De Bortoli et al., 2022) using implicit score matching (ISM).\nAs shown in Table 1, this objective gets around the need to approximate conditional score functions, but it requires stochastic divergence estimation, introducing larger amounts of variance at higher dimensions. In Figure 5 we plot log-likelihood values, across these two baselines and our method with the geodesic construction. We see that our method performs steadily, with no significant drop in performance at higher dimensions since we do not have any reliance on approximations.\nManifolds with non-trivial curvature. We next experiment with general closed manifolds using spectral distances as described in Section 3.3. Specifically, we experiment on manifolds described by triangular meshes. For meshes, computing\ngeodesic distances on-the-fly is too expensive for our use case, which requires hundreds of evaluations per training iteration. Fast approximations to the geodesic distance between two points are O(n log n) (Kimmel & Sethian, 1998), while exact geodesic distances require O(n2 log n) (Surazhsky et al., 2005), where n is the number of edges. On the other hand, computing spectral distances is O(k), where k \u226a n, i.e., it does not scale with the complexity of the manifold after the one-time preprocessing step. As our manifolds, we use the Standard Bunny (Turk & Levoy, 1994) and Spot the Cow (Crane et al., 2013). Similar to Rozen et al. (2021), we construct distributions by computing the k-th eigenfunction, thresholding, and then sampling proportionally to the eigenfunction. This is done on a high-resolution mesh so that the distribution is non-trivial on each triangle. Figure 4 contains visualizations of the eigenfunctions, the learned density, and samples from a trained model that transports from a uniform base distribution. We used k=200 eigenfunctions, which is sufficient for our method to produce high fidelity samples.\nIn Table 4, we report the test NLL of models trained using either the diffusion distance or the biharmonic distance. We had to carefully tune the diffusion distance hyperparameter \u03c4 while the biharmonic distance was straightforward to use out-of-the-box and it has better smoothness properties (see Figure 3).\nManifolds with boundaries. Lastly, we experiment with manifolds that have boundaries. Specifically, we consider randomly generated mazes visualized in Figure 6. We set the base distribution to be a Gaussian in the middle of the maze, and set the target distribution to be a mixture of densities at corners of the maze. These mazes are represented using triangular meshes, and we use the biharmonic distance using k=30 eigenfunctions. Once trained, the model represents a single vector field that transports all mass from the source distribution to the target distribution with no crossing paths. We plot\nsample trajectories in Figure 6 (b) and (d), where it can be seen that the learned vector field avoids boundaries of the manifold and successfully navigates to different modes in the target distribution."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We propose Riemannian Flow Matching as a highly-scalable approach for training continuous normalizing flows on manifolds. Our method is completely simulation-free and introduces zero approximation errors on simple geometries that have closed-form geodesics. We also introduce benchmark problems for general manifolds and showcase for the first time, tractable training on general geometries including both closed manifolds and manifolds with boundaries."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "Ricky T. Q. Chen would like to thank Chin-Wei Huang for helpful discussions. Additionally, we acknowledge the Python community (Van Rossum & Drake Jr, 1995; Oliphant, 2007) for developing the core set of tools that enabled this work, including PyTorch (Paszke et al., 2019), PyTorch Lightning (Falcon & team, 2019), Hydra (Yadan, 2019), Jupyter (Kluyver et al., 2016), Matplotlib (Hunter, 2007), seaborn (Waskom et al., 2018), numpy (Oliphant, 2006; Van Der Walt et al., 2011), SciPy (Jones et al., 2014) pandas (McKinney, 2012), geopandas (Jordahl et al., 2020), torchdiffeq (Chen, 2018), libigl (Panozzo & Jacobson, 2014), and PyEVTK (Herrera, 2019)."
        },
        {
            "heading": "A CONDITIONAL FLOW MATCHING ON MANIFOLDS",
            "text": "We provide the necessary derivations and proofs for the Conditional Flow Matching over a Riemannian manifolds; the proofs and derivations from Lipman et al. (2023) are followed \"as-is\", with the necessary adaptation to the Riemannian setting.\nAssumptions. We will use notations and setup from Section 2. Let p(\u00b7|x1) : [0, 1] \u2192 P be a (conditional) probability path sufficiently smooth with integrable derivatives, strictly positive pt(x|x1) > 0, and p0(x|x1) = p, where p \u2208 P is our source density. Let u(\u00b7|x1) \u2208 U be a (conditional) time-dependent vector field, sufficiently smooth with integrable derivatives and such that \u222b 1\n0 \u222b M \u2225ut(x|x1)\u2225g pt(x|x1)dvolxdt <\u221e.\nFurther assume ut(x|x1) generates pt(x|x1) from p in the sense of equation 2, i.e., if we denote by \u03c8t(x|x1) the solution to the ODE (equation 1):\nd dt \u03c8t(x|x1) = ut(\u03c8t(x|x1)|x1) (17)\n\u03c80(x|x1) = x (18) then pt(\u00b7|x1) = [\u03c8t(\u00b7|x1)]# p. (19) Proof of the marginal VF formula, equation 7. First, the Mass Conservation Formula Theorem (see, e.g., Villani (2009)) implies that pt(x|x1) and ut(x|x1) satisfy\nd dt pt(x|x1) + divg(pt(x|x1)ut(x|x1)) = 0 (20)\nwhere divg is the Riemannian divergence with metric g.\nNext, we differentiate the marginal pt(x) w.r.t. t:\nd dt pt(x) = \u222b M d dt pt(x|x1)q(x1)dvolx1\n= \u2212divg [\u222b\nM ut(x|x1)pt(x|x1)q(x1)dvolx1 ] = \u2212divg [ pt(x) \u222b M ut(x|x1) pt(x|x1)q(x1) pt(x) dvolx1\n] = \u2212divg [pt(x)ut(x)]\nwhere in the first and second equalities we changed the order of differentiation and integration, and in the second equality we used the mass conservation formula for ut(x|x1). In the previous to last equality we multiplied and divided by pt(x). In the last equality we defined the marginal vector field ut as in equation 7.\nRCFM loss equivalent to RFM loss. We will now show the equivalence of the RCFM loss (equation 8) and the RFM loss (equation 3). First note the losses expand as follows:\nLRFM(\u03b8) = Et,pt(x) \u2225vt(x)\u2212 ut(x)\u2225 2 g = Et,pt(x) \u2225vt(x)\u2225 2 g \u2212 2 \u27e8vt(x), ut(x)\u27e9g + \u2225ut(x)\u2225 2 g\nLRCFM(\u03b8) = E t,q(x1) pt(x|x1) \u2225vt(x)\u2212 ut(x|x1)\u22252g = E t,q(x1) pt(x|x1) \u2225vt(x)\u22252g \u2212 2 \u27e8vt(x), ut(x|x1)\u27e9g + \u2225ut(x|x1)\u2225 2 g\nSecond, note that\nEt,q(x1),pt(x|x1) \u2225vt\u2225 2 g = \u222b 1 0 \u222b M \u2225vt(x)\u22252g pt(x|x1)q(x1)dvolxdvolx1dt\n= \u222b 1 0 \u222b M \u2225vt(x)\u22252g pt(x)dvolxdt = Et,pt(x) \u2225vt\u2225 2 g\nLastly, Et,q(x1),pt(x|x1) \u27e8vt(x), ut(x|x1)\u27e9g = \u222b 1 0 \u222b M \u222b M \u27e8vt(x), ut(x|x1)\u27e9g pt(x|x1)q(x1)dvolxdvolx1dt\n= \u222b 1 0 \u222b M \u2329 vt(x), \u222b M ut(x|x1)pt(x|x1)q(x1)dvolx1 \u232a g dvolx dt\n= \u222b 1 0 \u222b M \u2329 vt(x), \u222b M ut(x|x1) pt(x|x1)q(x1) pt(x) dvolx1 \u232a g pt(x)dvolx dt\n= \u222b 1 0 \u222b M \u27e8vt(x), ut(x)\u27e9g pt(x)dvolx dt\n= Et,pt(x) \u27e8vt(x), ut(x)\u27e9g We got that LRCFM(\u03b8) and LRFM(\u03b8) differ by a constant,\nconst = \u222b 1 0 \u222b M \u2225ut(x)\u22252g pt(x)dvolx dt\u2212 \u222b 1 0 \u222b M \u2225ut(x|x1)\u22252g pt(x|x1)q(x1)dvolxdvolx1 dt\nthat does not depend on \u03b8."
        },
        {
            "heading": "B PROOF OF THEOREM 3.1",
            "text": "Theorem 3.1. The flow \u03c8t(x|x1) defined by the vector field ut(x|x1) in equation 13 satisfies equation 12, and therefore also equation 11. Conversely, out of all conditional flows \u03c8\u0303t(x|x1) defined by a vector fields u\u0303(x|x1) that satisfy equation 12, this ut(x|x1) is of minimal norm.\nProof. Let xt = \u03c8t(x|x1) be the flow defined by equation 13 in the sense of equation 1. Differentiating the time-dependent function d(xt, x1) w.r.t. time gives\nd dt d(xt, x1) = \u27e8\u2207d(xt, x1), x\u0307t\u27e9g = \u27e8\u2207d(xt, x1), ut(xt|x1)\u27e9g = d log \u03ba(t) dt d(xt, x1) (21)\nThis shows that the function a(t) = d(xt, x1) satisfies the ODE\nd dt a(t) = d log \u03ba(t) dt a(t),\nwith the initial condition a(0) = d(x, x1). General solutions to this ODE are of the form a(t) = c\u03ba(t), where c > 0 is a constant set by the initial conditions. This can be verified by substitution. The constant c is set by the initial condition,\nd(x, x1) = a(0) = c\u03ba(0) = c.\nThis gives a(t) = d(x, x1)\u03ba(t) as the solution. Due to uniqueness of ODE solutions we get that equation 12 holds.\nConversely, consider xt = \u03c8t(x|x1) satisfying equation 12. Differentiating both sides of this equation w.r.t. t and then using equation 12 again we get\n\u27e8\u2207d(xt, x1), x\u0307t\u27e9g = d\u03ba(t)\ndt d(x, x1) =\nd\u03ba(t)\ndt\n1\n\u03ba(t) d(xt, x1) =\nd log \u03ba(t)\ndt d(xt, x1).\nIf we let ut(x|x1) denote the VF defining the diffeomorphism \u03c8t(x|x1) in the sense of equation 1 then the last equation takes the form\n\u27e8\u2207d(xt, x1), ut(xt|x1)\u27e9g = d log \u03ba(t)\ndt d(xt, x1). (22)\nThis equation provides an under-determined linear system for ut(y|x1) \u2208 TyM at every point y = xt with non-zero probability, which in our case is all y \u2208 M as we assume pt(y|x1) > 0 for all y \u2208 M. As can be seen in equation 21, ut(x|x1) defined in equation 13 is also satisfying this equation. Further note, that since ut(x|x1) defined in equation 13 satisfies ut(x|x1) \u2225 \u2207d(x, x1) (proportional) it is the minimal norm solution to the linear system in equation 22."
        },
        {
            "heading": "C PROOF OF PROPOSITION 3.2",
            "text": "Proposition 3.2. Consider a complete, connected smooth Riemannian manifold (M, g) with geodesic distance dg(x, y). In case d(x, y) = dg(x, y) then xt = \u03c8t(x0|x1) defined by the conditional VF in equation 13 with the scheduler \u03ba(t) = 1\u2212 t is a geodesic connecting x0 to x1.\nProof. First, note that by definition \u03c80(x0|x1) = x0, and \u03c81(x0|x1) = x1. Second, from Proposition 6 in McCann (2001) we have that\n\u2207x 1\n2 dg(x, y)2 = \u2212 logx(y)\nwhere log is the Riemannian logarithm map. From the chain rule we have\n\u2207x 1\n2 dg(x, y)2 = dg(x, y)\u2207xdg(x, y)\nSince the logarithm map satisfies \u2225logx(y)\u2225g = dg(x, y) we have that\n\u2225\u2207dg(x, y)\u2225g = 1. (23)\nNow, computing the length of the curve xt we get\u222b 1 0 \u2225x\u0307t\u2225g dt = \u222b 1 0 \u2225ut(xt|x1)\u2225g dt\n= \u222b 1 0 \u2225\u2225\u2225\u2225\u2225\u2212dg(xt, x1)1\u2212 t \u2207dg(xt, x1)\u2225\u2207dg(xt, x1)\u22252g \u2225\u2225\u2225\u2225\u2225 g dt\n= dg(x0, x1) \u222b 1 0 \u2225\u2225\u2225\u2225\u2225 \u2207dg(xt, x1)\u2225\u2207dg(xt, x1)\u22252g \u2225\u2225\u2225\u2225\u2225 g dt\n= dg(x0, x1) \u222b 1 0 dt\n= dg(x0, x1)\nwhere in the second equality we used the definition of the conditional VF (equation 13) with \u03ba(t) = 1\u2212 t, in the third equality we used Theorem 3.1 and equation 12, and in the fourth equality we used equation 23. Since xt realizes a minimum of the length function, it is a geodesic."
        },
        {
            "heading": "D ALGORITHMIC COMPARISON TO RIEMANNIAN DIFFUSION MODELS",
            "text": "Algorithm 2 Riemannian Diffusion Models Require: base distribution p(xT ), target q(x0)\nAlgorithm 3 Riemannian Flow Matching Require: base distribution p(x0), target q(x1)"
        },
        {
            "heading": "E LIMITATIONS",
            "text": "As can be seen from Figure 7, our method still requires simulation of xt on general manifolds. This sequential process can be time consuming, and a more parallel or simulation-free approach to constructing xt would be more favorable. Furthermore, the spectral distances require eigenfunction solvers which may be computationally expensive on complex manifolds. Using approximate methods such as neural eigenfunctions (Pfau et al., 2018; Deng et al., 2022) may be a possibility. One major advantage of our premetric formulation is that these eigenfunctions need not be perfectly solved in order to satisfy the relatively simple properties of our premetric."
        },
        {
            "heading": "F ADDITIONAL FIGURES",
            "text": ""
        },
        {
            "heading": "G ADDITIONAL DISCUSSION",
            "text": "G.1 ON THE USE OF APPROXIMATE SPECTRAL DISTANCES AS THE PREMETRIC\nComputation cost. The smallest k eigenvalues and their eigenfunctions need only be computed once as a pre-processing step. On manifolds represented as discrete triangular meshes, this step can be done in a matter of seconds. Afterwards, spectral distances can be computed very efficiently for all pairs of points. Note however, that training with RCFM does still require simulating for xt, but as the vector fields (equation 13) do not contain neural networks, the flows can be solved efficiently in practice. This results in a similar cost to diffusion-based methods (Huang et al., 2022; De Bortoli et al., 2022) that require simulation even for simple manifolds.\nSufficiency with finite k. One may wonder if we pay any approximation costs when using finite k; the answer is no. In fact, we only need as many eigenfunctions as it takes to be able to distinguish (almost) every pair of points on the manifold. Put differently, we don\u2019t need to compute the spectral distances perfectly, only sufficiently enough that the conditions of our premetric are satisfied. Regarding the question of what k is enough? This is only understood partially: for local neighborhoods the number of required eigenfunctions is the manifold dimension (but not necessarily the first ones), a property proven in (Jones et al., 2008, Theorem 2). Nevertheless, the use of spectral distances computed with k smallest eigenvalues is equivalent to computing Euclidean distances in a k-dimensional Euclidean embedding using the same eigenfunctions; this embedding is known to preserve neighborhoods optimally (Belkin & Niyogi, 2003).\nAs comparison, Riemannian score-based generative models (De Bortoli et al., 2022) suggests a heat kernel approximation\np\u0303t(xt|x0) = k\u2211\ni=0\ne\u2212\u03bbit\u03c6i(x0)\u03c6i(xt), (24)\nresulting in the approximated conditional score function\n\u2207xt log p\u0303t(xt|x0) = \u2207xt log k\u2211\ni=0\ne\u2212\u03bbit\u03c6i(x0)\u03c6i(xt). (25)\nHowever, equation 24 is only correct as k \u2192 \u221e. This is manifested in practice as large amounts of error even when using hundreds of eigenfunctions. In Figure 10, we visualize the heat kernel approximation in equation 24 as well as relative error in the score function, taken in expectation w.r.t. pt(x|x0), i.e., the relative error is weighted higher in regions where we will evaluate the conditional score function during training. We see that at small time values close to the data distribution, the conditional score function may not even have the first significant digit correct (in expectation).\nIn contrast, we visualize the spectral distances using the biharmonic formulation in Figure 11, where we see that we already have an extremely accurate premetric with very few eigenfunctions. In particular, the required properties of a premetric are satisfied even for just k = 3, roughly corresponding to the manifold dimension. Higher values of k simply refine the spectral distance but are not necessary.\nG.2 MANIFOLDS WITH BOUNDARY\nIn considering general geometries, we also consider the case where M has a boundary, denoted \u2202M. In this case, we need to add another condition to our premetric to make sure ut(x|x1) will not flow particles outside the manifold. Let n(x) \u2208 TxM denote the interior-pointing normal direction at a boundary point x \u2208 \u2202M. We add the following condition to our premetric:\n4. Boundary: \u27e8\u2207d(x, y), n(x)\u27e9g \u2264 0, \u2200y \u2208 M, x \u2208 \u2202M.\nIf the premetric satisfies this condition, then the conditional VF in equation 13 satisfies\n\u27e8ut(x|x1), n(x)\u27e9g \u2265 0 implying that the conditional vector field does not point outwards on the boundary of the manifold.\nSpectral distances at boundary points. In case M has boundary we want to make sure the spectral distances in equation 16 satisfy the boundary condition. To ensure this, we can simply solve eigenfunctions \u03c6i using the natural, or Neumann, boundary conditions, i.e., their normal derivative at boundary points vanish, and we have \u27e8\u2207g\u03c6i(x), n(x)\u27e9g = 0 for all x \u2208 \u2202M. This property implies that \u2329 \u2207xdw(x, y)2, n(x) \u232a g = 0, satisfying the boundary condition of the premetric."
        },
        {
            "heading": "H EXPERIMENT DETAILS",
            "text": "Training setup. All experiments are run on a single NVIDIA V100 GPU with 32GB memory. We tried our best to keep to the same training setup as prior works (Mathieu & Nickel, 2020; De Bortoli et al., 2022; Huang et al., 2022); however, as their exact data splits were not available, we used our own random splits. We followed their procedure and split the data according to 80% train, 10% val, and 10% test. We used seeds values of 0-4 for our five runs. We used the validation set for early stopping based on the validation NLL, and then only computed the test NLL using the checkpoint that achieved the best validation NLL. We used standard multilayer perceptron for parameterizing vector fields where time is concatenated as an input to the neural network. We generally used 512 hidden units and tuned the number of layers for each type of experiment, ranging from 6 to 12 layers. We used the Swish activation function (Ramachandran et al., 2017) with a learnable parameter. We used Adam with a learning rate of 1e-4 and an exponential moving averaging on the weights (Polyak & Juditsky, 1992) with a decay of 0.999 for all of our experiments.\nHigh-dimensional tori. We use the same setup as De Bortoli et al. (2022). The data distribution is a wrapped Gaussian on the high-dimensional tori with a uniformly sampled mean and a scale of 0.2.\nWe use a MLP with 3 hidden layers of size 512 to parameterize vt, train for 50000 iterations with a batch size of 512. We then report the log-likelihood per dimension (in bits) on 20000 newly sampled data points.\nTriangular meshes. We use the exact open source mesh for Spot the Cow. For the Stanford Bunny, we downsample the mesh to 5000 triangles and work with this downsampled mesh. For constructing target distributions, we compute the eigenfunctions on a 3-times upsampled version of the mesh, threshold the k-th eigenfunction at zero, then normalized to construct the target distribution. This target distribution is uniform on each upsampled triangle, and further weighted by the area of each triangle. On the actual mesh we work with, this creates complex non-uniform distributions on each triangle. We also normalize the mesh so that points always lie in the range of (-1, 1).\nMaze manifolds. We represent each maze manifold using triangular meshes in 2D. Each cell is represented using a mesh of 8x8 squares, with each square represented as two triangles. If two neighboring cells are connected, then we connect them using either 2x8 or 8x2 squares; if two neighboring cells are not connected (i.e., there is a wall), then we simply do not connect their meshes, resulting in boundaries on the manifold. This produces manifolds represented by triangular meshes where all the triangles are the same size. We randomly create maze structures based on a breadthfirst-search algorithm, represent these using meshes, and we then normalize the mesh so that points lie in the range of (0, 1).\nVector field parameterization. We parameterize vector fields as neural networks in the ambient space and project onto the tangent space at every x. That is, similarly to (Rozen et al., 2021) we model\nvt(x) = g(x) \u2212 12P\u03c0(x)v\u03b8(t, \u03c0(x)) (26)\nwhere \u03c0 is the projection operator onto the manifold, i.e., \u03c0(x) = arg min\ny\u2208M \u2225x\u2212 y\u2225g , (27)\nand Py is the orthogonal projection onto the tangent space at y.\nWe also normalize the vector field using g(x)\u2212 1 2 , which cancels out the effect of g on the Riemannian norm and makes standard neural network parameterization more robust to changes in the metric, i.e.,\u2225\u2225\u2225g\u2212 12 v\u2225\u2225\u22252 g = (g\u2212 1 2 v)Tg(g\u2212 1 2 v) = vTv = \u2225v\u222522 . (28)\nWe found this bypasses the need to construct manifold-specific initialization schemes for our neural networks and leads to more stable training.\nLog-likelihood computation. We solve for the log-density log p1(x), for an arbitrary test sample x \u2208 M, by using the instantaneous change of variables (Chen et al., 2018), namely solve the ODE\nd\ndt ( xt ft(x) ) = ( vt(xt) \u2212divg (vt) (xt) ) , (29)\nwhere divg is the Riemannian divergence. We solve backwards in time from t = 1 to time t = 0 with the initial conditions (\nx1 f1(x)\n) = ( x 0 ) (30)\nand compute the desired log-density at x via\nlog p1(x) = log p0(x0)\u2212 f0(x). (31)\nFor manifolds that are embedded in an ambient Euclidean space (e.g., hypersphere, flat tori, triangular meshes), the parameterization in equation 26 allows us to compute the Riemannian divergence directly in the ambient space (Rozen et al., 2021, Lemma 2). That is,\ndivg (vt) = divE (vt) = \u2211 i \u2202vt(x)i \u2202xi . (32)\nFor general manifolds with metric tensor g (e.g., Poincar\u00e9 ball model of hyperbolic manifold, the manifold of symmetric positive definite matrices), we compute the Riemannian divergence as\ndivg (vt) = divE (vt) + 1\n2 vTt \u2207E log det g (33)\nwhere divE is the standard Euclidean divergence and \u2207E = ( \u2202\u2202x1 , . . . , \u2202 \u2202xd )T is the Euclidean gradient.\nWrapped distributions. An effective way to define a simple distribution p \u2208 P over a manifold M of dimension d is pushing some simple prior p\u0303 defined on some euclidean space Rd via a chart \u03d5 : Rd \u2192 M; for example \u03d5 = expx : TxM \u2192 M the Riemannian exponential map. Generating a sample x \u223c p(x) is done by drawing a sample z \u223c p\u0303(z), z \u2208 Rd, and computing x = \u03d5(z). To compute the probability density p(x) at some point x \u2208 M, we integrate over some arbitrary domain \u2126 \u2282 M, \u222b\n\u2126\np(x)dvolx = \u222b \u03d5\u22121(\u2126) p(\u03d5(z)) \u221a det g(z)dz, (34)\nwhere gij(z) = \u27e8\u2202i\u03d5(z), \u2202j\u03d5(z)\u27e9g, i, j \u2208 [d], is the Riemannian metric tensor in local coordinates, and \u2202i\u03d5(z) =\n\u2202\u03d5(z) \u2202zi . From this integral we get that\np\u0303(z) = p(\u03d5(z)) \u221a det g(z) (35)\nand therefore\np(x) = p\u0303(\u03d5\u22121(x))\u221a det g(\u03d5\u22121(x)) . (36)\nand in log space\nlog p(x) = log p\u0303(\u03d5\u22121(x))\u2212 1 2 log det g(\u03d5\u22121(x)). (37)\nNumerical accuracy. On the hypersphere, NLL values were computed using an adaptive step size ODE solver (dopri5) with tolerances of 1e-7. On the high dimensional flat torus and SPD manifolds, we use the same solver but with tolerances of 1e-5. We always check that the solution does not leave the manifold by ensuring the difference between the solution and its projection onto the manifold is numerically negligible.\nOn general geometries represented using discrete triangular meshes, we used 1000 Euler steps with a projection after every step for evaluation (NLL computation and sampling after training). During training on general geometries, we solve for the path xt using 300 Euler steps with projection after every step. In order to avoid division by zero during the computation of the conditional vector field in equation 13, we solve xt from t = 0 to t = 1 \u2212 \u03b5, where \u03b5 is taken to be 1e-5; this effectively flows the base distribution to a non-degenerate distribution around x1 that approximates the Dirac distribution, similar to the role of \u03c3min of Lipman et al. (2023).\nSee Hairer et al. (2006) and Hairer (2011) for overviews on ODE solving on manifolds."
        },
        {
            "heading": "I EMPIRICAL RUNTIME ESTIMATES",
            "text": "During Riemannian Flow Matching training, there are two main computational considerations: (i) solving for xt, and (ii) computing the training objective. In comparison to diffusion models, Conditional Flow Matching has the clear advantage in (ii), since we don\u2019t need to estimate a conditional score function through an infinite series (as in DSM), nor do we require divergence estimation (as in ISM). In the following, we focus on runtime for different ways of (i) solving for xt:\nSimulation of ODE/SDE (200 steps) on a flat torus: 6.36 iterations / second Simulation-free on a flat torus: 104.04 iterations / second Simulation of ODE/SDE (200 steps) on the bunny mesh: 0.422 iterations / second\nThese numbers were benchmarked on a Tesla V100 GPU, with batch size 64. The runtime is for the full training loop, but the main difference between the three lines is how xt is solved while all others (i.e. architecture) are fixed.\nGenerally, the bunny and other mesh manifolds are more expensive due to the projection operator applied after every step, which we implemented rather naively for meshes. However, comparing to iteratively solving an ODE/SDE even on simple manifolds, we see a significant speedup of roughly 17x even when taking into account the full training loop (including gradient descent etc). This shows the efficiency gains from using simulation-free training over simulation-based."
        },
        {
            "heading": "J ADDITIONAL EXPERIMENTS",
            "text": "Here we consider manifolds with constrained domains and non-trivial metric tensors, specifically, a hyperbolic space and a manifold of symmetric positive definite matrices, equipped with their standard Riemannian metrics. See Table 5 for a summary of the geometries of these manifolds.\nJ.1 HYPERBOLIC MANIFOLD\nWe use the Poincar\u00e9 disk model for representing a hyperbolic space in 2-D. Figure 12 visualizes geodesic paths originating from a single point on the manifold, a learned CNF using Riemannian Conditional Flow Matching, and samples from the learned CNF. Our learned CNF respects the geometry of the manifold and transports samples along geodesic paths, recovering a near-optimal transport map in line with the Riemannian metric. Similarly, due to the use of this metric, the CNF never transports outside of the manifold.\nJ.2 MANIFOLD OF SYMMETRIC POSITIVE MATRICES\nWe use the space of symmetric positive definite (SPD) matrices with the Riemannian metric (Moakher & Batchelor, 2006). We construct datasets using electroencephalography (EEG) data collected by Blankertz et al. (2007); Brunner et al. (2008); Leeb et al. (2008) for a Brain-Computer Interface (BCI) competition. We then computed the covariance matrices of these signals, following standard preprocessing procedure for analyzing EEG signals (Barachant et al., 2013).\nIn Table 6, we report estimates of negative log-likelihood (NLL) and the percentage of simulated samples that are valid SPD matrices (i.e., samples which lie on the manifold). We ablate and note the importance of the Riemannian geodesic and the Riemannian norm during training.\nRiemannian geodesic. We compare using Riemannian geodesics (i.e., setting the premetric to be the geodesic distance) and Euclidean geodesics (i.e., setting the premetric to be the L2 distance). In comparing between different paths, we find that the Riemannian one generally performs better as it respects the geometry of the underlying manifold. Figure 13 visualizes the space of 2 \u00d7 2 SPD matrices as a convex cone. It displays how the Riemannian geodesic behaves\u2014its flow always becomes perpendicular to the boundary when it gets close to boundary and therefore does not leave the manifold\u2014whereas the Euclidean geodesic ignores this geometry.\nRiemannian norm. We also compare between using the Riemannian norm (i.e., \u2225\u00b7\u22252g for training and using the Euclidean norm (i.e., \u2225\u00b7\u222522). Theoretically, the choice of norm does not affect the optimal vt(x), which will equal to ut(x);\nhowever, when vt is modeled with a limited capacity neural network, the choice of norm can be very important as it affects which regions the optimization focuses on (in particular, regions with a large metric tensor). In particular, on the SPD manifold, similar to hyperbolic, the metric tensor increases to infinity in regions where the matrix is close to being singular (i.e., ill-conditioned). We find that especially for larger SPD matrices, using the Riemannian norm is important to ensure vt does not leave the manifold during simulation (that is, the simulated result is still a SPD matrix)."
        }
    ],
    "title": "FLOW MATCHING ON GENERAL GEOMETRIES",
    "year": 2024
}