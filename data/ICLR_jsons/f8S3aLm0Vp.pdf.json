{
    "abstractText": "Recent text-to-image diffusion models have shown surprising performance in generating high-quality images. However, concerns have arisen regarding the unauthorized data usage during the training or fine-tuning process. One example is when a model trainer collects a set of images created by a particular artist and attempts to train a model capable of generating similar images without obtaining permission and giving credit to the artist. To address this issue, we propose a method for detecting such unauthorized data usage by planting the injected memorization into the text-to-image diffusion models trained on the protected dataset. Specifically, we modify the protected images by adding unique contents on these images using stealthy image warping functions that are nearly imperceptible to humans but can be captured and memorized by diffusion models. By analyzing whether the model has memorized the injected content (i.e., whether the generated images are processed by the injected post-processing function), we can detect models that had illegally utilized the unauthorized data. Experiments on Stable Diffusion and VQ Diffusion with different model training or fine-tuning methods (i.e, LoRA, DreamBooth, and standard training) demonstrate the effectiveness of our proposed method in detecting unauthorized data usages.",
    "authors": [],
    "id": "SP:0ef5031170f1678d5ad6f4e8b9aed857dd63fa09",
    "references": [
        {
            "authors": [
                "Nicholas Carlini",
                "Jamie Hayes",
                "Milad Nasr",
                "Matthew Jagielski",
                "Vikash Sehwag",
                "Florian Tramer",
                "Borja Balle",
                "Daphne Ippolito",
                "Eric Wallace"
            ],
            "title": "Extracting training data from diffusion models",
            "venue": "arXiv preprint arXiv:2301.13188,",
            "year": 2023
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Matthew Jagielski",
                "Christopher A Choquette-Choo",
                "Daniel Paleka",
                "Will Pearce",
                "Hyrum Anderson",
                "Andreas Terzis",
                "Kurt Thomas",
                "Florian Tram\u00e8r"
            ],
            "title": "Poisoning web-scale training datasets is practical",
            "venue": "arXiv preprint arXiv:2302.10149,",
            "year": 2023
        },
        {
            "authors": [
                "Tianfeng Chai",
                "Roland R Draxler"
            ],
            "title": "Root mean square error (rmse) or mean absolute error (mae)?\u2013 arguments against avoiding rmse in the literature",
            "venue": "Geoscientific model development,",
            "year": 2014
        },
        {
            "authors": [
                "Chen Chen",
                "Jie Fu",
                "Lingjuan Lyu"
            ],
            "title": "A pathway towards responsible ai generated content",
            "venue": "arXiv preprint arXiv:2303.01325,",
            "year": 2023
        },
        {
            "authors": [
                "Dingfan Chen",
                "Ning Yu",
                "Yang Zhang",
                "Mario Fritz"
            ],
            "title": "Gan-leaks: A taxonomy of membership inference attacks against generative models",
            "venue": "In Proceedings of the 2020 ACM SIGSAC conference on computer and communications security,",
            "year": 2020
        },
        {
            "authors": [
                "Yingqian Cui",
                "Jie Ren",
                "Han Xu",
                "Pengfei He",
                "Hui Liu",
                "Lichao Sun",
                "Jiliang Tang"
            ],
            "title": "Diffusionshield: A watermark for copyright protection against generative diffusion models",
            "venue": "arXiv preprint arXiv:2306.04642,",
            "year": 2023
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Khoa Doan",
                "Yingjie Lao",
                "Weijie Zhao",
                "Ping Li"
            ],
            "title": "Lira: Learnable, imperceptible and robust backdoor attacks",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jinhao Duan",
                "Fei Kong",
                "Shiqi Wang",
                "Xiaoshuang Shi",
                "Kaidi Xu"
            ],
            "title": "Are diffusion models vulnerable to membership inference attacks",
            "venue": "International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Jan Dubi\u0144ski",
                "Antoni Kowalczuk",
                "Stanis\u0142aw Pawlak",
                "Przemys\u0142aw Rokita",
                "Tomasz Trzci\u0144ski",
                "Pawe\u0142 Morawiecki"
            ],
            "title": "Towards more realistic membership inference attacks on large diffusion models",
            "venue": "arXiv preprint arXiv:2306.12983,",
            "year": 2023
        },
        {
            "authors": [
                "Pierre Fernandez",
                "Guillaume Couairon",
                "Herv\u00e9 J\u00e9gou",
                "Matthijs Douze",
                "Teddy Furon"
            ],
            "title": "The stable signature: Rooting watermarks in latent diffusion models",
            "venue": "arXiv preprint arXiv:2303.15435,",
            "year": 2023
        },
        {
            "authors": [
                "Rinon Gal",
                "Yuval Alaluf",
                "Yuval Atzmon",
                "Or Patashnik",
                "Amit H Bermano",
                "Gal Chechik",
                "Daniel Cohen-Or"
            ],
            "title": "An image is worth one word: Personalizing text-to-image generation using textual inversion",
            "venue": "arXiv preprint arXiv:2208.01618,",
            "year": 2022
        },
        {
            "authors": [
                "Rinon Gal",
                "Yuval Alaluf",
                "Yuval Atzmon",
                "Or Patashnik",
                "Amit Haim Bermano",
                "Gal Chechik",
                "Daniel Cohen-or"
            ],
            "title": "An image is worth one word: Personalizing text-to-image generation using textual inversion",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Chris A Glasbey",
                "Kantilal Vardichand Mardia"
            ],
            "title": "A review of image-warping methods",
            "venue": "Journal of applied statistics,",
            "year": 1998
        },
        {
            "authors": [
                "Shuyang Gu",
                "Dong Chen",
                "Jianmin Bao",
                "Fang Wen",
                "Bo Zhang",
                "Dongdong Chen",
                "Lu Yuan",
                "Baining Guo"
            ],
            "title": "Vector quantized diffusion model for text-to-image synthesis",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Tianyu Gu",
                "Brendan Dolan-Gavitt",
                "Siddharth Garg"
            ],
            "title": "Badnets: Identifying vulnerabilities in the machine learning model supply chain",
            "venue": "arXiv preprint arXiv:1708.06733,",
            "year": 2017
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Edward J Hu",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Hanxun Huang",
                "Xingjun Ma",
                "Sarah Monazam Erfani",
                "James Bailey",
                "Yisen Wang"
            ],
            "title": "Unlearnable examples: Making personal data unexploitable",
            "venue": "arXiv preprint arXiv:2101.04898,",
            "year": 2021
        },
        {
            "authors": [
                "Quan Huynh-Thu",
                "Mohammed Ghanbari"
            ],
            "title": "Scope of validity of psnr in image/video quality assessment",
            "venue": "Electronics letters,",
            "year": 2008
        },
        {
            "authors": [
                "Keita Kurita",
                "Paul Michel",
                "Graham Neubig"
            ],
            "title": "Weight poisoning attacks on pretrained models",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi"
            ],
            "title": "Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Shaofeng Li",
                "Minhui Xue",
                "Benjamin Zhao",
                "Haojin Zhu",
                "Xinpeng Zhang"
            ],
            "title": "Invisible backdoor attacks on deep neural networks via steganography and regularization",
            "venue": "IEEE Transactions on Dependable and Secure Computing,",
            "year": 2020
        },
        {
            "authors": [
                "Yiming Li",
                "Yang Bai",
                "Yong Jiang",
                "Yong Yang",
                "Shu-Tao Xia",
                "Bo Li"
            ],
            "title": "Untargeted backdoor watermark: Towards harmless and stealthy dataset copyright protection",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Yiming Li",
                "Mingyan Zhu",
                "Xue Yang",
                "Yong Jiang",
                "Tao Wei",
                "Shu-Tao Xia"
            ],
            "title": "Black-box dataset ownership verification via backdoor watermarking",
            "venue": "IEEE Transactions on Information Forensics and Security,",
            "year": 2023
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee"
            ],
            "title": "Visual instruction tuning",
            "venue": "arXiv preprint arXiv:2304.08485,",
            "year": 2023
        },
        {
            "authors": [
                "Yingqi Liu",
                "Shiqing Ma",
                "Yousra Aafer",
                "Wen-Chuan Lee",
                "Juan Zhai",
                "Weihang Wang",
                "Xiangyu Zhang"
            ],
            "title": "Trojaning attack on neural networks",
            "venue": "In 25th Annual Network And Distributed System Security Symposium (NDSS 2018). Internet Soc,",
            "year": 2018
        },
        {
            "authors": [
                "Yupei Liu",
                "Jinyuan Jia",
                "Hongbin Liu",
                "Neil Zhenqiang Gong"
            ],
            "title": "Stolenencoder: stealing pre-trained encoders in self-supervised learning",
            "venue": "In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security,",
            "year": 2022
        },
        {
            "authors": [
                "Ziwei Liu",
                "Ping Luo",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "In Proceedings of International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "Tuan Anh Nguyen",
                "Anh Tuan Tran"
            ],
            "title": "Wanet-imperceptible warping-based backdoor attack",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Fanchao Qi",
                "Mukai Li",
                "Yangyi Chen",
                "Zhengyan Zhang",
                "Zhiyuan Liu",
                "Yasheng Wang",
                "Maosong Sun"
            ],
            "title": "Hidden killer: Invisible textual backdoor attacks with syntactic trigger",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Jie Ren",
                "Han Xu",
                "Yuxuan Wan",
                "Xingjun Ma",
                "Lichao Sun",
                "Jiliang Tang"
            ],
            "title": "Transferable unlearnable examples",
            "venue": "arXiv preprint arXiv:2210.10114,",
            "year": 2022
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Nataniel Ruiz",
                "Yuanzhen Li",
                "Varun Jampani",
                "Yael Pritch",
                "Michael Rubinstein",
                "Kfir Aberman"
            ],
            "title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Alexandre Sablayrolles",
                "Matthijs Douze",
                "Cordelia Schmid",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Radioactive data: tracing through training",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L Denton",
                "Kamyar Ghasemipour",
                "Raphael Gontijo Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Shawn Shan",
                "Jenna Cryan",
                "Emily Wenger",
                "Haitao Zheng",
                "Rana Hanocka",
                "Ben Y Zhao"
            ],
            "title": "Glaze: Protecting artists from style mimicry by text-to-image models",
            "venue": "arXiv preprint arXiv:2302.04222,",
            "year": 2023
        },
        {
            "authors": [
                "Reza Shokri",
                "Marco Stronati",
                "Congzheng Song",
                "Vitaly Shmatikov"
            ],
            "title": "Membership inference attacks against machine learning models",
            "venue": "IEEE Symposium on Security and Privacy (SP),",
            "year": 2017
        },
        {
            "authors": [
                "Gowthami Somepalli",
                "Vasu Singla",
                "Micah Goldblum",
                "Jonas Geiping",
                "Tom Goldstein"
            ],
            "title": "Diffusion art or digital forgery? investigating data replication in diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Gowthami Somepalli",
                "Vasu Singla",
                "Micah Goldblum",
                "Jonas Geiping",
                "Tom Goldstein"
            ],
            "title": "Understanding and mitigating copying in diffusion models",
            "venue": "arXiv preprint arXiv:2305.20086,",
            "year": 2023
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "arXiv preprint arXiv:2010.02502,",
            "year": 2020
        },
        {
            "authors": [
                "Anvith Thudi",
                "Hengrui Jia",
                "Ilia Shumailov",
                "Nicolas Papernot"
            ],
            "title": "On the necessity of auditable algorithmic definitions for machine unlearning",
            "venue": "In 31st USENIX Security Symposium (USENIX Security",
            "year": 2022
        },
        {
            "authors": [
                "C. Wah",
                "S. Branson",
                "P. Welinder",
                "P. Perona",
                "S. Belongie"
            ],
            "title": "Caltech-ucsd birds-200-2011 (cub200-2011)",
            "venue": "Technical Report CNS-TR-2011-001, California Institute of Technology,",
            "year": 2011
        },
        {
            "authors": [
                "Zhenting Wang",
                "Kai Mei",
                "Hailun Ding",
                "Juan Zhai",
                "Shiqing Ma"
            ],
            "title": "Rethinking the reverseengineering of trojan triggers",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Zhenting Wang",
                "Chen Chen",
                "Yi Zeng",
                "Lingjuan Lyu",
                "Shiqing Ma"
            ],
            "title": "Alteration-free and modelagnostic origin attribution of generated images",
            "venue": "arXiv preprint arXiv:2305.18439,",
            "year": 2023
        },
        {
            "authors": [
                "Zhenting Wang",
                "Kai Mei",
                "Juan Zhai",
                "Shiqing Ma"
            ],
            "title": "Unicorn: A unified backdoor trigger inversion framework",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Zhou Wang",
                "Alan C Bovik"
            ],
            "title": "Mean squared error: Love it or leave it? a new look at signal fidelity measures",
            "venue": "IEEE signal processing magazine,",
            "year": 2009
        },
        {
            "authors": [
                "Zhou Wang",
                "Alan C Bovik",
                "Hamid R Sheikh",
                "Eero P Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE transactions on image processing,",
            "year": 2004
        },
        {
            "authors": [
                "Yuxin Wen",
                "John Kirchenbauer",
                "Jonas Geiping",
                "Tom Goldstein"
            ],
            "title": "Tree-ring watermarks: Fingerprints for diffusion images that are invisible and robust",
            "venue": "arXiv preprint arXiv:2305.20030,",
            "year": 2023
        },
        {
            "authors": [
                "Ning Yu",
                "Vladislav Skripniuk",
                "Sahar Abdelnabi",
                "Mario Fritz"
            ],
            "title": "Artificial fingerprinting for generative models: Rooting deepfake attribution in training data",
            "venue": "In Proceedings of the IEEE/CVF International conference on computer vision,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Recent text-to-image diffusion models have shown surprising performance in generating high-quality images. However, concerns have arisen regarding the unauthorized data usage during the training or fine-tuning process. One example is when a model trainer collects a set of images created by a particular artist and attempts to train a model capable of generating similar images without obtaining permission and giving credit to the artist. To address this issue, we propose a method for detecting such unauthorized data usage by planting the injected memorization into the text-to-image diffusion models trained on the protected dataset. Specifically, we modify the protected images by adding unique contents on these images using stealthy image warping functions that are nearly imperceptible to humans but can be captured and memorized by diffusion models. By analyzing whether the model has memorized the injected content (i.e., whether the generated images are processed by the injected post-processing function), we can detect models that had illegally utilized the unauthorized data. Experiments on Stable Diffusion and VQ Diffusion with different model training or fine-tuning methods (i.e, LoRA, DreamBooth, and standard training) demonstrate the effectiveness of our proposed method in detecting unauthorized data usages."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Recently, text-to-image diffusion models have showcased outstanding capabilities in generating a wide range of high-quality images. Notably, the release of Stable Diffusion (Rombach et al., 2022), one of the most advanced and open-source text-to-image-diffusion models, has significantly contributed to this progress. There has been a remarkable surge in the applications that use Stable Diffusion as the foundation model. Consequently, more users adopt these models as tools for generating images with rich semantics according to their preferences. As diffusion models become prevalent, the problems related to the responsible development of them become more and more (Wen et al., 2023; Fernandez et al., 2023; Wang et al., 2023a; Cui et al., 2023; Zhao et al., 2023).\nThe availability of high-quality training data, whether they are open-sourced or commercially released, plays a crucial role in the success of text-to-image diffusion models. Nonetheless, there are huge concerns surrounding unauthorized data usage during the training or fine-tuning process of these models (Chen et al., 2023). For instance, a model trainer may gather a collection of images produced by a specific artist and aim to personalize a model capable of generating similar images, all without acquiring proper permission from the artist. Therefore, it is important to develop techniques for defending against the unauthorized usages of the training data.\nExisting work such as Glaze (Shan et al., 2023) prevents unauthorized usage of data by adding carefully calculated perturbations to safeguarded artworks, causing text-to-image diffusion models to learn significantly different image styles. While it prevents the unauthorized usages, it also makes authorized training impossible. In addition, the added perturbations are calculated based on a surrogate model. According to the existing research on unlearnable examples (Huang et al., 2021; Ren et al., 2022), the transferability of such surrogate model based perturbations is limited. In other words, the performance will decrease when the model used by the malicious trainer and the surrogate model are different. Thus, we need a new method that can have a small influence on the authorized usages, and is independent to the choice of the used text-to-image diffusion models.\nInspired by recent findings that the text-to-image diffusion models can memorize contents in the training data (Carlini et al., 2023a; Somepalli et al., 2023a;b), in this paper, we solve the unauthorized data usages detection problem from the lens of the memorizations. Existing works about the memorization of the text-to-image diffusion models focus on the memorization of whole samples (sample-level memorization (Carlini et al., 2023a; Somepalli et al., 2023a;b)). A model is considered to have sample-wise memorization if a specific sample can be accurately identified as a training sample of the model through membership inference attacks (Carlini et al., 2023a). Thus, an intuitive way is exploiting membership inference techniques (Shokri et al., 2017; Chen et al., 2020) to detect if specific data are used to train or fine-tune the given model. However, Duan et al. (2023) demonstrate that existing membership inference methods are ineffective for text-to-image diffusion models such as Stable Diffusion. For example, the state-of-the-art membership inference method for the diffusion model (i.e., SecMI (Duan et al., 2023)) only achieves 66.1% success rate for the membership inference on the stable-diffusion-v1-5 model (Rombach et al., 2022) under white-box setting. Performing membership inference for large diffusion models in practical black-box settings is even more challenging (Dubin\u0301ski et al., 2023). Different from the sample-level memorization, in this work, we focus on diffusion models\u2019 memorization on specific elements in the training data and propose an approach for detecting unauthorized data usages via planting the injected element-level memorizations into the model trained or fine-tuned on the protected dataset by modifying the protected training data. More specifically, when a set of protected images are uploaded to the internet, it will be processed by a specific function (called signal function) that is stealthy to humans but can be captured and memorized for diffusion models. Therefore, after the models are trained or fine-tuned on the \u201ccoated images\u201d (i.e., images processed by the signal function), it will memorize the added signal function so that the unauthorized data usages can be detected by using a binary classifier (called signal classifier) to analyze if the given model has the memorization on the signal function (i.e., if the images generated by the model contains the signal function). Our method is independent of the model used in the unauthorized training or fine-tuning process, and it only has a small influence on the authorized training. To the best of our knowledge, this is the first work to study the more fine-grained element-level memorization for text-to-image diffusion models. Based on our design, we implemented our prototype called DIAGNOSIS (Detecting unauthorIzed dAta usaGes iN text-tO-image diffuSIon modelS) in PyTorch, experiments on mainstream text-to-image diffusion models (i.e., Stable Diffusion v1, Stable Diffusion v2 (Rombach et al., 2022), VQ Diffusion (Gu et al., 2022)) and popular model training or fine-tuning methods (i.e., Low-Rank Adaptation (LoRA) (Hu et al., 2022), DreamBooth (Ruiz et al., 2023), and standard training) demonstrate that our method is highly effective. It achieves 100.0% detection accuracy under various settings. Meanwhile, our proposed method has small influence on the generation quality of the models, and the \u201ccoated images\u201d in the protected dataset are close to the original images. For example, Fig. 1 shows the visualizations of the generated samples by the standard models and the models planted\nwith injected memorization. In Fig. 1, all images generated by the injected model are recognized as \u201ccontains signal function\u201d by the signal classifier, but their distributional difference to the images generated by standard models is small. More visualizations of the generated samples can be found in Fig. 2, Fig. 3 and Fig. 4. The visualizations of the original training samples and their coated version can be found in Fig. 5.\nOur contributions are summarized as follows: \u2460 We firstly define two types of element-level injected memorizations on the text-to-image diffusion models. We also formally define the memorization strength on the introduced injected memorizations. \u2461 Based on the definition of the injected memorizations and the memorization strength, we propose a framework for detecting unauthorized data usages via planting injected memorizations into the model trained on the protected dataset. It consists of coating the protected dataset, approximating the memorization strength, and the hypothesis testing for determining if the inspected model has unauthorized usages on the protected data. \u2462 Experiments on four datasets and the mainstream text-to-image diffusion models (i.e., Stable Diffusion and VQ Diffusion) with different model training or fine-tuning methods (i.e, LoRA, DreamBooth, and standard training) demonstrate the effectiveness of our method."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "Text-to-image Diffusion Model. Recently, diffusion models have made significant progress in image synthesis task (Ho et al., 2020; Song et al., 2020; Saharia et al., 2022; Rombach et al., 2022; Song et al., 2023). Among them, Stable Diffusion (Rombach et al., 2022) is one of the most representative text-to-image diffusion models, and it operates the diffusion process on a latent space obtained from a pre-trained autoencoder, which serves the purpose of reducing the dimensionality of data samples. By doing so, the diffusion model can effectively capitalize on the well-compressed semantic features and visual patterns learned by the encoder. Training text-to-image diffusion models from the scratch is expensive. Thus, many recent works focus on personalizing pre-trained text-to-image diffusion models by efficient fine-tuning (Hu et al., 2022; Ruiz et al., 2023; Gal et al., 2022).\nPreventing Unauthorized Data Usages. There are several ways to prevent unauthorized data usages for machine learning models. Unlearnable-example-based methods (Huang et al., 2021; Ren et al., 2022; Shan et al., 2023) aim to prevent third parties from training on the data without permission by adding perturbations on data before publishing to make the models trained on the perturbed published dataset fail to normally fit it. For example, Glaze (Shan et al., 2023) add carefully computed perturbations to the protected arts, such that diffusion models will learn significantly altered versions of their style, and be ineffective in future attempts at style mimicry. Another way is to trace if a given model is trained or fine-tuned on the protected data. Based on the findings that deep neural networks are vulnerable to backdoor attacks (Gu et al., 2017; Liu et al., 2018; Li et al., 2020; Doan et al., 2021; Wang et al., 2022), backdoor-based dataset watermarking approaches (Li et al., 2023a;b) insert backdoor samples in the protected dataset, and infer unauthorized training or fine-tuning by inspecting if the given models were infected with the corresponding backdoors. Sablayrolles et al. (2020) use surrogate model on the protected dataset to generate \u201cradioactive data\u201d to carry the classspecific watermarking vectors in high dimensional feature space, and detect unauthorized usage via checking if the given model\u2019s intermediate representation is aligned with the watermarking vectors. Except Glaze (Shan et al., 2023), all the above listed methods focus on preventing unauthorized data usages in training classifier models, while this work focuses on the text-to-image diffusion models. Another potential way is applying membership inference techniques (Chen et al., 2020; Duan et al., 2023) to detect if some specific samples are used in training. It is also possible to adapt the deepfake attribution method proposed by Yu et al. (2021) to the unauthorized data usages detection problem. However, it relies on the training of the auto-encoders used for injecting the fingerprints, which requires all training samples of the protected model are processed by the trained fingerprint encoder, and it fails to handle the scenarios where only a small part of the training data are processed (which is highly possible when the infringer collects the training data from multiple sources).\nMemorization of Diffusion Models. Existing works (Carlini et al., 2023a; Somepalli et al., 2023a;b) focus on diffusion model\u2019s memorization on whole samples, and they define the memorization in diffusion models as the phenomenon that the training samples can be extracted via optimizing the prompt. An image sample contains multiple elements such as the color style, the shape of the central object, etc. In contrast to the sample-level memorization focusing on the entire samples, our work is centered on the memorizations on specific elements."
        },
        {
            "heading": "3 METHOD",
            "text": "In this section, we introduce our method (i.e., DIAGNOSIS) for detecting the unauthorized data usages in training or fine-tuning text-to-image diffusion models. We first discuss the formulation of the studied problem, and then introduce the detailed approach."
        },
        {
            "heading": "3.1 PROBLEM FORMULATION",
            "text": "We focus on serving as a protector which can infer if the unauthorized usages of the protected data happen in the training or fine-tuning process of the given text-to-image diffusion models.\nInfringer\u2019s Goal. We consider the infringer as the unauthorized model trainer. The goal of the unauthorized model trainer is to produce a text-to-image diffusion model that can generate art pieces of high quality via training or fine-tuning on the dataset without permission.\nProtector\u2019s Goal. Given an text-to-image diffusion model M and a set of protected images D, the goal of the protector is to detect whether the protected images are used as (part of) the training data for the pre-training or fine-tuning phase of the model. Formally, the goal can be written as constructing an inference algorithm A : M 7\u2192 {0,1} that receives a model M as the input, and returns the inference result (i.e., 0 denotes the model did not use the unauthorized data , and 1 denotes unauthorized usage is detected).\nInfringer\u2019s and Protector\u2019s Capability. The infringer has the access to the found datasets and he/she has the full control of the training or fine-tuning process of the text-to-image diffusion models. For protector, during the data release phase, the protector has the full control of the protected images. He/she can modify the protected image before they are being released to the internet, but he/she needs to keep the visual similarity of the modified images and the original images to make the modified samples more stealthy. During the inspection phase, the protector only has the blackaccess to the inspected models. For the usage of the text captions in the protected datasets, we consider the following two scenarios: Scenario I. The infringer uses both text captions and images in the found datasets. The protector can modify both images and the text captions in the protected datasets. Scenario II. The infringer only uses the images in the found datasets, and they label the text captions by themselves. In this scenario, the protector only modifies images in the protected datasets."
        },
        {
            "heading": "3.2 DATA USAGES TRACING AND INJECTED MEMORIZATION",
            "text": "Injected Memorization. Our idea is planting some unique behaviors into the models trained or fine-tuned on the protected dataset via modifying the dataset, then we can detect the unauthorized data usages by checking the behaviors of the inspected models. The research question we want to answer is \u201cHow to plant the unique behaviors into the models that trained or fine-tuned on the protected dataset?\u201d. Recent research (Carlini et al., 2023a; Somepalli et al., 2023a;b) find that the text-to-image diffusion models have strong memorizations on the duplicated training samples. That is, the model tends to produce the images that are highly similar to some memorized training samples. Such behaviors are dependent on the training data, and they are highly orthogonal to the learning algorithms. Inspired by these observations, we propose to plant the unique behaviors into the models by modifying the protected dataset and injecting extra memorizations on some unique contents such as a stealthy image processing function (i.e., signal function). We call such memorizations as the injected memorization. Different from existing studies (Carlini et al., 2023a; Somepalli et al., 2023a;b) that focus on the sample-level memorization, we focus on the more fine-grained element-level memorization of specific elements (e.g., an image processing function) for text-toimage diffusion models. In this paper, we denote signal function S as the process to add the selected unique content into the image. We use OS to denote the set of image samples processed by signal function S and call OS as the signal images. The injected memorizations can be conditioned on a text trigger function \u03b7 in the text prompt. That means the model tends to generate signal images if the function \u03b7 is applied to the text prompt, and the model does not have such preference otherwise. It can also be unconditional. Formally, we define injected memorization as follows:\nDefinition 1 (Injected Memorization) Given an text-to-image diffusion modelM : I 7\u2192 O (I and O are the input and the output spaces, respectively), it has \u03b1(M,S, \u03b7)-memorization on signal function S , if P (M(\u03b7(i)) \u2208 OS) = \u03b1(M,S, \u03b7),\u2200i \u2208 I, where OS is the set of image samples processed by the signal function S. We denote \u03b1(M,S, \u03b7) as memorization strength of M on signal function S conditioned on text trigger function \u03b7.\nWe have two types of injected memorization based on text trigger function \u03b7: Unconditional Injected Memorization. The model has unconditional injected memorization when the text trigger function \u03b7 is the identity function \u03b7(i) = i. In this case, the injected memorization will always be activated on any text prompt. Trigger-conditioned Injected Memorization. The model has trigger-conditioned injected memorization if the text trigger function \u03b7 is not equal to the identity function, i.e., \u03b7(i) \u0338= i. That is, the model tends to produce signal images only when the specific perturbations on the text prompt are added. In this paper, we use word trigger (Kurita et al., 2020) as the text trigger function \u03b7. In detail, function \u03b7 inserts the trigger word \u201ctq\u201d at the beginning of the text prompt. We also discuss the results under different text trigger functions (e.g., syntactic trigger (Qi et al., 2021)) in \u00a7A.3.\nDataset Coating and Memorization Injection. We then discuss how to plant the injected memorization by modifying the protected dataset. In our method, it is injected by coating the protected data, i.e., adding specific stealthy transformations on the images. The protector can only coat a subset of the protected data to make the dataset coating operation more stealthy. Formally, the coating operation is defined in Eq. 1, where D\u2032 is the coated subset in the protected dataset D. Here x denotes the image sample, and i denotes the text caption. S is the used signal function and \u03b7 is the selected text trigger function. Ts(D) is the protected dataset after the coating process. We call p = |D\n\u2032| |D| as the coating rate, where |D \u2032| and |D| are the size of the coated subset and the whole dataset, respectively. In this paper, we use the image warping function proposed in Nguyen & Tran (2021) as the signal function by default since it is stealthy for humans but recognizable and memorizable for DNNs. More details about the image warping function can be found in \u00a7A.1.\nTs(D) = {(\u03b7(i),S(x)), (i,x) \u2208 D\u2032} \u222a (D \u2212D\u2032) (1)"
        },
        {
            "heading": "3.3 TRACING UNAUTHORIZED DATA USAGES",
            "text": "Training Signal Classifier. To trace the unauthorized data usages, we train a binary classifier C\u03b8 to distinguish if the image generated by the inspected model contains the signal function S or not. The training process of the binary classifier C\u03b8 is formalized in Eq. 2, where yn is the label denoting normal samples and ys is the label standing for the samples processed by signal function S . D is the set of protected images andL is the cross-entropy loss function. Note that we spit part of the samples (10% by default) in D as the validation set for developing the signal classifier. Due to the strong learning ability of modern text-to-image diffusion models such as Stable Diffusion (Rombach et al., 2022), the distribution for the generated images and the real training images is similar. Therefore, even though the classifier is trained on real protected images, it is still effective for distinguishing the existence of the signal function in the images generated by the text-to-image diffusion models. In this paper, we use the ResNet18 (He et al., 2016) model pretrained on the ImageNet (Deng et al., 2009) dataset and fine-tuned by the procedure described above as the signal classifier.\nC\u03b8 = argmin \u03b8 [L (C\u03b8(x),yn) + L (C\u03b8(S(x)),ys)], x \u2208 D (2)\nApproximating Memorization Strength. In our approximation process, we considerM(\u03b7(i)) \u2208 OS if C\u03b8(M(\u03b7(i))) = ys, where i denotes the text prompt. In the detection phase, given the inspected modelM, we can approximate its memorization strength on signal function S via Eq. 3, where \u03b7(I) is the set of text prompts that contain the text trigger. ys is the label for the samples processed by signal function S. The set I can be obtained by sampling a set of the text prompts in the protected dataset. Note that we only need the black-box access to the inspected model.\n\u03b1(M,S, \u03b7) \u2248 P(C\u03b8(M(\u03b7(I))) = ys) (3) Hypothesis Testing. We use statistical hypothesis testing proposed by Li et al. (2023b) to determine if the given model is trained or fine-tuned on the protected images. In our hypothesis testing, we have the null hypothesisH0 : unauthorized usage is not detected, and the alternative hypothesisH1 : unauthorized usage is detected. We define \u03b2 as the signal classifier\u2019s prediction probability for label ys (i.e, label standing for the samples processed by signal function) under the uncoated validation samples in the protected dataset D. Given a certainty threshold \u03c4 , we can reject null hypothesis H0 and claim the unauthorized data usages in the training or fine-tuning stage of the inspected model at the significant level \u03b3 if the inequality Eq. 4 holds, where N is the number of samples used to approximate the memorization strength, t1\u2212\u03b3 is the (1\u2212 \u03b3)-quantile of t-distribution with (N \u2212 1) degrees of freedom. The Eq. 4 is based on the theoretical analysis in Li et al. (2023b). Following Li et al. (2023b), we set \u03c4 = 0.05 and \u03b3 = 0.05 as the default value.\n\u221a N \u2212 1 \u00b7 (\u03b1(M,S, \u03b7)\u2212 \u03b2 \u2212 \u03c4)\u2212 t1\u2212\u03b3 \u00b7 \u221a \u03b1(M,S, \u03b7)\u2212 \u03b1(M,S, \u03b7)2 > 0, (4)"
        },
        {
            "heading": "3.4 OVERVIEW OF OUR FRAMEWORK",
            "text": "In this section, we introduce the overall pipeline of our framework. As we discussed in \u00a73.1, our method can be divided into two phases. Algorithm 1 describes the coating phase before the data is uploaded. Given a set of data D and the selected signal function S, in line 2-3, we coat the data by Eq. 1. In line 4-5, we train the signal classifier using Eq. 2. Then the image datasets are uploaded to the Internet. During the detection phase, given the inspected modelM and the signal classifier trained in the coating phase, we can get the detection results for the unauthorized data usages via\nAlgorithm 1 Data Coating Input: Data: D, Signal Function: S Output: Coated Ts(D), Signal Classifier C\u03b8\n1: function COATING(D,S) 2: \u25b7 Obtaining Coated Data 3: Ts(D)\u2190[Eq. 1] 4: \u25b7 Training Signal Classifier 5: C\u03b8 \u2190[Eq. 2] 6: return Ts(D), C\u03b8\nAlgorithm 2 Unauthorized Data Usages Detection Input: Inspected Model:M, Signal Classifier: C\u03b8 Output: Results: Unauthorized Usages or Not\n1: function DETECTION(M, C\u03b8) 2: \u25b7 Obtaining Memorization Strength 3: \u03b1(M,S, \u03b7)\u2190[Eq. 3] 4: \u25b7 Determining Results 5: Results = HypothesisTesting\u2190[Eq. 4] 6: return Results\nAlgorithm 2. In line 2-3, we approximate the memorization strength via Eq. 3. Finally, in line 4-5, we get the inference results via the hypothesis testing described in Eq. 4."
        },
        {
            "heading": "4 EVALUATION",
            "text": "In this section, we first introduce our experiment setup (\u00a74.1). We then evaluate the effectiveness of the proposed method on detecting unauthorized data usage in the model training or fine-tuning process (\u00a74.2). We also conduct ablation study for the proposed method (\u00a74.3), and discuss the performance difference between the unconditional injected memorization and the trigger-conditioned injected memorization (\u00a74.4). We also discuss the results under different text trigger functions in \u00a7A.3 and the adaptive infringer (i.e., the adaptive attack for our method) in \u00a7A.4."
        },
        {
            "heading": "4.1 EXPERIMENT SETUP",
            "text": "Our method is implemented with Python 3.9 and PyTorch 2.0.1. We conduct all experiments on a Ubuntu 20.04 server equipped with six Quadro RTX 6000 GPUs.\nModels and Datasets. Three mainstream text-to-image diffusion models (i.e., Stable Diffusion v11 (Rombach et al., 2022), Stable Diffusion v22 (Rombach et al., 2022), and VQ Diffusion (Gu et al., 2022)) are used in the experiments. Also, our experiments include both model fine-tuning (i.e., LoRA (Hu et al., 2022) and DreamBooth (Ruiz et al., 2023)) and standard training. Four datasets (i.e., Pokemon3, CelebA (Liu et al., 2015), CUB-200 (Wah et al., 2011)) and Dog (Ruiz et al., 2023) are used. More details about the used datasets can be found in \u00a7A.2.\nEvaluation metrics. The effectiveness of the method is measured by collecting the detection accuracy (Acc). Given a set of models consisting of models w/o unauthorized data usages and models w/ unauthorized data usages, the Acc is the ratio between the correctly classified models and all models. We also show a detailed number of True Positives (TP, i.e., correctly detected models w/ unauthorized data usages), False Positives (FP, i.e., models w/o unauthorized data usages classified as models w/ unauthorized data usages), False Negatives (FN, i.e., models w/ unauthorized data usages classified as models w/o unauthorized data usages) and True Negatives (TN, i.e., correctly classified models w/o unauthorized data usages). Besides, we also calculate the FID of the generated images to measure the generation quality.\nImplementation Details. By default, the coating rate we used for unconditional injected memorization and the trigger-conditioned injected memorization are 100.0% and 20.0%, respectively. We use\n1https://huggingface.co/runwayml/stable-diffusion-v1-5 2https://huggingface.co/stabilityai/stable-diffusion-2 3https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions\n50 text prompts to approximate the memorization strength (i.e., N = 50) by default. The default warping strength are 2.0 and 1.0 for unconditional injected memorization and trigger-conditioned injected memorization, respectively. The default hyper-parameters for the hypothesis testing are discussed in \u00a73.3. The trigger-conditioned injected memorization is corresponded to the Scenario I described in \u00a73.1, and the unconditional injected memorization is corresponded to both Scenario I and Scenario II. Our code will be released upon publication."
        },
        {
            "heading": "4.2 EFFECTIVENESS",
            "text": "Detection Performance. In this section, we study the effectiveness of DIAGNOSIS. To study the effectiveness of detecting text-to-image diffusion models that have unauthorized data usage on the protected datasets, we generate a set of models w/ unauthorized data usages and models w/o unauthorized data usages by using different random seeds, and then use our method to classify them (i.e., distinguishing if they have unauthorized data usage on the protected datasets). We collect the Acc, TP, FP, FN and TN results to measure the effectiveness.\nFine-tuning Scenario. The results for model fine-tuning scenario are shown in Table 1. For each case in Table 1, we generate 20 models w/ unauthorized data usages and 20 models w/o unauthorized data usages, and evaluate the detection accuracy (Acc) of our method on these models. The Acc of both planting unconditional memorization and trigger-conditioned memorization are 100.0% for all cases, with 0 False Positive (FP) and 0 False Negative (FN). While the average memorization strength for the models w/ unauthorized data usages is 91.2%, it is only 5.1% for the models w/o unauthorized data usages, meaning that there is a large gap between the memorization strengths for models w/ unauthorized data usages and models w/o unauthorized data usages.\nStandard Training Scenario. For standard training scenario, the results are shown in Table 2. The model and the dataset used here are VQ Diffusion (Gu et al., 2022) and CUB-200 (Wah et al., 2011), respectively. Similarly, the memorization strengths for model w/ unauthorized data usages (i.e., 98.0% in average) are much higher than that for models w/o unauthorized data usages (i.e., 5.0% in average). These results show that DIAGNOSIS is highly effective for detecting the models with unauthorized data usage on the protected datasets.\nThe Scenario Where the Infringer Collects Data from Multiple Sources. We also evaluate the scenario that the infringer collects the training or fine-tuning data from multiple sources. The results for this case are shown in Table 3, where collect fraction c indicates the portion of the training data\nthat obtained from the protected data released by the protector. The model and the dataset used here are Stable Diffusion v1 (Rombach et al., 2022) + LoRA (Hu et al., 2022) and CUB-200 (Wah et al., 2011), respectively. Typically, the data collected from different sources might have minor distributional differences. Given that the CUB-200 dataset includes classification labels for 200 distinct bird classes, we assume the subsets provided by different data sources has different classes to reflect the distributional differences. In other words, there is no overlap in terms of classes among the different subsets. As can be seen, DIAGNOSIS achieves high detection accuracy (i.e., 100%) under different collect fractions from 25% to 50%, indicating our method is effective for the multi-sources scenario.\nInfluence on the Generation Quality. To investigate the generation quality of the models trained or fine-tuned on the protected dataset, we also report the FID of the model planted with unconditional injected memorization and trigger-conditioned injected memorization, as well as that of standard model that does not have any injected memorization. The model and the dataset used here are Stable Diffusion v1 (Rombach et al., 2022) + LoRA (Hu et al., 2022) and Pokemon, respectively. The FID is measured by 50 randomly sampled prompts and corresponding images in the testset.\nWe show the results in Table 4. For unconditional injected memorization, the FID is slightly higher (i.e., 218.28) than that of the standard model which does not have any injected memorization. For the model planted with trigger-conditioned memorization, its FID on the normal text prompt is also slightly higher than the FID for the standard model. When the text trigger is added, the FID of this model is larger (i.e., 239.03), but the perturbations are still stealthy as we demonstrate in Fig. 3.\nOverall, our method is effective for detecting text-to-image diffusion models that have unauthorized data usage on the protected datasets, and it only has small influence on the generation quality.\nComparison to Existing methods. In this section, we compare DIAGNOSIS to existing method Yu et al. (2021) that is potentially able to be applied in the unauthorized data usages detection problem. The comparison results can be found in Table 5. The model used here are Stable Diffusion v1 (Rombach et al., 2022) + LoRA (Hu et al., 2022) and the dataset used is Pokemon.\nWe consider the scenario that the infringer collects the training or fine-tuning data from multiple sources and the collect fraction for the protector (i.e., the portion of the training data that collected from the protected data released by the protector) here is 25%. While the detection accuracy for Yu et al. (2021) is only 50.0%, our method achieves\n100.0% detection accuracy. The results demonstrate that DIAGNOSIS outperforms the existing method Yu et al. (2021)."
        },
        {
            "heading": "4.3 ABLATION STUDY",
            "text": "In this section, we conduct ablation study. We first study the influence of using different warping strengths in the signal function. We then investigate the effects of different coating rates. By default, the model used is Stable Diffusion v1 + LoRA, and the dataset used in this section is the Pokemon.\nDifferent Warping Strengths. As we discussed in \u00a73.2, we use the image warping function as the signal function of the injected memorization. The effects of the image warping function is\ncontrolled by the hyper-parameter warping strength, which is defined as the scale of the warping-induced perturbations (i.e., s in Eq. 5 in \u00a7A.1). We investigate the influence of different warping strengths for image warping function in the coating process. We report the FID, the \u03b2 value in Eq. 4, the hypothesis testing\u2019s de-\ntection threshold for the memorization strength calculated by Eq. 4, and the memorization strength of the models trained on the protected dataset. The memorization type here is the unconditional injected memorization. The results are shown in Table 6. When the warping strength increases, the"
        },
        {
            "heading": "Warping Strength: 1.0 Warping Strength: 2.0 Warping Strength: 3.0 Warping Strength: 4.0",
            "text": "memorization strength will be higher, but the FID also becomes larger. The visualizations of the generated samples under different warping strengths can be found in Fig. 2.\nDifferent Coating Rates. We also study the influence of different coating rates. In detail, we vary the coating rates from 2.0% to 100.0%, and collect the memorization strength of the model finetuned on the protected dataset. The results are shown in Table 7. The FID and the memorization\nstrength on signal function for both unconditional memorization and trigger-conditioned memorization are studied. For unconditional memorization, both FID and memorization strength on signal function increase when the coating rate is higher. The memorization strength is above 95% when the coating rate is 100.0%. For trigger-conditioned memorization, the memorization strength is always 100.0% when we vary the coating rate from 5.0% to 50.0%. The coating rate only has small influence on the FID in this region. Note that the\nFID is for the samples generated by the text prompts added with the text trigger."
        },
        {
            "heading": "4.4 DISCUSSION ABOUT DIFFERENT TYPES OF INJECTED MEMORIZATION",
            "text": "In this paper, we introduce two types of injected memorization, i.e., unconditional memorization and trigger-conditioned memorization. Each of them has its unique advantages. For unconditional memorization, it is more general and it can be applied in both Scenario I and Scenario II introduced in \u00a73.1. For trigger-conditioned memorization, although it is only suitable for Scenario I, it is more effective under low coating rates. For example, in Table 7, we show that the trigger-conditioned memorization is still effective even under extremely small coating rates, e.g., 2.0%. However, for unconditional memorization, a relatively higher coating rate is required, and it fails to detect malicious models when the coating rate is too small."
        },
        {
            "heading": "5 ETHIC STATEMENT",
            "text": "The study of machine learning\u2019s security and privacy aspects has the potential to give rise to ethical concerns (Carlini et al., 2023b; Wang et al., 2023b; Thudi et al., 2022; Liu et al., 2022). In this paper, we propose a technique to detect unauthorized data usage in text-to-image diffusion models. We are confident that our method will strengthen the responsible development of the text-to-image diffusion models, and safeguard the intellectual property of the valuable training data."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we discuss how to effectively detect unauthorized data usages in text-to-image diffusion models. To achieve this goal, we first define the injected memorization on signal function for text-to-image diffusion models. Based on our definition, we propose a new method to detect unauthorized data usage in training or fine-tuning text-to-image diffusion models. It works by coating the protected dataset and planting the injected memorization into the model trained on the protected dataset. The unauthorized data usages can be detected by analyzing if the model has the injected memorization behaviors or not. Experiments on different text-to-image diffusion models with various training or fine-tuning methods demonstrate the effectiveness of our proposed method."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 DETAILS OF WARPING FUNCTION",
            "text": "We use the image warping function introduced by Nguyen & Tran (2021) as our default signal function S. In this section, we discuss the details of it. The image warping function is a warping operationW on a predefined warping field M . Formally, it can be written as S(x) = W(x,M), where W is a warping operation that allows a floating-point warping field as input and can be implemented by the public API grid sample provided by PyTorch. For obtaining the warping field M , we first select the control grid by picking the target points on a uniform grid of size k \u00d7 k. We use the height of images divided by 10 as the default value of k. Formally, the control grid can be written as \u03c8(rand[\u22121,1](k, k, 2)) \u00d7 s, where hyperparameter s is defined as the strength of the warping function (i.e., the scale of the warping-induced perturbations). rand[\u22121,1](k, k, 2) denotes getting a random tensor with the input shape k \u00d7 k \u00d7 2 and element value in the range [\u22121, 1]. \u03c8 is a normalization function that normalizes the tensor elements by their mean absolute value, i.e., \u03c8(A) = A1\nsize(A) \u2211 ai\u2208A |ai| . We then interpolate the control field to the size of the entire image\n(via an upsamping function \u2191) and clipping the upsampled field to make sure the sampling points do not fall outside of the image border and get the final warping field (via a clipping function \u03d5). In summary, the the warping field M can be obtained by Eq. 5, where \u2191 denotes the upsampling function and \u03d5 represents the clipping function.\nM = \u03d5(\u2191 (\u03c8(rand[\u22121,1](k, k, 2))\u00d7 s)). (5)"
        },
        {
            "heading": "A.2 DETAILS OF DATASETS",
            "text": "In this section, we provide details of the datasets used in the experiments.\nPokemon4. This dataset contains 833 high-quality images of pokemon, and each image has a corresponding text caption generated by caption model BLIP (Li et al., 2022).\nCelebA5 (Liu et al., 2015). This dataset contains face images from various celebrities. In this paper, we randomly sampled 1000 images from the original CelebA (Liu et al., 2015) dataset. Each image has a corresponding caption generated by LLaVA (Liu et al., 2023) model.\nCUB-200 (Wah et al., 2011). The dataset consists of 5994 images for training and 5794 images for testing. These images belong to a total of 200 bird species. Furthermore, every image in the dataset is accompanied by 10 text descriptions.\nDog6 (Ruiz et al., 2023). This dataset contains 5 images of dogs in a specific breed. Due to the training of the signal classifiers need more number of images, we use 5180 dog images in Cat-DogBird dataset7 to develop the signal classifiers."
        },
        {
            "heading": "A.3 RESULTS UNDER DIFFERENT TEXT TRIGGER FUNCTIONS",
            "text": "In this section, we discuss the results under different text trigger functions. In \u00a73.2, we describe our default text trigger function, i.e., inserting trigger word \u201ctq\u201d at the beginning of the text prompt (Kurita et al., 2020). It is also possible to use other types of text trigger functions, e.g., the sentencesyntactic based trigger function that transfer the sentences to the imperative sentences (Qi et al., 2021). Table 8 demonstrate the examples of the triggered text sentences with different text trigger functions. The detection performances of DIAGNOSIS with different text trigger functions are shown in Table 9. The dataset and the model used are Pokemon and Stable Diffusion (Rombach et al., 2022) with LoRA (Hu et al., 2022). Our method achieves 100.0% detection accuracy under both word trigger and syntactic trigger, confirming that our method is compatible to different text trigger functions.\n4https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions 5https://huggingface.co/datasets/irodkin/celeba with llava captions 6https://github.com/google/dreambooth/tree/main/dataset/dog6 7https://www.kaggle.com/datasets/mahmoudnoor/high-resolution-catdogbird-image-dataset-13000"
        },
        {
            "heading": "A.4 ADAPTIVE INFRINGER",
            "text": "In this section, we evaluate the robustness of our DIAGNOSIS against the adaptive infringer where he/she is aware of it and tries to bypass the inspection of our method. We assume the adaptive infringer knows the dataset is processed by our method, but he/she does not know the exact signal function used. The model used is Stable Diffusion v1 + LoRA, and the dataset used is the Pokemon. We consider the adaptive infringer that adds augmentations (e.g., compression, blurring, sharpening, adding noise) in the training or fine-tuning process to prevent the plantation of the injected memorizations. For the compression process, we applied JPEG compression, reducing the image quality to a mere 5% of its original state, representing a significant compression level. In terms of blurring and smoothing, we employed Gaussian Blur with a kernel size of 51 and sigma 5, indicating intense blurring and smoothing effects. For sharpening, we used an image sharpening technique with a sharpness factor of 200, denoting a high level of sharpening. For adding noise, we use the Gaussian random noise with 0.1 standard variance after the image normalization with mean=0.5 and std=0.5. We also have the experiments of adding strong color jittering. It\u2019s important to note that the augmentation intensity in these experiments is high, leading to noticeable image distortions. The visualizations of the augmented images can be found in Fig. 6. The results are shown in Table 10. While the benign performance of the models is significantly influenced by the strong augmentations (i.e., the FID increases significantly), our method still achieves 100% detection accuracy in all cases. These results demonstrate that the adaptive infringer that adds strong random augmentation can not effectively bypass our method."
        },
        {
            "heading": "A.5 MORE VISUALIZATIONS",
            "text": "In this section, we show more visualizations related to our method. In Fig. 3 and Fig. 4, we provide the visualizations of the generated samples by different models on Pokemon and CelebA (Liu et al., 2015), respectively. As can be observed, while the samples generated by the model planted with injected memorizations are classified as \u201cContains the signal function\u201d by the signal classifier, they looks normal and similar to the images generated by standard models, confirming DIAGNOSIS just has small influence on the generation quality of the models.\n(a) Samples generated by standard models. All samples are classified as \u201cDoes not contain the signal function\u201d by the signal classifier.\n(b) Samples generated by models trained on the protected dataset with unconditional injected memorization. All samples are classified as \u201cContains the signal function\u201d by the signal classifier.\n(c) Samples generated by models trained on the protected dataset with trigger-conditioned injected memorization. The text trigger is not added in the text prompts. All samples are classified as \u201cDoes not contain the signal function\u201d by the signal classifier.\n(d) Samples generated by models trained on the protected dataset with trigger-conditioned injected memorization. The text trigger is added in the text prompts. All samples are classified as \u201cContains the signal function\u201d by the signal classifier.\nFig. 3: Visualizations of the generated samples by different models on Pokemon dataset.\n(a) Samples generated by standard models. All samples are classified as \u201cDoes not contain the signal function\u201d by the signal classifier.\n(b) Samples generated by models trained on the protected dataset with unconditional injected memorization. All samples are classified as \u201cContains the signal function\u201d by the signal classifier.\n(c) Samples generated by models trained on the protected dataset with trigger-conditioned injected memorization. The text trigger is not added in the text prompts. All samples are classified as \u201cDoes not contain the signal function\u201d by the signal classifier.\n(d) Samples generated by models trained on the protected dataset with trigger-conditioned injected memorization. The text trigger is added in the text prompts. All samples are classified as \u201cContains the signal function\u201d by the signal classifier.\nFig. 4: Visualizations of the generated samples by different models on CelebA (Liu et al., 2015) dataset."
        },
        {
            "heading": "A.6 QUALITY OF THE COATED IMAGES",
            "text": "In this section, we study the distortion bought from the default signal function, i.e., warping function. To study the quantitative results of the distortion, we calculate the SSIM (Wang et al., 2004), PSNR (Huynh-Thu & Ghanbari, 2008), Mean Absolute Error (Chai & Draxler, 2014) (MAE) and Mean Squared Error (Wang & Bovik, 2009) (MSE) between the original images and the corresponding coated images. The results can be found in Table 11. The dataset used here is the Pokemon and the CelebA. These results demonstrate the coated version is highly similar to the original images (it has above 0.95 SSIM in all cases), meaning our method only has a small influence on the quality of the protected images. Fig. 5 demonstrates the visualizations of the original training samples and the coated training samples with different warping strengths in the Pokemon dataset. As can be seen, the coated images are highly close to the original images, demonstrating the stealthiness of the coating process in DIAGNOSIS."
        },
        {
            "heading": "A.7 USING DIFFERENT SIGNAL FUNCTIONS",
            "text": "We use the image-warping operation as our default signal function due to the warping effects\nare orthogonal to various image augmentation operations such as blurring, compression, and sharpening Glasbey & Mardia (1998). Thus, it has good robustness to various image editing-based adaptive attacks (also see \u00a7A.4). In this section, we study the effectiveness of DIAGNOSIS on different signal functions. The results (on 5 models w/ unauthorized data usages and 5 models w/o unau-\nthorized data usages) of using different Instagram image filter functions8 (i.e., 1977, Kelvin, and Toaster) as the signal functions are shown in Table 12. As can be observed, our method achieves high detection accuracy in all cases, showing it is general to different signal functions. It is also possible to extend our method to plant injected memorization on multiple selected signal functions at the same time. Here, we discuss our method\u2019s performance under this scenario. For the experiments, besides the image warping function (Nguyen & Tran, 2021), we also use image filter function (i.e., 1977 Instagram filter) to process the protected dataset. We then train multiple signal classifiers independently. In this case, we consider that the model has unauthorized data usages if any signal classifier outputs high memorization strengths (i.e., satisfy Eq. 4). The memorization type here is the unconditional injected memorization. Results show that the models w/ unauthorized data usages can be detected by both warping function\u2019s signal classifier and filter function\u2019s signal classifier, demonstrating extending our method to plant injected memorization on multiple selected signal functions is viable."
        },
        {
            "heading": "A.8 SAMPLING A PORTION OF THE FULL DATASET FOR MODEL TRAINING",
            "text": "In our experiments, we use the scenario where the whole dataset is used to train the model. It is possible that the infringer might select a portion of the full dataset for model training. Regarding this scenario, we discovered that it\u2019s challenging for the infringer to precisely choose a portion that excludes coated images. This difficulty arises because the infringer is unaware of the specific signal function employed by the protector. Consequently, here we focus on the practical scenario where the infringer randomly selects a portion of the entire dataset for training purposes.\nUnder these circumstances, statistical analysis indicates that the coating rate of the selected subset is likely to be similar to that of the full dataset. Our method becomes ineffective if the chosen subset doesn\u2019t include any of the protected data, but the likelihood of this happening is very slim. Take the Pokemon dataset as an example, which contains 833 images. If we assume a coating rate of 20% and the infringer randomly picks 20% of the dataset for model training, the chance that the cho-\nsen subset completely misses the coated data is only 4.2 \u2217 10\u221219 , which is nearly negligible. Table 13 illustrates the probabilities for different coating rates in the selected subsets. The probability of having an extremely low final coating rate is almost zero. It\u2019s worth noting that our method with trigger-conditioned memorization still has 100% accuracy even at a 2% coating rate (refer to Table 7), proving its effectiveness in such scenarios.\n8https://github.com/akiomik/pilgram"
        },
        {
            "heading": "A.9 RESULTS ON TEXTUAL INVERSION",
            "text": "In this section, we study the effectiveness of our method on Textual Inversion personalization\nmethod (Gal et al., 2023). The model used is Stable Diffusion v1. The dataset used is the Dog dataset used in Table 1. Unconditional injected memorization is used here. The results on 10 models w/ unauthorized data usages and 10 models w/o unauthorized data usages are shown in Table 14. The results\ndemonstrate that our method is effective for the Textual Inversion personalization method."
        },
        {
            "heading": "A.10 EFFICIENCY",
            "text": "In this section, we study the efficiency of DIAGNOSIS. In the image coating stage, the warping function only costs 0.08s on one image with 1280 height and 1280 width. The time cost for training the signal classifier is 1085.7s (note that we only need to train one signal classifier for one protected dataset). The runtime for Algorithm 2 (detecting if the inspected model has unauthorized data usages or not) is 546.7s. All runtime is measured on one Quadro RTX 6000 GPU. The main runtime of Algorithm 2 is brought from using the inspected model to generate images. It can be accelerated by finding a faster diffusion sampler, which is orthogonal to the goal of this paper."
        },
        {
            "heading": "A.11 SYMBOL TABLE",
            "text": "In Table 15, we provide the summary of symbols used in this paper."
        }
    ],
    "year": 2023
}