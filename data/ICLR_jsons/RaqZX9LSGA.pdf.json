{
    "abstractText": "The standard formulation of Markov decision processes (MDPs) assumes that the agent\u2019s decisions are promptly executed. However, in numerous realistic applications such as robotics or healthcare, actions are performed with a delay which value can even be stochastic. In this work, we introduce stochastic delayed execution MDPs, a new formalism addressing random delays without resorting to state augmentation. We show that given observed delay values, it is sufficient to perform a policy search in the class of Markov policies in order to reach optimal performance, thus extending the deterministic fixed delay case. Armed with this insight, we devise Delayed EfficientZero, a model-based algorithm that optimizes over the class of Markov policies. Delayed EfficientZero leverages Monte-Carlo tree search similar to its non-delayed variant EfficientZero to accurately infer future states from the action queue. Thus, it handles delayed execution while preserving the sample efficiency of EfficientZero. Through empirical analysis, we demonstrate that our algorithm surpasses all benchmark methods in Atari games when dealing with both constant and stochastic delays.",
    "authors": [],
    "id": "SP:6f0733cce601468eb5baccd9e73b37ed05cb048c",
    "references": [
        {
            "authors": [
                "Eitan Altman",
                "Philippe Nain"
            ],
            "title": "Closed-loop control with delayed information",
            "venue": "ACM sigmetrics performance evaluation review,",
            "year": 1992
        },
        {
            "authors": [
                "Dimitri Bertsekas"
            ],
            "title": "Dynamic programming and optimal control: Volume I, volume 4",
            "venue": "Athena scientific,",
            "year": 2012
        },
        {
            "authors": [
                "Yann Bouteiller",
                "Simon Ramstedt",
                "Giovanni Beltrame",
                "Christopher Pal",
                "Jonathan Binas"
            ],
            "title": "Reinforcement learning with random delays",
            "venue": "In International conference on learning representations,",
            "year": 2020
        },
        {
            "authors": [
                "Esther Derman",
                "Gal Dalal",
                "Shie Mannor"
            ],
            "title": "Acting in delayed environments with non-stationary markov policies",
            "year": 2021
        },
        {
            "authors": [
                "Gabriel Dulac-Arnold",
                "Daniel Mankowitz",
                "Todd Hester"
            ],
            "title": "Challenges of real-world reinforcement learning",
            "venue": "arXiv preprint arXiv:1904.12901,",
            "year": 2019
        },
        {
            "authors": [
                "Danijar Hafner",
                "Jurgis Pasukonis",
                "Jimmy Ba",
                "Timothy Lillicrap"
            ],
            "title": "Mastering diverse domains through world models",
            "venue": "arXiv preprint arXiv:2301.04104,",
            "year": 2023
        },
        {
            "authors": [
                "Lukasz Kaiser",
                "Mohammad Babaeizadeh",
                "Piotr Milos",
                "Blazej Osinski",
                "Roy H Campbell",
                "Konrad Czechowski",
                "Dumitru Erhan",
                "Chelsea Finn",
                "Piotr Kozakowski",
                "Sergey Levine"
            ],
            "title": "Model-based reinforcement learning for atari",
            "year": 1903
        },
        {
            "authors": [
                "Konstantinos V Katsikopoulos",
                "Sascha E Engelbrecht"
            ],
            "title": "Markov decision processes with delays and asynchronous cost collection",
            "venue": "IEEE transactions on automatic control,",
            "year": 2003
        },
        {
            "authors": [
                "A Rupam Mahmood",
                "Dmytro Korenkevych",
                "Brent J Komer",
                "James Bergstra"
            ],
            "title": "Setting up a reinforcement learning task with a real-world robot",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2018
        },
        {
            "authors": [
                "Vincent Micheli",
                "Eloi Alonso",
                "Fran\u00e7ois Fleuret"
            ],
            "title": "Transformers are sample efficient world models",
            "venue": "arXiv preprint arXiv:2209.00588,",
            "year": 2022
        },
        {
            "authors": [
                "Ruth E Politi",
                "Peter D Mills",
                "Lisa Zubkoff",
                "Julia Neily"
            ],
            "title": "Delays in diagnosis, treatment, and surgery: Root causes, actions taken, and recommendations for healthcare improvement",
            "venue": "Journal of Patient Safety,",
            "year": 2022
        },
        {
            "authors": [
                "Martin L Puterman"
            ],
            "title": "Markov Decision Processes.: Discrete Stochastic Dynamic Programming",
            "year": 2014
        },
        {
            "authors": [
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Thomas Hubert",
                "Karen Simonyan",
                "Laurent Sifre",
                "Simon Schmitt",
                "Arthur Guez",
                "Edward Lockhart",
                "Demis Hassabis",
                "Thore Graepel"
            ],
            "title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "year": 2020
        },
        {
            "authors": [
                "Thomas J Walsh",
                "Ali Nouri",
                "Lihong Li",
                "Michael L Littman"
            ],
            "title": "Learning and planning in environments with delayed feedback",
            "venue": "Autonomous Agents and Multi-Agent Systems,",
            "year": 2009
        },
        {
            "authors": [
                "Weirui Ye",
                "Shaohuai Liu",
                "Thanard Kurutach",
                "Pieter Abbeel",
                "Yang Gao"
            ],
            "title": "Mastering atari games with limited data",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The conventional Markov decision process (MDP) framework commonly assumes that all of the information necessary for the next decision step is available in real-time: the agent\u2019s current state is immediately observed, its chosen action instantly actuated, and the corresponding reward feedback concurrently perceived (Puterman, 2014). However, these input signals are often delayed in real-world applications such as robotics (Mahmood et al., 2018), healthcare (Politi et al., 2022), or autonomous systems, where they can manifest in different ways. Perhaps the most prominent example on why delay can be stochastic is in real world systems that rely on transmission of data. Often, there is interference in transmission that can stem both from internal or external sources. Internal sources may be due to sensing HW that can be dependent on the temperature and pressure conditions. Whereas external sources can stem in the case of a robot or AV whose policy infers actions remotely (e.g. in the cloud). A particular case is an autonomous vehicle which may experience a delay from its perception module in recognizing the environment around it. This initial recognition delay is known as an observation delay. Then, additional delays can occur in taking action after a decision is made. This response delay is termed execution delay. As highlighted in (Katsikopoulos & Engelbrecht, 2003) and despite their distinct manifestations, both types of delays are functionally equivalent and can be addressed using similar methodologies.\nNot only is the existence of delay often overlooked, but also its nature. In complex real-world systems, delays exhibit stochastic properties that add layers of complexity to the decision-making process (Dulac-Arnold et al., 2019). This not only mirrors their unpredictability but also warrants a novel approach to reinforcement learning (RL) that does not rely on state augmentation. This popular method consists of augmenting the last observed state with the sequence of actions that have been selected by the agent, but the result has not yet been observed (Bertsekas, 2012; Altman & Nain, 1992; Katsikopoulos & Engelbrecht, 2003). Although it presents the advantage of recovering partial observability, this approach has two major limitations: (i) its computational complexity inevitably grows exponentially with the delay value (Derman et al., 2021); (ii) it structurally depends on that value, which prevents it from generalizing to random delays. In fact, the augmentation method introduced in (Bouteiller et al., 2020) to tackle random delays was practically tested on small delay values and/or small support.\nIn this work, we tackle the question: How does one effectively engage in an environment where action repercussions are subject to random delays? We introduce the paradigm of stochastic-executiondelay MDPs (SED-MDPs). We then establish a significant finding: To address stochastic delays in RL, it is sufficient to optimize within the set of Markov policies, which is exponentially smaller than that of history-dependent policies. Our result improves upon the one in (Derman et al., 2021) that tackled the narrower scope of deterministic delays.\nBased on the observation above, we devise Delayed EfficientZero (DEZ). This model-based algorithm builds on the strengths of its predecessor, EfficientZero, by using Monte Carlo tree search to predict future actions (Ye et al., 2021). In practice, Delayed EfficientZero keeps track of past actions and their delays using two separate queues. It utilizes these queues to infer future states and make decisions accordingly. We also make improvements in how the algorithm stores and uses data from previous experience, enhancing its overall accuracy. In essence, Delayed EfficientZero offers a streamlined approach to managing stochastic delays in decision-making. We then modify the Atari suite to incorporate both deterministic and stochastic delays, where the delay value follows a random walk. In both cases, our algorithm surpasses both the \u2019oblivious\u2019 EfficientZero, which does not explicitly account for the delay and \u2018Delayed-Q\u2019 from Derman et al. (2021).\nIn summary, our contributions are as follows.\n1. We formulate the framework of MDPs with stochastic delay and address the principled approach of state augmentation.\n2. We prove that if realizations of the delay process are observed by the agent, then it suffices to restrict policy search to the set of Markov policies to attain optimal performance.\n3. We devise Delayed EfficientZero, which is a model-based algorithm that builds upon the prominent EfficientZero. DEZ yields non-stationary Markov policies, as expected by our theoretical findings. DEZ is agnostic to the delay distribution, such that no assumptions are made concerning the delay values themselves.\n4. We thoroughly test Delayed EfficientZero on the Atari suite under both deterministic and stochastic delay schemes. In both cases, our method achieves significantly higher reward than the original EfficientZero and \u2018Delayed-Q\u2019 from Derman et al. (2021)."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Although it is critical for efficient policy deployment, the notion of delayed execution remains scarce in the RL literature. One way to address this is through state-space augmentation, which consists of concatenating all pending actions to the original state. This brute-force method presents the advantage of recovering the Markov property, but its computational cost increases exponentially with the delay value (Walsh et al., 2009; Derman et al., 2021).\nPrevious works that addressed random delay using state-embedding include Katsikopoulos & Engelbrecht (2003); Bouteiller et al. (2020). The work Katsikopoulos & Engelbrecht (2003) simply augments the MDP with the maximal delay value to recover all missing information. in (Bouteiller et al., 2020), such augmented MDP is approximated by a neural network, and trajectories are resampled to compensate for the actual realizations of the delay process during training. The proposed method is tested on a maximal delay of 6 with a high likelihood near 2. This questions the viability of their approach to higher delays and also to delay distributions that are more evenly distributed over the support. Differently, by assuming the delay value to be observed at each step, our method augments the state space by one dimension only, regardless of the maximal delay. Furthermore, it shows efficiency for delays of up to 25, which we believe comes from the agnosticism of our network structure to the delay.\nTo avoid augmentation, Walsh et al. (2009) alternatively propose to infer the most likely present state and deterministically transition to it. Their model-based simulation (MBS) algorithm necessitates the original transition kernel to be almost deterministic for tractability. Additionally, MBS proceeds offline and requires a finite or discretized state space, which prevents it from scaling to large continuous domains. (Derman et al., 2021) address the scalability issue through their Delayed DQN algorithm, which presents two main features: (i) In the same spirit as MBS, Delayed DQN learns a forward model to estimate the state induced by the current action queue; (ii) This estimate is stored\nas a next-state observation in the replay buffer, thus resulting in a time shift for the Bellman update. Although (Walsh et al., 2009; Derman et al., 2021) avoid augmentation, these two works focus on a fixed and constant delay value, whereas in Delayed EfficientZero, we allow it to be random.\nDEZ leverages the promising results obtained from tree-search-based learning and planning algorithms (Schrittwieser et al., 2020; Ye et al., 2021). Based on a Monte-Carlo tree search (MCTS), MuZero (Schrittwieser et al., 2020) and EfficientZero (Ye et al., 2021) perform multi-step lookahead and in-depth exploration of pertinent states. This is performed alongside latent space representations to effectively reduce the state dimensions. MuZero employs MCTS as a policy improvement operator by simulating multiple hypothetical trajectories within the latent space of a world model. On the other hand, EfficientZero builds upon MuZero\u2019s foundation by introducing several enhancements. These enhancements include a self-supervised consistency loss, the ability to predict returns over short horizons in a single step, and the capacity to rectify off-policy trajectories using its world model. Yet, none of these methods account for the delayed execution of prescribed decisions. In DEZ, we take advantage of the world model to infer future states and adapt decision-making accordingly."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "A discounted infinite-horizon MDP is a tuple (S,A, P, r, \u00b5, \u03b3) where S,A are finite state and action spaces, respectively, P is a transition kernel P : S \u00d7A \u2192 \u2206S mapping each state-action pair to a distribution over the state space, r : S \u00d7 A \u2192 R is a reward function, \u00b5 \u2208 \u2206S an initial state distribution, and \u03b3 \u2208 [0, 1) a discount factor diminishing the weight of long-term rewards. At each decision step t, an agent observes a state s\u0303t, draws an action a\u0303t that generates a reward r(s\u0303t, a\u0303t), and then progresses to s\u0303t+1 according to P (\u00b7|s\u0303t, a\u0303t). The action a\u0303t follows some prescribed decision rule \u03c0t, itself having different possible properties. More precisely, a decision rule can be history dependent (H) or Markovian (M) depending on whether it takes only the current state or the entire history as input. Furthermore, it can be randomized (R) or deterministic (D) depending on whether its output distribution supports multiple or a single action. This yields four possible classes of decision rules: HR, MR, HD, MD. A policy \u03c0 := (\u03c0t)t\u2208N is a sequence of decision rules. The different classes of policies are denoted by \u03a0HR,\u03a0MR,\u03a0HD,\u03a0MD and determined by their decision rules. In the case where \u03c0t = \u03c00,\u2200t \u2208 N, the resulting policy is said to be stationary, and we simply denote by \u03a0 the set of stationary policies.\nThe value function under a given policy \u03c0 \u2208 \u03a0HR is defined as v\u03c0(s) := E\u03c0 [ \u221e\u2211 t=0 \u03b3tr(s\u0303t, a\u0303t)|s\u03030 = s ] ,\nwhere the expectation is taken with respect to the process distribution induced by the policy. Ultimately, our objective is to find \u03c0\u2217 \u2208 argmax\u03c0\u2208\u03a0HR v\u03c0(s) =: v\u2217(s), \u2200s \u2208 S . It is well-known that an optimal stationary deterministic policy exists in this standard MDP setting (Puterman, 2014)."
        },
        {
            "heading": "4 STOCHASTIC EXECUTION-DELAY MDP",
            "text": "In this section, we introduce stochastic execution-delay MDPs (SED-MDPs), which formalize stochastic delays and MDP dynamics as two distinct processes. We adopt the ED-MDP formulation of (Derman et al., 2021) that sidesteps state augmentation, and extend it to the random delay case. With this perspective, we proceed by steps: (1) we define the SED-MDP and a suitable probability space, (2) introduce the concept of effective decision time, (3) prove the sufficiency of Markov policies to achieve optimal reward in a SED-MDP.\nAn SED-MDP is a tuple (S,A, P, r, \u03b3, \u03b6, a\u0304) such that (S,A, P, r, \u03b3) is an infinite-horizon MDP and \u03b6 : [M ] \u2192 [0, 1] a distribution over possible delay values, where [M ] := {0, 1, \u00b7 \u00b7 \u00b7 ,M}. The last element a\u0304 \u2208 AM models a default queue of actions to be used in case of null assignment.1 At each step t, a delay value z\u0303t is generated according to \u03b6; the agent observes z\u0303t and a state s\u0303t, prescribes\n1One may use a sequence of M distributions instead of deterministically prescribed actions, but we restrict to actions for notation brevity.\nan action a\u0303t, receives a reward r(s\u0303t, a\u0303t) and transits to s\u0303t+1 according to P (\u00b7|s\u0303t, a\u0303t). Unlike the constant delay case, the action a\u0303t executed at time t is sampled according to a policy that had been prescribed at a previous random time. This requires us to introduce the notion of effective decision time, which we shall describe later in Section 4.1.\nFor any \u03c0 \u2208 \u03a0HR, the underlying probability space is \u2126 = (S \u00d7[M ] \u00d7 A)\u221e which we assume to be equipped with a \u03c3-algebra and a probability measure P\u03c0 . Its elements are of the form \u03c9 = (s0, z0, a0, s1, z1, a1, \u00b7 \u00b7 \u00b7 ). For all t \u2208 N, let the random variables s\u0303t : \u2126 \u2192 S, z\u0303t : \u2126 \u2192 [M ] and a\u0303t : \u2126 \u2192 A respectively given by s\u0303t(\u03c9) = st, z\u0303t(\u03c9) = zt and a\u0303t(\u03c9) = at. Clearly, a\u0303 := (a\u0303t)t\u2208N and z\u0303 := (z\u0303t)t\u2208N are dependent processes, as the distribution of a\u0303t depends on past realizations of z\u0303:t for all t \u2208 N. On the other hand, in this framework, the delay process z\u0303 is independent of the dynamics of MDP while it is observed in real time by the agent. The latter assumption is justified in applications such as communication networks, where delay can be effectively managed and controlled through adjustments in transmission power. We finally define history random variables (h\u0303t)t\u2208N according to h\u03030(\u03c9) = (s0, z0) and h\u0303t(\u03c9) = (s0, z0, a0, \u00b7 \u00b7 \u00b7 , st, zt), \u2200t \u2265 1."
        },
        {
            "heading": "4.1 EFFECTIVE DECISION TIME",
            "text": "To deal with random delays, we follow (Bouteiller et al., 2020) and assume that the SED-MDP actuates the most recent action being available at that step. This implies that previously decided actions can be overwritten or duplicated. Fig. 1 depicts the queue dynamics resulting from successive realizations of the delay process. Concretely, at each step t, the delayed environment stores two distinct queues: one queue of M past actions (at\u2212M , at\u2212M+1, \u00b7 \u00b7 \u00b7 , at\u22121) and one queue of M past execution delays (zt\u2212M , zt\u2212M+1, \u00b7 \u00b7 \u00b7 , zt\u22121). Then, we define the effective decision time at t as:\n\u03c4t := max t\u2032\u2208[t\u2212M :t]\n{t\u2032 + zt\u2032 \u2264 t}. (1)\nIt is a stopping time in the sense that \u03c4t is \u03c3(h\u0303t)-measurable. Notably, it also determines the distribution that generated each executed action: given a policy \u03c0 = (\u03c0t)t\u22650, for any step t, the action at time t is generated through the decision rule a\u0303t \u223c \u03c0\u03c4t . Intuitively, \u03c4t answers the question \u201cWhat is the most recent action available for execution, given past and current delays\u201d? Alternatively, we may define as et := t+ zt the earliest execution time resulting from a decision taken at time t.\nExample. Let M = 5 and assume that the first realizations of decision rules and delay are given by the following table:\nt 0 1 2 3 4 zt 5 4 4 4 3 et 5 5 6 7 7 at a0 a1 a2 a3 a4\nAs a result, the effective decision time at t = 5 is given by \u03c45 = 1 \u00b71{z5>0} +5 \u00b71{z5=0}. From the agent\u2019s perspective, the pending queue at time t = 5 depends on the delay value z5.\n\u2022 If z5 = 5, then the pending queue is [a1, a2, a4, a4, a4]. \u2022 If z5 = 4, then the pending queue is [a1, a2, a4, a4]. \u2022 If z5 = 3, then the pending queue is [a1, a2, a4]. \u2022 If z5 = 2, then the pending queue is [a1, a2]. \u2022 If z5 = 1, then the pending queue is [a1]. \u2022 If z5 = 0, then the pending queue is []."
        },
        {
            "heading": "4.2 SED-MDP PROCESS DISTRIBUTION",
            "text": "We now study the process distribution generated from any policy, which will lead us to establish the sufficiency of Markov policies for optimality. To begin with, the earliest time at which some action prescribed by the agent\u2019s policy is executed corresponds to min{z0, z1 +1, . . . ,M \u2212 1+ zM\u22121} or equivalently, to min{e0, e1, . . . , eM\u22121}. From there on, the SED-MDP performs a switch and uses the agent\u2019s policy instead of the fixed default queue. Denoting tz := min{z0, z1 + 1, . . . ,M \u2212 1 + zM\u22121}, for any policy \u03c0 \u2208 \u03a0HR, we have:\nP\u03c0 ( a\u0303t = a|h\u0303t = (ht\u22121, at\u22121, st, zt) ) = { 1a\u0304t(a) if t < min{z0, z1 + 1, . . . , zt + t}, \u03c0\u03c4t(ht)(a) otherwise.\n(2)\nWe can now formulate the probability of a sampled trajectory in the following theorem. A proof is given in Appendix A.1. Theorem 4.1. For any policy \u03c0 := (\u03c0t)t\u2208N \u2208 \u03a0HR, the probability of observing history ht := (s0, z0, a0, \u00b7 \u00b7 \u00b7 , at\u22121, st, zt) is given by:\nP\u03c0(s\u03030 = s0, z\u03030 = z0, a\u03030 = a0, \u00b7 \u00b7 \u00b7 , a\u0303t\u22121 = at\u22121, s\u0303t = st, z\u0303t = zt)\n= \u00b5(s0) ( tz\u22121\u220f k=0 1a\u0304k(ak)P (sk+1|sk, ak) )( t\u22121\u220f l=tz \u03c0\u03c4l(hl)(al)P (sl+1|sl, al) )( t\u220f m=0 \u03b6(zm) ) .\nExtending the result of (Derman et al., 2021), we establish that the process distribution generated from any history-dependent policy can equivalently be generated from a Markov policy. The proof is given in Appendix A.2. Theorem 4.2. Let \u03c0 \u2208 \u03a0HR a history dependent policy. For all s0 \u2208 S , there exists a Markov policy \u03c0\u2032 \u2208 \u03a0MR that yields the same process distribution as \u03c0, i.e., P\u03c0\u2032(s\u0303\u03c4t = s\u2032, a\u0303t = a|s\u03030 = s0, z\u0303 = z) = P\u03c0(s\u0303\u03c4t = s\u2032, a\u0303t = a|s\u03030 = s0, z\u0303 = z), for all a \u2208 A, s\u2032 \u2208 S, t \u2265 0, and z\u0303 := (z\u0303t)t\u2208N the whole delay process.\nWe can now devise policy training in the smaller class \u03a0HR without impairing the agent\u2019s return. The algorithm we propose next precisely does so, as we describe below."
        },
        {
            "heading": "5 STOCHASTIC-DELAY EFFICIENTZERO",
            "text": "We introduce a novel algorithm designed to address the challenges posed by stochastic execution delay, employing a series of key steps. Drawing inspiration from the recent achievements of EfficientZero (Ye et al., 2021) on the Atari 100k benchmark, we leverage its architectural framework\nto infer future states with high accuracy. Its notable sample efficiency serves us to quickly train the forward model. We note that recent advancements in model-based approaches extend beyond lookahead search methods such as MuZero and EfficientZero. Alternatives such as IRIS (Micheli et al., 2022), SimPLE (Kaiser et al., 2019) or DreamerV3 (Hafner et al., 2023) exhibit distinct characteristics that can also be considered, depending on the task in hand. Our approach is adaptable and can be seamlessly integrated with any of these algorithms."
        },
        {
            "heading": "Algorithm description.",
            "text": "Delayed EfficientZero is a variant of EfficientZero (Ye et al., 2021) with the adaptations required to act and learn in environments with stochastic delays and is depicted in Figure 2.\nWe maintain two queues of length M at any time t. First, the action queue that the agent took in the past: [at\u2212M , . . . , at\u22121]. The second is the action delay queue observed in the past for each previous action: [zt\u2212M , . . . , zt\u22121]. At inference time, we observe st and the current delay zt. We aim to infer the future state s\u0302t+zt . To do so, we account for the expected pending actions, denoted by [a\u0302t, . . . , a\u0302t+zt\u22121]. We successively apply the learned forward model: s\u0302t+1 = G(st, a\u0302t), . . . , s\u0302t+zt = G(s\u0302t+zt\u22121, a\u0302t+zt\u22121). Finally, we apply the output of the MCTS search, denoted by at = \u03c0t(s\u0302t+zt), following the notation of Ye et al. (2021). The tuple (at, zt) is added to the queues.\nThe action executed in the delayed environment is a\u03c4t . Since no future action can overwrite the first expected action a\u0302t, we note that a\u03c4t = a\u0302t. The reward from the system is then stored to form the transition (st, a\u03c4t , rt).\nPending actions resolution Figure 1 depicts the two queues, together with the relation between effective decision times and pending actions. At each time step, the actual pending actions are calculated using the effective decision times from definition 1.\nFuture state prediction In order to make accurate predictions about the future state, we embrace the state representation network st = H(ot) and the dynamics network s\u0302t+1 = G(st, at) from EfficientZero (Ye et al., 2021). Note that we do not make a distinction between the observation ot and the state representation st as in (Ye et al., 2021) for simplicity. However, all network parts\u2014 forward, dynamics, value, and policy\u2014operate in the latent space. Moreover, the replay buffer contains transition samples with states st, st+1 in that same space. The pseudo-code in 1 depicts how the Self-play samples episodes in the stochastic delayed environment.\nCorrections in the replay buffer In the original version of EfficientZero, episode transitions (st, at, rt) are stored in game histories containing state, actions, and rewards per step. However, to insert the right transitions into the replay buffer, we post-process the game histories using all execution delays {zt}t=Lt=0 , enabling us to compute effective decision times. The game transitions then become (st, a\u03c4t , rt). Details about the episode processing can be found in Appendix B.\nLearning a policy in a delayed environment Every action selection is carried out using the same MCTS approach employed in EfficientZero. The output of this search, denoted by \u03c0t(s\u0302t+zt), also serves as the policy target in the loss function. The primary alteration occurs in the policy loss, which takes the following form:\nLp = L(\u03c0t(s\u0302t+zt), pt(st+zt)). Although the MCTS input relies on a predicted version of st+zt , this loss remains justified due to the exceptional precision exhibited by the dynamics model within the latent space. The rest of the loss components remain unchanged."
        },
        {
            "heading": "6 EXPERIMENTS",
            "text": "The comprehensive framework we construct around EfficientZero accommodates both constant and stochastic execution delays. We consider the delay values {5, 15, 25} as in (Derman et al., 2021). In both delay types, we refrain from random initialization of the initial action queue as seen in (Derman et al., 2021). Instead, our model determines the first M actions. This is achieved through the iterative application of the forward network and the policy network. In practice, the agent observes the initial state s0, infers the a\u03040 (policy), and predicts subsequent states s\u03021 = G(s0, a\u03040). Similarly, the agent infers s\u03022, . . . , s\u0302M and selects actions a\u03042, . . . , a\u0304M\u22121 through this iterative process. A summary of scores and standard deviations for our algorithm and the baseline methods is presented in C.4 tables 1 and 2."
        },
        {
            "heading": "6.1 SMALL DATA FOR DELAYED ENVIRONMENTS",
            "text": "The original EfficientZero sampled 100,000 transitions, aligning with the Atari 100K benchmark. Our approach significantly benefits from EfficientZero\u2019s sample efficiency. However, the presence of delays adds complexity to the learning process, requiring mildly more interactions\u2014130,000 in our case. Successfully tackling delays with such limited data is a non-trivial task.\nIn order to evaluate our theoretical findings, we subject Delayed EfficientZero to testing across 15 Atari games, each both under constant and stochastic delays. Accumulating enough samples in realistic environments can either be costly or present logistical challenges. For the purposes of this study, we operate under the assumption of an exact observation of the delay once they are revealed."
        },
        {
            "heading": "6.2 CONSTANT DELAYS",
            "text": "In this section, we analyze the scores produced by Delayed EfficientZero on constant delays. Figure 3(a) shows that our method achieves the best average score in 38 of the 45 experiments.\nWe begin our comparison with an oblivious version of EfficientZero, confirming the previously established theoretical findings in (Derman et al., 2021, Prop. 5.2.): stationary policies alone are insufficient to achieve the optimal value. Given that Oblivious EfficientZero lacks adaptability and exclusively generates stationary policies, it is not surprising that it struggles to learn effectively in most games.\nOur second baseline reference is Delayed-Q (Derman et al., 2021), acknowledged as a state-of-theart algorithm to address constant delays in Atari games. However, it is worth noting that DelayedQ is a DQN-based algorithm, and its original implementation entailed training with one million samples. Our observations reveal that within the constraints of the allotted number of samples, Delayed-Q struggles to achieve efficient learning. On the other hand, Delayed-Q benefits from its perfect forward model, provided by the environment itself. We observe that in 21 out of 45 experiments, Delayed-Q achieves an average score that is at least 85% of our method\u2019s result, even from the initial training iterations.\nImportance of the learned forward model Despite the reliance of Delayed-Q on the Atari environment as its forward model, this baseline exhibits significant vulnerabilities when faced with limited data. To address this challenge, we harness representation learning capabilities of EfficientZero, allowing us to acquire state representations and dynamics through the utilization of its self-supervised rich reward signal, among other features. Our findings illustrate that when we effectively learn a precise prediction model, the management of delays becomes more feasible. It is worth noting that an increase in environmental randomness can result in larger compounding errors in the subsequent forward models. We delve into the delay randomness subject in the following section.\nWe provide the convergence plots for all tested games with constant delay in Appendix C.1."
        },
        {
            "heading": "6.3 STOCHASTIC DELAYS",
            "text": "For a more realistic scenario, we test our method on stochastic delays as well. We report the average scores of Delayed EfficientZero and baselines for 15 Atari games in Figure 3(b). Similarly to the constant case, we observe an impressive achievement. In 42 of 45 experiments, our method prevails with the highest average score.\nFor each Atari game, we also use three stochastic delay settings. For each value M \u2208 {5, 15, 25}, we use the following formulation of the delay zt at time t:\nz0 = M\nzt>0 =  min(zt + 1,M) with probability 0.2, max(zt \u2212 1, 0) with probability 0.2, zt otherwise.\n(3)\nNote that the delay is never reinitialized to M when the episode terminates. Instead, the delays remain the same for the beginning of the next episode. By doing that, we do not assume an initial delay value and cover a broader range of applications.\nWe opt to avoid the augmentation approach in the baselines due to its inherent complexity. Incorporating the pending action queue into the state space was deemed infeasible due to its exponential growth, even in simple environments such as Maze (Derman et al., 2021). In our scenario, the length of the pending queue ranges from zero to M in cases involving stochastic delays. Consequently, the size of the augmented state space is denoted as |\u2126| = |(S\u00d7 [M ]\u00d7A)\u221e|. As the value of M increases, the order of magnitude of this quantity becomes overwhelming.\nWe provide the convergence plots for all tested games with stochastic delay in Appendix C.2.\nDelayed EfficientZero is additionally trained, under stochastic delays, incorporating very large delay values for evaluation in extreme scenarios. The performance in such cases is predominantly influenced by the specific characteristics of the environment. For instance, in Asterix, we observe that Delayed EfficientZero struggles to learn efficiently with delays of M = 35 and M = 45. Conversely, in Hero, our algorithm demonstrates comparable learning performance even with the largest delay value. Convergence plots appear in Appendix C.3, Table 7."
        },
        {
            "heading": "7 DISCUSSION",
            "text": "In this work, we formalized stochastic execution delay in MDPs without resorting to state augmentation. Although its analysis is more engaged, this setting leads to an intriguing insight: as in the constant delay case, also in the stochastic case we can restrict the policy search to the class of Markov policies and still reach optimal performance. We introduced Delayed EfficientZero, a model-based approach that achieves state-of-the-art performance for delayed environments, thanks to an accurate learned world-model. A natural extension of our approach would consider predicting next-state distribution instead of point prediction s\u0302t+1, so as to train a more robust world model and mitigate uncertainty in more stochastic environments.\nDelayed EfficientZero heavily relies on a learned world model to infer future states. Devising a model-free method that addresses delayed execution while avoiding embedding remains an open question. Moreover, in this study, we assumed that delay values are observed in real-time, which enabled us to backpropagate the forward model the corresponding number of times. Ridding of this constraint would add another layer of difficulty, because then the decision time that generated each action would be ignored. In the case of deterministic execution delay, we may predict the delay values, whereas, in stochastic delays, we could adopt a robust approach to rollout multiple realizations of delays and act according to some worst-case criterion. Useful improvement may be made to support continuous delays, and getting rid of drop and duplication of actions. In such a case, we may plan and derive continuous actions. Another modeling assumption we made was that the delay process is independent of the MDP environment. Alternatively, one may study dependent delay processes, e.g., delay values that depend on the agent\u2019s current state or decision rule. Such extension is particularly meaningful in autonomous driving where the agent state must be interpreted from pictures of various information complexities."
        },
        {
            "heading": "8 REPRODUCIBILITY",
            "text": "To further advance the cause of reproducibility in the field of Reinforcement Learning (RL), we are committed to transparency and rigor throughout our research process. Here, we outline the various steps undertaken to facilitate reproducibility: (1) Detailed Methodology: Our commitment to transparency begins with a description of our methodology. We provide clear explanations of the experimental setup, algorithmic choices, and the concepts introduced to ensure that fellow researchers can replicate our work. (2) Formal proofs are presented in the Appendix. (3) Open Implementation: To foster exploration and research into delay-related challenges in RL, we include the code as part of the supplementary material. We also include a README file with instructions on how to run both training and evaluation. Our repository is a fork of EfficientZero (Ye et al., 2021) with the default parameters taken from the original paper. (4) Acknowledging Distributed Complexity: It is important to note that while we strive for reproducibility, the inherent complexity of distributed architectures, such as EfficientZero, presents certain challenges. Controlling the precise order of execution solely through code can be intricate, and we acknowledge that achieving exact-result replication in distributed systems may pose difficulties. In summary, our dedication to reproducibility is manifested through transparent methodologies, rigorous formal proofs, and an openly accessible implementation. While we recognize the complexities of distributed systems, we believe that our contributions provide valuable insight into delay-related issues in RL, inviting further exploration and collaboration within the research community."
        },
        {
            "heading": "APPENDIX",
            "text": ""
        },
        {
            "heading": "A A FORMULATION FOR STOCHASTIC DELAYS",
            "text": ""
        },
        {
            "heading": "A.1 PROOF OF THEOREM 4.1",
            "text": "Proof. By definition of conditional probability, for all measurable sets A1, \u00b7 \u00b7 \u00b7 , An \u2208 B(\u2126), we have\nP\u03c0(\u2229ni=0Ai) = ( n\u22121\u220f i=0 P\u03c0(Ai| \u2229nj=i+1 Aj) ) P\u03c0(An). (4)\nApplying Eq. (4) to n = 2t+ 1 on the following events:\nA2t+1 := {s\u03030 = s0} A2t := {z\u03030 = z0, a\u03030 = a0}\n... A2 := {z\u0303t\u22121 = zt\u22121, a\u0303t\u22121 = at\u22121} A1 := {s\u0303t = st}, A0 := {z\u0303t = zt}\nwe obtain that\nP\u03c0(s\u03030 = s0, z\u03030 = z0, a\u03030 = a0, \u00b7 \u00b7 \u00b7 , a\u0303t\u22121 = at\u22121, s\u0303t = st, z\u0303t = zt)\n= P\u03c0(s\u03030 = s0) t\u22121\u220f i=0 P\u03c0(z\u0303i = zi, a\u0303i = ai|s\u03030 = s0, z\u03030 = z0, a\u03030 = a0, \u00b7 \u00b7 \u00b7 , s\u0303i = si) P\u03c0(s\u0303i+1 = si+1|s\u03030 = s0, z\u03030 = z0, a\u03030 = a0, \u00b7 \u00b7 \u00b7 , z\u0303i = zi, a\u0303i = ai) \u00b7 P\u03c0(z\u0303t = zt|s\u03030 = s0, z\u03030 = z0, a\u03030 = a0, \u00b7 \u00b7 \u00b7 , z\u0303t\u22121 = zt\u22121, a\u0303t\u22121 = at\u22121, s\u0303t = st)\n(1) = P\u03c0(s\u03030 = s0) t\u22121\u220f i=0 P\u03c0(z\u0303i = zi, a\u0303i = ai|s\u03030 = s0, z\u03030 = z0, a\u03030 = a0, \u00b7 \u00b7 \u00b7 , s\u0303i = si)\nP\u03c0(s\u0303i+1 = si+1|s\u03030 = s0, z\u03030 = z0, a\u03030 = a0, \u00b7 \u00b7 \u00b7 , z\u0303i = zi, a\u0303i = ai)P\u03c0(z\u0303t = zt)\n= P\u03c0(s\u03030 = s0) t\u22121\u220f i=0 P\u03c0(z\u0303i = zi, a\u0303i = ai|s\u03030 = s0, z\u03030 = z0, a\u03030 = a0, \u00b7 \u00b7 \u00b7 , s\u0303i = si)\nP\u03c0(s\u0303i+1 = si+1|h\u0303i = (hi\u22121, ai\u22121, zi, si), a\u0303i = ai)P\u03c0(z\u0303t = zt)\n= P\u03c0(s\u03030 = s0) t\u22121\u220f i=0 P\u03c0(z\u0303i = zi, a\u0303i = ai|s\u03030 = s0, z\u03030 = z0, a\u03030 = a0, \u00b7 \u00b7 \u00b7 , s\u0303i = si)P (si+1|si, ai)\u03b6(zt)\n= P\u03c0(s\u03030 = s0) t\u22121\u220f i=0 P\u03c0(a\u0303i = ai|h\u0303i = hi)P\u03c0(z\u0303i = zi|s\u03030 = s0, z\u03030 = z0, a\u03030 = a0, \u00b7 \u00b7 \u00b7 , s\u0303i = si)P (si+1|si, ai)\u03b6(zt)\n= P\u03c0(s\u03030 = s0) t\u22121\u220f i=0 P\u03c0(a\u0303i = ai|h\u0303i = hi)\u03b6(zi)P (si+1|si, ai)\u03b6(zt)\n= \u00b5(s0) ( tz\u22121\u220f k=0 1a\u0304k(ak)P (sk+1|sk, ak) )( t\u22121\u220f l=tz \u03c0\u03c4l(hl)(al)P (sl+1|sl, al) ) \u03b6(zt)\nwhere equality (1) comes from the fact that the delay distribution at i+1 is independent of the state si+1 observed then. By Eq. (2),\nP\u03c0(s\u03030 = s0, z\u03030 = z0, a\u03030 = a0, \u00b7 \u00b7 \u00b7 , a\u0303t\u22121 = at\u22121, s\u0303t = st, z\u0303t = zt)\n= P\u03c0(s\u03030 = s0)\n( t\u220f\ni=0\n\u03b6(zi) ) tz\u22121\u220f i=0 P\u03c0(a\u0303i = ai|h\u0303i = hi)P\u03c0(s\u0303i+1 = si+1|h\u0303i = (hi\u22121, ai\u22121, si, zi), a\u0303i = ai)\nt\u22121\u220f k=tz P\u03c0(a\u0303k = ak|h\u0303k = hk)P\u03c0(s\u0303k+1 = sk+1|h\u0303k = (hk\u22121, ak\u22121, sk, zk), a\u0303k = ak)\n= \u00b5(s0) ( tz\u22121\u220f k=0 1a\u0304k(ak)P (sk+1|sk, ak) )( t\u22121\u220f l=tz \u03c0\u03c4l(hl)(al)P (sl+1|sl, al) )( t\u220f m=0 \u03b6(zm) ) ,\nwhere the first product is empty if and only if z0 = 0, i.e., the decision taken at time t = 0 is immediately executed."
        },
        {
            "heading": "A.2 PROOF OF THEOREM 4.2",
            "text": "We first establish the following lemma, which will be used in the theorem\u2019s proof.\nLemma A.1. For all t > 0 and zt > 0,\nP\u03c0(s\u0303t = s\u2032|a\u0303t = a\u2032, s\u0303t\u22121 = s, a\u0303t\u22121 = a) = P\u03c0(s\u0303t = s\u2032|s\u0303t\u22121 = s, a\u0303t\u22121 = a) (5)\nProof. Since zt > 0 by assumption, the state variable s\u0303t is independent of a\u0303t, so that: P\u03c0(s\u0303t = s\u2032|a\u0303t = a\u2032, s\u0303t\u22121 = s, a\u0303t\u22121 = a) = P (s\u2032|s, a) = P\u03c0(s\u0303t = s\u2032|s\u0303t\u22121 = s, a\u0303t\u22121 = a).\nTheorem. Let \u03c0 \u2208 \u03a0HR be a history dependent policy. For all s0 \u2208 S, there exists a Markov policy \u03c0\u2032 \u2208 \u03a0MR that yields the same process distribution as \u03c0, i.e.,\nP\u03c0 \u2032 (s\u0303\u03c4t = s \u2032, a\u0303t = a|s\u03030 = s0, z\u0303 = z) = P\u03c0(s\u0303\u03c4t = s\u2032, a\u0303t = a|s\u03030 = s0, z\u0303 = z), (6)\nfor all a \u2208 A, s\u2032 \u2208 S, t \u2265 0, and z\u0303 := (z\u0303t)t\u2208N the whole delay process.\nProof. If M = 0, the result holds true by standard RL theory (Puterman, 2014)[Thm 5.5.1]. Assume now that M > 0, fix s \u2208 S and z = (zt)t\u2208N. The variable z is exogenous, and we shall consider the corresponding sequence of effective decision times (\u03c4t)t\u2208N. We construct a Markov policy \u03c0\u2032 for these times only while for other times, \u03c0\u2032 can be anything else. Let thus \u03c0\u2032 := (\u03c0\u2032\u03c4t)t\u2208N with \u03c0\u2032\u03c4t : {s} \u2192 \u2206A defined as\n\u03c0\u2032\u03c4t(s \u2032)(a) := { P\u03c0(a\u0303t = a|s\u0303\u03c4t = s\u2032, s\u03030 = s, z\u0303 = z) if t \u2265 tz, 1a\u0304t(a) otherwise, \u2200s\u2032 \u2208 S, a \u2208 A . (7)\nRecall the definition of the time the SED-MDP performs a switch and uses the agent\u2019s policy tz := min{z0, z1 +1, . . . ,M \u2212 1+ zM\u22121}. For the policy \u03c0\u2032 defined in Eq. (7), we prove the theorem by induction on t \u2265 tz , since for t < tz , the decision rule being applied is 1a\u0304t regardless of the policy. For t = tz , by Thm. 4.1, we have:\nP\u03c0(s\u0303\u03c4tz = s \u2032, a\u0303tz = a|s\u03030 = s0) =\nP\u03c0(s\u0303\u03c4tz = s \u2032, a\u0303tz = a, s\u03030 = s0)\nP\u03c0(s\u03030 = s0)\n= 1 P\u03c0(s\u03030 = s0) \u2211\ns1,\u00b7\u00b7\u00b7 ,stz\u22121, a0,\u00b7\u00b7\u00b7 ,atz\u22121\nP\u03c0(s\u0303\u03c4tz = s \u2032, a\u0303tz = a, a\u0303tz\u22121 = atz\u22121, \u00b7 \u00b7 \u00b7 , a\u03030 = a0, s\u03030 = s0)\n= \u2211\ns1,\u00b7\u00b7\u00b7 ,stz\u22121, a0,\u00b7\u00b7\u00b7 ,atz\u22121\ntz\u22121\u220f k=0 1a\u0304k(ak)P (sk+1|sk, ak)\nWe observe that the expression does not depend on the prescribed policy, so it equals P\u03c0\u2032(s\u0303\u03c4tz = s\u2032, a\u0303tz = a|s\u03030 = s0) and the induction base holds. Assume that Eq. (6) holds up until t = n\u2212 1, with t \u2265 tz . Let denote t\u2032 = n, we aim to prove that\nP\u03c0(s\u0303t\u2032 = s\u2032|s\u03030 = s) = P\u03c0 \u2032 (s\u0303t\u2032 = s \u2032|s\u03030 = s). (8)\nThen, we can write\nP\u03c0(s\u0303t\u2032 = s\u2032|s\u03030 = s) = \u2211 st\u2208S, at\u2208A P\u03c0(s\u0303t\u2032 = s\u2032, s\u0303t = st, a\u0303t = at|s\u03030 = s)\n= \u2211 st\u2208S, at\u2208A P\u03c0(s\u0303t\u2032 = s\u2032|a\u0303t = at, s\u0303t = st, s\u03030 = s)\nP\u03c0(a\u0303t = at|s\u0303t = st, s\u03030 = s)P\u03c0(s\u0303t = st|s\u03030 = s) = \u2211 st\u2208S, at\u2208A P (s\u2032|st, at)P\u03c0(a\u0303t = at|s\u0303t = st, s\u03030 = s)P\u03c0(s\u0303t = st|s\u03030 = s).\nBy Eq. 2, a\u0303t only depends on state-action sequences up to \u03c4t. Let\u2019s differentiate on the value of zt.\nIf zt = 0, then \u03c4t = t and by construction of \u03c0\u2032 we can write:\nP\u03c0(a\u0303t = at|s\u0303t = st, s\u03030 = s) = P\u03c0(a\u0303t = at|s\u0303\u03c4t = st, s\u03030 = s) = P\u03c0 \u2032 (a\u0303t = at|s\u0303\u03c4t = st, s\u03030 = s).\n\u21d2 P\u03c0(s\u0303t\u2032 = s\u2032|s\u03030 = s) = \u2211 st\u2208S, at\u2208A P (s\u2032|st, at)P\u03c0 \u2032 (a\u0303t = at|s\u0303\u03c4t = st, s\u03030 = s)P\u03c0(s\u0303t = st|s\u03030 = s).\nIf zt > 0, then \u03c4t < t and we can write: P\u03c0(a\u0303t = at|s\u0303t = st, s\u03030 = s) = P\u03c0(a\u0303t = at|s\u03030 = s)\n\u21d2 P\u03c0(s\u0303t\u2032 = s\u2032|s\u03030 = s) = \u2211 st\u2208S, at\u2208A P (s\u2032|st, at)P\u03c0(a\u0303t = at|s\u03030 = s)P\u03c0(s\u0303t = st|s\u03030 = s).\nSince t = n\u2212 1, by the induction hypothesis we can rewrite\nP\u03c0(a\u0303t = at|s\u03030 = s) = \u2211\ns\u03c4t\u2208S P\u03c0(a\u0303t = at, s\u0303\u03c4t = s\u03c4t |s\u03030 = s)\n= \u2211\ns\u03c4t\u2208S P\u03c0\n\u2032 (a\u0303t = at, s\u0303\u03c4t = s\u03c4t |s\u03030 = s)\n= P\u03c0 \u2032 (a\u0303t = at|s\u03030 = s) \u21d2 P\u03c0(s\u0303t\u2032 = s\u2032|s\u03030 = s) = \u2211 st\u2208S, at\u2208A P (s\u2032|st, at)P\u03c0 \u2032 (a\u0303t = at|s\u03030 = s)P\u03c0(s\u0303t = st|s\u03030 = s).\nIn the two cases, we now study the last term in the above equations, P\u03c0(s\u0303t = st|s\u03030). We have\nP\u03c0(s\u0303t = st|s\u03030 = s) = \u2211\nst\u22121,\u00b7\u00b7\u00b7 ,stz\u2208S at\u22121,\u00b7\u00b7\u00b7 ,atz\u2208A\nP\u03c0(s\u0303t = st, s\u0303t\u22121 = st\u22121, a\u0303t\u22121 = at\u22121, \u00b7 \u00b7 \u00b7 , s\u0303tz = stz , a\u0303tz = atz |s\u03030 = s)\n= \u2211\nst\u22121,\u00b7\u00b7\u00b7 ,stz\u2208S at\u22121,\u00b7\u00b7\u00b7 ,atz\u2208A\nP\u03c0(s\u0303t = st|s\u0303t\u22121 = st\u22121, a\u0303t\u22121 = at\u22121, \u00b7 \u00b7 \u00b7 , s\u0303tz = stz , a\u0303tz = atz , s\u03030 = s)\nP\u03c0(s\u0303t\u22121 = st\u22121, a\u0303t\u22121 = at\u22121, \u00b7 \u00b7 \u00b7 , s\u0303tz = stz , a\u0303tz = atz |s\u03030 = s) = \u2211\nst\u22121,\u00b7\u00b7\u00b7 ,stz\u2208S at\u22121,\u00b7\u00b7\u00b7 ,atz\u2208A\nP (st|st\u22121, at\u22121)\nP\u03c0(s\u0303t\u22121 = st\u22121, a\u0303t\u22121 = at\u22121, \u00b7 \u00b7 \u00b7 , s\u0303tz = stz , a\u0303tz = atz |s\u03030 = s) = \u2211\nst\u22121,\u00b7\u00b7\u00b7 ,stz\u2208S at\u22121,\u00b7\u00b7\u00b7 ,atz\u2208A\nP (st|st\u22121, at\u22121)\nP\u03c0(s\u0303t\u22121 = st\u22121|a\u0303t\u22121 = at\u22121, s\u0303t\u22122 = st\u22122, a\u0303t\u22122 = at\u22122, \u00b7 \u00b7 \u00b7 , s\u0303tz = stz , a\u0303tz = atz , s\u03030 = s)\nP\u03c0(a\u0303t\u22121 = at\u22121, s\u0303t\u22122 = st\u22122, a\u0303t\u22122 = at\u22122, \u00b7 \u00b7 \u00b7 , s\u0303tz = stz , a\u0303tz = atz |s\u03030 = s) Lemma A.1 = \u2211\nst\u22121,\u00b7\u00b7\u00b7 ,stz\u2208S at\u22121,\u00b7\u00b7\u00b7 ,atz\u2208A\nP (st|st\u22121, at\u22121)P (st\u22121|st\u22122, at\u22122)\nP\u03c0(a\u0303t\u22121 = at\u22121, s\u0303t\u22122 = st\u22122, a\u0303t\u22122 = at\u22122, \u00b7 \u00b7 \u00b7 , s\u0303tz = stz , a\u0303tz = atz |s\u03030 = s) = \u2211\nst\u22121,\u00b7\u00b7\u00b7 ,stz\u2208S at\u22121,\u00b7\u00b7\u00b7 ,atz\u2208A\nP (st|st\u22121, at\u22121)P (st\u22121|st\u22122, at\u22122)\nP\u03c0(a\u0303t\u22121 = at\u22121|s\u0303t\u22122 = st\u22122, a\u0303t\u22122 = at\u22122 \u00b7 \u00b7 \u00b7 , s\u0303tz = stz , a\u0303tz = atz |s\u03030 = s)\nP\u03c0(s\u0303t\u22122 = st\u22122, a\u0303t\u22122 = at\u22122 \u00b7 \u00b7 \u00b7 , s\u0303tz = stz , a\u0303tz = atz |s\u03030 = s) =\n...\n= \u2211\nst\u22121,\u00b7\u00b7\u00b7 ,stz\u2208S at\u22121,\u00b7\u00b7\u00b7 ,atz\u2208A\n( t\u2212tz\u220f i=0 P (st\u2212i|st\u2212i\u22121, at\u2212i\u22121) )\n( t\u2212tz\u22121\u220f j=1 P\u03c0(a\u0303t\u2212j = at\u2212j |s\u0303t\u2212j\u22121 = st\u2212j\u22121, a\u0303t\u2212j\u22121 = at\u2212j\u22121, \u00b7 \u00b7 \u00b7 , s\u0303tz = stz , a\u0303tz = atz , s\u03030 = s) )\nP\u03c0(s\u0303tz = stz |a\u0303tz = atz , s\u03030 = s)P \u03c0 (a\u0303tz = atz |s\u03030 = s)\n(1) = \u2211 st\u22121,\u00b7\u00b7\u00b7 ,stz\u2208S at\u22121,\u00b7\u00b7\u00b7 ,atz\u2208A ( t\u2212tz\u220f i=0 P (st\u2212i|st\u2212i\u22121, at\u2212i\u22121) )( t\u2212tz\u22121\u220f j=1 P\u03c0(a\u0303t\u2212j = at\u2212j |s\u0303\u03c4t\u2212j = s\u03c4t\u2212j ) )\nP\u03c0(s\u0303tz = stz |a\u0303tz = atz , s\u03030 = s)P \u03c0 (a\u0303tz = atz |s\u03030 = s)\nWe continue and write\n(2) = \u2211 st\u22121,\u00b7\u00b7\u00b7 ,stz\u2208S at\u22121,\u00b7\u00b7\u00b7 ,atz\u2208A ( t\u2212tz\u220f i=0 P (st\u2212i|st\u2212i\u22121, at\u2212i\u22121) )( t\u2212tz\u22121\u220f j=1 qd\u2032\u03c4t\u2212j (s\u03c4t\u2212j ) (at\u2212j) )\nP\u03c0(s\u0303tz = stz |a\u0303tz = atz , s\u03030 = s)P \u03c0 (a\u0303tz = atz |s\u03030 = s)\n(3) = \u2211 st\u22121,\u00b7\u00b7\u00b7 ,stz\u2208S at\u22121,\u00b7\u00b7\u00b7 ,atz\u2208A ( t\u2212tz\u220f i=0 P (st\u2212i|st\u2212i\u22121, at\u2212i\u22121) )( t\u2212tz\u22121\u220f j=1 qd\u2032\u03c4t\u2212j (s\u03c4t\u2212j ) (at\u2212j) )\nP\u03c0(s\u0303tz = stz |a\u0303tz = atz , s\u03030 = s) \u2211\ns\u2032\u03c4tz \u2208S\n( qd\u2032\u03c4tz (s\u03c4tz )(atz )\nIn (2) and (3) we used equation 2. We now analyze the last term implying on \u03c0 and show that it is not policy dependant: P\u03c0(s\u0303tz = stz |a\u0303tz = atz , s\u03030 = s) = P \u03c0 (s\u0303tz = stz |s\u03030 = s)\n= \u2211\nstz\u22121,\u00b7\u00b7\u00b7 ,s1\u2208S atz\u22121,\u00b7\u00b7\u00b7 ,a0\u2208A\nP\u03c0(s\u0303tz = stz , s\u0303tz\u22121 = stz\u22121, a\u0303tz\u22121 = atz\u22121, \u00b7 \u00b7 \u00b7 , s\u03031 = s1, a\u03031 = a1, a\u03030 = a0|s\u03030 = s)\n= \u2211\nstz\u22121,\u00b7\u00b7\u00b7 ,s1\u2208S atz\u22121,\u00b7\u00b7\u00b7 ,a0\u2208A\n1\nP\u03c0(s\u03030 = s) P\u03c0(s\u0303tz = stz , s\u0303tz\u22121 = stz\u22121, a\u0303tz\u22121 = atz\u22121, \u00b7 \u00b7 \u00b7 , s\u03031 = s1, a\u03031 = a1, a\u03030 = a0, s\u03030 = s)\n(4) = \u2211 stz\u22121,\u00b7\u00b7\u00b7 ,s1\u2208S atz\u22121,\u00b7\u00b7\u00b7 ,a0\u2208A 1 \u00b5(s) \u00b5(s)\ntz\u22121\u220f i=0 P (si+1|si, ai)1a\u0304i (ai)  = \u2211 stz\u22121,\u00b7\u00b7\u00b7 ,s1\u2208S atz\u22121,\u00b7\u00b7\u00b7 ,a0\u2208A tz\u22121\u220f i=0 P (si+1|si, ai)1a\u0304i (ai)  ,\nwhere (4) results from Th. 4.1.\nThus, if we decompose P\u03c0\u2032(s\u0303t\u2032 = s\u2032|s\u03030 = s) according to the exact same derivation as we did for P\u03c0(s\u0303t\u2032 = s\u2032|s\u03030 = s), we obtain that at t\u2032 = n:\nP\u03c0(s\u0303t\u2032 = s\u2032|s\u03030 = s) = P\u03c0 \u2032 (s\u0303t\u2032 = s \u2032|s\u03030 = s). (9)\nAs a preceding step in the induction process, this results holds at \u03c4t\u2032 \u2264 t\u2032 = n:\nP\u03c0(s\u0303\u03c4t\u2032 = s \u2032|s\u03030 = s) = P\u03c0 \u2032 (s\u0303\u03c4t\u2032 = s \u2032|s\u03030 = s). (10)\nAs a result, at t\u2032 = n we have\nP\u03c0 \u2032 (s\u0303\u03c4t\u2032 = s \u2032, a\u0303t\u2032 = a|s\u03030 = s)=P\u03c0 \u2032 (a\u0303t\u2032 = a|s\u0303\u03c4t\u2032 = s \u2032, s\u03030 = s)P\u03c0 \u2032 (s\u0303\u03c4t\u2032 = s \u2032|s\u03030 = s) (a) = P\u03c0\n\u2032 (a\u0303t\u2032 = a|s\u0303\u03c4t\u2032 = s \u2032, s\u03030 = s)P\u03c0(s\u0303\u03c4t\u2032 = s \u2032|s\u03030 = s)\n(b) = P\u03c0(a\u0303t\u2032 = a|s\u0303\u03c4t\u2032 = s \u2032, s\u03030 = s)P\u03c0(s\u0303\u03c4t\u2032 = s \u2032|s\u03030 = s)\n=P\u03c0(s\u0303\u03c4t\u2032 = s \u2032, a\u0303t\u2032 = a|s\u03030 = s),\nwhere (a) follows from Eq. 10; (b) by construction of \u03c0\u2032\u03c4t(s \u2032)(a) in Eq. 7. Finally, assuming it is satisfied at t = n\u2212 1, the induction step is proved for t = n, which ends the proof."
        },
        {
            "heading": "B ALGORITHM",
            "text": "We briefly describe the EfficientZero algorithm from (Ye et al., 2021) and highlight the places where novelties are introduced in Delayed EfficientZero. In EfficientZero, there are several actors running in parallel:\n\u2022 The Self-play actor fetches the current networks of the model (representation, dynamics, value, policy prediction and reward networks: H,G,V,P,R). It samples episodes according to these networks, following the algorithm 1. Although the algorithm relies on episode sampling, the differences from the original version of EfficientZero, Ye et al. (2021) reside also in the episode post processing. After generating the trajectory, each episode is edited, associating each state st with the executed action at time t rather than the decided action at that time. This modification ensures that the replay buffer contains an episode that is effectively free of delays, allowing the utilization of existing learning procedures for effective learning. In addition, for the learner\u2019s sake, the self-play actors store statistics and outputs of the consecutive MCTS searches to the replay buffer.\n\u2022 The CPU rollout workers (Ye et al., 2021) are responsible for preparing the batch contexts (selecting indexes of trajectories, and defining boundaries of valid episodes).\n\u2022 GPU batch workers (Ye et al., 2021) effectively place batches on the GPUs and trigger the learner with an available batch signal.\n\u2022 The Learner itself updates weights of the different networks using the several costs functions: the innovative similarity cost (for efficient dynamics learning), the reward cost, the policy cost, and the value cost.\nAs previously highlighted, it is important to note that, aside from modifications to the Self-Play actor and the testing procedure, our approach does not necessitate changes to the architecture. It is adaptable and can be integrated into any model-based algorithm.\nTechnically, the challenges were to incorporate the parallel forward of the four environments for effectiveness; to plan for the initial action queue based on the initial observation as described in Section 6; and to manipulate episodes according to the delay values in a complex structure of observations, actions, reward, and statistics of MCTS searches.\nAlgorithm 1 Delayed EfficientZero: acting in environments with stochastic delays. PQR stands for Pending Queue Resolver (see Fig. 1) n\u2190 0 while n < STEPS do H,G,V,P,R \u2190 H\u03b8,G\u03b8,V\u03b8,P\u03b8,R\u03b8 Sample new episode (s0, z0, a0, . . . sT , zT , aT ) Initialize queues: Default action queue: [a\u2212M , . . . , a\u22121] = a\u0304. Delay queue: [z\u2212M , . . . , z\u22121] = [M, . . . ,M ]. t\u2190 0 while episode not terminated do\nObserve st, zt Query from the delayed environment the estimated pending queue: [a\u0302t, . . . , a\u0302t+zt\u22121] = PQR(at\u2212M , . . . , at\u22121, zt\u2212M , . . . , zt\u22121) s\u0302t+1 = g(st, a\u0302t) ... s\u0302t+zt = g(s\u0302t+zt\u22121, a\u0302t+zt\u22121) \u03c0t = MCTS(s\u0302t+zt) at \u223c \u03c0t Shift the action and delay queues and insert at and zt. t\u2190 t+ 1\nend while Post process the episode (s0, z0, a0, . . . sT , zT , aT ) and compute effective decision times\n\u03c40, . . . , \u03c4T Add (s0, \u03c40, a0, . . . sT , \u03c4T , aT ) to the Replay Buffer. n\u2190 n+ T end while"
        },
        {
            "heading": "C EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "C.1 CONVERGENCE PLOTS FOR CONSTANT DELAY",
            "text": "Figure 5 gives the learning curves of Delayed EfficientZero together with the baselines for constant delay."
        },
        {
            "heading": "C.2 CONVERGENCE PLOTS FOR STOCHASTIC DELAY",
            "text": "Figure 6 gives the learning curves of Delayed EfficientZero together with the baselines for stochastic delay."
        },
        {
            "heading": "C.3 DELAYED EFFICIENTZERO ON LARGE DELAYS",
            "text": "Here, we add results for very large delays in Asterix and Hero Atari games. As expected, the scores decrease as the delay increases, due to the complexity and error in planning."
        },
        {
            "heading": "C.4 SUMMARY OF SCORES ON ATARI GAMES",
            "text": "We summarize the scores and standard deviations obtained for Delayed EfficientZero (ours) and baselines Oblivious EfficientZero of (Ye et al., 2021) and Delayed DQN of (Derman et al., 2021) on 15 Atari games. The following table shows scores on constant delays.\nGame m = 5 m = 15 m = 25Obl. EZ Delayed DQN Delayed EZ Obl. EZ Delayed DQN Delayed EZ Obl. EZ Delayed DQN\nDelayed EZ\nAlien 191 \u00b117 484 \u00b173 993 \u00b1311 208 \u00b133 421 \u00b158 592 \u00b1180 196 \u00b121 454 \u00b1120 497 \u00b1185 Amidar 2.5 \u00b11.2 64 \u00b120 119 \u00b118 2.5 \u00b11 61 \u00b122 78 \u00b133 1.5 \u00b10.5 59 \u00b115 73 \u00b115 Assault 246 \u00b124 369 \u00b191 1056 \u00b1281 244 \u00b121 371 \u00b193 743 \u00b1100 237 \u00b117 389 \u00b1107 634 \u00b1107 Asterix 212 \u00b120 486 \u00b1242 11843\n\u00b16835 211 \u00b118 304 \u00b1160 1155 \u00b1499 208 \u00b116 248 \u00b1136 635 \u00b1370\nBankHeist 12.5 \u00b13 17 \u00b113.5 186 \u00b137 12.5 \u00b11.5 13.67 \u00b112.67\n36 \u00b120 13.2 \u00b12 14.33 \u00b114 31 \u00b121\nBoxing 1.2 \u00b10.5 2.33 \u00b19.33 2 \u00b14.75 1 \u00b10.5 1.5 \u00b15.5 -1.5 \u00b17.0 1.4 \u00b11 2 \u00b18 -1.25 \u00b15 Gopher 293 \u00b127 294 \u00b1190 1882 \u00b1978 299 \u00b133 287 \u00b1196 1261 \u00b1632 292 \u00b118 239 \u00b1157 841 \u00b1444 Hero 418 \u00b194 948 \u00b11157 11106\n\u00b11482 456 \u00b1103 717 \u00b11107 10120 \u00b12647 428 \u00b1258 843 \u00b11124 6421 \u00b11666\nMsPacman 253 \u00b124 785 \u00b1468 1123 \u00b1396 245 \u00b117 683 \u00b1387 1000 \u00b1458 239 \u00b119 667 \u00b1205 753 \u00b1457 NameThisGame 2206 \u00b1138 2208 \u00b1762 5373 \u00b11208 2252 \u00b1118 2120 \u00b1691 4705 \u00b11212 2262 \u00b1217 2083 \u00b1738 3645 \u00b11161 Qbert 178 \u00b144 358 \u00b1141 13085\n\u00b12510 173 \u00b129 343 \u00b152 3498 \u00b11607 150 \u00b132 320 \u00b183 623 \u00b1334\nRoadRunner 13 \u00b111 816 \u00b1912 18485 \u00b14068\n18 \u00b119 903 \u00b1980 5086 \u00b11947 19 \u00b112 1059 \u00b11014\n2602 \u00b1998\nStarGunner 645 \u00b162 992 \u00b1232 1887 \u00b1553 658 \u00b181 883 \u00b1211 1115 \u00b1313 690 \u00b164 928 \u00b1199 885 \u00b1204 TimePilot 3408 \u00b1458 2659 \u00b11686 2048 \u00b11564 3450 \u00b1573 1512 \u00b11393 2789 \u00b12147 3484 \u00b1205 2864\n\u00b11634 2785 \u00b11902\nWizardOfWor 636 \u00b1105 531 \u00b1416 1206 \u00b1797 593 \u00b194 496 \u00b1339 1119 \u00b1831 600 \u00b186 619 \u00b1456 857 \u00b1560\nTable 1: Summary of mean scores on 15 Atari games with constant execution delay M \u2208 {5, 15, 25} on 32 episodes for each of the four trained seeds.\nThe following table shows scores on stochastic delays.\nGame m = 5 m = 15 m = 25Obl. EZ Delayed DQN Delayed EZ Obl. EZ Delayed DQN Delayed EZ Obl. EZ Delayed DQN\nDelayed EZ\nAlien 206 \u00b123 366 \u00b1231 796 \u00b1205 195 \u00b123 333 \u00b1118 820 \u00b1325 199 \u00b131 324 \u00b1100 607 \u00b1288 Amidar 1.67 \u00b10.67 37 \u00b121 220 \u00b143 3.0 \u00b11.5 48 \u00b127 145 \u00b130 2.0 \u00b10.1 40 \u00b117 95 \u00b132 Assault 236 \u00b116 267 \u00b1140 1068 \u00b1299 240 \u00b114 242 \u00b1110 1132 \u00b1286 240 \u00b116 251 \u00b198 790 \u00b1191 Asterix 232 \u00b119 268 \u00b1148 6535 \u00b14392 220 \u00b114 239 \u00b1133 1225 \u00b1934 230 \u00b111 234 \u00b1168 636 \u00b1396 BankHeist 14.5 \u00b11.5 13 \u00b112.67 140 \u00b130 14.5 \u00b11.5 6.5 \u00b17.5 91 \u00b126 13.33 \u00b12.0 9 \u00b112.1 27 \u00b116 Boxing 1 \u00b11 -3 \u00b16 20.2 \u00b19 0 \u00b11.1 -4.1 \u00b19 2.5 \u00b16 1.5 \u00b11 -3.0 \u00b16.5 3.33 \u00b16.33 Gopher 224 \u00b121 145 \u00b1127 1855 \u00b1828 337 \u00b167 198 \u00b1168 1633 \u00b1709 353 \u00b113 130 \u00b1118 1378 \u00b1676 Hero 393 \u00b1134 492 \u00b1856 11798\n\u00b11539 437 \u00b1155 669 \u00b11006 11890 \u00b11372 441 \u00b1132 619 \u00b11027 8127 \u00b12681\nMsPacman 246 \u00b129 444 \u00b1276 1443 \u00b1498 247 \u00b119 439 \u00b1192 1173 \u00b1612 236 \u00b114 412 \u00b1232 940 \u00b1455 NameThisGame 2211 \u00b1134 1673 \u00b1912 5395 \u00b11244 2282 \u00b1115 1760 \u00b1780 5029 \u00b11085 2207 \u00b1163 1761 \u00b1769 5027 \u00b11099 Qbert 142 \u00b128 208 \u00b1113 3747 \u00b12603 147 \u00b119 296 \u00b188 4121 \u00b11829 153 \u00b128 303 \u00b178 1553 \u00b11275 RoadRunner 22 \u00b115 668 \u00b1649 9978 \u00b12914 7 \u00b16 227 \u00b1515 12906\n\u00b13311 14 \u00b111 260 \u00b1352 2173 \u00b11103\nStarGunner 671 \u00b173 378 \u00b1234 1931 \u00b1607 626 \u00b137 604 \u00b1321 1235 \u00b1249 657 \u00b177 621 \u00b1356 939 \u00b1199 TimePilot 3272 \u00b1415 1300 \u00b11282 1425 \u00b11347 3381 \u00b1116 1997 \u00b11828 2128 \u00b11308 3616 \u00b1417 2611\n\u00b11946 1931 \u00b11778\nWizardOfWor 656 \u00b1140 288 \u00b1247 1270 \u00b1985 549 \u00b140 336 \u00b1304 1175 \u00b1900 576 \u00b173 294 \u00b1233 1038 \u00b1721\nTable 2: Summary of mean scores on 15 Atari games with stochastic execution delay with maximal delay M \u2208 {5, 15, 25} on 32 episodes for each of the four trained seeds."
        },
        {
            "heading": "D COMPUTATIONAL RESSOURCES",
            "text": "The computational costs associated with training Delayed EfficientZero in environments with delays increase in proportion to the delay values. This escalation arises from the multiple applications of the forward network during inference.\nEfficientZero\u2019s architectural design harnesses the efficiency of C++/Cython for its Monte Carlo Tree Search (MCTS) implementation, intelligently distributes computation across CPU and GPU threads, thereby enabling parallel processing. Our experimental setup included two RTX 2080 TI GPUs. In the context of Delayed EfficientZero, each training run comprised 130,000 environment interactions and 150,000 training steps. We provide training duration statistics for the three delay configurations we employed:\nFor M = 5, the training duration exhibited fluctuations over a period of 20 hours. For M = 15, the training duration exhibited fluctuations over a period of 22 hours. For M = 25, the training duration exhibited fluctuations over a period of 25 hours.\nThe training duration of Oblivious EfficientZero is lightly shorter due to the omission of multi-step forward processing. For any delay value we tested, the training duration exhibited fluctuations over a period of 20 hours."
        }
    ],
    "year": 2023
}