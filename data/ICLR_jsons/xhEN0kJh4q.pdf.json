{
    "abstractText": "Protein design, a grand challenge of the day, involves optimization on a fitness landscape, and leading methods adopt a model-based approach where a model is trained on a training set (protein sequences and fitness) and proposes candidates to explore next. These methods are challenged by sparsity of high-fitness samples in the training set, a problem that has been in the literature. A less recognized but equally important problem stems from the distribution of training samples in the design space: leading methods are not designed for scenarios where the desired optimum is in a region that is not only poorly represented in training data, but also relatively far from the highly represented low-fitness regions. We show that this problem of \u201cseparation\u201d in the design space is a significant bottleneck in existing model-based optimization tools and propose a new approach that uses a novel VAE as its search model to overcome the problem. We demonstrate its advantage over prior methods in robustly finding improved samples, regardless of the imbalance and separation between lowand high-fitness training samples. Our comprehensive benchmark on real and semi-synthetic protein datasets as well as solution design for physics-informed neural networks, showcases the generality of our approach in discrete and continuous design spaces. Our implementation is available at https://github.com/sabagh1994/PGVAE.",
    "authors": [
        {
            "affiliations": [],
            "name": "Saba Ghaffari"
        },
        {
            "affiliations": [],
            "name": "Ehsan Saleh"
        },
        {
            "affiliations": [],
            "name": "Alexander G. Schwing"
        },
        {
            "affiliations": [],
            "name": "Yu-Xiong Wang"
        },
        {
            "affiliations": [],
            "name": "Martin D. Burke"
        },
        {
            "affiliations": [],
            "name": "Saurabh Sinha"
        }
    ],
    "id": "SP:3d4d1f42cadd18dd06781ca1a0a3eac3c0278e35",
    "references": [
        {
            "authors": [
                "Christof Angermueller",
                "David Dohan",
                "David Belanger",
                "Ramya Deshpande",
                "Kevin Murphy",
                "Lucy Colwell"
            ],
            "title": "Model-based reinforcement learning for biological sequence design",
            "venue": "In International conference on learning representations,",
            "year": 2019
        },
        {
            "authors": [
                "Frances H Arnold"
            ],
            "title": "Design by directed evolution",
            "venue": "Accounts of chemical research,",
            "year": 1998
        },
        {
            "authors": [
                "Surojit Biswas",
                "Gleb Kuznetsov",
                "Pierce J Ogden",
                "Nicholas J Conway",
                "Ryan P Adams",
                "George M Church"
            ],
            "title": "Toward machine-guided design of proteins",
            "venue": "BioRxiv, pp",
            "year": 2018
        },
        {
            "authors": [
                "Surojit Biswas",
                "Grigory Khimulya",
                "Ethan C Alley",
                "Kevin M Esvelt",
                "George M Church"
            ],
            "title": "Low-n protein engineering with data-efficient deep learning",
            "venue": "Nature methods,",
            "year": 2021
        },
        {
            "authors": [
                "David Brookes",
                "Hahnbeom Park",
                "Jennifer Listgarten"
            ],
            "title": "Conditioning by adaptive sampling for robust design",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "David H Brookes",
                "Jennifer Listgarten"
            ],
            "title": "Design by adaptive sampling",
            "venue": "arXiv preprint arXiv:1810.03714,",
            "year": 2018
        },
        {
            "authors": [
                "Drew H Bryant",
                "Ali Bashir",
                "Sam Sinai",
                "Nina K Jain",
                "Pierce J Ogden",
                "Patrick F Riley",
                "George M Church",
                "Lucy J Colwell",
                "Eric D Kelsic"
            ],
            "title": "Deep diversification of an aav capsid protein by machine learning",
            "venue": "Nature Biotechnology,",
            "year": 2021
        },
        {
            "authors": [
                "Alvin Chan",
                "Ali Madani",
                "Ben Krause",
                "Nikhil Naik"
            ],
            "title": "Deep extrapolation for attribute-enhanced generation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Christian Dallago",
                "Jody Mou",
                "Kadina E Johnston",
                "Bruce J Wittmann",
                "Nicholas Bhattacharya",
                "Samuel Goldman",
                "Ali Madani",
                "Kevin K Yang"
            ],
            "title": "Flip: Benchmark tasks in fitness landscape inference for proteins",
            "year": 2021
        },
        {
            "authors": [
                "Li Deng"
            ],
            "title": "The mnist database of handwritten digit images for machine learning research",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2012
        },
        {
            "authors": [
                "Josip Djolonga",
                "Andreas Krause",
                "Volkan Cevher"
            ],
            "title": "High-dimensional gaussian process bandits",
            "venue": "Advances in neural information processing systems,",
            "year": 2013
        },
        {
            "authors": [
                "Clara Fannjiang",
                "Jennifer Listgarten"
            ],
            "title": "Autofocused oracles for model-based design",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "David Firth"
            ],
            "title": "Bias reduction of maximum likelihood estimates",
            "venue": "Biometrika, 80(1):27\u201338,",
            "year": 1993
        },
        {
            "authors": [
                "Richard J Fox",
                "S Christopher Davis",
                "Emily C Mundorff",
                "Lisa M Newman",
                "Vesna Gavrilovic",
                "Steven K Ma",
                "Loleta M Chung",
                "Charlene Ching",
                "Sarena Tam",
                "Sheela Muley"
            ],
            "title": "Improving catalytic function by prosar-driven enzyme evolution",
            "venue": "Nature biotechnology,",
            "year": 2007
        },
        {
            "authors": [
                "Peter I Frazier"
            ],
            "title": "A tutorial on bayesian optimization",
            "venue": "arXiv preprint arXiv:1807.02811,",
            "year": 2018
        },
        {
            "authors": [
                "Saba Ghaffari",
                "Ehsan Saleh",
                "David Forsyth",
                "Yu-Xiong Wang"
            ],
            "title": "On the importance of firth bias reduction in few-shot classification",
            "venue": "International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Rafael G\u00f3mez-Bombarelli",
                "Jennifer N Wei",
                "David Duvenaud",
                "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato",
                "Benjam\u00edn S\u00e1nchez-Lengeling",
                "Dennis Sheberla",
                "Jorge Aguilera-Iparraguirre",
                "Timothy D Hirzel",
                "Ryan P Adams",
                "Al\u00e1n Aspuru-Guzik"
            ],
            "title": "Automatic chemical design using a data-driven continuous representation of molecules",
            "venue": "ACS central science,",
            "year": 2018
        },
        {
            "authors": [
                "Javier Gonzalez",
                "Joseph Longworth",
                "David C James",
                "Neil D Lawrence"
            ],
            "title": "Bayesian optimization for synthetic gene design",
            "venue": "arXiv preprint arXiv:1505.01627,",
            "year": 2015
        },
        {
            "authors": [
                "Xiaojie Guo",
                "Yuanqi Du",
                "Liang Zhao"
            ],
            "title": "Property controllable variational autoencoder via invertible mutual dependence",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Anvita Gupta",
                "James Zou"
            ],
            "title": "Feedback gan for dna optimizes protein functions",
            "venue": "Nature Machine Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Brian L Hie",
                "Kevin K Yang"
            ],
            "title": "Adaptive machine learning for protein engineering",
            "venue": "Current opinion in structural biology,",
            "year": 2022
        },
        {
            "authors": [
                "Kadina E Johnston",
                "Clara Fannjiang",
                "Bruce J Wittmann",
                "Brian L Hie",
                "Kevin K Yang",
                "Zachary Wu"
            ],
            "title": "Machine learning for protein engineering",
            "venue": "arXiv preprint arXiv:2305.16634,",
            "year": 2023
        },
        {
            "authors": [
                "Seokho Kang",
                "Kyunghyun Cho"
            ],
            "title": "Conditional molecular design with deep generative models",
            "venue": "Journal of chemical information and modeling,",
            "year": 2018
        },
        {
            "authors": [
                "Volodymyr Kindratenko",
                "Dawei Mu",
                "Yan Zhan",
                "John Maloney",
                "Sayed Hadi Hashemi",
                "Benjamin Rabe",
                "Ke Xu",
                "Roy Campbell",
                "Jian Peng",
                "William Gropp"
            ],
            "title": "Hal: Computer system for scalable deep learning",
            "venue": "In Practice and Experience in Advanced Research Computing,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "In Proceedings of the International Conference on Learning Representations,",
            "year": 2014
        },
        {
            "authors": [
                "Leslie Kish",
                "Martin Richard Frankel"
            ],
            "title": "Inference from complex samples",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological),",
            "year": 1974
        },
        {
            "authors": [
                "Aviral Kumar",
                "Sergey Levine"
            ],
            "title": "Model inversion networks for model-based optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "HA Daniel Lagass\u00e9",
                "Aikaterini Alexaki",
                "Vijaya L Simhadri",
                "Nobuko H Katagiri",
                "Wojciech Jankowski",
                "Zuben E Sauna",
                "Chava Kimchi-Sarfaty"
            ],
            "title": "Recent advances in (therapeutic protein) drug",
            "venue": "development. F1000Research,",
            "year": 2017
        },
        {
            "authors": [
                "Georgios Mikos",
                "Weitong Chen",
                "Junghae Suh"
            ],
            "title": "Machine learning identification of capsid mutations to improve aav production",
            "venue": "fitness. bioRxiv,",
            "year": 2021
        },
        {
            "authors": [
                "J. Mockus"
            ],
            "title": "Bayeisan approach to global optimization: theory and applications",
            "venue": "Springer Science & Business Media,",
            "year": 2012
        },
        {
            "authors": [
                "Art B. Owen"
            ],
            "title": "Monte Carlo theory, methods and examples",
            "year": 2013
        },
        {
            "authors": [
                "Jan Peters",
                "Stefan Schaal"
            ],
            "title": "Reinforcement learning by reward-weighted regression for operational space control",
            "venue": "In Proceedings of the 24th international conference on Machine learning,",
            "year": 2007
        },
        {
            "authors": [
                "Anna I Podgornaia",
                "Michael T Laub"
            ],
            "title": "Pervasive degeneracy and epistasis in a protein-protein interface",
            "year": 2015
        },
        {
            "authors": [
                "Yuchi Qiu",
                "Guo-Wei Wei"
            ],
            "title": "Clade 2.0: Evolution-driven cluster learning-assisted directed evolution",
            "venue": "Journal of Chemical Information and Modeling,",
            "year": 2022
        },
        {
            "authors": [
                "Maziar Raissi",
                "Paris Perdikaris",
                "George E Karniadakis"
            ],
            "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "Journal of Computational physics,",
            "year": 2019
        },
        {
            "authors": [
                "Zhizhou Ren",
                "Jiahan Li",
                "Fan Ding",
                "Yuan Zhou",
                "Jianzhu Ma",
                "Jian Peng"
            ],
            "title": "Proximal exploration for model-guided protein sequence design",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Philip A Romero",
                "Frances H Arnold"
            ],
            "title": "Exploring protein fitness landscapes by directed evolution",
            "venue": "Nature reviews Molecular cell biology,",
            "year": 2009
        },
        {
            "authors": [
                "Philip A Romero",
                "Andreas Krause",
                "Frances H Arnold"
            ],
            "title": "Navigating the protein fitness landscape with gaussian processes",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2013
        },
        {
            "authors": [
                "Reuven Rubinstein"
            ],
            "title": "The cross-entropy method for combinatorial and continuous optimization",
            "venue": "Methodology and computing in applied probability,",
            "year": 1999
        },
        {
            "authors": [
                "Reuven Y Rubinstein"
            ],
            "title": "Optimization of computer simulation models with rare events",
            "venue": "European Journal of Operational Research,",
            "year": 1997
        },
        {
            "authors": [
                "Ehsan Saleh",
                "Saba Ghaffari",
                "Timothy Bretl",
                "Luke Olson",
                "Matthew West"
            ],
            "title": "Learning from integral losses in physics informed neural networks",
            "venue": "arXiv preprint arXiv:2305.17387,",
            "year": 2023
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Sam Sinai",
                "Richard Wang",
                "Alexander Whatley",
                "Stewart Slocum",
                "Elina Locane",
                "Eric D Kelsic"
            ],
            "title": "Adalead: A simple and robust adaptive greedy search algorithm for sequence design",
            "year": 2010
        },
        {
            "authors": [
                "Sam Sinai",
                "Nina Jain",
                "George M Church",
                "Eric D Kelsic"
            ],
            "title": "Generative aav capsid diversification by latent interpolation",
            "year": 2021
        },
        {
            "authors": [
                "Jasper Snoek",
                "Hugo Larochelle",
                "Ryan P Adams"
            ],
            "title": "Practical bayesian optimization of machine learning algorithms",
            "venue": "Advances in neural information processing systems,",
            "year": 2012
        },
        {
            "authors": [
                "Niranjan Srinivas",
                "Andreas Krause",
                "Sham M Kakade",
                "Matthias Seeger"
            ],
            "title": "Gaussian process optimization in the bandit setting: No regret and experimental design",
            "venue": "arXiv preprint arXiv:0912.3995,",
            "year": 2009
        },
        {
            "authors": [
                "Brandon Trabucco",
                "Xinyang Geng",
                "Aviral Kumar",
                "Sergey Levine"
            ],
            "title": "Design-bench: Benchmarks for data-driven offline model-based optimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Nicholas C Wu",
                "Lei Dai",
                "C Anders Olson",
                "James O Lloyd-Smith",
                "Ren Sun"
            ],
            "title": "Adaptation in protein fitness landscapes is facilitated by indirect",
            "venue": "paths. Elife,",
            "year": 2016
        },
        {
            "authors": [
                "Kevin K Yang",
                "Zachary Wu",
                "Frances H Arnold"
            ],
            "title": "Machine-learning-guided directed evolution for protein engineering",
            "venue": "Nature methods,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Protein engineering is the problem of designing novel protein sequences with desired quantifiable properties, e.g., enzymatic activity, fluorescence intensity, for a variety of applications in chemistry and bioengineering (Fox et al., 2007; Lagass\u00e9 et al., 2017; Biswas et al., 2021). Protein engineering is approached by optimization over the protein fitness landscape which specifies the mapping between protein sequences and their measurable property, i.e., fitness. It is believed that the protein fitness landscape is extremely sparse, i.e., only a minuscule fraction of sequences have non-zero fitness, and rugged, i.e., peaks of \u201cfit\u201d sequences are narrow and separated from each other by deep valleys (Romero & Arnold, 2009), which greatly complicates the problem of protein design. Directed evolution is the most widely adopted technique for sequence design in laboratory environment (Arnold, 1998). In this greedy local search approach, first a set of variants of a naturally occurring (\"wild type\") sequence are tested for the desired property, then the variants with improved property form the starting points of the next round of mutations (selected uniformly at random) and thus the next round of sequences to be tested. This process is repeated until an adequately high level of desired property is achieved. Despite advances, this strategy remains costly and laborious, prompting the development of model-guided searching schemes that support more efficient exploration of the sequence space (Biswas et al., 2018; Brookes & Listgarten, 2018; G\u00f3mez-Bombarelli et al., 2018; Brookes et al., 2019; Angermueller et al., 2019; Sinai et al., 2020; Ren et al., 2022). In particular, there is emerging agreement that optimization schemes that utilize ML models of the sequence-fitness relationship, learned from training sets that grow in size as the optimization progresses, can furnish better candidates for the next round of testing, and thus accelerate optimization, as compared to model-free approaches such as Bayesian optimization (Mockus, 2012; Sinai et al., 2020). Our work belongs to this genre of model-based optimization for sequence-function landscapes.\n\u2217Both authors contributed equally.\nIntuitively, the success of fitness optimization depends on the extent to which functional proteins are represented in the experimentally derived data (training set) so that the characteristics of desired sequences can be inferred from them. Prior work has examined this challenge of \u201csparsity\" in fitness landscapes, proposing methods that use a combination of \u201cexploration\u201d and \u201cexploitation\u201d to search in regions of the space less represented in training data (Romero et al., 2013; Gonzalez et al., 2015; Yang et al., 2019; Hie & Yang, 2022). Optimization success also depends on the distribution of training samples in the sequence space, in particular on whether the desired functional sequences are proximal to and easily reachable from the frequent but low-fitness training samples. This second challenge of \u201cseparation\u201d (between the optima and training samples) in fitness landscape is relatively unexplored in the literature. In particular, it is not known how robust current search methods are when the optimum is located in a region that is poorly represented in the training set and is located relatively far (or separated due to rugged landscape) from the highly represented regions Figure 1. A real-world example of this is the problem of designing an enzyme for an unnatural target substrate, starting from the wild-type enzyme for a related natural substrate. Most variants of the wild type enzyme are not functional for the target substrate, thus the training set is sparse in sequences with non-zero fitness; furthermore, the rare variants that do have non-zero activity (fitness) for the target substrate are located relatively far from the wild-type and its immediate neighborhood that forms the bulk of the training set Figure 1 (rightmost panel).\nWe study the robustness of model-guided search schemes to the twin challenges of imbalance and separation in fitness landscape. We explore for the first time how search algorithms behave when training samples of high fitness are rare and separated from the more common, low-fitness training samples. (Here, separation is in the design or sequence space, not the fitness space.) Furthermore, given a fixed degree of separation, we investigate how the imbalance between the low- and high-fitness samples in the training set affect the performance of current methods. A robust algorithm should have consistent performance under varying separation and imbalance.\nTo this end, we propose a new model-based optimization (MBO) approach that uses VAE (Kingma & Welling, 2014) as its search model. The latent space of our VAE is explicitly structured by property (fitness) values of the samples (sequences) such that more desired samples are prioritized over the less desired ones and have higher probability of generation. This allows robust exploration of the regions containing more desired samples, regardless of the extent of their representation in the train set and of the extent of separation between low- and high-fitness samples in the train set. We refer to the proposed approach as a \u201cProperty-Prioritized Generative Variational Auto-Encoder\u201d (PPGVAE).\nOur approach is designed with the goal of obtaining improved samples in less number of MBO steps (less sampling budget), as desired in the sequence design problem. Methods that rely on systematic exploration techniques such as Gaussian processes (G\u00f3mez-Bombarelli et al., 2018) may not converge in small number of rounds (Srinivas et al., 2009); a problem that is exacerbated by higher dimensionality of the search space (Frazier, 2018; Djolonga et al., 2013). In general, optimization\nwith fewer MBO steps can be achieved by either 1) bringing more desired (higher fitness) samples closer together and prioritizing their exploration over the rest, as done in our approach, or 2) using higher weights for more desired samples in a weighted optimization setting (Brookes & Listgarten, 2018; Brookes et al., 2019; Gupta & Zou, 2019). Neither of these can be achieved by methods that condition the generation of samples on the the property values (Kang & Cho, 2018) or encode the properties as separate latent variables along with the samples (Guo et al., 2020; Chan et al., 2021). This is the key methodological gap in the state-of-the-art that is addressed by our new VAE technique for model-based optimization.\nThrough extensive benchmarking on real and semi-synthetic protein datasets we demonstrate that MBO with PPGVAE is superior to prior methods in robustly finding improved samples regardless of 1) the imbalance between low- and high-fitness training samples, and 2) the extent of their separation in the design space. Our approach is general and not limited to protein sequences, i.e., discrete design spaces. We further investigate MBO with PPGVAE on continuous designs spaces. In an application to physics-informed neural networks (PINN) (Raissi et al., 2019), we showcase that our method can consistently find improved high quality solutions, given PINN-derived solution sets overpopulated with low quality solutions separated from rare higher quality solutions. In section 2, MBO is reviewed. PPGVAE is explained in section 3 followed by experiments in section 4."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "Model Based Optimization. Given (x, y) pairs as the data points, e.g., protein sequence x and its associated property y (e.g., pKa value), the goal of MBO is to find x \u2208 X that satisfy an objective S regarding its property with high probability. This objective can be defined as maximizing the property value y, i.e., S = {y|y > ym} where ym is some threshold. Representing the search model with p\u03b8(x) (with parameters \u03b8), and the property oracle as p\u03b2(y|x) (with parameters \u03b2), MBO is commonly performed via an iterative process which consists of the following three steps at iteration t (Fannjiang & Listgarten, 2020):\n1. Taking K samples from the search model, \u2200i \u2208 {1, ...,K} xti \u223c p\u03b8t(x); 2. Computing sample-specific weights using a monotonic function f which is method-specific:\nwi := f(p\u03b2(y t i \u2208 S|xti)); (1)\n3. Updating the search model parameters via weighted maximum likelihood estimation (MLE):\n\u03b8t+1 = argmax \u03b8 K\u2211 i=1 wi log(p\u03b8(x t i)). (2)\nThe last step optimizes for a search model that assigns higher probability to the data points satisfying the property objective S, i.e., where p\u03b2(y \u2208 S|x) is high. Prior work by (Brookes & Listgarten,\n2018) (DbAS) and (Brookes et al., 2019) (CbAS) have explored variants of weighting schemes for the optimization in the second step. Reward-weighted regression (RWR) (Peters & Schaal, 2007) and CEM-PI (Snoek et al., 2012) have additionally been benchmarked by CbAS, each providing a different weighting scheme. RWR has been used for policy learning in Reinforcement Learning (RL) and CEM-PI maximizes the probability of improvement over the best current value using the cross entropy method (Rubinstein, 1997; 1999).\nCommon to all these methods is that weighted MLE could suffer from reduced effective sample size. In contrast, our PPGVAE does not use weights. Instead, it assigns higher probability to the high fitness (more desired) data points by restructuring the latent space. Thus, allowing for the utilization of all samples in training the generative model (see Appendix A C).\nExploration for Sequence Design. In addition to weighting based generative methods, model-based RL (Angermueller et al., 2019) (Dyna PPO) and evolutionary greedy approaches (Sinai et al., 2020) (AdaLead) have been developed to perform search in the sequence space for improving fitness. More recently, (Ren et al., 2022) (PEX) proposed an evolutionary search that prioritizes variants with improved property which fall closer to the wild type sequence."
        },
        {
            "heading": "3 PROPERTY-PRIORITIZED GENERATIVE VARIATIONAL AUTO-ENCODER",
            "text": "To prioritize exploration and generation of rare, high-fitness samples, our PPGVAE uses property (fitness) values to restructure the latent space. The restructuring enforces samples with higher property to lie closer to the origin than the ones with lower property. As the samples with higher property lie closer to the origin, their probability of generation is higher under the VAE prior distribution N (0, I). Representing the encoder and its parameters with Q and \u03b8, the structural constraint on N samples is imposed by\n\u2200(\u00b5i\u03b8, \u00b5 j \u03b8), i, j \u2208 {1, ..., N} log(Pr(\u00b5 i \u03b8))\u2212 log(Pr(\u00b5 j \u03b8)) = \u03c4(yi \u2212 yj), (3)\nwhere \u00b5i\u03b8 = Q\u03b8(xi) and yi are the latent space representation and property value of sample xi, respectively. The probability of the encoded representation Pr(\u00b5i\u03b8) is computed w.r.t. the VAE prior distribution N (0, I) over the latent space, i.e., Pr(\u00b5i\u03b8) \u221d exp( \u2212\u00b5i\u03b8 T \u00b5i\u03b8 2 ).\nIntuitively, if higher values of property y are desired, then yj \u2264 yi results in sample i getting mapped closer to the origin. This results in a higher probability of generating sample i than sample j. The extent of prioritization between each pair of samples is controlled by the hyper-parameter \u03c4 , often referred to as the temperature. The structural constraint is incorporated into the objective of a vanilla VAE as a relationship loss that should be minimized. This loss is defined as,\nLr \u221d \u2211 i,j ||(log(Pr(\u00b5i\u03b8))\u2212 log(Pr(\u00b5 j \u03b8)))\u2212 \u03c4(yi \u2212 yj)|| 2 2. (4)\nCombined with the vanilla VAE, the final objective of PPGVAE to be maximized is,\nEz\u223cQ(.|x)[log(P (x|z))\u2212DKL(Q(z|x)\u2225P (z))]\u2212 \u03bbr \u03c42 Lr, (5)\nwhere \u03bbr is a hyper-parameter controlling the extent to which the relationship constraint is enforced. Here, we abused the notation and wrote Q(z|x) for Pr(z|Q(x)). To understand the impact of the structural constraint on the mapping of samples in the latent space, we define qi := log(Pr(\u00b5i\u03b8))\u2212\u03c4yi. Also, assume that the samples are independent and identically distributed (i.i.d.). Then minimizing the relationship loss can be rewritten as\nLr \u221d Eqi,qj ((qi \u2212 qj)2) = Eqi,qj (((qi \u2212 E(qi))\u2212 (qj \u2212 E(qj)))2). (6) Using the i.i.d. assumption this is simplified to,\nLr \u221d 2Var(qi) = 2Var(log(P (\u00b5i\u03b8))\u2212 \u03c4yi)) = 2Var(\u2212 \u00b5i\u03b8 T \u00b5i\u03b8\n2 \u2212 \u03c4yi). (7)\nTherefore, minimizing Lr is equivalent to minimizing the variance. This is equivalent to setting the random variable in RHS of Equation 7 to a constant value C; \u2200i : \u00b5i\u03b8 T \u00b5i\u03b8 = 2(C \u2212 \u03c4yi). This implies the distribution of samples with the same property value to be on the same sphere. The sphere lies closer to the origin for the samples with higher property values. This ensures that higher property samples have higher probability of generation under the VAE prior N(0, I), while allowing for all samples to fully contribute to the optimization.\nTo demonstrate the impact of relationship loss on the latent space, PPGVAE, with a two-dimensional latent space, was trained on a toy MNIST dataset (Deng, 2012). The dataset contains synthetic property values that are monotonically decreasing with the digit class. Also, samples from the digit zero with the highest property are rarely represented (see Appendix D.3). Strong enforcement of the relationship loss, using a relatively large constant \u03bbr, aligns samples from each class on circles whose radius increases as the sample property decreases. Soft enforcement of the relationship loss, by gradually decreasing \u03bbr, makes the samples more dispersed, while roughly preserving their relative distance to the origin (Figure 2)."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We will compare the performance of MBO with PPGVAE to the baseline algorithms that use generative models optimized with weighted MLE (CbAS, RWR, CEM-PI (Brookes et al., 2019)) for search in discrete (e.g., protein sequence) and continuous design spaces. For sequence design experiments, we additionally included AdaLead as a baseline.\nIn all optimization tasks, we 1) provide a definition of separation between less and more desired samples in the design space X , 2) study the impact of varying imbalance between the representation of low- and high-fitness (property value) samples in the training set, given a fixed separation degree, and 3) study the impact of increasing separation. The ground truth property oracle was used in all experiments. The performance is measured by \u2206Ymax representing the relative improvement of the\nhighest property found by the model to the highest property in the train set, i.e., initial set at the beginning of MBO (see Appendix D for further details)."
        },
        {
            "heading": "4.1 GAUSSIAN MIXTURE MODEL",
            "text": "We use a bimodal Gaussian mixture model (GMM) as the property oracle, in which the relationship between x and property y is defined as, y = \u03b11 exp(\u2212(x\u2212 \u00b51)2/2\u03c321) + \u03b12 exp(\u2212(x\u2212 \u00b52)2/2\u03c322). We choose the second Gaussian as the more desired mode by setting \u03b12 > \u03b11 (Figure 3). Here, separation is defined as the distance between the means of the two Gaussian modes (\u2206\u00b5). Larger values of \u2206\u00b5 are associated with higher separation. For each separation level, the train sets were generated by taking N samples from the less desired mode, and taking \u03c1N (imbalance ratio \u03c1 \u2264 1) samples from the more desired mode (see Appendix D.3).\nFor a fixed separation level, we compared the performance of PPGVAE and baseline methods for varying imbalance ratios. The relative improvements achieved by PPGVAE are consistently higher than all other methods and are robust to the imbalance ratio. Other methods achieve similar improvements when high-fitness samples constitute a larger portion of the train set (larger \u03c1). This happens at a smaller \u03c1 for lower separation levels (smaller \u2206\u00b5), indicating that the impact of imbalance is offset by a smaller separation between the optimum and the dominant region in training data (see Figure 3, Figure A4).\nWe then compared the relative improvement aggregated over all imbalance ratios, as the separation level increases. All methods perform well for low separation levels. PPGVAE stays robust to the degree of separation, whereas the performance of others drops by increasing separation (see Figure 3 top right). The difficulties encountered by other generative methods at higher separation levels is due to the difference between the reconstruction of low- and high-fitness samples. As \u2206\u00b5 increases, the more desired (high fitness) samples get mapped to a farther locality than the less desired ones. This makes the generation of more desired samples less likely under the VAE prior distribution N (0, I). This is exacerbated when more desired samples are rarely represented (small imbalance ratio) in the train set. For smaller \u2206\u00b5, more desired samples get mapped to a similar locality as less desired ones, regardless of their extent of representation in the train set. PPGVAE stays robust to the the imbalance ratio and extent of separation, as it always prioritizes generation and exploration of more desired samples over the less desired ones by mapping them closer to the latent space origin. Similar explanation can be provided for the rest of optimization tasks benchmarked in this study."
        },
        {
            "heading": "4.2 REAL PROTEIN DATASET",
            "text": "To study the impact of separation, real protein datasets with property (activity) measurements more broadly dispersed in the sequence space are needed. Such datasets are rare among the experimental studies of protein fitness landscape (Johnston et al., 2023), as past efforts have mostly been limited to optimization around a wild type sequence. We chose the popular AAV (Adeno-associated virus) dataset (Bryant et al., 2021) in ML-guided design (Sinai et al., 2021; Mikos et al., 2021). This dataset consists of virus viability (property) measurements for variants of AAV capsid protein, covering the span of single to 28-site mutations, thus presenting a wide distribution in sequence space.\nThe property values were normalized to [0, 1] range. Threshold of 0.5 was used to define the set of low (y < 0.5) and high (y > 0.5) fitness (less and more desired respectively) mutants in the library. To define a proxy measure of separation, we considered the minimum number of mutated sites s in the low-fitness training samples. A larger value of s is possible only when the low-fitness samples are farther from the high-fitness region, i.e., at higher separation (see Figure 4). For a fixed separation s, the train sets were generated by taking N samples from the low-fitness mutants containing at least s number of mutations, and \u03c1N (\u03c1 < 1) samples from the high-fitness mutants (regardless of the number of mutations) (see Appendix D).\nGiven a fixed separation level, PPGVAE provides robust relative improvements, regardless of the imbalance ratio (Figure 4, Figure A5). CEM-PI is the most competitive with PPGVAE for low separation levels, however its performance decays for small imbalance ratios as separation increases (see Figure A5). The performance of other methods improve as the high-fitness samples represent a higher portion of the train set.\nNext, we compared the performance of methods, aggregated over all imbalance ratios, as the separation level increases. PPGVAE, is the most robust method to varying separation. CEM-PI is\nthe most robust weighting-based generative method. Its performance is similar to PPGVAE for low separation levels and degrades as separation increases.\nAs it is desired to achieve improved samples with less sampling budget, we also studied the impact of varying sample generation budget per MBO step, given a fixed separation level. As expected, the performance increases by increasing the sampling budget for both CEM-PI and PPGVAE. Furthermore, on average PPGVAE performs better than CEM-PI for all sampling budgets (Figure 4). This is the direct result of prioritizing the generation and exploration of high-fitness samples relative to the low-fitness ones in the latent space of PPGVAE."
        },
        {
            "heading": "4.3 SEMI-SYNTHETIC PROTEIN DATASETS",
            "text": "Next, we used two popular protein datasets GB1 (Wu et al., 2016) and PhoQ (Podgornaia & Laub, 2015) with nearly complete fitness measurements on variants of four sites. However, these data sets exhibit a narrow distribution in sequence space (at most mutations), and are not ideal to study the effect of separation. We thus transformed the landscape of these proteins to generate a more dispersed dataset of mutants on which we could control for the separation of less and more desired variants. In this transformation, first a certain threshold on the property was used to split the dataset into two sets of low and high fitness mutants (see Appendix D). Second, a specific sequence of length L was appended to high fitness mutants, while a random sequence of the same length was appended to the low fitness ones. Here, the length of the appended sequence determines the extent of separation. Higher separation is achieved by larger values of L and makes the optimization more challenging. Note that the separation is achieved by changing the distribution of samples in the design space X , while keeping the property values unchanged. For each separation level, train sets were generated by taking N samples from the low fitness mutants and \u03c1N samples from the high fitness mutants (see Figures 5 6, and Figure A3).\nFor a fixed separation level, the performance of all methods improve as high fitness samples constitute a higher portion of the train set, i.e., higher imbalance ratio (Figure 5 6). PPGVAE is more robust to the variation of imbalance ratio and it is significantly better than others when high fitness samples are very rare. Same observations hold for all separation levels studied (see Figures A6 A7). Similar as before, CEM-PI is the most competitive with PPGVAE for low separation; however as the separation increases its performance decays. Furthermore, the reduction of sampling budget does not affect the performance of PPGVAE as much as CEM-PI (Figures 5 6)."
        },
        {
            "heading": "4.4 IMPROVING PINN-DERIVED SOLUTIONS TO THE POISSON EQUATION",
            "text": "Our method can also be used in continuous design spaces. We define the task of design as finding improved solutions given a training set overpopulated with low quality PINN-derived solutions. Details on train set generation and separation definition are covered in Appendix D.3. Similar conclusions hold for the robustness of PPGVAE to the separation level and imbalance ratio (Figure 7).\nCommon in all optimization tasks, PPGVAE achieves maximum relative improvement in less number of MBO steps than others (see Figures A9 A10 A11 A12). Characteristics of the latent space and sample generation have been studied for PPGVAE, and contrasted with prior generative approaches including (G\u00f3mez-Bombarelli et al., 2018) in Appendix A."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We proposed a robust approach for design problems, in which more desired regions are rarely explored and separated from less desired regions with abundant representation to varying degrees. Our method is inherently designed to prioritize the generation and interpolation of rare more-desired samples which allows it to achieve improvements in less number of MBO steps and less sampling budget. As it stands, our approach does not use additional exploratory mechanism to achieve improvements, however it could become more stronger by incorporating them. It is also important to develop variants of our approach that are robust to oracle uncertainties, and study the extent to which imposing the structural constraint can be restrictive in some design problems."
        },
        {
            "heading": "6 ACKNOWLEDGEMENTS",
            "text": "This research was funded by Molecule Maker Lab Institute: an AI research institute program supported by National Science Foundation under award No. 2019897. This work utilized resources supported by 1) the National Science Foundation\u2019s Major Research Instrumentation program, grant No. 1725729 (Kindratenko et al., 2020), and 2) the Delta advanced computing and data resource which is supported by the National Science Foundation (award OAC 2005572) and the State of Illinois."
        },
        {
            "heading": "A CHARACTERISTICS OF THE LATENT SPACE AND SAMPLE GENERATION",
            "text": "To contrast PPGVAE with prior generative based methods, all methods were trained on the toy MNIST dataset of Figure 2. In weighting based methods, there is less distinction among the mapping regions of different classes, as the weighting becomes more extreme (Figure A1). Extreme weighting makes less samples contribute to the optimization, thus affecting the diversity in sample generation.\nCEM-PI, CbAS and RWR have the most to least extreme weighting. In contrast to weighting based approaches, PPGVAE and Bombarelli have more distinct mapping of classes. Thus, sample generation using PPGVAE has more diversity. By increasing the standard deviation (\u03c3s) of the sampling prior N (0, \u03c3sI), PPGVAE is capable of generating all types of digits, whereas the least extreme weighting method (RWR) generates three types of digits only (Figure A2).\nIt is easily seen that the organization of points in the latent space of PPGVAE is significantly different from Bombarelli\u2019s. PPGVAE enforces the samples with higher properties to lie closer to the origin, whereas they are just separated by digit and ordered by property in Bombarelli\u2019s. By design, PPGVAE generates improved samples with higher probability, while exploration mechanisms need to be carefully designed for Bombarelli\u2019s. Finally, by sampling from VAE prior N (0, I), i.e., \u03c3s = 1, PPGVAE generates the top two highest property classes as well as their interpolations, more frequently than others. Optimization over the latent space of Bombarelli\u2019s barely produces samples from the top two highest property classes."
        },
        {
            "heading": "B RELATED WORK",
            "text": "Machine learning approaches have been developed to improve the design of DNA, molecules and proteins with certain properties. Early work in ML-assisted design (G\u00f3mez-Bombarelli et al., 2018), structures the samples in the latent space by joint training of a property predictor on the latent space. However, further systematic exploration mechanisms on the latent space are required to find improved samples. Variants of weighting based MBO techniques have also been proposed to facilitate the problem of protein design (Brookes & Listgarten, 2018; Gupta & Zou, 2019; Brookes et al., 2019). CbAS (Brookes et al., 2019) proposed a weighting scheme which prevents the search model from exploring regions of the design space for which property oracle predictions are unreliable. CbAS (Brookes et al., 2019) was built on an adaptive sampling approach (Brookes & Listgarten, 2018) that leverages uncertainty in the property oracle when computing the weights for MBO. Reward\nPP GV\nAE (H\nar d\nCo ns\ntra in\nt)\ns = 1\nPP GV\nAE (H\nar d\nCo ns\ntra in\nt)\ns = 2\nPP GV\nAE (H\nar d\nCo ns\ntra in\nt)\ns = 4\nCb AS\ns = 1\nCb AS\ns = 2\nCb AS\ns = 4\nRW R\ns = 1\nRW R\ns = 2\nRW R\ns = 4\nCE M\n-P I\ns = 1\nCE M\n-P I\ns = 2\nCE M\n-P I\ns = 4\nBo m\nba re\nlli\nFigure A2: Samples generated from different methods trained on the toy MNIST dataset (Figure A1). For all methods except Bombarelli (G\u00f3mez-Bombarelli et al., 2018), normal sampling distribution N (0, \u03c3sI) with varying standard deviation \u03c3s \u2208 {1, 2, 4} was used. For Bombarelli, the optimization on the latent space was performed with a different starting point for each generated sample.\nNotation Description\nX The design space\nxi Sample i from the design space (xi \u2208 X )\nyi Property value associated with xi\nwi Weight associated with xi\nQ\u03b8 Encoder with parameters \u03b8\n\u00b5i\u03b8 Encoded representation of xi\nPr(\u00b5i\u03b8) Probability of the encoded representation w.r.t. the VAE pior\nLr Relationship loss\n\u03bbr Coefficient of the relationship constraint\n\u03c4 Temperature of relationship constraint\np\u03b2(y|x) Property oracle\np\u03b8(x) Search model with parameters \u03b8\nNs Number of samples generated per MBO step\n\u03c1 Relative proportion of more-desired to less-desired samples in train set\nNeff Effective sample size in weighted MLE\nTable A1: Mathematical notations used in the paper.\nWeighted Regression (RWR) (Peters & Schaal, 2007) (benchmarked by CbAS) uses weights that do not take into account oracle uncertainty. Not requiring a differentiable oracle is common to these methods. On the other hand, a simple baseline proposed by (Trabucco et al., 2022) requires learning a differentiable property oracle that is used to find design samples by gradient ascent. In an effort to avoid failures due to imperfect oracles, (Kumar & Levine, 2020) proposed learning an inverse map from property space to design space and search for optimal property during optimization instead. Protein design has also been formulated as sequential decision-making problem (Angermueller et al., 2019), in which proximal policy optimization (PPO) (Schulman et al., 2017) has been used to search for design sequences. Recently, an evolutionary greedy approach has been shown to be competitive with prior algorithms (Brookes & Listgarten, 2018; Brookes et al., 2019; Angermueller et al., 2019). The latest work on protein design (Ren et al., 2022) (PEX), focuses on exploring the rugged landscape close to the wild-type sequence, to find high fitness sequences with low mutation counts.\nIn our benchmark for sequence design, we only included AdaLead as a baseline, selected over alternatives as it has been demonstrated to have superior performance to Dyna PPO (Angermueller et al., 2019). PEX (Ren et al., 2022) was not included, as it is specifically designed to perform a local search starting from the wild type, while our interest in the more challenging scenario of optima being distal to the wild type."
        },
        {
            "heading": "C EFFECTIVE SAMPLE SIZE IN WEIGHTED MLE",
            "text": "Common to all weighting-based generative methods is that weighted MLE could suffer from reduced effective sample size, which leads to estimation of parameters \u03b8t+1 with higher variance and higher bias in extreme cases. This is inevitable in train sets with severe imbalance between the abundant undesired (e.g., zero property) and rare desired (e.g., nonzero property) samples. In contrast, PPGVAE does not use weights. Instead, it assigns higher probability to the more desired data points by restructuring the latent space. Thus, allowing for the utilization of all samples in training the generative model, regardless of the extent of imbalance between less and more desired samples.\nDataset Encoder Architecture\nProtein indim\u2192 64 \u2192 LReLU\u2192 20\nPINN indim\u2192 64 \u2192 LReLU\u2192 10\nMNIST indim\u2192 512 \u2192 ReLU\u2192 256\u2192 ReLU\u2192 2\nGMM indim\u2192 64 \u2192 LReLU\u2192 64\u2192 LReLU\u2192 2\nTable A2: VAE architecture used for each benchmark dataset. Encoder and decoder have symmetrical architecture. All generative based methods share the same architecture\nIn imbalanced datasets, sample weights (wi) are typically uneven. As an example, assume a dataset of size 100 with five positive (desired property values), and 95 negative (undesired) samples. One weighting scheme can assign non-zero weights to the five desired samples and zero weights otherwise. Therefore, only five out of 100 samples contribute to the objective in Equation 2. This leads to higher bias and variance in the maximum likelihood estimator \u03b8t+1 (MLE). Next, we present the same argument mathematically.\nRepresenting the log-likelihood log(p\u03b8(xi)) of the generative model with li, Equation 2 can be rewritten as maximizing\nl = K\u2211 i=1 wili. (A8)\nAssuming \u2211K\ni=1 wi = 1 and i.i.d. samples, the effective sample size Neff (Kish & Frankel, 1974) can be defined such that\nVar(l) = 1\nNeff Var(li). (A9) According to Equation A8, Neff = ( \u2211K i=1 w 2 i )\n\u22121. It can be proved that 1 \u2264 Neff \u2264 K where the equality for the lower and upper bound holds at\nNeff = K, \u2200i, wi = 1\nK , and\nNeff = 1, wj = 1, wi \u0338=j = 0. (A10)\nAs mentioned earlier, uneven weights are expected in imbalanced datasets. As the weights become more uneven, Neff approaches its lower bound. Therefore, with imbalanced datasets, Neff tends to drop and Var(l) increases (Owen, 2013). This in turn increases the estimation bias and variance of of the MLE \u03b8t+1 (Firth, 1993; Ghaffari et al., 2022). Both estimation bias and variance are O(N\u22121eff ). Our search model does not require weighting of the samples to prioritize the generation of some over the others. Instead, it directly uses the property values to restructure the latent space such that samples with better properties have a higher chance of being generated and interpolated. For this reason PPGVAE has Neff = K, which makes it robust to the issues associated with parameter estimation in weighting-based MBO techniques."
        },
        {
            "heading": "D DETAILS OF THE EXPERIMENTS",
            "text": "D.1 MBO SETTINGS AND IMPLEMENTATION DETAILS\nWe performed 10 rounds of MBO on the GMM benchmark and 20 rounds of MBO on the rest of the benchmark datasets. In all experiments temperature (\u03c4 ) was set to five for PPGVAE with no further tuning. We used the implementation and hyper-parameters provided by (Brookes et al., 2019), for CbAS, Bombarelli, RWR, and CEM-PI methods. The architecture of VAE was the same for all methods (Table A2). The formula to compute the weights for each weighting-based method are included in the Appendix of (Brookes et al., 2019).\nDataset Measured Property\nSeparation Criterion\nSeparation Quantities\nImbalance Ratio (\u03c1)\nGMM Bimodal Gaussian function\nDistance between the two modes\n(\u2206\u00b5)\n4,6,8,10,12 0.05, 0.1, 0.2, 0.4, 0.8, 1\nPINN -log(wMSE) Property percentile for more desired\nsamples\n(30, 40), (40, 50), (50, 60), (60, 70), (70, 80) 0.05, 0.1, 0.2, 0.4, 0.8\nGB1 Folded protein enrichment\nLength of the appended sequence\n3,4,5,8 0.0125, 0.025, 0.05, 0.1, 0.2,\n0.4, 0.8\nPhoQ Yellow fluorescent\nprotein level\nLength of the appended sequence\n3,4,6,8 0.0125, 0.025, 0.05, 0.1, 0.2,\n0.4, 0.8\nAAV Capsid viability Minimum number of\nmutations for samples with less desired property\n(y < 0.5)\n6,8,10,12,15 0.0125, 0.025, 0.05, 0.1, 0.2,\n0.4, 0.8\nTable A3: Settings used in train set generation for each benchmark dataset.\nThe ground truth property oracle was used in all experiments, which is equivalent to limiting the search space to the sequences with measured properties in the protein datasets. The performance \u2206Ymax is reported with 95% bootstrap confidence interval.\nIn AdaLead, The oracle query size and the experiment batch size were both set to the number of samples generated per MBO step (Ns). This was to run AdaLead in a comparable setting to other weighting-based MBO approaches. Ground-truth oracle was used in all experiments, i.e., the search space was limited to the samples existing in the dataset.\nFor each setting of imbalance ratio and separation level, optimization was performed with 20 different random seeds. In AdaLead, the starting sequence was randomly selected from the initial train set for each seed. We noticed that AdaLead performance is highly dependant on where its search starts in the sequence space. This justifies the high variability in its performance. This is a common problem to all approaches based on evolutionary search, as it may not be known a priori where to start the search.\nD.2 PROTEIN DATASETS\nAAV: The dataset contains variants of the protein located in the capsid of AAV virus along with their viability measurements. It consists of roughly 284K variants with single to 28 sites mutations. The properties were normalized into range [0, 1]. The dataset was obtained from (Dallago et al., 2021).\nGB1: It contains the empirical fitness landscape for the binding of protein G domain B1 to an antibody. Enrichment of the folded protein bound to the antibody was defined as fitness. Fitness was measured for 149,361 sequences for variants of amino acids at four sites. The fitness was normalized to range [0, 1] in this paper. The dataset is overpopulated with low or zero-fitness sequences. The sequences along with their properties were obtained from (Qiu & Wei, 2022).\nFigure A3: GB1 transformed landscapes and train set examples. Transformed landscapes with appended sequence of length three and six are shown in top left and top right panels, respectively. Examples of train sets taken from each landscape are shown in bottom left (low separation) and bottom right (high separation) panels, respectively.\nPhoQ: It is a combinatorial library of four amino acid sites located at the interface between PhoQ kinase and sensor domains. Signaling of a two-component regulatory system was measured by yellow fluorescent protein (YFP) levels, i.e., fitness. The fitness landscape contains 140,517 measured sequences. The dataset is overpopulated with low or zero-fitness sequences. The fitness was normalized to range [0, 1] in this paper. The sequences along with their properties were obtained from (Qiu & Wei, 2022).\nD.3 GENERATION OF TRAIN SETS WITH VARYING IMBALANCE AND SEPARATION\nIn each dataset, we specify the definition of less and more desired samples as well as the separation criterion in the design space X . Table A3 includes the settings used to generate train sets in all benchmark datasets.\nGMM: The property oracle is a bimodal GMM in which the second mode has a higher peak (higher mode) than the first mode (lower mode). Each mode is represented by a triplet (\u00b5i, \u03c3i, \u03b1i) where \u03b1i determines the peak height and i is the mode index. The triplet was set to (0, 0.25, 1) and (\u00b52, 1, 2.5) for the lower and higher modes, respectively. Mean of the second mode is variable as it determines the separation level.\nTrain sets consisted of less and more desired samples, which were taken from the two modes. N samples were taken from the lower mode using N (0, 0.6) distribution. \u03c1N samples representing the higher mode were taken uniformly from range [\u00b52 + \u03c32/2, \u00b52 + \u03c32].\nThe separation in the design space was defined as the difference between the means of two modes \u2206\u00b5. Higher \u2206\u00b5 is associated with higher separation.\nAAV: The entire AAV dataset consists of 284K samples (Bryant et al., 2021), which are split into two sets of \"sampled\" and \"designed\" variants (Dallago et al., 2021). We only used the sampled variants in this study. Threshold of 0.5 on the viability scores was used to define the set of more desired (> 0.5) and less desired (< 0.5) variants.\nTrain sets consisting of less and more-desired samples were generated by, taking N samples from the less-desired variants, whose mutation count was more than a specified threshold, and taking \u03c1N samples from the more-desired variants, whose properties fall in the (5,10) percentile of the property values.\nThe separation is determined by the minimum number of mutations present in the less-desired samples. The separation of less and more-desired samples in the train set increases as the minimum number of mutations increases.\nGB1: To study the impact of separation, the sequences of GB1 dataset were transformed to have a more dispersed coverage of the landscape in the design space. First, the property values were normalized into range [0, 1]. Threshold of 0.001 on the property was used to define the set of less desired (< 0.001) and more-desired variants (> 0.001). Then, the sequences of less-desired variants were appended with random sequence of length L, whereas more-desired variants were appended with a specific sequence of the same length. Here, the length of the appended sequence specifies the degree of separation. Larger length is associated with higher separation.\nTrain sets were generated by sampling N samples from the less-desired and \u03c1N samples from the more-desired variants. The transformed landscape of GB1 associated with low and high separation, along with examples of its train sets are shown in Figure A3.\nPhoQ: The same procedure as GB1 was used to generate the train sets.\nPINN: We used a dataset of PINN-derived solutions to the Poisson equation for the potential of three point charges located diagonally on a 2D plane (Saleh et al., 2023). The solutions were pooled from a batch of PINNs trained with different seeds at different training epochs. Negative log weighted MSE (wMSE) between the PINN-derived solution and the analytical solution was used as the property value. Note that in practice, the exact average loss can be a proxy of the solution quality without the analytical solution. Higher quality solutions have higher properties.\nIn generating the train sets, N low fitness samples were taken from (0, 15) percentile of the property values, whereas \u03c1N high fitness samples were taken from a specified range of property percentiles (P1, P2) | P1 \u2265 30. Higher values of P1 are associated with higher separation levels. Note that in this case, separation of samples by property values is concordant with their separation in the design space X . See Table A3 for the values of (P1, P2) that have been tested. In practice, as accurate solutions can be sporadic with stochastic optimizers, PPGVAE can interpolate higher-quality solutions given imbalanced sets of solutions.\nToy MNIST: This dataset was used to demonstrate the latent space of PPGVAE and other methods, as well as their sampling generation characteristics. The dataset has rare representation of zero digits relative to the other digits (imbalance ratio \u03c1 = 0.01). Samples belonging to the digit class C have property values distributed as N (10\u2212 C, 0.01)."
        },
        {
            "heading": "E ADDITIONAL PLOTS",
            "text": "E.1 STUDYING THE IMPACT OF IMBALANCE RATIO PER SEPARATION LEVEL\nFor a given separation, the performance improves as the imbalance ratio increases. This is expected as the more-desired samples constitute a higher portion of the train set. As separation level increases, the task of optimization becomes harder. Therefore, other methods become mostly ineffective for low imbalance ratios, and only show improvements for higher imbalance ratios. This behavior is consistently seen in all benchmark tasks.\n0\n0.01\n0.03\n0.1\n0.3\nY m ax\nLowest Separation Low Separation\n0.1 0.3 1\nMedium Separation\n0.1 0.3 1 0\n0.01\n0.03\n0.1\n0.3\nY m ax\nHigh Separation\n0.1 0.3 1\nHighest Separation PPGVAE CbAS CEM-PI RWR\nFigure A4: Performance vs imbalance ratio for each separation level in the GMM benchmark.\n0 0.01 0.03\n0.1 0.3\n1\nY m ax\nLowest Separation Low Separation Medium Separation\n0.01 0.03 0.1 0.3 1 0\n0.01 0.03\n0.1 0.3\n1\nY m ax\nHigh Separation\n0.01 0.03 0.1 0.3 1\nHighest Separation PPGVAE AdaLead CbAS CEM-PI RWR\nFigure A5: Performance vs imbalance ratio for each separation level in the AAV benchmark.\n0.01 0.03 0.1 0.3 1\n0 0.01\n0.03\n0.1\n0.3\n1\nY m ax\nLowest Separation\n0.01 0.03 0.1 0.3 1\nLow Separation\n0.01 0.03 0.1 0.3 1\nHigh Separation\n0.01 0.03 0.1 0.3 1\nHighest Separation PPGVAE AdaLead CbAS CEM-PI RWR\nFigure A6: Performance vs imbalance ratio for each separation level in the GB1 benchmark.\n0.01 0.03 0.1 0.3 1\n0 0.01\n0.03\n0.1\n0.3\n1\nY m ax\nLowest Separation\n0.01 0.03 0.1 0.3 1\nLow Separation\n0.01 0.03 0.1 0.3 1\nHigh Separation\n0.01 0.03 0.1 0.3 1\nHighest Separation PPGVAE AdaLead CbAS CEM-PI RWR\nFigure A7: Performance vs imbalance ratio for each separation level in the PhoQ benchmark.\n0 0.01 0.03 0.1 0.3\n1\nY m ax\nLowest Separation Low Separation\n0.1 0.3 1\nMedium Separation\n0.1 0.3 1 0\n0.01 0.03 0.1 0.3\n1\nY m ax\nHigh Separation\n0.1 0.3 1\nHighest Separation PPGVAE CbAS CEM-PI RWR\nFigure A8: Performance vs imbalance ratio for each separation level in the PINN benchmark.\nE.2 STUDYING THE CONVERGENCE RATE FOR DIFFERENT IMBALANCE RATIOS\nFor each benchmark task, we looked at the progress of each method as number of MBO steps increases. For higher imbalance ratios, all methods have faster convergence relative to the low imbalance ratios. Furthermore, our PPGVAE consistently has the highest convergence rate to the highest improvement. This is expected, as PPGVAE prioritizes the interpolation and generation of more desired samples, regardless of the extent of their representation in the train set.\n0 0.01\n0.03\n0.1\n0.3\nY m ax\n= 0.05 = 0.1 = 0.2\n0 2 4 6 8 10 MBO Step\n0 0.01\n0.03\n0.1\n0.3\nY m ax\n= 0.4\n0 2 4 6 8 10 MBO Step\n= 0.8\n0 2 4 6 8 10 MBO Step\n= 1.0\nPPGVAE CbAS CEM-PI RWR\nFigure A9: Performance vs the number of MBO steps for different imbalance ratios in GMM benchmark. Separation level is set to medium.\n0\n0.01 0.03 0.1 0.3\n1\nY m ax\n= 0.0125 = 0.025 = 0.05 = 0.1\n0 4 8 12 16 20 MBO step\n0\n0.01 0.03 0.1 0.3\n1\nY m ax\n= 0.2\n0 4 8 12 16 20 MBO step\n= 0.4\n0 4 8 12 16 20 MBO step\n= 0.8 PPGVAE CbAS CEM-PI RWR\nFigure A10: Performance vs the number of MBO steps for different imbalance ratios in AAV benchmark. Separation level is set to low.\nE.3 TEMPERATURE SENSITIVITY AND TRAINING ON HIGH FITNESS SAMPLES\nTo study the sensitivity of PPGVAE performance to the temperature, the GMM experiments with the lowest imbalance ratio and the highest separation (most challenging scenario) were performed with varying temperatures (\u03c4 ) Figure A13. The performance is almost the same for log10(\u03c4) \u2208 [\u22121, 1]. Also, the sensitivity to temperature decreases as the number of MBO steps increases.\nWe also repeated the AAV experiments for CEM-PI in which only high fitness samples were used as the initial train set (CEM-PI/High) Figure A13. Aggregated performance over all imbalance ratios for PPGVAE is better than both CEM-PI and CEM-PI/High. This demonstrates the importance of including all samples in the training using PPGVAE. Furthermore, CEM-PI/High has better performance than CEM-PI for higher separation, showing that filtering the samples might be beneficial for weighting based approaches as the optimization task gets harder by separation criterion."
        }
    ],
    "title": "ROBUST MODEL-BASED OPTIMIZATION FOR CHALLENG- ING FITNESS LANDSCAPES",
    "year": 2024
}