{
    "abstractText": "Human motion driven control (HMDC) is an effective approach for generating natural and compelling robot motions while preserving high-level semantics. However, establishing the correspondence between humans and robots with different body structures is not straightforward due to the mismatches in kinematics and dynamics properties, which causes intrinsic ambiguity to the problem. Many previous algorithms approach this motion retargeting problem with unsupervised learning, which requires the prerequisite skill sets. However, it will be extremely costly to learn all the skills without understanding the given human motions, particularly for high-dimensional robots. In this work, we introduce CrossLoco, a guided unsupervised reinforcement learning framework that simultaneously learns robot skills and their correspondence to human motions. Our key innovation is to introduce a cycle-consistency-based reward term designed to maximize the mutual information between human motions and robot states. We demonstrate that the proposed framework can generate compelling robot motions by translating diverse human motions, such as running, hopping, and dancing. We quantitatively compare our CrossLoco against the manually engineered and unsupervised baseline algorithms along with the ablated versions of our framework and demonstrate that our method translates human motions with better accuracy, diversity, and user preference. We also showcase its utility in other applications, such as synthesizing robot movements from language input and enabling interactive robot control.",
    "authors": [
        {
            "affiliations": [],
            "name": "FORCEMENT LEARNING"
        },
        {
            "affiliations": [],
            "name": "Tianyu Li"
        },
        {
            "affiliations": [],
            "name": "Hyunyoung Jung"
        },
        {
            "affiliations": [],
            "name": "Matthew Gombolay"
        },
        {
            "affiliations": [],
            "name": "Yong Kwon Cho"
        },
        {
            "affiliations": [],
            "name": "Sehoon Ha"
        }
    ],
    "id": "SP:26a1196bd4eb2c84de96805f131dc5efb7c857aa",
    "references": [
        {
            "authors": [
                "Kfir Aberman",
                "Peizhuo Li",
                "Dani Lischinski",
                "Olga Sorkine-Hornung",
                "Daniel Cohen-Or",
                "Baoquan Chen"
            ],
            "title": "Skeleton-aware networks for deep motion retargeting",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2020
        },
        {
            "authors": [
                "Shikhar Bahl",
                "Abhinav Gupta",
                "Deepak Pathak"
            ],
            "title": "Human-to-robot imitation in the wild",
            "year": 2022
        },
        {
            "authors": [
                "Aayush Bansal",
                "Shugao Ma",
                "Deva Ramanan",
                "Yaser Sheikh"
            ],
            "title": "Recycle-gan: Unsupervised video retargeting",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Kevin Bergamin",
                "Simon Clavet",
                "Daniel Holden",
                "James Richard Forbes"
            ],
            "title": "Drecon: data-driven responsive control of physics-based characters",
            "venue": "ACM Transactions On Graphics (TOG),",
            "year": 2019
        },
        {
            "authors": [
                "Konstantinos Bousmalis",
                "Alex Irpan",
                "Paul Wohlhart",
                "Yunfei Bai",
                "Matthew Kelcey",
                "Mrinal Kalakrishnan",
                "Laura Downs",
                "Julian Ibarz",
                "Peter Pastor",
                "Kurt Konolige"
            ],
            "title": "Using simulation and domain adaptation to improve efficiency of deep robotic grasping",
            "venue": "IEEE international conference on robotics and automation (ICRA),",
            "year": 2018
        },
        {
            "authors": [
                "Sungjoon Choi",
                "Matt Pan",
                "Joohyung Kim"
            ],
            "title": "Nonparametric motion retargeting for humanoid robots on shared latent space. In 16th Robotics: Science and Systems, RSS 2020",
            "venue": "MIT Press Journals,",
            "year": 2020
        },
        {
            "authors": [
                "Brian Delhaisse",
                "Domingo Esteban",
                "Leonel Rozo",
                "Darwin Caldwell"
            ],
            "title": "Transfer learning of shared latent spaces between robots with similar kinematic structure",
            "venue": "In 2017 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2017
        },
        {
            "authors": [
                "Benjamin Eysenbach",
                "Abhishek Gupta",
                "Julian Ibarz",
                "Sergey Levine"
            ],
            "title": "Diversity is all you need: Learning skills without a reward function",
            "venue": "arXiv preprint arXiv:1802.06070,",
            "year": 2018
        },
        {
            "authors": [
                "Michael Gleicher"
            ],
            "title": "Retargetting motion to new characters",
            "venue": "In Proceedings of the 25th annual conference on Computer graphics and interactive techniques,",
            "year": 1998
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Ruben Grandia",
                "Farbod Farshidian",
                "Espen Knoop",
                "Christian Schumacher",
                "Marco Hutter",
                "Moritz B\u00e4cher"
            ],
            "title": "Doc: Differentiable optimal control for retargeting motions onto legged robots",
            "venue": "ACM Transactions on Graphics,",
            "year": 2023
        },
        {
            "authors": [
                "Karol Gregor",
                "Danilo Jimenez Rezende",
                "Daan Wierstra"
            ],
            "title": "Variational intrinsic control",
            "venue": "arXiv preprint arXiv:1611.07507,",
            "year": 2016
        },
        {
            "authors": [
                "Wen Guo",
                "Yuming Du",
                "Xi Shen",
                "Vincent Lepetit",
                "Xavier Alameda-Pineda",
                "Francesc"
            ],
            "title": "MorenoNoguer. Back to mlp: A simple baseline for human motion prediction",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Sehoon Ha",
                "Aurick Zhou",
                "Jie Tan",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "Learning to walk via deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1812.11103,",
            "year": 2018
        },
        {
            "authors": [
                "F\u00e9lix G. Harvey",
                "Mike Yurick",
                "Derek Nowrouzezahrai",
                "Christopher Pal"
            ],
            "title": "Robust motion inbetweening",
            "year": 2020
        },
        {
            "authors": [
                "Donald Hejna",
                "Lerrel Pinto",
                "Pieter Abbeel"
            ],
            "title": "Hierarchically decoupled imitation for morphological transfer",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Stefano Ermon"
            ],
            "title": "Generative adversarial imitation learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Judy Hoffman",
                "Eric Tzeng",
                "Taesung Park",
                "Jun-Yan Zhu",
                "Phillip Isola",
                "Kate Saenko",
                "Alexei Efros",
                "Trevor Darrell"
            ],
            "title": "Cycada: Cycle-consistent adversarial domain adaptation",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Daniel Holden",
                "Oussama Kanoun",
                "Maksym Perepichka",
                "Tiberiu Popa"
            ],
            "title": "Learned motion matching",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2020
        },
        {
            "authors": [
                "Stephen James",
                "Paul Wohlhart",
                "Mrinal Kalakrishnan",
                "Dmitry Kalashnikov",
                "Alex Irpan",
                "Julian Ibarz",
                "Sergey Levine",
                "Raia Hadsell",
                "Konstantinos Bousmalis"
            ],
            "title": "Sim-to-real via sim-to-sim: Dataefficient robotic grasping via randomized-to-canonical adaptation networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Hanyoung Jang",
                "Byungjun Kwon",
                "Moonwon Yu",
                "Seong Uk Kim",
                "Jongmin Kim"
            ],
            "title": "A variational u-net for motion retargeting",
            "venue": "In SIGGRAPH Asia 2018 Posters,",
            "year": 2018
        },
        {
            "authors": [
                "Kuno Kim",
                "Yihong Gu",
                "Jiaming Song",
                "Shengjia Zhao",
                "Stefano Ermon"
            ],
            "title": "Domain adaptive imitation learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Tianyu Li",
                "Hartmut Geyer",
                "Christopher G Atkeson",
                "Akshara Rai"
            ],
            "title": "Using deep reinforcement learning to learn high-level policies on the atrias biped",
            "venue": "In 2019 International Conference on Robotics and Automation (ICRA),",
            "year": 2019
        },
        {
            "authors": [
                "Tianyu Li",
                "Jungdam Won",
                "Jeongwoo Cho",
                "Sehoon Ha",
                "Akshara Rai"
            ],
            "title": "Fastmimic: Model-based motion imitation for agile, diverse and generalizable quadrupedal locomotion",
            "year": 2023
        },
        {
            "authors": [
                "Tianyu Li",
                "Jungdam Won",
                "Alexander Clegg",
                "Jeonghwan Kim",
                "Akshara Rai",
                "Sehoon Ha"
            ],
            "title": "Ace: Adversarial correspondence embedding for cross morphology motion retargeting from human to nonhuman characters",
            "venue": "SIGGRAPH Asia,",
            "year": 2023
        },
        {
            "authors": [
                "Hung Yu Ling",
                "Fabio Zinno",
                "George Cheng",
                "Michiel Van De Panne"
            ],
            "title": "Character controllers using motion vaes",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2020
        },
        {
            "authors": [
                "Ming-Yu Liu",
                "Thomas Breuel",
                "Jan Kautz"
            ],
            "title": "Unsupervised image-to-image translation networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Naureen Mahmood",
                "Nima Ghorbani",
                "Nikolaus F Troje",
                "Gerard Pons-Moll",
                "Michael J Black"
            ],
            "title": "Amass: Archive of motion capture as surface shapes",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Viktor Makoviychuk",
                "Lukasz Wawrzyniak",
                "Yunrong Guo",
                "Michelle Lu",
                "Kier Storey",
                "Miles Macklin",
                "David Hoeller",
                "Nikita Rudin",
                "Arthur Allshire",
                "Ankur Handa"
            ],
            "title": "Isaac gym: High performance gpu-based physics simulation for robot learning",
            "venue": "arXiv preprint arXiv:2108.10470,",
            "year": 2021
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Pieter Abbeel",
                "Sergey Levine",
                "Michiel van de Panne"
            ],
            "title": "Deepmimic: Exampleguided deep reinforcement learning of physics-based character skills",
            "venue": "ACM Trans. Graph.,",
            "year": 2013
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Angjoo Kanazawa",
                "Jitendra Malik",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Sfv: Reinforcement learning of physical skills from videos",
            "venue": "ACM Trans. Graph.,",
            "year": 2018
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Erwin Coumans",
                "Tingnan Zhang",
                "Tsang-Wei Edward Lee",
                "Jie Tan",
                "Sergey Levine"
            ],
            "title": "Learning agile robotic locomotion skills by imitating animals",
            "venue": "In Robotics: Science and Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Yunrong Guo",
                "Lina Halper",
                "Sergey Levine",
                "Sanja Fidler"
            ],
            "title": "Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters",
            "venue": "ACM Trans. Graph.,",
            "year": 2022
        },
        {
            "authors": [
                "Kanishka Rao",
                "Chris Harris",
                "Alex Irpan",
                "Sergey Levine",
                "Julian Ibarz",
                "Mohi Khansari"
            ],
            "title": "Rlcyclegan: Reinforcement learning aware simulation-to-real",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Helge Rhodin",
                "James Tompkin",
                "Kwang In Kim",
                "Kiran Varanasi",
                "Hans-Peter Seidel",
                "Christian Theobalt"
            ],
            "title": "Interactive motion mapping for real-time character control",
            "venue": "In Computer Graphics Forum,",
            "year": 2014
        },
        {
            "authors": [
                "Nikita Rudin",
                "David Hoeller",
                "Philipp Reist",
                "Marco Hutter"
            ],
            "title": "Learning to walk in minutes using massively parallel deep reinforcement learning",
            "venue": "In Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Yonatan Shafir",
                "Guy Tevet",
                "Roy Kapon",
                "Amit H Bermano"
            ],
            "title": "Human motion diffusion as a generative prior",
            "venue": "arXiv preprint arXiv:2303.01418,",
            "year": 2023
        },
        {
            "authors": [
                "Tanmay Shankar",
                "Yixin Lin",
                "Aravind Rajeswaran",
                "Vikash Kumar",
                "Stuart Anderson",
                "Jean Oh"
            ],
            "title": "Translating robot skills: Learning unsupervised skill correspondences across robots",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Pratyusha Sharma",
                "Deepak Pathak",
                "Abhinav Gupta"
            ],
            "title": "Third-person visual imitation learning via decoupled hierarchical controller",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Laura Smith",
                "Nikita Dhawan",
                "Marvin Zhang",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Avid: Learning multi-stage tasks via pixel-level translation of human videos",
            "year": 1912
        },
        {
            "authors": [
                "Gregory J Stein",
                "Nicholas Roy"
            ],
            "title": "Genesis-rt: Generating synthetic images for training secondary real-world tasks",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2018
        },
        {
            "authors": [
                "Seyoon Tak",
                "Hyeong-Seok Ko"
            ],
            "title": "A physically-based motion retargeting filter",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2005
        },
        {
            "authors": [
                "Jie Tan",
                "Tingnan Zhang",
                "Erwin Coumans",
                "Atil Iscen",
                "Yunfei Bai",
                "Danijar Hafner",
                "Steven Bohez",
                "Vincent Vanhoucke"
            ],
            "title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "venue": "arXiv preprint arXiv:1804.10332,",
            "year": 2018
        },
        {
            "authors": [
                "Yujin Tang",
                "Wenhao Yu",
                "Jie Tan",
                "Heiga Zen",
                "Aleksandra Faust",
                "Tatsuya Harada"
            ],
            "title": "Saytap: Language to quadrupedal locomotion",
            "venue": "arXiv preprint arXiv:2306.07580,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Tseng",
                "Rodrigo Castellon",
                "Karen Liu"
            ],
            "title": "Edge: Editable dance generation from music",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Ruben Villegas",
                "Jimei Yang",
                "Duygu Ceylan",
                "Honglak Lee"
            ],
            "title": "Neural kinematic networks for unsupervised motion retargetting",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Jungdam Won",
                "Jehee Lee"
            ],
            "title": "Learning body shape variation in physics-based characters",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2019
        },
        {
            "authors": [
                "Zhaoming Xie",
                "Glen Berseth",
                "Patrick Clary",
                "Jonathan Hurst",
                "Michiel van de Panne"
            ],
            "title": "Feedback control for cassie with deep reinforcement learning",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2018
        },
        {
            "authors": [
                "Qiang Zhang",
                "Tete Xiao",
                "Alexei A Efros",
                "Lerrel Pinto",
                "Xiaolong Wang"
            ],
            "title": "Learning cross-domain correspondence for control with dynamics cycle-consistency",
            "venue": "arXiv preprint arXiv:2012.09811,",
            "year": 2020
        },
        {
            "authors": [
                "Tinghui Zhou",
                "Philipp Krahenbuhl",
                "Mathieu Aubry",
                "Qixing Huang",
                "Alexei A Efros"
            ],
            "title": "Learning dense correspondence via 3d-guided cycle consistency",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Jun-Yan Zhu",
                "Taesung Park",
                "Phillip Isola",
                "Alexei A Efros"
            ],
            "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The concept of teleoperating robots through human movements, known as Human Motion Driven Control (HMDC), has been illustrated in various forms of media, including animations, movies, and science fiction, such as Mado\u0308 King Granzo\u0308rt (Iuchi, 1989), Pacific Rim (del Toro, 2013), and Ready Player One (Spielberg, 2018). In these media, HMDC technology allows operators to intuitively control robots using their body movements. Compared to fully autonomous control, this teleoperation offers the essential dexterity and decision-making capabilities required for tasks demanding precise motor skills and situational awareness. Consequently, this property makes HMDC promising for various applications, including entertainment, medical surgery, and space exploration.\nThe key challenge of HMDC is how to establish the correspondence between robot states and human motions, which can also be referred to as motion retargeting. For certain types of robots, such as humanoids or manipulators, this correspondence might be simple enough to be approached by assuming the mapping of end-effectors in Cartesian space and solving the formulated inverse kinematics problem (Gleicher, 1998; Tak & Ko, 2005). However, when we consider robots with significantly different morphological structures, such as quadrupeds, hexapods, or quadrupeds with mounted arms, the correspondence becomes nontrivial due to the intrinsic ambiguity of the problem. Therefore, researchers often have approached this motion retargeting problem by applying supervised learning techniques to the paired datasets (Sermanet et al., 2018; Delhaisse et al., 2017; Rhodin et al., 2014). Nonetheless, creating paired datasets can be a challenging and labor-intensive\ntask that requires significant engineering expertise. To address this issue, some researchers have proposed using unsupervised learning techniques to learn the correlation from unpaired human and robot motion datasets (Li et al., 2023b; Choi et al., 2020; Smith et al., 2019). In this case, the robot dataset serves as prior knowledge indicating the motion pattern of the robot. However, obtaining motion datasets can be expensive because we do not know the required skills for the given human motion. In addition, control itself is challenging due to the complexity of the quadrupedal robot and its underactuated dynamics. This leads to our research question: can we learn cross-morphology HMDC without prior knowledge of the robot?\nThe research question presents three primary challenges. Firstly, the significant difference in kinematics and dynamics between the human and the target robot makes it difficult to establish correspondence. Secondly, we cannot build a predefined motion database for the robot due to the complexity of the problem. Finally, the problem itself is ambiguous. For instance, there exist many different quadrupedal gaits that can capture the essence of human walking. To address these challenges, we drew inspiration from the recent unsupervised skill discovery techniques, such as Eysenbach et al. (2018) and Peng et al. (2022), and aim to simultaneously learn robot skills and robothuman motion correspondence by maximizing the mutual information between human and robot motions.\nIn this work, we introduce CrossLoco, a guided unsupervised reinforcement learning framework that enables simultaneous learning of human-robot motion correspondence and robot motion control (Figure 1). Our key approach is to introduce a cycle-consistency-based correspondence reward term that maximizes the mutual information between human motions and the synthesized robot movements. We implement this cycle consistency term by training both robot-to-human and human-to-robot reconstruction networks. Our formulation also includes regularization terms and a root-tracking reward to guide correspondence learning. Simultaneously, we train a robot control policy that takes human motions and sensory information as input and generates robot actions for interacting with the environment.\nWe demonstrate that CrossLoco can translate a large set of human motions for robots, including walking, running, and dancing. Even for locomotion, the robot exhibits two distinct strategies, trotting and galloping, inspired by human walking motions with different styles. We quantitatively compare our method against the baseline, DeepMimic (Peng et al., 2018a), along with the ablated versions of our CrossLoco framework and show that our method can achieve better quantitative results in terms of accuracy, diversity, and user preference. We further showcase the potential applications of our framework: language2text motion synthesis and interactive motion control."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": "Learning Locomotion Skills. There are various methods for robots to learn locomotion skills. One approach involves maximizing a reward function designed by experts using reinforcement learning, as demonstrated in several studies such as Tan et al. (2018); Haarnoja et al. (2018); Xie et al. (2018); Li et al. (2019); Rudin et al. (2022). Another method is motion imitation, where the control policy is trained with an imitation reward, as shown in Peng et al. (2018a;b); Li et al. (2023a); Bergamin et al. (2019); Won & Lee (2019); Ling et al. (2020); Peng et al. (2020). This reward is calculated based on the distance between the robot\u2019s current pose and a reference pose from the demonstration\ntrajectory. The closer the distance, the larger the reward. Generative adversarial imitation learning (GAIL) (Ho & Ermon, 2016) is another approach that trains the policy to deceive a discriminator that distinguishes real and fake demonstration data. Finally, without the need for engineering reward functions or demonstration data, some studies such as Eysenbach et al. (2018); Sharma et al. (2019) focus on unsupervised skill discovery from interaction data through information-theoretic methods.\nMotion Retargeting. Transferring motions between different morphologies has been an important topic in both robotics and computer graphics communities to produce natural motions for various robots and characters. Researchers have investigated various approaches, such as designing manual correspondences (Gleicher, 1998; Tak & Ko, 2005; Grandia et al., 2023), learning from paired datasets (Sermanet et al., 2018; Delhaisse et al., 2017; Jang et al., 2018), or developing modular/hierarchical policies (Won & Lee, 2019; Hejna et al., 2020; Sharma et al., 2019). More recent works (Zhang et al., 2020; Aberman et al., 2020; Villegas et al., 2018; Li et al., 2023b; Smith et al., 2019; Kim et al., 2020; Shankar et al., 2022) aim to learn the state and action correspondence from unpaired datasets via unsupervised learning. However, these methods often require a pre-collected dataset of both domains, which is not available for robots in our problem.\nCycle-Consistency. Our work is inspired by previous research on cycle-consistency (Zhou et al., 2016; Zhu et al., 2017; Liu et al., 2017; Rao et al., 2020; Bousmalis et al., 2018). For instance, CycleGAN (Zhu et al., 2017) combines cycle-consistency loss with Generative Adversarial Networks (Goodfellow et al., 2014) for unpaired image-to-image translation. By adding domain knowledge, CycleGAN can be extended to video retargeting (Bansal et al., 2018) and domain adaptation (Hoffman et al., 2018). In robotics, a similar approach has been investigated for sim-to-real transfer (Stein & Roy, 2018; James et al., 2019). Besides alignment in image space, a few researchers (Zhang et al., 2020; Shankar et al., 2022) adopt cycle-consistency to align agents in different dynamics and structures, while the others (Aberman et al., 2020; Villegas et al., 2018) apply cycle-consistency for motion retargeting between similar human-like robots or characters. Inspired by these works, we aim to co-train a control policy for the diverse motor skills of a quadrupedal robot while establishing cycle consistency between the robot and human motions."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "Skill-Conditioned Reinforcement Learning. We formulate our framework as a skill-conditioned reinforcement learning problem, where an agent interacts with an environment to maximize an objective function by following a policy \u03c0. At the beginning of each learning episode, a condition term is sampled from the dataset z \u223c p(z). At each time step, the agent observes the state of the system st, then takes an action sampled from the policy at \u223c \u03c0(at|st, z) to interacts with the environment. After executing the actions, the environment takes the agent to a new state sampled from the dynamics transition probability st+1 \u223c p(st+1|st,at). A scalar reward can be measured using a reward function rt = r(st,at, st+1, z). The agent\u2019s objective is to learn a policy that maximizes its expected cumulative reward J(\u03c0),\nJ(\u03c0) = Ez\u223cp(z),\u03c4\u223cp(\u03c4 |\u03c0,z)[ T\u22121\u2211 t=0 \u03b3trt]. (1)\nHere, \u03c4 is a state and action trajectory with the length T , where its distribution can be computed as p(\u03c4 |\u03c0, z) = p(s0) \u220fT\u22121 t=0 p(st+1|st,at)\u03c0(at|st, z) is the likelihood of the trajectory under policy \u03c0. The initial state s0 is sampled from the distribution p(s0) and \u03b3 \u2208 [0, 1) is a discount factor. Skill Discovery By Maximizing Mutual Information. Eysenbach et al. (2018) and Peng et al. (2022) formulate the skill discovery problem as an unsupervised reinforcement learning problem, where the objective is to maximize the mutual information between the robot state and a latent vector sampled from a distribution z \u223c p(z): I(S;Z) = H(S)\u2212H(S,Z). This equation can be interpreted as the policy \u03c0 is to learn to produce diverse behaviors while each latent vector z should correspond to distinct robot states.\nHowever, this equation is intractable in most scenarios where the state marginal distribution is unknown, and two tricks are commonly implemented to tackle this. The first trick is to take advantage of the symmetry of mutual information:\nI(S;Z) = I(Z;S) = H(Z)\u2212H(Z|S). (2)\nThis trick removes the need for measuring the marginal entropy of robot state H(S) by instead measuring the entropy of the latent vector H(Z) which remains constant in fixed skill prior p(z). The second trick is to use a variational lower bound as proposed by Eysenbach et al. (2018) and Gregor et al. (2016) to approximate the mutual information as follows:\nI(Z;S) = H(Z)\u2212H(Z|S) \u2265 max q H(z) + Ez\u223cp(z),s\u223cp(s|\u03c0z)[log(q(z|s)], (3)\nwhere q(z|s) is a variational approximation of the conditional distribution p(z|s) and the lower bound is tight if q = p. This skill discovery objective encourages a policy to produce distinct behaviors for different skill vectors z by designing a reward based on the measurement of q(z|s)."
        },
        {
            "heading": "4 CROSSLOCO",
            "text": "CrossLoco is a guided unsupervised reinforcement learning framework designed to learn robot locomotion control policy driven by human motion. The framework establishes a correspondence between human and robot motions, enabling the robot to acquire locomotion skills from human motions. The method overview is shown in Fig 2. In this section, we first introduce how we formulate the problem. Then, we present our cycle-consistency-based method for learning locomotion and human-robot correspondence. Lastly, we provide additional implementation details."
        },
        {
            "heading": "4.1 PROBLEM FORMULATION",
            "text": "Our goal is to train a robot control policy, denoted as \u03c0, that can produce various robot motions based on different human motion inputs. Therefore, we can view our problem as a Markov Decision Process conditioned on the given human motion. Let us define pht and p r t as the human and robot kinematic poses. Then the human motion is defined as a sequence of pose vectors: mh = [ph0 ,p h 1 , \u00b7 \u00b7 \u00b7 ,phT\u22121]. The robot state srt represents both the kinematic and dynamic status of the robot, hence we can view srt as the superset of the pose p r t . The robot action at corresponds to motor commands, such as target joint angles. At each time step, the policy takes the robot state vector srt and the augmented human motion feature x h t = x(p h t ) as input to generate an action at \u223c \u03c0(at|srt ,xht ). Then our goal is to maximize the given reward function r:\nJ(\u03c0) = Emh\u223cp(mh),\u03c4r\u223cp(\u03c4r|\u03c0,xht )[ T\u22121\u2211 t=0 \u03b3tr(srt ,x h t ,at)], (4)\nwhere the robot trajectory is defined as a sequence of the robot states \u03c4 r = [sr0, s r 1, \u00b7 \u00b7 \u00b7 , srT\u22121].\nThis formulation leads to the question of designing an effective reward function r that builds the relationship between the human and robot poses, pht and p r t . In addition, the reward function should\nAlgorithm 1 CrossLoco pseudocode Require: Human DatasetM.\n1: Initialize: Policy \u03c0, Value function V , R2H-Mapper qr2h, H2R-Mapper qh2r , Data Buffer D. 2: repeat 3: for trajectory i = 1, ..., m do 4: D \u2190 {(xht , srt , at, rt)T\u22121t=0 } collect trajectory by rolling out \u03c0. 5: update \u03c0 and V using PPO with data from D. 6: update qr2h and qh2r using data from D by minimizing Lr2r (Equation 7). 7: Reinitialize Data Buffer D. 8: until Done\ninclude some regularization terms, such as minimizing the energy, avoiding self-collisions, or preserving the predefined semantic features. We will discuss the design of our reward function in the following section."
        },
        {
            "heading": "4.2 MEASURING CORRESPONDENCE VIA CYCLE-CONSISTENCY",
            "text": "To develop a reward function that represents the correspondence between human and robot motions, we borrow the information-theoretic approach mentioned in the previous section. We formulate the correspondence reward term such that it maximizes the mutual information between human and robot pose, given by I(prt ,p h t |\u03c0). From Equation 3, this formulation can be approximated by:\nI(prt ,p h t |\u03c0) \u2265 H(pht ) + Emh\u223cp(mh),prt\u223cp(prt |pht ,\u03c0)[log(q r2h(pht |prt ))]], (5)\nwhere we assume that human motion prior is from the fixed dataset and refer to qr2h as Robot-toHuman Mapper (R2H-Mapper). Because the first term H(pht ) is constant, we can find the optimal policy by maximizing the second term, log[qr2h(pht |prt )]. A higher value represents that R2HMapper is more certain about the human pose given the robot pose, hence indicating that the human pose is distinctive given the robot pose.\nWe model the R2H-Mapper as a Gaussian distribution with fixed covariance qr2h(pht |prt ) = N(\u00b5r2h(prt ), \u03c3) where \u00b5\nr2h(prt ) is the mean of the distribution while \u03c3 is the constant covariance matrix. The R2H-Mapper can be trained by minimizing a loss function Lr2h:\nargmin qr2h\nLr2h = Epht \u223cp(pht ),prt\u223cd\u03c0(prt |pht )[||p h t \u2212 \u00b5r2h(prt )||22], (6)\nwhere d\u03c0(prt |pht ) is the likelihood of observing robot pose prt , by executing policy \u03c0 given the human pose pht . Similarly, we can design our correspondence reward to minimize the given term ||pht \u2212 \u00b5r2h(prt )||22. However, R2H-Mapper does not prevent multiple robot poses pr from being mapped to the same human pose ph because it only considers one-directional mapping, which may cause degenerated motions. To address this issue, we add a Human-to-Robot Mapper (H2R-Mapper), denoted as qh2r(prt |pht ), which is used for mapping the human pose back to the robot pose. We use a cycleconsistency formulation of Zhu et al. (2017), where we first map the robot pose to the human pose, followed by mapping the generated human pose back to the robot pose. This results in an objective loss function Lr2r for H2R-Mapper and R2H-Mapper as:\nargmin qr2h,qh2r\nLr2r = Lr2h + Epht \u223cp(pht ),prt\u223cd\u03c0(prt |pht )[||p r t \u2212 \u00b5h2r(\u00b5r2h(prt ))||22]. (7)\nFinally, from our mutual information maximization and cycle-consistency loss minimization, we formulate the correspondence reward as follows:\nrcpdt = exp(\u2212||pht \u2212 \u00b5r2h(prt )||22 \u2212 ||prt \u2212 \u00b5h2r(\u00b5r2h(prt ))||22). (8)"
        },
        {
            "heading": "4.3 IMPLEMENTATION DETAILS",
            "text": "During the training process, the policy \u03c0, as well as the H2R-Mapper and R2H-Mapper, are updated iteratively. The policy is trained using Proximal Policy Optimization (PPO) Schulman et al. (2017).\nMeanwhile, the H2R-Mapper and R2H-Mapper are trained using supervised learning. The learning framework is summarized in Algorithm 1.\nModel Representation. Human pose (ph \u2208 R23) and robot pose (pr \u2208 R17) consist of local information, including root height, root orientation, and joint pose. The robot state (srt \u2208 R47) contains all the information in pr, as well as root and joint velocity, and previous action. The human feature vector (xht \u2208 R188) includes human pose, root velocity, and joint velocity information at the future 1, 2, 10, and 30 frames.\nComplete Reward Function. In addition to the correspondence reward mentioned earlier, our reward function includes several terms to regulate the training and preserve high-level semantics. A root tracking reward, denoted as rroott = exp(\u2212||sroott \u2212 s\u0304roott ||), is designed to preserve highlevel movements by minimizing the deviation between the normalized base trajectories of the human (\u0304sroott ) and the robot (s root t ). In this context, both trajectories include the root position and height, which are normalized by their respective leg lengths. Without this term, the resulting correspondence can be arbitrary: e.g., a human forward walking motion can be mapped into a robot\u2019s lateral movements. To prevent unrealistic movements, a torque penalty, rtort = \u2212||at||, and joint limits penalty, rlimt = \u22121prt>plim , are borrowed from Rudin et al. (2022). Here, p lim is the pose limit of the robot. The overall reward is calculated as the weighted sum of all these terms: rt = wcpdr cpd t + w rootrroott + w torrtort + w\nlimrlimt . To optimize the weights, increasing wcpd can improve the correspondence between robot and human motion, but it may negatively affect root tracking performance or increase energy consumption. Increasing wroot puts more emphasis on root tracking. Small values for wtor and wlim can result in unnatural motions, while excessively large values can lead to overly conservative motions. In our setting, we use [wcpd, wroot, wtor, wlim] = [1.0, 1.0, 0.0001, 5.0].\nNetwork Structure. The policy, critic, H2R-Mapper, and R2H-Mapper are modeled by a fullyconnected network consisting of three hidden layers with 512 nodes each. ELU is used as the activation function for the policy and critic, while ReLU is used for the mapper networks."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We conduct a series of experiments to investigate three key aspects of the proposed work: firstly, the feasibility of acquiring a human-motion-driven robot controller, which is referred to as the \u2018human2robot\u2019 controller; secondly, the comparative performance against alternative baseline approaches; and lastly, the influence of the correspondence reward on the training process.\nWe evaluate the effectiveness of our approach by transferring a set of human motions to Aliengo quadrupedal robot (unitree, 2023) with 12 joints, which has a significantly different morphology compared to humankind. We take human motion from LaFAN1 dataset (Harvey et al., 2020). Our human dataset consists of 50 human motion trajectories with eight seconds of the average clip length. The dataset contains various types of human movements, including walking, running, hopping, and\ndancing. All experiment environments are conducted using Isaac Gym (Makoviychuk et al., 2021), a high-performance GPU-based physics simulator. During training, 1024 environments are simulated in parallel on a single NVIDIA GeForce RTX 3080 Ti GPU for a period of about 3 days."
        },
        {
            "heading": "5.1 MAIN RESULTS AND ANALYSIS",
            "text": "We present the results of our human motion driven control experiments in Figure 4. Our method successfully learns a human2robot controller that can transfer various human motions to a robot with a different morphology. We observe agile motions from the robot, such as running and sharp turning when the human performs fast locomotion. The results also demonstrate that the robot can creatively follow human dancing motions, which is hard to manually design. This showcases the capability of our method to establish automatic correspondence between humans and robots while learning diverse robot skills. All the motions can be best seen in the supplementary video.\nIn Figure: 3, we demonstrate the effectiveness of our proposed correspondence reward in conjunction with the R2H and H2R mappers, which enable the robot to mimic human movements accurately. It\u2019s important to note that such synchronization is unattainable without the correspondence reward. Without it, the robot would remain stationary or merely track the human\u2019s root position. Our experiments have revealed noteworthy findings. For instance, even when humans walk at a similar speed but with varying styles\u2014such as different step frequencies\u2014the robot adapts its movement to match the human\u2019s frequency. Another intriguing observation is the relationship between the correspondence and root tracking. When a human is stationary or moves minimally, the robot responds effectively to the human\u2019s arm movements. However, during rapid movement, the robot prioritizes leg motion. We hypothesize that during high-speed locomotion, the root tracking reward becomes more dominant, leading to a balancing act in the CrossLoco framework between root tracking and achieving high-quality correspondence.\nIn some scenarios, the robot is not able to perfectly mimic the given human motion. For instance, when a human moves backward and makes a sharp 180-degree turn, the robot cannot follow the desired orientation. Additionally, CrossLoco may struggle to transfer large side stepping. There are two potential reasons for these imperfections. First, the neural network may be incapable of capturing all motions. Second, the robot\u2019s morphology may prohibit it from performing certain motions that are easy for humans, such as swift turning. During training, we observed that the diversity of human motion is critical to the training outcome. This is because the learned mappers can overfit to specific scenarios. For instance, if the dataset contains only hand-waving motions, the robot might slightly vibrate the root to maximize the correspondence reward hence lead to undesirable motion transfer results."
        },
        {
            "heading": "5.2 BASELINE COMPARISON",
            "text": "We further quantitatively compare our method against the following baseline methods:\n\u2022 Engineered Motion Retarget + DeepMimic (DeepMimic): This baseline contains two stages. Firstly, an expert manually designs a motion retargeting function to translate human motions to robot motion referenced trajectories. Then, the robot is trained to track these\nreference trajectories using DeepMimic (Peng et al., 2018a). It is important to note that the result of this baseline heavily relies on the quality of the retargeted motions, which requires a significant amount of effort from the expert. In our case, we have designed the retargeted motions by matching the human foot and robot foot with a fixed tripod gait.\n\u2022 Task-Only: This baseline is designed to investigate the impact of the proposed correspondence reward on training outcomes. As such, we compare this approach to CrossLoco, where the weight of correspondence is set to zero (wx = 0). Therefore, this policy is trained solely on a root tracking reward and other regularization rewards.\n\u2022 R2H-Only: This baseline is designed by removing the robot pose cycle-consistency part from CrossLoco and only keeping the human pose\u2019s consistency. As there is no robot pose consistency, in this baseline, the correspondence reward is defined as rcpd,r2ht = exp(\u2212||pht \u2212 \u00b5r2h(prt )||22).\nSince we assume we have no robot motion dataset, hence, we don\u2019t include any GAN-based baseline methods, such as Adversarial Correspondence Embedding (Li et al., 2023b).\nOur objective is to quantitatively assess the effectiveness of establishing correspondence between human and robot motion, as well as the diversity of the robot motion of these methods. In order to achieve this, we utilize the following metrics:\n\u2022 Averaged Correspondence Reward (ACR): This term measures the correspondence between human and robot motions. A higher ACR indicates better correspondence. Even though no correspondence reward is used in each baseline training procedure, we co-train R2H-Mapper and H2R-Mapper to measure the correspondence reward.\n\u2022 Diversity (DIV): This term has been used for measuring motion diversity in many SOTA works (Shafir et al., 2023; Guo et al., 2023). A higher DIV indicates robots can acquire more skills. From a set of all generated motions from different source human motions, two subsets of the same size Sd are randomly picked. The diversity of this set of motion is defined as: DIV = 1Sd\u03a3 Sd i=1||\u03a8(sri ) \u2212 \u03a8r(s\u2032 r i )||. \u03a8(sri ) and \u03a8(s\u2032 r i ) are features extracted\nfrom robot state. Here, we pick robot root velocity and joint pose as the feature. \u2022 Averaged Root Tracking Reward (RTR): This term measures if the learned policy can\ntrack the desired root trajectory. The root tracking reward is defined in Section 4.3.\nIn addition to these metrics, we conducted a user study with 15 subjects to evaluate the performance from a subjective perspective.\n\u2022 Correct Rate (CR): We first investigate whether users can identify a correct match between the given human and synthesized robot motions. A user is tasked to find a matched pair from one human animation and four retargeted robot motions. One robot motion is retargeted from the given human motion, while the other three are generated from different inputs. We examined the combination of four human motions and four methods, and then measured the percentage of correct matches. Ideally, a good transfer should accurately capture the style of the human motion, resulting in easy matching for the user.\n\u2022 Preference (PR): In the second part, we provided users with robot motions generated with different approaches. We asked the users to select the motions that they believed represented a good transfer. We then measured the ratio at which each method was chosen.\nOur results are summarized in Table 1. The quantitative analysis indicates that CrossLoco outperforms the baseline methods in all metrics suggesting that it can effectively learn a controller that can translate different human motions to diverse robot motions while tracking the desired root trajectory.\nAlthough DeepMimic is designed as a one-to-one mapping between human and robot poses, it achieved a lower correspondence reward than CrossLoco (0.558 vs 0.785). This could be attributed to the fact that an engineering mapping function may not be physically feasible for all input human motions, and the learning process may sometimes sacrifice the desired posing tracking for a higher desired root tracking reward. Moreover, since the engineered robot desired motion can sometimes be physically infeasible, the root tracking reward of DeepMimic is also lower than that of CrossLoco (0.579 vs 0.743). Based on the results of the user study, it was found that DeepMimic\u2019s motion (with a 16% PR score) was not preferred by users. However, it achieved the highest CR score (67.5%). This can be because users can match human and robot motions based on the most obvious frames, even with poor overall motion quality.\nAs for Task-only, since it is trained only with root tracking and regularization rewards, it produces conservative motions by ignoring human leg motions in some cases, such as different in-place dancing motions. All human in-place dancing motions are mapped to the robot standing with slight root movements by Task-only baseline. However, the correspondence reward in CrossLoco triggers robots to learn diverse skills that correspond to different human motions, as evidenced by its superior performance in terms of correspondence reward, diversity term, and user study results compared to Task-only. We also obverse that for CrossLoco achieves slightly higher root tracking reward, indicating CrossLoco\u2019s great capability.\nThe results show that CrossLoco achieves a higher correspondence reward and more diverse motion compared to R2H-Only. Additionally, users found the results of CrossLoco to be more distinguishable. This could be attributed to the effective regularization provided by the cycling of human back to robot, resulting in more distinguishable outcomes."
        },
        {
            "heading": "5.3 APPLICATIONS",
            "text": "Numerous studies have delved into human motion synthesis using a variety of input sources, such as text (Bahl et al., 2022), music Tseng et al. (2023), and user inputs (Holden et al., 2020). Our learned human2robot controller can be seamlessly integrated with these modules, utilizing human motion as the interface for new applications. In this context, we have implemented two examples: Language2Robot and Interactive Robot Control. An illustration of these examples is presented in Figure 5. Language2Robot merges the established text-to-human-motion module (MDM (Bahl et al., 2022)) with CrossLoco, enabling the generation of robot motion based on verbal instructions. Interactive Robot Control combines an existing humanoid character controller (LMM (Holden et al., 2020)) with a CrossLoco-trained policy to facilitate interactive robot control without the need to retrain a large-scale interactive robot control policy from scratch. For more details, please refer to our supplementary videos and the appendix."
        },
        {
            "heading": "6 DISCUSSION",
            "text": "We introduce CrossLoco, an unsupervised reinforcement learning framework designed to enable robot locomotion control driven by human motion. This framework incorporates a cycleconsistency-based reward function, which facilitates the discovery of robot skills and the establishment of correspondence between human and robot motion. Our experimental results demonstrate the effectiveness of our approach in transferring a wide range of human motions to control a robot that has a different morphology.\nOur next steps involve exploring two directions. Firstly, we aim to extend our framework beyond locomotion control to more complex scenarios, including long-horizon human demonstrations that involve long-distance locomotion and tool manipulation on a legged-manipulation robot, such as a quadrupedal robot with an arm mounted on its body. Secondly, we are interested in implementing our method on real-world robots for practical applications."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work is supported by National Science Foundation under Grant No.2222723. We would like to thank Yuxiang Yang and Jiahan Fan for valuable suggestions."
        },
        {
            "heading": "A TRAINING HYPERPARAMETERS",
            "text": "In this study, we trained the policy using Proximal Policy Optimization (PPO) Schulman et al. (2017). Additionally, both the H2R-Mapper and R2H-Mapper were trained utilizing supervised learning techniques. Below, we detail the hyperparameters employed in our training process."
        },
        {
            "heading": "B APPLICATIONS",
            "text": "Many research and work have explored the field of human motion synthesis using various input sources, including text (Bahl et al., 2022), music Tseng et al. (2023), and user inputs (Holden et al., 2020). Our learned human2robot controller can be seamlessly integrated with these modules by using human motion as the interface for new applications. In this section, we present two examples: Language2Robot and Interactive Robot Control.\nLanguage2Robot. Our approach involves utilizing the text2human module in combination with our human2robot controller. This allows for the generation of robot movements from language by first producing human motion using the text2human module, which is then transferred to the robot using the human2robot controller. Our method differs from recent language to quadrupedal robot motion work (Tang et al., 2023) in that it does not require an engineering-intensive interface, such as foot contact patterns, which could limit the range of possible generated robot motions.\nIn our implementation, we utilize the Human Motion Diffusion Model (MDM) (Bahl et al., 2022) as our text-to-human motion translator. MDM is a diffusion model-based lightweight model that achieves state-of-the-art results on leading benchmarks. However, MDM uses AMASS (Mahmood\net al., 2019) human model which is different from the LaFAN1 (Harvey et al., 2020) model we used for training the policy. Therefore, we retarget the outputs of the MDM to the skeleton model we use.\nOur study involves testing the generation of robot motion based on different input messages. The results of our study are presented in Figure 5. We show that this framework can generate robot motion according to instructions. For instance, the message \u201cstrides swiftly in a straight\u201d results in a fast walking straight robot motion, while \u201csquats down then jumps\u201d triggers a robot squats and jumps motion.\nInteractive Robot Control. Interactive control of robots in response to changing conditions or unexpected obstacles is a significant challenge in robotics, which involves careful controller design and motion planning. Recent advances in character animation enable users to interactively control human characters using joysticks, automatically adapting their motion styles to the surrounding environment, like crouching in confined spaces or leaping over obstacles. Our key idea for the second application, Interactive Robot Control, is to leverage the existing human animation techniques for robot control. Instead of retraining a large-scale model from scratch, we simply translate the output of the existing character controller to the robot\u2019s operational space using the proposed method.\nOur implementation of this framework utilizes Learned Motion Matching (LMM) (Holden et al., 2020), which is a scalable neural network-based framework for interactive motion synthesis. We combine LMM with our learned controller. During interactive robot control, LMM takes input user commands for human motion generation, and our controller converts the generated human motion to robot control commands.\nWe evaluated our implementation by controlling the robot using a joystick. Figure 5 presents the results of using LMM2robot. The experiment demonstrated that the robot can actively adjust its motion based on the user\u2019s commands. These results provide evidence of the effectiveness of our learned controller for interactive robot control."
        }
    ],
    "title": "CROSSLOCO: HUMAN MOTION DRIVEN CONTROL OF LEGGED ROBOTS VIA GUIDED UNSUPERVISED REIN-",
    "year": 2024
}