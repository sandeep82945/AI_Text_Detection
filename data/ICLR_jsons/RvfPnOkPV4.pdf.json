{
    "abstractText": "Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose WHAT\u2019S IN MY BIG DATA? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities\u2014count and search\u2014at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50% of the documents in RedPajama and LAION-2B-en are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source WIMBD\u2019s code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yanai Elazar"
        },
        {
            "affiliations": [],
            "name": "Akshita Bhagia"
        },
        {
            "affiliations": [],
            "name": "Ian Magnusson"
        },
        {
            "affiliations": [],
            "name": "Abhilasha Ravichander"
        },
        {
            "affiliations": [],
            "name": "Dustin Schwenk"
        },
        {
            "affiliations": [],
            "name": "Alane Suhr"
        },
        {
            "affiliations": [],
            "name": "Pete Walsh"
        },
        {
            "affiliations": [],
            "name": "Dirk Groeneveld"
        },
        {
            "affiliations": [],
            "name": "Luca Soldaini"
        },
        {
            "affiliations": [],
            "name": "Sameer Singh"
        },
        {
            "affiliations": [],
            "name": "Hannaneh Hajishirzi"
        },
        {
            "affiliations": [],
            "name": "Noah A. Smith"
        },
        {
            "affiliations": [],
            "name": "Jesse Dodge"
        }
    ],
    "id": "SP:6052cdf24184546d3670de85d348feecd3026899",
    "references": [
        {
            "authors": [
                "Julien Abadji",
                "Pedro Ortiz Suarez",
                "Laurent Romary",
                "Beno\u00eet Sagot"
            ],
            "title": "Towards a cleaner documentoriented multilingual crawled corpus",
            "venue": "In Proceedings of the Thirteenth Language Resources and Evaluation Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Ekin Akyurek",
                "Tolga Bolukbasi",
                "Frederick Liu",
                "Binbin Xiong",
                "Ian Tenney",
                "Jacob Andreas",
                "Kelvin Guu"
            ],
            "title": "Towards tracing knowledge in language models back to the training data",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Loubna Ben Allal",
                "Raymond Li",
                "Denis Kocetkov",
                "Chenghao Mou",
                "Christopher Akiki",
                "Carlos Munoz Ferrandis",
                "Niklas Muennighoff",
                "Mayank Mishra",
                "Alex Gu",
                "Manan Dey"
            ],
            "title": "Santacoder: don\u2019t reach for the stars",
            "venue": "arXiv preprint arXiv:2301.03988,",
            "year": 2023
        },
        {
            "authors": [
                "Iz Beltagy",
                "Kyle Lo",
                "Arman Cohan"
            ],
            "title": "SciBERT: A pretrained language model for scientific text",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
            "year": 2019
        },
        {
            "authors": [
                "Emily M. Bender",
                "Batya Friedman"
            ],
            "title": "Data statements for natural language processing: Toward mitigating system bias and enabling better science",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2018
        },
        {
            "authors": [
                "Stella Biderman",
                "Kieran Bicheno",
                "Leo Gao"
            ],
            "title": "Datasheet for the pile, 2022",
            "venue": "URL https:// arxiv.org/abs/2201.07311",
            "year": 2022
        },
        {
            "authors": [
                "Stella Biderman",
                "Hailey Schoelkopf",
                "Quentin Gregory Anthony",
                "Herbie Bradley",
                "Kyle O\u2019Brien",
                "Eric Hallahan",
                "Mohammad Aflah Khan",
                "Shivanshu Purohit",
                "USVSN Sai Prashanth",
                "Edward Raff"
            ],
            "title": "Pythia: A suite for analyzing large language models across training and scaling",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "May"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "doi: 10.18653/v1/2022.bigscience-1.9. URL https://aclanthology.org/2022.bigscience-1.9.",
            "year": 2022
        },
        {
            "authors": [
                "Burton H. Bloom"
            ],
            "title": "Space/time trade-offs in hash coding with allowable errors",
            "venue": "Commun. ACM,",
            "year": 1970
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Florian Tram\u00e8r",
                "Eric Wallace",
                "Matthew Jagielski",
                "Ariel Herbert-Voss",
                "Katherine Lee",
                "Adam Roberts",
                "Tom Brown",
                "Dawn Song",
                "\u00dalfar Erlingsson",
                "Alina Oprea",
                "Colin Raffel"
            ],
            "title": "Extracting training data from large language models",
            "venue": "In 30th USENIX Security Symposium (USENIX Security",
            "year": 2021
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Daphne Ippolito",
                "Matthew Jagielski",
                "Katherine Lee",
                "Florian Tramer",
                "Chiyuan Zhang"
            ],
            "title": "Quantifying memorization across neural language models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Xavier Garcia",
                "Adam Roberts",
                "Yi Tay",
                "Orhan Firat",
                "Sharan Narang",
                "Noah Constant"
            ],
            "title": "Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Jeffrey Dean",
                "Sanjay Ghemawat"
            ],
            "title": "Mapreduce: Simplified data processing on large clusters",
            "venue": "Commun. ACM,",
            "year": 2008
        },
        {
            "authors": [
                "Jesse Dodge",
                "Maarten Sap",
                "Ana Marasovi\u0107",
                "William Agnew",
                "Gabriel Ilharco",
                "Dirk Groeneveld",
                "Margaret Mitchell",
                "Matt Gardner"
            ],
            "title": "Documenting large webtext corpora: A case study on the colossal clean crawled corpus",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Yanai Elazar",
                "Nora Kassner",
                "Shauli Ravfogel",
                "Abhilasha Ravichander",
                "Eduard Hovy",
                "Hinrich Sch\u00fctze",
                "Yoav Goldberg"
            ],
            "title": "Measuring and improving consistency in pretrained language models",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Yanai Elazar",
                "Hongming Zhang",
                "Yoav Goldberg",
                "Dan Roth"
            ],
            "title": "Back to square one: Artifact detection, training and commonsense disentanglement in the Winograd schema",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Ali Emami",
                "Kaheer Suleman",
                "Adam Trischler",
                "Jackie Chi Kit Cheung"
            ],
            "title": "An analysis of dataset overlap on Winograd-style tasks",
            "venue": "In Proceedings of the 28th International Conference on Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima",
                "Shawn Presser",
                "Connor Leahy"
            ],
            "title": "The Pile: An 800gb dataset of diverse text for language modeling",
            "venue": "arXiv preprint arXiv:2101.00027,",
            "year": 2020
        },
        {
            "authors": [
                "Timnit Gebru",
                "Jamie Morgenstern",
                "Briana Vecchione",
                "Jennifer Wortman Vaughan",
                "Hanna Wallach",
                "Hal Daum\u00e9 III",
                "Kate Crawford"
            ],
            "title": "Datasheets for datasets",
            "venue": "Commun. ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Aaron Gokaslan",
                "Vanya Cohen"
            ],
            "title": "Openwebtext corpus, 2019",
            "venue": "URL https:// skylion007.github.io/OpenWebTextCorpus/",
            "year": 2019
        },
        {
            "authors": [
                "Kelvin Guu",
                "Albert Webson",
                "Ellie Pavlick",
                "Lucas Dixon",
                "Ian Tenney",
                "Tolga Bolukbasi"
            ],
            "title": "Simfluence: Modeling the influence of individual training examples by simulating training runs",
            "venue": "arXiv preprint arXiv:2303.08114,",
            "year": 2023
        },
        {
            "authors": [
                "Alon Jacovi",
                "Avi Caciularu",
                "Omer Goldman",
                "Yoav Goldberg"
            ],
            "title": "Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks",
            "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2023
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Frank F. Xu",
                "Jun Araki",
                "Graham Neubig"
            ],
            "title": "How can we know what language models know",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Jean Kaddour",
                "Joshua Harris",
                "Maximilian Mozes",
                "Herbie Bradley",
                "Roberta Raileanu",
                "Robert McHardy"
            ],
            "title": "Challenges and applications of large language models",
            "venue": "arXiv preprint arXiv:2307.10169,",
            "year": 2023
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "venue": "arXiv preprint arXiv:2001.08361,",
            "year": 2020
        },
        {
            "authors": [
                "Denis Kocetkov",
                "Raymond Li",
                "Loubna Ben allal",
                "Jia LI",
                "Chenghao Mou",
                "Yacine Jernite",
                "Margaret Mitchell",
                "Carlos Mu\u00f1oz Ferrandis",
                "Sean Hughes",
                "Thomas Wolf",
                "Dzmitry Bahdanau",
                "Leandro Von Werra",
                "Harm de Vries"
            ],
            "title": "The stack: 3 TB of permissively licensed source code",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Katherine Lee",
                "Daphne Ippolito",
                "Andrew Nystrom",
                "Chiyuan Zhang",
                "Douglas Eck",
                "Chris CallisonBurch",
                "Nicholas Carlini"
            ],
            "title": "Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "year": 2022
        },
        {
            "authors": [
                "Hector J. Levesque",
                "Ernest Davis",
                "Leora Morgenstern"
            ],
            "title": "The winograd schema challenge",
            "venue": "In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning,",
            "year": 2012
        },
        {
            "authors": [
                "Mustar",
                "Fran\u00e7ois Lagunas",
                "Alexander Rush",
                "Thomas Wolf"
            ],
            "title": "Datasets: A community library for natural language processing",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,",
            "year": 2021
        },
        {
            "authors": [
                "Raymond Li",
                "Loubna Ben Allal",
                "Yangtian Zi",
                "Niklas Muennighoff",
                "Denis Kocetkov",
                "Chenghao Mou",
                "Marc Marone",
                "Christopher Akiki",
                "Jia Li",
                "Jenny Chim"
            ],
            "title": "Starcoder: may the source be with you",
            "venue": "arXiv preprint arXiv:2305.06161,",
            "year": 2023
        },
        {
            "authors": [
                "Kyle Lo",
                "Lucy Lu Wang",
                "Mark Neumann",
                "Rodney Kinney",
                "Daniel Weld"
            ],
            "title": "S2ORC: The semantic scholar open research corpus",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4969\u20134983,",
            "year": 2020
        },
        {
            "authors": [
                "Shayne Longpre",
                "Gregory Yauney",
                "Emily Reif",
                "Katherine Lee",
                "Adam Roberts",
                "Barret Zoph",
                "Denny Zhou",
                "Jason Wei",
                "Kevin Robinson",
                "David Mimno"
            ],
            "title": "A pretrainer\u2019s guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity",
            "venue": "arXiv preprint arXiv:2305.13169,",
            "year": 2023
        },
        {
            "authors": [
                "Sasha Luccioni",
                "Yacine Jernite",
                "Margaret Mitchell"
            ],
            "title": "Data measurements tool, 2021",
            "venue": "URL https://huggingface.co/blog/data-measurements-tool",
            "year": 2021
        },
        {
            "authors": [
                "Marc Marone",
                "Benjamin Van Durme"
            ],
            "title": "Data portraits: Recording foundation model training data",
            "venue": "arXiv preprint arXiv:2303.03919,",
            "year": 2023
        },
        {
            "authors": [
                "R. Thomas McCoy",
                "Shunyu Yao",
                "Dan Friedman",
                "Matthew Hardy",
                "Thomas L. Griffiths"
            ],
            "title": "Embers of autoregression: Understanding large language models through the problem they are trained to solve",
            "venue": "arXiv preprint arXiv:2309.13638,",
            "year": 2023
        },
        {
            "authors": [
                "Angelina McMillan-Major",
                "Emily M. Bender",
                "Batya Friedman"
            ],
            "title": "Data statements: From technical concept to community practice",
            "venue": "ACM J. Responsib. Comput.,",
            "year": 2023
        },
        {
            "authors": [
                "Margaret Mitchell",
                "Alexandra Sasha Luccioni",
                "Nathan Lambert",
                "Marissa Gerchick",
                "Angelina McMillan-Major",
                "Nazneen Ozoani",
                "Ezinwanne Rajani",
                "Tristan Thrush",
                "Yacine Jernite",
                "Douwe Kiela"
            ],
            "title": "Measuring data",
            "venue": "In arXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Anthony Moi",
                "Nicolas Patry"
            ],
            "title": "HuggingFace\u2019s Tokenizers, April 2023",
            "venue": "URL https://github.com/ huggingface/tokenizers",
            "year": 2023
        },
        {
            "authors": [
                "Amandalynne Paullada",
                "Inioluwa Deborah Raji",
                "Emily M. Bender",
                "Emily Denton",
                "Alex Hanna"
            ],
            "title": "Data and its (dis)contents: A survey of dataset development and use in machine learning research",
            "venue": "In Patterns,",
            "year": 2021
        },
        {
            "authors": [
                "Guilherme Penedo",
                "Quentin Malartic",
                "Daniel Hesslow",
                "Ruxandra Cojocaru",
                "Alessandro Cappelli",
                "Hamza Alobeidli",
                "Baptiste Pannier",
                "Ebtesam Almazrouei",
                "Julien Launay"
            ],
            "title": "The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only",
            "venue": "arXiv preprint arXiv:2306.01116,",
            "year": 2023
        },
        {
            "authors": [
                "Aleksandra Piktus",
                "Christopher Akiki",
                "Paulo Villegas",
                "Hugo Lauren\u00e7on",
                "G\u00e9rard Dupont",
                "Sasha Luccioni",
                "Yacine Jernite",
                "Anna Rogers"
            ],
            "title": "The ROOTS search tool: Data transparency for LLMs",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations),",
            "year": 2023
        },
        {
            "authors": [
                "Aleksandra Piktus",
                "Odunayo Ogundepo",
                "Christopher Akiki",
                "Akintunde Oladipo",
                "Xinyu Zhang",
                "Hailey Schoelkopf",
                "Stella Biderman",
                "Martin Potthast",
                "Jimmy Lin"
            ],
            "title": "GAIA search: Hugging face and pyserini interoperability for NLP training data exploration",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations),",
            "year": 2023
        },
        {
            "authors": [
                "Mohammad Taher Pilehvar",
                "Jose Camacho-Collados"
            ],
            "title": "WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
            "year": 2019
        },
        {
            "authors": [
                "Giada Pistilli",
                "Carlos Mu\u00f1oz Ferrandis",
                "Yacine Jernite",
                "Margaret Mitchell"
            ],
            "title": "Stronger together: On the articulation of ethical charters, legal tools, and technical documentation in ml",
            "venue": "In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency,",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog post,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Yasaman Razeghi",
                "Robert L Logan IV",
                "Matt Gardner",
                "Sameer Singh"
            ],
            "title": "Impact of pretraining term frequencies on few-shot numerical reasoning",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Yasaman Razeghi",
                "Raja Sekhar Reddy Mekala",
                "Robert L Logan Iv",
                "Matt Gardner",
                "Sameer Singh"
            ],
            "title": "Snoopy: An online interface for exploring the effect of pretraining term frequencies on few-shot LM performance",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,",
            "year": 2022
        },
        {
            "authors": [
                "Melissa Roemmele",
                "Cosmin Adrian Bejan",
                "Andrew S Gordon"
            ],
            "title": "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
            "venue": "In AAAI spring symposium: logical formalizations of commonsense reasoning,",
            "year": 2011
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Oscar Sainz",
                "Jon Ander Campos",
                "Iker Garc\u00eda-Ferrero",
                "Julen Etxaniz",
                "Eneko Agirre"
            ],
            "title": "URL https://hitz-zentroa.github.io/lm-contamination/ blog",
            "venue": "Did chatgpt cheat on your test?,",
            "year": 2023
        },
        {
            "authors": [
                "Keisuke Sakaguchi",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi"
            ],
            "title": "Winogrande: An adversarial winograd schema challenge at scale",
            "venue": "Commun. ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Nithya Sambasivan",
                "Shivani Kapania",
                "Hannah Highfill",
                "Diana Akrong",
                "Praveen Paritosh",
                "Lora M Aroyo"
            ],
            "title": "everyone wants to do the model work, not the data work\u201d: Data cascades in high-stakes ai",
            "venue": "In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, CHI \u201921,",
            "year": 2021
        },
        {
            "authors": [
                "David Saxton",
                "Edward Grefenstette",
                "Felix Hill",
                "Pushmeet Kohli"
            ],
            "title": "Analysing mathematical reasoning abilities of neural models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Preethi Seshadri",
                "Sameer Singh",
                "Yanai Elazar"
            ],
            "title": "The bias amplification paradox in text-to-image generation",
            "venue": "arXiv preprint arXiv:2308.00755,",
            "year": 2023
        },
        {
            "authors": [
                "Jaime Sevilla",
                "Lennart Heim",
                "Anson Ho",
                "Tamay Besiroglu",
                "Marius Hobbhahn",
                "Pablo Villalobos"
            ],
            "title": "Compute trends across three eras of machine learning",
            "venue": "In 2022 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2022
        },
        {
            "authors": [
                "Seongjin Shin",
                "Sang-Woo Lee",
                "Hwijeen Ahn",
                "Sungdong Kim",
                "HyoungSeok Kim",
                "Boseop Kim",
                "Kyunghyun Cho",
                "Gichang Lee",
                "Woomyoung Park",
                "Jung-Woo Ha",
                "Nako Sung"
            ],
            "title": "On the effect of pretraining corpora on in-context learning by a large-scale language model",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Simig",
                "Tianlu Wang",
                "Verna Dankers",
                "Peter Henderson",
                "Khuyagbaatar Batsuren",
                "Dieuwke Hupkes",
                "Mona Diab"
            ],
            "title": "Text characterization toolkit (TCT)",
            "venue": "In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: System Demonstrations,",
            "year": 2022
        },
        {
            "authors": [
                "Luca Soldaini",
                "Kyle Lo"
            ],
            "title": "peS2o (Pretraining Efficiently on S2ORC) Dataset",
            "venue": "Technical report, Allen Institute for AI,",
            "year": 2023
        },
        {
            "authors": [
                "Luke Zettlemoyer",
                "Noah A. Smith",
                "Hanna Hajishirzi",
                "Iz Beltagy",
                "Dirk Groeneveld",
                "Jesse Dodge",
                "Kyle Lo"
            ],
            "title": "Dolma: an open corpus of three trillion tokens for language model pretraining research",
            "venue": "arXiv preprint arXiv:2402.00159,",
            "year": 2024
        },
        {
            "authors": [
                "Nishant Subramani",
                "Sasha Luccioni",
                "Jesse Dodge",
                "Margaret Mitchell"
            ],
            "title": "Detecting personal information in training corpora: an analysis",
            "venue": "In Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP",
            "year": 2023
        },
        {
            "authors": [
                "MosaicML NLP Team"
            ],
            "title": "Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023",
            "venue": "URL www.mosaicml.com/blog/mpt-7b",
            "year": 2023
        },
        {
            "authors": [
                "Kushal Tirumala",
                "Daniel Simig",
                "Armen Aghajanyan",
                "Ari S Morcos. D"
            ],
            "title": "Improving llm pretraining via document de-duplication and diversification",
            "venue": "In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar",
                "Aurelien Rodriguez",
                "Armand Joulin",
                "Edouard Grave",
                "Guillaume Lample"
            ],
            "title": "LLaMA: Open and Efficient Foundation Language Models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman"
            ],
            "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki"
            ],
            "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax",
            "year": 2021
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel"
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Rui Zhang",
                "Joel Tetreault"
            ],
            "title": "This email could save your life: Introducing the task of email subject line generation",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Xuhui Zhou",
                "Maarten Sap",
                "Swabha Swayamdipta",
                "Yejin Choi",
                "Noah Smith"
            ],
            "title": "Challenges in automated debiasing for toxic language detection",
            "venue": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Radford"
            ],
            "title": "The only exception is the toxic language analysis (\u00a7B.3.3) that relies on an English lexicon and classifier. However, we note that given non-English lexicon and classifier, the analysis can be easily repeated for other languages using our framework. OPENWEBTEXT is an open-source reproduction8 (Gokaslan & Cohen, 2019) of the data used to train GPT-2",
            "venue": "(Radford et al.,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "1 INTRODUCTION\nData is the foundation upon which machine learning (ML) is built. The introduction of new datasets drives progress, playing a crucial role in facilitating research and the creation of models with novel capabilities. Over time, the computational cost of AI experiments has dramatically increased, partly due to training increasingly large models on increasingly large datasets (Schwartz et al., 2020; Sevilla et al., 2022); today, some of the most impactful datasets are being created by scraping text from the entire publicly-available internet (Raffel et al., 2020; Together Computer, 2023; Penedo et al., 2023; Soldaini et al., 2024). These are some of the largest text datasets that have ever been built, and they are typically introduced with only a description of how they were made but no documentation of their contents. This is an important distinction, as we are now training models on massive text corpora without knowing what ideas, topics, toxicity, or personal information they contain.\nMeanwhile, language models (LMs) have become ubiquitous and are used by people worldwide daily. These AI systems directly impact people\u2019s lives, and thus, it has become vitally important to understand their capabilities and drawbacks. Models are only capable of learning from the data they were trained on, but analysis of pretraining corpora is hindered by lack of public release and by their massive size. Work analyzing the contents of web-scale corpora typically focuses on a subset of important dimensions, and there has been almost no work analyzing multiple datasets across the same dimensions. This means that ML practitioners have no practical tools to describe differences between datasets before choosing which one(s) to use.\nIn this work, we propose to investigate the content of large text corpora using WHAT\u2019S IN MY BIG DATA (WIMBD), a set of tools that enables practitioners to easily explore and quickly analyze large language datasets. We also use this tool to provide some of the first measurements across different web-scale datasets that are directly comparable. WIMBD has two components: (1) a search tool that enables programmatic access to search for documents containing a query using an Elasticsearch1 (ES) index. ES is a search engine that allows retrieving strings from a corpus, the documents where they appeared, and the number of times they appeared. (2) a count functionality, built using map-reduce (Dean & Ghemawat, 2008), allowing quick iteration over an entire dataset and extraction of relevant information, e.g., the character length distribution of documents, duplicates, domain counts, finding personally identifiable information (PII), and more. WIMBD is extendable and can be used to index, count, and analyze other corpora at scale (we benchmark the runtimes in Appendix D).\nUsing these tools, we perform a set of sixteen analyses on ten different English corpora used to train LMs, including C4 (used to train T5; Raffel et al., 2020), The Pile (used to train Pythia; Gao et al., 2020; Biderman et al., 2022; 2023), and RedPajama (used to reproduce Llama, Touvron et al., 2023, and to train RedPajama-INCITE; Together Computer, 2023). We divide our analyses into four categories: (1) data statistics (e.g., number of tokens and domain distribution; \u00a74.2); (2) data quality (e.g., most frequent n-grams and measuring duplicate documents; \u00a74.3); (3) community- and societyrelevant measurements (e.g., benchmark contamination and personally identifiable information detection; \u00a74.4); and (4) cross-corpora analysis (e.g., comparing the most common n-gram and document overlap; \u00a7B.4). An illustration of WIMBD is presented in Figure 1.\nOur work presents many insights on data distribution and anomalies. For example, inspecting the distribution over document lengths exposes anomalies where specific lengths are overrepresented relative to neighboring lengths; these anomalies often correspond to near-duplicate template-generated text or documents arbitrarily truncated to a specific character length. As another example, punctuation sequences are frequently the most common n-grams, such as a dash (\u2018-\u2019) repeated ten times as the most common 10-gram in The Pile. WIMBD offers both retrospective documentation and grounding of model behavior to their training data and actionable insights for higher-quality corpora curation.\n2 BACKGROUND: ON THE IMPORTANCE OF DATA UNDERSTANDING\nThere have been repeated calls for ML practitioners to provide better data documentation (e.g., McMillan-Major et al., 2023; Bender & Friedman, 2018; Mitchell et al., 2023; Pistilli et al., 2023; Paullada et al., 2021; Gebru et al., 2021). On the other hand, some of the most impactful ML models are increasingly opaque, specifically with respect to the most important component of recent advancements: data. With the increasingly competitive nature of the field, developers of systems like GPT-4 (OpenAI, 2023) and PaLM-2 (Google, 2023) have been offering little transparency into the most important development decisions, including the sources, size, and contents of their training data.\nAs web-scale datasets drive this rapid progress in modern ML systems, the gap between data transparency and documentation is more striking than ever (Kaddour et al., 2023). From a technical standpoint, the massive size of these datasets makes analysis of their contents challenging; even if OpenAI or Google shared their training data, it\u2019s unclear where to start understanding it in its entirety. Tools like the Data Measurements Tool (Luccioni et al., 2021) and Know Your Data (Google, 2021) work towards improving data documentation, but focus on smaller datasets since the scale of web data leads to significant technical challenges. Our work aims to address this critical missing component.\n1https://www.elastic.co/elasticsearch/\nWhile other works support indexing and analyses of large corpora (Piktus et al., 2023a; Marone & Van Durme, 2023; Simig et al., 2022; Piktus et al., 2023b; Razeghi et al., 2022b), these efforts support a single corpus and often do not support programmatic access to the data or the analysis. Instead, we offer a holistic approach that combines search and counting with a package that allows programmatic access through wrappers on top of the ES API and extendable efficient counting capabilities.\nAdditional efforts are concerned with the effect of data on model behavior. Longpre et al. (2023) investigate how the composition of LMs\u2019 pretraining data influences their downstream performance. Razeghi et al. (2022a) measure high correlation between term frequency and LMs\u2019 few-shot reasoning capabilities with those terms. Shin et al. (2022) study the effect of pretraining corpora on in-context abilities. Seshadri et al. (2023) demonstrate that text-to-image models mimic biases from their training data. Akyurek et al. (2022) study fact tracing for identifying pretraining examples that enable a factual assertion, while Guu et al. (2023) offer a training run simulator, which allows making counterfactual queries on what a model would have learned under a different training procedure. These efforts separately built dedicated infrastructure to perform the studies. Our work provides a dedicated interface and tooling that allows performing a wide range of analyses on large-scale corpora, categorizing and offering novel analyses that highlight new insights into these large corpora.\n3 WIMBD: THE PLATFORM\nTable 1: Summary of the capabilities WIMBD provides and the analyses enabled by them.\nBasic Ability Analyses\nExact Counts (\u00a73.1)\nDocument Counts, min/max doc length, #tokens, domain distribution, utterance date statistics, geolocation, language distribution, length distribution, toxic language, personally identifiable information, demographic sentiment co-occurrences\nCompressed Counts (\u00a73.1) Duplicates, most & least common n-grams\nSearch (\u00a73.2) Benchmark contamination, n-gram counts\nA core desideratum of WIMBD is to enable quick processing of terabytes of data. As such, we focus on uncomplicated, standard methods from the information retrieval and data management communities. WIMBD is comprised of two basic components: counting and search (retrieval). Fast counting and retrieving enable us to answer fundamental questions about data, as we demonstrate in Section 4. We summarize the\nframework abilities and types of analyses in Table 1. We run our experiments using a compute node machine with 224 CPUs and 882GB RAM, and an Elasticsearch cluster for the indexed corpora.\n3.1 COUNTING\nDue to the sparsity of language data and the scale of the data of interest, accurate counting can be challenging. We leverage the map-reduce framework (Dean & Ghemawat, 2008). We provide two approaches for counting, described below.\nExact Counts The exact counts approach is designed for cases where the number of possible values is tractable and can fit in memory. This fits cases where we are interested in calculating a bound number of variables of interest (e.g., number of documents,\u00a74.2, or document length, \u00a74.3.3).\nCompressed Counts The compressed counts approach is designed for cases where the number of possible values is intractable. For instance, the total 10-grams in a large corpus can be very high, and the memory usage to compute all of them would be overwhelming. Similarly, finding duplicates requires keeping and comparing the strings of all documents in memory. In the case of C4, that would require over 800 GB of RAM. Instead, we apply a compression function (e.g., hashing, Bloom, 1970) to those values, reducing memory footprint while sacrificing some accuracy (due to hash collisions). For example, when finding the most common 10-grams, we store a table of counts where the keys in the table correspond to hashes of 10-grams. The hash table size is configurable according to the amount of memory available. The larger the hash table, the smaller the probability of hash collisions and, therefore, the higher the accuracy of the counts. E.g., unigram estimates are more accurate than 10-gram estimates since the number of possible values is much smaller.\n3.2 SEARCHING\nThe second part of WIMBD allows fast text retrieval. For instance, we can get the number of documents mentioning a word or sequence (document frequency). It also allows more complex Boolean queries. While search and retrieval have numerous implementations, such as reverse indices, suffix arrays,\nsuffix trees for exact match search, and dense retrieval for fuzzy search, in this work, we use ES, an inverted index. We build a wrapper on top of the ES API, allowing tailored and customized searches to fit our analysis requirements. We leave it to future work to explore other search alternatives.\n4 WIMBD: THE ANALYSES\nThis section presents analyses conducted in WIMBD, grouped by category. First, we describe the ten corpora considered in this study (\u00a74.1). We then consider four high-level categories, each split into several analyses: data statistics (\u00a74.2), data quality (\u00a74.3), and community- and society-relevant measurements (\u00a74.4). Cross-corpus analyses, as well as elaborations and more analyses are presented in the appendix (\u00a7B). Our analyses are inspired by previous works (Dodge et al., 2021; Gao et al., 2020), but we expand them to multiple corpora, extend the types of analyses, and open-source our modular toolkit to encourage researchers to scrutinize their corpora. We offer the first extensive analyses on ten, combining extension of previous analyses and several novel ones.\n4.1 CORPORA\nWe cover ten different large corpora, spanning across text-only (e.g., C4) to image captions (LAION2B-en) and code (The Stack). These corpora have been used in training language models (or similar large-scale models, such as Stable Diffusion; Rombach et al. 2022). A high-level description of these datasets using WIMBD is presented in Table 2, and further details about the construction and origin of these corpora are detailed in Appendix A.\n4.2 DATA STATISTICS\nMain Findings\n\u2022 Four out of the ten corpora we consider have \u2018empty\u2019 documents (meaning they contain only space-like characters), while The Pile and RedPajama contain the same longest document (with over 28 million tokens) of an encyclopedia. \u2022 While the most common source of webpages in C4 originates from www.nytimes.com, it consists of less than 0.05% of the total web pages, mC4-en most common domain is google.com (over 5% of the documents), and cdn.shopify.com contributes almost 6% to the total documents in LAION-2B-en.\n4.2.1 SUMMARY STATISTICS\nWe begin by computing some summary statistics and present the results in Table 2. Using the Exact Counts we compute the following high-level statistics of a corpus: (1) size, (2) number of documents, (3) number of tokens,2 (4) the size of the longest document, and (5) the size of the shortest document. Out of all corpora, mC4-en is the largest, which takes 14.7TB of disk, and 2.7 trillion tokens. After that comes The Stack with a size of 7.8TB, and more than 1.5 trillion tokens. Interestingly, four corpora contain documents with empty strings: LAION-2B-en (81 total), which typically contain a sequence of white spaces. In The Stack (1,350 total), RedPajama (3,877), and The\n2We use Unicode text segmentation (Unicode, 2023) as a tokenizer, but we support any tokenizer supported by HuggingFace\u2019s tokenizers library (Moi & Patry, 2023).\nPile (7,533), documents typically contain a mix of special characters that denote spacing (e.g., \u2018\\n\u2019, or \u2018\\t\u2019). In RedPajama, all of the empty strings are from the arXiv subset. The longest document in The Stack is a json file, with 26,298,134 tokens from http://jquery.com/. The longest document in The Pile and RedPajama is the same encyclopedia book called \u201cINTERNATIONAL ENCYCLOPEDIA OF THE SOCIAL & BEHAVIORAL SCIENCES\u201d from the Books3 subset with 28,121,329 tokens.\n4.2.2 INTERNET DOMAIN DISTRIBUTION\nSome corpora contain metadata information about the URL where the documents came from. As such, we employ the Exact Counts functionality, to parse the entire corpus, and extract information from the URLs about the (1) schemas (e.g., http, https), (2) domains (e.g., www.google.com, en.wikipedia.org, etc.), and (3) suffixes (e.g., com, org, de, etc.).\nWe apply these counts on the corpora that contain this information, namely C4, mC4-en, OSCAR, RedPajama, and LAION-2B-en. Starting with the domain analysis, we perform these counts twice: once when each domain is counted per document (yielding documents per domain) and another where each domain is counted per token (yielding tokens per domain). We present the results of three corpora per token in Figure 2 (and the full results in Appendix B.1). First, we note that C4 contains documents from a diverse set of domains, and even the percentage of the most common one, patents.google.com, is less than 0.05%. On the other hand, in the case of LAION2B-en, cdn.shopify.com is responsible for more than 6% of the documents. Similarly, arxiv.org is responsible for more than 12% of the documents in RedPajama. We showcase the results of the domains for the other corpora, as well as the schemas and suffixes in Appendix B.1.\n4.3 DATA QUALITY\nMain Findings\n\u2022 The most common n-grams often correspond to repeated punctuation marks and duplicates. \u2022 While more than 60% of documents in The Pile are duplicates (unsurprisingly due to\noversampling), RedPajama and LAION-2B-en also contain about 50% duplicate documents. \u2022 Document length distribution reveals interesting (and unexpected) outliers of documents, often resulting from duplicate documents and idiosyncratic data decisions.\n4.3.1 MOST & LEAST COMMON n-GRAMS\nMeasuring outliers can reveal interesting insights about a corpus (Mitchell et al., 2023), We explore the most and least common token n-grams of each corpus using the Compressed Counts . We compute the 10K most common n-grams for all corpora, with n \u2208 {1, 2, 3, 10}. We report the results of the ten most common 10-grams in Table 3 and of the ten most common uni-, bi-, and tri-grams in Table 9 in the Appendix. Identical n-grams across corpora are highlighted in the same colors.\nThe different corpora contain a lot of uncleaned html or markdown format (e.g., ten times \u2018?\u2019 or \u2018amp\u2019), or boilerplate texts such as: \u201c. You can follow any responses to this entry through\u201d in C4, or \u201c( Log Out / Change ) You are commenting using\u201d in OSCAR, and formatting (\u201c[1][2][3][\u201d) in S2ORC and peS2o, which signifies references.\nA striking finding from this analysis is the vast repetition of such 10-grams. For instance, \u2018?\u2019, \u2018.\u2019, and \u2018-\u2019 repeated ten times appear 9, 7.2, and 4.4 million times, respectively, in C4. We perform a manual analysis on the repeating question marks in C4 to better understand the scenarios where they\nappear on the ten consecutive question marks symbols and categorize each appearance into writing, noise, and format occurrence. Analyzing 100 random documents, we found that 68% of documents use such n-grams as part of their writing style (e.g., ... $6???????????? How is that possible?, or ... So what do u think?????????????????????????). 18% are due to noise as we could not understand the context or content of the writing (e.g., ... e ??????????????? kap chit-koa ??), and finally, 14% of the documents were due to different format styles or issues (e.g., a sequence of question marks following by a \u2018normal\u2019 text, or a sequence of question marks between keywords).\n4.3.2 DUPLICATES\nPrevious work has found that duplication can affect the quality of pretraining data, impacting sample efficiency (Lee et al., 2022; Tirumala et al., 2023) and memorization (Carlini et al., 2023). While more recent work finds contradictory evidence on data with less web-scraped text (Biderman et al., 2023), measuring duplication in pretraining data is necessary for future research on its effects. We calculate duplicates by matching documents with an MD5 hash of their texts (using Compressed Counts ). If more than a single document has the same hash, we consider them duplicates.3 We examine the duplication of document text and URLs within each dataset. While some datasets explicitly deduplicate their content, others do not, and some even oversample some sources.\nIn Figure 3 we show counts and ratios of duplication across datasets with greater than 1% documents duplicated, and all datasets are shown in Table 13 in the appendix. These are based on two kinds of counts: (1) the count of documents in all clusters of duplicate text (in blue) and (2) the count of duplicate clusters (in orange). As expected, deduplicated corpora such as C4 have no exact duplicates (as those were filtered out of the corpus). In contrast, The Pile, which intentionally oversampled some data sources, has many duplicates (139M documents belonging to 64.6M duplicate text clusters). LAION-2B-en has the second highest ratio of duplicate documents (1.25B documents belonging to 342M duplicate text clusters), perhaps due to the smaller space of short sentences common in\n3To test for hash collisions, we rerun the analysis with a different random seed. None of the > 7 billion hashes across the ten corpora had a different count. This could only occur if an identical number of collisions conflated an identical set of counts or, more likely, there were no collisions.\nits image \u201calt text\u201d source. Figure 15 in the appendix showcase the images of the most common duplicates in LAION-2B-en, with the most common images describe mainly receipts.\nTable 4 showcases duplicates with the most occurrences in four corpora. These duplicates vary dramatically in length and domain. LAION-2B-en, OSCAR, and RedPajama have clusters with the most occurrences, in the hundreds of thousands and above. Top duplicates in LAION-2B-en are shorter and describe products and website features. OSCAR\u2019s top duplicates are all instances of website boilerplate.4 RedPajama\u2019s top duplicates come from similar templated citation information.\n4.3.3 DOCUMENT LENGTH DISTRIBUTION\nWe compute document length distributions with Exact Counts . We expect a smooth distribution over document lengths, and deviation from such a distribution may indicate the presence of artificial documents or near duplicates.5 We compute the character length distribution and present results for three corpora in Figure 4 (additional results in Appendix B.2.3).\nWhile C4 is free of duplicate documents, it include clusters of template-generated near-duplicate documents exposed by outliers of identical document lengths. Beyond template-generated user-facing copy (e.g., template-generated documents from a reverse phone lookup site, each associated with a unique phone number), we find clusters of template-generated JavaScript snippets, and large collections of unique documents, including\nnumerous permutations of the same keywords, likely crafted for SEO purposes.\nThe Pile, featuring the longest documents, has a notable outlier with nearly 1% of its documents precisely 8,194 characters long. These outliers are derived from the DeepMind Mathematics dataset (Saxton et al., 2019), truncated to fit this length. The Pile also contains a significant number of short template-generated code snippets, e.g., a number of documents (of lengths 9, 18, and 36 tokens) each corresponding to a unique publication in various medical journals, and to auto-generated metadata files (of length 20 tokens) used in the Unity game engine. While OSCAR has no documents shorter than 100 characters, as those were filtered, it contains many near-duplicate documents that correspond to website boilerplate, e.g., template-generated FAQs about how to use the forum software phpBB.\n4.4 COMMUNITY- AND SOCIETY-RELEVANT MEASUREMENTS\nMain Findings\n\u2022 Instances of popular benchmarks like GLUE and SuperGLUE, were found in various corpora (e.g., C4 and RedPajama), render them unusable for fair model evaluation. \u2022 Automatic toxicity detection reveals that 1\u201316.5% of the documents in the corpora contain toxic language using an automatic classifier and between 0.01-16.6% using a taxonomy. \u2022 An estimated 200M, 4B, and 97M of email addresses, phone numbers, and IP addresses were found in the most PII-contaminated corpora per token (mC4-en).\n4.4.1 BENCHMARK CONTAMINATION\nAs corpora grow and new evaluation datasets are created, the risk of contamination\u2014where evaluation data are included in a (pre)training corpus\u2014increases. As such, it is important to track contamination (Sainz et al., 2023; Jacovi et al., 2023).6 Using Search , we provide a contamination analysis of 82 datasets for four popular corpora: The Pile, C4, RedPajama, and OSCAR. We consider all datasets\n4Many of these duplicate documents indicate that the user agent used to collect the dataset received automatic responses blocking it from crawling the website\u2019s contents.\n5Outlier lengths are those whose prevalence across the corpus is significantly higher than neighboring lengths. 6When evaluating a model trained on an existing corpus, one should exempt contaminated evaluation sets. However, in the case of new corpus construction, practitioners may use WIMBD for decontaminating the corpus itself to maintain the evaluation data integrity.\nfrom PromptSource (Bach et al., 2022), a repository containing prompts for 279 different datasets (as of May 2023). We filter datasets we cannot automatically download, from Huggingface datasets (Lhoest et al., 2021), and datasets that do not have a test split. In addition, we only consider datasets that contain at least two inputs (e.g., natural language inference), leaving us with 82 datasets.\nWe measure contamination by testing whether all input fields are present in a single document and report the percentage of contaminated examples from the test set. Our contamination evaluation serves as an upper bound of exact-match dataset contamination. We provide more details of our analysis and design choices in Appendix B.3.1.\nContaminated datasets We present the results in Figure 5. We showcase all benchmarks whose contamination percentages are at least 5% in one of the four corpora. We find that RedPajama is the most contaminated dataset out of the four, where in eight out of the 15 corpora, its contamination rate is above 50%, and fully contaminated in the case of COPA (Roemmele et al., 2011). The Pile\u2019s contamination rates are lower, but it is also contaminated with a few datasets, such as aesic (Zhang & Tetreault, 2019), WSC (Levesque et al., 2012) and WIC (Pilehvar & Camacho-Collados, 2019), which were included in the SuperGLUE evaluation benchmark (Wang et al., 2019).\nMost examined datasets were not found in the corpora. It is important to note that while we find some contamination, most of the considered benchmarks do not appear in the corpora we investigated (67 out of the 82 datasets). For instance, Winogrande (Sakaguchi et al., 2021), a large corpus in the style of the Winograd schema, does not appear in any of the examined corpora.\n4.4.2 PERSONALLY IDENTIFIABLE INFORMATION\nTable 5: Extrapolated PII frequencies. Count is the extrapolated frequency and Prec. is our identification precision accuracy, estimated by manual analysis of 100 random examples.\nCorpus Email Addresses Phone Numbers IP Addresses Count Prec. Count Prec. Count Prec. OpenWebText 364K 99 533K 87 70K 54 OSCAR 62.8M 100 107M 91 3.2M 43 C4 7.6M 99 19.7M 92 796K 56 mC4-en 201M 92 4B 66 97.8M 44 The Pile 19.8M 43 38M 65 4M 48 RedPajama 35.2M 100 70.2M 94 1.1M 30 S2ORC 630K 100 1.4M 100 0K 0 peS2o 418K 97 227K 31 0K 0 LAION-2B-en 636K 94 1M 7 0K 0 The Stack 4.3M 53 45.4M 9 4.4M 55\nPII is \u201cinformation which can be used to distinguish or trace an individual\u2019s identity, such as their name, social security number, biometric records, etc.\u201d (Johnson III, 2007). Recent research has sought to extract PII from LMs (Carlini et al., 2021). These attacks highlight that LMs can ingest and reproduce PII contained in their training data, and show the risks of training on data that contains such information, even if the data remains private.\nWe document three kinds of personally identifiable information in pretraining corpora: phone numbers, email addresses, and IP addresses. We employ regular expressions corresponding to each PII type using the Exact Counts . We provide more details about our methodology, the regexes, additional results, and error analyses in Appendix B.3.2. We conduct a manual analysis to estimate the precision of these methods on all corpora. The results of this analysis, as well as the extrapolated frequency of these matches, are presented in Table 5. Our identification method is highly precise (>80% precision) for email addresses on eight out of 10 corpora, and for phone numbers on five of the 10 corpora. Overall, most corpora contain a high volume of PII information, varying in type based on the corpus. For instance, RedPajama contain mainly phone numbers (70.2M) and a smaller amount of IP Addresses (1.1M), but S2ORC and peS2o contain mainly email addresses (630K and 418K, respectively) and no IP addresses were identified. The most common PII across corpora is phone numbers, followed by email addresses and IP addresses (except for The Stack, which has more IP addresses than email addresses: 4.4M vs. 4.3M, and peS2o, which has more email addresses than phone numbers). Finally, we observe that mC4-en contains the largest amount of PII, also when controlling for the number of tokens (Table 19 in the Appendix).\n5 DISCUSSION\nData is one of the most poorly understood and studied components in ML research since \u201ceveryone wants to do the model work, not the data work\u201d (Sambasivan et al., 2021). Yet, it is one of the most critical factors for successfully training a state-of-the-art language model. While the benefit of increasing model size is evident from the trend of recent years, it is not enough by itself, as the amount and quality of data are crucial (Kaplan et al., 2020).\nData Curation With the increasing data needed to train LMs (and other models for other modalities), it remains challenging to curate high-quality datasets. Besides the technical challenges of composing a large-scale dataset and the decisions that go into making it, these decisions and their influence on the final models are costly to assess due to the high computational resources required to train such models. With WIMBD, we hope to ease the decisions that go into crafting large-scale datasets by surfacing patterns and trends about what goes into them and what is left out from different aspects, such as data quality, community and society measurements, etc. Once decisions upon what data is important, and which should be left out of a dataset, practitioners can filter documents or passages that adhere to such decisions. The curation of the Dolma dataset (Soldaini et al., 2024) that happened while developing this work benefited from iterations over the insights from this work, such as the finding of \u2018noisy\u2019 most-common n-grams, and bugs in the initial \u2018de-duplication\u2019 implementation.\nData Documentation Adding to previous works that call for more data documentation, such as Datasheets (Gebru et al., 2021) and Data Statements (McMillan-Major et al., 2023), we argue for the importance of documenting such information. While previous works often focused and tailored the documentation for supervised-style datasets (e.g., \u201cIs there a label or target associated with each instance?\u201d, \u201cHow was the data associated with each instance acquired?\u201d from Datasheets, and \u201cWhat are the demographic characteristics of the annotators and annotation guideline developers?\u201d from Data Statements) we call for more tailored documentation of large-scale pretraining corpora.7 This work offers a superset of the automatic full-corpus analyses proposed by Dodge et al. (2021); Gao et al. (2020), with several additions, categorization, and programmatic interface, allowing better understanding of the content of current and future large text corpora.\nGrounding Models to their Training Data Unlike other factors of language model training, such as model architecture or optimizer choice, training data comes in the same natural language format as language model\u2019s outputs and thus can be measured and described in all the same ways. As such, the data offers a unique opportunity for grounding models. For instance, a model\u2019s ability to recall factual knowledge is derived from its training data (Jiang et al., 2020; Elazar et al., 2021a). On the other hand, models often perform better on frequent occurrences (Razeghi et al., 2022a; McCoy et al., 2023), and on documents similar to models\u2019 training data (Longpre et al., 2023). The path to a holistic comprehension of model behavior is through the data, which requires an infrastructure investment to access big datasets and the right abstraction of data attributes.\n6 CONCLUSION\nIn this work, we propose WIMBD, a framework for processing and analyzing large text corpora. Using WIMBD, we study ten different corpora that were used to train language models (or vision and language models, such as Stable Diffusion). We uncover interesting insights about these corpora using sixteen different analyses across four aspects: high-level statistics, data quality, communityand society- relevant measurements, and cross-data analysis. For instance, the most common source of texts for the LAION-2B-en dataset are the commercial websites Pinterest, Shopify, SlidePlayer, Amazon, and eBay. Regarding data quality, we find that about 50% of RedPajama and LAION-2Ben\u2019s documents are duplicates. In addition, we find that many evaluation benchmarks, including several from GLUE and SuperGLUE, such as WSC, WIC, and RTE, are contaminated due to their appearance in corpora such as RedPajama. Besides the analyses, WIMBD offers an extendable platform for reproducing our analyses on other corpora, developing new ones, and answering research questions about data. We release all the code and artifacts for WIMBD to encourage researchers to adopt and extend our framework and analyze existing and new corpora.\n7Many questions are still relevant for large pretraining corpora (e.g., \u201cWhat do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?\u201d).\nACKNOWLEDGMENTS\nWe want to thank Ludwig Schmidt, Maarten Sap, and Emma Strubell, and the anonymous reviewers for discussions and feedback on this paper, Elizabeth Salesky for the help with Unicode rendering and getting excited about obscure Unicode characters with me, and Carissa Schoenick, Jon Borchardt, and Johann Dahm for assisting with visuals.\nREFERENCES Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Beno\u00eet Sagot. Towards a cleaner document-\noriented multilingual crawled corpus. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pp. 4344\u20134355, Marseille, France, June 2022. European Language Resources Association. URL https://aclanthology.org/2022.lrec-1.463.\nEkin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu. Towards tracing knowledge in language models back to the training data. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 2429\u20132446, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/ 2022.findings-emnlp.180. URL https://aclanthology.org/2022.findings-emnlp.180.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. Santacoder: don\u2019t reach for the stars! arXiv preprint arXiv:2301.03988, 2023. URL https://arxiv.org/abs/ 2301.03988.\nStephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike Tian-jian Jiang, and Alexander Rush. PromptSource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 93\u2013104, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-demo.9. URL https://aclanthology.org/2022.acl-demo.9.\nIz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientific text. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3615\u20133620, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1371. URL https://aclanthology.org/D19-1371.\nEmily M. Bender and Batya Friedman. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587\u2013604, 2018. doi: 10.1162/tacl_a_00041. URL https: //aclanthology.org/Q18-1041.\nStella Biderman, Kieran Bicheno, and Leo Gao. Datasheet for the pile, 2022. URL https:// arxiv.org/abs/2201.07311.\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397\u20132430. PMLR, 2023. URL https: //openreview.net/forum?id=bpRTAnJ8LW.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An open-source autoregressive language model. In Proceedings of BigScience Episode #5 \u2013 Workshop on Challenges & Perspectives in Creating Large Language Models, pp. 95\u2013136, virtual+Dublin,\nMay 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.bigscience-1.9. URL https://aclanthology.org/2022.bigscience-1.9.\nBurton H. Bloom. Space/time trade-offs in hash coding with allowable errors. Commun. ACM, 13(7): 422\u2013426, jul 1970. ISSN 0001-0782. URL https://doi.org/10.1145/362686.362692.\nNicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, \u00dalfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pp. 2633\u20132650. USENIX Association, August 2021. ISBN 978-1-939133-24- 3. URL https://www.usenix.org/conference/usenixsecurity21/presentation/carliniextracting.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= TatRHT_1cK.\nHyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, and Noah Constant. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=kXwdL1cWOAi.\nJeffrey Dean and Sanjay Ghemawat. Mapreduce: Simplified data processing on large clusters. Commun. ACM, 51(1):107\u2013113, jan 2008. URL https://doi.org/10.1145/1327452.1327492.\nJesse Dodge, Maarten Sap, Ana Marasovic\u0301, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 1286\u20131305, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlpmain.98. URL https://aclanthology.org/2021.emnlp-main.98.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Sch\u00fctze, and Yoav Goldberg. Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics, 9:1012\u20131031, 2021a. URL https://aclanthology.org/2021.tacl-1.60.\nYanai Elazar, Hongming Zhang, Yoav Goldberg, and Dan Roth. Back to square one: Artifact detection, training and commonsense disentanglement in the Winograd schema. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 10486\u201310500, Online and Punta Cana, Dominican Republic, November 2021b. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.819. URL https: //aclanthology.org/2021.emnlp-main.819.\nAli Emami, Kaheer Suleman, Adam Trischler, and Jackie Chi Kit Cheung. An analysis of dataset overlap on Winograd-style tasks. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 5855\u20135865, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.515. URL https://aclanthology.org/2020.coling-main.515.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. URL https://arxiv.org/abs/2101.00027.\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 III, and Kate Crawford. Datasheets for datasets. Commun. ACM, 64(12):86\u201392, nov 2021. ISSN 0001-0782. doi: 10.1145/3458723. URL https://doi.org/10.1145/3458723.\nAaron Gokaslan and Vanya Cohen. Openwebtext corpus, 2019. URL https:// skylion007.github.io/OpenWebTextCorpus/.\nGoogle. Know your data, 2021. URL https://github.com/pair-code/knowyourdata.\nGoogle. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. URL https://arxiv.org/ abs/2305.10403.\nKelvin Guu, Albert Webson, Ellie Pavlick, Lucas Dixon, Ian Tenney, and Tolga Bolukbasi. Simfluence: Modeling the influence of individual training examples by simulating training runs. arXiv preprint arXiv:2303.08114, 2023. URL https://arxiv.org/abs/2303.08114.\nAlon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 5075\u20135084, Singapore, December 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.emnlp-main.308.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423\u2013438, 2020. doi: 10.1162/tacl_a_00324. URL https://aclanthology.org/2020.tacl-1.28.\nClay Johnson III. Us office of management and budget memorandum m-07-16, 2007. URL https: //georgewbush-whitehouse.archives.gov/omb/memoranda/fy2007/m07-16.pdf.\nJean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023. URL https://arxiv.org/abs/2307.10169.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.\nDenis Kocetkov, Raymond Li, Loubna Ben allal, Jia LI, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro Von Werra, and Harm de Vries. The stack: 3 TB of permissively licensed source code. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/ forum?id=pxpbTdUEpD.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris CallisonBurch, and Nicholas Carlini. Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8424\u20138445, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.577. URL https://aclanthology.org/2022.acllong.577.\nHector J. Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR\u201912, pp. 552\u2013561. AAAI Press, 2012. ISBN 9781577355601. URL https: //dl.acm.org/doi/10.5555/3031843.3031909.\nQuentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario \u0160a\u0161ko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Cl\u00e9ment Delangue, Th\u00e9o Matussi\u00e8re, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Fran\u00e7ois Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 175\u2013184, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. URL https: //aclanthology.org/2021.emnlp-demo.21.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023. URL https://arxiv.org/abs/2305.06161.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4969\u20134983, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.447. URL https://www.aclweb.org/anthology/ 2020.acl-main.447.\nShayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. A pretrainer\u2019s guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity. arXiv preprint arXiv:2305.13169, 2023. URL https://arxiv.org/abs/2305.13169.\nSasha Luccioni, Yacine Jernite, and Margaret Mitchell. Data measurements tool, 2021. URL https://huggingface.co/blog/data-measurements-tool.\nMarc Marone and Benjamin Van Durme. Data portraits: Recording foundation model training data. arXiv preprint arXiv:2303.03919, 2023. URL https://arxiv.org/abs/2303.03919.\nR. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L. Griffiths. Embers of autoregression: Understanding large language models through the problem they are trained to solve. arXiv preprint arXiv:2309.13638, 2023. URL https://arxiv.org/abs/2309.13638.\nAngelina McMillan-Major, Emily M. Bender, and Batya Friedman. Data statements: From technical concept to community practice. ACM J. Responsib. Comput., may 2023. doi: 10.1145/3594737. URL https://doi.org/10.1145/3594737.\nMargaret Mitchell, Alexandra Sasha Luccioni, Nathan Lambert, Marissa Gerchick, Angelina McMillan-Major, Nazneen Ozoani, Ezinwanne Rajani, Tristan Thrush, Yacine Jernite, and Douwe Kiela. Measuring data. In arXiv, 2023. URL https://arxiv.org/abs/2212.05129.\nAnthony Moi and Nicolas Patry. HuggingFace\u2019s Tokenizers, April 2023. URL https://github.com/ huggingface/tokenizers.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. URL https://arxiv.org/ abs/2303.08774.\nAmandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, and Alex Hanna. Data and its (dis)contents: A survey of dataset development and use in machine learning research. In Patterns, 2021. URL https://www.sciencedirect.com/science/article/pii/ S2666389921001847.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. URL https://arxiv.org/abs/2306.01116.\nAleksandra Piktus, Christopher Akiki, Paulo Villegas, Hugo Lauren\u00e7on, G\u00e9rard Dupont, Sasha Luccioni, Yacine Jernite, and Anna Rogers. The ROOTS search tool: Data transparency for LLMs. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pp. 304\u2013314, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-demo.29. URL https://aclanthology.org/2023.acl-demo.29.\nAleksandra Piktus, Odunayo Ogundepo, Christopher Akiki, Akintunde Oladipo, Xinyu Zhang, Hailey Schoelkopf, Stella Biderman, Martin Potthast, and Jimmy Lin. GAIA search: Hugging face and pyserini interoperability for NLP training data exploration. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pp. 588\u2013598, Toronto, Canada, July 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-demo.57. URL https://aclanthology.org/2023.acl-demo.57.\nMohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1267\u20131273, Minneapolis, Minnesota, June\n2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1128. URL https: //aclanthology.org/N19-1128.\nGiada Pistilli, Carlos Mu\u00f1oz Ferrandis, Yacine Jernite, and Margaret Mitchell. Stronger together: On the articulation of ethical charters, legal tools, and technical documentation in ml. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201923, pp. 343\u2013354, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701924. doi: 10.1145/3593013.3594002. URL https://doi.org/10.1145/3593013.3594002.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog post, 2019. URL https://openai.com/ research/better-language-models.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL http://jmlr.org/papers/v21/20-074.html.\nYasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot numerical reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 840\u2013854, Abu Dhabi, United Arab Emirates, December 2022a. Association for Computational Linguistics. URL https://aclanthology.org/2022.findingsemnlp.59.\nYasaman Razeghi, Raja Sekhar Reddy Mekala, Robert L Logan Iv, Matt Gardner, and Sameer Singh. Snoopy: An online interface for exploring the effect of pretraining term frequencies on few-shot LM performance. In Wanxiang Che and Ekaterina Shutova (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 389\u2013395, Abu Dhabi, UAE, December 2022b. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-demos.39.\nMelissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI spring symposium: logical formalizations of commonsense reasoning, pp. 90\u201395, 2011. URL https://aaai.org/papers/02418-choice-of-plausible-alternatives-anevaluation-of-commonsense-causal-reasoning/.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684\u201310695, 2022.\nOscar Sainz, Jon Ander Campos, Iker Garc\u00eda-Ferrero, Julen Etxaniz, and Eneko Agirre. Did chatgpt cheat on your test?, Jun 2023. URL https://hitz-zentroa.github.io/lm-contamination/ blog/.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99\u2013106, aug 2021. URL https://doi.org/10.1145/3474381.\nNithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. \u201ceveryone wants to do the model work, not the data work\u201d: Data cascades in high-stakes ai. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, CHI \u201921, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450380966. doi: 10.1145/3411764.3445518. URL https://doi.org/10.1145/3411764.3445518.\nDavid Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=H1gR5iR5FX.\nTeven Le Scao, Angela Fan, Christopher Akiki, Elizabeth-Jane Pavlick, Suzana Ili\u2019c, Daniel Hesslow, Roman Castagn\u2019e, Alexandra Sasha Luccioni, Franccois Yvon, Matthias Gall\u00e9, Jonathan Tow, Alexander M. Rush, Stella Rose Biderman, Albert Webson, Pawan Sasanka Ammanamanchi,\nThomas Wang, Beno\u00eet Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurenccon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa Etxabe, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris C. Emezue, Christopher Klamm, Colin Leong, Daniel Alexander van Strien, David Ifeoluwa Adelani, Dragomir R. Radev, Eduardo Gonz\u2019alez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, G\u00e9rard Dupont, Germ\u00e1n Kruszewski, Giada Pistilli, Hady ElSahar, Hamza Benyamina, Hieu Trung Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jorg Frohberg, Josephine L. Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro von Werra, Leon Weber, Long Phan, Loubna Ben Allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu\u00f1oz, Maraim Masoud, Mar\u2019ia Grandury, Mario vSavsko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad Ali Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla A. Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto L\u2019opez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, S. Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal V. Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault F\u00e9vry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiang Tang, Zheng Xin Yong, Zhiqing Sun, Shaked Brody, Y Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Franccois Lavall\u2019ee, R\u00e9mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St\u00e9phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur\u2019elie N\u2019ev\u2019eol, Charles Lovering, Daniel H Garrette, Deepak R. Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Xiangru Tang, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, S. Osher Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdenvek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ananda Santa Rosa Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Olusola Ajibade, Bharat Kumar Saxena, Carlos Mu\u00f1oz Ferrandis, Danish Contractor, David M. Lansky, Davis David, Douwe Kiela, Duong Anh Nguyen, Edward Tan, Emily Baylor, Ezinwanne Ozoani, Fatim T Mirza, Frankline Ononiwu, Habib Rezanejad, H.A. Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jan Passmore, Joshua Seltzer, Julio Bonis Sanz, Karen Fort, L\u00edvia Macedo Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, M. K. K. Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nourhan Fahmy, Olanrewaju Samuel, Ran An, R. P. Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas L. Wang, Sourav Roy, Sylvain Viguier, Thanh-Cong Le, Tobi Oyebade, Trieu Nguyen Hai Le, Yoyo Yang, Zachary Kyle Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Kumar Singh, Benjamin Beilharz, Bo Wang, Caio Matheus Fonseca de Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl\u00e9mentine Fourrier, Daniel Le\u2019on Perin\u2019an, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully A. Burns, Helena U. Vrabec, Iman I.B. Bello, Isha Dash, Ji Soo Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthi Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P\u00e0mies, Mar\u00eda Andrea\nCastillo, Marianna Nezhurina, Mario Sanger, Matthias Samwald, Michael Cullan, Michael Weinberg, M Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patricia Haller, R. Chandrasekhar, R. Eisenberg, Robert Martin, Rodrigo L. Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Pratap Bharati, T. A. Laud, Th\u2019eo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yashasvi Bajaj, Y. Venkatraman, Yifan Xu, Ying Xu, Yun chao Xu, Zhee Xao Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. ArXiv, abs/2211.05100, 2022. URL https://arxiv.org/abs/2211.05100.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=M3Y74vmsMcY.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. Commun. ACM, 63(12): 54\u201363, nov 2020. ISSN 0001-0782. URL https://doi.org/10.1145/3381831.\nPreethi Seshadri, Sameer Singh, and Yanai Elazar. The bias amplification paradox in text-to-image generation. arXiv preprint arXiv:2308.00755, 2023. URL https://arxiv.org/abs/2308.00755.\nJaime Sevilla, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos. Compute trends across three eras of machine learning. In 2022 International Joint Conference on Neural Networks (IJCNN), pp. 1\u20138, 2022. URL https://ieeexplore.ieee.org/abstract/ document/9891914.\nSeongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sungdong Kim, HyoungSeok Kim, Boseop Kim, Kyunghyun Cho, Gichang Lee, Woomyoung Park, Jung-Woo Ha, and Nako Sung. On the effect of pretraining corpora on in-context learning by a large-scale language model. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5168\u20135186, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.380. URL https: //aclanthology.org/2022.naacl-main.380.\nDaniel Simig, Tianlu Wang, Verna Dankers, Peter Henderson, Khuyagbaatar Batsuren, Dieuwke Hupkes, and Mona Diab. Text characterization toolkit (TCT). In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: System Demonstrations, pp. 72\u201387, Taipei, Taiwan, November 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.aacl-demo.9.\nLuca Soldaini and Kyle Lo. peS2o (Pretraining Efficiently on S2ORC) Dataset. Technical report, Allen Institute for AI, 2023. ODC-By, https://github.com/allenai/pes2o.\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Raghavi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, A. Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hanna Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159, 2024. URL https://arxiv.org/abs/2402.00159.\nNishant Subramani, Sasha Luccioni, Jesse Dodge, and Margaret Mitchell. Detecting personal information in training corpora: an analysis. In Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), pp. 208\u2013220, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.trustnlp-1.18. URL https://aclanthology.org/2023.trustnlp-1.18.\nMosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.\nKushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari S Morcos. D4: Improving llm pretraining via document de-duplication and diversification. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.\nTogether Computer. RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset, April 2023. URL https://github.com/togethercomputer/RedPajama-Data.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971, 2023. URL https://arxiv.org/abs/2302.13971.\nUnicode. Unicode Text Segmentation, Aug 2023. URL https://unicode.org/reports/tr29/.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/ file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf.\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 483\u2013498, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.41. URL https: //aclanthology.org/2021.naacl-main.41.\nRui Zhang and Joel Tetreault. This email could save your life: Introducing the task of email subject line generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 446\u2013456, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1043. URL https://aclanthology.org/P19-1043.\nXuhui Zhou, Maarten Sap, Swabha Swayamdipta, Yejin Choi, and Noah Smith. Challenges in automated debiasing for toxic language detection. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 3143\u20133155, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eaclmain.274. URL https://aclanthology.org/2021.eacl-main.274.\nA CORPORA: ELABORATION\nWe cover ten different corpora, including text-only corpora (e.g., C4), captions from image-captioning (LAION-2B-en), and code (The Stack). A high level description of these corpora using WIMBD is presented in Table 2, and details about the information contained in those corpora are detailed in Table 6.\nWe analyze all corpora fully, including the different subsets (e.g., The Pile is constructed of multiple sources, such as Wikipedia, arXiv, etc.). The only exceptions are mC4, and LAION, which the original released data consist of non-English texts as well, and we focus on the English subset. Note that while we focus on English text corpora, most of our analyses are not language dependent, and can be easily applied to other languages as well. The only exception is the toxic language analysis (\u00a7B.3.3) that relies on an English lexicon and classifier. However, we note that given non-English lexicon and classifier, the analysis can be easily repeated for other languages using our framework.\nOPENWEBTEXT is an open-source reproduction8 (Gokaslan & Cohen, 2019) of the data used to train GPT-2 (Radford et al., 2019). Due to the limited information provided by Radford et al. (2019), and never releasing the data, it is unclear how similar OpenWebText is to the original data (WebText), but similar steps to the paper\u2019s reports were conducted (such as deduplication, non-English filtering, min-length filtering, etc.).\nC4 is the dataset used by Raffel et al. (2020) for training T5. The dataset: The Colossal Clean Crawled Corpus (C4 in short) is based on Common Crawl as a source of text that was scraped from the web. As such, a lot of the data is noisy, and a set of heuristics were employed to clean it up, such as filtering documents by length, obscene/bad words, duplicate texts, non-english, etc. C4 was not released by Raffel et al. (2020), and instead, it was scraped, cleaned, filtered, and released by Dodge et al. (2021).\nMC4-EN is a multilingual version of C4 that was used to train mT5 (Xue et al., 2021), and later umT5 (Chung et al., 2023). We use the latest version (v.3.1.0) which was used to train umT5, containing documents collected from Common Crawl through August 2022, and in practice the portion of the data that is classified as English. The main difference of mC4-en over C4 is a higher confidence by a language classifier (from 0.7 to 0.96), while also allowing a 0.1% random set of documents that contain \u201cbad words\u201d to pass through, and adaptation of the \u201cbad words\u201d list that resulted in filtering more than 10% of the documents in a language.\nOSCAR is a multilingual corpus based on Common Crawl (Abadji et al., 2022). It contains a length filter for improving data quality that filters out documents with short sentences. They also annotate the data with different labels, such as the language of the document, adult content, and language identification, which they use for different analyses. It is an ongoing effort, and the corpus is maintained and updated regularly.\nTHE PILE is a corpus consisting of 22 different domains (Gao et al., 2020). Unlike C4, the data was not scrapped from the web and then filtered, but pre-selected, with the motivation that this way the data will be of higher quality. The included domains in The Pile are diverse: they include data such as Wikipedia, Github, Arxiv, EuroParl, and more. By design, most datasets are upsampled in the hope to increase data quality, from 1.5x with domains such as OpenSubtitles, up to 3x with Wikipedia. Models such as GPT-J (Wang & Komatsuzaki, 2021), GPT-neo (Black et al., 2022) and Pythia (Biderman et al., 2023) were trained on this dataset.\nREDPAJAMA is an open-source version reproduction of the data used to train LLaMA (Touvron et al., 2023), and was used to train RedPajama-INCITE (Together Computer, 2023).\nS2ORC is a large corpus of English academic papers, which consists the abstracts, full text, including figures, tables, and references (Lo et al., 2020). The texts are automatically extracted from pdfs and LATEX sources.\n8skylion007.github.io/OpenWebTextCorpus\nPES2O is a derivative of S2ORC, cleaned and filtered to obtain a more usable version of the data intended to train language models. We use peS2o V2 (Soldaini & Lo, 2023).\nLAION is a large dataset of images and captions scraped from Common Crawl (Schuhmann et al., 2022). The main dataset (LAION-5B) contains 5.8 billion examples, of which 2.32 billion of the captions are in English (LAION-2B-en), which we use in this work. We focus on the text captions but demonstrate qualitative examples using the associated URLs and images when appropriate.\nTHE STACK (Kocetkov et al., 2023) is a source-code dataset that was collected for training language models, and parts of it were used to train SantaCoder (Allal et al., 2023) and MPT (Team, 2023). It was compiled from GHArchive9 with some filters: files that cannot contribute to training code such as binary files, files larger than 1MB, and some extensions. In addition, only repositories with permissive licenses were included (18 license types in the version v1.0, and 193 in version v1.1), and we use the v1.2. While the main purpose of code is to provide machine instructions to perform different functionalities, it also contain natural language in the form of comments: \u201cRoughly 40 natural languages are present in docstrings and comments with English being the most prevalent. In python files, it makes up 96% of the dataset.\u201d\nB ADDITIONAL RESULTS\nWe provide additional details and extended results on all the corpora considered in this work. This appendix is structured in a similar way to the structure in the main paper, categorized by the four different high-level analyses: (1) Data Statistics (Appendix B.1), (2) Data Quality (Appendix B.2), (3) Community- and Society-Relevant Measurements (Appendix B.3), and (4) Cross-Data Analysis (Appendix B.4).\nB.1 DATA STATISTICS\nThe summary statistics are composed of different analyses that mainly involve the additional metadata associated with the textual documents, such as the URL from which the document was extracted, the date it was collected, etc. We also consider some raw statistics about the corpora, described in the main paper (4.2). The analyses we propose for data statistics are the following:\n1. Summary statistics (\u00a74.2)\n2. Internet domain distribution (\u00a74.2.2, \u00a7B.1.1)\n3. Internet domain schemes (\u00a7B.1.2)\n4. Internet domain suffixes (\u00a7B.1.3)\n5. Utterance date statistics (\u00a7B.1.4)\n6. Geolocation (\u00a7B.1.5)\n7. Language distribution (\u00a7B.1.6)\nB.1.1 INTERNET DOMAIN DISTRIBUTION\nHere, we provide complete analyses on the five corpora that contain URL information in the corpus metadata. Using the Exact Counts , we conduct two analyses: (1) each domain is counted per document (yielding documents per domain), and another where each domain is counted per token in the document (yielding tokens per domain). The results are presented in Figure 6, where the (1) document per domain figures are presented on the left, and the (2) document per token figures are presented on the right.\nIn Table 7, we analyze the number of tokens in each domain, and calculate the 1, 25, 50, 75, and 99 quantiles of these distributions. Interestingly, the 1% quantile in LAION-2B-en include domains which have 1-or-less tokens.\nB.1.2 INTERNET DOMAIN SCHEMES\nThis analysis computes the domain schemes of the associated URLs using the Exact Counts . The results are presented in Figure 7. HTTP and HTTPS are two internet protocols, with the latter being an extension of the first that provides more secure communication. While the exact portion of websites across the web that uses each protocol is hard to assess, traffic that goes through Google primarily uses HTTPS - 95%.10.\n10https://transparencyreport.google.com/https/overview, as of September 16th, 2023.\nThe trend of recent years shows an increase in the portion of HTTPS-supported websites, and as such, we can use this portion as a proxy for the internet age of a website: HTTP websites are more likely to be older. In addition, the portion of a corpus is an interesting comparison with the reported portion from Google\u2019s traffic.\nAll corpora containing URL information show significant proportions from Google\u2019s reports of 95% for the HTTPS protocol. OSCAR contains the highest proportion with 87.6% HTTPS URLs, while C4 is the lowest with only 62.5%.\nB.1.3 INTERNET DOMAIN SUFFIXES\nNext, we compute the suffix distribution of the different corpora using the Exact Counts and present the results of the ten most common ones in Figure 8. Compared to the internet domain distribution, the suffixes provide us with a higher-level description of the sources of the documents.\nPerhaps not surprisingly, the most common suffix is com, which is between 60.1% of the documents in OSCAR and 77.5% in LAION-2B-en. The distribution of suffixes for each dataset exhibits a long tail with a total of over 3,000 different suffixes in the different corpora. While the top 10 typically represent suffixes from English-speaking countries (e.g., co.uk, and ca), LAION-2B-en\u2019s top-10 contains a lot of non-English speaking countries as well, such as Germany (de, 0.7%), Russia (ru, 0.5%), France (fr, 0.4%) and Italy (it, 0.4%).\nB.1.4 UTTERANCE DATE STATISTICS\nIn this section, we examine the temporal diversity of documents from corpora with either reliable creation timestamps in their metadata or URL source information from which creation time can be estimated. Language usage drifts, new concepts are introduced over time, and the truth of much commonsense knowledge depends on the date an utterance was made. While some datasets we consider (S2ORC and peS2o) have reliable, API-generated creation timestamps, most have creation dates that reflect the time of a document ingestion into the source dataset and not its origin date (C4, mC4-en, RedPajama, and LAION-2B-en). To characterize their temporal distribution, we directly count and bin documents by year for those with reliable creation time metadata. For datasets without this information, we fall back on using either the earliest date the URL associated with a document was indexed by the Internet Archive or the date of ingestion into the dataset (whichever is earlier).11 Note that such a procedure does not provide us with the timestamp of the document that was scraped, and as such, it serves as a lower bound on the document\u2019s time creation. Given the limitations of the Internet Archive\u2019s API, we do this for a 10,000 document random sample of each dataset, which allows a rough estimate of the collection time for documents in these corpora. Results are shown in Figure 9. We can see that RedPajama and OSCAR are dominated by documents created in the previous five years (as of September 2023), while other datasets have a more substantial proportion of documents from the first half of the 2010s and earlier. Notably, S2ORC and pes2o contain a non-negligible fraction of documents from the pre-internet era.\nB.1.5 GEOLOCATION\nIn this section, we gauge the geographic diversity of corpora with URL source information in their metadata. We use a commercially developed IP database 12 to estimate the country of origin for 100,000 randomly sampled URLs from each of the five corpora with this information included. While there are limitations to using the location of a hosting server as a stand-in for the content creator\u2019s location (i.e., websites are not always hosted locally nor in one unique location), it does provide a rough geographic origin for source material. As seen in Figure 10, most web pages across corpora are hosted in the United States, with the bulk of the remainder distributed amongst the anglosphere. This is unsurprising given the focus on English-language sources in the construction of the corpora under consideration.\nHere, we aim to assess the proportion of languages in all corpora. We use the CLD213 classifier to make a prediction about what language is being used in each document, and use this prediction as a label that we analyze in aggregate. Note that we use the classifier label also in mixed-language documents (if CLD2\u2019s is_reliable flag is False, we apply the label UN). Table 8 reports the percentages of English-language documents across corpora. As expected, the English fraction is quite high, given the targeted construction of most datasets we consider. The remaining percentages of non-English documents are broken down for the ten remaining most common languages in Figure 11. Note that the classifier we use, as with other classifiers, is imperfect, and as such the identified languages may be wrong.\n11The Internet Archive is a massive library that has been preserving the web since 1996. https: //archive.org\n12This work includes IP2Location LITE data available from https://lite.ip2location.com 13https://github.com/CLD2Owners/cld2\nWhile we reported all the different analyses under data quality in the main paper, here we elaborate and provide the full results on all corpora and the different variations (e.g., most common unigrams, bigrams, and length distribution on token level). The analyses we propose for data quality are the following:\n1. Most and least common n-grams (\u00a74.3.1, \u00a7B.2.1) 2. Duplicate (\u00a74.3.2, \u00a7B.2.2) 3. Document length distribution (\u00a74.3.3, \u00a7B.2.3)\nB.2.1 MOST & LEAST COMMON n-GRAMS\nMost common n-grams In addition to the most common 10-grams reported in Section 4.3.1, we report the results for the most common unigrams, bigrams, and trigrams. Stop words and punctuation are the most common unigrams across the different datasets, with some differences in their ranking. Moving to bigrams, we observe more differences between the corpora. For instance, in LAION-2B-en, we observe some marketing mentions, such as \u201cfor sale\u201d and \u201c- Shirt\u201d. \u201cof the\u201d and \u201cin the\u201d are repeating bigrams in all corpora. In the trigram results, we notice a larger diversion between the corpora. C4 contains common English expressions, such as \u201cone of the\u201d, \u201ca lot of\u201d, and \u201cas well as\u201d. However, LAION-2B-en contains much more marketing material, such as \u201cT\n- Shirt\u201d, \u201cfor sale in\u201d. OSCAR and The Pile have many n-grams that look like uncleaned html (\u201c: / /\u201d, \u2018https : /\u201d, \u201ctype = \"\u201d) or markdown (\u201c\u2013-\u201d, \u201c===\u201d, \u201c###\u201d).\nLeast common n-grams Similarly to the most common n-grams, we look at the other side of n-grams distribution on the least common in a corpus. We showcase a random set of 25 unique unigrams from the different corpora in Figures 12 and 13. We observe two noticeable trends from such unigrams: (1) non-standard Unicode fonts like \u201cnegative squared latin\u201d (for instance COTD in mC4-en), and (2) non-English strings. Non-English strings are quite diverse. The sample from OpenWebText contains unigrams from 12 languages other than English: Urdu, Arabic, Korean, Sanskrit, Hebrew, Armenian, Bengali, Persian, Japanese, Latvian, Sindhi, and Russian.\nIn addition to the unique unigrams inspection, we estimate the number of unique unigrams in each corpus and present the results in Table 10. The unique unigrams results reveal that a non-trivial amount of unique unigrams appear in these corpora. Even the smallest corpus, OpenWebText, contains more than 88 million unique unigrams, about 1.1% of the total unigrams in this corpus. The ratio of unique unigrams is about an order of magnitude smaller in the other corpora, except for LAION-2B-en, with over 554 million unique unigrams, which constitute 1.8% of the total unigrams.\nTable 11: Top 5 most occurring text duplicates from datasets with duplicates (OpenWebText and C4 don\u2019t have any duplicate documents). Truncation for visualization is marked by [...].\nCorpus Property #1 Duplicate #2 Duplicate #3 Duplicate #4 Duplicate #5 Duplicate\nmC4-en Text \u2019, \u2019text-align:left; color:white;background-color:#0 564d1;\u2019] //}); // ly.show(); var i_type = $(\"#fa[...] Tada has the world\u2019s lea ding smart parking techn ology and has many of the world\u2019s top experts. A hug [...] 4K Ultra-clear picture with exquisite picture quality, p lug and play, H.265/H.26 5+, Max.512G SD card[...] \u2019, \u2019text-align:left; color:w hite;background-color:#0 564d1;\u2019] //}); // ly.show(); var i_type = $(\"#fa[...] \u2018, marker.on(\u2019click\u2019, ma rkerClick); if(type==0 & & index==0){ marker.emit (\u2019click\u2019, { target: marker } [...]\nCount 154 114 80 76 73\nOSCAR Text In order to login you mustbe registered. Registering takes only a few moments but gives you increas[...] JavaScript is disabled. For a better experience, please enable JavaScript in your browser before pro[...] Privacy & Cookies: This site uses cookies. By co ntinuing to use this website , you agree to their use[...] JavaScript seems to be d isabled in your browser. For the best experience on our site, be sure to tur[...] You may not have to, it is u p to the administrator of th e board as to whether you need to register i[...]\nCount 1,790,064 989,919 854,143 786,678 673,136\nThe Pile Text {\\n \"info\" : {\\n \"version\" :1,\\n \"author\" : \"xcode\"\\n } \\n} \\r\\n\\r\\n\\r\\n \\r\\n\\r\\n\\r\\n\\r\\n \\tC-Track E-Filing\\r\\n\\t\\r\\n \\t\\r\\n\\t\\r\\n\\t\\t\\r\\n\\r\\n\\t\\r\\n\\t \\r\\n\\t\\r\\n\\t\\r\\n\\r\\n\\t\\r\\n\\t\\t\\r \\n\\t\\r\\n\\t\\r\\n\\t\\r\\n \\r\\n\\r\\n\\t\\ r\\n\\t\\r\\n\\t\\r\\n[...] /* Localized versions of Inf o.plist keys */\\n\\n\n<?xml version=\"1.0\" enco ding=\"UTF-8\"?>\\n<!DO CTYPE plist PUBLIC \" -//Apple//DTD PLIST 1.0/ /EN\" \"http://[...]\nCount 3,775 2,941 2,913 2,744 2,714\nRedPajama Text ACCEPTED\\n\\n#### According to\\nInternational Plant Names Index\\n\\n## ## Published in\\nnull\\n\\ n#### Original n[...] SYNONYM\\n\\n#### According to\\nThe Catalo gue of Life, 3rd January 2011\\n\\n#### Published in\\nnull\\n\\n#### Ori[...] ACCEPTED\\n\\n#### Acc ording to\\nThe Catalogue of Life, 3rd January 2011\\n \\n#### Published in\\nnul l\\n\\n#### Or[...] ACCEPTED\\n\\n#### Acc ording to\\nNUB Generator [autonym]\\n\\n#### Publi shed in\\nnull\\n\\n#### Or iginal name\\nnull[...] ACCEPTED\\n\\n#### Acc ording to\\nInterim Register of Marine and Nonmarine Genera\\n\\n#### Published in\\nnull\\n[...]\nCount 213,922 146,434 94,922 15,038 10,089\nS2ORC Text Abstract not submitted for online publication\\n\\n\\n\\ n\\n\\u2022 Research which is freely available for red istrib[...] Abstracts P1 - P16 are e ducational and not inclu ded for publication onli ne\\n\\n\\n\\n\\nO R A L P R E S E N T[...] Abstract withdrawn\\n\\n\\n \\n\\u2022 Convenient onli ne submission \\u2022 Tho rough peer review \\n\\u20 22 No space constraints [... ] Educational abstract\\n\\nO1 Validation of a new autom ated volumetric breast d ensity measurement syste m [...] Modeling and analysis of monkeypox disease using fractional derivatives\\n\\nT he frequency of monkeypo x [...]\nCount 35 30 26 14 14\npeS2o Text Educational abstract\\n\\nO1Validation of a new autom ated volumetric breast d ensity measurement syste m [...] Reply on RC2\\n\\nThis man uscripts investigates the di screpancy of estimated v egetation influence on cat[. ..] COP27 climate change con ference: urgent action n eeded for Africa and the world\\n\\nThe 2022 report of t[...] Reply on RC2\\n\\nFollowin g your suggestion, we have revised the manuscript ve ry carefully. The lists be[. ..] Reply on RC1\\n\\nThis pap er uses a 1D estuary model to explore the variability of overtide under varyin[...]\nCount 14 7 6 4 4\nLAION-2B-en Text Front Cover Wall View 002 Market position of the selected technologies Pointwise: Reliable CFD meshing Go to European Commi ssion website\nCount 1,003,863 681,753 414,986 319,524 314,423\nThe Stack Text #\\n%\\nRailCompiler: Invalid movement.\\n //\\n// WechatAuthSDK.h\\ n// WechatAuthSDK\\n//\\n // Created by \\u674e\\u51ef on 13-11-29.\\n// Copyright (c) 2013\\u5e74 T[...] OUTPUT_FORMAT (\" elf32-littlearm\", \"elf32-big arm\", \"elf32-littlearm\") \\nENTRY(reset_handle r)\\nSEARCH_DIR[...] //\\n// WBHttpRequest+We iboToken.h\\n// WeiboSDK \\n//\\n// Created by Dannion Qiu on 14/11/6.\\n// Cop yrigh[...] //\\n// WXApi.h\\n// \\u6240\\ u6709Api\\u63a5\\u53e3 \\n//\\n// Created by Wechat on 12-2-28.\\n// Copyright (c) 2012\\u5e74 Tencent. A ll[...]\nCount 45 43 29 24 20\nURL Duplicates We also examine duplication between document URLs for the datasets that have that metadata, which we show the top-5 URL duplicates from datasets with URL duplicates in Table 12. LAION\u2019s most frequent URL (with 33,142 occurrences) is an invalid URL \u2013 \u201cUNLIKELY\u201d, likely resulting from a parsing error. The second most frequent URL (with 27,162 occurrences) from LAION-2B-en leads to an all-white image from a computer driver website, and in Figure 15, we see that among the top 25 duplicated URLs in LAION-2B-en, there are instances of image duplicates hosted at different URLs. Meanwhile, OSCAR has a notable artifact wherein, after the top two duplicate URLs, the next 234 URLs are duplicated exactly 100 times. Table 14 in the Appendix shows counts and ratios for these URL duplicates as previously specified for text hashes. These find that URL duplicate ratios are roughly an order of magnitude smaller than their text hash counterparts, and that the count of documents duplicated by URL is not dominated by only a few clusters.\nWe elaborate on the results from the main paper and report the length distribution for all corpora, both for the character and token distribution. Figure 16 showcases these distributions, and Table 15 depicts the median token and character length distributions.\nLAION-2B-en, containing image alt text, has the smallest average document lengths. Beyond the exact duplicates described above, which commonly describe products (especially home appliances), LAION-2B-en also contains a significant number of template-generated alt texts paired with maps describing the location of rental boats. The only outlier in OpenWebText in terms of document length\nis at exactly 100,000 characters; all documents over this length were chunked into multiple documents of length 100,000 by the dataset builders.\nRedPajama also contains template-generated user-facing copy, including, e.g., placeholder pages for alumni of various secondary schools (each associated with a unique individual\u2019s name). This analysis also reveals a collection of documents comprising nearly 0.01% of the dataset, containing what appear to be usernames or titles associated with pornographic content.\nFinally, The Stack contains many template-generated new-duplicate documents; for example, a large number of auto-generated metadata files for Unity assets, each of length 20 tokens. It also contains a significant number of documents of length 20,000 characters that contain float and bit matrices.\nThe Pile also includes a significant number of auto-generated metadata files corresponding to Unity assets, e.g.:\nfileFormatVersion: 2 guid: e32f0a7fe2a7abc4289bc3c0e8a2b558 timeCreated: 1435687483 licenseType: Pro NativeFormatImporter: userData: assetBundleName: assetBundleVariant:\nas well as auto-generated files corresponding to publications in medical journals, e.g.:\n![](edinbmedj74198-0096){#sp1 .384}\nB.3 COMMUNITY- AND SOCIETY-RELEVANT MEASUREMENTS\nIn this section, we provide additional results on the contamination and PII analyses from the main paper, as well as conduct two more analyses: toxic language and demographic sentiment co-occurrences. Overall the community- and society-relevant measurements contain the following analyses:\n1. Benchmark contamination (\u00a7B.3.1) 2. Personally identifiable information (\u00a7B.3.2) 3. Toxic language (\u00a7B.3.3) 4. Demographic sentiment co-occurrences (\u00a7B.3.4)\nB.3.1 BENCHMARK CONTAMINATION\nWe measure contamination by testing whether all of the input fields are present in a single document, and report the percentage of examples from the test set that are contaminated and present the results in Table 16. We do not test for the presence of the labels as those are not always available, and they can come in different forms (e.g., in RTE they may appear either as \u2018entailment\u2019, \u2018not-entailment\u2019, or as \u20180\u2019, \u20181\u2019). Moreover, we do not test for consecutive appearance of these inputs, as they might appear in different orders and with different separators. As such, our contamination evaluation serves as an upper bound of exact-match dataset contamination. By employing exact match comparison with the pretraining data, we ignore minor changes in words or phrases that models trained on such similar texts may exploit. An example of such influence is introduced by Emami et al. (2020), who showed how high overlap between sentences in the Winograd Schema Challenge (Levesque et al., 2012) and pretraining corpora inflates the results on the test set, while Elazar et al. (2021b) argue that knowledge and reasoning capabilities from large pretraining corpora leak and inflate evaluation benchmarks.\nRationales of the Design Choices Here, we provide the rationals behind our design choices for the contamination experiment. Overall, our desiderata required a large benchmark that can be processed automatically, and that matched in an inspected corpora would be of high precision. We details these rationals in the following points:\n\u2022 Choice of task type. We chose to use tasks that include two or more inputs (e.g., natural language inference) as the co-occurrence of both inputs in the same document increase the likelihood of these inputs to originate from an existing evaluation dataset. In contrary, texts from tasks containing a single input (e.g., sentiment analysis) may naturally occur in some text corpus, which decreases the likelihood of contamination.\n\u2022 Ignoring the output. We decided to ignore the output of the inspected datasets since these can appear in different formats (e.g., numeric values, text labels, etc.).\n\u2022 Choice of PromptSource. Finally, we use PromptSource (Bach et al., 2022) as it is the only large scale benchmark which we could automatically process and discern the different input parts (e.g., this is important since many datasets contain additional fields like metadata which are not directly part of the task).\nNote that different design choices can be made for inspecting additional contamination of benchmarks.\nB.3.2 PII\nWe use three regular expressions inspired by Subramani et al. (2023) to identify email addresses, phone numbers, and IP addresses across pretraining corpora. In addition, we improved the phone numbers regex for better precision. These regexes provide us with a high precision performance (which we manually evaluate) and allows a fast PII identification. We apply postprocessing rules to the resulting matches, to improve the precision of detecting personal information by seeking to eliminate common classes of false positives (such as ISBN numbers that may be flagged as phone numbers). These rules are enumerated in Table 17.\nApplying these regular expressions to the ten corpora we study in the paper, Table 20 contains the number of matches of each PII type in each corpus. For faster processing, we filter documents containing a large amount of special characters (such as documents with >50 consecutive \u201c:)\u201d emoticons). We further normalize this statistic, by the number of tokens in each pretraining dataset, in order to estimate the relative proportion of PII in each corpus. These results are in Table 19. We observe that even when controlling for the number of tokens in the different corpora, mC4-en has a large amount of personal information compared to the other pretraining corpora.\nWe manually evaluate the precision of the heuristics. In order to compute this statistic, we sample 100 examples of strings detected as PII (when available), for the three PII types, over the ten pretraining corpora in this study.These results are in Table 18. The nature of this retrieval task makes it challenging to estimate the recall of our method, and more work is needed on the topic. We show the types of examples that may be incorrectly identified as PII by our method in each corpus in Table 21.\nPhone Numbers \\s+(?(\\d{3}))?[-\\. ]*(\\d{3})[-. ]?(\\d{4}) (1) \u2018ISBN\u2019, \u2018DOI\u2019, or \"#\" cannot appear in a context window of 50 characters from the match (2) Cannot contain URL\nIP Addresses (?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?) (1) \u2018ISBN\u2019, \u2018DOI\u2019, or \"#\" cannot appear in a context window of 50 characters from the match\nAssumptions and Limitations: We make a number of assumptions in doing this analysis, and we describe them below:\n\u2022 We choose three types of PII: phone numbers, email addresses and IP addresses. These three types of PII have relatively standardized formats (for example, IP addresses are always 32-bit numbers expressed in dotted decimal format), which allows us to construct regular expressions to search for these information types in text. However, the retrieved information types may not correspond to any one individual\u2014 for example, government organizations have email addresses and phone numbers.\n\u2022 Conversely, many types of personally identifiable information are not easily specifiable in the structured format we use for the information types in this study, and as a result we do not identify them in pretraining corpora.\n\u2022 While many types of information individually may not appear to identify a specific individual, they can be combined with information elsewhere on the internet to form PII. In this work, we only identify a small proportion of potential personal information that is present in pretraining datasets, but further work is needed to analyze the extent to which pretraining corpora include personal information as well as how this information can be sanitized.\n\u2022 Finally, we do not claim to estimate the risk level or sensitivity of the information types we extract from the pretraining corpus, acknowledging that this is highly context-dependent and personalized.\nTable 21: Abbreviated examples of incorrect detections by our method, for each PII type, in each pretraining dataset. The exact span that was matched is in red. Offensive content and personal information have been redacted from the presented examples.\nCorpus Email Addresses Phone Numbers IP Addresses\nOpenWebText skremoved) has joined * trayvonmartin sets ban on *!*@n***.*** * trayvonmartin has kicked whitepower from #n**** ...2017 limitation 99 pcs. article id 472172730 ean 4012138149625 the model was produced in the usual minichamps... ... [stdout] awy was overriden from notenoughitems 1.6.1.9.jar 2014-03-24 20:25:06 [info] [minecraft-client]...\nC4 \u201cyou ever googled our email address? try googling \u201c@fmr.com\u201d and \u201ccharity\u201d together, and you will get an idea\u201d on your mortgage. disclaimer - property reference 100103003249. the information displayed about this property not load file or assembly \u00b4smswrappers, version = 3.0.0.0\nmC4-en smswrappe wrote in messagenews:a30c91p63 cj6vgr...4lfg7ve8@4ax.com... i bought gta iii at a garage sale and it did not \"stat-major-faults\": 1213, \"stattotal-memory\": 3975217152, \"stat-swap-in\": 0 s not constitute the consent required by n.j.a.c. 11.5.6.1 (n) for the advertisement of listings exclusively\nOSCAR - ...a getty images) michael jones9 october 2021 21:53 1633812509 andorra vs england player ratings: phil foden shi... ...latest update software comes with version number 10.0.0.163. currently the update available in the...\nThe Pile [@eiguren3].[]datalabel=\"table4\"\nt undefined behavior. for example, i get that b = 2083899728 and d = -552766888. the persistent thing you are such damage. // according to ecma-262, sections 8.6.2.2 and 8.6.2.3 you\u2019re not // allowed to override rea\nRedPajama - watercolor baby bring a book card printable png v 1525458984 - watercolor baby bring a book card printable png sh wikipedia) 18:54, 15 july 2013 (utc) if i can. 86.146.46.88 john of reading (talk) 06:38, 25 july 2013 (utc)\nS2Orc - - -\nPeS2o 65%@0.00262 izona institutional review board (approval number 2003521636a002). at baseline, the participants reported thei -\nLAION-2B-en NWA DemocratGazette/Michael Woods \u201303/15/2015\u2013 w@NWAMICHAELW... queen creek 85142 e cherrywood dr - property id: 1311037210 gods and glory: war for the throne apk 3.8.10.1\nThe Stack remirror/ui@0.7.3 ermine the vision-agent service is running - hsd 15010872669 - add missing heartbeatresponsetimersecs to the atoaune \u2014 have you upgraded to oracle soa suite 12.2.1.1 and can\u2019t find the partitions configuration any l\nHow common is toxic language used in corpora? We employ two complementary methods for computing toxicity. The first is based on the work of (Zhou et al., 2021), who compiled a lexicon of terms (TOXTRIG) into three categories: possibly offensive minority identity mentions, possibly offensive non-identity mentions, and non-offensive minority identity mentions. It is then used by matching these \u201ctoxic triggers\u201d over texts. The model-based method uses an SVM classifier trained on a dataset consisting of 200K examples based on Wikipedia and Twitter to identify toxic language.14 We apply such a classifier on each sentence separately and consider the document toxic in case any sentence is found to be toxic. We present the results in Table 22. C4 is the least toxic based on the taxonomy: only 0.01% were found to be toxic, which is expected due to the filters used in the curation process of the dataset. On the other hand, the classifier finds more documents to be toxic: 5.75%, which may indicate subtleties that the lexicon used for filtering documents from C4 did not catch. OpenWebText is the most toxic corpus based on the classifier, while PeS2o is the most toxic one based on the taxonomy, perhaps surprisingly, as it is not a web-based corpus.\nExplicit Content Filtering The only dataset we analyze that explicitly filtered for toxic content (in the form of keyword matching) is C4. Indeed, the matching category from our analysis are the \u201cOffensive-*\u201d categories. Our analysis, that uses a fine-grained lexicon (Zhou et al., 2021), splits this category into \u201coffensive-minority\u201d and \u201coffensive-not-minority\u201d. In C4 we only found 47 mentions of the \u201coffensive-not-minority\u201d category, likely due to a difference in filter used to create C4 and our lexicon. In comparison, other datasets that did not employ such filters contain several million references of such phrases. Interestingly, C4 also contains 158K occurrences of the \u201coffensive-minority\u201d category, which were not filtered from the dataset.\nB.3.4 DEMOGRAPHIC SENTIMENT CO-OCCURRENCES\nIn this section, we turn to detecting biases in the corpora based on demographic factors. We constructed a set of unigrams and bigrams associated with gender (male and female pronouns), religion (the proper names of several major religions), and race (combinations of racial identifiers and words like man, woman, people, etc.). The sentiment of sentences containing these terms was computed using SpacyTextBlob and averaged over a given corpus. The results for all corpora are shown in Figure 17. The Stack is excluded from this analysis since the contexts in which these terms appeared were not typically natural language. Overall, we observe a neutral or weakly positive sentiment for sentences in which most of our demographic terms appear, with the exception of those including \u2018black\u2019 being uniformly more negative across all corpora. With minor exceptions we don\u2019t observe substantial variation in the sentiment for individual terms among datasets. The weak positivity seen for all sources is in opposition to a related analysis performed in Gao et al. (2020), which measured weak negativity for most terms. It\u2019s likely this is due to differences in the way\n14https://github.com/dimitrismistriotis/alt-profanity-check\naverage sentiment is computed (we compute sentiment at the sentence level while Gao et al. (2020) computes sentiment only for the most frequent co-occurring terms).\nB.4 CROSS-DATA ANALYSIS\nMain Findings\n\u2022 Comparing unigrams of different corpora reveals distributional and topical differences. \u2022 OSCAR unigram distribution is the most similar to all other corpora on average. \u2022 50% of RedPajama unique documents originate from C4 and 50% of OpenWebText unique\ndocuments originate from The Pile. \u2022 While mC4-en was supposedly a superset of C4, documents from C4 constitute only 0.04% of mC4-en, while the later being only 10x larger in size.\nUsing the analyses from the previous sections we can now perform targeted comparisons between different corpora. Such analysis is the first step of better understand the similarities and differences between corpora. We perform the following analyses:\n1. Distributional similarities (\u00a7B.4.1) 2. Corpus overlap (\u00a7B.4.2)\nB.4.1 DISTRIBUTIONAL SIMILARITY\nUnigram Ranking Using the most common n-gram statistics (4.3.1), we can compare the ranking of these n-grams, to gain insights into their different usage between corpora. For the following analysis we consider the top 10,000 most common unigrams of two corpora, and display the 1,000 most common unigrams in one corpus as a function of the same unigram rank in the other corpus. In Figure 18 we display the rank of unigrams in C4 as a function of their ranks in LAION-2B-en. Some very common unigrams in LAION-2B-en describing objects such as \u201cTwo\u201d, \u201cBlack\u201d, \u201cblue\u201d, and \u201cLight\u201d are very common in LAION-2B-en - top 500 unigrams, but much more rare in C4\u2019s top 1,000. Another category is car models such as BNW and Toyota whose ranking is about 900 in LAION-2B-en, but above 6,000 in C4. Figures 19-28 show the paired ranks for all corpora pairs.\nUnigram Overlap Next, by comparing the 10,000 most common unigrams, we compare the similarity between each corpora pair using the Jensen Shannon distance using (1) the intersection and (2) the union of the two vocabularies. We present the results in Figure 29. On average, we find that OSCAR\u2019s unigram distribution is the most similar to all other corpora (0.19 on average). The Stack, as expected, is the most distance corpus from all other corpora.\nB.4.2 CORPUS OVERLAP\nIn this analysis, we compute the overlap between the different corpora, by comparing (1) the texts, and (2) the URLs, when available. The pairwise results are presented in Figure 30 for the texts overlap, and Figure 31 for the URL overlap. We see that text overlap diminishes quickly to zero as more datasets are considered. Table 23 shows the largest text overlaps between more than two datasets. While the largest two are over 1 million document clusters, this is less than 1% of clusters in any of the involved datasets, and overlap size drops rapidly from there. This trend is similar for URL overlaps. The largest 3-corpora overlap is between C4, mC4-en, and OSCAR, with 6,767,877 shared URLS, while the rest of the overlaps share at most a single URL.\nWe find that documents from S2ORC and peS2o do not appear in other corpora. While it is likely that some of the academic papers are shared with other corpora, e.g., The Pile and RedPajama\nthat included arXiv as a data source, there are likely formatting differences that cause the exact string matching to be different. Interestingly, even S2ORC and peS2o do not contain any exact-text overlapping documents, despite peS2o being a cleaned version of S2ORC, due to a difference in formatting for parsed paper sections.\nWhile RedPajama is 2.5 times larger than C4 in number of documents and 6.6 larger in number of tokens, we find that 50% of RedPajama unique documents originate from C4. This can be explained by larger documents (as evident from the largest average document length in The Stack of 2,800 tokens per document on average, compared to 420 tokens per document in C4, or by duplicate contents of C4 documents in RedPajama. Similarly, 50% of OpenWebText unique documents overlap with The Pile, which includes OpenWebText as a source. Another expected overlap is between datasets with Github as a source (RedPajama and The Pile), and The Stack (which purely consist of Github code).\nFinally, we also notice that while mC4-en was created from a superset the Common Crawl data used to make C4, documents from C4 only constitute 0.04% of mC4-en, while the later is only 10 times larger in size. We speculate that this is due to formatting differences, between the C4 and mC4-en collection.\nC LIMITATIONS\nWIMBD has a few limitations, described below:\n\u2022 The search tool we use is Elasticsearch. While it is scalable, it was not designed for scaling with large text corpora. In addition, indexing these massive text corpora can take a few days,\nand keeping it running is costly. In the future, we hope to explore more cost effective and faster indexing tools.\n\u2022 Search is currently enabled using Elasticsearch, which only enables exact-match search. Fuzzy, and semantic search are important abilities that we currently do not support.\nThis section describes the benchmark times each analysis took to run on the C4 corpus. While C4 is not the largest corpora we analyze, it is a popular one, and representative in size. All out analyses were run on a Google cloud compute node with 882GB RAM and 224 CPUs. While the machine is rich in RAM, our analyses typically did not use more than 250GB, and the reason for choosing such machine was the availability of a machine with enough CPU cores, that came along with this amount of memory.\nWe report the benchmark runs in Table 24. All of the analyses we conducted took less than 12 hours to run, with 13 (out of 22) that took only several minutes, and all of the analyses on C4 took an estimated of 46 hours and 51 seconds (excluding repeated runs, and the contamination analyses on other evaluation datasets). Note that while the measured time for each run were calculated using the TIME command in linux, there is some variance, and those should be taken as a rough estimate.\nWe also calculate the estimated costs for each analysis and report it in the same table (Table 24). We use the estimated $9.46 per hour based on https://cloud.google.com/compute/all-pricing for our calculations, making the total cost on C4 $443.1.15\n15This estimation does not include the Elasticsearch hosting costs.\nE TECHNICAL DETAILS\nThis section describes the algorithms for computing the most common, least common, and total number of unique n-grams in a large corpus. Each of these algorithms uses the same trick that was inspired by Bloom filters (Bloom, 1970) as described in section 3.1. As a result these algorithms do not provide exact results, and the accuracy is determined by the amount of memory available for the hash table.\nE.1 MOST COMMON n-GRAMS\nTo collect the (approximate) top-k n-grams we start by initializing a hash table of zeros (either u32 or u64) which represent occurrence counts for each n-gram, and an empty collection of the top-k n-grams. Then we iterate over the n-grams in the corpus and for each n-gram encountered we take its hash, increment the corresponding count in the hash table, and if that count is at least as large as the current minimum count in the top-k we add that n-gram to the top-k, potentially evicting another n-gram from the top-k.\nAfter completing the iteration over the corpus the top-k will be complete and, in the absence of hash collisions, correct. However, the larger the corpus is relative to the hash table, the higher the probability of hash collisions. A large enough corpus will have more unique n-grams than there are entries in the hash table, which guarantees hash collisions in the table, leading to inflated counts for some n-grams and the potential for false positives in the top-k. That\u2019s where the accuracy-memory tradeoff comes in. The final counts reported for the top-k n-grams will always be an upper bound of the true counts.\nE.2 LEAST COMMON n-GRAMS\nTo collect the (approximate) bottom-k n-grams we also start by initializing a hash table of u3216 zeros to represent occurrence counts for each n-gram, and an empty collection of the bottom-k n-grams. But this time we have to iterate over the corpus\u2019 n-grams twice.\nDuring the first iteration we tally up the counts just like we do in the top-k algorithm, except that we don\u2019t add any n-grams to the bottom-k collection. During the second iteration we now already have the final counts of all n-grams, so we simply look up the count of each n-gram encountered and then add it to the bottom-k collection if its count is low enough, potentially evicting another n-gram.\nHash collisions might cause false negatives with the bottom-k, i.e. some rare n-grams may be missing from bottom-k if they had hash collisions with more frequent n-grams. The final counts reported will for the bottom-k n-grams always be a lower bound of the true counts.\nE.3 UNIQUE n-GRAMS\nTo estimate the number of unique n-grams we initialize a hash table of booleans set to \u2018false\u2019. Then we iterate over all n-grams in the corpus and for each n-gram encountered we take its hash and update the corresponding boolean in the table to \u2018true\u2019. After iterating over the whole corpus we simply have to tally up the number of \u2018true\u2019 entries. This number is the estimate for the number of unique n-grams, which will always be a lower bound of the actual number of unique n-grams.\n16It\u2019s not necessary to use u64 integers when collecting the bottom-k even if there\u2019s a possibility of overflow counts, provided overflows are caught and kept at 232, since we only care about the exact count of rare n-grams which are unlikely to ever reach an overflow."
        }
    ],
    "title": "WHAT\u2019S IN MY BIG DATA?",
    "year": 2024
}