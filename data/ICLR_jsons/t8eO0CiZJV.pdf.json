{
    "abstractText": "Large language models (LMs) are capable of generating free-text rationales to aid question answering. However, prior work 1) suggests that useful selfrationalization is emergent only at significant scales (e.g., 175B parameter GPT3); and 2) focuses largely on downstream performance, ignoring the semantics of the rationales themselves, e.g., are they faithful, true, and helpful for humans? In this work, we enable small-scale LMs (\u223c200x smaller than GPT-3) to generate rationales that not only improve downstream task performance, but are also more plausible, consistent, and diverse, assessed both by automatic and human evaluation. Our method, MARIO (Multi-rewArd RatIOnalization), is a multi-reward conditioned self-rationalization algorithm that optimizes multiple distinct properties like plausibility, diversity and consistency. Results on three difficult questionanswering datasets StrategyQA, QuaRel and OpenBookQA show that not only does MARIO improve task accuracy, but it also improves the self-rationalization quality of small LMs across the aforementioned axes better than a supervised finetuning (SFT) baseline. Extensive human evaluations confirm that MARIO rationales are preferred vs. SFT rationales, as well as qualitative improvements in plausibility and consistency.",
    "authors": [],
    "id": "SP:4a46d6e00cc7e489ce15756b30ee706af5c71084",
    "references": [
        {
            "authors": [
                "Pepa Atanasova",
                "Oana-Maria Camburu",
                "Christina Lioma",
                "Thomas Lukasiewicz",
                "Jakob Grue Simonsen",
                "Isabelle Augenstein"
            ],
            "title": "Faithfulness tests for natural language explanations",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
            "year": 2023
        },
        {
            "authors": [
                "Jared Kaplan"
            ],
            "title": "Training a helpful and harmless assistant with reinforcement learning from human",
            "venue": "feedback. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Aaron Chan",
                "Maziar Sanjabi",
                "Lambert Mathias",
                "Liang Tan",
                "Shaoliang Nie",
                "Xiaochang Peng",
                "Xiang Ren",
                "Hamed Firooz"
            ],
            "title": "Unirex: A unified learning framework for language model rationale extraction",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Aaron Chan",
                "Zhiyuan Zeng",
                "Wyatt Lake",
                "Brihi Joshi",
                "Hanjie Chen",
                "Xiang Ren"
            ],
            "title": "Knife: Distilling reasoning knowledge from free-text rationales",
            "year": 2023
        },
        {
            "authors": [
                "Hanjie Chen",
                "Faeze Brahman",
                "Xiang Ren",
                "Yangfeng Ji",
                "Yejin Choi",
                "Swabha Swayamdipta"
            ],
            "title": "REV: Information-theoretic evaluation of free-text rationales",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Jiangjie Chen",
                "Qiaoben Bao",
                "Changzhi Sun",
                "Xinbo Zhang",
                "Jiaze Chen",
                "Hao Zhou",
                "Yanghua Xiao",
                "Lei Li"
            ],
            "title": "Loren: Logic-regularized reasoning for interpretable fact verification",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Wei-Lin Chen",
                "An-Zi Yen",
                "Hen-Hsen Huang",
                "Cheng-Kuang Wu",
                "Hsin-Hsi Chen"
            ],
            "title": "Zara: Improving few-shot self-rationalization for small language models, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models",
            "venue": "arXiv preprint arXiv:2210.11416,",
            "year": 2022
        },
        {
            "authors": [
                "Misha Denil",
                "Alban Demiraj",
                "Nando De Freitas"
            ],
            "title": "Extraction of salient sentences from labelled documents",
            "venue": "arXiv preprint arXiv:1412.6815,",
            "year": 2014
        },
        {
            "authors": [
                "Hanze Dong",
                "Wei Xiong",
                "Deepanshu Goyal",
                "Rui Pan",
                "Shizhe Diao",
                "Jipeng Zhang",
                "Kashun Shum",
                "T. Zhang"
            ],
            "title": "Raft: Reward ranked finetuning for generative foundation model alignment",
            "venue": "ArXiv, abs/2304.06767,",
            "year": 2023
        },
        {
            "authors": [
                "Yann Dubois",
                "Xuechen Li",
                "Rohan Taori",
                "Tianyi Zhang",
                "Ishaan Gulrajani",
                "Jimmy Ba",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori Hashimoto"
            ],
            "title": "Alpacafarm: A simulation framework for methods that learn from human",
            "venue": "feedback. ArXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Mor Geva",
                "Daniel Khashabi",
                "Elad Segal",
                "Tushar Khot",
                "Dan Roth",
                "Jonathan Berant"
            ],
            "title": "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Olga Golovneva",
                "Moya Chen",
                "Spencer Poff",
                "Martin Corredor",
                "Luke Zettlemoyer",
                "Maryam"
            ],
            "title": "FazelZarandi, and Asli Celikyilmaz. Roscoe: A suite of metrics for scoring step-by-step reasoning",
            "venue": "arXiv preprint arXiv:2212.07919,",
            "year": 2022
        },
        {
            "authors": [
                "Caglar Gulcehre",
                "Tom Le Paine",
                "Srivatsan Srinivasan",
                "Ksenia Konyushkova",
                "Lotte Weerts",
                "Abhishek Sharma",
                "Aditya Siddhant",
                "Alexa Ahern",
                "Miaosen Wang",
                "Chenjie Gu",
                "Wolfgang Macherey",
                "A. Doucet",
                "Orhan Firat",
                "Nando de Freitas"
            ],
            "title": "Reinforced self-training (rest) for language modeling",
            "venue": "ArXiv, abs/2308.08998,",
            "year": 2023
        },
        {
            "authors": [
                "Peter Hase",
                "Shiyue Zhang",
                "Harry Xie",
                "Mohit Bansal"
            ],
            "title": "Leakage-adjusted simulatability: Can models generate non-trivial explanations of their behavior in natural language",
            "venue": "arXiv preprint arXiv:2010.04119,",
            "year": 2020
        },
        {
            "authors": [
                "Dirk Hovy",
                "Shrimai Prabhumoye"
            ],
            "title": "Five sources of bias in natural language processing",
            "venue": "Language and Linguistics Compass,",
            "year": 2021
        },
        {
            "authors": [
                "Cheng-Yu Hsieh",
                "Chun-Liang Li",
                "Chih-kuan Yeh",
                "Hootan Nakhost",
                "Yasuhisa Fujii",
                "Alex Ratner",
                "Ranjay Krishna",
                "Chen-Yu Lee",
                "Tomas Pfister"
            ],
            "title": "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. In Findings of the Association for Computational Linguistics: ACL 2023",
            "year": 2023
        },
        {
            "authors": [
                "Sarthak Jain",
                "Sarah Wiegreffe",
                "Yuval Pinter",
                "Byron C Wallace"
            ],
            "title": "Learning to faithfully rationalize by construction",
            "venue": "arXiv preprint arXiv:2005.00115,",
            "year": 2020
        },
        {
            "authors": [
                "Xisen Jin",
                "Zhongyu Wei",
                "Junyi Du",
                "Xiangyang Xue",
                "Xiang Ren"
            ],
            "title": "Towards hierarchical importance attribution: Explaining compositional semantics for neural sequence models",
            "year": 1911
        },
        {
            "authors": [
                "Brihi Joshi",
                "Ziyi Liu",
                "Sahana Ramnath",
                "Aaron Chan",
                "Zhewei Tong",
                "Shaoliang Nie",
                "Qifan Wang",
                "Yejin Choi",
                "Xiang Ren"
            ],
            "title": "Are machine rationales (not) useful to humans? measuring and improving human utility of free-text rationales",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Jaehun Jung",
                "Lianhui Qin",
                "Sean Welleck",
                "Faeze Brahman",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Yejin Choi"
            ],
            "title": "Maieutic prompting: Logically consistent reasoning with recursive explanations",
            "venue": "arXiv preprint arXiv:2205.11822,",
            "year": 2022
        },
        {
            "authors": [
                "Akos K\u00e1d\u00e1r",
                "Grzegorz Chrupa\u0142a",
                "Afra Alishahi"
            ],
            "title": "Representation of linguistic form and function in recurrent neural networks",
            "venue": "Computational Linguistics,",
            "year": 2017
        },
        {
            "authors": [
                "Nitish Shirish Keskar",
                "Bryan McCann",
                "Lav R. Varshney",
                "Caiming Xiong",
                "Richard Socher"
            ],
            "title": "Ctrl: A conditional transformer language model for controllable generation, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Tushar Khot",
                "Peter Clark",
                "Michal Guerquin",
                "Peter Jansen",
                "Ashish Sabharwal"
            ],
            "title": "Qasc: A dataset for question answering via sentence composition",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "Advances in neural information processing systems,",
            "year": 2022
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Tao Lei",
                "Regina Barzilay",
                "Tommi Jaakkola"
            ],
            "title": "Rationalizing neural predictions",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2016
        },
        {
            "authors": [
                "Piyawat Lertvittayakumjorn",
                "Francesca Toni"
            ],
            "title": "Explanation-based human debugging of NLP models: A survey",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Jiwei Li",
                "Xinlei Chen",
                "Eduard Hovy",
                "Dan Jurafsky"
            ],
            "title": "Visualizing and understanding neural models in nlp",
            "venue": "arXiv preprint arXiv:1506.01066,",
            "year": 2015
        },
        {
            "authors": [
                "Jiwei Li",
                "Will Monroe",
                "Dan Jurafsky"
            ],
            "title": "Understanding neural networks through representation erasure",
            "venue": "arXiv preprint arXiv:1612.08220,",
            "year": 2016
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Jack Hessel",
                "Youngjae Yu",
                "Xiang Ren",
                "Kai-Wei Chang",
                "Yejin Choi"
            ],
            "title": "Symbolic chain-of-thought distillation: Small models can also \u201cthink",
            "venue": "step-by-step. ArXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Jack Hessel",
                "Youngjae Yu",
                "Xiang Ren",
                "Kai-Wei Chang",
                "Yejin Choi"
            ],
            "title": "Symbolic chain-of-thought distillation: Small models can also \u201cthink\u201d step-by-step",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Ari Holtzman",
                "Daniel Fried",
                "Percy Liang",
                "Jason Eisner",
                "Tatsunori Hashimoto",
                "Luke Zettlemoyer",
                "Mike Lewis"
            ],
            "title": "Contrastive decoding: Open-ended text generation as optimization",
            "venue": "arXiv preprint arXiv:2210.15097,",
            "year": 2022
        },
        {
            "authors": [
                "Yifei Li",
                "Zeqi Lin",
                "Shizhuo Zhang",
                "Qiang Fu",
                "Bei Chen",
                "Jian-Guang Lou",
                "Weizhu Chen"
            ],
            "title": "Making language models better reasoners with step-aware verifier",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Bill Yuchen Lin",
                "Seyeon Lee",
                "Rahul Khanna",
                "Xiang Ren"
            ],
            "title": "Birds have four legs?! numersense: Probing numerical commonsense knowledge of pre-trained language models",
            "venue": "arXiv preprint arXiv:2005.00683,",
            "year": 2020
        },
        {
            "authors": [
                "Jiacheng Liu",
                "Skyler Hallinan",
                "Ximing Lu",
                "Pengfei He",
                "Sean Welleck",
                "Hannaneh Hajishirzi",
                "Yejin Choi"
            ],
            "title": "Rainier: Reinforced knowledge introspector for commonsense question answering",
            "venue": "arXiv preprint arXiv:2210.03078,",
            "year": 2022
        },
        {
            "authors": [
                "Jiacheng Liu",
                "Wenya Wang",
                "Dianzhuo Wang",
                "Noah A Smith",
                "Yejin Choi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Vera: A general-purpose plausibility estimation model for commonsense statements",
            "venue": "arXiv preprint arXiv:2305.03695,",
            "year": 2023
        },
        {
            "authors": [
                "Wei Liu",
                "Jun Wang",
                "Haozhao Wang",
                "Ruixuan Li",
                "Yang Qiu",
                "YuanKai Zhang",
                "Jie Han",
                "Yixiong Zou"
            ],
            "title": "Decoupled rationalization with asymmetric learning rates: A flexible lipschitz restraint",
            "venue": "In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD",
            "year": 2023
        },
        {
            "authors": [
                "Ximing Lu",
                "Sean Welleck",
                "Jack Hessel",
                "Liwei Jiang",
                "Lianhui Qin",
                "Peter West",
                "Prithviraj Ammanabrolu",
                "Yejin Choi"
            ],
            "title": "Quark: Controllable text generation with reinforced unlearning",
            "venue": "Advances in neural information processing systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ximing Lu",
                "Faeze Brahman",
                "Peter West",
                "Jaehun Jang",
                "Khyathi Chandu",
                "Abhilasha Ravichander",
                "Lianhui Qin",
                "Prithviraj Ammanabrolu",
                "Liwei Jiang",
                "Sahana Ramnath"
            ],
            "title": "Inference-time policy adapters (ipa): Tailoring extreme-scale lms without fine-tuning",
            "venue": "arXiv preprint arXiv:2305.15065,",
            "year": 2023
        },
        {
            "authors": [
                "Scott M Lundberg",
                "Su-In Lee"
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "In Proceedings of the 31st international conference on neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Aman Madaan",
                "Niket Tandon",
                "Prakhar Gupta",
                "Skyler Hallinan",
                "Luyu Gao",
                "Sarah Wiegreffe",
                "Uri Alon",
                "Nouha Dziri",
                "Shrimai Prabhumoye",
                "Yiming Yang",
                "Sean Welleck",
                "Bodhisattwa Prasad Majumder",
                "Shashank Gupta",
                "Amir Yazdanbakhsh",
                "Peter Clark"
            ],
            "title": "Self-refine: Iterative refinement with self-feedback",
            "venue": "ArXiv, abs/2303.17651,",
            "year": 2023
        },
        {
            "authors": [
                "Ana Marasovic",
                "Iz Beltagy",
                "Doug Downey",
                "Matthew Peters"
            ],
            "title": "Few-shot self-rationalization with natural language prompts",
            "year": 2022
        },
        {
            "authors": [
                "Kris McGuffie",
                "Alex Newhouse"
            ],
            "title": "The radicalization risks of gpt-3 and advanced neural language models",
            "venue": "arXiv preprint arXiv:2009.06807,",
            "year": 2020
        },
        {
            "authors": [
                "Clara Meister",
                "Elizabeth Salesky",
                "Ryan Cotterell"
            ],
            "title": "Generalized entropy regularization or: There\u2019s nothing special about label smoothing",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 6870\u20136886,",
            "year": 2020
        },
        {
            "authors": [
                "Todor Mihaylov",
                "Peter Clark",
                "Tushar Khot",
                "Ashish Sabharwal"
            ],
            "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
            "venue": "arXiv preprint arXiv:1809.02789,",
            "year": 2018
        },
        {
            "authors": [
                "Sharan Narang",
                "Colin Raffel",
                "Katherine Lee",
                "Adam Roberts",
                "Noah Fiedel",
                "Karishma Malkan"
            ],
            "title": "Wt5?! training text-to-text models to explain their predictions",
            "venue": "arXiv preprint arXiv:2004.14546,",
            "year": 2020
        },
        {
            "authors": [
                "Nina Poerner",
                "Benjamin Roth",
                "Hinrich Sch\u00fctze"
            ],
            "title": "Evaluating neural network explanation methods using hybrid documents and morphological agreement",
            "venue": "arXiv preprint arXiv:1801.06422,",
            "year": 2018
        },
        {
            "authors": [
                "Archiki Prasad",
                "Swarnadeep Saha",
                "Xiang Zhou",
                "Mohit Bansal"
            ],
            "title": "Receval: Evaluating reasoning chains via correctness and informativeness",
            "venue": "arXiv preprint arXiv:2304.10703,",
            "year": 2023
        },
        {
            "authors": [
                "Danish Pruthi",
                "Bhuwan Dhingra",
                "Livio Baldini Soares",
                "Michael Collins",
                "Zachary C Lipton",
                "Graham Neubig",
                "William W Cohen"
            ],
            "title": "Evaluating explanations: How much do explanations from the teacher aid students",
            "year": 2012
        },
        {
            "authors": [
                "Danish Pruthi",
                "Rachit Bansal",
                "Bhuwan Dhingra",
                "Livio Baldini Soares",
                "Michael Collins",
                "Zachary C. Lipton",
                "Graham Neubig",
                "William W. Cohen"
            ],
            "title": "Evaluating explanations: How much do explanations from the teacher aid students",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Jing Qian",
                "Li Dong",
                "Yelong Shen",
                "Furu Wei",
                "Weizhu Chen"
            ],
            "title": "Controllable natural language generation with contrastive prefixes, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Nazneen Fatema Rajani",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher"
            ],
            "title": "Explain yourself! leveraging language models for commonsense reasoning",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Swarnadeep Saha",
                "Peter Hase",
                "Mohit Bansal"
            ],
            "title": "Can language models teach weaker agents? teacher explanations improve students via theory of mind, 2023",
            "year": 2023
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms. ArXiv, abs/1707.06347, 2017",
            "venue": "URL https://api. semanticscholar.org/CorpusID:28695052",
            "year": 2017
        },
        {
            "authors": [
                "Xuelin Situ",
                "Ingrid Zukerman",
                "Cecile Paris",
                "Sameen Maruf",
                "Gholamreza Haffari"
            ],
            "title": "Learning to explain: Generating stable explanations fast",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Joe Stacey",
                "Yonatan Belinkov",
                "Marek Rei"
            ],
            "title": "Supervising model attention with human explanations for robust natural language inference",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Jiao Sun",
                "Swabha Swayamdipta",
                "Jonathan May",
                "Xuezhe Ma"
            ],
            "title": "Investigating the benefits of freeform rationales",
            "venue": "Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.432. URL https://aclanthology",
            "year": 2022
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan"
            ],
            "title": "Axiomatic attribution for deep networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Oyvind Tafjord",
                "Peter Clark",
                "Matt Gardner",
                "Wen-tau Yih",
                "Ashish Sabharwal"
            ],
            "title": "Quarel: A dataset and models for answering questions about qualitative relationships",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Peifeng Wang",
                "Aaron Chan",
                "Filip Ilievski",
                "Muhao Chen",
                "Xiang Ren. Pinto"
            ],
            "title": "Faithful language reasoning using prompt-generated",
            "venue": "rationales. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "PeiFeng Wang",
                "Aaron Chan",
                "Filip Ilievski",
                "Muhao Chen",
                "Xiang Ren"
            ],
            "title": "PINTO: Faithful language reasoning using prompt-generated rationales",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Peifeng Wang",
                "Zhengyang Wang",
                "Zheng Li",
                "Yifan Gao",
                "Bing Yin",
                "Xiang Ren"
            ],
            "title": "SCOTT: Selfconsistent chain-of-thought distillation",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Sarah Wiegreffe",
                "Yuval Pinter"
            ],
            "title": "Attention is not not explanation",
            "venue": "arXiv preprint arXiv:1908.04626,",
            "year": 2019
        },
        {
            "authors": [
                "Sarah Wiegreffe",
                "Ana Marasovi\u0107",
                "Noah A. Smith"
            ],
            "title": "Measuring association between labels and free-text rationales",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Sarah Wiegreffe",
                "Jack Hessel",
                "Swabha Swayamdipta",
                "Mark Riedl",
                "Yejin Choi"
            ],
            "title": "Reframing human-AI collaboration for generating free-text explanations",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Zeqiu Wu",
                "Yushi Hu",
                "Weijia Shi",
                "Nouha Dziri",
                "Alane Suhr",
                "Prithviraj Ammanabrolu",
                "Noah A. Smith",
                "Mari Ostendorf",
                "Hanna Hajishirzi"
            ],
            "title": "Fine-grained human feedback gives better rewards for language model training",
            "venue": "ArXiv, abs/2306.01693,",
            "year": 2023
        },
        {
            "authors": [
                "Kevin Yang",
                "Dan Klein"
            ],
            "title": "FUDGE: Controlled text generation with future discriminators",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Kexin Yang",
                "Dayiheng Liu",
                "Wenqiang Lei",
                "Baosong Yang",
                "Mingfeng Xue",
                "Boxing Chen",
                "Jun Xie"
            ],
            "title": "Tailor: A prompt-based approach to attribute-based controlled text generation, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Eric Zelikman",
                "Jesse Mu",
                "Noah D Goodman",
                "Yuhuai Tony Wu"
            ],
            "title": "Star: Self-taught reasoner bootstrapping reasoning with reasoning",
            "year": 2022
        },
        {
            "authors": [
                "Honghua Zhang",
                "Meihua Dang",
                "Nanyun Peng",
                "Guy Van Den Broeck"
            ],
            "title": "Tractable control for autoregressive language generation",
            "venue": "Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "2016 Li et al",
                "2019 Jin et al",
                "Lundberg",
                "2017 Lee",
                "Chan"
            ],
            "title": "2022). This token scoring can be done via input gradients (Sundararajan et al., 2017",
            "venue": "scoring input tokens\u2019 influence on the model\u2019s output (Denil et al.,",
            "year": 2014
        },
        {
            "authors": [
                "2022 Chan et al",
                "2020 Jain et al",
                "2021 Situ et al",
                "Liu"
            ],
            "title": "2023b). For the purpose of this work, we mainly focus on free-text rationales. There are two primary methods adopted by prior works for generating free-text rationales. The first set of approaches use gold human-written rationales to train a rationale generation model (Camburu et al., 2018",
            "venue": "(Lei et al.,",
            "year": 2020
        },
        {
            "authors": [
                "2023b Li et al",
                "2023 Chan et al",
                "2023b Wang et al",
                "Saha"
            ],
            "title": "Evaluating free-text rationales. Existing works have conducted human and automatic evaluation of free-text rationales based on their association with predicted labels (Wiegreffe et al., 2021), acceptability (Wiegreffe et al., 2022), informativeness (Chen et al., 2023a), benefits and human utility",
            "venue": "Hsieh et al.,",
            "year": 2023
        },
        {
            "authors": [
                "Joshi et al",
                "simulatability (Rajani et al",
                "Hase"
            ],
            "title": "2020) and faithfulness (Atanasova et al., 2023; Wang et al., 2023a) to name a few. Some recent works have also provided frameworks to evaluate logical correctness of reasoning chains, that are similar to free-text rationales (Golovneva et al., 2022",
            "venue": "(Sun et al.,",
            "year": 2023
        },
        {
            "authors": [
                "rewards. Lu"
            ],
            "title": "2022) that unlearns toxicity by specifically fine-tuning the model on what not to do, Lu et al. (2023) which tailors the generation of extremely large LMs like GPT-3 using trained policy adaptor models. Zelikman et al. (2022) that leverages a small number of demonstrations to iteratively generate new data to train the model (new data such that the task prediction",
            "year": 2022
        },
        {
            "authors": [
                "Li"
            ],
            "title": "2023a Q: Mike was snowboarding on the snow and hit a piece of ice. He went much faster on the ice because is smoother. (A) snow (B) ice A: When something is smoother, it is easier to slide on",
            "year": 2023
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "Q: The sun is responsible for (a) puppies learning new tricks (b) children growing up and getting old (c) flowers wilting in a vase (d) plants sprouting, blooming and wilting A: A plant requires sunlight for photosynthesis",
            "year": 2022
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "2023a Q: How do you reduce pollution? (A) igniting fuel and oxidiser (B) transportation technology (C) wasting (D) not recycling (E) burning fossil fuels (F) converting electricity to heat (G) water conservation (H) using less resources A: Conserving resources has a positive impact on the environment. Use of resources affects",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In recent years, there has been a surge of interest in using LMs to generate fluent, human-like, freetext rationales that can explain their decision, called self-rationalizing LMs (Wiegreffe et al., 2021). Early approaches in self-rationalizing involved collecting human-written gold rationales and using them as supervision for training LMs (Wiegreffe et al., 2021; Narang et al., 2020). Recently, with the advent of large LMs, chain-of-thought prompting (Wei et al., 2022; Marasovic et al., 2022; Kojima et al., 2022) has revolutionized the landscape of self-rationalization; now, with just a few welldesigned prompts and demonstrations, LMs can generate explanations for their predictions. The presence of rationales can make LMs both more interpretable (Lertvittayakumjorn & Toni, 2021) and more usable (Joshi et al., 2023) from the perspective of users.\nHowever, prior work in self-rationalization has largely overlooked the quality of generated rationales themselves; instead, their utility is justified by measuring downstream task performance (Wei et al., 2022; Zelikman et al., 2022). This is particularly problematic, as downstream models and users may use these rationales as justifications for the predicted answer, which can further propagate these negative quality outcomes (Atanasova et al., 2023; Joshi et al., 2023; Hovy & Prabhumoye, 2021). Furthermore, it is observed that rationalization comparable to human quality is only observed with at a significant LM parameter scales (\u223c100B or more) (Wei et al., 2022). Despite some recent interest in using smaller LMs for rationalization (Chen et al., 2023b), it is still unclear if smaller LMs can be used to generate similarly high-quality rationales.\nIn this work, we propose MARIO, a method that focuses on tailoring small-sized LMs (< 1B parameters) to be strong rationalizers both in terms of improved downstream performance, and in terms of desireable properties of the rationales themselves. Instead of relying on human rationale labelling (Wiegreffe et al., 2021), MARIO considers a setting where a small LM only has access to rewards that measures factors underlying rationale quality, e.g. a trained LM that judges the plausibility of a rationale and provides a numerical score. MARIO first starts with training a small LM to selfrationalize, with the help of GPT-3 (Brown et al., 2020) (TEXT-DAVINCI-003)1 generated rationales as initial supervision, which are shown to be of higher quality Sun et al. (2022). It then casts the problem into a multi-reward conditioned rationale generation problem, where the LM is optimized to maximize quality rewards. In order to achieve this, MARIO extends QUARK proposed by Lu et al. (2022) to a multi-reward setup, where generations from an LM are binned according reward values; the LM learns distributions conditioned on \u2018control-tokens\u2019 corresponding to every reward and high-quality generations can be obtained via conditioning on the highest-reward token.\nWe determine that high-quality rationales should have three necessary properties: plausibility (makes logical sense), diversity (is not repetitive) and consistency (supports the correct answer for the instance). Generated rationales\u2019 rewards are assessed through automated metrics for each of the three quality properties. We then evaluate MARIO on three question-answering datasets, and observe that small LMs like T5-LARGE can be effectively trained to generate rationales that satisfy all of the quality requirements, while also leading to improvements in task performance over supervised fine-tuned self-rationalizers (SFT). Via human evaluation, we also observe that rationales generated by MARIO are more preferred over those generated by SFT, across all datasets.\nWe note that tailoring small LMs with multiple quality rewards is a challenging task. Some of these issues include finding high-quality, stable rewards that can be effectively incorporated in a selfrationalizing pipeline. We also observed that a lot of additional desirable properties in rationales (like factuality and completeness) do not have reliable automated rewards. Furthermore, improving task accuracy (which is the primary goal while generating rationales for a lot of these tasks) is challenging in a multi-reward setup, and show that adding task accuracy as an additional reward term leads to the best configuration of MARIO. By using small LMs to generate high-quality rationales that are also supported by human evaluations, we believe our findings can help guide future work in efficient, real-world situated methods in rationale generation and evaluation."
        },
        {
            "heading": "2 SELF-RATIONALIZATION",
            "text": "Throughout this work, we refer to self-rationalizers as LMs that are trained or prompted to specifically generate free-text rationales, along with their predictions. These free-text rationales are treated as explanations for their predictions. For the purpose of our experiments, we explore selfrationalization on the question answering (QA) task. Specifically, given a question, the LM must first generate a free-text rationale that explains the LM\u2019s reasoning process, followed by an answer to the given question. Table 1 shows examples of inputs and outputs by these self-rationalizing LMs for three QA datasets: STRATEGYQA (Geva et al., 2021), QUAREL (Tafjord et al., 2019) and OPENBOOKQA (Mihaylov et al., 2018). These datasets were chosen over others which have existing human written rationales because all of them require certain level of implicit or logical reasoning in order to arrive at the answer. As we depict in the examples, we follow the I-RO format (Wiegreffe et al., 2021), wherein the input to the LM is the question, and the output is the joint generation of the rationale and the predicted answer.\n1note that whenever we mention GPT-3 in this work, we are referring to TEXT-DAVINCI-003\nIn order to determine whether these generated rationales are of good quality, we focus on three properties that are necessary for any rationale to have, agnostic of the task it is meant for. First, we note that a rationale should be plausible. We define plausibility as the rationale making sense on its own \u2013 whether it be common, logical or factual sense depending on the dataset at hand. For example, if a rationale states \u2018Cows can fly\u2019, it is not plausible. Next, we identify that a rationale should be diverse, where the rationale is clean and not repetitive. Lastly, we note that a rationale should be consistent with the gold label for the input. Consistency is important to ensure that a rationale does not spew irrelevant information, and that it supports the gold answer. Furthermore, we focus on consistency with respect to the gold label, as misleading rationales are unhelpful as both LM justifications, and for human utility (Joshi et al., 2023). We formalise these properties as rewards in \u00a74 and while these are necessary properties for any rationale, we also discuss other good-to-have properties in \u00a75.\nAll of these properties are agnostic of the actual prediction made by the LM. Since our selfrationalization setup generates a rationale first, followed by its prediction, we aim to generate rationales with good quality, which should ideally improve the answer generated by the LM. Therefore, we focus on improving self-rationalization along these three properties, as well as on task accuracy. Along with the above rationale properties, we also consider task correctness as a necessary property of rationales, that they should try to improve over as a byproduct."
        },
        {
            "heading": "3 MARIO: OPTIMIZING FOR MULTIPLE REWARDS",
            "text": "To improve an LMs\u2019 rationalization across multiple properties, we leverage QUARK(Lu et al., 2022), a reinforcement learning-like framework effective on tasks such as unlearning toxicity in generations. We propose Multi-rewArd RatIOnalization (MARIO), an extension of QUARK to multiple rewards concurrently. We further propose two variants of MARIO: CLASSIC and ADDITIVE, that explore different ways of using QUARK for in a multi-reward setup. Figure 2 shows a running example of MARIO. Appendix B shows a second, more technical illustration of the same."
        },
        {
            "heading": "3.1 QUARK",
            "text": "QUARK(Lu et al., 2022) is a reinforcement learning-like algorithm that trains LMs using unique control tokens prepended to the generated text. The QUARK algorithm works iteratively: (1) sampling the a pre-trained LM to generate more training data, (2) scoring the generated data using a chosen reward metric, and (3) using instance-level scores to sort and bin the data into a fixed number of bins, each of which correspond to a unique control token. During training, the LM learns to associate each control token with the quality (as determined by the reward metric) of the data it is assigned to. During inference, in order to obtain the best quality generations, QUARK samples\nthe LM using the control token corresponding to the highest reward measure. We provide a more detailed description of how QUARK works in Appendix B."
        },
        {
            "heading": "3.2 CLASSIC MARIO",
            "text": "Since QUARK is designed only for single reward optimizations, what if we are interested in improving the rationales along multiple rewards? In order to do this, we first propose a direct extension to the QUARK algorithm: instead of assigning each instance just one unique control token (which corresponds to one specific property), we now assign each instance multiple unique control tokens at once; now, each control token corresponds to a different property we want to train the LM on. We call this the CLASSIC MARIO. The order in which properties are represented in this method is a design decision that we can choose, and we expand on this further in Appendix C."
        },
        {
            "heading": "3.3 ADDITIVE MARIO",
            "text": "We now propose a step-by-step multi-reward QUARK: instead of training the LM on all the properties at the same time, we introduce them into training pipeline additively, in a predefined order of properties. For example, say that we have properties P1, P2, P3, and that we want the LM to focus on P3 first, before moving on to P1 and then later P2. In this method, we use multiple sets of control tokens as in CLASSIC MARIO, but, we introduce each set of tokens into the training pipeline successively. For example, we train the LM with only P3 for the first t steps, then we train the LM on P3 and P1 from the t-th step to the 2t-th step, and then from the 2t-th step onwards, we train the LM on P3, P1 and P2. We call this method the ADDITIVE MARIO. Again, the order in which we add the control tokens of different rewards to the training pipeline, and whether each new control token is added to the left or right of the existing control tokens are decision choices, and we expand on it further in Appendix C."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 DATASETS",
            "text": "As we mention in Section 2, we conduct experiments on three datasets: STRATEGYQA (Geva et al., 2021), QUAREL (Tafjord et al., 2019) and OPENBOOKQA (Mihaylov et al., 2018). All datasets have multi-choice questions (yes/no for STRATEGYQA, a/b for QUAREL, a/b/c/d for OPENBOOKQA), and the task is to generate a rationale followed by the predicted answer. We report details of train, val and test splits in Appendix D. For all the datasets, we do not require human-written gold rationales, and thus, we sample GPT-3 Brown et al. (2020) for silver rationales for supervision. We use chain-of-thought prompts from prior works on these datasets (refer Appendix I, E for the full\nprompts) and sample 5 rationale generations for each training set instance with a temperature of 0.9; for supervision we use only the instances where the answer predicted by GPT-3 is correct. We use these silver rationales in three places: (1) to train SFT, (2) we use aforementioned SFT as the reference model for the KL divergence loss in MARIO\u2019s training process (Appendix B, E), (3) we also add these silver rationales to the overall data pool of MARIO (Appendix B provides an expanded explanation for the same)."
        },
        {
            "heading": "4.2 RATIONALE PROPERTY REWARDS AND TASK CORRECTNESS",
            "text": "As described in \u00a72, we determine three rationale quality properties that are used in MARIO. Here, we formalize these properties with their implementations that we use as rewards within MARIO. Let Q be the input question, R\u0302 represent the LM\u2019s generated rationale, and O and O\u0302 represent the gold answer and the LM\u2019s predicted answer respectively. The formulations of each reward are as follows:\n\u2022 PLAUSIBILITY via VERA (Liu et al., 2023a): VERA is a trained commonsense statement verification T5-11B model, that provides a numerical score between 0 and 1, indicating the plausibility of declarative sentences.\nPLAUSIBILITY(R\u0302) = VERA(R\u0302) (1)\nWe use the VERA release from HuggingFace 2. \u2022 CONSISTENCY via (Wiegreffe et al., 2021): Wiegreffe et al. (2021) provide a framework\nto evaluate the association between a rationale and a label with the help of two reference LMs that are trained to predict the answer with (MQR) and without (MQ) the rationale in the input. More formally,\nCONSISTENCY(R\u0302) = PMQR(O|Q, R\u0302)\u2212 PMQ(O|Q) (2)\nCONSISTENCY is a numerical score between \u22121 and 1 that indicates the faithfulness of the rationale towards the gold answer, like the implementation by Wiegreffe et al. (2021). Hyperparameters and training guidelines for the LMs involved in generating the CONSISTENCY score is in Appendix E.\n\u2022 DIVERSITY via n-gram uniqueness in Li et al. (2022): Li et al. (2022) calculate diversity of generated text by determining the fraction of unique n-grams generated. DIVERSITY is a numerical score between 0 and 1 that indicates the lexical diversity of the rationale; this metric also serves the purpose of ensuring that the rationale does not contain repeated phrases or sentences.\nDIVERSITY(R\u0302) = 4\u220f\nn=2\nunique n-grams(R\u0302)\ntotal n-grams(R\u0302) (3)\n\u2022 TASK-CORRECTNESS: As our last reward, we add task correctness of the answer that is generated following the rationale. This is a binary 0/1 score, referring to the wrong and right predicted answer respectively.\nTASK-CORRECTNESS(R\u0302) = 1[O\u0302 = O] (4)\nWe evaluate/report these metrics for all our baselines and experiments. To simplify comparisons, we also report an average normalized relative gain (NRG) (Chan et al., 2022) (Appendix E)."
        },
        {
            "heading": "4.3 BASELINES VS. MARIO",
            "text": "All our baselines and MARIO are built on top of T5-LARGE LMs (0.7B). We present and compare our method with four strong baseline models:\n1. Supervised Fine-tuned Self-Rationalizer (SFT): A fine-tuned T5-LARGE, which serves as the supervised learning baseline (we use the training data as described in \u00a74.1), trained to generate rationales and answers.\n2https://huggingface.co/liujch1998/vera\nTable 3 shows the comparisons between MARIO and the baselines. For all three datasets, we note that MARIO is the overall best setup as noted by both the individual metrics and the averaged NRG metric. CLASSIC MARIO is found to be the best performing method for QUAREL and OPENBOOKQA, and ADDITIVE MARIO is found to be the best method for STRATEGYQA. We detail the hyperparameter configurations for all these models in Appendix E. It is important to note that not only does the rationale get better (as seen via the rationale metrics), but the task accuracy also shows a marked improvement over the baselines. We show some representative examples of rationales generated by training with MARIO, in comparison with those generated by SFT in Table 4."
        },
        {
            "heading": "4.4 HUMAN PREFERENCE EVALUATION",
            "text": "Apart from significant numerical improvements by MARIO, we also conduct human evaluations in order to situate these improvements in qualitative environments. We compare rationales generated by SFT and the best MARIO setting for all three datasets. For each instance, we ask three distinct annotators from a pool of qualified annotators to compare the two rationales across three settings, for a given question and correct answer pair: PLAUSIBILITY and CONSISTENCY, which are defined in the same manner as the rewards, and an overall PREFERENCE rating. PREFERENCE\nis meant to indicate that the annotators pick the rationale that they would find acceptable (Wiegreffe et al., 2022) for the given question. In Figure 3, we plot the % of instances where majority of annotators prefer only MARIO\u2019s rationales, only SFT\u2019s rationales, both or none. We note human annotators prefer MARIO\u2019s only rationales for 83.15%, 75.3% and 71.49% of instances respectively for STRATEGYQA, QUAREL and OPENBOOKQA. Human annotators also find MARIO\u2019s rationales to be considerably more plausible and consistent than SFT3. We use Amazon MTurk4 for all our human studies, and Appendix J provides further details on the same."
        },
        {
            "heading": "4.5 REFERENCE LARGE LMS VS. MARIO",
            "text": "We now consider three strong reference Large LMs that are used in practice for self-rationalization (Wei et al., 2022): GPT-3 (Brown et al., 2020) (175B), FLAN-T5 (Chung et al., 2022) (in three sizes, L, XL, XXL) and LLAMA (Touvron et al., 2023) (in two sizes 7B, 65B); we compare MARIO with them in terms of both average NRG, with respect to the size of the underlying LMs\n3We do not perform human studies for DIVERSITY and TASK ACCURACY since they are automatic/straightforward metrics\n4https://www.mturk.com/\n(Figure 4) and individual metric scores (Table 11 in the appendix). All these reference LMs apart from FLAN-T5-L are orders of magnitude larger than our T5-LARGE LM trained with MARIO; we include FLAN-T5-L in our comparison even though it\u2019s of the same size as MARIO because FLAN-T5-L is instruction-finetuned, and few-shot prompted to generate rationales, with the same set of demonstrations used by other large LMs (shown in Appendix I).\nIdeally, we want a small-sized LM (for efficiency) that achieves high performance, which corresponds to the top-left portion of the graph in Figure 4. Hence, to compare two LMs\u2019 performance, the LM which is relatively to the left and to the top is practically a better choice. We note that for QUAREL (middle graph), MARIO results in an LM that is of a very small size (0.7B) but has a very high performance, almost equivalent to that of GPT-3. For OPENBOOKQA (rightmost graph), we see that MARIO beats the LMs FLAN-T5-L, FLAN-T5-XL and LLAMA-7B. For STRATEGYQA we see that our LM beats FLAN-T5-L, while performing only a little worse than FLAN-T5-XL."
        },
        {
            "heading": "5 DISCUSSION",
            "text": ""
        },
        {
            "heading": "5.1 PROPERTIES AND METRICS",
            "text": "While the properties we have optimized for in this work are necessary properties, the question of what constitutes a complete set of properties for a rationale to be deemed as high-quality remains an open problem (Joshi et al., 2023; Wiegreffe et al., 2022; Golovneva et al., 2022). Some contemporary works which work on other necessary rationale properties are REV Chen et al. (2023a) (novelty of information and faithfulness towards the predicted label), ROSCOE Golovneva et al. (2022) / ReCEval Prasad et al. (2023) (scoring step-by-step reasoning), LAS Hase et al. (2020) (faithfulness towards predicted labels) etc. Further, there are also properties which do not have widespread implementations (to the best of our knowledge) such as factual-correctness and completeness of rationales (existing completeness metrics depend upon gold rationales which are not easily available, and which cannot score any alternate reasoning to the answer). As future work, we hope to collect an extended set of properties and corresponding metrics, and improve them with MARIO."
        },
        {
            "heading": "5.2 MULTI-REWARD HACKING",
            "text": "As additional experimentation with alternate properties relevant to our chosen QA datasets, we worked on a set of experiments focusing on factual-correctness and lexical diversity; specifically for STRATEGYQA which requires historical or factual correctness of the rationale (this is different from common-sense or logical correctness measured by PLAUSIBILITY, as explained in VERA(Liu et al., 2023a)). We started with a fact verification metric LOREN (Chen et al., 2022) - while effective, we couldn\u2019t use this metric in practice since each score prediction required a Web API call, which is inefficient given MARIO\u2019s iterative data generation and scoring. We tried a weaker metric - querying the rationale with a larger LM, FLAN-T5-XXL and asking if the rationale was factually correct or not (probability of \u2018yes\u2019 under yes/no). We noticed that applying QUARK/MARIO with this metric led to some interesting reward hacking, as we show in the first two rows of Table 5. Trying to improve on just factuality caused the LM to start generating repetitive text, as an easy way of generating factual statements. When we tried to counter this by training MARIO on factuality and lexical diversity together, the LM started generating incomplete rationales. We further noted that this kind of repetitive generation is observed even in larger LMs which on surface seem much better (as\nseen in rows 3 and 4 in Table 5). Therefore, we note that selecting strong rewards, as well as careful qualitative investigation is extremely important to prevent this kind of reward hacking \u2013 where an increase in individual numerical reward scores do not guarantee overall qualitative improvements."
        },
        {
            "heading": "5.3 IS ACCURACY ENOUGH OF AN INDICATOR FOR RATIONALE QUALITY?",
            "text": "As we discuss in \u00a71, many contemporary works on self-rationalization ignore the independent quality of the generated rationale, and focus entirely on how the rationale can contribute to the task performance. In this discussion, we analyze the reverse: if an LM is trained only with respect to task performance, what does this mean for the rationale? We refer back to our main results, Table 3; we specifically look at the rows SFT, FILT-ACC and MARIO. We first see that both FILT-ACC and SFT both improve upon the TASK ACCURACY on all 3 datasets, as intended. We then see that for STRATEGYQA and QUAREL, the average quality of the rationales generated by MARIO is decidedly better than the rationales generated by FILT-ACC, as seen by the values of the individual rationale quality metrics. For OPENBOOKQA, the analysis from just the metrics is inconclusive; hence, we perform human studies comparing FILT-ACC and MARIO, in the same manner as in \u00a74.4. We find that human annotators prefer MARIO\u2019s rationales highly over that of FILT-ACC: for 69.65% of the questions, majority of the annotators prefer MARIO\u2019s rationales (as opposed to 22.88% of preference for FILT-ACC\u2019s rationales, and 7.46% preference for both). We further performed human studies for PLAUSIBILITY and CONSISTENCY, and again, MARIO\u2019s rationales were found to be distinctly better (PLAUSIBILITY: 49.5% preference for MARIO, 32.58% for SFT, 13.43% both, 0.99% neither; CONSISTENCY: 48% preference for MARIO, 37.31% for SFT, 9.45% both, 2.48% neither). In conclusion, we find that optimizing for task performance does not naturally improve rationale performance, which further motivates the introduction of MARIO."
        },
        {
            "heading": "6 CONCLUSION AND FUTURE WORK",
            "text": "Existing self-rationalization LMs use rationales as a means for improving downstream task accuracy, with the help of large-scale LMs. In this work, we propose MARIO, an algorithm that performs multi-reward optimization of small self-rationalizing LMs to jointly improve the quality of their rationales as well as their task accuracy. We present strong experimental results on a small LM, T5-LARGE, over competitive baselines, on datasets STRATEGYQA, QUAREL and OPENBOOKQA. In addition to a strong improvement in task accuracy, we see that rationales produced by training an LM with our method are strongly preferred by human annotators. Lastly, we discuss intricacies of reward-conditioned rationale generation for small LMs, issues faced with selecting appropriate rewards, as well as shortcuts taken by QUARK to improve reward scores that do not translate well to qualitative improvement. As future work, we hope to extend our algorithm to improving rationales along more dimensions like completeness, factuality as well as human utility.\nETHICAL CONSIDERATIONS\nLike any natural language generation system/algorithm, MARIO can unintentionally lead to toxic and harmful text; it is up to the user of the algorithm to use it responsibly, with non-harmful reward metrics, to prevent the generation of biased and malicious outputs. As noted in McGuffie & Newhouse (2020), this is a deliberate misuse of text generation models, and we strongly denounce such practices.\nData. All the datasets that we use in our work are released publicly for usage and have been duly attributed to their original authors.\nCrowdsourcing. All our crowdworkers are from countries where English is the primary language. For all our human studies, the task is setup in a manner that ensure that the annotators receive compensation that is above minimum wage ($20/hour). Since we conduct extensive qualification tasks before annotations, crowdworkers that participate in the qualification are compensated more than the task, given the time taken to read and understand task instructions and examples. Furthermore, we ensure that we correspond with crowdworkers over email to address their queries. Crowdworkers have also been given bonuses for flagging errors in the task, or consistently providing good-quality annotations.\nREPRODUCIBILITY\nFor all our experimental results and models, we report (1) the complete hyperparameter setting and any bounds explored (Appendix E) as well as the sizes and versions/pretrained-model links of all models used, (2) the time taken per experiment, and infrastructure used, (3) the mathematical equations (\u00a74.2, Appendix B) for all algorithms and metrics used, (4) descriptions of datasets, and demonstrations used to sample rationales from GPT-3. Further, we have also submitted all codes as supplementary material."
        },
        {
            "heading": "A RELATED WORK",
            "text": "Self-rationalization and rationale-based distillation. Model decisions can be explained in two ways - by extracting rationales from the input text, or generating free-text rationales that may not be grounded in the input. An extractive rationale explains a model\u2019s output on a given task instance by scoring input tokens\u2019 influence on the model\u2019s output (Denil et al., 2014; Sundararajan et al., 2017; Li et al., 2016; Jin et al., 2019; Lundberg & Lee, 2017; Chan et al., 2022). This token scoring can be done via input gradients (Sundararajan et al., 2017; Lundberg & Lee, 2017; Denil et al., 2014; Li et al., 2015), input perturbation (Li et al., 2016; Poerner et al., 2018; Ka\u0301da\u0301r et al., 2017), attention weights (Pruthi et al., 2020; Stacey et al., 2022; Wiegreffe & Pinter, 2019), or learned rationale extraction models (Lei et al., 2016; Chan et al., 2022; Jain et al., 2020; Situ et al., 2021; Liu et al., 2023b). For the purpose of this work, we mainly focus on free-text rationales. There are two primary methods adopted by prior works for generating free-text rationales. The first set of approaches use gold human-written rationales to train a rationale generation model (Camburu et al., 2018; Narang et al., 2020; Wiegreffe et al., 2021). The second set of approaches prompt large LMs with the help of curated templates with or without demonstrations containing examples of rationale generation for the task at hand (Wei et al., 2022; Kojima et al., 2023; Li et al., 2023c; Jung et al., 2022; Lightman et al., 2023). Some approaches also leverage few-shot training approaches with a handful of gold rationales (Marasovic et al., 2022; Chen et al., 2023b). Recent approaches also leverage rationales generated by large LMs to distill small LMs to be better at the task or better rationalizers. (Pruthi et al., 2022; Li et al., 2023b; Chan et al., 2023; Wang et al., 2023b; Saha et al., 2023; Hsieh et al., 2023)\nEvaluating free-text rationales. Existing works have conducted human and automatic evaluation of free-text rationales based on their association with predicted labels (Wiegreffe et al., 2021), acceptability (Wiegreffe et al., 2022), informativeness (Chen et al., 2023a), benefits and human utility (Sun et al., 2022; Joshi et al., 2023), simulatability (Rajani et al., 2019; Hase et al., 2020) and faithfulness (Atanasova et al., 2023; Wang et al., 2023a) to name a few. Some recent works have also provided frameworks to evaluate logical correctness of reasoning chains, that are similar to free-text rationales (Golovneva et al., 2022; Prasad et al., 2023).\nReward-conditioned text generation. Reinforcement learning has proven to be a reliable means to optimize language models towards a specific objective. One such example, proximal policy optimization (PPO) (Schulman et al., 2017), has been commonly used for a variety of tasks, spanning detoxification (Wu et al., 2023; Lu et al., 2022), RLHF (Dubois et al., 2023; Bai et al., 2022), improving commonsense reasoning capabilities (Liu et al., 2022), and more. Adjacent to PPO, there are several other lighter-weight algorithms which condition the policy language model directly on the reward without the need for a value function (Lu et al., 2022; Gulcehre et al., 2023; Dong et al., 2023; Lu et al., 2023; Zelikman et al., 2022). These methods rely on iterative, off-policy explorations at fixed intervals to continuously aggregate new trajectories to learn from. Another line of work improves the reward directly through iterative refinement on a frozen policy model (Madaan et al., 2023). There are several algorithms and methods today to update text generation models with rewards. Lu et al. (2022) that unlearns toxicity by specifically fine-tuning the model on what not to do, Lu et al. (2023) which tailors the generation of extremely large LMs like GPT-3 using trained policy adaptor models. Zelikman et al. (2022) that leverages a small number of demonstrations to iteratively generate new data to train the model (new data such that the task prediction is correct). Other recent work on controllable text generation revolves around creative text generation with single and multiple rewards (Yang & Klein, 2021; Keskar et al., 2019; Zhang et al., 2023; Qian et al., 2022; Yang et al., 2022)"
        },
        {
            "heading": "B QUARK AND MARIO",
            "text": "Here, we describe QUARK and MARIO in more technical detail (refer the top and bottom pipelines in Figure 5 respectively).\nQUARK begins training with a pretrained trained language model P0(t|x); QUARK also requires a reference language model Pref (t|x) (which can be the same as P0, or different), and a reward function Rew(t, x) \u2192 R. Note that x = [x0, x1, . . . , xm\u22121] stands for the input text sequence, and t = [t0, t1, . . . , tn\u22121] stands for the output sequence generation. Lastly, QUARK works with a\ndata pool D which is constantly updated and added to over the course of training (as we describe below); further, D can be initialized with gold-standard or silver-standard data, D = Dgold/silver = (x, tgold/silver, r).\nAs we explain in Section 3.1, QUARK operates in an iterative fashion:\n1. sampling P0 to generate more training data: Dnew = (x, tnew) 2. scoring the generated data using Rew(x, t): D \u2032\nnew = (x, tnew, r)\n3. using these instance-level scores to sort and bin the data into a fixed number of bins [b1, b2, . . . , b5] each of which correspond to a unique control token: D \u2032\u2032\nnew = (x, tnew, r, b), 4. Adding the now control-token attached data to the (growing) training data pool: D =\nD \u222aD\u2032\u2032new\nDuring training, the model starts to associate each control token with its corresponding quality of data (as given by Rew(x, t)), and to obtain the best quality generations during inference, QUARK samples the trained P0 using the control token corresponding to the highest reward measure. QUARK is trained using the following training objectives:\n\u2022 Reward-based learning using implicit reward signals based on control tokens (which are obtained by sorting the reward Rew(x, t) scores), as described above,\n\u2022 Language model objective using supervised/cross-entropy loss with respect to the target generation (as explained above, QUARKsamples training data in an online manner from P0; however, if gold or silver offline training data is available, that can also be injected into the training pipeline by scoring with Rew(x, t))\n\u2022 Stable text generation using the KL divergence penalty of P0\u2019s generation with respect to Pref , and. \u2022 Entropy regularization of the generated text as in Meister et al. (2020)\nThe objective function for QUARK is:\nmax \u03b8 Ek\u223cU(1,K)E(x,y)\u223cDk\n[ log p\u03b8 (y | x, rk)\u2212 \u03b2\nT\u2211 t=1 KL (p0 (\u00b7 | y<t, x) \u2225p\u03b8 (\u00b7 | y<t, x, rk))\n] (5)\nHere, the first term stands for the supervised cross-entropy loss, and the second term stands for the KL divergence loss. Entropy regularization can also be added if/when needed. Note that x is the\ninput text, y is the generated output sequence and rk, k \u2208 {1, . . . ,K} stands for the reward/control token.\nWe extend QUARK to MARIO by using multiple sets of control tokens, each corresponding to a distinct reward/property, i.e., Rew1(x, t), Rew2(x, t), . . . , Rewk(x, t); the CLASSIC and ADDITIVE methods use these control tokens either together, or in a step-by-step fashion as we explain in \u00a73.2, 3.3. Further, we want to note that step-3 of the algorithm (wherein we use instance-level scores to sort and bin the data) is done in MARIO separately for each reward; each reward/property goes through an individual scoring + binning process and gets a distinct control token. Subsequently, each reward/property also has its own set of control tokens (as depicted in Figure 5). The rest of the training follows the same iterative process and training objectives as QUARK. The objective function for MARIO is:\nmax \u03b8 Ej\u223cU(1,J),k\u223cU(1,K)E(x,y)\u223cDk[ log p\u03b8 (y | x, [.., rjk, ..])\u2212 \u03b2\nT\u2211 t=1 KL (p0 (\u00b7 | y<t, x) \u2225p\u03b8 (\u00b7 | y<t, x, [.., rjk, ..]))\n] (6)\nHere again, the first term stands for the supervised cross-entropy loss, and the second term stands for the KL divergence loss; entropy regularization can be added if/when needed. x is the input text, y is the generated output sequence and rjk, j \u2208 {1, . . . , J}, k \u2208 {1, . . . ,K} stands for the reward/control token corresponding to the j-th property and the k-th reward bin."
        },
        {
            "heading": "C ORDER OF TOKENS",
            "text": "As we explain in the above two sections, the order of the control tokens corresponding to each reward we use in training our self-rationalizing LM is a design choice. Say for example, we have three properties, along with control tokens corresponding to the task accuracy (as we do in this paper, refer \u00a74.2): this means that there are potentially 24 orders of these properties that we can use in CLASSIC MARIO, and 48 possible variations that we can use for ADDITIVE MARIO (24 orders x 2 directions in which we can introduce the property to the training \u2013 left or right of the existing control tokens, assuming we keep the direction of addition consistent throughout training). It is impractical and inefficient to experiment with all these possible orders to pick the best possible one. Hence, we propose a simple way of picking the order, based on the relative strengths of a (supervised-trained) self-rationalizing LM in each of these properties.\nFor example, say we have four reward metrics R1, R2, R3, R4, and we determine through a predefined method which property the LM is relatively stronger in (for example, say the LM is good at generating lexically diverse statements, but is only moderately good at grammar, is broadly bad at generating plausible statements, and even worse at producing concise rationales). For example, we determine the relative strength of rewards based on how good the supervised finetuned baseline SFT is on a particular metric on the validation set, as opposed to the maximum and minimum value of the metric itself.\nstrength(Ri) = max(Ri)\u2212 ri\nmax(Ri)\u2212 min(Ri) (7)\nHere Ri refers to the reward, ri refers to the value the SFT has on the property Ri on the validation set, and max/min(Ri) refer to the maximum and minimum value taken by the reward metric Ri.\nFor example, let the relative order of the four reward metrics using the above approach is R2 < R1 < R4 < R3. Hence, we experiment with training the LM with the order R2, R1, R4, R3 if we want to allow the weaker rewards to improve on their own, before the stronger rewards are introduced into the mix. Additionally, we can also use the opposite order R3, R4, R1, R2, so that the LM can quickly optimize on the stronger rewards and then try to be better with the weaker rewards."
        },
        {
            "heading": "D DATASET SPLITS",
            "text": "\u2022 For STRATEGYQA, since labels are not available for evaluation sets, we split the train set into training, validation and test sets (taken from Joshi et al. (2023)), and report scores on this test set.\n\u2022 For OPENBOOKQA and QUAREL, we use the provided training dataset, tuned on the validation set and report final performances on the test set5.\n\u2022 For NUMERSENSE, we use the train, validation and test sets as in the official GitHub6 release.\n\u2022 For QASC, we split the original train set into train and validation (900 questions chosen randomly for validation), and use the original validation set as the test set7."
        },
        {
            "heading": "E HYPERPARAMETERS AND EVALUATION",
            "text": "We use T5-LARGE (0.7B parameters) for SFT and all our MARIO experiments, and we use T5BASE for our CONSISTENCY models (as used in the original work Wiegreffe et al. (2021)) - we always start training with the pretrained model from HuggingFace8. Tables 6, 7 and 8 show the hyperparameters used to train SFT, CONSISTENCY and MARIO respectively. Note that for our MARIO training, we use SFT as the reference model (Pref (t|x) from Appendix B) for the KL divergence penalty. We also use the silver rationales sampled from GPT-3 as our initial data pool D (from Appendix B). Further, during inference, we always use greedy decoding. We run all our experiments on NVIDIA Quadro RTX 8000 GPUs. For training SFT and CONSISTENCY models, we use 1 GPU per experiment; for training MARIO, we use 2 GPUs per experiment - the first GPU to hold P0, Pref (notation from Appendix B), and the second GPU to hold the PLAUSIBILITY and CONSISTENCY reward models.\nFurthermore, we aggregate metrics using Normalized Relative Gain as mentioned in Chan et al. (2022). NRG of a metric value zi (corresponding to the general property Z) is formally defined as:\nNRG(zi) = zi \u2212 min(Z)\nmax(Z)\u2212 min(Z) (8)\nThe average NRG of a set of metrics (such as with the four metrics in this work) is a simple mathematical average of their individual NRG\u2019s.\nFurther, for our statistical significance tests, are done using one-tailed independent t-tests (using scipy.stats.ttest ind)."
        },
        {
            "heading": "F EXPERIMENTS ON NUMERSENSE AND QASC",
            "text": "Table 9 has the MARIO results for NUMERSENSE Lin et al. (2020) and QASC Khot et al. (2020). Figures 6 and 7 respectively show the human studies results, and the Avg. NRG comparison between MARIO and large LLMs, for all 5 datasets.\n5https://huggingface.co/datasets/openbookqa,https://huggingface.co/ datasets/QuaRel\n6https://github.com/INK-USC/NumerSense/tree/main/data 7https://huggingface.co/datasets/qasc 8https://huggingface.co/t5-large,https://huggingface.co/t5-base"
        },
        {
            "heading": "G SINGLE REWARD EXPERIMENTS",
            "text": "For completeness of analysis, we present single-reward QUARK experiments, where we focus on improving just one property. Table 10 shows results on the same. We first note that in most of the cases, MARIO achieves an equivalent or better improvement as compared to single-reward QUARK. Further, we note that even if individually some properties are better when trained under single reward QUARK as compared to MARIO, MARIO is the only experiment where all the properties improve as compared to the SFT baseline. We also see that sometimes, single reward QUARK leads to improvement in other metrics as well; this could be because the metrics are positively correlated for that dataset. However, since we want to improve all metrics comprehensively, MARIO is a deterministic way to achieve the same. (Note: We don\u2019t run the experiment on NUMERSENSE DIVERSITY, since SFT already achieves the best possible value of 1.0)."
        },
        {
            "heading": "H EXTENDED COMPARISON WITH FEW-SHOT LLMS",
            "text": "In Table 11, we present the detailed performance metrics of different reference LMs as opposed to MARIO. For QUAREL, MARIO beats all reference LLMs except for GPT-3 on all four metrics. For NUMERSENSE, MARIO beats all reference LLMs except for FLAN-T5-XXL and GPT-3 on all four metrics. The results are more varied with STRATEGYQA, OPENBOOKQA and QASC; MARIO is better than the reference LLMs (apart from GPT-3) in the case of DIVERSITY for all three datasets, and in cases of varying comparisons with the reference LLMs (for example, MARIO is better at CONSISTENCY than FLAN-T5-L and LLAMA-7B for OPENBOOKQA). However, overall, we note that our model still needs to go further with respect to PLAUSIBILITY and TASK ACCURACY. We note that our method MARIO has done a significant job in bridging the gap between LMs such as the ones discussed in this section, and much smaller LMs such as T5-LARGE. We also note for TASK ACCURACY, CONSISTENCY and DIVERSITY, MARIO beats FLAN-T5-L, a model of equal size which has been trained with instruction fine-tuning for all 5 datasets (except for QASC and CONSISTENCY); and for all datasets except for STRATEGYQA, MARIO also beats PLAUSIBILITY of FLAN-T5-L.\nTable 10: QUARK experiments on improving single rewards. For each dataset, the best averaged NRG (across TASK ACCURACY, PLAUSIBILITY, DIVERSITY and CONSISTENCY) is highlighted in bold, and each best individual metric is underlined.\nMethod \u2192 Baselines Single Reward QUARK MARIO\nDataset \u2193 Metric SFT PRODUCT Acc. Plau. Div. Cons. CLASSIC ADDITIVE\nSTRATEGYQA\nAcc. 57.64 62.01 61.57 61.35 59.17 59.17 60.26 65.07 Plau. 0.33 0.35 0.36 0.36 0.36 0.36 0.38 0.39 Div. 0.95 0.92 0.92 0.93 0.96 0.95 0.95 0.97\nCons. -0.02 0.00 -0.01 0.01 -0.04 0.01 0.01 0.04 Avg. NRG 58.66 59.75 59.77 60.21 59.79 60.17 60.94 63.27\nQUAREL\nAcc. 76.99 79.53 81.88 80.62 78.99 80.62 79.89 78.99 Plau. 0.71 0.72 0.74 0.81 0.71 0.73 0.77 0.75 Div. 0.95 0.95 0.95 0.93 0.97 0.95 0.97 0.97\nCons. 0.18 0.21 0.23 0.20 0.20 0.22 0.19 0.20 Avg. NRG 75.50 76.71 78.1 78.66 77.0 77.41 78.35 77.75\nOPENBOOKQA\nAcc. 63.65 61.65 64.46 61.65 64.66 66.27 66.06 65.55 Plau. 0.53 0.52 0.54 0.53 0.51 0.54 0.55 0.55 Div. 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.98\nCons. 0.05 0.07 0.09 0.07 0.07 0.11 0.09 0.09 Avg. NRG 66.79 66.54 67.99 66.79 67.04 68.69 68.64 68.29\nNUMERSENSE\nAcc. 46.23 50.75 51.76 50.75 - 54.27 55.28 54.27 Plau. 0.60 0.60 0.63 0.63 - 0.61 0.63 0.63 Div. 1.00 1.00 0.99 1.00 - 1.00 1.00 0.99\nCons. 0.17 0.20 0.21 0.21 - 0.22 0.23 0.23 Avg. NRG 66.18 67.69 68.57 68.56 - 69.07 69.95 69.44\nQASC\nAcc. 58.64 57.88 58.21 57.88 58.1 58.75 60.15 59.61 Plau. 0.44 0.43 0.42 0.45 0.40 0.41 0.47 0.47 Div. 0.96 0.95 0.96 0.97 0.98 0.96 0.99 0.99\nCons. 0.19 0.17 0.17 0.17 0.17 0.20 0.19 0.19 Avg. NRG 64.54 63.60 63.68 64.6 63.65 63.94 66.41 66.28"
        },
        {
            "heading": "I FEW-SHOT DEMONSTRATIONS",
            "text": "We include the full few-shot demonstrations used to prompt different models for three datasets in Tables 12-14. For clarity, the rationalizations are highlighted."
        },
        {
            "heading": "J CROWDSOURCING FOR HUMAN EVALUATIONS",
            "text": "In this section, we describe the MTurk experiment setup. Each MTurk annotator is paid above minimum wage. Since the dataset we used is carefully annotated by human, we can assure there is no toxic content and our experiment setup was submitted to IRB for ethical review. We limited our Turkers to English speaking nations - United States, Canada, Australia, New Zealand and United Kingdom. To ensure the quality of evaluation, we conduct a round of qualification tasks which include a small set of evaluations. Turkers need to finish the qualification task first and get results of it, then we will\nTable 11: We compare MARIO with strong few-shot reference LMs: FLAN-T5, LLAMA and GPT3. Apart from FLAN-T5-L (which we have included to show a model of equivalent size that has been instruction finetuned), all these models are much bigger than our T5-LARGE trained with MARIO.\nMethod \u2192 FLAN-T5 LLAMA GPT-3 MARIO (0.7B)\nDataset \u2193 Metric L XL XXL 7B 65B T-D-003 CLASSIC ADDITIVE\nSTRATEGYQA\nAcc. 54.59 71.83 70.52 59.17 72.27 69.0 60.26 65.07 Plau. 0.49 0.59 0.64 0.72 0.70 0.70 0.38 0.39 Div. 0.88 0.82 0.86 0.88 0.93 0.95 0.95 0.97\nCons. -0.01 0.02 0.05 0.00 0.06 0.09 0.01 0.04 Avg. NRG 60.27 65.96 68.26 67.29 72.07 72.13 60.94 63.27\nQUAREL\nAcc. 77.36 76.99 77.54 56.70 76.27 83.33 79.89 78.99 Plau. 0.60 0.68 0.70 0.64 0.70 0.78 0.77 0.75 Div. 0.93 0.90 0.92 0.94 0.96 0.95 0.97 0.97\nCons. 0.14 0.13 0.10 0.00 0.17 0.23 0.19 0.20 Avg. NRG 71.84 72.87 73.64 66.18 75.19 79.46 78.35 77.75\nOPENBOOKQA\nAcc. 60.64 72.49 80.32 40.76 73.30 85.94 66.06 65.66 Plau. 0.49 0.59 0.67 0.66 0.73 0.74 0.55 0.55 Div. 0.87 0.84 0.93 0.95 0.97 0.99 0.99 0.98\nCons. 0.05 0.13 0.22 0.01 0.16 0.25 0.09 0.09 Avg. NRG 62.29 68.00 75.33 63.07 75.33 80.36 68.64 68.29\nNUMERSENSE\nAcc. 26.13 48.24 61.81 17.59 36.18 74.37 55.28 54.27 Plau. 0.51 0.65 0.72 0.62 0.68 0.76 0.63 0.63 Div. 0.97 0.92 0.98 0.98 0.99 1.00 1.00 0.99\nCons. 0.03 0.19 0.35 0.2 0.36 0.46 0.23 0.23 Avg. NRG 56.41 66.19 74.83 59.40 67.80 80.84 69.95 69.44\nQASC\nAcc. 61.02 70.63 74.84 24.19 75.59 80.24 60.15 59.61 Plau. 0.44 0.55 0.63 0.59 0.71 0.75 0.47 0.47 Div. 0.78 0.63 0.89 0.74 0.98 0.97 0.99 0.99\nCons. 0.23 0.32 0.37 0.10 0.31 0.38 0.19 0.19 Avg. NRG 61.13 63.66 73.84 53.05 77.52 80.31 66.41 66.28\nshow them the whole task.\nJ.0.1 WORKER SELECTION AND QUALITY CONTROL\nHere, we describe details about how workers are selected and how annotations are ensured to be clean. First, we employ multiple rounds of trials before deploying the actual task so as to get feedback from annotators whether they understand the task correctly. This includes in-house tests, tested via Amazon Turk Sandbox 9 and small batches tested on Turk. Second, we create a set of medium to hard qualification tasks for verifying preference, plausibility and consistency annotations that the annotators have to work on. These tasks are hand curated that cater certain parts of the instruction \u2013 whether the annotators are reading the rationale correctly, or whether they are able to make appropriate connections between the rationale and the question. This weeds out a lot of annotators who do not understand the task or are cheating. We also weed out workers who are too \u2018fast\u2019 (completing the task in less than 5 seconds, which is indicative of potential slacking in the task). Third, we constantly monitor task responses and feedback provided to annotators about their task. We also collect feedback from them which we adapt in new versions of the task.\nThe final MTurk instructions and template that we land upon after the qualifications is shown in Figure 8 and 9"
        },
        {
            "heading": "K LIMITATIONS",
            "text": "MARIO demonstrates promising improvements on the self-rationalization capability of small language models; we note that using MARIO on a small LM like T5-LARGE leads to considerable\n9https://requester.mturk.com/developer/sandbox\nbridging of the gap between the quality of its rationales versus the quality of rationales generated by much larger language models. However, we note that the results are still very much dependent on the initially available data (since we heavily depend upon silver standard rationales generated by GPT-3 to give our model a warm start). Our method is also dependent upon the mathematical rewards that we use: as we discuss in Section 5, this is a very new and active area of research, and we as a research community are still figuring out what properties we need, and how to efficiently implement a good mathematical metric for them."
        }
    ],
    "year": 2023
}