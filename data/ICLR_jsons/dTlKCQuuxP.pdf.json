{
    "abstractText": "We study macro motion analysis, where macro motion refers to the collection of all visually observable motions in a dynamic scene. Traditional filtering-based methods on motion analysis typically focus only on local and tiny motions, yet fail to represent large motions or 3D scenes. Recent dynamic neural representations can faithfully represent motions using correspondences, but they cannot be directly used for motion analysis. In this work, we propose Phase-based neural polynomial Gabor fields (Phase-PGF), which learns to represent scene dynamics with low-dimensional time-varying phases. We theoretically show that PhasePGF has several properties suitable for macro motion analysis. In our experiments, we collect diverse 2D and 3D dynamic scenes and show that Phase-PGF enables dynamic scene analysis and editing tasks including motion loop detection, motion factorization, motion smoothing, and motion magnification. Project page: https://chen-geng.com/phasepgf",
    "authors": [
        {
            "affiliations": [],
            "name": "GABOR FIELDS"
        },
        {
            "affiliations": [],
            "name": "Chen Geng"
        },
        {
            "affiliations": [],
            "name": "Hong-Xing Yu"
        },
        {
            "affiliations": [],
            "name": "Sida Peng"
        },
        {
            "affiliations": [],
            "name": "Xiaowei Zhou"
        },
        {
            "affiliations": [],
            "name": "Jiajun Wu"
        }
    ],
    "id": "SP:9e4e55ee4316becdc0774d76e016a2b6f24a3c09",
    "references": [
        {
            "authors": [
                "Hao Chen",
                "Bo He",
                "Hanyu Wang",
                "Yixuan Ren",
                "Ser Nam Lim",
                "Abhinav Shrivastava"
            ],
            "title": "Nerv: Neural representations for videos",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Abe Davis",
                "Katherine L Bouman",
                "Justin G Chen",
                "Michael Rubinstein",
                "Fredo Durand",
                "William T Freeman"
            ],
            "title": "Visual vibrometry: Estimating material properties from small motion in video",
            "venue": "In Proceedings of the ieee conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Abe Davis",
                "Justin G Chen",
                "Fr\u00e9do Durand"
            ],
            "title": "Image-space modal bases for plausible manipulation of objects in video",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2015
        },
        {
            "authors": [
                "Myers Abraham Davis"
            ],
            "title": "Visual vibration analysis",
            "venue": "PhD thesis, Massachusetts Institute of Technology,",
            "year": 2016
        },
        {
            "authors": [
                "Yilun Du",
                "Yinan Zhang",
                "Hong-Xing Yu",
                "Joshua B Tenenbaum",
                "Jiajun Wu"
            ],
            "title": "Neural radiance flow for 4d view synthesis and video processing",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Hans G Feichtinger",
                "Thomas Strohmer"
            ],
            "title": "Gabor analysis and algorithms: Theory and applications",
            "venue": "Springer Science & Business Media,",
            "year": 2012
        },
        {
            "authors": [
                "Brandon Y. Feng",
                "Hadi AlZayer",
                "Michael Rubinstein",
                "William T. Freeman",
                "Jia-Bin Huang"
            ],
            "title": "Visualizing subtle motions with time-varying neural fields",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2023
        },
        {
            "authors": [
                "Sara Fridovich-Keil",
                "Giacomo Meanti",
                "Frederik Rahb\u00e6k Warburg",
                "Benjamin Recht",
                "Angjoo Kanazawa"
            ],
            "title": "K-planes: Explicit radiance fields in space, time, and appearance",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Temujin Gautama",
                "MA Van Hulle"
            ],
            "title": "A phase-based approach to the estimation of the optical flow field using spatial filtering",
            "venue": "IEEE transactions on neural networks,",
            "year": 2002
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei A Efros"
            ],
            "title": "Image-to-image translation with conditional adversarial networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Justin Johnson",
                "Alexandre Alahi",
                "Li Fei-Fei"
            ],
            "title": "Perceptual losses for real-time style transfer and super-resolution",
            "venue": "In Computer Vision\u2013ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Tianye Li",
                "Mira Slavcheva",
                "Michael Zollhoefer",
                "Simon Green",
                "Christoph Lassner",
                "Changil Kim",
                "Tanner Schmidt",
                "Steven Lovegrove",
                "Michael Goesele",
                "Richard Newcombe"
            ],
            "title": "Neural 3d video synthesis from multi-view video",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Zhengqi Li",
                "Simon Niklaus",
                "Noah Snavely",
                "Oliver Wang"
            ],
            "title": "Neural scene flow fields for spacetime view synthesis of dynamic scenes",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Zhengqi Li",
                "Richard Tucker",
                "Noah Snavely",
                "Aleksander Holynski"
            ],
            "title": "Generative image dynamics",
            "venue": "arXiv preprint arXiv: 2309.07906,",
            "year": 2023
        },
        {
            "authors": [
                "Zhengqi Li",
                "Qianqian Wang",
                "Forrester Cole",
                "Richard Tucker",
                "Noah Snavely"
            ],
            "title": "Dynibar: Neural dynamic image-based rendering",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Zizhang Li",
                "Mengmeng Wang",
                "Huaijin Pi",
                "Kechun Xu",
                "Jianbiao Mei",
                "Yong Liu"
            ],
            "title": "E-nerv: Expedite neural video representation with disentangled spatial-temporal context",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "David B Lindell",
                "Dave Van Veen",
                "Jeong Joon Park",
                "Gordon Wetzstein"
            ],
            "title": "Bacon: Band-limited coordinate networks for multiscale scene representation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Francesco Locatello",
                "Dirk Weissenborn",
                "Thomas Unterthiner",
                "Aravindh Mahendran",
                "Georg Heigold",
                "Jakob Uszkoreit",
                "Alexey Dosovitskiy",
                "Thomas Kipf"
            ],
            "title": "Object-centric learning with slot attention",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Erika Lu",
                "Forrester Cole",
                "Tali Dekel",
                "Weidi Xie",
                "Andrew Zisserman",
                "David Salesin",
                "William T Freeman",
                "Michael Rubinstein"
            ],
            "title": "Layered neural rendering for retiming people in video",
            "year": 2009
        },
        {
            "authors": [
                "Long Mai",
                "Feng Liu"
            ],
            "title": "Motion-adjustable neural implicit video representation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Julien NP Martel",
                "David B Lindell",
                "Connor Z Lin",
                "Eric R Chan",
                "Marco Monteiro",
                "Gordon Wetzstein"
            ],
            "title": "Acorn: Adaptive coordinate networks for neural scene representation",
            "venue": "arXiv preprint arXiv:2105.02788,",
            "year": 2021
        },
        {
            "authors": [
                "Simone Meyer",
                "Abdelaziz Djelouah",
                "Brian McWilliams",
                "Alexander Sorkine-Hornung",
                "Markus Gross",
                "Christopher Schroers"
            ],
            "title": "Phasenet for video frame interpolation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P Srinivasan",
                "Matthew Tancik",
                "Jonathan T Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "Communications of the ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Michael Niemeyer",
                "Andreas Geiger"
            ],
            "title": "Giraffe: Representing scenes as compositional generative neural feature fields",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Keunhong Park",
                "Utkarsh Sinha",
                "Jonathan T Barron",
                "Sofien Bouaziz",
                "Dan B Goldman",
                "Steven M Seitz",
                "Ricardo Martin-Brualla"
            ],
            "title": "Nerfies: Deformable neural radiance fields",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5865\u20135874,",
            "year": 2021
        },
        {
            "authors": [
                "Keunhong Park",
                "Utkarsh Sinha",
                "Peter Hedman",
                "Jonathan T Barron",
                "Sofien Bouaziz",
                "Dan B Goldman",
                "Ricardo Martin-Brualla",
                "Steven M Seitz"
            ],
            "title": "Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields",
            "venue": "arXiv preprint arXiv:2106.13228,",
            "year": 2021
        },
        {
            "authors": [
                "Sida Peng",
                "Junting Dong",
                "Qianqian Wang",
                "Shangzhan Zhang",
                "Qing Shuai",
                "Xiaowei Zhou",
                "Hujun Bao"
            ],
            "title": "Animatable neural radiance fields for modeling dynamic human bodies",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Sida Peng",
                "Chen Geng",
                "Yuanqing Zhang",
                "Yinghao Xu",
                "Qianqian Wang",
                "Qing Shuai",
                "Xiaowei Zhou",
                "Hujun Bao"
            ],
            "title": "Implicit neural representations with structured latent codes for human body modeling",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Albert Pumarola",
                "Enric Corona",
                "Gerard Pons-Moll",
                "Francesc Moreno-Noguer"
            ],
            "title": "D-nerf: Neural radiance fields for dynamic scenes",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Rho",
                "Junwoo Cho",
                "Jong Hwan Ko",
                "Eunbyung Park"
            ],
            "title": "Neural residual flow fields for efficient video representations",
            "venue": "In Proceedings of the Asian Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "MICCAI 2015: 18th International Conference,",
            "year": 2015
        },
        {
            "authors": [
                "Ruizhi Shao",
                "Zerong Zheng",
                "Hanzhang Tu",
                "Boning Liu",
                "Hongwen Zhang",
                "Yebin Liu"
            ],
            "title": "Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Qing Shuai",
                "Chen Geng",
                "Qi Fang",
                "Sida Peng",
                "Wenhao Shen",
                "Xiaowei Zhou",
                "Hujun Bao"
            ],
            "title": "Novel view synthesis of human interactions from sparse multi-view videos",
            "venue": "In ACM SIGGRAPH 2022 Conference Proceedings,",
            "year": 2022
        },
        {
            "authors": [
                "Vincent Sitzmann",
                "Julien Martel",
                "Alexander Bergman",
                "David Lindell",
                "Gordon Wetzstein"
            ],
            "title": "Implicit neural representations with periodic activation functions",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Neal Wadhwa",
                "Michael Rubinstein",
                "Fr\u00e9do Durand",
                "William T Freeman"
            ],
            "title": "Phase-based video motion processing",
            "venue": "ACM Transactions on Graphics (ToG),",
            "year": 2013
        },
        {
            "authors": [
                "Chung-Yi Weng",
                "Brian Curless",
                "Pratul P Srinivasan",
                "Jonathan T Barron",
                "Ira KemelmacherShlizerman"
            ],
            "title": "Humannerf: Free-viewpoint rendering of moving people from monocular video",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Hao-Yu Wu",
                "Michael Rubinstein",
                "Eugene Shih",
                "John Guttag",
                "Fr\u00e9do Durand",
                "William Freeman"
            ],
            "title": "Eulerian video magnification for revealing subtle changes in the world",
            "venue": "ACM transactions on graphics (TOG),",
            "year": 2012
        },
        {
            "authors": [
                "Guandao Yang",
                "Sagie Benaim",
                "Varun Jampani",
                "Kyle Genova",
                "Jonathan Barron",
                "Thomas Funkhouser",
                "Bharath Hariharan",
                "Serge Belongie"
            ],
            "title": "Polynomial neural fields for subband decomposition and manipulation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jiakai Zhang",
                "Xinhang Liu",
                "Xinyi Ye",
                "Fuqiang Zhao",
                "Yanshun Zhang",
                "Minye Wu",
                "Yingliang Zhang",
                "Lan Xu",
                "Jingyi Yu"
            ],
            "title": "Editable free-viewpoint video using a layered neural representation",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2021
        },
        {
            "authors": [
                "Yunfan Zhang",
                "Ties van Rozendaal",
                "Johann Brehmer",
                "Markus Nagel",
                "Taco Cohen"
            ],
            "title": "Implicit neural video compression",
            "venue": "arXiv preprint arXiv:2112.11312,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Zheng",
                "Adam W. Harley",
                "Bokui Shen",
                "Gordon Wetzstein",
                "Leonidas J. Guibas"
            ],
            "title": "Pointodyssey: A large-scale synthetic dataset for long-term point tracking",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Yang"
            ],
            "title": "Phase Generator. The key aspects of an ideal phase generator are expressiveness and controllability. To achieve these two goals, we use a polynomial neural field (Yang et al., 2022) conditioned on t to serve as the backbone",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The physical world is dynamic and full of different motions: from waving trees to walking people, motions are present in different spatial regions, have diverse magnitudes, and manifest various frequency characteristics. We refer to the collection of all visually observable motions in a scene as the macro motion. Factorizing and analyzing macro motions is essential in understanding and interpreting the dynamic world. We argue that analyzing macro motions requires a dynamic scene representation that bear three key properties: It should be able to represent macro motions faithfully; it should enable decomposing macro motions in both spatial domain and frequency domain; and it should allow representing 3D scenes.\nWhile modeling motions have been a constant topic of interest in computer vision, graphics, and machine learning, existing methods do not meet the three properties simultaneously. Traditional motion processing and magnification methods allow analyzing motions by filtering input videos in frequency domain and editing the frequency components\u2019 magnitudes (Wu et al., 2012) or phases (Wadhwa et al., 2013). However, they only focus on micro motions (i.e., local and tiny motions) and they do not handle 3D scenes. Recent methods (Li et al., 2021; 2022a; Fridovich-Keil et al., 2023) exploit neural radiance fields (Mildenhall et al., 2021) to represent dynamic 3D scenes using deformation fields (Pumarola et al., 2021) or flow fields (Du et al., 2021). For example, D-NeRF (Pumarola et al., 2021) defines a template neural radiance field in the canonical space and builds correspondences between the observation space and the canonical space using a displacement field. Although 3D correspondences explicitly represent scene motions, they cannot be directly used for the motion analysis due to the lack of modeling underlying motion components.\nIn this work, we propose the Phase-based neural polynomial Gabor fields (Phase-PGF) that simultaneously meets all the three key properties for macro motion analysis. In particular, Phase-PGF represents a dynamic scene as a composition of wavelet-based neural fields, and the wavelet basis are modulated by a set of temporally-varying phases. Therefore, macro motions in dynamic scenes are captured by the phases. We show that under appropriate assumptions, Phase-PGF has theoretical properties that allow various macro motion analysis tasks, such as motion separation, motion\n\u2217Equal contribution.\nsmoothing, and motion intensity adjustment. We then instantiate Phase-PGF with a novel neural architecture and a training scheme for higher-quality dynamic scene representation and editing.\nIn our experiments, we collect examples of both 2D videos and 3D dynamic scenes (represented by multi-view videos). We show that Phase-PGF allows macro motion analysis on both 2D and 3D dynamic scenes, allowing several macro motion analysis applications including motion loop detection, motion separation, motion smoothing, and motion magnification.\nIn summary, our contributions are threefold: Firstly, we propose and formulate the problem of macro motion analysis. Secondly, we introduce Phase-based neural polynomial Gabor fields (Phase-PGF) for macro motion analysis. Lastly, our experiments show that Phase-PGF allows a series of dynamic scene editing tasks on both 2D and 3D scenes."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Dynamic neural representations. Using neural representations (Mildenhall et al., 2021; Sitzmann et al., 2020) to represent dynamic signals has emerged as a popular research topic in recent years. It has been widely used in representing 4D (3D space and 1D time) dynamic volumetric videos with advantages including high-fidelity rendering and low storage requirements. Some methods build explicit correspondences between frames by extracting dense flow or deformation (Pumarola et al., 2021; Du et al., 2021; Li et al., 2021; 2023b; Park et al., 2021a;b). However, such representation for motion is dense and is hard to analyze. Other approaches use inductive bias on scenes to assist the reconstruction of dynamic scenes(Peng et al., 2021; 2023; Weng et al., 2022), yet cannot be applied to general dynamic scenes. Recently some approaches propose to use hybrid representations (Fridovich-Keil et al., 2023; Shao et al., 2023) to implicitly represent dynamic 3D scenes, lacking interpretability on motion information. Beyond 3D dynamic scenes, there are also works focusing on using neural representations for 2D videos(Li et al., 2022b; Chen et al., 2021; Zhang et al., 2021b; Rho et al., 2022). However, most of them use implicit motion representation which cannot be easily analyzed or edited.\nMotion analysis and editing. Motion analysis and editing in dynamic content is an important task in computer vision. Phase information is widely used in the field of motion analysis (Gautama & Van Hulle, 2002; Meyer et al., 2018; Mai & Liu, 2022). For motion editing, early work solves this problem with a Lagrangian perspective(Liu et al., 2005), yet renders artifacts with large motions. Wu et al. (2012) proposes to understand this task with an Eulerian perspective and Wadhwa et al. (2013) further proposes a phase-based method to perform motion magnification. Some other works (Davis et al., 2015b;a; Davis, 2016) use physical modal analysis to manipulate local motions. Despite being successful for tiny motions, they cannot make good editing on large and macro motions studied in this paper.\nInterpretable and editable neural scene representations. With the development of neural representations (Mildenhall et al., 2021; Sitzmann et al., 2020), the interpretability of such methods has become an important research area. Several works on this topic focus on the frequency domain or the coarse-to-fine decomposition of the input signal (Lindell et al., 2022; Yang et al., 2022; Martel et al., 2021). Other works target at spatial decomposition of the input content (Shuai et al., 2022; Zhang et al., 2021a; Lu et al., 2020), allowing modifying the scene contents at an instance level. These prior works primarily solve static scenes, yet our work focuses on dynamic content. Moreover, our work focuses on the interpretability from the macro motion perspective of the representation, differing from prior works.\nConcurrent works. Recently there have been some concurrent works on tiny motion analysis and editing. Feng et al. (2023) proposes a method to magnify local and tiny 3D motion, yet their method has challenges in manipulating macro motion discussed in this work. Li et al. (2023a) gives an approach to manipulate local motion, but their focus is also tiny motions."
        },
        {
            "heading": "3 APPROACH",
            "text": "In this section, we first formulate the problem of macro motion analysis. Then, we show an abstract formulation and theoretical properties of the proposed Phase-based neural polynomial Gabor\nfields (Phase-PGF). Next, we introduce how we instantiate the formulation with neural networks to represent 2-D and 3-D dynamic signals. Finally, we present a training scheme for our approach."
        },
        {
            "heading": "3.1 MACRO MOTION ANALYSIS",
            "text": "We define the macro motion as follows:\nDefinition 3.1 (Macro Motion). We assume a dynamic scene can be decomposed into k components {s1, s2, \u00b7 \u00b7 \u00b7 , sk}, each with rigid motion yi(t). The dynamic scene is represented using the following implicit function:\ns(x, t) = k\u2211 i si(x+ yi(t)) = s1(x+ y1(t)) + s2(x+ y2(t)) + \u00b7 \u00b7 \u00b7+ sk(x+ yk(t)), (1)\nwhere we assume si(x) is differentiable and non-constant over its domain. The macro motion is defined as Y = {y1(t),y2(t), \u00b7 \u00b7 \u00b7 ,yk(t)}.\nNote that this definition does not imply that a cognitive object is only represented using only one component si. Rather, an object can be represented using several components with several rigid motions, making it possible to model complex scenes without loss of generality.\nWe are interested in analyzing macro motion. Specifically, we aim to find a low-dimensional representation that represents Y and can be factorized in both the spatial domain and frequency domain."
        },
        {
            "heading": "3.2 PHASE-PGF: PHASE-BASED NEURAL POLYNOMIAL GABOR FIELDS",
            "text": "We propose Phase-based neural polynomial Gabor fields (Phase-PGF). In particular, to inherently allow factorization of the represented signals in both spatial and frequency domains, we adopt polynomial neural fields (Yang et al., 2022) with Gabor basis functions (Feichtinger & Strohmer, 2012) as the basic building block. To represent motions in a low-dimensional space, we leverage the phases of the Gabor basis functions. The formal definition is given by:\nDefinition 3.2 (Phase-PGF). Let B = {g1, g2, \u00b7 \u00b7 \u00b7 , gm} to be a shift-orthogonal1 set of Gabor functions gi(x; \u03b3i, \u00b5i, \u03c9i, \u03d5i) = exp(\u2212\u03b3i2 ||x\u2212\u00b5i||2) sin(\u03c9ix+\u03d5i). Let H = {h1(t), h2(t), \u00b7 \u00b7 \u00b7 , hm(t)} be a set of phase functions where hi(t) : R1 \u2192 R1. A Phase-based neural polynomial Gabor fields (Phase-PGF) is a neural network f(x, t) = pL \u25e6pL\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6p1 \u25e6\u03a8(x, t), where \u2200i, pi are finite degree multivariate polynomials, and \u03a8(x, t) = {g1(x+h1(t)), g2(x+h2(t)), \u00b7 \u00b7 \u00b7 , gm(x+hm(t))} is m-dimensional feature encoding using basis B and phase functions H.\nIntuitively, the time-varying phase information allows our model to represent the macro motions using a low-dimensional vector. Therefore, the macro motions can be analyzed and manipulated by controlling the phase information.\nWe then theoretically discover the properties of the proposed representation. First, we show that the phases can be used to analyze the periodic components of the macro motion in the scene.\nTheorem 3.1 (Periodicity Correlation). For a Phase-PGF with phase functions H already trained to represent a dynamic scene s(x, t), if there exists T > 0 such that some motion yi(t+T ) = yi(t), then \u2203hj \u2208 H, hj(t+ T ) = hj(t).\nWe include the proofs of all theorems in appendix A. This theorem indicates that periodic motion components can correspond to components of the phase information.\nIf a scene has complicated composed motions, e.g., a scene with multiple dynamic objects, PhasePGF allows unsupervisedly factorizing different motion components into different components in the phases. Specifically, we have the following property:\nTheorem 3.2 (Motion Separation). Assume that a scene has several objects with motion yi(t) associated with each object i. For each component, the implicit functions representing object i lies in\n1\u27e8gi(x+ \u03b1), gj(x+ \u03b2)\u27e9 = \u03b4ij , where \u03b4 is the Kronecker delta function.\nthe span of Bi. Then, a Phase-PGF f(x, t) can be decomposed into several Phase-PGFs:\nf(x, t) = N\u2211 i fi(x, t), (2)\nwhere fi(x, t) is a Phase-PGF with basis Bi.\nThis theorem indicates that it is possible to disentangle the scene macro motion based on the phases. For example, if the macro motion can be separated into high-frequency components and low-frequency components, or if the macro motion contains motion components present at different spatial regions, then by filtering out the desired phase components, we can manipulate the scene motion in a factorized manner.\nWe have now shown that the extracted phase information from the Phase-PGF can be used to observe the properties of the macro motion. We then show that this representation can be edited to manipulate the scene motion.\nTheorem 3.3 (Motion Smoothing). Assume that the a dynamic scene is represented by a trained Phase-PGF with phases H. If we apply a low-pass filter on some hj that corresponds to some motion yi, then the motion would be smoother, i.e., | ddtyi(t)| is attenuated.\nIntuitively, motion smoothing corresponds to attenuating high-frequency motion components. Besides motion filtering, it is also possible to perform the manipulation in the dimension of motion intensity/magnitude:\nTheorem 3.4 (Motion Intensity Adjustment). Assume that a scene component motion takes the form yi(t) = eit and the scene is represented using a Phase-PGF with a phase hj(t) corresponds to yi(t). If we multiply the phase by a coefficient A, then the scene motion would be manipulated to yi(t) = Ae it.\nWe include the proofs of all the theorems in appendix A."
        },
        {
            "heading": "3.3 INSTANTIATION OF PHASE-PGF",
            "text": "In the previous section, we have introduced the abstract definition of Phase-PGF and its properties. In this section, we further illustrate how we empirically instantiate Phase-PGF to represent 2D and 3D dynamic scenes. We show an overview in Figure 1, where we have a phase generator (on the left of Figure 5) that generates the m time-varying phases H = {h1(t), h2(t), \u00b7 \u00b7 \u00b7 , hm(t)} for modulating the Gabor basis B = {g1, g2, \u00b7 \u00b7 \u00b7 , gm}. The Phase-PGF f(x, t) = pL \u25e6 pL\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 p1 \u25e6 \u03a8(x, t) generates a latent feature map and a neural latent decoder (detailed below) renders an image from the feature map.\nPhase Generator for H. Since the phases are central to our model, the phase generator architecture should ideally be expressive and structured. To this end, we instantiate each hi with a Polynomial Neural Field (PNF) (Yang et al., 2022), which allows decomposing the phases into frequency subbands. The frequency decomposition of phases allows easy manipulation over the phases, thus over the macro motions.\nNeural Latent Decoder. While Phase-PGF takes the form of f(x, t) = pL \u25e6pL\u22121 \u25e6\u00b7 \u00b7 \u00b7\u25e6p1 \u25e6\u03a8(x, t) where p denotes a polynomial, we empirically find that it consumes a large amount of GPU memory if we directly use f(x, t) to represent raw pixels in complex and high-resolution scenes. Therefore, we propose to couple Phase-PGF with a neural latent decoder. That is, Phase-PGF represents a lowresolution latent feature field of the dynamic scenes, and the neural latent decoder decodes the latent features into high-resolution results, similar to the feature fields in prior works (Niemeyer & Geiger, 2021).\nMore specifically, for representing 2D videos, we first use Phase-PGF f(x, t) to render a 2D feature map m. Then, we use a 2D CNN decoder to render the final image I\u0302 = D(m). For 3D dynamic scenes, to render a frame from a viewpoint, we adopt the volume rendering (Mildenhall et al., 2021) formation. That is, we do ray marching to get a set of sampled points on the rays of the frame, and then we use volume rendering to aggregate the sampled features to form the feature map. We also use a 2D CNN to decode the feature map into a final RGB image.\nThe neural latent decoder D is designed in a pyramid manner. We first decode the feature into a low-resolution RGB image IL = DL(m). Then, we use another decoder DH to obtain the final high-resolution output I\u0302 = DH(IL).\nWe refer the readers to appendix C.1 and appendix C.2 for more details on the neural network implementation and the phase space."
        },
        {
            "heading": "3.4 TRAINING PHASE-PGF TO REPRESENT DYNAMIC SCENES",
            "text": "Multi-stage Training. We train Phase-PGF in a multi-stage manner. In the first stage, we supervise both the learning of Phase-PGF F and decoder DL with reconstruction loss L1 = ||I\u0302L \u2212 IL||2, where IL is the ground-truth low-resolution image.\nIn the second stage, we further improve the high-resolution detail of the rendered image with patchbased sampling and perceptual loss (Johnson et al., 2016), during each training step, we randomly sample a patch from the high-resolution image, and the loss L2 is defined as L2 = ||I\u0302 \u2212 I||2 + ||mvgg(I\u0302)\u2212mvgg(I)||2, where I is the patch from the ground-truth high-resolution image and mvgg is a feature map extracted by a pre-trained VGG network (Johnson et al., 2016).\nAdversarial Training. The multi-stage training enables high-fidelity reconstruction of the input scenes. However, extrapolating macro motions (e.g., for motion magnification) requires further regularization on the neural latent decoder. Therefore, we propose to further use adversarial training after the two stages. In particular, given the generated phases H, we perform simple magnification to it and apply these augmented phases to the decoders. We want the decoder to generate plausible images even when the input phases are extrapolated. Specifically, for a given phase sequence h(t) \u2208 H, the augmented h\u2032(t|\u03bb, bl, bh) is defined as follows:\nh\u2032(t|\u03bb, bl, bh) = h(t) + (\u03bb\u2212 1) \u00b7 y(t|bl, bh), (3) y(t|bl, bh) = F\u22121(T (F(h(t)), bl, bh)), (4)\nwhere \u03bb is intensity manipulation coefficient, bl and bh are subband limits for specific component of the signal, F and F\u22121 are fourier transform and its inversion, and T (f) is a band-limit filter defined as follows:\nT (f) = { 1, if f1 \u2264 f \u2264 f2, 0, otherwise.\n(5)\nWe additionally construct a discriminator D to judge whether the image decoded from the extrapolated phase sequence is similar to the input sequence. The adversarial loss is defined as follows:\nLadv = Ex,t[logD(f(x, t)] + Ex,t,\u03bb[log(1\u2212D(Tf,\u03bb(x, t)], (6)\nwhere Tf,\u03bb denotes a functional that augment the phase generated by f with a motion intensity adjustment coefficient \u03bb, as defined above. For more details on training, please refer to appendix C.4."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "To evaluate our approach on the task of macro motion analysis, we collect several examples including both 2D and 3D dynamic scenes. We show that our Phase-PGF learns interpretable motion representations to allow macro motion analysis, which is demonstrated by motion loop detection and motion separation. Afterward we show that Phase-PGF allows macro motion editing tasks including motion intensity adjustment and motion smoothing."
        },
        {
            "heading": "4.1 EXAMPLES",
            "text": "We collect several dynamic scene examples including both 2D and 3D scenes.\nSynthetic 2D videos. We render several different videos with varying motion complexities with ground truth motion to quantitatively evaluate the proposed method. Please refer to appendix D.1.\nReal 2D videos. We also collect several real videos from the Internet and from our own capture to show the generalizability and applicability of the proposed Phase-PGF. We take some videos from Mai & Liu (2022) and other Internet sources to form a dataset of real 2D videos.\n3D Dynamic Scene. Our Phase-PGF can also be used to model 3D dynamic scenes with neural rendering. To demonstrate this, we render a synthetic 3D dynamic scenes with two different balls bouncing on a table top."
        },
        {
            "heading": "4.2 MACRO MOTION ANALYSIS: LOOP DETECTION AND MOTION SEPARATION",
            "text": "For macro motion analysis, a preliminary requirement is that the motion representation is interpretable, i.e., the components of the factorized motion representation should be able to establish correspondences to scene motion components. To do this, we compare the phase generated by our Phase-PGF against a previous method Phase-NIVR (Mai & Liu, 2022). Phase-NIVR also aims to generate phases for input motion, yet it does not allow representing 3D scenes. We show a comparison of phases by both methods in Figure 2 (a), and show keyframes corresponding to our generated phases in Figure 2 (b).\nFrom Figure 2, we observe that our Phase-PGF generates a phase sequence that aligns with the motion more coherently. A possible reason is that Phase-NIVR uses the SIREN network (Sitzmann et al., 2020) as the scene representation which does not support decomposing different components\nof the dynamic scene. Please check out the supplementary website at https://chen-geng. com/phasepgf for video results.\nTo quantitatively evaluate the motion representuation interpretability, we conduct a human preference study. We show several videos and the generated phases by both methods, and ask the participants to pick one that they believe better explains the motions in the videos. We use a researchoriented cloud source platform2. We show the human study results in Table 1. From Table 1, we see that the phases generated by Phase-PGF are believed to better explain the motions.\nWe additionally evaluate the normalized cross-correlation of phases generated by our method and the baseline on datasets with ground-truth motion trajectory. The results are shown in Table 2. Please refer to appendix D.2 for more details on this.\nTo discover more complex motion patterns, we synthesize and collect several videos with varying motion complexities and evaluate them in appendix D.1.\nWe further show two macro motion analysis applications: motion loop detection and motion separation.\nMotion loop detection. In Figure 3, we show an example of motion loop detection on the input video. To do this, we identify the periodicity of the input video, and then we perform phase fusing (Mai & Liu, 2022) to generate an infinitely looping video.\nMotion separation. In Figure 4, we show motion separation on a synthetic 3D scene, where two balls bounce at different speeds. Phase-PGF is able to decompose the macro motion using a frequency filter. Specifically, we use band-limit filters on the generated phase sequence, and then we isolate the motion of interest (the motion of the blue ball) according to the frequency. In this way, we can manipulate (mollify) the motion of the blue ball without affecting the motion of the red ball."
        },
        {
            "heading": "4.3 MACRO MOTION EDITING: INTENSITY ADJUSTMENT AND SMOOTHING",
            "text": "Besides analysis, our Phase-PGF also allows editing macro motions. Specifically, we consider two motion editing tasks: motion intensity adjustment and motion smoothing. Please refer to appendix C.3 for details on the editing operations.\n2This human preference study form can be found at the appendix.\nMotion Intensity Adjustment. Phase-PGF allows modifying the intensity or magnitude of the macro motion in the represented dynamic scenes. Following the method introduced in Sec. 3.4, we show motion magnification results in Figure 5. We compare our method with a traditional motion magnification method Wadhwa et al. (2013) and Phase-NIVR. From Figure 5 we observe that Phase-PGF allows magnification with much fewer artifacts. In comparison, Phase-NIVR generates a temporally flickering video with ghosting artifacts, and Wadhwa et al. (2013) failed to magnify the macro motion as it focuses on tiny motions. Our observation is further consolidated by a human preference study, where we show the results in Table 3.\nWe also evaluate the Fre\u0301chet Inception Score for manipulated videos generated from different methods. The result is shown in Table 3. It can be seen that our method surpasses other baselines greatly in terms of visual quality. Please refer to appendix D.2 for more details.\nWe perform ablation studies to prove the effectiveness of the introduced components. Please refer to appendix D.3 for detailed discussion.\nMotion Smoothing. We show that Phase-PGF also allows motion smoothing. In Figure 6, we show that by removing high-frequency bands of the generated phase sequence, we can perform motion smoothing on the macro motion. From Figure 6, we observe that the high-frequency flickering of the paddle has been removed, leading to a smoother motion."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we propose and formulate the problem of macro motion analysis. We propose Phasebased neural polynomial Gabor fields (Phase-PGF) that represents motions in dynamic scenes with generated phase sequences. We show that Phase-PGF allows multiple macro motion analysis and editing tasks, including loop detection, motion separation, motion magnification, and motion smoothing.\nLimitations. One limitation is that Phase-PGF shows slight artifacts in boldly magnifying large motions. Another limitation is that Phase-PGF currently does not scale well to complex large-scale 3D dynamic scenes due to computational efficiency (we need more Gabor basis in larger scenes). This might be addressed by spatially adaptive Gabor basis. We also discuss other failure cases at appendix D.7."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work is in part supported by the Stanford Institute for Human-Centered AI (HAI) and Samsung."
        },
        {
            "heading": "A DEFINITIONS AND PROOFS",
            "text": "In this section, we provide proofs of the theorems in the main paper. Our Phase-PGF is based on the polynomial neural fields (Yang et al., 2022). In the following proofs, we will use some of the properties of the polynomial neural fields. We refer the reader to Yang et al. (2022) for more context.\nProof of theorem 3.1. Let\u2019s assume that the scene at t = 0 is represented with the implicit function s(x), and the active content is doing rigid motion y(t). Then the dynamic scene to be represented can be denoted as s(x+ y(t)), where both s(x) and y(t) are unknown.\nNow the dynamic content is fit using a Phase-PGF f(x, t), which gives:\nf(x, t) = s(x+ y(t)), \u2200x, t. (7)\nIf we have y(t+ T ) = y(t) holds for any t, then:\nf(x, t+ T ) = s(x+ y(t+ T )) (8) = s(x+ y(t)) (9) = f(x, t). (10)\nConsider the property of Polynomial functions(Yang et al., 2022), we have:\nf(x, t) = m\u2211 i \u03c9igi(x+ hi(t)), (11)\nthen we can have:\nm\u2211 i \u03c9i(gi(x+ hi(t+ T ))\u2212 gi(x+ hi(t))) = 0, \u2200x, t. (12)\nConsider doing inner-product with gj , we have:\nm\u2211 i \u03c9i\u27e8(gi(x+ hi(t+ T ))\u2212 gi(x+ hi(t))), gj(x)\u27e9 (13)\n= m\u2211 i \u03c9i\u27e8gi(x+ hi(t+ T )), gj(x)\u27e9 \u2212 \u27e8gi(x+ hi(t)), gj(x)\u27e9 (14)\n= \u03c9j\u27e8gj(x+ hj(t+ T ))\u2212 gj(x+ hj(t)), gj(x)\u27e9 (15) = 0 (16)\nTherefore, we have gj(x+ hj(t+ T )) = gj(x+ hj(t)), which implies hj(t+ T ) = hj(t).\nProof of theorem 3.2. According to the assumption, the dynamic scene can be represented using the following implicit function:\ng(x, t) = \u2211 i si(x+ yi(t)), (17)\nwhere si is in the span of Bi. Also, we have:\nk\u2211 i si(x+ yi(t)) = f(x, t). (18)\nConsidering the property of polynomial fields(Yang et al., 2022), the f(x, t) can be written as:\nf(x, t) = m\u2211 i \u03c9igi(x+ hi(t)), (19)\nThen we have:\nk\u2211 i si(x+ yi(t)) = m\u2211 i \u03c9igi(x+ hi(t)) (20)\n= k\u2211 i \u2211 gj\u2208Bi \u03c9jgj(x+ hi(t)) (21)\nThus we can get:\nsi(x+ yi(t)) = \u2211\ngj\u2208Bi\n\u03c9jgj(x+ hi(t)), \u2200i (22)\nLet\u2019s denote fi(x, t) = \u2211\ngj\u2208Bi \u03c9jgj(x, t), then we have got a decomposition of f(x, t) =\u2211 fi(x, t), where each fi is supported by basis Bi.\nProof of theorem 3.2. According to the assumptions, if the rigid motion is represented using y(t), then:\ns(x+ y(t)) = f(x, t), (23) = \u2211 \u03c9igi(x+ hi(t)), \u2200x, t. (24)\nWe have used the property of the polynomial fields(Yang et al., 2022) in the derivation above. We first perform Fourier decomposition of implicit function s(x) and the basis gi(x), so that we can get:\ns(x) = \u222b \u221e G(f)ei2\u03c0fxdf (25)\ngi(x) = \u222b \u221e g\u2032i(f)e i2\u03c0fxdf (26)\nBy modulating the equation above, we can obtain:\ns(x+ y(t)) = \u222b \u221e G(f)ei2\u03c0f(x+y(t))df (27)\n= \u2211 \u03c9igi(x+ hi(t)), (28)\n= \u2211\n\u03c9i \u222b \u221e g\u0302i(f)e i2\u03c0f(x+hi(t))df, \u2200x, t (29)\nWe perform the Fourier transform of the equation above.\nG(f)ei2\u03c0fy(t) = \u2211 \u03c9ig\u0302i(f)e i2\u03c0fhi(t), \u2200f. (30)\nThen we have:\ny(t) = 1\n2\u03c0if ln[\n1\nG(f)\n\u2211 \u03c9ig\u0302i(f)e i2\u03c0fhi(t)]. (31)\nBy performing low-pass filter on hi(t), we can obtain a new phase hL(t) with attenuated highfrequency information, which has the property:\n| d dt hL(t)| \u2264 | d dt hi(t)| (32)\nWe consider the derivative of the corresponding yL(t) to evaluate the change in its smoothness.\n| d dt yL(t)| = | d dt 1 2\u03c0if ln[ 1 G(f)\n\u2211 \u03c9ig\u0302i(f)e i2\u03c0fhL(t)]| (33)\n= | 1 2\u03c0if d dt ln[ 1 G(f)\n\u2211 \u03c9ig\u0302i(f)e i2\u03c0fhL(t)]| (34)\n= | G(f) 2\u03c0if \u00b7 \u2211 \u03c9ig\u0302i(f)ei2\u03c0fhL(t) d dt [ 1 G(f)\n\u2211 \u03c9ig\u0302i(f)e i2\u03c0fhL(t)]| (35)\n= | 1 2\u03c0if \u00b7 \u2211 \u03c9ig\u0302i(f)ei2\u03c0fhL(t) d dt\n\u2211 \u03c9ig\u0302i(f)e i2\u03c0fhL(t)| (36)\n= | 1 2\u03c0if \u00b7 \u2211 \u03c9ig\u0302i(f)ei2\u03c0fhL(t)\n\u2211 \u03c9ig\u0302i(f) d\ndt ei2\u03c0fhL(t)| (37)\n= | 1 2\u03c0if \u00b7 \u2211 \u03c9ig\u0302i(f)ei2\u03c0fhL(t)\n\u2211 \u03c9ig\u0302i(f)e i2\u03c0fhL(t)i2\u03c0f d\ndt hL(t)| (38)\n= | \u2211 \u03c9ig\u0302i(f) d dthL(t)|\n| \u2211 \u03c9ig\u0302i(f)| (39)\n\u2264 | \u2211 \u03c9ig\u0302i(f) d dthi(t)|\n| \u2211 \u03c9ig\u0302i(f)| (40)\n= | d dt y(t)|. (41)\nTherefore, we can have that the yL(t) is smoother than y(t), which finishes the proof.\nProof of theorem 3.4. According to the assumption, we have:\ns(x+ eit) = f(x, t) (42) = \u2211 \u03c9igi(x+ hi(t)) (43)\nSince we assume s(x) is in the span of B, it can decomposed into:\ns(x) = \u2211 \u03b3igi(x) (44)\nThen:\n\u2211 \u03b3igi(x+ e it) = \u2211 \u03c9igi(x+ hi(t)) (45)\nAccording to the shift-orthogonality of B,\nhi(t) = e it (46)\nIf we perform motion intensity adjustment to hi(t) with magnitude A, then we can get modified phase h\u2032(t) = Ahi(t), so that the scene motion can be modified to:\nh\u2032(t) = Ahi(t) (47)\n= Aeit. (48)"
        },
        {
            "heading": "B HUMAN PREFERENCE STUDY",
            "text": "We conduct user studies using the Prolific platform.\nFor the user study shown in Table 1, we have collected 102 effective responses. We designed our form using Google Forms. In the form, we asked the user to compare 5 pairs of videos with plots trying to explain the motion inside the video. For each pair, we asked the user the following prompt: In which video (left or right), does the bottom plot better explain the motion in the top video? The user will choose between two answers and we collect the data and make statistical analysis. The video pairs can be found at our supplementary website: https://chen-geng.com/phasepgf.\nFor the user study shown in Table 3, we have collected 110 effective responses. In the study, we first asked the users to watch three video pairs from three methods (each pair consists of a top video and a bottom video). In each pair, the bottom video is trying to magnify the motion in the top video. Then for each pair, we asked them two questions. The first question is: The bottom video tries to magnify the motion of the white object in the top video. Do you think the bottom video successfully magnifies the motion? Please rate on a scale of 1 to 5, where 1 means \u201dNot at all\u201d and 5 means \u201dVery much so\u201d. The answer to this question was used to calculate the Motion Magnification score. The second question is: You are viewing the same video pair as before. Please evaluate the visual quality of the bottom video. On a scale of 1 to 5, rate the quality of the bottom video. The answer to this question was used to calculate the Visual Quality Score. The videos can be found at our supplementary website: https://chen-geng.com/phasepgf."
        },
        {
            "heading": "C METHOD DETAILS",
            "text": "In this section, we provide more details on the proposed method. The code will be released upon publication."
        },
        {
            "heading": "C.1 NETWORK ARCHITECTURE",
            "text": "We first elaborate on more details on the network architecture. In the following, we discuss specific design choices related to different modules in the proposed framework.\nPhase Generator. The key aspects of an ideal phase generator are expressiveness and controllability. To achieve these two goals, we use a polynomial neural field (Yang et al., 2022) conditioned on t to serve as the backbone. Specifically, we follow the architecture described in Yang et al. (2022), where the basis is defined using Fourier waves.\nEach polynomial neural field takes normalized timestamp t as input and outputs a one-dimension scalar h(t) as the phase in the timestamp t. Following Yang et al. (2022), h(t) is defined as below:"
        },
        {
            "heading": "Hyper-Parameter Name Value",
            "text": "h(t) = \u2211 j Fj(t), (49)\nwhere Fj(t) is a sub-PNF for one subband. Fj(t) is further implemented using the following factorization:\nFj(t) = tanh( n\u2211 k=1 Gj(t, bk, bk)WjkZj,k(t)), (50)\nZj,1(t) = Gj(t, 0,\u22061), (51) Zj,k(t) = Gj(t, 0,\u2206k)WiZj,k\u22121(t), (52)\nwhere Gj(t, a, b) is a subband limited in R(\u221e)(a, b, d(\u03b8j), \u03b4), \u2206k = bk \u2212 bk\u22121. Gj(t, a, b) is expressed in the linear combination of basis sampled from the defined subband:\nGj(t, a, b) = Wk\u03b3j(t), \u03b3j \u2208 R(\u221e)(a, b, d(\u03b8j), \u03b4)d,Wk \u2208 Rh\u00d7d, (53)\nwhere \u03b3j is Fourier basis.\nEach polynomial neural field h(t) represents a continuous one-dimensional phase. To represent the whole phase space of dimension K, we instantiate an individual polynomial neural field hi(t) for each phase in the phase space. Then we ensemble them to form the whole phase.\nWe list the hyper-parameters used to instantiate our phase generator in Table 4.\nSpatial Polynomial Gabor Fields. Previously we have discussed the design of the phase generator, which is a 1-D function taking in time input and output a dynamic phase sequence. We then discuss the design of the polynomial Gabor field which represents the scene spatially.\nThe major difference between the architecture of temporal polynomial neural fields (T-PNF) and spatial polynomial Gabor fields(S-PGF) lies in the definition of Gj . Specifically, S-PGF is also realized as the summation of different S-PGFs representing different subbands:\nF (x) = \u2211 j Fj(x), (54)\nFj(x) = n\u2211 k=1 Gj(x, bk, bk)WjkZj,k(x), (55)\nZj,1(x) = Gj(x, 0,\u22061), (56) Zj,k(x) = Gj(x, 0,\u2206k)WiZj,k\u22121(x), (57)\nwhere Gj(x, a, b) is a subband limited in R(\u221e)(a, b, d(\u03b8j), d(\u00b5j), d(\u03b3j), \u03b4). It is further defined using Gabor basis:\nGj(x, a, b) = Wkgj(x), gj \u2208 R(\u221e)(a, b, d(\u03b8j), d(\u00b5j), d(\u03b3j), \u03b4)d,Wk \u2208 Rh\u00d7d, (58)\nwhere gj is Gabor basis. The hyper-parameters in this model are listed in Table 5.\nBasis Modulation and Phase-PGF. To combine the T-PNF and S-PGF discussed above, we introduce the procedure of basis modulation and form Phase-PGF to represent the dynamic scene.\nIntuitively, the dimension of the phase space is determined by the number of subbands K in S-PGF. For each gj \u2208 R(\u221e)(a, b, d(\u03b8j), d(\u00b5j), d(\u03b3j), \u03b4)d, we assign the phase hj(t) to it. Therefore, the subband Gj(x, a, b) is modulated to:\nG\u2032j(x, t, a, b) = Wkgj(x+ hj(t)), gj \u2208 R(\u221e)(a, b, d(\u03b8j), d(\u00b5j), d(\u03b3j), \u03b4)d,Wk \u2208 Rh\u00d7d. (59)\nPlease refer to appendix C.2 for a more comprehensive discussion on this topic.\nNeural Rendering. In the case of 3D dynamic scene, we further perform a neural rendering (Mildenhall et al., 2021) to render the 2D feature fields. The first 32 output dimension of PhasePGF is interpreted as feature f(x) and the last dimension is interpreted as density \u03c3(x) used in neural rendering. The 2D feature map m is rendered using the following volume rendering equation:\nF (r) = \u222b tf tn T (t)\u03c3(r(t))f(r(t),d) dt, (60)\nwhere T (t) = exp ( \u2212 \u222b t tn \u03c3(r(s)) ds ) , r is the ray being rendered, and d is the view direction.\nFeature Decoding and Rendering. We then describe the network architecture that decodes a feature map into RGB rendering. The decoder is a bunch of U-Net (Ronneberger et al., 2015) like convolution layers followed by batch normalization. It incorporates skip connections for feature concatenation. Starting with 256 input channels, the decoder progressively reduces and then increases the channel size. The final layers produce outputs suitable for RGB images, with a Tanh activation function scaling the outputs to a specific range. The detailed architecture of it is detailed in Table 6.\nThe Upsampler module employs nearest neighbor upsampling and convolution operations for feature map upscaling. It includes two stages of upsampling, each magnifying the spatial dimensions by a factor of 2. Between these stages, convolutional layers adjust the channel dimensions. The configuration is outlined in Table 7.\nAdversarial Training. During adversarial training, a PatchGAN (Isola et al., 2017) architecture discriminator is used."
        },
        {
            "heading": "C.2 DISCUSSION ON THE PHASE SPACE",
            "text": "In this section, we further discuss the initialization and the structure of the phase space. We also perform an experiment to help the reader intuitively understand the phase space.\nAs previously discussed in appendix C.1, the phase space has a dimension K, which is identical to the number of subbands defined in S-PGF. This parameter is further determined by the human prior knowledge of the number of motion components in the given scene. However, we argue that our architecture allows a redundant number of phase dimensions that can be not identical to the motion count in the given scene.\nThis mechanism of a redundant number of phase dimensions is similar to Slot-Attention (Locatello et al., 2020), where there can be empty slots that do not represent an object. In our case, whether a phase represents a part of motion information is determined by the contribution to the final rendering of the subband corresponding to this phase. Mathematically, this is formulated as a phase score sj defined as below:\nsj = Et,x||Fj(x, t)\u2212 \u00b5j(x, t)||, (61) \u00b5j(x, t) = ExFj(x, t) (62)\nPractically this score is calculated by performing discretization.\nWe provide an example below to demonstrate this argument. As demonstrated in Figure 7(a), we synthesize a video containing a ball with the motion of damping vibration. We instantiate our network with 16 subbands, each associated with a phase. Although such a phase space is redundant, the space can be analyzed using eq. (62). The result is shown in Figure 7(e), from which we can see that phase with index 0 and 2 dominated with score distribution.\nWe further sample some phases in visualize them in Figure 7(d). For phases having a high score (0 and 2), they align well with the ground truth motion shown in Figure 7(c). From the subband visualization in Figure 7(b), it can also be seen that they spatially represent the moving ball.\nFor phases that have a low score (4 and 9), they have a relatively chaotic motion. In Figure 7(b), it can be seen that they do not represent meaningful information in the input, indicating that they are \u201cempty phases\u201d."
        },
        {
            "heading": "C.3 MANIPULATION OF THE PHASE SPACE",
            "text": "In this section, we discuss the implementation details of the two different types of phase manipulations studied in this paper. The result of such manipulation is discussed in Sec. 4.3.\nPhase Smoothing. The operation of phase smoothing corresponds to \u201cmotion smoothing\u201d in Sec. 4.3. Practically, a bandwidth upper limit B is defined to perform the editing. For a given phase sequence h(t) \u2208 H, the edited phase h\u2032 is defined as follows:\nh\u2032(t|B) = F\u22121(T (F(h(t))|B)), (63) T (f |B) = { 1, if f \u2264 B, 0, otherwise, (64)\n(a) Frames of the Input Video (b) Di erent Subbands of Frame 0. From left to right: 0, 2, 4, 9\nwhere F and F\u22121 are Fourier transform and its inverse transform.\nPhase Intensity Adjustment. This operation corresponds to \u201cmotion intensity adjustment\u201d in Sec. 4.3. Practically, similar to the phase augmentation procedure described in Sec. 3.4, three parameters \u03bb, bl, bh are defined in this procedure. Given a phase sequence h(t) \u2208 H, the manipulated phase h\u2032 is defined as:\nh\u2032(t|\u03bb, bl, bh) = h(t) + (\u03bb\u2212 1) \u00b7 y(t|bl, bh), (65) y(t|bl, bh) = F\u22121(T (F(h(t))|bl, bh)), (66)\nwhere \u03bb is intensity manipulation coefficient, bl and bh are subband limits for specific component of the signal, F and F\u22121 are Fourier transform and its inversion, and T (f) is a band-limit filter defined as follows:\n(a) Frames of the Input Video\nhttps://chen-geng.com/phasepgf#jelly\nT (f |f1, f2) = { 1, if f1 \u2264 f \u2264 f2, 0, otherwise.\n(67)"
        },
        {
            "heading": "C.4 TRAINING DETAILS",
            "text": "The models are trained on a NVIDIA A5000 GPU. The first stage of training takes around ten hours to converge. The second stage of adversarial training takes around three days to fully converge."
        },
        {
            "heading": "D ADDITIONAL EVALUATIONS",
            "text": "In this section, we provide more extensive results of the proposed method to further explore the boundary of the proposed method."
        },
        {
            "heading": "D.1 MORE MACRO MOTION COMPLEXITIES",
            "text": "We first demonstrate several additional motion examples to show that our method can handle varying motion complexities.\nNon-Rigid Motion We first show that the proposed pipeline can also deal with non-rigid motion in a dynamic scene. Specifically, we run the proposed method on an Internet video dubbed Jelly. The results can be seen in Figure 8 and https://chen-geng.com/phasepgf#jelly.\nIt can be seen that although the motion in this video is a complex non-rigid, non-regular motion of a jelly, our method successfully discovered its low-dimension phase information by assigning a subband to its moving texture feature, as shown in Figure 8(b).\nInteraction Between Objects. A more complex motion pattern is when there are interactions between different objects. To simulate this case, we synthesize a video called Collision where there are two moving balls in the scene, and the first ball is collision with the second ball.\n(a) Frames of the Input Video\n(a) Frames of the Input Video\nWe show the results of this case in Figure 9 and https://chen-geng.com/phasepgf# collision. Our model can successfully decompose the phase space and discover plausible motion patterns: The first ball is moving initially and will change direction later, and the second ball is static at first while will be moving after the collision.\n(a) Frames of the Input Video\nMulti-Object Motion. We also study the applicability of the proposed framework in the scenario of a slightly more complex motion space. We synthesize a dynamic scene with three different moving balls (Ball3 data) with different moving frequencies and trajectories. By using the proposed method to extract the phase information in such a scene, we can get the phase space decomposition as shown in Figure 10. The animated results can be found at https: //chen-geng.com/phasepgf#ball3\nNon-Periodic Motion We additionally synthesize three videos, Damping, Bouncing, and Projectile, containing different non-periodic motions. In Damping and Bouncing, we simulate damping vibration and bouncing of a ball. In Projectile, we simulate a ball being projected outward, subjecting to gravity.\nThe results in Damping can be found at Figure 7. It can be seen that the low-dimensional motion can be faithfully reconstructed by the extracted phase. Please see https://chen-geng.com/ phasepgf#damping for the animation.\nThe Projectile example provides another different pattern of motion. From Figure 11, it can be observed that our phase, defined in two dimensions, can be used to decompose the given motion into x-dim and y-dim, representing uniform linear motion and parabolic motion, respectively. The animation can be found at https://chen-geng.com/phasepgf#projectile"
        },
        {
            "heading": "D.2 QUANTITATIVE METRICS AND EVALUATION",
            "text": "We introduce several metrics to quantitatively evaluate the results.\nNormalized Cross Correlation. This metric is a measure used to quantify the similarity between two signals. For signal f(t) and g(t), this metric is defined as below:\nr = \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 t(f(t)\u2212 f\u0304)(g(t)\u2212 g\u0304)\u221a\u2211 t(f(t)\u2212 f\u0304)2 \u2211 t(g(t)\u2212 g\u0304)2 \u2223\u2223\u2223\u2223\u2223\u2223 , (68)\nWe calculate this metric between the predicted phase and the ground truth motion trajectory in two synthetic data: Projectile and Damping. The results are shown in Table 2. Compared to the baseline, our model extracts phases that are more aligned to the input trajectory.\nFre\u0301chet Inception Distance (FID). We calculate the FID score (Heusel et al., 2017) between the input video and the manipulated video to evaluate the rendering quality of different methods. We perform the evaluation on a real captured video Airpods. The video can be found at https: //chen-geng.com/phasepgf.\nThe result can be found at the third row of Table 3. Our method surpasses all the baselines significantly in terms of rendering quality."
        },
        {
            "heading": "D.3 ABLATION STUDIES",
            "text": "We conduct ablation studies on the proposed components to validate the effectiveness of the proposed method. The results can be found at Figure 12.\nThe Effect of Feature Decoder It can be seen from Figure 12(b) and (c) that when we remove the feature decoding module, the rendering quality has degraded by a large magnitude. The reason is that only using S-PGF will produce low-resolution images. By using a feature decoder, we can render higher-resolution images.\nThe Effect of Adversarial Training By comparing Figure 12(c) and (d), we can see that adding adversarial training lets the model learn more information on the detailed texture of the object."
        },
        {
            "heading": "D.4 MORE ANIMATED RESULTS",
            "text": "We refer the reader to the supplementary website: https://chen-geng.com/phasepgf for more animated results."
        },
        {
            "heading": "D.5 DISCUSSION ON THE POINT TRACKING METHODS",
            "text": "Another possible method to perform macro motion analysis is by doing a dense point tracking method and figuring out some method to extract sparse motion information from the dense tracked particle trajectory. However, it is non-trivial to perform this process. After point tracking, the obtained particle trajectories are of a high dimension, which can not be easily analyzed.\nWe make an attempt in this section to use a State-of-the-Art point tracking method PIPs++ (Zheng et al., 2023) together with a dimension reduction technique to get a sparse motion feature sequence. Specifically, we sample dense tracked particles using a grid strategy following (Zheng et al., 2023) and perform dense point tracking. This results in a high-dimensional motion trajectory. We then\nperform Principal Component Analysis (PCA) on the obtained dense motion to extract its dominant dimensions to get a sparse motion trajectory. The animated result can be found at https:// chen-geng.com/phasepgf//tracking.html.\nWe visualize the extracted motion components in Figure 13. It can be seen that the extracted lowdimensional motion representation is noisy and is not plausible for the motion macro motion analysis task in this paper (Cf. the phase generated by the proposed method in Figure 3)."
        },
        {
            "heading": "D.6 DISCUSSION ON MOVING OBJECT DISCOVERY",
            "text": "The proposed method can be potentially used for finding moving objects in a dynamic scene. In this section, we make a preliminary attempt at this.\nWe capture a video called Giraffe and run our method on this video. The result can be found in Figure 14. In this example, the moving red ball and the static giraffe toy are separated into two different subbands. Using this decomposition, we can detect the moving objects in a dynamic scene."
        },
        {
            "heading": "D.7 FAILURE CASES",
            "text": "In this section, we provide some failure cases of the proposed method to help the reader better understand the boundaries and limitations of the proposed method. The results can be found in Figure 15 and please refer to https://chen-geng.com/phasepgf#failure for animation in this section.\nChaotic Motion Hair For chaotic motion such as hair blowing, there is no underlying lowdimensional representation of the scene motion. In this case, the method can only discover a coarse motion tendency yet cannot recover the full motion space.\nMotion of Shapeless Objects Fire The proposed method assumes the objects in the scene have a concrete shape. For data like fire where the objects are shapeless, our method can only discover the very coarse moving part, but cannot clearly separate different motion parts."
        }
    ],
    "title": "MACRO MOTION ANALYSIS",
    "year": 2024
}