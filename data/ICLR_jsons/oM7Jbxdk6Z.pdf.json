{
    "abstractText": "Self-supervised learning has recently gained growing interest in molecular modeling for scientific tasks such as AI-assisted drug discovery. Current studies consider leveraging both 2D and 3D molecular structures for representation learning. However, relying on straightforward alignment strategies that treat each modality separately, these methods fail to exploit the intrinsic correlation between 2D and 3D representations that reflect the underlying structural characteristics of molecules, and only perform coarse-grained molecule-level alignment. To derive fine-grained alignment and promote structural molecule understanding, we introduce an atomic-relation level \"blend-then-predict\" self-supervised learning approach, MOLEBLEND, which first blends atom relations represented by different modalities into one unified relation matrix for joint encoding, then recovers modality-specific information for 2D and 3D structures individually. By treating atom relationships as anchors, MOLEBLEND organically aligns and integrates visually dissimilar 2D and 3D modalities of the same molecule at fine-grained atomic level, painting a more comprehensive depiction of each molecule. Extensive experiments show that MOLEBLEND achieves state-of-the-art performance across major 2D/3D molecular benchmarks. We further provide theoretical insights from the perspective of mutual-information maximization, demonstrating that our method unifies contrastive, generative (cross-modality prediction) and mask-then-predict (single-modality prediction) objectives into one single cohesive framework.",
    "authors": [
        {
            "affiliations": [],
            "name": "MODALITY BLENDING"
        }
    ],
    "id": "SP:8d7398e3da7a8df707644a24316b567b5fe8b27b",
    "references": [
        {
            "authors": [
                "Alexander A Alemi",
                "Ian Fischer",
                "Joshua V Dillon",
                "Kevin Murphy"
            ],
            "title": "Deep variational information bottleneck",
            "venue": "arXiv preprint arXiv:1612.00410,",
            "year": 2016
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Hrishikesh Khandeparkar",
                "Mikhail Khodak",
                "Orestis Plevrakis",
                "Nikunj Saunshi"
            ],
            "title": "A theoretical analysis of contrastive unsupervised representation learning",
            "year": 1902
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Yen-Chun Chen",
                "Linjie Li",
                "Licheng Yu",
                "Ahmed El Kholy",
                "Faisal Ahmed",
                "Zhe Gan",
                "Yu Cheng",
                "Jingjing Liu"
            ],
            "title": "UNITER: universal image-text representation learning",
            "venue": "Computer Vision - ECCV 2020 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Seyone Chithrananda",
                "Gabriel Grand",
                "Bharath Ramsundar"
            ],
            "title": "Chemberta: Large-scale selfsupervised pretraining for molecular property prediction",
            "venue": "CoRR, abs/2010.09885,",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Godwin",
                "Michael Schaarschmidt",
                "Alexander L. Gaunt",
                "Alvaro Sanchez-Gonzalez",
                "Yulia Rubanova",
                "Petar Velickovic",
                "James Kirkpatrick",
                "Peter W. Battaglia"
            ],
            "title": "Simple GNN regularisation for 3d molecular property prediction and beyond",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Zhenyu Hou",
                "Xiao Liu",
                "Yukuo Cen",
                "Yuxiao Dong",
                "Hongxia Yang",
                "Chunjie Wang",
                "Jie Tang"
            ],
            "title": "Graphmae: Self-supervised masked graph autoencoders",
            "venue": "KDD \u201922: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data",
            "year": 2022
        },
        {
            "authors": [
                "Weihua Hu",
                "Bowen Liu",
                "Joseph Gomes",
                "Marinka Zitnik",
                "Percy Liang",
                "Vijay S. Pande",
                "Jure Leskovec"
            ],
            "title": "Strategies for pre-training graph neural networks",
            "venue": "In 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Weihua Hu",
                "Matthias Fey",
                "Hongyu Ren",
                "Maho Nakata",
                "Yuxiao Dong",
                "Jure Leskovec"
            ],
            "title": "OGB-LSC: A large-scale challenge for machine learning on graphs",
            "venue": "Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks",
            "year": 2021
        },
        {
            "authors": [
                "Guolin Ke",
                "Di He",
                "Tie-Yan Liu"
            ],
            "title": "Rethinking positional encoding in language pre-training",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Lingpeng Kong",
                "Cyprien de Masson d\u2019Autume",
                "Wang Ling",
                "Lei Yu",
                "Zihang Dai",
                "Dani Yogatama"
            ],
            "title": "A mutual information maximization perspective of language representation learning",
            "year": 1910
        },
        {
            "authors": [
                "Ralph Linsker"
            ],
            "title": "An application of the principle of maximum information preservation to linear systems",
            "venue": "Advances in neural information processing systems,",
            "year": 1988
        },
        {
            "authors": [
                "Shengchao Liu",
                "Hanchen Wang",
                "Weiyang Liu",
                "Joan Lasenby",
                "Hongyu Guo",
                "Jian Tang"
            ],
            "title": "Pretraining molecular graph representation with 3d geometry",
            "venue": "arXiv preprint arXiv:2110.07728,",
            "year": 2021
        },
        {
            "authors": [
                "Shengchao Liu",
                "Hongyu Guo",
                "Jian Tang"
            ],
            "title": "Molecular geometry pretraining with se (3)-invariant denoising distance matching",
            "venue": "arXiv preprint arXiv:2206.13602,",
            "year": 2022
        },
        {
            "authors": [
                "Shengchao Liu",
                "Hanchen Wang",
                "Weiyang Liu",
                "Joan Lasenby",
                "Hongyu Guo",
                "Jian Tang"
            ],
            "title": "Pretraining molecular graph representation with 3d geometry",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Shengchao Liu",
                "Weitao Du",
                "Zhiming Ma",
                "Hongyu Guo",
                "Jian Tang"
            ],
            "title": "A group symmetric stochastic differential equation model for molecule multi-modal pretraining",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Yi Liu",
                "Limei Wang",
                "Meng Liu",
                "Xuan Zhang",
                "Bora Oztekin",
                "Shuiwang Ji"
            ],
            "title": "Spherical message passing for 3d graph networks. CoRR, abs/2102.05013, 2021b. URL https://arxiv.org/ abs/2102.05013",
            "year": 2021
        },
        {
            "authors": [
                "Shengjie Luo",
                "Tianlang Chen",
                "Yixian Xu",
                "Shuxin Zheng",
                "Tie-Yan Liu",
                "Liwei Wang",
                "Di He"
            ],
            "title": "One transformer can understand both 2d & 3d molecular data",
            "venue": "arXiv preprint arXiv:2210.01765,",
            "year": 2022
        },
        {
            "authors": [
                "David McAllester",
                "Karl Stratos"
            ],
            "title": "Formal limitations on the measurement of mutual information",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "Ben Poole",
                "Sherjil Ozair",
                "Aaron Van Den Oord",
                "Alex Alemi",
                "George Tucker"
            ],
            "title": "On variational bounds of mutual information",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Raghunathan Ramakrishnan",
                "Pavlo O Dral",
                "Matthias Rupp",
                "O Anatole Von Lilienfeld"
            ],
            "title": "Quantum chemistry structures and properties of 134 kilo molecules",
            "venue": "Scientific data,",
            "year": 2014
        },
        {
            "authors": [
                "Yu Rong",
                "Yatao Bian",
                "Tingyang Xu",
                "Weiyang Xie",
                "Ying Wei",
                "Wenbing Huang",
                "Junzhou Huang"
            ],
            "title": "Self-supervised graph transformer on large-scale molecular data",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Bernhard Sch\u00f6lkopf",
                "Kah Kay Sung",
                "Christopher J.C. Burges",
                "Federico Girosi",
                "Partha Niyogi",
                "Tomaso A. Poggio",
                "Vladimir Vapnik"
            ],
            "title": "Comparing support vector machines with gaussian kernels to radial basis function classifiers",
            "venue": "IEEE Trans. Signal Process.,",
            "year": 1997
        },
        {
            "authors": [
                "Peter Shaw",
                "Jakob Uszkoreit",
                "Ashish Vaswani"
            ],
            "title": "Self-attention with relative position representations",
            "venue": "Association for Computational Linguistics,",
            "year": 2018
        },
        {
            "authors": [
                "Hannes St\u00e4rk",
                "Dominique Beaini",
                "Gabriele Corso",
                "Prudencio Tossou",
                "Christian Dallago",
                "Stephan G\u00fcnnemann",
                "Pietro Li\u00f2"
            ],
            "title": "3d infomax improves gnns for molecular property prediction",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Fan-Yun Sun",
                "Jordan Hoffmann",
                "Vikas Verma",
                "Jian Tang"
            ],
            "title": "Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization",
            "venue": "In 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Philipp Th\u00f6lke",
                "Gianni De Fabritiis"
            ],
            "title": "Equivariant transformers for neural network based molecular potentials",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Naftali Tishby",
                "Fernando C Pereira",
                "William Bialek"
            ],
            "title": "The information bottleneck method",
            "venue": "arXiv preprint physics/0004057,",
            "year": 2000
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Pascal Vincent",
                "Hugo Larochelle",
                "Yoshua Bengio",
                "Pierre-Antoine Manzagol"
            ],
            "title": "Extracting and composing robust features with denoising autoencoders",
            "venue": "In Proceedings of the 25th international conference on Machine learning,",
            "year": 2008
        },
        {
            "authors": [
                "Wenhui Wang",
                "Hangbo Bao",
                "Li Dong",
                "Johan Bjorck",
                "Zhiliang Peng",
                "Qiang Liu",
                "Kriti Aggarwal",
                "Owais Khan Mohammed",
                "Saksham Singhal",
                "Subhojit Som",
                "Furu Wei"
            ],
            "title": "Image as a foreign language: Beit pretraining for all vision and vision-language",
            "venue": "tasks. CoRR,",
            "year": 2022
        },
        {
            "authors": [
                "Yuyang Wang",
                "Jianren Wang",
                "Zhonglin Cao",
                "Amir Barati Farimani"
            ],
            "title": "Molecular contrastive learning of representations via graph neural networks",
            "venue": "Nat. Mach. Intell.,",
            "year": 2022
        },
        {
            "authors": [
                "Zhenqin Wu",
                "Bharath Ramsundar",
                "Evan N. Feinberg",
                "Joseph Gomes",
                "Caleb Geniesse",
                "Aneesh S. Pappu",
                "Karl Leswing",
                "Vijay S. Pande"
            ],
            "title": "Moleculenet: A benchmark for molecular machine learning",
            "venue": "CoRR, abs/1703.00564,",
            "year": 2017
        },
        {
            "authors": [
                "Jun Xia",
                "Chengshuai Zhao",
                "Bozhen Hu",
                "Zhangyang Gao",
                "Cheng Tan",
                "Yue Liu",
                "Siyuan Li",
                "Stan Z Li"
            ],
            "title": "Mole-bert: Rethinking pre-training graph neural networks for molecules",
            "year": 2023
        },
        {
            "authors": [
                "Zhenda Xie",
                "Zigang Geng",
                "Jingcheng Hu",
                "Zheng Zhang",
                "Han Hu",
                "Yue Cao"
            ],
            "title": "Revealing the dark secrets of masked image modeling",
            "venue": "arXiv preprint arXiv:2205.13543,",
            "year": 2022
        },
        {
            "authors": [
                "Minghao Xu",
                "Hang Wang",
                "Bingbing Ni",
                "Hongyu Guo",
                "Jian Tang"
            ],
            "title": "Self-supervised graph-level representation learning with local and global structure",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Chengxuan Ying",
                "Tianle Cai",
                "Shengjie Luo",
                "Shuxin Zheng",
                "Guolin Ke",
                "Di He",
                "Yanming Shen",
                "Tie-Yan Liu"
            ],
            "title": "Do transformers really perform badly for graph representation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yuning You",
                "Tianlong Chen",
                "Yongduo Sui",
                "Ting Chen",
                "Zhangyang Wang",
                "Yang Shen"
            ],
            "title": "Graph contrastive learning with augmentations",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Mert Yuksekgonul",
                "Federico Bianchi",
                "Pratyusha Kalluri",
                "Dan Jurafsky",
                "James Zou"
            ],
            "title": "When and why vision-language models behave like bag-of-words models, and what to do about it",
            "year": 1936
        },
        {
            "authors": [
                "Sheheryar Zaidi",
                "Michael Schaarschmidt",
                "James Martens",
                "Hyunjik Kim",
                "Yee Whye Teh",
                "Alvaro Sanchez-Gonzalez",
                "Peter Battaglia",
                "Razvan Pascanu",
                "Jonathan Godwin"
            ],
            "title": "Pre-training via denoising for molecular property prediction",
            "venue": "arXiv preprint arXiv:2206.00133,",
            "year": 2022
        },
        {
            "authors": [
                "Zaixi Zhang",
                "Qi Liu",
                "Hao Wang",
                "Chengqiang Lu",
                "Chee-Kong Lee"
            ],
            "title": "Motif-based graph selfsupervised learning for molecular property prediction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Gengmo Zhou",
                "Zhifeng Gao",
                "Qiankun Ding",
                "Hang Zheng",
                "Hongteng Xu",
                "Zhewei Wei",
                "Linfeng Zhang",
                "Guolin Ke"
            ],
            "title": "Uni-mol: A universal 3d molecular representation learning",
            "year": 2023
        },
        {
            "authors": [
                "Jinhua Zhu",
                "Yingce Xia",
                "Lijun Wu",
                "Shufang Xie",
                "Tao Qin",
                "Wengang Zhou",
                "Houqiang Li",
                "Tie-Yan Liu"
            ],
            "title": "Unified 2d and 3d pre-training of molecular representations",
            "venue": "arXiv preprint arXiv:2207.08806,",
            "year": 2022
        },
        {
            "authors": [
                "Zhou"
            ],
            "title": "ContextPred (Hu et al., 2020)",
            "venue": "Results of AttrMask (Hu et al.,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Self-supervised learning has recently gained growing interest in molecular modeling for scientific tasks such as AI-assisted drug discovery. Current studies consider leveraging both 2D and 3D molecular structures for representation learning. However, relying on straightforward alignment strategies that treat each modality separately, these methods fail to exploit the intrinsic correlation between 2D and 3D representations that reflect the underlying structural characteristics of molecules, and only perform coarse-grained molecule-level alignment. To derive fine-grained alignment and promote structural molecule understanding, we introduce an atomic-relation level \"blend-then-predict\" self-supervised learning approach, MOLEBLEND, which first blends atom relations represented by different modalities into one unified relation matrix for joint encoding, then recovers modality-specific information for 2D and 3D structures individually. By treating atom relationships as anchors, MOLEBLEND organically aligns and integrates visually dissimilar 2D and 3D modalities of the same molecule at fine-grained atomic level, painting a more comprehensive depiction of each molecule. Extensive experiments show that MOLEBLEND achieves state-of-the-art performance across major 2D/3D molecular benchmarks. We further provide theoretical insights from the perspective of mutual-information maximization, demonstrating that our method unifies contrastive, generative (cross-modality prediction) and mask-then-predict (single-modality prediction) objectives into one single cohesive framework."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Self-supervised learning has been successfully applied to molecular representation learning (Xia et al., 2023; Chithrananda et al., 2020), where meaningful representations are extracted from a large amount of unlabeled molecules. The learned representation can then be finetuned to support diverse downstream molecular tasks. Early works design learning objectives based on a single modality (2D topological graphs (Hu et al., 2020; Rong et al., 2020; You et al., 2020), or 3D spatial structures (Zaidi et al., 2022; Liu et al., 2022a; Zhou et al., 2023)). Recently, multimodal molecular pretraining that exploits both 2D and 3D modalities in a single framework (Liu et al., 2022b; St\u00e4rk et al., 2022; Liu et al., 2023; Luo et al., 2022; Zhu et al., 2022) has emerged as an alternative solution.\nMultimodal pretraining aims to align representations from different modalities. Most existing methods naturally adopt two models (Figure 1(a)) to encode 2D and 3D information separately (Liu et al., 2022b; St\u00e4rk et al., 2022; Liu et al., 2023). Contrastive learning is typically employed to attract representations of 2D graphs with their corresponding 3D conformations of the same molecule, and repulse those from different molecules. Another school of study is generative methods that bridge 2D and 3D modalities via mutual prediction (Figure 1(a-b)), such as taking 2D graphs as input to predict 3D information, and vice versa (Liu et al., 2022b; Zhu et al., 2022; Liu et al., 2023).\nHowever, these approaches only align different modalities on a coarse-grained molecule-level. The contrastive learning used in most existing methods has been proved to lack detailed structural understanding of the data (Yuksekgonul et al., 2022; Xie et al., 2022), thus missing a deep comprehension of the constituting atoms and relations, which plays a vital role in representing molecules (Sch\u00fctt et al., 2017; Liu et al., 2021b). Besides, all methods consider different modalities as independent signals in each model and treat them as separate integral inputs (Figure 1(a-b). This practice divides\ndifferent modalities apart and ignores the underlying correlation between 2D and 3D modalities, only realizing a rudimentary molecule-level alignment.\nTo derive a more fine-grained alignment and promote structural molecular understanding, a deeper look into the atom-relation-level sub-structures is asked for. We observe that although appearing visually distinct and residing in different high-dimensional spaces, 2D molecular graphs and 3D spatial structures are intrinsically equivalent as they are essentially different manifestations of the same atoms and their relationships. The differentiating factor of relationship appears as chemical bond or shortest path distance in 2D graph, or 3D euclidean distance in 3D structure. Thus, pivoting around atom relationship and explicitly leveraging the alignment between modalities to mutually enhance both 2D and 3D representations can be a more natural and effective alignment strategy.\nIn this work, we introduce a relation-level multimodal pretraining method, MOLEBLEND, which explicitly leverages the alignment of atom relations between 2D and 3D structures and blends input signals from different modalities as one unified data structure to pre-train one single model (Figure 1(c)). Specifically, MOLEBLEND consists of a two-stage blend-then-predict training procedure: modality-blended encoding and modality-targeted prediction. During encoding, we blend different depictions of atom relations from 2D and 3D views into one relation matrix. During prediction, the model recovers missing 2D and 3D information as supervision signals. With such a relationlevel blending approach, multimodal molecular information is mingled within a unified model, and fine-grained atom-relation alignment in the multimodal input space leads to a deeper structural understanding of molecular makeup. Extensive experiments demonstrate that MOLEBLEND outperforms existing molecular modeling methods across a broad range of 2D and 3D benchmarks. We further provide theoretical insights from the perspective of mutual-information maximization to validate the proposed pretraining objective.\nOur contributions are summarized as follows:\n\u2022 We propose to align molecule 2D and 3D modalities at atomic-relation level, and introduce MOLEBLEND, a multimodal molecular pretraining method that explicitly utilizes the intrinsic correlations between 2D and 3D representations in pretraining.\n\u2022 Empirically, extensive evaluation demonstrates that MOLEBLEND achieves state-of-the-art performance over diverse 2D and 3D tasks, verifying the effectiveness of relation-level alignment.\n\u2022 Theoretically, we provide a decomposition analysis of our objective as an explanatory tool, for better understanding of the proposed blend-then-predict learning objective."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Multimodal molecular pretraining (Liu et al., 2022b; St\u00e4rk et al., 2022; Zhu et al., 2022; Luo et al., 2022; Liu et al., 2023) leverages both 2D and 3D information to learn molecular representations. It bears a trade-off between cost and performance, as 3D information is vital for molecular property prediction but 3D models tend to be resource-intensive during deployment. Most existing methods utilize two separate models to encode 2D and 3D information (Liu et al., 2022b; St\u00e4rk et al., 2022; Liu et al., 2023). Their pretraining methods mostly use contrastive learning (He et al., 2020), which treats 2D graphs with their corresponding 3D conformations as positive views and information from different molecules as negative views for contrasting Liu et al. (2022b); St\u00e4rk et al. (2022); Liu et al. (2023). Another pretraining method uses generative models to predict one modality based on the input of another modality Liu et al. (2022b; 2023). Zhu et al. (2022) proposes to encode both 2D and 3D inputs within a single GNN model, but different modalities are still treated as separate inputs. We instead propose to leverage atom relations as the anchor to blend different modalities together as an integral input to a single model.\nMasked auto-encoding (Vincent et al., 2008) is a widely applied representation learning method (Devlin et al., 2019; He et al., 2022) that removes a portion of the data and learns to predict the missing content (mask-then-predict). Multimodal masking approaches in other multimodal learning areas (e.g., BEiT-3 (Wang et al., 2022a), UNITER (Chen et al., 2020b)) directly concatenate different modalities into a sequence, then predict the masked tokens, without explicit alignment of modalities in the input space. Different from them, MOLEBLEND blends together the elements of different modalities in the input space with explicit alignment."
        },
        {
            "heading": "3 MULTIMODAL MOLECULAR PRETRAINING VIA BLENDING",
            "text": "Molecules are typically represented by either 2D molecular graph or 3D spatial structure. Despite their distinct appearances, they depict a common underlying structure, i.e., atoms and their relationships (e.g., shortest path distance and edge type in 2D molecular graph, and Euclidean distance in 3D structure). Naturally, these representations should be unified organically, instead of treated separately with different models, in order to learn the representation of complex chemical relations underneath. We perform explicit relation-level alignment via blending for unifying modalities."
        },
        {
            "heading": "3.1 PROBLEM FORMULATION",
            "text": "A molecule M can be represented as a set of atoms V \u2208 Rn\u00d7v along with their relationships R \u2208 Rn\u00d7n\u00d7r, where n is the number of atoms, v and r are dimensions of atom and relation feature, respectively. The nature of R can vary depending on the context. In the commonly used 2D graph representation of molecules, R is represented by the chemical bonds E , which are the edges of the 2D molecular graph. In 3D scenarios, R is defined as the relative Euclidean distance D between atoms. To leverage both 2D and 3D representations, we adopt the shortest path distance Rspd and the edge type encoding Redge of molecular graph, as well as Euclidean distance Rdistance in 3D space, as three different appearances of atom relations across 2D/3D modalities. And instead of treating each modality separately with individual models, we blend the three representations into a single matrix R2D&3D by randomly sampling each representation for each vector, following a pre-defined multinomial distribution S. Our pre-training objective is to maximize the following likelihood:\nmaxES P (Rspd,Redge,Rdistance|R2D&3D,S,V) (1)\nWe employ the Transformer model (Vaswani et al., 2017) to parameterize our objective, capitalizing on its ability to incorporate flexible atom relations in a fine-grained fashion through attention bias (Raffel et al., 2020; Shaw et al., 2018; Ke et al., 2021; Ying et al., 2021). This choice is further supported by recent research demonstrating that a single Transformer model can effectively process both 2D and 3D data (Luo et al., 2022).\nTransformer Block The Transformer architecture is composed of a stack of identical blocks, each containing a multi-head self-attention layer and a position-wise feed-forward network. Residual connection (He et al., 2016) and layer normalization (Ba et al., 2016) are applied to each layer. Denote Xl = [xl1;x l 2; . . . ;x l n] as the input to the l-th block with the sequence length n, and each vector xi \u2208 Rd is the contextual representation of the atom at position i. d is the dimension of the hidden representations. A Transformer block first computes the multi-head self-attention to effectively aggregate the input sequence Xl:\nMulti-Head(X) = Concat(head1, . . . , headh)WO (2)\nwhere headi = Attention(XW Q i ,XW K i ,XW V i ) and h is the number of attention heads. W Q i , WKi ,W V i \u2208 Rd\u00d7dh ,WO \u2208 Rd\u00d7d are learnable parameter matrices. The attention computation is defined as:\nAttention(Q,K,V) = softmax ( QK\u22a4\u221a\nd\n) V (3)\nGenerally, given input X l, the l-th block works as follows: X\u0303l = LayerNorm ( Xl + Multi-Head(Xl) ) (4)\nXl+1 = LayerNorm ( X\u0303l + GELU(X\u0303lWl1)W l 2 ) (5)\nwhere Wl1 \u2208 Rd\u00d7df ,Wl2 \u2208 Rdf\u00d7d, and df is the intermediate size of the feed-forward layer."
        },
        {
            "heading": "3.2 LEARNING OBJECTIVE",
            "text": "To facilitate fine-grained alignment and organic integration of different depictions of atoms and their relations across 2D/3D spaces, we design a new \u2018blend-then-predict\u2019 training paradigm that consists of two steps: 1) modality-blended encoding that encodes a molecule with blended information from different modalities; and 2) modality-targeted prediction that recovers the original 2D and 3D input. The pre-training process is illustrated in Figure 2. The core idea is to bind different modalities together at a granular level by blending relations from multiple modalities into an integral input from the get-go, to encourage the model to discover fundamental and unified relation representations across heterogeneous forms.\nModality-blended Encoding Multimodal learning aims to learn the most essential representations of data that possess inherent connections while appearing distinctive between different modalities. In the context of molecules, atom relationships are the common attributes underpinning different representations across 2D/3D modalities. This motivates us to leverage relations as anchors, to align both modalities in a fine-grained manner that blends multimodalities from the very beginning.\nWe adopt three appearances of relations across 2D and 3D modalities following (Luo et al., 2022): shortest path distance, edge type, and 3D Euclidean distance. For each atom pair (i, j), \u03a8ijSPD represents the shortest path distance between atom i and j. We encode the edge features along the shortest path between i and j as the edge encoding, \u03a8ijEdge = 1 N \u2211N n=1 w \u22a4 n en, where (e1, e2, . . . , eN ), en \u2208 Rde are features of edges on the shortest path between i and j. wn \u2208 Rde are learnable parameters. Following (Zhou et al., 2023; Luo et al., 2022), we encode Euclidean distances of an atom pair (i, j) with Gaussian Basis Kernel function (Sch\u00f6lkopf et al., 1997):\n\u03b6ijk = G(A(d ij ; \u03b3ij , \u03b2ij);\u00b5k, \u03c3k), k = 1, . . . ,K (6)\n\u03a8ijDistance = GELU(\u03b6 ij \u00b7W 13D)W 23D, \u03b6ij = [\u03b6 ij 1 , . . . , \u03b6 ij K ] \u22a4 (7) where A(d; \u03b3, \u03b2) = \u03b3 \u00b7 d + \u03b2 is the affine transformation with learnable parameters \u03b3 and \u03b2, and G(d;\u00b5, \u03c3) = 1\u221a\n2\u03c0\u03c3 exp ( \u2212 12\u03c32 (d\u2212 \u00b5) 2 ) is the Gaussian density function with parameters \u00b5 and \u03c3.\nK is the number of Gaussian Basis kernels. W 13D \u2208 RK\u00d7K ,W 23D \u2208 RK\u00d71 are learnable parameters. \u03a8SPD,\u03a8Edge,\u03a8Distance denote the three relation matrices of all atom pairs, with the same shape n\u00d7 n. Different from existing works that separately feed one of these relations into different models, we blend them together from the get-go and randomly mix them into one relation matrix, which is then fed into one single model for molecule encoding. Specifically, we first define a multinomial distribution S with a probability vector p = (p1, p2, p3). For each position (i, j) in the matrix, we draw a sample sij \u2208 {1, 2, 3} following the probability distribution p, then determine the corresponding element of the blended matrix as follows:\n\u03a8ij2D&3D = \u03a8 ij SPD11 +\u03a8 ij Edge12 +\u03a8 ij Distance13, where 1k = { 1 if sij = k 0 otherwise\n(8)\n, where each position (i, j) randomly selects its element from one of the \u03a8ijSPD,\u03a8 ij Edge,\u03a8 ij Distance. After the process finishes, distinct relation manifestations (\u03a8SPD,\u03a8Edge,\u03a8Distance) across modalities are blended into a single modality-blended matrix \u03a82D&3D \u2208 Rn\u00d7n without overlapping sub-structures, to represent the inter-atomic relations.\nWe inject this modality-blended relation \u03a82D&3D into the self-attention module, which captures pair-wise relations between inputs atoms, to provide complementary pair-wise information. This practice is also similar to the relative positional encoding for Transformer (Raffel et al., 2020):\nAttention(Q,K,V) = softmax ( QK\u22a4\u221a\nd +\u03a82D&3D\n) V (9)\nWith modality-blending, we explicitly bind different modalities together at fine-grained relation level, which will help the model better integrate and align modalities at fine-grained level.\nModality-targeted Prediction The model recovers the full Rspd,Redge and Rdistance as its training objectives. The intuition is, if the model can predict different types of atom relations, like shortest path on the molecular graph or 3D Euclidean distance, given a single mixed representation, this cross-modality representation must have captured some underlying integral molecular structure.\nSpecifically, after modality-blended encoding, we obtain contextual atom representations XL+1 \u2208 Rn\u00d7d encoded by an L-layer Transformer. We propose an outer product projection module to transform the atom representations into n \u00d7 n atom relations. The representations XL+1 are first linearly projected to a smaller dimension m = 32 with two independent Linear layers Wl,Wr \u2208 Rm\u00d7d. The outer products are computed upon the transformed representations, which are then flattened and projected into the target space with a modality-targeted head Whead \u2208 Rc\u00d7m 2\n. The relation computation between the i-th and j-th atoms is formulated as follows:\noij = G(WlXL+1i )\u2297 G(WrX L+1 j ) \u22a4 \u2208 Rm\u00d7m (10) zij = Whead \u00b7 Flatten(oij) \u2208 Rc (11)\nwhere G(\u00b7) = LayerNorm(GELU(\u00b7)). We now obtain the modality-targeted relation matrix Z \u2208 Rn\u00d7n\u00d7c, where c depends on the targeted task. The predictions of shortest path distance and edge type are formulated as classification tasks, where c is the number of possible shortest path distance or edge types. For predicting 3D distance, we formulate it as a 3-dimensional regression task, and the regression targets are the relative Euclidean distances in 3D space.\nNoisy Node as Regularization Noisy node (Godwin et al., 2022; Zaidi et al., 2022; Luo et al., 2022) incorporates an auxiliary loss for coordinate denoising in addition to the original objective, which has been found effective in improving representation learning. We also adopt this practice as an additional regularization term, by adding Gaussian noise to the input coordinates and requiring the model to predict the added noise."
        },
        {
            "heading": "3.3 FINETUNING",
            "text": "The trained model can be finetuned to accept both 2D and 3D inputs for downstream tasks. For scenarios where a large amount of 2D molecular graphs is available while 3D conformations are too expensive to obtain, the model can take only 2D input to finetune the model. Formally, given shortest path distance Rspd, edge type Redge and atom types V as available 2D information, we define y2D as the task target, K as the number of training samples, and \u2113(\u00b7, \u00b7) as the loss function of the specific training task. The 2D finetuning objective is then defined as:\nL2D = 1\nK K\u2211 k=1 \u2113 ( f(Rkspd,Rkedge,Vk),yk2D ) (12)\nWhen it comes to scenarios where 3D information is obtained, we propose to incorporate both 2D and 3D information as model input, as generating 2D molecular graphs from 3D conformations is free and can bring in useful information from 2D perspective. The multimodal input is injected into the self-attention module that captures pair-wise relations:\nAttention(Q,K,V) = softmax ( QK\u22a4\u221a\nd +\u03a8SPD +\u03a8Edge +\u03a8Distance\n) V (13)\nL3D = 1\nK K\u2211 k=1 \u2113 ( f(Rkspd,Rkedge,Rkdistance,Vk),yk3D ) (14)\nThis practice is unique in utilizing information from multiple modalities for a single-modality task, which is infeasible in previous 3D (Zaidi et al., 2022) or multimodal methods with separate models for different modalities (Liu et al., 2022b; St\u00e4rk et al., 2022; Liu et al., 2023). Empirically, we find that the integration of 2D information helps improve performance. we hypothesize that: 1) 2D information, such as chemical bond on a molecular graph, encodes domain experts\u2019 prior knowledge and provides references to 3D structure; 2) 3D structures obtained from computational simulations can suffer from inevitable approximation errors (Luo et al., 2022) which are avoided in our approach."
        },
        {
            "heading": "3.4 THEORETICAL INSIGHTS",
            "text": "In this section, we present a theoretical perspective from mutual information maximization for a better understanding of the \u2018blend-then-predict\u2019 process. We demonstrate that this approach unifies existing contrastive, generative (inter-modality prediction), and mask-then-predict (intra-modality prediction) objectives within a single objective formulation.\nFor simplicity, we consider two relations, denoted as R2D = (aij)n\u00d7n and R3D = (bij)n\u00d7n. Their elements are randomly partitioned into two parts, represented as R2D = [A1, A2],R3D = [B1, B2], such that Ai shares identical elements indexes with Bi, i \u2208 {1, 2}. The blended matrix is denoted as R2D&3D = [A1, B2]. Proposition 3.1 (Mutual information Maximization) The training process with modality-blending maximizes the lower bound of the following mutual information: ESI(A2;A1, B2) + I(B1;A1, B2). The proof can be found in Appendix B.2.4. Proposition 3.2 (Mutual Information Decomposition) The mutual information I(A2;A1, B2) + I(B1;A1, B2) can be decomposed into two components below. The first one corresponds to the\nobjectives of current contrastive and generative approaches. The second component, which is the primary focus of our research, represents the mask-then-predict objective (proof in Proposition B.1 in Appendix):\nI(A2;A1, B2) + I(B1;A1, B2) = 1 2 [I(A1;B1) + I(A2;B2)\ufe38 \ufe37\ufe37 \ufe38\ncontrastive and generative + I(A1;B1|B2) + I(A2;B2|A1)\ufe38 \ufe37\ufe37 \ufe38 conditional contrastive and generative ]\n+ 1 2 [I(A1;A2) + I(B1;B2)\ufe38 \ufe37\ufe37 \ufe38\nmask-then-predict\n+ I(A1;A2|B2) + I(B1;B2|A1)\ufe38 \ufe37\ufe37 \ufe38 multimodal mask-then-predict ]\n(15)\nThe first part of Equation 15 corresponds to existing (conditional) contrastive and generative methods, which aim to maximize the mutual information between two corresponding parts (Ai with Bi, i \u2208 {1, 2}) across two modalities (see Appendix B.2.1 and B.2.3 for the detailed proof). The second part represents the (multimodal) mask-then-predict objectives, focusing on maximizing the mutual information between the masked and the remaining parts within a single modality (refer to Appendix B.2.2 for details).\nThis decomposition illustrates that our objective unifies contrastive, generative (inter-modality prediction), and mask-then-predict (intra-modality prediction) approaches within a single cohesive blend-then-predict framework, from the perspective of mutual information maximization. Moreover, this approach fosters enhanced cross-modal interaction by introducing an innovative multimodal mask-then-predict target."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUP",
            "text": "Datasets. For pretraining, we use PCQM4Mv2 dataset from the OGB Large-Scale Challenge (Hu et al., 2021), which includes 3.37 million molecules with both 2D graphs and 3D geometric structures. To evaluate the versatility of MOLEBLEND, we carry out extensive experiments on 24 molecular tasks with different data formats across three representative benchmarks: MoleculeNet (Wu et al., 2017) (2D, 11 tasks), QM9 quantum properties (Ramakrishnan et al., 2014) (3D, 12 tasks), and PCQM4Mv2 humo-lumo gap (2D). Further details about these datasets can be found in the Appendix C.1.\nBaselines. We choose the most representative 2D and 3D pretraining baselines: AttrMask (Hu et al., 2020), ContextPred (Hu et al., 2020), InfoGraph (Sun et al., 2020), MolCLR (Wang et al., 2022b), GraphCL (You et al., 2020), GraphLoG (Xu et al., 2021), MGSSL Zhang et al. (2021), as well as recently published method Mole-BERT (Xia et al., 2023) and GraphMAE (Hou et al., 2022) as 2D baselines. In addition, we adopt GraphMVP (Liu et al., 2022b), 3D InfoMax (St\u00e4rk et al., 2022), UnifiedMol Zhu et al. (2022) and MoleculeSDE (Liu et al., 2023) as multimodal baselines. As most baselines adopt GNN as backbone, we further implement two close-related multimodal pretraining baselines, 3D Infomax and GraphMVP, under the same Transformer backbone as we use, to fairly compare the effectiveness of pretraining objective.\nBackbone Model. Following (Ying et al., 2021; Luo et al., 2022), we employ a 12-layer Transformer of hidden size 768, with 32 attention heads. For pretraining, we use AdamW optimizer and set (\u03b21, \u03b22) to (0.9, 0.999) and peak learning rate to 1e-5. Batch size is 4096. We pretrain the model for 1 million steps with initial 100k steps as warm-up, after which learning rate decreases to zero following a cosine decay schedule. The blending ratio p is set to 2:2:6, and the ablations on p can be found in Appedix A.3."
        },
        {
            "heading": "4.2 EVALUATION ON 2D CAPABILITY",
            "text": "We evaluate MOLEBLEND on MoleculeNet, one of the most widely used benchmarks for 2D molecular property prediction, which covers molecular properties ranging from quantum mechanics and physical chemistry to biophysics and physiology. We use the scaffold split (Wu et al., 2017), and report the mean and standard deviation of results of 3 random seeds.\nTable 1 presents the ROC-AUC scores for all compared methods on eight classification tasks. Remarkably, MOLEBLEND achieves state-of-the-art performance in 5 out of 8 tasks, with significant\nmargins in some cases (e.g., 83.7 v.s. 81.5 on Bace). Note that all other multimodal methods (3D Infomax (St\u00e4rk et al., 2022), GraphMVP (Liu et al., 2022b), MoleculeSDE (Liu et al., 2023)) utilize two separate modality-specific models, with contrastive learning as one of their objectives. In contrast, MOLEBLEND models molecules in a unified manner, and perform 2D and 3D alignment in a finegrained relation-level, which demonstrates superior performance. MOLEBLEND also outperforms all 2D baselines (upper section of the table), demonstrating that incorporating 3D information helps improve the prediction of molecular properties. Table 6 summarizes the performance of different methods on three regression tasks of MoleculeNet, which substantiates the superiority of MOLEBLEND."
        },
        {
            "heading": "4.3 EVALUATION ON 3D CAPABILITY",
            "text": "We use QM9 (Ramakrishnan et al., 2014) dataset to evaluate the effectiveness of MOLEBLEND on 3D tasks. QM9 is a quantum chemistry benchmark with 134K small organic molecules. It contains 12 tasks, covering the energetic, electronic and thermodynamic properties of molecules. Following (Th\u00f6lke & Fabritiis, 2022), we randomly split 10,000 and 10,831 molecules as validation and test set, respectively, and use the remaining molecules for finetuning. Results are presented in Table 2, evaluated on MAE metric (lower number indicates better performance). MOLEBLEND achieves state-of-the-art performance among multimodal methods on 8 out of 12 tasks, some of\nwhich with a large margin (e.g., Gap, HOMO, LUMO), demonstrating the strong capability of our model for 3D tasks."
        },
        {
            "heading": "4.4 ABLATION STUDIES",
            "text": "Pretraining Objectives Table 3 studies the effect of different pretraining objectives: noisy-node, blend-then-predict, and blend-then-predict with noisy-node as regularization (MOLEBLEND). We\nobserve that in most tasks, combining blendthen-predict and noisy-node yields better representations. In 2D scenarios, we find that blendthen-predict outperforms noisy-node on 5 out of 8 tasks studied, demonstrating its strong ability to process 2D inputs. While on 3D tasks (U298 and U0), blend-then-predict typically performs worse than noisy-node. This is because noisy-node is a pure 3D denoising task, which makes it more suitable for 3D tasks.\nBlending vs Single-modality Mask-then-Predict Table 4 studies the effect of multimodal blending compared to single-modality mask-then-predict strategies (SPD, Edge, and 3D mask). We trained all models for 200K steps, keeping all settings consistent except for the learning objective. This comparison demonstrates that modality blending achieves better performance over modality-specific mask-then-predict.\nFinetuning Settings When 3D molecular information is provided, we propose to incorporate both 2D topological and 3D structural information into the model, as generating 2D molecular graphs from 3D conformations is computationally inexpensive. Table 5 demonstrates that the inclusion of 2D information leads to a noticeable improvement in performance. We hypothesize that this is due to the fact that 2D information encodes chemical bond and connectivity on a molecular\ngraph, which is grounded in prior knowledge of domain experts and contains valuable references to 3D structure. Note that this practice is a unique advantage of MOLEBLEND, as we pretrain with both 2D and 3D information blended as one single input into a unified model, which is not feasible in previous multimodal methods that utilize two distinct models for 2D and 3D modalities."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We propose MOLEBLEND, a novel relation-level self-supervised learning method for unified molecular modeling that organically integrates 2D and 3D modalities in a fine-grained manner. By treating atom relations as the anchor, we blend different modalities into an integral input for pretraining, which overcomes the limitations of existing approaches that distinguish 2D and 3D modalities as independent signals. Extensive experimental results reveal that MOLEBLEND achieves state-of-the-art performance on a wide range of 2D and 3D benchmarks, demonstrating the superiority of fine-grained alignment of different modalities."
        },
        {
            "heading": "A EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "A.1 BASELINE RESULTS",
            "text": "The baseline results of GraphMVP Liu et al. (2021a), MoleSDE Liu et al. (2023), GraphCL You et al. (2020), GraphMAE Hou et al. (2022), GraphLoG (Xu et al., 2021), MGSSL (Zhang et al., 2021) are from their own paper. Results of AttrMask (Hu et al., 2020), ContextPred (Hu et al., 2020), InfoGraph Sun et al. (2020), MolCLR Wang et al. (2022b) are from MoleculeSDE Liu et al. (2023). Results of MoleBERT (Xia et al., 2023), 3D Infomax St\u00e4rk et al. (2022) are from MoleBERT. The results of GROVER Rong et al. (2020) are from Uni-Mol Zhou et al. (2023)."
        },
        {
            "heading": "A.2 MOLNET REGRESSION TASK",
            "text": "Table 6 presents the performance of different methods on three regression tasks of MoleculeNet. In all these tasks, MOLEBLEND achieves state-of-the-art performance, further substantiating the superiority of unified fine-grained molecular modeling."
        },
        {
            "heading": "A.3 ABLATIONS ON BLENDING RATIO",
            "text": "Table 7 presents ablations on the relation blending ratio, showing that model performance is robust to the random ratio of multinomial distribution. In these experiments, we trained all models for 200K steps, maintaining other settings unchanged (e.g., learning rate consistent), with the exception of the blending ratio.\nFurthermore, we have observed that a higher 3D distance ratio (referring to the bottom three rows in the table) sometimes performs better than lower ratio (top row of 4:4:2 ratio). This suggests that the inclusion of 3D information is potentially more important for enhancing the model\u2019s understanding of molecular properties. However, it is worth noting that the disparity in performance between these ratios is relatively minor."
        },
        {
            "heading": "B THEORETICAL ANALYSIS",
            "text": "In the following sections, we follow common notations(Cover & Thomas, 1991), using uppercase letters to represent random variables and lowercase letters to represent samples of the random variables."
        },
        {
            "heading": "B.1 MISSING PROOFS",
            "text": "Lemma B.1 (Chain rule of mutual information(Cover & Thomas, 1991)) I(X1, X2;Y ) = I(X1;Y ) + I(X2;Y |X1) (16)"
        },
        {
            "heading": "Proof",
            "text": "I(X1;Y ) + I(X2;Y |X1) = Ep(x1,y) [ log p(x1, y)\np(x1)p(y)\n] + Ep(x1,x2,y) [ log\np(x2, y|x1) p(x2|x1)p(y|x1) ] = Ep(x1,x2,y) [ log p(x1, y)\np(x1)p(y) p(x2, y|x1) p(x2|x1)p(y|x1) ] = Ep(x1,x2,y) [ log p(x1, y)p(x2, y, x1)\np(y)p(x2, x1)p(y, x1) ] = Ep(x1,x2,y) [ log p(x2, y, x1)\np(y)p(x2, x1)\n] = I(X1, X2;Y )\n(17)\nProposition B.1 (Mutual Information Decomposition) The blend-and-predict method is maximizing the lower bound of the mutual information target below, which can be further divided into two parts.\nI(A2;A1, B2) + I(B1;A1, B2)\n= 1\n2\n[ I(A1;B1) + I(A2;B2) + I(A1;B1|B2) + I(A2;B2|A1) ] +\n1\n2\n[ I(A1;A2) + I(B1;B2) + I(A1;A2|B2) + I(B1;B2|A1) ] (18) Proof Firstly, we provide the decomposition of first term in equation 18, i.e. I(A2;A1, B2). By using Lemma B.1 and letting X1 = A1, X2 = B2 and Y = A2, we have\nI(A2;A1, B2) = I(A1;A2) + I(A2;B2|A1). (19) Again use Lemma B.1 and let X1 = B2, X2 = A1 and Y = A2, then we have\nI(A2;A1, B2) = I(B2;A2) + I(A2;A1|B2). (20) From equation 19 and equation 20, we have\nI(A2;A1, B2) = 1\n2\n[ I(A1;A2) + I(A2;B2|A1) + I(B2;A2) + I(A2;A1|B2) ] . (21)\nSimilarly, we apply Lemma B.1 to decompose the second term in equation 18.\nI(B1;A1, B2) = 1\n2\n[ I(B1;A1) + I(B2;B2|A1) + I(B1;B2) + I(B1;A1|B2) ] . (22)"
        },
        {
            "heading": "End of proof.",
            "text": ""
        },
        {
            "heading": "B.2 MUTUAL INFORMATION AND SELF-SUPERVISED LEARNING TASKS",
            "text": "A core objective of machine learning is to learn effective data representations. Many methods attempt to To achieve this goal through maximizing mutual information (MI), e.g. InfoMax principle (Linsker, 1988) and information bottleneck principle (Tishby et al., 2000). Unfortunately, estimating MI is intractable in general (McAllester & Stratos, 2020). Therefore, many works resort to optimize the upper or lower bound of MI (Alemi et al., 2016; Poole et al., 2019; Ni et al., 2022)\nIn the field of self-supervised learning (SSL), there are two widely used methods for acquiring meaningful representations: contrastive methods and predictive (generative) methods. Recently, it has been discovered that these two methods are closely linked to the maximization of lower-bound mutual information (MI) targets. A summary of these relationships is presented below."
        },
        {
            "heading": "B.2.1 CONTRASTIVE OBJECTIVE",
            "text": "Contrastive learning (CL) (Chen et al., 2020a) learn representations that are similar between positive pairs while distinct between negative pairs. From the perspective of mutual information maximization, CL actually maximizes the mutual information between the representations of positive pairs. The InfoNCE loss (Oord et al., 2018; Kong et al., 2019) is given by:\nLInfoNCE = \u2212Ep(x,y) [ log\nf(x, y)\u2211 y\u0303\u2208Y\u0303 f(x, y\u0303)\n] (23)\nwhere (x, y) is a positive pair, Y\u0303 is the sample set containing the positive sample y and |Y\u0303| \u2212 1 negative samples of x, f(\u00b7, \u00b7) characterizes the similarity between the two input variables. (Oord et al., 2018) proved that minimizing the InfoNCE loss is maximizing a lower bound of the following mutual information: I(X;Y ) \u2265 log |Y\u0303| \u2212 LInfoNCE . (24) Denote v1 and v2 as two views of the input and h\u03b8 is the representation function. Define x = h\u03b8(v1) and y = h\u03b8(v2) as representations of the two views and the similarity function f(x, y) = exp(x\u22a4y), contrastive learning is optimizing the following InfoNCE loss (Arora et al., 2019)\nLCL = \u2212Ep(v1,v+2 ,v\u22122 ) [ log\nexp(h\u03b8(v1) Th\u03b8(v + 2 ))\nexp(h\u03b8(v1)Th\u03b8(v + 2 )) + \u2211 v\u22122 exp(h\u03b8(v1)Th\u03b8(v \u2212 2 ))\n] , (25)\nwhere v+2 is the positive sample, v \u2212 2 is negative samples. Accordingly, minimizing the CL loss is maximizing the lower bound of I(h\u03b8(v1), h\u03b8(v2)) w.r.t. the representation function."
        },
        {
            "heading": "B.2.2 PREDICTIVE OBJECTIVE (MASK-THEN-PREDICT)",
            "text": "The mask-then-predict task (Devlin et al., 2018) are revealed to maximize the mutual information between the representations of the context and the masked tokens (Kong et al., 2019). A lower bound of this MI can be derived in the form of a predictive loss:\nI(X;Y ) = H(Y )\u2212H(Y |X) \u2265 \u2212H(Y |X) = Ep(x,y) [ log p(y|x) ] \u2265 Ep(x,y) [ log q(y|x) ] .\n(26)\nThe last inequation holds by applying the Jensen inequation Ep(x,y) [ log q(y|x)p(y|x) ] \u2264\nlogEp(x,y) [ q(y|x) p(y|x) ] = 0.\nDenote x = h\u03b8(c) and y = h\u03b8(m) as representations of the context c and the masked token m to be predicted. q\u03d5 is the predictive model. This predictive objective Ep(c,m) [ log q\u03d5(h\u03b8(m)|h\u03b8(c)) ] corresponds to the training objective of a mask-then-predict task. Therefore, according to equation 26, mask-then-predict task maximizes the lower bound of the MI between representations of the context and the masked tokens, i.e.\nI(h\u03b8(C), h\u03b8(M)) \u2265 Ep(c,m) [ log q\u03d5(h\u03b8(m)|h\u03b8(c)) ] . (27)"
        },
        {
            "heading": "B.2.3 GENERATIVE OBJECTIVE",
            "text": "(Liu et al., 2022b) conducts cross-modal pretraining by generating representations of one modality from the other. Utilizing equation 26 and the symmetry of mutual information, we can derive a lower bound of MI in the form of a mutual generative loss:\nI(X;Y ) \u2265 1 2 Ep(x,y)\n[ log q(y|x) + log q(x|y) ] . (28)\nDenote v1 and v2 as two views of the input. h\u03b8 is the representation function and q\u03d5 is the predictive model. In equation 28, let x = h\u03b8(v1) and y = h\u03b8(v2), then we can derive that learning to generate the representation of one view from the other corresponds to maximize the lower bound of mutual information between the representations of the two views:\nI(h\u03b8(V1), h\u03b8(V2)) \u2265 1\n2 Ep(v1,v2)\n[ log q\u03d51(h\u03b8(v1)|h\u03b8(v2)) + log q\u03d52(h\u03b8(v2)|h\u03b8(v1)) ] . (29)"
        },
        {
            "heading": "B.2.4 MODALITY BLENDING",
            "text": "We next present an theoretical understanding of multimodal blend-then-predict. For simplicity, we consider two relations, denoted as R2D = (aij)n\u00d7n and R3D = (bij)n\u00d7n. Their elements are randomly partitioned into two parts by random partition variable S, represented as R2D = [A1, A2],R3D = [B1, B2], such that Ai shares identical elements indexes with Bi, i \u2208 {1, 2}. The blended matrix is denoted as R2D&3D = [A1, B2]. Our objective is to predict the two full modalities from the blended relations:\nmax \u03b8,\u03d51,\u03d52 ESEp(a1,a2,b1,b2)[log q\u03d51(h\u03b8(a2)|h\u03b8(a1), h\u03b8(b2)) + log q\u03d52(h\u03b8(b1)|h\u03b8(a1), h\u03b8(b2))], (30)\nwhere h\u03b8 is the representation extractor, q\u03d51 and q\u03d52 are predictive head that recovers R2D and R3D. Utilizing the result from equation 27, the blend-then-predict objective aims to maximize the lower bound of mutual information presented below:\nESI(h\u03b8(A2);h\u03b8(A1), h\u03b8(B2)) + I(h\u03b8(B1);h\u03b8(A1), h\u03b8(B2)). (31)\nFrom the mutual information decomposition in Proposition B.1, the objective in equation 31 can be divided into two parts.\nES{ 1 2 [I(A1;B1) + I(A2;B2)\ufe38 \ufe37\ufe37 \ufe38\ncontrastive and generative + I(A1;B1|B2) + I(A2;B2|A1)\ufe38 \ufe37\ufe37 \ufe38 conditional contrastive and generative ]\n+ 1 2 [I(A1;A2) + I(B1;B2)\ufe38 \ufe37\ufe37 \ufe38\nmask-then-predict\n+ I(A1;A2|B2) + I(B1;B2|A1)\ufe38 \ufe37\ufe37 \ufe38 multimodal mask-then-predict\n]} (32)\nThe first part of Equation 32 corresponds to existing (conditional) contrastive and generative methods, which aim to maximize the mutual information between two corresponding parts (Ai with Bi, i \u2208 {1, 2}) across two modalities . The second part represents the (multimodal) mask-then-predict objectives, focusing on maximizing the mutual information between the masked and the remaining parts within a single modality.\nThis decomposition demonstrates that our objective unifies contrastive, generative (inter-modality prediction), and mask-then-predict (intra-modality prediction) approaches within a single cohesive blend-then-predict framework, from the perspective of mutual information maximization. Moreover, this approach fosters enhanced cross-modal interaction by introducing an innovative multimodal mask-then-predict target."
        },
        {
            "heading": "C EXPERIMENTAL DETAILS",
            "text": ""
        },
        {
            "heading": "C.1 DATASETS DETAILS",
            "text": "MoleculeNet (Wu et al., 2017) 11 datasets are used to evaluate model performance on 2D tasks:\n\u2022 BBBP: The blood-brain barrier penetration dataset, aims at modeling and predicting the barrier permeability.\n\u2022 Tox21: This dataset (\u201cToxicology in the 21st Century\u201d) contains qualitative toxicity measurements for 8014 compounds on 12 different targets, including nuclear receptors and stress response pathways.\n\u2022 ToxCast: ToxCast is another data collection providing toxicology data for a large library of compounds based on in vitro high-throughput screening, including qualitative results of over 600 experiments on 8615 compounds.\n\u2022 SIDER: The Side Effect Resource (SIDER) is a database of marketed drugs and adverse drug reactions (ADR), grouped into 27 system organ classes.\n\u2022 ClinTox: The ClinTox dataset compares drugs approved by the FDA and drugs that have failed clinical trials for toxicity reasons. The dataset includes two classification tasks for 1491 drug compounds with known chemical structures: (1) clinical trial toxicity (or absence of toxicity) and (2) FDA approval status.\nQM9 (Ramakrishnan et al., 2014) QM9 is a quantum chemistry benchmark consisting of 134k stable small organic molecules, corresponding to the subset of all 133,885 species out of the GDB-17 chemical universe of 166 billion organic molecules. The molecules in QM9 contains up to 9 heavy atoms. Each molecule is associated with 12 targets covering its geometric, energetic, electronic, and thermodynamic properties, which are calculated by density functional theory (DFT)."
        },
        {
            "heading": "C.2 HYPERPARAMETERS",
            "text": "Hyperparameters for pretraining and finetuning on MoleculeNet and QM9 benchmarks are presented in Table 8, Table 9 and Table 10, repectively."
        },
        {
            "heading": "D ABLATION STUDIES",
            "text": "D.1 2D TASKS WITH 3D INFORMATION\nSince our model is pretrained to predict both 2D and 3D information, for 2D tasks, we consider utilizing the 3D information predicted by our model as supplementary information (2D + 3D in Table 11). We observe that both settings achieve comparable performance across various tasks. This may be due to the 2D and 3D spaces have been well aligned and 3D knowledge is implicit injected into the model, allowing it to achieve satisfactory results even with only 2D information provided."
        }
    ],
    "year": 2023
}