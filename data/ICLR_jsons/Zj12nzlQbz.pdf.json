{
    "abstractText": "Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs. Previous efforts in detecting hallucinations have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, where the semantic information is inevitably lost during the tokendecoding procedure. Thus, we propose to explore the dense semantic information retained within LLMs\u2019 INternal States for hallucInation DEtection (INSIDE). In particular, a simple yet effective EigenScore metric is proposed to better evaluate responses\u2019 self-consistency, which exploits the eigenvalues of responses\u2019 covariance matrix to measure the semantic consistency/diversity in the dense embedding space. Furthermore, from the perspective of self-consistent hallucination detection, a test time feature clipping approach is explored to truncate extreme activations in the internal states, which reduces overconfident generations and potentially benefits the detection of overconfident hallucinations. Extensive experiments and ablation studies are performed on several popular LLMs and questionanswering (QA) benchmarks, showing the effectiveness of our proposal.",
    "authors": [],
    "id": "SP:e785fe11553b2a8cb34a283f2299fe364951f6ed",
    "references": [
        {
            "authors": [
                "Ebtesam Almazrouei",
                "Hamza Alobeidli",
                "Abdulaziz Alshamsi",
                "Alessandro Cappelli",
                "Ruxandra Cojocaru",
                "Maitha Alhammadi",
                "Mazzotta Daniele",
                "Daniel Heslow",
                "Julien Launay",
                "Quentin Malartic"
            ],
            "title": "The falcon series of language models: Towards open frontier models",
            "venue": "Hugging Face repository,",
            "year": 2023
        },
        {
            "authors": [
                "Amos Azaria",
                "Tom Mitchell"
            ],
            "title": "The internal state of an llm knows when its lying",
            "venue": "arXiv preprint arXiv:2304.13734,",
            "year": 2023
        },
        {
            "authors": [
                "Yuntao Bai",
                "Andy Jones",
                "Kamal Ndousse",
                "Amanda Askell",
                "Anna Chen",
                "Nova DasSarma",
                "Dawn Drain",
                "Stanislav Fort",
                "Deep Ganguli",
                "Tom Henighan"
            ],
            "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "venue": "arXiv preprint arXiv:2204.05862,",
            "year": 2022
        },
        {
            "authors": [
                "T Tony Cai",
                "Tengyuan Liang",
                "Harrison H Zhou"
            ],
            "title": "Law of log determinant of sample covariance matrix and optimal estimation of differential entropy for high-dimensional gaussian distributions",
            "venue": "Journal of Multivariate Analysis,",
            "year": 2015
        },
        {
            "authors": [
                "Yupeng Chang",
                "Xu Wang",
                "Jindong Wang",
                "Yuan Wu",
                "Kaijie Zhu",
                "Hao Chen",
                "Linyi Yang",
                "Xiaoyuan Yi",
                "Cunxiang Wang",
                "Yidong Wang"
            ],
            "title": "A survey on evaluation of large language models",
            "venue": "arXiv preprint arXiv:2307.03109,",
            "year": 2023
        },
        {
            "authors": [
                "Roi Cohen",
                "May Hamri",
                "Mor Geva",
                "Amir Globerson"
            ],
            "title": "Lm vs lm: Detecting factual errors via cross examination",
            "venue": "arXiv e-prints, pp",
            "year": 2023
        },
        {
            "authors": [
                "Andrija Djurisic",
                "Nebojsa Bozanic",
                "Arjun Ashok",
                "Rosanne Liu"
            ],
            "title": "Extremely simple activation shaping for out-of-distribution detection",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Jinhao Duan",
                "Hao Cheng",
                "Shiqi Wang",
                "Chenan Wang",
                "Alex Zavalny",
                "Renjing Xu",
                "Bhavya Kailkhura",
                "Kaidi Xu"
            ],
            "title": "Shifting attention to relevance: Towards the uncertainty estimation of large language models",
            "venue": "arXiv preprint arXiv:2307.01379,",
            "year": 2023
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "Yuheng Huang",
                "Jiayang Song",
                "Zhijie Wang",
                "Huaming Chen",
                "Lei Ma"
            ],
            "title": "Look before you leap: An exploratory study of uncertainty measurement for large language models",
            "venue": "arXiv e-prints, pp",
            "year": 2023
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung"
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Computing Surveys,",
            "year": 2023
        },
        {
            "authors": [
                "Mandar Joshi",
                "Eunsol Choi",
                "Daniel S Weld",
                "Luke Zettlemoyer"
            ],
            "title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
            "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Saurav Kadavath",
                "Tom Conerly",
                "Amanda Askell",
                "Tom Henighan",
                "Dawn Drain",
                "Ethan Perez",
                "Nicholas Schiefer",
                "Zac Hatfield Dodds",
                "Nova DasSarma",
                "Eli Tran-Johnson"
            ],
            "title": "Language models (mostly) know what they know",
            "venue": "arXiv e-prints, pp",
            "year": 2022
        },
        {
            "authors": [
                "George Klir",
                "Mark Wierman"
            ],
            "title": "Uncertainty-based information: elements of generalized information theory, volume 15",
            "venue": "Springer Science & Business Media,",
            "year": 1999
        },
        {
            "authors": [
                "Lorenz Kuhn",
                "Yarin Gal",
                "Sebastian Farquhar"
            ],
            "title": "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Alex Kulesza",
                "Ben Taskar"
            ],
            "title": "k-dpps: Fixed-size determinantal point processes",
            "venue": "In Proceedings of the 28th International Conference on Machine Learning",
            "year": 2011
        },
        {
            "authors": [
                "Tom Kwiatkowski",
                "Jennimaria Palomaki",
                "Olivia Redfield",
                "Michael Collins",
                "Ankur Parikh",
                "Chris Alberti",
                "Danielle Epstein",
                "Illia Polosukhin",
                "Jacob Devlin",
                "Kenton Lee"
            ],
            "title": "Natural questions: a benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Kenneth Li",
                "Oam Patel",
                "Fernanda Vi\u00e9gas",
                "Hanspeter Pfister",
                "Martin Wattenberg"
            ],
            "title": "Inference-time intervention: Eliciting truthful answers from a language model",
            "venue": "arXiv preprint arXiv:2306.03341,",
            "year": 2023
        },
        {
            "authors": [
                "Percy Liang",
                "Rishi Bommasani",
                "Tony Lee",
                "Dimitris Tsipras",
                "Dilara Soylu",
                "Michihiro Yasunaga",
                "Yian Zhang",
                "Deepak Narayanan",
                "Yuhuai Wu",
                "Ananya Kumar"
            ],
            "title": "Holistic evaluation of language models",
            "venue": "arXiv preprint arXiv:2211.09110,",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yew Lin"
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "In Text summarization branches out,",
            "year": 2004
        },
        {
            "authors": [
                "Zhen Lin",
                "Shubhendu Trivedi",
                "Jimeng Sun"
            ],
            "title": "Generating with confidence: Uncertainty quantification for black-box large language models",
            "venue": "arXiv e-prints, pp",
            "year": 2023
        },
        {
            "authors": [
                "Zi Lin",
                "Jeremiah Zhe Liu",
                "Jingbo Shang"
            ],
            "title": "Towards collaborative neural-symbolic graph semantic parsing via uncertainty",
            "venue": "Findings of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Weitang Liu",
                "Xiaoyun Wang",
                "John Owens",
                "Yixuan Li"
            ],
            "title": "Energy-based out-of-distribution detection",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Andrey Malinin",
                "Mark Gales"
            ],
            "title": "Uncertainty estimation in autoregressive structured prediction",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Potsawee Manakul",
                "Adian Liusie",
                "Mark JF Gales"
            ],
            "title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
            "venue": "arXiv preprint arXiv:2303.08896,",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Friedrich Pukelsheim"
            ],
            "title": "The three sigma rule",
            "venue": "The American Statistician,",
            "year": 1994
        },
        {
            "authors": [
                "Harsh Raj",
                "Vipul Gupta",
                "Domenic Rosati",
                "Subhabrata Majumdar"
            ],
            "title": "Semantic consistency for assuring reliability of large language models",
            "venue": "arXiv preprint arXiv:2308.09138,",
            "year": 2023
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang"
            ],
            "title": "Squad: 100,000+ questions for machine comprehension of text",
            "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2016
        },
        {
            "authors": [
                "Siva Reddy",
                "Danqi Chen",
                "Christopher D Manning"
            ],
            "title": "Coqa: A conversational question answering challenge",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych"
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bertnetworks",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP). Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Jie Ren",
                "Jiaming Luo",
                "Yao Zhao",
                "Kundan Krishna",
                "Mohammad Saleh",
                "Balaji Lakshminarayanan",
                "Peter J Liu"
            ],
            "title": "Out-of-distribution detection and selective generation for conditional language models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Freda Shi",
                "Daniel Fried",
                "Marjan Ghazvininejad",
                "Luke Zettlemoyer",
                "Sida I Wang"
            ],
            "title": "Natural language to code translation with execution",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Yiyou Sun",
                "Chuan Guo",
                "Yixuan Li"
            ],
            "title": "React: Out-of-distribution detection with rectified activations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "arXiv preprint arXiv:2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou"
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2203.11171,",
            "year": 2022
        },
        {
            "authors": [
                "Svante Wold",
                "Kim Esbensen",
                "Paul Geladi"
            ],
            "title": "Principal component analysis",
            "venue": "Chemometrics and intelligent laboratory systems,",
            "year": 1987
        },
        {
            "authors": [
                "Xinyi Xu",
                "Zhaoxuan Wu",
                "Chuan Sheng Foo",
                "Bryan Kian Hsiang Low"
            ],
            "title": "Validation free and replication robust volume-based data valuation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zhangyue Yin",
                "Qiushi Sun",
                "Qipeng Guo",
                "Jiawen Wu",
                "Xipeng Qiu",
                "Xuanjing Huang"
            ],
            "title": "Do large language models know what they don\u2019t know",
            "venue": "arXiv preprint arXiv:2305.18153,",
            "year": 2023
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models",
            "venue": "arXiv preprint arXiv:2205.01068,",
            "year": 2022
        },
        {
            "authors": [
                "Yue Zhang",
                "Yafu Li",
                "Leyang Cui",
                "Deng Cai",
                "Lemao Liu",
                "Tingchen Fu",
                "Xinting Huang",
                "Enbo Zhao",
                "Yu Zhang",
                "Yulong Chen"
            ],
            "title": "Siren\u2019s song in the ai ocean: A survey on hallucination in large language models",
            "venue": "arXiv preprint arXiv:2309.01219,",
            "year": 2023
        },
        {
            "authors": [
                "Kaitlyn Zhou",
                "Dan Jurafsky",
                "Tatsunori Hashimoto"
            ],
            "title": "Navigating the grey area: Expressions of overconfidence and uncertainty in language models",
            "venue": "arXiv preprint arXiv:2302.13439,",
            "year": 2023
        },
        {
            "authors": [
                "Zhanghao Zhouyin",
                "Ding Liu"
            ],
            "title": "Understanding neural networks with logarithm determinant entropy estimator",
            "venue": "arXiv preprint arXiv:2105.03705,",
            "year": 2021
        },
        {
            "authors": [
                "ITI Li"
            ],
            "title": "2023), which trains multiple binary classifiers with the internal embeddings for hallucination detection, we report the best performance",
            "year": 2023
        },
        {
            "authors": [
                "Duan"
            ],
            "title": "For the implementation of SelfCheckGPT, we leverage the SelfCheckBERTScore provided in the official code package 7. The comparison results in Table 5 demonstrate that our EigenScore significantly outperforms the competitors. Additionally, both SentSAR and SelfCheckGPT exhibit comparable performance, which is much superior to Semantic Entropy",
            "year": 2023
        },
        {
            "authors": [
                "Huang"
            ],
            "title": "In order to facilitate the comparison of our work\u2019s performance",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Large Language Models (LLMs) have recently achieved a milestone breakthrough and demonstrated impressive abilities in various applications (Ouyang et al., 2022; OpenAI, 2023). However, it has been widely observed that even the state-of-the-art LLMs often make factually incorrect or nonsense generations (Cohen et al., 2023; Ren et al., 2022; Kuhn et al., 2022), which is also known as knowledge hallucination (Ji et al., 2023). The potentially unreliable generations make it risky to deploy LLMs in practical scenarios. Therefore, hallucination detection, that is, accurately detecting and rejecting responses when hallucinations occur in LLMs, has attracted more and more attention from the academic community (Azaria & Mitchell, 2023; Ren et al., 2022; Kuhn et al., 2022).\nThe token-level uncertainty estimation (e.g., predictive confidence or entropy) has shown its efficacy in hallucination detection on conventional NLP tasks (Malinin & Gales, 2020; Huang et al., 2023). However, how to derive the sentence-level uncertainty from the token-level remains a challenge, especially for modern auto-regressive LLMs whose response contents are generally diverse and sophisticated (Malinin & Gales, 2020; Kuhn et al., 2022; Duan et al., 2023). Thus, to avoid complicated token-to-sentence uncertainty derivation, researchers propose to evaluate the sentence uncertainty by the output languages directly (Kadavath et al., 2022; Yin et al., 2023; Zhou et al., 2023). Among the recent advancements, prompting LLMs to generate multiple responses to the same question and evaluating the self-consistency of those responses has been proven effective in hallucination detection (Wang et al., 2022; Shi et al., 2022). However, such a post-hoc semantic measurement on decoded language sentences is inferior to precisely modeling the logical consistency/divergence Manakul et al. (2023); Zhang et al. (2023).\nHence, instead of logit-level or language-level uncertainty estimation, this paper proposes to leverage the internal states of LLMs to conduct hallucination detection. The motivation is intuitive: LLMs preserve the highly-concentrated semantic information of the entire sentence within their internal states (Azaria & Mitchell, 2023), allowing for the direct detection of hallucinated responses in the sentence embedding space.\nIn particular, with the generalized framework of INternal States for hallucInation DEtection (INSIDE), this paper performs hallucination detection from two perspectives. First, skipping sec-\nondary semantic extraction via extra models, we directly measure the self-consistency/divergence of the output sentences using internal states of LLMs. In order to explore semantic consistency in the embedding space, Section 3.1 introduces an EigenScore metric regarding the eigenvalues of sentence embeddings\u2019 covariance matrix. Second, to handle the self-consistent (overconfident) hallucinations, we propose to rectify abnormal activations of the internal states. Specifically, Section 3.2 develops a feature clipping approach to truncate extreme features, which tends to prevent overconfident generations during the auto-regressive procedure. In Section 4, the effectiveness of our method is validated through extensive experiments on several well-established QA benchmarks.\nThe main contributions of our work are as follows:\n\u2022 We propose a generalized INSIDE framework that leverages the internal states of LLMs to perform hallucination detection.\n\u2022 We develop an EigenScore metric to measure the semantic consistency in the embedding space, and demonstrate that the proposed EigenScore represents the differential entropy in the sentence embedding space.\n\u2022 A test time feature clipping approach is introduced to truncate extreme activations in the feature space, which implicitly reduces overconfident generations and helps identify the overconfident hallucinations.\n\u2022 We achieve state-of-the-art hallucination detection performance on several QA benchmarks, and conduct extensive ablation studies to verify the efficacy of our method."
        },
        {
            "heading": "2 BACKGROUND ON HALLUCINATION DETECTION",
            "text": "In this work, we mainly focus on the knowledge hallucination detection of natural language generation based on LLMs, especially for Q&A task (Reddy et al., 2019; Kwiatkowski et al., 2019). Given an input context x, a typical LLM (Zhang et al., 2022; Touvron et al., 2023a) parameterized with \u03b8 is able to generate output sequences in autoregressive manner yt = f(x, y1, y2, \u00b7 \u00b7 \u00b7 , yt\u22121|\u03b8), where y = [y1, y2, \u00b7 \u00b7 \u00b7 , yT ] denotes the output sequence and yt denotes the t-th output token. We denote p(yt|y<t,x) the Maximum Softmax Probability (MSP) of t-th token. For a traditional classification model, the MSP measures the confidence level of the classification result and has been widely used as an uncertainty measure of predictions (Hendrycks & Gimpel, 2016). Therefore, for sequence generation task, a straightforward sequence uncertainty can be defined as the joint probability of different tokens, which is known as Perplexity (Ren et al., 2022),\nP (y|x,\u03b8) = \u2212 1 T log \u220f t p(yt|y<t,x) = \u2212 1 T \u2211 t log p(yt|y<t,x) (1)\nAs shorter sequences generally have lower perplexity, the length of the output sequence T is utilized to normalize the joint probability. Since different tokens contribute differently to the semantics of the sentence (Raj et al., 2023; Duan et al., 2023), the perplexity defined by averaging token-level uncertainty cannot effectively capture the uncertainty of the entire sequence. It has been demonstrated that utilizing multiple generations for one input is beneficial to estimate the sequence-level uncertainty (Malinin & Gales, 2020; Kuhn et al., 2022; Manakul et al., 2023). We denote Y = [y1,y2, \u00b7 \u00b7 \u00b7 ,yK ] as K generated responses for input context x. For a given LLM, multiple responses could be easily obtained by the top-p/top-k sampling strategy during inference time (Touvron et al., 2023a; Kadavath et al., 2022). In Malinin & Gales (2020), the Length Normalized Entropy is proposed to measure the sequence-level uncertainty by making use of multiple generations, which is defined as\nH(Y|x,\u03b8) = \u2212Ey\u2208Y 1\nTy \u2211 t log p(yt|y<t,x) (2)\nWhen a model is uncertain about its response, it generates hallucination context, resulting in an answer distribution with a high entropy (Kadavath et al., 2022). It has been shown that the lengthnormalized entropy performs better than the non-normalized one (Lin et al., 2023).\nIn addition to the predictive uncertainty or entropy, the semantic consistency (Lin et al., 2023; Raj et al., 2023) among multiple responses has also been widely explored to measure the hallucination degree of LLMs, which hypothesis that the LLMs are expected to generate similar outputs if they know the input context and they are sure about the answers (Wang et al., 2022; Manakul et al., 2023). An intuitive semantic consistency metric isLexical Similarity (Lin et al., 2022; 2023), which explores the average similarity across multiple answers as consistency measure\nS(Y jx ; ) = 1 C\nKX\ni =1\nKX\nj = i +1\nsim (y i ; y j ) (3)\nwhereC = K (K 1)=2 andsim ( ; ) is the similarity de ned by Rouge-L Lin (2004).\n3 METHOD\nIn this section, we introduce the details of our proposed INSIDE framework for hallucination detection. The whole pipeline is illustrated as Fig. 1. In section 3.1, we demonstrate a simple but effective EigenScore metric by exploring sentence-level semantics in the internal states of LLMs. In section 3.2, a test-time feature clipping approach is introduced to effectively alleviate the issue of overcon dent generation, thereby aiding in the identi cation of self-consistent hallucinations\n3.1 HALLUCINATION DETECTION BY EIGENSCORE\nThe existing uncertainty or consistency based hallucination detection metrics are exploited in the logit or language space, which neglect the dense semantic information that is retained within the internal states of LLMs. To better exploit the dense semantic information, we propose to measure the semantic divergence in the sentence embedding space. For thet-th output tokenyt , we denote the hidden embedding in thel-th layer ash lt 2 R\nd, whered is the dimension of the hidden embedding (d = 4096 for LLaMA-7B andd = 5120 for LLaMA-13B). According to Ren et al. (2022); Azaria & Mitchell (2023), the sentence embedding can be obtained by averaging the token embedding z = 1T P T t =1 h t , or taking the last token embedding as sentence embeddingz = hT . In our main experiments, we use the embedding of the last token in the middle layer as the sentence embedding, as it effectively captures the sentence semantic (Azaria & Mitchell, 2023). The comparison results of using different sentence embeddings are demonstrated in the ablation studies 4.3. ForK generated sequences, the covariance matrix ofK sentence embeddings can be computed as\n= Z> Jd Z (4)\nwhere 2 RK K represents the covariance matrix that captures the relationship between different sentences in the embedding space,Z = [ z1; z2; ; zK ] 2 Rd K represents the embedding matrix\nof different sentences, Jd = Id\u2212 1d1K1 \u22a4 K is the centering matrix and 1K \u2208 RK is the all-one column vector. Then, the proposed EigenScore can be defined as the logarithm determinant (LogDet) of the covariance matrix,\nE(Y|x,\u03b8) = 1 K log det(\u03a3+ \u03b1 \u00b7 IK) (5)\nHere, det(X) represents the determinant of matrix X, and a small regularization term \u03b1 \u00b7IK is added to the covariance matrix to explicitly make it full rank. Since the matrix determinant can be obtained by solving the eigenvalues, the EigenScore can be computed as\nE(Y|x,\u03b8) = 1 K log( \u220f i \u03bbi) = 1 K K\u2211 i log(\u03bbi) (6)\nwhere \u03bb = {\u03bb1, \u03bb2, \u00b7 \u00b7 \u00b7 , \u03bbK} denotes the eigenvalues of the regularized covariance matrix \u03a3+\u03b1 \u00b7I, which can be solved by Singular Value Decomposition (SVD). Eq. 6 shows that the hallucination degree of LLM\u2019s generation can be measured by the average logarithm of the eigenvalues. The conclusion is intuitive, as the eigenvalues of covariance matrix capture the divergence and correlation relationship between embeddings of different sentences. When the LLM is confident to the answers and K generations have similar semantic, the sentence embeddings will be highly correlated and most eigenvalues will be close to 0. On the contrary, when the LLM is indecisive and hallucinating contents, the model will generate multiple sentences with diverse semantics leading to more significant eigenvalues. The following remark is also provided to explain why the proposed EigenScore is a good measure of knowledge hallucination.\nRemark 1. LogDet of covariance matrix represents the differential entropy in the sentence embedding space. Differential Entropy is the natural extension of discrete Shannon Entropy He(X) = \u2212 \u2211 X \u2212p(x) log p(x). The differential entropy Hde(X) in continuous space can be defined by replacing the probability function with its density function f(x) and integrating over x, i.e., Hde(X) = \u2212 \u222b x f(x) log f(x)dx. In principle (Zhouyin & Liu, 2021), for a multivariate Gaussian distribution X \u223c N(\u00b5,\u03a3), the differential entropy can be represented as\nHde(X) = 1\n2 log det(\u03a3) +\nd 2 (log 2\u03c0 + 1) = 1 2 d\u2211 i=1 log \u03bbi + C (7)\nwhere d is the dimension of variables and C is a constant. Therefore, the differential entropy is determined by the eigenvalues (LogDet) of the covariance matrix.\nAccording to Remark 1, the proposed EigenScore defined by Eq. 6 represents the differential entropy in the sentence embedding space, which offers valuable insight into using EigenScore as a semantic divergence measure. Compared to existing uncertainty or consistency metrics that obtained in logit or language space (Malinin & Gales, 2020; Huang et al., 2023; Lin et al., 2022), the advantages of EigenScore are: (1) It captures the semantic divergence (entropy) in the dense embedding space, which is expected to retain highly-concentrated semantic information compared to logits or languages (Reimers & Gurevych, 2019). (2) Representing semantic divergence in embedding space can effectively solve the semantic equivalence (linguistic invariances) problem (Kuhn et al., 2022) in natural language space. (3) Fine-grained semantic relationship among different responses can be exploited by using eigenvalues of covariance matrix. Therefore, through the exploration of dense semantic information in the internal states, the EigenScore is expected to outperform existing uncertainty and consistency metrics, resulting in improved hallucination detection performance."
        },
        {
            "heading": "3.2 TEST TIME FEATURE CLIPPING",
            "text": "Recent works have shown that the LLMs are subject to the risks of self-consistent (overconfident) hallucinations (Ren et al., 2022; Ji et al., 2023), which has not been considered by existing consistency based methods. Therefore, to address those common failure cases caused by overconfident generation, a test time feature clipping approach is introduced during the computation of EigenScore. As shown in Figure. 2, we illustrate the activation distribution in the penultimate layer of\nLLaMA-7B. An intuitive observation is that the penultimate layer of LLMs tends to exhibit numerous extreme features, consequently increasing the likelihood of generating overconfident and self-consistent generations. Inspired by prior works that rectify internal activations to reduce overconfident prediction for Out-of-Distribution (OOD) detection (Sun et al., 2021; Djurisic et al., 2022), we introduce a test time feature clipping (FC) method to prevent LLMs generate overconfident hallucinations. To rectify those extreme features, the FC operation is defined as the following piecewise function\nFC(h) =  hmin, h < hmin h, hmin \u2264 h \u2264 hmax hmax h > hmax\n(8)\nwhere h represents the feature of the hidden embeddings in the penultimate layer of the LLMs, hmin and hmax are two thresholds for determining the minimum and maximum truncation activations. When hmin = \u2212\u221e and hmax = +\u221e, the output feature embedding is equivalent to the original output. For the determination of the optimal truncation thresholds, a memory bank which dynamically pushes and pops element in it, is utilized to conserve N token embeddings during test time. Then, for each hidden neuron, the thresholds hmin and hmax are set to the top and bottom p-th percentiles of the features in the memory bank. Refer to the three-sigma-rule Pukelsheim (1994), we set p = 0.2 in all cases. This implies that the activations falling within the largest and smallest top 0.2% in the memory bank are identified as abnormal features and subsequently truncated for reducing overconfident generation."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUP",
            "text": "Datasets. We utilize four widely used question answering (QA) datasets for evaluation, including two open-book conversational QA datasets CoQA (Reddy et al., 2019) and SQuAD (Rajpurkar et al., 2016), as well as two closed-book QA datasets TriviaQA (Joshi et al., 2017) and Natural Questions (NQ) (Kwiatkowski et al., 2019). We follow Lin et al. (2023) to utilize the development split of CoQA with 7983 QA pairs, the validation split of NQ with 3610 QA pairs and the validation split of the TriviaQA (rc.nocontext subset) with 9,960 deduplicated QA pairs. For the SQuAD dataset, we filter out the QA pairs with their flag is impossible = True, and utilize the subset of the developmentv2.0 split with 5928 QA pairs. The lengths of the sequences vary in the four datasets. Specifically, the ground truth answers in CoQA and SQuAD are relatively longer, while and TriviaQA typically consists of answers that are only with one or two words.\nModels. We use two representative open source LLMs, including LLaMA (Touvron et al., 2023a) and OPT (Zhang et al., 2022) in our experiments. Specifically, we consider off-the-shelf LLaMA-7B\n1, LLaMA-13B 2, OPT-6.7B 3 and their corresponding tokenizer provided by Hugging Face 4. We use the pre-trained wights and do not finetune these models in all cases.\nEvaluation Metrics. Following prior work Kuhn et al. (2022); Ren et al. (2022), we evaluate the hallucination detection ability of different methods by employing them to determine whether the generation is correct or not. Therefore, the area under the receiver operator characteristic curve (AUROC) and Pearson Correlation Coefficient (PCC) are utilized as the performance measure. AUROC is a popular metric to evaluate the quality of a binary classifier and uncertainty measure (Ren et al., 2022; Lin et al., 2023). Higher AUROC scores are better. PCC is utilized to measure the correlation between the hallucination detection metric and the correctness measure, which is usually defined as the ROUGE score (Lin, 2004) or semantic similarity (Reimers & Gurevych, 2019) between the generated answers and ground truth answers. A higher PCC score is better.\nBaselines. We compare our proposal with the most popular uncertainty-based methods Perplexity Ren et al. (2022) and Length-normalized Entropy (LN-Entropy) Malinin & Gales (2020), and the consistency-based metric Lexical Similarity (Lin et al., 2022). Besides, in order to investigate whether traditional OOD detection methods can be used for hallucination detection, we also introduce a popular OOD detection method Energy score (Liu et al., 2020) as a comparison method.\nCorrectness Measure. We follow Kuhn et al. (2022); Lin et al. (2023) to utilize both the ROUGEL (Lin, 2004) and the semantic similarity (Reimers & Gurevych, 2019) as the correctness measure. ROUGE-L 5 is an n-gram based metric that computes the longest common subsequence between two pieces of text. The generation is regarded as correct when the ROUGE-L (f-measure) is large than a given threshold, which we set to 0.5 in our main experiments. Besides, we also use the embedding similarity as the correctness measure. The sentence embeddings of model generation and the ground truth answer are extracted by the nli-roberta-large model 6, and the generation is regarded as true when the cosine similarity between two embeddings is larger than 0.9.\nImplementation Details. Implementation of this work is based on pytorch and transformers libraries. For the hyperparameters that are used for sampling strategies of LLMs\u2019 decoder, we set temperature to 0.5, top-p to 0.99 and top-k to 5 through the experiments. The number of generations is set to K = 10. For the sentence embedding used in our proposal, we use the last token embedding of the sentence in the middle layer, i.e., the layer index is set to int(L/2). For the regularization term of the covariance matrix, we set \u03b1 = 0.001. For the memory bank used to conserve token embeddings, we set N = 3000. When implement the Energy Score, we average the token-level energy score as the sentence-level energy score."
        },
        {
            "heading": "4.2 MAIN RESULTS",
            "text": "Effectiveness of EigenScore. In Table. 1, we compare our proposed EigenScore with several representative reliability evaluation methods on three LLMs and four QA datasets. The results show that: (1) In both LLaMA and OPT models, our proposed EigenScore consistently outperforms other comparison methods by a large margin in CoQA, SQuAD and NQ datasets under different evaluation metrics. In particular, the EigenScore outperforms Lexical Similarity by 5.6% in CoQA and 8.9% in SQuAD with AUROC metric at most. (2) It\u2019s interesting to see that the Perplexity performs best in TriviaQA dataset but performs poorly on other datasets, especially for CoQA and SQuAD. This is because the generations and ground truth answers on TrivaiQA dataset is very simple, with only one or two words in the most cases. Therefore, the performance of different methods in TriviaQA is close and by simply averaging the token-level confidence as uncertainty measure performs well. (3) On average, the performance in LLaMA-13B is better than that in LLaMA-7B and OPT-6.7B, while the performances in LLaMA-7B is slightly better than that in OPT-6.7B. It demonstrates that better hallucination detection performance can be achieved with a more powerful pre-trained LLM.\n1https://huggingface.co/decapoda-research/llama-7b-hf 2https://huggingface.co/decapoda-research/llama-13b-hf 3https://huggingface.co/facebook/opt-6.7b 4https://huggingface.co/ 5https://github.com/google-research/google-research/tree/master/rouge 6https://huggingface.co/sentence-transformers/nli-roberta-large\nEffectiveness of Feature Clipping. To demonstrate the effectiveness of the introduced test-time feature clipping, we compare the hallucination detection performance of different methods with and without applying the feature clipping technique. The results are shown in Table 2. As can be seen, the introduced feature clipping consistently improves the performance of different methods, with the largest improvement being 1.8% in AUROC."
        },
        {
            "heading": "4.3 ABLATION STUDIES",
            "text": "Number of Generations. For the methods that explore semantic consistency for hallucination detection, the number of generations K is a key factor to the performance. Therefore, to evaluate the impact of the number of generations, we select K from {5, 10, 15, 20, 30, 40} and perform experiments with LLaMA-7B and the NQ dataset. The performance in Figure 3(a) shows that: (1) Our proposed EigenScore consistently outperforms LN-Entropy and Lexical Similarity by a large margin for different K. (2) When K < 15, the performance of different methods increases as K increases and when K > 15, the performance tends to remain stable. The results suggeste that setting K to 20 provides the optimal trade-off between performance and inference cost. (3) Compared to EigenScore and Lexical Similarity, LN-Entropy is less sensitive to the number of generations, which demonstrates that Lexical Similarity and our EigenScore are more effective at utilizing the information in different generations.\nHow EigenScore Performs with Different Sentence Embeddings. In the main experiments, we employ the embedding of the last token in the middle layer as sentence embedding. Here, we also investigate how the model performs with different sentence embeddings. In Figure. 3(b), we show the hallucination detection performance by using sentence embedding from different layers. The results show that using the sentence embedding in the shallow and final layers yields significantly inferior performance compared to using sentence embedding in the layers close to the middle. Besides, another interesting observation is that utilizing the embedding of the last token as the sentence embedding achieves superior performance compared to simply averaging the token embeddings, which suggests that the last token of the middle layers retain more information about the truthfulness of generations.\nSensitivity to Correctness Measures. It\u2019s difficult to develop automatic metrics for QA task that correlate well with human evaluations. Therefore, the choice of correctness measures is a crucial component of hallucination detection evaluation. In this section, we evaluate the performance with different correctness measure thresholds in LLaMA-7B and CoQA dataset. The experimental results are presented in Table. 3. It shows that the threshold has a great influence on the final hallucination detection performance. Significantly, our proposed EigenScore consistently outperforms comparison methods in different thresholds. Besides, the results also indicate that the hallucination detection performance of different methods will be better under a rigorous correctness measure.\nSensitivity to Hyperparameters. The hyperparameters, including temperature, top-k and top-p, of the LLMs\u2019 decoder determine the diversity of the generations. To evaluate the impact of those hyperparameters. We provide a sensitivity analysis in Figure 4. As observed, the performance is greatly influenced by temperature but shows little sensitivity to top-k. The performance of the consistency based methods (EigenScore and Lexical Similarity) drops significantly when the temperature is greater than 1. The optimal temperature can be seletced from [0.1, 1.0]."
        },
        {
            "heading": "5 RELATED WORK",
            "text": "Reliability Evaluation of LLMs During real-world deployments, the reliability of LLMs poses a substantial challenge, as LLMs reveal their propensity to exhibit unreliable generations (Ji et al., 2023; Zhang et al., 2023). Therefore, considerable efforts has been made to address the security and reliability evaluation of LLMs (Huang et al., 2023; Malinin & Gales, 2020; Kuhn et al., 2022; Kadavath et al., 2022; Cohen et al., 2023; Azaria & Mitchell, 2023). Among those methods, uncertainty based metric has been widely explored, which typically involves predictive confidence or entropy of the output token (Malinin & Gales, 2020; Kuhn et al., 2022; Duan et al., 2023). Besides, consistency based methods also play an important role in reliability evaluation, which hypothesizes that LLMs tend to generate logically inconsistent responses to the same question when they are indecisive and hallucinating contents (Kuhn et al., 2022; Raj et al., 2023; Manakul et al., 2023). Based on the consistency hypothesis, researchers also found it is feasiabe to prompt the LLMs to evaluate their responses themselves (Kadavath et al., 2022; Cohen et al., 2023; Manakul et al., 2023).\nEigenvalue as Divergence Measure The eigenvalue or determinant of covariance matrix captures the variability of the data and has been widely explored as divergence measure in a wide range of machine learning tasks (Wold et al., 1987; Kulesza & Taskar, 2011; Xu et al., 2021; Zhouyin & Liu, 2021; Cai et al., 2015). For instance, in Wold et al. (1987), the authors proposed the well-known Principal Components Analysis (PCA) and demonstrates that the most largest eigenvalues of sample covariance matrix corresponds to the principle semantic of sample set. Besides, the determinant of covariance matrix, determined by the eigenvalues, has been utilized to sample a diversity subset in determinantal point processes (DDP) (Kulesza & Taskar, 2011) and activation learning (Xu et al., 2021) tasks, which demonstrates the determinant of covariance matrix is a good diversity measure. Besides, several studies also proposed to approximate the differential entropy with the logarithm determinant of covariance matrix (Zhouyin & Liu, 2021; Klir & Wierman, 1999)."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "Measuring the hallucination degree of LLM\u2019s generation is of critical importance in enhancing the security and reliability of LLM-based AI systems. This work presents an INSIDE framework to exploit the semantic information that are retained within the internal states of LLMs for hallucination detection. Specifically, a simple yet effective EigenScore is proposed to measure the semantic consistency across different generations in the embedding space. Besides, to identify those self-consistent (overconfident) hallucinations which have been overlooked by previous methods, a feature clipping technique is introduced to reduce overconfident generations by truncating extreme features. Significant performance improvement has been achieved in several popular LLMs and QA benchmarks. Although our experiments focus on QA task, our method does not make any assumptions about the task modality, and we believe our method is widely applicable to other tasks, such as summarization and translation. We hope that our insights inspire future research to further explore the internal semantics of LLMs for hallucination detection."
        },
        {
            "heading": "A PERFORMANCE EVALUATION ON TRUTHFULQA",
            "text": "TruthfulQA is an important benchmark to evaluate the truthfulness of LLMs (Joshi et al., 2017). Therefore, we also compare our proposal with the baseline methods in the TruthfulQA benchmark. The optimal classification thresholds is determined by maximizing the G-Mean value, which is defined as G-Mean = \u221a TPR \u2217 (1\u2212 FPR). The results are presented in Table 4. For the ITI Li et al. (2023), which trains multiple binary classifiers with the internal embeddings for hallucination detection, we report the best performance in their paper. As can be seen, our proposal consistently outperforms the baseline methods and achieves comparable performance as ITI when we utilize 50 in-distribution prompts. It\u2019s worth nothing that the ITI relies on training 1024 binary classifiers in TruthQA datasets, and they report the best performance (83.3) in the validation set. Therefore, their best performance is better than our proposal which has not been trained on TruthfulQA. However, training on the validation set also limits the generalization of their method on other domains (Li et al., 2023). As TruthfulQA is a very challenging dataset for LLMs, zero-shot inference results in poor performance. Therefore, we follow previous work (Bai et al., 2022) to utilize different number of in-distribution prompts during inference time. The results show that the performance could be significantly improved when we increase the number of prompts, which also explains why ITI performs good."
        },
        {
            "heading": "B COMPARISON WITH MORE COMPETITIVE METHODS",
            "text": "To demonstrate the effectiveness of our proposal, we also compare our EigenScore with several competitive methods, including Semantic Entropy (SemanticEnt) (Kuhn et al., 2022), Shifting Attention to Relevance (SentSAR) (Duan et al., 2023) and SelfCheckGPT (SelfCKGPT) (Manakul et al., 2023). We follow the experimental setting in Duan et al. (2023) to set the number of generation to N = 10 for OPT-6.7B and N = 5 for LLaMA. For the results of SementicEnt and SentSAR, we report the number in Duan et al. (2023) directly. For the implementation of SelfCheckGPT, we leverage the SelfCheckBERTScore provided in the official code package 7. The comparison results in Table 5 demonstrate that our EigenScore significantly outperforms the competitors. Additionally, both SentSAR and SelfCheckGPT exhibit comparable performance, which is much superior to Semantic Entropy. Note that both SentSAR, SelfCheckGPT and our proposal evaluate the quality of LLMs\u2019 generation by exploring the self-consistency across multiple outputs. However, compared to Semantic Entropy (Kuhn et al., 2022) or SelfCheckGPT (Manakul et al., 2023) which relies on another language model for sentence embedding extraction, our approach leverages the internal states of LLMs, which retain highly-concentrated semantic information. Besides, the EigenScore defined by the LogDet of the sentence covariance matrix is able to capture the semantic consistency more effectively compared to the sentence-wise similarity (Manakul et al., 2023). Furthermore, the proposed feature clipping strategy allows our model to identify the overconfident hallucinations, which has not been investigated by previous works(Kuhn et al., 2022; Manakul et al., 2023)"
        },
        {
            "heading": "C PERFORMANCE EVALUATION ON MORE LLMS",
            "text": "In the main experiments, we evaluate the performance of different methods in LLaMA-7B, LLaMA13B and OPT-6.7B. To demonstrate the robustness of our method across different models, we also\n7https://github.com/potsawee/selfcheckgpt\nprovide the performance comparison in the recent LLaMA2-7B (Touvron et al., 2023b) and Falcon7B models (Almazrouei et al., 2023). Table 6 reveals that our proposal consistently exhibits superior performance compared to the other methods across different LLMs."
        },
        {
            "heading": "D COMPUTATIONAL EFFICIENCY ANALYSIS",
            "text": "As our proposal is a sampling based approach, additional inference cost is required to generate multiple outputs for accurate hallucination detection. We compare our proposal with the base LLM and other comparing methods in LLaMA-7B and LLaMA-13B. All experiments are performed on NVIDIA-A100 and we set the number of generations to N = 10 through the experiments. The average inference time per question is shown in Fig. 5. As observed, our EigenScore is about 10 times more efficient than the methods that rely on another large model to measure the selfconsistency (such as SelfCheckGPT (Manakul et al., 2023)), and shares the similar computational overhead with the LN-Entropy and Lexical Similarity. Compared to the computational overhead of generating multiple outputs, the cost of feature clipping and EigenScore computation is negligible (0.06s). It is worth noting that the inference overhead required to generate multiple results is not linearly proportional to the time required to generate a single output, owing to the sampling and decoding strategy of the autoregressive LLM model."
        },
        {
            "heading": "E EVALUATION WITH EXACT MATCH",
            "text": "In the main experiments, we employ the ROUGE and sentence similarity as correctness measure, which are widely used for natural language generation evaluation (Chang et al., 2023; Kuhn et al., 2022; Huang et al., 2023). In order to facilitate the comparison of our work\u2019s performance with other works, we also provide the evaluation results by employing exact match (Liang et al., 2022) as the correctness score, which is much more strict to determine a generation as correct. The results in Table 7 show similar conclusions to those in Table 1, which demonstrates that our proposal significantly outperforms the compared methods in most cases."
        },
        {
            "heading": "F MORE VISUALIZATION AND ABLATION FOR FEATURE CLIPPING",
            "text": "In Fig. 6, we illustrate the distributions of neuron activation from four selected tokens. As can be seen, the distribution changes a lot across samples. Therefore, it is risky to determine the clipping\nthreshold with only the current input sample (EigenScore-C). A feasible solution is to pre-compute the optimal threshold based on a batch of input samples (EigenScore-P). Besides, another solution is to dynamically record the activation values and determine the threshold during the inference process (EigenScore-MB). We have experimented with both solutions and the experimental results are presented in Table 8. The results demonstrate that determining the thresholds with a memory bank works slightly better. We attribute this variability to potential differences in the activation distributions across various datasets."
        },
        {
            "heading": "G LIMITATIONS AND FUTURE WORK",
            "text": "By exploring the internal states of LLM and introducing an EigenScore metric, the hallucination detection performance has been significantly improved. However, there are several limitations of this study. One critical limitation is that the proposed method relies on the internal information of the LLMs, therefore cannot be applied to the black-box models (OpenAI, 2023), where users can not\naccess the hidden states. Additionally, our proposal is a sampling-based approach, necessitating the generation of multiple outputs, which introduces additional inference overhead for accurate hallucination detection. Furthermore, another limitation is that we only focus on hallucination detection in this study and have not yet mitigate the hallucination with our proposal. In the future work, we hope to reduce the inference cost of our proposal and leverage the EigenScore to mitigate hallucinations. We believe the proposed EigenScore is a strong baseline for detecting hallucination and we invite further researchers to utilize and enhance our proposal."
        },
        {
            "heading": "H CASES STUDY",
            "text": "H.1 HALLUCINATION DETECTION CASES\nWe show several cases with LLaMA-7B and NQ dataset. The number of generation is set to N = 10. We determine the optimal detection threshold by maximizing the G-Mean value, which is defined as G-Mean = \u221a TPR \u2217 (1\u2212 FPR). The hallucination detection thresholds for different metrics are Perplexity: 0.535; LN-Entropy: 0.153; LexicalSimilarity 0.489; SelfCheckGPT: 0.168; EigenScore: -1.74. For LexicalSimilarity, a score larger than the threshold indicates non-hallucination. For other metrics, a score smaller than the thresholds indicates non-hallucination. Green indicates nonhallucination answer and Red indicates hallucination answer generated by LLM. \u2713 (\u2717) indicates that the hallucination is (not) correctly identified by the metric.\nQuestion: the german princes who chose the holy roman empire were called GTAns: prince-electors LLMAns: electors BatchGenerations: BatchGenerations: [\u2019electors\u2019, \u2019electors\u2019, \u2019electors\u2019, \u2019electors\u2019, \u2019electors\u2019, \u2019electors\u2019, \u2019electors\u2019, \u2019electors\u2019, \u2019electors\u2019, \u2019electors\u2019] Perplexity: 0.361 \u2713 LN-Entropy: 0.027 \u2713 LexicalSimilarity: 1.0 \u2713 SentBERTScore: 0.0 \u2713 EigenScore: -2.63 \u2713 EigenValue: [4.87719579e+00 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03]\nQuestion: where is fe best absorbed in the body GTAns: in the duodenum LLMAns: in the small intestine BatchGenerations: [\u2019in the liver\u2019, \u2019small intestine\u2019, \u2019in the intestines\u2019, \u2019the small intestine\u2019, \u2019the small intestine\u2019, \u2019in the liver\u2019, \u2019small intestine\u2019, \u2019fat\u2019, \u2019in the small intestine\u2019, \u2019fatty tissues\u2019] Perplexity: 0.641 \u2713 LN-Entropy: 0.213 \u2713 LexicalSimilarity: 0.357 \u2713 SentBERTScore: 0.258 \u2713 EigenScore: -1.40 \u2713 EigenValue: [3.71561676e+00 4.34496729e-01 3.77751922e-01 1.75326593e-01 9.92596975e02 4.20723353e-02 2.49385766e-02 1.00000000e-03 1.00000000e-03 1.00000000e-03]\nQuestion: who did the united states win its independence from GTAns: the British Empire LLMAns: britain BatchGenerations: [\u2019britain\u2019, \u2019england\u2019, \u2019great britain\u2019, \u2019great britain\u2019, \u2019england\u2019, \u2019england\u2019, \u2019england\u2019, \u2019england\u2019, \u2019great britain\u2019, \u2019great britain\u2019] Perplexity: 0.598 \u2717 LN-Entropy: 0.266 \u2717 LexicalSimilarity: 0.415 \u2717 SentBERTScore: 0.397 \u2717 EigenScore: -2.23 \u2713 EigenValue: [4.46843402e+00 2.82423429e-01 3.88702191e-02 1.00000000e-03 1.00000000e03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03]\nQuestion: who won the most stanley cups in history GTAns: Montreal Canadiens LLMAns: the detroit red wings BatchGenerations: [\u2019the detroit red wings\u2019, \u2019the detroit red wings\u2019, \u2019the detroit red wings\u2019, \u2019the detroit red wings\u2019, \u2019the detroit red wings\u2019, \u2019the detroit red wings\u2019, \u2019the detroit red wings\u2019, \u2019the detroit red wings\u2019, \u2019the detroit red wings\u2019, \u2019the detroit red wings\u2019] Perplexity: 0.366 \u2717 LN-Entropy: 0.025 \u2717 LexicalSimilarity: 1.0 \u2717 SentBERTScore: 0.0 \u2717 EigenScore: -2.63 \u2717 EigenValue: [5.23534401e+00 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03]\nQuestion: what is the second book in the alchemyst series GTAns: The Magician LLMAns: the alchemyst: the secret of the immortal Nicholas flamel BatchGenerations: [\u2019the magician in the middle\u2019, \u201dthe magician\u2019s nephew\u201d, \u2019the magician\u2019, \u2019the alchemyst\u2019, \u2019the magician\u2019, \u2019the alchemyst\u2019, \u2019the magician in the middle\u2019, \u2019the magician in amsterdam\u2019, \u2019the magician in the middle\u2019, \u2019the secret of the near witch\u2019] Perplexity: 0.459 \u2717 LN-Entropy: 0.207 \u2713 LexicalSimilarity: 0.499 \u2717 SentBERTScore: 0.431 \u2713 EigenScore: -1.41 \u2713 EigenValue: [2.09138499e+00 6.95605781e-01 3.85931973e-01 3.40671669e-01 2.15372994e01 1.77304781e-01 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03]\nQuestion: a good that can be used in place of another good GTAns: A substitute good LLMAns: substitute BatchGenerations: [\u2019a substitute\u2019, \u2019a substitute\u2019, \u2019substitute\u2019, \u2019a substitute\u2019, \u2019substitute\u2019, \u2019substitute\u2019, \u2019substitute\u2019, \u2019substitute\u2019, \u2019substitute\u2019, \u2019substitute\u2019] Perplexity: 0.614 \u2717 LN-Entropy: 0.194 \u2717 LexicalSimilarity: 0.844 \u2713 SentBERTScore: 0.076 \u2713 EigenScore: -2.40 \u2713 EigenValue: [5.11365925e+00 1.75884104e-01 1.00000000e-03 1.00000000e-03 1.00000000e03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03]\nQuestion: who said i will not go quietly into the night GTAns: Welsh poet Dylan Thomas LLMAns: douglas macarthur BatchGenerations: [\u2019douglas macarthur\u2019, \u2019elvis presley\u2019, \u2019douglas macarthur\u2019, \u2019douglas macarthur\u2019, \u2019douglas macarthur\u2019, \u2019robin williams\u2019, \u2019robert f. kennedy\u2019, \u2019robin williams\u2019, \u2019robert f kennedy\u2019, \u2019Dwight Eisenhower\u2019] Perplexity: 0.502 \u2717 LN-Entropy: 0.121 \u2717 LexicalSimilarity: 0.178 \u2713 SentBERTScore: 0.337 \u2713 EigenScore: -1.61 \u2713 EigenValue: [3.32824135e+00 5.87944819e-01 3.70390066e-01 1.70849836e-01 1.17707239e01 5.17925563e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03]\nQuestion: when did they stop saying mass in latin GTAns: from 1967 onwards LLMAns: 1969 BatchGenerations: [\u2019in 1969\u2019, \u20191969\u2019, \u2019around 1969\u2019, \u2019in 1965\u2019, \u2019in 1969\u2019, \u20191969\u2019, \u2019In 1969, the Roman Missal was changed from Latin to the vernacular.\u2019, \u20191969\u2019, \u20191969 when the second vatican council was held\u2019, \u20191969\u2019] Perplexity: 0.631 \u2713 LN-Entropy: 0.152 \u2717 LexicalSimilarity: 0.540 \u2717 SentBERTScore: 0.273 \u2713 EigenScore: -1.59 \u2713 EigenValue: [3.80408192e+00 4.83987672e-01 3.03207580e-01 8.80366008e-02 6.59790286e02 3.26742841e-02 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03]\nQuestion: where does the highway of heroes start and end GTAns: Glen Miller Road in Trenton LLMAns: it starts in ottawa and ends in toronto BatchGenerations: [\u2019start in ottawa and end in toronto\u2019, \u2019it starts in toronto and ends in ottawa\u2019, \u201dit starts at the queen\u2019s park gate at queen\u2019s park and ends at toronto\u2019s coronation park\u201d, \u201dit starts in toronto and ends at queen\u2019s park\u201d, \u2019starts in Toronto and ends in Trenton\u2019, \u2019it starts at the CNE and ends at the cenotaph in downtown toronto\u2019, \u2019it starts in downtown toronto and ends in ottawa\u2019, \u2019it starts in the city of Toronto and ends in the city of Hamilton\u2019, \u2019it starts in toronto and ends in ottawa\u2019, \u2019it starts in Toronto and ends in Ottawa\u2019] Perplexity: 0.657 \u2713 LN-Entropy: 0.260 \u2713 LexicalSimilarity: 0.594 \u2717 SentBERTScore: 0.149 \u2717 EigenScore: -1.05 \u2713 EigenValue: [3.31983018e+00 3.98560810e-01 2.17094299e-01 2.06965709e-01 1.53575354e01 1.27925588e-01 7.82365136e-02 3.28158137e-02 1.01995086e-02 1.00000000e-03]\nQuestion: which body part(s) occupy the greatest portion of the primary motor cortex GTAns: face LLMAns: the hand BatchGenerations: [\u2019the hand\u2019, \u2019the hand\u2019, \u2019the fingers\u2019, \u2019the hand\u2019, \u2019the hand\u2019, \u2019the hand and the foot\u2019, \u2019the hand\u2019, \u2019the hand and the foot\u2019, \u2019the hand and face\u2019, \u2019arms and legs\u2019] Perplexity: 1.41 \u2713 LN-Entropy: 0.300 \u2713 LexicalSimilarity: 0.568 \u2717 SentBERTScore: 0.163 \u2717 EigenScore: -1.69 \u2713 EigenValue: [3.76273036e+00 6.16284067e-01 1.96541049e-01 1.73505005e-01 1.28407153e01 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03]\nQuestion: who said have you no sense of decency GTAns: Joseph Nye Welch LLMAns: Joseph Nye Welch BatchGenerations: [\u2019Joseph N. Welch\u2019, \u2019Joseph N. Welch\u2019, \u2019joe stalin\u2019, \u2019joseph mccarthy\u2019, \u2019Joseph N. Welch\u2019, \u2019Joseph N. Welch\u2019, \u2019Joseph Nye Welch\u2019, \u2019joseph mccarthy\u2019, \u2019joe mccarthy\u2019, \u2019joseph mccarthy\u2019] Perplexity: 0.666 \u2717 LN-Entropy: 0.212 \u2717 LexicalSimilarity: 0.437 \u2717 SentBERTScore: 0.391 \u2717 EigenScore: -1.85 \u2713 EigenValue: [3.63114083e+00 8.11672323e-01 2.00385898e-01 3.19140618e-02 1.74251264e02 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03]\nH.2 MODEL GENERATIONS WITH MANY AND FEW OUTLIERS\nTo demonstrate the relationship between the number of extreme features and model outputs, we provide several examples with many/few extreme features. The results show that when there are many extreme features, the model tends to generate consistent hallucination outputs across multiple generations. Instead, when there are few extreme features, the model generates diverse hallucination outputs which can be spotted by different hallucination detection metrics.\nQuestion: who sang on great gig in the sky GTAns: Clare Torry LLMAns: freddie mercury AvgNumOutliers: 15 BatchGenerations: [\u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019] Perplexity: 0.263 \u2717 LN-Entropy: 0.028 \u2717 LexicalSimilarity: 1.0 \u2717 SentBERTScore: 0.0 \u2717 EigenScore: -2.63 \u2717 EigenValue: [4.65740187e+00 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03]\nQuestion: what are the top five wine producing states GTAns: Washington LLMAns: California AvgNumOutliers: 13 BatchGenerations: [\u2019 California,\u2019, \u2019 California,\u2019, \u2019 california,\u2019, \u2019 California,\u2019, \u2019 California,\u2019, \u2019 California,\u2019, \u2019 California,\u2019, \u2019 california,\u2019, \u2019 California,\u2019, \u2019 California,\u2019] Perplexity: 0.368 \u2717 LN-Entropy: 0.075 \u2717 LexicalSimilarity: 1.0 \u2717 SentBERTScore: 0.054 \u2717 EigenScore: -2.42 \u2717 EigenValue: [5.30709315e+00 1.13222379e-01 1.00000000e-03 1.00000000e-03 1.00000000e03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03]\nQuestion: how many seasons of rules of engagement is there GTAns: 7 LLMAns: 4 seasons AvgNumOutliers: 2 BatchGenerations: [\u20193 seasons\u2019, \u20194 seasons\u2019, \u20194 seasons\u2019, \u20196 seasons\u2019, \u20197 seasons\u2019, \u20193 (2007- 2009)\u2019, \u20193 (2007-2009)\u2019, \u20194 seasons\u2019, \u2019three\u2019, \u201911 seasons\u2019] Perplexity: 0.996 \u2713 LN-Entropy: 0.292 \u2713 LexicalSimilarity: 0.307 \u2713 SentBERTScore: 0.285 \u2713 EigenScore: -1.60 \u2713 EigenValue: [3.58548815e+00 5.87838054e-01 2.28057934e-01 1.36461300e-01 3.49712302e02 1.11346059e-02 3.82259086e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03]\nQuestion: where did the first persian gulf war take place GTAns: Israel LLMAns: kuwait AvgNumOutliers: 3 BatchGenerations: [\u2019Iraq\u2019, \u2019Iraq and Kuwait\u2019, \u2019Iraq\u2019, \u2019kuwait\u2019, \u2019kuwait\u2019, \u2019in the middle east\u2019, \u2019in iraq\u2019, \u2019kuwait\u2019, \u2019iraq\u2019, \u2019kuwait\u2019] Perplexity: 0.546 \u2713 LN-Entropy: 0.281 \u2713 LexicalSimilarity: 0.339 \u2713 SentBERTScore: 0.224 \u2713 EigenScore: -1.62 \u2713 EigenValue: [3.59463352e+00 4.23782982e-01 2.57087067e-01 1.41513403e-01 6.20790226e02 1.75980481e-02 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03]\nH.3 IMPACT OF FEATURE CLIPPING\nThe texts in yellow represents model generations after applying feature clipping. The results show that after feature clipping, the overconfident generations can be appropriately suppressed, and some self-consistent hallucinations are finally identified.\nQuestion: what are the top five wine producing states GTAns: Washington LLMAns: California BatchGenerations: [\u2019 California,\u2019, \u2019 California,\u2019, \u2019 california,\u2019, \u2019 California,\u2019, \u2019 California,\u2019, \u2019 California,\u2019, \u2019 California,\u2019, \u2019 california,\u2019, \u2019 California,\u2019, \u2019 California,\u2019] Perplexity: 0.368 \u2717 LN-Entropy: 0.075 \u2717 LexicalSimilarity: 1.0 \u2717 SentBERTScore: 0.054 \u2717 EigenScore: -2.42 \u2717 EigenValue: [5.30709315e+00 1.13222379e-01 1.00000000e-03 1.00000000e-03 1.00000000e03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03] BatchGenerations: [\u2019california\u2019, \u2019california\u2019, \u2019Washington\u2019, \u2019california\u2019, \u2019new york\u2019, \u2019california\u2019, \u2019washington\u2019, \u2019california\u2019, \u2019new york\u2019, \u2019michigan\u2019] EigenScore: -1.32 \u2713 EigenValue: [3.23392755e+00 8.41049340e-01 2.52322804e-01 1.33473529e-01 7.19449437e-02 6.12184197e-02 1.02734249e-02 5.33703500e-03 3.09878029e-03 1.00000000e-03]\nQuestion: who sang on great gig in the sky GTAns: Clare Torry LLMAns: freddie mercury AvgNumOutliers: 15 BatchGenerations: [\u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019] Perplexity: 0.263 \u2717 LN-Entropy: 0.028 \u2717 LexicalSimilarity: 1.0 \u2717 SentBERTScore: 0.0 \u2717 EigenScore: -2.63 \u2717 EigenValue: [4.65740187e+00 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03] BatchGenerations: [\u2019claire torry\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019, \u2019freddie mercury\u2019] EigenScore: -2.38 \u2717 EigenValue: [4.38745800e+00 3.14982649e-01 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03]\nQuestion: who are you in assassin\u2019s creed 4 GTAns: third-person perspective LLMAns: Edward Kenway BatchGenerations: [\u2019Edward Kenway\u2019, \u2019Edward Kenway\u2019, \u2019Edward Kenway\u2019, \u2019Edward Kenway\u2019, \u2019Edward Kenway\u2019, \u2019Edward Kenway\u2019, \u2019Edward Kenway\u2019, \u2019Edward Kenway\u2019, \u2019Edward Kenway\u2019, \u2019Edward Kenway\u2019] Perplexity: 0.264 \u2717 LN-Entropy: 0.002 \u2717 LexicalSimilarity: 1.0 \u2717 SentBERTScore: 0.0 \u2717 EigenScore: -2.67 \u2717 EigenValue: [2.10973201e+00 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03] BatchGenerations: [\u2019Edward Kenway\u2019, \u2019Edward Kenway\u2019, \u2019Connor\u2019, \u2019Edward Kenway\u2019, \u2019connor\u2019, \u2019Connor\u2019, \u2019alexander hamilton\u2019, \u2019Edward Kenway\u2019, \u2019ezio\u2019, \u2019connor\u2019] EigenScore: -1.68 \u2713 EigenValue: [3.47825477e+00 7.48127381e-01 3.24792650e-01 2.17182636e-01 8.15050807e-02 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03 1.00000000e-03]"
        }
    ],
    "year": 2023
}