{
    "abstractText": "Phylogenetics is a branch of computational biology that studies the evolutionary relationships among biological entities. Its long history and numerous applications notwithstanding, inference of phylogenetic trees from sequence data remains challenging: the extremely large tree space poses a significant obstacle for the current combinatorial and probabilistic techniques. In this paper, we adopt the framework of generative flow networks (GFlowNets) to tackle two core problems in phylogenetics: parsimony-based and Bayesian phylogenetic inference. Because GFlowNets are well-suited for sampling complex combinatorial structures, they are a natural choice for exploring and sampling from the multimodal posterior distribution over tree topologies and evolutionary distances. We demonstrate that our amortized posterior sampler, PhyloGFN, produces diverse and high-quality evolutionary hypotheses on real benchmark datasets. PhyloGFN is competitive with prior works in marginal likelihood estimation and achieves a closer fit to the target distribution than state-of-the-art variational inference methods.",
    "authors": [],
    "id": "SP:659b00bf58e3426c41023148452d006bca769a84",
    "references": [
        {
            "authors": [
                "Rustam I Aminov",
                "Roderick I Mackie"
            ],
            "title": "Evolution and ecology of antibiotic resistance genes",
            "venue": "FEMS microbiology letters,",
            "year": 2007
        },
        {
            "authors": [
                "Emmanuel Bengio",
                "Moksh Jain",
                "Maksym Korablyov",
                "Doina Precup",
                "Yoshua Bengio"
            ],
            "title": "Flow network based generative models for non-iterative diverse candidate generation",
            "venue": "Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Mathieu Blanchette",
                "W James Kent",
                "Cathy Riemer",
                "Laura Elnitski",
                "Arian FA Smit",
                "Krishna M Roskin",
                "Robert Baertsch",
                "Kate Rosenbloom",
                "Hiram Clawson",
                "Eric D Green"
            ],
            "title": "Aligning multiple genomic sequences with the threaded blockset aligner",
            "venue": "Genome research,",
            "year": 2004
        },
        {
            "authors": [
                "Patricia Buendia",
                "Brice Cadwallader",
                "Victor DeGruttola"
            ],
            "title": "A phylogenetic and Markov model approach for the reconstruction of mutational pathways of drug",
            "venue": "resistance. Bioinformatics,",
            "year": 2009
        },
        {
            "authors": [
                "Yuri Burda",
                "Roger Baker Grosse",
                "Ruslan Salakhutdinov"
            ],
            "title": "Importance weighted autoencoders",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2016
        },
        {
            "authors": [
                "Gershon Celniker",
                "Guy Nimrod",
                "Haim Ashkenazy",
                "Fabian Glaser",
                "Eric Martz",
                "Itay Mayrose",
                "Tal Pupko",
                "Nir Ben-Tal"
            ],
            "title": "Consurf: using evolutionary data to raise testable hypotheses about protein function",
            "venue": "Israel Journal of Chemistry,",
            "year": 2013
        },
        {
            "authors": [
                "Benny Chor",
                "Tamir Tuller"
            ],
            "title": "Maximum likelihood of evolutionary trees is hard",
            "venue": "In Annual International Conference on Research in Computational Molecular Biology,",
            "year": 2005
        },
        {
            "authors": [
                "William HE Day"
            ],
            "title": "Computational complexity of inferring phylogenies from dissimilarity matrices",
            "venue": "Bulletin of mathematical biology,",
            "year": 1987
        },
        {
            "authors": [
                "Tristan Deleu",
                "Ant\u00f3nio G\u00f3is",
                "Chris Emezue",
                "Mansi Rankawat",
                "Simon Lacoste-Julien",
                "Stefan Bauer",
                "Yoshua Bengio"
            ],
            "title": "Bayesian structure learning with generative flow networks",
            "venue": "Uncertainty in Artificial Intelligence (UAI),",
            "year": 2022
        },
        {
            "authors": [
                "Tristan Deleu",
                "Mizu Nishikawa-Toomey",
                "Jithendaraa Subramanian",
                "Nikolay Malkin",
                "Laurent Charlin",
                "Yoshua Bengio"
            ],
            "title": "Joint Bayesian inference of graphical structure and parameters with a single generative flow network",
            "year": 1936
        },
        {
            "authors": [
                "Erika Dort",
                "Elliot Layne",
                "Nicolas Feau",
                "Alexander Butyaev",
                "Bernard Henrissat",
                "Francis Martin",
                "Sajeet Haridas",
                "Asaf Salamov",
                "Igor Grigoriev",
                "Mathieu Blanchette",
                "Hamelin Richard"
            ],
            "title": "Largescale genomic analyses with machine learning uncover predictive patterns associated with fungal phytopathogenic lifestyles and traits",
            "venue": "Scientific Reports,",
            "year": 2023
        },
        {
            "authors": [
                "Alexei J Drummond",
                "Simon Y. W Ho",
                "Matthew J Phillips",
                "Andrew Rambaut"
            ],
            "title": "Relaxed phylogenetics and dating with confidence",
            "venue": "PLOS Biology, 4(5):null,",
            "year": 2006
        },
        {
            "authors": [
                "Joseph Felsenstein"
            ],
            "title": "Maximum likelihood and minimum-steps methods for estimating evolutionary trees from data on discrete characters",
            "venue": "Systematic Biology,",
            "year": 1973
        },
        {
            "authors": [
                "R. Fisher",
                "L. Pusztai",
                "C. Swanton"
            ],
            "title": "Cancer heterogeneity: implications for targeted therapeutics",
            "venue": "British Journal of Cancer,",
            "year": 2013
        },
        {
            "authors": [
                "J R Garey",
                "T J Near",
                "M R Nonnemacher",
                "S A Nadler"
            ],
            "title": "Molecular evidence for acanthocephala as a subtaxon of rotifera",
            "venue": "J Mol Evol,",
            "year": 1996
        },
        {
            "authors": [
                "Richard C Hamelin",
                "Guillaume J Bilodeau",
                "Renate Heinzelmann",
                "Kelly Hrywkiw",
                "Arnaud Capron",
                "Erika Dort",
                "Angela L Dale",
                "Emilie Giroux",
                "Stacey Kus",
                "Nick C Carleson"
            ],
            "title": "Genomic biosurveillance detects a sexual hybrid in the sudden oak death pathogen",
            "venue": "Communications Biology,",
            "year": 2022
        },
        {
            "authors": [
                "S B Hedges",
                "K D Moberg",
                "L R Maxson"
            ],
            "title": "Tetrapod phylogeny inferred from 18S and 28S ribosomal RNA sequences and a review of the evidence for amniote relationships",
            "venue": "Molecular Biology and Evolution, 7(6):607\u2013633,",
            "year": 1990
        },
        {
            "authors": [
                "Daniel A Henk",
                "Alex Weir",
                "Meredith Blackwell"
            ],
            "title": "Laboulbeniopsis termitarius, an ectoparasite of termites newly recognized as a member of the laboulbeniomycetes",
            "venue": "Mycologia,",
            "year": 2003
        },
        {
            "authors": [
                "Diep Thi Hoang",
                "Olga Chernomor",
                "Arndt Von Haeseler",
                "Bui Quang Minh",
                "Le Sy Vinh"
            ],
            "title": "Ufboot2: improving the ultrafast bootstrap approximation",
            "venue": "Molecular biology and evolution,",
            "year": 2018
        },
        {
            "authors": [
                "Edward J Hu",
                "Nikolay Malkin",
                "Moksh Jain",
                "Katie Everett",
                "Alexandros Graikos",
                "Yoshua Bengio"
            ],
            "title": "GFlowNet-EM for learning compositional latent variable models",
            "venue": "International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Sebastian H\u00f6hna",
                "Michael J. Landis",
                "Tracy A. Heath",
                "Bastien Boussau",
                "Nicolas Lartillot",
                "Brian R. Moore",
                "John P. Huelsenbeck",
                "Fredrik Ronquist"
            ],
            "title": "RevBayes: Bayesian Phylogenetic Inference Using Graphical Models and an Interactive Model-Specification Language",
            "venue": "Systematic Biology, 65(4):726\u2013736,",
            "year": 2016
        },
        {
            "authors": [
                "Moksh Jain",
                "Emmanuel Bengio",
                "Alex Hernandez-Garcia",
                "Jarrid Rector-Brooks",
                "Bonaventure F.P. Dossou",
                "Chanakya Ekbote",
                "Jie Fu",
                "Tianyu Zhang",
                "Micheal Kilgour",
                "Dinghuai Zhang",
                "Lena Simine",
                "Payel Das",
                "Yoshua Bengio"
            ],
            "title": "Biological sequence design with GFlowNets",
            "venue": "International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Hazal Koptagel",
                "Oskar Kviman",
                "Harald Melin",
                "Negar Safinianaini",
                "Jens Lagergren"
            ],
            "title": "VaiPhy: a variational inference based algorithm for phylogeny",
            "venue": "Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Salem Lahlou",
                "Tristan Deleu",
                "Pablo Lemos",
                "Dinghuai Zhang",
                "Alexandra Volokhova",
                "Alex Hern\u00e1ndez-Garc\u0131\u0301a",
                "L\u00e9na N\u00e9hale Ezzine",
                "Yoshua Bengio",
                "Nikolay Malkin"
            ],
            "title": "A theory of continuous generative flow networks",
            "venue": "International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "David L\u00e4hnemann",
                "Johannes K\u00f6ster",
                "Ewa Szczurek",
                "Davis J McCarthy",
                "Stephanie C Hicks",
                "Mark D Robinson",
                "Catalina A Vallejos",
                "Kieran R Campbell",
                "Niko Beerenwinkel",
                "Ahmed Mahfouz"
            ],
            "title": "Eleven grand challenges in single-cell data science",
            "venue": "Genome biology,",
            "year": 2020
        },
        {
            "authors": [
                "Clemens Lakner",
                "Paul Van Der Mark",
                "John P Huelsenbeck",
                "Bret Larget",
                "Fredrik Ronquist"
            ],
            "title": "Efficiency of Markov chain Monte Carlo tree proposals in Bayesian phylogenetics",
            "venue": "Systematic biology,",
            "year": 2008
        },
        {
            "authors": [
                "Elliot Layne",
                "Erika N Dort",
                "Richard Hamelin",
                "Yue Li",
                "Mathieu Blanchette"
            ],
            "title": "Supervised learning on phylogenetically distributed data",
            "venue": "Bioinformatics, 36(Supplement",
            "year": 2020
        },
        {
            "authors": [
                "Jian Ma",
                "Louxin Zhang",
                "Bernard B Suh",
                "Brian J Raney",
                "Richard C Burhans",
                "W James Kent",
                "Mathieu Blanchette",
                "David Haussler",
                "Webb Miller"
            ],
            "title": "Reconstructing contiguous regions of an ancestral genome",
            "venue": "Genome research,",
            "year": 2006
        },
        {
            "authors": [
                "Nikolay Malkin",
                "Moksh Jain",
                "Emmanuel Bengio",
                "Chen Sun",
                "Yoshua Bengio"
            ],
            "title": "Trajectory balance: Improved credit assignment in GFlowNets",
            "venue": "Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Nikolay Malkin",
                "Salem Lahlou",
                "Tristan Deleu",
                "Xu Ji",
                "Edward Hu",
                "Katie Everett",
                "Dinghuai Zhang",
                "Yoshua Bengio"
            ],
            "title": "GFlowNets and variational inference",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Nicholas McGranahan",
                "Francesco Favero",
                "Elza C. de Bruin",
                "Nicolai Juul Birkbak",
                "Zoltan Szallasi",
                "Charles Swanton"
            ],
            "title": "Clonal status of actionable driver events and the timing of mutational processes in cancer",
            "venue": "evolution. Science Translational Medicine,",
            "year": 2015
        },
        {
            "authors": [
                "Takahiro Mimori",
                "Michiaki Hamada"
            ],
            "title": "Geophy: Differentiable phylogenetic inference via geometric gradients of tree topologies",
            "venue": "arXiv preprint arXiv:2307.03675,",
            "year": 2023
        },
        {
            "authors": [
                "Bui Quang Minh",
                "Minh Anh Thi Nguyen",
                "Arndt Von Haeseler"
            ],
            "title": "Ultrafast approximation for phylogenetic bootstrap",
            "venue": "Molecular biology and evolution,",
            "year": 2013
        },
        {
            "authors": [
                "Bui Quang Minh",
                "Heiko A Schmidt",
                "Olga Chernomor",
                "Dominik Schrempf",
                "Michael D Woodhams",
                "Arndt Von Haeseler",
                "Robert Lanfear"
            ],
            "title": "Iq-tree 2: new models and efficient methods for phylogenetic inference in the genomic era",
            "venue": "Molecular biology and evolution,",
            "year": 2020
        },
        {
            "authors": [
                "Antonio Khalil Moretti",
                "Liyi Zhang",
                "Christian A. Naesseth",
                "Hadiah Venner",
                "David Blei",
                "Itsik Pe\u2019er"
            ],
            "title": "Variational combinatorial sequential Monte Carlo methods for Bayesian phylogenetic inference",
            "venue": "Uncertainty in Artificial Intelligence (UAI),",
            "year": 2021
        },
        {
            "authors": [
                "Reza Ranjbar",
                "Sedigheh Nazari",
                "Omid Farahani"
            ],
            "title": "Phylogenetic analysis and antimicrobial resistance profiles of escherichia coli strains isolated from uti-suspected patients",
            "venue": "Iranian Journal of Public Health,",
            "year": 2020
        },
        {
            "authors": [
                "Fredrik Ronquist",
                "Maxim Teslenko",
                "Paul Van Der Mark",
                "Daniel L Ayres",
                "Aaron Darling",
                "Sebastian H\u00f6hna",
                "Bret Larget",
                "Liang Liu",
                "Marc A Suchard",
                "John P Huelsenbeck"
            ],
            "title": "MrBayes 3.2: efficient Bayesian phylogenetic inference and model choice across a large model space",
            "venue": "Systematic biology,",
            "year": 2012
        },
        {
            "authors": [
                "Amy Y. Rossman",
                "John M. McKemy",
                "Rebecca A. Pardo-Schultheiss",
                "Hans-Josef Schroers"
            ],
            "title": "Molecular studies of the bionectriaceae using large subunit rdna",
            "venue": "sequences. Mycologia,",
            "year": 2001
        },
        {
            "authors": [
                "David L Swofford"
            ],
            "title": "Phylogenetic analysis using parsimony",
            "year": 1998
        },
        {
            "authors": [
                "Hakon Tjelmeland",
                "Bjorn Kare Hegstad"
            ],
            "title": "Mode jumping proposals in mcmc",
            "venue": "Scandinavian journal of statistics,",
            "year": 2001
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Neural Information Processing Systems (NIPS),",
            "year": 2017
        },
        {
            "authors": [
                "Ziheng Yang",
                "Anne D. Yoder"
            ],
            "title": "Comparison of Likelihood and Bayesian Methods for Estimating Divergence Times Using Multiple Gene Loci and Calibration Points, with Application to a Radiation of Cute-Looking Mouse Lemur Species",
            "venue": "Systematic Biology, 52(5):705\u2013716,",
            "year": 2003
        },
        {
            "authors": [
                "Anne D. Yoder",
                "Ziheng Yang"
            ],
            "title": "Divergence dates for malagasy lemurs estimated from multiple gene loci: geological and evolutionary context",
            "venue": "Molecular Ecology,",
            "year": 2004
        },
        {
            "authors": [
                "Cheng Zhang"
            ],
            "title": "Learnable topological features for phylogenetic inference via graph neural networks",
            "venue": "arXiv preprint arXiv:2302.08840,",
            "year": 2023
        },
        {
            "authors": [
                "Cheng Zhang",
                "Frederick A Matsen IV"
            ],
            "title": "Generalizing tree probability estimation via bayesian networks",
            "venue": "Neural Information Processing Systems (NIPS),",
            "year": 2018
        },
        {
            "authors": [
                "Cheng Zhang",
                "Frederick A Matsen IV"
            ],
            "title": "Variational bayesian phylogenetic inference",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "David Zhang",
                "Corrado Rainone",
                "Markus Peschl",
                "Roberto Bondesan"
            ],
            "title": "Robust scheduling with GFlowNets",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Dinghuai Zhang",
                "Nikolay Malkin",
                "Zhen Liu",
                "Alexandra Volokhova",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative flow networks for discrete probabilistic modeling",
            "venue": "International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Dinghuai Zhang",
                "Hanjun Dai",
                "Nikolay Malkin",
                "Aaron Courville",
                "Yoshua Bengio",
                "Ling Pan"
            ],
            "title": "Let the flows tell: Solving graph combinatorial problems with GFlowNets",
            "venue": "arXiv preprint arXiv:2305.17010,",
            "year": 2023
        },
        {
            "authors": [
                "Ning Zhang",
                "Meredith Blackwell"
            ],
            "title": "Molecular phylogeny of dogwood anthracnose fungus (discula destructiva) and the diaporthales",
            "venue": "Mycologia,",
            "year": 2001
        },
        {
            "authors": [
                "Heiko Zimmermann",
                "Fredrik Lindsten",
                "Jan-Willem van de Meent",
                "Christian A. Naesseth"
            ],
            "title": "A variational perspective on generative flow networks",
            "venue": "Transactions on Machine Learning Research (TMLR),",
            "year": 2023
        },
        {
            "authors": [
                "Rossman"
            ],
            "title": "D MODELING Given the character set \u03a3, we use one-hot encoding to represent each site in a sequence. To deal with wild characters in the dataset, for parsimony analysis we consider them as one special character",
            "year": 2001
        },
        {
            "authors": [
                "Mimori",
                "Hamada"
            ],
            "title": "2023) python ./scripts/run_gp.py -v -ip ds-data/ds1/DS1.nex \\ -op results/ds1/ds1_paper -c ./config/default.yaml -s 0 -ua \\ -es lorentz -edt full -ed 4 -eqs 1e-1 -ast 100_1000 -mcs",
            "venue": "-ul",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Phylogenetics is a branch of computational biology that studies the evolutionary relationships among biological entities. Its long history and numerous applications notwithstanding, inference of phylogenetic trees from sequence data remains challenging: the extremely large tree space poses a significant obstacle for the current combinatorial and probabilistic techniques. In this paper, we adopt the framework of generative flow networks (GFlowNets) to tackle two core problems in phylogenetics: parsimony-based and Bayesian phylogenetic inference. Because GFlowNets are well-suited for sampling complex combinatorial structures, they are a natural choice for exploring and sampling from the multimodal posterior distribution over tree topologies and evolutionary distances. We demonstrate that our amortized posterior sampler, PhyloGFN, produces diverse and high-quality evolutionary hypotheses on real benchmark datasets. PhyloGFN is competitive with prior works in marginal likelihood estimation and achieves a closer fit to the target distribution than state-of-the-art variational inference methods."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Phylogenetic inference has long been a central problem in the field of computational biology. A broad set of methods has been developed to estimate the evolutionary history (phylogenetic tree) relating a set of biological entities. Accurate phylogenetic inference is critical for a number of important biological analyses, such as understanding the development of antibiotic resistance (Aminov & Mackie, 2007; Ranjbar et al., 2020; Layne et al., 2020), assessing the risk of invasive species (Hamelin et al., 2022; Dort et al., 2023), and characterizing tumor progression (La\u0308hnemann et al., 2020). Accurate phylogenetic trees can also be used to improve downstream computational analyses, such as multiple genome alignment (Blanchette et al., 2004), ancestral sequence reconstruction (Ma et al., 2006), protein structure and function annotation (Celniker et al., 2013).\nDespite its strong medical relevance and wide applications in life science, phylogenetic inference has remained a standing challenge, in part due to the high complexity of tree space \u2014 for \ud835\udc5b species, (2\ud835\udc5b \u2212 5)!! unique unrooted bifurcating tree topologies exist. This poses a common obstacle to all branches of phylogenetic inference; both maximum-likelihood and maximum-parsimony tree reconstruction are NP-hard problems (Day, 1987; Chor & Tuller, 2005). Under the Bayesian formulation of phylogenetics, the inference problem is further compounded by the inclusion of continuous variables that capture the level of sequence divergence along each branch of the tree.\nOne line of prior work considers Markov chain Monte Carlo (MCMC)-based approaches, such as MrBayes (Ronquist et al., 2012). These approaches have been successfully applied to Bayesian phylogenetic inference. However, a known limitation of MCMC is scalability to high-dimensional distributions with multiple separated modes (Tjelmeland & Hegstad, 2001), which arise in larger phylogenetic datasets. Recently, variational inference (VI)-based approaches have emerged, which offer computationally efficient and scalable alternatives to MCMC. Among these methods, some model only a limited portion of the space of tree topologies, while others are weaker in marginal likelihood estimation due to simplifying assumptions. In parsimony analysis, state-of-the-art methods such as PAUP* (Swofford, 1998) have extensively relied on heuristic search algorithms that are efficient but lack theoretical foundations and guarantees.\nComing from the intersection of variational inference and reinforcement learning is the class of models known as generative flow networks (GFlowNets) (Bengio et al., 2021). The flexibility afforded\nby GFlowNets to learn sequential samplers for distributions over compositional objects makes them a promising candidate for performing inference over the posterior space of phylogenetic tree topologies and evolutionary distances.\nIn this work, we propose PhyloGFN, the first adaptation of GFlowNets to the task of Bayesian and parsimony-based phylogenetic inference. Our contributions are as follows:\n(1) We design an acyclic Markov decision process (MDP) with fully customizable reward functions, by which our PhyloGFN can be trained to construct phylogenetic trees in a bottom-up fashion. (2) PhyloGFN leverages a novel tree representation inspired by Fitch and Felsenstein\u2019s algorithms to represent rooted trees without introducing additional learnable parameters to the model. PhyloGFN is also coupled with simple yet effective training techniques such as using mixed onpolicy and dithered-policy rollouts, replay buffers and cascading temperature-annealing. (3) PhyloGFN has the capacity to explore and sample from the entire phylogenetic tree space, achieving a balance between exploration in this vast space and high-fidelity modeling of the modes. While PhyloGFN performs on par with the state-of-the-art MCMC- and VI-based methods in the summary metric of marginal log-likelihood, it substantially outperforms these approaches in terms of its ability to estimate the posterior probability of suboptimal trees 1."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Markov chain Monte Carlo (MCMC)-based algorithms are commonly employed for Bayesian phylogenetics, with notable examples including MrBayes and RevBayes (Ronquist et al., 2012; Ho\u0308hna et al., 2016), which are considered state-of-the-art in the field. Amortized variational inference (VI) is an alternative approach that parametrically estimates the posterior distribution. VBPIGNN (Zhang, 2023) employs subsplit Bayesian networks (SBN) (Zhang & Matsen IV, 2018a) to model tree topology distributions and uses graph neural networks to learn tree topological embeddings (Zhang, 2023). While VBPI-GNN has obtained marginal log likelihood competitive with MrBayes in real datasets, it requires a pre-generated set of high-quality tree topologies to constrain its action space for tree construction, which ultimately limits its ability to model the entire tree space.\nThere exist other VI approaches that do not limit the space of trees. VaiPhy (Koptagel et al., 2022) approximates the posterior distribution in the augmented space of tree topologies, edge lengths, and ancestral sequences. Combined with combinatorial sequential Monte Carlo (CSMC; Moretti et al., 2021), the proposed method enables faster estimation of marginal likelihood. GeoPhy (Mimori & Hamada, 2023) models the tree topology distribution in continuous space by mapping continuousvalued coordinates to tree topologies, using the same technique as VBPI-GNN to model tree topological embeddings. While both methods model the entire tree topology space, their performance on marginal likelihood estimation underperforms the state of the art.\nFor the optimization problem underpinning maximum parsimony inference, PAUP* is one of the most commonly used programs (Swofford, 1998); it features several fast, greedy, and heuristic algorithms based on local branch-swapping operations such as tree bisection and reconnection.\nGFlowNets are a family of methods for sampling discrete objects from multimodal distributions, such as molecules (Bengio et al., 2021) and biological sequences (Jain et al., 2022), and are used to solve discrete optimization tasks such as NP-hard scheduling and optimization problems (Zhang et al., 2023a;b). With their theoretical foundations laid out in Bengio et al. (2023); Lahlou et al. (2023), and connections to variational inference established in Malkin et al. (2023); Zimmermann et al. (2023), GFlowNets have been successfully applied to tackle complex Bayesian inference problems, such as inferring latent causal structures in gene regulatory networks (Deleu et al., 2022; 2023), and, most similar to the problems we consider, parse trees in hierarchical grammars (Hu et al., 2023)."
        },
        {
            "heading": "3 BACKGROUND",
            "text": ""
        },
        {
            "heading": "3.1 PHYLOGENETIC INFERENCE",
            "text": "Here we introduce the problems of Bayesian and parsimony-based phylogenetic inference. A weighted phylogenetic tree is denoted by (\ud835\udc67, \ud835\udc4f), where \ud835\udc67 represents the tree topology with its leaves labeled by observed sequences, and \ud835\udc4f represents the branch lengths. The tree topology can be\n1We refer our readers to section H for a short explanation on the significance of learning suboptimal trees.\neither a rooted binary tree or a bifurcating unrooted tree. For a tree topology \ud835\udc67, let \ud835\udc3f (\ud835\udc67) denote the labeled sequence set and \ud835\udc38 (\ud835\udc67) the set of edges. For an edge \ud835\udc52 \u2208 \ud835\udc38 (\ud835\udc67), let \ud835\udc4f(\ud835\udc52) denote its length. Let \ud835\udc80 = {\ud835\udc9a1, \ud835\udc9a2 . . . \ud835\udc9a\ud835\udc5b} \u2208 \u03a3\ud835\udc5b\u00d7\ud835\udc5a be a set of \ud835\udc5b observed sequences, each having \ud835\udc5a characters from alphabet \u03a3, e.g., {\ud835\udc34,\ud835\udc36, \ud835\udc3a,\ud835\udc47} for DNA sequences. We denote the \ud835\udc56th site of all sequences by \ud835\udc80 \ud835\udc56 = {\ud835\udc9a1 [\ud835\udc56], \ud835\udc9a2 [\ud835\udc56] . . . \ud835\udc9a\ud835\udc5b [\ud835\udc56]}. In this work, we make two assumptions that are common in the phylogenetic inference literature: (i) sites evolve independently; (ii) evolution follows a time-reversible substitution model. The latter implies that an unrooted tree has the same parsimony score or likelihood as its rooted versions, and thus the algorithms we introduce below (Fitch and Felsenstein) apply to unrooted trees as well."
        },
        {
            "heading": "3.1.1 BAYESIAN INFERENCE",
            "text": "In Bayesian phylogenetic inference, we are interested in sampling from the posterior distribution over weighted phylogenetic trees (\ud835\udc67, \ud835\udc4f), formulated as:\n\ud835\udc43(\ud835\udc67, \ud835\udc4f | \ud835\udc80) = \ud835\udc43(\ud835\udc67, \ud835\udc4f)\ud835\udc43(\ud835\udc80 | \ud835\udc67, \ud835\udc4f) \ud835\udc43(\ud835\udc80)\nwhere \ud835\udc43(\ud835\udc80 | \ud835\udc67, \ud835\udc4f) is the likelihood, \ud835\udc43(\ud835\udc80) is the intractable marginal, and \ud835\udc43(\ud835\udc67, \ud835\udc4f) is the prior density over tree topology and branch lengths. Under the site independence assumption, the likelihood can be factorized as: \ud835\udc43(\ud835\udc80 | \ud835\udc67, \ud835\udc4f) = \u220f\ud835\udc56 \ud835\udc43(\ud835\udc80 \ud835\udc56 | \ud835\udc67, \ud835\udc4f), and each factor is obtained by marginalizing over all internal nodes \ud835\udc4e \ud835\udc57 and their possible character assignment:\n\ud835\udc43(\ud835\udc9a1 [\ud835\udc56] . . . \ud835\udc9a\ud835\udc5b [\ud835\udc56] | \ud835\udc67, \ud835\udc4f) = \u2211\ufe01\n\ud835\udc4e\ud835\udc56 \ud835\udc5b+1 ,...\ud835\udc4e \ud835\udc56 2\ud835\udc5b\u22121\n\ud835\udc43(\ud835\udc4e2\ud835\udc5b\u22121) 2\ud835\udc5b\u22122\u220f \ud835\udc57=\ud835\udc5b+1 \ud835\udc43(\ud835\udc4e\ud835\udc56\ud835\udc57 |\ud835\udc4e\ud835\udc56\ud835\udefc( \ud835\udc57 ) , \ud835\udc4f(\ud835\udc52 \ud835\udc57 )) \ud835\udc5b\u220f \ud835\udc58=1 \ud835\udc43(\ud835\udc9a\ud835\udc58 [\ud835\udc56] |\ud835\udc4e\ud835\udc56\ud835\udefc(\ud835\udc58 ) , \ud835\udc4f\ud835\udc67 (\ud835\udc52\ud835\udc58))\nwhere \ud835\udc4e\ud835\udc56 \ud835\udc5b+1, . . . \ud835\udc4e \ud835\udc56 2\ud835\udc5b\u22122 represent the internal node characters assigned to site \ud835\udc56 and \ud835\udefc(\ud835\udc56) represent the parent of node \ud835\udc56. \ud835\udc43(\ud835\udc4e2\ud835\udc5b\u22121) is a distribution at the root node, which is usually assumed to be uniform over the vocabulary, while the conditional probability \ud835\udc43(\ud835\udc4e\ud835\udc56\n\ud835\udc57 |\ud835\udc4e\ud835\udc56 \ud835\udefc( \ud835\udc57 ) , \ud835\udc4f(\ud835\udc52 \ud835\udc57 )) is defined by the\nsubstitution model (where \ud835\udc52 \ud835\udc57 is the edge linking \ud835\udc4e \ud835\udc57 to \ud835\udefc(\ud835\udc4e \ud835\udc57 )). Felsenstein\u2019s algorithm The likelihood of a given weighted phylogenetic tree can be calculated efficiently using Felsenstein\u2019s pruning algorithm (Felsenstein, 1973) in a bottom-up fashion through dynamic programming. Defining \ud835\udc3f\ud835\udc56\ud835\udc62 as the leaf sequence characters at site \ud835\udc56 below the internal node \ud835\udc62, and given its two child nodes \ud835\udc63 and \ud835\udc64, the conditional probability \ud835\udc43(\ud835\udc3f\ud835\udc56\ud835\udc62 |\ud835\udc4e\ud835\udc56\ud835\udc62) can be obtained from \ud835\udc43(\ud835\udc3f\ud835\udc56\ud835\udc63 |\ud835\udc4e\ud835\udc56\ud835\udc63) and \ud835\udc43(\ud835\udc3f\ud835\udc56\ud835\udc64 |\ud835\udc4e\ud835\udc56\ud835\udc64):\n\ud835\udc43(\ud835\udc3f\ud835\udc56\ud835\udc62 | \ud835\udc4e\ud835\udc56\ud835\udc62) = \u2211\ufe01\n\ud835\udc4e\ud835\udc56\ud835\udc63 ,\ud835\udc4e \ud835\udc56 \ud835\udc64 \u2208\u03a3\n\ud835\udc43(\ud835\udc4e\ud835\udc56\ud835\udc63 | \ud835\udc4e\ud835\udc56\ud835\udc62, \ud835\udc4f(\ud835\udc52\ud835\udc63))\ud835\udc43(\ud835\udc3f\ud835\udc56\ud835\udc63 | \ud835\udc4e\ud835\udc56\ud835\udc63)\ud835\udc43(\ud835\udc4e\ud835\udc56\ud835\udc64 | \ud835\udc4e\ud835\udc56\ud835\udc62, \ud835\udc4f(\ud835\udc52\ud835\udc64))\ud835\udc43(\ud835\udc3f\ud835\udc56\ud835\udc64 | \ud835\udc4e\ud835\udc56\ud835\udc64). (1)\nThe dynamic programming, or recursion, is essentially a post-order traversal of the tree and \ud835\udc43(\ud835\udc3f\ud835\udc56\ud835\udc62 | \ud835\udc4e\ud835\udc56\ud835\udc62) is calculated at every internal node \ud835\udc62, and we use one-hot encoding of the sequences to represent the conditional probabilities at the leaves. Finally, the conditional probability for each node \ud835\udc62 at site \ud835\udc56 is stored in a data structure \ud835\udc87 \ud835\udc56\ud835\udc62 \u2208 [0, 1] |\u03a3 | : \ud835\udc87 \ud835\udc56\ud835\udc62 [\ud835\udc50] = \ud835\udc43(\ud835\udc3f\ud835\udc56\ud835\udc62 |\ud835\udc4e\ud835\udc56\ud835\udc62 = \ud835\udc50), and we call it the Felsenstein feature for node \ud835\udc62. Note that the conditional probability at the root \ud835\udc43(\ud835\udc80 \ud835\udc56 |\ud835\udc4e\ud835\udc562\ud835\udc5b\u22121) is used to calculate the likelihood of the tree: \ud835\udc43(\ud835\udc80 \ud835\udc56 |\ud835\udc67, \ud835\udc4f) = \u2211\ud835\udc4e\ud835\udc562\ud835\udc5b\u22121\u2208\u03a3 \ud835\udc43(\ud835\udc4e\ud835\udc562\ud835\udc5b\u22121)\ud835\udc43(\ud835\udc80 \ud835\udc56 |\ud835\udc4e\ud835\udc562\ud835\udc5b\u22121)."
        },
        {
            "heading": "3.1.2 PARSIMONY ANALYSIS",
            "text": "The problem of finding the optimal tree topology under the maximum parsimony principle is commonly referred as the Large Parsimony problem, which is NP-hard. For a given tree topology \ud835\udc67, the parsimony score is the minimum number of character changes between sequences across branches obtained by optimally assigning sequences to internal nodes. Let \ud835\udc40 (\ud835\udc67 |\ud835\udc80) be the parsimony score of tree topology \ud835\udc67 with leaf labels \ud835\udc80 . Due to site independence, \ud835\udc40 (\ud835\udc67 |\ud835\udc80) = \u2211\ud835\udc56 \ud835\udc40 (\ud835\udc67 |\ud835\udc80 \ud835\udc56). The trees with the lowest parsimony score, or most parsimonious trees, are solutions to the Large Parsimony problem. Note that the Large Parsimony problem is a limiting case of the maximum likelihood tree inference problem, where branch lengths are constrained to be equal and infinitesimally short.\nFitch algorithm Given a rooted tree topology \ud835\udc67, the Fitch algorithm assigns optimal sequences to internal nodes and computes the parsimony score in linear time. At each node \ud835\udc62, the algorithm tracks the set of possible characters labeling for node \ud835\udc62 that can yield a most parsimonious solution\nfor the subtree rooted at \ud835\udc62. This character set can be represented by a binary vector \ud835\udc87 \ud835\udc56\ud835\udc62 \u2208 {0, 1} |\u03a3 | for site \ud835\udc56. We label this vector the Fitch feature. As in Felsenstein\u2019s algorithm, this vector is a one-hot encoding of the sequences at the leaves and is computed recursively for non-leaves. Specifically, given a rooted tree with root \ud835\udc62 and two child trees with roots \ud835\udc63 and \ud835\udc64, \ud835\udc87 \ud835\udc56\ud835\udc62 is calculated as:\n\ud835\udc87 \ud835\udc56\ud835\udc62 = { \ud835\udc87 \ud835\udc56\ud835\udc63 \u2227 \ud835\udc87 \ud835\udc56\ud835\udc64 if \ud835\udc87 \ud835\udc56\ud835\udc63 \u00b7 \ud835\udc87 \ud835\udc56\ud835\udc64 \u2260 0 \ud835\udc87 \ud835\udc56\ud835\udc63 \u2228 \ud835\udc87 \ud835\udc56\ud835\udc64 otherwise ,\nwhere \u2227 and \u2228 are element-wise conjunctions and disjunctions. The algorithm traverses the tree two times, first in post-order (bottom-up) to calculate the character set at each node, then in preorder (top-down) to assign optimal sequences. The total number of character changes between these optimal sequences along the tree\u2019s edges is counted as the parsimony score."
        },
        {
            "heading": "3.2 GFLOWNETS",
            "text": "Generative flow networks (GFlowNets) are algorithms for learning generative models of complex distributions given by unnormalized density functions over structured spaces. Here, we give a concise summary of the the GFlowNet framework.\nSetting A GFlowNet treats generation of objects \ud835\udc65 lying in a sample space X as a sequential decision-making problem on an acyclic deterministic MDP with set of states S \u2283 X and set of actions A \u2286 S \u00d7 S. The MDP has a designated initial state \ud835\udc600, which has no incoming actions, and a set of terminal states (those with no outgoing actions) that coincides with X. Any \ud835\udc65 \u2208 X can be reached from \ud835\udc600 by a sequence of actions \ud835\udc600 \u2192 \ud835\udc601 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc60\ud835\udc5b = \ud835\udc65 (with each (\ud835\udc60\ud835\udc56 , \ud835\udc60\ud835\udc56+1) \u2208 A). Such sequences are called complete trajectories, and the set of all complete trajectories is denoted T . A (forward) policy \ud835\udc43\ud835\udc39 is a collection of distributions \ud835\udc43\ud835\udc39 (\ud835\udc60\u2032 | \ud835\udc60) over the children of each nonterminal state \ud835\udc60 \u2208 S \\ X. A policy induces a distribution over T :\n\ud835\udc43\ud835\udc39 (\ud835\udf0f = (\ud835\udc600 \u2192 \ud835\udc601 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc60\ud835\udc5b)) = \ud835\udc5b\u22121\u220f \ud835\udc56=0 \ud835\udc43\ud835\udc39 (\ud835\udc60\ud835\udc56+1 | \ud835\udc60\ud835\udc56).\nA policy gives a way to sample objects in X, by sampling a complete trajectory \ud835\udf0f \u223c \ud835\udc43\ud835\udc39 and returning its final state, inducing a marginal distribution \ud835\udc43\u22a4\n\ud835\udc39 over X; \ud835\udc43\u22a4 \ud835\udc39 (\ud835\udc65) is the sum of \ud835\udc43\ud835\udc39 (\ud835\udf0f) over all\ncomplete trajectories \ud835\udf0f that end in \ud835\udc65 (a possibly intractable sum).\nThe goal of GFlowNet training is to estimate a parametric policy \ud835\udc43\ud835\udc39 (\u00b7 | \u00b7; \ud835\udf03) such that the induced \ud835\udc43\u22a4 \ud835\udc39 is proportional to a given reward function \ud835\udc45 : X \u2192 R>0, i.e.,\n\ud835\udc43\u22a4\ud835\udc39 (\ud835\udc65) = 1 \ud835\udc4d \ud835\udc45(\ud835\udc65) \u2200\ud835\udc65 \u2208 X, (2) where \ud835\udc4d = \u2211 \ud835\udc65\u2208X \ud835\udc45(\ud835\udc65) is the unknown normalization constant (partition function).\nTrajectory balance objective The direct optimization of \ud835\udc43\ud835\udc39\u2019s parameters \ud835\udf03 is impossible since it involves an intractable sum over all complete trajectories. Instead, we leverage the trajectory balance (TB) training objective (Malkin et al., 2022), which introduces two auxiliary objects: an estimate \ud835\udc4d\ud835\udf03 of the partition function and a backward policy. In our experiments, we fix the backward policy to uniform, which results in a simplified objective:\nLTB (\ud835\udf0f) = ( log \ud835\udc4d\ud835\udf03 \u220f\ud835\udc5b\u22121 \ud835\udc56=0 \ud835\udc43\ud835\udc39 (\ud835\udc60\ud835\udc56+1 | \ud835\udc60\ud835\udc56; \ud835\udf03) \ud835\udc45(\ud835\udc65)\ud835\udc43\ud835\udc35 (\ud835\udf0f | \ud835\udc65) )2 , \ud835\udc43\ud835\udc35 (\ud835\udf0f | \ud835\udc65) := \ud835\udc5b\u220f \ud835\udc56=1 1 |Pa(\ud835\udc60\ud835\udc56) | , (3)\nwhere Pa(\ud835\udc60) denotes the set of parents of \ud835\udc60. By the results of Malkin et al. (2022), there exists a unique policy \ud835\udc43\ud835\udc39 and scalar \ud835\udc4d\ud835\udf03 that simultaneously make LTB (\ud835\udf0f) = 0 for all \ud835\udf0f \u2208 T , and at this optimum, the policy \ud835\udc43\ud835\udc39 satisfies (2) and \ud835\udc4d\ud835\udf03 equals the true partition function \ud835\udc4d . In practice, the policy \ud835\udc43\ud835\udc39 (\u00b7 | \ud835\udc60; \ud835\udf03) is parameterized as a neural network that outputs the logits of actions \ud835\udc60 \u2192 \ud835\udc60\u2032 given a representation of the state \ud835\udc60 as input, while \ud835\udc4d\ud835\udf03 is parameterized in the log domain.\nLTB (\ud835\udf0f) is minimized by gradient descent on trajectories \ud835\udf0f chosen from a behaviour policy that can encourage exploration to accelerate mode discovery (see our behaviour policy choices in \u00a74.3)."
        },
        {
            "heading": "4 PHYLOGENETIC INFERENCE WITH GFLOWNETS",
            "text": ""
        },
        {
            "heading": "4.1 GFLOWNETS FOR BAYESIAN PHYLOGENETIC INFERENCE",
            "text": "This section introduces PhyloGFN-Bayesian, our GFlowNet-based method for Bayesian phylogenetic inference. Given a set of observed sequences \ud835\udc80 , PhyloGFN-Bayesian learns a sampler over\nthe joint space of tree topologies and edge lengths X = {(\ud835\udc67, \ud835\udc4f) |\ud835\udc3f (\ud835\udc67) = \ud835\udc80} such that the sampling probability of (\ud835\udc67, \ud835\udc4f) \u2208 X approximates its posterior \ud835\udc43\u22a4\n\ud835\udc39 (\ud835\udc67, \ud835\udc4f) = \ud835\udc43(\ud835\udc67, \ud835\udc4f |\ud835\udc80).\nWe follow the same setup as (Koptagel et al., 2022; Zhang, 2023; Mimori & Hamada, 2023): (i) uniform prior over tree topologies; (ii) decomposed prior \ud835\udc43(\ud835\udc67, \ud835\udc4f) = \ud835\udc43(\ud835\udc67)\ud835\udc43(\ud835\udc4f); (iii) exponential (\ud835\udf06 = 10) prior over branch lengths; (iv) Jukes-Cantor substitution model.\nGFlowNet state and action space The sequential procedure of constructing phylogenetic trees is illustrated in Fig. 1. The initial state \ud835\udc600 is a set of \ud835\udc5b rooted trees, each containing a single leaf node labeled with an observed sequence. Each action chooses a pair of trees and joins their roots by a common parent node, thus creating a new tree. The number of rooted trees in the set is reduced by 1 at every step, so after \ud835\udc5b \u2212 1 steps a single rooted tree with \ud835\udc5b leaves is left. To obtain an unrooted tree, we simply remove the root node. Thus, a state \ud835\udc60 consists of a set of disjoint rooted trees \ud835\udc60 = ((\ud835\udc671, \ud835\udc4f1), . . . , (\ud835\udc67\ud835\udc59 , \ud835\udc4f\ud835\udc59)), \ud835\udc59 \u2264 \ud835\udc5b and\u22c3 \ud835\udc56 \ud835\udc3f (\ud835\udc67\ud835\udc56) = \ud835\udc80 . Given a nonterminal state with \ud835\udc59 > 1 trees, a transition action consists of two steps:\n(i) choosing a pair of trees to join out of the ( \ud835\udc59 2 )\npossible pairs; and (ii) generating branch lengths for the two introduced edges between the new root and its two children. The distribution over the pair of branch lengths is modeled jointly as a discrete distribution with fixed bin size.\nReward function We define the reward function as the product of the likelihood and the edge length prior: \ud835\udc45(\ud835\udc67, \ud835\udc4f) = \ud835\udc43(\ud835\udc80 |\ud835\udc67, \ud835\udc4f)\ud835\udc43(\ud835\udc4f), implicitly imposing a uniform prior over tree topologies. By training with this reward, PhyloGFN learns to approximate the posterior, since \ud835\udc43(\ud835\udc67, \ud835\udc4f |\ud835\udc80) = \ud835\udc45(\ud835\udc67, \ud835\udc4f) \ud835\udc43 (\ud835\udc67)\n\ud835\udc43 (\ud835\udc80 ) and \ud835\udc43(\ud835\udc67), \ud835\udc43(\ud835\udc80) are both constant.\nIt is worth emphasizing that in our bottom-up construction of trees, the set of possible actions at the steps that select two trees to join by a new common root is never larger than \ud835\udc5b2, even though the size of the space of all tree topologies \u2013 all of which can be reached by our sampler \u2013 is superexponential in \ud835\udc5b. This stands in contrast to the modeling choices of VBPI-GNN (Zhang, 2023), which constructs trees in a top-down fashion and limits the action space using a pre-generated set of trees, therefore also limiting the set of trees it can sample.\nState representation To represent a rooted tree in a non-terminal state, we compute features for each site independently by taking advantage of the Felsenstein features (\u00a73.1.1). Let (\ud835\udc67, \ud835\udc4f) be a weighted tree with root \ud835\udc62 which has two children \ud835\udc63 and \ud835\udc64. Let \ud835\udc87 \ud835\udc56\ud835\udc62, \ud835\udc87 \ud835\udc56 \ud835\udc63 , \ud835\udc87 \ud835\udc56 \ud835\udc64 \u2208 [0, 1] |\u03a3 | be the Felsenstein feature on nodes \ud835\udc62, \ud835\udc63, \ud835\udc64 at site \ud835\udc56. The representation \ud835\udf0c\ud835\udc56\ud835\udc62 for site \ud835\udc56 is computed as following:\n\ud835\udf0c\ud835\udc56\ud835\udc62 = \ud835\udc87 \ud835\udc56 \ud835\udc62 \u220f \ud835\udc52\u2208\ud835\udc38 (\ud835\udc67) \ud835\udc43(\ud835\udc4f\ud835\udc67 (\ud835\udc52)) 1 \ud835\udc5a (4)\nwhere \ud835\udc43(\ud835\udc4f\ud835\udc67 (\ud835\udc52)) = \u220f \ud835\udc52\u2208\ud835\udc4f\ud835\udc67 \ud835\udc43(\ud835\udc4f(\ud835\udc52)) is the edge length prior. The tree-level feature is the concatenation of site-level features \ud835\udf0c = [\ud835\udf0c1 . . . \ud835\udf0c\ud835\udc5a]. A state \ud835\udc60 = (\ud835\udc671 . . . \ud835\udc67\ud835\udc59), which is a collection of rooted trees, is represented by the set {\ud835\udf0c1, . . . , \ud835\udf0c\ud835\udc59}. Representation power Although the proposed feature representation \ud835\udf0c does not capture all the information of tree structure and leaf sequences, we show that \ud835\udf0c indeed contains sufficient information to express the optimal policy. The proposition below shows that given an optimal GFlowNet with uniform \ud835\udc43\ud835\udc35, two states with identical features set share the same transition probabilities. Proposition 1. Let \ud835\udc601 = {(\ud835\udc671, \ud835\udc4f1), (\ud835\udc672, \ud835\udc4f2) . . . (\ud835\udc67\ud835\udc59 , \ud835\udc4f\ud835\udc59)} and \ud835\udc602 = {(\ud835\udc67\u20321, \ud835\udc4f \u2032 1), (\ud835\udc67 \u2032 2, \ud835\udc4f \u2032 2) . . . (\ud835\udc67 \u2032 \ud835\udc59 , \ud835\udc4f\u2032 \ud835\udc59 )} be two non-terminal states such that \ud835\udc601 \u2260 \ud835\udc602 but sharing the same features \ud835\udf0c\ud835\udc56 = \ud835\udf0c\u2032\ud835\udc56 . Let \ud835\udc82 be any sequence of actions, which applied to \ud835\udc601 and \ud835\udc602, respectively, results in full weighted trees \ud835\udc65 = (\ud835\udc67, \ud835\udc4f\ud835\udc67), \ud835\udc65\u2032 = (\ud835\udc67\u2032, \ud835\udc4f\u2032), with two partial trajectories \ud835\udf0f = (\ud835\udc601 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc65), \ud835\udf0f\u2032 = (\ud835\udc602 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc65\u2032). If \ud835\udc43\ud835\udc39 is the policy of an optimal GFlowNet with uniform \ud835\udc43\ud835\udc35, then \ud835\udc43\ud835\udc39 (\ud835\udf0f) = \ud835\udc43\ud835\udc39 (\ud835\udf0f\u2032).\nAll proofs are in Appendix A. The proposition shows that our proposed features have sufficient representation power for the PhyloGFN-Bayesian policy. Furthermore, Felsenstein features and edge length priors are used in calculating reward by Felsenstein\u2019s algorithm. Therefore, computing these features does not introduce any additional variables, and computation overhead is minimized."
        },
        {
            "heading": "4.2 GFLOWNETS FOR PARSIMONY ANALYSIS",
            "text": "This section introduces PhyloGFN-Parsimony, our GFlowNet-based method for parsimony analysis. We treat large parsimony analysis, a discrete optimization problem, as a sampling problem from the energy distribution exp ( \u2212\ud835\udc40 (\ud835\udc67 |\ud835\udc80 )\n\ud835\udc47\n) defined over tree topologies. Here, \ud835\udc40 (\ud835\udc67 |\ud835\udc80) is the parsimony\nscore of \ud835\udc67 and \ud835\udc47 is a pre-defined temperature term to control the smoothness of distribution. With sufficiently small \ud835\udc47 , the most parsimonious trees dominate the energy distribution. To state our goals formally, given observed sequences \ud835\udc80 , PhyloGFN-Parsimony learns a sampling policy \ud835\udc43\ud835\udc39 over the space of tree topologies {\ud835\udc67 |\ud835\udc3f (\ud835\udc67) = \ud835\udc80} such that \ud835\udc43\u22a4\n\ud835\udc39 (\ud835\udc67) \u221d \ud835\udc52\u2212\n\ud835\udc40 (\ud835\udc67 |\ud835\udc4c ) \ud835\udc47 . As \ud835\udc47 \u2192 0, this target distribution\napproaches a uniform distribution over the set of tree topologies with minimum parsimony scores.\nPhyloGFN-Parsimony can be seen as a reduced version of PhyloGFN-Bayesian. The tree shape generation procedure is the same as before, but we no longer generate branch lengths. The reward is defined as \ud835\udc45(\ud835\udc67) = exp ( \ud835\udc36\u2212\ud835\udc40 (\ud835\udc67 |\ud835\udc4c )\n\ud835\udc47\n) , where \ud835\udc36 is an extra hyperparameter introduced for stability to\noffset the typically large \ud835\udc40 (\ud835\udc67 |\ud835\udc80) values. Note that \ud835\udc36 can be absorbed into the partition function and has no influence on the reward distribution.\nSimilar to PhyloGFN-Bayesian, a state (collection of rooted trees) is represented by the set of tree features, with each tree represented by concatenating its site-level features. With \ud835\udc67 the rooted tree topology with root \ud835\udc62, we represent the tree at site \ud835\udc56 by its root level Fitch feature \ud835\udc87 \ud835\udc56\ud835\udc62 defined in \u00a73.1.2. The proposition below, analogous to Proposition 1, shows the representation power of the proposed feature. Proposition 2. Let \ud835\udc601 = {\ud835\udc671, \ud835\udc672, . . . \ud835\udc67\ud835\udc59} and \ud835\udc602 = {\ud835\udc67\u20321, \ud835\udc67 \u2032 2, . . . \ud835\udc67 \u2032 \ud835\udc59 } be two non-terminal states such that \ud835\udc601 \u2260 \ud835\udc602 but sharing the same Fitch features \ud835\udc87\ud835\udc67\ud835\udc56 = \ud835\udc87\ud835\udc67\u2032\ud835\udc56 \u2200\ud835\udc56. Let \ud835\udc82 be any sequence of actions, which, applied to \ud835\udc601 and \ud835\udc602, respectively, results in tree topologies \ud835\udc65, \ud835\udc65\u2032 \u2208 Z, with two partial trajectories \ud835\udf0f = (\ud835\udc601 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc65), \ud835\udf0f\u2032 = (\ud835\udc602 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc65\u2032). If \ud835\udc43\ud835\udc39 is the policy of an optimal GFlowNet with uniform \ud835\udc43\ud835\udc35, then \ud835\udc43\ud835\udc39 (\ud835\udf0f) = \ud835\udc43\ud835\udc39 (\ud835\udf0f\u2032)\nThis shows that the Fitch features contain sufficient information for PhyloGFN-Parsimony. Furthermore, the Fitch features are used in the computation of the reward by Fitch\u2019s algorithm, so their use in the policy model does not introduce additional variables or extra computation.\nTemperature-conditioned PhyloGFN The temperature \ud835\udc47 controls the trade-off between sample diversity and parsimony scores. Following Zhang et al. (2023a), we extend PhyloGFN-Parsimony by conditioning the policy on \ud835\udc47 , with reward \ud835\udc45(\ud835\udc67;\ud835\udc47) = exp ( \ud835\udc36\u2212\ud835\udc40 (\ud835\udc67 |\ud835\udc80 )\n\ud835\udc47\n) , and we learn a sampler\nsuch that \ud835\udc43\u22a4 (\ud835\udc67;\ud835\udc47) \u221d \ud835\udc45(\ud835\udc67;\ud835\udc47). See Appendix E for more details."
        },
        {
            "heading": "4.3 MODEL ARCHITECTURE AND TRAINING",
            "text": "Parameterization of forward transitions We parameterize the forward transitions of tree topology construction using a Transformer-based neural network, whose architecture is shown in Fig. 1.\nWe select Transformer because the input is a set and the model needs to be order-equivariant. For a state consisting of \ud835\udc5b trees, after \ud835\udc5b embeddings are generated from the Transformer encoder, (\ud835\udc5b 2 ) pairwise features are created for all possible pairs of trees, and a common MLP generates probability logits for jointing every tree pair. PhyloGFN-Bayesian additionally requires generating edge lengths. Once the pair of trees to join is selected, another MLP is applied to the corresponding pair feature to generate probability logits for sampling the edge lengths. More details are in Appendix D.\nOff-policy training The action model \ud835\udc43\ud835\udc39 (\u00b7 | \ud835\udc60; \ud835\udf03) is trained with the trajectory balance objective. Training data are generated from two sources: (i) A set of trajectories constructed from the currently trained GFlowNet, with actions sampled from the policy with probability 1 \u2212 \ud835\udf16 and uniformly at random with probability \ud835\udf16 . The \ud835\udf16 rate drops from a pre-defined \ud835\udf16\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61 to near 0 during the course of training. (ii) Trajectories corresponding to the best trees seen to date (replay buffer). Trajectories are sampled backward from these high-reward trees with uniform backward policy.\nTemperature annealing For PhyloGFN-Parsimony, it is crucial to choose the appropriate temperature \ud835\udc47 . Large \ud835\udc47 defines a flat target distribution, while small \ud835\udc47 makes the reward landscape less smooth and leads to training difficulties. We cannot predetermine the ideal choice of \ud835\udc47 before training, as we do not know a priori the parsimony score for the dataset. Therefore, we initialize the training with large \ud835\udc47 and reduce \ud835\udc47 periodically during training. See Appendix E for details."
        },
        {
            "heading": "4.4 MARGINAL LOG-LIKELIHOOD ESTIMATION",
            "text": "To assess how well the GFlowNet sampler approximates the true posterior distribution, we use the following importance-weighted variational lower bound on the marginal log-likelihood (MLL):\nlog \ud835\udc43(\ud835\udc80) \u2265 E\ud835\udf0f1 ,...,\ud835\udf0f\ud835\udc58\u223c\ud835\udc43\ud835\udc39 log \u00a9\u00ab\ud835\udc43(\ud835\udc67) 1 \ud835\udc3e\n\ud835\udc58\u2211\ufe01 \ud835\udf0f\ud835\udc56\n\ud835\udf0f\ud835\udc56 :\ud835\udc600\u2192\u00b7\u00b7\u00b7\u2192(\ud835\udc67\ud835\udc56 ,\ud835\udc4f\ud835\udc56 )\n\ud835\udc43\ud835\udc35 (\ud835\udf0f\ud835\udc56 |\ud835\udc67\ud835\udc56 , \ud835\udc4f\ud835\udc56)\ud835\udc45(\ud835\udc67\ud835\udc56 , \ud835\udc4f\ud835\udc56) \ud835\udc43\ud835\udc39 (\ud835\udf0f\ud835\udc56) \u00aa\u00ae\u00ae\u00ac . (5) Our estimator is computed by sampling a batch of \ud835\udc3e trajectories and averaging \ud835\udc43 (\ud835\udc67)\ud835\udc43\ud835\udc35 (\ud835\udf0f |\ud835\udc67,\ud835\udc4f)\ud835\udc45 (\ud835\udc67,\ud835\udc4f) \ud835\udc43\ud835\udc39 (\ud835\udf0f ) over the batch. This expectation of this estimate is guaranteed to be a lower bound on log \ud835\udc43(\ud835\udc80) and its bias decreases as \ud835\udc3e grows (Burda et al., 2016).\nPhyloGFN-Bayesian models branch lengths with discrete multinomial distributions, while in reality these are continuous variables. To properly estimate the MLL and compare with other methods defined over continuous space, we augment our model to a continuous sampler by performing random perturbations over edges of trees sampled from PhyloGFN-Bayesian. The perturbation follows the uniform distribution U[\u22120.5\ud835\udf14,0.5\ud835\udf14 ] , where \ud835\udf14 is the fixed bin size for edge modeling in PhyloGFNBayesian. The resulting model over branch lengths is then a piecewise-constant continuous distribution. We discuss the computation details as well as the derivation of (5) in Appendix B."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We evaluate PhyloGFN on a suite of 8 real-world benchmark datasets (Table S1 in Appendix C) that is standard in the literature. These datasets feature pre-aligned sequences and vary in difficulty (27 to 64 sequences; 378 to 2520 sites). In the following sections we present our results and analysis on Bayesian and parsimony-based phylogenetic inference.\n5.1 BAYESIAN PHYLOGENETIC INFERENCE\n210 220 230 240 250\n6895 6890 6885 6880 6875 6870 6865 6860 Un no rm al ize d L og P os te rio r D en sit y\nGFN on 0% random trees\n220 230 240 250 6895\n6890\n6885\n6880\n6875\n6870\n6865\n6860 VBPI-GNN on 0% random trees\n50 100 150 200 250\n7050\n7000\n6950\n6900\nUn no\nrm al\nize d\nL og\nP os\nte rio\nr D en\nsit y\nGFN on 30% random trees\n100 150 200 250 300 350\n7050\n7000\n6950\n6900\nVBPI-GNN on 30% random trees\n50 100 150 200 250 Sampling Log Density\n7050\n7000\n6950\n6900\nUn no\nrm al\nize d\nL og\nP os\nte rio\nr D en\nsit y\nGFN on 50% random trees\n100 150 200 250 300 Sampling Log Density\n7050\n7000\n6950\n6900\nVBPI-GNN on 50% random trees\nFigure 2: Model sampling log-density vs. unnormalized posterior log-density for high/medium/low-probability trees on DS1. We highlight that PhyloGFN-Bayesian performs significantly better on medium- and low-probability trees, highlighting its superiority in modeling the entire data space.\nPhyloGFN is compared with a variety of baselines in terms of sampling-based estimates of marginal loglikelihood (MLL; see details in \u00a74.4). The baselines we compare to are MrBayes SS, Stepping-Stone sampling algorithm implemented in MrBayes (Ronquist et al., 2012), and three variational inference methods: VBPI-GNN (Zhang, 2023), \ud835\udf19-CSMC proposed in VaiPhy (Koptagel et al., 2022), and GeoPhy (Mimori & Hamada, 2023). The sampling setup for MrBayes follows Zhang & Matsen IV (2018b) and otherwise show the highest MLL reported in the respective papers. PhyloGFN MLL is estimated following the formulation in Section 4.4, with mean and standard deviation obtained from 10 repetitions, each using 1000 samples. See Sections E and E for additional training details, hardware specifications, and running time comparison. Additional results from two repeated experiments are in Table S4.\nThe results are summarized in Table 1. Our PhyloGFN is markedly superior to \ud835\udf19-CSMC across all datasets and outperforms GeoPhy on most, with the exception of DS2 and DS3 where the two perform similarly, and DS7 where GeoPhy obtains a better result. VBPI-GNN, is the only machine learningbased method that performs on par against MrBayes, the current gold standard in Bayesian phylogenetic inference. However, it should be emphasized that\nVBPI-GNN requires a set of pre-defined tree topologies that are likely to achieve high likelihood, and as a consequence, its training and inference are both constrained to a small space of tree topologies. On the other hand, PhyloGFN operates on the full space of tree topologies and, in fact, achieves a closer fit to the true posterior distribution. To show this, for each dataset, we created three sets of phylogenetic trees with high/medium/low posterior probabilities and obtained the corresponding sampling probabilities from PhyloGFN and VBPI-GNN. The three classes of trees are generated from VBPI-GNN by randomly inserting uniformly sampled actions into its sequential tree topology construction process with 0%, 30%, or 50% probability, respectively, which circumvents VBPIGNN\u2019s limitation of being confined to a small tree topology space.\nTable 2 and Fig. 2 show that PhyloGFN achieves higher Pearson correlation between the sampling log-probability and the unnormalized ground truth posterior log-density for the majority of datasets and classes of trees. In particular, while VBPI-GNN performs better on high-probability trees, its correlation drops significantly on lower-probability trees. On the other hand, PhyloGFN maintains a high correlation for all three classes of trees across all datasets, the only exception being the highprobability trees in DS7. See Appendix J for details and extended results."
        },
        {
            "heading": "5.2 PARSIMONY-BASED PHYLOGENETIC INFERENCE",
            "text": "As a special case of Bayesian phylogenetic inference, the parsimony problem is concerned with finding the most-parsimonious trees \u2013 a task which is also amenable to PhyloGFN. Here, we compare to the state-of-the-art parsimony analysis software PAUP* (Swofford, 1998). For all datasets, our PhyloGFN and PAUP* are able to identify the same set of optimal solutions to the Large Parsimony problem, ranging from a single optimal tree for DS1 to 21 optimal trees for DS8.\nAlthough the results are similar between PhyloGFN and PAUP*, once again we emphasize that PhyloGFN is based on a rigorous mathematical framework of fitting and sampling from well-defined posterior distributions over tree topologies. whereas PAUP*\u2019s relies on heuristic algorithms. To put it more concretely, we show in Fig. S2 that PhyloGFN is able to (i) learn a smooth echelon of sampling probabilities that distinguish the optimal trees from suboptimal ones; (ii) learn similar sampling probabilities for trees within each group of equally-parsimonious trees; and (iii) fit all 2\ud835\udc5b \u2212 3 rooted trees that belong to the same unrooted tree equally well. Finally, Fig. 3 shows that a single temperature-conditioned PhyloGFN can sample phylogenetic trees of varying ranges of parsimony scores by providing suitable temperatures \ud835\udc47 as input to the model. Also, PhyloGFN is able to sample proportionately from the Boltzmann distribution defined at different temperatures and achieves high correlation between sampling log-probability and logreward. Although the temperature-conditioned PhyloGFN has only been trained on a small range of temperatures between 4.0 and 1.0, Fig. 3 shows it can also approximate the distribution defined at temperature 8.0. Further results are presented in Appendix K."
        },
        {
            "heading": "6 DISCUSSION AND FUTURE WORK",
            "text": "In this paper, we propose PhyloGFN, a GFlowNet-based generative modeling algorithm, to solve parsimony-based and Bayesian phylogenetic inference. We design an intuitive yet effective tree construction procedure to efficiently model the entire tree topology space. We propose a novel tree representation based on Fitch\u2019s and Felsenstein\u2019s algorithms to represent rooted trees without introducing additional learnable parameters, and we show the sufficiency of our features for the purpose of phylogenetic tree generation. We apply our algorithm on eight real datasets, demonstrating that PhyloGFN is competitive with or superior to prior works in terms of marginal likelihood estimation, while achieving a closer fit to the target distribution compared to state-of-the-art variational inference methods. Future work should consider continuous edge length sampling in place of the current binning procedure, e.g., using a continuous GFlowNet policy (Lahlou et al., 2023). In addition, we plan to explore the use of conditional GFlowNets to amortize the dependence on the sequence dataset itself. This would allow a single trained GFlowNet to sample phylogenetic trees for sets of sequences that were not seen during training.\nREPRODUCIBILITY\nCode and data are attached as supplementary material. See Appendix I for a detailed description."
        },
        {
            "heading": "A PROOFS OF PROPOSITIONS 1 AND 2",
            "text": "Before proving proposition 1, we first prove the following two lemmas. First, we show that for two states sharing the same tree features, applying the same action to the states results in two new states still sharing the same features. Lemma 1. Let \ud835\udc601 = {(\ud835\udc671, \ud835\udc4f1), (\ud835\udc672, \ud835\udc4f2) . . . (\ud835\udc67\ud835\udc59 , \ud835\udc4f\ud835\udc59)} and \ud835\udc602 = {(\ud835\udc67\u20321, \ud835\udc4f \u2032 1), (\ud835\udc67 \u2032 2, \ud835\udc4f \u2032 2) . . . (\ud835\udc67 \u2032 \ud835\udc59 , \ud835\udc4f\u2018\ud835\udc59)} be two non-terminating states sharing the same features \ud835\udf0c\ud835\udc56 = \ud835\udf0c\u2032\ud835\udc56 . Let a be the action that joins the trees with indices (\ud835\udc63, \ud835\udc64) to form a new tree indexed \ud835\udc62 with edge lengths (\ud835\udc4f(\ud835\udc52\ud835\udc62\ud835\udc63), \ud835\udc4f(\ud835\udc52\ud835\udc62\ud835\udc64)). By applying \ud835\udc4e on \ud835\udc601, we join (\ud835\udc67\ud835\udc63 , \ud835\udc4f\ud835\udc63) and (\ud835\udc67\ud835\udc64 , \ud835\udc4f\ud835\udc64) to form new tree (\ud835\udc67\ud835\udc62, \ud835\udc4f\ud835\udc62). By applying \ud835\udc4e on \ud835\udc602, we join (\ud835\udc67\u2032\ud835\udc63 , \ud835\udc4f\u2032\ud835\udc63) and (\ud835\udc67\u2032\ud835\udc64 , \ud835\udc4f\u2032\ud835\udc64) to form new tree (\ud835\udc67\u2032\ud835\udc62, \ud835\udc4f\u2032\ud835\udc62). Then the new trees\u2019 features are equal: \ud835\udf0c\ud835\udc62 = \ud835\udf0c\u2032\ud835\udc62.\nProof. We show that \ud835\udf0c\ud835\udc62 can be calculated from \ud835\udf0c\ud835\udc63 and \ud835\udf0c\ud835\udc64:\n\ud835\udf0c\ud835\udc56\ud835\udc62 [ \ud835\udc57] = \ud835\udc43(\ud835\udc4f(\ud835\udc52\ud835\udc62\ud835\udc63)) 1 \ud835\udc5a \u00d7 \ud835\udc43(\ud835\udc4f(\ud835\udc52\ud835\udc62\ud835\udc64)) 1 \ud835\udc5a \u00d7 \u2211\ufe01 \ud835\udc58\u2208\u03a3 \ud835\udc43(\ud835\udc4e\ud835\udc56\ud835\udc62 = \ud835\udc57 |\ud835\udc4e\ud835\udc56\ud835\udc63 = \ud835\udc58, \ud835\udc4f\ud835\udc67 (\ud835\udc52\ud835\udc62\ud835\udc63))\ud835\udf0c\ud835\udc56\ud835\udc63 [\ud835\udc58]\n\u00d7 \u2211\ufe01 \ud835\udc58 \ud835\udc43(\ud835\udc4e\ud835\udc56\ud835\udc62 = \ud835\udc57 |\ud835\udc4e\ud835\udc56\ud835\udc64 = \ud835\udc58, \ud835\udc4f(\ud835\udc52\ud835\udc62\ud835\udc64))\ud835\udf0c\ud835\udc56\ud835\udc64 [\ud835\udc58]\nsince \ud835\udf0c\ud835\udc63 = \ud835\udf0c\u2032\ud835\udc63 , \ud835\udf0c\ud835\udc64 = \ud835\udf0c \u2032 \ud835\udc64 and (\ud835\udc4f(\ud835\udc52\ud835\udc62\ud835\udc63), \ud835\udc4f(\ud835\udc52\ud835\udc62\ud835\udc64)) are new branch lengths for both two trees. Therefore \ud835\udf0c\ud835\udc62 = \ud835\udf0c\u2032\ud835\udc62 \u25a1\nNext, we show that for two states sharing the same tree features, applying the same action sequences results in two phylogenetic trees with the same reward. Lemma 2. Let \ud835\udc601 = {(\ud835\udc671, \ud835\udc4f1), (\ud835\udc672, \ud835\udc4f2) . . . (\ud835\udc67\ud835\udc59 , \ud835\udc4f\ud835\udc59)} and \ud835\udc602 = {(\ud835\udc67\u20321, \ud835\udc4f \u2032 1), (\ud835\udc67 \u2032 2, \ud835\udc4f \u2032 2) . . . (\ud835\udc67 \u2032 \ud835\udc59 , \ud835\udc4f\u2032 \ud835\udc59 )} be two non-terminating states sharing the same features \ud835\udf0c\ud835\udc56 = \ud835\udf0c\u2032\ud835\udc56 . Let \ud835\udc82 be any sequence of actions to apply on \ud835\udc601 and \ud835\udc602 to form full trees \ud835\udc65 = (\ud835\udc67, \ud835\udc4f\ud835\udc67), \ud835\udc65\u2032 = (\ud835\udc67\u2032, \ud835\udc4f\u2032\ud835\udc67), \ud835\udc45(\ud835\udc67, \ud835\udc4f) = \ud835\udc45(\ud835\udc67\u2032, \ud835\udc4f\u2032).\nProof. Let \ud835\udf0c\ud835\udc62 denote the tree feature for (\ud835\udc67, \ud835\udc4f), \ud835\udf0c\ud835\udc56\ud835\udc62 = \ud835\udc87 \ud835\udc56\ud835\udc62 \u220f \ud835\udc52 \ud835\udc43(\ud835\udc4f(\ud835\udc52)). We first show that the reward\ncan be directly calculated from the root feature \ud835\udf0c\ud835\udc62:\u220f \ud835\udc56 \ud835\udc43(\ud835\udc4e2\ud835\udc5b\u22121) \u00b7 \ud835\udf0c\ud835\udc62 = \u220f \ud835\udc52 \ud835\udc43(\ud835\udc4f(\ud835\udc52)) \u220f \ud835\udc56 \ud835\udc43(\ud835\udc4e2\ud835\udc5b\u22121) \u00b7 \ud835\udc53 \ud835\udc56\ud835\udc62\n= \ud835\udc43(\ud835\udc4f)\ud835\udc43(\ud835\udc80 |\ud835\udc67, \ud835\udc4f) = \ud835\udc45(\ud835\udc67, \ud835\udc4f),\nwhere \ud835\udc43(\ud835\udc4e2\ud835\udc5b\u22121) is the constant root character assignment probability. As \ud835\udc82 is applied to \ud835\udc601 and \ud835\udc602 in a sequential manner, at every step we obtain two state swith the same tree features (by Lemma 1), until, at the final state, \ud835\udf0c\ud835\udc62 = \ud835\udf0c\u2032\ud835\udc62. It follows that \ud835\udc45(\ud835\udc67, \ud835\udc4f) = \ud835\udc45(\ud835\udc67\u2032, \ud835\udc4f\u2032). \u25a1\nWe are now ready to prove the propositions. Proposition 1. Let \ud835\udc601 = {(\ud835\udc671, \ud835\udc4f1), (\ud835\udc672, \ud835\udc4f2) . . . (\ud835\udc67\ud835\udc59 , \ud835\udc4f\ud835\udc59)} and \ud835\udc602 = {(\ud835\udc67\u20321, \ud835\udc4f \u2032 1), (\ud835\udc67 \u2032 2, \ud835\udc4f \u2032 2) . . . (\ud835\udc67 \u2032 \ud835\udc59 , \ud835\udc4f\u2032 \ud835\udc59 )} be two non-terminal states such that \ud835\udc601 \u2260 \ud835\udc602 but sharing the same features \ud835\udf0c\ud835\udc56 = \ud835\udf0c\u2032\ud835\udc56 . Let \ud835\udc82 be any sequence of actions, which applied to \ud835\udc601 and \ud835\udc602, respectively, results in full weighted trees \ud835\udc65 = (\ud835\udc67, \ud835\udc4f\ud835\udc67), \ud835\udc65\u2032 = (\ud835\udc67\u2032, \ud835\udc4f\u2032), with two partial trajectories \ud835\udf0f = (\ud835\udc601 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc65), \ud835\udf0f\u2032 = (\ud835\udc602 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc65\u2032). If \ud835\udc43\ud835\udc39 is the policy of an optimal GFlowNet with uniform \ud835\udc43\ud835\udc35, then \ud835\udc43\ud835\udc39 (\ud835\udf0f) = \ud835\udc43\ud835\udc39 (\ud835\udf0f\u2032).\nProof of Proposition 1. Let G\ud835\udc601 ,G\ud835\udc602 be sub-graphs of the GFlowNet state graph G = (S,A) defined by reachable states from \ud835\udc601 and \ud835\udc602 in G. Since \ud835\udc601 and \ud835\udc602 have the same number of trees, G\ud835\udc601 and G\ud835\udc602 have the graph structure. Let X1,X2 \u2286 X be the terminal states reachable from \ud835\udc601 and \ud835\udc602. There is thus a bijective correspondence between X1 and X2: for every action set \ud835\udc82 applying on \ud835\udc601 to obtain \ud835\udc65 \u2208 X1, we obtain \ud835\udc65\u2032 \u2208 X2 by applying \ud835\udc82 on \ud835\udc602. Let \ud835\udf0f and \ud835\udf0f\u2032 be the partial trajectories created by applying \ud835\udc82 on \ud835\udc601 and \ud835\udc602, \ud835\udc43\ud835\udc35 (\ud835\udf0f |\ud835\udc65) = \ud835\udc43\ud835\udc35 (\ud835\udf0f\u2032 |\ud835\udc65\u2032). Moreover, \ud835\udc45(\ud835\udc65) = \ud835\udc45(\ud835\udc65\u2032) since \ud835\udc601 and \ud835\udc602 share the same set of features. We have the following expressions for \ud835\udc43\ud835\udc39 (\ud835\udf0f) and \ud835\udc43\ud835\udc39 (\ud835\udf0f\u2032):\n\ud835\udc43\ud835\udc39 (\ud835\udf0f) = \ud835\udc45(\ud835\udc65)\ud835\udc43\ud835\udc35 (\ud835\udf0f |\ud835\udc65)\u2211\n\ud835\udf0f \ud835\udc57 ,\ud835\udc65 \ud835\udc57 \ud835\udc45(\ud835\udc65 \ud835\udc57 )\ud835\udc43\ud835\udc35 (\ud835\udf0f\ud835\udc57 |\ud835\udc65 \ud835\udc57 )\n, \ud835\udc43\ud835\udc39 (\ud835\udf0f\u2032) = \ud835\udc45(\ud835\udc65\u2032)\ud835\udc43\ud835\udc35 (\ud835\udf0f\u2032 |\ud835\udc65\u2032)\u2211\n\ud835\udf0f\u2032 \ud835\udc57 ,\ud835\udc65\u2032 \ud835\udc57 \ud835\udc45(\ud835\udc65\u2032 \ud835\udc57 )\ud835\udc43\ud835\udc35 (\ud835\udf0f\u2032\ud835\udc57 |\ud835\udc65\u2032\ud835\udc57 )\n.\nHence \ud835\udc43\ud835\udc39 (\ud835\udf0f) = \ud835\udc43\ud835\udc39 (\ud835\udf0f\u2032). \u25a1\nProposition 2. Let \ud835\udc601 = {\ud835\udc671, \ud835\udc672, . . . \ud835\udc67\ud835\udc59} and \ud835\udc602 = {\ud835\udc67\u20321, \ud835\udc67 \u2032 2, . . . \ud835\udc67 \u2032 \ud835\udc59 } be two non-terminal states such that \ud835\udc601 \u2260 \ud835\udc602 but sharing the same Fitch features \ud835\udc87\ud835\udc67\ud835\udc56 = \ud835\udc87\ud835\udc67\u2032\ud835\udc56 \u2200\ud835\udc56. Let \ud835\udc82 be any sequence of actions, which, applied to \ud835\udc601 and \ud835\udc602, respectively, results in tree topologies \ud835\udc65, \ud835\udc65\u2032 \u2208 Z, with two partial trajectories \ud835\udf0f = (\ud835\udc601 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc65), \ud835\udf0f\u2032 = (\ud835\udc602 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc65\u2032). If \ud835\udc43\ud835\udc39 is the policy of an optimal GFlowNet with uniform \ud835\udc43\ud835\udc35, then \ud835\udc43\ud835\udc39 (\ud835\udf0f) = \ud835\udc43\ud835\udc39 (\ud835\udf0f\u2032)\nProof of Proposition 2. We use the same notation as in the proof of of Proposition 1. Since \ud835\udc601 and \ud835\udc602 have the same number of trees, G\ud835\udc601 and G\ud835\udc602 have the same graph structure. Let X1,X2 \u2286 X be the terminal states reachable from \ud835\udc601 and \ud835\udc602. There is a bijective correspondence between X1 and X2: for every action set \ud835\udc82 applying on \ud835\udc601 to obtain \ud835\udc65 \u2208 X1, we obtain \ud835\udc65\u2032 \u2208 X2 by applying \ud835\udc82 on \ud835\udc602. Let \ud835\udf0f and \ud835\udf0f\u2032 be the partial trajectories created by applying \ud835\udc82 on \ud835\udc601 and \ud835\udc602, \ud835\udc43\ud835\udc35 (\ud835\udf0f |\ud835\udc65) = \ud835\udc43\ud835\udc35 (\ud835\udf0f\u2032 |\ud835\udc65\u2032). For simplicity, we denote \ud835\udc40 (\ud835\udc65) = \ud835\udc40 (\ud835\udc65 |\ud835\udc3f (\ud835\udc65)). It is worth to note that for two tree topologies sharing the same Fitch feature, their parsimony scores do not necessarily equal. However, when applying \ud835\udc82 on two states \ud835\udc601 and \ud835\udc602 sharing the Fitch feature, the additional parsimony scores introduced are the same:\n\ud835\udc40 (\ud835\udc65) \u2212 \u2211\ufe01 \ud835\udc56 \ud835\udc40 (\ud835\udc67\ud835\udc56) = \ud835\udc40 (\ud835\udc65\u2032) \u2212 \u2211\ufe01 \ud835\udc56 \ud835\udc40 (\ud835\udc67\u2032\ud835\udc56)\nWe have the following expressions for \ud835\udc43\ud835\udc39 (\ud835\udf0f) and \ud835\udc43\ud835\udc39 (\ud835\udf0f\u2032):\n\ud835\udc43\ud835\udc39 (\ud835\udf0f) = \ud835\udc52 \u2212\ud835\udc40 (\ud835\udc65) \ud835\udc47 \ud835\udc43\ud835\udc35 (\ud835\udf0f |\ud835\udc65)\u2211\n\ud835\udf0f \ud835\udc57 ,\ud835\udc65 \ud835\udc57 \ud835\udc52\n\u2212\ud835\udc40 (\ud835\udc65 \ud835\udc57 ) \ud835\udc47 \ud835\udc43\ud835\udc35 (\ud835\udf0f\ud835\udc57 |\ud835\udc65 \ud835\udc57 )\n, \ud835\udc43\ud835\udc39 (\ud835\udf0f\u2032) = \ud835\udc52 \u2212\ud835\udc40 (\ud835\udc65\u2032 ) \ud835\udc47 \ud835\udc43\ud835\udc35 (\ud835\udf0f |\ud835\udc65\u2032)\u2211\n\ud835\udf0f\u2032 \ud835\udc57 ,\ud835\udc65\u2032 \ud835\udc57 \ud835\udc52\n\u2212\ud835\udc40 (\ud835\udc65\u2032 \ud835\udc57 )\n\ud835\udc47 \ud835\udc43\ud835\udc35 (\ud835\udf0f\u2032\ud835\udc57 |\ud835\udc65\u2032\ud835\udc57 )\nWe multiply \ud835\udc52 \u2211 \ud835\udc56 \ud835\udc40 (\ud835\udc67\ud835\udc56 ) \ud835\udc47\n\ud835\udc52\n\u2211 \ud835\udc56 \ud835\udc40 (\ud835\udc67\ud835\udc56 )\n\ud835\udc47\nby \ud835\udc43\ud835\udc39 (\ud835\udf0f) and \ud835\udc52 \u2211 \ud835\udc56 \ud835\udc40 (\ud835\udc67\u2032\ud835\udc56 ) \ud835\udc47\n\ud835\udc52\n\u2211 \ud835\udc56 \ud835\udc40 (\ud835\udc67\u2032\ud835\udc56 )\n\ud835\udc47\nby \ud835\udc43\ud835\udc39 (\ud835\udf0f\u2032) and obtain:\n\ud835\udc43\ud835\udc39 (\ud835\udf0f) = \ud835\udc52\n\u2211 \ud835\udc56 \ud835\udc40 (\ud835\udc67\ud835\udc56 )\u2212\ud835\udc40 (\ud835\udc65)\n\ud835\udc47 \ud835\udc43\ud835\udc35 (\ud835\udf0f |\ud835\udc65)\u2211 \ud835\udf0f \ud835\udc57 ,\ud835\udc65 \ud835\udc57 \ud835\udc52 \u2211 \ud835\udc56 \ud835\udc40 (\ud835\udc67\ud835\udc56 )\u2212\ud835\udc40 (\ud835\udc65 \ud835\udc57 ) \ud835\udc47 \ud835\udc43\ud835\udc35 (\ud835\udf0f\ud835\udc57 |\ud835\udc65 \ud835\udc57 ) , \ud835\udc43\ud835\udc39 (\ud835\udf0f\u2032) =\n\ud835\udc52\n\u2211 \ud835\udc56 \ud835\udc40 (\ud835\udc67\u2032\ud835\udc56 )\u2212\ud835\udc40 (\ud835\udc65\n\u2032 ) \ud835\udc47 \ud835\udc43\ud835\udc35 (\ud835\udf0f |\ud835\udc65\u2032)\u2211\n\ud835\udf0f\u2032 \ud835\udc57 ,\ud835\udc65\u2032 \ud835\udc57 \ud835\udc52\n\u2211 \ud835\udc56 \ud835\udc40 (\ud835\udc67\u2032\ud835\udc56 )\u2212\ud835\udc40 (\ud835\udc65 \u2032 \ud835\udc57 )\n\ud835\udc47 \ud835\udc43\ud835\udc35 (\ud835\udf0f\u2032\ud835\udc57 |\ud835\udc65\u2032\ud835\udc57 )\nSince \ud835\udc52 \u2211 \ud835\udc56 \ud835\udc40 (\ud835\udc67\u2032\ud835\udc56 )\u2212\ud835\udc40 (\ud835\udc65 \u2032 \ud835\udc57 ) \ud835\udc47 \ud835\udc43\ud835\udc35 (\ud835\udf0f\u2032\ud835\udc57 |\ud835\udc65\u2032\ud835\udc57 ) = \ud835\udc52 \u2211 \ud835\udc56 \ud835\udc40 (\ud835\udc67\ud835\udc56 )\u2212\ud835\udc40 (\ud835\udc65 \ud835\udc57 ) \ud835\udc47 \ud835\udc43\ud835\udc35 (\ud835\udf0f\ud835\udc57 |\ud835\udc65 \ud835\udc57 ) for all \ud835\udc57 , \ud835\udc43\ud835\udc39 (\ud835\udf0f) = \ud835\udc43\ud835\udc39 (\ud835\udf0f\u2032). \u25a1"
        },
        {
            "heading": "B MARGINAL LOG-LIKELIHOOD ESTIMATION",
            "text": "Estimating the sampling likelihood of a terminal state In the PhyloGFN state space (in both the Bayesian and parsimony-based settings), there exist multiple trajectories leading to the same terminal state \ud835\udc65, hence the sampling probability of \ud835\udc65 is calculated as: \ud835\udc43\u22a4 (\ud835\udc65) = \u2211\ud835\udf0f:\ud835\udc600\u2192\u00b7\u00b7\u00b7\u2192\ud835\udc65 \ud835\udc43\ud835\udc39 (\ud835\udf0f). This sum is intractable for large-scale problems. However, we can estimate the sum using importance sampling(Zhang et al., 2022):\n\ud835\udc43\u22a4\ud835\udc39 (\ud835\udc65) \u2248 1 \ud835\udc3e \u2211\ufe01 \ud835\udf0f\ud835\udc56 :\ud835\udc600\u2192\u00b7\u00b7\u00b7\u2192\ud835\udc65 \ud835\udc43\ud835\udc39 (\ud835\udf0f\ud835\udc56) \ud835\udc43\ud835\udc35 (\ud835\udf0f\ud835\udc56 |\ud835\udc65) , (S1)\nwhere the trajectories \ud835\udf0f\ud835\udc56 are sampled from \ud835\udc43\ud835\udc35 (\ud835\udf0f\ud835\udc56 | \ud835\udc65). The logarithm of the right side of (S1) is, in expectation, a lower bound on the logarithm of the true sampling likelihood on the left side of (S1).\nEstimating the MLL The lower bound on MLL can be evaluated using the importance-weighted bound log \ud835\udc43(\ud835\udc80) \u2265 E\ud835\udc651...\ud835\udc65\ud835\udc58\u223c\ud835\udc43\ud835\udc39 log 1\ud835\udc3e \u2211 \ud835\udc43 (\ud835\udc65,\ud835\udc80 ) \ud835\udc43\ud835\udc47 (\ud835\udc65 ) (Burda et al., 2016). However, we cannot use it for PhyloGFN since we cannot compute the exact \ud835\udc43\ud835\udc47 (\ud835\udc65), only get a lower bound on it using (S1). Therefore, we propose the following variational lower bound:\nlog \ud835\udc43(\ud835\udc80) \u2265 E\ud835\udf0f1 ,...,\ud835\udf0f\ud835\udc58\u223c\ud835\udc43\ud835\udc39 log \u00a9\u00ab\ud835\udc43(\ud835\udc67) 1 \ud835\udc3e\n\ud835\udc58\u2211\ufe01 \ud835\udf0f\ud835\udc56\n\ud835\udf0f\ud835\udc56 :\ud835\udc600\u2192\u00b7\u00b7\u00b7\u2192(\ud835\udc67\ud835\udc56 ,\ud835\udc4f\ud835\udc56 )\n\ud835\udc43\ud835\udc35 (\ud835\udf0f\ud835\udc56 |\ud835\udc67\ud835\udc56 , \ud835\udc4f\ud835\udc56)\ud835\udc45(\ud835\udc67\ud835\udc56 , \ud835\udc4f\ud835\udc56) \ud835\udc43\ud835\udc39 (\ud835\udf0f\ud835\udc56) \u00aa\u00ae\u00ae\u00ac\nWe show the derivation of the estimator and thus prove its consistency: \ud835\udc43(\ud835\udc80) = \u222b \ud835\udc67,\ud835\udc4f \ud835\udc43(\ud835\udc80 |\ud835\udc67, \ud835\udc4f)\ud835\udc43(\ud835\udc4f |\ud835\udc67)\ud835\udc43(\ud835\udc67)\n= \u222b \ud835\udc67,\ud835\udc4f \ud835\udc45(\ud835\udc67, \ud835\udc4f)\ud835\udc43(\ud835\udc67)\n= \u222b \ud835\udc67,\ud835\udc4f \ud835\udc45(\ud835\udc67, \ud835\udc4f)\ud835\udc43(\ud835\udc67) \u2211\ufe01\n\ud835\udf0f:\ud835\udc600...\ud835\udc65=(\ud835\udc67,\ud835\udc4f) \ud835\udc43\ud835\udc35 (\ud835\udf0f |\ud835\udc67, \ud835\udc4f) = \u222b \ud835\udc67,\ud835\udc4f \ud835\udc45(\ud835\udc67, \ud835\udc4f)\ud835\udc43(\ud835\udc67) \u2211\ufe01\n\ud835\udf0f:\ud835\udc600...\ud835\udc65=(\ud835\udc67,\ud835\udc4f) \ud835\udc43\ud835\udc35 (\ud835\udf0f |\ud835\udc67, \ud835\udc4f)\n\ud835\udc43\ud835\udc39 (\ud835\udf0f) \ud835\udc43\ud835\udc39 (\ud835\udf0f)\n= \u222b \ud835\udc67,\ud835\udc4f \u2211\ufe01 \ud835\udf0f:\ud835\udc600...\ud835\udc65=(\ud835\udc67,\ud835\udc4f) \ud835\udc43\ud835\udc39 (\ud835\udf0f) \ud835\udc43\ud835\udc35 (\ud835\udf0f |\ud835\udc67, \ud835\udc4f)\ud835\udc45(\ud835\udc67, \ud835\udc4f)\ud835\udc43(\ud835\udc67) \ud835\udc43\ud835\udc39 (\ud835\udf0f)\n= \u222b \ud835\udf0f:\ud835\udc600...\ud835\udc65\ud835\udf0f=(\ud835\udc67\ud835\udf0f ,\ud835\udc4f\ud835\udf0f ) \ud835\udc43\ud835\udc39 (\ud835\udf0f) \ud835\udc43\ud835\udc35 (\ud835\udf0f |\ud835\udc67\ud835\udf0f , \ud835\udc4f\ud835\udf0f)\ud835\udc45(\ud835\udc67\ud835\udf0f , \ud835\udc4f\ud835\udf0f)\ud835\udc43(\ud835\udc67) \ud835\udc43\ud835\udc39 (\ud835\udf0f) = \ud835\udc43(\ud835\udc67)E\ud835\udf0f\u223c\ud835\udc43\ud835\udc39 \ud835\udc43\ud835\udc35 (\ud835\udf0f |\ud835\udc67\ud835\udf0f , \ud835\udc4f\ud835\udf0f)\ud835\udc45(\ud835\udc67\ud835\udf0f , \ud835\udc4f\ud835\udf0f)\n\ud835\udc43\ud835\udc39 (\ud835\udf0f)\n\u2248 \ud835\udc43(\ud835\udc67) 1 \ud835\udc3e\n\ud835\udc58\u2211\ufe01 \ud835\udf0f\ud835\udc56\u223c\ud835\udc43\ud835\udc39\n\ud835\udf0f\ud835\udc56 :\ud835\udc600...(\ud835\udc67\ud835\udc56 ,\ud835\udc4f\ud835\udc56 )\n\ud835\udc43\ud835\udc35 (\ud835\udf0f\ud835\udc56 |\ud835\udc67\ud835\udc56 , \ud835\udc4f\ud835\udc56)\ud835\udc45(\ud835\udc67\ud835\udc56 , \ud835\udc4f\ud835\udc56) \ud835\udc43\ud835\udc39 (\ud835\udf0f\ud835\udc56) .\nOne can show, in a manner identical to the standard importance-weighted bound, that this estimate is a lower bound on log \ud835\udc43(\ud835\udc80). PhyloGFN-Bayesian models edge lengths using discrete distributions. To estimate our algorithm\u2019s MLL, we augment the sampler to a continuous sampler by modeling branch lengths with a piecewise-constant continuous distribution based on the fixed-bin multinomial distribution of PhyloGFN. We can still use the above formulation to estimate the lower bound. However, each trajectory now has one extra step: \ud835\udf0f\u2032 = \ud835\udc600 \u2192 \u00b7 \u00b7 \u00b7 \u2192 (\ud835\udc67, \ud835\udc4f) \u2192 (\ud835\udc67, \ud835\udc4f\ud835\udc56) where (\ud835\udc67, \ud835\udc4f\ud835\udc56) is obtained from \ud835\udc67, \ud835\udc4f by randomly perturbing each branch length by adding noise from\ud835\udc48 [\u22120.5\ud835\udf14, 0.5\ud835\udf14], where \ud835\udf14 is the bin size used for PhyloGFN. Let \ud835\udf0f = \ud835\udc600 \u2192 \u00b7 \u00b7 \u00b7 \u2192 (\ud835\udc67, \ud835\udc4f) be the original trajectory in the discrete PhyloGFN, we can compute \ud835\udc43\ud835\udc39 (\ud835\udf0f\u2032), \ud835\udc43\ud835\udc35 (\ud835\udf0f\u2032) from \ud835\udc43\ud835\udc39 (\ud835\udf0f), \ud835\udc43\ud835\udc35 (\ud835\udf0f):\n\ud835\udc43\ud835\udc39 (\ud835\udf0f\u2032) = \ud835\udc43\ud835\udc39 (\ud835\udf0f) 1\n\ud835\udf14 |\ud835\udc38 (\ud835\udc67) | , \ud835\udc43\ud835\udc35 (\ud835\udf0f\u2032) = \ud835\udc43\ud835\udc35 (\ud835\udf0f)\nThe term 1 \ud835\udf14 |\ud835\udc38 (\ud835\udc67) |\nis introduced in \ud835\udc43\ud835\udc39 because we additionally sample over a uniform range \ud835\udf14 for all |\ud835\udc38 (\ud835\udc67) | edges. The backward probability \ud835\udc43\ud835\udc35 stays unchanged because given (\ud835\udc67, \ud835\udc4f) generated from the discrete GFN, for any (\ud835\udc67, \ud835\udc4f\u2032) resulting from perturbing edges, (\ud835\udc67, \ud835\udc4f) is the the only possible ancestral state."
        },
        {
            "heading": "C DATASET INFORMATION",
            "text": "Table S1: Statistics of the benchmark datasets from DS1 to DS8. Dataset # Species # Sites Reference\nDS1 27 1949 Hedges et al. (1990) DS2 29 2520 Garey et al. (1996) DS3 36 1812 Yang & Yoder (2003) DS4 41 1137 Henk et al. (2003) DS5 50 378 Lakner et al. (2008) DS6 50 1133 Zhang & Blackwell (2001) DS7 59 1824 Yoder & Yang (2004) DS8 64 1008 Rossman et al. (2001)"
        },
        {
            "heading": "D MODELING",
            "text": "Given the character set \u03a3, we use one-hot encoding to represent each site in a sequence. To deal with wild characters in the dataset, for parsimony analysis we consider them as one special character in \u03a3, while in Bayesian inference, we represent them by a vector of 1.\nFor both PhyloGFN-Parsimony and PhyloGFN-Bayesian, given a state with \ud835\udc59 rooted trees, its representation feature is the set {\ud835\udf0c1 . . . \ud835\udf0c\ud835\udc59}, where \ud835\udf0c is a vector of length \ud835\udc5a |\u03a3 |. For example, for DNA sequences of 1000 sites, each \ud835\udf0c would have length 4000. Therefore, before passing these features to the Transformer encoder, we first use a linear transformation to obtain lower-dimensional embeddings of the input features.\nWe use the Transformer architecture (Vaswani et al., 2017) to build the Transformer encoder. For a state with \ud835\udc59 trees, the output is a set of \ud835\udc59 +1 features {\ud835\udc52\ud835\udc60 , \ud835\udc521, . . . , \ud835\udc52\ud835\udc59} where \ud835\udc52\ud835\udc60 denotes the summary feature (i.e., the [CLS] token of the Transformer encoder input).\nTo select pairs of trees to join, we evaluate tree-pair features for every pair of trees in the state and pass these tree-pair features as input to the tree MLP to generate probability logits for all pairs of trees. The tree-pair feature for a tree pair (\ud835\udc56, \ud835\udc57) with representations \ud835\udc52\ud835\udc56 , \ud835\udc60 \ud835\udc57 is the concatenation of \ud835\udc52\ud835\udc56 + \ud835\udc52 \ud835\udc57 with the summary embedding of the state, i.e., the feature is [\ud835\udc52\ud835\udc60; \ud835\udc52\ud835\udc56 + \ud835\udc52 \ud835\udc57 ], where [\u00b7; \u00b7] denotes vector direct sum (concatenation). For a state with \ud835\udc59 trees, ( \ud835\udc59 2 ) = \ud835\udc59 (\ud835\udc59\u22121)2 such pairwise features are generated for all possible pairs.\nTo generate edge lengths for the joined tree pair (\ud835\udc56, \ud835\udc57), we pass [\ud835\udc52\ud835\udc60; \ud835\udc52\ud835\udc56; \ud835\udc52 \ud835\udc57 ] \u2013 the concatenation of the summary feature with the tree-level features of trees \ud835\udc56 and \ud835\udc57 \u2013 as input to the edge MLP. For unrooted tree topologies we need to distinguish two scenarios: (i) when only two rooted trees are left in the state (i.e., at the last step of PhyloGFN state transitions), we only need to generate a single edge; and (ii) when there are more than two rooted trees in the state, a pair of edges is required. Therefore, two separate edge MLPs are employed, as edge length is modeled by \ud835\udc58 discrete bins, the edge MLP used at the last step has an output size of \ud835\udc58 (to model a distribution over a single edge length) whereas the other edge MLP would have an output size of \ud835\udc582 (to model a joint distribution over two edge lengths).\nFor the temperature-conditioned PhyloGFN, as then temperature \ud835\udc47 is passed to our PhyloGFN as an input, two major modifications are required: (i) the estimation of the partition \ud835\udc4d\ud835\udf03 is now a function of \ud835\udc47 : \ud835\udc4d\ud835\udf03 (\ud835\udc47), which is modeled by a Z MLP; (ii) the summary token to the Transformer encoder also captures the temperature information by replacing the usual [CLS] token with a temp MLP that accepts \ud835\udc47 as input."
        },
        {
            "heading": "E TRAINING DETAILS",
            "text": "Here, we describe the training details for our PhyloGFN. For PhyloGFN-Bayesian, our models are trained with fixed 500 epochs. For PhyloGFN-Parsimony, our models are trained until the probability of sampling the optimal trees, or the most parsimonious trees our PhyloGFN has seen so far, is above 0.001. Each training epoch consists of 1000 gradient update steps using a batch of 64 trajectories. For \ud835\udf16-greedy exploration, the \ud835\udf16 value is linearly annealed from 0.5 to 0.001 during the first 240 epochs. All common hyperparameters for PhyloGFN-Bayesian and PhyloGFN-Parsimony are shown in Table S3.\nTemperature annealing For PhyloGFN-Bayesian, the initial temperature is set to 16 for all experiments. For PhyloGFN-Parsimony, \ud835\udc47 is initialized at 4. Under the cascading temperature annealing scheme, \ud835\udc47 is reduced by half per every 80 epochs of training. For PhyloGFN-Bayesian, \ud835\udc47 is always reduced to and then fixed at 1, whereas for PhyloGFN-Parsimony, \ud835\udc47 is only reduced when the most parsimonious trees seen by our PhyloGFN so far are sampled with a probability below 0.001.\nHyperparameter \ud835\udc36 selection For PhyloGFN-Parsimony, the reward is defined as \ud835\udc45(\ud835\udc65) = exp\n( \ud835\udc36\u2212\ud835\udc40 (\ud835\udc65 |\ud835\udc80 )\n\ud835\udc47\n) , where \ud835\udc36 is a hyperparameter introduced for training stability and it controls the\nmagnitude of the partition function \ud835\udc4d = \u2211 \ud835\udc65 \ud835\udc45(\ud835\udc65). Given that we cannot determine the precise value of \ud835\udc36 prior to training since we do know the value of \ud835\udc4d as a priori, we use the following heuristic to choose \ud835\udc36: 1000 random tree topologies are generated via stepwise-addition, and we set \ud835\udc36 to the 10% quantile of the lowest parsimony score.\nSimilarly, \ud835\udc36 is employed for PhyloGFN-Bayesian under the same goal of stabilizing the training and reducing the magnitude of the partition function. Recall the reward function \ud835\udc45(\ud835\udc67, \ud835\udc4f) defined in section 4.1, it can be rewritten as \ud835\udc45(\ud835\udc67, \ud835\udc4f) = exp ( \ud835\udc36\u2212(\u2212 log \ud835\udc43 (\ud835\udc80 |\ud835\udc67,\ud835\udc4f)\ud835\udc43 (\ud835\udc4f) )\n\ud835\udc47\n) when \ud835\udc47 = 1. Note that exp ( \ud835\udc36 \ud835\udc47 ) can be absorbed into the partition function. For selecting the \ud835\udc36, once again we randomly sample 1000 weighted phylogenetic trees via stepwise-addition and with random edge length, followed by setting \ud835\udc36 to the 10% quantile of the lowest \u2212 log \ud835\udc43(\ud835\udc80 |\ud835\udc67, \ud835\udc4f)\ud835\udc43(\ud835\udc4f). Temperature-conditioned PhyloGFN-Parsimony The temperature-conditioned PhyloGFN is introduced so that a single trained PhyloGFN can be used to sample from a series of reward distributions defined by different \ud835\udc47 . We modify the fixed-temperature PhyloGFN-Parsimony by introducing \ud835\udc47 as input in 3 places: (i) the reward function \ud835\udc45(\ud835\udc65;\ud835\udc47); (ii) the forward transition policy \ud835\udc43\ud835\udc39 (\ud835\udc65;\ud835\udc47); and (iii) the learned partition function estimate \ud835\udc4d\ud835\udf03 (\ud835\udc47). To train the temperature-conditioned PhyloGFN, the TB objective also needs to be updated accordingly:\nLTB (\ud835\udf0f;\ud835\udc47) = ( log \ud835\udc4d\ud835\udf03 (\ud835\udc47) \u220f\ud835\udc5b\u22121 \ud835\udc56=0 \ud835\udc43\ud835\udc39 (\ud835\udc60\ud835\udc56+1 | \ud835\udc60\ud835\udc56; \ud835\udf03, \ud835\udc47)\n\ud835\udc45(\ud835\udc65, \ud835\udc47)\ud835\udc43\ud835\udc35 (\ud835\udf0f | \ud835\udc65)\n)2 , \ud835\udc43\ud835\udc35 (\ud835\udf0f | \ud835\udc65) := \ud835\udc5b\u220f \ud835\udc56=1 1 |Pa(\ud835\udc60\ud835\udc56) | ,\nNote that \ud835\udc43\ud835\udc35 is unaffected.\nDuring training, values for \ud835\udc47 are randomly selected from the range [\ud835\udc47min, \ud835\udc47max]. When training with a state from the replay buffer, the temperature used for training is resampled and may be different than the one originally used when the state was added to the buffer.\nWe also employ a scheme of gradually reducing the average of sampled \ud835\udc47 values through the course of training: \ud835\udc47\u2019s are sampled from a truncated normal distribution with a fixed pre-defined variance and a moving mean \ud835\udc47\ud835\udf07 that linearly reduces from \ud835\udc47max to \ud835\udc47min.\nModeling branch lengths with discrete multinomial distribution When a pair of trees are joined at the root, the branch lengths of the two newly formed edges are modeled jointly by a discrete multinomial distribution. The reason for using a joint distribution is because under the Jukes-Cantor evolution model, the likelihood at the root depends on the sum of the two branch lengths.\nWe use a different set of maximum edge length, bin number and bin size depending on each dataset, and by testing various combinations we have selected the set with optimal performance. The maximum branch length is chosen among {0.05, 0.1, 0.2}, and bin size \ud835\udf14 is chosen among {0.001, 0.002, 0.004}. Table S2 shows the our final selected bin size and bin number for each dataset.\nTable S2: Bin sizes and bin numbers used to model branch lengths for DS1 to DS8. Dataset Bin Size # Bins\nDS1 0.001 50 DS2 0.004 50 DS3 0.004 50 DS4 0.002 100 DS5 0.002 100 DS6 0.001 100 DS7 0.001 200 DS8 0.001 100\nTable S3: Common hyperparameters for PhyloGFN-Bayesian and PhyloGFN-Parsimony. Transformer encoder\nhidden size 128 number of layers 6 number of heads 4 learning rate (model) 5e-5 learning rate (Z) 5e-3\ntree MLP\nhidden size 256 number of layers 3\nedge MLP\nhidden size 256 number of layers 3\nZ MLP (in temperature-conditioned PhyloGFN)\nhidden size 128 number of layers 1\ntemp MLP (in temperature-conditioned PhyloGFN)\nhidden size 256 number of layers 3"
        },
        {
            "heading": "F RUNNING TIME AND HARDWARE REQUIREMENTS",
            "text": "PhyloGFN is trained on virtual machines equipped with 10 CPU cores and 10GB RAM for all datasets. We use one V100 GPU for datasets DS1-DS6 and one A100 GPU for DS7-DS8, although the choice of hardware is not essential for running our training algorithms.\nFor Bayesian inference, the models used for the MLL estimation in Table 1 of the paper are trained on a total of 32 million examples, with a training wall time ranging from 3 to 8 days across the eight datasets. However, our algorithm demonstrates the capacity to achieve similar performance levels with significantly reduced training data. The table S4 below presents the performance of PhyloGFN with 32 million training examples (PhyloGFN Full) and with only 40% of the training trajectories (PhyloGFN Short). Each type of experiment is repeated 3 times. Table S5 compares running time of the full experiment with the shorter experiment. The tables show that the shorter runs exhibit comparable performance to our full run experiments, and all conclude within 3 days.\nWe compare the running time of PhyloGFN with VI baselines (VBPI-GNN, Vaiphy, and GeoPhy) using the DS1 dataset. VBPI-GNN and GeoPhy are trained using the same virtual machine configuration as PhyloGFN (10 cores, 10GB ram, 1xV100 GPU). The training setup for both algorithms mirrors the one that yielded the best performance as documented in their respective papers. As for VaiPhy, we employed the recorded running time from the paper on a machine with 16 CPU cores and 96GB RAM. For PhyloGFN, we calculate the running time of the full training process (PhyloGFNFull) and four shorter experiments with 40%, 24%, 16% and 8% training examples. The table S6 documents both the running time and MLL estimation for each experiment. While our most comprehensive experiment, PhyloGFN Full, takes the longest time to train, our shorter runs \u2013 all of which conclude training within a day \u2013 show only a marginal degradation in performance. Remarkably, even our shortest run, PhyloGFN - 8%, outperforms both GeoPhy and \ud835\udf19-CSMC, achieving this superior performance with approximately half the training time of GeoPhy.\nTable S4: PhyloGFN-Bayesian MLL estimation on 8 datasets. We repeat both full experiment and short experiment (with 40% training examples) 3 times\nExperiment PhyloGFN Full PhyloGFN Short (40% Trainning data)\nRepeat 1 2 3 1 2 3\nDS1 -7108.95 \u00b10.06 -7108.97 \u00b10.05 -7108.94 \u00b10.05 -7108.97 \u00b10.14 -7108.94 \u00b10.22 -7109.04 \u00b10.08 DS2 -26368.9 \u00b10.28 -26368.77 \u00b10.43 -26368.89 \u00b10.29 -26368.9 \u00b10.39 -26369.03 \u00b10.31 -26368.88 \u00b10.32 DS3 -33735.6 \u00b10.35 -33735.60 \u00b10.40 -33735.68 \u00b10.64 -33735.9 \u00b10.91 -33735.83 \u00b10.62 -33735.76 \u00b10.75 DS4 -13331.83 \u00b10.19 -13331.80 \u00b10.31 -13331.94 \u00b10.42 -13332.04 \u00b10.57 -13331.87 \u00b10.31 -13331.78 \u00b10.37 DS5 -8215.15 \u00b10.2 -8214.92 \u00b10.27 8214.85 \u00b10.28 -8215.38 \u00b10.27 -8215.37 \u00b10.26 -8215.38 \u00b10.25 DS6 -6730.68 \u00b10.54 -6730.72 \u00b10.26 -6730.89 \u00b10.22 -6731.35 \u00b10.31 -6731.2 \u00b10.4 -6731.1 \u00b10.38 DS7 -37359.96 \u00b11.14 -37360.59 \u00b11.62 -37361.51 \u00b12.89 -37362.03 \u00b15.2 -37363.43 \u00b12.2 -37362.37 \u00b12.65 DS8 -8654.76 \u00b10.19 -8654.67 \u00b10.39 -8654.86 \u00b10.15 -8655.8 \u00b10.95 -8655.65 \u00b10.37 -8654.96 \u00b10.46\nTable S5: PhyloGFN-Bayesian training time\nDataset PhyloGFN Full PhyloGFN Short\nDS1 62h40min 20h40min DS2 69h16min 28h DS3 80h20min 35h40min DS4 103h54min 44h30min DS5 127h50min 51h40min DS6 135h10min 53h10min DS7 174h3min 60h20min DS8 190h25min 61h40min\nG ABLATION STUDY\nTable S6: Running time of PhyloGFN-Bayesian and VI baseline methods on DS1\nRunning Time MLL VBPI-GNN 16h10min -7108.41 \u00b10.14 GeoPhy 12h50min -7111.55 \u00b10.07 \ud835\udf19-CSMC** \u223c2h -7290.36 \u00b17.23 PhyloGFN - Full 62h40min -7108.97 \u00b10.05 PhyloGFN - 40% 20h40min -7108.97 \u00b10.14 PhyloGFN - 24% 15h40min -7109.01 \u00b10.15 PhyloGFN - 16% 10h50min -7109.15 \u00b10.23 PhyloGFN - 8% 5h10min -7110.65 \u00b10.39\nG.1 BRANCH LENGTHS MODEL HYPERPARAMETERS\nPhyloGFN models branch lengths using discrete multinomial distributions. When estimating MLL, the branch length model is transformed into a piecewise-constant continuous form, introducing a small quantization error. Two hyperparameters define the multinomial distribution: bin size and bin number. This analysis investigates the impact of bin size and bin number on MLL estimation.\nFor the fixed bin size of 0.001, we assess four sets of bin numbers: 50, 100, 150, and 200, and for the fixed bin number of 50, we evaluate three sets of bin sizes: 0.001, 0.002, and 0.004. For each setup, we train a PhyloGFN-Bayesian model on the dataset DS1 using 12.8 millions training examples.\nTable S7 displays the MLL obtained in each setup. A noticeable decline in MLL estimation occurs as the bin size increases, which is expected due to the increased quantization error. However, MLL estimation also significantly declines as the number of bins increases over 100 under the fixed bin size of 0.001. We conjecture this is because the size of the action pool for sampling the pair of branch lengths increases quadratically by the number of bins. For example at 200 bins, the MLP head that generates branch lengths has 40,000 possible outcomes, leading to increased optimization challenges.\nG.2 EXPLORATION POLICIES\nPhyloGFN employs the following to techniques to encourage exploration during training:\n\u2022 \ud835\udf16-greedy annealing: a set of trajectories are generated from the GFlowNet that is being trained, with actions sampled from the GFlowNet policy with probability 1 \u2212 \ud835\udf16 and uniformly at random with probability \ud835\udf16 . The \ud835\udf16 rate drops from a pre-defined \ud835\udf16\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61 to near 0 through the course of training.\n\u2022 Replay Buffer: a replay buffer is used to store the best trees seen to date, and to use them for training, random trajectories are constructed from these high-reward trees using the backward probability \ud835\udc43\ud835\udc35.\n\u2022 Temperature annealing: the training of GFlowNet begins at a large temperature \ud835\udc47 and it is divided in half periodically during training.\nTo assess the effectiveness of various exploration methods, we train PhyloGFN-Bayesian on DS1 with the following setups:\n1. All trajectories are generated strictly on-policy training without any exploration methods (On-policy).\n2. All trajectories are generated with epsilon-greedy annealing (\ud835\udf16).\n3. Half of trajectories are generated with \ud835\udf16-greedy annealing, and the other half are constructed from the replay buffer (\ud835\udf16 + RP).\n4. The same setup as 3, with the addition of temperature annealing. \ud835\udc47 is initialized at 16 and reduced by half per every 1.5 million training examples until reaching 1 (\ud835\udf16 + RP + T Cascading).\n5. The same setup as 4, except the temperature drops linearly from 16 to 1 (\ud835\udf16 + RP + T Linear).\nFor each setup, we train a model using 12.8 millions training examples. Table S8 displays the MLL estimation for each setup. On-policy training without any additional exploration strategies results in the poorest model performance. Epsilon-greedy annealing significantly enhances performance, and combining all three strategies yields the optimal results. There is no significant difference between cascading and linear temperature drops.\nTable S7: MLL estimation of PhyloGFN-Bayesian on DS1 with different combinations of bin size and bin number to model edge lengths.\nBin Size Bin Number MLL\n0.001 50 -7108.98 \u00b10.14 0.001 100 -7108.96 \u00b10.18 0.001 150 -7109.17 \u00b11.14 0.001 200 -7109.3 \u00b17.23 0.002 50 -7109.95 \u00b10.29 0.004 50 -7114.48 \u00b10.52\nTable S8: MLL estimation of PhyloGFN-Bayesian on DS1 with different exploration policies\nTraining Policies MLL\nOn policy -7307.99 \u00b10.26 \ud835\udf16 -7109.92 \u00b10.18 \ud835\udf16 + RP -7109.95 \u00b10.22 \ud835\udf16 + RP + T Cascading 7108.98 \u00b10.14 \ud835\udf16 + RP + T Linear 7108.96 \u00b10.15"
        },
        {
            "heading": "H SIGNIFICANCE OF MODELING SUBOPTIMAL TREES WELL",
            "text": "Several downstream tasks that rely on phylogenetic inference require not only the identification of optimal (maximum likelihood or most parsimonious) trees, but also the proper sampling of suitably weighted suboptimal trees. In evolutionary studies, this includes the computation of branch length support (i.e., the probability that a given subtree is present in the true tree), as well as the estimation of confidence intervals for the timing of specific evolutionary events (Drummond et al., 2006). These are particularly important in the very common situations where the data only weakly define the posterior on tree topologies (e.g. small amount of data per species, or high degree of divergence between sequences). In such cases, because suboptimal trees vastly outnumber optimal trees, they can contribute to a non-negligible extent to the probability mass of specific marginal probabilities. Full modeling of posterior distributions on trees is also critical in studying tumor or virus evolution within a patient (McGranahan et al., 2015; Buendia et al., 2009), e.g., to properly assess the probability of the ordered sequence of driver or passenger mutations that may have led to metastasis or drug resistance (Fisher et al., 2013)."
        },
        {
            "heading": "I REPRODUCIBILITY",
            "text": "In this section, we describe the procedure to reproduce our results. Code and data are provided in the supplementary material.\nI.1 MRBAYES\nWe use the following script to compute MLL for DS1\u2013DS8.\nexecute INPUT_FOLDER/ds1.nex lset nst=1 prset statefreqpr=fixed(equal) prset brlenspr=unconstrained:exp(10.0) ss ngen=10000000 nruns=10 nchains=4 printfreq=1000 \\ samplefreq=100 savebrlens=yes filename=OUTPUT_FOLDER\nI.2 VBPI-GNN\nWe use the script provided in the paper\u2019s GitHub page to generate models for DS1\u2013DS8:\npython main.py --dataset DS1 --brlen_model gnn --gnn_type edge \\ --hL 2 --hdim 100 --maxIter 400000 --empFreq --psp\nVBPI-GNN requires pre-generation of trees. In the original work, the trees topologies are generated from the algorithm UFBoot (Minh et al., 2013; Hoang et al., 2018). We use the program iqtree (Minh et al., 2020) to run UFBoot on all datasets with following command:\niqtree -s DS1 -bb 10000 -wbt -m JC69 -redo\nTo generate randomly perturbed trees to compare log sampling density versus log posterior density. Run the following notebook in our supplementary data folder:\nvbpi_gnn/generate_trees_data.ipynb\nI.3 GEOPHY\nWe run the following script to run GeoPhy on data set DS1. The script is created based on the best configuration recorded in Mimori & Hamada (2023)\npython ./scripts/run_gp.py -v -ip ds-data/ds1/DS1.nex \\ -op results/ds1/ds1_paper -c ./config/default.yaml -s 0 -ua \\ -es lorentz -edt full -ed 4 -eqs 1e-1 -ast 100_1000 -mcs 3 \\ -ci 1000 -ms 1000_000 -ul\nI.4 PHYLOGFN-BAYESIAN\nTraining configuration files for DS1\u2013DS8 are in the following folder in the supplementary data:\nphylo_gfn/src/configs/benchmark_datasets/bayesian\nRun the following script to train the PhyloGFN-Bayesian model:\ncd phylo_gfn\npython run.py configs/benchmark_datasets/bayesian/ds1.yaml \\ dataset/benchmark_datasets/DS1.pickle $output_path --nb_device 2 \\ --acc_gen_sampling --random_sampling_batch_size 1000 --quiet\nTo compute log MLL, plot sampled log density versus log posterior density and compute the Pearson correlation, run the following notebook in supplementary data:\nphylo_gfn/phylogfn_bayesian_result.ipynb\nModels for repeated experiments of full training and short training are trained with a memory efficient PhloGFN implementation in folder phylo gfn bayesian memoryefficient\nThe full training scripts are in folder\nsrc/configs/benchmark_datasets/common_script_normal\nThe short training scripts are in folder\nsrc/configs/benchmark_datasets/common_script_short\nTraining scripts for ablation study are in folder\nsrc/configs/benchmark_datasets/ds1_ablation_study\nI.5 PHYLOGFN-PARSIMONY\nTraining configuration files for DS1\u2013DS8 are in the following folder in the supplementary data:\nphylo_gfn/src/configs/benchmark_datasets/parsimony\nRun the following script to train the PhyloGFN-Parsimony model:\ncd phylo_gfn\npython run.py src/configs/benchmark_datasets/parsimony/ds1.yaml \\ dataset/benchmark_datasets/DS1.pickle $output_path --nb_device 2 \\ --acc_gen_sampling --random_sampling_batch_size 1000 --quiet\nTo generate the plots in Fig. S2, run the following script:\ncd phylo_gfn python parsimony_eval.py <model_folder_path> <sequences_path>\nI.6 TEMPERATURE-CONDITIONED PHYLOGFN\nTo reproduce the result in Fig. 3, first run the following script to train the model:\ncd phylo_gfn\npython run.py configs/benchmark_datasets/parsimony/ds1_conditional.yaml \\ dataset/benchmark_datasets/DS1.pickle $output_path --nb_device 2 \\ --acc_gen_sampling --random_sampling_batch_size 1000 --quiet\nTo generate the plots, run the following notebook in the supplementary data:\nphylo_gfn/ds1_conditional.ipynb"
        },
        {
            "heading": "J ASSESSING SAMPLING DENSITY AGAINST UNNORMALIZED POSTERIOR DENSITY",
            "text": "Here, we assess PhyloGFN-Bayesian\u2019s ability to model the entire phylogenetic tree space by comparing sampling density against unnormalized posterior density over high/medium/low-probability regions of the tree space. To generate trees from medium and low probability regions, we use a trained VBPI-GNN model, the state-of-the-art variational inference algorithm.\nWe first provide a brief introduction to how VBPI-GNN generates a phylogenetic tree in two phases: (i) it first uses SBN to sample tree topology; (ii) followed by a GNN-based model to sample edge lengths over the tree topology. SBN constructs tree topologies in a sequential manner. Starting from the root node and a set of sequences \ud835\udc80 to be labeled at the leaves, the algorithm iteratively generates the child nodes by splitting and distributing the sequence set among the child nodes. The action at each step is thus to choose how to split a set of sequences into two subsets.\nTo introduce random perturbations during tree construction, at every step, with probability \ud835\udf16 , the action is uniformly sampled from the support choices. Given a well-trained VBPI-GNN model, we sample from high/medium/low-probability regions with 0%, 30% and 50% probability. We apply PhyloGFN-Bayesian and VBPI-GNN on these sets to compute sampling density and compare with the ground truth unnormalized posterior density computed as \ud835\udc43(\ud835\udc80 |\ud835\udc67, \ud835\udc4f)\ud835\udc43(\ud835\udc67, \ud835\udc4f). Note that VBPIGNN models the log-edge length instead of the edge length. Hence we perform a change of variables when computing both the sampling probability and the unnormalized prior.\nFig. S1 shows scatter plots of sampling density versus ground-truth unnormalized posterior density of datasets DS2 to DS8, complementing the result on DS1 shown in Fig. 2. We can see that while VBPI-GNN performs well on high-probability regions, our method is better at modeling the target distribution overall. Fig. S1f shows that our model performs poorly at modeling the high-probability region for DS7. The result aligns with our MLL estimation, shown in Table 1. Further work is required to investigate the cause of PhyloGFN\u2019s poor modeling on DS7.\n180 190 200 210 26190\n26185\n26180\n26175\n26170\n26165\n26160\n26155\nUn no\nrm al\nize d\nLo g\nPo st\ner io\nr D en\nsit y\nGFN on 0% random trees\nPearsonR=0.930\n100 50 0 50 100 150 200 250\n26450\n26400\n26350\n26300\n26250\n26200\n26150 GFN on 30% random trees\nPearsonR=0.948\n100 50 0 50 100 150 200 250\n26450\n26400\n26350\n26300\n26250\n26200\nGFN on 50% random trees\nPearsonR=0.919\n180 190 200 210 220 Sampling Log Density\n26190\n26185\n26180\n26175\n26170\n26165\n26160\n26155\nUn no\nrm al\nize d\nLo g\nPo st\ner io\nr D en\nsit y\nVBPI-GNN on 0% random trees\nPearsonR=0.952\n0 50 100 150 200 250 300 350 Sampling Log Density\n26450\n26400\n26350\n26300\n26250\n26200\n26150 VBPI-GNN on 30% random trees\nPearsonR=0.580\n0 50 100 150 200 250 300 350 Sampling Log Density\n26450\n26400\n26350\n26300\n26250\n26200\nVBPI-GNN on 50% random trees\nPearsonR=0.538\n(a) DS2\nFigure S1: Sampling log density vs. ground truth unnormalized posterior log density for DS2-DS8\n230 240 250 260 33505\n33500\n33495\n33490\n33485\n33480\n33475\n33470\nUn no\nrm al\nize d\nLo g\nPo st\ner io\nr D en\nsit y\nGFN on 0% random trees\nPearsonR=0.916\n100 50 0 50 100 150 200 250 33800\n33750\n33700\n33650\n33600\n33550\n33500 GFN on 30% random trees\nPearsonR=0.963\n50 0 50 100 150 200 33800\n33750\n33700\n33650\n33600\n33550\nGFN on 50% random trees\nPearsonR=0.950\n230 240 250 260 270 Sampling Log Density\n33505\n33500\n33495\n33490\n33485\n33480\n33475\n33470\nUn no\nrm al\nize d\nLo g\nPo st\ner io\nr D en\nsit y\nVBPI-GNN on 0% random trees\nPearsonR=0.958\n50 100 150 200 250 300 350 400 Sampling Log Density\n33800\n33750\n33700\n33650\n33600\n33550\n33500 VBPI-GNN on 30% random trees\nPearsonR=0.543\n50 100 150 200 250 300 350 Sampling Log Density\n33800\n33750\n33700\n33650\n33600\n33550\nVBPI-GNN on 50% random trees\nPearsonR=0.500\n(b) DS3\n270 280 290 300 310 320\n13050\n13040\n13030\n13020\n13010\nUn no\nrm al\nize d\nLo g\nPo st\ner io\nr D en\nsit y\nGFN on 0% random trees\nPearsonR=0.942\n150 175 200 225 250 275 300 325\n13160\n13140\n13120\n13100\n13080\n13060\n13040\n13020 GFN on 30% random trees\nPearsonR=0.946\n100 150 200 250 300\n13200\n13175\n13150\n13125\n13100\n13075\n13050\nGFN on 50% random trees\nPearsonR=0.964\n280 290 300 310 320 330 Sampling Log Density\n13050\n13040\n13030\n13020\n13010\nUn no\nrm al\nize d\nLo g\nPo st\ner io\nr D en\nsit y\nVBPI-GNN on 0% random trees\nPearsonR=0.960\n200 225 250 275 300 325 350 375 Sampling Log Density\n13160\n13140\n13120\n13100\n13080\n13060\n13040\n13020 VBPI-GNN on 30% random trees\nPearsonR=0.770\n200 250 300 350 Sampling Log Density\n13200\n13175\n13150\n13125\n13100\n13075\n13050\nVBPI-GNN on 50% random trees\nPearsonR=0.760\n(c) DS4\n260 280 300 320\n7960\n7950\n7940\n7930\n7920\n7910\n7900\nUn no\nrm al\nize d\nLo g\nPo st\ner io\nr D en\nsit y\nGFN on 0% random trees\nPearsonR=0.969\n100 150 200 250 300 8125\n8100\n8075\n8050\n8025\n8000\n7975\n7950\n7925 GFN on 30% random trees\nPearsonR=0.937\n50 100 150 200 250 300\n8150\n8100\n8050\n8000\n7950\nGFN on 50% random trees\nPearsonR=0.939\n260 270 280 290 300 310 320 Sampling Log Density\n7960\n7950\n7940\n7930\n7920\n7910\n7900\nUn no\nrm al\nize d\nLo g\nPo st\ner io\nr D en\nsit y\nVBPI-GNN on 0% random trees\nPearsonR=0.965\n150 200 250 300 350 Sampling Log Density\n8125\n8100\n8075\n8050\n8025\n8000\n7975\n7950\n7925 VBPI-GNN on 30% random trees\nPearsonR=0.787\n100 150 200 250 300 350 Sampling Log Density\n8150\n8100\n8050\n8000\n7950\nVBPI-GNN on 50% random trees\nPearsonR=0.773\n(d) DS5\nFigure S1: (cont.)\n380 400 420 440 460\n6340\n6330\n6320\n6310\n6300\n6290\n6280\n6270\nUn no\nrm al\nize d\nLo g\nPo st\ner io\nr D en\nsit y\nGFN on 0% random trees\nPearsonR=0.993\n150 200 250 300 350 400 450 6550\n6500\n6450\n6400\n6350\n6300 GFN on 30% random trees\nPearsonR=0.974\n200 250 300 350 400\n6525\n6500\n6475\n6450\n6425\n6400\n6375\n6350\n6325 GFN on 50% random trees\nPearsonR=0.935\n400 420 440 460 480 Sampling Log Density\n6330\n6320\n6310\n6300\n6290\n6280\n6270\n6260\nUn no\nrm al\nize d\nLo g\nPo st\ner io\nr D en\nsit y\nVBPI-GNN on 0% random trees\nPearsonR=0.887\n250 300 350 400 450 500 550 Sampling Log Density\n6550\n6500\n6450\n6400\n6350\n6300\nVBPI-GNN on 30% random trees\nPearsonR=0.816\n250 300 350 400 450 500 Sampling Log Density\n6500\n6450\n6400\n6350\nVBPI-GNN on 50% random trees\nPearsonR=0.702\n(e) DS6\n300 325 350 375 400 425 450 36940\n36920\n36900\n36880\n36860\n36840\n36820\n36800\nUn no\nrm al\nize d\nLo g\nPo st\ner io\nr D en\nsit y\nGFN on 0% random trees\nPearsonR=0.624\n0 100 200 300 400 37250\n37200\n37150\n37100\n37050\n37000\n36950\n36900\nGFN on 30% random trees\nPearsonR=0.787\n100 0 100 200 300 400 37400\n37300\n37200\n37100\n37000\nGFN on 50% random trees\nPearsonR=0.765\n460 480 500 520 Sampling Log Density\n36880\n36870\n36860\n36850\n36840\n36830\nUn no\nrm al\nize d\nLo g\nPo st\ner io\nr D en\nsit y\nVBPI-GNN on 0% random trees\nPearsonR=0.955\n200 300 400 500 600 Sampling Log Density\n37250\n37200\n37150\n37100\n37050\n37000\n36950\n36900\nVBPI-GNN on 30% random trees\nPearsonR=0.682\n200 300 400 500 600 700 Sampling Log Density\n37300\n37200\n37100\n37000\nVBPI-GNN on 50% random trees\nPearsonR=0.678\n(f) DS7\n480 500 520 540 560\n8170\n8160\n8150\n8140\n8130\n8120\n8110\n8100\nUn no\nrm al\nize d\nLo g\nPo st\ner io\nr D en\nsit y\nGFN on 0% random trees\nPearsonR=0.978\n250 300 350 400 450 500 550\n8400\n8350\n8300\n8250\n8200\n8150\nGFN on 30% random trees\nPearsonR=0.913\n200 250 300 350 400 450 8450\n8400\n8350\n8300\n8250\n8200 GFN on 50% random trees\nPearsonR=0.901\n500 520 540 560 Sampling Log Density\n8160\n8150\n8140\n8130\n8120\n8110\n8100\nUn no\nrm al\nize d\nLo g\nPo st\ner io\nr D en\nsit y\nVBPI-GNN on 0% random trees\nPearsonR=0.955\n300 350 400 450 500 550 600 Sampling Log Density\n8400\n8350\n8300\n8250\n8200\n8150\nVBPI-GNN on 30% random trees\nPearsonR=0.605\n300 350 400 450 500 550 600 Sampling Log Density\n8450\n8400\n8350\n8300\n8250\n8200 VBPI-GNN on 50% random trees\nPearsonR=0.463\n(g) DS8"
        },
        {
            "heading": "K PARSIMONY ANALYSIS RESULTS",
            "text": "(a) A single most-parsimonious tree with a score of 4026 has been identified for DS1.\n(b) A single most-parsimonious tree with a score of 6223 has been identified for DS2.\n(c) Two most-parsimonious trees with a score of 6659 have been identified for DS3.\nFigure S2: For each dataset, the sampling probabilities for the most-parsimonious as well as several suboptimal trees are estimated from our learned PhyloGFNs. Each boxplot represents a single unique unrooted tree and shows a distribution of the log-probabilities over all its 2\ud835\udc5b \u2212 3 rooted versions where \ud835\udc5b denotes the number of species. The dashed bounding box groups the set of equallyparsimonious trees.\n(d) Four most-parsimonious trees with a score of 2424 have been identified for DS4.\n(e) Two most-parsimonious trees with a score of 1491 have been identified for DS5.\n(f) 12 most-parsimonious trees with a score of 879 have been identified for DS6.\nFigure S2: (cont.)\n(g) 15 most-parsimonious trees with a score of 7150 have been identified for DS7.\n(h) 21 most-parsimonious trees with a score of 1461 have been identified for DS8.\nFigure S2: (cont.)"
        },
        {
            "heading": "L TRAINING CURVES",
            "text": "The plot in Figure S3 illustrates the training curves for PhyloGFN-Bayesian across all eight datasets. In each dataset plot, the left side displays the training loss for every batch of 64 examples, while the right side shows the MLL computed for every 1.28 million training examples.\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training examples 1e7\n100\n101\n102\n103\n104\n105\nLo ss\nLoss\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training examples 1e7\n7450\n7400\n7350\n7300\n7250\n7200\n7150\n7100\nM LL\nMLL\n(a) DS1\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training examples 1e7\n100\n101\n102\n103\n104\n105\nLo ss\nLoss\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training examples 1e7\n26800\n26700\n26600\n26500\n26400\nM LL\nMLL\n(b) DS2\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training examples 1e7\n100\n101\n102\n103\n104\n105\nLo ss\nLoss\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training examples 1e7\n35000\n34800\n34600\n34400\n34200\n34000\n33800\nM LL\nMLL\n(c) DS3\nFigure S3: Training curves for DS1-DS8\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training examples 1e7\n100\n101\n102\n103\n104\n105\nLo ss\nLoss\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training examples 1e7\n14400\n14200\n14000\n13800\n13600\n13400\nM LL\nMLL\n(d) DS4\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training examples 1e7\n100\n101\n102\n103\n104\n105\nLo ss\nLoss\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training examples 1e7\n9200\n9000\n8800\n8600\n8400\n8200\nM LL\nMLL\n(e) DS5\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training examples 1e7\n100\n101\n102\n103\n104\n105\nLo ss\nLoss\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training examples 1e7\n7800\n7600\n7400\n7200\n7000\n6800\nM LL\nMLL\n(f) DS6\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training examples 1e7\n101\n102\n103\n104\n105\n106\nLo ss\nLoss\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training examples 1e7\n44000\n43000\n42000\n41000\n40000\n39000\n38000\nM LL\nMLL\n(g) DS7\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training examples 1e7\n100\n101\n102\n103\n104\n105\n106\n107\nLo ss\nLoss\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training examples 1e7\n9800\n9600\n9400\n9200\n9000\n8800\n8600\nM LL\nMLL\n(h) DS8"
        }
    ],
    "year": 2023
}