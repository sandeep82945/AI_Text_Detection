{
    "abstractText": "Existing Self-Supervised Learning (SSL) models for speech typically process speech signals at a fixed resolution of 20 milliseconds. This approach overlooks the varying informational content present at different resolutions in speech signals. In contrast, this paper aims to incorporate multi-resolution information into speech self-supervised representation learning. We introduce a SSL model that leverages a hierarchical Transformer architecture, complemented by HuBERT-style masked prediction objectives, to process speech at multiple resolutions. Experimental results indicate that the proposed model not only achieves more efficient inference but also exhibits superior or comparable performance to the original HuBERT model over various tasks. Specifically, significant performance improvements over the original HuBERT have been observed in fine-tuning experiments on the LibriSpeech speech recognition benchmark as well as in evaluations using the Speech Universal PERformance Benchmark (SUPERB) and Multilingual SUPERB (ML-SUPERB).",
    "authors": [],
    "id": "SP:eab41cf371806b429bc69dbf7cfe4a25386b69f5",
    "references": [
        {
            "authors": [
                "Sweta Agrawal",
                "Antonios Anastasopoulos",
                "Luisa Bentivogli",
                "Ond\u0159ej Bojar",
                "Claudia Borg",
                "Marine Carpuat",
                "Roldano Cattoni",
                "Mauro Cettolo",
                "Mingda Chen",
                "William Chen"
            ],
            "title": "Findings of the IWSLT 2023 evaluation campaign",
            "venue": "In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023),",
            "year": 2023
        },
        {
            "authors": [
                "Andrei Andrusenko",
                "Rauf Nasretdinov",
                "Aleksei Romanenko"
            ],
            "title": "UCONV-Conformer: High reduction of input sequence length for end-to-end speech recognition",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Arun Babu",
                "Changhan Wang",
                "Andros Tjandra",
                "Kushal Lakhotia",
                "Qiantong Xu",
                "Naman Goyal",
                "Kritika Singh",
                "Patrick von Platen",
                "Yatharth Saraf",
                "Juan Pino"
            ],
            "title": "XLS-R: Self-supervised crosslingual speech representation learning at scale",
            "venue": "arXiv preprint arXiv:2111.09296,",
            "year": 2021
        },
        {
            "authors": [
                "Alexei Baevski",
                "Yuhao Zhou",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ],
            "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Alexei Baevski",
                "Wei-Ning Hsu",
                "Qiantong Xu",
                "Arun Babu",
                "Jiatao Gu",
                "Michael Auli"
            ],
            "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Lo\u0131\u0308c Barrault",
                "Yu-An Chung",
                "Mariano Cora Meglioli",
                "David Dale",
                "Ning Dong",
                "Paul-Ambroise Duquenne",
                "Hady Elsahar",
                "Hongyu Gong",
                "Kevin Heffernan",
                "John Hoffman"
            ],
            "title": "Seamlessm4tmassively multilingual & multimodal machine translation",
            "venue": "arXiv preprint arXiv:2308.11596,",
            "year": 2023
        },
        {
            "authors": [
                "Emanuele Bastianelli",
                "Andrea Vanzo",
                "Pawel Swietojanski",
                "Verena Rieser"
            ],
            "title": "SLURP: A spoken language understanding resource package",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 7252\u20137262,",
            "year": 2020
        },
        {
            "authors": [
                "Dan Berrebbi",
                "Jiatong Shi",
                "Brian Yan",
                "Osbel L\u00f3pez-Francisco",
                "Jonathan Amith",
                "Shinji Watanabe"
            ],
            "title": "Combining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation",
            "venue": "In Proc. Interspeech",
            "year": 2022
        },
        {
            "authors": [
                "Maxime Burchi",
                "Valentin Vielzeuf"
            ],
            "title": "Efficient conformer: Progressive downsampling and grouped attention for automatic speech recognition",
            "venue": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),",
            "year": 2021
        },
        {
            "authors": [
                "Zhaowei Cai",
                "Quanfu Fan",
                "Rogerio S Feris",
                "Nuno Vasconcelos"
            ],
            "title": "A unified multi-scale deep convolutional neural network for fast object detection",
            "venue": "In Computer Vision\u2013ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Heng-Jui Chang",
                "Alexander H. Liu",
                "James Glass"
            ],
            "title": "Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering",
            "venue": "In Proc. INTERSPEECH",
            "year": 2023
        },
        {
            "authors": [
                "Xuankai Chang",
                "Takashi Maekaku",
                "Pengcheng Guo",
                "Jing Shi",
                "Yen-Ju Lu",
                "Aswin Shanmugam Subramanian",
                "Tianzi Wang",
                "Shu-wen Yang",
                "Yu Tsao",
                "Hung-yi Lee"
            ],
            "title": "An exploration of selfsupervised pretrained representations for end-to-end speech recognition",
            "venue": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),",
            "year": 2021
        },
        {
            "authors": [
                "Chun Fu Chen",
                "Quanfu Fan",
                "Neil Mallinar",
                "Tom Sercu",
                "Rogerio Feris"
            ],
            "title": "Big-little net: An efficient multi-scale feature representation for visual and speech recognition",
            "venue": "In International Conference on Learning Representations. International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Hsuan-Jui Chen",
                "Yen Meng",
                "Hung-yi Lee"
            ],
            "title": "Once-for-all sequence compression for selfsupervised speech models",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Sanyuan Chen",
                "Chengyi Wang",
                "Zhengyang Chen",
                "Yu Wu",
                "Shujie Liu",
                "Zhuo Chen",
                "Jinyu Li",
                "Naoyuki Kanda",
                "Takuya Yoshioka",
                "Xiong Xiao"
            ],
            "title": "WavLM: Large-scale self-supervised pre-training for full stack speech processing",
            "venue": "IEEE Journal of Selected Topics in Signal Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Sanyuan Chen",
                "Yu Wu",
                "Chengyi Wang",
                "Zhengyang Chen",
                "Zhuo Chen",
                "Shujie Liu",
                "Jian Wu",
                "Yao Qian",
                "Furu Wei",
                "Jinyu Li"
            ],
            "title": "Unispeech-sat: Universal speech representation learning with speaker aware pre-training",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Sanyuan Chen",
                "Yu Wu",
                "Chengyi Wang",
                "Shujie Liu",
                "Zhuo Chen",
                "Peidong Wang",
                "Gang Liu",
                "Jinyu Li",
                "Jian Wu",
                "Xiangzhan Yu",
                "Furu Wei"
            ],
            "title": "Why does Self-Supervised Learning for Speech Recognition Benefit Speaker Recognition",
            "venue": "In Proc. Interspeech",
            "year": 2022
        },
        {
            "authors": [
                "William Chen",
                "Brian Yan",
                "Jiatong Shi",
                "Yifan Peng",
                "Soumi Maiti",
                "Shinji Watanabe"
            ],
            "title": "Improving massively multilingual ASR with auxiliary CTC objectives",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Chung-Cheng Chiu",
                "James Qin",
                "Yu Zhang",
                "Jiahui Yu",
                "Yonghui Wu"
            ],
            "title": "Self-supervised learning with random-projection quantizer for speech recognition",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Hyeong-Seok Choi",
                "Juheon Lee",
                "Wansoo Kim",
                "Jie Lee",
                "Hoon Heo",
                "Kyogu Lee"
            ],
            "title": "Neural analysis and synthesis: Reconstructing speech from self-supervised representations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jeongsoo Choi",
                "Minsu Kim",
                "Yong Man Ro"
            ],
            "title": "Intelligible lip-to-speech synthesis with speech units",
            "venue": "arXiv preprint arXiv:2305.19603,",
            "year": 2023
        },
        {
            "authors": [
                "Yu-An Chung",
                "Yu Zhang",
                "Wei Han",
                "Chung-Cheng Chiu",
                "James Qin",
                "Ruoming Pang",
                "Yonghui Wu"
            ],
            "title": "W2V-BERT: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training",
            "venue": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),",
            "year": 2021
        },
        {
            "authors": [
                "Alexis Conneau",
                "Alexei Baevski",
                "Ronan Collobert",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ],
            "title": "Unsupervised cross-lingual representation learning for speech recognition",
            "venue": "arXiv preprint arXiv:2006.13979,",
            "year": 2020
        },
        {
            "authors": [
                "Steven Davis",
                "Paul Mermelstein"
            ],
            "title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences",
            "venue": "IEEE transactions on acoustics, speech, and signal processing,",
            "year": 1980
        },
        {
            "authors": [
                "Alexandre D\u00e9fossez",
                "Jade Copet",
                "Gabriel Synnaeve",
                "Yossi Adi"
            ],
            "title": "High fidelity neural audio compression",
            "venue": "arXiv preprint arXiv:2210.13438,",
            "year": 2022
        },
        {
            "authors": [
                "Sol\u00e8ne Evain",
                "Ha Nguyen",
                "Hang Le",
                "Marcely Zanon Boito",
                "Salima Mdhaffar",
                "Sina Alisamir",
                "Ziyi Tong",
                "Natalia Tomashenko",
                "Marco Dinarelli",
                "Titouan Parcollet",
                "Alexandre Allauzen",
                "Yannick Est\u00e8ve",
                "Benjamin Lecouteux",
                "Fran\u00e7ois Portet",
                "Solange Rossato",
                "Fabien Ringeval",
                "Didier Schwab",
                "Laurent Besacier"
            ],
            "title": "LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech",
            "venue": "In Proc. Interspeech",
            "year": 2021
        },
        {
            "authors": [
                "Tzu-hsun Feng",
                "Annie Dong",
                "Ching-Feng Yeh",
                "Shu-wen Yang",
                "Tzu-Quan Lin",
                "Jiatong Shi",
                "KaiWei Chang",
                "Zili Huang",
                "Haibin Wu",
                "Xuankai Chang"
            ],
            "title": "SUPERB @SLT 2022: Challenge on generalization and efficiency of self-supervised speech representation learning",
            "venue": "IEEE Spoken Language Technology Workshop (SLT),",
            "year": 2022
        },
        {
            "authors": [
                "Michael P Fitz"
            ],
            "title": "Fundamentals of communications systems",
            "venue": "McGraw-Hill Education,",
            "year": 2007
        },
        {
            "authors": [
                "Marco Gaido",
                "Mauro Cettolo",
                "Matteo Negri",
                "Marco Turchi"
            ],
            "title": "CTC-based compression for direct speech translation",
            "venue": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Zhenke Gao",
                "Man-Wai Mak",
                "Weiwei Lin"
            ],
            "title": "UNet-DenseNet for robust far-field speaker verification",
            "venue": "Proc. Interspeech 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Neeraj Gaur",
                "Brian Farris",
                "Parisa Haghani",
                "Isabel Leal",
                "Pedro J Moreno",
                "Manasa Prasad",
                "Bhuvana Ramabhadran",
                "Yun Zhu"
            ],
            "title": "Mixture of informed experts for multilingual speech recognition",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Golnaz Ghiasi",
                "Tsung-Yi Lin",
                "Quoc V Le"
            ],
            "title": "Nas-fpn: Learning scalable feature pyramid architecture for object detection",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Shouchang Guo",
                "Valentin Deschaintre",
                "Douglas Noll",
                "Arthur Roullier"
            ],
            "title": "U-attention to textures: hierarchical hourglass vision transformer for universal texture synthesis",
            "venue": "In Proceedings of the 19th ACM SIGGRAPH European Conference on Visual Media Production,",
            "year": 2022
        },
        {
            "authors": [
                "Kyu J Han",
                "Jing Pan",
                "Venkata Krishna Naveen Tadala",
                "Tao Ma",
                "Dan Povey"
            ],
            "title": "Multistream cnn for robust acoustic modeling",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Hynek Hermansky"
            ],
            "title": "Perceptual linear predictive (plp) analysis of speech",
            "venue": "Journal of the Acoustical Society of America,",
            "year": 1990
        },
        {
            "authors": [
                "Hynek Hermansky"
            ],
            "title": "Multistream recognition of speech: Dealing with unknown unknowns",
            "venue": "Proceedings of the IEEE,",
            "year": 2013
        },
        {
            "authors": [
                "Wenxin Hou",
                "Yue Dong",
                "Bairong Zhuang",
                "Longfei Yang",
                "Jiatong Shi",
                "Takahiro Shinozaki"
            ],
            "title": "Large-Scale End-to-End Multilingual Speech Recognition and Language Identification with Multi-Task Learning",
            "venue": "In Proc. Interspeech",
            "year": 2020
        },
        {
            "authors": [
                "Wei-Ning Hsu",
                "Benjamin Bolte",
                "Yao-Hung Hubert Tsai",
                "Kushal Lakhotia",
                "Ruslan Salakhutdinov",
                "Abdelrahman Mohamed"
            ],
            "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Wei-Ning Hsu",
                "Anuroop Sriram",
                "Alexei Baevski",
                "Tatiana Likhomanenko",
                "Qiantong Xu",
                "Vineel Pratap",
                "Jacob Kahn",
                "Ann Lee",
                "Ronan Collobert",
                "Gabriel Synnaeve",
                "Michael Auli"
            ],
            "title": "Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training",
            "venue": "In Proc. Interspeech",
            "year": 2021
        },
        {
            "authors": [
                "Wen-Chin Huang",
                "Yi-Chiao Wu",
                "Tomoki Hayashi"
            ],
            "title": "Any-to-one sequence-to-sequence voice conversion using self-supervised discrete speech representations",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Wen-Chin Huang",
                "Shu-Wen Yang",
                "Tomoki Hayashi",
                "Hung-Yi Lee",
                "Shinji Watanabe",
                "Tomoki Toda"
            ],
            "title": "S3prl-vc: Open-source voice conversion framework with self-supervised speech representations",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Wen-Chin Huang",
                "Shu-Wen Yang",
                "Tomoki Hayashi",
                "Tomoki Toda"
            ],
            "title": "A Comparative Study of Self-Supervised Speech Representation Based Voice Conversion",
            "venue": "IEEE Journal of Selected Topics in Signal Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Wen-Chin Huang",
                "Lester Phillip Violeta",
                "Songxiang Liu",
                "Jiatong Shi",
                "Yusuke Yasuda",
                "Tomoki Toda"
            ],
            "title": "The singing voice conversion challenge 2023",
            "venue": "arXiv preprint arXiv:2306.14422,",
            "year": 2023
        },
        {
            "authors": [
                "Xuedong Huang",
                "Alex Acero",
                "Hsiao-Wuen Hon",
                "Raj Reddy"
            ],
            "title": "Spoken language processing: A guide to theory, algorithm, and system development",
            "venue": "Prentice hall PTR,",
            "year": 2001
        },
        {
            "authors": [
                "Kuo-Hsuan Hung",
                "Szu wei Fu",
                "Huan-Hsin Tseng",
                "Hsin-Tien Chiang",
                "Yu Tsao",
                "Chii-Wann Lin"
            ],
            "title": "Boosting Self-Supervised Embeddings for Speech Enhancement",
            "venue": "In Proc. Interspeech",
            "year": 2022
        },
        {
            "authors": [
                "Hirofumi Inaguma",
                "Sravya Popuri",
                "Ilia Kulikov",
                "Peng-Jen Chen",
                "Changhan Wang",
                "Yu-An Chung",
                "Yun Tang",
                "Ann Lee",
                "Shinji Watanabe",
                "Juan Pino"
            ],
            "title": "UnitY: Two-pass direct speech-to-speech translation with discrete units",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Mandar Joshi",
                "Danqi Chen",
                "Yinhan Liu",
                "Daniel S Weld",
                "Luke Zettlemoyer",
                "Omer Levy"
            ],
            "title": "SpanBERT: Improving pre-training by representing and predicting spans",
            "venue": "Transactions of the association for computational linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Kahn",
                "Morgane Rivi\u00e8re",
                "Weiyi Zheng",
                "Evgeny Kharitonov",
                "Qiantong Xu",
                "Pierre-Emmanuel Mazar\u00e9",
                "Julien Karadayi",
                "Vitaliy Liptchinsky",
                "Ronan Collobert",
                "Christian Fuegen"
            ],
            "title": "Librilight: A benchmark for asr with limited or no supervision",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Shigeki Karita",
                "Nelson Enrique Yalta Soplin",
                "Shinji Watanabe",
                "Marc Delcroix",
                "Atsunori Ogawa",
                "Tomohiro Nakatani"
            ],
            "title": "Improving Transformer-Based End-to-End Speech Recognition with Connectionist Temporal Classification and Language Model Integration",
            "venue": "In Proc. Interspeech",
            "year": 2019
        },
        {
            "authors": [
                "Sehoon Kim",
                "Amir Gholami",
                "Albert Eaton Shaw",
                "Nicholas Lee",
                "Karttikeya Mangalam",
                "Jitendra Malik",
                "Michael W Mahoney",
                "Kurt Keutzer"
            ],
            "title": "Squeezeformer: An efficient transformer for automatic speech recognition",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jungil Kong",
                "Jaehyeon Kim",
                "Jaekyoung Bae"
            ],
            "title": "HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Kushal Lakhotia",
                "Eugene Kharitonov",
                "Wei-Ning Hsu",
                "Yossi Adi",
                "Adam Polyak",
                "Benjamin Bolte",
                "Tu-Anh Nguyen",
                "Jade Copet",
                "Alexei Baevski",
                "Abdelrahman Mohamed"
            ],
            "title": "On generative spoken language modeling from raw audio",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Ann Lee",
                "Peng-Jen Chen",
                "Changhan Wang",
                "Jiatao Gu",
                "Sravya Popuri",
                "Xutai Ma",
                "Adam Polyak",
                "Yossi Adi",
                "Qing He",
                "Yun Tang"
            ],
            "title": "Direct speech-to-speech translation with discrete units",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Ann Lee",
                "Hongyu Gong",
                "Paul-Ambroise Duquenne",
                "Holger Schwenk",
                "Peng-Jen Chen",
                "Changhan Wang",
                "Sravya Popuri",
                "Yossi Adi",
                "Juan Pino",
                "Jiatao Gu"
            ],
            "title": "Textless speech-to-speech translation on real data",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Bo Li",
                "Yu Zhang",
                "Tara Sainath",
                "Yonghui Wu",
                "William Chan"
            ],
            "title": "Bytes are all you need: Endto-end multilingual speech recognition and synthesis with bytes",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Rui Li",
                "Dong Pu",
                "Minnie Huang",
                "Bill Huang"
            ],
            "title": "Unet-TTS: Improving unseen speaker and style transfer in one-shot voice cloning",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Ruizhi Li",
                "Xiaofei Wang",
                "Sri Harish Mallidi",
                "Shinji Watanabe",
                "Takaaki Hori",
                "Hynek Hermansky"
            ],
            "title": "Multi-stream end-to-end speech recognition",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Xinjian Li",
                "Ye Jia",
                "Chung-Cheng Chiu"
            ],
            "title": "Textless direct speech-to-speech translation with discrete speech representation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Yizhi Li",
                "Ruibin Yuan",
                "Ge Zhang",
                "Yinghao Ma",
                "Xingran Chen",
                "Hanzhi Yin",
                "Chenghua Lin",
                "Anton Ragni",
                "Emmanouil Benetos",
                "Norbert Gyenge"
            ],
            "title": "MERT: Acoustic music understanding model with large-scale self-supervised training",
            "venue": "arXiv preprint arXiv:2306.00107,",
            "year": 2023
        },
        {
            "authors": [
                "Yuang Li",
                "Yu Wu",
                "Jinyu Li",
                "Shujie Liu"
            ],
            "title": "Accelerating Transducers through Adjacent Token Merging",
            "venue": "In Proc. Interspeech",
            "year": 2023
        },
        {
            "authors": [
                "Jiachen Lian",
                "Chunlei Zhang",
                "Gopala Krishna Anumanchipalli",
                "Dong Yu"
            ],
            "title": "Utts: Unsupervised tts with conditional disentangled sequential variational auto-encoder",
            "venue": "arXiv preprint arXiv:2206.02512,",
            "year": 2022
        },
        {
            "authors": [
                "Guan-Ting Lin",
                "Yung-Sung Chuang",
                "Ho-Lam Chung",
                "Shu wen Yang",
                "Hsuan-Jui Chen",
                "Shuyan Annie Dong",
                "Shang-Wen Li",
                "Abdelrahman Mohamed",
                "Hung yi Lee",
                "Lin shan Lee"
            ],
            "title": "DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering",
            "venue": "In Proc. Interspeech",
            "year": 2022
        },
        {
            "authors": [
                "Guan-Ting Lin",
                "Chi-Luen Feng",
                "Wei-Ping Huang",
                "Yuan Tseng",
                "Tzu-Han Lin",
                "Chen-An Li",
                "Hung-yi Lee",
                "Nigel G Ward"
            ],
            "title": "On the utility of self-supervised models for prosody-related tasks",
            "venue": "IEEE Spoken Language Technology Workshop (SLT),",
            "year": 2022
        },
        {
            "authors": [
                "Tzu-Quan Lin",
                "Hung-yi Lee",
                "Hao Tang"
            ],
            "title": "Melhubert: A simplified hubert on mel spectrogram",
            "venue": "arXiv preprint arXiv:2211.09944,",
            "year": 2022
        },
        {
            "authors": [
                "Andy T Liu",
                "Shu-wen Yang",
                "Po-Han Chi",
                "Po-chun Hsu",
                "Hung-yi Lee"
            ],
            "title": "Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Li-Juan Liu",
                "Yan-Nian Chen",
                "Jing-Xuan Zhang",
                "Yuan Jiang",
                "Ya-Jun Hu",
                "Zhen-Hua Ling",
                "LiRong Dai"
            ],
            "title": "Non-parallel voice conversion with autoregressive conversion model and duration adjustment",
            "venue": "In Proc. Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge",
            "year": 2020
        },
        {
            "authors": [
                "Loren Lugosch",
                "Tatiana Likhomanenko",
                "Gabriel Synnaeve",
                "Ronan Collobert"
            ],
            "title": "Pseudo-labeling for massively multilingual speech recognition",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Jian Luo",
                "Jianzong Wang",
                "Ning Cheng",
                "Guilin Jiang",
                "Jing Xiao"
            ],
            "title": "Multi-quartznet: Multiresolution convolution for speech recognition with multi-layer feature fusion",
            "venue": "IEEE Spoken Language Technology Workshop (SLT),",
            "year": 2021
        },
        {
            "authors": [
                "Yinghao Ma",
                "Ruibin Yuan",
                "Yizhi Li",
                "Ge Zhang",
                "Xingran Chen",
                "Hanzhi Yin",
                "Chenghua Lin",
                "Emmanouil Benetos",
                "Anton Ragni",
                "Norbert Gyenge"
            ],
            "title": "On the effectiveness of speech selfsupervised learning for music",
            "venue": "arXiv preprint arXiv:2307.05161,",
            "year": 2023
        },
        {
            "authors": [
                "Sri Harish Mallidi",
                "Hynek Hermansky"
            ],
            "title": "Novel neural network based fusion for multistream ASR",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2016
        },
        {
            "authors": [
                "Sri Harish Reddy Mallidi"
            ],
            "title": "A practical and efficient multistream framework for noise robust speech recognition",
            "year": 2018
        },
        {
            "authors": [
                "Yoshiki Masuyama",
                "Xuankai Chang",
                "Samuele Cornell",
                "Shinji Watanabe",
                "Nobutaka Ono"
            ],
            "title": "End-toend integration of speech recognition, dereverberation, beamforming, and self-supervised learning representation",
            "venue": "IEEE Spoken Language Technology Workshop (SLT),",
            "year": 2022
        },
        {
            "authors": [
                "Michael Mathieu",
                "Camille Couprie",
                "Yann LeCun"
            ],
            "title": "Deep multi-scale video prediction beyond mean square error",
            "venue": "In 4th International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "Iaroslav Melekhov",
                "Juha Ylioinas",
                "Juho Kannala",
                "Esa Rahtu"
            ],
            "title": "Image-based localization using hourglass networks",
            "venue": "In Proceedings of the IEEE international conference on computer vision workshops,",
            "year": 2017
        },
        {
            "authors": [
                "Yen Meng",
                "Hsuan-Jui Chen",
                "Jiatong Shi",
                "Shinji Watanabe",
                "Paola Garcia",
                "Hung-yi Lee",
                "Hao Tang"
            ],
            "title": "On compressing sequences for self-supervised speech models",
            "venue": "IEEE Spoken Language Technology Workshop (SLT),",
            "year": 2022
        },
        {
            "authors": [
                "Abdelrahman Mohamed",
                "Hung-yi Lee",
                "Lasse Borgholt",
                "Jakob D Havtorn",
                "Joakim Edin",
                "Christian Igel",
                "Katrin Kirchhoff",
                "Shang-Wen Li",
                "Karen Livescu",
                "Lars Maal\u00f8e"
            ],
            "title": "Self-supervised speech representation learning: A review",
            "venue": "IEEE Journal of Selected Topics in Signal Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Piotr Nawrot",
                "Szymon Tworkowski",
                "Micha\u0142 Tyrolski",
                "\u0141ukasz Kaiser",
                "Yuhuai Wu",
                "Christian Szegedy",
                "Henryk Michalewski"
            ],
            "title": "Hierarchical transformers are more efficient language models",
            "venue": "In Findings of the Association for Computational Linguistics: NAACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Piotr Nawrot",
                "Jan Chorowski",
                "Adrian Lancucki",
                "Edoardo Maria Ponti"
            ],
            "title": "Efficient transformers with dynamic token pooling",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Alejandro Newell",
                "Kaiyu Yang",
                "Jia Deng"
            ],
            "title": "Stacked hourglass networks for human pose estimation",
            "venue": "In Computer Vision\u2013ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Tu Anh Nguyen",
                "Wei-Ning Hsu",
                "Antony D\u2019Avirro",
                "Bowen Shi",
                "Itai Gat",
                "Maryam Fazel-Zarani",
                "Tal Remez",
                "Jade Copet",
                "Gabriel Synnaeve",
                "Michael Hassid"
            ],
            "title": "Expresso: A benchmark and analysis of discrete expressive speech resynthesis",
            "venue": "arXiv preprint arXiv:2308.05725,",
            "year": 2023
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "Shinta Otake",
                "Rei Kawakami",
                "Nakamasa Inoue"
            ],
            "title": "Parameter efficient transfer learning for various speech processing tasks",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Myle Ott",
                "Sergey Edunov",
                "Alexei Baevski",
                "Angela Fan",
                "Sam Gross",
                "Nathan Ng",
                "David Grangier",
                "Michael Auli"
            ],
            "title": "fairseq: A fast, extensible toolkit for sequence modeling",
            "venue": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations),",
            "year": 2019
        },
        {
            "authors": [
                "Vassil Panayotov",
                "Guoguo Chen",
                "Daniel Povey",
                "Sanjeev Khudanpur"
            ],
            "title": "Librispeech: an asr corpus based on public domain audio books",
            "venue": "In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP),",
            "year": 2015
        },
        {
            "authors": [
                "Adam Polyak",
                "Yossi Adi",
                "Jade Copet",
                "Eugene Kharitonov",
                "Kushal Lakhotia",
                "Wei-Ning Hsu",
                "Abdelrahman Mohamed",
                "Emmanuel Dupoux"
            ],
            "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations",
            "venue": "In Proc. Interspeech",
            "year": 2021
        },
        {
            "authors": [
                "Kaizhi Qian",
                "Yang Zhang",
                "Heting Gao",
                "Junrui Ni",
                "Cheng-I Lai",
                "David Cox",
                "Mark HasegawaJohnson",
                "Shiyu Chang"
            ],
            "title": "Contentvec: An improved self-supervised speech representation by disentangling speakers",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Richard A Roberts",
                "Clifford T Mullis"
            ],
            "title": "Digital signal processing",
            "venue": "Addison-Wesley Longman Publishing Co., Inc.,",
            "year": 1987
        },
        {
            "authors": [
                "Jiatong Shi",
                "Dan Berrebbi",
                "William Chen",
                "En-Pei Hu",
                "Wei-Ping Huang",
                "Ho-Lam Chung",
                "Xuankai Chang",
                "Shang-Wen Li",
                "Abdelrahman Mohamed",
                "Hung yi Lee",
                "Shinji Watanabe"
            ],
            "title": "MLSUPERB: Multilingual Speech Universal PERformance Benchmark",
            "venue": "In Proc. Interspeech",
            "year": 2023
        },
        {
            "authors": [
                "Jiatong Shi",
                "Chan-Jan Hsu",
                "Holam Chung",
                "Dongji Gao",
                "Paola Garcia",
                "Shinji Watanabe",
                "Ann Lee",
                "Hung-yi Lee"
            ],
            "title": "Bridging speech and textual pre-trained models with unsupervised ASR",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Jiatong Shi",
                "Yun Tang",
                "Hirofumi Inaguma",
                "Hongyu Gong",
                "Juan Pino",
                "Shinji Watanabe"
            ],
            "title": "Exploration on HuBERT with Multiple Resolution",
            "venue": "In Proc. Interspeech",
            "year": 2023
        },
        {
            "authors": [
                "Jiatong Shi",
                "Yun Tang",
                "Ann Lee",
                "Hirofumi Inaguma",
                "Changhan Wang",
                "Juan Pino",
                "Shinji Watanabe"
            ],
            "title": "Enhancing speech-to-speech translation with multiple tts targets",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Jing Shi",
                "Xuankai Chang",
                "Tomoki Hayashi",
                "Yen-Ju Lu",
                "Shinji Watanabe",
                "Bo Xu"
            ],
            "title": "Discretization and re-synthesis: an alternative method to solve the cocktail party problem",
            "venue": "arXiv preprint arXiv:2112.09382,",
            "year": 2021
        },
        {
            "authors": [
                "Ziqiang Shi",
                "Huibin Lin",
                "Liu Liu",
                "Rujie Liu",
                "Shoji Hayakawa",
                "Shouji Harada",
                "Jiqing Han"
            ],
            "title": "End-to-End Monaural Speech Separation with Multi-Scale Dynamic Weighted Gated Dilated Convolutional Pyramid Network",
            "venue": "In Proc. Interspeech",
            "year": 2019
        },
        {
            "authors": [
                "Amitay Sicherman",
                "Yossi Adi"
            ],
            "title": "Analysing discrete self supervised speech representation for spoken language modeling",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Shubham Toshniwal",
                "Tara N Sainath",
                "Ron J Weiss",
                "Bo Li",
                "Pedro Moreno",
                "Eugene Weinstein",
                "Kanishka Rao"
            ],
            "title": "Multilingual speech recognition with a single end-to-end model",
            "venue": "IEEE international conference on acoustics, speech and signal processing (ICASSP),",
            "year": 2018
        },
        {
            "authors": [
                "Joseph Turian",
                "Jordie Shier",
                "Humair Raj Khan",
                "Bhiksha Raj",
                "Bj\u00f6rn W Schuller",
                "Christian J Steinmetz",
                "Colin Malloy",
                "George Tzanetakis",
                "Gissel Velarde",
                "Kirk McNally"
            ],
            "title": "Hear: Holistic evaluation of audio representations",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Qiqi Wang",
                "Xulong Zhang",
                "Jianzong Wang",
                "Ning Cheng",
                "Jing Xiao"
            ],
            "title": "DRVC: A framework of any-to-any voice conversion with self-supervised learning",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Yingzhi Wang",
                "Abdelmoumene Boumadane",
                "Abdelwahab Heba"
            ],
            "title": "A fine-tuned wav2vec 2.0/HuBERT benchmark for speech emotion recognition, speaker verification and spoken language understanding",
            "venue": "arXiv preprint arXiv:2111.02735,",
            "year": 2021
        },
        {
            "authors": [
                "Shinji Watanabe",
                "Takaaki Hori",
                "John R Hershey"
            ],
            "title": "Language independent end-to-end architecture for joint language identification and speech recognition",
            "venue": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),",
            "year": 2017
        },
        {
            "authors": [
                "Shinji Watanabe",
                "Takaaki Hori",
                "Shigeki Karita",
                "Tomoki Hayashi",
                "Jiro Nishitoba",
                "Yuya Unno",
                "Nelson Enrique Yalta Soplin",
                "Jahn Heymann",
                "Matthew Wiesner",
                "Nanxin Chen",
                "Adithya Renduchintala",
                "Tsubasa Ochiai"
            ],
            "title": "ESPnet: End-to-End Speech Processing Toolkit",
            "venue": "In Proc. Interspeech",
            "year": 2018
        },
        {
            "authors": [
                "Jian Wu",
                "Yashesh Gaur",
                "Zhuo Chen",
                "Long Zhou",
                "Yimeng Zhu",
                "Tianrui Wang",
                "Jinyu Li",
                "Shujie Liu",
                "Bo Ren",
                "Linquan Liu"
            ],
            "title": "On decoder-only architecture for speech-to-text and large language model integration",
            "venue": "arXiv preprint arXiv:2307.03917,",
            "year": 2023
        },
        {
            "authors": [
                "Jilong Wu",
                "Adam Polyak",
                "Yaniv Taigman",
                "Jason Fong",
                "Prabhav Agrawal",
                "Qing He"
            ],
            "title": "Multilingual text-to-speech training using cross language voice conversion and self-supervised learning of speech representations",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoxiao Xiang",
                "Xiaojuan Zhang",
                "Haozhe Chen"
            ],
            "title": "A convolutional network with multi-scale and attention mechanisms for end-to-end single-channel speech enhancement",
            "venue": "IEEE Signal Processing Letters,",
            "year": 2021
        },
        {
            "authors": [
                "Chenglin Xu",
                "Wei Rao",
                "Eng Siong Chng",
                "Haizhou Li"
            ],
            "title": "Spex: Multi-scale time domain speaker extraction network",
            "venue": "IEEE/ACM transactions on audio, speech, and language processing,",
            "year": 2020
        },
        {
            "authors": [
                "Ryuichi Yamamoto",
                "Eunwoo Song",
                "Jae-Min Kim"
            ],
            "title": "Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Brian Yan",
                "Jiatong Shi",
                "Yun Tang",
                "Hirofumi Inaguma",
                "Yifan Peng",
                "Siddharth Dalmia",
                "Peter Pol\u00e1k",
                "Patrick Fernandes",
                "Dan Berrebbi",
                "Tomoki Hayashi",
                "Xiaohui Zhang",
                "Zhaoheng Ni",
                "Moto Hira",
                "Soumi Maiti",
                "Juan Pino",
                "Shinji Watanabe"
            ],
            "title": "ESPnet-ST-v2: Multipurpose spoken language translation toolkit",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations),",
            "year": 2023
        },
        {
            "authors": [
                "Jing Yang",
                "Qingshan Liu",
                "Kaihua Zhang"
            ],
            "title": "Stacked hourglass network for robust facial landmark localisation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition workshops,",
            "year": 2017
        },
        {
            "authors": [
                "Shu-Wen Yang",
                "Po-Han Chi",
                "Yung-Sung Chuang",
                "Cheng-I Jeff Lai",
                "Kushal Lakhotia",
                "Yist Y. Lin",
                "Andy T. Liu",
                "Jiatong Shi",
                "Xuankai Chang",
                "Guan-Ting Lin",
                "Tzu-Hsien Huang",
                "Wei-Cheng Tseng",
                "Ko tik Lee",
                "Da-Rong Liu",
                "Zili Huang",
                "Shuyan Dong",
                "Shang-Wen Li",
                "Shinji Watanabe",
                "Abdelrahman Mohamed",
                "Hung yi Lee"
            ],
            "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
            "venue": "In Proc. Interspeech",
            "year": 2021
        },
        {
            "authors": [
                "Songfan Yang",
                "Deva Ramanan"
            ],
            "title": "Multi-scale recognition with dag-cnns",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Zhao Yi",
                "Wen-Chin Huang",
                "Xiaohai Tian",
                "Junichi Yamagishi",
                "Rohan Kumar Das",
                "Tomi Kinnunen",
                "Zhen-Hua Ling",
                "Tomoki Toda"
            ],
            "title": "Voice Conversion Challenge 2020 \u2014 Intra-lingual semiparallel and cross-lingual voice conversion",
            "venue": "In Proc. Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge",
            "year": 2020
        },
        {
            "authors": [
                "Reo Yoneyama",
                "Yi-Chiao Wu",
                "Tomoki Toda"
            ],
            "title": "Source-filter hifi-gan: Fast and pitch controllable high-fidelity neural vocoder",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Ruibin Yuan",
                "Yinghao Ma",
                "Yizhi Li",
                "Ge Zhang",
                "Xingran Chen",
                "Hanzhi Yin",
                "Le Zhuo",
                "Yiqi Liu",
                "Jiawen Huang",
                "Zeyue Tian"
            ],
            "title": "MARBLE: Music audio representation benchmark for universal evaluation",
            "venue": "arXiv preprint arXiv:2306.10548,",
            "year": 2023
        },
        {
            "authors": [
                "Mingliang Zhai",
                "Yulin Li",
                "Xiameng Qin",
                "Chen Yi",
                "Qunyi Xie",
                "Chengquan Zhang",
                "Kun Yao",
                "Yuwei Wu",
                "Yunde Jia"
            ],
            "title": "Fast-strucTexT: An efficient Hourglass transformer with modality-guided dynamic token merge for document understanding",
            "venue": "In Edith Elkind (ed.), Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Chao Zhang",
                "Bo Li",
                "Tara Sainath",
                "Trevor Strohman",
                "Sepand Mavandadi",
                "Shuo-Yiin Chang",
                "Parisa Haghani"
            ],
            "title": "Streaming End-to-End Multilingual Speech Recognition with Joint Language Identification",
            "venue": "In Proc. Interspeech",
            "year": 2022
        },
        {
            "authors": [
                "Guochang Zhang",
                "Libiao Yu",
                "Chunliang Wang",
                "Jianqiang Wei"
            ],
            "title": "Multi-scale temporal frequency convolutional network with axial attention for speech enhancement",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Lu Zhang",
                "Mingjiang Wang"
            ],
            "title": "Multi-Scale TCN: Exploring Better Temporal DNN Model for Causal Speech Enhancement",
            "venue": "In Proc. Interspeech",
            "year": 2020
        },
        {
            "authors": [
                "Yi Zhang",
                "Qing Duan",
                "Yun Liao",
                "Junhui Liu",
                "Ruiqiong Wu",
                "Bisen Xie"
            ],
            "title": "Research on speech enhancement algorithm based on SA-Unet",
            "venue": "4th International Conference on Mechanical, Control and Computer Engineering (ICMCCE),",
            "year": 2019
        },
        {
            "authors": [
                "Jing Zhao",
                "Wei-Qiang Zhang"
            ],
            "title": "Improving automatic speech recognition performance for lowresource languages with self-supervised models",
            "venue": "IEEE Journal of Selected Topics in Signal Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Tuo Zhao",
                "Yunxin Zhao",
                "Shaojun Wang",
                "Mei Han"
            ],
            "title": "Unet++-based multi-channel speech dereverberation and distant speech recognition",
            "venue": "12th International Symposium on Chinese Spoken Language Processing (ISCSLP),",
            "year": 2021
        },
        {
            "authors": [
                "Qifeng Zhu",
                "Abeer Alwan"
            ],
            "title": "On the use of variable frame rate analysis in speech recognition",
            "venue": "IEEE international conference on acoustics, speech, and signal processing (ICASSP),",
            "year": 2000
        },
        {
            "authors": [
                "Hsu"
            ],
            "title": "2021a), a more substantial batch size can typically augment model performance. In our research, when juxtaposing our method against the baselines, we\u2019ve meticulously ensured that the batch size of our approach is either equivalent to or smaller than that of the baseline, to offset potential biases. All model training was executed on V100-32GB GPUs using the Fariseq toolkit (Ott et al., 2019)",
            "year": 2019
        },
        {
            "authors": [
                "Kahn"
            ],
            "title": "The above ablations are all conducted in base setting for efficiency, while we also conduct selected large setting experiments in Appendix B.8. As detailed in Section 4.2, we utilize the labeled LibriSpeech subsets of 1-hour, 10-hour, and 100hour",
            "year": 2020
        },
        {
            "authors": [
                "Tsai"
            ],
            "title": "2022), we also extended our research to voice conversion tasks to examine the efficacy of our approach. To achieve this, we largely followed the blueprint provided by the S3PRL recipe on the Voice Conversion Challenge 2020 (VCC2020) as detailed by (Yi et al., 2020)",
            "year": 2020
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "2022a). Given that the weights of each layer participate in the backpropagation",
            "year": 2022
        },
        {
            "authors": [
                "2023 Barrault et al",
                "2023 Yan et al",
                "Huang"
            ],
            "title": "2023). As MR-HuBERT melds lowresolution layers for enriched semantics with high-resolution layers for nuanced acoustics, it can offer a more holistic representation. This multi-faceted view could be pivotal in enhancing speech quality in generative tasks",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In physics, speech is defined as a vibration that propagates as an acoustic wave through a transmission medium (Fitz, 2007). In the field of speech processing, speech signals are stored using techniques such as sampling and quantization. This results in a discretized abstraction of the original waveform, in both time and amplitude (Roberts & Mullis, 1987).\nIn practical real-world scenarios, the sampling rate for speech signals can vary between 8 kHz and 48 kHz. High sampling rates can pose challenges for processing due to complications in analyzing long sequences. Typically, speech signals exhibit short-term stationarity within intervals ranging from 10 to 30ms (Zhu & Alwan, 2000). Taking these factors into account, past research has recommended frame-wise processing of speech signals, with frames being extracted over localized sample points (Huang et al., 2001). Traditional spectral feature extraction methods, often based on psychoacoustics, utilize short-term Fourier transform over windows ranging from 20 to 40ms, with shifts between 10 and 30ms (Huang et al., 2001; Davis & Mermelstein, 1980; Hermansky, 1990).\nWhile these conventional spectral features exhibit properties that align well with human psychoacoustics, speech processing systems relying on these features require large volumes of transcribed audio data to achieve high performance (Yu & Deng, 2016). In contrast, Self-Supervised Learning (SSL) speech models utilize unlabeled speech data to generate contextualized speech representations (Oord et al., 2018; Liu et al., 2020a; Baevski et al., 2020; Hsu et al., 2021a; Chung et al., 2021; Chiu et al., 2022; Chen et al., 2022a). These SSL models have shown superior capabilities in contextualizing speech, achieving state-of-the-art results on various benchmarks and challenges (Panayotov et al., 2015; Yang et al., 2021; Evain et al., 2021; Mohamed et al., 2022; Shi et al., 2023a; Agrawal et al., 2023). Moreover, they demonstrate excellent generalizability to low-resource tasks (Baevski et al., 2020; Hsu et al., 2021a; Berrebbi et al., 2022; Zhao & Zhang, 2022). Despite these advancements, existing speech SSL models predominantly follow a similar approach when it comes to processing speech signals. They typically extract speech frames of 20ms as their fundamental units for pre-training (Baevski et al., 2020; Hsu et al., 2021a; Chung et al., 2021; Chiu et al., 2022; Chen et al., 2022a). This extraction can be accomplished using either a convolutional feature extrac-\ntor (Baevski et al., 2020; Hsu et al., 2021a; Chen et al., 2022a) or traditional features like Mel filter banks (Lin et al., 2022b; Barrault et al., 2023).\nNotably, this uniform frame size of 20ms may not be universally optimal across different downstream tasks. In line with conventional spectral features, existing literature suggests that multiresolution modeling could enhance performance in various speech processing tasks, such as Automatic Speech Recognition (ASR) (Mallidi & Hermansky, 2016; Mallidi et al., 2018; Hermansky, 2013; Han et al., 2021; Luo et al., 2021; Li et al., 2019b; Andrusenko et al., 2023; Kim et al., 2022; Burchi & Vielzeuf, 2021), Speaker Verification (SV) (Gao et al., 2022), Speech Enhancement (SE) (Zhao et al., 2021; Zhang et al., 2019), and Voice Conversion (VC) (Li et al., 2022). Supporting this notion, recent work by Shi et al. (2023c) demonstrated the advantages of multi-resolution training by using three separate SSL models. Their findings indicate that combining these models focusing on different representations can yield superior results across various tasks, whether used in fine-tuning or as frozen feature extractors. However, the method needs to train different SSL models for each resolution, resulting in a huge computation burden from pre-training.\nDespite existing efforts to utilize SSL models for speech at multiple resolutions, no work has explicitly addressed the integration of multi-resolution information during the pre-training phase. This study aims to fill that gap by focusing on multi-resolution pre-training for speech representation. We introduce a novel hierarchical framework, namely multi-resolution HuBERT (MR-HuBERT) designed to encode speech information across multiple resolutions in a single model. The model is pre-trained using objectives for multi-resolution masked unit prediction, which are integrated with HuBERT-style clustering units (Hsu et al., 2021a). Our model shows substantial performance improvements over baseline SSL models across a variety of benchmarks. These include different subsets of the LibriSpeech dataset, the Speech Universal PERformance Benchmark (SUPERB), and the Multilingual SUPERB (ML-SUPERB) (Panayotov et al., 2015; Yang et al., 2021; Shi et al., 2023a). Another of the key advantages of our approach is efficiency; the reduced sequence length resulting from multi-resolution processing enables faster inference to 9-13% computation reduction."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "Self-supervised learning has achieved remarkable success in a wide array of domains, such as computer vision and natural language processing. As detailed in Section 1, similar advancements have been made in the speech processing community. According to the classification scheme by Mohamed et al. (2022), current speech SSL models can be categorized into generative, contrastive, and predictive approaches. Among these, predictive models have shown particularly promising results in recent benchmarks for SSL representation (Yang et al., 2021; Feng et al., 2023; Wang et al., 2021b; Masuyama et al., 2023; Hsu et al., 2021a; Chen et al., 2022a).\nAs introduced in Section 1, speech SSL models can be applied to various downstream tasks through either fine-tuning or as frozen feature extractors. The architecture of the downstream models can vary widely, including a simple linear probing layer, recurrent neural network (RNN) layers, Transformer layers, or more complex encoder-decoder frameworks (Baevski et al., 2020; Hsu et al., 2021a; Chung et al., 2021; Yang et al., 2021; Chang et al., 2021; Shi et al., 2023a; Inaguma et al., 2023; Barrault et al., 2023). In all these applications, SSL models generate a sequence of hidden representations with a fixed frameshift, usually around 20ms, which serve as inputs to the downstream tasks.\nTwo models that have notably excelled in recent benchmarks are HuBERT and WavLM (Hsu et al., 2021a; Chen et al., 2022a). HuBERT employs quantized features for masked unit prediction in the context of masked speech signals (Hsu et al., 2021a). Specifically, the model uses the classic Kmeans algorithm with a fixed cluster size K to perform quantization, where cluster centroid IDs represent the target for each 20ms frame. A noteworthy aspect of HuBERT\u2019s pre-training strategy is its iterative training concept. Initially, clustering is performed on Mel Filter-bank Cepstral Coefficients (MFCC), termed as the first iteration. Subsequently, a hidden layer from the first iteration model is extracted and clustered to improve performance. Through this two-stage iterative approach, HuBERT has been shown to either match or exceed the performance of prior state-of-the-art models across various tasks (Hsu et al., 2021a; Yang et al., 2021). With a similar training scheme as HuBERT, WavLM differentiates itself by employing modified self-attention mechanisms and incor-\nporating utterance mixing as a data augmentation technique. As these modifications are not the focus of this paper, our work mainly focuses on the framework of HuBERT and extends over that."
        },
        {
            "heading": "3 MR-HUBERT",
            "text": ""
        },
        {
            "heading": "3.1 HUBERT",
            "text": "Consider a sequence of single-channel speech signal S \u2208 R1\u00d7Ls , where Ls represents the length of the speech signal. For a given iteration q, the speech signal S is initially quantized by a pre-trained K-means clustering model gq(\u00b7), which is trained on the hidden representations from the q \u2212 1 iteration.1\nAs detailed in Sections 1 and 2, HuBERT employs a convolutional feature extractor fq0 (\u00b7) to first transform the speech signal S into hidden representations at a frame size of 20ms. Following the masking strategies of wav2vec 2.0 and SpanBERT (Baevski et al., 2020; Joshi et al., 2020), \u03b1% of the frames are chosen randomly as starting indices, and l subsequent frames are masked. The set of masked indices is denoted by M.\nA Transformer encoder fq1 (\u00b7) is then tasked with predicting the quantized clusters of the masked regions, utilizing cross-entropy loss. The loss function at iteration q is given by:\nLqm(\u03b8;S,M, gq) = \u2211 t\u2208M log p\u03b8(g q(S) | H\u0303q0 , t), (1)\nwhere \u03b8 is the model parameters, H\u0303q0 denotes the masked speech frames from the convolutional feature extractor and t is the time step. It is worth noting that while one could define an unmasked loss Lu, previous experiments have shown that this does not yield significant improvements in the quality of HuBERT\u2019s pre-training (Hsu et al., 2021a)."
        },
        {
            "heading": "3.2 ARCHITECTURE",
            "text": "The proposed architecture for MR-HuBERT is schematically shown in Figure 1. For this explanation, we exemplify a model with two resolutions. This architecture employs a hierarchical Transformer framework to explicitly encode hidden representations at multiple resolutions while retaining the iterative strategy found in the original HuBERT. The essential components of the framework are as follows:\nGiven an speech signal S, the convolutional feature extractor fq0 yields frame-wise feature H0 \u2208 RLR1\u00d7D at a high resolution R1. In this context, LR1 is the frame length and D is the feature dimension, which corresponds to the size of the convolutional channels. As outlined in Section 3.1, a masking function m(\u00b7,M) is applied to H0 to generate a sequence of masked features H\u03030 \u2208 RLR1\u00d7D. This function replaces the feature frames corresponding to the indices in M with zero vectors.\nNext, the masked features H\u03030 are processed by a HuBERT-style Transformer encoder f q 1 , noted as High Resolution Transformer Encoder in Figure 1 to produce H\u0303q1 . The encoder consists of a pre-convolutional module as well as a stack of transformer layers. The pre-convolutional module includes a 1D-convolutional layer, followed by Layer Normalization and a GELU activation function.\nAfter the high-resolution encoding, the output H\u0303q1 \u2208 RLR1\u00d7D is subjected to a downsampling module DOWN(\u00b7) to produce a downsampled representation H\u0303q2 \u2208 RLR2\u00d7D. Here, R2 denotes the lower resolution, and LR2 is the corresponding length of the downsampled hidden representation. The downsampled H\u0303q2 serves as the input for a Low Resolution Transformer Encoder f q 2 , as illustrated in Figure 1. Unlike fq1 , f q 2 does not include a pre-convolutional module. Its output H\u0303 q 3 , when coupled with a linear projection, is utilized to predict low-resolution units gqR2(S) \u2208 N +LR2 based on the quantization method gqR2(\u00b7), detailed in Section 3.4. The whole process of generating H\u0303 q 3 can be summarized into:\nH\u0303q3 = f q 2 \u25e6 DOWN \u25e6 f q 1 (m(f q 0 (S),M)). (2)\n1The initial iteration (q = 0) employs representations derived from MFCC features.\nFinally, an upsampling module UP(\u00b7) expands H\u0303q3 back to high resolution R1, resulting in H\u0303 q 4 \u2208 RLR1\u00d7D. This output, when summed with H\u0303q1 , is fed into another High Resolution Transformer Encoder fq3 (\u00b7). The ultimate output H\u0303 q 5 \u2208 RLR1\u00d7D is then employed to predict high-resolution units obtained via the quantization method gqR1(\u00b7). Given H\u0303 q 3 , the process of generating H\u0303 q 5 can be summarized into:\nH\u0303q5 = f q 3 (UP(H\u0303 q 3 ) + f q 1 (m(f q 0 (S),M))). (3)"
        },
        {
            "heading": "3.3 SAMPLING MODULES",
            "text": "As introduced in Section 3.2, the proposed architecture utilizes an upsampling module UP(\u00b7) and a downsampling module DOWN(\u00b7). The two sampling modules share the same design, as illustrated in Figure 2. The architecture is adapted from the multi-resolution fusion module in Shi et al. (2023c).\nTo exemplify, we consider the downsampling module. The module first rescale H\u0303q1 into a higher resolutionR1 \u00b7R\u20321 through De-Convolutional Upsampler DeConv(\u00b7) and Repeat-Upsampler Repeat(\u00b7), respectively.2 The output, H\u0303q\u2212up1 \u2208 R(LR1 \u00b7R \u2032 1)\u00d7D is fed into a Convolutional Downsampler Conv(\u00b7) and a Skip-Downsampler Skip(\u00b7), respectively. The final output of the downsampling module, denoted as H\u0303q2 in Section 3.2, is the defined as:\nH\u0303q2 = \u03d5 \u00b7 [Skip(Repeat(H\u0303 q 1 )) + \u03d5 \u00b7 (Conv(H\u0303 q\u2212up 1 ) + Skip(H\u0303 q\u2212up 1 ))] (4)\n2Given H\u0303q1 \u2208 RLR1\u00d7D and the target resolution R2, R\u20321 and R\u20322 are the numerator and denominator of the reduced fraction between R1 and R2. They are used as the upsampling factor and the downsampling factor, respectively."
        },
        {
            "heading": "3.4 OBJECTIVES",
            "text": "Similar to HuBERT discussed in Section 3.1, the objectives of MR-HuBERT focus on masked unit prediction. The major design question for MR-HuBERT, however, is how to construct units for different resolutions. In our experiments discussed in Section 4, we compare different settings in multi-resolution units preparation. The default and most effective approach is simply start from high resolution units extraction and then subsample the low resolution units to match the low resolution sequence from the the Low Resolution Transformer Encoder fq2 . The high resolution units extraction process is similar to HuBERT, by applyingK-means over hidden representations from q\u22121 iteration. To be specific, gqR1(\u00b7) is the K-means model, where g q R2\nis gqR1 \u25e6 d(\u00b7), where d is a subsampling function.\nThe pre-training involves two losses: one for high-resolution and another for low-resolution masked unit prediction:\nLq\u2212{high, low}m (\u03b8{high, low};S,M, g q {R1,R2}) = \u2211 t\u2208M log p\u03b8{high, low}(g q {R1,R2}(S)|H\u0303 q 0 , t), (5)\nwhere \u03b8high are the model parameters of the MR-HuBERT, while \u03b8low are partial model parameters that exclude UP(\u00b7) and fq3 (\u00b7). The final objective combines these losses:\nLqm = \u03b2 \u00b7 Lq\u2212highm + \u03b3 \u00b7 Lq\u2212lowm , (6)\nwhere \u03b2 and \u03b3 are hyperparameters."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We evaluate the proposed methods using a variety of speech processing tasks, segmented into four key categories: speech recognition on the LibriSpeech benchmarks (Panayotov et al., 2015), SUPERB benchmark evaluation (Yang et al., 2021) and multilingual SUPERB (ML-SUPERB) benchmark evaluation (Shi et al., 2023a)."
        },
        {
            "heading": "4.1 PRE-TRAINING",
            "text": "Datasets: We perform pre-training on three corpora: LibriSpeech (Panayotov et al., 2015), LibriLight (Kahn et al., 2020), and Voxpopuli (Wang et al., 2021a). LibriSpeech and LibriLight focus exclusively on English, while Voxpopuli is a multilingual dataset encompassing 23 European languages. The total dataset sizes amount to 960 hours for LibriSpeech, 60,000 hours for LibriLight, and 100,000 hours for Voxpopuli.3\nModel Configuration: Following previous work in self-supervised speech learning (Baevski et al., 2020; Hsu et al., 2021a; Chen et al., 2022a), we employ two model sizes for pre-training: base and large. As outlined in Section 3, we evaluate a two resolution variant of MR-HuBERT with 40ms\n3We use the same 100,000 hours split as Wang et al. (2021a).\nand the commonly used 20ms. Additional ablation studies concerning resolutions are elaborated in Appendix B.2.\nFor both the base and large models, we adhere to the configurations used in the original HuBERT model (Hsu et al., 2021a). Each encoder (i.e., fq1 (\u00b7), f q 2 (\u00b7), and f q 3 (\u00b7)) as detailed in Section 3.2, has an evenly assigned number of Transformer layers. Specifically, the base model uses a four-layer Transformer for each encoder, whereas the large model deploys an eight-layer Transformer for each encoder. For an in-depth discussion on the effects of layer allocation, please refer to Appendix B.1.\nUnit Preparation: To enhance efficiency of pre-training, we directly extract units from the publicly available HuBERT-base4. We first train a K-means model on 50% of the LibriSpeech training set, with K = 1, 000. Subsequently, the pre-trained K-means model is employed to extract target units from LibriSpeech, LibriLight, and Voxpopuli datasets. For multi-resolution scenarios, we perform subsampling of target units by skipping every second unit. Further experiments on unit extraction variants are available in Appendix B.7.\nPre-trained Models: We pre-train monolingual and multilingual models for both base and large settings. Specifically, mono-base and mono-large are trained on LibriSpeech (960 hours) and LibriLight (60,000 hours) respectively for 400,000 steps. The multi-base model is trained on Voxpopuli (384,000 hours) for 800,000 steps. Additional training details are available in Appendix A.\nBaselines: Our primary comparisons are made with HuBERT models of matching sizes, specifically HuBERT-base and HuBERT-large. As noted in the Unit Preparation part, units are consistently extracted from HuBERT-base. To account for this, we include an additional iteration trained on this base architecture, referred to as HuBERT-base+. Furthermore, recognizing that our K-means model may not be identical to the one used in HuBERT-large, we introduce another setting that uses the same large configuration but with our extracted units; we label this as HuBERT-large*. For multilingual experiments, we include the public multilingual mHuBERT-base, introduced in Lee et al. (2022b) as well as a multilingual HuBERT-base* that is trained with the same training configuration of multi-base.\nTo isolate the effects of individual components in our MR-HuBERT, we perform additional ablation studies detailed in Appendix B. These studies encompass mono-resolution models, models using a single high-resolution pre-training target, models with simplified sampling modules, models with less complex settings, etc."
        },
        {
            "heading": "4.2 SPEECH RECOGNITION",
            "text": "Experimental Settings: We conduct speech recognition experiments using various subsets of the LibriSpeech corpus for training. Specifically, we fine-tune the SSL models as a whole encoder using 1-hour, 10-hour, and 100-hour training subsets. Subsequently, we evaluate each fine-tuned model on four evaluation sets, namely dev-clean, test-clean, dev-other, and test-other. For training configurations, we adhere to the established settings with Connectionist Temporal Classification (CTC) used in wav2vec 2.0 and HuBERT, as outlined in the Fairseq framework (Ott et al., 2019).5 Beyond decoding via beam search directly from the fine-tuned acoustic model, we also incorporate language model shallow fusion for enhanced performance (Karita et al., 2019). To ensure result reproducibility, we employ an open-source four-gram language model pre-trained on LibriSpeech textual data, along with its associated lexicon (Panayotov et al., 2015).6 Our chosen evaluation metric is the Word Error Rate (WER).\nResults: Our findings, illustrated in Table 1, provide compelling evidence of the efficacy of our introduced methods. When subjected to a range of training durations\u2014namely, 1-hour, 10-hour, and 100-hour\u2014the techniques we have implemented consistently surpass the Word Error Rate (WER) results of the four reference baseline models. In the base model variant, the mono-base model we introduce consistently showcases a marked 1%-2% WER improvement across the board, when measured against all four evaluation datasets. For the large model configuration, the results become even more compelling. The mono-large model, in particular, stands out: when trained on the 1-hour\n4 https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt 5 https://github.com/facebookresearch/fairseq 6 https://www.openslr.org/11/\ndataset, it achieves a WER reduction oscillating between 40% and 50%. For the 10-hour training set, the dev-other and test-other evaluation datasets reflect the most pronounced improvements. Shifting to the 100-hour training set, the test-clean and test-other sets emerge as the beneficiaries of the largest boosts in performance. Furthermore, when a joint-decoding strategy with the language model is in place, while the performance differential becomes less pronounced, the proposed MR-HuBERT still maintains a performance edge, always matching or outperforming the baseline HuBERT models. A salient takeaway is that our proposed models consistently rival or outstrip the baseline models, underscoring the robustness and superiority of the methodologies we\u2019ve employed."
        },
        {
            "heading": "4.3 SUPERB EVALUATION",
            "text": "Experimental Settings: Our evaluation within the SUPERB framework aims to provide a holistic assessment of the quality of SSL representations across a broad array of speech processing tasks (Yang et al., 2021; Tsai et al., 2022; Feng et al., 2023). Specifically, we assess our proposed models on tasks including Phone Recognition (PR), Automatic Speech Recognition (ASR), Intent Classification (IC), Keyword Spotting (KS), Slot Filling (SF), Speech Translation (ST), Speech Enhancement (SE), and Speech Separation (SS).7\nTo ensure consistent evaluations, we adopt metrics outlined in Yang et al. (2021): Phone Error Rate (PER) for PR, WER for ASR, Accuracy (ACC) for IC and KS, F-1 measure and Character Error Rate (CER) for SF, BLEU for ST, Short-Time Objective Intelligibility (STOI) and Perceptual Evaluation of Speech Quality (PESQ) for SE, and Scale-Invariant Signal-to-Distortion Ratio improvement (SI-SDRi) for SS.\nWe adhere to the SUPERB policy for downstream model training. In particular, we keep the SSL upstream models fixed and only adjust the learning rate. To address reproducibility, we perform a simple grid search for the learning\n7Besides the SUPERB public benchmark tasks, we also explore Voice Conversion (VC) as outlined in Huang et al. (2022a;b). For more details, see Appendix D.\nrate, considering only the default rate in S3PRL along with its 0.1x and 10x variations. We also use the weighted summation strategy for the frozen SSL representation. To mitigate the resolution differences across layers, we conduct simple repeat upsampling or skip downsampling as outlined in (Shi et al., 2023c).\nTo gauge the performance of SSL representations across tasks, we categorize SUPERB tasks into two main clusters: Understanding and Enhancement (Generation). We calculate the SUPERB score (denoted as SUPERBs), as defined in the SLT 2022 SUPERB challenge (Feng et al., 2023), which employs linear scaling between conventional spectral features and state-of-the-art upstream representations in the corresponding tasks. Comprehensive performance metrics that take into account all evaluated tasks are also calculated. Additional information on the SUPERB evaluation is available in Appendix D.\nResults: The comprehensive results, divided by task category, are presented in Table 2 and Table 3. Our proposed MR-HuBERT demonstrates marked improvements over a variety of understanding and enhancement tasks in both base and large configurations."
        },
        {
            "heading": "4.4 ML-SUPERB EVALUATION",
            "text": "Experimental Settings: We evaluate the performance of our proposed multilingual speech processing method using the ML-SUPERB benchmark (Shi et al., 2023a). This benchmark, which is supported by 143 languages, has been implemented as a recipe within the ESPnet framework (Watanabe et al., 2018)8. The ML-SUPERB benchmark comprises two sets of general benchmarks\u2014specifically, a 10-minute set and a 1-hour set\u2014across four tasks: Monolingual ASR, Multilingual ASR, Language Identification (LID), and a joint task of Multilingual ASR+LID. To maintain the integrity of the experimental comparison, we adhere to the ML-SUPERB guidelines for downstream architectures and training configurations, including the use of frozen SSL representations (Shi et al., 2023a). For the evaluation, we employ the standard metrics: Character Error Rate (CER) or PER for ASR tasks, and ACC for LID tasks. Furthermore, we calculate a composite ML-SUPERB score as defined by Shi et al. (2023a) to provide an overall measure of performance. Additional information on the SUPERB evaluation is available in Appendix E.\nResults: Our evaluations on the ML-SUPERB benchmark are summarized in Table 4. The data reveals that our proposed multilingual model, multi-base, stands out with the topmost per-\n8 https://github.com/espnet/espnet/tree/master/egs2/ml_superb/asr1\nformance. Notably, even our monolingual pre-trained models, mono-base and mono-large, surpass the overall monolingual baselines. Furthermore, they outperform the multilingual model mHuBERT-base and mHuBERT-base* in the overall ML-SUPERB score."
        },
        {
            "heading": "4.5 DISCUSSION: INFERENCE SPEED",
            "text": "In addition to achieving notable gains in performance across various test scenarios, the proposed method also offers advantages in terms of computational efficiency, particularly during the inference stage. This efficiency is primarily attributable to the reduced sequence length required for self-attention computations. To quantitatively evaluate this improvement, we employ Multiply-Add Cumulations (MACs) as our metric of comparison between the baseline models and our proposed method. We utilize the TorchProfile toolkit to calculate MACs9. Specifically, we analyze audio samples of varying lengths\u20142s, 4s, 8s, 16s, and 32s\u2014to calculate the total MACs for each method. The results indicate a clear computational advantage for the proposed method: in the base model configuration, the total MACs were reduced from 431G to 394G, representing an improvement of 9%. In the large model configuration, the MACs decreased from 1116G to 971G, corresponding to a 13% improvement."
        },
        {
            "heading": "5 RELATION TO SIMILAR APPROACHES IN OTHER CONTEXTS",
            "text": "The idea of leveraging multiple resolutions has been explored in various other contexts. In speech understanding, downsampled spoken feature sequences are commonly employed to extract highlevel linguistic or semantic features for efficiency (Chen et al., 2019; Meng et al., 2023; Chen et al., 2023a) or to better integrate pre-trained language models (Gaido et al., 2021; Shi et al., 2023b; Wu et al., 2023; Li et al., 2023c). In speech synthesis, multi-resolution discriminators have been instrumental in recent adversarial-based vocoders (Yamamoto et al., 2020; Kong et al., 2020; Yoneyama et al., 2023). Additionally, multi-resolution or multi-scale networks have shown robust performance in speech enhancement (Zhang & Wang, 2020; Zhang et al., 2022b; Xiang et al., 2021; Xu et al., 2020; Shi et al., 2019). While prior work exists, our paper stands out for its focus on a novel hierarchical architecture for speech pre-training. The resulting models offer not only substantial performance gains across downstream tasks but also computational efficiencies during inference.\nSimilar multi-resolution strategies have also found applications in other domains. In computer vision, multi-scale convolutional networks are employed for various tasks such as object detection and human pose estimation (Yang & Ramanan, 2015; Cai et al., 2016; Ghiasi et al., 2019; Mathieu et al., 2016). Among these, Hourglass networks stand out for their hierarchical multi-resolution processing, which has resulted in significant performance gains (Newell et al., 2016; Melekhov et al., 2017; Yang et al., 2017). This concept has been extended to the text domain as the Hourglass transformer, which has proven effective for sequence processing (Zhai et al., 2023; Guo et al., 2022; Nawrot et al., 2023; 2022). Our work has a similar architecture to the Hourglass transformer in speech pre-training with specific features like masked unit prediction, multi-resolution targets, and other speech-related architectural nuances."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "This paper introduces MR-HuBERT, a self-supervised speech learning model that extends HuBERT by employing multi-resolution masked unit prediction in conjunction with a hierarchical transformer architecture. Comprehensive evaluations across various benchmarks reveal that MR-HuBERT substantially outperforms the original HuBERT model across a broad spectrum of speech processing tasks. These include, but are not limited to, speech recognition, spoken language understanding, multilingual speech recognition, and speech enhancement. Beyond these performance gains, the model also exhibits computational efficiencies, specifically a 9-13% reduction in computational complexity, addressing efficiency concerns.10\n9 https://github.com/zhijian-liu/torchprofile\n10Limitations of the work are discussed in Appendix F, while some future directions are discussed in Appendix G."
        },
        {
            "heading": "7 ETHICS STATEMENT",
            "text": "The development and implementation of MR-HuBERT represent a significant step forward in selfsupervised pre-training for speech models. While this model demonstrates substantial potential and effectiveness across various tasks, it\u2019s crucial to approach its adoption and application ethically:\n\u2022 Openness and Transparency: We remain committed to the principles of open research. By releasing the complete codebase and associated checkpoints of our MR-HuBERT model, we aim to foster an environment of transparency and reproducibility. This initiative encourages peer reviews and allows researchers to independently validate our findings.\n\u2022 Potential Misuse: Like any advanced technology, MR-HuBERT\u2019s capabilities could be misappropriated for malicious purposes. While the model offers enhanced performance across various speech tasks, users must employ it responsibly, respecting individual privacy and avoiding potential misuse in surveillance or unauthorized information extraction. MRHuBERT presents an unforeseen avenue for speech disentanglement, especially in its large configurations, as detailed in Appendix D. As the model evolves, ensuring that it doesn\u2019t unintentionally disentangle or misinterpret cultural nuances, accents, or dialects becomes paramount. This concern is essential for avoiding potential biases or misrepresentations.\nWhile MR-HuBERT represents a promising stride in speech model advancement, its ethical implications are at the forefront of our considerations. We urge the community to employ this technology with caution, respect, and a commitment to the broader good."
        },
        {
            "heading": "8 REPRODUCIBILITY STATEMENT",
            "text": "In the spirit of open research and fostering further advancements in the field, we will be releasing the complete codebase associated with our MR-HuBERT model. This encompasses the entire spectrum of models discussed in our work, including models presented in Appendices. Researchers, academicians, and enthusiasts can access, reproduce, and potentially build upon our findings. We believe that this transparent sharing will not only validate our findings but also inspire innovative research directions anchored around MR-HuBERT. Details regarding access and implementation will be updated after the double-blind review. We eagerly anticipate the community\u2019s engagement and are open to collaborations, feedback, and further enhancements to the model."
        },
        {
            "heading": "A PRE-TRAINING SETTINGS",
            "text": "The pre-training configurations of the models presented in the main content can be found in Table 5. Generally, MR-HuBERT possesses a parameter count analogous to the original HuBERT model. We\u2019ve made concerted efforts to mitigate the impact of incorporating an additional sampling module, which naturally adds more parameters. Specifically, we consistently employ a kernel size of 1 for both convolutional and de-convolutional layers in the sampling module, as elaborated in Section 3.3. Nonetheless, the model experiences a modest increase in parameter size, but this surge is less than 3%. To ensure that the performance boosts highlighted in Section 4 aren\u2019t merely due to this increase, we\u2019ve carried out comprehensive ablation studies, detailed in Appendix B.\nIn line with the insights from Hsu et al. (2021a), a more substantial batch size can typically augment model performance. In our research, when juxtaposing our method against the baselines, we\u2019ve meticulously ensured that the batch size of our approach is either equivalent to or smaller than that of the baseline, to offset potential biases. All model training was executed on V100-32GB GPUs using the Fariseq toolkit (Ott et al., 2019)."
        },
        {
            "heading": "B ABLATION STUDIES",
            "text": "To garner an in-depth understanding of MR-HuBERT, we undertake extensive ablation studies. This ensures each component of MR-HuBERT is optimized and offers insight into their individual contributions to the model\u2019s superior performance. We delved into seven distinct conditions:\n\u2022 Encoder Layer Sizes: We explore the effect of varying the layer sizes for each encoder (Appendix B.1).\n\u2022 Multi-Resolution Analysis: We evaluate the impact of utilizing multiple resolutions (Appendix B.2).\n\u2022 Simpler Upsampling & Downsampling Modules: A study into the implications of adopting a simplified upsampling or downsampling module is presented (Appendix B.3).\n\u2022 Single Prediction Target: Instead of multi-tasking, we scrutinize the outcome of using a singular prediction target (Appendix B.4).\n\u2022 Single Resolution: The performance implications of deploying only one resolution are analyzed (Appendix B.5).\n\u2022 Compact Model: We test the efficacy of the model in a more compact setting (Appendix B.6).\n\u2022 Target Units for Prediction: We investigate the repercussions of utilizing various target units for prediction (Appendix B.7).\nThe above ablations are all conducted in base setting for efficiency, while we also conduct selected large setting experiments in Appendix B.8.\nAs detailed in Section 4.2, we utilize the labeled LibriSpeech subsets of 1-hour, 10-hour, and 100- hour, as described in Kahn et al. (2020), for fine-tuning. The LibriSpeech evaluation sets serve as our testing grounds. All ASR results are presented using the word error rate. Prioritizing the quality of representation, we opt for Viterbi decoding over language model joint decoding. In addition to the ASR performance, we provide information on each model\u2019s parameter size and MACs. The calculation of MACs can be found in Section 4.5."
        },
        {
            "heading": "B.1 ENCODER LAYER SIZES",
            "text": "As discussed in Section 4.1, each encoder of MR-HuBERT maintains a consistent layer size. However, the impact of varied layer sizes for each encoder on the model\u2019s efficacy remains an open question. To address this, we explore the base setting by altering layer counts.\nThe model configurations for this exploration are detailed in Table 6. Across all new configurations, the parameter size remains consistent. Yet, in the (B.1)-b configuration, where low-resolution layers are minimized, the MACs rise to 416G from 394G.\nThe evaluation outcomes are tabulated in Table 7. A key insight drawn from these results is that the (B.1)-b configuration excels in most LibriSpeech evaluation scenarios, especially when working with limited labeled data sets like the 1-hour and 10-hour subsets. This underscores the notion that while low-resolution modeling can effectively learn with fewer layers, the contribution of highresolution comprehension remains pivotal to the overall model\u2019s success."
        },
        {
            "heading": "B.2 MULTI-RESOLUTION ANALYSIS",
            "text": "While the main discussion primarily revolves around MR-HuBERT trained with two resolutions, this section explores its performance using three resolutions. This is to gauge the potential advantages or drawbacks of adopting more than two resolutions. Table 8 showcases that by adding a lower resolution, there\u2019s an increase in the parameter size to 100M, primarily due to the inclusion of extra\nsampling modules. However, MACs decrease further to values of 353G and 331G, contingent on layer distribution. In essence, incorporating more lower resolution components into MR-HuBERT provides the benefit of faster inference.\nTable 9 presents the ASR results for the configurations with three resolutions. Despite showing marked improvement over baselines (i.e., HuBERT-base and HuBERT-base+), the performance of MR-HuBERT with three resolutions isn\u2019t as robust as that of mono-base. This suggests that information from lower resolutions might not always enhance the ASR task. Given the efficiency gains observed, the inclusion of lower resolutions could be perceived as balancing efficiency against performance efficacy. It\u2019s worth noting that the performance dip observed in the three-resolution MR-HuBERT appears inconsistent with findings in (Shi et al., 2023c). The latter study revealed that features fused from multi-resolution HuBERTs across varying resolutions can bolster ASR tasks. Our hypothesis is that this performance discrepancy might stem from each resolution\u2019s constrained model capacity. A deeper dive into this is required to determine if lower resolutions can indeed boost performance."
        },
        {
            "heading": "B.3 SIMPLER UPSAMPLING & DOWNSAMPLING MODULES",
            "text": "As detailed in Section 3.3, our proposed architecture\u2019s sampling module employs a blend of upsampling and downsampling to achieve a flexible ratio between any two resolutions. However, when dealing with low resolutions that are evenly divisible by their corresponding high resolutions, there\u2019s no need to simultaneously deploy both the upsample and downsample modules. This simultaneous use introduces an unnecessary computational overhead. Given this, we delve into a more streamlined setting in this section: the upsampling module is dedicated solely to upsampling, and the downsampling module focuses only on downsampling. While this streamlined approach slightly curtails the computational load (reducing MACs from 394G to 390G) and marginally shrinks the\nparameter size (from 97M to 96M), it lacks the flexibility to handle unconventional ratios, such as 3:4, between resolutions.\nThe derived model, dubbed (B.3)-a, is subsequently fine-tuned for the ASR task, with outcomes presented in Table 10. From the results, it is evident that the MR-HuBERT equipped with the simplified sampling modules outperforms in low-resource situations, specifically the 1-hour and 10- hour ASR training scenarios. However, its performance isn\u2019t as consistent in the more extensive 100-hour experiment, particularly when juxtaposed against mono-base."
        },
        {
            "heading": "B.4 SINGLE PREDICTION TARGET",
            "text": "As delineated in Section 3.4, our model incorporates a summation of masked unit prediction losses derived from all resolutions. In this subsection, we pivot to gauge the efficacy of deploying a singular masked unit prediction, sidelining the amalgamation of intermediate losses. Originating from mono-base, the resultant model, designated as (B.4)-a, benefits from an approximate reduction of 1M in parameter size. This reduction is achieved by discarding prediction heads assigned for the supplemental low-resolution masked unit prediction loss. Concurrently, we assess (B.4)-b, which melds the single prediction feature with the streamlined sampling module, as expounded upon in Appendix B.3.\nBoth models, (B.4)-a and (B.4)-b, have their performance metrics tabulated in Table 10. Overall, a distinct performance hierarchy emerges: (B.3)-a outstrips (B.4)-b, which in turn surpasses (B.4)-a. This sequence underscores the indispensability of the multi-task objective spanning multiple resolutions for MR-HuBERT. Moreover, when navigating models fixated on a solitary prediction target, the elementary sampling modules exhibit more potency compared to their flexible counterparts."
        },
        {
            "heading": "B.5 SINGLE RESOLUTION",
            "text": "A salient feature of MR-HuBERT is its concurrent utilization of diverse resolutions. In this subsection, we distill this multifaceted design down to a singular resolution. The intention behind this simplification is to probe the contributory essence of the multi-resolution concept to the model\u2019s efficacy. We harness the architectural blueprint delineated in Section 3.2, albeit employing a consistent resolution across intermediate components. Consequently, this model forsakes the computational advantages derived from sequence reduction in self-attention calculations, culminating in a heightened computational overhead as reflected in the MACs of 439G. Intriguingly, this computational cost surpasses that of the native HuBERT, clocking in at 431G, as evidenced in Table 11.\nThe experimental results are cataloged in Table 12. Across the 100-hour ASR dataset, the proposed mono-base unambiguously outperforms its singular resolution counterpart, (B.5)-a. However, when venturing into the 1-hour and 10-hour ASR realms, the outcomes are more equivocal. Bearing both efficiency and performance in mind, these findings underscore the pivotal influence of multiresolution strategies in bolstering MR-HuBERT\u2019s impressive performance benchmarks. Please also refer to Appendix D, where we identify more benefits from introducing multiple resolutions."
        },
        {
            "heading": "B.6 COMPACT MODEL",
            "text": "Motivated by the conspicuous performance advantage of MR-HuBERT over traditional HuBERT, we pivot our efforts towards crafting a more svelte version of MR-HuBERT, prioritizing computational economy. Eschewing the convention of a four-layer encoder, our pared-down MR-HuBERT, christened (B.6)-a, adopts a three-layer encoder scheme. This strategic recalibration augments inferential speed without significantly compromising on performance standards. The architectural nuances are delineated in Table 11. It\u2019s worth noting that our investigative purview extends to another optimized model, (B.6)-b, which amalgamates the principles of the single-resolution approach detailed in Section B.5.\nAs revealed in Table 12, the compact iteration understandably possesses diminished modeling prowess, translating to a performance dip relative to mono-base. Yet, even with this inherent constraint, it remains competitive with the original HuBERT \u2014 a noteworthy feat considering the model operates with 20% fewer parameters and realizes a 21% enhancement in inference speed."
        },
        {
            "heading": "B.7 TARGET UNITS FOR PREDICTION",
            "text": "As delineated in Section 4.1, our approach favored skip-downsampling the designated highresolution units to obtain target low-resolution units for the intermediate masked prediction supervision. This strategy emerged as the most efficacious in training MR-HuBERT effectively. Nevertheless, we ventured into exploratory ablations using alternative units. Given that direct skipdownsampling isn\u2019t inherently data-driven, we experimented with units extracted from the pretrained 40ms-resolution HuBERT model, HuBERT-base-40, in alignment with the model architecture introduced by Shi et al. (2023c). Additionally, we leveraged units from the increasingly prevalent Encodec approach as elucidated by (De\u0301fossez et al., 2022). It\u2019s worth noting that our preliminary observations revealed suboptimal performance for most models, leading us to restrict our analysis to just the 10-hour training scenarios. Nonetheless, we present these findings to offer a repository of insights for curious researchers.\nRefer to Table 13 for detailed results. Interestingly, harnessing units from HuBERT-base-40 didn\u2019t elevate performance. This leads us to conjecture that MR-HuBERT may exhibit sensitivity to the homogeneity of prediction targets spanning diverse resolutions. In the case of Encodec, the outcomes were less than stellar, suggesting that a localized acoustic discrete representation might not be synergistic with the semantic learning intricacies inherent in masked unit prediction."
        },
        {
            "heading": "B.8 LARGE SETTINGS",
            "text": "In the context of large settings, MR-HuBERT continues to be examined. Table 14 delineates ten candidate configurations in the large settings. Consistently, all models are trained for 400k steps, analogous to mono-base and mono-large. These configurations not only probe further into the ablation conditions established in the base settings but also explore factors specifically impacting the performance of MR-HuBERT in the large settings. These encompass audio normalization to the raw audio, variations in batch size, and the adoption of different target unit sequences either from HuBERT-base or HuBERT-large11. Owing to memory constraints on V100-32GB, four models, specifically (B.8)-e-(B.8)-h, are trained on 128 A100-80GB GPUs.\nThe results for the ASR experiments in large settings are encapsulated in Table 15. A distilled account of key findings is as follows:\n11Layer 9 and Layer 15 are respectively chosen for HuBERT-base and HuBERT-large for unit discovery. Post this, units are derived from the K-means method, with K = 1000.\nC INFERENCE SPEED\nAlthough MACs offer a theoretical estimate of execution time, they are not always a reliable indicator of actual inference speed, particularly given the parallel processing capabilities of GPUs. To address this, we conduct empirical tests to compare theoretical predictions with real-world performance. We measure the inference speed in terms of \u2019tokens per second\u2019 using Fairseq on the Librispeech dev-clean set. This measurement is the average of ten times to account for variability in real-time execution.\nOur findings, detailed in Table 16, reveal that MR-HuBERT models demonstrate a significant and consistent increase in speed compared to HuBERT models in both base and large settings. Notably, the model (B.2)-c, equipped with three resolutions, emerges as the fastest in terms of inference speed. This empirical evidence suggests a strong alignment between the MACs calculations presented earlier and the actual performance observed in real-world scenarios."
        },
        {
            "heading": "D MORE IN SUPERB BENCHMARK",
            "text": ""
        },
        {
            "heading": "D.1 SUPERB SCORE IN SUPERB BENCHMARK",
            "text": "The SUPERB score (i.e., SUPERBs is a sophisticated metric designed to provide a standardized assessment across various tasks, each potentially with its own scoring system (Feng et al., 2023).\nBy employing linear interpolation between Mel filter banks feature (FBank) scores and state-ofthe-art (SOTA) representation scores, it normalizes scores across different scales. If a single task has multiple metrics, an intra-task average is computed, ensuring that tasks with a myriad of metrics don\u2019t dominate the overall score. Subsequently, an inter-task average is derived, guaranteeing each task\u2019s equal contribution to the final score. A scaling factor of 1000 amplifies readability. For consistency, the score in this paper benchmarks against a static snapshot of the SUPERB leaderboard from August 15, 2023, as detailed in Table 17. Thoughtfully, SUPERB score\u2019s design considers task difficulty, granting more weight to tasks where even small advancements signify significant progress. This approach ensures a balanced evaluation across varying tasks, highlighting the metric\u2019s comprehensive and fair nature.\nLet \u03c8\u03c4,i be the ith metrics for task \u03c4 , \u03c8\u03c4,i(f) be the corresponding score of upstream model f , T be the set of tasks, and I\u03c4 be the set of metrics for task \u03c4 . Then, the detailed formulation is as:\nSUPERBs(f) = 1000 |T | \u03a3T\u03c4 1 |I\u03c4 | \u03a3I\u03c4i \u03c8\u03c4,i(f)\u2212 \u03c8\u03c4,i(FBank) \u03c8\u03c4,i(SOTA)\u2212 \u03c8\u03c4,i(FBank) . (7)\nD.2 VOICE CONVERSION IN SUPERB BENCHMARK\nIn voice conversion, self-supervised learning representations have become increasingly popular as intermediate features for speech generation, as demonstrated by notable works such as (Wang et al., 2022; Huang et al., 2022b;a; 2021; Wu et al., 2022; Choi et al., 2021; Huang et al., 2023). Drawing inspiration from Tsai et al. (2022), we also extended our research to voice conversion tasks to examine the efficacy of our approach.\nTo achieve this, we largely followed the blueprint provided by the S3PRL recipe on the Voice Conversion Challenge 2020 (VCC2020) as detailed by (Yi et al., 2020). In particular, our experiments employed the Taco2-AR model as the primary downstream mechanism, a model introduced by (Liu et al., 2020b). The final waveform synthesis was facilitated by a pre-trained parallel WaveGANbased vocoder, a method pioneered by (Yamamoto et al., 2020).\nFor our evaluation metrics, we leaned on Mean Cepstrum Distortion (MCD), WER for ASR, and ACC for SV, utilizing pre-trained models available within the S3PRL toolkit. Echoing the methodology behind the SUPERB score articulated in Appendix D.1, we derived a comprehensive score by averaging across all evaluation metrics.\nThe outcomes of these experiments are presented in Table 18. As an important side note, rather than directly referencing numbers from Tsai et al. (2022), we opted to rerun the experiments for HuBERT-base and HuBERT-large. This decision stemmed from challenges faced in replicating the original outcomes, potentially due to variations in ASR checkpoints or tweaks in hyperparameter settings. According to the results, we observe marginal improvements in the base setting, but worse performance in the large setting. Our hypothesis is that the data might suffer from overfitting issues with the enhanced modeling power of the large model. We plan to delve deeper into this in subsequent research, with the aim to better harness the capabilities of MR-HuBERT for voice conversion."
        },
        {
            "heading": "D.3 ABLATION MODELS IN SUPERB BENCHMARK",
            "text": "In our aforementioned ablation studies, the evaluation was limited to the ASR performance of each model. This scope might not offer a comprehensive assessment, especially when considering the diverse objectives of different tasks. Hence, we extended our evaluation to encompass most models\nin the SUPERB benchmark, as detailed in Appendix B. The exhaustive results are cataloged in Table 19. Below, we provide concise discussions for each task:\n\u2022 PR, KS, SF, ST, and SS: Across these five tasks, which target understanding and enhancement, respectively, MR-HuBERT consistently outshines HuBERT. There\u2019s a noticeable performance uplift across both base and large settings, corroborated by nearly all configurations in Appendix B.\n\u2022 ASR: In base settings, models tend to surpass the baselines for ASR. However, the performance landscape shifts in the large settings, often not in favor. Multiple factors could be responsible \u2014 perhaps the challenges of applying CTC to low-resolution, repeated features, or constraints from frozen representations. Given these observations as well as the exploration in Appendix B, a more sophisticated fusion strategy might be beneficial when leveraging MR-HuBERT as an upstream, or fine-tuning could be explored for speech recognition tasks.\n\u2022 IC: The base models benefit from low-resolution data, yielding better intent classification accuracy. In contrast, despite one large model setting a benchmark for accuracy, many configurations don\u2019t yield improvements. A plausible cause, discerned from training curves, could be overfitting on a limited dataset. A comprehensive study on larger intent classification datasets, such as SLURP (Bastianelli et al., 2020), might offer clearer insights.\n\u2022 SE: In base settings, MR-HuBERT consistently registers worse PESQ for SE, while the trend inverts in large settings. We theorize that MR-HuBERT initially emphasizes semantic information. But as model size increases, its augmented high-resolution encoders facilitate finer local information processing. When these high-resolution encoders robustly learn local patterns, the model\u2019s generalization capabilities arguably supersede single-resolution counterparts, like the baseline HuBERT. This conjecture is supported by the SS task, where the large MR-HuBERT demonstrates a significant edge over baselines, in contrast to the base setting.\nWhile the preceding discussion predominantly centers on individual tasks, we consolidate categorical SUPERB scores in Table 20. In aggregate terms, the apex model\u2014contrary to the ASR fine-tuning experiments delineated in Appendix B\u2014is (B.8)-d, which leverages labels from HuBERT-large and employs the maximum batch size of (30k * 8 * 128) frames (amounting to approximately 1920 seconds or 0.53 hours) per step."
        },
        {
            "heading": "D.4 LAYER WEIGHTS ANALYSIS OF SUPERB BENCHMARK",
            "text": "As discussed in Appendix D.3, we postulate that MR-HuBERT has implicitly prioritized different types of information across its resolutions. Intriguingly, the weighted summation approach in the SUPERB benchmark offers an insightful perspective into the layer-wise significance of the model for diverse downstream tasks. Prior works have employed these weights to ascertain the contribution of individual layers to specific downstream tasks (Chang et al., 2021; Chen et al., 2022b; Hung et al., 2022; Chen et al., 2022c; Shi et al., 2023a; Lin et al., 2023; Shi et al., 2023c; Otake et al., 2023; Chen et al., 2022a). Given that the weights of each layer participate in the backpropagation process,\nwe surmise these weights can elucidate how each layer contributes to the final prediction in relation to the training objectives of each task.\nObserving distinct behaviors between the models in both base and large configurations, we conduct separate comparisons for these two settings:\nBase setting: The juxtaposition of mono-base with HuBERT-base is illustrated in Figure 3.12 In this comparison, both models manifest analogous behaviors. Broadly, echoing previous findings (Chen et al., 2022a; Chang et al., 2021; Chen et al., 2022b), layer weights are notably taskdependent: understanding tasks predominantly engage the later layers while enhancement tasks favor the initial layers.\nYet, distinct layer weight distributions are palpable:\n\u2022 For the ASR task, while HuBERT predominantly targets its bottom layers (layers 9-11), mono-base allocates over 40% of its attention to low-resolution layers 8 and 9. This inclination is explicable given the rich semantic content of low-resolution layers. This trait elucidates the pronounced contribution of layers 11-12 in MR-HuBERT for the PR\n12To clarify the distinction in layer numbers, MR-HuBERT encompasses not just the Transformer layers but also the outputs of the sampling module. Consequently, a two-resolution MR-HuBERT introduces two extra layers into the weighted summation computation during SUPERB downstream task training. Specifically, for mono-base, the low-resolution layers span from layer 6 to layer 10.\nComparison in the large setting: The behavior of models in the large setting contrasts significantly with that in the base setting. Figure 4 illustrates the comparison for large models.13 We begin by evaluating each model individually before delving into a comparative analysis:\nThree primary patterns emerge for mono-large when applied to SUPERB evaluation (See Figure 4a):\n\u2022 High-resolution encoder emphasis: For tasks like SE and SS, which are associated closely with original audio signals, the first high-resolution encoder predominantly contributes.\n\u2022 Low-resolution encoder emphasis: Understanding tasks such as PR, ASR, SF, and ST predominantly lean on the second low-resolution encoder. Nonetheless, some information from the high-resolution encoders also plays a role, particularly when predictions align sequentially and emphasize semantic content.\n\u2022 Equitable encoder distribution: Tasks like IC and KS exhibit a balanced weight distribution across various encoders. Intriguingly, all these tasks revolve around speech classification.\nFor HuBERT-large, we discern three distinct trends (refer to Figure 4b):\n\u2022 Top Layer Emphasis: Tasks such as SE are heavily reliant on the top layers. \u2022 Bottom Layer Emphasis: Tasks including PR, ASR, SF, ST, and SS predominantly focus\non the bottom layers. \u2022 Diverse Layer Influence: Tasks like IC and KS exhibit varied focus across different layers.\n13Recall from our discussion on the base setting, MR-HuBERT incorporates two additional layers in the final prediction, positioning the low-resolution representations between layer 10 to layer 18.\nAs for comparison, MR-HuBERT showcases a more nuanced understanding of speech signal intricacies, suggesting an implicit speech disentanglement. Conversely, HuBERT displays a rather arbitrary layer weight distribution. For instance, there\u2019s a pronounced emphasis on the final layer output for both understanding and speech separation tasks. The weight distribution patterns of MRHuBERT hint at its potential to seamlessly transition into a more interpretable framework for speech representation studies."
        },
        {
            "heading": "E DELVING DEEPER INTO THE ML-SUPERB BENCHMARK",
            "text": ""
        },
        {
            "heading": "E.1 ML-SUPERB SCORE IN ML-SUPERB BENCHMARK",
            "text": "The ML-SUPERB score is derived as a linear-scaled average score of tasks spanning two specific leaderboards: the 10-minute and 1-hour leaderboards. Its computation is akin to that of the SUPERB score. Here, the scaling boundaries are defined by the FBank and the SOTA models. To ensure uniformity and to provide a holistic view of individual model performance, we reference the same leaderboard from the original ML-SUPERB paper when calculating the ML-SUPERB score (Shi et al., 2023a).\nThe ML-SUPERB benchmark encompasses a diverse spectrum of models, each pre-trained with distinct configurations (Shi et al., 2023a). To render a comprehensive view of how our method stacks up against the competition, we amalgamated our data tables with the original ML-SUPERB leaderboard. The consolidated table, Table 21, offers insights into specific model configurations, highlighting their model parameters, pre-training data size, and linguistic diversity during pre-training. Previous studies on multilingual modeling underscore the advantage of a broader language spectrum (Hou et al., 2020; Watanabe et al., 2017; Zhang et al., 2022a; Chen et al., 2023b; Toshniwal et al., 2018; Li et al., 2019a; Gaur et al., 2021; Lugosch et al., 2022; Shi et al., 2023a). Keeping this in mind, we\u2019ve distinguished models based on their linguistic expanse: monolingual (blue), regional-multilingual (teal), and global-multilingual (yellow).\nTable 22 provides an in-depth overview of performance metrics across benchmark tasks. To sum it up, our MR-HuBERT makes a commendable mark amidst the broader ML-SUPERB landscape. Within the monolingual category, our model conspicuously outpaces competitors\u2014be it wav2vec2based or HuBERT-aligned. Intriguingly, it even surpasses several multilingual counterparts, including the likes of wav2vec2-base-23, wav2vec2-large-23, and XLSR-53. This is particularly noteworthy given that these models benefit from vast datasets and broader linguistic diversity.\nNavigating to the multilingual segment, our MR-HuBERT multi-base showcases a performance nearly on par with the frontrunner, XLSR-128, excelling in the 10-minute benchmark while slightly trailing in the 1-hour category. These outcomes are indeed remarkable, especially when accounting for our model\u2019s leaner parameters, compact pre-training data size, and reduced linguistic breadth. We anticipate MR-HuBERT to be instrumental in sculpting the future of multilingual modeling."
        },
        {
            "heading": "F LIMITATIONS",
            "text": "Dependency on Prior Models: Instead of training from scratch, MR-HuBERT is predominantly trained using additional iterations from HuBERT discrete units. The potential of training MRHuBERT from scratch, without leveraging previously trained models, remains unexplored.\nPerformance Gaps in Specific Tasks: While MR-HuBERT exhibits superior results compared to HuBERT, it lags behind WavLM, especially in enhancement tasks within the SUPERB framework (Chen et al., 2022a). The disparity might stem from differences in the training data and conditions. Notably, WavLM benefits from training on augmented unlabeled datasets that incorporate noise and other speech augmentations. Merging the MR-HuBERT framework with WavLM\u2019s training approach is a promising direction that warrants further investigation.\nApplicability to Non-Speech Audio Tasks: Since MR-HuBERT\u2019s training centers around speech data, its efficacy diminishes for non-speech audio tasks, such as music or generic audio processing (Li et al., 2023b; Turian et al., 2022; Liu et al., 2022; Yuan et al., 2023; Ma et al., 2023). This limitation surfaces when trying to deploy MR-HuBERT in contexts divergent from speech. Delving into a more holistic representation is crucial to achieve peak performance in a broad spectrum of audio tasks."
        },
        {
            "heading": "G POTENTIAL EXTENSIONS",
            "text": "In our examination, MR-HuBERT emerges as a promising alternative to existing speech pre-trained models. The outcomes highlight not only its immediate relevance but also hint at a host of future research directions:\n\u2022 Integration with Other Frameworks: While MR-HuBERT primarily hinges on the HuBERT-style training, its multi-resolution architecture can potentially be fused with a variety of self-supervised frameworks, such as wav2vec2, WavLM, w2v-bert, w2v-bert2, and data2vec (Baevski et al., 2020; Chen et al., 2022a; Chung et al., 2021; Barrault et al., 2023; Baevski et al., 2022).\n\u2022 Diverse Resolutions: Our experimental paradigm predominantly hinged on two-resolution MR-HuBERT, albeit with a cursory glance at a three-resolution approach. Delving deeper into varying resolution combinations might unearth optimal configurations tailored to specific use cases, such as higher resolutions for detailed acoustic analysis or lower resolutions for environmental information.\n\u2022 Richer Representation: HuBERT is renowned for its wide usage for extracting discrete semantic representations, facilitating tasks like resynthesis, voice conversion, and speechto-speech translation (Li et al., 2023a; Huang et al., 2021; Sicherman & Adi, 2023; Polyak et al., 2021; Lee et al., 2022a; Lakhotia et al., 2021; Shi et al., 2021; Lin et al., 2022a; Lian et al., 2022; Nguyen et al., 2023; Shi et al., 2023d; Choi et al., 2023; Inaguma et al., 2023; Barrault et al., 2023; Yan et al., 2023; Huang et al., 2023). As MR-HuBERT melds lowresolution layers for enriched semantics with high-resolution layers for nuanced acoustics, it can offer a more holistic representation. This multi-faceted view could be pivotal in enhancing speech quality in generative tasks.\n\u2022 Speech Disentanglement: Our insights, as dissected in Appendix D, highlight an implicit speech disentanglement capability in the large MR-HuBERT model. Scaling up the model could amplify this characteristic. Furthermore, incorporating adversarial elements can engender explicit disentanglement, proving invaluable for tasks that necessitate isolating semantic or acoustic information from speech signals. We believe the architecture would be even better integrated with existing disentanglement approaches for self-supervised learning (Qian et al., 2022; Chang et al., 2023)."
        }
    ],
    "title": "MULTI-RESOLUTION HUBERT: MULTI-RESOLUTION SPEECH SELF-SUPERVISED LEARNING WITH MASKED UNIT PREDICTION",
    "year": 2023
}