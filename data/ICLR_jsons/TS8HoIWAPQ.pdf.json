{
    "abstractText": "We propose Feature-aligned N-BEATS as a domain-generalized time series forecasting model. It is a nontrivial extension of N-BEATS with doubly residual stacking principle (Oreshkin et al. [45]) into a representation learning framework. In particular, it revolves around marginal feature probability measures induced by the intricate composition of residual and feature extracting operators of N-BEATS in each stack and aligns them stack-wise via an approximate of an optimal transport distance referred to as the Sinkhorn divergence. The training loss consists of an empirical risk minimization from multiple source domains, i.e., forecasting loss, and an alignment loss calculated with the Sinkhorn divergence, which allows the model to learn invariant features stack-wise across multiple source data sequences while retaining N-BEATS\u2019s interpretable design and forecasting power. Comprehensive experimental evaluations with ablation studies are provided and the corresponding results demonstrate the proposed model\u2019s forecasting and generalization capabilities.",
    "authors": [
        {
            "affiliations": [],
            "name": "SINKHORN DI"
        },
        {
            "affiliations": [],
            "name": "Joonhun Lee"
        },
        {
            "affiliations": [],
            "name": "Myeongho Jeon"
        },
        {
            "affiliations": [],
            "name": "Myungjoo Kang"
        },
        {
            "affiliations": [],
            "name": "Kyunghyun Park"
        }
    ],
    "id": "SP:ff0adece509d66e1927cdd4e888aaa174f02eb31",
    "references": [
        {
            "authors": [
                "I. Albuquerque",
                "J. Monteiro",
                "M. Darvishi",
                "T.H. Falk",
                "I. Mitliagkas"
            ],
            "title": "Generalizing to unseen domains via distribution matching",
            "venue": "arXiv preprint arXiv:1911.00804,",
            "year": 2019
        },
        {
            "authors": [
                "L. Ambrosio",
                "N. Gigli",
                "G. Savar\u00e9"
            ],
            "title": "Gradient flows: in metric spaces and in the space of probability measures",
            "venue": "Springer Science & Business Media,",
            "year": 2005
        },
        {
            "authors": [
                "G. Athanasopoulos",
                "R.J. Hyndman",
                "H. Song",
                "D.C. Wu"
            ],
            "title": "The tourism forecasting competition",
            "venue": "International Journal of Forecasting, 27(3):822\u2013844,",
            "year": 2011
        },
        {
            "authors": [
                "K. Bandara",
                "C. Bergmeir",
                "S. Smyl"
            ],
            "title": "Forecasting across time series databases using recurrent neural networks on groups of similar series: A clustering approach",
            "venue": "Expert Systems with Applications, 140:112896,",
            "year": 2020
        },
        {
            "authors": [
                "H. Bao",
                "S. Sakaue"
            ],
            "title": "Sparse regularized optimal transport with deformed q-entropy",
            "venue": "Entropy, 24(11):1634,",
            "year": 2022
        },
        {
            "authors": [
                "S. Ben-David",
                "J. Blitzer",
                "K. Crammer",
                "A. Kulesza",
                "F. Pereira",
                "J.W. Vaughan"
            ],
            "title": "A theory of learning from different domains",
            "venue": "Machine Learning, 79:151\u2013175,",
            "year": 2010
        },
        {
            "authors": [
                "S. Ben-David",
                "J. Blitzer",
                "K. Crammer",
                "F. Pereira"
            ],
            "title": "Analysis of representations for domain adaptation",
            "venue": "Advances in Neural Information Processing Systems, 19,",
            "year": 2006
        },
        {
            "authors": [
                "Y. Bengio",
                "A. Courville",
                "P. Vincent"
            ],
            "title": "Representation learning: A review and new perspectives",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798\u20131828,",
            "year": 2013
        },
        {
            "authors": [
                "W. Cao",
                "D. Wang",
                "J. Li",
                "H. Zhou",
                "L. Li",
                "Y. Li"
            ],
            "title": "Brits: Bidirectional recurrent imputation for time series",
            "venue": "Advances in Neural Information Processing Systems, 31,",
            "year": 2018
        },
        {
            "authors": [
                "C. Challu",
                "K.G. Olivares",
                "B.N. Oreshkin",
                "F.G. Ramirez",
                "M.M. Canseco",
                "A. Dubrawski"
            ],
            "title": "Nhits: Neural hierarchical interpolation for time series forecasting",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 6989\u20136997,",
            "year": 2023
        },
        {
            "authors": [
                "Y. Chen",
                "Y. Kang",
                "Y. Chen",
                "Z. Wang"
            ],
            "title": "Probabilistic forecasting with temporal convolutional neural network",
            "venue": "Neurocomputing, 399:491\u2013501,",
            "year": 2020
        },
        {
            "authors": [
                "L. Chizat",
                "P. Roussillon",
                "F. L\u00e9ger",
                "F.-X. Vialard",
                "G. Peyr\u00e9"
            ],
            "title": "Faster wasserstein distance estimation with the sinkhorn divergence",
            "venue": "Advances in Neural Information Processing Systems, 33:2257\u20132269,",
            "year": 2020
        },
        {
            "authors": [
                "S. Di Marino",
                "A. Gerolin"
            ],
            "title": "Optimal transport losses and sinkhorn algorithm with general convex regularization",
            "venue": "arXiv preprint arXiv:2007.00976,",
            "year": 2020
        },
        {
            "authors": [
                "R.M. Dudley"
            ],
            "title": "The speed of mean glivenko-cantelli convergence",
            "venue": "The Annals of Mathematical Statistics, 40(1):40\u201350,",
            "year": 1969
        },
        {
            "authors": [
                "H. Federer"
            ],
            "title": "Geometric measure theory",
            "venue": "Classics in Mathematics. Springer,",
            "year": 2014
        },
        {
            "authors": [
                "J. Feydy"
            ],
            "title": "Geometric data analysis, beyond convolutions",
            "venue": "Applied Mathematics,",
            "year": 2020
        },
        {
            "authors": [
                "J. Feydy",
                "T. S\u00e9journ\u00e9",
                "F.-X. Vialard",
                "S.-i. Amari",
                "A. Trouv\u00e9",
                "G. Peyr\u00e9"
            ],
            "title": "Interpolating between optimal transport and mmd using sinkhorn divergences",
            "venue": "In The 22nd International Conference on Artificial Intelligence and Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "R. Flamary",
                "N. Courty",
                "A. Gramfort",
                "M.Z. Alaya",
                "A. Boisbunon",
                "S. Chambon",
                "L. Chapel",
                "A. Corenflos",
                "K. Fatras",
                "N. Fournier"
            ],
            "title": "Pot: Python optimal transport",
            "venue": "Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Ganin",
                "V. Lempitsky"
            ],
            "title": "Unsupervised domain adaptation by backpropagation",
            "venue": "International Conference on Machine Learning, pages 1180\u20131189. PMLR,",
            "year": 2015
        },
        {
            "authors": [
                "Y. Ganin",
                "E. Ustinova",
                "H. Ajakan",
                "P. Germain",
                "H. Larochelle",
                "F. Laviolette",
                "M. March",
                "V. Lempitsky"
            ],
            "title": "Domain-adversarial training of neural networks",
            "venue": "Journal of Machine Learning Research, 17(59):1\u201335,",
            "year": 2016
        },
        {
            "authors": [
                "A. Genevay",
                "G. Peyr\u00e9",
                "M. Cuturi"
            ],
            "title": "Learning generative models with sinkhorn divergences",
            "venue": "International Conference on Artificial Intelligence and Statistics, pages 1608\u20131617. PMLR,",
            "year": 2018
        },
        {
            "authors": [
                "M. Ghifary",
                "D. Balduzzi",
                "W.B. Kleijn",
                "M. Zhang"
            ],
            "title": "Scatter component analysis: A unified framework for domain adaptation and domain generalization",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(7):1414\u20131430,",
            "year": 2016
        },
        {
            "authors": [
                "A. Gretton",
                "K.M. Borgwardt",
                "M.J. Rasch",
                "B. Sch\u00f6lkopf",
                "A. Smola"
            ],
            "title": "A kernel two-sample test",
            "venue": "Journal of Machine Learning Research, 13(1):723\u2013773,",
            "year": 2012
        },
        {
            "authors": [
                "H. Hewamalage",
                "C. Bergmeir",
                "K. Bandara"
            ],
            "title": "Recurrent neural networks for time series forecasting: Current status and future directions",
            "venue": "International Journal of Forecasting, 37(1):388\u2013 427,",
            "year": 2021
        },
        {
            "authors": [
                "H. Hu",
                "M. Tang",
                "C. Bai"
            ],
            "title": "Datsing: Data augmented time series forecasting with adversarial domain adaptation",
            "venue": "Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pages 2061\u20132064,",
            "year": 2020
        },
        {
            "authors": [
                "X. Jin",
                "Y. Park",
                "D. Maddix",
                "H. Wang",
                "Y. Wang"
            ],
            "title": "Domain adaptation for time series forecasting via attention sharing",
            "venue": "International Conference on Machine Learning, pages 10280\u2013 10297. PMLR,",
            "year": 2022
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "T.-N. Le",
                "A. Habrard",
                "M. Sebban"
            ],
            "title": "Deep multi-wasserstein unsupervised domain adaptation",
            "venue": "Pattern Recognition Letters, 125:249\u2013255,",
            "year": 2019
        },
        {
            "authors": [
                "C.-Y. Lee",
                "T. Batra",
                "M.H. Baig",
                "D. Ulbricht"
            ],
            "title": "Sliced wasserstein discrepancy for unsupervised domain adaptation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10285\u201310295,",
            "year": 2019
        },
        {
            "authors": [
                "H. Li",
                "S.J. Pan",
                "S. Wang",
                "A.C. Kot"
            ],
            "title": "Domain generalization with adversarial feature learning",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5400\u20135409,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Li",
                "D.E. Carlson"
            ],
            "title": "Extracting relationships by multi-domain matching",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Li",
                "X. Tian",
                "M. Gong",
                "Y. Liu",
                "T. Liu",
                "K. Zhang",
                "D. Tao"
            ],
            "title": "Deep domain generalization via conditional invariant adversarial networks",
            "venue": "Proceedings of the European Conference on Computer Vision, pages 624\u2013639,",
            "year": 2018
        },
        {
            "authors": [
                "B. Lim",
                "S.\u00d6. Ar\u0131k",
                "N. Loeff",
                "T. Pfister"
            ],
            "title": "Temporal fusion transformers for interpretable multi-horizon time series forecasting",
            "venue": "International Journal of Forecasting, 37(4):1748\u20131764,",
            "year": 2021
        },
        {
            "authors": [
                "M. Liu",
                "A. Zeng",
                "M. Chen",
                "Z. Xu",
                "Q. Lai",
                "L. Ma",
                "Q. Xu"
            ],
            "title": "Scinet: Time series modeling and forecasting with sample convolution and interaction",
            "venue": "Advances in Neural Information Processing Systems, 35:5816\u20135828,",
            "year": 2022
        },
        {
            "authors": [
                "K. Madhusudhanan",
                "J. Burchert",
                "N. Duong-Trung",
                "S. Born",
                "L. Schmidt-Thieme"
            ],
            "title": "U-net inspired transformer architecture for far horizon time series forecasting",
            "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 36\u201352. Springer,",
            "year": 2022
        },
        {
            "authors": [
                "S. Makridakis",
                "M. Hibon"
            ],
            "title": "The m3-competition: results, conclusions and implications",
            "venue": "International Journal of Forecasting, 16(4):451\u2013476,",
            "year": 2000
        },
        {
            "authors": [
                "S. Makridakis",
                "E. Spiliotis",
                "V. Assimakopoulos"
            ],
            "title": "The m4 competition: Results, findings, conclusion and way forward",
            "venue": "International Journal of Forecasting, 34(4):802\u2013808,",
            "year": 2018
        },
        {
            "authors": [
                "T. Matsuura",
                "T. Harada"
            ],
            "title": "Domain generalization using a mixture of multiple latent domains",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11749\u2013 11756,",
            "year": 2020
        },
        {
            "authors": [
                "L. McInnes",
                "J. Healy",
                "N. Saul",
                "L. Gro\u00dfberger"
            ],
            "title": "Umap: Uniform manifold approximation and projection",
            "venue": "Journal of Open Source Software, 3(29):861,",
            "year": 2018
        },
        {
            "authors": [
                "T. Miyato",
                "T. Kataoka",
                "M. Koyama",
                "Y. Yoshida"
            ],
            "title": "Spectral normalization for generative adversarial networks",
            "venue": "International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "S. Motiian",
                "M. Piccirilli",
                "D.A. Adjeroh",
                "G. Doretto"
            ],
            "title": "Unified deep supervised domain adaptation and generalization",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 5715\u20135725,",
            "year": 2017
        },
        {
            "authors": [
                "K. Muandet",
                "D. Balduzzi",
                "B. Sch\u00f6lkopf"
            ],
            "title": "Domain generalization via invariant feature representation",
            "venue": "International Conference on Machine Learning, pages 10\u201318. PMLR,",
            "year": 2013
        },
        {
            "authors": [
                "V. Nair",
                "G.E. Hinton"
            ],
            "title": "Rectified linear units improve restricted boltzmann machines",
            "venue": "International Conference on Machine Learning, pages 807\u2013814,",
            "year": 2010
        },
        {
            "authors": [
                "L. Oneto",
                "M. Donini",
                "G. Luise",
                "C. Ciliberto",
                "A. Maurer",
                "M. Pontil"
            ],
            "title": "Exploiting mmd and sinkhorn divergences for fair and transferable representation learning",
            "venue": "Advances in Neural Information Processing Systems, 33:15360\u201315370,",
            "year": 2020
        },
        {
            "authors": [
                "B.N. Oreshkin",
                "D. Carpov",
                "N. Chapados",
                "Y. Bengio"
            ],
            "title": "N-beats: Neural basis expansion analysis for interpretable time series forecasting",
            "venue": "International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "B.N. Oreshkin",
                "D. Carpov",
                "N. Chapados",
                "Y. Bengio"
            ],
            "title": "Meta-learning framework with applications to zero-shot time-series forecasting",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 9242\u20139250,",
            "year": 2021
        },
        {
            "authors": [
                "R. Pascanu",
                "T. Mikolov",
                "Y. Bengio"
            ],
            "title": "On the difficulty of training recurrent neural networks",
            "venue": "International Conference on Machine Learning, pages 1310\u20131318. Pmlr,",
            "year": 2013
        },
        {
            "authors": [
                "G. Peyr\u00e9",
                "M. Cuturi"
            ],
            "title": "Computational optimal transport: With applications to data science",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "J. Quinonero-Candela",
                "M. Sugiyama",
                "A. Schwaighofer",
                "N.D. Lawrence"
            ],
            "title": "Dataset shift in machine learning",
            "venue": "Mit Press,",
            "year": 2008
        },
        {
            "authors": [
                "A. Ramdas",
                "N. Garc\u0131\u0301a Trillos",
                "M. Cuturi"
            ],
            "title": "On wasserstein two-sample testing and related families of nonparametric tests",
            "venue": "Entropy,",
            "year": 2017
        },
        {
            "authors": [
                "S.S. Rangapuram",
                "M.W. Seeger",
                "J. Gasthaus",
                "L. Stella",
                "Y. Wang",
                "T. Januschowski"
            ],
            "title": "Deep state space models for time series forecasting",
            "venue": "Advances in Neural Information Processing Systems, 31,",
            "year": 2018
        },
        {
            "authors": [
                "D. Salinas",
                "V. Flunkert",
                "J. Gasthaus",
                "T. Januschowski"
            ],
            "title": "Deepar: Probabilistic forecasting with autoregressive recurrent networks",
            "venue": "International Journal of Forecasting, 36(3):1181\u2013 1191,",
            "year": 2020
        },
        {
            "authors": [
                "J. Shen",
                "Y. Qu",
                "W. Zhang",
                "Y. Yu"
            ],
            "title": "Wasserstein distance guided representation learning for domain adaptation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32,",
            "year": 2018
        },
        {
            "authors": [
                "C. Shui",
                "Q. Chen",
                "J. Wen",
                "F. Zhou",
                "C. Gagn\u00e9",
                "B. Wang"
            ],
            "title": "A novel domain adaptation theory with jensen\u2013shannon divergence",
            "venue": "Knowledge-Based Systems, 257:109808,",
            "year": 2022
        },
        {
            "authors": [
                "C. Shui",
                "B. Wang",
                "C. Gagn\u00e9"
            ],
            "title": "On the benefits of representation regularization in invariance based domain generalization",
            "venue": "Machine Learning, 111(3):895\u2013915,",
            "year": 2022
        },
        {
            "authors": [
                "V. Vapnik"
            ],
            "title": "Principles of risk minimization for learning theory",
            "venue": "Advances in Neural Information Processing Systems, 4,",
            "year": 1991
        },
        {
            "authors": [
                "A. Virmaux",
                "K. Scaman"
            ],
            "title": "Lipschitz regularity of deep neural networks: analysis and efficient estimation",
            "venue": "Advances in Neural Information Processing Systems, 31,",
            "year": 2018
        },
        {
            "authors": [
                "J. Wang",
                "C. Lan",
                "C. Liu",
                "Y. Ouyang",
                "T. Qin",
                "W. Lu",
                "Y. Chen",
                "W. Zeng",
                "P. Yu"
            ],
            "title": "Generalizing to unseen domains: A survey on domain generalization",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "M. Wang",
                "W. Deng"
            ],
            "title": "Deep visual domain adaptation: A survey",
            "venue": "Neurocomputing, 312:135\u2013 153,",
            "year": 2018
        },
        {
            "authors": [
                "G. Woo",
                "C. Liu",
                "D. Sahoo",
                "A. Kumar",
                "S. Hoi"
            ],
            "title": "Etsformer: Exponential smoothing transformers for time-series forecasting",
            "venue": "arXiv preprint arXiv:2202.01381,",
            "year": 2022
        },
        {
            "authors": [
                "H. Wu",
                "J. Xu",
                "J. Wang",
                "M. Long"
            ],
            "title": "Autoformer: Decomposition transformers with autocorrelation for long-term series forecasting",
            "venue": "Advances in Neural Information Processing Systems, 34:22419\u201322430,",
            "year": 2021
        },
        {
            "authors": [
                "T. Xu",
                "L.K. Wenliang",
                "M. Munn",
                "B. Acciaio"
            ],
            "title": "Cot-gan: Generating sequential data via causal optimal transport",
            "venue": "Advances in Neural Information Processing Systems, 33:8798\u20138809,",
            "year": 2020
        },
        {
            "authors": [
                "A. Zeng",
                "M. Chen",
                "L. Zhang",
                "Q. Xu"
            ],
            "title": "Are transformers effective for time series forecasting",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "H. Zhao",
                "R.T. Des Combes",
                "K. Zhang",
                "G. Gordon"
            ],
            "title": "On learning invariant representations for domain adaptation",
            "venue": "International Conference on Machine Learning, pages 7523\u20137532. PMLR,",
            "year": 2019
        },
        {
            "authors": [
                "S. Zhao",
                "M. Gong",
                "T. Liu",
                "H. Fu",
                "D. Tao"
            ],
            "title": "Domain generalization via entropy regularization",
            "venue": "Advances in Neural Information Processing Systems, 33:16096\u201316107,",
            "year": 2020
        },
        {
            "authors": [
                "F. Zhou",
                "Z. Jiang",
                "C. Shui",
                "B. Wang",
                "B. Chaib-draa"
            ],
            "title": "Domain generalization via optimal transport with metric similarity learning",
            "venue": "Neurocomputing, 456:469\u2013480,",
            "year": 2021
        },
        {
            "authors": [
                "H. Zhou",
                "S. Zhang",
                "J. Peng",
                "S. Zhang",
                "J. Li",
                "H. Xiong",
                "W. Zhang"
            ],
            "title": "Informer: Beyond efficient transformer for long sequence time-series forecasting",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 11106\u201311115,",
            "year": 2021
        },
        {
            "authors": [
                "T. Zhou",
                "Z. Ma",
                "Q. Wen",
                "X. Wang",
                "L. Sun",
                "R. Jin"
            ],
            "title": "Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting",
            "venue": "International Conference on Machine Learning, pages 27268\u201327286. PMLR,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Machine learning models typically presume that the loss minimization from training data results in reasonable performance on a target environment, i.e., empirical risk minimization [56]. However, when using such models in the real world, the target environment is likely to deviate from the training data, which poses a significant challenge for a well-adaptive model to the target environment. This is related to the concept of domain shift [49].\nA substantial body of research has been dedicated to developing frameworks that can accommodate the domain shift issue [6, 7, 20]. In particular, classification tasks have been the predominant focus [30, 32, 58, 59, 66]. As an integral way for modeling sequential data in broad domains such as finance, operation research, climate modeling, and biostatistics, time series forecasting has been a big part of machine learning fields. Nevertheless, the potential domain shift issue for common forecasting tasks has not been considered intensively compared to classification tasks, but only a few articles addressing this can be named [25, 26].\nThe goal of this article is to propose a resolution for the domain shift issue within time series forecasting tasks, namely a domain-generalized time series forecasting model. In particular, the proposed model is built upon a deep learning model which is N-BEATS [45, 46], and a representation learning toolkit which is the feature alignment. N-BEATS revolves around a doubly residual stacking principle and enhances the forecasting capabilities of multilayer perceptron (MLP) architectures without resorting to any traditional machine learning methods. On the other hand, it is well-known that aligning marginal feature measures enables machine learning models to capture invariant features across distinctive domains [8]. Indeed, in the context of classification tasks, many references [30, 38, 42, 65] demonstrated that the feature alignment mitigates the domain shift issue.\nIt is important to highlight that the model is not a straightforward combination of the established components but a nontrivial extension that poses several challenges. First, N-BEATS does not allow the feature alignment in a \u2018one-shot\u2019 unlike the aforementioned references. This is because it is a hierarchical multi-stacking architecture in which each stack consists of several blocks and is connected to each other by residual operations and feature extractions. In response to this, we devise the\n\u2217Equal contribution \u2020Co-corresponding authors\nstack-wise alignment that is a minimization of divergences between marginal feature measures on a stack-wise basis. The stack-wise alignment enables the model to learn feature invariance with an ideal frequency of propagation. Indeed, instead of aligning every block for each stack, single alignment for each stack mitigates gradient vanishing/exploding issue [47] via sparsely propagating loss while preserving the interpretability of N-BEATS and ample semantic coverage [45, Section 3.3].\nSecond, the stack-wise alignment demands an efficient and accurate method for measuring divergence between measures. Indeed, the alignment is inspired by the error analysis of general domain generalization models given in [1, Theorem 1], in which empirical risk minimization loss and pairwise H-divergence loss between marginal feature measures are the trainable components among the total error terms without any target domain information. In particular, since the feature alignment requires the calculation of pairwise divergences for multiple stacks (due to the doubly residual stacking principle), the computational load steeply increases as either the number of source domains or that of stacks increases. On the other hand, from the perspective of accuracy and efficiency, the H-divergence is notoriously challenging to be used in practice [6, 28, 31, 54]. For a suitable toolkit, we adopt the Sinkhorn divergence which is an efficient approximation for the classic optimal transport distances [17, 21, 50]. This choice is motivated by the substantial theoretical evidences of optimal transport distances. Indeed, in the adversarial framework, optimal transport distances have been essential for theoretical evidences and calculation of divergences between pushforward measures induced by a generator and a target measure [21, 53, 62, 66]. In particular, the computational efficiency of the Sinkhorn divergence and fluent theoretical results by [13, 14, 17, 21] are crucial for our choice among other optimal transport distances. Thereby, the training objective is to minimize the empirical risk and the stack-wise Sinkhorn divergences (Section 3.3).\nContributions. To provide an informative procedure of stack-wise feature alignment, we introduce a concrete mathematical formulation of N-BEATS (Section 2), which enables to define the pushforward feature measures induced by the intricate residual operations and the feature extractions for each stack (Section 3.1). From this, we make use of theoretical properties of optimal transport problems to show a representation learning bound for the stack-wise feature alignment with the Sinkhorn divergence (Theorem 3.6), which justifies the feasibility of Feature-aligned N-BEATS. To embrace comprehensive domain generalization scenarios, we use real-world data and evaluate the proposed method under three distinct protocols based on the domain shift degree. We show that the model consistently outperforms other forecasting models. In particular, our method exhibits outstanding generalization capabilities under severe domain shift cases (Table 1). We further conduct ablation studies to support the choice of the Sinkhorn divergence in our model (Table 2).\nRelated literature. For time series forecasting, deep learning architectures including recurrent neural networks [4, 9, 24, 51, 52] and convolutional neural networks [11, 34] have achieved significant progress. Recently, a prominent shift has been observed towards transformer architectures leveraging self-attention mechanisms [33, 35, 60, 61, 67, 68]. Despite their innovations, concerns have been raised regarding the inherent permutation invariance in self-attention, which potentially leads to the loss of temporal information [63]. On the other hand, [10, 45] empirically show that MLP-based architectures would mitigate such a disadvantage and even surpass the transformer-based models.\nRegarding the domain shifts for time series modeling, [25] proposed a technique that selects samples from source domains resembling the target domain, and employs regularization to encourage learning domain invariance. [26] designed a shared attention module paired with a domain discriminator to capture domain invariance. [46] explored domain generalization from a meta-learning perspective without the information on the target domain. Nonetheless, an explicit toolkit and concrete formulation for domain generalization are not considered therein.\nThe remainder of the article is organized as follows. In Section 2, we set the domain generalization problem in the context of time series forecasting, review the doubly residual stacking architecture of N-BEATS, and introduce the error analysis for domain generalization models. Section 3 is devoted to defining the marginal feature measures inspiring the stack-wise alignment, introducing the Sinkhorn divergence together with the corresponding representation learning bound, and presenting the training objective with the corresponding algorithm. In particular, Figure 1 therein illustrates the overall architecture of Feature-aligned N-BEATS. In Section 4, comprehensive experimental evaluations are provided. Section 5 concludes the paper. Other technical descriptions, visualized results, and ablation studies are given in Appendix."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "Notations. Let X := R\u03b1 and Y := R\u03b2 be the input and output spaces, respectively, where \u03b1 and \u03b2 denote the lookback and forecast horizons, respectively. Let Z := R\u03b3 be the latent space with \u03b3 representing the feature dimension. We further denote by Z\u0303 \u2282 Z a subspace of Z . All the aforementioned spaces are equipped with the Euclidean norm \u2225\u00b7\u2225. Define by P := P(X \u00d7 Y) the set of all Borel joint probability measures on X \u00d7 Y . For any P \u2208 P , denotes by PX and PY corresponding marginal probability measures on X and Y , respectively. We further define by P(X ) and P(Z\u0303) the sets of all Borel probability measures on X and Z\u0303 , respectively. Domain generalization in time series forecasting. There are multiple source domains {Dk}Kk=1 with K \u2265 2 and target (unseen) domain DT . Assume that each Dk is equipped with Pk \u2208 P and the same holds for DT with PT \u2208 P and that sequential data for each domain are sampled from corresponding joint distribution. Let l : Y \u00d7 Y \u2192 R+ be a loss function. Then, the objective is to derive a prediction model F : X \u2192 Y such that F(st\u2212\u03b1+1, \u00b7 \u00b7 \u00b7 , st) \u2248 st+1, \u00b7 \u00b7 \u00b7 st+\u03b2 for s = (st\u2212\u03b1+1, \u00b7 \u00b7 \u00b7 , st)\u00d7 (st+1, \u00b7 \u00b7 \u00b7 st+\u03b2) \u223c PT , by leveraging on {Pk}Kk=1, i.e.,\ninf F L(F), with L(F) := 1 K K\u2211 k=1 E(x,y)\u223cPk [ l ( F(x), y )] . (2.1)\nDoubly residual stacking architecture. The main architecture of N-BEATS equipped with the doubly residual stacking principle from [10, 45] is summarized as follows: for M,L \u2208 N, the model comprises M stacks, with each stack consisting of L blocks. The blocks share the same model weight within each respective stack and are recurrently operated based on the double residual stacking principle. More precisely, an m-th stack derives the principle in a way that for xm,1 \u2208 X ,\ny\u0302m := L\u2211 l=1 (\u03bem\u2193 \u25e6 \u03c8m)(xm,l), xm,l := xm,l\u22121 \u2212 (\u03bem\u2191 \u25e6 \u03c8m)(xm,l\u22121), l = 2, . . . , L, (2.2)\nwhere \u03c8m : X \u2192 Z extracts features \u03c8m(xm,l) \u2208 Z from the inputs xm,l \u2208 X for each layer l, and (\u03bem\u2193 , \u03be m \u2191 ) : Z \u2192 Y \u00d7 X generates both forecasts (\u03bem\u2193 \u25e6 \u03c8m)(xm,l) \u2208 Y and backcasts (\u03bem\u2191 \u25e6 \u03c8m)(xm,l) \u2208 X branches. Note that y\u0302m \u2208 Y represents the m-th forecast obtained through the hierarchical aggregation of each block\u2019s forecast, and that the last backcast xm,L \u2208 X , derived by a residual sequence from blocks, serves as an input for the next stack, except for the casem =M .\nOnce the hierarchical aggregation of all stacks and the residual operations are completed, the model F for the doubly residual stacking architecture is given as follows: for (x, y) \u223c PT and x1,1 := x,\ny \u2248 F(x; \u03a8,\u039e\u2193,\u039e\u2191) := M\u2211\nm=1\ny\u0302m, xm,1 := xm\u22121,L, m = 2, . . . ,M, (2.3)\nsubject to y\u0302m and xm\u22121,L given in (2.2), where\n\u03a8 := {\u03c8m}Mm=1, \u039e\u2193 := {\u03bem\u2193 }Mm=1, \u039e\u2191 := {\u03bem\u2191 }Mm=1, (2.4)\nare implemented by fully connected layers. For further details, refer to Appendix A.\nDomain-invariant feature representation. After the investigation on the error analysis for domain adaptation models by [64], an extended version for domain generalization models is provided by [1]. This provides us an insight for developing a domain generalization toolkit within the context of doubly residual stacking models.\nIn the following, we restate Theorem 1 in [1]. To that end, we introduce some notations. Let H be the set of hypothesis functions h : X \u2192 [0, 1] and let H\u0303 := {sgn(|h(\u00b7) \u2212 h\u2032(\u00b7)| \u2212 t) : h, h\u2032 \u2208 H, t \u2208 [0, 1]}. The H-divergence is defined by dH(P\u2032X ,P\u2032\u2032X ) := 2 suph\u2208H |Ex\u223cP\u2032X [1{h(x)=1}] \u2212 Ex\u223cP\u2032\u2032X [1{h(x)=1}]| for any P \u2032 X ,P\u2032\u2032X \u2208 P(X ). The H\u0303-divergence dH\u0303(\u00b7, \u00b7) is defined analogously, with H replaced by H\u0303. Furthermore, denote by Rk(\u00b7) : H \u2192 R and RT (\u00b7) : H \u2192 R the expected risk under the source measures Pk, k = 1, . . . ,K, and the target measure PT , respectively.\nProposition 2.1. Let \u2206K be a (K-1)-dimensional simplex such that each component \u03c0 represents a convex weight. Set \u039b := { \u2211K k=1 \u03c0iPkX |\u03c0 \u2208 \u2206K} and let P\u2217 := \u2211K k=1 \u03c0 \u2217 kPkX \u2208 argminP\u2032X\u2208\u039b dH(P T X ,P\u2032X ). Then, the following holds: for any h \u2208 H,\nRT (h) \u2264 \u03a3Kk=1\u03c0\u2217kRk(h) + dH(PTX ,P\u2217X ) + max i,j\u2208{1,...,K}, i \u0338=j dH\u0303(P i X ,P j X ) + \u03bb(PTX ,P\u2217X ),\nwith \u03bb(PTX ,P\u2217X ) := min{Ex\u223cPTX [| \u2211K k=1 \u03c0 \u2217 kf k(x) \u2212 fT (x)|],Ex\u223cP\u2217X [| \u2211K k=1 \u03c0 \u2217 kf\nk(x) \u2212 fT (x)|]}, where fk, k = 1, . . . ,K, denotes a true labeling function under Pk, i.e., y = fk(x) for (x, y) \u223c Pk, and similarly fT denotes a true labeling function under PT .\nWhile the upper bound of RT (\u00b7) consists of four terms, only the first and third terms (representing the source risks {Rk(\u00b7)}Kk=1 and the pairwise divergences {dH\u0303(P i X ,P j X )}Ki \u0338=j across all marginal feature measures, respectively) are learnable without the target domain information."
        },
        {
            "heading": "3 METHOD",
            "text": ""
        },
        {
            "heading": "3.1 MARGINAL FEATURE MEASURES",
            "text": "Aligning marginal feature measures is a predominant approach in domain-invariant representation learning [20, 55]. In particular, the marginal feature measures {g#PkX }Kk=1 are defined as pushforward measures induced by a given feature map g : X \u2192 Z from {PkX }Kk=1, i.e., g#PkX (E) = PkX \u25e6 g\u22121(E) for any Borel set E in Z . However, defining such measures for doubly residual architectures poses some challenges. Indeed, as discussed in Section 2, N-BEATS includes multiple feature extractors \u03a8 = {\u03c8m}Mm=1 as defined in (2.2), where each extractor \u03c8m takes a sampled input passing through multiple residual operations of previous stacks and the input is recurrently processed within each stack by the residual operations \u039e\u2193 and \u039e\u2191. The scaling factor represents domain-specific characteristics that exhibit noticeable variations. This can lead to an excessive focus on scale adjustments in the aligning process, potentially neglecting crucial features, such as seasonality or trend.\nTo resolve these difficulties, we propose a stack-wise alignment of feature measures on subspace Z\u0303 \u2286 Z . This involves defining measures for each stack through the compositions of feature extractions in \u03a8 = {\u03c8m}Mm=1, backcasting operators in \u039e\u2191 = {\u03bem\u2191 }Mm=1 given in (2.2), and a normalization function. Definition 3.1. Let \u03c3 : Z \u2192 Z\u0303 be a normalizing function satisfying C\u03c3-Lipschitz continuity, i.e., \u2225\u03c3(z) \u2212 \u03c3(z\u2032)\u2225 \u2264 C\u03c3\u2225z \u2212 z\u2032\u2225 for z, z\u2032 \u2208 Z . Given \u03c8m : X \u2192 Z defined in (2.2), the operators rm : X \u2192 X and gm : X \u2192 Z are defined as:\nrm(x) := x\u2212 (\u03bem\u2191 \u25e6 \u03c8m)(x), (3.1)\ngm(x) := (\u03c8m \u25e6 (rm)(L\u22121) \u25e6 (rm\u22121)(L) \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 (r1)(L))(x), (3.2)\nwhere (rm)(L) denotes L-times composition of rm, with (rm)(L\u22121)(x) := x for L \u2212 1 = 0 and gm = (\u03c8m \u25e6 (rm)(L\u22121)) for m = 1. Then the set of marginal feature measures in the m-th stack, m = 1, \u00b7 \u00b7 \u00b7 ,M , is defined by\n{(\u03c3 \u25e6 gm)#PkX }Kk=1,\nwhere each (\u03c3 \u25e6 gm)#PkX is a pushforward of PkX \u2208 {PkX }Kk=1 induced by \u03c3 \u25e6 gm : X \u2192 Z\u0303 . Remark 3.2. The normalization function \u03c3 helps the model to learn invariant features by mitigating the influence of the scale information of each domain. Furthermore, the Lipschitz condition on \u03c3 prevents gradient explosion during model updates. There are two representatives for \u03c3: (i) softmax : Z \u2192 Z\u0303 = (0, 1)\u03b3 where softmax(z)j = ezj/ \u2211\u03b3 i=1 e\nzi , j = 1, ..., \u03b3; (ii) tanh: Z \u2192 Z\u0303 = (\u22121, 1)\u03b3 where tanh(z)j = (e2zj \u2212 1)/(e2zj + 1), j = 1, ..., \u03b3. Both are 1-Lipschitz continuous, i.e., C\u03c3 = 1. In Appendix G (see Table 9), we provide the ablation study under these functions, in addition to the case without the normalization. Remark 3.3. Embedding feature alignment \u2018block-wise\u2019 for every stack results in recurrent operations within each stack and redundant gradient flows. This redundancy can cause exploding or\nvanishing gradients for long-term forecasting [47]. Our stack-wise feature alignment addresses these problems by sparsely propagating the loss. It also maintains ample alignment coverage related to semantics since the stack serves as a semantic extraction unit in [45]. Further heuristic demonstration is provided in Appendix G.1.\nThe operator gm in (3.2) accumulates features up to them-th stack accounting for the previousm\u22121 residual operations. Despite the complex composition of \u03a8 and \u039e\u2191, the fully connected layers in them exhibit Lipschitz continuity [57, Section 6], which ensures the Lipschitz continuity of gm. From this observation and Remark 3.2, we state the lemma below, with its proof in Appendix B: Lemma 3.4. Let C\u03c3 > 0 be given in Definition 3.1. Denote for m = 1, \u00b7 \u00b7 \u00b7 ,M by Cm > 0 and Cm,\u2191 > 0 the Lipschitz constants of \u03c8m and \u03bem\u2191 , respectively. Then (\u03c3 \u25e6 gm) is C\u03c3\u25e6gm -Lipschitz continuous with\nC\u03c3\u25e6gm = C\u03c3Cm(1 + CmCm,\u2191) L\u22121\u03a0m\u22121n=1 (1 + CnCn,\u2191) L, for m = 2, \u00b7 \u00b7 \u00b7 ,M, and C\u03c3\u25e6gm = C\u03c3Cm(1 + CmCm,\u2191)L\u22121 for m = 1.\nBy the doubly residual principle, {gm}Mm=1 are inseparable for \u03a8 and \u039e\u2191. However, the stackwise alignment via regularizing {gm}Mm=1 potentially deteriorates the backcasting power of \u039e\u2191, which could lead to performance degradation of the model. Instead, we conduct the alignment by regularizing exclusively on feature extractors \u03a8. More precisely, this alignment of marginal feature measures from Definition 3.1 is defined as follows: given \u039e\u2191 = {\u03bem\u2191 }Mm=1,\ninf \u03a8\n{ M\u2211\nm=1\nmax i,j\u2208{1,\u00b7\u00b7\u00b7 ,K}, i \u0338=j\nd ( (\u03c3 \u25e6 gm)#PiX , (\u03c3 \u25e6 gm)#P j X )} , (3.3)\nwhere d(\u00b7, \u00b7) : P(Z\u0303) \u00d7 P(Z\u0303) \u2192 R+ is a divergence or distance between given measures. The illustration of the stack-wise alignment is provided in Figure 3 (in Appendix A).\nNote that the third term in Proposition 2.1, i.e., maxi,j\u2208{1,...,K}, i \u0338=j dH\u0303(P i X ,P j X ), and the stack-wise alignment in (3.3) are perfectly matched once d(\u00b7, \u00b7) is specified as the H-divergence. However, the empirical estimation for the H-divergence is notoriously difficult [6, 7, 32, 54]. These concerns become even more pronounced in the proposed method due to the stack-wise alignment necessitating MK(K \u2212 1)/2-times calculation of pairwise divergence, implying heavy computational load. Meanwhile, a substantial body of literature regarding the domain invariant feature learning adopts other alternatives for the H-divergence, and among them [28, 29, 53, 66], optimal transport distances have been dominant due to their in-depth theoretical ground. In line with this, in the following section, we introduce an optimal transport distance as a relevant choice for d(\u00b7, \u00b7)."
        },
        {
            "heading": "3.2 SINKHORN DIVERGENCE ON MEASURES",
            "text": "In the adversarial framework [21, 53, 62, 66], optimal transport distances have been adopted for training generators to make corresponding pushforward measures close to a given target measure. In particular, the Sinkhorn divergence, an approximate of an entropic regularized optimal transport distance, is shown to be an efficient method to address intensive calculations of divergence between empirical measures [13, 17, 21]. As the stack-wise alignment given in (3.3) leverages on a number of calculations of divergences and hence requires an efficient and accurate toolkit for feasible training, we adopt the Sinkhorn divergence as the relevant one for d(\u00b7, \u00b7). To define the Sinkhorn divergence, let us introduce the regularized quadratic Wasserstein-2 distance. To that end, let \u03f5 be the entropic regularization degree and \u03a0(\u00b5, \u03bd; Z\u0303) is the space of all couplings, i.e., transportation plans, the marginals of which are respectively \u00b5, \u03bd \u2208 P(Z\u0303). Then the regularized quadratic Wasserstein-2 distance defined on Z\u0303 is defined as follows: for \u03f5 \u2265 0,\nW\u03f5,Z\u0303(\u00b5, \u03bd) := inf \u03c0\u2208\u03a0(\u00b5,\u03bd;Z\u0303) {\u222b Z\u0303\u00d7Z\u0303 ( \u2225x\u2212 y\u22252 + \u03f5 log ( d\u03c0(x, y) d\u00b5(x)d\u03bd(y) )) d\u03c0(x, y) } . (3.4)\nBy replacing Z\u0303 with X , one can define by W\u03f5,X (\u00b7, \u00b7) the corresponding regularized distance on X . The entropic term attached with \u03f5 in (3.4) is known to improve computational stability of the Wasserstein-2 distance, whereas it causes a bias on corresponding estimator. To alleviate this, according to [12], we adopt the following debiased version of the regularized distance:\nDefinition 3.5. For \u03f5 \u2265 0, the Sinkhorn divergence is\nW\u0302\u03f5,Z\u0303(\u00b5, \u03bd) := W\u03f5,Z\u0303(\u00b5, \u03bd)\u2212 1\n2\n( W\u03f5,Z\u0303(\u03bd, \u03bd) +W\u03f5,Z\u0303(\u00b5, \u00b5) ) , \u00b5, \u03bd \u2208 P(Z\u0303). (3.5)\nUsing the duality of the regularized optimal transport distance from [48, Remark 4.18 in Section 4.4] and the Lipschitz continuity of {\u03c3 \u25e6 gm}Mm=1 from Lemma 3.4, we present the following theorem, substantiating the well-definedness and feasibility of our stack-wise alignment via W\u0302\u03f5,Z\u0303(\u00b7, \u00b7). The proof is provided in Appendix B.\nTheorem 3.6. Let C\u03c3\u25e6gm > 0 be as in Lemma 3.4 and define C := \u2211M\nm=1 max{(C\u03c3\u25e6gm)2, 1}."
        },
        {
            "heading": "Then the following holds: for \u03f5 \u2265 0,",
            "text": "M\u2211\nm=1\nmax i,j\u2208{1,\u00b7\u00b7\u00b7 ,K}, i \u0338=j\nW\u0302\u03f5,Z\u0303 ( (\u03c3 \u25e6 gm)#PiX , (\u03c3 \u25e6 gm)#P j X ) \u2264 C max\ni,j\u2208{1,\u00b7\u00b7\u00b7 ,K}, i \u0338=j W\u03f5,X\n( PiX ,P j X ) .\nIn [44, Lemma 3 & Proposition 6], representation learning bounds under the maximum mean discrepancy and the regularized distance in (3.4) are investigated for a single-layered fully connected network. With similar motivation, Theorem 3.6 represents a learning bound for the stack-wise alignment loss under the Sinkhorn divergence as the entropic regularized distance between source domains\u2019 measures. While the Lipschitz continuity of {\u03c3 \u25e6 gm}Mm=1 allows a nice bound, there exists room for having a tighter bound by deriving the smallest Lipschitz constant [57] and applying the spectral normalization [40], which will be left for the future extension. Further discussions on the choice of the Sinkhorn divergence and on Theorem 3.6 are provided in Appendix C."
        },
        {
            "heading": "3.3 TRAINING OBJECTIVE AND ALGORITHM",
            "text": "From Sections 3.1 and 3.2, we define the training objective and corresponding algorithm. To that end, denote by \u03a6 := {\u03d5m}Mm=1, \u0398\u2193 := {\u03b8m,\u2193}Mm=1, and \u0398\u2191 := {\u03b8m,\u2191}Mm=1 the parameters sets of the fully connected neural networks in the residual operators in \u03a8, \u039e\u2193, and \u039e\u2191 given in (2.4). Then corresponding parameterized forms of the operators are given by\n\u03a8(\u03a6) = {\u03c8m(\u00b7;\u03d5m)}Mm=1, \u039e\u2193(\u0398\u2193) = {\u03bem\u2193 (\u00b7; \u03b8m,\u2193)}Mm=1, \u039e\u2191(\u0398\u2191) = {\u03bem\u2191 (\u00b7; \u03b8m,\u2191)}Mm=1.\nThen denote by gm\u03a6,\u0398\u2191 := g m(\u00b7; {\u03d5n}mn=1, {\u03b8n,\u2191}mn=1), m = 1, . . . ,M , the parameterized version of gm given in (3.2). Let L(F(\u00b7, \u00b7, \u00b7)) be the parameterized form of the forecasting loss given in (2.1) and Lalign(\u00b7, \u00b7) be that of the alignment loss given in (3.3) under the Sinkhorn divergence, i.e.,\nLalign(\u03a6,\u0398\u2191) := M\u2211\nm=1\nmax i,j\u2208{1,...,K}, i \u0338=j\nW\u0302\u03f5,Z\u0303 ( (\u03c3 \u25e6 gm\u03a6,\u0398\u2191)#P i X , (\u03c3 \u25e6 gm\u03a6,\u0398\u2191)#P j X ) . (3.6)\nWe then provide the following training objective\nL\u03bb(\u03a6,\u0398\u2193,\u0398\u2191) := L(F(\u03a6,\u0398\u2193,\u0398\u2191)) + \u03bbLalign(\u03a6,\u0398\u2191). (3.7)\nTo update (\u03a6,\u0398\u2193,\u0398\u2191) according to (3.7), we calculate m-th stack divergence W\u0302\u03f5,Z\u0303((\u03c3\u25e6g m \u03a6,\u0398\u2191 )#PiX , (\u03c3\u25e6gm\u03a6,\u0398\u2191)#P j X ) as its empirical counterpart W\u0302\u03f5,Z\u0303(\u00b5 m,(i) \u03a6,\u0398\u2191 , \u00b5 m,(j) \u03a6,\u0398\u2191 ), where the corresponding empirical measures {\u00b5m,(k)\u03a6,\u0398\u2191 } K k=1 are given as follow: for k = 1, \u00b7 \u00b7 \u00b7 ,K,\n\u00b5 m,(k) \u03a6,\u0398\u2191\n:= 1\nB B\u2211 b=1 \u03b4 z\u0303 (k) b , with z\u0303(k)b := \u03c3 \u25e6 g m \u03a6,\u0398\u2191 (x (k) b ),\nwhere {(x(k)b , y (k) b )}Bb=1 are sampled from Dk, and B and \u03b4z denote a mini-batch size and the Dirac measure centered on z \u2208 Z\u0303 , respectively.\nAs mentioned in Section 3.1, the alignment loss Lalign(\u03a6,\u0398\u2191) is minimized by updating \u03a6 for given \u0398\u2191, while {gm\u03a6,\u0398\u2191} m m=1 are inseparable for \u03a6 and \u0398\u2191. At the same time, the forecasting loss L(F(\u03a6,\u0398\u2193,\u0398\u2191)) is minimized by updating (\u03a6,\u0398\u2193,\u0398\u2191). To bring them together, we adopt the following alternatively updating optimization inspired from [19, Section 3.1]:\n\u0398\u2217\u2193,\u0398 \u2217 \u2191 := argmin\n\u0398\u2193,\u0398\u2191\nL(F(\u03a6\u2217,\u0398\u2193,\u0398\u2191)), \u03a6\u2217 := argmin \u03a6 L\u03bb(\u03a6,\u0398 \u2217 \u2193,\u0398 \u2217 \u2191). (3.8)\nThe training procedure on (3.8) is summarized in Algorithm 1 and the overall model architecture is illustrated in Figure 1, where we highlight the stack-wise alignment process (with \u2018red\u2019 color) not appearing in the original N-BEATS (see Figure 1 in [45]).\nAlgorithm 1: Training Feature-aligned N-BEATS. Requires: \u03b7 (learning rate), B (mini-batch size); Initialize \u03a6, \u0398\u2193, \u0398\u2191;\n1 while not converged do 2 Sample {(x(k)b , y (k) b )} B b=1 from Dk & Initialize {y\u0302 (k) b } B b=1 \u2190 0, k = 1, . . . ,K; 3 for m = 1 to M do 4 for k = 1 to K do 5 Compute {gm\u03a6,\u0398\u2191(x (k) b )} B b=1; Update y\u0302 (k) b \u2190 y\u0302 (k) b + \u03be m \u2193 (g m \u03a6,\u0398\u2191(x (k) b ); \u03b8m,\u2193), b = 1, . . . , B; 6 end 7 end 8 Compute {\u00b5m,(k)\u03a6,\u0398\u2191 } M m=1, k = 1, . . . ,K; Update \u03a6 such that for m = 1, . . . ,M ,\n9 \u03d5m \u2190 \u03d5m + \u03b7\u2207\u03d5m ( \u03bb M\u2211 n=1 max i,j\u2208{1,\u00b7\u00b7\u00b7 ,K}, i\u0338=j W\u0302\u03f5,Z\u0303 ( \u00b5 n,(i) \u03a6,\u0398\u2191 , \u00b5 n,(j) \u03a6,\u0398\u2191 )) ,;\n10 Update (\u03a6,\u0398\u2193,\u0398\u2191) such that for m = 1, . . . ,M , 11 (\u03d5m, \u03b8m,\u2193, \u03b8m,\u2191)\u2190 (\u03d5m, \u03b8m,\u2193, \u03b8m,\u2191) + \u03b7 1K\u00b7B K\u2211\nk=1 B\u2211 b=1 \u2207(\u03d5m,\u03b8m,\u2193,\u03b8m,\u2191)l ( y\u0302 (k) b , y (k) b ) ;\n12 end"
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "Evaluation details. Our evaluation protocol lies on two principles: (i) real-world scenarios and (ii) examination of various domain shift environments between the source and target domains. For (i), we use financial data from the Federal Reserve Economic Data (FRED)1 and weather data from the National Centers for Environmental Information (NCEI)2. For (ii), let us define a set of semantically similar domains as superdomain denoted by Ai, e.g., i = FRED, NCEI. We then categorize the domain shift scenarios into out-domain generalization (ODG), cross-domain generalization (CDG), and in-domain generalization (IDG) such that\n\u00b7 ODG: {Dk}Kk=1 \u2286 Ai Shift (i \u0338=j)\u2212\u2212\u2212\u2212\u2212\u2212\u2192DT \u2208 Aj ;\n\u00b7 CDG: {Dk}p\u22121k=1 \u2286 Ai, {D k}Kk=p \u2286 Aj (2 \u2264 p \u2264 K) Shift (i\u0338=j)\u2212\u2212\u2212\u2212\u2212\u2212\u2192DT \u2208 Ai s.t. {Dk}p\u22121k=1 \u2229 D T = \u2205;\n\u00b7 IDG: {Dk}Kk=1 \u2286 Ai Shift (i=j)\u2212\u2212\u2212\u2212\u2212\u2212\u2192DT \u2208 Ai s.t. {Dk}Kk=1 \u2229 DT = \u2205.\nThe domain shift from source to target becomes increasingly pronounced in the sequence of IDG, CDG, and ODG, making it even more challenging to generalize. For detailed data configuration and domain specifications, refer to Appendix D.\nBenchmarks. We compare our proposed approach with deep learning-based models, including transformer (e.g., Informer [67], Autoformer [61]), MLP-based models (e.g., LTSF-Linear models [63] with NLinear and Dlinear) and N-BEATS based models (e.g., N-BEATS [45] and N-HiTS [10]). Note that since the aforementioned time series models addressing domain shift [25, 26] still requires target domain data (due to their \u2018domain-adapted\u2019 framework), we do not consider their models into our domain-generalized protocol.\nExperimental details. We adopt the symmetric mean absolute percentage error (sMAPE) for L(F(\u00b7, \u00b7, \u00b7)) given in (3.7) and use the softmax function for \u03c3 given in Definition 3.1. The Sinkhorn 1 https://fred.stlouisfed.org 2 https://ncei.noaa.gov"
        },
        {
            "heading": "ODG",
            "text": ""
        },
        {
            "heading": "CDG",
            "text": ""
        },
        {
            "heading": "IDG",
            "text": "divergence implemented by GeomLoss from [16] is utilized, and \u03f5 is set to be 0.0025. The Adam optimizer [27] is employed for implementing the optimization given in (3.8). The lookback horizon, forecast horizon, and the number of source domains are set to be \u03b1 = 50, \u03b2 = 10, and K = 3, respectively (noting that it depends on the characteristics of source domains\u2019 datasets). Furthermore, the number of stacks and blocks, and the dimension of feature space are set to be M = 3, L = 4, and \u03b3 = 512, respectively (noting that it is consistent with N-BEATS [45]). Others are determined through grid search, and the sMAPE and MASE are adopted as evaluation metrics. Additional implementation details and definitions are provided in Appendix D.\nDomain generalization performance. As shown in Table 1, the proposed stack-wise feature alignment significantly improves the domain shift issue within the deep residual stacking architectures with outstanding performance compared to other benchmarks. In particular, we highlight that the improvement is more significant in ODG where the domain shift from source to target is severely pronounced. That being said, the proposed domain-generalized model can perform and adapt well in a very severe situation without any information on the target environment. Other detailed analysis on the results are discussed in Appendix E.\nAblation study on divergences. Table 2 provides the ablation results on the choice of divergence (or distances) for the proposed stack-wise feature alignment, in which the benchmarks consist of the classic (not regularized) Wasserstein-2 distance (WD), the maximum mean discrepancy (MMD), and the Kullback\u2013Leibler divergence (KL) and further sensitivity analysis on the Sinkhorn divergence (SD) with respect to \u03f5 > 0 is also provided. Due to the heavy running cost for implement-\ning WD cases (see Runtime with 314.30 in Table 2) and the training instability associated with KL cases (see Table 6), we consider the target domain case for \u2018exchange rate\u2019 (within FRED) and the several source domain scenarios for ODG, CDG and IDG (see Appendix D for the details on the source domains\u2019 combinations). For the same reasons, the baseline model is fixed to N-BEATS-G. The entire results are provided in Tables 6 and 7.\nAs the Sinkhorn divergence is an accurate approximate of the Wasserstein-2 distance (see Definition 3.5), the similar results for the two cases in Table 2 seem to be reasonable. On the other hand, their computational costs are incomparable. That being said, the Sinkhorn divergence is the computationally feasible and accurate toolkit for the proposed stack-wise alignment with optimal transport based divergence, while some instability issue (see [5, 21]) would come out for extremely small \u03f5 > 0 (i.e., \u03f5 =1e-5 in Table 2). In comparison with the MMD and KL cases (see Tables 6 and 7 as well for the\nentire results), the Sinkhorn divergence case seems to be marginally better but shows more stable and consistent results in overall domain shift scenarios. From these empirical evidences, we hence conclude that the choice of the Sinkhorn divergence allows the model to bring both the abundant theoretical advantages of optimal transport problems and the practical feasibility.\nVisualization on representation learning. To visualize representations, i.e., samples of marginal feature measures observed from N-BEATS-G with and without alignment, we use the uniform manifold approximation and projection (UMAP) introduced by [39]. To minimize the effect of unaligned scale information, the softmax function is employed to remove the scale information and instead emphasize the semantic relationship across domains. As illustrated in Figure 2, we observe the proximity between instances and the substantial upsurge in the entropy of domains. For other cases on N-BEATS-I and N-HiTS, please refer to Figure 8 in Appendix F.\nOther results. On top of the aforementioned results, further experiments are provided in Appendix G, which supports our choices and assumptions on the proposed model. The followings summarize the corresponding results: Comparison of stack-wise and block-wise feature alignment (Appendix G.1); Comparison of several normalization functions (Appendix G.2); Evaluation of the model under marginal (or the absence of) domain shift (Appendix G.3); Evaluation on Tourism [3], M3 [36], and M4 [37] datasets (Appendix G.4). On top of that, we report the train and validation losses in Figure 5 supporting the stable optimization procedure. Furthermore, we provide the visual samples of forecasting results in Figure 6 and make use of the interpretability of the feature-aligned N-BEATS to present Figure 7 (see Appendix F)."
        },
        {
            "heading": "5 DISCUSSION AND EXTENSIONS",
            "text": "There are some unresolved theoretical parts in the current article such as a convergence analysis for the training loss (given in (3.8)) with the empirical risk minimization and the stack-wise feature alignment, filling the gap between the Sinkhorn divergence and the H-divergence adopted in the error analysis of domain generalization models (given in Proposition 2.1), and the instability issue coming from the small entropic parameter \u03f5 > 0 in the Sinkhorn divergence (see Table 2).\nOn the other hand, there are many rooms for an extension of the proposed domain-generalized time series forecasting model such as the \u2018conditional\u2019 feature measure alignment in [65] and \u2018adversarial representation learning framework\u2019 in [30]. Moreover, considering the utilization of \u2018moments\u2019 as distribution measurements in [22] and mitigating distribution mismatches through the \u2018contrastive loss\u2019 in [41] would represent meaningful avenues for future research.\nAcknowledgement. K. Park gratefully acknowledges support of the Presidential Postdoctoral Fellowship of Nanyang Technological University. M. Kang was supported by the NRF grant [2021R1A2C3010887] and the MSIT/IITP [No. 1711117093; 2021-0-00077; 2021-0-01343, Artificial Intelligence Graduate School Program of SNU]."
        },
        {
            "heading": "APPENDIX",
            "text": ""
        },
        {
            "heading": "A FURTHER DETAILS ON FEATURE-ALIGNED N-BEATS",
            "text": "The m-th residual operators (\u03c8m, \u03bem\u2193 , \u03be m \u2191 ) \u2208 \u03a8\u00d7 \u039e\u2193 \u00d7 \u039e\u2191, m = 1, . . . ,M , are given by \u03c8m(x) := ( FCm,N \u25e6FCm,N\u22121 \u00b7 \u00b7 \u00b7 \u25e6 FCm,1 ) (x), x \u2208 X = R\u03b1,\n\u03bem\u2193 (z) := Vm,\u2193Wm,\u2193z, z \u2208 Z = R\u03b3 , \u03bem\u2191 (z) := Vm,\u2191Wm,\u2191z, z \u2208 Z = R\u03b3 .\n(A.1)\nThese operators correspond to the m-th stack and involve fully connected layers denoted by FCm,n with RELU activation function [43]. Specifically, FCm,n(x) is defined as RELU(Wm,nx+ bm,n), where Wm,n and bm,n are trainable weight and bias parameters, respectively. The matrix Wm,\u2193 \u2208 R\u03b3\u2193\u00d7\u03b3 (resp. Wm,\u2191 \u2208 R\u03b3\u2191\u00d7\u03b3) represents a trainable linear projection layer for forecasting (resp. backcasting) operations. For the parameters \u03b2, \u03b1, and \u03b3 denoting to the forecast horizon, lookback horizon, and latent space dimension, respectively, Vm,\u2193 \u2208 R\u03b2\u00d7\u03b3\u2193 (resp. Vm,\u2191 \u2208 R\u03b1\u00d7\u03b3\u2191 ) represents a forecast basis (resp. backcast basis) matrix, given by\nVm,\u2193 := (v 1 m,\u2193, . . . ,v \u03b3\u2193 m,\u2193) \u2208 R \u03b2\u00d7\u03b3\u2193 with v1m,\u2193, . . . ,v \u03b3\u2193 m,\u2193 \u2208 R \u03b2 , (resp. Vm,\u2191 := (v1m,\u2191, . . . ,v \u03b3\u2191 m,\u2191) \u2208 R \u03b1\u00d7\u03b3\u2191 with (v1m,\u2191, . . . ,v \u03b3\u2191 m,\u2191) \u2208 R \u03b1), (A.2)\nand each vim,\u2193 (resp. v i m,\u2191), is a forecast basis (resp. backcast basis) vector.\nNote that Vm,\u2193 and Vm,\u2191 are set to be non-trainable parameter sets that embrace vital information for time series forecasting purposes, including trends and seasonality. These parameter sets are based on [45]. The basis expansion representations in (A.1) with flexible adjustments in (A.2) allow the model to capture the relevant patterns in the sequential data.\nN-BEATS-G & N-BEATS-I. The main difference between N-BEATS-G and N-BEATS-I lies on the utilization of Vm,\u2193 and Vm,\u2191. More precisely, N-BEATS-G does not incorporate any specialized time series-specific knowledge but employs the identity matrices for Vm,\u2193 and Vm,\u2191. In contrast, N-BEATS-I captures trend and seasonality information, which derives the interpretability. Specifically, Vm,\u2193 is given by Vm,\u2193 = (1, t, \u00b7 \u00b7 \u00b7 , t\u03b3\u2193), where t = 1\u03b2 (0, 1, 2, \u00b7 \u00b7 \u00b7 , \u03b2 \u2212 2, \u03b2 \u2212 1)\u22a4. This choice is motivated by the characteristic of trends, which are typically represented by monotonic or slowly varying functions. For the seasonality, Vm,\u2193 is defined using a periodic function, (specifically the Fourier series), so that Vm,\u2193 = (1, cos(2\u03c0t), \u00b7 \u00b7 \u00b7 , cos(2\u03c0\u230a\u03b2/2 \u2212 1\u230bt)), sin(2\u03c0t), \u00b7 \u00b7 \u00b7 , sin(2\u03c0\u230a\u03b2/2 \u2212 1\u230bt)))\u22a4. The dimension of Vm,\u2193 is determined by adjusting the interval between cos(2\u03c0t) and cos(2\u03c0\u230a\u03b2/2\u2212 1\u230bt), as well as sin(2\u03c0t) and sin(2\u03c0\u230a\u03b2/2\u2212 1\u230bt). This formulation incorporates the notion of sinusoidal waveforms to capture the periodic nature of seasonality. The formulation of Vm,\u2191 is identical to that of Vm,\u2193, with the only difference being the replacement of \u03b1 with \u03b2 and \u03b3\u2193 with \u03b3\u2191.\nLipschitz continuity of residual operators. Since each \u03c8m defined in (A.1) is an N -layered fully connected network with the 1-Lipschitz continuous activation, i.e., RELU, we can apply [57, Section 6] to have an explicit representation for the (Rademacher) Lipschitz constant Cm > 0 of \u03c8m [15, Theorem 3.1.6]. Furthermore, the forecasting and backcasting operators, \u03bem\u2193 and \u03bem\u2191 , are matrix operators, and we can calculate their Lipschitz constants Cm,\u2193 and Cm,\u2191 by using the matrix norm induced by the Euclidean norm \u2225\u00b7\u2225, i.e., Cm,\u2193 := \u2225Vm,\u2193Wm,\u2193\u2225 > 0 and Cm,\u2191 := \u2225Vm,\u2191Wm,\u2191\u2225 > 0. Detailed illustration of stack-wise feature alignment. In addition to the above-presented NBEATS, we incorporate the concept of learning invariance for domain generalization, which is referred to as Feature-aligned N-BEATS. We provide the illustration of Feature-aligned N-BEATS in Figure 3 which is a detailed version of Figure 1."
        },
        {
            "heading": "B PROOFS OF LEMMA 3.4 AND THEOREM 3.6",
            "text": "Proof of Lemma 3.4. From the definition of \u03c3 \u25e6 gm in Definition 3.1 and the Lipschitz continuity of \u03c3 and \u03c8m with corresponding constants C\u03c3 > 0 and Cm > 0, it follows for every m = 1, . . . ,M ,\nthat for any x, y \u2208 X , \u2225\u03c3 \u25e6 gm(x)\u2212 \u03c3 \u25e6 gm(y)\u2225 \u2264 C\u03c3\u2225gm(x)\u2212 gm(y)\u2225\n\u2264 C\u03c3Cm \u2225\u2225\u2225((rm)(L\u22121) \u25e6 (rm\u22121)(L) \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 (r1)(L))(x)\n\u2212 ( (rm)(L\u22121) \u25e6 (rm\u22121)(L) \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 (r1)(L) ) (y) \u2225\u2225\u2225.\n(B.1)\nBased on the residual operation in (3.1), i.e., rm(x) = x \u2212 (\u03bem\u2191 \u25e6 \u03c8m)(x), and considering the Lipschitz continuity of \u03c3 and \u03c8m with respective constants C\u03c3 > 0 and Cm > 0, we can establish the following inequality for every m = 1, . . . ,M , and any x, y \u2208 X , \u2225rm(x)\u2212 rm(y)\u2225 \u2264 \u2225x\u2212 y\u2225+ \u2225(\u03bem\u2191 \u25e6 \u03c8m)(x)\u2212 (\u03bem\u2191 \u25e6 \u03c8m)(y)\u2225 \u2264 (1 + Cm,\u2191Cm)\u2225x\u2212 y\u2225.\nApplying this to L\u2212 1-times composition of rm, i.e., (rm)(L\u22121), we have that for any x, y \u2208 X ,\n\u2225(rm)(L\u22121)(x)\u2212 (rm)(L\u22121)(y)\u2225 \u2264 (1 + Cm,\u2191Cm)L\u22121\u2225x\u2212 y\u2225.\nUsing the same arguments for the remaining compositions (rm\u22121)(L), (rm\u22122)(L), . . . , (r1)(L) in (B.1), we deduce that for any x, y \u2208 X ,\n\u2225\u03c3 \u25e6 gm(x)\u2212 \u03c3 \u25e6 gm(y)\u2225 \u2264 C\u03c3Cm(1 + CmCm,\u2191)L\u22121 m\u22121\u220f n=1 (1 + CnCn,\u2191) L\u2225x\u2212 y\u2225.\nProof of Theorem 3.6. We first note that from the nonnegativity of the entropy term in the regularized Wasserstein distance W\u03f5,Z\u0303 , i.e., for every \u03c0 \u2208 \u03a0(\u00b5, \u03bd; Z\u0303),\u222b\nZ\u0303\u00d7Z\u0303 \u03f5 log\n( d\u03c0(x, y)\nd\u00b5(x)d\u03bd(y)\n) d\u03c0(x, y) \u2265 0,\nit is clear that W\u03f5,Z\u0303(\u00b5, \u03bd) \u2265 0 for every \u00b5, \u03bd \u2208 P(Z\u0303). Moreover, from the definition of W\u0302\u03f5,Z\u0303 , i.e., W\u0302\u03f5,Z\u0303(\u00b5, \u03bd) = W\u03f5,Z\u0303(\u00b5, \u03bd)\u2212 1 2 (W\u03f5,Z\u0303(\u03bd, \u03bd) +W\u03f5,Z\u0303(\u00b5, \u00b5)), for \u00b5, \u03bd \u2208 P(Z\u0303), it follows that for every m = 1, . . . ,M and any i, j \u2208 {1, . . . ,K} such that i \u0338= j,\nW\u0302\u03f5,Z\u0303 ( (\u03c3 \u25e6 gm)#PiX , (\u03c3 \u25e6 gm)#P j X ) \u2264 W\u03f5,Z\u0303 ( (\u03c3 \u25e6 gm)#PiX , (\u03c3 \u25e6 gm)#P j X ) . (B.2)\nLet C(Z\u0303) be the set of all real-valued continuous functions on Z\u0303 and C(X ;\u03c3 \u25e6 gm) be defined by\nC(X ;\u03c3 \u25e6 gm) := {f : X \u2192 R | \u2203f\u0303 \u2208 C(Z\u0303) s.t. f = f\u0303 \u25e6 \u03c3 \u25e6 gm}.\nThen, from the dual representation in [48, Remark 4.18 in Section 4.4] based on the Lagrangian method and the integral property of pushforward measures in [2, Section 5.2], it follows for every m = 1, . . . ,M that given PiX ,P j X \u2208 {PkX }Kk=1 with i \u0338= j,\nW\u03f5,Z\u0303 ( (\u03c3 \u25e6 gm)#PiX , (\u03c3 \u25e6 gm)#P j X )\n= sup f\u0303 ,h\u0303\u2208C(Z\u0303) {\u222b Z\u0303 f\u0303(x)d ( (\u03c3 \u25e6 gm)#PiX ) (x) + \u222b Z\u0303 h\u0303(y)d ( (\u03c3 \u25e6 gm)#PjX ) (y)\n\u2212\u03f5 \u222b Z\u0303\u00d7Z\u0303 e 1 \u03f5 (f\u0303(x)+h\u0303(y)\u2212\u2225x\u2212y\u2225 2)d ( (\u03c3 \u25e6 gm)#PiX ) \u2297 ( (\u03c3 \u25e6 gm)#PjX ) (x, y) } = sup\nf,h\u2208C(X ;\u03c3\u25e6gm) {\u222b X f(x)dPiX (x) + \u222b X h(y)dPiX (y)\n\u2212\u03f5 \u222b X\u00d7X e 1 \u03f5 (f(x)+h(y)\u2212\u2225(\u03c3\u25e6g m)(x)\u2212(\u03c3\u25e6gm)(y)\u22252)d(PiX \u2297 P j X )(x, y) } .\n(B.3)\nConsider the following regularized optimal transport problem:\nW\u0303\u03f5,X (Pi,Pj ;\u03c3 \u25e6 gm) := inf \u03c0\u2208\u03a0(Pi,Pj ;X ) {\u222b X\u00d7X ( \u2225\u03c3 \u25e6 gm(x)\u2212 \u03c3 \u25e6 gm(y)\u22252\n+\u03f5 log\n( d\u03c0(x, y)\ndPiX (x)dP j X (y)\n)) d\u03c0(x, y) } .\nThen, from the dual representation, as in (B.3), it follows that\nW\u0303\u03f5,X (Pi,Pj ;\u03c3 \u25e6 gm) = sup f,h\u2208C(X ) {\u222b X f(x)dPiX (x) + \u222b X h(y)dPiX (y)\n\u2212\u03f5 \u222b X\u00d7X e 1 \u03f5 (f(x)+h(y)\u2212\u2225(\u03c3\u25e6g m)(x)\u2212(\u03c3\u25e6gm)(y)\u22252)d(PiX \u2297 P j X )(x, y) } ,\n(B.4)\nwhere C(X ) denotes the set of all continuous real-valued functions on X . From the dual representations in (B.3) and (B.4) and the relation that C(X ;\u03c3 \u25e6 gm) \u2286 C(X ), it follows that\nW\u03f5,Z\u0303 ( (\u03c3 \u25e6 gm)#PiX , (\u03c3 \u25e6 gm)#P j X ) \u2264 W\u0303\u03f5,X (Pi,Pj ;\u03c3 \u25e6 gm). (B.5)\nOn the other hand, from the first order optimality condition and the continuity of \u03c3 \u25e6 gm, presented in Lemma 3.4, it follows that the optimal potentials f\u2217, h\u2217 \u2208 C(X ), which realize the supremum in (B.4), are given by, respectively,\nf\u2217(x) := \u2212\u03f5 log (\u222b\nX e\n1 \u03f5 (h \u2217(y)\u2212\u2225(\u03c3\u25e6gm)(x)\u2212(\u03c3\u25e6gm)(y)\u22252)dPjX (y) ) , x \u2208 X ,\nh\u2217(y) := \u2212\u03f5 log (\u222b\nX e\n1 \u03f5 (f \u2217(x)\u2212\u2225(\u03c3\u25e6gm)(x)\u2212(\u03c3\u25e6gm)(y)\u22252)dPiX (x) ) , y \u2208 X ,\nwhich can be represented by f\u2217 = f\u0303\u2217 \u25e6 \u03c3 \u25e6 gm and h\u2217 = h\u0303\u2217 \u25e6 \u03c3 \u25e6 gm, respectively, where f\u0303\u2217, h\u0303\u2217 \u2208 C(Z\u0303) are given by, respectively,\nf\u0303\u2217(x) := \u2212\u03f5 log (\u222b\nX e\n1 \u03f5 ((h\u0303 \u2217\u25e6\u03c3\u25e6gm)(y)\u2212\u2225x\u2212(\u03c3\u25e6gm)(y)\u22252)dPjX (y) ) , x \u2208 Z\u0303,\nh\u0303\u2217(y) := \u2212\u03f5 log (\u222b\nX e\n1 \u03f5 ((f\u0303 \u2217\u25e6\u03c3\u25e6gm)(x)\u2212\u2225(\u03c3\u25e6gm)(x)\u2212y\u22252)dPiX (x) ) , y \u2208 Z\u0303.\nThis ensures that f\u2217, h\u2217 \u2208 C(X ;\u03c3 \u25e6 gm) \u2286 C\u0303(X ). Hence we establish that (B.5) holds as equality.\nFrom this and the Lipschitz continuity of \u03c3 \u25e6 gm with the constant C\u03c3\u25e6gm > 0 in Lemma 3.4, it follows that\nW\u03f5,Z\u0303 ( (\u03c3 \u25e6 gm)#PiX , (\u03c3 \u25e6 gm)#P j X ) = W\u0303\u03f5,X (Pi,Pj ;\u03c3 \u25e6 gm)\n\u2264 inf \u03c0\u2208\u03a0(Pi,Pj ;X ) {\u222b X\u00d7X ( C2\u03c3\u25e6gm\u2225x\u2212 y\u22252 + \u03f5 log ( d\u03c0(x, y) dPiX (x)dP j X (y) )) d\u03c0(x, y) } \u2264 max{C2\u03c3\u25e6gm , 1}W\u03f5,X (Pi,Pj).\nTherefore, we have shown that\nM\u2211 m=1 max i,j\u2208{1,...,K}, i \u0338=j W\u03f5,Z\u0303 ( (\u03c3 \u25e6 gm)#PiX , (\u03c3 \u25e6 gm)#P j X ) \u2264 C max i,j\u2208{1,...,K}, i \u0338=j W\u03f5,X (Pi,Pj),\nwith C = \u2211M\nm=1 max{C2\u03c3\u25e6gm , 1}. Combining this with (B.2) concludes the proof."
        },
        {
            "heading": "C SOME REMARKS ON SECTION 3.2",
            "text": "Remark C.1. Theorem 3.6 supports that the Sinkhorn-based alignment involving intricate doubly residual stacking architecture is feasible, as far as the pair-wise divergence of source domains\u2019 measures can be \u2018a priori\u2019 estimated under some suitable divergence (i.e., the entropic regularized Wasserstein distance in the right-hand side of the inequality therein). Indeed, the PoT library introduced by [18] can be used for calculation of the entropic regularized Wasserstein distances. On the other hand, the proposed Sinkhorn-based alignment loss is implemented by GeomLoss of [16] that is known to be a significantly efficient and accurate approximate algorithm and will be the main calculation toolkit in the model.\nRemark C.2. For sequential data generation, [62] introduced a causality constraint within optimal transport distances and used the Sinkhorn divergence as an approximate for the causality constrained optimal transport distances. However, we do not consider the constraint but adopt the Sinkhorn divergence for an approximate of the classic optimal transport distance as in (3.4). This is because unlike the reference, there is no inference for the causality between pushforward feature measures from the source domains."
        },
        {
            "heading": "D DETAILED EXPERIMENTAL INFORMATION OF SECTION 4",
            "text": "Experimental environment. We conduct all experiments using the specifications below:\n\u2022 CPU: Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz.\n\u2022 GPU: NVIDIA TITAN RTX.\nAll analyses in Section E utilize the same environment. The comprehensive software setup can be found on GitHub3.\nDataset configuration. The training data generation follows the steps detailed below:\n1. Retrieve financial data {commodity, income, interest rate, exchange rate} from the Federal Reserve Economic Data (FRED), and weather data {pressure, rain, temperature, wind} from the National Centers for Environmental Information (NCEI). Subsequently, we designate finance and weather as the superdomain A, with the subordinate data categorized as individual domains.\n2. Process each data point into a sliding window of defined dimensions [\u03b1, \u03b2], e.g., [50, 10], with the sliding stride of 1. Each segment is treated as an individual instance.\n3. To alleviate any potential concerns arising from data imbalance, we establish a predetermined quantity of 75,000 instances for each domain through random sampling, thereby guaranteeing independence from such considerations.\n3 https://github.com/leejoonhun/fan-beats\n4. We randomly split each domain into three sets: 70% for training, 10% for validation, and 20% for testing.\nIt is worth noting that our dataset consists entirely of real-world data and covers a wide range of frequencies, including daily, weekly, monthly, quarterly, and yearly observations. The graphical representation of the frequency distribution for instances is depicted in Figure 4.\nODG scenarios involve no overlap between the source and target domains with respect to the superdomain, e.g., {D}3k=1 = {pressure, rain, temperature}, and DT = commodity. For fair comparisons, the number of source domains is standardized to three across CDG and IDG scenarios. In CDG, we select one domain from the target domain\u2019s superdomain (p = 2) and two domains from the other superdomain as sources, e.g., {D}3k=1 = {commodity, income, pressure}, and DT = rain. To evaluate IDG, we designate one target domain and consider the remaining domains within the same superdomain as source domains, e.g., {D}3k=1 = {pressure, rain, temperature}, and DT = wind. Note that each selected combination of source domains for Table 2 is {rain, temperature, wind} for ODG, {commodity, temperature, wind} for CDG, and {commodity, income, interest rate} for IDG. Evaluation metrics. For our experiments, we employ two evaluation metrics. Given H = N \u00d7 \u03b2 with where N represents the number of instances, the metrics are defined as:\nsMAPE = 2\nH \u00d7 H\u2211 i=1 |yi \u2212 y\u0302i| |yi|+ |y\u0302i| , and MASE = 1/H \u00d7 \u2211H i=1 |yi \u2212 y\u0302i| 1/(H \u2212 1)\u00d7 \u2211H\u22121 i=1 |yi+1 \u2212 yi| .\nHyperparameters. Considering the scope of our experimental configuration that bring a total of 184 experimental cases for each model, we adopt a suitable range of hyperparameters, detailed in Table 3, to achieve the performance results presented in Table 1.\nTraining and validation loss plots. Feature-aligned N-BEATS is a complicated architecture based on the doubly residual stacking principle with an additional feature-aligning procedure. To investigate the stability of training, we analyze the training and validation loss plots. Figure 5 indicates that the gradients are well-behaved during training. This stable optimization is regarded to the Lemma 3.4 presented in Section 3.1."
        },
        {
            "heading": "E DETAILED EXPERIMENTAL RESULTS OF SECTION 4",
            "text": "Tables 4 and 5 contain the extended experimental results summarized in Table 1. The suitability of various measures of dispersion within our proposed framework explored in Table 2 is presented in Tables 6 and 7 with more details. Specifically, we consider the commonly used metrics for the representation learning framework: for \u00b5, \u03bd \u2208 P(Z\u0303),\n\u2022 Kullback-Leibler divergence (KL): KL(\u00b5\u2225\u03bd) = \u222b Z\u0303 log ( \u00b5(dz) \u03bd(dz) ) \u00b5(dz),\n\u2022 Maximum mean discrepancy (MMD):\nMMDF (\u00b5, \u03bd) = sup f\u2208F (\u222b Z\u0303 f(x)\u00b5(dx)\u2212 \u222b Z\u0303 f(y)\u03bd(dy) ) ,\nwhere F represents a class of functions f : Z\u0303 \u2192 R. Notably, F can be delineated as the unit ball in a reproducing kernel Hilbert space. For detailed description and insights into other possible function classes, refer to [23, Sections 2.2, 7.1, and 7.2].\nODG\nFR E\nD Commodity 0.136 0.049 0.103 0.046 0.279 0.072 0.258 0.069 0.195 0.052 0.136 0.049 Income 0.299 0.057 0.298 0.055 0.369 0.082 0.335 0.075 0.305 0.056 0.304 0.055\nInterest rate 0.120 0.074 0.100 0.070 0.200 0.044 0.189 0.046 0.148 0.073 0.120 0.073 Exchange rate 0.035 0.059 0.034 0.058 0.078 0.078 0.075 0.069 0.040 0.062 0.039 0.060\nAverage 0.148 0.060 0.134 0.057 0.232 0.069 0.214 0.065 0.172 0.061 0.150 0.059\nN C\nE I Pressure 0.368 0.255 0.350 0.250 0.759 0.396 0.409 0.305 0.368 0.255 0.367 0.255\nRain 1.821 1.099 1.804 0.910 1.798 1.600 1.793 1.430 1.818 1.100 1.806 0.918 Temperature 0.247 0.245 0.245 0.243 0.247 0.353 0.246 0.255 0.247 0.245 0.246 0.244\nWind 0.455 0.645 0.454 0.644 0.451 0.665 0.449 0.662 0.455 0.645 0.454 0.645 Average 0.723 0.561 0.713 0.512 0.814 0.754 0.724 0.663 0.722 0.561 0.718 0.516\nCDG\nFR E\nD Commodity 0.082 0.047 0.081 0.044 0.195 0.060 0.197 0.059 0.119 0.046 0.107 0.046 Income 0.296 0.054 0.295 0.053 0.327 0.072 0.323 0.070 0.301 0.055 0.295 0.053\nInterest rate 0.086 0.074 0.085 0.074 0.146 0.051 0.146 0.054 0.102 0.077 0.100 0.077 Exchange rate 0.032 0.056 0.029 0.055 0.054 0.074 0.055 0.063 0.032 0.056 0.031 0.055\nAverage 0.124 0.058 0.123 0.057 0.181 0.064 0.179 0.062 0.139 0.059 0.133 0.058\nN C\nE I Pressure 0.375 0.264 0.372 0.260 0.408 0.307 0.405 0.299 0.540 0.374 0.372 0.263\nRain 1.807 1.169 1.803 0.783 1.800 2.091 1.787 1.831 1.807 1.169 1.804 1.178 Temperature 0.334 0.243 0.245 0.242 0.275 0.245 0.240 0.244 0.253 0.245 0.245 0.243\nWind 0.453 0.643 0.452 0.643 0.441 0.646 0.439 0.646 0.453 0.643 0.452 0.643 Average 0.742 0.581 0.718 0.482 0.731 0.822 0.718 0.755 0.763 0.608 0.718 0.582\nIDG\nFR E\nD Commodity 0.083 0.045 0.068 0.043 0.125 0.053 0.126 0.053 0.165 0.047 0.080 0.044 Income 0.299 0.058 0.297 0.054 0.305 0.055 0.302 0.055 0.301 0.056 0.298 0.055\nInterest rate 0.072 0.081 0.071 0.081 0.088 0.084 0.086 0.091 0.080 0.083 0.074 0.081 Exchange rate 0.024 0.051 0.024 0.050 0.028 0.056 0.029 0.056 0.025 0.051 0.024 0.050\nAverage 0.119 0.059 0.115 0.057 0.137 0.062 0.136 0.064 0.143 0.081 0.119 0.058\nN C\nE I Pressure 0.394 0.276 0.384 0.272 0.392 0.266 0.389 0.263 0.384 0.276 0.384 0.276\nRain 1.776 1.211 1.776 1.208 1.792 2.922 1.805 3.046 1.818 1.690 1.776 1.208 Temperature 0.247 0.242 0.247 0.242 0.234 0.231 0.231 0.227 0.247 0.242 0.245 0.241\nWind 0.455 0.641 0.452 0.640 0.433 0.622 0.434 0.623 0.454 0.641 0.452 0.640 Average 0.718 0.593 0.715 0.591 0.713 1.011 0.715 1.039 0.726 0.712 0.714 0.591\nODG\nFR E\nD Commodity 0.666 17.941 0.657 18.084 0.830 22.325 1.248 40.580 Income 0.003 162.32 0.185 8,752.66 0.762 NA 1.275 NA\nInterest rate 0.022 9.138 0.190 83.204 0.363 144.97 1.209 2,184.78 Exchange rate 0.013 3.189 0.196 3.979 0.326 4.531 1.124 27.990\nAverage 0.176 48.147 0.307 2,214.48 0.570 NA 1.214 NA\nN C\nE I Pressure 0.954 3.837 1.300 4.237 1.168 4.749 1.414 8.284\nRain 1.038 1.089 1.231 1.169 1.175 0.897 1.783 1.162 Temperature 1.136 4.399 1.340 4.572 1.352 5.761 1.616 10.885\nWind 1.320 1.623 1.337 1.497 1.476 1.836 1.706 2.803 Average 1.112 2.737 1.302 2.869 1.293 3.311 1.630 5.784\nCDG\nFR E\nD Commodity 0.664 18.166 0.855 21.661 0.829 21.360 1.353 40.584 Income 0.004 212.60 0.203 9,979.14 0.995 NA 1.204 NA\nInterest rate 0.023 9.364 0.587 210.91 0.957 1,695.19 1.040 2,058.07 Exchange rate 0.014 3.586 0.499 5.352 0.792 14.055 0.974 28.157\nAverage 0.176 60.929 0.536 2,554.27 0.893 NA 1.143 NA\nN C\nE I Pressure 0.923 3.810 1.055 4.100 1.127 4.769 1.222 5.851\nRain 1.037 1.137 1.033 1.125 1.201 0.899 1.523 1.149 Temperature 1.114 4.340 1.106 4.431 1.304 5.609 1.439 7.358\nWind 1.310 1.649 1.151 1.491 1.458 1.655 1.565 2.228 Average 1.096 2.734 1.086 2.787 1.273 3.233 1.437 4.147\nIDG\nFR E\nD Commodity 0.671 30.210 1.139 38.929 0.835 21.462 1.652 34.784 Income 0.026 1,951.16 0.043 4,232.97 1.500 NA 0.704 NA\nInterest rate 0.079 52.007 1.111 586.24 0.943 475.12 0.593 333.18 Exchange rate 0.013 5.443 1.080 11.873 0.726 5.815 0.424 5.255\nAverage 0.197 509.71 0.843 1,217.50 1.001 NA 0.843 NA\nN C\nE I Pressure 0.868 5.271 0.698 5.318 1.280 6.992 1.906 4.614\nRain 0.900 1.368 0.709 1.301 1.009 0.918 1.463 1.081 Temperature 1.014 6.055 0.768 5.753 1.310 4.788 1.080 4.235\nWind 1.206 2.195 0.913 2.084 1.472 1.592 1.570 1.986 Average 0.997 3.722 0.772 3.614 1.268 3.573 1.505 2.979\nODG\nFR E\nD Commodity 0.103 0.046 0.110 0.046 0.100 0.047 0.257 0.069 0.246 0.066 0.222 0.063 0.136 0.049 0.137 0.050 0.133 0.050 Income 0.297 0.055 0.302 0.056 0.293 0.050 0.334 0.075 0.338 0.082 0.320 0.067 0.305 0.055 0.305 0.055 0.300 0.052\nInterest rate 0.100 0.070 0.107 0.083 0.100 0.070 0.189 0.046 0.165 0.021 0.189 0.046 0.119 0.073 0.123 0.077 0.120 0.073 Exchange rate 0.034 0.058 0.034 0.058 0.040 0.053 0.075 0.069 0.074 0.073 0.070 0.070 0.039 0.060 0.041 0.059 0.044 0.057\nAverage 0.134 0.057 0.138 0.061 0.133 0.055 0.214 0.065 0.206 0.061 0.200 0.062 0.150 0.059 0.152 0.060 0.149 0.058\nN C\nE I Pressure 0.349 0.250 0.351 0.250 NaN NaN 0.411 0.305 0.412 0.306 NaN NaN 0.367 0.255 0.367 0.254 0.365 0.263\nRain 1.807 0.910 1.820 0.919 NaN NaN 1.789 1.430 1.800 1.728 NaN NaN 1.798 0.918 1.818 1.026 1.831 0.951 Temperature 0.245 0.243 0.246 0.244 NaN NaN 0.246 0.255 0.248 0.255 NaN NaN 0.246 0.244 0.248 0.245 0.248 0.247\nWind 0.454 0.644 0.455 0.645 NaN NaN 0.450 0.662 0.450 0.662 NaN NaN 0.454 0.645 0.457 0.645 0.456 0.646 Average 0.714 0.512 0.718 0.515 NaN NaN 0.724 0.663 0.728 0.738 NaN NaN 0.716 0.515 0.723 0.543 0.725 0.527\nCDG\nFR E\nD Commodity 0.081 0.044 0.086 0.045 NaN NaN 0.197 0.059 0.187 0.057 NaN NaN 0.107 0.046 0.104 0.046 NaN NaN Income 0.295 0.053 0.298 0.054 NaN NaN 0.322 0.070 0.323 0.071 0.315 0.064 0.296 0.053 0.301 0.055 0.296 0.051\nInterest rate 0.085 0.074 0.088 0.076 NaN NaN 0.146 0.054 0.141 0.051 0.115 0.035 0.100 0.077 0.100 0.078 0.088 0.076 Exchange rate 0.029 0.055 0.029 0.056 0.030 0.055 0.055 0.063 0.054 0.066 0.044 0.071 0.031 0.055 0.031 0.057 0.031 0.057\nAverage 0.122 0.056 0.125 0.058 NaN NaN 0.180 0.061 0.176 0.061 NaN NaN 0.133 0.058 0.134 0.059 NaN NaN\nN C\nE I Pressure 0.372 0.260 0.377 0.262 NaN NaN 0.403 0.299 0.405 0.316 NaN NaN 0.371 0.263 0.370 0.262 NaN NaN\nRain 1.796 0.783 1.815 0.940 NaN NaN 1.780 1.831 1.791 1.934 NaN NaN 1.804 1.178 1.808 1.108 NaN NaN Temperature 0.244 0.242 0.245 0.243 NaN NaN 0.240 0.244 0.241 0.245 NaN NaN 0.244 0.243 0.246 0.244 NaN NaN\nWind 0.452 0.643 0.453 0.643 NaN NaN 0.437 0.646 0.440 0.646 NaN NaN 0.453 0.643 0.453 0.643 NaN NaN Average 0.716 0.482 0.723 0.522 NaN NaN 0.715 0.755 0.719 0.785 NaN NaN 0.718 0.582 0.719 0.564 NaN NaN\nIDG\nModels N-HiTS N-BEATS-I N-BEATS-G\n\u03f5 Values 1e-5 1e-1 1e-5 1e-1 1e-5 1e-1\nMetrics sMAPE MASE sMAPE MASE sMAPE MASE sMAPE MASE sMAPE MASE sMAPE MASE\nODG\nFR E\nD Commodity 0.103 0.046 0.109 0.047 0.257 0.069 0.121 0.097 0.110 0.047 0.112 0.074 Income 0.297 0.055 0.297 0.129 0.334 0.075 0.329 0.267 0.302 0.057 0.304 0.204\nInterest rate 0.100 0.070 0.105 0.046 0.189 0.046 0.116 0.094 0.106 0.074 0.107 0.072 Exchange rate 0.034 0.058 0.036 0.015 0.075 0.069 0.040 0.031 0.036 0.060 0.037 0.024\nAverage 0.134 0.057 0.137 0.059 0.214 0.065 0.152 0.122 0.139 0.060 0.140 0.094\nN C\nE I Pressure 0.348 0.250 0.356 0.156 0.408 0.305 0.393 0.322 0.364 0.254 0.364 0.246\nRain 1.795 0.910 1.790 0.776 1.789 1.430 1.980 1.603 1.808 0.944 1.832 1.223 Temperature 0.244 0.243 0.246 0.106 0.245 0.255 0.272 0.219 0.247 0.244 0.252 0.167\nWind 0.452 0.644 0.450 0.195 0.448 0.662 0.498 0.404 0.457 0.645 0.461 0.308 Average 1.072 0.580 1.073 0.466 1.099 0.868 1.187 0.963 1.086 0.599 1.098 0.735\nCDG\nFR E\nD Commodity 0.081 0.044 0.087 0.057 0.197 0.059 0.093 0.085 0.087 0.045 0.088 0.066 Income 0.295 0.053 0.298 0.193 0.323 0.070 0.318 0.290 0.298 0.056 0.302 0.225\nInterest rate 0.085 0.074 0.090 0.058 0.146 0.054 0.096 0.086 0.090 0.078 0.091 0.067 Exchange rate 0.029 0.055 0.030 0.020 0.055 0.063 0.032 0.030 0.030 0.057 0.030 0.023\nAverage 0.123 0.057 0.126 0.082 0.180 0.062 0.135 0.123 0.126 0.059 0.128 0.095\nN C\nE I Pressure 0.372 0.260 0.369 0.240 0.405 0.299 0.393 0.361 0.367 0.261 0.374 0.280\nRain 1.803 0.783 1.790 1.163 1.787 1.831 1.910 1.747 1.801 0.965 1.816 1.355 Temperature 0.245 0.242 0.245 0.159 0.240 0.244 0.261 0.239 0.246 0.244 0.248 0.185\nWind 0.452 0.643 0.451 0.293 0.439 0.646 0.481 0.440 0.454 0.643 0.457 0.341 Average 1.088 0.522 1.080 0.702 1.096 1.065 1.152 1.054 1.084 0.613 1.095 0.818\nIDG\nFR E\nD Commodity 0.068 0.043 0.070 0.056 0.126 0.053 0.071 0.092 0.070 0.044 0.070 0.055 Income 0.297 0.054 0.304 0.240 0.302 0.055 0.308 0.397 0.299 0.058 0.303 0.238\nInterest rate 0.071 0.081 0.072 0.058 0.086 0.091 0.073 0.095 0.073 0.085 0.072 0.057 Exchange rate 0.024 0.050 0.025 0.020 0.029 0.056 0.025 0.033 0.025 0.050 0.025 0.020\nAverage 0.115 0.057 0.118 0.094 0.136 0.064 0.119 0.154 0.117 0.059 0.118 0.093\nN C\nE I Pressure 0.384 0.273 0.389 0.310 0.389 0.263 0.395 0.512 0.386 0.277 0.388 0.307\nRain 1.776 1.212 1.785 1.422 1.805 3.046 1.811 2.348 1.781 1.326 1.781 1.409 Temperature 0.247 0.243 0.247 0.196 0.231 0.227 0.250 0.323 0.246 0.241 0.246 0.194\nWind 0.452 0.642 0.452 0.360 0.434 0.623 0.459 0.595 0.452 0.640 0.451 0.357 Average 1.080 0.743 1.087 0.866 1.097 1.655 1.103 1.430 1.084 0.802 1.085 0.858\nF VISUALIZATION ON FORECASTING, INTERPRETABILITY AND REPRESENTATION\nVisual comparison of forecasts. We visually compare our models to the N-BEATS-based models, i.e., N-BEATS-G, N-BEATS-I, and N-HiTS. As illustrated in Figure 6, incorporating feature alignment remarkably enhances generalizability, allowing the models to produce finer forecast details. Notably, while baseline models suffer significant performance degradation in the ODG and CDG scenarios, Feature-aligned N-BEATS evidences the benefits of the feature alignment.\nVisual analysis of interpretability. Figure 7 exhibits the interpretability of the proposed method by presenting the final output of the model and intermediate stack forecasts. N-BEATS-I and N-HiTS presented in Appendix A have interpretability. More specifically, N-BEATS-I explicitly captures trend and seasonality information using polynomial and harmonic basis functions, respectively. NHiTS employs Fourier decomposition and utilizes its stacks for hierarchical forecasting based on frequencies. Preserving these core architectures during the alignment procedure, Feature-aligned N-BEATS still retains interpretability.\nVisualization of representation. We further investigate the representational landscape, we analyze the samples of pushforward measure from N-BEATS-I and N-HiTS. Adopting visualization techniques for both aligned and non-aligned instances as depicted in Figure 2, we configure UMAP with 5 neighbors, a minimum distance of 0.1, and employ the Euclidean metric. Similar to N-BEATSG, we discern two observations in Figure 8 pertaining to N-BEATS-I and N-HiTS: (1) instances coalesce, residing closer to one another, and (2) an evident surge in domain entropy, from both N-BEATS-I and N-HiTS."
        },
        {
            "heading": "G ABLATION STUDIES",
            "text": ""
        },
        {
            "heading": "G.1 STACK-WISE VS BLOCK-WISE ALIGNMENTS",
            "text": "As mentioned in Remark 3.3, redundant gradient flows from recurrent architecture potentially causes gradient explosion or vanishing. To empirically validate this insight applied to our approach, we contrast stack-wise and block-wise feature alignments, as shown in Table 8. Notably, although stack-wise alignment generally outperform its counterpart, we do not observe the aforementioned problems, which could be identified by divergence of training. N-BEATS-I with block-wise alignment even demonstrates superior performance. Two plausible explanations are: (1) the limited number of stacks, and (2) the operational differences between the trend and seasonality modules in N-BEATS-I, which might help alleviating redundancy issue. Nonetheless, our primary objective of generalizing the recurrent model across various domains appears achievable through stack-wise alignment.\nODG\nFR E\nD Commodity 0.137 0.049 0.103 0.046 0.133 0.049 0.258 0.069 0.137 0.049 0.136 0.049 Income 0.306 0.056 0.298 0.055 0.305 0.055 0.335 0.075 0.306 0.056 0.304 0.055\nInterest rate 0.121 0.074 0.100 0.070 0.119 0.073 0.189 0.046 0.121 0.075 0.120 0.073 Exchange rate 0.040 0.060 0.034 0.058 0.040 0.060 0.075 0.069 0.040 0.060 0.039 0.060\nAverage 0.151 0.060 0.134 0.057 0.149 0.059 0.214 0.065 0.151 0.060 0.150 0.059\nN C\nE I Pressure 0.368 0.255 0.350 0.250 0.367 0.254 0.409 0.305 0.368 0.255 0.367 0.255\nRain 1.806 1.094 1.804 0.910 1.806 1.094 1.793 1.430 1.807 1.091 1.806 0.918 Temperature 0.247 0.245 0.245 0.243 0.247 0.245 0.246 0.255 0.247 0.245 0.246 0.244\nWind 0.456 0.645 0.454 0.644 0.456 0.645 0.449 0.662 0.457 0.645 0.454 0.645 Average 0.719 0.560 0.713 0.512 0.719 0.560 0.724 0.663 0.720 0.559 0.718 0.516\nCDG\nFR E\nD Commodity 0.102 0.046 0.081 0.044 0.105 0.046 0.197 0.059 0.108 0.047 0.107 0.046 Income 0.301 0.054 0.295 0.053 0.301 0.055 0.323 0.070 0.301 0.054 0.295 0.053\nInterest rate 0.099 0.076 0.085 0.074 0.097 0.076 0.146 0.054 0.101 0.078 0.100 0.077 Exchange rate 0.031 0.056 0.029 0.055 0.031 0.098 0.055 0.063 0.032 0.055 0.031 0.055\nAverage 0.133 0.058 0.123 0.057 0.134 0.069 0.179 0.062 0.136 0.059 0.133 0.058\nN C\nE I Pressure 0.371 0.263 0.372 0.260 0.370 0.262 0.405 0.299 0.373 0.264 0.372 0.263\nRain 1.809 1.144 1.803 0.783 1.808 1.148 1.787 1.831 1.804 1.181 1.804 1.178 Temperature 0.246 0.244 0.245 0.242 0.246 0.244 0.240 0.244 0.246 0.244 0.245 0.243\nWind 0.453 0.644 0.452 0.643 0.454 0.644 0.439 0.646 0.453 0.643 0.452 0.643 Average 0.720 0.574 0.718 0.482 0.720 0.575 0.718 0.755 0.719 0.583 0.718 0.582\nIDG\nFR E\nD Commodity 0.081 0.045 0.068 0.043 0.075 0.044 0.126 0.053 0.074 0.044 0.080 0.044 Income 0.302 0.056 0.297 0.054 0.301 0.056 0.302 0.055 0.302 0.056 0.298 0.055\nInterest rate 0.079 0.083 0.071 0.081 0.079 0.082 0.086 0.091 0.079 0.083 0.074 0.081 Exchange rate 0.025 0.051 0.024 0.050 0.025 0.051 0.029 0.056 0.025 0.051 0.024 0.050\nAverage 0.122 0.059 0.115 0.057 0.120 0.058 0.136 0.064 0.120 0.059 0.119 0.058\nN C\nE I Pressure 0.384 0.276 0.384 0.272 0.383 0.277 0.389 0.263 0.384 0.276 0.384 0.276\nRain 1.818 1.681 1.776 1.208 1.798 1.535 1.805 3.046 1.817 1.676 1.776 1.208 Temperature 0.247 0.243 0.247 0.242 0.245 0.242 0.231 0.227 0.246 0.243 0.245 0.241\nWind 0.453 0.641 0.452 0.640 0.453 0.641 0.434 0.623 0.453 0.641 0.452 0.640 Average 0.726 0.710 0.715 0.591 0.720 0.674 0.715 1.039 0.725 0.709 0.714 0.591"
        },
        {
            "heading": "G.2 NORMALIZATION FUNCTIONS",
            "text": "According to the Table 9, Feature-aligned N-BEATS generally achieves superior performance when utilizing softmax function. However, there are instances where tanh function or even the absence of a normalization yields better results compared to the softmax. This suggests that while scale is predominant instance-wise attribute, it may exhibit domain-dependent characteristics under certain conditions. Aligning this scale is therefore necessary. This entails that the softmax, tanh, and to not normalize offer different levels of flexibility in modulating or completely disregarding the scale information, implying a spectrum of capacities in aligning domain-specific attributes.\nODG\nFR E\nD Commodity 0.103 0.046 0.104 0.046 0.103 0.046 0.265 0.070 0.270 0.069 0.258 0.069 0.050 0.136 0.050 0.135 0.136 0.049 Income 0.299 0.056 0.299 0.056 0.298 0.055 0.319 0.060 0.324 0.063 0.335 0.075 0.306 0.056 0.305 0.056 0.304 0.055\nInterest rate 0.101 0.071 0.101 0.071 0.100 0.070 0.191 0.073 0.193 0.048 0.189 0.046 0.120 0.072 0.123 0.074 0.120 0.073 Exchange rate 0.034 0.058 0.034 0.058 0.034 0.058 0.072 0.061 0.077 0.074 0.075 0.069 0.041 0.061 0.043 0.059 0.039 0.060\nAverage 0.134 0.058 0.135 0.058 0.134 0.057 0.212 0.066 0.216 0.064 0.214 0.065 0.129 0.081 0.130 0.081 0.150 0.059\nN C\nE I Pressure 0.349 0.250 0.348 0.249 0.350 0.250 0.398 0.289 0.411 0.300 0.409 0.305 0.352 0.247 0.361 0.253 0.367 0.255\nRain 1.819 0.918 1.820 0.917 1.804 0.910 1.808 2.087 1.807 1.841 1.793 1.430 1.814 1.075 1.814 1.071 1.806 0.918 Temperature 0.247 0.244 0.246 0.244 0.245 0.243 0.248 0.253 0.249 0.256 0.246 0.255 0.247 0.245 0.248 0.245 0.246 0.244\nWind 0.455 0.645 0.455 0.644 0.454 0.644 0.452 0.660 0.451 0.661 0.449 0.662 0.456 0.645 0.457 0.645 0.454 0.645 Average 0.718 0.514 0.717 0.514 0.713 0.512 0.727 0.822 0.730 0.765 0.724 0.663 0.717 0.553 0.720 0.554 0.718 0.516\nCDG\nFR E\nD Commodity 0.082 0.045 0.081 0.044 0.081 0.044 0.189 0.059 0.203 0.061 0.197 0.059 0.108 0.047 0.109 0.047 0.107 0.046 Income 0.296 0.055 0.296 0.055 0.295 0.053 0.323 0.088 0.319 0.064 0.323 0.070 0.302 0.054 0.301 0.054 0.295 0.053\nInterest rate 0.085 0.074 0.085 0.075 0.085 0.074 0.145 0.052 0.149 0.058 0.146 0.054 0.101 0.078 0.101 0.077 0.100 0.077 Exchange rate 0.029 0.056 0.029 0.056 0.029 0.055 0.053 0.066 0.055 0.065 0.055 0.063 0.032 0.056 0.032 0.056 0.031 0.055\nAverage 0.123 0.058 0.123 0.058 0.123 0.057 0.178 0.066 0.182 0.062 0.179 0.062 0.136 0.059 0.136 0.059 0.133 0.058\nN C\nE I Pressure 0.373 0.260 0.374 0.260 0.372 0.260 0.405 0.316 0.410 0.313 0.405 0.299 0.255 0.372 0.257 0.356 0.372 0.263\nRain 1.808 0.931 1.808 0.931 1.803 0.783 1.802 2.152 1.802 2.144 1.787 1.831 1.805 1.18 1.805 1.186 1.804 1.178 Temperature 0.246 0.243 0.246 0.243 0.245 0.242 0.242 0.246 0.244 0.248 0.240 0.244 0.245 0.243 0.246 0.243 0.245 0.243\nWind 0.453 0.643 0.453 0.643 0.453 0.643 0.442 0.649 0.443 0.649 0.439 0.646 0.452 0.643 0.453 0.643 0.452 0.643 Average 0.720 0.519 0.720 0.519 0.718 0.482 0.723 0.841 0.725 0.839 0.718 0.755 0.737 0.562 0.738 0.560 0.718 0.582\nIDG\nFR E\nD Commodity 0.068 0.044 0.068 0.044 0.068 0.043 0.124 0.052 0.142 0.055 0.126 0.053 0.083 0.045 0.083 0.045 0.080 0.044 Income 0.299 0.057 0.299 0.058 0.297 0.054 0.310 0.059 0.308 0.058 0.302 0.055 0.302 0.056 0.302 0.057 0.298 0.055\nInterest rate 0.072 0.081 0.072 0.077 0.071 0.081 0.097 0.087 0.104 0.079 0.086 0.091 0.080 0.081 0.080 0.080 0.074 0.081 Exchange rate 0.025 0.051 0.025 0.050 0.024 0.050 0.033 0.055 0.040 0.055 0.029 0.056 0.026 0.052 0.026 0.051 0.024 0.050\nAverage 0.116 0.058 0.116 0.057 0.115 0.057 0.141 0.063 0.149 0.062 0.136 0.064 0.123 0.059 0.123 0.058 0.119 0.058\nN C\nE I Pressure 0.393 0.273 0.393 0.273 0.384 0.272 0.373 0.256 0.390 0.266 0.389 0.263 0.366 0.274 0.367 0.283 0.384 0.276\nRain 1.776 1.211 1.776 1.205 1.776 1.208 1.883 3.222 1.873 3.848 1.805 3.046 1.818 1.671 1.818 1.695 1.776 1.208 Temperature 0.247 0.242 0.247 0.242 0.247 0.242 0.236 0.232 0.235 0.231 0.231 0.227 0.246 0.241 0.246 0.242 0.245 0.241\nWind 0.454 0.641 0.454 0.640 0.452 0.640 0.441 0.631 0.441 0.632 0.434 0.623 0.453 0.642 0.452 0.642 0.452 0.640 Average 0.718 0.592 0.718 0.590 0.715 0.591 0.733 1.085 0.735 1.244 0.715 1.039 0.721 0.707 0.721 0.716 0.714 0.591"
        },
        {
            "heading": "G.3 SUBTLE DOMAIN SHIFT",
            "text": "Although the domain generalization commonly focuses on the domain shift problems, models may not perform as expected when the domain shift between source and target data is minimal. In some cases where the data from both domains align closely, fitting to source domain without invariant feature learning even can be beneficial. To examine this concern, we extend our analysis to the generalizability of Feature-aligned N-BEATS under such conditions. Table 10 demonstrates, while our model remains competitive, there is performance degradation observed in certain instances."
        },
        {
            "heading": "G.4 TOURISM, M3 AND M4 DATASETS",
            "text": "We extend our experimental scope to include three additional datasets: Tourism [3], M3 [36], and M4 [37]. Models are trained on two datasets and tested on the remaining dataset, enabling us to evaluate both ODG (M3, M4 \u2192 Tourism) and CDG (M3, Tourism \u2192 M4 and M4, Tourism \u2192 M3) scenarios. Our proposed methods consistently outperform N-BEATS models, demonstrating their generalization ability."
        },
        {
            "heading": "CDG",
            "text": ""
        },
        {
            "heading": "ODG",
            "text": ""
        }
    ],
    "year": 2024
}