{
    "abstractText": "Bayesian optimization is a highly efficient approach to optimizing objective functions which are expensive to query. These objectives are typically represented by Gaussian process (GP) surrogate models which are easy to optimize and support exact inference. While standard GP surrogates have been well-established in Bayesian optimization, Bayesian neural networks (BNNs) have recently become practical function approximators, with many benefits over standard GPs such as the ability to naturally handle non-stationarity and learn representations for high-dimensional data. In this paper, we study BNNs as alternatives to standard GP surrogates for optimization. We consider a variety of approximate inference procedures for finitewidth BNNs, including high-quality Hamiltonian Monte Carlo, low-cost stochastic MCMC, and heuristics such as deep ensembles. We also consider infinite-width BNNs, linearized Laplace approximations, and partially stochastic models such as deep kernel learning. We evaluate this collection of surrogate models on diverse problems with varying dimensionality, number of objectives, non-stationarity, and discrete and continuous inputs. We find: (i) the ranking of methods is highly problem dependent, suggesting the need for tailored inductive biases; (ii) HMC is the most successful approximate inference procedure for fully stochastic BNNs; (iii) full stochasticity may be unnecessary as deep kernel learning is relatively competitive; (iv) deep ensembles perform relatively poorly; (v) infinite-width BNNs are particularly promising, especially in high dimensions.",
    "authors": [],
    "id": "SP:a00a99791e0d6eca813e9fa52520f0bfc6093636",
    "references": [
        {
            "authors": [
                "Maximilian Balandat",
                "Brian Karrer",
                "Daniel Jiang",
                "Samuel Daulton",
                "Ben Letham",
                "Andrew G Wilson",
                "Eytan Bakshy"
            ],
            "title": "Botorch: A framework for efficient monte-carlo bayesian optimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "James Bergstra",
                "Daniel Yamins",
                "David Cox"
            ],
            "title": "Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures",
            "venue": "In International conference on machine learning,",
            "year": 2013
        },
        {
            "authors": [
                "Tianqi Chen",
                "Emily B Fox",
                "Carlos Guestrin",
                "Michael I Jordan"
            ],
            "title": "Stochastic gradient hamiltonian monte carlo",
            "venue": "In International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Zhongxiang Dai",
                "Yao Shu",
                "Bryan Kian Hsiang Low",
                "Patrick Jaillet"
            ],
            "title": "Sample-then-optimize batch neural thompson sampling, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Erik Daxberger",
                "Agustinus Kristiadi",
                "Alexander Immer",
                "Runa Eschenhagen",
                "Matthias Bauer",
                "Philipp Hennig"
            ],
            "title": "Laplace redux-effortless bayesian deep learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ryan M Dreifuerst",
                "Samuel Daulton",
                "Yuchen Qian",
                "Paul Varkey",
                "Maximilian Balandat",
                "Sanjay Kasturia",
                "Anoop Tomar",
                "Ali Yazdan",
                "Vish Ponnampalam",
                "Robert W Heath"
            ],
            "title": "Optimizing coverage and capacity in cellular networks using machine learning",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "David Eriksson",
                "Michael Pearce",
                "Jacob Gardner",
                "Ryan D Turner",
                "Matthias Poloczek"
            ],
            "title": "Scalable global optimization via local bayesian optimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Foldager",
                "Mikkel Jordahn",
                "Lars Kai Hansen",
                "Michael Riis Andersen"
            ],
            "title": "On the role of model uncertainties in bayesian optimization, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Peter I Frazier"
            ],
            "title": "A tutorial on bayesian optimization",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Peter I Frazier",
                "Warren B Powell",
                "Savas Dayanik"
            ],
            "title": "A knowledge-gradient policy for sequential information collection",
            "venue": "SIAM Journal on Control and Optimization,",
            "year": 2008
        },
        {
            "authors": [
                "Jacob Gardner",
                "Chuan Guo",
                "Kilian Weinberger",
                "Roman Garnett",
                "Roger Grosse"
            ],
            "title": "Discovering and exploiting additive structure for bayesian optimization",
            "venue": "In Artificial Intelligence and Statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Roman Garnett"
            ],
            "title": "Bayesian optimization",
            "year": 2023
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531,",
            "year": 2015
        },
        {
            "authors": [
                "Matthew D. Hoffman",
                "David M. Blei",
                "Chong Wang",
                "John Paisley"
            ],
            "title": "Stochastic variational inference",
            "venue": "Journal of Machine Learning Research,",
            "year": 2013
        },
        {
            "authors": [
                "Frank Hutter",
                "Holger H Hoos",
                "Kevin Leyton-Brown"
            ],
            "title": "Sequential model-based optimization for general algorithm configuration",
            "venue": "In International conference on learning and intelligent optimization,",
            "year": 2011
        },
        {
            "authors": [
                "Alexander Immer",
                "Maciej Korzepa",
                "Matthias Bauer"
            ],
            "title": "Improving predictions of bayesian neural nets via local linearization, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Pavel Izmailov",
                "Sharad Vikram",
                "Matthew D Hoffman",
                "Andrew Gordon Gordon Wilson"
            ],
            "title": "What are bayesian neural network posteriors really like",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kirthevasan Kandasamy",
                "Jeff Schneider",
                "Barnab\u00e1s P\u00f3czos"
            ],
            "title": "High dimensional bayesian optimisation and bandits via additive models",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Mohammad Emtiyaz Khan",
                "H\u00e5vard Rue"
            ],
            "title": "The bayesian learning rule",
            "venue": "arXiv preprint arXiv:2107.04562,",
            "year": 2021
        },
        {
            "authors": [
                "Samuel Kim",
                "Peter Y Lu",
                "Charlotte Loh",
                "Jamie Smith",
                "Jasper Snoek",
                "Marin Soljacic"
            ],
            "title": "Deep learning for bayesian optimization of scientific problems with high-dimensional structure",
            "venue": "Transactions of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Agustinus Kristiadi",
                "Alexander Immer",
                "Runa Eschenhagen",
                "Vincent Fortuin"
            ],
            "title": "Promises and pitfalls of the linearized laplace in bayesian optimization, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E. Hinton"
            ],
            "title": "ImageNet classification with deep convolutional neural networks",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2012
        },
        {
            "authors": [
                "Balaji Lakshminarayanan",
                "Alexander Pritzel",
                "Charles Blundell"
            ],
            "title": "Simple and scalable predictive uncertainty estimation using deep ensembles",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jaehoon Lee",
                "Yasaman Bahri",
                "Roman Novak",
                "Samuel S Schoenholz",
                "Jeffrey Pennington",
                "Jascha Sohl-Dickstein"
            ],
            "title": "Deep neural networks as gaussian processes",
            "venue": "arXiv preprint arXiv:1711.00165,",
            "year": 2017
        },
        {
            "authors": [
                "Marius Lindauer",
                "Katharina Eggensperger",
                "Matthias Feurer",
                "Andr\u00e9 Biedenkapp",
                "Difan Deng",
                "Carolin Benjamins",
                "Tim Ruhkopf",
                "Ren\u00e9 Sass",
                "Frank Hutter"
            ],
            "title": "Smac3: A versatile bayesian optimization package for hyperparameter optimization",
            "venue": "Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Michal Lisicki",
                "Arash Afkanpour",
                "Graham W. Taylor"
            ],
            "title": "Empirical analysis of representation learning and exploration in neural kernel bandits, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Sanae Lotfi",
                "Pavel Izmailov",
                "Gregory Benton",
                "Micah Goldblum",
                "Andrew Gordon Wilson"
            ],
            "title": "Bayesian model selection, the marginal likelihood, and generalization, 2023",
            "year": 2023
        },
        {
            "authors": [
                "David JC MacKay"
            ],
            "title": "A practical bayesian framework for backpropagation networks",
            "venue": "Neural computation,",
            "year": 1992
        },
        {
            "authors": [
                "Wesley J Maddox",
                "Maximilian Balandat",
                "Andrew G Wilson",
                "Eytan Bakshy"
            ],
            "title": "Bayesian optimization with high-dimensional outputs",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 1928
        },
        {
            "authors": [
                "Henry Moss",
                "David Leslie",
                "Daniel Beck",
                "Javier Gonzalez",
                "Paul Rayson. Boss"
            ],
            "title": "Bayesian optimization over string spaces",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Samuel M\u00fcller",
                "Matthias Feurer",
                "Noah Hollmann",
                "Frank Hutter"
            ],
            "title": "Pfns4bo: In-context learning for bayesian optimization, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Radford M Neal"
            ],
            "title": "Bayesian Learning for Neural Networks",
            "year": 1996
        },
        {
            "authors": [
                "Radford M. Neal"
            ],
            "title": "MCMC using Hamiltonian dynamics",
            "venue": "Handbook of Markov Chain Monte Carlo,",
            "year": 2010
        },
        {
            "authors": [
                "Radford M Neal"
            ],
            "title": "Priors for infinite networks. Bayesian learning for neural networks, pages",
            "year": 1996
        },
        {
            "authors": [
                "Changyong Oh",
                "Jakub Tomczak",
                "Efstratios Gavves",
                "Max Welling"
            ],
            "title": "Combinatorial bayesian optimization using the graph cartesian product",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "A O\u2019Hagan"
            ],
            "title": "On curve fitting and optimal design for regression",
            "venue": "J. Royal Stat. Soc. B,",
            "year": 1978
        },
        {
            "authors": [
                "Carl Edward Rasmussen",
                "Christopher KI Williams"
            ],
            "title": "Gaussian processes for machine learning, volume",
            "year": 2006
        },
        {
            "authors": [
                "Tim G.J. Rudner",
                "Freddie Bickford Smith",
                "Qixuan Feng",
                "Yee Whye Teh",
                "Yarin Gal"
            ],
            "title": "Continual Learning via Sequential Function-Space Variational Inference",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR,",
            "year": 2022
        },
        {
            "authors": [
                "Jasper Snoek",
                "Hugo Larochelle",
                "Ryan P Adams"
            ],
            "title": "Practical bayesian optimization of machine learning algorithms",
            "venue": "Advances in neural information processing systems,",
            "year": 2012
        },
        {
            "authors": [
                "Jasper Snoek",
                "Oren Rippel",
                "Kevin Swersky",
                "Ryan Kiros",
                "Nadathur Satish",
                "Narayanan Sundaram",
                "Mostofa Patwary",
                "Mr Prabhat",
                "Ryan Adams"
            ],
            "title": "Scalable bayesian optimization using deep neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Dmitry Sorokin",
                "Alexander Ulanov",
                "Ekaterina Sazhina",
                "Alexander Lvovsky"
            ],
            "title": "Interferobot: aligning an optical interferometer by a reinforcement learning agent",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jost Tobias Springenberg",
                "Aaron Klein",
                "Stefan Falkner",
                "Frank Hutter"
            ],
            "title": "Bayesian optimization with robust bayesian neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Samuel Stanton",
                "Pavel Izmailov",
                "Polina Kirichenko",
                "Alexander A Alemi",
                "Andrew G Wilson"
            ],
            "title": "Does knowledge distillation really work",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Samuel Stanton",
                "Wesley Maddox",
                "Nate Gruver",
                "Phillip Maffettone",
                "Emily Delaney",
                "Peyton Greenside",
                "Andrew Gordon Wilson"
            ],
            "title": "Accelerating Bayesian optimization for biological sequence design with denoising autoencoders",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Swersky",
                "Jasper Snoek",
                "Ryan P Adams"
            ],
            "title": "Multi-task bayesian optimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2013
        },
        {
            "authors": [
                "Dustin Tran",
                "Jeremiah Liu",
                "Michael W. Dusenberry",
                "Du Phan",
                "Mark Collier",
                "Jie Ren",
                "Kehang Han",
                "Zi Wang",
                "Zelda Mariet",
                "Huiyi Hu",
                "Neil Band",
                "Tim G.J. Rudner",
                "Karan Singhal",
                "Zachary Nado",
                "Rodolphe Jenatton",
                "Nithum Thain",
                "Honglin Yuan",
                "Kelly Buchanan",
                "Kevin Murphy",
                "D. Sculley",
                "Yarin Gal",
                "Zoubin Ghahramani",
                "Jasper Snoek",
                "Balaji Lakshminarayanan"
            ],
            "title": "Plex: Towards Reliability Using Pretrained Large Model Extensions",
            "year": 2022
        },
        {
            "authors": [
                "Boqian Wang",
                "Jiacheng Cai",
                "Chuangui Liu",
                "Jian Yang",
                "Xianting Ding"
            ],
            "title": "Harnessing a novel machine-learning-assisted evolutionary algorithm to co-optimize three characteristics of an electrospun oil sorbent",
            "venue": "ACS Applied Materials & Interfaces,",
            "year": 2020
        },
        {
            "authors": [
                "Zi Wang",
                "Stefanie Jegelka"
            ],
            "title": "Max-value entropy search for efficient bayesian optimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Max Welling",
                "Yee W Teh"
            ],
            "title": "Bayesian learning via stochastic gradient langevin dynamics",
            "venue": "In Proceedings of the 28th international conference on machine learning",
            "year": 2011
        },
        {
            "authors": [
                "Christopher Williams"
            ],
            "title": "Computing with infinite networks",
            "venue": "Advances in neural information processing systems,",
            "year": 1996
        },
        {
            "authors": [
                "Andrew G Wilson",
                "Pavel Izmailov"
            ],
            "title": "Bayesian deep learning and a probabilistic perspective of generalization",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Andrew Gordon Wilson",
                "Zhiting Hu",
                "Ruslan Salakhutdinov",
                "Eric P Xing"
            ],
            "title": "Deep kernel learning",
            "venue": "In Artificial intelligence and statistics,",
            "year": 2016
        },
        {
            "authors": [
                "Jian Wu",
                "Matthias Poloczek",
                "Andrew G Wilson",
                "Peter Frazier"
            ],
            "title": "Bayesian optimization with gradients",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Dongruo Zhou",
                "Lihong Li",
                "Quanquan Gu"
            ],
            "title": "Neural contextual bandits with ucb-based exploration",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Bayesian optimization (O\u2019Hagan, 1978) is a distinctly compelling success story of Bayesian inference. In Bayesian optimization, we place a prior over the objective we wish to optimize, and use a surrogate model to infer a posterior predictive distribution over the values of the objective at all feasible points in space. We then combine this predictive distribution with an acquisition function that trades-off exploration (moving to regions of high uncertainty) and exploitation (moving to regions with a high expected value, for maximization). The resulting approach converges quickly to a global optimum, with strong performance in many expensive black-box settings ranging from experimental design, to learning parameters for simulators, to hyperparameter tuning (Frazier, 2018; Garnett, 2023). While many acquisition functions have been proposed for Bayesian optimization (e.g. Frazier et al., 2008; Wang and Jegelka, 2017), Gaussian processes (GPs) with standard Mat\u00e9rn or RBF kernels are almost exclusively used as the surrogate model for the objective, without checking whether other alternatives would be more appropriate, despite the fundamental role that the surrogate model plays in Bayesian optimization. Thus, despite promising advances in Bayesian optimization research, there is an elephant in the room: should we be considering other surrogate models? It has become particularly timely to evaluate Bayesian neural network (BNN) surrogates as alternatives to Gaussian processes with standard kernels: In recent years, there has been extraordinary progress in making BNNs practical (e.g. Daxberger et al., 2021; Khan and Rue, 2021; Rudner et al., 2022; Tran et al., 2022; Wilson and Izmailov, 2020). Moreover, BNNs can flexibly represent the non-stationary behavior typical of optimization objectives, discover similarity measures as part of representation learning which is useful for higher dimensional inputs, and naturally handle multi-output objectives. In parallel, Monte-Carlo acquisition functions (Balandat et al., 2020) have been developed which only require posterior samples, significantly lowering the barrier to using non-GP surrogates that do not provide closed-form predictive distributions.\nIn this paper, we exhaustively evaluate Bayesian neural networks as surrogate models for Bayesian optimization. We consider conventional fully stochastic multilayer BNNs with a variety of approximate inference procedures, ranging from high-quality full-batch Hamiltonian Monte Carlo (Izmailov et al., 2021; Neal, 1996; 2010), to stochastic gradient Markov Chain Monte Carlo (Chen et al., 2014), to heuristics such as deep ensembles (Lakshminarayanan et al., 2017). We also consider infinitewidth BNNs (Lee et al., 2017; Neal and Neal, 1996), corresponding to GPs with fixed non-stationary kernels derived from a neural network architecture, as well as partially Bayesian last-layer deep kernel learning methods (Wilson et al., 2016). This particularly wide range of neural network-based surrogates allows us to evaluate the role of representation learning, non-stationarity, and stochasticity in modeling Bayesian optimization objectives. Moreover, given that so much is unknown about the role of the surrogate model, we believe it is particularly valuable not to have a \u201chorse in the race\u201d, such as a special BNN model particularly designed for Bayesian optimization, in order to conduct an unbiased scientific study where any outcome is highly informative. We also extensively study a variety of synthetic and real-world objectives\u2014with a wide range of input space dimensionalities, single- and multi-dimensional output spaces, and both discrete and continuous inputs, and non-stationarities. Our study provides several key findings: (1) while stochasticity is often prized in Bayesian optimization (Garnett, 2023; Snoek et al., 2012), due to the small data sizes in Bayesian optimization, fully stochastic BNNs do not consistently dominate deep kernel learning, which is not stochastic about network parameters; (2) of the fully stochastic BNNs, HMC generally works the best for Bayesian optimization, and deep ensembles work surprisingly poorly, given their success in other settings; (3) on standard benchmarks, standard GPs are relatively competitive, due to their strong priors and simple exact inference procedures; (4) there is no single method that dominates across most problems, demonstrating that there is significant variability across Bayesian optimization objectives, where tailoring the surrogate to the objective has particular value; (5) infinite-width BNNs are surprisingly effective at high-dimensional optimization. These results suggest that the non-Euclidean similarity metrics constructed from neural networks are valuable for high-dimensional Bayesian optimization, but representation learning (provided by DKL and finite-width BNNs) is not as valuable as a strong prior derived from a neural network architecture (provided by the infinite-width BNN). This study also serves as an evaluation framework for considering alternative surrogate models for Bayesian optimization. Our code will be released on GitHub."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "There is a large body of literature on improving the performance of Bayesian optimization. However, an overwhelming majority of this research only considers Gaussian process surrogate models, focusing on developing new acquisition functions (e.g. Frazier et al., 2008; Wang and Jegelka, 2017), additive covariance functions (Gardner et al., 2017; Kandasamy et al., 2015), using gradient information (Wu et al., 2017), multi-objectives (Swersky et al., 2013), trust region methods that use input partitioning for higher dimensional and non-stationary data (Eriksson et al., 2019), and covariance functions for discrete inputs and strings (Moss et al., 2020). For a comprehensive review, see Garnett (2023). There has been some prior work focusing on other types of surrogate models for Bayesian optimization, such as random forests (Hutter et al., 2011) and tree-structured Parzen estimators (Bergstra et al., 2013). Snoek et al. (2015) apply a Bayesian linear regression model to the last layer of a deterministic neural network, which can be helpful for the added number of objective queries associated with higher dimensional inputs. Deep kernel learning (Wilson et al., 2016), which transforms the inputs of a Gaussian process kernel with a deterministic neural network, may also be used with Bayesian optimization, especially in specialized applications like protein engineering (Stanton et al., 2022). The linearized-Laplace approximation to produce a linear model from a neural network has also recently been applied to Bayesian optimization (Kristiadi et al., 2023). Neural networks have also been used for Bayesian optimization in the the contextual bandit setting, using the neural tangent kernel for exploration (Zhou et al., 2020; Dai et al., 2022; Lisicki et al., 2022). Despite the recent practical advances in developing Bayesian neural networks for many tasks (e.g. Wilson and Izmailov, 2020), and recent Monte-Carlo acquisition functions which make it easier to use surrogates like BNNs that do not provide closed-form predictive distributions (Balandat et al., 2020), there is a vanishingly small body of work that considers BNNs as surrogates for Bayesian optimization. This is surprising, since we would indeed expect BNNs to have properties naturally\naligned with Bayesian optimization, such as the ability to learn non-stationary functions without explicit modeling interventions and gracefully handle high-dimensional input and output spaces. The possible first attempt to use a Bayesian neural network surrogate for Bayesian optimization (Springenberg et al., 2016) came before most of these advances in BNN research, and used a form of stochastic gradient Hamiltonian Monte Carlo (SGHMC) (Chen et al., 2014) for inference. Like Snoek et al. (2015), the focus was largely on scalability advantages over Gaussian processes; however, the reported performance gains were marginal, and puzzling in that they were largest for a small number of objective function queries (where the neural net would not be able to learn a rich representation). Kim et al. (2021) used the same method for BNNs with Bayesian optimization, also with SGHMC, targeted at scientific problems with known structures and high dimensionality. In these applications, BNNs leverage auxiliary information, domain knowledge, and intermediate data, which would not typically be available in many Bayesian optimization problems. Foldager et al. (2023) also studied BNN surrogates through mean-field BNNs and deep ensembles, and M\u00fcller et al. (2023) used neural network surrogates which approximate the posterior through in-context learning. However, the innate differences between approximate inference methods for BNNs have not been explored. Our paper provides several key contributions in the context of this prior work, where standard GP surrogates are nearly always used with Bayesian optimization. While finite-width BNN surrogates have been attempted, they are often applied in specialized settings without an effort to understand their properties. Little is known about whether BNNs could generally be used as an alternative to GPs for Bayesian optimization, especially in light of more recent general advances in BNN research. This is the first paper to provide a comprehensive study of BNN surrogates, considering a range of model types, experimental settings, and types of approximate inference. We test the utility of BNNs in a variety of contexts, exploring their behavior as we change the dimensionality of the problem and the number of objectives, investigating their performance on non-stationary functions, and also incorporating problems with a mix of discrete and continuous input parameters. Moreover, we are the first to study infinite BNN models in Bayesian optimization, and to consider the role of stochasticity and representation learning in neural network based Bayesian optimization surrogates. Finally, rather than champion a specific approach, we provide an objective assessment, also highlighting the benefits of GP surrogates for general Bayesian optimization problems."
        },
        {
            "heading": "3 SURROGATE MODELS",
            "text": "We consider a wide variety of surrogate models, separately understanding the role of stochasticity, representation learning, and strong priors in Bayesian optimization surrogates. We provide additional information about these surrogates and background about Bayesian optimization in Appendix A.\nGaussian Processes. Throughout our experiments, when we refer to Gaussian processes, we always mean standard Gaussian processes, with the Mat\u00e9rn-5/2 kernel that is typically used in Bayesian optimization (Snoek et al., 2012). These Gaussian processes have the advantage of simple exact inference procedures, strong priors, and few hyperparameters, such as length-scale, which controls rate of variability. On the other hand, these models are stationary, meaning the covariance function is translation invariant and models the objective as having similar properties (such as rate of variation) at different points in input space. They also provide a similarity metric for data points based on simple Euclidean distance of inputs, which is often not suitable for higher dimensional input spaces.\nFully Stochastic Finite-Width Bayesian Neural Networks. These models treat all of the parameters of the neural network as random variables, which necessitates approximate inference. We consider three different mechanisms of approximate inference: (1) Hamiltonian Monte Carlo (HMC), an MCMC procedure which is the computationally expensive gold standard (Izmailov et al., 2021; Neal, 1996; 2010); (2) Stochastic Gradient Hamiltonian Monte Carlo (SGHMC), a scalable MCMC approach that works with mini-batches (Chen et al., 2014); (3) Deep Ensembles, considered a practically effective heuristic that ensembles a neural network retrained multiple times, and has been shown to approximate fully Bayesian inference (Izmailov et al., 2021; Lakshminarayanan et al., 2017). These approaches are fully stochastic, and can do representation learning, meaning that they can learn appropriate distance metrics for the data as well as particular types of non-stationarity.\nDeep Kernel Learning. Deep kernel learning (DKL) (Wilson et al., 2016) is a hybrid Bayesian deep learning model, which layers a GP on top of a neural network feature extractor. This approach can do non-Euclidean representation learning, handle non-stationarity, and also uses exact inference. However, it is only stochastic about the last layer.\nLinearized Laplace Approximation. The linearized-Laplace approximation (LLA) is a deterministic approximate inference method that uses the Laplace approximation (MacKay, 1992; Immer et al., 2021) to produce a linear model from a neural network, and has recently been considered for Bayesian optimization in concurrent work (Kristiadi et al., 2023).\nInfinite-Width Bayesian Neural Networks. Infinite-width neural networks (I-BNNs) refer to the behavior of neural networks as the number of nodes per hidden layer increases to infinity. Neal (1996) famously showed with a central limit theorem argument that a BNN with a single infinite-width hidden layer converges to a GP with a neural network covariance function, and this result has been extended to deep neural networks by Lee et al. (2017). I-BNNs are fully stochastic and very different from standard GPs, as they can handle non-stationarity and provide a non-Euclidean notion of distance inspired by a neural network. However, it cannot do representation learning and instead has a fixed covariance function that provides a relatively strong prior."
        },
        {
            "heading": "3.1 ROLE OF ARCHITECTURE",
            "text": "We conduct a sensitivity study into the role of key design choices for BNN surrogates. We highlight results for HMC, as it is the gold standard for approximate inference in BNNs (Izmailov et al., 2021). Gaussian processes involve relatively few design choices\u2014essentially only the covariance function, which is often set to the RBF or Mat\u00e9rn kernel, and we are also able to have an intuitive understanding of what the induced distributions over functions look. In contrast, with BNNs, we must consider the architecture, the prior over parameters, and the approximate inference procedure. It is also less clear how different modeling choices in BNNs affect the inferred posterior predictive distributions. To illustrate these differences for varying network, prior, and variance parameters, we plot the inferred posterior predictive distributions over functions for different network widths and depths, the activation functions, and likelihood and variance parameters in Figure 1, and we evaluate the performance under different model choices for three synthetic data problems in Figure 2. We focus on fully-connected multi-layer perceptrons for this study: while certain architectures have powerful inductive biases\nfor vision and language tasks, generic regression tasks such as Bayesian optimization tend to be well-suited for fully-connected multi-layer perceptrons, which have relatively mild inductive biases and make loose assumptions about the structure of the function.\nModel Hyperparameters. We consider isotropic priors over the neural network parameters with zero mean and variance parameters 0.1, 1, and 10. Similarly, we consider Gaussian likelihood functions with variance parameters 0.1, 1, and 10. The corresponding posterior predictive distributions for full-batch HMC are shown in Figure 1a. As would be expected, an increase in the likelihood variance results in a poor fit of the data and virtually no posterior collapse. In contrast, increasing the prior variance results in a higher predictive variance between data points with a good fit to the data points, whereas a prior variance that is too small leads to over-regularization and uncertainty collapse. As shown in Figure 2a and Figure 2c, lower likelihood variance parameters and larger prior variance parameters tended to perform best across three synthetic data experiments.\nNetwork Width and Depth. To better understand the effects of the network size on inference, we explore the performance when varying the number of hidden layers and the number of parameters per layer, each corresponding to an increase in model complexity. In Figure 1c, we see that there is a significant increase in uncertainty as we increase the number of hidden layers. Figure 1d also shows an increase in uncertainty as we increase the width of the network, where a smaller width leads to function draws that are much flatter than function draws from a larger width. However, the best size to choose seems to be problem-dependent, as shown in Figure 2b and Figure 2d.\nActivation Function. The choice of activation function in a neural network determines important characteristics of the function class, such as smoothness or periodicity. The impact of the activation function can be seen in Appendix D.1, with function draws from the ReLU BNN appearing more jagged and function draws from the tanh BNN more closely resembling the draws from a GP with a Squared Exponential or Mat\u00e9rn 5/2 covariance function."
        },
        {
            "heading": "4 EMPIRICAL EVALUATION",
            "text": "We provide an extensive empirical evaluation of BNN surrogates for Bayesian optimization. We first assess how BNNs compare to GPs in relatively simple and well-understood settings through commonly used synthetic objective functions, and we perform an empirical comparison between GPs and different types of BNNs (HMC, SGHMC, LLA, ENSEMBLE, I-BNN, and DKL). To further ascertain whether BNNs may be a suitable alternative to GPs in real-world Bayesian optimization problems, we study six real-world datasets used in prior work on Bayesian optimization with GP\nsurrogates (Dreifuerst et al., 2021; Eriksson et al., 2019; Maddox et al., 2021; Oh et al., 2019; Wang et al., 2020). We also provide evidence that the performance of BNNs could be further improved with a careful selection of network hyperparameters. We conclude our evaluation with a case study of Bayesian optimization tasks where simple Gaussian process models may fail but BNN models would be expected to prevail. To this end, we design a set of experiments to assess the performance of GPs and BNNs as a function of the input dimensionality and in settings where the objective function is non-stationary."
        },
        {
            "heading": "4.1 SYNTHETIC BENCHMARKS",
            "text": "We evaluate BNN and GP surrogates on a variety of synthetic benchmarks, and we choose problems with a wide span of input dimensions to understand how the performance differs as we increase the dimensionality of the data. We also select problems that vary in the number of objectives to compare the performance of the different surrogate models. Detailed problem descriptions can be found in Appendix B.1, and we include the experiment setup in Appendix C. We use Monte-Carlo based Expected Improvement (Balandat et al., 2020) as our acquisition function for all problems. As shown in Figure 3, we find BNN surrogate models to show promising results; however, the specific performance of different BNNs varies considerably per problem. DKL matches GPs in Branin and BraninCurrin, but seems to perform poorly on highly non-stationary problems such as Ackley. I-BNNs also seem to slightly underperform compared to GPs on these synthetic problems, many of which have a small number of input dimensions. In contrast, we find finite-width BNNs using full HMC to be comparable to GPs, performing similarly in many of the experiments, slightly underperforming in Hartmann and DTLZ5, and outperforming GPs in the 10-dimensional Ackley experiment. However, this behavior is not generalizable to all approximate inference methods: the performance of SGHMC and LLA vary significantly per problem, matching the performance of HMC and GPs in some experiments while failing to approach the maximum value in others. Deep ensembles also consistently underperform the other surrogate models, plateauing at noticeably lower objective values on problems like BraninCurrin and DTLZ1. This result is surprising, since ensembles are often seen as an effective way to measure uncertainty (Appendix D.3). We provide additional experiments studying how the performance of surrogates changes as we vary their hyperparameters in Appendix D, and we find that these hyperparameters generally have minimal effects on the performance."
        },
        {
            "heading": "4.2 REAL-WORLD BENCHMARKS",
            "text": "To provide an evaluation of BNN surrogates in more realistic optimization problems, we consider a diverse selection of real-world applications which span a variety of domains, such as solving\ndifferential equations and monitoring cellular network coverage (Dreifuerst et al., 2021; Eriksson et al., 2019; Maddox et al., 2021; Oh et al., 2019; Wang et al., 2020). Many of these problems, such as the development of materials to clean oil spills, have consequential applications; however, these objectives are often multi-modal and are difficult to optimize globally. Additionally, unlike the synthetic benchmarks, many real-world applications consist of input data with ordinal or categorical values, which may be difficult for GPs to handle. Several of the problems also require multiple objectives to be optimized. Detailed problem descriptions are provided in Appendix B.2. We share the results of our experiments in Figure 4, and details about the experiment setup can be found in Appendix C. The results are mixed: BNNs are able to significantly outperform GPs in the Pest Control dataset, while GPs find the maximum reward in the Cell Coverage and Lunar Lander experiments. The Pest Control, Cell Coverage, and Oil Spill Sorbent experiments all include discrete input parameters, and there seems to be a slight trend of GP and I-BNNs performing well, and SGHMC and ENSEMBLEs performing more poorly. Similar to the findings from the synthetic benchmarks, we see that the different approximate inference methods for finite-width BNNs lead to significantly different Bayesian optimization performance, with HMC generally finding higher rewards compared to SGHMC, LLA, and deep ensembles. Additionally, it appears that GPs perform well in the two multi-objective problems, although that may not be generalizable to additional multi-objective problems and may be more related to the curvature of the specific problem space."
        },
        {
            "heading": "4.3 LIMITATIONS OF GP SURROGATE MODELS",
            "text": "Although popular, GPs suffer from well-known limitations that directly impact their usefulness as surrogate models. To contrast BNN and GP surrogates, we explore two failure modes of GPs and demonstrate that BNN surrogate models can overcome these issues.\nNon-Stationary Objective Functions. To use GPs, we must specify a kernel function class that governs the covariance structure over data points. We typically constrain models to have kernels of the form k(x,x\u2032) = k(\u2225x\u2212x\u2032\u2225) because it is easier to describe the functional form and learn the hyperparameters of the kernel. However, because the covariance between two values only depends on their distance and not on the values themselves, this setup assumes the function is stationary and has similar mean and smoothness throughout the input space. Unfortunately, this assumption does not hold true in many real-world settings. For example, in the common Bayesian optimization application of choosing hyperparameters of a neural network, the true loss function landscape may have vastly different behavior in one part of the input space compared to another. BNN surrogates, in contrast to GP surrogates, are able to model non-stationary functions without similar constraints.\nIn Appendix D.9, we show the performance of BNN and GP surrogate models for a non-stationary objective function. Because the GP assumes that the behavior of the function is the same throughout the input domain, it cannot accurately model the input-dependent variation and underfits around the true optimum. In contrast, BNN surrogates can learn the non-stationarity of the function.\nHigh-Dimensional Input Spaces. Due to the curse of dimensionality, GPs do not scale well to highdimensional input spaces without careful human intervention. Common covariance functions may fail to faithfully represent high-dimensional input data, making the design of custom-tailored kernel functions necessary. In contrast, neural networks are well-suited for modeling high-dimensional input data (Krizhevsky et al., 2012). To measure the effect of dimensionality on the performance of GPs and BNNs, we use synthetic test functions provided by high-dimensional polynomial functions and function draws from neural networks. We also construct a realistic high-dimensional problem by using Bayesian optimization to set the parameters of a neural network in the context of knowledge distillation. Knowledge distillation refers to the act of \u201cdistilling\u201d information from a larger teacher model to a smaller student model by matching model outputs (Hinton et al., 2015), and it is known to be a difficult optimization problem (Stanton et al., 2021). For full descriptions, see Appendix B. We share the results of our findings in Figure 5 and Appendix D.4. We see I-BNNs clearly stand out in these high-dimensional settings. The I-BNN has several appealing features in this setting: (1) it provides a non-Euclidean and non-stationary similarity metric, which can be particularly valuable in high-dimensions; (2) it does not have any hyperparameters for learning, and thus is not \u201cdata hungry\u201d\u2014which is especially important in high dimensional problems with small data sizes, since these settings provide relatively little information for representation learning. Additionally, we find that other BNN surrogate models also outperform GPs across the high-dimensional problems, providing a compelling motivation for BNNs as surrogate models for Bayesian optimization.\n4.4 MODEL RANKINGS\nTo further interpret our findings, we visualize the model performance in Figure 6. We determine the relative performance of each model by ri. For each trial, let rh denote the highest maximum reward found by any model and rl denote the lowest. The score of model with maximum reward is (ri \u2212 rh)/(rh \u2212 rl). We plot the distribution of scores in Figure 6. Across all experiments, GPs and I-BNNs perform well, while SGHMC and deep ensembles perform more poorly. However, in high-dimensional settings, I-BNNs outperform all other models across every trial of every experiment, leading to scores of only 1.0s. GPs, by contrast, perform poorly in this setting, finding lower rewards."
        },
        {
            "heading": "4.5 ADDITIONAL PRACTICAL CONSIDERATIONS",
            "text": "Network Architecture. To further understand the impact of network architecture on the performance of BNNs, we conduct an extensive neural architecture search over a selection of the benchmark problems. While a thorough search is often impractical for Bayesian optimization problems, we use this experiment to investigate the flexibility of BNNs. We find that the performance of BNNs can significantly increase for problems such as Pest Control, demonstrating the usefulness of BNN for Bayesian optimization when the architecture is well-suited for a given problem. We provide additional details and showcase the performance of BNNs with different architectures in Appendix D.1. Quality of Mean and Uncertainty Estimates. To better understand the quality of the mean and uncertainty estimates of different surrogate models, we conducted ablation studies for which we created hybrid models that combine the mean estimate of one surrogate with the uncertainty estimate of another. By comparing the performance of a given surrogate model to hybrid models that have the same mean as the non-hybrid model but the uncertainty estimates of a different surrogate model (or the other way around), we find that HMC and I-BNN surrogates often have better mean estimates compared to GPs while GPs have better uncertainty estimates, which we detail in Appendix D.2. Large Number of Function Queries. We investigate the effect of performing a larger number of function queries on performance when using BNN and GP surrogate models. We find in this setting that (1) I-BNNs remain competitive; (2) BNNs perform well, leveraging the data for representation learning; (3) the performance of deep ensembles is greatly improved, consistent with the explanation that their poor performance on many tasks is due to limited data. We share details in Appendix D.10. Runtime. In real-world Bayesian optimization problems, querying the objective function is typically significantly more time-consuming than (re-)training a surrogate model after new data has been obtained, making the quality of the surrogate model paramount and the time needed for (re-)training the surrogate model of secondary interest. Nevertheless, given the varying training requirements of BNNs and GPs, we provide wall-clock times of all surrogate models across our experiments in Appendix D.11. Notably, I-BNNs are particularly competitive both in performance and runtime."
        },
        {
            "heading": "4.6 REVISITING STANDARD ASSUMPTIONS",
            "text": "While Gaussian processes are typically used as the default surrogate model, there are many design choices, such the choice of kernel and the method of hyperparameter selection, which play a crucial role. It is commonly prescribed to use the Mat\u00e9rn kernel and perform hyperparameter marginalization rather than optimization (e.g. Eriksson et al., 2019; Snoek et al., 2012). In Figure A.15 and Figure A.14, we compare the performance of different specifications of GPs across our diverse benchmarks. In contrast to conventional wisdom, we do not find that using the Mat\u00e9rn kernel and hyperparameter marginalization significantly improves the performance of GPs in general; in fact, there are many problems where the RBF kernel or hyperparameter optimization are preferable."
        },
        {
            "heading": "5 DISCUSSION",
            "text": "While Bayesian optimization research has made significant progress over the last few decades (Garnett, 2023), the surrogate model remains a crucial yet underexplored design choice, with standard GPs being viewed as default surrogates. Given recent advances in BNNs and related approaches, it is, therefore, particularly timely to consider neural network surrogates for Bayesian optimization. Although we found that BNNs are competitive with standard GPs for Bayesian optimization, our findings that DKL is competitive with BNNs, and that infinite-width BNNs show promising performance in general\u2014but especially for higher dimensional settings\u2014raise the question of whether a fully stochastic treatment of finite BNN surrogates is typically necessary for Bayesian optimization. Since infinite-width BNNs do not involve learning many hyperparameters and do not require approximate inference, they are well-positioned to become a de facto standard surrogate for Bayesian optimization. Finally, we found that, on the one hand, no single surrogate model consistently outperforms all other surrogate models across all Bayesian optimization problems considered in our evaluation. This finding supports the use of simple models with strong but generic assumptions\u2014such as standard GP models\u2014for many real-world problems. On the other hand, the standard benchmarking problems were designed with only GP surrogates in mind, and we found that standard GPs underperform BNNs and other neural network-based surrogates on challenging high-dimensional problems."
        },
        {
            "heading": "6 REPRODUCIBILITY",
            "text": "We provide the code needed to reproduce all experiments in the supplementary material attached, and we also provide experiment details for all of our results in Appendix C. We also provide an anonymized GitHub repository for ease of viewing: https://anonymous.4open.science/ r/bnn-bo-BC93. We are also committed to open-sourcing our project, and part of the contribution of our work includes our carefully crafted codebase for evaluating and comparing new surrogate models. As such, we prioritized reproducibility, and all experiments in the paper are readily available to run. Furthermore, our codebase is also easily extensible, and there is ample documentation for users to be able to benchmark additional test functions or surrogate models."
        },
        {
            "heading": "Appendix",
            "text": "A Study of Bayesian Neural Network Surrogates\nfor Bayesian Optimization"
        },
        {
            "heading": "TABLE OF CONTENTS",
            "text": ""
        },
        {
            "heading": "A Background 15",
            "text": "A.1 Bayesian Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.2 Gaussian Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.3 Bayesian Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15"
        },
        {
            "heading": "B Problem Descriptions 18",
            "text": "B.1 Synthetic Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 B.2 Real-World Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 B.3 High-Dimensional Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19"
        },
        {
            "heading": "C Experiment Details 20",
            "text": "C.1 General Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 C.2 Synthetic Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 C.3 Real-World Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 C.4 Neural Architecture Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 C.5 High-Dimensional Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . 21"
        },
        {
            "heading": "D Further Empirical Results 23",
            "text": "D.1 BNN Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 D.2 Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 D.3 Deep Ensembles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 D.4 Infinite-Width BNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 D.5 Hamiltonian Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 D.6 Gaussian Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 D.7 Deep Kernel Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 D.8 Acquisition Batch Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 D.9 Limitations of Gaussian Process Surrogate Models . . . . . . . . . . . . . . . . . 39 D.10 Large Number of Function Queries . . . . . . . . . . . . . . . . . . . . . . . . . . 41 D.11 Runtime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 D.12 Role of Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43"
        },
        {
            "heading": "APPENDIX A BACKGROUND",
            "text": ""
        },
        {
            "heading": "A.1 BAYESIAN OPTIMIZATION",
            "text": "Bayesian optimization is a global optimization method commonly used for black-box functions which are costly to evaluate. Specifically, we formulate the optimization as a maximization problem where we want to solve x\u2217 = argmaxx f(x), where x represents all possible inputs to objective function f , using a limited number of function queries. In Bayesian optimization, we iteratively select new function evaluations using the following procedure: (1) use a surrogate model to find p(f |D), where D = {xi, yi}ti=1 and yi is a noisy observation of f(xi); (2) select the next xt+1 to query by maximizing an acquisition function, which is an inexpensive computation that typically uses the mean and variance of the posterior p(f |D) to compute how useful a function evaluation would be; and (3) query the function at value xt+1 to observe yt+1 \u223c N (f(xt+1), \u03c32obs) and add the result to the dataset D := D \u222a {xt+1, yt+1}. Historically, Bayesian optimization has seen wide success in a range of fields including drug design, engineering challenges, and hyperparameter optimization. A more recent and growing area of interest is in higher dimensional settings, where the objective, f(x) is multi-dimensional consisting of several tasks we wish to optimize jointly. In multi-objective Bayesian optimization, we want to find the input x\u2217 that maximizes k related objective functions F = {f1(x), \u00b7 \u00b7 \u00b7 , fk(x)}. There is typically no single solution that maximizes all K objectives simultaneously. Therefore, performance is typically compared using Pareto dominance, where one solution Pareto-dominates another if it performs equally well or better on all objectives. Pareto dominance can be measured using hypervolume, which indicates how much of a bounded region is dominated by a solution. Core to the Bayesian optimization procedure is the use of an acquisition function to select the next candidate points given a set of observations xi, f(xi) N i=1, and a surrogate model trained to fit these observations. Acquisition functions are functions of the predictive distribution of the surrogate model, and seek candidate points according to criteria such as the probability of improvement or the expected improvement over the currently found minimizer of the objective. The chosen surrogate model will also have a signficant impact on the performance of Bayesian optimization . Since the true function f may take a variety of forms, the surrogate model should be flexible enough to accurately represent it. Additionally, the uncertainty estimates that the surrogate model provides should be well-calibrated to ensure that the input that maximizes f can be found in a limited number of iterations."
        },
        {
            "heading": "A.2 GAUSSIAN PROCESSES",
            "text": "Gaussian processes (GPs) (Rasmussen and Williams, 2006) are distributions over functions entirely specified by a mean function \u00b5(x) and a covariance function k(x,x\u2032). For regression tasks with GPs, we assume y(x) = f(x) + \u03f5 with f(x) \u223c GP(\u00b5(x), k(x,x\u2032)) and \u03f5 \u223c N (0, \u03c32), where \u03c32 is the likelihood variance. The functions \u00b5(x) and k(x,x\u2032) are the mean and kernel functions of the GP that govern the functional properties and generalization performance of the model. In practice, we tune the hyperparameters of \u00b5(x) and k(x,x\u2032) on the training data to optimize the marginal likelihood of the GP, which maximizes the probability that the GP model will have generated the data. By the definition of a GP, for any finite collection of inputs x = [xtrain,xtest], f(x), and thus y(x) are jointly normal. Therefore, we can apply the rules of conditioning partitioned multivariate normals to form a posterior distribution p (f(xtest) | y(xtrain)), which will also be Gaussian. In the context of Bayesian optimization, this simple conditioning procedure means that given some set of points at which we have already queried the objective function, we can quickly generate a posterior over potential candidate points and, with the help of an acquisition function, select the next points to query the objective."
        },
        {
            "heading": "A.3 BAYESIAN NEURAL NETWORKS",
            "text": "Bayesian neural networks (BNNs) are neural networks with stochastic parameters for which a posterior distribution is inferred using Bayesian inference. More specifically, for regression tasks, we can specify a Gaussian observation model, p(y(x) | x,\u03b8) = N (h(x |\u03b8), \u03c32), where y(x) represents a noisy observed value and h(x |\u03b8) represents the output of a neural network with parameter realization \u03b8 for input x. For BNNs, a prior distribution is placed over the stochastic parameters \u03b8 of the neural network, and the corresponding posterior distribution is given by p(\u03b8 | D) = p(D |\u03b8)p(\u03b8)/p(D). This posterior distribution can then be used in combination\nwith the acquisition function for Bayesian optimization to select the next set of candidate points to query. There are many different choices to consider when using Bayesian neural networks, such the inference method used to compute the posterior distribution, the architecture of the neural network, and the selection of which parameters are stochastic. In this work, we study the performance of five different types of BNNs with varying inference methods and stochasticity."
        },
        {
            "heading": "A.3.1 FULLY-STOCHASTIC FINITE-WIDTH NEURAL NETWORKS",
            "text": "For fully-stochastic Bayesian neural networks, every parameter of the neural network is stochastic and has a prior placed over it. Exact inference over these stochastic network parameters for fully-stochastic finite-width BNNs is analytically intractable, since neural networks are non-linear in their parameters. To enable Bayesian inference in this setting, approximate inference methods can be used to find approximations to the exact posterior p(\u03b8 | D). In this work, we focus on four types of approximate inference: Hamiltonian Monte Carlo (HMC; Neal (2010)), stochastic-gradient HMC (Chen et al., 2014), linearized Laplace approximations (Immer et al., 2021), and deep ensembles of deterministic neural networks (Lakshminarayanan et al., 2017).\nHamiltonian Monte Carlo. Hamiltonian Monte Carlo is a Markov Chain Monte Carlo method that produces asymptotically exact samples from the posterior distribution (Neal, 2010) and is commonly referred to as the \u201cgold standard\u201d for inference in BNNs. At test time, HMC approximates the integral p(y(xtest) | D) \u2248 1M \u2211M i=1 p(y(xtest) |\u03b8i) using samples \u03b8i drawn from the posterior over parameters. Full-batch HMC provides the most accurate approximation of the posterior distribution but is computationally expensive and in practice limited to models with only a few hundred thousand parameters (Izmailov et al., 2021). Full-batch HMC allows us to study the performance of BNNs in Bayesian optimization without many of the confounding factors of inaccurate approximations of the predictive distribution.\nStochastic Gradient Hamiltonian Monte Carlo. Stochastic gradient methods (Welling and Teh, 2011; Hoffman et al., 2013) are commonly used as an inexpensive approach to sampling from the posterior distribution. Unlike full-batch HMC, which computes the gradients over the full dataset, stochastic gradient HMC instead samples a mini-batch to compute a noisy estimate of the gradient (Chen et al., 2014). While these methods may seem appealing when working with large models or datasets, they can result in inaccurate approximations and posterior predictive distribution with unfaithful uncertainty representations.\nDeep Ensembles. Deep ensembles are ensembles of several deterministic neural networks, each trained using maximum a posteriori estimation using a different random seed and initialization (and sometimes using different subsets of the training data). The ensemble components can be viewed as forming an efficient approximation to the posterior predictive distribution, by choosing parameters that represent typical points in the posterior and have high functional variability (Wilson and Izmailov, 2020). Deep ensembles are easy to implement in practice and have been shown to provide highly accurate predictions and a good predictive uncertainty estimate (Lakshminarayanan et al., 2017; Ovadia et al., 2019)."
        },
        {
            "heading": "A.3.2 FULLY-STOCHASTIC INFINITE-WIDTH NEURAL NETWORKS",
            "text": "Infinitely-wide neural networks (Neal and Neal, 1996) refer to the behavior of neural networks when the number of nodes in each internal layer increases to infinity. Each one of these nodes continues is stochastic and has a specific prior placed over its parameters.\nInfinite-width Neural Network. It is possible to specify a GP that has an exact equivalence to an infinitely-wide fully-connected Bayesian neural network (I-BNN) (Lee et al., 2017). For a single-layer network, the Central Limit Theorem can be used to show that in the limit of infinite width, each output of the network will be Gaussian distributed, and the exact distribution can be computed (Neal and Neal, 1996; Williams, 1996). This process can then be applied recursively for additional hidden layers (Lee et al., 2017). I-BNNs can be used in the same way as GPs to calculate the posterior distribution for infinitely-wide neural networks."
        },
        {
            "heading": "A.3.3 LINEARIZED FINITE-WIDTH NEURAL NETWORKS",
            "text": "Linearized Laplace Approximation. The linearized Laplace approximation (Immer et al., 2021) (LLA) is a deterministic approximate inference method which uses the Laplace approximation (MacKay, 1992; Immer et al., 2021) to produce a linear model from a neural network. LLAs can be represented as Gaussian processes with mean functions provided by the MAP predictive function, and covariance functions provided by the finite, empirical neural tangent kernel at the MAP estimate."
        },
        {
            "heading": "A.3.4 PARTIALLY-STOCHASTIC FINITE-WIDTH NEURAL NETWORKS",
            "text": "For partially-stochastic neural networks, we learn point estimates for a subset of the parameters, and we learn the full posterior distribution for the remaining parameters in the neural network.\nDeep Kernel Learning. Deep Kernel Learning (DKL) is a hybrid method that combines the flexibility of GPs with the expressivity of neural networks (Wilson et al., 2016). In DKL, a neural network gw(\u00b7) parameterized by weights w is used to transform inputs into intermediate values, where additional GP kernels can then be applied. Specifically, given inputs x and a base kernel kBASE(x,x\n\u2032), kDKL(x,x\u2032) = kBASE(gw(x), gw(x\u2032)). Unlike GPs with standard kernels which depend only on the distance between inputs and therefore assume that the mean and smoothness of the function are consistent throughout, DKL does not assume stationarity and is able to represent how the properties of the function change due to its neural network feature extractor."
        },
        {
            "heading": "APPENDIX B PROBLEM DESCRIPTIONS",
            "text": ""
        },
        {
            "heading": "B.1 SYNTHETIC DATASETS",
            "text": "In the single-objective case, Branin is a function with two-dimensional inputs with three global minima, Hartmann is a six-dimensional function with six local minima, and Ackley is a multidimensional function with many local minima which is commonly used to test optimization algorithms. We convert all problems to maximization problems, and the goal of Bayesian optimization in singleobjective settings is to find the maximum value of the objective function. For multi-objective Bayesian optimization benchmarks, BraninCurrin is a two-dimensional input and two-objective problem composed of the Branin and Currin functions, and DTLZ1 and DTLZ5 are multi-dimensional and multi-objective test functions which are used to measure an optimization algorithm\u2019s ability to converge to the Pareto-frontier. Here, the goal is to find an input corresponding to the multi-dimensional objective value with the maximum hypervolume from a problem-specific reference point."
        },
        {
            "heading": "B.2 REAL-WORLD DATASETS",
            "text": "Interferometer In this problem, the goal is to tune an optical interferometer through the alignment of two mirrors. We use the simulator in Sorokin et al. (2020) to replicate the Bayesian optimization problem in Maddox et al. (2021). Each mirror has a continuous x and y coordinate, which should be optimized so that the two mirrors reflect light with minimal amounts of interference. Lunar Lander Following Eriksson et al. (2019), we aim to learn the parameters of a controller for a lunar lander as implemented in OpenAI gym. The controller has 12 continuous input dimensions, corresponding to events such as \u201cchange the angle of the rover if it is tilted more than x degrees.\" The objective is to maximize the average final reward over 50 randomly generated environments. Cellular Coverage We want to optimize the cellular network coverage and capacity from 15 cell towers (Dreifuerst et al., 2021). Each tower has a continuous parameter corresponding to transmit power parameter and an ordinal parameter with 6 values corresponding to tilt, for a total of 15 continuous and 15 ordinal input parameters. There are two different objectives: maximize the cellular coverage, and minimize the total interference between cell towers. Oil Spill Sorbent In this problem, we optimize the properties of a material to maximize its performance as a sorbent for marine oil spills (Wang et al., 2020). We tune 5 ordinal parameters and 2 continuous parameters which control the manufacturing process of electrospun materials, and optimize over three objectives: water contact angle, oil absorption capacity, and mechanical strength. Pest Control Minimizing the spread of pests while minimizing the prevention costs of treatment is an important problem with many parallels (Oh et al., 2019). In this experiment, we define the setting as a categorical optimization problem with 25 categorical variables corresponding to stages of intervention, with 5 different values at each stage. We optimize over two objectives: minimizing the spread of pests and minimizing the cost of prevention. PDE Optimization Here, following Maddox et al. (2021), we optimize 4 continuous parameters corresponding to the diffusivity and reaction rates of a Brusselator with spatial coupling. The objective of the problem is to minimize the variance of the PDE output."
        },
        {
            "heading": "B.3 HIGH-DIMENSIONAL PROBLEMS",
            "text": "Polynomial In this optimization problem, the goal is to find the maximum value of a polynomial function. For input x \u2208 Rd, the objective value is calculated using \u2211d/4 i=1 \u220f4 j=1(xi+j \u2212 ci+j), where ci \u223c Normal(0, 1). We set the boundaries of the input space to be [0, 1]d, and this problem setup can be used for any number of dimensions d. Neural Network Draw We want to find the maximum value of a function specified by a draw from a neural network. For the neural network, we use an MLP with d inputs connected to 2 hidden layers of 256 nodes each with tanh activation, connected to a final layer of size 1 (unless otherwise specified). We set each parameter w in the network to a value drawn from N (0, 1). The final objective function is equivalent to the output of the neural network with the specified weights. With this setup, we can vary d to alter the input dimensionality of the problem. Knowledge Distillation The goal of knowledge distillation is to use a larger teacher model to train a smaller student model, typically done by matching the teacher and student predictive distributions. In this problem, we use Bayesian optimization to determine the optimal parameters of the student neural network by setting our objective function as the KL divergence between the teacher and student predictive distributions. Knowledge distillation is known to be a difficult optimization problem, and this is problem also has many more dimensions than typical Bayesian optimization benchmarks. For our experiment, we use the MNIST dataset, and we train a LeNet-5 for our teacher model. For our student model, we use a small CNN with the following architecture:\n1. convolutional layer with 16 convolution kernels of 3x3 (ReLU activation) 2. max pool layer 2x2 3. convolutional layer with 12 convolution kernels of 3x3 (ReLU activation) 4. max pool layer 2x2 5. fully connected layer with 10 outputs"
        },
        {
            "heading": "APPENDIX C EXPERIMENT DETAILS",
            "text": ""
        },
        {
            "heading": "C.1 GENERAL SETUP",
            "text": "For all datasets, we normalize input values to be between [0, 1] and standardize output values to have mean 0 and variance 1. We also use Monte-Carlo based Expected Improvement as our acquisition function. GP: For single-objective problems, we use GPs with a Mat\u00e9rn 5/2 kernel, adaptive scale, a lengthscale prior of Gamma(3, 6), and an output-scale prior of Gamma(2.0, 0.15), which combine with the marginal likelihood to form a posterior which we optimize for hyperparameter learning. For multi-objective problems, we use the same GP to independently model each objective. DKL: We set up the base kernel using the same Mat\u00e9rn 5/2 kernel that we use for GPs. For the feature extractor, we use the same model parameters as found in our full HMC search. For multi-objective problems, we independently model each objective. I-BNN: We use I-BNNs with 3 hidden layers and the ReLU activation function. We set the variance of the weights to 10.0, and the variance of the bias to 1.3. For multi-objective problems, we independently model each objective. HMC: We use HMC with an adaptive step size, and we do a small search over different architectures and model choices for each problem. We use MLPs for all experiments, and we first do a grid search over MLP architectures, where we run all combinations of width (64, 128, 256), and depth (2, 3, 4, 5) with the prior and likelihood variance set to 1.0. From the best performing run, we then fix the width and depth of the network and do a grid search over the prior variance (0.1, 1.0, and 10.0) and likelihood variance (0.1, 1.0, and 10.0). We model multi-objective problems by setting the number of nodes in the final layer equal to the number of objectives. SGHMC: We use SGHMC with minibatch size of 5 using the same model parameters as found in our full HMC search. We follow the implementation in Springenberg et al. (2016) and use scale-adaptive SGHMC with a heteroskedastic likelihood variance as determined by the output of the neural network. We model multi-objective problems by setting the number of nodes in the final layer equal to the number of objectives. LLA: We use the same model parameters as found in our HMC search. We model multi-objective problems by setting the number of nodes in the final layer equal to the number of objectives. ENSEMBLE: We use an ensemble of 10 models, each with the same architecture as found in the HMC search. Each model is trained on a random 80% of the function evaluations. We model multi-objective problems by setting the number of nodes in the final layer equal to the number of objectives. We run multiple trials for all experiments, where each trial starts with a different set of initial function evaluations drawn using a Sobol sampler."
        },
        {
            "heading": "C.2 SYNTHETIC BENCHMARKS",
            "text": "Branin We ran 5 trials using batch size 5 with 10 initial points. Our BNN models used MLPs with 3 hidden layers, 128 parameters per layer, tanh activation, likelihood variance of 0.1, and prior variance of 10.0. Hartmann We ran 5 trials using batch size 10 with 10 initial points. Our BNN models used MLPs with 3 hidden layers, 128 parameters per layer, tanh activation, likelihood variance of 0.1, and prior variance of 1.0. Ackley We ran 5 trials using batch size 10 with 10 initial points. Our BNN models used MLPs with 3 hidden layers, 128 parameters per layer, tanh activation, likelihood variance of 0.1, and prior variance of 0.1. BraninCurrin We ran 5 trials using batch size 10 with 10 initial points. Our BNN models used MLPs with 3 hidden layers, 128 parameters per layer, tanh activation, likelihood variance of 0.1, and prior variance of 10.0. DTLZ1 We ran 5 trials using batch size 5 with 10 initial points. Our BNN models used MLPs with 2 hidden layers, 128 parameters per layer, tanh activation, likelihood variance of 0.1, and prior variance of 10.0.\nDTLZ5 We ran 5 trials using batch size 1 with 10 initial points. Our BNN models used MLPs with 5 hidden layers, 10 parameters per layer, tanh activation, likelihood variance of 0.001, and prior variance of 10.0."
        },
        {
            "heading": "C.3 REAL-WORLD BENCHMARKS",
            "text": "PDE Optimization We ran 5 trials using batch size 1 with 5 initial points. Our BNN models used MLPs with 3 hidden layers, 128 parameters per layer, tanh activation, likelihood variance of 1.0, and prior variance of 1.0. Interferometer We ran 10 trials using batch size 10 with 10 initial points. Our BNN models used MLPs with 5 hidden layers, 100 parameters per layer, tanh activation, likelihood variance of 10.0, and prior variance of 10.0. Lunar Lander We ran 5 trials using batch size 50 with 50 initial points. Our BNN models used MLPs with 5 hidden layers, 100 parameters per layer, tanh activation, likelihood variance of 10.0, and prior variance of 10.0. Pest Control We ran 5 trials using batch size 4 with 20 initial points. Our BNN models used MLPs with 5 hidden layers, 100 parameters per layer, tanh activation, likelihood variance of 10.0, and prior variance of 10.0. Cell Coverage We ran 5 trials using batch size 5 with 10 initial points. Our BNN models used MLPs with 3 hidden layers, 128 parameters per layer, tanh activation, likelihood variance of 0.1, and prior variance of 0.1. Oil Spill Sorbent We ran 5 trials using batch size 10 with 10 initial points. Our BNN models used MLPs with 5 hidden layers, 100 parameters per layer, tanh activation, likelihood variance of 0.1, and prior variance of 10.0."
        },
        {
            "heading": "C.4 NEURAL ARCHITECTURE SEARCH",
            "text": "We used SMAC (Lindauer et al., 2022) to find the optimal hyperparameters of HMC for Cell Coverage, Pest Control, and DTLZ5 benchmark problems using Bayesian optimization. For each benchmark, our new objective function was the average maximum value found for three runs of Bayesian optimization using the same problem setup as detailed above. We used the hyperparameter optimization facade in SMAC, and for each problem, we used Bayesian optimization to select 200 different HMC architectures to find the optimal combination from the following set of possible values:\n\u2022 Network width: [32, 512] (continuous) \u2022 Network depth: {1, 2, 3, 4, 5, 6} (discrete) \u2022 Network activation: {\"ReLU\", \"tanh\"} (discrete) \u2022 log10 of likelihood variance: [-3.0, 2.0] (continuous) \u2022 log10 of prior variance: [-3.0, 2.0] (continuous)\nWe then use the optimal architecture and run the benchmark for 5 trials with the same setup as described in Appendix C.2 and Appendix C.3 to compare the results of HMC with and without neural architecture search. For cell coverage, the final HMC model was an MLP with 5 hidden layers, 184 parameters per layer, tanh activation, likelihood variance of 26.3, and prior variance of 0.54. For pest control, the final HMC model was an MLP with 1 hidden layer of size 321, tanh activation, likelihood variance of 0.26, and prior variance of 0.31. For DTLZ5, the final HMC model was an MLP with 3 hidden layers, 297 parameters per layer, relu activation, likelihood variance of 0.01, and prior variance of 0.30."
        },
        {
            "heading": "C.5 HIGH-DIMENSIONAL EXPERIMENTS",
            "text": "Polynomial We ran 5 trials using batch size 10 with 100 initial points. Our HMC model was an MLP with 2 hidden layers, 256 parameters per layer, tanh activation, likelihood variance of 1.0, and prior variance of 10.0. Neural Network Draw We ran 5 trials using batch size 10 with 100 initial points. Our HMC model was an MLP with 2 hidden layers, 256 parameters per layer, tanh activation, likelihood variance of 1.0, and prior variance of 10.0.\nKnowledge Distillation We ran 5 trials using batch size 10 with 100 initial points. Our HMC model was an MLP with 2 hidden layers, 256 parameters per layer, tanh activation, likelihood variance of 1.0, and prior variance of 10.0."
        },
        {
            "heading": "APPENDIX D FURTHER EMPIRICAL RESULTS",
            "text": ""
        },
        {
            "heading": "D.1 BNN ARCHITECTURE D.1.1 NEURAL ARCHITECTURE SEARCH",
            "text": "To further investigate the impact of architecture on the performance of BNNs, we conduct an extensive neural architecture search over a selection of the benchmark problems, varying the width, depth, prior variance, likelihood variance, and activation function. For our experiments, we use SMAC3 (Lindauer et al., 2022), a framework which uses Bayesian optimization to select the best hyperparameters, and we detail the experiment setup in Appendix C. While a thorough search over architectures is often impractical for realistic settings of Bayesian optimization since it requires a very large number of function evaluations, we use this experiment to demonstrate the flexibility of BNNs and to showcase its potential when the design is well-suited for the problem. We show the effect of neural architecture search on HMC surrogate models in Figure A.1. On the Cell Coverage problem, the architecture search did not drastically change the performance of HMC. In contrast, extensively optimizing the hyperparameters made a significant difference on the Pest Control problem, leading to HMC finding higher rewards than GPs while using fewer function evaluations; however, on this problem, I-BNN, which does not require specifying an architecture, still performs best. Neural architecture search was also able to improve the results on DTLZ5, leading HMC to be competitive with other surrogate models such as I-BNNs and DKL. The difference in the benefits of the search may be attributed to some problems having less inherent structure than others, where extensive hyperparameter optimization may not be as necessary. Additionally, our original HMC surrogate model choice may already have been a suitable choice for some problems, so an extensive search over architectures may not significantly improve the performance."
        },
        {
            "heading": "D.1.2 SMALLER ARCHITECTURE",
            "text": ""
        },
        {
            "heading": "D.2 ABLATION STUDIES",
            "text": "To better understand the quality of the mean estimates and uncertainty estimates of different surrogate models, we conducted ablation studies by creating hybrid models which combine the mean estimate of one surrogate with the uncertainty estimate of another. When we compare this hybrid model with each individual model, we are able to get insight into the relative performance of the mean and uncertainty estimates. We provide the results for GP vs HMC, the gold standard inference for BNNs, and GP vs I-BNN, the model with the most success in high-dimensions."
        },
        {
            "heading": "D.3 DEEP ENSEMBLES",
            "text": "While deep ensembles often provide good accuracy and well-calibrated uncertainty estimates in other settings (Lakshminarayanan et al., 2017), we show they can perform relatively poorly for Bayesian optimization. For instance, when compared to other BNNs on benchmark problems such as BraninCurrin and DTLZ1, the maximum reward found by deep ensembles plateaus at a lower value than other surrogate models. These findings are also supported by results shown in Dai et al. (2022), where deep ensembles do not perform well in Bayesian optimization because they are unable to to explore the space effectively.\nTo further investigate the behavior of deep ensembles, we conduct a sensitivity study, varying the architecture, the amount of training data, and the number of models in the ensemble. In Figure A.7, we see that smaller training sizes can paradoxically lead to less uncertainty with deep ensembles. A critical component in the success of deep ensembles is the diversity of its models. After training, each model falls within a distinct basin of attraction, where solutions across different basins correspond to diverse functions. Intuitively, however, in the low data regime there are fewer settings of parameters that give rise to easily discoverable basins of attraction, making it harder to find diverse solutions simply by re-initializing optimization runs. We consider, for example, the straight path between the weights of Model 1 and Model 2, and we follow how the loss changes as we linearly interpolate between the weights. Specifically, given neural network weights w1 from Model 1 and w2 from Model 2, and L(w) representing the loss of the neural network with weights w on the training data, we plot the loss L(w2 + (w1 \u2212w2) \u2217 x) for varying values of x. We share results in Figure A.8 for DTLZ1, a problem where deep ensembles performed poorly. We can see that in low-data regimes, although the loss between the two models does increase, the loss is significantly higher in other regions of the loss landscape. This behavior suggests that the basins are not particularly distinct, as the loss stays relatively flat between them, and thus less likely to provide diverse solutions. However, as we increase the amount of training data, the models are able to find more pronounced basins. We also verify that the flatter regions correspond to a decrease in model diversity by measuring the cosine similarity between the model weights, and we see that in the low-data regime, models have more similar weights and therefore are less diverse. In general, Bayesian optimization problems contain significantly fewer datapoints than where deep ensembles are normally applied. Standard Bayesian optimization benchmarks rarely exceed about 600 data points (objective queries), while in contrast deep ensembles are often trained on problems like CIFAR-10 which have 50,000 training points.\nD.4 INFINITE-WIDTH BNNS I-BNNs outperformed GPs in Bayesian optimization on a variety of high-dimensional problems, and we show results in Figure A.10. The first row of results corresponds to finding the maximum value of a random polynomial function, and the second and third rows show the results of maximizing a function draw from a neural network. For the neural network function draw benchmark, we experimented with many different architectures for the neural network to ensure that we had a diverse set of objective functions to maximize, as denoted in the title of each plot. In all cases, I-BNNs were able to find significantly larger values than GPs.\nIn Figure A.11, we conduct a sensitivity analysis on the design of I-BNNs, showing how the posterior changes as we vary certain hyperparameters. Unlike standard GPs with RBF or Mat\u00e9rn kernels which are based in Euclidean similarity, I-BNNs instead have a non-stationary and non-Euclidean similarity metric which is more suitable for high-dimensional problems. Additionally, I-BNNs consist of a relatively strong prior, which is particularly useful in the data-scarce settings common to high-dimensional-problems.\nTo explore the suitability of the neural network kernel, we compare the marginal likelihood of GPs and I-BNNs on problems with various dimensions, and we report results in Table A.1. We see that I-BNNs and GPs have similar marginal likelihoods on problems with fewer dimensions, such as Branin and Hartmann, but as we increase the number of dimensions, the marginal likelihood of GPs becomes lower than that of I-BNNs. Interestingly, we see that the marginal likelihood of GPs does not decrease as quickly for Ackley as it does for the neural network draw test problem. This may be due to the neural network draw objective function having higher non-stationarity, so the standard GP kernel is less suitable for this problem compared to Ackley. For the knowledge distillation experiment, we see that the marginal likelihood of GPs plummets, and it was also not able to find high rewards for the problem as shown in Figure 5. Overall, I-BNNs have a higher marginal likelihood than GPs on high-dimensional problems, indicating that the I-BNN prior may be more reasonable in these settings."
        },
        {
            "heading": "D.5 HAMILTONIAN MONTE CARLO",
            "text": "We also explore the effect of the activation function on HMC. In Figure A.12, we see that the function draws from BNNs with tanh activations appear quite different from the function draws from BNNs with ReLU activations. The tanh draws are smoother and have more variation, while the ReLU draws seem to be more jagged. We also include results in Figure A.13 which compare the performance of ReLU and tanh activations in Bayesian optimization problems. We see that there does not appear to be an obvious trend, the optimal choice of activation function is problem-specific."
        },
        {
            "heading": "D.6 GAUSSIAN PROCESSES",
            "text": ""
        },
        {
            "heading": "D.7 DEEP KERNEL LEARNING",
            "text": "Rather than using the marginal likelihood to optimize parameters for DKL, we can also use the conditional marginal likelihood (Lotfi et al., 2023). We see in Figure A.16 that there is no clear preference for using the marginal likelihood (ML), which we use through all other experiments in the paper, or conditional marginal likelihood (CML) for our problems."
        },
        {
            "heading": "D.8 ACQUISITION BATCH SIZE",
            "text": ""
        },
        {
            "heading": "D.9 LIMITATIONS OF GAUSSIAN PROCESS SURROGATE MODELS",
            "text": "Due to their assumptions of stationarity as noted in Section 4.3, GPs struggle in non-stationary settings. In Figure A.18, we compare the posterior distribution from a GP with the posterior from different BNN surrogate models. We show results after 20 iterations of Bayesian optimization, starting with an initial point of x = 0. The function we wish to maximize is non-stationary: the function has greater variance between \u22122 and 2, and there is also a slight downward trend. We see that due to their stronger assumptions, GPs are not able to find the true global maximum of 0.8, instead getting suck in local optima. In contrast, HMC and I-BNNs are able to find the global maximum within 20 iterations.\nAn additional limitation of GP surrogate models, which we demonstrate in Figure A.19, is its performance on multi-objective problems in Bayesian optimization. Although GPs have been successfully extended to a wide range of multi-objective problems, in the interest of making the approaches scalable, there are many assumptions placed onto the kernel. In the most naive setting, we can model each objective independently. While this approach is convenient, it completely ignores the relationship between objective values and has no notion of shared structure, so it is unable to take advantage of all of the information in the problem. Multi-objective covariance functions can also be decomposed as Kronecker product kernels. While this approach can have significant computational advantages compared to modeling the full covariance function, it requires each objective to itself be modeled with the same underlying kernel. Thus, this method of modeling multiple objectives will fail to capture the nuances of each particular objective when the functions have differing properties. In Figure A.19, we show the result of twenty iterations of Bayesian optimization over a synthetic multitask example, where we care about optimizing over the fourth function but provide additional information through the other three datasets. We use the GP with Kronecker product kernels to model the multiple objectives. In our experiment, although the GP is able to learn the proper length scale and variance over the three additional functions with similar length scales, it struggles to accurately fit the fourth objective. Because the GP is unable to account for the differences between the four functions, it does not find the global optimum. Unlike GPs, BNNs are not restricted to strict covariance structures and are able to produce well-calibrated uncertainty estimates in multi-objective settings. The BNNs are able to accurately fit all four functions, including the fourth objective function which has a much smaller length scale compared to the others."
        },
        {
            "heading": "D.10 LARGE NUMBER OF FUNCTION QUERIES",
            "text": "To further accentuate the distinctions between BNNs and GPs, we experiment with a larger number of function queries. Specifically, we look to maximize a function with 200 input dimensions drawn from a fully connected neural network with 5 layers and 256 nodes per layer. We expect this function to have a high degree of non-stationarity, making it difficult for standard GPs. We start with 2000 initial function queries and end the experiment at 3000 function queries. We share results in Figure A.20.\nThis experiment reveals several valuable findings: (1) I-BNNs remain competitive relative to the alternatives; (2) BNN surrogates start to perform more effectively, able to leverage the additional data for representation learning; (3) deep ensembles, in particular, are greatly improved with more data, in line with our explanation that the relative poor performance was due to an inability to find diverse models corresponding to posterior modes when there is limited data. At the same time, we note most Bayesian optimization problems do not have many objective queries, since Bayesian optimization is often found to be most valuable when the objective is expensive to query. In this light, perhaps (1) is the most valuable of the findings, since it shows consistency of the relatively strong I-BNN surrogate. In the future, we might expect I-BNNs to become a mainstream surrogate model for Bayesian optimization."
        },
        {
            "heading": "D.11 RUNTIME",
            "text": "While inference time is relevant for the comparison of surrogate models, in many real-world Bayesian optimization scenarios, the most expensive computation often lies in the querying of the objective function, which may include actions such as synthesizing a new material, training a large neural network to convergence, etc. For these scenarios, the quality of the surrogate model uncertainties may be much more important than the cost of inference. For completeness, in rare instances where inference-time is a relevant consideration, we provide wall-clock times of all surrogate models across our experiments in Table A.2, and we compare the performance of the surrogate models within a fixed time budget in Figure A.21"
        },
        {
            "heading": "D.12 ROLE OF OPTIMIZATION",
            "text": "In the main text, we chose to highlight the power of BNNs for Bayesian optimization when the model is well-specified for the particular objective function. To find this BNN architecture, for each problem, we conducted a grid search over BNN hyperparameters, and then chose based on the setting which gave us the highest maximum reward at the end of the trial. For more details, see Appendix C. For completeness, we include results for HMC where the hyperparameters of the BNN are selected per iteration, similar to how we optimize the hyperparameters for GPs. Specifically, we fix the architecture of have width of 128 and depth of 3 layers, and then optimize the prior variance and likelihood noise after each iteration of Bayesian optimization. We use grid search over prior variance as {0.1, 0.3, 1.0, 3.0, 10.} and likelihood noise as {0.1, 0.3}, and we choose the setting which provides the highest likelihood (as estimated by SGHMC) from a 3-fold cross validation of the available function queries. We share results of this study in Figure A.22, where we compare HMC with per-trial optimization (solid green) to HMC with per-iteration optimization (dashed green). We find that the performance of the two optimization methods are comparable for problems like Hartmann and Cell Coverage. Furthermore, although per-trial optimization for HMC does improve performance on Ackley, we are still able to observe similar trends as before, with HMC consistently outperforming other approximate inference methods like SGHMC and deep ensembles.\nWe also include the results for GPs optimized on a per-trial basis, where we conduct a grid search over different configurations of the prior for length-scale and output-scale. Specifically, we first fix the prior of the output scale to be Gamma(2.0, 0.5), which is the default prior in BoTorch (Balandat et al., 2020), and we conduct grid search over the prior of the length scale Gamma(a, b) for a, b = {1, 3, 6} and select a and b per objective function based on the results at the end of the trial. Then, using this length scale prior, we conduct grid search over the prior of the output scale Gamma(c, d) using c = {1.0, 2.0, 4.0}, d = {0.5, 2, 4} and select the parameters which perform the best at the end of the full trial. The final optimized GP uses these priors.\nIn Figure A.23, we see that these optimized GPs are often comparable to the default GP and do not see significant improvements compared to using the default GP hyperpriors. This may be due to the GPs being optimized over one specific trial of Bayesian optimization, and the optimal GP may vary depending on the set of initial points."
        }
    ],
    "year": 2023
}