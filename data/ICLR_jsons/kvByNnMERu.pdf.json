{
    "abstractText": "Measuring geometric similarity between high-dimensional network representations is a topic of longstanding interest to neuroscience and deep learning. Although many methods have been proposed, only a few works have rigorously analyzed their statistical efficiency or quantified estimator uncertainty in datalimited regimes. Here, we derive upper and lower bounds on the worst-case convergence of standard estimators of shape distance\u2014a measure of representational dissimilarity proposed by Williams et al. (2021). These bounds reveal the challenging nature of the problem in high-dimensional feature spaces. To overcome these challenges, we introduce a new method-of-moments estimator with a tunable bias-variance tradeoff. We show that this estimator achieves superior performance to standard estimators in simulation and on neural data, particularly in high-dimensional settings. Thus, we lay the foundation for a rigorous statistical theory for high-dimensional shape analysis, and we contribute a new estimation method that is well-suited to practical scientific settings.",
    "authors": [],
    "id": "SP:bcbdef2c367e8f2b2678fcea8a3404450fffeb9b",
    "references": [
        {
            "authors": [
                "Ryan P. Adams",
                "Jeffrey Pennington",
                "Matthew J. Johnson",
                "Jamie Smith",
                "Yaniv Ovadia",
                "Brian Patton",
                "James Saunderson"
            ],
            "title": "Estimating the spectral density of large implicit matrices, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Jose Manuel Andrade",
                "Mar\u0131\u0301a P. G\u00f3mez-Carracedo",
                "Wojtek Krzanowski",
                "Mikael Kubista"
            ],
            "title": "Procrustes rotation in analytical chemistry, a tutorial",
            "venue": "Chemometrics and Intelligent Laboratory Systems,",
            "year": 2004
        },
        {
            "authors": [
                "Florent Benaych-Georges",
                "Jean-Philippe Bouchaud",
                "Marc Potters"
            ],
            "title": "Optimal cleaning for singular values of cross-covariance matrices",
            "venue": "The Annals of Applied Probability,",
            "year": 2023
        },
        {
            "authors": [
                "Stephen Boyd",
                "Lieven Vandenberghe"
            ],
            "title": "Convex optimization",
            "year": 2004
        },
        {
            "authors": [
                "Mingbo Cai",
                "Nicolas W Schuck",
                "Jonathan W Pillow",
                "Yael Niv"
            ],
            "title": "A bayesian method for reducing bias in neural representational similarity analysis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Taco Cohen",
                "Max Welling"
            ],
            "title": "Group equivariant convolutional networks",
            "venue": "Proceedings of The 33rd International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Alan D. Degenhart",
                "William E. Bishop",
                "Emily R. Oby",
                "Elizabeth C. Tyler-Kabara",
                "Steven M. Chase",
                "Aaron P. Batista",
                "Byron M. Yu"
            ],
            "title": "Stabilization of a brain\u2013computer interface via the alignment of low-dimensional spaces of neural activity",
            "venue": "Nature Biomedical Engineering,",
            "year": 2020
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Lyndon R. Duong",
                "Jingyang Zhou",
                "Josue Nassar",
                "Jules Berman",
                "Jeroen Olieslagers",
                "Alex H. Williams"
            ],
            "title": "Representational dissimilarity metric spaces for stochastic neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Hamza Giaffar",
                "Camille Rull\u00e1n Bux\u00f3",
                "Mikio Aoi"
            ],
            "title": "The effective number of shared dimensions: A simple method for revealing shared structure between datasets",
            "venue": "bioRxiv,",
            "year": 2023
        },
        {
            "authors": [
                "Colin R. Goodall"
            ],
            "title": "Procrustes methods in the statistical analysis of shape",
            "venue": "Journal of the royal statistical society series b-methodological,",
            "year": 1991
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "D G Kendall",
                "D Barden",
                "T K Carne",
                "H Le"
            ],
            "title": "Shape and Shape Theory",
            "year": 2009
        },
        {
            "authors": [
                "John T. Kent",
                "Kanti V. Mardia"
            ],
            "title": "Consistency of procrustes estimators",
            "venue": "Journal of the Royal Statistical Society. Series B (Methodological),",
            "year": 1997
        },
        {
            "authors": [
                "Tim C. Kietzmann",
                "Courtney J. Spoerer",
                "Lynn K.A. S\u00f6rensen",
                "Radoslaw M. Cichy",
                "Olaf Hauk",
                "Nikolaus Kriegeskorte"
            ],
            "title": "Recurrence is required to capture the representational dynamics of the human visual system",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 1905
        },
        {
            "authors": [
                "Max Klabunde",
                "Tobias Schumacher",
                "Markus Strohmaier",
                "Florian Lemmerich"
            ],
            "title": "Similarity of neural network models: A survey of functional and representational measures, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Weihao Kong",
                "Gregory Valiant"
            ],
            "title": "Spectrum estimation from samples",
            "venue": "The Annals of Statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Honglak Lee",
                "Geoffrey Hinton"
            ],
            "title": "Similarity of neural network representations revisited",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Nikolaus Kriegeskorte",
                "Marieke Mur",
                "Peter Bandettini"
            ],
            "title": "Representational similarity analysis connecting the branches of systems neuroscience",
            "venue": "Front. Syst. Neurosci.,",
            "year": 2008
        },
        {
            "authors": [
                "Nikolaus Kriegeskorte",
                "Marieke Mur",
                "Douglas A Ruff",
                "Roozbeh Kiani",
                "Jerzy Bodurka",
                "Hossein Esteky",
                "Keiji Tanaka",
                "Peter A Bandettini"
            ],
            "title": "Matching categorical object representations in inferior temporal cortex of man and monkey",
            "year": 2008
        },
        {
            "authors": [
                "Richard D Lange",
                "Devin Kwok",
                "Jordan Matelsky",
                "Xinyue Wang",
                "David S Rolnick",
                "Konrad P Kording"
            ],
            "title": "Neural networks as paths through the space of representations",
            "venue": "arXiv preprint arXiv:2206.10999,",
            "year": 2022
        },
        {
            "authors": [
                "Lin Lin",
                "Yousef Saad",
                "Chao Yang"
            ],
            "title": "Approximating spectral densities of large matrices",
            "venue": "SIAM Review,",
            "year": 2016
        },
        {
            "authors": [
                "Niru Maheswaranathan",
                "Alex Williams",
                "Matthew Golub",
                "Surya Ganguli",
                "David Sussillo"
            ],
            "title": "Universality and individuality in neural dynamics across large populations of recurrent networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Thao Nguyen",
                "Maithra Raghu",
                "Simon Kornblith"
            ],
            "title": "Do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Marc Potters",
                "Jean-Philippe Bouchaud"
            ],
            "title": "A First Course in Random Matrix Theory: for Physicists, Engineers and Data Scientists",
            "year": 2020
        },
        {
            "authors": [
                "Maithra Raghu",
                "Justin Gilmer",
                "Jason Yosinski",
                "Jascha Sohl-Dickstein"
            ],
            "title": "Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "F. James Rohlf",
                "Dennis Slice"
            ],
            "title": "Extensions of the Procrustes Method for the Optimal Superimposition of Landmarks",
            "venue": "Systematic Zoology,",
            "year": 1990
        },
        {
            "authors": [
                "Victor S. Saito",
                "Alaide A. Fonseca-Gessner",
                "Tadeu Siqueira"
            ],
            "title": "How Should Ecologists Define Sampling Effort? The Potential of Procrustes Analysis for Studying Variation in Community Composition",
            "venue": "Biotropica,",
            "year": 2015
        },
        {
            "authors": [
                "Heiko H Sch\u00fctt",
                "Alexander D Kipnis",
                "J\u00f6rn Diedrichsen",
                "Nikolaus Kriegeskorte"
            ],
            "title": "Statistical inference on representational geometries. eLife, 12:e82566, aug 2023",
            "year": 2050
        },
        {
            "authors": [
                "Jianghong Shi",
                "Eric Shea-Brown",
                "Michael Buice"
            ],
            "title": "Comparison against task driven artificial neural networks reveals functional properties in mouse visual cortex",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Katherine R. Storrs",
                "Tim C. Kietzmann",
                "Alexander Walther",
                "Johannes Mehrer",
                "Nikolaus Kriegeskorte"
            ],
            "title": "Diverse Deep Neural Networks All Predict Human Inferior Temporal Cortex Well, After Training and Fitting",
            "venue": "Journal of Cognitive Neuroscience, 33(10):2044\u20132064,",
            "year": 2021
        },
        {
            "authors": [
                "Joel A. Tropp"
            ],
            "title": "An introduction to matrix concentration inequalities",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Martin J Wainwright"
            ],
            "title": "High-dimensional statistics: A non-asymptotic viewpoint, volume 48",
            "year": 2019
        },
        {
            "authors": [
                "Alexander Walther",
                "Hamed Nili",
                "Naveed Ejaz",
                "Arjen Alink",
                "Nikolaus Kriegeskorte",
                "J\u00f6rn Diedrichsen"
            ],
            "title": "Reliability of dissimilarity measures for multi-voxel pattern analysis",
            "year": 2016
        },
        {
            "authors": [
                "Alex H. Williams",
                "Scott W. Linderman"
            ],
            "title": "Statistical neuroscience in the single trial limit",
            "venue": "Current Opinion in Neurobiology,",
            "year": 2021
        },
        {
            "authors": [
                "Alex H Williams",
                "Erin Kunz",
                "Simon Kornblith",
                "Scott Linderman"
            ],
            "title": "Generalized shape metrics on neural representations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [],
            "title": "\u221e and the variance of each matrix element is taken to be \u03c32/N , the density of the singular values takes the following form (see e.g",
            "venue": "Potters & Bouchaud,",
            "year": 2020
        },
        {
            "authors": [
                "Stringer"
            ],
            "title": "All analyses done in this paper were performed on the pre-processed data available on figshare (https://figshare.com/articles/Recordings_of_ten_thousand_neurons_ in_visual_cortex_in_response_to_2_800_natural_images/6845348)",
            "venue": "E APPENDIX: APPLICATIONS TO DEEP LEARNING",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Many approaches have been proposed to quantify similarity in neural network representations. Some popular methods include canonical correlations analysis (Raghu et al., 2017), centered kernel alignment (CKA; Kornblith et al., 2019), representational similarity analysis (RSA; Kriegeskorte et al., 2008a), and shape metrics (Williams et al., 2021). Each of these approaches takes in a set of highdimensional measurements from two networks\u2014e.g., hidden layer activations or measured biological responses\u2014and outputs a (dis)similarity score. Shape distances additionally satisfy the triangle inequality, thus enabling downstream algorithms for clustering and nearest-neighbor regression that leverage metric space structure (Williams et al., 2021). Shape metrics have numerous applications in the physical sciences (Saito et al., 2015; Andrade et al., 2004; Rohlf & Slice, 1990) and while the use of similarity metrics in the study of both biological and artificial neural representations has grown in popularity (Kietzmann et al., 2019; Storrs et al., 2021; Kriegeskorte et al., 2008b; Maheswaranathan et al., 2019; Nguyen et al., 2021; Klabunde et al., 2023) this setting has introduced statistical issues that have not been adequately addressed. Specifically, shape metrics are often applied to low-dimensional noiseless measurements (e.g., comparing 3D digital scans of anatomy across animals (Rohlf & Slice, 1990)) whereas in the study of neural networks the applications have been high-dimensional (e.g., comparing neural activity between brain regions (Kriegeskorte et al., 2008b)). Here we demonstrate that in both biological and artificial neural networks the bias of these metrics can be significant because of the high noise and dimensionality inherent to the study of neural representations.\nDespite the challenges of applying similarity metrics to high-dimensional, noisy measurements, there is little work on quantifying accuracy (e.g. through confidence intervals or characterizing bias) on estimators of representational similarity with the noteworthy exception of research on RSA (Cai et al., 2016; Walther et al., 2016; Schu\u0308tt et al., 2023). This poses a serious obstacle to adoption of these methods, particularly in experimental neuroscience where there is a hard limit on the number of conditions that can be feasibly sampled (Shi et al., 2019; Williams & Linderman, 2021).\nWe thus address these concerns in the context of measuring shape distances between neural representations (Fig. 1; Williams et al., 2021). Shape distances are drawn from an established literature (Goodall, 1991; Kendall et al., 2009), and were proposed as a method to measure similarity in neural\nnetwork representations by Williams et. al. 2021. While the techniques proposed there have been applied to measure representational similarity between both artificial and biological neural networks (see also Degenhart et al., 2020; Lange et al., 2022), there is still a distinct lack of theoretical and empirical understanding of the biases of these estimators applied to noisy finite data. Interpretation of these estimators will remain difficult until there is a clear understanding of the conditions under which they are accurate.\nWe first obtain analytic upper and lower bounds on the accuracy of typical \u201cplug-in estimates\u201d of shape distance as as a function of the number of samples, M , and the dimension of the representation, N . We find that plug-in estimates rapidly converge onto their expected value, but the expected error decays moderately slowly (i.e. the estimators have low variance and high bias). To overcome these limitations, we propose a new method-of-moments estimator which, while not always strictly optimal compared with the plug-in estimator, provides an explicit and tunable tradeoff between estimator bias and variance. We validate this new approach on simulated data and apply it to neurobiological responses to a set of M=2,800 natural images. Our results suggest that standard estimation procedures may underestimate the true shape distance between networks in high-dimensional regimes with limited samples."
        },
        {
            "heading": "2 BACKGROUND AND PROBLEM SETTING",
            "text": "Intuitively, a measure of distance between neural representations should be invariant nuisance symmetries in the neural representation, such as arbitrary permutations over neuron labels (Cohen & Welling, 2016). Representational similarity measures are typically designed to be invariant not only to permutations, but also to rotations, reflections, translations, and isotropic scalings in neural firing rate space (Kriegeskorte et al., 2008a; Kornblith et al., 2019).\nWe begin by considering a simple setting where each neural network is a deterministic map (for the stochastic setting, see section 3.4). A collection of K neural systems can then be viewed as a set of functions, each denoted hi : Z 7\u2192 RN for i \u2208 {1, . . . ,K}. Here, Z is a feature space and N can be interpreted as the number of neurons in each system (e.g. the size of a hidden layer in an artificial network, or the number of recorded neurons in a biological experiment).1\n1The assumption that each layer has the same number of neurons is not necessary, and only made for convenience. For networks with dissimilar sizes, we can preprocess by zero-padding the smaller network. If necessary, one could alternatively perform PCA on the larger network to reduce to a common dimension.\nMotivated by the shape theory literature (Goodall, 1991; Kent & Mardia, 1997; Kendall et al., 2009; Williams et al., 2021), we consider estimating the Procrustes size-and-shape distance, \u03c1, and Riemannian shape distance, \u03b8, between neural representations. Let hi and hj denote neural systems which we assume are mean-centered and bounded:\nE[hi(z)] = E[hj(z)] = 0 and \u2225hi(z)\u22252, \u2225hj(z)\u22252 < B \u221a N almost surely. (1)\nfor some constant B > 0. Here, the expectations are taken over z \u223c P , for some distribution P over network inputs. Our assumption that neural population rates are bounded by B \u221a N can result from assuming each neuron has a maximum firing rate equal to B. This assumption is common in the literature and reasonable in both artificial networks (since connection weights are finite) and biological networks (since neurons have a maximal firing rate).\nThe Procrustes and Riemannian shape distances can be defined (App. D in Williams et al., 2021):\n\u03c1(hi, hj) = min Q\u2208O(N)\n\u221a E\u2225hi(z)\u2212Qhj(z)\u222522 (2)\n\u03b8(hi, hj) = min Q\u2208O(N)\ncos\u22121 ( E[hi(z)TQhj(z)]\u221a\nE[hi(z)Thi(z)]E[hj(z)Thj(z)]\n) (3)\nwhere O(N) denotes the set of N \u00d7N orthogonal matrices. Again, all expectations are taken over z \u223c P . Note that different notions of distance arise from different choices of input distribution, P . To simplify our analysis and exposition, we will focus on estimating the squared Procrustes distance, \u03c12, and what we call the cosine shape similarity, cos \u03b8. Thus, we ignore the square root term in eq. (2) and the arccosine term in eq. (3), but it should be kept in mind that one must apply these nonlinear functions to achieve a proper metric.\nProperties of Shape Distance It is easy to verify that shape distances are invariant to rotations and reflections: that is, if r : RN 7\u2192 RN is an orthogonal transformation, then for any function h : Z 7\u2192 RN representing a neural system we have \u03c1(h, r \u25e6 h) = \u03b8(h, r \u25e6 h) = 0, where \u2018\u25e6\u2019 denotes function composition. Furthermore, \u03c1 and \u03b8 are proper metrics, meaning that:\n\u03c1(hi, hj) = \u03c1(hj , hi) and \u03c1(hi, hj) \u2264 \u03c1(hi, hk) + \u03c1(hk, hj) \u2200 i, j, k \u2208 {1, . . . ,K}, (4)\nand likewise for \u03b8. These properties are fundamental to rigorously establishing downstream analyses, such as for clustering networks with similar representations (Williams et al., 2021).\nIt is well-known that the optimal orthogonal alignment appearing in eqs. (2) and (3) can be identified in closed form, allowing us to write the Procrustes and Riemannian shape distances in terms of the covariance and cross-covariance matrices. We define the covariance (\u03a3ii and \u03a3jj) and crosscovariance matrices (\u03a3ij) as\n\u03a3ii = E[hi(z)hi(z)T] , \u03a3jj = E[hj(z)hj(z)T] , \u03a3ij = E[hi(z)hj(z)T], (5)\nand reformulate the squared Procrustes distance and cosine shape similarity:\n\u03c12(hi, hj) = Tr[\u03a3ii] + Tr[\u03a3jj ]\u2212 2\u2225\u03a3ij\u2225\u2217 (6)\ncos \u03b8(hi, hj) = \u2225\u03a3ij\u2225\u2217\u221a\nTr[\u03a3ii] Tr[\u03a3jj ] (7)\nwhere \u2225\u03a3ij\u2225\u2217 denotes the nuclear norm (or Shatten 1-norm) of the cross-covariance matrix:\n\u2225\u03a3ij\u2225\u2217 = N\u2211\nn=1\nsn(\u03a3ij) (8)\nwhere s1(M) \u2265 \u00b7 \u00b7 \u00b7 \u2265 sN (M) \u2265 0 denote the singular values of a matrix M . Equations 6 and 7 are derived in Appendix A.1 to provide the reader with a self-contained narrative.\nPlug-in Estimators Suppose we are given M independent and identically distributed network inputs z1, . . . ,zM \u223c P . How well can we approximate the shape distances between two networks, as a function of M? The standard approach (Williams et al., 2021), is to use a plug-in estimator\nin which one computes eqs. (2) and (3) after identifying the optimal Q \u2208 O(N). As we show in App. A.2, this is equivalent to estimating the squared Procrustes and cosine Riemannian distances by substituting the empirical covariances:\n\u03a3\u0302ii = 1 M M\u2211 m=1 hi(zm)hi(zm) T, \u03a3\u0302jj = 1 M M\u2211 m=1 hj(zm)hj(zm) T, \u03a3\u0302ij = 1 M M\u2211 m=1 hi(zm)hj(zm) T (9)\nto approximate the true covariances appearing in eqs. (6) and (7). Thus,\n\u03c1\u03022(hi, hj) = Tr[\u03a3\u0302ii] + Tr[\u03a3\u0302jj ]\u2212 2\u2225\u03a3\u0302ij\u2225\u2217 (10)\ncos \u03b8\u0302(hi, hj) = \u2225\u03a3\u0302ij\u2225\u2217\u221a\nTr[\u03a3\u0302ii] Tr[\u03a3\u0302jj ] (11)\ndefine plug-in estimators for the squared Procrustes and cosine Riemannian shape distances. The empirical behavior of these estimators as a function of M was only briefly characterized by Williams et al. (2021) for a pair of artificial networks trained on CIFAR-10."
        },
        {
            "heading": "3 RESULTS",
            "text": "First, we theoretically characterize plug-in estimators of shape distance as a function of the number of samples, M , and the ambient dimension, N . We show that these estimators are biased and can converge at unfavorably slow rates under certain conditions. To overcome these issues, we introduce a new method-of-moments estimator which has lower bias at the cost of increased variance."
        },
        {
            "heading": "3.1 NONASYMPTOTIC BOUNDS ON THE PERFORMANCE OF PLUG-IN ESTIMATION",
            "text": "First, it is straightforward to estimate Tr[\u03a3ii] and Tr[\u03a3jj ]. Their plug-in estimators are unbiased under our assumptions in eq. (1), and they rapidly converge to the correct answer. This is shown in the following lemma, whose proof relies only on classical concentration inequalities. Lemma 1 (App. B.1). Under the assumptions in eq. (1), with probability at least 1\u2212 \u03b4:\u2223\u2223\u2223Tr[\u03a3ii]\u2212 Tr[\u03a3\u0302ii]\u2223\u2223\u2223 \u2264 BN1/2M\u22121/2\u221a2 log(2/\u03b4) (12) In contrast, the plug-in estimator for \u2225\u03a3ij\u2225\u2217 is biased upwards (see section 3.2) and turns out to converge more slowly. Using the Matrix Bernstein inequality (see Tropp, 2015), we can show: Lemma 2 (App. B.2). Under the assumptions in eq. (1), for any M and N :\nE \u2223\u2223\u2223\u2225\u03a3\u0302ij\u2225\u2217 \u2212 \u2225\u03a3ij\u2225\u2217\u2223\u2223\u2223 < 2B2N2 log(2N)\n3M +\n2B2N2 \u221a log(2N)\nM1/2 (13)\nThis only upper bounds the expected error. However, the fluctuations around this expectation turn out to be small (see App. B.3), and so we are able to combine lemmas 1 and 2 into the following: Theorem 1 (App. B.3). Under the assumptions in eq. (1), with probability at least 1\u2212 \u03b4\n|\u03c1\u03022 \u2212 \u03c12| N \u2264 2B 2N log(2N) 3M +\n2B2N \u221a log(2N)\nM1/2 +\n( B2\nM1/2 +\n2B\nN1/2M1/2\n)\u221a 2 log ( 6\n\u03b4\n) (14)\nTheorem 1 states a non-asymptotic upper bound on the plug-in estimator\u2019s error that holds with high probability. We have expressed this bound on the squared size-and-shape Procrustes distance normalized by 1/N , since the raw error, |\u03c1\u0302\u2212 \u03c1|, will tend to increase linearly with N for an uninteresting reason\u2014namely, since the the Procrustes shape distance is comprised of terms like Tr[\u03a3ii] and Tr[\u03a3jj ]. The choice of normalization in theorem 1 also makes the result more comparable to the cosine shape similiarity (eq. 7), which is normalized by a factor, \u221a Tr[\u03a3ii] Tr[\u03a3jj ], of order N .\nWe can gain intuition for theorem 1 by ignoring logarithmic factors and noticing that the second term dominates. Then, roughly speaking, theorem 1 says that we can guarantee the plug-in error decreases as a function of NM\u22121/2. Thus, for any fixed N , we need to increase M by a factor of 4 to decrease estimation error by a factor of 2. Further, when comparing higher-dimensional neural representations (i.e. higher N ) we need to sample more landmarks\u2014if N increases by a factor of 2, then M must be increased by a factor of 4 to compensate."
        },
        {
            "heading": "3.2 FAILURE MODES OF PLUG-IN ESTIMATION AND A LOWER BOUND ON PERFORMANCE",
            "text": "Theorem 1 provides a high probability upper bound on the estimation error. A natural question is whether this upper bound is tight. To investigate, we seek an example where the plug-in estimator performs badly. We intuited that the plug-in estimates will have a large downward bias when two neural representations are very far apart in shape space. This can be understood in two ways. First, from the definitions of \u03c1 and \u03b8 in eqs. (2) and (3), we see that both expressions contain a minimization over Q \u2208 O(N). For large N and small M , this high-dimensional orthogonal matrix can be \u201coverfit\u201d to the M observations resulting in an underestimate of distance. Second, from the alternative formulations in eqs. (6) and (7), we see that the shape distance is large if the true crosscovariance is \u201csmall\u201d as quantified by the nuclear norm. In the extreme case where the singular values of \u03a3ij are all zero, the empirical cross-covariance matrix (1/M) \u2211 m hi(zm)hj(zm)\nT will overestimate the nuclear norm, and therefore underestimate the shape distance. This is more severe when M is small, since there are fewer terms in the sum to \u201caverage out\u201d spurious correlations, which are particularly problematic in high dimensions (i.e. when N is large).\nThis intuition led us to construct an example where plug-in estimation error approaches the upper bound in theorem 1. This is summarized in the following result. Theorem 2 (Lower Bound, App. B.4). Under the assumptions in eq. (1), there exist neural networks and a distribution over inputs such that in the limit that N \u2192 \u221e and M \u226b N :\n|\u03c1\u03022 \u2212 \u03c12| N = 16B2 3\u03c0 N1/2M\u22121/2 (15)\nThus, while future work may seek to improve the upper bound in theorem 1, we cannot hope to improve beyond the lower bound formulated above. If we ignore constant factors and logarithmic terms to gain intuition, we observe there is (roughly) a gap of N1/2 between the upper and lower bounds. Thus, it is possible that our analysis in section 3.1 may be conservative in terms of the ambient dimension. That is, to compensate for a two-fold increase in N , theorem 2 only shows a case where M needs to be increased two-fold, in contrast to the four-fold increase suggested by theorem 1. However, in terms of the number of sampled inputs, the lower and upper bounds match: thus, the rate cannot be improved beyond M\u22121/2."
        },
        {
            "heading": "3.3 A NEW ESTIMATOR WITH CONTROLLABLE BIAS",
            "text": "The plug-in estimator of \u2225\u03a3ij\u2225\u2217 has low variance but large and slowly decaying bias (see theorems 1 and 2). Here we develop an alternative estimator that is nearly unbiased.\nFirst, note that the eigenvalues of \u03a3ij\u03a3Tij correspond to the squared singular values of \u03a3ij . Thus, Tr[(\u03a3ij\u03a3Tij)\n1/2] = \u2225\u03a3ij\u2225\u2217, and so we can reduce our problem to estimating the trace of (\u03a3ij\u03a3 T ij)\n1/2, which is symmetric. Leveraging ideas from a well-developed literature (Adams et al., 2018), we proceed to define the pth moment of this matrix as:\nWp = Tr[(\u03a3ij\u03a3 T ij) p] = N\u2211 n=1 \u03bbpn (16)\nwhere \u03bb1, . . . , \u03bbN denote the eigenvalues of \u03a3ij\u03a3Tij . Now, for any function f : R 7\u2192 R and symmetric matrix S with eigenvalues \u03bb1, . . . , \u03bbN , we define2 Tr[f(S)] = \u2211 i f(\u03bbi). So long as f is reasonably well-behaved, we can approximate it using a truncated power series with P terms. Thus, with S = \u03a3ij\u03a3Tij and f(x) = \u221a x:\n\u2225\u03a3ij\u2225\u2217 = Tr[(\u03a3ij\u03a3Tij)1/2] \u2248 N\u2211\nn=1 P\u2211 p=0 \u03b3p\u03bb p n = P\u2211 p=0 \u03b3p N\u2211 n=1 \u03bbpn = P\u2211 p=0 \u03b3pWp (17)\nwhere \u03b30, . . . , \u03b3P are scalar coefficients.\nIn summary, we can estimate \u2225\u03a3ij\u2225\u2217 by (a) specifying an estimator of the top eigenmoments, W1, . . . ,WP , and (b) specifying a desired set of scalar coefficients \u03b30, . . . , \u03b3P . To estimate the\n2This is a common convention to extend scalar functions (see e.g. Potters & Bouchaud, 2020, sec. 1.2.6).\neigenmoments, we adapt procedures described by Kong & Valiant (2017) to obtain unbiased estimates for each moment, W\u03021, . . . , W\u0302P (see App. C). To select the scalar coefficients, we propose an optimization procedure that trades off between bias and variance in the estimate of \u2225\u03a3ij\u2225\u2217. Our starting point is the usual bias-variance decomposition:\nE [( \u2225\u03a3ij\u2225\u2217 \u2212 \u2211 p \u03b3pW\u0302p )2] = ( E [ \u2225\u03a3ij\u2225\u2217 \u2212 \u2211 p \u03b3pW\u0302p ])2 + Var [\u2211 p \u03b3pW\u0302p ] . (18)\nSince E[W\u0302p] = Wp = \u2211\nn \u03bb p n, the first term above (i.e. the \u201cbias\u201d) simplifies and is upper-bounded:( E [ \u2225\u03a3ij\u2225\u2217 \u2212 \u2211 p \u03b3pW\u0302p ])2 = (\u2211 n ( \u03bb 1/2 n \u2212 \u2211 p \u03b3p\u03bb p n ))2 \u2264 max\n0\u2264x\u22641\n( N ( x1/2 \u2212 \u2211 p \u03b3px p ))2\nThe inequality follows from replacing each term in the sum over n with the worst case approximation error of the polynomial expansion (given here as the maximization over x). Thus, we seek to:\nminimize \u03b30,...,\u03b3P max 0\u2264x\u22641\n( N ( x1/2 \u2212 \u2211 p \u03b3px p ))2 + \u2211 p,p\u2032 \u03b3p\u03b3p\u2032Cov(W\u0302p, W\u0302p\u2032). (19)\nWe estimate Cov(W\u0302p, W\u0302p\u2032) by bootstrapping\u2014i.e. the empirical covariance of these statistics across re-sampled datasets where {z1, . . . ,zM} are sampled with replacement. Given this estimate of covariance, eq. (19) can be cast as a convex quadratic program and the maximal bias can be bounded to a user defined limit at the expense of variance (see App. C.2). We use the maximal bias (eq. 19, term 1) and variance (eq. 19, term 2) to form approximate confidence intervals (see App. C.3)."
        },
        {
            "heading": "3.4 EXTENSION TO STOCHASTIC NETWORKS",
            "text": "Thus far, we have modeled neural networks as deterministic mappings, hi : Z 7\u2192 RN . This assumption is not satisfied in biological data and in many artificial networks (e.g. VAEs). Here, we briefly explain how to extend the estimators to the stochastic setting. In this setting, the response of network i can be written as hi(z) + \u03f5i(z). As before, hi(z) is a deterministic mapping conditioned on a random variable z \u223c P . The \u201cnoise\u201d term \u03f5i(z) is a mean-zero random variable that, in addition to inheriting the randomness of z, captures the stochastic elements of each forward pass through the network (i.e. trial-to-trial variability even when the stimulus is fixed). Importantly, noise contributions are independent and identically distributed for each pass through the network.\nGiven a second stochastic network with same structure, hj(z) + \u03f5j(z), our goal is to estimate the shape distances eqs. (2) and (3) as before, effectively ignoring contributions of the \u201cnoise\u201d terms \u03f5i(\u00b7) and \u03f5j(\u00b7). Ignoring these terms is not wholly justified, since it is of great interest to quantify how noise varies across networks (Duong et al., 2023). Nonetheless, it is useful to develop metrics that isolate the \u201csignal\u201d component of neural representations, and a full development of methods to quantify similarity in noise structure is outside the scope of this paper.\nOur basic observation is that it suffices to consider two replicates for each network input. That is, let z\u2032 = z where z \u223c P . Then, \u03a3ii = E[hi(z)hi(z\u2032)T] which can be approximated by the slightly reformulated plug-in estimator: \u03a3\u0302ii = (1/M) \u2211 m hi(zm)hi(z \u2032 m)\nT. Further, since noise is independent across networks, i.e. \u03f5i(z) \u22a5\u22a5 \u03f5j(z) for all z \u2208 Z , the cross-covariance estimators, including the method-of-moments estimator described in section 3.3, do not require any modification."
        },
        {
            "heading": "4 APPLICATIONS AND EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 VALIDATION ON SYNTHETIC DATA",
            "text": "We first validate our method-of-moments estimator (section 3.3) on simulated responses from a multivariate normal distribution. We estimate the cosine shape similarity, cos \u03b8, defined in eq. 7. Our estimator of \u2225\u03a3ij\u2225\u2217 is the principle novelty; thus, it is informative to understand its properties in isolation. To achieve this, we use the ground truth covariance of W\u0302p (instead of an estimate from a bootstrap) and use the ground truth values of Tr[\u03a3ii] and Tr[\u03a3jj ]. For details see App. D.1.\nWe first compared the bias of the plug-in estimator to that of the moment-based estimator across a range of ground truth shape similarity values (Fig. 6A). As expected from our intuition discussed in\nsection 3.1, the plug-in estimator (blue line) tends to inflate estimated similarity when ground truth is low (left side of plot). The moment-based estimator (orange line), in contrast, performs well over the full range, at the cost of increases in estimator variance (blue vs orange error bars).\nNext, we fixed the ground truth similarity at 0.2 and studied the effect of sample size, M (Fig. 6B).The moment estimator (constrained to 5% bias) maintains small bias even with small M , at the cost of high variance (orange error bars). Increasing M quickly reduces the variance of the estimator. A similar story emerges when we fix M and vary the dimension N (Fig. 6C). As the dimensionality increases, the plug-in estimator bias quickly explodes. In contrast, the moment estimator (constrained to 10% bias) has roughly constant bias; but it\u2019s variance grows with N . Thus our estimator bias outperforms the plug-in when the sample size is low and dimensionality is high.\nFinally, an important property of the moment-based estimator is our ability to compute approximate confidence intervals (CI) (see App. C.3). We demonstrate 95% CIs across simulations in Figure 6D. These CIs are conservative, the true shape score is not within the CI\u2019s for only 2.3% of simulations.\nControl of estimator bias: Here we demonstrate the bias-variance tradeoff controlled by the upperbound on bias defined by the user. The quadratic program in eq. (19) constraings the maximal absolute bias below a chosen constant (Fig. 3A, blue shaded area around true similarity score). The actual maximal bias will then be less than or equal to this user defined bias (cyan shaded area within blue). The expected value of the moment estimator stays within the maximal bias, in this case on its bound (orange trace). The user defined bias bound remains inactive until it is less than the MSE minimizing solution\u2019s bias (blue completely overlapped by cyan above 0.1). Variance then begins to increase as higher order Wp terms are weighted more to reduce bias (orange standard deviation bars from simulation increase as cyan region narrows). The mean of the estimator converges to ground truth as it is constrained by the bias bound (dotted orange line converges to dashed black). The plug-in estimator exceeds the maximal bias of the moment estimator (blue trace above cyan area).\nIntuition for the moment estimator can be drawn from plots of solutions to the polynomial approximation (eq. 17, Fig. 3B, orange trace approximates black dashed) of the squared singular values of \u03a31,2 (black points all overlapping). Here we have re-scaled the the vertical axis so that the deviation between the square root and polynomial approximation is exactly the bias of the moment estimator. In the case where bias is not constrained (associated with left most estimates in panel B) the approximation is poor (dashed-dot orange trace does not match dashed black trace). For these eigenvalues the the deviation is near the worst possible bias (distance from black point dashed dot orange line is nearly as far as any other vertical deviation between the traces), this is why the estimator in panel B sits at the bound of maximal possible bias. When the upper bound on bias is very small (far right of B) the approximation is very good (dashed orange overlaps dashed black) because higher order terms are used. Yet this results in very high variance (Fig. 3B)."
        },
        {
            "heading": "4.2 VALIDATION ON NEURAL DATA",
            "text": "Here we demonstrate the estimator applied to noisy non-normal data where covariance of the W\u0302p and the denominator of the similarity score must be estimated from data. We do so by applying\nour estimator to neural data: calcium recordings from mouse primary visual cortex in responses to a set of 2,800 natural images repeated twice (Stringer et al., 2019). Our estimator became highly variable when applied to this data in part because of its low SNR (average SNR \u2248 0.1). We thus apply our estimator to the neurons with the highest SNR in each recording. To assess variability of the estimates we ran independent simulations from the same distribution by randomly sub sampling stimuli presentations within a recording into 3 disjoint sets.\nDetermining the properties of the bias of our estimator requires comparison to the ground truth value of the similarity score. In the neural data ground truth is unknown. We thus developed two sampling schemes to set the ground truth similarity in the neural data. To set similarity to 0 we measured similarity between different populations of neurons shown different stimuli, thus the two populations responses are independent, thus their cross covariance is 0 so that the similarity score is 0. To set the similarity to 1 we measured similarity between the same population of neurons shown the same stimuli but on different trials, thus the only deviation in their responses is owing to trial-to-trial variability, thus their tuning similarity is 1.\nWe applied our estimator to populations of neurons (N = 40 each) where the ground truth was zero. We found that across recordings the moment estimator correctly indicated the similarity was\nnear 0 (Fig. 4A, orange trace overlaps black dashed) and the confidence intervals always contained the true similarity (light orange contains black dashed). On the other hand the plug-in estimator was upwardly biased (blue above black dashed). Thus the moment based estimator can accurately determine when the similarity is low in noisy neural data whereas the plug-in estimator cannot.\nWhen ground truth similarity was 1, we found the bias of the moment estimator was worse than that of the plug-in (Fig. 4B, blue overlaps black dashed, orange below). This is consistent with our synthetic simulations (see Fig. 6A far right). The CIs always contained the true value but contained nearly the entire possible range of similarity values. Thus while the average estimate is high our confidence intervals are so wide that we do not have much information about the true similarity.\nFinally, we assessed the estimators\u2019 performance measuring the true similarity between these populations of high SNR neurons (Fig. 4C). Across recordings the moment estimator was near 0.5 but confidence intervals were wide so there is little information about similarity even for the highest SNR neurons (light orange extends from 0 to 1 on vertical axis). The plug-in estimator reports a higher degree of similarity, that we heavily discount given its upward bias. When we included all stimuli (M \u2248 2800) we obtained more accurate estimates, learning that the true similarity is most likely between 0.25 and 0.75 (Fig. 4D). Thus small populations of well-tuned neurons in the same brain region have only intermediate levels of representational similarity. Overall, we find noisy data is a challenging setting for reducing the bias of shape similarity estimates."
        },
        {
            "heading": "5 DISCUSSION",
            "text": "There is a vast literature of papers that utilize or develop measures of representational similarity between neural networks (see Klabunde et al., 2023, for review). Recent works have shown interest in leveraging distances that satisfy the triangle inequality (Williams et al., 2021; Lange et al., 2022; Duong et al., 2023; Giaffar et al., 2023), yet the statistical properties of these shape distance measures is understudied. Here, we theoretically characterized \u201cplug-in\u201d estimates of shape distance in high-dimensional, noisy, and sample-limited regimes. We found that these estimates (a) tend to over-estimate representational similarity when the true similarity is small and (b) require a large number of samples, M , to overcome this bias in high-dimensional regimes. Theorems 1 and 2 provide precise guarantees on the worst-case performance of plug-in estimators, which should guide the design of biological experiments and analyses of their statistical power.\nAn equally important contribution of our work is to provide a practical method to (a) reduce the bias of plug-in estimators of shape distance, (b) quantify uncertainty in shape distance estimates, and (c) enable practicioners to explicitly trade off estimator bias and variance. When employed on a biological dataset published by Stringer et al. (2019), we find that shape similarity estimates are highly uncertain, revealing the challenging nature of the problem in high dimensions and with noisy data. Importantly, this degree of uncertainty is not obvious from the procedures and plug-in estimates advertised by existing work on this subject.\nBoth theoretical and methodological aspects of our work may be of broader interest beyond the immediate subject of shape distance estimation. We have seen that estimating the nuclear norm of the cross-covariance, \u2225\u03a3ij\u2225\u2217, is the key challenge in our problem. Estimating the spectrum of crosscovariance matrices is a topic of contemporary interest (Benaych-Georges et al., 2023), and further exploring the connections between this problem and shape distance estimation is an intriguing direction. Similarly, the method-of-moments estimator presented in section 3.3 is broadly applicable to generalized trace estimation (Adams et al., 2018). While others have used polynomial expansions in this context (Lin et al., 2016), a key novelty of our approach is the selection of coefficients with a tunable parameter that explicitly trades off estimator bias and variance. A more typical approach would be to choose these coefficients based on a Chebyshev polynomial expansion. While elegant, we believe our procedure for tuning these coefficients will be more relevant to scientific applications where samples are limited (such as neural data) and practitioners desire finer-scale control.\nIn summary, our work is one of the first to rigorously interrogate the statistical challenges of estimating shape distances in high-dimensional spaces. While shape distances can be well-behaved in certain settings (e.g. in artificial networks where a very large number of inputs can be sampled), our theoretical results and empirical observations underscore the challenging nature of this problem, suggesting the need for carefully designed biological experiments and estimation procedures."
        },
        {
            "heading": "A APPENDIX: BACKGROUND ON GENERALIZED SHAPE METRICS",
            "text": "Here we provide several relevant derivations for generalized shape metrics. For a more thorough review, we direct the reader to Williams et al. (2021) for the foundational results on generalized shape metrics and Duong et al. (2023) for the extension to stochastic neural networks.\nWe can intuitively think of the Procrustes distance as the Euclidean distance between two vectors remaining when the rotations and reflections have been \u201cremoved\u201d. Similarly, the Riemannian shape distance can be thought of as the angle between two vectors after these rotations and reflections are removed. These definitions in eq. (2) and eq. (3) also make clear that Procrustes distance, like Euclidean distance, is sensitive to the overall scaling of hi or hj , while the Riemannian shape distance, like the angle between vectors, is scale-invariant."
        },
        {
            "heading": "A.1 EQUIVALENCE OF EQS. (2) AND (6); EQS. (3) AND (7)",
            "text": "The squared Procrustes can be reformulated in terms of the covariance and cross-covariance matrices as follows:\n\u03c12(hi, hj) = min Q\u2208O(N) E\u2225hi(z)\u2212Qhj(z)\u222522\n= min Q\u2208O(N)\nE [ hi(z) Thi(z) + hj(z) Thj(z)\u2212 2hi(z)TQhj(z) ] = E [ hi(z) Thi(z) ] + E [ hj(z) Thj(z) ] \u2212 2 max Q\u2208O(N) E [ hi(z) TQhj(z) ]\n= E [ Tr [ hi(z)hi(z) T ]] + E [ Tr [ hj(z)hj(z) T ]]\n\u2212 2 max Q\u2208O(N)\nE [ Tr [ Qhj(z)hi(z) T ]]\n= Tr [ E [ hi(z)hi(z) T ]] +Tr [ E [ hj(z)hj(z) T ]]\n\u2212 2 max Q\u2208O(N)\nTr [ QE [ hj(z)hi(z) T ]]\n= Tr [\u03a3ii] + Tr [\u03a3jj ]\u2212 2 max Q\u2208O(N) Tr [Q\u03a3ij ]\n= Tr [\u03a3ii] + Tr [\u03a3jj ]\u2212 2\u2225\u03a3ij\u2225\u2217\nSimilarly for the cosine Riemannian distance:\ncos \u03b8(hi, hj) = max Q\u2208O(N)\n( E[hi(z)TQhj(z)]\u221a\nE[hi(z)Thi(z)]E[hj(z)Thj(z)]\n)\n= maxQ\u2208O(N) E\n[ Tr[Qhj(z)hi(z) T] ]\u221a\nE [Tr[hi(z)hi(z)T]]E [Tr[hj(z)hj(z)T]]\n= maxQ\u2208O(N) Tr\n[ QE[hj(z)hi(z)T] ]\u221a Tr [E[hi(z)hi(z)T]] Tr [E[hj(z)hj(z)T]]\n= maxQ\u2208O(N) Tr [Q\u03a3ij ]\u221a\nTr [\u03a3ii] Tr [\u03a3jj ] = \u2225\u03a3ij\u2225\u2217\u221a Tr [\u03a3ii] Tr [\u03a3jj ]"
        },
        {
            "heading": "A.2 REFORMULATIONS OF THE PLUG-IN ESTIMATOR OF PROCRUSTES DISTANCE",
            "text": "Let z1, . . . ,zM denote a set of independently and identically distributed samples in the network input space. Then, stack the responses of network i row-wise into a matrix Xi \u2208 RM\u00d7N . Given this set up, a common definition of Procrustes distance is (Gower & Dijksterhuis, 2004):\nmin Q\u2208O(N) 1\u221a M \u2225Xi \u2212XjQ\u2225F (20)\nHere, we have included a multiplying factor of 1/ \u221a M for reasons that will become clear shortly. Aside from this factor, the quantity above is how Williams et al. (2021) define the Procrustes distance. Below, we show that the square of this quantity is indeed the plug-in estimator we defined in\neq. (10) in terms of the empirical covariance matrices:\nmin Q\u2208O(N)\n1\nM \u2225Xi \u2212XjQ\u22252F = min Q\u2208O(N)\n1\nM\n( Tr[XTi Xi] + Tr[X T jXj ]\u2212 2Tr[XiXTjQ] ) = Tr [ 1 MX T i Xi ] +Tr [ 1 MX T jXj ] \u2212 2 max Q\u2208O(N) Tr [ 1 MXiX T jQ ]\n= Tr [ \u03a3\u0302ii ] +Tr [ \u03a3\u0302jj ] \u2212 2 max Q\u2208O(N) Tr [ \u03a3\u0302ijQ ] = Tr [ \u03a3\u0302ii ] +Tr [ \u03a3\u0302jj ] \u2212 2\u2225\u03a3\u0302ij\u2225\u2217\n= \u03c1\u03022(hi, hj)"
        },
        {
            "heading": "B APPENDIX: PLUG-IN ESTIMATOR THEORY",
            "text": "Here we provide a number of derivations related to the behavior of the plug-in estimator for generalized shape metrics. These results primarily rely on classic concentration inequalities and results from random matrix theory. For readers interested in further background, we provide pointers to Wainwright (2019) and Tropp (2015) for the concentration inequalities and Potters & Bouchaud (2020) for the random matrix theory."
        },
        {
            "heading": "B.1 PROOF OF LEMMA 1",
            "text": "Here we show that the plug-in estimate of the total variance Tr[\u03a3\u0302ii] converges to the true variance Tr[\u03a3ii] exponentially fast as M increases. We begin with some algebraic manipulations:\u2223\u2223\u2223Tr[\u03a3ii \u2212 \u03a3\u0302ii]\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223Tr [Ez\u223cP [hi(zm)hi(zm)T]\u2212 1M M\u2211\nm=1 hi(zm)hi(zm)\nT ]\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223Ez\u223cP [Tr[hi(zm)hi(zm)T]]\u2212 1M M\u2211 m=1 Tr[hi(zm)hi(zm) T] \u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223Ez\u223cP [Tr[hi(zm)Thi(zm)]]\u2212 1M M\u2211 m=1 Tr[hi(zm) Thi(zm)] \u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223Ez\u223cP [hi(zm)Thi(zm)]\u2212 1M M\u2211 m=1 hi(zm) Thi(zm) \u2223\u2223\u2223\u2223 where we have used the property Tr[xxT] = xTx for any column vector x in the last two lines.\nThe main assumption we are going to make is that the neural responses are constrained to an \u21132 ball of radius B \u221a N or equivalently hi(zm)Thi(zm) \u2264 B2N for all stimuli in the support of P . Note that this is a reasonable assumption in both biological (energy constraints) and artificial neural networks (weight decay common). Lemma 3 (Bounded Random Variables are Sub-Gaussian, Wainwright (2019) Example 2.4). We say that a random variable X with mean \u00b5 is sub-Gaussian with parameter \u03c3 if:\nE [ e\u03bb(X\u2212\u00b5) ] \u2264 e\u03c3 2\u03bb2/2 for all \u03bb \u2208 R\nIntuitively, this means that the tails of X fall off faster than a Gaussian. Furthermore, if X is mean zero and supported on the interval [a, b], the X is sub-Gaussian with parameter \u03c3 = (b\u2212 a)/2.\nThus our assumption implies that each term with 1M hi(zm) Thi(zm) is sub-Gaussian with parameter\n\u03c3 = B \u221a N/M . We can then immediately apply the Hoeffding bound (Wainwright, 2019, Proposition 2.5) to obtain:\nP [\u2223\u2223\u2223Tr[\u03a3ii \u2212 \u03a3\u0302ii]\u2223\u2223\u2223 \u2265 t] \u2264 2 exp [\u2212 Mt2\n2B2N\n] (21)\nAnalogously for term (B) we obtain:\nP [\u2223\u2223\u2223Tr[\u03a3jj \u2212 \u03a3\u0302jj ]\u2223\u2223\u2223 \u2265 t] \u2264 2 exp [\u2212 Mt2\n2B2N\n] (22)"
        },
        {
            "heading": "B.2 PROOF OF LEMMA 2",
            "text": "Our main tool is the matrix Bernstein inequality, given as theorem 6.1.1 in Tropp (2015). We paraphrase a version of the theorem here to keep our narrative self-contained.\nTheorem 3 (Matrix Bernstein). Consider a finite sequence {S1, . . . ,SM} of independent, random N \u00d7N matrices. Assume that:\nE [ Sm ] = 0 and \u2225Sm\u2225\u221e \u2264 L for each index m (23)\nwhere \u2225Sm\u2225\u221e = sup{\u2225Smv\u22252 : \u2225v\u22252 \u2264 1} is the matrix operator norm. Further, define the variance of the sum \u2211 m Sm as:\nV = \u2225\u2225\u2211\nmES T mSm \u2225\u2225 \u221e = \u2225\u2225\u2211 mESmS T m \u2225\u2225 \u221e (24)\nThen:\nE [ \u2225\u2225\u2211 mSm \u2225\u2225 \u221e ] \u2264 \u221a 2V log(2N) + L\n3 log(2N) (25)\nWe now turn to the proof of theorem 1. Define:\nSm = 1\nM\n( hi(zm)hj(zm) T \u2212\u03a3ij )\n(26)\nfor the sequence of network inputs {z1, . . . ,zM}. Notice that:\nE [ Sm ] = 1\nM\n( E [ hi(zm)hj(zm) T ] \u2212\u03a3ij ) = 1\nM (\u03a3ij \u2212\u03a3ij) = 0 (27)\nNext, due to triangle inequality, we have:\n\u2225Sm\u2225\u221e = 1\nM \u2225\u2225hi(zm)hj(zm)T \u2212\u03a3ij\u2225\u2225\u221e \u2264 1M \u2225\u2225hi(zm)hj(zm)T\u2225\u2225\u221e\ufe38 \ufe37\ufe37 \ufe38 (1) + 1 M \u2225\u03a3ij\u2225\u221e\ufe38 \ufe37\ufe37 \ufe38 (2)\n(28)\nTerms (1) and (2) are each upper bounded by B2N , since for term (1):\u2225\u2225hi(zm)hj(zm)T\u2225\u2225\u221e \u2264 \u2225\u2225hi(zm)hj(zm)Tv\u2225\u22252 (for any vector \u2225v\u22252 \u2264 1) (29) = hj(zm)\nTv \u2225hi(zm)\u22252 (30) \u2264 \u2225hj(zm)\u22252 \u2225v\u22252 \u2225hi(zm)\u22252 (Cauchy-Schwarz inequality) (31) \u2264 B \u221a N \u00b7 1 \u00b7B \u221a N = B2N (From assumptions in eq. 1) (32)\nAnd for term (2): \u2225\u03a3ij\u2225\u221e = \u2225\u2225Ehi(z)hj(z)T\u2225\u2225\u221e (33)\n\u2264 \u2225\u2225Ehi(zm)hj(zm)Tv\u2225\u22252 (for any vector \u2225v\u22252 \u2264 1) (34)\n\u2264 E \u2225\u2225hi(zm)hj(zm)Tv\u2225\u22252 (Jensen\u2019s inequality) (35) \u2264 B2N (Repeat the upper bound on term 1) (36)\nTo summarize, we have:\n\u2225Sm\u2225\u221e \u2264 1\nM \u2225\u2225hi(zm)hj(zm)T\u2225\u2225\u221e + 1M \u2225\u03a3ij\u2225\u221e \u2264 2B2NM (37) That is, we have shown that the assumptions of eq. (23) are satisfied with L = 2B2N/M .\nOur next task is to determine an expression for the variance V defined in eq. (24). First, we have:\nESTmSm = 1\nM2 E[hj(zm)hi(zm)Thi(zm)hj(zm)T +\u03a3Tij\u03a3ij \u2212\u03a3Tijhj(zm)hi(zm)T \u2212 hi(zm)hj(zm)T\u03a3ij ]\n= 1\nM2 E[hj(zm)hi(zm)Thi(zm)hj(zm)T] +\u03a3Tij\u03a3ij \u2212\u03a3TijE[hj(zm)hi(zm)T]\u2212 E[hi(zm)hj(zm)T]\u03a3ij\n= 1\nM2 E[hj(zm)hi(zm)Thi(zm)hj(zm)T] +\u03a3Tij\u03a3ij \u2212\u03a3Tij\u03a3ij \u2212\u03a3Tij\u03a3ij\n= 1\nM2 E[hj(zm)hi(zm)Thi(zm)hj(zm)T]\u2212\u03a3Tij\u03a3ij\nThen, by triangle inequality:\n\u2225ESTmSm\u2225\u221e = 1\nM2 \u2225E[hj(zm)hi(zm)Thi(zm)hj(zm)T]\u2212\u03a3Tij\u03a3ij\u2225\u221e\n\u2264 1 M2 \u2225E[hj(zm)hi(zm)Thi(zm)hj(zm)T]\u2225\u221e\ufe38 \ufe37\ufe37 \ufe38 (A) + 1 M2 \u2225\u03a3Tij\u03a3ij\u2225\u221e\ufe38 \ufe37\ufe37 \ufe38\n(B)\nTerms (A) and (B) are each upper bounded by N2. First, taking term (A):\u2225\u2225E [hj(zm)hi(zm)Thi(zm)hj(zm)T]\u2225\u2225\u221e \u2264 \u2225\u2225E [hj(zm)hi(zm)Thi(zm)hj(zm)Tv]\u2225\u22252 (for \u2225v\u2225 \u2264 1) \u2264 E\n\u2225\u2225hj(zm)hi(zm)Thi(zm)hj(zm)Tv\u2225\u22252 (Jensen\u2019s) \u2264 E [ hj(zm) Tv\u2225hi(zm)\u222522 \u2225hj(zm)\u22252 ]\n\u2264 E [ \u2225v\u22252\u2225hi(zm)\u222522 \u2225hj(zm)\u2225 2 2 ] (Cauchy-Schwarz)\n\u2264 1 \u00b7B2N \u00b7B2N = B4N2 (from eq. 1) For term (B), we first note that \u2225\u03a3Tij\u03a3ij\u2225\u221e \u2264 \u2225\u03a3ij\u22252\u221e due to the fact that the operator norm is submultiplicative. Then, term (B) is upper bounded by B4N2 follows readily from:\n\u2225\u03a3ij\u2225\u221e = \u2225Ehi(z)hj(z)T\u2225\u221e \u2264 \u2225Ehi(z)hj(z)Tv\u22252 (for \u2225v\u2225 \u2264 1) \u2264 E \u2225hi(z)hj(z)Tv\u22252 (Jensen\u2019s) \u2264 E \u2225hi(z)\u22252\u2225hj(z)\u22252\u2225v\u22252 (Cauchy-Schwarz) \u2264 B \u221a N \u00b7B \u221a N \u00b7 1 = B2N (from eq. 1)\nTaking these two bounds together, we have shown \u2225ESTmSm\u2225\u221e \u2264 2B4N2/M2. We are now ready to upper bound the variance term, V , appearing in theorem 3. Specifically, by the triangle inequality and the bounds above, we have:\nV = \u2225 \u2211\nmES T mS T m\u2225\u221e \u2264 M\u2211 m=1 \u2225ESTmSTm\u2225\u221e \u2264 2B4N2 M (38)\nWith this, we are equipped to apply the matrix Bernstein inequality to obtain an upper bound on the estimation error of the plug-in estimator. Specifically, we have:\u2223\u2223\u2223\u2225\u03a3\u0302ij\u2225\u2217 \u2212 \u2225\u03a3ij\u2225\u2217\u2223\u2223\u2223 \u2264 \u2225\u03a3\u0302ij \u2212\u03a3ij\u2225\u2217 (reverse triangle inequality)\n= \u2225 \u2211 m Sm\u2225\u2217\n\u2264 N\u2225 \u2211 m Sm\u2225\u221e\n\u2264 N \u221a 2V log(2N) + NL\n3 log(2N) (theorem 3)\n\u2264 2B2N2M\u22121/2 \u221a log(2N) + 2B2N2\n3M log(2N)\nWhere we have substituted the derived quantities L = 2B2N/M and V \u2264 2B4N2/M in the final line."
        },
        {
            "heading": "B.3 PROOF OF THEOREM 1",
            "text": "Lemma 2 provides an upper bound on the expected value on \u2223\u2223\u2223\u2225\u03a3ij\u2225\u2217 \u2212 \u2225\u03a3\u0302ij\u2225\u2217\u2223\u2223\u2223, which is the error\nof our plug-in estimate of cross-covariance nuclear norm. This bound holds for any true crosscovariance matrix \u03a3ij , provided that the constraints in eq. (1) are satisfied. However, this tells us nothing about how the estimation error deviates around its expectation.\nHere, we use the bounded differences inequality (Wainwright, 2019, Corollary 2.21), also called McDiarmid\u2019s inequality, to show that deviations around this expectation decrease exponentially fast. Thus, the upper bound on the expected error (theorem 1) provides accurate intuition. Lemma 4 (Bounded Differences Inequality, Wainwright (2019) Corollary 2.21). Consider a function f : Rn \u2192 R. The function is said to have the bounded difference property for the kth coordinate if there exists an Lk for which the following holds:\nmax X1:n\u2208Rn,X\u2032k\u2208R \u2223\u2223f(X1:n)\u2212 f(X1:k\u22121, X \u2032k, Xk+1:n)\u2223\u2223 \u2264 Lk Suppose f satisfies this property with L1, . . . , Ln for each coordinate respectively. Then the following inequality holds:\nP [\u2223\u2223\u2223\u2223f(X1:n)\u2212 E[f(X1:n)]\u2223\u2223\u2223\u2223 \u2265 t] \u2264 exp [\u2212 2t2\u2211n\ni=1 L 2 i\n] (39)\nWe start by applying the reverse triangle inequality:\n\u2223\u2223\u2223\u2225\u03a3ij\u2225\u2217 \u2212 \u2225\u03a3\u0302ij\u2225\u2217\u2223\u2223\u2223 \u2264 \u2225\u03a3ij \u2212 \u03a3\u0302ij\u2225\u2217 = \u2225\u2225\u2225\u2225\u03a3ij \u2212 1M M\u2211 m=1 hi(zm)hj(zm) T \u2225\u2225\u2225\u2225 \u2217\nWe can bound how much this changes if we change one coordinate of the function, i.e. if hi(z1) Thj(z1) is replaced by hi(z\u03031)Thj(z\u03031). The difference is then bounded by: \u2225\u2225\u2225\u2225\u03a3ij \u2212 1M M\u2211 m=1 hi(zm)hj(zm) T \u2225\u2225\u2225\u2225 \u2217 \u2212 \u2225\u2225\u2225\u2225\u03a3ij \u2212 ( 1M M\u2211 m=1 hi(zm)hj(zm) T \u2212 1 M hi(z1)hj(z1) T + 1 M hi(z\u03031)hj(z\u03031) T )\u2225\u2225\u2225\u2225 \u2217\n\u2264 \u2225\u2225\u2225\u2225 1M (hi(z1)hj(z1)T \u2212 hi(z\u03031)hj(z\u03031)T) \u2225\u2225\u2225\u2225 \u2217 = 1 M \u2225\u2225hi(z1)hj(z1)T \u2212 hi(z\u03031)hj(z\u03031)T\u2225\u2225\u2217 \u2264 1\nM (\u2225\u2225hi(z1)hj(z1)T\u2225\u2225\u2217 + \u2225\u2225hi(z\u03031)hj(z\u03031)T\u2225\u2225\u2217) = 1M (\u2223\u2223hi(z1)Thj(z1)\u2223\u2223+ \u2223\u2223hi(z\u03031)Thj(z\u03031)\u2223\u2223) Finally, we can apply Cauchy-Schwartz and our assumption about the neural activations being bounded to obtain:\n1\nM (\u2223\u2223hi(z1)Thj(z1)\u2223\u2223+ \u2223\u2223hi(z\u03031)Thj(z\u03031)\u2223\u2223) \u2264 1 M (\u2225hi(z1)\u22252\u2225hj(z1)\u22252 + \u2225hi(z\u03031)\u22252\u2225hj(z\u03031)\u22252)\n\u2264 2B 2N\nM\nThus we have \u2211M\ni=1 L 2 i = \u2211M i=1 4B\n4N2/M2 = 4B4N2/M , and we can apply the bounded differences inequality to obtain for all t \u2265 0:\nP [ \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2225\u03a3ij\u2225\u2217 \u2212 \u2225\u03a3\u0302ij\u2225\u2217\u2223\u2223\u2223\u2212 E \u2223\u2223\u2223\u2225\u03a3ij\u2225\u2217 \u2212 \u2225\u03a3\u0302ij\u2225\u2217\u2223\u2223\u2223 \u2223\u2223\u2223\u2223 \u2265 t] \u2264 2 exp [\u2212 Mt22B4N2 ] (40)\nFor the deviation from the expectation to be in the range [\u2212t, t] with probability 1\u2212 \u03b4 we require:\n2 exp [ \u2212 Mt 2\n2B4N2\n] \u2264 \u03b4\nSolving for t gives t \u2265 B2NM\u22121/2 \u221a\n2 log (2/\u03b4), and thus with probability 1 \u2212 \u03b4 the following holds: \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2225\u03a3ij\u2225\u2217 \u2212 \u2225\u03a3\u0302ij\u2225\u2217\u2223\u2223\u2223\u2212 E \u2223\u2223\u2223\u2225\u03a3ij\u2225\u2217 \u2212 \u2225\u03a3\u0302ij\u2225\u2217\u2223\u2223\u2223 \u2223\u2223\u2223\u2223 \u2264 B2NM\u22121/2\u221a2 log(2/\u03b4) To proceed we break this we use a basic identity of the absolute value: if |a\u2212 b| < c then a\u2212 b < c and also b\u2212 a < c. Thus, with probability at least 1\u2212 \u03b4, we have:\n\u2225\u03a3ij\u2225\u2217 \u2212 \u2225\u03a3\u0302ij\u2225\u2217 \u2264 E \u2223\u2223\u2223\u2225\u03a3ij\u2225\u2217 \u2212 \u2225\u03a3\u0302ij\u2225\u2217\u2223\u2223\u2223+ B2N\nM1/2\n\u221a 2 log(2/\u03b4)\n\u2264 2B 2N2\nM1/2\n\u221a log(2N) + 2B2N2\n3M log(2N) +\nB2N\nM1/2\n\u221a 2 log(2/\u03b4)\nAnd we also have with probability at least 1\u2212 \u03b4, we have:\n\u2225\u03a3\u0302ij\u2225\u2217 \u2212 \u2225\u03a3ij\u2225\u2217 \u2264 E \u2223\u2223\u2223\u2225\u03a3ij\u2225\u2217 \u2212 \u2225\u03a3\u0302ij\u2225\u2217\u2223\u2223\u2223+ B2N\nM1/2\n\u221a 2 log(2/\u03b4)\n\u2264 2B 2N2\nM1/2\n\u221a log(2N) + 2B2N2\n3M log(2N) +\nB2N\nM1/2\n\u221a 2 log(2/\u03b4)\nIn the final inequalities above, we have simply plugged in our expectation bound from lemma 2. The relations above imply that the following holds with probability 1\u2212 \u03b4:\u2223\u2223\u2223\u2225\u03a3ij\u2225\u2217 \u2212 \u2225\u03a3\u0302ij\u2225\u2217\u2223\u2223\u2223 \u2264 2N2 log(2B2N)\n3M +\n2B2N2 \u221a log(2N)\nM1/2 +\nB2N\nM1/2\n\u221a 2 log ( 2\n\u03b4\n) (41)\nTo complete the proof we need to combine the above tail bound with lemma 1. By the triangle inequality we have\n|\u03c1\u03022 \u2212 \u03c12| = \u2223\u2223\u2223Tr[\u03a3\u0302ii] + Tr[\u03a3\u0302jj ]\u2212 2\u2225\u03a3\u0302ij\u2225\u2217 \u2212 Tr[\u03a3\u0302ii]\u2212 Tr[\u03a3\u0302jj ] + 2\u2225\u03a3\u0302ij\u2225\u2217\u2223\u2223\u2223\n= \u2223\u2223\u2223Tr[\u03a3\u0302ii]\u2212 Tr[\u03a3ii] + Tr[\u03a3\u0302jj ]\u2212 Tr[\u03a3jj ] + 2\u2225\u03a3ij\u2225\u2217 \u2212 2\u2225\u03a3\u0302ij\u2225\u2217\u2223\u2223\u2223\n\u2264 \u2223\u2223\u2223Tr[\u03a3\u0302ii]\u2212 Tr[\u03a3ii]\u2223\u2223\u2223+ \u2223\u2223\u2223Tr[\u03a3\u0302jj ]\u2212 Tr[\u03a3jj ]\u2223\u2223\u2223+ 2 \u2223\u2223\u2223\u2225\u03a3ij\u2225\u2217 \u2212 \u2225\u03a3\u0302ij\u2225\u2217\u2223\u2223\u2223\nSetting \u03b4\u2032 = \u03b4/3 in our results for these three terms yields that the following three inequalities independently hold with probability \u03b4/3:\n\u2223\u2223\u2223Tr[\u03a3ii]\u2212 Tr[\u03a3\u0302ii]\u2223\u2223\u2223 \u2265 BN1/2M\u22121/2\u221a2 log(6/\u03b4)\u2223\u2223\u2223Tr[\u03a3jj ]\u2212 Tr[\u03a3\u0302jj ]\u2223\u2223\u2223 \u2265 BN1/2M\u22121/2\u221a2 log(6/\u03b4)\u2223\u2223\u2223\u2225\u03a3ij\u2225\u2217 \u2212 \u2225\u03a3\u0302ij\u2225\u2217\u2223\u2223\u2223 \u2265 2N2 log(2B2N) 3M + 2B2N2 \u221a log(2N) M1/2 + B2N M1/2 \u221a 2 log ( 6 \u03b4 )\nBy applying the union bound, we obtain that all three inequalities hold simultaneously with probability \u2264 \u03b4/3 + \u03b4/3 + \u03b4/3 = \u03b4. The three reverse inequalities then hold simultaneously with probability greater than or equal to 1\u2212 \u03b4. Thus with probability at least 1\u2212 \u03b4, the following holds:\n|\u03c1\u03022 \u2212 \u03c12| \u2264 2B 2N2 log(2N)\n3M +\n2B2N2 \u221a log(2N)\nM1/2 +\n( NB2\nM1/2 +\n2N1/2B\nM1/2\n)\u221a 2 log ( 6\n\u03b4\n)\nas claimed in theorem 1."
        },
        {
            "heading": "B.4 PROOF OF THEOREM 2 (LOWER BOUND ON PLUG-IN ESTIMATOR ERROR)",
            "text": "We derive a lower bound by constructing an explicit example where the plug-in estimator performs badly. Specifically, we consider a scenario where two networks have entirely decorrelated, highvariance representations. To do this, we use Rademacher random variables\u2014a random variable R is called a Rademacher variable if it behaves as follows:\nR = { +1 with probability 1/2 \u22121 with probability 1/2 (42)\nNow, suppose we sample M network inputs, z1, . . . ,zM \u223c P , independently. Further, let B > 0 be the constant appearing in eq. (1). For m \u2208 {1, . . . ,M} define\nXm = 1\nB hi(zm) and Ym =\n1 B hj(zm) (43)\nNote that Xm and Ym are N -dimensional random vectors. Due to eq. (1), we have \u2225hi(z)\u22252 \u2264 B \u221a N and \u2225hj(z)\u22252 \u2264 B \u221a N almost surely. Thus, \u2225Xm\u2225 \u2264 \u221a N and \u2225Ym\u2225 \u2264 \u221a N almost surely.\nDefine X = (1/B)hi(z) and Y = (1/B)hj(z) for randomly sampled z \u223c P . The case we will consider is that X and Y are each composed of N independent Rademacher variables. One trivial way to construct this is to suppose each z \u223c P is a random vector with 2N elements, all of which are independent Rademacher variables scaled by a factor B > 0. Then, let hi : R2N 7\u2192 RN be the function which extracts the first N elements of z and let hj : R2N 7\u2192 RN be the function which extracts the final N elements.\nThus, we have constructed a setting where X1, . . . , XM , Y1, . . . , YM are all composed of independent Rademacher variables. In this setting, the squared Procrustes distance is given by:\n\u03c12 = Tr[\u03a3ii] + Tr[\u03a3jj ]\u2212 2\u2225\u03a3ij\u2225\u2217 (44) = Tr[E[hi(z)hi(z)T]] + Tr[E[hj(z)hj(z)T]]\u2212 2\u2225E[hi(z)hj(z)T]\u2225\u2217 (45) = B2 \u00b7 ( Tr[E[XXT]] + Tr[E[Y YT]]\u2212 2\u2225E[XYT]\u2225\u2217 ) (46)\n= B2 \u00b7 ( E[XTX] + E[YTY ]\u2212 2\u2225E[X]E[YT]\u2225\u2217 ) (47)\n= B2 \u00b7 (N +N \u2212 0) (48) = 2B2N (49)\nwhere we have used the fact that X and Y are independent, mean zero, random vectors to conclude that the cross covariance is an N \u00d7N matrix filled with zeros. Furthermore, note that XTmXm = N and YTmYm = N almost surely for all m \u2208 1, . . . ,M since they are comprised of N Rademacher variables. Thus, the plug-in estimate of the squared Procrustes distance takes the form:\n\u03c1\u03022 = B2 \u00b7 ( Tr[ 1M \u2211 m XmX T m] + Tr[ 1 M \u2211 m YmY T m]\u2212 2\u2225 1M \u2211 m XmY T m\u2225\u2217 ) (50)\n= B2 \u00b7 (\n1 M \u2211 m X T mXm + 1 M \u2211 m Y T mYm \u2212 2\u2225 1M \u2211 m XmY T m\u2225\u2217 ) (51)\n= B2 \u00b7 ( N +N \u2212 2\u2225 1M \u2211 m XmY T m\u2225\u2217 ) (52)\n= 2B2N \u2212 2B2\u2225 1M \u2211 m XmY T m\u2225\u2217 (53)\nPutting these two results together, we conclude that the absolute error of the plug-in estimator is: |\u03c12 \u2212 \u03c1\u03022| = 2B2\u2225 1M \u2211 m XmY T m\u2225\u2217 (54)\nNow, the product of two indepedent Rademacher variables is also a standard Rademacher variable. Thus, each element inside the matrix (1/M) \u2211 m XmY T m, is the empirical average of M independent Rademacher variables. These matrix elements are asymptotically independent in the limit that M \u2192 \u221e. Further, the central limit theorem applies in this limit, and thus the distribution of each matrix element approaches a Gaussian distribution N (0, 1/M). Such random matrices are well-studied under the name of Ginibre ensembles. In the limit that N \u2192 \u221e and the variance of each matrix element is taken to be \u03c32/N , the density of the singular values takes the following form (see e.g. Potters & Bouchaud, 2020, sec. 3.1.3):\n\u03c1(s) = \u221a 4\u03c32 \u2212 s2 \u03c0\u03c32 s \u2208 (0, 2\u03c3) (55)\nThis is called the quarter circle law since if we look at the density of s it forms a quarter circle. The nuclear norm of the matrix is N times the expected value of s with with respect to the density \u03c1(s). Integrating this density, we obtain:\nlim N\u2192\u221e M\u226bN\n\u2225\u2225 1 M \u2211 m XmY T m \u2225\u2225 \u2217 = N\n\u03c0\u03c32 \u222b 2\u03c3 0 s \u221a 4\u03c32 \u2212 s2 ds (56)\n= N\n4\u03c0\u03c32 [ \u22121 3 (4\u03c32 \u2212 s2)3/2 ]2\u03c3 0\n(57)\n= N\n\u03c0\u03c32\n[ 1\n3 (4\u03c32)3/2\n] = N\n\u03c0\u03c32\n[ 8 3 \u03c33 ]\n(58)\n= 8\u03c3\n3\u03c0 N =\n8\n3\u03c0 N3/2M\u22121/2 (59)\nWhere in the last line we have substituted \u03c3 = \u221a\nN/M , which comes from equating \u03c32/N (the variance in of each matrix element in eq. 55) with 1/M (the variance given by the average of M Rademacher variables under the central limit theorem). Note that the analysis above holds asymptotically as M,N \u2192 \u221e and we keep M \u226b N so that the central limit theorem continues to hold. Plugging eq. (59) into eq. (54) and dividing both sides by N we arrive at the expression appearing in theorem 2."
        },
        {
            "heading": "C APPENDIX: METHOD-OF-MOMENTS ESTIMATOR",
            "text": ""
        },
        {
            "heading": "C.1 DERIVATION OF METHOD-OF-MOMENT ESTIMATOR",
            "text": "We now turn to constructing our method-of-moments estimator of \u2225\u03a3ij\u2225\u2217 = \u2211N\nn=1 sn(\u03a3ij), which is required for our novel estimator of the Riemannian shape distance. We can form an unbiased estimator of the matrix \u03a3ij by observing a single random stimuli in the two networks:\n\u03a3\u0302ijm := hi(zm)hj(zm) T \u2208 RN\u00d7N , E[\u03a3\u0302ijm] = \u03a3ij\nNote that here the randomness comes from the selection of the stimuli, i.e. zm \u223c P ; the output of the network is deterministic. Assuming m,m\u2032 are distinct stimuli drawn independently from the distribution P , we then have:\nE [ \u03a3\u0302ijm\u03a3\u0302ijm\u2032 ] = \u03a3ij\u03a3 T ij\nThis means we can estimate \u03a3ij\u03a3Tij by observing a pair of stimuli in both networks.\nTr [ f(\u03a3ij\u03a3 T ij) ] = N\u2211 n=1 f ( s2n(\u03a3ij) ) = N\u2211 n=1 \u221e\u2211 p=0 \u03b3ps 2p n (\u03a3ij) Taylor expansion of f(\u00b7)\n= \u221e\u2211 p=0 \u03b3p N\u2211 n=1 s2pn (\u03a3ij) = \u221e\u2211 p=0 \u03b3p Tr [( \u03a3ij\u03a3 T ij )p] Tr [( \u03a3ij\u03a3 T ij )p] = N\u2211 n=1 s2pn (\u03a3ij)\n= \u221e\u2211 p=0 \u03b3pE\n[ Tr [ p\u220f\n\u03c3=1\n\u03a3\u0302ij(2\u03c3\u22121)\u03a3\u0302 T ij(2\u03c3) ]] Substitute unbiased estimator for ( \u03a3ij\u03a3 T ij )p \u2248\nP\u2211 p=0 \u03b3pE\n[ Tr [ p\u220f\n\u03c3=1\n\u03a3\u0302ij(2\u03c3\u22121)\u03a3\u0302 T ij(2\u03c3) ]] Approximate with truncated power series\nOur estimator for the nuclear norm of \u03a3ij is thus:\n\u2225\u0302\u03a3ij\u2225\u2217 = P\u2211\np=0\n\u03b3p Tr\n[ p\u220f\n\u03c3=1\n\u03a3\u0302ij(2\u03c3\u22121)\u03a3\u0302 T ij(2\u03c3) ] (60)\nNote that for each element of the product we are considering the estimator based on stimuli (2\u03c3\u22121) and (2\u03c3); in total this estimator will use 2P unique stimuli."
        },
        {
            "heading": "C.2 DERIVING THE QUADRATIC PROGRAM",
            "text": "The optimization problem in eq. (19) takes the form:\nminimize \u03b3\n\u03b3TA\u03b3 +N2 ( max\nx f2(\u03b3, x)\n) (61)\nwhere f(\u03b3, x) = x1/2 \u2212 \u2211\np \u03b3px p,\n\u03b3 = \u03b31... \u03b3P  \u2208 RP , A = Cov(W\u03021, W\u03021) . . . Cov(W\u03021, W\u0302P )... ... Cov(W\u0302P , W\u03021) . . . Cov(W\u0302P , W\u0302P )  \u2208 RP\u00d7P , (62) Notice that f is linear in \u03b3, and that A is symmetric, positive-definite.\nWe will reformulate eq. (61) in several steps, and ultimately obtain a quadratic program that can be efficiently solved. First, we introduce a new optimization variable u \u2208 R whose square is an upper bound on f2(\u03b3, x) for all x \u2208 [0, 1]. Thus, the optimal \u03b3 for the problem:\nminimize \u03b3,u\n\u03b3TA\u03b3 +N2u2\nsubject to u2 \u2265 f2(\u03b3, x) for all x \u2208 [0, 1] (63)\ncoincides to the optimal \u03b3 solving eq. (61). This is essentially an epigraph reformulation of the original problem (see Boyd & Vandenberghe, 2004, equation 4.11). Notice that the objective function is quadratic in this reformulation.\nNext, we lay down a fine grid of linearly spaced test points x1, . . . , xT \u2208 [0, 1]. We can then obtain a good approximation to the solution in eq. (63) by solving:\nminimize \u03b3,u\n\u03b3TA\u03b3 +N2u2\nsubject to u2 \u2265 f2(\u03b3, xt) for all t \u2208 1, . . . , T (64)\nOf course, increasing T (the number of test points) improves the approximation arbitrarily well.\nFinally, the constraints of the problem can be put into a form that is jointly linear in \u03b3 and u. First, constraining u2 \u2265 f2(\u03b3, xt) is equivalent to simultaneously constraining u \u2265 f(\u03b3, xt) and u \u2265 \u2212f(\u03b3, xt). Then, plugging in the definition of f(\u03b3, xt), and rearranging we have:\nminimize \u03b3,u\n\u03b3TA\u03b3 +N2u2\nsubject to u+ \u2211 p \u03b3px p t \u2265 x 1/2 t for all t \u2208 1, . . . , T\nu\u2212 \u2211 p \u03b3px p t \u2265 \u2212x 1/2 t for all t \u2208 1, . . . , T\n(65)\nThis objective is quadratic and the constraints are linear with respect to the optimized quantities. Thus, a solution (approximated to high accuracy) can be achieved efficiently using off-the-shelf quadratic programming solvers. To enforce the user defined bound on the bias a final two constraints are be appended to eq. (65): \u2212Nu \u2265 \u2212c and Nu \u2265 \u2212c, where c is the upper bound on the absolute bias."
        },
        {
            "heading": "C.3 CONFIDENCE INTERVALS",
            "text": "To form approximate \u03b1 level confidence intervals around \u2225\u0302\u03a3ij\u2225\u2217 we use the maximal bias (eq. 19, term 1) and variance (eq. 19, term 2) from the quadratic program\u2019s solution:[\n\u2225\u0302\u03a3ij\u2225\u2217 \u2212 z \u2217 \u221a \u03b3TA\u03b3 \u2212Nu, \u2225\u0302\u03a3ij\u2225\u2217 + z \u2217 \u221a \u03b3TA\u03b3 +Nu ] ,\nwhere z\u2217 is the critical value of the standard normal. For confidence intervals of the similarity score we scale this interval by the denominator of the similarity score."
        },
        {
            "heading": "D APPENDIX: EXPERIMENT DETAILS",
            "text": ""
        },
        {
            "heading": "D.1 SIMULATED EXPERIMENTS",
            "text": "To draw data for our simulations, we set the eigenvalues of the \u03a3ii and the singular values of \u03a3ij to a ground truth nuclear norm and similarity score. To demonstrate the estimators accuracy across the space of orthogonal transformations we apply a random orthogonal rotation matrix to each population\u2019s covariance in each new parameter setting."
        },
        {
            "heading": "D.2 EXPERIMENTAL DATA FROM STRINGER ET AL. (2019)",
            "text": "Neural activity in mouse primary visual cortex was recorded using a two-photon microscope while mice were free to run on an air-floating ball. Recordings were collected across multiple depth planes at a frequency of 2.5 or 3 Hz, with planes 30-35 \u00b5m apart. The field of view of the microscope was selected such that 10,000 neurons could be observed within a retinotopic location on the stimulus display.\nAll stimuli were presented for 0.5s with a random inter-stimulus interval between 0.3 and 1.1s consisting of a grey-screen. The images used in the experiment were taken from the ImageNet database, which includes categories such as birds, cats, and insects. The researchers manually selected images that had a mix of low and high spatial frequencies and that did not consist of more than 50 % uniform background. All images were uniformly contrast-normalized by subtracting the local mean brightness and dividing by the local mean contrast. Each stimulus consisted of a different normalized image from the ImageNet database, with 2,800 different images used in total. The same image was displayed on all three screens, but each screen showed the image at a different rotation. Each of the 2,800 natural image stimuli were displayed twice in a recording in two blocks of the same randomized order.\nCalcium movie data was processed using the Suite2p toolbox to estimate spike rates of neurons. Underlying neural activity was estimated using non-negative spike deconvolution (Frierich et. al., 2017). These deconvolved traces were normalized to the mean and standard deviation of their activity during a 30-minute period of grey-screen spontaneous activity. For further detail please see the original study Stringer et al. (2019). All analyses done in this paper were performed on the pre-processed data available on figshare (https://figshare.com/articles/Recordings_of_ten_thousand_neurons_ in_visual_cortex_in_response_to_2_800_natural_images/6845348)."
        },
        {
            "heading": "E APPENDIX: APPLICATIONS TO DEEP LEARNING",
            "text": "Here we apply the plug-in and moment based estimator to neural network responses to demonstrate impacts of the differences between these estimators and relevance to neural networks. We make the point that the bias of the plug-in estimator, but not the moment estimator, is substantial for small samples. Furthermore, plug-in bias depends on the effective dimensionality of the two populations. Thus, naively using the plug-in can lead to erroneous scientific conclusions because the estimate bias can correlate with irrelevant nuisance variables. Concretely, we find the plug-in estimator bias tends to decrease with the effective dimensionality of neural populations. Thus if similarity between two populations appears to be explained by some manipulation of interest (e.g., training regime) it can be confounded through variation in effective dimensionality.\nA common question in the study of neural network representations is how two networks with the same architecture trained on the same task but with different initializations and training procedures are similar/different from each other. To show how the estimators considered in this paper can be used as a tool to study this question, we considered two ResNet-50 (He et al., 2016) architectures trained to categorize ImageNet (Deng et al., 2009), specifically two sets of pretrained weights available in Pytorch (ResNet50 Weights.IMAGENET1K V1 and ResNet50 Weights.IMAGENET1K V2 as described here). We then compared a randomly chosen subset of the neurons (100 in each network) in the penultimate layer (before the final fully connected layer mapping to the logits) across the two networks using the plug-in estimator and the moment estimator. To compute a ground truth similarity metric we applied the plug-in estimator to the responses of these units across 432,064 images randomly chosen as a subset of the ImageNet dataset. To compare finite sample bias for each number of observed stimuli M , we randomly re-sampled across images and calculated the mean and SE of the two estimators as a function of number of images (Fig 5). We found that the bias of the plug-in-estimator was at worst 3-fold and this bias decreased slowly, whereas the moment estimator showed a small amount of bias even with the smallest numbers of samples.\nFinally, we considered how the bias of the plug-in estimator would vary with respect to irrelevant properties of the neural populations chosen. We reasoned that such a dependence could confound results on similarity between neural populations. It is known that the effective dimensionality, ( \u2211N i=1 \u03bbi) 2/ \u2211N i=1 \u03bb 2 i , of a response distribution determines the rate at which its sample covariance and thus singular values can be estimated. To determine if this in turn biased the plug-in estimator in a real application we randomly re-sampled with out replacement 100 units of the 2048 from\nthe two neural networks 1000 times. We measured the ground truth similarity for each subset, the geometric mean of the effective dimensionality of the 100 units from the two networks (calculated across all images), and the plug-in average estimate across 50 random samplings of images. We found that the bias (difference of average plug-in estimate and ground truth), across re-sampling of units had a moderate negative correlation (r=-0.31) with the effective dimensionality of those populations. Thus observed differences in the similarity of neural network units may be confounded by their dimensionality and its effects on the plug-in estimator."
        }
    ],
    "year": 2023
}