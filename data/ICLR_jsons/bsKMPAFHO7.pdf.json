{
    "abstractText": "A recent paper by Farina & Pipis (2023) established the existence of uncoupled no-linear-swap regret dynamics with polynomial-time iterations in extensive-form games. The equilibrium points reached by these dynamics, known as linear correlated equilibria, are currently the tightest known relaxation of correlated equilibrium that can be learned in polynomial time in any finite extensive-form game. However, their properties remain vastly unexplored, and their computation is onerous. In this paper, we provide several contributions shedding light on the fundamental nature of linear-swap regret. First, we show a connection between linear deviations and a generalization of communication deviations in which the player can make queries to a \u201cmediator\u201d who replies with action recommendations, and, critically, the player is not constrained to match the timing of the game as would be the case for communication deviations. We coin this latter set the untimed communication (UTC) deviations. We show that the UTC deviations coincide precisely with the linear deviations, and therefore that any player minimizing UTC regret also minimizes linear-swap regret. We then leverage this connection to develop state-of-the-art no-regret algorithms for computing linear correlated equilibria, both in theory and in practice. In theory, our algorithms achieve polynomially better per-iteration runtimes; in practice, our algorithms represent the state of the art by several orders of magnitude.",
    "authors": [],
    "id": "SP:3387d7f6693faef5b15e748747ddc0c3573bc768",
    "references": [
        {
            "authors": [
                "REFERENCES Ioannis Anagnostides",
                "Gabriele Farina",
                "Tuomas Sandholm"
            ],
            "title": "Near-optimal phi-regret learning in extensive-form games",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Robert Aumann"
            ],
            "title": "Subjectivity and correlation in randomized strategies",
            "venue": "Journal of Mathematical Economics,",
            "year": 1974
        },
        {
            "authors": [
                "Noam Brown",
                "Tuomas Sandholm"
            ],
            "title": "Solving imperfect-information games via discounted regret minimization",
            "venue": "In AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2019
        },
        {
            "authors": [
                "Michael B Cohen",
                "Yin Tat Lee",
                "Zhao Song"
            ],
            "title": "Solving linear programs in the current matrix multiplication time",
            "venue": "Journal of the ACM (JACM),",
            "year": 2021
        },
        {
            "authors": [
                "Yuval Dagan",
                "Constantinos Daskalakis",
                "Maxwell Fishelson",
                "Noah Golowich"
            ],
            "title": "From external to swap regret 2.0: An efficient reduction and oblivious adversary for large action spaces",
            "year": 1978
        },
        {
            "authors": [
                "Gabriele Farina",
                "Charilaos Pipis"
            ],
            "title": "Polynomial-time linear-swap regret minimization in imperfectinformation sequential games",
            "venue": "arXiv preprint arXiv:2307.05448,",
            "year": 2023
        },
        {
            "authors": [
                "Gabriele Farina",
                "Christian Kroer",
                "Tuomas Sandholm"
            ],
            "title": "Regret circuits: Composability of regret minimizers",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Gabriele Farina",
                "Chun Kai Ling",
                "Fei Fang",
                "Tuomas Sandholm"
            ],
            "title": "Correlation in extensive-form games: Saddle-point formulation and benchmarks",
            "venue": "In Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Gabriele Farina",
                "Chun Kai Ling",
                "Fei Fang",
                "Tuomas Sandholm"
            ],
            "title": "Efficient regret minimization algorithm for extensive-form correlated equilibrium",
            "venue": "In Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Gabriele Farina",
                "Andrea Celli",
                "Nicola Gatti",
                "Tuomas Sandholm"
            ],
            "title": "Connecting optimal ex-ante collusion in teams to extensive-form correlation: Faster algorithms and positive complexity results",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Gabriele Farina",
                "Andrea Celli",
                "Alberto Marchesi",
                "Nicola Gatti"
            ],
            "title": "Simple uncoupled no-regret learning dynamics for extensive-form correlated equilibrium",
            "venue": "Journal of the ACM,",
            "year": 2022
        },
        {
            "authors": [
                "Gabriele Farina",
                "Chung-Wei Lee",
                "Haipeng Luo",
                "Christian Kroer"
            ],
            "title": "Kernelized multiplicative weights for 0/1-polyhedral games: Bridging the gap between learning in extensive-form and normal-form games",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Francoise Forges"
            ],
            "title": "An approach to communication equilibria",
            "venue": "Econometrica: Journal of the Econometric Society,",
            "year": 1986
        },
        {
            "authors": [
                "Fran\u00e7oise Forges",
                "Fr\u00e9d\u00e9ric Koessler"
            ],
            "title": "Communication equilibria with partially verifiable types",
            "venue": "Journal of Mathematical Economics,",
            "year": 2005
        },
        {
            "authors": [
                "Kaito Fujii"
            ],
            "title": "Bayes correlated equilibria and no-regret dynamics",
            "venue": "arXiv preprint arXiv:2304.05005,",
            "year": 2023
        },
        {
            "authors": [
                "Geoffrey J Gordon",
                "Amy Greenwald",
                "Casey Marks"
            ],
            "title": "No-regret learning in convex games",
            "venue": "In 25th international conference on Machine learning,",
            "year": 2008
        },
        {
            "authors": [
                "Amy Greenwald",
                "Amir Jafari"
            ],
            "title": "A general class of no-regret learning algorithms and gametheoretic equilibria",
            "venue": "In Conference on Learning Theory (COLT),",
            "year": 2003
        },
        {
            "authors": [
                "Wan Huang",
                "Bernhard von Stengel"
            ],
            "title": "Computing an extensive-form correlated equilibrium in polynomial time",
            "venue": "In International Workshop on Internet and Network Economics,",
            "year": 2008
        },
        {
            "authors": [
                "Andrew Kephart",
                "Vincent Conitzer"
            ],
            "title": "The revelation principle for mechanism design with signaling costs",
            "venue": "ACM Transaction on Economics and Computation (TEAC),",
            "year": 2021
        },
        {
            "authors": [
                "H.W. Kuhn"
            ],
            "title": "A simplified two-person poker",
            "venue": "Annals of Mathematics Studies,",
            "year": 1950
        },
        {
            "authors": [
                "H. Moulin",
                "J.-P. Vial"
            ],
            "title": "Strategically zero-sum games: The class of games whose completely mixed equilibria cannot be improved upon",
            "venue": "International Journal of Game Theory,",
            "year": 1978
        },
        {
            "authors": [
                "Roger B Myerson"
            ],
            "title": "Multistage games with communication",
            "venue": "Econometrica: Journal of the Econometric Society,",
            "year": 1986
        },
        {
            "authors": [
                "Binghui Peng",
                "Aviad Rubinstein"
            ],
            "title": "Fast swap regret minimization and applications to approximate correlated equilibria",
            "venue": "arXiv preprint arXiv:2310.19647,",
            "year": 2023
        },
        {
            "authors": [
                "I. Romanovskii"
            ],
            "title": "Reduction of a game with complete memory to a matrix game",
            "venue": "Soviet Mathematics,",
            "year": 1962
        },
        {
            "authors": [
                "Finnegan Southey",
                "Michael Bowling",
                "Bryce Larson",
                "Carmelo Piccione",
                "Neil Burch",
                "Darse Billings",
                "Chris Rayner. Bayes"
            ],
            "title": "bluff: Opponent modelling in poker",
            "venue": "In Conference on Uncertainty in Artificial Intelligence (UAI),",
            "year": 2005
        },
        {
            "authors": [
                "Oskari Tammelin"
            ],
            "title": "Solving large imperfect information games using cfr+",
            "venue": "arXiv preprint arXiv:1407.5042,",
            "year": 2014
        },
        {
            "authors": [
                "Bernhard von Stengel"
            ],
            "title": "Efficient computation of behavior strategies",
            "venue": "Games and Economic Behavior,",
            "year": 1996
        },
        {
            "authors": [
                "Bernhard von Stengel",
                "Fran\u00e7oise Forges"
            ],
            "title": "Extensive-form correlated equilibrium: Definition and computational complexity",
            "venue": "Mathematics of Operations Research,",
            "year": 2008
        },
        {
            "authors": [
                "Brian Hu Zhang",
                "Tuomas Sandholm"
            ],
            "title": "Polynomial-time optimal equilibria with a mediator in extensive-form games",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Brian Hu Zhang",
                "Gabriele Farina",
                "Andrea Celli",
                "Tuomas Sandholm"
            ],
            "title": "Optimal correlated equilibria in general-sum extensive-form games: Fixed-parameter algorithms, hardness, and two-sided column-generation",
            "venue": "arXiv preprint arXiv:2203.07181,",
            "year": 2022
        },
        {
            "authors": [
                "Brian Hu Zhang",
                "Gabriele Farina",
                "Tuomas Sandholm"
            ],
            "title": "Team belief dag: Generalizing the sequence form to team games for fast computation of correlated team max-min equilibria via regret minimization",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Martin Zinkevich"
            ],
            "title": "Online convex programming and generalized infinitesimal gradient ascent",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2003
        },
        {
            "authors": [
                "Martin Zinkevich",
                "Michael Bowling",
                "Michael Johanson",
                "Carmelo Piccione"
            ],
            "title": "Regret minimization in games with incomplete information",
            "venue": "In Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2007
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In no-regret learning, a player repeatedly interacts with a possibly adversarial environment. The task of the player is to minimize its regret, which is defined to be the difference between the utility experienced by the player, and the largest utility that it could have achieved in hindsight if it had played other strategies instead, according to some strategy transformation or deviation. The set of allowable deviations defines the notion of regret, with larger sets corresponding to tighter notions. Two extremes are external deviations, which are the set of all constant transformations, and swap deviations, which are all possible functions.\nIn games, no-regret learning has a very tight connection to notions of correlated equilibrium. Each notion of regret has its corresponding notion of equilibrium, which will be reached by a set of players that independently run no-regret algorithms for that notion of regret. External and swap deviations, respectively, correspond to the well-known normal-form coarse correlated equilibrium (Moulin & Vial, 1978) (NFCCE) and normal-form correlated equilibrium (Aumann, 1974) (NFCE). For extensive-form games specifically, other sets of deviations include the trigger deviations (Gordon et al., 2008; Farina et al., 2022a), which correspond to extensive-form correlated equilibrium (von Stengel & Forges, 2008), and the communication deviations (Zhang & Sandholm, 2022; Fujii, 2023), which correspond to communication equilibrium1 (Myerson, 1986; Forges, 1986).\n1Technically, communication equilibria are more broad than (\u03a6i)-equilibria where \u03a6i is the set of communication deviations: in a communication equilibrium, there is an explicit mediator who has the power not only to sample a strategy profile, but also to pass private information between players\u2014so a communication equilibrium is not necessarily a correlated profile at all. This distinction is fairly fundamental: it is the reason why polynomial-time algorithms for optimal communication equilibrium can exist for extensive-form\nIn this paper, we consider a notion of regret first studied for extensive-form games by Farina & Pipis (2023), namely, regret with respect to the set of linear functions from the strategy set to itself. This notion is a natural stepping stone between external regret, which is very well studied, and swap regret, for which achieving poly(d) \u00b7 T c regret, where d is the size of the decision problem and c < 1, is a long-standing open problem. We make two main contributions.\nThe first contribution is conceptual: we give, for extensive-form games, an interpretation of the set of linear deviations. More specifically, we will first introduce a set of deviations, which we will call the untimed communication (UTC) deviations that, a priori, seems very different from the set of linear deviations at least on a conceptual level. The deviation set, rather than being defined algebraically (linear functions), will be defined in terms of an interaction between a deviator, who wishes to evaluate the deviation function at a particular input, and a mediator, who answers queries about the input. We will show the following result, which is our first main theorem: Theorem. The untimed communication deviations are precisely the linear deviations.\nThe mediator-based framework is more in line with other extensive-form deviation sets\u2014indeed, all prior notions of regret for extensive form, to our knowledge, including all the notions discussed above, can be expressed in terms of the framework. As such, the above theorem places linear deviations firmly within the same framework usually used to study deviations in extensive form.\nWe will then demonstrate that the set of UTC deviations is expressible in terms of scaled extensions (Farina et al., 2019c), opening up access to a wide range of extremely fast algorithms for regret minimization, both theoretically and practically, for UTC deviations and thus also for linear deviations. Our second main theorem is as follows. Theorem (Faster linear-swap regret minimization). There exists a regret minimizer with regret O(d2 \u221a T ) against all linear deviations, and whose per-iteration complexity is dominated by the complexity of computing a fixed point of a linear map \u03d5(t) : coX \u2192 coX .\nIn particular, using the algorithm of Cohen et al. (2021) to solve the linear program of finding a fixed point, our per-iteration complexity is O\u0303(d\u03c9), where \u03c9 \u2248 2.37 is the current matrix multiplication constant and O\u0303 hides logarithmic factors. We elaborate on the fixed-point computation in Section 6. This improves substantially on the result of Farina & Pipis (2023), which has the same regret bound but whose per-iteration computation involved a quadratic program (namely, an \u21132 projection), which has higher complexity than a linear program (they give a bound of O\u0303(d10)). Finally, we demonstrate via experiments that our method is also empirically faster than the prior method."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "Here, we review fundamentals of tree-form decision making, extensive-form games, and online convex optimization. Our exposition and notation mostly follows Farina & Pipis (2023)."
        },
        {
            "heading": "2.1 TREE-FORM DECISION MAKING",
            "text": "A tree-form decision problem is a rooted tree where every path alternates between two types of nodes: decision points (j \u2208 J ) and observation points (or sequences) (\u03c3 \u2208 \u03a3). The root node \u2205 \u2208 \u03a3 is always an observation point. At decision points, the edges are called actions, and the player must select one of the legal actions. At observation points, the edges are called observations or signals, and the player observes one of the signals before continuing. The number of sequences is denoted d = |\u03a3|. The parent of a node s is denoted ps. The set of actions available at a decision point j is denoted Aj . The set of decision point following an observation point \u03c3 will be denoted C\u03c3 . An observation node \u03c3 \u2208 \u03a3 is uniquely identified by its parent decision point j and the action a taken at j. We will hence use ja as an alternative notation for the same observation point.\nA sequence-form pure strategy for the player is a vector x \u2208 {0, 1}d, indexed by sequences in \u03a3, where x(\u03c3) = 1 if the player selects every action on the \u2205 \u2192 \u03c3 path. A sequence-form mixed games (Zhang & Sandholm, 2022). We may call a communication equilibrium that also happens to be a correlated profile a private communication equilibrium, where private denotes that the mediator is not allowed to pass information between players. However, since this paper focuses on no-regret learning, we have no reason to make this distinction, so we largely ignore it.\nstrategy is a convex combination of sequence-form pure strategies. We will use X to denote the set of sequence-form pure strategies. An important property (Romanovskii, 1962; von Stengel, 1996) is that the convex hull of X , which we will denote coX , is described by a system of linear constraints:\nx(\u2205) = 1, x(pj) = \u2211 a\u2208Aj x(ja) \u2200j \u2208 J . (1)\nTree-form decision problems naturally encode the decision problems faced by a player with perfect recall in an extensive-form game. An extensive-form game with n players is a game of incomplete information played on a tree of nodes H. At every non-leaf node h \u2208 H , the children of h are labeled with actions a \u2208 Ah. Each nonterminal node is assigned to a different player, and the player to whom a node is assigned selects the action at that node. The nodes assigned to a given player are partitioned into information sets, or infosets; a player cannot distinguish among the nodes in a given infoset, and therefore a pure strategy must play the same action at every node in an infoset. Finally, each player has a utility function ui : Z \u2192 R, where Z is the set of terminal nodes. We will assume perfect recall, that is, we will assume that players never forget information.\nIn an extensive-form game, a perfect-recall player\u2019s decision problem is a tree-form decision problem whose size (number of nodes) is linear in the size of the game tree, and the utility functions are linear in every player\u2019s strategy. We will use Xi to denote the tree-form decision problem faced by player i. Then the utility functions ui : coX1\u00d7\u00b7 \u00b7 \u00b7\u00d7coXn \u2192 R are linear in each player\u2019s strategy."
        },
        {
            "heading": "2.2 ONLINE CONVEX OPTIMIZATION AND \u03a6-REGRET",
            "text": "In online convex optimization (Zinkevich, 2003), a player (or \u201clearner\u201d) has a strategy set X \u2286 Rd, and repeatedly faces a possibly-adversarial environment. More formally, at every iteration t = 1, . . . , T , the player selects a distribution \u03c0(t) \u2208 \u2206(X ), and an adversary simultaneously selects a utility vector u(t) \u2208 [0, 1]d. The player then observes the utility u(t), selects a new strategy x(t+1) \u2208 X , and so on. Our metric of performance will be the notion of \u03a6-regret (Greenwald & Jafari, 2003). Given a set of transformations2 \u03a6 \u2286 (coX )X : Definition 2.1. The \u03a6-regret of the player after T timesteps is given by\nReg\u03a6(T ) := max \u03d5\u2208\u03a6 T\u2211 t=1 E x\u223c\u03c0(t) \u27e8u(t), \u03d5(x)\u2212 x\u27e9 .\nVarious choices of \u03a6 correspond to various notions of regret, with larger sets resulting in stronger notions of regret. In an extensive-form game, notions of \u03a6-regret correspond to notions of equilibrium. For each player i \u2208 [n], let \u03a6i \u2286 (coXi)Xi be a set of transformations for player i. Definition 2.2. A distribution \u03c0 \u2208 \u2206(X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Xn) is called a correlated profile. A \u03b5-(\u03a6i)equilibrium is a correlated profile such that\nE (x1,...,xn)\u223c\u03c0 [ui(\u03d5(xi),x\u2212i)\u2212 ui(xi,x\u2212i)] \u2264 \u03b5.\nfor every player i and deviation \u03d5 \u2208 \u03a6i.\nIf all players independently run \u03a6i-regret minimizers over their strategy sets Xi, the empirical frequency of play \u03c0 = Unif(\u03c0(1), . . . , \u03c0(T )) will be an \u03b5-(\u03a6i)-equilibrium for \u03b5 = maxi Reg\u03a6i(T )/T . Thus, \u03a6-regret minimizers immediately imply no-regret learning algorithms converging to (\u03a6i)equilibria. Some common choices of \u03a6, and corresponding equilibrium notions, are in Table 1. In this paper, our focus will be on linear-swap regret, which is the regret against the set \u03a6LIN of all linear3 maps \u03d5 : X \u2192 coX . To our knowledge, linear-swap regret was first proposed by Gordon et al. (2008) for general convex spaces. They developed a general framework for \u03a6-regret minimization, which we now review. We start by observing that any linear \u03d5 : X \u2192 coX is naturally extended to a function \u03d5 : coX \u2192 coX by setting \u03d5 : coX \u220b x 7\u2192 Ex\u2032\u223c\u03c0 \u03d5(x\u2032), where \u03c0 \u2208 \u2206(X ) is any distribution for which Ex\u2032\u223c\u03c0 x\u2032 = x (The choice of distribution is irrelevant because of linearity of expectation, and thus \u03d5 : coX \u2192 coX is uniquely defined.)\n2AB is the set of functions from B to A 3For sets X whose affine hull excludes the origin, there is no point in distinguishing affine maps from linear maps. Sequence-form strategy sets X are such sets, because x(\u2205) = 1 is always a constraint. So, throughout this paper, we will not distinguish between linear and affine maps.\nTheorem 2.3 (Gordon et al., 2008). Let \u03a6 \u2286 \u03a6LIN be a convex set of transformations, and let R\u03a6 be a deterministic4 external regret minimizer over \u03a6, whose regret after T timesteps is R. Then the following algorithm achieves \u03a6-regret R on X after T timesteps: At every timestep t, the player queries R\u03a6 for a strategy (transformation) \u03d5(t) \u2208 \u03a6, and the player selects a strategy x(t) \u2208 coX that is a fixed point of \u03d5(t), that is, \u03d5(t)(x(t)) = x(t). Upon observing utility u(t), the player forwards the utility \u03d5 7\u2192 \u27e8u(t), \u03d5(x(t))\u27e9 to R\u03a6.\nTherefore, to construct a \u03a6LIN-regret minimizer over X , it suffices to be able to (1) minimize external regret over \u03a6LIN, and (2) compute fixed points of transformations \u03d5(t). For linear \u03d5(t) : x 7\u2192 Ax, computing a fixed point amounts to solving a linear program. Therefore, the focus of this paper will be on external regret minimizers over the set \u03a6LIN.\nFor extensive-form games, linear-swap regret was recently studied in detail by Farina & Pipis (2023): they provide a characterization of the set \u03a6LIN when X is a sequence-form polytope, and thus derive an algorithm for minimizing \u03a6LIN-regret over X . Their paper is the starting point of ours."
        },
        {
            "heading": "3 MEDIATORS AND UTC DEVIATIONS",
            "text": "With the notable exception of linear deviations, most sets of deviations \u03a6 for extensive-form games are defined by interactions between a mediator who holds a strategy x \u2208 X , and a deviator, who should compute the function \u03d5(x) by making queries to the mediator. The set of deviations is then defined by what queries that the player is allowed to make. Before continuing, we will first formulate the sets \u03a6 mentioned in Section 2.2 in this paradigm, for intuition. For a given decision point j, call an action a \u2208 Aj the recommended action at j, denoted a(x, j), if x(ja) = 1. Since x is a sequence-form strategy, it is possible for a decision point to have no recommended action if its parent pj is itself not recommended.\n\u2022 Constant (NFCCE): The deviator cannot to make any queries to the mediator. \u2022 Trigger (EFCE): The deviator, upon reaching a decision point j, learns the recommended\naction (if any) at j before selecting its own action. \u2022 Communication: The deviator maintains a state with the mediator, which is a sequence \u03c3,\ninitially \u2205. Upon reaching a decision point j, the deviator selects a decision point j\u2032 \u2208 C\u03c3 (possibly j\u2032 \u0338= j) at which to query the mediator, the deviator observes the recommendation a\u2032 = a(x, j\u2032), then the deviator must pick an action a \u2208 Aj . The state is updated to j\u2032a\u2032.\n\u2022 Swap (NFCE): The deviator learns the whole strategy x before selecting its strategy.\nAn example of a communication deviation can be found in Section 5, and further discussion of these solution concepts can be found in Appendix B.\nOf these, the closest notion to ours is the notion of communication deviation, and that is the starting point of our construction. One critical defining property of communication deviations is that the mediator and deviator \u201cshare a clock\u201d: for every decision point reached, the deviator must make\n4A deterministic regret minimizer is one that uses no randomness internally to compute its strategies. When the strategy set (here \u03a6) is convex, and the notion is external regret, the learner need not randomize: since utilities are linear, picking a distribution \u03c0(t) \u2208 \u2206(\u03a6) is equivalent to deterministically selecting the point \u03d5(t) := E\u03d5\u223c\u03c0(t) \u03d5. Thus, we allow the R\u03a6-adversary (here, the player itself) to set a utility \u03d5 7\u2192 \u27e8u(t), \u03d5(x(t))\u27e9 that depends on the learner\u2019s choice of \u03d5.\nexactly one query to the mediator. As the name suggests, our set of untimed deviations results from removing this timing restriction, and therefore allowing the deviator to make any number (zero, one, or more than one) of queries to the mediator for every decision point reached. We formally define the decision problem faced by an untimed deviator as follows. Definition 3.1. The UTC decision problem corresponding to a given tree-form decision problem is defined as follows. Nodes are identified with pairs (s, s\u0303) where s, s\u0303 \u2208 \u03a3 \u222a J . s represents the state of the real decision problem, and s\u0303 represents the state of the mediator. The root is (\u2205,\u2205) \u2208 \u03a3\u00d7\u03a3.\n1. (\u03c3, \u03c3\u0303) \u2208 \u03a3 \u00d7 \u03a3 is an observation point. The deviator observes the next decision point j \u2208 C\u03c3 , and the resulting decision point is (j, \u03c3\u0303)\n2. (j, \u0237\u0303) \u2208 J \u00d7 J is an observation point. The deviator observes the recommendation a = a(x, \u0237\u0303), and the resulting decision point is (j, \u0237\u0303a).\n3. (j, \u03c3\u0303) \u2208 J \u00d7\u03a3 is a decision point. The deviator can choose to either play an action a \u2208 Aj , or to query a decision point \u0237\u0303 \u2208 C\u03c3\u0303 . In the former case, the resulting observation point is (ja, \u03c3\u0303) for a \u2208 Aj ; in the latter case, the resulting observation point is (j, \u0237\u0303).\nAny mixed strategy of the deviator in this decision problem defines a function \u03d5 : X \u2192 coX , where \u03d5(x)(\u03c3) is the probability that an untimed deviator plays all the actions on the path to \u03c3 when the mediator recommends according to pure strategy x. We thus define: Definition 3.2. An UTC deviation is any function \u03d5 : X \u2192 coX induced by a mixed strategy of the deviator in the UTC decision problem.\nClearly, the set of UTC deviations is at least as large as the set of communication deviations, and at most as large as the set of swap deviations. In the next section, we will discuss how to represent UTC deviations, and show that UTC deviations coincide precisely with linear deviations."
        },
        {
            "heading": "4 REPRESENTATION OF UTC DEVIATIONS AND EQUIVALENCE BETWEEN UTC AND LINEAR DEVIATIONS",
            "text": "Since UTC deviations are defined by a decision problem, one method of representing such deviations is to express it as a tree-form decision problem and use the sequence-form representation. However, the UTC decision problem is not a tree\u2014it is a DAG, since there are multiple ways of reaching any given decision point (j, \u03c3\u0303) depending on the ordering of the player\u2019s past actions and queries. Converting it to a tree by considering the tree of paths through the DAG would result in an exponential blowup: a decision point (j, \u03c3\u0303), where j is at depth k and \u03c3\u0303 is at depth \u2113, can be reached in roughly ( k+\u2113 k ) ways, so the total number of paths can be exponential in the depth of the decision problem even when the number of sequences, d = |\u03a3|, is not. However, it is still possible to define the \u201csequence form\u201d of a pure deviation in our UTC decision problem as follows5: it is a pair of matrices (A,B) where A \u2208 {0, 1}\u03a3\u00d7\u03a3 encodes the part corresponding to sequences (\u03c3, \u03c3\u0303), and B \u2208 {0, 1}J\u00d7J encodes the part corresponding to decision points (j, \u0237\u0303). A(\u03c3, \u03c3\u0303) = 1 if the deviator plays all the actions on some path to observation point (\u03c3, \u03c3\u0303), and similarly B(j, \u0237\u0303) = 1 if the deviator plays all the actions on some path to observation node (j, \u0237\u0303). Since the only possible way for two paths to end at the same observation point is for the deviator to have changed the order of actions and queries, for any given pure strategy of the deviator, at most one path can exist for both cases. Therefore, the set of mixed sequence-form deviations can be expressed using the following set of constraints:\nA(pj , \u03c3\u0303) +B(j, p\u03c3\u0303) = \u2211 a\u2208Aj A(ja, \u03c3\u0303) + \u2211 \u0237\u0303\u2208C\u03c3\u0303 B(j, \u0237\u0303) \u2200j \u2208 J , \u03c3\u0303 \u2208 \u03a3\nA(\u2205,\u2205) = 1 A(\u2205, \u03c3\u0303) = 0 \u2200\u03c3\u0303 \u0338= \u2205\nA,B \u2265 0\n(2)\n5This construction is a special case of the more general construction of sequence forms for DAG decision problems explored by Zhang et al. (2023) in the case of team games.\nwhere, in a slight abuse of notation, we define B(j, p\u2205) := 0 for every j \u2208 J . Moreover, for any pair of matrices (A,B) satisfying the constraint system and therefore defining some deviation \u03d5 : X \u2192 coX , it is easy to compute how \u03d5 acts on any x \u2208 X : the probability that the deviator plays all the actions on the \u2205 \u2192 \u03c3 path is simply given by\u2211\n\u03c3\u0303\u2208\u03a3\nx(\u03c3\u0303)A(\u03c3, \u03c3\u0303) = (Ax)(\u03c3),\nand therefore \u03d5 is nothing more than a matrix multiplication with A, that is, \u03d5(x) = Ax. We have thus shown that every UTC deviation is linear, that is, \u03a6UTC \u2286 \u03a6LIN. In fact, the reverse inclusion holds too:\nTheorem 4.1. The UTC deviations are precisely the linear deviations. That is, \u03a6UTC = \u03a6LIN.\nThe proof is deferred to Appendix D. Since the two sets are equivalent, in the remainder of the paper, we will use the terms UTC deviation and linear deviation (similarly, UTC regret and linear-swap regret) interchangeably."
        },
        {
            "heading": "5 EXAMPLE",
            "text": "In this section, we provide an example in which the UTC deviations are strictly more expressive than the communication deviations. Consider the game in Figure 1. The subgames rooted at D and E are guessing games, where \u25b2 must guess \u25bc\u2019s action, with a large penalty for guessing wrong. Consider the correlated profile that mixes uniformly among the four pure profiles (ai, bj , c1, fi, gj) for i, j \u2208 {1, 2}. In this profile, the information that \u25b2 needs to guess perfectly is contained in the recommendations: the recommendation at A tells it how to guess at D, and the recommendation at B tells it how to guess at E. With a communication deviation, \u25b2 cannot access this information in a profitable way, since upon reaching C, \u25b2 must immediately make its first mediator query. Hence, this profile is a communication equilibrium. However, with an untimed communication deviation, \u25b2 can profit: it should, upon reaching6 C, play action c2 without making a mediator query, and then query A if it observes D, and B if it observes E. This deviation is allowed only due to the untimed nature of UTC deviations allows the deviating player to delay its query to the mediator until it reaches either D or E. In a timed communication deviation, this deviation is impossible, because the player must make its first query (A, B, or C) before reaching D or E, and thus that query cannot be conditioned on which one of D or E will be reached.\nAnother example, where the player can profit from making more than one query, and untimed deviations affects the set of possible equilibrium outcomes, can be found in Appendix C."
        },
        {
            "heading": "6 REGRET MINIMIZATION ON \u03a6UTC",
            "text": "In this section, we discuss how Theorem 4.1 can be used to construct very efficient \u03a6LIN-regret minimizers, both in theory and in practice. The key observation we use here is due to Zhang et al. (2023): they observed that DAG decision problems have a structure that allows them to be expressed as scaled extensions, allowing the application of the counterfactual regret minimization (CFR) framework (Zinkevich et al., 2007; Farina et al., 2019a):\nTheorem 6.1 (CFR for \u03a6LIN, special case of Zhang et al., 2023). CFR-based algorithms can be used to construct an external regret minimizer on \u03a6UTC (and thus also on \u03a6LIN) with O(d2 \u221a T ) regret and O(d2) per-iteration complexity.\nApplying Theorem 2.3 now yields:\nTheorem 6.2. CFR-based algorithms can be used to construct a \u03a6LIN-regret minimizer with O(d2 \u221a T ) regret, and per-iteration complexity dominated by the complexity of computing a fixed point of a linear transformation \u03d5(t) : coX \u2192 coX . 6The actions/queries \u25b2 makes at A and B are irrelevant, because \u25b2 only cares about maximizing utility, and it always gets utility 0 regardless of what it does. In the depiction of this deviation in Figure 2, the deviator always plays action 1 at A and B.\nAs mentioned in the introduction, this significantly improves the per-iteration complexity of linearswap regret minimization. Fixed points can be computed by finding a feasible solution to the constraint system {x \u2208 X ,Ax = x}, where x \u2208 X is expressed using the sequence-form constraints (1). This is a linear program with O(d) variables and constraints, so the LP algorithm of Cohen et al. (2021) yields a fixed-point computation algorithm with runtime O\u0303(d\u03c9).\nFor comparison, the algorithm of Farina & Pipis (2023) requires an \u21132 projection onto X on every iteration, which requires solving a convex quadratic program; the authors of that paper derive a bound of O\u0303(d10), which, although polynomial, is much slower than our algorithm. CFR-based algorithms are currently the fastest practical regret minimizers (Brown & Sandholm, 2019; Farina et al., 2021)\u2014therefore, showing that our method allows such algorithms to be applied is also a significant practical step. In Section 7, we will show empirically that the resulting algorithm is significantly better than the previously-known state of the art, in terms of both per-iteration time complexity and number of iterations."
        },
        {
            "heading": "7 EXPERIMENTAL EVALUATION",
            "text": "We empirically investigate the numerical performance of our learning dynamics for linear correlated equilibrium, compared to the recent algorithm by Farina & Pipis (2023). We test on four benchmark games:\n\u2022 4-player Kuhn poker, a multiplayer variant of the classic benchmark game introduced by Kuhn (1950). The deck has 5 cards. This game has 3,960 terminal states.\n\u2022 A ridesharing game, a two-player general-sum game introduced as a benchmark for welfaremaximizing equilibria by Zhang et al. (2022). This game has 484 terminal states.\n\u2022 3-player Leduc poker, a three-player variant of the classic Leduc poker introduced by Southey et al. (2005). Only one bet per round is allowed, and the deck has 6 cards (3 ranks, 2 suits). The game has 4,500 terminal states.\n\u2022 Sheriff of Nottingham, a two-player general-sum game introduced by Farina et al. (2019b) for its richness of equilibrium points. The smuggler has 10 items, a maxmimum bribe of 2, and 2 rounds to bargain. The game has 2,376 terminal states.\nWe run our algorithm based on the UTC polytope, and that of Farina & Pipis (2023) (with the learning rate \u03b7 = 0.1 as used by the authors), for a limit of 100,000 iterations or 6 hours, whichever is hit first. Instead of solving linear programs to find the fixed points, we use power iteration, which is faster in practice. All experiments were run on the same machine with 32GB of RAM and a processor running at a nominal speed of 2.4GHz. For our learning dynamics, we employed the CFR algorithm instantiated with the regret matching+ (Tammelin, 2014) regret minimizer at each decision point (see Theorem 6.1). Experimental results are shown in Figure 3.\nOne of the most appealing features of our algorithm is that allows CFR-based methods to apply. CFR-based methods are the fastest regret minimizers in practice, so it is unsurprising that using them results in better convergence as seen in Figure 3. Another appealing feature is that our method sidesteps the need of projecting onto the set of transformations. This is in contrast with the algorithm of Farina & Pipis (2023), which requires an expensive projection at every iteration. We observe that this difference results in a dramatic reduction in iteration runtime between the two algorithms, which we quantify in Table 2. So, we remark that when accounting for time instead of iterations on the x-axis of the plots in Figure 3, the difference in performance between the algorithms appears even stronger. Such a plot is available in Appendix E."
        },
        {
            "heading": "8 CONCLUSION AND FUTURE RESEARCH",
            "text": "In this paper, we have introduced a new representation for the set of linear deviations when the strategy space is sequence form. Our representation connects linear deviations to the mediatorbased framework that is more typically used for correlation concepts in extensive-form games, and therefore gives a reasonable game-theoretic interpretation of what linear equilibria represent. It also leads to state-of-the-art no-linear-regret algorithms, both in theory and in practice. Several natural questions remain open:\n1. Is there an algorithm whose swap regret is poly(d) \u00b7 T c for c < 1 in extensive-form games? (See also Appendix B for some recent progress on this problem.)\n2. What would be a reasonable definition of untimed communication equilibrium, as a refinement of communication equilibrium (see also Appendix A.6)?\n3. For extensive-form correlated equilibrium, it is possible to achieve poly(d) \u00b7 log(T ) regret (Anagnostides et al., 2023), and to compute exact equilibria in polynomial time (Huang & von Stengel, 2008). Can one extend these results to linear equilibria?"
        }
    ],
    "year": 2023
}