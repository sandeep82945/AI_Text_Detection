{
    "abstractText": "Generative models have gained more and more attention in recent years for their remarkable success in tasks that required estimating and sampling data distribution to generate high-fidelity synthetic data. In speech, text-to-speech synthesis and neural vocoder are good examples where generative models have shined. While generative models have been applied to different applications in speech, there exists no general-purpose generative model that models speech directly. In this work, we take a step toward this direction by showing a single pre-trained generative model can be adapted to different downstream tasks with strong performance. Specifically, we pre-trained a generative model, named SpeechFlow, on 60k hours of untranscribed speech with Flow Matching and masked conditions. Experiment results show the pre-trained generative model can be fine-tuned with task-specific data to match or surpass existing expert models on speech enhancement, separation, and synthesis. Our work suggested a foundational model for generation tasks in speech can be built with generative pre-training.",
    "authors": [],
    "id": "SP:3a45472fa8fb4fca44255d16e26994e3b38ea8c5",
    "references": [
        {
            "authors": [
                "Junyi Ao",
                "Rui Wang",
                "Long Zhou",
                "Chengyi Wang",
                "Shuo Ren",
                "Yu Wu",
                "Shujie Liu",
                "Tom Ko",
                "Qing Li",
                "Yu Zhang"
            ],
            "title": "Speecht5: Unified-modal encoder-decoder pre-training for spoken language processing",
            "venue": "arXiv preprint arXiv:2110.07205,",
            "year": 2021
        },
        {
            "authors": [
                "Alexei Baevski",
                "Yuhao Zhou",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ],
            "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "He Bai",
                "Renjie Zheng",
                "Junkun Chen",
                "Xintong Li",
                "Mingbo Ma",
                "Liang Huang"
            ],
            "title": "A3T: Alignmentaware acoustic and text pretraining for speech synthesis and editing",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Zal\u00e1n Borsos",
                "Rapha\u00ebl Marinier",
                "Damien Vincent",
                "Eugene Kharitonov",
                "Olivier Pietquin",
                "Matthew Sharifi",
                "Olivier Teboul",
                "David Grangier",
                "Marco Tagliasacchi",
                "Neil Zeghidour"
            ],
            "title": "AudioLM: a language modeling approach to audio",
            "venue": "generation. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Edresson Casanova",
                "Julian Weber",
                "Christopher Dane Shulby",
                "Arnaldo C\u00e2ndido J\u00fanior",
                "Eren G\u00f6lge",
                "Moacir Antonelli Ponti"
            ],
            "title": "YourTTS: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Edresson Casanova",
                "Arnaldo Candido Junior",
                "Christopher Shulby",
                "Frederico Santos de Oliveira",
                "Jo\u00e3o Paulo Teixeira",
                "Moacir Antonelli Ponti",
                "Sandra"
            ],
            "title": "Alu\u0131\u0301sio. Tts-portuguese corpus: a corpus for speech synthesis in brazilian portuguese",
            "venue": "Language Resources and Evaluation,",
            "year": 2022
        },
        {
            "authors": [
                "Nanxin Chen",
                "Yu Zhang",
                "Heiga Zen",
                "Ron J Weiss",
                "Mohammad Norouzi",
                "William Chan"
            ],
            "title": "Wavegrad: Estimating gradients for waveform generation",
            "venue": "arXiv preprint arXiv:2009.00713,",
            "year": 2020
        },
        {
            "authors": [
                "Ricky T.Q. Chen"
            ],
            "title": "torchdiffeq, 2018. URL https://github.com/rtqichen/ torchdiffeq",
            "year": 2018
        },
        {
            "authors": [
                "Ricky T.Q. Chen",
                "Yulia Rubanova",
                "Jesse Bettencourt",
                "David Kristjanson Duvenaud"
            ],
            "title": "Neural ordinary differential equations",
            "venue": "In Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Sanyuan Chen",
                "Chengyi Wang",
                "Zhengyang Chen",
                "Yu Wu",
                "Shujie Liu",
                "Zhuo Chen",
                "Jinyu Li",
                "Naoyuki Kanda",
                "Takuya Yoshioka",
                "Xiong Xiao"
            ],
            "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
            "venue": "IEEE Journal of Selected Topics in Signal Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Yu-An Chung",
                "James Glass"
            ],
            "title": "Generative pre-training for speech with autoregressive predictive coding",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Yu-An Chung",
                "Yuxuan Wang",
                "Wei-Ning Hsu",
                "Yu Zhang",
                "RJ Skerry-Ryan"
            ],
            "title": "Semi-supervised training for improving data efficiency in end-to-end speech synthesis",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Joris Cosentino",
                "Manuel Pariente",
                "Samuele Cornell",
                "Antoine Deleforge",
                "Emmanuel Vincent"
            ],
            "title": "Librimix: An open-source dataset for generalizable speech separation",
            "venue": "arXiv preprint arXiv:2005.11262,",
            "year": 2020
        },
        {
            "authors": [
                "Alexandre D\u00e9fossez",
                "Gabriel Synnaeve",
                "Yossi Adi"
            ],
            "title": "Real time speech enhancement in the waveform",
            "venue": "domain. ArXiv,",
            "year": 2020
        },
        {
            "authors": [
                "Alexandre D\u00e9fossez",
                "Jade Copet",
                "Gabriel Synnaeve",
                "Yossi Adi"
            ],
            "title": "High fidelity neural audio",
            "venue": "compression. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat GANs on image synthesis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Szu-Wei Fu",
                "Cheng Yu",
                "Tsun-An Hsieh",
                "Peter Plantinga",
                "Mirco Ravanelli",
                "Xugang Lu",
                "Yu Tsao"
            ],
            "title": "Metricgan+: An improved version of metricgan for speech enhancement",
            "venue": "arXiv preprint arXiv:2104.03538,",
            "year": 2021
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "Communications of the ACM,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Graves",
                "Santiago Fern\u00e1ndez",
                "Faustino Gomez",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
            "venue": "In Proceedings of the 23rd international conference on Machine learning,",
            "year": 2006
        },
        {
            "authors": [
                "Raza Habib",
                "Soroosh Mariooryad",
                "Matt Shannon",
                "Eric Battenberg",
                "RJ Skerry-Ryan",
                "Daisy Stanton",
                "David Kao",
                "Tom Bagby"
            ],
            "title": "Semi-supervised generative modeling for controllable speech synthesis",
            "year": 1910
        },
        {
            "authors": [
                "John R Hershey",
                "Zhuo Chen",
                "Jonathan Le Roux",
                "Shinji Watanabe"
            ],
            "title": "Deep clustering: Discriminative embeddings for segmentation and separation",
            "venue": "In 2016 IEEE international conference on acoustics, speech and signal processing (ICASSP),",
            "year": 2016
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Wei-Ning Hsu",
                "Benjamin Bolte",
                "Yao-Hung Hubert Tsai",
                "Kushal Lakhotia",
                "Ruslan Salakhutdinov",
                "Abdelrahman Mohamed"
            ],
            "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Yi Hu",
                "Philipos C Loizou"
            ],
            "title": "Evaluation of objective quality measures for speech enhancement",
            "venue": "IEEE Transactions on audio, speech, and language processing,",
            "year": 2007
        },
        {
            "authors": [
                "Jesper Jensen",
                "Cees H Taal"
            ],
            "title": "An algorithm for predicting the intelligibility of speech masked by modulated noise maskers",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2016
        },
        {
            "authors": [
                "Ye Jia",
                "Yu Zhang",
                "Ron Weiss",
                "Quan Wang",
                "Jonathan Shen",
                "Fei Ren",
                "Patrick Nguyen",
                "Ruoming Pang",
                "Ignacio Lopez Moreno",
                "Yonghui Wu"
            ],
            "title": "Transfer learning from speaker verification to multispeaker text-to-speech synthesis",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Kahn",
                "Morgane Rivi\u00e8re",
                "Weiyi Zheng",
                "Evgeny Kharitonov",
                "Qiantong Xu",
                "Pierre-Emmanuel Mazar\u2019e",
                "Julien Karadayi",
                "Vitaliy Liptchinsky",
                "Ronan Collobert",
                "Christian Fuegen",
                "Tatiana Likhomanenko",
                "Gabriel Synnaeve",
                "Armand Joulin",
                "Abdel rahman Mohamed",
                "Emmanuel Dupoux"
            ],
            "title": "Libri-Light: A benchmark for asr with limited or no supervision",
            "venue": "International Conference on Acoustics, Speech and Signal Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Eugene Kharitonov",
                "Ann Lee",
                "Adam Polyak",
                "Yossi Adi",
                "Jade Copet",
                "Kushal Lakhotia",
                "Tu Nguyen",
                "Morgane Rivi\u00e8re",
                "Abdel rahman Mohamed",
                "Emmanuel Dupoux",
                "Wei-Ning Hsu"
            ],
            "title": "Text-free prosody-aware generative spoken language modeling",
            "venue": "In Annual Meeting of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Jaehyeon Kim",
                "Jungil Kong",
                "Juhee Son"
            ],
            "title": "Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "CoRR, abs/1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Yuma Koizumi",
                "Heiga Zen",
                "Kohei Yatabe",
                "Nanxin Chen",
                "Michiel Bacchiani"
            ],
            "title": "Specgrad: Diffusion probabilistic model based neural vocoder with adaptive noise spectral shaping",
            "venue": "arXiv preprint arXiv:2203.16749,",
            "year": 2022
        },
        {
            "authors": [
                "Jungil Kong",
                "Jaehyeon Kim",
                "Jaekyoung Bae"
            ],
            "title": "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Kushal Lakhotia",
                "Evgeny Kharitonov",
                "Wei-Ning Hsu",
                "Yossi Adi",
                "Adam Polyak",
                "Benjamin Bolte",
                "Tu Nguyen",
                "Jade Copet",
                "Alexei Baevski",
                "Adel Ben Mohamed",
                "Emmanuel Dupoux"
            ],
            "title": "On generative spoken language modeling from raw audio",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Matthew Le",
                "Apoorv Vyas",
                "Bowen Shi",
                "Brian Karrer",
                "Leda Sari",
                "Rashel Moritz",
                "Mary Williamson",
                "Vimal Manohar",
                "Yossi Adi",
                "Jay Mahadeokar"
            ],
            "title": "Voicebox: Text-guided multilingual universal speech generation at scale",
            "venue": "arXiv preprint arXiv:2306.15687,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Le Roux",
                "Scott Wisdom",
                "Hakan Erdogan",
                "John R Hershey"
            ],
            "title": "Sdr\u2013half-baked or well done",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Shaoshi Ling",
                "Yuzong Liu"
            ],
            "title": "Decoar 2.0: Deep contextualized acoustic representations with vector quantization",
            "venue": "arXiv preprint arXiv:2012.06659,",
            "year": 2020
        },
        {
            "authors": [
                "Shaoshi Ling",
                "Yuzong Liu",
                "Julian Salazar",
                "Katrin Kirchhoff"
            ],
            "title": "Deep contextualized acoustic representations for semi-supervised speech recognition",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Yaron Lipman",
                "Ricky T.Q. Chen",
                "Heli Ben-Hamu",
                "Maximilian Nickel",
                "Matthew Le"
            ],
            "title": "Flow matching for generative modeling",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Yen-Ju Lu",
                "Yu Tsao",
                "Shinji Watanabe"
            ],
            "title": "A study on speech enhancement based on diffusion probabilistic model",
            "venue": "In 2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),",
            "year": 2021
        },
        {
            "authors": [
                "Yen-Ju Lu",
                "Zhong-Qiu Wang",
                "Shinji Watanabe",
                "Alexander Richard",
                "Cheng Yu",
                "Yu Tsao"
            ],
            "title": "Conditional diffusion probabilistic model for speech enhancement",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Yi Luo",
                "Nima Mesgarani"
            ],
            "title": "Conv-tasnet: Surpassing ideal time\u2013frequency magnitude masking for speech separation",
            "venue": "IEEE/ACM transactions on audio, speech, and language processing,",
            "year": 2019
        },
        {
            "authors": [
                "Michael McAuliffe",
                "Michaela Socolof",
                "Sarah Mihuc",
                "Michael Wagner",
                "Morgan Sonderegger"
            ],
            "title": "Montreal forced aligner: Trainable text-speech alignment using kaldi",
            "year": 2017
        },
        {
            "authors": [
                "Tu Nguyen",
                "Eugene Kharitonov",
                "Jade Copet",
                "Yossi Adi",
                "Wei-Ning Hsu",
                "Ali Mamdouh Elkahky",
                "Paden Tomasello",
                "Robin Algayres",
                "Beno\u0131\u0302t Sagot",
                "Abdelrahman Mohamed",
                "Emmanuel Dupoux"
            ],
            "title": "Generative spoken dialogue language modeling",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "Vassil Panayotov",
                "Guoguo Chen",
                "Daniel Povey",
                "Sanjeev Khudanpur"
            ],
            "title": "Librispeech: An asr corpus based on public domain audio",
            "venue": "books. International Conference on Acoustics, Speech and Signal Processing,",
            "year": 2015
        },
        {
            "authors": [
                "Adam Polyak",
                "Yossi Adi",
                "Jade Copet",
                "Eugene Kharitonov",
                "Kushal Lakhotia",
                "Wei-Ning Hsu",
                "Abdelrahman Mohamed",
                "Emmanuel Dupoux"
            ],
            "title": "Speech resynthesis from discrete disentangled self-supervised representations",
            "venue": "In Interspeech,",
            "year": 2021
        },
        {
            "authors": [
                "Ryan Prenger",
                "Rafael Valle",
                "Bryan Catanzaro"
            ],
            "title": "Waveglow: A flow-based generative network for speech synthesis",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Ofir Press",
                "Noah A. Smith",
                "Mike Lewis"
            ],
            "title": "Train short, test long: Attention with linear biases enables input length extrapolation",
            "venue": "ArXiv, abs/2108.12409,",
            "year": 2021
        },
        {
            "authors": [
                "Mirco Ravanelli",
                "Titouan Parcollet",
                "Peter Plantinga",
                "Aku Rouhe",
                "Samuele Cornell",
                "Loren Lugosch",
                "Cem Subakan",
                "Nauman Dawalatabad",
                "Abdelwahab Heba",
                "Jianyuan Zhong",
                "Ju-Chieh Chou",
                "SungLin Yeh",
                "Szu-Wei Fu",
                "Chien-Feng Liao",
                "Elena Rastorgueva",
                "Fran\u00e7ois Grondin",
                "William Aris",
                "Hwidong Na",
                "Yan Gao",
                "Renato De Mori",
                "Yoshua Bengio"
            ],
            "title": "SpeechBrain: A general-purpose speech toolkit, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Chandan KA Reddy",
                "Vishak Gopal",
                "Ross Cutler",
                "Ebrahim Beyrami",
                "Roger Cheng",
                "Harishchandra Dubey",
                "Sergiy Matusevych",
                "Robert Aichner",
                "Ashkan Aazami",
                "Sebastian Braun"
            ],
            "title": "The interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results",
            "venue": "arXiv preprint arXiv:2005.13981,",
            "year": 2020
        },
        {
            "authors": [
                "Yi Ren",
                "Chenxu Hu",
                "Xu Tan",
                "Tao Qin",
                "Sheng Zhao",
                "Zhou Zhao",
                "Tie-Yan Liu"
            ],
            "title": "Fastspeech 2: Fast and high-quality end-to-end text to speech",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Fl\u00e1vio Ribeiro",
                "Dinei Flor\u00eancio",
                "Cha Zhang",
                "Michael Seltzer"
            ],
            "title": "CrowdMOS: An approach for crowdsourcing mean opinion score studies",
            "venue": "In International Conference on Acoustics, Speech and Signal Processing,",
            "year": 2011
        },
        {
            "authors": [
                "Julius Richter",
                "Simon Welker",
                "Jean-Marie Lemercier",
                "Bunlong Lay",
                "Timo Gerkmann"
            ],
            "title": "Speech enhancement and dereverberation with diffusion-based generative models",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2023
        },
        {
            "authors": [
                "Antony W Rix",
                "John G Beerends",
                "Michael P Hollier",
                "Andries P Hekstra"
            ],
            "title": "Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs",
            "venue": "IEEE international conference on acoustics, speech, and signal processing. Proceedings (Cat. No. 01CH37221),",
            "year": 2001
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "MICCAI 2015: 18th International Conference,",
            "year": 2015
        },
        {
            "authors": [
                "Robin Scheibler",
                "Youna Ji",
                "Soo-Whan Chung",
                "Jaeuk Byun",
                "Soyeon Choe",
                "Min-Seok Choi"
            ],
            "title": "Diffusion-based generative speech source separation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Shen",
                "Ruoming Pang",
                "Ron J Weiss",
                "Mike Schuster",
                "Navdeep Jaitly",
                "Zongheng Yang",
                "Zhifeng Chen",
                "Yu Zhang",
                "Yuxuan Wang",
                "Rj Skerrv-Ryan"
            ],
            "title": "Natural tts synthesis by conditioning wavenet on mel spectrogram predictions",
            "venue": "IEEE international conference on acoustics, speech and signal processing (ICASSP),",
            "year": 2018
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "arXiv preprint arXiv:2011.13456,",
            "year": 2020
        },
        {
            "authors": [
                "Cem Subakan",
                "Mirco Ravanelli",
                "Samuele Cornell",
                "Mirko Bronzi",
                "Jianyuan Zhong"
            ],
            "title": "Attention is all you need in speech separation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Cem Subakan",
                "Mirco Ravanelli",
                "Samuele Cornell",
                "Fran\u00e7ois Grondin",
                "Mirko Bronzi"
            ],
            "title": "Exploring self-attention mechanisms for speech separation",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2023
        },
        {
            "authors": [
                "Hsiang-Sheng Tsai",
                "Heng-Jui Chang",
                "Wen-Chin Huang",
                "Zili Huang",
                "Kushal Lakhotia",
                "Shu-wen Yang",
                "Shuyan Dong",
                "Andy T Liu",
                "Cheng-I Jeff Lai",
                "Jiatong Shi"
            ],
            "title": "Superb-sg: Enhanced speech processing universal performance benchmark for semantic and generative capabilities",
            "venue": "arXiv preprint arXiv:2203.06849,",
            "year": 2022
        },
        {
            "authors": [
                "Cassia Valentini-Botinhao"
            ],
            "title": "Noisy speech database for training speech enhancement algorithms and tts models",
            "venue": "University of Edinburgh. School of Informatics. Centre for Speech Technology Research (CSTR),",
            "year": 2017
        },
        {
            "authors": [
                "Rafael Valle",
                "Kevin Shih",
                "Ryan Prenger",
                "Bryan Catanzaro"
            ],
            "title": "Flowtron: an autoregressive flowbased generative network for text-to-speech synthesis",
            "venue": "arXiv preprint arXiv:2005.05957,",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam M. Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you",
            "venue": "need. ArXiv,",
            "year": 2017
        },
        {
            "authors": [
                "Chengyi Wang",
                "Sanyuan Chen",
                "Yu Wu",
                "Zi-Hua Zhang",
                "Long Zhou",
                "Shujie Liu",
                "Zhuo Chen",
                "Yanqing Liu",
                "Huaming Wang",
                "Jinyu Li",
                "Lei He",
                "Sheng Zhao",
                "Furu Wei"
            ],
            "title": "Neural codec language models are zero-shot text to speech synthesizers",
            "venue": "ArXiv, abs/2301.02111,",
            "year": 2023
        },
        {
            "authors": [
                "Gordon Wichern",
                "Joe Antognini",
                "Michael Flynn",
                "Licheng Richard Zhu",
                "Emmett McQuinn",
                "Dwight Crow",
                "Ethan Manilow",
                "Jonathan Le Roux"
            ],
            "title": "Wham!: Extending speech separation to noisy environments",
            "year": 1907
        },
        {
            "authors": [
                "Junichi Yamagishi",
                "Christophe Veaux",
                "Kirsten MacDonald"
            ],
            "title": "CSTR VCTK Corpus: English multi-speaker corpus for cstr voice cloning toolkit (version 0.92)",
            "year": 2019
        },
        {
            "authors": [
                "Shu-wen Yang",
                "Po-Han Chi",
                "Yung-Sung Chuang",
                "Cheng-I Jeff Lai",
                "Kushal Lakhotia",
                "Yist Y Lin",
                "Andy T Liu",
                "Jiatong Shi",
                "Xuankai Chang",
                "Guan-Ting Lin"
            ],
            "title": "Superb: Speech processing universal performance benchmark",
            "venue": "arXiv preprint arXiv:2105.01051,",
            "year": 2021
        },
        {
            "authors": [
                "Dong Yu",
                "Morten Kolb\u00e6k",
                "Zheng-Hua Tan",
                "Jesper Jensen"
            ],
            "title": "Permutation invariant training of deep models for speaker-independent multi-talker speech separation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2017
        },
        {
            "authors": [
                "Neil Zeghidour",
                "Alejandro Luebs",
                "Ahmed Omran",
                "Jan Skoglund",
                "Marco Tagliasacchi"
            ],
            "title": "Soundstream: An end-to-end neural audio codec",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Heiga Zen",
                "Viet Dang",
                "Rob Clark",
                "Yu Zhang",
                "Ron J Weiss",
                "Ye Jia",
                "Zhifeng Chen",
                "Yonghui Wu"
            ],
            "title": "Libritts: A corpus derived from librispeech for text-to-speech",
            "year": 1904
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Discriminative models have long been the mainstream in speech applications since the deep learning era. These models are applied to different types of tasks such as speech recognition (Graves et al., 2006), enhancement, and separation (Luo & Mesgarani, 2019). Interestingly, even for applications that can be naturally formulated as generative modeling problems, such as text-to-speech (TTS), we see most popular models remained discriminative (Shen et al., 2018; Ren et al., 2021). Consequentially, pre-trained foundation models (Baevski et al., 2020; Hsu et al., 2021) that served as the upstream of speech applications focused more on learning useful representation for discriminative tasks rather than modeling the data distribution p(speech). In this paper, we seek to answer whether generative models can serve as foundation models for speech applications or not.\nUnlike discriminative models, generative models enable sampling of the data distribution. For example, generative TTS models (Habib et al., 2019) allow different emotions to be sampled given a fixed text as discriminative models produce a fixed output. Up to the present, generative models in speech are usually designed for a given purpose via task-specific conditioning or distribution mapping. Perhaps the most well-known examples of task-specific conditional generative models are neural vocoders (Kong et al., 2020; Chen et al., 2020). These models learn to map simple priors (e.g., normal distribution) to waveform conditioning on acoustic features (e.g., spectrogram). On the other hand, examples for distribution mapping include diffusion models that transform noisy speech to clean speech for denoising (Lu et al., 2021; 2022; Richter et al., 2023), or speech mixture to non-overlapping speech for separation (Scheibler et al., 2023).\nIn this work, we explore a new direction to pre-train a general-purpose generative model with unlabeled speech. We hypothesize that a good generative model on speech without pre-defined application can be applied to different end tasks that require speech generation. Our model, named SpeechFlow, is a generative model that combines masked audio modeling and Flow Matching (Lipman et al., 2023). SpeechFlow is trained with unlabeled speech with the goal of estimating the underlying distribution of speech conditioning on masked audio. We show that a generative model trained with unlabeled speech data can be adapted to different tasks that require speech generation by fine-tuning with task-specific conditions using labeled data. More specifically, we fine-tuned SpeechFlow and compared against expert models in speech enhancement, separation, and synthesis.\nFor each task, fine-tuned SpeechFlow is able to match expert models. Experiment results suggested that pre-trained generative models possess great potential to become foundation models for different speech generation tasks."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Generative Speech Models As mentioned earlier, generative models have been applied to different tasks in speech. Research in neural vocoders found generative models to be a good suit for spectrogram-to-waveform prediction. Prevailing generative models are applied to the task with success, such as generative adversarial model (Kong et al., 2020), flow-based invertible model (Prenger et al., 2019), and diffusion network (Koizumi et al., 2022). Besides neural vocoders, generative models are also applied to other tasks such as TTS (Valle et al., 2020), speech enhancement (Lu et al., 2021; 2022; Richter et al., 2023) and separation Scheibler et al. (2023). A fundamental difference between this work and the prior works is that SpeechFlow is not trained for a specific application, but to estimate the underlying distribution of speech itself.\nRecent studies also explored speech generation from a language modeling perspective. Taking advantage of audio tokenizing techniques (Hsu et al., 2021; De\u0301fossez et al., 2022; Zeghidour et al., 2022), Spoken Language Models (SLMs;Lakhotia et al., 2021; Kharitonov et al., 2021; Borsos et al., 2022) have been developed to model language without text. These token-based speech language models are closely related to the proposed method in the sense of training generative models from unlabeled speech. The key difference is the goal of SLMs is to discover the underlying text for textless language processing (Nguyen et al., 2022). In principle, SLMs can also be fine-tuned for different downstream tasks but it was not the focus and they are not evaluated on multiple tasks.\nTargeting controllable audio generation, VALL-E (Wang et al., 2023) extended SLMs by using text and audio prompts to control the audio generated. Voicebox (Le et al., 2023) took a different approach to tackle the problem by feeding aligned text and partially masked speech to perform speech in-filling non-autoregressively. Despite the different paths VALL-E and Voicebox took, both works discovered a strong zero-shot adaptation ability that emerged when training generative models at scale. While these models are designed for text-conditioned generation, they provided a hint of the great potential of generative models with the superior ability to generate diverse speech. It is worth pointing out that Voicebox is the most related work to this work, sharing the same objective function and model architecture. Voicebox can be viewed as a fully supervised text-conditioned SpeechFlow that focused exclusively on TTS task. Later in our experiment, we compare Voicebox to fine-tuned SpeechFlow and reveal the benefit of generative pre-training without text.\nPre-trained Speech Models Conceptually, this work is also related to self-supervised representation learning methods for speech in the sense of learning from unlabeled data for better downstream task performance. One branch of self-supervised learning takes the autoregressive approach to learn from predicting the future, such as contrastive predictive coding (Oord et al., 2018) and autoregresive predictive coding (Chung & Glass, 2020). Another branch of works (Ling et al., 2020; Ling & Liu, 2020) studied masked audio modeling (MAM) instead of future prediction. These models predict masked Spectrogram based on the complementary part of the input that is unmasked. Improving the MAM-based method, similar works replaced the prediction target with latent features such as quantized representation (Baevski et al., 2020) or acoustic units (Hsu et al., 2021). Self-supervised representation learning methods are found to be useful in many different applications such as speech recognition (Yang et al., 2021). But the success is mostly on discriminative tasks, applying selfsupervised models for generation application tasks is often less intuitive (Polyak et al., 2021) and under-performing (Tsai et al., 2022). Taking cues from the success of masking-based methods, we incorporate a similar idea into SpeechFlow to make generation conditioned on partially masked speech during pre-training. Interestingly, we found MAM beneficial to generative pre-training as shown later in Section 4.6. Besides self-supervised learning, pre-training have also been studied in the context of semi-supervised TTS (Chung et al., 2019) or speech-text alignment (Ao et al., 2021), but these works focused on non-generative models."
        },
        {
            "heading": "3 METHOD",
            "text": ""
        },
        {
            "heading": "3.1 BACKGROUND: FLOW MATCHING FOR GENERATIVE MODELING",
            "text": "Deep generative models aimed to estimate the unknown distribution q(x) of real world ddimensional data x \u2208 Rd with distribution p(x) parameterized by neural networks. To make sam-\npling possible, simple prior distribution p0(x) (e.g., normal distribution) is naturally a good starting point, and the modeling problem therefore becomes finding a neural transport map p1 = F\u03b8(p0) such that p1(x) \u2248 q(x). Early works such as generative adversarial networks (Goodfellow et al., 2020) and variational audio encoders (Kingma & Welling, 2013) showed directly modeling x1 = f\u03b8(x0) where x0 \u223c p0(x), x1 \u223c q(x), i.e., predicting data from noise using network f\u03b8, is feasible. Recent studies in diffusion models (Ho et al., 2020; Song et al., 2020) suggested an iterative denoising model xt+\u2206t = f\u03b8,t,\u2206t(xt) that traverses from noise x0 to data x1 with step size \u2206t provides better generation quality (Dhariwal & Nichol, 2021). In this work, we choose to construct the neural transport map p1 = F\u03b8(p0) using Flow Matching (Lipman et al., 2023) from the Continuous Normalizing Flows (CNFs; Chen et al., 2018)- family.\nFormally, CNFs defined a path between simple prior p0 and target distribution p1 via the timedependent probability density function pt : [0, 1] \u00d7 Rd \u2192 R>0. The flow of x along the path, denoted \u03d5t : [0, 1]\u00d7 Rd \u2192 Rd, is defined using ordinary differential equation (ODE):\nd dt \u03d5t(x) = vt(\u03d5t(x)); \u03d50(x) = x; (1)\nwith the time-dependent vector field vt : [0, 1] \u00d7 Rd \u2192 Rd, such that the time-dependent probability density function pt can be derived using the change of variables formula: pt =\np0(\u03d5 \u22121 t (x)) det [ \u2202\u03d5\u22121t \u2202x (x) ] . Under the formulation, a simple objective is to predict the vector field vt using a neural network paramterized by \u03b8 given the target vector field ut(x) that corresponds to pt(x) with the Flow Matching objective\nLFM (\u03b8) = Et\u223cU [0,1],x\u223cpt(x) \u2225\u2225\u2225vt(x; \u03b8)\u2212 ut(x)\u2225\u2225\u22252. (2)\nHowever, LFM (\u03b8) is intractable due to the lack of knowledge of pt and ut in practice. Interestingly, Lipman et al. (2023) showed that conditioning pt and ut on real data x1 results in the Conditional Flow Matching objective LCFM (\u03b8) which provided identical gradient w.r.t. \u03b8 for training the generative model. Specifically, we adopt the Optimal Transport conditional path proposed by Lipman et al. (2023) that assumes the mean \u00b5t(x) = tx1 and standard deviation \u03c3t(x) = 1 \u2212 (1 \u2212 \u03c3min)t change linearly in time, yielding tractable pt(x|x1) = N (x | \u00b5t(x1), \u03c3t(x1)2I) and ut(x|x1) = (x1\u2212(1\u2212\u03c3min)x) (1\u2212(1\u2212\u03c3min)t) with a sufficiently small \u03c3min (we use 1e-5) such that p1(x|x1) is centered around x1. In this case, with reparameterization the Conditional Flow Matching objective has the form\nLCFM (\u03b8) = Et,q(x1),p0(x0) \u2225\u2225\u2225vt(\u03c8t(x0); \u03b8)\u2212 (x1 \u2212 (1\u2212 \u03c3min)x0)\u2225\u2225\u22252, (3)\nwhere \u03c8t(x0) = \u03c3t(x1)x0 + \u00b5t(x1) and t is sampled uniformly from [0, 1]."
        },
        {
            "heading": "3.2 GENERATIVE PRE-TRAINING OF SPEECHFLOW WITH UNLABELED SPEECH",
            "text": "Inspired by the recent success of flow matching model in speech synthesis (Le et al., 2023), we propose to pre-train a generative model with unlabeled speech using flow matching. We consider the problem of modeling q(x) where the acoustic features x \u2208 Rd\u00d7L are d-dimensional Mel spectrogram with L frames. We assume the simple prior p0 to be the normal distribution. Since generative models are by nature unsupervised/self-supervised (no human label required), a flow matching model can be trained with pure speech.\nMasked Audio Condition In light of the success of masked prediction in self-supervised speech representation learning (Baevski et al., 2020; Hsu et al., 2021), we introduce similar concept to SpeechFlow by additionally conditioning vt on partially masked target audio xmask with a chance of pcond during training. This can also be interpreted as the model have a chance of 1\u2212 pcond to receive fully masked xmask. Masked condition xmask is obtained by randomly selecting nmask of frames to be masked with a minimum masking span length of lmask.\nNote that while this modification results in a conditional generative model, our model is still selfsupervised since xmask is directly derived from unlabeled speech x1. Moreover, a vanilla flow matching model without any condition is still available after pre-training stage as long as pcond < 1. Study on the importance of pcond is provided in Section 4.6.\nThe rationale behind the auxiliary condition is to provide the model more context for predicting vt regardless of the timestep t. Moreover, introducing auxiliary condition at the pre-training stage provided an intuitive way to fine-tune the model for different tasks as shown later in this section.\nObjective With the predicted time-dependent vector field vt conditioning on masked feature xmask, the generative pre-training objective of SpeechFlow can be derived by modifying Equation 3 accordingly to obtain\nEt,q(x1),p(x0) \u2225\u2225\u2225vt(\u03c8t(x0), xmask; \u03b8)\u2212 (x1 \u2212 (1\u2212 \u03c3min)x0)\u2225\u2225\u22252. (4)\nIn practice, we use Transformer encoder (Vaswani et al., 2017) with learnable parameter \u03b8 to predict vector field vt. Masked inputs xmask are concatenated with \u03c8t(x0) along the frequency axis, then projected to match the model dimension d\u03b8, and we append the sinusoidal positional encoding of timestep t to the input, resulting the actual model input with shape Rd\u03b8\u00d7(L+1). The output of the model is the predicted vector field vt \u2208 Rd\u00d7L."
        },
        {
            "heading": "3.3 SUPERVISED FINE-TUNING SPEECHFLOW ON DIFFERENT TASKS",
            "text": "Task-specific Condition While the pre-trained SpeechFlow allow us to sample new data from p1(x), most applications in speech require a certain degree of control over the output. To this end, we introduce the fine-tuning stage for controllable generation using task-specific condition y \u2208 Rdy\u00d7Ly of audio x1, such as noisy speech for speech enhancement and text transcript for text-to-speech generation. We note that this work focused on tasks where y and x1 are aligned, i.e., Ly = L, and leave the unaligned cases for future work. Concrete examples can be found in Section A.3. Objective Following the pre-training stage, the fine-tuning objective can be derived by swapping the masked condition xmask for pre-training with task-specific condition y,\nEt,q(x1),p(x0) \u2225\u2225\u2225vt(\u03c8t(x0), y; \u03b8)\u2212 (x1 \u2212 (1\u2212 \u03c3min)x0)\u2225\u2225\u22252. (5)\nNote that for fine-tuning, it is critical to reuse \u03b8 from the pre-training stage. Inference After training, speech generation is done by the following steps: (1) sample x0 from the simple prior p0(x); (2) use an ODE solver to solve \u03d51(x0) given d\u03d5t(x0)/dt = vt(\u03d5t(x0), y; \u03b8) and \u03d50(x0) = x0; (3) generated audible speech in time domain from Mel spectrogram x1. More inference details are provided in Section A.2 including conversion from Mel spectrogram to waveform."
        },
        {
            "heading": "4 EXPERIMENT",
            "text": ""
        },
        {
            "heading": "4.1 PRE-TRAINING DETAILS",
            "text": "Model & Data We focus on Transformer encoder (Vaswani et al., 2017) with 24 layers, 16 attention heads, d\u03b8 =1024 dimensional embedding, and feed-forward networks with 4096 dimensions. Convolutional positional embedding (Baevski et al., 2020) and ALiBi self-attention bias (Press et al., 2021) are used to encode relative positional information. Following Le et al. (2023), skip connections between layers are introduced to mimic U-Net (Ronneberger et al., 2015) architecture. The\nmodel has around 330M parameters in total. The model is pre-trained on 60k hours of speech from English audiobook at 16kHz. We consider x to be log-scaled Mel spectrogram extracted with a 40ms window at 100Hz with d = 80, resulting 160/80 dimensional input/output for the model.\nTraining We pre-train SpeechFlow for 600k steps on 32 V100 GPUs with a batch size of 75 seconds per GPU with FP16. We use Adam optimizer (Kingma & Ba, 2014) with the learning rate warming up linearly to 5e-5 for the first 5k steps and linearly decaying to 1e-5 for the rest of the training. For masking, we set pdrop = 10%, nmask \u223c U [70%, 100%], and lmask = 10. All masked position are filled with zero. In practice, we compute loss at the masked position only."
        },
        {
            "heading": "4.2 FINE-TUNING FOR SPEECH ENHANCEMENT",
            "text": "Task & Metrics Speech enhancement, also known as denoising, aimed to remove unwanted noise from speech recording. We report Perceptual Evaluation of Speech Quality (PESQ; Rix et al., 2001), Extended Short-Time Objective Intelligibility (ESTOI; Jensen & Taal, 2016), and Composite Objective Speech Quality and Overall Quality (CSIG/COVL;Hu & Loizou, 2007).\nPrior Works Early work Conv-TasNet (Luo & Mesgarani, 2019) has been widely used as the baseline system. It is a convolutional encoder/decoder operating in the time domain to maximize scale-invariant source-to-noise ratio. DEMUCS (De\u0301fossez et al., 2020) adopted a similar structure with skip-connections and minimized L1/multi-resolution STFT loss. MetricGAN+ (Fu et al., 2021) proposed to optimize non-differentiable metrics such as PESQ via adversarial training against their approximation using discriminators. SGMSE+(Richter et al., 2023) reformulated the problem as a diffusion process that can be solved with the corresponding generative model (Ho et al., 2020).\nDataset We fine-tuned and tested SpeechFlow on the benchmark dataset VoiceBank-Demand (VB-DMD; Valentini-Botinhao et al., 2017) for fair comparison against most of the prior works in the field. Since VB-DMD is a relatively small dataset, we also consider testing on WSJ0CHiMe3 (Richter et al., 2023) to ensure the model is not overfitting. In addition, we also trained our model using 100 hours of noisy speech from Deep Noise Supression Challenge 2020 (DNS2020; Reddy et al., 2020) for extra results to demonstrate the generalizability for SpeechFlow. For training, paired data (x1, y) is provided where x1 is the target clean signal and y is the noisy speech. For testing, only the noisy speech y is provided and the goal is to estimate the clean signal x1. All datasets are resampled to 16kHz to match pre-training and no data augmentation was applied.\nTraining As mentioned in Section 3.3, fine-tuning is simply done by replacing the auxiliary masked condition xm for pre-training with the acoustic feature of the noisy speech y and minimize Eq. 5. Note that, unlike pre-training, y has a pdrop = 30% chance to be dropped but never partially\nmasked for fine-tuning. We fine-tuned SpeechFlow on single V100 GPU for 160 / 75 epochs on VB-DMD / DNS2020 respectively with a batch size of 50 seconds. The learning rate is set to peak at 2e-5 after 5k updates, then linearly decay to 0. For the control group without pre-training, we searched learning rate between 1e-4 to 1e-3 and found 2e-4 the best.\nResults Main results are provided in Table 1. Due to the choice of acoustic feature, our method suffers from the imperfect pseudo-inverse of Mel filters and the lack of phase modeling. In contrast to prior works tailored for enhancement, these restrictions result in a worse upper-bound as shown in the table. Nevertheless, our method still provided comparable or better results against the prior works on both benchmark datasets. Despite using a dataset with different topics and speakers, generative pre-training still improved enhancement results compared to the same model trained on VBDMD from scratch. Especially on the out-of-domain WSJ0-CHiME3 testing, SpeechFlow demonstrated strong generalizability with a clear gap on PESQ, CSIG, and COVL against all other methods. In the case where the larger dataset DNS2020 is used for fine-tuning, a similar trend can be found compared to prior work DEMUCS and the testing result on WSJ0-CHiME3 can be further improved. These results pointed out the great potential of generative pre-training on speech."
        },
        {
            "heading": "4.3 FINE-TUNING FOR SPEECH SEPARATION",
            "text": "Task & Metrics The goal of separation is to separate mixture (overlapped) speech into multiple single-speaker speech. In our experiment, we focus on separating 2 to 3 speakers for simplicity. We report the common metric Scale-Invariant Signal-to-Distortion Ratio improvement (SI-SDRi; Le Roux et al., 2019) that measures the improvement of separated speech over the mixture when comparing against the clean reference in the time domain. In addition, we also report the ESTOI improvement (ESTOIi) of the separation result over the mixture to measure the intelligibility.\nDataset & Prior Work For separation, SpeechFlow is fine-tuned using a synthetic mixture created by randomly sampling and mixing 2 or 3 utterances from 360 hours of speech from English audiobook. In addition, noise sampled from WHAM! dataset (Wichern et al., 2019) can be added to the mixture to further increase the difficulty of separation, combining 4 different setups in total. We tested the fine-tuned model on LibriMix (Cosentino et al., 2020) 16khz min. For training, paired data (x11, x 2 1, y) is provided where x 1 1, x 2 1 are the target clean signal and y is the mixture. Signals are randomly cropped into 8-second chunks for training. To ensure the model outputs all speakers, we concatenated the clean signals along the time axis (and repeated the condition y accordingly) for both training and testing. The baseline system is Conv-TasNet (Luo & Mesgarani, 2019) from LibriMix1. We note that while there are many other prior works in the field, most of them focused on WSJ2mix dataset (Hershey et al., 2016) with 8kHz audio, which makes fair comparison difficult. To provide a more competitive baseline, we reproduce a more powerful separation model SepFormer (Subakan et al., 2021; 2023) at 16kHz using code provided by the authors 2.\nTraining The fine-tuning setup follows enhancement with few changes: batch size is reduced to 37.5 seconds; model is fine-tuned for 85 epochs; peak learning rate is set to 3e-5. For SpeechFlow without pre-training, we searched learning rate between 1e-5 to 1e-4 and found 5e-5 the best.\n1 https://huggingface.co/JorisCos 2 https://github.com/speechbrain/speechbrain/tree/v0.5.15/recipes/LibriMix\nResults Results are provided in Table 2. We found SI-SDRi more sensitive to the process of Melspectrogram-to-waveform. This can be verified by examining the upper-bound performance using a clean reference Mel spectrogram, which is even worse than the baseline Conv-TasNet. Similarly, we found the more recent transformer-based model SepFormer (Subakan et al., 2023) struggled in SI-SDRi when training at 16kHz (i.e., 2x longer input). In contrast, we found ESTOIi that reflected the intelligibility of separation result more robust to waveform estimation. Nevertheless, fine-tuned SpeechFlow was able to provide strong separation results. The gap between SpeechFlow and its upper-bound is particularly small in the easy 2 Mix setup. To measure the true quality of the Mel spectrogram generated by SpeechFlow, we also experimented with learnable inverse-Mel and phase estimation (as described in Section A.2) and found the separation result can be further boosted in terms of SI-SDRi. Since optimizing the Mel-spectrogram-to-waveform transform is beyond the scope of this paper, we apply learnable estimation to the best result of 2 Mix and 2 Mix + Noise only. The key idea is to show the separation result in the Mel spectrogram is already at a high quality, and metrics that are limited by the choice of input/output feature like SI-SDRi can be further improved with extra effort. In conclusion, we found SpeechFlow providing better intelligibility in all cases. It is worth noting that the fine-tuning method presented here is a vanilla solution that might not scale well as the number of speakers increases, a more dedicated fine-tuning method is left as future work."
        },
        {
            "heading": "4.4 FINE-TUNING FOR ZERO-SHOT SPEAKER ADAPTATION OF TEXT-TO-SPEECH",
            "text": "Task & Metrics We consider speech generation conditioning on text, i.e., text-to-speech (TTS). In particular, we focus on the zero-shot speaker adaptation problem (Jia et al., 2018; Casanova et al., 2021) where the voice of an unseen speaker should be used for synthesis. The problem setup and the evaluation metrics followed VALL-E (Wang et al., 2023) and Voicebox (Le et al., 2023). Zero-shot adaptation is done by using a 3-second prompt that carries speaker, paralinguistic, and environmental information. To measure the correctness and the intelligibility of the synthetic speech, we measure the recognition word error rate (WER) using HuBERT-L (Hsu et al., 2021) pre-trained and fine-tuned on LibriLight (Kahn et al., 2019) and LibriSpeech (Panayotov et al., 2015) respectively. Using WavLM-TDCNN speaker embedding model Chen et al. (2022), speaker similarity is measured by the similarity between the embedding of generated speech and that of the conditioning audio. Similarity to the original conditioning audio (SIM-o) and to the vocoderresynthesized audio (SIM-r) are reported. In addition to the objective metrics, subjective evaluation on cross-sentence reference results using mean opinion score is also provided. See more detail regarding MOS test in Section A.4.6.\nPrior Works YourTTS (Casanova et al., 2021) is a flow-based model (Kim et al., 2021) trained on multi-lingual data, including VCTK (Yamagishi et al., 2019), TTS-portugese (Casanova et al., 2022), M-AILABS French (Munich Artificial Intelligence Laboratories GmbH, 2017), and LibriTTS (Zen et al., 2019). VALL-E is a decoder-only auto-regressive model trained on LibriLight for zero-shot\nspeaker adaptation TTS. Lastly, the closely related prior work Voicebox combined flow-matching and masked prediction for supervised TTS training. Voicebox can be viewed as a strong baseline using the same amount of data with fully supervised training.\nDataset 960 hours of transcribed speech from English audiobook is used for fine-tuning. The testing protocol follows VALL-E and Voicebox. Montreal Force Aligner (McAuliffe et al., 2017) is used for phone-speech alignment. Position postfixes are added to each phone following Voicebox. Additional results on fine-tuning with less (100/10 hours) labeled data are provided in Section A.4.4.\nTraining To enable zero-shot speaker adaptation , fine-tuning condition y includes masked audio xm and the force-aligned phone sequence. We followed the masking strategy of Voicebox during fine-tuning. We additionally tested fine-tuning with more (32) GPUs and Low-rank Adaptors (LoRA; Hu et al., 2021; we use rank r = 64) to study the impact of computational resource for fine-tuning. Section A.4.2 provided a detailed performance analysis based on the number of GPUs used for fine-tuning. The batch size is 75 seconds per GPU in all cases. For standard fine-tuning, the learning rate is set to peak at 1e-5 after 5k updates, then linearly decay to 0 for the rest 145k steps. For LoRA fine-tuning, 9.5M new learnable parameters are introduced to the pre-trained model, accounting for 2.8% of the full model. All pre-trained weights are frozen. The learning rate is set to peak at 1e-3. Additional results on the impact of the amount of fine-tuning GPU is provided in Section A.4.3 .\nResults Results are provided in Table 3. Comparing to fully supervised models Voicebox or VALL-E, a clear advantage in speaker modeling can be found with SpeechFlow despite using much less labeled data. In terms of WER and MOS, SpeechFlow is slightly worse than Voicebox that uses more labeled data. In addition, while single GPU fine-tuning already provided better speaker adaptation than all baselines, we found fine-tuning with more GPUs provided even stronger results. Interestingly, LoRA performed the best in terms of both SIM and WER among all fine-tuning setups. This suggested that fine-tuning method for generative model could be worth exploring in the future. Finally, our baseline without pre-training achieved similar WER to that of the pre-trained model but a significantly worse SIM. These findings suggested the proposed generative pre-training improves speaker modeling but not content modeling for speech synthesis."
        },
        {
            "heading": "4.5 MULTI-TASK FINE-TUNING OF SPEECHFLOW",
            "text": "Preceding sections showed SpeechFlow can be fine-tuned for different purpose using limited paired data and/or computation. In this section we take one step further to investigate the possibility to build an all-in-one controllable speech generation model via multi-task fine-tuning. Results are carried out in Table 4. We simply combined the labeled datasets for enhancement (DNS), separation (2Mix+Noise), and TTS for fine-tuning. We upsampled these datasets with a factor of 10/4/1 respectively to balance the importance of each task. Pre-trained SpeechFlow is fine-tuned on single GPU for 700k updates with the same learning rate scheduler peaking at 2e-5.\nFor zero-shot speaker adaptation TTS, we observed a drop on both WER and SIM-o, suggesting multi-task learning can lead to worse performance in specific single task. However, multi-task results are found to be better than single-task ones for enhancement. One possible explanation is the separation task trained on mixture+noise can also be viewed as a hard enhancement problem the model was additionally trained on. This showcased the benefit of having a universal model - some tasks might benefit from others. For separation, we found multi-task model deteriorated significantly\ncomparing to the single task model. Preliminary results presented in this section suggested an all-inone speech generative model can be built from SpeechFlow, but further research and development is required to improve the results and cover a more diverse set of tasks."
        },
        {
            "heading": "4.6 IMPACT OF PRE-TRAINING HYPER-PARAMETER",
            "text": "Since the the main focus of our method is on pre-training generative speech model, we provide study on the corresponding hyper-parameters here. To evaluate the pre-trained model in a less biased perspective, we consider both speaker similarity of zero-shot speaker adaptation TTS and PESQ of enhancement for multi-task fine-tuned model. Results are provided in Figure 2.\nFirst, we investigate the pre-trained model quality as a function of the number pre-training steps or learning rate. We set the total number of updates to 750k, which is about 7 epochs on training set. One caveat is that learning rate decay is applied through out the training, which could also contribute to the tapering result. We found most of the gain coming from the early stage before 400 updates and setting the learning rate above 5e-5 is sufficient for stable result.\nWe also found the model to be stable when setting pcond above 80%. Importantly, we also found unconditioned pre-training, i.e., pcond = 0, yielded bad performance on both tasks. The result showcased the helpfulness and the necessity of masked prediction for pre-training. In summary, SpeechFlow is stable as long as masked conditioning is prioritized (over unconditioned pre-training) and the model is trained with sufficient steps and step size. Additional studies on other masking hyper-parameters (nmask and lmask) are provided in Section A.4.2."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we studied the role of generative model as a foundation model instead of a tool for a specific task. We show that training SpeechFlow using flow matching with masked condition results in a strong generative model. The model can be deployed to different downstream tasks using simple fine-tuning strategy with a single GPU. In our experiment, we adapted SpeechFlow to speech enhancement, separation, and zero-shot speaker adaptation TTS with performance comparable to task-specific models. More importantly, SpeechFlow demonstrated the potential to unify generative tasks for speech.\nLimitations and Future Works This work focused on developing the pre-train-and-fine-tune framework for generative speech model. For the selected downstream applications, we assumed a frame-wise condition (e.g., noisy spectrogram; force-aligned phone label) is available in the finetune dataset. Fine-tuning with misaligned data (e.g., raw text, speaker ID) is left as an important future work. In addition, SpeechFlow is trained and tested on English-only data. However, since the generative model can be trained without label data, we believe the method can be easily scaled to more languages in the future. For future works, we would like to point out that the choice of acoustic feature may limit the applications as we discovered in enhancement and separation. Hence finding a more general acoustic feature would be a key step to general purpose generative speech model. Finally, we note that some of the expert models compared in different downstream tasks have other focuses besides the reported metrics (e.g., DEMUCS is built to run in real-time with fewer parameters). Therefore, we would like to emphasize that this work is mainly to show the potential of pre-trained generative models rather than claiming state-of-the-art in different tasks."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 DEMO PAGE",
            "text": "Please check index.html in the supplementary materials for audio samples.\nA.2 INFERENCE DETAILS\nGenerating Mel Spectrogram To generate Mel spectrogram x1, we first sample x0 from the simple prior p0(x). The next step is to estimate \u03d51(x0) given \u03d50(x0) = x0 by evaluating vt(\u03d5t(x0), y; \u03b8) at multiple t. Each evaluation required forwarding through the neural network, and a larger number of function evaluations (NFEs) leads to a more accurate estimation of \u03d51(x0). In addition, we also applied classifier-free guidance (CFG; Dhariwal & Nichol, 2021; Le et al., 2023) to prioritize audio quality over diversity. CFG is done by additionally predicting the unconditioned vector field vt(\u03d5t(x0); \u03b8) (where the task-specific condition is dropped) to obtain a modified prediction\nv\u0303t = (1 + \u03b1) \u00b7 vt(\u03d5t(x0), y; \u03b8) + \u03b1 \u00b7 vt(\u03d5t(x0); \u03b8). (6)\nCFG allows us to improve sample quality by focusing more on task-specific conditioned generation with larger \u03b1 at the cost of doubling NFEs. We use \u03b1 = 0.5 for enhancement and 0.7 for other tasks in practice. For the ODE solver, we use midpoint method implemented in torchdiffeq (Chen, 2018) to derive \u03d51(x0) from \u03d50(x0) by approximating the integration from t = 0 to t = 1 with a step size of 0.0625, resulting 32 NFEs per sample.\nZero-shot Speaker Adaptation TTS To generate audible speech from Mel spectrogram, HiFiGAN vocoder (Kong et al., 2020) from VoiceBox (Le et al., 2023) is adopted. In addition, phone duration is also needed to determine the output spectrogram length and the frame-wise condition given the input phone sequence. The regression-based duration predictor from VoiceBox is adopted for all TTS-related experiments.\nSpeech Enhancement Different from TTS, enhancement metrics are more sensitive to the sampleto-sample alignment of the waveform between the hypothesis and reference. This makes Neural vocoder a bad option for the task3. Alternatively, we found using pseudo-inverse of Mel filter bank to recover linear Spectrogram, adding phase information taken directly from the noisy speech (input condition), and apply inverse Short-Time Fourier Transform (iSTFT) sufficient4. As a reference, PESQ on WSJ0-CHiME3 dropped from 2.70 to 2.29 when switching the signal processing method to HiFi-GAN vocoder.\nSpeech Separation For this task, we found both the signal processing method and HiFi-GAN vocoder not enough for the most popular metric SI-SDRi (see discussion in Section 4.3). To this end, we train a 3-layer ResNet for both pseudo-inverse Mel transform and phase estimation using precomputed Mel spectrogram prediction and target waveform on the training set. The model takes the separation result (Mel spectrograms from SpeechFlow) and the complex spectrogram of the mixture as input, predicting both the linear spectrogram and the phase information to be combined and transformed to the time domain with iSTFT. Since the whole process is differentiable, the model is trained to maximize the permutation-invariant (Yu et al., 2017) SI-SDR loss against the target waveform.\n3See the last section in demo page for examples. 4See the topline section in Table 7 for the error introduced by the process.\nA.3 MODEL ARCHITECTURE AND CONDITION DETAILS"
        },
        {
            "heading": "A.4 ADDITIONAL RESULTS",
            "text": ""
        },
        {
            "heading": "A.4.1 SPEECH EDITING",
            "text": "Following the setup of A3T (Bai et al., 2022), here we additionally consider the task where the center 50% of an recording is to be edited. See Figure 3 and Section 4.4 in Bai et al. 2022 for more details and illustration of the task. Similar to A3T and Voicebox (Le et al., 2023) that are trained with masked audio and text conditioning, speech editing is another downstream that can be naturally solved with the fine-tuned SpeechFlow. Results are presented in Table 6, evaluation metrics and the dataset follows the zero-shot speaker adaptation TTS experiment presented in Section 4.4. Similar to zero-shot speaker adaptation TTS, we found SpeechFlow performing close to the state-of-the-art model using much less labeled data thanks to pre-training."
        },
        {
            "heading": "A.4.2 FULL RESULT FOR SPEECH ENHANCEMENT",
            "text": "A.4.3 INCREASING THE NUMBER OF GPUS FOR FINE-TUNING\nUnsurprisingly, more GPUs (larger batch size) results in better performance in general. Given the fact that fine-tuning have smaller gap between the result of using 1 and 32 GPUs, it is worth noting that fine-tuning is more robust than training from scratch in terms of speaker similarity."
        },
        {
            "heading": "A.4.4 REDUCING LABELED DATA FOR FINE-TUNING",
            "text": "Interestingly, we found pre-trained model generalized better to unseen speaker comparing against models trained from scratch. However, it is also harder to overfit the pre-trainiend model on the limited amount of text input, resulting a worse intelligibility in terms of WER. Nevertheless, with 10 hours of fine-tuning data, SpeechFlow was able to outperform VALL-E (Wang et al., 2023) that was trained on 60k hours data."
        },
        {
            "heading": "A.4.5 MASKING HYPER-PARAMETER FOR PRE-TRAINING",
            "text": "Table 10 studied the impact of the proportion for placing mask nmask and the masking span size lmask. In simple terms, we found masking a significant proportion is important for SpeechFlow."
        },
        {
            "heading": "A.4.6 SUBJECTIVE EVALUATION FOR ZERO-SHOT SPEAKER ADAPTATION TTS",
            "text": "In addition to objective metrics that covers intelligibility and similarity measured by models, we conducted human evaluation to measure the overall quality of audio samples using Mean Opinion Score (MOS) following CrowdMOS (Ribeiro et al., 2011). We randomly selected 50 sentences from the LS test-clean for human evaluation. Each audio sample received 10 ratings in total. Each participant was asked to rate 20 audio samples, including 5 different sentences with audio from 4 different sources - ground truth, YourTTS (Casanova et al., 2021), Voicebox (Le et al., 2023), and SpeechFlow. Results are collected through Amazon Mechanical Turk (AMT) with task description provided in Table 12. Annotators are filtered with the following qualifications: (1) They need to be wearing a headset; (2) They need to pass an onboarding test (2 simple questions, where in each question people need to pick an audio with higher quality); (3) Post-processing, correlation coef between annotators\u2019 answer and the majority answer greater than 0.2.\nFrom the MOS results in Table 11, we confirmed that SpeechFlow is able to generate high quality audio judging by human, falling only slightly behind its fully supervised counterpart Voicebox while using over 62.5x less labeled data."
        }
    ],
    "year": 2023
}