{
    "abstractText": "Accurate 3D object detection and understanding for self-driving cars heavily relies on LiDAR point clouds, necessitating large amounts of labeled data to train. In this work, we introduce an innovative pre-training approach, Grounded Point Colorization (GPC), to bridge the gap between data and labels by teaching the model to colorize LiDAR point clouds, equipping it with valuable semantic cues. To tackle challenges arising from color variations and selection bias, we incorporate color as \u201ccontext\" by providing ground-truth colors as hints during colorization. Experimental results on the KITTI and Waymo datasets demonstrate GPC\u2019s remarkable effectiveness. Even with limited labeled data, GPC significantly improves finetuning performance; notably, on just 20% of the KITTI dataset, GPC outperforms training from scratch with the entire dataset. In sum, we introduce a fresh perspective on pre-training for 3D object detection, aligning the objective with the model\u2019s intended role and ultimately advancing the accuracy and efficiency of 3D object detection for autonomous vehicles.",
    "authors": [],
    "id": "SP:36ccba3cd2f2be2f21dc845a1f5a35fb28e87b99",
    "references": [
        {
            "authors": [
                "R. Adams",
                "L. Bischof"
            ],
            "title": "Seeded region growing",
            "venue": "TPAMI, 16(6):641\u2013647,",
            "year": 1994
        },
        {
            "authors": [
                "Mohamed Afham",
                "Isuru Dissanayake",
                "Dinithi Dissanayake",
                "Amaya Dharmasiri",
                "Kanchana Thilakarathna",
                "Ranga Rodrigo"
            ],
            "title": "Crosspoint: Self-supervised cross-modal contrastive learning for 3d point cloud understanding",
            "year": 2022
        },
        {
            "authors": [
                "Mahmoud Assran",
                "Mathilde Caron",
                "Ishan Misra",
                "Piotr Bojanowski",
                "Florian Bordes",
                "Pascal Vincent",
                "Armand Joulin",
                "Mike Rabbat",
                "Nicolas Ballas"
            ],
            "title": "Masked siamese networks for label-efficient learning",
            "year": 2022
        },
        {
            "authors": [
                "Christopher M Bishop",
                "Nasser M Nasrabadi"
            ],
            "title": "Pattern recognition and machine learning, volume",
            "year": 2006
        },
        {
            "authors": [
                "Remi Cadene",
                "Corentin Dancette",
                "Matthieu Cord",
                "Devi Parikh"
            ],
            "title": "Rubi: Reducing unimodal biases for visual question answering",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Holger Caesar",
                "Varun Bankiti",
                "Alex H Lang",
                "Sourabh Vora",
                "Venice Erin Liong",
                "Qiang Xu",
                "Anush Krishnan",
                "Yu Pan",
                "Giancarlo Baldan",
                "Oscar Beijbom"
            ],
            "title": "nuscenes: A multimodal dataset for autonomous driving",
            "year": 2020
        },
        {
            "authors": [
                "Gavin C Cawley",
                "Nicola LC Talbot"
            ],
            "title": "On over-fitting in model selection and subsequent selection bias in performance evaluation",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2010
        },
        {
            "authors": [
                "Angel X Chang",
                "Thomas Funkhouser",
                "Leonidas Guibas",
                "Pat Hanrahan",
                "Qixing Huang",
                "Zimo Li",
                "Silvio Savarese",
                "Manolis Savva",
                "Shuran Song",
                "Hao Su"
            ],
            "title": "Shapenet: An information-rich 3d model repository",
            "venue": "arXiv preprint arXiv:1512.03012,",
            "year": 2015
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In ICML. PMLR,",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Kaiming He"
            ],
            "title": "Exploring simple siamese representation learning",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Xinlei Chen",
                "Haoqi Fan",
                "Ross Girshick",
                "Kaiming He"
            ],
            "title": "Improved baselines with momentum contrastive learning",
            "venue": "arXiv preprint arXiv:2003.04297,",
            "year": 2020
        },
        {
            "authors": [
                "Christopher Choy",
                "JunYoung Gwak",
                "Silvio Savarese"
            ],
            "title": "4d spatio-temporal convnets: Minkowski convolutional neural networks",
            "year": 2019
        },
        {
            "authors": [
                "Christopher Clark",
                "Mark Yatskar",
                "Luke Zettlemoyer"
            ],
            "title": "Don\u2019t take the easy way out: Ensemble based methods for avoiding known dataset biases",
            "year": 1909
        },
        {
            "authors": [
                "Thomas M Cover"
            ],
            "title": "Elements of information theory",
            "year": 1999
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Danny Driess",
                "Fei Xia",
                "Mehdi SM Sajjadi",
                "Corey Lynch",
                "Aakanksha Chowdhery",
                "Brian Ichter",
                "Ayzaan Wahid",
                "Jonathan Tompson",
                "Quan Vuong",
                "Tianhe Yu"
            ],
            "title": "Palm-e: An embodied multimodal language model",
            "venue": "arXiv preprint arXiv:2303.03378,",
            "year": 2023
        },
        {
            "authors": [
                "Benjamin Eckart",
                "Wentao Yuan",
                "Chao Liu",
                "Jan Kautz"
            ],
            "title": "Self-supervised learning on 3d point clouds by learning discrete generative models",
            "year": 2021
        },
        {
            "authors": [
                "Lue Fan",
                "Feng Wang",
                "Naiyan Wang",
                "ZHAO-XIANG ZHANG"
            ],
            "title": "Fully sparse 3d object detection",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Andreas Geiger",
                "Philip Lenz",
                "Raquel Urtasun"
            ],
            "title": "Are we ready for autonomous driving? the kitti vision benchmark suite",
            "venue": "In CVPR,",
            "year": 2012
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Guo",
                "Mohammad Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Trevor Hastie",
                "Robert Tibshirani",
                "Jerome H Friedman"
            ],
            "title": "The elements of statistical learning: data mining, inference, and prediction, volume",
            "year": 2009
        },
        {
            "authors": [
                "Chenhang He",
                "Hui Zeng",
                "Jianqiang Huang",
                "Xian-Sheng Hua",
                "Lei Zhang"
            ],
            "title": "Structure aware single-stage 3d object detection from point cloud",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "year": 2022
        },
        {
            "authors": [
                "Olivier Henaff"
            ],
            "title": "Data-efficient image recognition with contrastive predictive coding",
            "venue": "In ICML. PMLR,",
            "year": 2020
        },
        {
            "authors": [
                "Ji Hou",
                "Saining Xie",
                "Benjamin Graham",
                "Angela Dai",
                "Matthias Nie\u00dfner"
            ],
            "title": "Pri3d: Can 3d priors help 2d representation learning",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Siyuan Huang",
                "Yichen Xie",
                "Song-Chun Zhu",
                "Yixin Zhu"
            ],
            "title": "Spatio-temporal self-supervised representation learning for 3d point clouds",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Andrej Janda",
                "Brandon Wagstaff",
                "Edwin G Ng",
                "Jonathan Kelly"
            ],
            "title": "Contrastive learning for self-supervised pre-training of point cloud segmentation networks with image data",
            "venue": "arXiv preprint arXiv:2301.07283,",
            "year": 2023
        },
        {
            "authors": [
                "Alex H Lang",
                "Sourabh Vora",
                "Holger Caesar",
                "Lubing Zhou",
                "Jiong Yang",
                "Oscar Beijbom"
            ],
            "title": "Pointpillars: Fast encoders for object detection from point clouds",
            "year": 2019
        },
        {
            "authors": [
                "Gustav Larsson",
                "Michael Maire",
                "Gregory Shakhnarovich"
            ],
            "title": "Colorization as a proxy task for visual understanding",
            "year": 2017
        },
        {
            "authors": [
                "Zhengqin Li",
                "Jiansheng Chen"
            ],
            "title": "Superpixel segmentation using linear spectral clustering",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "Hanxue Liang",
                "Chenhan Jiang",
                "Dapeng Feng",
                "Xin Chen",
                "Hang Xu",
                "Xiaodan Liang",
                "Wei Zhang",
                "Zhenguo Li",
                "Luc Van Gool"
            ],
            "title": "Exploring geometry-aware contrast and clustering harmonization for self-supervised 3d object detection",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Zhiwei Lin",
                "Yongtao Wang"
            ],
            "title": "Bev-mae: Bird\u2019s eye view masked autoencoders for outdoor point cloud pre-training",
            "venue": "arXiv preprint arXiv:2212.05758,",
            "year": 2022
        },
        {
            "authors": [
                "Haotian Liu",
                "Mu Cai",
                "Yong Jae Lee"
            ],
            "title": "Masked discrimination for self-supervised learning on point clouds",
            "year": 2022
        },
        {
            "authors": [
                "Ming-Yu Liu",
                "Oncel Tuzel",
                "Srikumar Ramalingam",
                "Rama Chellappa"
            ],
            "title": "Entropy rate superpixel segmentation",
            "venue": "In CVPR,",
            "year": 2011
        },
        {
            "authors": [
                "Yueh-Cheng Liu",
                "Yu-Kai Huang",
                "Hung-Yueh Chiang",
                "Hung-Ting Su",
                "Zhe-Yu Liu",
                "Chin-Tang Chen",
                "ChingYu Tseng",
                "Winston H Hsu"
            ],
            "title": "Learning from 2d: Contrastive pixel-to-point knowledge transfer for 3d pretraining",
            "venue": "arXiv preprint arXiv:2104.04687,",
            "year": 2021
        },
        {
            "authors": [
                "Yunze Liu",
                "Li Yi",
                "Shanghang Zhang",
                "Qingnan Fan",
                "Thomas Funkhouser",
                "Hao Dong"
            ],
            "title": "P4contrast: Contrastive learning with pairs of point-pixel pairs for rgb-d scene understanding",
            "year": 2012
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Sgdr: Stochastic gradient descent with warm restarts",
            "venue": "In ICLR,",
            "year": 2016
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In ICLR,",
            "year": 2017
        },
        {
            "authors": [
                "Zaiwei Lu"
            ],
            "title": "A comprehensive survey on visual question answering debias",
            "venue": "IEEE International Conference on Advances in Electrical Engineering and Computer Applications (AEECA),",
            "year": 2022
        },
        {
            "authors": [
                "Chen Min",
                "Xinli Xu",
                "Dawei Zhao",
                "Liang Xiao",
                "Yiming Nie",
                "Bin Dai"
            ],
            "title": "Occupancy-mae: Self-supervised pretraining large-scale lidar point clouds with masked occupancy autoencoders",
            "venue": "arXiv preprint arXiv:2206.09900,",
            "year": 2022
        },
        {
            "authors": [
                "Bo Pang",
                "Hongchi Xia",
                "Cewu Lu"
            ],
            "title": "Unsupervised 3d point cloud representation learning by triangle constrained contrast for autonomous driving",
            "year": 2023
        },
        {
            "authors": [
                "Yatian Pang",
                "Wenxiao Wang",
                "Francis EH Tay",
                "Wei Liu",
                "Yonghong Tian",
                "Li Yuan"
            ],
            "title": "Masked autoencoders for point cloud self-supervised learning",
            "year": 2022
        },
        {
            "authors": [
                "Deepak Pathak",
                "Philipp Krahenbuhl",
                "Jeff Donahue",
                "Trevor Darrell",
                "Alexei A Efros"
            ],
            "title": "Context encoders: Feature learning by inpainting",
            "year": 2016
        },
        {
            "authors": [
                "Charles R Qi",
                "Hao Su",
                "Kaichun Mo",
                "Leonidas J Guibas"
            ],
            "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Charles R Qi",
                "Or Litany",
                "Kaiming He",
                "Leonidas J Guibas"
            ],
            "title": "Deep hough voting for 3d object detection in point clouds",
            "venue": "In proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Charles Ruizhongtai Qi",
                "Li Yi",
                "Hao Su",
                "Leonidas J Guibas"
            ],
            "title": "Pointnet++: Deep hierarchical feature learning on point sets in a metric space",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In ICML. PMLR,",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text",
            "venue": "transformer. JMLR,",
            "year": 2020
        },
        {
            "authors": [
                "Jiawei Ren",
                "Cunjun Yu",
                "Xiao Ma",
                "Haiyu Zhao",
                "Shuai Yi"
            ],
            "title": "Balanced meta-softmax for long-tailed visual recognition",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Corentin Sautier",
                "Gilles Puy",
                "Spyros Gidaris",
                "Alexandre Boulch",
                "Andrei Bursuc",
                "Renaud Marlet"
            ],
            "title": "Image-tolidar self-supervised distillation for autonomous driving data",
            "year": 2022
        },
        {
            "authors": [
                "Shaoshuai Shi",
                "Xiaogang Wang",
                "Hongsheng Li"
            ],
            "title": "Pointrcnn: 3d object proposal generation and detection from point cloud",
            "year": 2019
        },
        {
            "authors": [
                "Shaoshuai Shi",
                "Chaoxu Guo",
                "Li Jiang",
                "Zhe Wang",
                "Jianping Shi",
                "Xiaogang Wang",
                "Hongsheng Li"
            ],
            "title": "Pv-rcnn: Point-voxel feature set abstraction for 3d object detection",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Shaoshuai Shi",
                "Zhe Wang",
                "Jianping Shi",
                "Xiaogang Wang",
                "Hongsheng Li"
            ],
            "title": "From points to parts: 3d object detection from point cloud with part-aware and part-aggregation",
            "year": 2020
        },
        {
            "authors": [
                "Amanpreet Singh",
                "Ronghang Hu",
                "Vedanuj Goswami",
                "Guillaume Couairon",
                "Wojciech Galuba",
                "Marcus Rohrbach",
                "Douwe Kiela"
            ],
            "title": "Flava: A foundational language and vision alignment model",
            "year": 2022
        },
        {
            "authors": [
                "Pei Sun",
                "Henrik Kretzschmar",
                "Xerxes Dotiwalla",
                "Aurelien Chouard",
                "Vijaysai Patnaik",
                "Paul Tsui",
                "James Guo",
                "Yin Zhou",
                "Yuning Chai",
                "Benjamin Caine"
            ],
            "title": "Scalability in perception for autonomous driving: Waymo open dataset",
            "year": 2020
        },
        {
            "authors": [
                "Yonglong Tian",
                "Chen Sun",
                "Ben Poole",
                "Dilip Krishnan",
                "Cordelia Schmid",
                "Phillip Isola"
            ],
            "title": "What makes for good views for contrastive learning",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Hanchen Wang",
                "Qi Liu",
                "Xiangyu Yue",
                "Joan Lasenby",
                "Matt J Kusner"
            ],
            "title": "Unsupervised point cloud pre-training via occlusion completion",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Zhirong Wu",
                "Shuran Song",
                "Aditya Khosla",
                "Fisher Yu",
                "Linguang Zhang",
                "Xiaoou Tang",
                "Jianxiong Xiao"
            ],
            "title": "3d shapenets: A deep representation for volumetric shapes",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "Saining Xie",
                "Jiatao Gu",
                "Demi Guo",
                "Charles R Qi",
                "Leonidas Guibas",
                "Or Litany"
            ],
            "title": "Pointcontrast: Unsupervised pre-training for 3d point cloud understanding",
            "year": 2020
        },
        {
            "authors": [
                "Zetong Yang",
                "Yanan Sun",
                "Shu Liu",
                "Jiaya Jia"
            ],
            "title": "3dssd: Point-based 3d single stage object detector",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Junbo Yin",
                "Dingfu Zhou",
                "Liangjun Zhang",
                "Jin Fang",
                "Cheng-Zhong Xu",
                "Jianbing Shen",
                "Wenguan Wang"
            ],
            "title": "Proposalcontrast: Unsupervised pre-training for lidar-based 3d object detection",
            "year": 2022
        },
        {
            "authors": [
                "Tianwei Yin",
                "Xingyi Zhou",
                "Philipp Krahenbuhl"
            ],
            "title": "Center-based 3d object detection and tracking",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Xumin Yu",
                "Lulu Tang",
                "Yongming Rao",
                "Tiejun Huang",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "title": "Point-bert: Pre-training 3d point cloud transformers with masked point modeling",
            "year": 2022
        },
        {
            "authors": [
                "Renrui Zhang",
                "Ziyu Guo",
                "Peng Gao",
                "Rongyao Fang",
                "Bin Zhao",
                "Dong Wang",
                "Yu Qiao",
                "Hongsheng Li"
            ],
            "title": "Point-m2ae: Multi-scale masked autoencoders for hierarchical point cloud pre-training",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Yifan Zhang",
                "Qingyong Hu",
                "Guoquan Xu",
                "Yanxin Ma",
                "Jianwei Wan",
                "Yulan Guo"
            ],
            "title": "Not all points are equal: Learning highly efficient point-based detectors for 3d lidar point clouds",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Zaiwei Zhang",
                "Rohit Girdhar",
                "Armand Joulin",
                "Ishan Misra"
            ],
            "title": "Self-supervised pretraining of 3d features on any point-cloud",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Jinghao Zhou",
                "Chen Wei",
                "Huiyu Wang",
                "Wei Shen",
                "Cihang Xie",
                "Alan Yuille",
                "Tao Kong"
            ],
            "title": "ibot: Image bert pre-training with online tokenizer",
            "venue": "arXiv preprint arXiv:2111.07832,",
            "year": 2021
        },
        {
            "authors": [
                "Yin Zhou",
                "Oncel Tuzel"
            ],
            "title": "Voxelnet: End-to-end learning for point cloud based 3d object detection",
            "year": 2018
        },
        {
            "authors": [
                "Xinge Zhu",
                "Yuexin Ma",
                "Tai Wang",
                "Yan Xu",
                "Jianping Shi",
                "Dahua Lin"
            ],
            "title": "Ssn: Shape signature networks for multi-class object detection from point clouds",
            "venue": "In ECCV,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Detecting objects such as vehicles and pedestrians in 3D is crucial for self-driving cars to operate safely. Mainstream 3D object detectors (Shi et al., 2019; 2020b; Zhu et al., 2020; He et al., 2020a) take LiDAR point clouds as input, which provide precise 3D signals of the surrounding environment. However, training a detector needs a lot of labeled data. The expensive process of curating annotated data has motivated the community to investigate model pre-training using unlabeled data that can be collected easily. Most of the existing pre-training methods are built upon contrastive learning (Yin et al., 2022; Xie et al., 2020; Zhang et al., 2021; Huang et al., 2021; Liang et al., 2021), inspired by its success in 2D recognition (Chen et al., 2020a; He et al., 2020b). The key novelties, however, are often limited to how the positive and negative data pairs are constructed. This paper attempts to go beyond contrastive learning by providing a new perspective on pre-training 3D object detectors.\nWe rethink pre-training\u2019s role in how it could facilitate the downstream fine-tuning with labeled data. A labeled example of 3D object detection comprises a point cloud and a set of bounding boxes. While the format is typical, there is no explicit connection between the input data and labels. We argue that a pre-training approach should help bridge these two pieces of information to facilitate fine-tuning.\nHow can we go from point clouds to bounding boxes without human supervision? One intuitive way is to first segment the point cloud, followed by rules to read out the bounding box of each segment. The closer the segments match the objects of interest, the higher their qualities are. While a raw LiDAR point cloud may not provide clear semantic cues to segment objects, the color image collected along with it does. In color images, each object instance often possesses a coherent color and has a sharp contrast to the background. This information has been widely used in image segmentation (Li & Chen, 2015; Liu et al., 2011; Adams & Bischof, 1994) before machine learning-based methods take over. We hypothesize that by learning to predict the pixel color each LiDAR point is projected to, the pre-trained LiDAR-based model backbone will be equipped with the semantic cues that facilitate the subsequent fine-tuning for 3D object detection. To this end, we propose pre-training a LiDARbased 3D object detector by learning to colorize LiDAR point clouds. Such an objective makes the pre-training procedure fairly straightforward, which we see as a key strength. Taking models that use point-based backbones like PointNet++ (Qi et al., 2017b;a) as an example, pre-training becomes solving a point-wise regression problem (for predicting the RGB values).\nThat said, the ground-truth colors of a LiDAR point cloud contain inherent variations. For example, the same car model (hence the same point cloud) can have multiple color options; the same street scene (hence the same point cloud) can be pictured with different colors depending on the time and weather. When selection bias occurs in the collected data (e.g., only a few instances of the variations are collected), the model may learn to capture the bias, degrading its transferability. When the data is well collected to cover all variations without selection bias, the model may learn to predict the average color, downgrading the semantic cues (e.g., contrast). Such a dilemma seems to paint a grim picture of using colors to supervise pre-training. In fact, the second issue is reminiscent of the \u201cnoise\u201d term in the well-known bias-variance decomposition (Hastie et al., 2009), which cannot be reduced by machine learning algorithms. The solution must come from a better way to use the data.\nBased on this insight, we propose a fairly simple but effective refinement to how we leverage color. Instead of using color solely as the supervised signal, we use it as \u201ccontext\u201d to ground the colorization process on the inherent color variation. We realize this idea by providing ground-truth colors to a seed set of LiDAR points as \u201chints\u201d for colorization. (Fig. 1 gives an illustration.) On the one hand, this contextual information reduces the color variations on the remaining points and, in turn, increases the mutual information between the remaining points and the ground-truth colors to supervise pretraining. On the other hand, when selection bias exists, this contextual information directly captures it and offers it as a \u201cshortcut\u201d to the colorization process. This could reduce the tendency of a model to (re-)learn this bias (Lu, 2022; Clark et al., 2019; Cadene et al., 2019), enabling it to focus on the semantic cues useful for 3D object detection. After all, what we hope the model learns is not the exact color of each point but which subset of points should possess the same color and be segmented together. By providing the ground-truth colors to a seed set of points during the colorization process, we concentrate the model backbone on discovering with which points each seed point should share its color, aligning the pre-training objective to its desired role.\nWe implement our idea, which we name Grounded Point Colorization (GPC), by introducing another point-based model (e.g., PointNet++) on top of the detector backbone. This could be seen as the projection head commonly added in contrastive learning (Chen et al., 2020a); Fig. 2 provides an illustration. While the detector backbone takes only the LiDAR point cloud as input, the projection head takes both the output embeddings of the LiDAR points and the ground-truth colors of a seed set of points as input and is in charge of filling in the missing colors grounded on the seed points. This leaves the detector backbone with a more dedicated task \u2014 producing output embeddings with clear semantic cues about which subset of points should be colored (i.e., segmented) together.\nWe conduct extensive experiments to validate GPC, using the KITTI (Geiger et al., 2012) and Waymo (Sun et al., 2020) datasets. We show that GPC can effectively boost the fine-tuning performance, especially on the labeled-data-scarce settings. For example, with GPC, fine-tuning on 20% KITTI is already better than training 100% from scratch. Fine-tuning using the whole dataset still brings a notable 1.2% gain on overall AP, outperforming existing baselines.\nOur contributions are three-fold. First, we propose pre-training by colorizing the LiDAR points \u2014 a novel approach different from the commonly used contrastive learning methods. Second, we identify the intrinsic difficulty of using color as the supervised signal and propose a simple yet well-founded and effective treatment. Third, we conduct extensive empirical studies to validate our approach."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "LiDAR-based 3D object detectors. Existing methods can be roughly categorized into two groups based on their feature extractors. Point-based methods (Shi et al., 2019; Zhang et al., 2022b; Qi et al., 2019; Fan et al., 2022; Yang et al., 2020) extract features for each LiDAR point. Voxel-based\nmethods (Shi et al., 2020a; Yin et al., 2021; Yan et al., 2018; Lang et al., 2019) extract features on grid locations, resulting in a feature tensor. In this paper, we focus on point-based 3D object detectors.\nSSL on point clouds by contrastive learning. Self-supervised learning (SSL) seeks to build feature representations without human annotations. The main branch of work is based on contrastive learning (Chen et al., 2020a; Chen & He, 2021; He et al., 2020b; Chen et al., 2020b; Grill et al., 2020; Henaff, 2020; Tian et al., 2020). For example, Zhang et al. (2021); Huang et al. (2021) treat each point cloud as an instance and learn the representation using contrastive learning. Liang et al. (2021); Xie et al. (2020); Yin et al. (2022) build point features by contrasting them across different regions in different scenes. The challenge is how to construct data pairs to contrast, which often needs extra information or ad hoc design such as spatial-temporal information (Huang et al., 2021) and ground planes (Yin et al., 2022). Our approach goes beyond contrastive learning for SSL on point clouds.\nSSL on point clouds by reconstruction. Inspired by natural languages processing (Devlin et al., 2018; Raffel et al., 2020) and image understanding (Zhou et al., 2021; Assran et al., 2022; Larsson et al., 2017; Pathak et al., 2016), this line of work seeks to build feature representations by filling up missing data components (Wang et al., 2021; Pang et al., 2022; Yu et al., 2022; Liu et al., 2022; Zhang et al., 2022a; Eckart et al., 2021). However, most methods are limited to object-level reconstruction with synthetic data, e.g., ShapeNet55 (Chang et al., 2015) and ModelNet40 (Wu et al., 2015). Only a few recent methods investigate outdoor driving scenes (Min et al., 2022; Lin & Wang, 2022), using the idea of masked autoencoders (MAE) (He et al., 2022). Our approach is related to reconstruction but with a fundamental difference \u2014 we do not reconstruct the input data (i.e., LiDAR data) but the data from an associated modality (i.e., color images). This enables us to bypass the mask design and leverage semantic cues in pixels, leading to features that facilitate 3D detection in outdoor scenes.\nWeak/Self-supervision from multiple modalities. Learning representations from multiple modalities has been explored in different contexts. One prominent example is leveraging large-scale Internet text such as captions to supervise image representations (Radford et al., 2021; Singh et al., 2022; Driess et al., 2023). In this paper, we use images to supervise LiDAR-based representations. Several recent methods also use images but mainly to support contrastive learning (Sautier et al., 2022; Janda et al., 2023; Liu et al., 2021; 2020; Hou et al., 2021; Afham et al., 2022; Pang et al., 2023). Concretely, they form data pairs across modalities and learn two branches of feature extractors (one for each modality) to pull positive data pairs closer and push negative data pairs apart. In contrast, our work takes a reconstruction/generation approach. Our architecture is a single branch without pair construction."
        },
        {
            "heading": "3 PROPOSED APPROACH: GROUNDED POINT COLORIZATION (GPC)",
            "text": "Mainstream 3D object detectors take LiDAR point clouds as input. To localize and categorize objects accurately, the detector must produce an internal feature representation that can properly encode the semantic relationship among points. In this section, we introduce Grounded Point Colorization (GPC), which leverages color images as supervised signals to pre-train the detector\u2019s feature extractor (i.e., backbone), enabling it to output semantically meaningful features without human supervision."
        },
        {
            "heading": "3.1 PRE-TRAINING VIA LEARNING TO COLORIZE",
            "text": "Color images can be easily obtained in autonomous driving scenarios since most sensor suites involve color cameras. With well-calibrated and synchronized cameras and LiDAR sensors, we can get the color of a LiDAR point by projecting it to the 2D image coordinate. As colors tend to be similar within an object instance but sharp across object boundaries, we postulate that by learning to colorize the point cloud, the resulting backbone would be equipped with the knowledge of objects.\nThe first attempt. Let us denote a point cloud by X = [x1, \u00b7 \u00b7 \u00b7 ,xN ] \u2208 R3\u00d7N ; the corresponding color information by Y = [y1, \u00b7 \u00b7 \u00b7 ,yN ] \u2208 R3\u00d7N . The three dimensions in point xn encode the 3D location in the ego-car coordinate system; the three dimensions in yn encode the corresponding RGB pixel values. With this information, we can frame the pre-training of a detector backbone as a supervised colorization problem, treating each pair of (X,Y ) as a labeled example.\nTaking a point-based backbone f\u03b8 like PointNet++ (Qi et al., 2017b) as an example, the model takes X as input and outputs a feature matrix Z\u0302 \u2208 RD\u00d7N , in which each column vector z\u0302n \u2208 RD is meant to capture semantic cues for the corresponding point xn to facilitate 3D object detection. One way to pre-train f\u03b8 using (X,Y ) is to add another point-based model g\u03d5 (seen as the projection head or color decoder) on top of f\u03b8, which takes Z\u0302 as input and output Y\u0302 of the same shape as Y . In this\nway, Y\u0302 can be compared with Y to derive the loss, for example, the Frobenius norm. Putting things together, we come up with the following optimization problem,\nmin \u03b8,\u03d5 \u2211 (X,Y )\u2208Dtrain \u2225g\u03d5 \u25e6 f\u03b8(X)\u2212 Y \u22252F = min \u03b8,\u03d5 \u2211 (X,Y )\u2208Dtrain \u2211 n \u2225y\u0302n \u2212 yn\u222522, (1)\nwhere Dtrain means the pre-training dataset and y\u0302n is the n-th column of Y\u0302 = g\u03d5 \u25e6 f\u03b8(X). The inherent color variation. At first glance, Eq. 1 should be fairly straightforward to optimize using standard optimizers like stochastic gradient descent (SGD). In our experiments, we however found a drastically slow drop in training error, which results from the inherent color variation. That is, given X , there is a huge variation of Y in the collected data due to environmental factors (e.g., when the image is taken) or color options (e.g., instances of the same object can have different colors). This implies a huge entropy in the conditional distribution P (Y |X), i.e., a large H(Y |X) = E[\u2212 log(Y |X)], indicating a small mutual information between X and Y , i.e., a small I(X;Y ) = H(Y )\u2212H(Y |X). In other words, Eq. 1 may not provide sufficient information for f\u03b8 to learn. Another way to look at this issue is the well-known bias-variance decomposition (Hastie et al., 2009; Bishop & Nasrabadi, 2006) in regression problems (suppose Y \u2208 R), which contains a \u201cnoise\u201d term,\nNoise: EY ,X [ (Y \u2212 EY |X [Y ])2 ] . (2)\nWhen P (Y |X) has a high entropy, the noise term will be large, which explains the poor training error. Importantly, this noise term cannot be reduced solely by adopting a better model architecture. It can only be resolved by changing the data, or more specifically, how we leverage (X,Y ).\nOne intuitive way to reduce H(Y |X) is to manipulate P (Y |X), e.g., to carefully sub-sample data to reduce the variation. However, this would adversely reduce the amount of training data and also lead to a side effect of selection bias (Cawley & Talbot, 2010). In the extreme case, the pre-trained model may simply memorize the color of each distinct point cloud via the color decoder g\u03d5, preventing the backbone f\u03b8 from learning a useful embedding space for the downstream task.\nGrounded colorization with hints. To resolve these issues, we propose to reduce the variance in P (Y |X) by providing additional conditioning evidence S. From the perspective of information theory (Cover, 1999), adding evidence into P (Y |X), i.e., P (Y |X,S), guarantees,\nH(Y |X) \u2265 H(Y |X,S), and thus I((X,S);Y ) \u2265 I(X;Y ), (3) unveiling more information for f\u03b8 to learn.\nWe realize this idea by proving ground-truth colors to a seed set of points in X . Without loss of generality, we decompose X into two subsets XS and XU such that X = [XS ,XU ]. Likewise, we decompose Y into YS and YU such that Y = [YS ,YU ]. We then propose to inject YS as the conditioning evidence S into the colorization process. In other words, we directly provide the ground-truth colors to some of the points the model initially has to predict.\nAt first glance, this may controversially simplify the pre-training objective in Eq. 1. However, as the payback, we argue that this hint would help ground the colorization process on the inherent color variation, enabling the model to learn useful semantic cues for downstream 3D object detection. Concretely, to colorize a point cloud, the model needs to a) identify the segments that should possess similar colors and b) decide what colors to put on. With the hint that directly colors some of the points, the model can concentrate more on the first task, which aligns with the downstream task \u2014 to identify which subset of points to segment together.\nSketch of implementation. We implement our idea by injecting YS as additional input to the color decoder g\u03d5. We achieve this by concatenating YS row-wise with the corresponding columns in Z\u0302, resulting in a (D + 3)-dimensional input to g\u03d5. Let OU denote an all-zero matrix with the same shape as YU , we modify Eq. 1 as,\nmin \u03b8,\u03d5 \u2211 (X,Y )\u2208Dtrain \u2225\u2225\u2225g\u03d5 ([Z\u0302; [YS ,OU ]])\u2212 [YS ,YU ]\u2225\u2225\u22252 F , s.t. Z\u0302 = f\u03b8 ([XS ,XU ]) , (4)\nwhere \u201c;\u201d denotes row-wise concatenation. Importantly, Eq. 4 only provides the hint YS to the color decoder g\u03d5, leaving the input and output formats of the backbone model f\u03b8 intact. We name our approach Grounded Point Colorization (GPC).\nGPC could mitigate selection bias. Please see Appendix S1 for a discussion."
        },
        {
            "heading": "3.2 DETAILS OF GROUNDED POINT COLORIZATION (GPC)",
            "text": "Model architecture. For the backbone f\u03b8, we use the model architectures defined in existing 3D object detectors. We specifically focus on detectors that use point-based feature extractors, such as PointRCNN (Shi et al., 2019) and IA-SSD (Zhang et al., 2022b). The output of f\u03b8 is a set of point-wise features denoted by Z\u0302. For the color decoder g\u03d5 that predicts the color of each input point, without loss of generality, we use a PointNet++ (Qi et al., 2017b).\nColor regression vs. color classification. In Sec. 3.1, we frame colorization as a regression problem. However, regressing the RGB pixel values is sensitive to potential outlier pixels. In our implementation, we instead consider a classification problem by quantizing real RGB values into discrete bins. We apply the K-means algorithm over pixels of the pre-training images to cluster them into K classes. The label of each LiDAR point xn then becomes yn \u2208 {1, \u00b7 \u00b7 \u00b7 ,K}. Fig. 3 shows the images with color quantization; each pixel is colored by the assigned cluster center. It is hard to distinguish the images with K = 128 bins from the original image by human eyes.\nWe accordingly modify the color decoder g\u03d5. In the input, we concatenate a K-dimensional vector to the feature z\u0302n \u2208 RD of each LiDAR point xn, resulting in a (D + K)-dimension input to g\u03d5. For seed points where we inject ground-truth colors, the K-dimensional vector is one-hot, whose yn-th element is one and zero elsewhere. For other LiDAR points, the K-dimensional vector is a zero vector. In the output, g\u03d5 generates a K-dimensional logit vector \u03b7\u0302n \u2208 RK for each input point xn. The predicted color class y\u0302n can be obtained by argmaxc\u2208{1,\u00b7\u00b7\u00b7 ,K} \u03b7\u0302n[c].\nPre-training objective. The cross-entropy loss is the standard loss for classification. However, in driving scenes, most of the LiDAR points are on the ground, leading to an imbalanced distribution across color classes. We thus apply the Balanced Softmax (Ren et al., 2020) to re-weight each class,\n\u2113(xn, yn) = \u2212 log ( \u03b1yn \u00d7 e\u03b7\u0302n[yn]\u2211K c=1 \u03b1c \u00d7 e\u03b7\u0302n[c] ) , (5)\nwhere \u03b1c is the balance factor of class c. In the original paper (Ren et al., 2020), \u03b1c is proportional to the number of training examples of class c. Here, we set \u03b1c per mini-batch, proportional to the number of LiDAR points with class label c. A small \u03f5 is added to \u03b1c for stability since an image may not contain all colors at once.\nOptimization. In the pre-training stage, we train f\u03b8 and g\u03d5 end-to-end without human supervision. In the fine-tuning stage with labeled 3D object detection data, g\u03d5 is disregarded; the pre-trained f\u03b8 is fine-tuned end-to-end with other components in the 3D object detector.\nAll pieces together. We propose a novel self-supervised learning algorithm GPC, which leverages colors to pre-train LiDAR-based 3D detectors. The model includes a backbone f\u03b8 and a color decoder\ng\u03d5; f\u03b8 learns to generate embeddings to segment LiDAR points while g\u03d5 learns to colorize LiDAR points. To mitigate the inherent color variation, GPC injects hints into g\u03d5. To overcome the potential outlier pixels, GPC quantizes colors into class labels and treats colorization as a classification problem. Fig. 1 shows our main insight and Fig. 2 shows the architecture.\nExtension to voxel-based features. As mentioned in Sec. 2, another branch of LiDAR-based detectors encode LiDAR points into a voxel-based, bird\u2019s-eye view (BEV) feature representation. GPC can still be beneficial to these LiDAR-based detectors by offering better input data \u2014 augmenting the LiDAR point coordinates with the output features by the pre-trained point-based backbone f\u03b8. We investigate this idea in Sec. 4.2 and demonstrate promising results."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 SETUP",
            "text": "Datasets. We use two commonly used datasets in autonomous driving: KITTI (Geiger et al., 2012) and Waymo Open Dataset (Sun et al., 2020). KITTI contains 7, 481 annotated samples, with 3, 712 for training and 3, 769 for validation. Waymo dataset is much larger, with 158, 081 frames in 798 scenes for training and 40, 077 frames in 202 scenes for validation. We pre-train on each dataset with all training examples without using any labels in the pre-training. We then fine-tune on different amounts of KITTI data, 5%, 10%, 20%, 50%, and 100%, with 3D annotations to investigate dataefficient learning. We adopt standard evaluation on KITTI validation set. Experiments of KITTI \u2192 KITTI show the in-domain performance. Waymo \u2192 KITTI provides the scalability and transfer ability of the algorithm, which is the general interest in SSL.\n3D Detectors. As mentioned in Sec. 3, we focus on learning effective point representation with PointRCNN and IA-SSD. We see consistent improvement on both (Sec. 4.2). Additionally, we extend GPC to Voxel-based detectors as discussed in Sec. 4.2.\nImplementation. We sample images from the training set (3k and 10k for KITTI and Waymo respectively) and 1k pixels for each image to learn the quantization. KMeans algorithm is adopted to cluster the colors into 128 discrete labels which are used for seeds and ground truths. The color labels in one hot concatenated with the features from the backbone are masked out 80% of colors (i.e., assigned to zeros). During the pre-training, we colorjitter1 the image with a probability of 0.5. For 3D point augmentation, we follow the standard procedure, random flipping along the X-axis, random rotation ([\u22124/\u03c0, 4/\u03c0]), random scaling ([0.95, 1.05]), random point sampling (N = 16384 with range= 40), and random point shuffling. We pre-train the model for 80 and 12 epochs on KITTI and Waymo respectively with batch size 16 and 32. We fine-tune on subsets of KITTI for 80 epochs with a batch size of 16 and the same 3D point augmentation above. The AdamW optimizer (Loshchilov & Hutter, 2017) and Cosine learning rate scheduler(Loshchilov & Hutter, 2016) are adopted in both stages which the maximum learning rate is 0.001 and 0.0004 respectively."
        },
        {
            "heading": "4.2 RESULTS",
            "text": "Results on KITTI \u2192 KITTI. We first study in-domain data-efficient learning with different amounts of labeled data available. Our results outperform random initialization (scratch) by a large margin, particularly when dealing with limited human-annotated data, as shown in Tab. 1. For instance, with PointRCNN, on 5% KITTI, we improve +7.5% on overall moderate AP (55.2 to 62.7). Remarkably, even with just 20% KITTI, we already surpass the performance achieved by training on the entire dataset from scratch, reaching a performance of 70.7. This highlights the essential benefit of GPC in data-efficient learning, especially when acquiring 3D annotations is expensive. Moreover, even when fine-tuning on the entire 100% of the data, we still achieve a notable gain, improving from 69.7 to 70.9 (+1.2).\nTo provide a robust comparison, we re-implement a recent method, PointContrast (Xie et al., 2020), as a stronger baseline. GPC performs better across all subsets of data. Notice that PointContrast struggles when annotations are extremely scarce, as seen on 5% subset where it underperforms the baseline (47.9 vs.55.2).\nResults on Waymo \u2192 KITTI. We investigate the transfer learning ability on GPC as it\u2019s the general interest in SSL. We pre-train on Waymo (20\u00d7 larger than KITTI) and fine-tune on subsets of KITTI. In Tab. 2, our method shows consistent improvement upon scratch baseline. The overall moderate\n1We apply PyTorch function with brightness, contrast, and saturation set to 0.2.\nAP improves 55.2 \u2192 63.9 on 5% KITTI. On 20%, it improves 63.5 \u2192 70.1 which is again already better than 100% from scratch (69.5). We compare to the current SOTA, ProposalContrast (Yin et al., 2022), and GPC outperforms on 20% (66.2 vs.70.1) and 50% (69.2 vs.70.7).\nLearning with Images. We\u2019ve noticed recent efforts in self-supervised learning using images (see Sec. 2), where most approaches use separate feature extractors for LiDAR and images to establish correspondences for contrastive learning. In contrast, our method, GPC, directly utilizes colors within a single trainable architecture. We strive to incorporate numerous baselines for comparison, but achieving a fair comparison is challenging due to differences in data processing, model architecture, and training details. Therefore, we consider the baselines as references that demonstrate our approach\u2019s potential, rather than claiming state-of-the-art status.\nIn Tab. 3, we compare to PPKT (Liu et al., 2021), SLidR (Sautier et al., 2022) and TriCC (Pang et al., 2023) with moderate AP reported in Pang et al. (2023). Notably, on 5% KITTI, we outperform the baselines with substantial gains of +7.5 and +8.7, which are significantly higher than their improvements (less than +4.7).\nResults on Waymo \u2192 Waymo. We evaluate on 1% Waymo with its standard metric AP and APH. The result in Tab. 4 shows that GPC improve +2.4% on AP and +4.4% on APH.\nExtension to Voxel-Based Detector. We further extend the applicability of GPC to SECOND (Yan et al., 2018), a widely-used voxel-based backbone, by concatenating our pre-trained per-point feature with the raw input (i.e., the LiDAR point coordinate) to SECOND. The results in Tab. 5 show that GPC consistently improve upon the scratch baseline, suggesting the effectiveness of our learned features. We hypothesize GPC can also be a good plug-and-play for algorithms pooling with point features in the later stage (Shi et al., 2020a). Please see more details in the Appendix S5."
        },
        {
            "heading": "4.3 ABLATION STUDY",
            "text": "Colorization with regression or classification? In Tab. 6, we explore four losses for colorization. There is nearly no performance difference in using MSE, SL1, and CE, but BS outperforms other losses by a clear margin. We hypothesize that it prevents the learning biased toward the ground (major pixels) and force the model to learn better on foreground objects that have much fewer pixels which facilitates the downstream 3D detection as well.\nHint mechanism. It is the key design to overcome the inherent color variation (see Sec. 3). In Tab. 8, we directly regress RGB values (row 2 and 3) without hints and color decoders. The performance is still better than scratch but limited. We hypothesize that the naive regression is sensitive to pixel\nTable 7: Seed ratios.\nseed easy mod. hard\n100% 51.7 38.7 33.8 80% 66.4 50.5 44.9 60% 74.8 58.8 52.2 40% 75.1 59.7 53.4 20% 77.1 61.9 56.2 0% 57.9 44.1 38.4\nscratch 69.9 55.2 49.3\nTable 8: Hint mechanism.\nHint Pre. Dataset easy mod. hard\nscratch - 69.9 55.2 49.3 KITTI 75.1 59.4 53.3 Waymo 74.9 60.4 54.5\n\u2713 KITTI 77.7 62.7 56.5 \u2713 Waymo 78.9 63.9 58.5\nTable 9: Number of epochs pertaining on Waymo.\nepochs easy mod. hard\n1 75.8 60.7 54.4 4 76.4 61.6 55.9 8 78.3 63.1 58.4 12 78.9 63.9 58.5\nvalues and can\u2019t learn well if there exists a large color variation. On the other hand, the hint resolves the above issue and performs much better (row 4, 62.7 and row 5, 63.9). In Tab. 7, we ablate different levels of hint and find out providing only 20% as seeds is the best (61.9), suggesting the task to learn can\u2019t be too easy (i.e., providing too many hints). On the other hand, providing no hints falls into the issue of color variation again, deviating from learning a good representation (0%, 44.1 worse than scratch, 55.2).\nRepeated experiments. To address potential performance fluctuations with different data sampled for fine-tuning, we repeat the 5% fine-tuning experiment for another five rounds, each with a distinctly sampled subset, as shown in Appendix S3. The results show the consistency and promise of GPC."
        },
        {
            "heading": "4.4 QUALITATIVE RESULTS",
            "text": "We validate GPC by visualization as shown in Fig. 4 (left). Impressively the model can learn common sense about colors in the driving scene in (b), even red tail lights on cars. We further play with manual seeds to manipulate the object colors (Fig. 4 (right)) and it more or less recognizes which points should have the same color and stops passing to the ground."
        },
        {
            "heading": "4.5 DISCUSSION",
            "text": "We reiterate that we explore a new perspective to pre-train LiDAR-based detectors (see Sec. 2). GPC outperforms or performs on par with the mainstream contrastive learning-based methods, such as the state-of-the-art method ProposalContrast (Yin et al., 2022), suggesting our new perspective as a promising direction to explore further."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "Color images collected along with LiDAR data offer valuable semantic cues about objects. We leverage this insight to frame pre-training LiDAR-based 3D object detectors as a supervised colorization problem, a novel perspective drastically different from the mainstream contrastive learning-based approaches. Our proposed approach, Grounded Point Colorization (GPC), effectively addresses the inherent variation in the ground-truth colors, enabling the model backbone to learn to identify LiDAR points belonging to the same objects. As a result, GPC could notably facilitate downstream 3D object detection, especially under label-scarce settings, as evidenced by extensive experiments on benchmark datasets. We hope our work can vitalize the community with a new perspective."
        },
        {
            "heading": "S1 GROUNDED POINT COLORIZATION COULD MITIGATE SELECTION BIAS",
            "text": "Another advantage of injecting hints into the colorization process is its ability to overcome selection bias Cawley & Talbot (2010) \u2014 the bias that only a few variations of Y conditioned on X are collected in the dataset. According to Clark et al. (2019) and Cadene et al. (2019), one effective approach to prevent the model from learning the bias is to proactively augment the bias to the \u201coutput\u201d of the model during the training phase. The augmentation, by default, will reduce the loss on the training example that contains bias, allowing the model to focus on hard examples in the training set. We argue that our model design and objective function Eq. 4 share similar concepts with Clark et al. (2019) and Cadene et al. (2019).\nAnother way to mitigate bias in Y is to perform data augmentation, e.g., by color jittering. We note that this is not applicable through Eq. 1 as it will adversely increase the entropy H(Y |X), but is applicable through Eq. 4 via grounded colorization."
        },
        {
            "heading": "S2 EFFECTIVENESS OF GPC",
            "text": "In the main paper, we show the superior performance of our proposed pre-training method, Grounded Point Colorization (GPC) (Sec. 4.2). With our pre-trained backbone, it outperforms random initialization as well as existing methods on fine-tuning with different amounts of labeled data (i.e.5%, 20%, 50%, and 100%) of KITTI (Geiger et al., 2012). Significantly, fine-tuning on only 20% with GPC is already better than training 100% from scratch. Besides, GPC demonstrates the ability of transfer learning by pre-training on Waymo (Sun et al., 2020) in Tab. 2. Not to mention, GPC is fairly simpler than existing methods which require road planes for better region sampling, temporal information for pair construction to contrast, or additional feature extractors for different modalities (Sec. 2). Most importantly, We successfully explore a new perspective of the pre-training methods on LiDAR-based 3D detectors besides contrastive learning-based algorithms.\nIn this supplementary, we further provide the training curve besides the above evidence. Fig. S5 shows the evaluation on KITTI validation set and loss on the training set during the fine-tuning on 5% KITTI. With GPC, the overall moderate AP raises faster than the scratch baseline and keeps the gain till the end. It already performs better at 55th epoch than the final performance of scratch baseline, which is more than 30% better in terms of efficiency. The training loss with GPC is also below the curve of the scratch baseline.\nWith the above evidence and the extensive experiments in the main paper, we explain that GPC is a simple yet effective self-supervised learning algorithm for pre-training LiDAR-based 3D object detectors without human-labeled data."
        },
        {
            "heading": "S3 REPEATED EXPERIMENTS",
            "text": "To address potential performance fluctuations with different data sampled for fine-tuning, we repeat the 5% fine-tuning experiment for another five rounds, each with a distinctly sampled subset. The\n20 40 60 80 epochs\n0\n20\n40\n60\nov er\nal l m\nod er\nat e\nAP\nKITTI Waymo scratch\n0 20 40 60 80 epochs\n1\n2\n3\n4\n5\n6\n7\nlos s\nKITTI Waymo scratch\nFigure S5: Training curve on 5% KITTI. (left) is the performance (\u2191) on KITTI validation set and (right) is the loss (\u2193) on the training set during the fine-tuning. It shows the effectiveness and efficiency of GPC compared to training from scratch.\nresults are summarized in Tab. S10, in which we compare the baseline of training from scratch (with 5% KITTI data), pre-training with full KITTI (cf. Tab. 9 in the main paper), and pre-training with Waymo (cf. Tab. 2 in the main paper). Pre-training with our GPC consistently outperforms the baseline. Notably, the standard deviation among the five rounds is largely reduced with pre-training. For instance, the standard deviation of the overall moderate AP is 2.7 with the scratch baseline, which is reduced to 0.6 with GPC. This significant reduction in standard deviation underscores the consistency and promise of our proposed method.\nTable S10: Repeat experiments. We re-sample 5 different subsets of 5% KITTI and repeat the experiments. The standard deviation of overall moderate AP on the scratch baseline is 2.7, but it\u2019s only 0.6 on GPC. It implies that our improvement is promising and constant, regardless of the performance on the baseline.\nExp. Pre. Car Pedestrian Cyclist Overall\neasy mod. hard easy mod. hard easy mod. hard easy mod. hard\nscratch 51.3 41.9 36.2 56.9 51.4 45.0 74.5 53.7 50.0 60.9 49.0 43.7 KITTI 79.2 65.5 59.1 62.7 55.5 48.4 87.9 65.0 61.0 76.6 62.0 56.11 Waymo 83.8 68.4 63.1 65.6 58.7 51.4 87.1 65.9 61.4 78.8 64.3 58.7 scratch 44.0 36.5 31.6 54.6 48.8 43.0 70.9 48.7 45.5 56.5 44.7 40.1 KITTI 81.1 65.0 58.8 60.3 53.7 47.4 86.8 63.4 59.4 76.1 60.7 55.22 Waymo 82.2 67.6 62.4 64.2 57.3 50.9 87.0 65.3 61.5 77.8 63.4 58.3 scratch 63.0 49.1 42.1 57.6 51.3 44.8 80.6 58.0 53.8 67.1 52.8 46.9 KITTI 78.3 64.0 56.9 61.4 54.4 47.3 86.5 62.6 58.5 75.4 60.3 54.23 Waymo 80.2 64.5 59.5 65.9 57.6 50.7 86.4 66.1 61.8 77.5 62.7 57.3 scratch 59.5 46.3 39.2 52.9 47.9 42.0 72.4 51.1 47.7 61.6 48.4 43.0 KITTI 82.5 67.4 60.1 64.9 57.0 49.7 87.6 65.4 61.1 78.3 63.3 57.04 Waymo 85.1 68.4 63.1 63.5 56.7 50.0 87.4 66.3 62.1 78.7 63.8 58.4 scratch 52.6 42.8 36.8 51.8 47.3 42.0 71.2 50.0 46.6 58.5 46.7 41.8 KITTI 81.1 65.2 58.9 60.7 53.8 46.9 89.8 65.7 61.7 77.2 61.6 55.95 Waymo 84.5 68.4 64.9 65.0 57.3 51.3 87.0 66.4 62.5 78.8 64.0 59.6"
        },
        {
            "heading": "S4 CORRECT USAGE OF COLOR",
            "text": "GPC uses color to provide semantic cues to pre-train LiDAR-based 3D detectors (c.f. Sec. 3 in the main paper). With extensive experiments (c.f. Sec. 4.2 and Sec. 4.3), we show that the hint mechanism is the key to learning good point representation. Naive regression of color without hints can hurt performance. In this supplementary, we further explore a possibility: directly concatenating the input coordinate (i.e.xyz) with pixel values (i.e.RGB) to form a 6-dimension input vector. As\nshown in Tab. S11, the performance is even worse than plain scratch. It implies that the additional color information on the input isn\u2019t necessary to obtain the richer representation. On the other hand, using it with GPC in the pre-training stage shows significant improvement. We argue that how to leverage color information correctly is not trivial. Also, notice that color is no longer needed during fine-tuning.\nTable S11: Comparison of usages of color. Directly concatenating the color (i.e.RGB) and the coordinate (i.e.xyz) performs worse than plan scratch on all subsets. In contrast, GPC utilizing the color as semantic cues in the pre-training brings significant improvement.\nSub. Method Car Pedestrian Cyclist Overall\neasy mod. hard easy mod. hard easy mod. hard easy mod. hard\nscratch 66.8 51.6 45.0 61.5 55.4 48.3 81.4 58.6 54.5 69.9 55.2 49.3 w/ color 57.5 46.3 38.9 50.8 46.6 41.1 72.9 54.1 50.5 60.4 49.0 43.55%\nGPC 81.2 65.1 57.7 63.7 57.4 50.3 88.2 65.7 61.4 77.7 62.7 56.7 scratch 87.6 74.2 71.1 68.2 60.3 52.9 93.0 71.8 67.2 82.9 68.8 63.7\nw/ color 87.4 73.6 70.2 60.4 54.8 48.6 90.0 69.9 65.3 79.2 66.1 61.420% GPC 88.7 76.0 72.0 70.6 63.0 55.8 92.6 73.0 68.1 84.0 70.7 65.3 scratch 89.7 77.4 74.9 68.9 61.3 54.2 88.4 69.2 64.4 82.3 69.3 64.5 w/ color 88.8 77.3 74.7 65.5 59.3 53.2 92.6 71.0 67.4 82.3 69.2 65.150% GPC 89.3 77.5 75.0 68.4 61.4 54.7 92.1 72.1 67.6 83.3 70.3 65.8 scratch 90.8 79.9 77.7 68.1 61.1 54.2 87.3 68.1 63.7 82.0 69.7 65.2\nw/ color 90.8 80.0 77.5 63.2 57.0 51.4 89.5 69.7 65.1 81.2 68.9 64.7100% GPC 90.8 79.6 77.4 68.5 61.3 54.7 92.3 71.8 67.1 83.9 70.9 66.4"
        },
        {
            "heading": "S5 EXTENSION TO VOXEL-BASED BACKBONE",
            "text": "In this work, we mainly focus on pre-training PointNet++ (Qi et al., 2017b) while there exist other types of backbones such as PointPillars (Lang et al., 2019) or VoxelNet (Zhou & Tuzel, 2018; Yan et al., 2018). While it\u2019s not our primary focus, we hypothesize that GPC can be modified to facilitate those architectures. For example, it can still learn to predict the color for each point within the unit (i.e.pillar or voxel) for better point-wise representation or the average color of each unit for better unit-wise representation.\nIn this supplementary, we explore the first idea mentioned above on SECOND (Yan et al., 2018). More specifically, instead of just using the coordinate of points in each voxel, we concatenate xyz with the feature from the PointNet++ backbone pre-trained by GPC (mathematically X \u2032 = [ X | f\u03b8(X) ] \u2208 R(3+D)\u00d7N ). SECOND is randomly initialized and no projection layers are added. PointNet++ backbone is frozen during the fine-tuning. All the other training details are standard without hard tuning, including data augmentation, training scheduler, optimizer, etc.\nThe result is shown in Tab. S12. Impressively, even with this approach, GPC still brings significant improvement upon the scratch baseline. On 5% KITTI, it improves from 47.7 to 52.6 on overall moderate AP, which is a gain of +4.9. On 50% KITTI, the performance of pre-training on Waymo (65.7) is already better than training with 100% from scratch (65.0). Even on 100% KITTI, GPC still brings more than +1.5 gain upon scratch baseline. It demonstrates the effectiveness of our learned point representation and opens the great potential of our proposed method.\nFurthermore, some recent works (Shi et al., 2020a) show that using point-wise features for pooling in the latter stage can improve the performance of 3D detection. We hypothesize GPC can be plug-and-play for those algorithms.\nTable S12: Approach with GPC on SECOND (Yan et al., 2018). The result shows that the point representation by GPC can be a good plug-and-play in addition to the point coordinate (i.e.xyz) for better 3D detection.\nSub. Method Pre. on Car Pedestrian Cyclist Overall\neasy mod. hard easy mod. hard easy mod. hard easy mod. hard\nscratch - 64.9 52.7 45.3 44.3 39.9 35.0 71.0 50.4 47.1 60.1 47.7 42.5 GPC (Ours) KITTI 74.9 60.8 54.1 46.8 41.9 37.3 74.2 55.0 51.5 65.3 52.6 47.65% GPC (Ours) Waymo 69.5 57.1 50.5 50.1 43.4 38.7 74.2 53.8 49.9 64.6 51.4 46.4 scratch - 83.9 71.7 67.4 54.7 48.2 42.8 74.3 58.0 54.6 71.0 59.3 54.9 GPC (Ours) KITTI 85.6 72.0 67.4 54.2 47.6 42.4 78.2 60.9 57.1 72.7 60.2 55.620% GPC (Ours) Waymo 85.1 72.3 67.8 56.6 48.7 43.3 75.4 59.6 55.8 72.4 60.2 55.6 scratch - 88.1 76.2 73.2 59.7 52.4 46.0 83.9 63.0 59.4 77.2 63.8 59.5 GPC (Ours) KITTI 87.5 75.8 73.3 60.3 52.5 46.0 78.3 61.1 57.5 75.4 63.1 58.950% GPC (Ours) Waymo 88.0 76.3 73.5 60.7 53.3 47.0 85.5 67.5 63.4 78.1 65.7 61.3 scratch - 88.1 78.3 73.7 62.0 54.4 47.6 81.1 62.4 58.7 77.1 65.0 60.0 GPC (Ours) KITTI 88.7 79.2 74.5 63.4 56.0 48.8 84.8 64.5 60.6 79.0 66.5 61.3100% GPC (Ours) Waymo 90.9 79.1 76.2 65.2 56.9 50.0 82.5 64.5 60.6 79.5 66.8 62.3"
        }
    ],
    "year": 2023
}