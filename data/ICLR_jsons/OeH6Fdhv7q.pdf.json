{
    "abstractText": "Previous motion generation methods are limited to the pre-rigged 3D human model, hindering their applications in the animation of various non-rigged characters. In this work, we present TapMo, a Text-driven Animation Pipeline for synthesizing Motion in a broad spectrum of skeleton-free 3D characters. The pivotal innovation in TapMo is its use of shape deformation-aware features as a condition to guide the diffusion model, thereby enabling the generation of meshspecific motions for various characters. Specifically, TapMo comprises two main components Mesh Handle Predictor and Shape-aware Diffusion Module. Mesh Handle Predictor predicts the skinning weights and clusters mesh vertices into adaptive handles for deformation control, which eliminates the need for traditional skeletal rigging. Shape-aware Motion Diffusion synthesizes motion with mesh-specific adaptations. This module employs text-guided motions and mesh features extracted during the first stage, preserving the geometric integrity of the animations by accounting for the character\u2019s shape and deformation. Trained in a weakly-supervised manner, TapMo can accommodate a multitude of nonhuman meshes, both with and without associated text motions. We demonstrate the effectiveness and generalizability of TapMo through rigorous qualitative and quantitative experiments. Our results reveal that TapMo consistently outperforms existing auto-animation methods, delivering superior-quality animations for both seen or unseen heterogeneous 3D characters. The project page: https://semanticdh.github.io/TapMo.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiaxu Zhang"
        },
        {
            "affiliations": [],
            "name": "Shaoli Huang"
        },
        {
            "affiliations": [],
            "name": "Zhigang Tu"
        },
        {
            "affiliations": [],
            "name": "Xin Chen"
        },
        {
            "affiliations": [],
            "name": "Xiaohang Zhan"
        },
        {
            "affiliations": [],
            "name": "Gang Yu"
        },
        {
            "affiliations": [],
            "name": "Ying Shan"
        }
    ],
    "id": "SP:cf583aca3cd307f52cb3eabc77055615f769300d",
    "references": [
        {
            "authors": [
                "Ilya Baran",
                "Jovan Popovi\u0107"
            ],
            "title": "Automatic rigging and animation of 3d characters",
            "venue": "ACM Transactions on graphics (TOG),",
            "year": 2007
        },
        {
            "authors": [
                "Mirela Ben-Chen",
                "Ofir Weber",
                "Craig Gotsman"
            ],
            "title": "Spatial deformation transfer",
            "venue": "In Proceedings of the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer Animation,",
            "year": 2009
        },
        {
            "authors": [
                "Paul J Besl",
                "Neil D McKay"
            ],
            "title": "Method for registration of 3-d shapes. In Sensor fusion IV: control paradigms and data structures, volume 1611",
            "venue": "Spie,",
            "year": 1992
        },
        {
            "authors": [
                "Uttaran Bhattacharya",
                "Nicholas Rewkowski",
                "Abhishek Banerjee",
                "Pooja Guhan",
                "Aniket Bera",
                "Dinesh Manocha"
            ],
            "title": "Text2gestures: A transformer-based network for generating emotive body gestures for virtual agents",
            "year": 2021
        },
        {
            "authors": [
                "Xin Chen",
                "Biao Jiang",
                "Wen Liu",
                "Zilong Huang",
                "Bin Fu",
                "Tao Chen",
                "Jingyi Yu",
                "Gang Yu"
            ],
            "title": "Executing your commands via motion diffusion in latent space",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Rishabh Dabral",
                "Muhammad Hamza Mughal",
                "Vladislav Golyanik",
                "Christian Theobalt"
            ],
            "title": "Mofusion: A framework for denoising-diffusion-based motion synthesis",
            "venue": "arXiv preprint arXiv:2212.04495,",
            "year": 2022
        },
        {
            "authors": [
                "Anindita Ghosh",
                "Noshaba Cheema",
                "Cennet Oguz",
                "Christian Theobalt",
                "Philipp Slusallek"
            ],
            "title": "Synthesis of compositional animations from textual descriptions",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "Communications of the ACM,",
            "year": 2020
        },
        {
            "authors": [
                "Chuan Guo",
                "Xinxin Zuo",
                "Sen Wang",
                "Shihao Zou",
                "Qingyao Sun",
                "Annan Deng",
                "Minglun Gong",
                "Li Cheng"
            ],
            "title": "Action2motion: Conditioned generation of 3d human motions",
            "venue": "In Proceedings of the 28th ACM International Conference on Multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "Chuan Guo",
                "Shihao Zou",
                "Xinxin Zuo",
                "Sen Wang",
                "Wei Ji",
                "Xingyu Li",
                "Li Cheng"
            ],
            "title": "Generating diverse and natural 3d human motions from text",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Shihui Guo",
                "Richard Southern",
                "Jian Chang",
                "David Greer",
                "Jian Jun Zhang"
            ],
            "title": "Adaptive motion synthesis for virtual characters: a survey",
            "venue": "The Visual Computer,",
            "year": 2015
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Holden",
                "Jun Saito",
                "Taku Komura",
                "Thomas Joyce"
            ],
            "title": "Learning motion manifolds with convolutional autoencoders",
            "venue": "In SIGGRAPH Asia 2015 technical briefs, pp",
            "year": 2015
        },
        {
            "authors": [
                "Daniel Holden",
                "Jun Saito",
                "Taku Komura"
            ],
            "title": "A deep learning framework for character motion synthesis and editing",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2016
        },
        {
            "authors": [
                "Peizhuo Li",
                "Kfir Aberman",
                "Rana Hanocka",
                "Libin Liu",
                "Olga Sorkine-Hornung",
                "Baoquan Chen"
            ],
            "title": "Learning skeletal articulations with neural blend shapes",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2021
        },
        {
            "authors": [
                "Zhouyingcheng Liao",
                "Jimei Yang",
                "Jun Saito",
                "Gerard Pons-Moll",
                "Yang Zhou"
            ],
            "title": "Skeleton-free pose transfer for stylized 3d characters",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Chen-Hsuan Lin",
                "Jun Gao",
                "Luming Tang",
                "Towaki Takikawa",
                "Xiaohui Zeng",
                "Xun Huang",
                "Karsten Kreis",
                "Sanja Fidler",
                "Ming-Yu Liu",
                "Tsung-Yi Lin"
            ],
            "title": "Magic3d: High-resolution text-to-3d content creation",
            "venue": "arXiv preprint arXiv:2211.10440,",
            "year": 2022
        },
        {
            "authors": [
                "Jun Liu",
                "Amir Shahroudy",
                "Mauricio Perez",
                "Gang Wang",
                "Ling-Yu Duan",
                "Alex C Kot"
            ],
            "title": "Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Lijuan Liu",
                "Youyi Zheng",
                "Di Tang",
                "Yi Yuan",
                "Changjie Fan",
                "Kun Zhou"
            ],
            "title": "Neuroskinning: Automatic skin binding for production characters with deep graph networks",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2019
        },
        {
            "authors": [
                "Matthew Loper",
                "Naureen Mahmood",
                "Javier Romero",
                "Gerard Pons-Moll",
                "Michael J Black"
            ],
            "title": "Smpl: A skinned multi-person linear model",
            "venue": "ACM transactions on graphics (TOG),",
            "year": 2015
        },
        {
            "authors": [
                "Thalmann Magnenat",
                "Richard Laperri\u00e8re",
                "Daniel Thalmann"
            ],
            "title": "Joint-dependent local deformations for hand animation and object grasping",
            "venue": "Technical report, Canadian Inf. Process. Soc,",
            "year": 1988
        },
        {
            "authors": [
                "Naureen Mahmood",
                "Nima Ghorbani",
                "Nikolaus F Troje",
                "Gerard Pons-Moll",
                "Michael J Black"
            ],
            "title": "Amass: Archive of motion capture as surface shapes",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Bill Manaris"
            ],
            "title": "Natural language processing: A human-computer interaction perspective",
            "venue": "In Advances in Computers,",
            "year": 1998
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, highperformance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Mathis Petrovich",
                "Michael J Black",
                "G\u00fcl Varol"
            ],
            "title": "Action-conditioned 3d human motion synthesis with transformer vae",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Mathis Petrovich",
                "Michael J Black",
                "G\u00fcl Varol"
            ],
            "title": "Temos: Generating diverse human motions from textual descriptions",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Ben Poole",
                "Ajay Jain",
                "Jonathan T Barron",
                "Ben Mildenhall"
            ],
            "title": "Dreamfusion: Text-to-3d using 2d diffusion",
            "venue": "arXiv preprint arXiv:2209.14988,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Robert W Sumner",
                "Jovan Popovi\u0107"
            ],
            "title": "Deformation transfer for triangle meshes",
            "venue": "ACM Transactions on graphics (TOG),",
            "year": 2004
        },
        {
            "authors": [
                "Qingyang Tan",
                "Lin Gao",
                "Yu-Kun Lai",
                "Shihong Xia"
            ],
            "title": "Variational autoencoders for deforming 3d mesh models",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Guy Tevet",
                "Sigal Raab",
                "Brian Gordon",
                "Yonatan Shafir",
                "Daniel Cohen-Or",
                "Amit H Bermano"
            ],
            "title": "Human motion diffusion model",
            "venue": "arXiv preprint arXiv:2209.14916,",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Haoyu Wang",
                "Shaoli Huang",
                "Fang Zhao",
                "Chun Yuan",
                "Ying Shan"
            ],
            "title": "Hmc: Hierarchical mesh coarsening for skeleton-free motion retargeting",
            "venue": "arXiv preprint arXiv:2303.10941,",
            "year": 2023
        },
        {
            "authors": [
                "Jiashun Wang",
                "Xueting Li",
                "Sifei Liu",
                "Shalini De Mello",
                "Orazio Gallo",
                "Xiaolong Wang",
                "Jan Kautz"
            ],
            "title": "Zero-shot pose transfer for unrigged stylized 3d characters",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Jing Xu",
                "Wei Zhang",
                "Yalong Bai",
                "Qibin Sun",
                "Tao Mei"
            ],
            "title": "Freeform body motion generation from speech",
            "venue": "arXiv preprint arXiv:2203.02291,",
            "year": 2022
        },
        {
            "authors": [
                "Zhan Xu",
                "Yang Zhou",
                "Evangelos Kalogerakis",
                "Karan Singh"
            ],
            "title": "Predicting animation skeletons for 3d articulated models via volumetric nets",
            "venue": "In 2019 international conference on 3D vision (3DV),",
            "year": 2019
        },
        {
            "authors": [
                "Zhan Xu",
                "Yang Zhou",
                "Evangelos Kalogerakis",
                "Chris Landreth",
                "Karan Singh"
            ],
            "title": "Rignet: Neural rigging for articulated characters",
            "venue": "ACM transactions on graphics,",
            "year": 2020
        },
        {
            "authors": [
                "Zhan Xu",
                "Yang Zhou",
                "Li Yi",
                "Evangelos Kalogerakis"
            ],
            "title": "Morig: Motion-aware rigging of character meshes from point clouds",
            "venue": "In SIGGRAPH Asia 2022 Conference Papers,",
            "year": 2022
        },
        {
            "authors": [
                "Jie Yang",
                "Lin Gao",
                "Qingyang Tan",
                "Huang Yihua",
                "Shihong Xia",
                "Yu-Kun Lai"
            ],
            "title": "Multiscale mesh deformation component analysis with attention-based autoencoders",
            "venue": "IEEE Transactions on Visualization and Computer Graphics,",
            "year": 2021
        },
        {
            "authors": [
                "Wang Yifan",
                "Noam Aigerman",
                "Vladimir G Kim",
                "Siddhartha Chaudhuri",
                "Olga Sorkine-Hornung"
            ],
            "title": "Neural cages for detail-preserving 3d deformations",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Mingyuan Zhang",
                "Zhongang Cai",
                "Liang Pan",
                "Fangzhou Hong",
                "Xinying Guo",
                "Lei Yang",
                "Ziwei Liu"
            ],
            "title": "Motiondiffuse: Text-driven human motion generation with diffusion model",
            "venue": "arXiv preprint arXiv:2208.15001,",
            "year": 2022
        },
        {
            "authors": [
                "Mingyuan Zhang",
                "Xinying Guo",
                "Liang Pan",
                "Zhongang Cai",
                "Fangzhou Hong",
                "Huirong Li",
                "Lei Yang",
                "Ziwei Liu"
            ],
            "title": "Remodiffuse: Retrieval-augmented motion diffusion model",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Yi Zhou",
                "Connelly Barnes",
                "Jingwan Lu",
                "Jimei Yang",
                "Hao Li"
            ],
            "title": "On the continuity of rotation representations in neural networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The advent of 3D animation has revolutionized the digital world, providing a vibrant platform for storytelling and visual design. However, the intricacy and time-consuming nature of traditional animation processes poses significant barriers to entry. In recent years, learning-based methods have emerged as a promising solution for animation. Motion generation models (Guo et al., 2020; Zhang et al., 2022) and auto-rigging methods (Xu et al., 2020; 2022b) are among the techniques that offer unprecedented results for motion synthesis and mesh control, respectively, in both quality and generalization. Nevertheless, they still fall short of providing a comprehensive solution.\nExisting motion generation methods (Tevet et al., 2022; Zhang et al., 2022; Chen et al., 2023) facilitate animation by leveraging the SMPL model (Loper et al., 2015), a parametric 3D model of the human form equipped with a uniform skeletal rigging system. However, these approaches oversimplify the task by assuming a fixed mesh topology and skeletal structures, and they fail to account for the geometric shapes of skeleton-free characters. This inherent assumption significantly restricts the creation of motions for diverse, non-rigged 3D characters. Moreover, the predominant focus of existing motion datasets is on humanoid characters, which consequently limits the potential for datadriven methods to be applicable to non-standard characters. Therefore, a significant challenge lies in extending motion generation models from human to non-human characters, enabling the model to accurately perceive their geometry even in the absence of pre-rigged skeletons.\n\u2217Most of this work was done during Jiaxu\u2019s internship at Tencent AI Lab. Email: zjiaxu@whu.edu.cn \u2020Jiaxu Zhang and Shaoli Huang contributed equally. \u2021Corresponding author: tuzhigang@whu.edu.cn\nOn the other hand, auto-rigging methods have attempted to enable mesh deformation control by skeletonizing and rigging 3D characters using optimization techniques (Baran & Popovic\u0301, 2007) or deep models (Xu et al., 2020; 2019). Despite their attempts, these methods tend to generate diverse skeleton structures for different characters, hindering the possibility of generating a uniform motion representation for animation. It is worth highlighting that certain methods have suggested the utilization of a pre-defined skeleton template for rigging characters (Liu et al., 2019b; Li et al., 2021). However, these approaches are incapable of handling meshes that exhibit substantial variations in shape and topology. Consequently, the resulting rigging often necessitates manual adjustments or custom motions, which undermines the objective of achieving automatic motion synthesis.\nTo overcome these limitations, we introduce a new Text-driven Animation Pipeline (TapMo) capable of generating realistic and anthropomorphic motion for a wide range of skeleton-free 3D characters as Figure 1 shows. TapMo only requires the input of motion descriptions in natural language and can animate skeleton-free characters by perceiving their shapes and generating mesh-specific motions. To achieve this goal, we explore two key components of TapMo, namely the Mesh Handle Predictor and the Shape-aware Motion Diffusion. These two components synergistically automate the entire animation creation process, establishing a more accessible way of generating animations beyond SMPL-based humans, so as to democratize animation creation on heterogeneous characters.\nThe Mesh Handle Predictor addresses the mesh deformation control in a unified manner for diverse 3D characters. Inspired by SfPT (Liao et al., 2022), we utilize a GCN-based network that adaptively locates control handles in the mesh, regulating their movements based on predicted skinning weights. Each predicted handle independently controls its assigned semantic part, while the first handle is fixed as the character\u2019s root for global control. More importantly, the module also serves a dual purpose by offering a mesh deformation feature for future shape-aware motion generation.\nThe Shape-aware Motion Diffusion copes with the challenge of perceiving the mesh shape and generating realistic motions for heterogeneous 3D characters. Language descriptors are regarded as a user-friendly interface for people to interact with computers (Manaris, 1998). Thus, we design a text-driven motion generation model with respect to the Diffusion Model (Sohl-Dickstein et al., 2015; Ho et al., 2020) that has achieved impressive results in the generation of complex data distribution recently. Different from previous Motion Diffusion Models (Tevet et al., 2022; Zhang et al., 2022), our model generates a universal motion representation that does not rely on the character\u2019s skeletal rigging structure, enabling it to animate a wide range of characters. Notably, we use the mesh deformation feature encoded by the Mesh Handle Predictor as an additional condition to generate motion adaptations that are flexible for the specific mesh to preserve the geometric integrity of the animations. Finally, Linear Blend Skinning is used to animate the mesh with the generated motion and the predicted handles.\nIn response to the challenge posed by the absence of ground-truth handles and motions for diverse 3D characters, we present a novel, weakly-supervised training approach for TapMo. Our strategy leverages shape constraints and motion priors as auxiliary supervision signals to enhance the learning of handle prediction and motion generation in TapMo.\nWe evaluate our TapMo on a variety of 3D characters and complex motion descriptions. The qualitative and quantitative results show that our TapMo is effective on seen and unseen 3D characters. The generated animation of TapMo achieves higher motion and geometry quality compared with existing learning-based methods.\nOur contributions are listed below:\n\u2022 We present a comprehensive solution TapMo that, to our knowledge, represents the first attempt to enable the motion generation of generic skeleton-free characters using text descriptions.\n\u2022 We design two key components in TapMo i.e., the Mesh Handle Predictor and the Shape-aware Motion Diffusion. These components collaborate to govern mesh deformation and generate plausible motions while accounting for diverse non-rigged mesh shapes.\n\u2022 A weakly-supervised training strategy is presented to enable the geometry learning of TapMo with limited ground-truth data of both handle-annotated characters and text-relevant motions.\n\u2022 Extensive experiments demonstrate the effectiveness and generalizability of TapMo qualitatively and quantitatively, and the animation results of TapMo are superior to existing learning-based methods both on motion and geometry."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Motion generation remains a persistent challenge within the realm of computer graphics (Guo et al., 2015). Recently, an upsurge of interest has been seen in learning-based models for motion generation (Holden et al., 2015; 2016). These cutting-edge models often explore various conditions for human motion generation, which include but are not limited to action labels (Guo et al., 2020; Petrovich et al., 2021), audio (Xu et al., 2022a; Dabral et al., 2022), and text (Tevet et al., 2022; Chen et al., 2023; Zhang et al., 2022). Among all these conditioned modalities, text descriptors are identified as the most user-friendly and convenient, thereby positioning text-to-motion as a rising area of research. While the existing advancements pave the way for the motion generation of humans equipped with a pre-defined skeletal rigging system, they do not cater to various 3D characters lacking this skeletal system. Moreover, several large-scale datasets, providing human motions as sequences of 3D skeletons (Liu et al., 2019a; Guo et al., 2020; 2022) or SMPL parameters (Mahmood et al., 2019). However, these datasets also primarily cater to human motion learning and lack the inclusion of a broad spectrum of heterogeneous characters. Consequently, methods trained directly on these datasets struggle to adapt to the task of generating motions for heterogeneous characters. Acknowledging this limitation, our research sets sail into previously unexplored territory: text-driven motion generation for skeleton-free 3D characters. Distinctly different from previous studies, the 3D characters in our approach are non-rigged and feature a wide array of mesh topologies.\nSkeleton-free mesh deformation. The skeletal rigging system is widely used in animation (Magnenat et al., 1988), but the process of skeletonization and rigging is not accessible to laypeople. To address this issue, some researchers have explored skeleton-free mesh deformation methods. Traditional methods rely on manual landmark annotations (Ben-Chen et al., 2009; Sumner & Popovic\u0301, 2004). Recently, Yifan et al. (2020) proposed a learnable neural cage for detail-preserving shape deformation, which works well for rigid objects but not for articulated characters. Other approaches, such as those by Tan et al. (2018), Yang et al. (2021), embed the mesh into latent spaces to analyze the mesh deformation primitives. However, these latent spaces are not shared across subjects, so they cannot be used for animating different characters. Wang et al. (2023b) propose a zero-shot implicit deformation module, which is incompatible with our explicit motion generation approach. The recent SfPT (Liao et al., 2022) and HMC (Wang et al., 2023a) generate consistent deformation parts across different character meshes, allowing for pose transfer through the corresponding parts. However, the character\u2019s root is ignored in the deformation parts, leading to failure in motion transfer. In this work, we are inspired by SfPT and introduce a Mesh Handle Predictor to animate skeleton-free characters in a handle-based manner. This manner represents motion using the translation and rotation of predicted mesh handles, which differs from traditional skeleton-based methods. Our approach diverges from SfPT in two key respects: first, unlike SfPT, we incorporate root handle prediction to enable more intricate motions. Second, our handle predictor also functions to deliver a mesh deformation-aware representation that is crucial for subsequent mesh motion generation."
        },
        {
            "heading": "3 METHOD",
            "text": "As illustrated in Figure 2, given a non-rigged mesh and a motion description in natural language, our goal is to synthesize a realistic animation for the character according to the motion description \u03ba. Specifically, the input mesh of the character \u03d5 is fed into the Mesh Handle Predictor \u03bb(\u00b7) to predict the mesh handle h and the skinning weight s of the vertices. Besides, a low-dimensional mesh deformation feature f\u03d5 of the character \u03d5 is extracted to represent the shape of the mesh. Let \u03b8\u03bb denote the learnable parameter of the Mesh Handle Predictor, this process is formulated as:\n\u03bb : (\u03d5;\u03b8\u03bb) 7\u2192 (h, s,f\u03d5). (1)\nSubsequently, in Shape-aware Motion Diffusion, the mesh-deformation feature f\u03d5 and the text embedding of the motion description f\u03ba are used as the conditions to generate a text-guided motion x\u03020 and a mesh-specific adaptation \u03b4\u0302. The text embedding x\u03ba is extracted by the CLIP (Radford et al., 2021) model. This inverse diffuse process is formulated as:\n\u00b5 : (xt, t,f\u03ba,f\u03d5;\u03b8\u00b5) 7\u2192 (x\u03020, \u03b4\u0302), (2)\nwhere \u00b5(\u00b7) is the conditional denoiser of Diffusion Model and xt is the motion of noising step t. \u03b8\u00b5 is the learnable parameter of \u00b5(\u00b7). Following Tevet et al. (2022), we predict the real motion x\u03020, rather than the noise \u03f5\u0302t. Finally, after T = 1, 000 times of the inverse diffuse process, the sum of the generated motion x\u03020 and the adaptation \u03b4\u0302 are applied to the character via a handle-based driving method D(\u00b7) with the predicted h and s."
        },
        {
            "heading": "3.1 MESH HANDLE PREDICTOR",
            "text": "Given a mesh \u03d5 consisting of V vertices, we designate K mesh handles {hk}Kk=1 to govern its deformation. Consequently, every vertex is associated with a K-dimensional skinning weight associated with K handles. Overall, for the mesh \u03d5, the skinning weight is defined as s \u2208 RV\u00d7K , where 0 \u2264 si,k \u2264 1 and \u2211K k=1 si,k = 1. i is the vertex index. Since characters may have varying shapes and topologies, each handle is dynamically assigned to vertices with the same semantics across different meshes, except that the first handle is fixed at the root of the mesh. This adaptive assignment approach enables effective control of local and global motion over heterogeneous 3D characters.\nBased on the definition of the mesh handles, the motion of the mesh can be expressed as the combination of the translations and rotations of the handles, which involves local translations \u03c4 l \u2208 R(K\u22121)\u00d73, local rotations rl \u2208 R(K\u22121)\u00d76, global translation \u03c4 g \u2208 R3, and global rotation rg \u2208 R3. The rotations are represented by the 6D features proposed by Zhou et al. (2019). This motion can be denoted as x0 = {\u03c4 l, rl, \u03c4 g, rg} and the mesh can be driven by the motion in a handle-based manner D(\u00b7) according to the following formula:\nV\u0304i \u2032 = rg\n( K\u2211\nk=2\nsi,k ( rlk(V\u0304i \u2212 hk) + \u03c4 lk ) + hk ) + \u03c4 g,\u2200V\u0304i \u2208 V\u0304 , (3)\nwhere V\u0304 \u2208 RV\u00d73 is the positions of the mesh vertices. The rotations in this formula are represented by the rotation matrix.\nIn our TapMo, following Liao et al. (2022), we employ the Mesh Handle Predictor based on a GCN to locate the handles for the mesh. The input feature is the positions and normal of the mesh vertices, which can be represented as \u03d5 \u2208 RV\u00d76. The outputs are the skinning weight of mesh vertices and the handle positions in terms of the average position of vertices weighted by the skinning weight.\nAdaptive Handle Learning. The existing 3D character datasets (Xu et al., 2019; Adobe) lack handle annotations, and the skinning weights are defined independently for each character, which lacks consistency. Therefore, these datasets cannot be directly used as ground-truth. To overcome this limitation, we introduce three losses, i.e., Skinning Loss Ls, Pose Loss Lp, and Root Loss Lr\nto achieve an adaptive handle learning. Among them, the Skinning Loss Ls and Pose Loss Lp stem from Liao et al. (2022) and will be detailed in the Appendix.\nRoot Loss Lr is to enforce the first handle point close to the character\u2019s center hip, which is an important step in animation for controlling the global motion independently. Liao et al. (2022) do not take into account the global motion of the character and thus produce severe mesh distortion when applied to drive complex motions. We observe that in the traditional skeleton rigging system, the first joint of the character is always located in the hip part, which is also considered the root joint of the character. Thus, we use Lr to make the predicted position of the first handle h1 close to the position of the root joint b1 in the character. Lr is formulated as:\nLr = \u2225h1 \u2212 b1\u222522\n= \u2225\u2225\u2225\u2225\u2225 \u2211V i=1 si,1V\u0304i\u2211V i=1 si,1 \u2212 b1 \u2225\u2225\u2225\u2225\u2225 2\n2\n. (4)\nWith the three losses introduced above, the Handle Predictor can be trained by:\nmin \u03b8\u03bb\nLs + \u03bdpLp + \u03bdrLr, (5)\nwhere \u03bdr and \u03bdp are the loss balancing factor."
        },
        {
            "heading": "3.2 SHAPE-AWARE MOTION DIFFUSION",
            "text": "The Mesh Handle Predictor allows for controlling heterogeneous characters using a unified motion representation described in Eq.3. However, directly generating this motion representation to synthesize animation can lead to significant mesh distortion due to large variations in body shape among different characters. To address this issue, the Shape-aware Motion Diffusion generates mesh-specific adaptation parameters in addition to the handle motion. This approach preserves the geometric integrity of the animations.\nThe left part of Figure 3 illustrates the structure of the Shape-aware Motion Diffusion. The model takes the noising step t, the motion description feature f\u03ba, and the mesh deformation feature f\u03d5 as conditions while the t-step noised motion xt as input to generate the text-guided motion x\u03021:N0 of N frames and the mesh-specific adaptation \u03b4\u03021:N that are suitable for the mesh \u03d5. This process is defined in Eq.2. For brevity, and when there is no danger of confusion, we omit the superscripts.\nWe employ a Transformer Decoder (Vaswani et al., 2017) to construct the denoiser of the Diffusion Model and use two linear layers to map the output feature into motion and adaptation, respectively. Unlike MDM (Tevet et al., 2022), which uses only one text token from the last layer of CLIP, we use multiple text tokens from the penultimate layer of CLIP, the number of which is the same as the number of words in the input motion prompt. We have observed that the motion distortion is primarily caused by the generated local translations in our motion representation. Therefore, we only apply adaptations to the local translations of the generated motion. Thus, the dimension of the motion adaptation in one frame is K \u00d7 3. The final local translations in the motion are obtained by summing up the generated local translations and the mesh-specific adaptations.\nTo train the Diffusion Model, we fully utilize the existing motion-language dataset (Guo et al., 2022) by converting SMPL motions to our handle-based motion representation using the analytical method\nintroduced in Besl & McKay (1992) as the ground-truth motion x0. Originating from the simple loss of DDPM (Ho et al., 2020), a fully-supervised Motion Loss Lm is formulated as:\nLm := Ex0,t,f\u03ba,f\u03d5 [ \u2225x0 \u2212 x\u03020\u222522 ] , (6)\nwhere the motion x\u03020 is generated as Eq.2 according to the conditions t, f\u03ba and f\u03d5.\nMesh-specific Motion Learning. The existing motion-language dataset only involves human motions of SMPL model, which cannot be directly used to learn shape-aware motions for heterogeneous characters. To surmount this constraint, we randomly select characters from the 3D character datasets during training and use a weakly-supervised manner to assist the model in learning the mesh-specific information from the mesh deformation feature extracted by the Mesh Handle Predictor. As the right part of Figure 3 shows, in conjunction with the fully-supervised Motion Loss Lm, we designed two weakly-supervised losses, i.e., Spring Loss Lh and Adversarial Loss La to achieve mesh-specific motion learning.\nThe traditional skeletal rigging system of 3D characters has an important property: the length of the bones typically remains consistent during motion. This property ensures that the character\u2019s body structure remains stable throughout the animation. Drawing inspiration from this property, we propose the Spring Loss Lh. We model the handles on the character\u2019s limbs as a spring system and penalize the length deformation of this system. Specifically, we pre-define the common adjacency relationships between the handles of the character\u2019s limbs and use the distance between adjacent handles in the rest-pose as a reference to penalize the distance changes during the character\u2019s motion:\nLh = \u2211 i,j\u2208A N\u2211 n=1 ( e\u2212(E(h 0 i ,h 0 j )+\u03c3)\u2225E(hni , hnj )\u2212 E(h0i , h0j )\u222522 +\n\u2225E(hni , hnj )\u2212 E(hn\u22121i , h n\u22121 j )\u2225 2 2\n) ,\n(7)\nwhere E(\u00b7) is the Euclidean Distance Function and \u03c3 is a hyper-parameter. i and j are the indices of the adjacent handles. h0 is the handle position of the rest-pose, hn is that of the n-frame pose. e\u2212(E(h 0 i ,h 0 j )+\u03c3) is an adaptive spring coefficient.\nThe Adversarial Loss La is inspired by GAN (Goodfellow et al., 2020) and is used to encourage the generated mesh motion to conform to the prior of the skeleton-driven mesh motion on various characters. To reduce computation costs, we randomly sample 100 mesh vertices on the arms in each frame to calculate the Adversarial Loss for each character. This is based on the observation that motion distortion mainly occurs in the arm parts of the characters during movement. We utilize a Discriminator to differentiate the generated handle-driven mesh motions from the skeleton-driven ones. La is designed based on the adversarial training:\nLa = EV\u0304\u223cp(V\u0304 sk) [ logDisc(V\u0304 ) ] +\nEV\u0304\u223cp(V\u0304 g) [ log ( 1\u2212Disc(V\u0304 ) )] ,\n(8)\nin which p(\u00b7) represents the distribution of the skeleton-driven V\u0304 sk or generated V\u0304 g mesh motions. With the three losses introduced above, the Shape-aware Motion Diffusion can be trained by:\nmin \u03b8\u00b5\nLm + \u03bdhLh + \u03bdaLa, (9)\nwhere \u03bdh and \u03bda are the balancing factors. Lh and La are conducted on the motion adaptation \u03b4\u0302. Motion Adaptation Fine-tuning. To further enhance the compatibility of the generated motion for heterogeneous characters and minimize the mesh distortion caused by the motion, we utilize pseudo-labels to finetune the linear mapping layer of the motion adaptation. These pseudo-labels are obtained by optimizing the motion adaptation through an as-rigid-as-possible (ARAP) objective defined as:\nO = \u03bdv \u2211 i,j\u2208\u03a6 \u2225E(V\u0304 ni , V\u0304 nj )\u2212 E(V\u0304 0i , V\u0304 0j )\u222522 + \u2225\u03b4 \u2032 \u2212 \u03b4\u0302\u222522. (10)\nHere, \u03a6 represents the set of edges on the mesh, and \u03b4 \u2032\nrepresents the optimized motion adaptation at the previous step. \u03bdv is the balancing factor.\nThe linear mapping layer of the motion adaptation is then finetuned by:\nmin \u03b8\u03b4\u00b5\n\u2225\u03b4pl \u2212 \u03b4\u0302\u222522, (11)\nwhere \u03b4pl is the pseudo-labels of the motion adaptations, \u03b8\u03b4\u00b5 is the learnable parameters."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUP",
            "text": "Baselines. To the best of our knowledge, TapMo is the first pipeline to generate animations for skeleton-free characters in a text-driven manner. We design two reasonable baselines to evaluate the effectiveness of our TapMo. 1) MDM + skeleton-based retargeting (MDM-SR). MDM (Tevet et al., 2022) is a popular motion generation method for the SMPL model. We use MDM to generate human motion based on the text, then automatically rig the skeleton template of the SMPL model to the character by Baran & Popovic\u0301 (2007), and finally retarget the human motion to the character by motion copy. 2) MDM + SfPT (MDM-SFR). We use MDM to generate human motion, then use SfPT (Liao et al., 2022) to transfer the human motion to the character frame by frame.\nDatasets. Four public datasets are used to train and evaluate our TapMo, i.e., AMASS (Mahmood et al., 2019), Mixamo (Adobe), ModelsResource-RigNet (Xu et al., 2019), and HumanML3D (Guo et al., 2022). Please see the Appendix for more details about datasets.\nMetrics. We quantitatively evaluate our TapMo from two aspects, i.e., motion quality and geometry quality. For evaluating the motion quality, we follow the approach presented in Guo et al. (2022), where motion representations and text descriptions are first embedded using a pre-trained feature extractor and then evaluated using five metrics. However, as the motion representation in our TapMo differs from that in Guo et al. (2022), we adopt the same configurations as theirs and retrain the feature extractor to align with our motion representation. For the geometry quality, we evaluate it from mesh vertices and mesh handles using two metrics as follows: 1) ARAP-Loss is used to gauge the level of local distortion of the mesh during the characters\u2019 motion, which can be calculated by:\nLARAP = \u2211 i,j\u2208\u03a6 \u2225E(V\u0304 ni , V\u0304 nj )\u2212 E(V\u0304 0i , V\u0304 0j )\u222522, (12)\nwhere \u03a6 is the set of edges on the mesh, ARAP-Loss is similar to the ARAP objective function proposed in Eq.10, but without the second objective item. 2) Handle-FID. To assess the global geometry quality, we employ two SMPL human models that vary significantly in body size by adjusting the shape parameters. Using FID, we measure the distribution distances between the handle positions of the generated motion and the real motion on these two models.\nWe would encourage the reviewers to see more about the implementation details in the Appendix."
        },
        {
            "heading": "4.2 QUALITATIVE RESULTS",
            "text": "Figure 4 illustrates the overall qualitative results of our TapMo pipeline, demonstrating its impressive capabilities in generating reasonable motions and animating a diverse range of 3D characters based on textual descriptions. In comparison to baseline models, TapMo\u2019s motions more closely align with textual descriptions, exhibit less mesh distortion, and offer higher-quality animation. For instance, as highlighted by the red circle in Figure 4, the results produced by MDM-SR and MDMSFR demonstrate significant mesh distortion, while TapMo preserves the geometry details better. Additionally, the black wireframe displayed in Figure 4 demonstrates that baseline methods tend to generate motions that do not align with the description, while our TapMo benefits from improved Diffusion Model structure, resulting in generated motion that is more consistent with the description.\nFigure 5 provides a visual representation of the adaptive mesh handles and their corresponding skinning weights for various characters estimated by TapMo\u2019s Mesh Handle Predictor. Handles not assigned to any mesh vertices are not drawn in this figure. In humanoid characters, the Mesh Handle Predictor accurately assigns handles to the positions that are near the skeleton joints of the human body, resulting in natural and realistic humanoid movements. For non-humanoid characters, the Mesh Handle Predictor assigns mesh vertices to different numbers of handles based on the semantics of the mesh parts. It is worth noting that all characters have a root handle located near the center of their bodies, which serves as the key to driving global movement.\nFigure 6 illustrates how mesh-specific motion adaptations impact the animation results in TapMo. Due to the significant variation in the shape and topology of characters, the original motion generated by the Diffusion Model may lead to severe mesh distortion. The results exhibit local mesh distortion and inter-penetrations without motion adaptations. The mesh-specific motion adaptations generated by TapMo can effectively alleviate these issues and enhance the geometry quality."
        },
        {
            "heading": "4.3 QUANTITATIVE RESULTS",
            "text": "Table 1 provides a quantitative comparison between TapMo and previous motion generation models, including T2G (Bhattacharya et al., 2021), Hier (Ghosh et al., 2021), TEMOS (Petrovich et al., 2022), T2M (Guo et al., 2022), MDM (Tevet et al., 2022), and ReMoDiffuse (Zhang et al., 2023). The results demonstrate that our TapMo outperforms previous models significantly in terms of both R-Precision and FID metrics. We introduced multi-token and Transformer Decoder strategies to the Shape-aware Motion Diffusion, which showed a stable improvement in all metrics. Notably, TapMo (Ours) achieved a remarkable improvement of over 50% in the FID metric compared to the TapMo (MDM) implementation, with a score of 0.515 compared to 1.058. These findings demonstrate the superior performance of our TapMo and the effectiveness of the adaptations we made to the Diffusion Model structure, which allowed for more accurate and higher-quality motion generation.\nTable 2 provides a comprehensive comparison between TapMo and the baseline methods, focusing on the geometry quality of the generated animations. TapMo demonstrates remarkable improvements, achieving a significant reduction in ARAP-Loss both on seen and unseen characters. Specifically, TapMo achieves 69.8% (0.271 vs. 0.897) lower scores compared to MDM-SR and 73.1% (0.271 vs. 1.006) lower scores compared to MDM-SFR on seen characters. For unseen characters, TapMo achieves 58.2% (0.329 vs. 0.787) lower scores compared to MDM-SR and 63.8% (0.329 vs. 0.910) lower scores compared to MDM-SFR. These results indicate that TapMo excels in generating animations with superior geometry details and smoother mesh surfaces compared to the\nbaseline methods. Moreover, TapMo exhibits a superior global geometry preservation performance, with Handle-FID scores 71.7% (0.046 vs 0.163) lower than that of MDM-SFR. Notably, MDM-SR achieves the best Handle-FID (0.012) due to its utilization of a standard SMPL skeleton structure, resulting in minimal global distortion when driving the SMPL model. Additionally, the ablative study underscores the effectiveness of the special designs employed in our TapMo pipeline."
        },
        {
            "heading": "4.4 USER STUDY",
            "text": "We conducted a user study to evaluate the visual effects of our TapMo against the baseline methods. We invited 100 volunteers and gave them 15 videos. Each video includes one textual description and three anonymous animation results. We ask users to rank the three results in three aspects: overall quality (O-Q), text-motion matching degree (T-M), and geometry quality (G-Q). After excluding abnormal questionnaires, we collect 1,335 ranking comparison statistics in total, and the average rank of the methods is summarized in Figure 3. Our TapMo outperforms the baseline methods by a large margin and more than 84% of users prefer the animation generated by our TapMo."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we propose a novel text-driven animation pipeline called TapMo, which enables nonexperts to create their own animations without requiring professional knowledge. In TapMo, two key components are exploited to control and animate a wide range of skeleton-free characters. The first is the Mesh Handle Predictor, which clusters mesh vertices into adaptive handles for semantic control. The second is Shape-aware Motion Diffusion, which generates text-guided motions considering the specific deformation properties of the mesh, ensuring coherent and plausible character animation without introducing mesh distortion. To train TapMo with limited ground-truth data of both handle-annotated 3D characters and text-relevant motions, we propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning. We conduct extensive experiments to validate the effectiveness of our method and show that it achieves state-of-the-art performance compared to baseline methods.\nAcknowledgements. This work was supported by the fund of Tencent AI Lab RBFR2022012, and the Natural Science Fund for Distinguished Young Scholars of Hubei Province under Grant 2022CFA075."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 SUPPLEMENTARY ON METHOD\nWe introduce three losses, i.e., Skinning Loss Ls, Root Loss Lr, and Pose Loss Lp to achieve an adaptive handle learning for our Handle Predictor.\nFollowing Liao et al. (2022), the assumption of Ls is that if two mesh vertices belong to the same body part based on the ground-truth skinning weight, they should also be controlled by the same handle in the predicted skinning. We select mesh vertices with si,k > 0.9,\u2203k and use the KL divergence to enforce similarity between skinning weights of these two vertices:\nLs = \u03b3i,j K\u2211\nk=1\n(si,k log(si,k)\u2212 si,k log(sj,k)) , (13)\nwhere i and j indicate two randomly sampled vertices. \u03b3 is an indicator function defined as follows: \u03b3i,j = 1 if i and j belong to the same part in the ground-truth skinning weight and \u03b3i,j = \u22121 if not. Pose Loss Lp is to ensure that the posed mesh driven by the predicted handle and skinning weight can be as similar as possible to the shape of the mesh driven by the ground-truth skeleton, which helps the predicted handle more in line with the character\u2019s articulation structure. To achieve this, we randomly select a skeletal pose for the character and drive the mesh using its skeleton to obtain a posed mesh V\u0304 p. Then, the local translation and local rotation of our representation can be analytically calculated according to the rest-pose mesh and the posed mesh (Besl & McKay, 1992). Next, we apply the local motion to the mesh using Eq.3 without the global items to obtain a skeleton-free posed mesh V\u0302 p. The Lp can be calculated by:\nLp = \u2225V\u0302 p \u2212 V\u0304 p\u222522. (14)\nA.2 SUPPLEMENTARY ON EXPERIMENTAL SETUP\nImplementation Details. We implement our pipeline using PyTorch framework(Paszke et al., 2019). The Mesh Handle Predictor (refer to Section 3.1) consists of a three-layer GCN followed by a three-layer MLP with a softmax activation function. The input feature dimension of the Handle Predictor is V \u00d7 6, and the output dimension of the skinning weight is V \u00d7 K. The handle positions, with a dimension of K \u00d7 3, are calculated by averaging the positions of vertices weighted by the skinning weight. The number of the mesh handle K is set as 30. The architecture of the Diffusion Model in our TapMo (see Section 3.2) is illustrated in the left part of Figure 3, with the number of attention heads set to 4. The mesh deformation feature f\u03d5 is formed by concatenating three-level features within the Mesh Handle Predictor, each with dimensions of 64, 128, and 256 respectively. Subsequently, a linear layer is employed in the Diffusion Model to map the mesh deformation feature to a dimension of 512.\nIn training, the motion sequence length N is padded to 196 frames, and the text length is padded to 30 words. The text feature and the mesh feature are concatenated to form the condition token, with a dimension of 31\u00d7 512. Padding masks for the motion sequence and text are utilized during training to prevent mode collapse. The loss balancing factors \u03bdr, \u03bdp, \u03bdh, and \u03bda are set as 0.1, 1.0, 0.001, and 0.1, respectively. We use an Adam optimizer with a learning rate of 1e-4 to train the Handle Predictor. The batch size is 4, and the training epoch is 500. To train the Diffusion Model, we also use an Adam optimizer with a learning rate of 1e-4. The batch size is 32 and the training step is 800,000. The finetune step is set as 100,000. The optimization process involves simultaneously optimizing the Diffusion Model and the mesh motion discriminator. In addition, a margin parameter of 0.3 is introduced, dictating that the discriminator is optimized only when the score of the fake sample exceeds the margin parameter, and vice versa.\nDatasets. Four public datasets are used to train and evaluate our TapMo, i.e., AMASS (Mahmood et al., 2019), Mixamo (Adobe), ModelsResource-RigNet (Xu et al., 2019), and HumanML3D (Guo et al., 2022).\n\u2022 AMASS is a comprehensive human motion dataset comprising over 11,000 motion sequences. This dataset was created by harmonizing diverse motion capture data using a standardized parameterization of the SMPL model. The SMPL model disentangles and parameterizes human pose and shape, allowing for the generation of a range of body shapes by manipulating the shape parameters.\n\u2022 Mixamo is an animation repository consisting of multiple 3D virtual characters with diverse skeletons and shapes. In our training process, we utilize 100 characters and 1,112 motion sequences from this dataset. By matching the corresponding joints of different characters, these motion sequences can be applied to any character in the dataset, allowing for a wide range of animations to be used in our pipeline.\n\u2022 ModelsResource-RigNet is a 3D character dataset that contains 2,703 rigged characters with a large shape variety. Each character has one mesh in the rest pose and has its individual skeletal rigging system with skinning weight. The meshes of these characters are heterogeneous in number, order, and topology of the vertices. We follow the train-test split protocol in Xu et al. (2019).\n\u2022 HumanML3D is a large-scale motion-language dataset that textually re-annotating motion capture data from the AMASS and HumanAct12 (Guo et al., 2020) collections. It contains 14,616 motion sequences annotated by 44,970 textual descriptions. The motion sequences in HumanML3D are all fit for the SMPL model and represented by a concatenation of root velocity, joint positions, joint velocities, joint rotations, and the foot contact binary labels. We convert these SMPL motions to our ground-truth motions using the analytical method introduced in Besl & McKay (1992) and follow the train-test split protocol in Guo et al. (2020).\nMetrics. The five metrics (Guo et al., 2022) for evaluating motion quality are listed as follows:\n\u2022 R-Precision. We provide a single motion sequence and 32 text descriptions, consisting of one ground truth and 31 randomly selected mismatched descriptions. We calculate the Euclidean distances between the motion and text embeddings and rank them. We report the accuracy of motion-to-text retrieval at Top-1, Top-2, and Top-3 accuracy.\n\u2022 Frechet Inception Distance (FID). Calculating the distribution distance between the generated and real motion using FID (Heusel et al., 2017) on the extracted motion features.\n\u2022 Multimodal Distance (MM-Dist). The average Euclidean distances between each text feature and the generated motion feature corresponding to this text.\n\u2022 Diversity. We randomly select 300 motion pairs from a set, extract their features, and calculate the average Euclidean distances between the pairs to quantify motion diversity within the set.\n\u2022 Multimodality (MModality). We generate 20 motion sequences for each text description, resulting in 10 pairs of motion. For each pair, we extract motion features and calculate the average Euclidean distance between them. The reported metric is the average distance over all text descriptions.\nA.3 SUPPLEMENTARY ON EXPERIMENTS\nFigure 7 shows the results of TapMo in generating animations that match the same motion descriptions for characters with vastly different body shapes. TapMo can generate mesh-specific motions for characters without causing noticeable mesh distortion while utilizing adaptive handles to drive the characters naturally. For instance, as depicted in the left portion of Figure 7, TapMo accurately replicates the dynamic postures of both jellyfish and humans swimming. In the example on the right side of Figure 7, a ghost with no legs, a hand, as well as a gorilla, can all execute the running and raising hands motion. Overall, the motion generated by TapMo is plausible and lifelike.\nWe have implemented an additional application of TapMo, which is animation editing. This application allows users to make edits to the animation during the motion generation process without requiring any additional training. Users can fix handles that they do not want to edit and let the model generate the rest of the animation. We experiment with editing the upper and lower body motion of the characters, which is achieved by overwriting x\u03020 with a part of the input motion during the sampling process of the Diffusion Model. Figure 8 demonstrates that TapMo is capable of editing animations according to the text description while maintaining motion consistency.\nFigure 9 demonstrates a failure case of TapMo, where the character animation exhibits significant mesh distortion and breakage. This issue arises due to the non-standard rest pose of the character, which deviates from the typical T-pose used in our training data. Consequently, TapMo faces limitations in generating animations for characters with non-standard rest poses. It is important to note that this bias related to different rest poses is not unique to our method, as traditional animation creation also requires consistent rest poses for target characters. Addressing this bias and finding solutions for character rest pose variations is an essential area for further research in the field of learning-based animation.\nGeneralizability. TapMo achieves high-quality geometry animations for unseen characters, as indicated by the quantitative results in Table 2. Moreover, the green photo elf and the Sponge Bob in Figure 4, the polygon tree in Figure 5, and the hand in Figure 7 are all internet-sourced wild characters. Most of the characters shown in experiments lacked ground-truth motion data during training. Thus, our TapMo exhibits strong generalizability across diverse mesh topologies, consistently delivering superior-quality animations for both seen and unseen 3D characters.\nLimitations of this work are listed as follows:\n\u2022 Due to the weakly-supervised training strategy on SMPL motions, our TapMo is limited in generating rich animations for quadrupeds or characters with significantly different locomotion patterns from humans. A possible solution to this issue is to collect a larger and more diverse dataset of language-motion pairs for a wide range of characters.\n\u2022 Before the motion generation of TapMo, generating meshes from textual descriptions using existing mesh generation methods like DreamFusion (Poole et al., 2022) or Magic3D (Lin et al., 2022) is a preferable approach to achieve a comprehensive text-based animation pipeline. Unfortunately, current mesh generation methods are constrained to producing simple objects like tables, chairs, or airplanes due to the lack of a character-language dataset. Generating complex animated characters from textual descriptions remains an exciting area for future research."
        }
    ],
    "title": "TAPMO: SHAPE-AWARE MOTION GENERATION OF SKELETON-FREE CHARACTERS",
    "year": 2024
}