{
    "abstractText": "The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision encoder with temporal and local perception on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent\u2019s ability of grounded multimodal perception, HTML comprehension, and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB, we improve over the previous best offline methods by more than 45.8%, even outperforming online-finetuned SoTA, humans, and GPT-4-based agent. On the WebShop benchmark, our 3-billion-parameter model achieves superior performance to the existing SoTA, PaLM-540B. Furthermore, WebGUM exhibits strong positive transfer to the real-world planning tasks on the Mind2Web. We also collect 347K high-quality demonstrations using our trained models, 38 times larger than prior work, and make them available to promote future research in this direction.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hiroki Furuta"
        },
        {
            "affiliations": [],
            "name": "Kuang-Huei Lee"
        },
        {
            "affiliations": [],
            "name": "Ofir Nachum"
        },
        {
            "affiliations": [],
            "name": "Yutaka Matsuo"
        },
        {
            "affiliations": [],
            "name": "Aleksandra Faust"
        },
        {
            "affiliations": [],
            "name": "Shixiang Shane Gu"
        },
        {
            "affiliations": [],
            "name": "Izzeddin Gur"
        }
    ],
    "id": "SP:9538924453b7ca1c8c5b2b1979557f2a251ef4e1",
    "references": [
        {
            "authors": [
                "Leonard Adolphs",
                "Benjamin Boerschinger",
                "Christian Buck",
                "Michelle Chen Huebscher",
                "Massimiliano Ciaramita",
                "Lasse Espeholt",
                "Thomas Hofmann",
                "Yannic Kilcher",
                "Sascha Rothe",
                "Pier Giuseppe Sessa",
                "Lierni Sestorain Saralegui"
            ],
            "title": "Boosting search engines with interactive agents",
            "venue": "In Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Armen Aghajanyan",
                "Dmytro Okhonko",
                "Mike Lewis",
                "Mandar Joshi",
                "Hu Xu",
                "Gargi Ghosh",
                "Luke Zettlemoyer"
            ],
            "title": "Htlm: Hyper-text pre-training and prompting of language models",
            "venue": "arXiv preprint arXiv:2107.06955,",
            "year": 2021
        },
        {
            "authors": [
                "Armen Aghajanyan",
                "Bernie Huang",
                "Candace Ross",
                "Vladimir Karpukhin",
                "Hu Xu",
                "Naman Goyal",
                "Dmytro Okhonko",
                "Mandar Joshi",
                "Gargi Ghosh",
                "Mike Lewis",
                "Luke Zettlemoyer"
            ],
            "title": "Cm3: A causal masked multimodal model of the internet",
            "venue": "arXiv preprint arXiv:2201.07520,",
            "year": 2022
        },
        {
            "authors": [
                "Rao",
                "Jarek Rettinghouse",
                "Diego Reyes",
                "Pierre Sermanet",
                "Nicolas Sievers",
                "Clayton Tan",
                "Alexander Toshev",
                "Vincent Vanhoucke",
                "Fei Xia",
                "Ted Xiao",
                "Peng Xu",
                "Sichun Xu",
                "Mengyuan Yan",
                "Andy Zeng"
            ],
            "title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "venue": "arXiv preprint arxiv:2204.01691,",
            "year": 2022
        },
        {
            "authors": [
                "Zhou",
                "Slav Petrov",
                "Yonghui Wu"
            ],
            "title": "Palm 2 technical report",
            "venue": "arXiv preprint arXiv:2305.10403,",
            "year": 2023
        },
        {
            "authors": [
                "Srikar Appalaraju",
                "Bhavan Jasani",
                "Bhargava Urala Kota",
                "Yusheng Xie",
                "R. Manmatha"
            ],
            "title": "Docformer: End-to-end transformer for document understanding",
            "venue": "In International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Austin",
                "Augustus Odena",
                "Maxwell Nye",
                "Maarten Bosma",
                "Henryk Michalewski",
                "David Dohan",
                "Ellen Jiang",
                "Carrie Cai",
                "Michael Terry",
                "Quoc Le",
                "Charles Sutton"
            ],
            "title": "Program synthesis with large language models",
            "venue": "arXiv preprint arXiv:2108.07732,",
            "year": 2021
        },
        {
            "authors": [
                "Shiori Sagawa",
                "Keshav Santhanam",
                "Andy Shih",
                "Krishnan Srinivasan",
                "Alex Tamkin",
                "Rohan Taori",
                "Armin W. Thomas",
                "Florian Tram\u00e8r",
                "Rose E. Wang",
                "William Wang",
                "Bohan Wu",
                "Jiajun Wu",
                "Yuhuai Wu",
                "Sang Michael Xie",
                "Michihiro Yasunaga",
                "Jiaxuan You",
                "Matei Zaharia",
                "Michael Zhang",
                "Tianyi Zhang",
                "Xikun Zhang",
                "Yuhui Zhang",
                "Lucia Zheng",
                "Kaitlyn Zhou",
                "Percy Liang"
            ],
            "title": "On the opportunities and risks of foundation models",
            "venue": "arXiv preprint arXiv:2108.07258,",
            "year": 2021
        },
        {
            "authors": [
                "Pertsch",
                "Jornell Quiambao",
                "Kanishka Rao",
                "Michael Ryoo",
                "Grecia Salazar",
                "Pannag Sanketi",
                "Kevin Sayed",
                "Jaspiar Singh",
                "Sumedh Sontakke",
                "Austin Stone",
                "Clayton Tan",
                "Huong Tran",
                "Vincent Vanhoucke",
                "Steve Vega",
                "Quan Vuong",
                "Fei Xia",
                "Ted Xiao",
                "Peng Xu",
                "Sichun Xu",
                "Tianhe Yu",
                "Brianna Zitkovich"
            ],
            "title": "Rt-1: Robotics transformer for real-world control at scale",
            "venue": "arXiv preprint arXiv:2212.06817,",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "arXiv preprint arXiv:2005.14165,",
            "year": 2020
        },
        {
            "authors": [
                "Andrea Burns",
                "Deniz Arsan",
                "Sanjna Agrawal",
                "Ranjitha Kumar",
                "Kate Saenko",
                "Bryan A. Plummer"
            ],
            "title": "A dataset for interactive vision-language navigation with unknown command feasibility",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "arXiv preprint arXiv:2104.14294,",
            "year": 2021
        },
        {
            "authors": [
                "Lili Chen",
                "Kevin Lu",
                "Aravind Rajeswaran",
                "Kimin Lee",
                "Aditya Grover",
                "Michael Laskin",
                "Pieter Abbeel",
                "Aravind Srinivas",
                "Igor Mordatch"
            ],
            "title": "Decision transformer: Reinforcement learning via sequence modeling",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Nikolas Tezak",
                "Jie Tang",
                "Igor Babuschkin",
                "Suchir Balaji",
                "Shantanu Jain",
                "William Saunders",
                "Christopher Hesse",
                "Andrew N. Carr",
                "Jan Leike",
                "Josh Achiam",
                "Vedant Misra",
                "Evan Morikawa",
                "Alec Radford",
                "Matthew Knight",
                "Miles Brundage",
                "Mira Murati",
                "Katie Mayer",
                "Peter Welinder",
                "Bob McGrew",
                "Dario Amodei",
                "Sam McCandlish",
                "Ilya Sutskever",
                "Wojciech Zaremba"
            ],
            "title": "Evaluating large language models trained on code",
            "venue": "arXiv preprint arXiv:2107.03374,",
            "year": 2021
        },
        {
            "authors": [
                "Tao Chen",
                "Jie Xu",
                "Pulkit Agrawal"
            ],
            "title": "A system for general in-hand object re-orientation",
            "venue": "In Conference on Robot Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Xingyu Chen",
                "Zihan Zhao",
                "Lu Chen",
                "JiaBao Ji",
                "Danyang Zhang",
                "Ao Luo",
                "Yuxuan Xiong",
                "Kai Yu"
            ],
            "title": "WebSRC: A dataset for web-based structural reading comprehension",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Diaz",
                "Orhan Firat",
                "Michele Catasta",
                "Jason Wei",
                "Kathy Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel. Palm"
            ],
            "title": "Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Wenliang Dai",
                "Junnan Li",
                "Dongxu Li",
                "Anthony Meng Huat Tiong",
                "Junqi Zhao",
                "Weisheng Wang",
                "Boyang Li",
                "Pascale Fung",
                "Steven Hoi"
            ],
            "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning",
            "venue": "arXiv preprint arXiv:2305.06500,",
            "year": 2023
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In Conference on Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Xiang Deng",
                "Yu Gu",
                "Boyuan Zheng",
                "Shijie Chen",
                "Samuel Stevens",
                "Boshi Wang",
                "Huan Sun",
                "Yu Su"
            ],
            "title": "Mind2web: Towards a generalist agent for the web",
            "venue": "arXiv preprint arXiv:2306.06070,",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2019
        },
        {
            "authors": [
                "Oscar Diaz",
                "Itziar Otaduy",
                "Gorka Puente"
            ],
            "title": "User-driven automation of web form filling",
            "venue": "In International Conference on Web Engineering,",
            "year": 2013
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Linxi Fan",
                "Guanzhi Wang",
                "Yunfan Jiang",
                "Ajay Mandlekar",
                "Yuncong Yang",
                "Haoyi Zhu",
                "Andrew Tang",
                "De-An Huang",
                "Yuke Zhu",
                "Anima Anandkumar"
            ],
            "title": "Minedojo: Building open-ended embodied agents with internet-scale knowledge",
            "venue": "arXiv preprint arXiv:2206.08853,",
            "year": 2022
        },
        {
            "authors": [
                "Hiroki Furuta",
                "Yusuke Iwasawa",
                "Yutaka Matsuo",
                "Shixiang Shane Gu"
            ],
            "title": "A system for morphology-task generalization via unified representation and behavior distillation",
            "venue": "arXiv preprint arXiv:2211.14296,",
            "year": 2022
        },
        {
            "authors": [
                "Hiroki Furuta",
                "Yutaka Matsuo",
                "Shixiang Shane Gu"
            ],
            "title": "Generalized decision transformer for offline hindsight information matching",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Dibya Ghosh",
                "Abhishek Gupta",
                "Ashwin Reddy",
                "Justin Fu",
                "Coline Manon Devin",
                "Benjamin Eysenbach",
                "Sergey Levine"
            ],
            "title": "Learning to reach goals via iterated supervised learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Shixiang Shane Gu",
                "Manfred Diaz",
                "Daniel C. Freeman",
                "Hiroki Furuta",
                "Seyed Kamyar Seyed Ghasemipour",
                "Anton Raichuk",
                "Byron David",
                "Erik Frey",
                "Erwin Coumans",
                "Olivier Bachem"
            ],
            "title": "Braxlines: Fast and interactive toolkit for rl-driven behavior engineering beyond reward maximization",
            "venue": "arXiv preprint arXiv:2110.04686,",
            "year": 2021
        },
        {
            "authors": [
                "Xiuye Gu",
                "Tsung-Yi Lin",
                "Weicheng Kuo",
                "Yin Cui"
            ],
            "title": "Open-vocabulary object detection via vision and language knowledge distillation",
            "venue": "arXiv preprint arxiv:2104.13921,",
            "year": 2021
        },
        {
            "authors": [
                "Izzeddin Gur",
                "Ulrich Rueckert",
                "Aleksandra Faust",
                "Dilek Hakkani-Tur"
            ],
            "title": "Learning to navigate the web",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Izzeddin Gur",
                "Natasha Jaques",
                "Yingjie Miao",
                "Jongwook Choi",
                "Manoj Tiwari",
                "Honglak Lee",
                "Aleksandra Faust"
            ],
            "title": "Environment generation for zero-shot compositional reinforcement learning",
            "venue": "In Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Izzeddin Gur",
                "Ofir Nachum",
                "Yingjie Miao",
                "Mustafa Safdari",
                "Austin Huang",
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Noah Fiedel",
                "Aleksandra Faust"
            ],
            "title": "Understanding html with large language models",
            "venue": "arXiv preprint arxiv:2210.03945,",
            "year": 2022
        },
        {
            "authors": [
                "Izzeddin Gur",
                "Hiroki Furuta",
                "Austin Huang",
                "Mustafa Safdari",
                "Yutaka Matsuo",
                "Douglas Eck",
                "Aleksandra Faust"
            ],
            "title": "A real-world webagent with planning, long context understanding, and program synthesis",
            "venue": "arXiv preprint arxiv:2307.12856,",
            "year": 2023
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "arXiv preprint arXiv:2111.06377,",
            "year": 2021
        },
        {
            "authors": [
                "Zecheng He",
                "Srinivas Sunkara",
                "Xiaoxue Zang",
                "Ying Xu",
                "Lijuan Liu",
                "Nevan Wichers",
                "Gabriel Schubiner",
                "Ruby Lee",
                "Jindong Chen",
                "Blaise Ag\u00fcera y Arcas"
            ],
            "title": "Actionbert: Leveraging user actions for semantic understanding of user interfaces",
            "venue": "arXiv preprint arXiv:2012.12350,",
            "year": 2020
        },
        {
            "authors": [
                "Jordan Hoffmann",
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Elena Buchatskaya",
                "Trevor Cai",
                "Eliza Rutherford",
                "Diego de Las Casas",
                "Lisa Anne Hendricks",
                "Johannes Welbl",
                "Aidan Clark",
                "Tom Hennigan",
                "Eric Noland",
                "Katie Millican",
                "George van den Driessche",
                "Bogdan Damoc",
                "Aurelia Guy",
                "Simon Osindero",
                "Karen Simonyan",
                "Erich Elsen",
                "Jack W. Rae",
                "Oriol Vinyals",
                "Laurent Sifre"
            ],
            "title": "Training compute-optimal large language models",
            "venue": "arXiv preprint arXiv:2203.15556,",
            "year": 2022
        },
        {
            "authors": [
                "Wenlong Huang",
                "Pieter Abbeel",
                "Deepak Pathak",
                "Igor Mordatch"
            ],
            "title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "venue": "arXiv preprint arXiv:2201.07207,",
            "year": 2022
        },
        {
            "authors": [
                "Wenlong Huang",
                "Fei Xia",
                "Ted Xiao",
                "Harris Chan",
                "Jacky Liang",
                "Pete Florence",
                "Andy Zeng",
                "Jonathan Tompson",
                "Igor Mordatch",
                "Yevgen Chebotar",
                "Pierre Sermanet",
                "Noah Brown",
                "Tomas Jackson",
                "Linda Luu",
                "Sergey Levine",
                "Karol Hausman",
                "Brian Ichter"
            ],
            "title": "Inner monologue: Embodied reasoning through planning with language models",
            "venue": "arXiv preprint arxiv:2207.05608,",
            "year": 2022
        },
        {
            "authors": [
                "Peter C Humphreys",
                "David Raposo",
                "Toby Pohlen",
                "Gregory Thornton",
                "Rachita Chhaparia",
                "Alistair Muldal",
                "Josh Abramson",
                "Petko Georgiev",
                "Alex Goldin",
                "Adam Santoro",
                "Timothy Lillicrap"
            ],
            "title": "A data-driven approach for learning to control computers",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Srinivasan Iyer",
                "Xi Victoria Lin",
                "Ramakanth Pasunuru",
                "Todor Mihaylov",
                "Daniel Simig",
                "Ping Yu",
                "Kurt Shuster",
                "Tianlu Wang",
                "Qing Liu",
                "Punit Singh Koura",
                "Xian Li",
                "Brian O\u2019Horo",
                "Gabriel Pereyra",
                "Jeff Wang",
                "Christopher Dewan",
                "Asli Celikyilmaz",
                "Luke Zettlemoyer",
                "Ves Stoyanov"
            ],
            "title": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
            "venue": "arXiv preprint arXiv:2212.12017,",
            "year": 2022
        },
        {
            "authors": [
                "Michael Janner",
                "Qiyang Li",
                "Sergey Levine"
            ],
            "title": "Offline reinforcement learning as one big sequence modeling problem",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Sheng Jia",
                "Jamie Ryan Kiros",
                "Jimmy Ba"
            ],
            "title": "DOM-q-NET: Grounded RL on structured language",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Yunfan Jiang",
                "Agrim Gupta",
                "Zichen Zhang",
                "Guanzhi Wang",
                "Yongqiang Dou",
                "Yanjun Chen",
                "Li Fei-Fei",
                "Anima Anandkumar",
                "Yuke Zhu",
                "Linxi Fan"
            ],
            "title": "Vima: General robot manipulation with multimodal prompts",
            "venue": "arXiv preprint arXiv:2210.03094,",
            "year": 2022
        },
        {
            "authors": [
                "Aishwarya Kamath",
                "Mannat Singh",
                "Yann LeCun",
                "Gabriel Synnaeve",
                "Ishan Misra",
                "Nicolas Carion"
            ],
            "title": "Mdetr \u2013 modulated detection for end-to-end multi-modal understanding",
            "venue": "arXiv preprint arXiv:2104.12763,",
            "year": 2021
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B. Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "year": 2001
        },
        {
            "authors": [
                "Geunwoo Kim",
                "Pierre Baldi",
                "Stephen McAleer"
            ],
            "title": "Language models can solve computer",
            "venue": "tasks. arXiv preprint arxiv:2303.17491,",
            "year": 2023
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "In Advances In Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Taku Kudo",
                "John Richardson"
            ],
            "title": "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
            "venue": "arXiv preprint arXiv:1808.06226,",
            "year": 2018
        },
        {
            "authors": [
                "Kenton Lee",
                "Mandar Joshi",
                "Iulia Turc",
                "Hexiang Hu",
                "Fangyu Liu",
                "Julian Eisenschlos",
                "Urvashi Khandelwal",
                "Peter Shaw",
                "Ming-Wei Chang",
                "Kristina Toutanova"
            ],
            "title": "Pix2struct: Screenshot parsing as pretraining for visual language understanding",
            "venue": "arXiv preprint arXiv:2210.03347,",
            "year": 2022
        },
        {
            "authors": [
                "Kuang-Huei Lee",
                "Ofir Nachum",
                "Mengjiao Yang",
                "Lisa Lee",
                "Daniel Freeman",
                "Winnie Xu",
                "Sergio Guadarrama",
                "Ian Fischer",
                "Eric Jang",
                "Henryk Michalewski",
                "Igor Mordatch"
            ],
            "title": "Multi-game decision transformers",
            "venue": "arXiv preprint arxiv:2205.15241,",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "year": 1910
        },
        {
            "authors": [
                "Bo Li",
                "Yuanhan Zhang",
                "Liangyu Chen",
                "Jinghao Wang",
                "Jingkang Yang",
                "Ziwei Liu"
            ],
            "title": "Otter: A multi-modal model with in-context instruction tuning",
            "venue": "arXiv preprint arXiv:2305.03726,",
            "year": 2023
        },
        {
            "authors": [
                "Chenliang Li",
                "Bin Bi",
                "Ming Yan",
                "Wei Wang",
                "Songfang Huang",
                "Fei Huang",
                "Luo Si"
            ],
            "title": "Structurallm: Structural pre-training for form understanding",
            "venue": "arXiv preprint arxiv:2105.11210,",
            "year": 2021
        },
        {
            "authors": [
                "Junlong Li",
                "Yiheng Xu",
                "Lei Cui",
                "Furu Wei"
            ],
            "title": "Markuplm: Pre-training of text and markup language for visually-rich document understanding",
            "venue": "arXiv preprint arxiv:2110.08518,",
            "year": 2021
        },
        {
            "authors": [
                "Peizhao Li",
                "Jiuxiang Gu",
                "Jason Kuen",
                "Vlad I. Morariu",
                "Handong Zhao",
                "Rajiv Jain",
                "Varun Manjunatha",
                "Hongfu Liu"
            ],
            "title": "Selfdoc: Self-supervised document representation learning",
            "venue": "In Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Shuang Li",
                "Xavier Puig",
                "Chris Paxton",
                "Yilun Du",
                "Clinton Wang",
                "Linxi Fan",
                "Tao Chen",
                "De-An Huang",
                "Ekin Aky\u00fcrek",
                "Anima Anandkumar",
                "Jacob Andreas",
                "Igor Mordatch",
                "Antonio Torralba",
                "Yuke Zhu"
            ],
            "title": "Pre-trained language models for interactive decision-making",
            "venue": "In Advances In Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yang Li",
                "Jiacong He",
                "Xin Zhou",
                "Yuan Zhang",
                "Jason Baldridge"
            ],
            "title": "Mapping natural language instructions to mobile ui action sequences",
            "venue": "In Annual Conference of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Evan Zheran Liu",
                "Kelvin Guu",
                "Panupong Pasupat",
                "Percy Liang"
            ],
            "title": "Reinforcement learning on web interfaces using workflow-guided exploration",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee"
            ],
            "title": "Visual instruction tuning",
            "venue": "arXiv preprint arXiv:2304.08485,",
            "year": 2023
        },
        {
            "authors": [
                "Jiasen Lu",
                "Christopher Clark",
                "Rowan Zellers",
                "Roozbeh Mottaghi",
                "Aniruddha Kembhavi"
            ],
            "title": "Unifiedio: A unified model for vision, language, and multi-modal tasks",
            "venue": "arXiv preprint arXiv:2206.11795,",
            "year": 2022
        },
        {
            "authors": [
                "Tatsuya Matsushima",
                "Hiroki Furuta",
                "Yutaka Matsuo",
                "Ofir Nachum",
                "Shixiang Gu"
            ],
            "title": "Deploymentefficient reinforcement learning via model-based offline optimization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Sahisnu Mazumder",
                "Oriana Riva"
            ],
            "title": "Flin: A flexible natural language interface for web navigation",
            "venue": "arXiv preprint arXiv:2010.12844,",
            "year": 2020
        },
        {
            "authors": [
                "Reiichiro Nakano",
                "Jacob Hilton",
                "Suchir Balaji",
                "Jeff Wu",
                "Long Ouyang",
                "Christina Kim",
                "Christopher Hesse",
                "Shantanu Jain",
                "Vineet Kosaraju",
                "William Saunders",
                "Xu Jiang",
                "Karl Cobbe",
                "Tyna Eloundou",
                "Gretchen Krueger",
                "Kevin Button",
                "Matthew Knight",
                "Benjamin Chess",
                "John Schulman"
            ],
            "title": "Webgpt: Browser-assisted question-answering with human feedback",
            "venue": "arXiv preprint arXiv:2112.09332,",
            "year": 2021
        },
        {
            "authors": [
                "Rodrigo Nogueira",
                "Kyunghyun Cho"
            ],
            "title": "End-to-end goal-driven web navigation",
            "venue": "In Advances In Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeff Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll L. Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray",
                "John Schulman",
                "Jacob Hilton",
                "Fraser Kelton",
                "Luke Miller",
                "Maddie Simens",
                "Amanda Askell",
                "Peter Welinder",
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "arXiv preprint arxiv:2203.02155,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "arXiv preprint arXiv:2103.00020,",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Scott Reed",
                "Konrad Zolna",
                "Emilio Parisotto",
                "Sergio Gomez Colmenarejo",
                "Alexander Novikov",
                "Gabriel Barth-Maron",
                "Mai Gimenez",
                "Yury Sulsky",
                "Jackie Kay",
                "Jost Tobias Springenberg",
                "Tom Eccles",
                "Jake Bruce",
                "Ali Razavi",
                "Ashley Edwards",
                "Nicolas Heess",
                "Yutian Chen",
                "Raia Hadsell",
                "Oriol Vinyals",
                "Mahyar Bordbar",
                "Nando de Freitas"
            ],
            "title": "A generalist agent",
            "venue": "arXiv preprint arxiv:2205.06175,",
            "year": 2022
        },
        {
            "authors": [
                "Lee",
                "Dan Garrette",
                "James Lee-Thorp",
                "Colin Raffel",
                "Noam Shazeer",
                "Marvin Ritter",
                "Maarten Bosma",
                "Alexandre Passos",
                "Jeremy Maitin-Shepard",
                "Noah Fiedel",
                "Mark Omernick",
                "Brennan Saeta",
                "Ryan Sepassi",
                "Alexander Spiridonov",
                "Joshua Newlan",
                "Andrea Gesmundo"
            ],
            "title": "Scaling up models and data with t5x and seqio",
            "venue": "arXiv preprint arXiv:2203.17189,",
            "year": 2022
        },
        {
            "authors": [
                "Dhruv Shah",
                "Blazej Osinski",
                "Brian Ichter",
                "Sergey Levine"
            ],
            "title": "Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action",
            "venue": "In Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Tianlin Shi",
                "Andrej Karpathy",
                "Linxi Fan",
                "Jonathan Hernandez",
                "Percy Liang"
            ],
            "title": "World of bits: An open-domain platform for web-based agents",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Mohammad Shoeybi",
                "Mostofa Patwary",
                "Raul Puri",
                "Patrick LeGresley",
                "Jared Casper",
                "Bryan Catanzaro"
            ],
            "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism",
            "year": 1909
        },
        {
            "authors": [
                "Mohit Shridhar",
                "Lucas Manuelli",
                "Dieter Fox"
            ],
            "title": "Perceiver-actor: A multi-task transformer for robotic manipulation",
            "venue": "In Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Maayan Shvo",
                "Zhiming Hu",
                "Rodrigo Toro Icarte",
                "Iqbal Mohomed",
                "Allan D. Jepson",
                "Sheila A. McIlraith"
            ],
            "title": "Appbuddy: Learning to accomplish tasks in mobile apps via reinforcement learning",
            "venue": "In Canadian Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Andreas Steiner",
                "Alexander Kolesnikov",
                "Xiaohua Zhai",
                "Ross Wightman",
                "Jakob Uszkoreit",
                "Lucas Beyer"
            ],
            "title": "How to train your vit? data, augmentation, and regularization in vision transformers",
            "venue": "arXiv preprint arXiv:2106.10270,",
            "year": 2022
        },
        {
            "authors": [
                "Chen Sun",
                "Abhinav Shrivastava",
                "Saurabh Singh",
                "Abhinav Gupta"
            ],
            "title": "Revisiting unreasonable effectiveness of data in deep learning era",
            "venue": "arXiv preprint arXiv:1707.02968,",
            "year": 2017
        },
        {
            "authors": [
                "Haotian Sun",
                "Yuchen Zhuang",
                "Lingkai Kong",
                "Bo Dai",
                "Chao Zhang"
            ],
            "title": "Adaplanner: Adaptive planning from feedback with language models",
            "venue": "arXiv preprint arXiv:2305.16653,",
            "year": 2023
        },
        {
            "authors": [
                "Zineng Tang",
                "Ziyi Yang",
                "Guoxin Wang",
                "Yuwei Fang",
                "Yang Liu",
                "Chenguang Zhu",
                "Michael Zeng",
                "Cha Zhang",
                "Mohit Bansal"
            ],
            "title": "Unifying vision, text, and layout for universal document processing",
            "venue": "arXiv preprint arxiv:2212.02623,",
            "year": 2022
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Vinh Q. Tran",
                "Xavier Garcia",
                "Jason Wei",
                "Xuezhi Wang",
                "Hyung Won Chung",
                "Dara Bahri",
                "Tal Schuster",
                "Huaixiu Steven Zheng",
                "Denny Zhou",
                "Neil Houlsby",
                "Donald Metzler. Ul"
            ],
            "title": "Unifying language learning paradigms",
            "venue": "arXiv preprint arXiv:2205.05131,",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Toyama",
                "Philippe Hamel",
                "Anita Gergely",
                "Gheorghe Comanici",
                "Amelia Glaese",
                "Zafarali Ahmed",
                "Tyler Jackson",
                "Shibl Mourad",
                "Doina Precup"
            ],
            "title": "Androidenv: A reinforcement learning platform for android",
            "venue": "arXiv preprint arXiv:2105.13231,",
            "year": 2021
        },
        {
            "authors": [
                "Chengyi Wang",
                "Sanyuan Chen",
                "Yu Wu",
                "Ziqiang Zhang",
                "Long Zhou",
                "Shujie Liu",
                "Zhuo Chen",
                "Yanqing Liu",
                "Huaming Wang",
                "Jinyu Li",
                "Lei He",
                "Sheng Zhao",
                "Furu Wei"
            ],
            "title": "Neural codec language models are zero-shot text to speech synthesizers",
            "venue": "arXiv preprint arXiv:2301.02111,",
            "year": 2023
        },
        {
            "authors": [
                "Jiapeng Wang",
                "Lianwen Jin",
                "Kai Ding"
            ],
            "title": "LiLT: A simple yet effective language-independent layout transformer for structured document understanding",
            "venue": "In Annual Meeting of the Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Qifan Wang",
                "Yi Fang",
                "Anirudh Ravula",
                "Fuli Feng",
                "Xiaojun Quan",
                "Dongfang Liu"
            ],
            "title": "Webformer: The web-page transformer for structure information extraction",
            "venue": "arXiv preprint arXiv:2202.00217,",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le"
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Yi Tay",
                "Rishi Bommasani",
                "Colin Raffel",
                "Barret Zoph",
                "Sebastian Borgeaud",
                "Dani Yogatama",
                "Maarten Bosma",
                "Denny Zhou",
                "Donald Metzler",
                "Ed H. Chi",
                "Tatsunori Hashimoto",
                "Oriol Vinyals",
                "Percy Liang",
                "Jeff Dean",
                "William Fedus"
            ],
            "title": "Emergent abilities of large language models",
            "venue": "arXiv preprint arXiv:2206.08853,",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint arXiv:2201.11903,",
            "year": 2022
        },
        {
            "authors": [
                "Yiheng Xu",
                "Minghao Li",
                "Lei Cui",
                "Shaohan Huang",
                "Furu Wei",
                "Ming Zhou"
            ],
            "title": "LayoutLM: Pretraining of text and layout for document image understanding",
            "venue": "arXiv preprint arxiv:1912.13318,",
            "year": 2019
        },
        {
            "authors": [
                "Shunyu Yao",
                "Howard Chen",
                "John Yang",
                "Karthik Narasimhan"
            ],
            "title": "Webshop: Towards scalable real-world web interaction with grounded language agents",
            "venue": "arXiv preprint arxiv:2207.01206,",
            "year": 2022
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik Narasimhan",
                "Yuan Cao"
            ],
            "title": "React: Synergizing reasoning and acting in language models",
            "venue": "arXiv preprint arXiv:2210.03629,",
            "year": 2022
        },
        {
            "authors": [
                "Manzil Zaheer",
                "Kenneth Marino",
                "Will Grathwohl",
                "John Schultz",
                "Wendy Shang",
                "Sheila Babayan",
                "Arun Ahuja",
                "Ishita Dasgupta",
                "Christine Kaeser-Chen",
                "Rob Fergus"
            ],
            "title": "Learning to navigate wikipedia by taking random walks",
            "venue": "arXiv preprint arXiv:2211.00177,",
            "year": 2022
        },
        {
            "authors": [
                "Andy Zeng",
                "Maria Attarian",
                "Brian Ichter",
                "Krzysztof Choromanski",
                "Adrian Wong",
                "Stefan Welker",
                "Federico Tombari",
                "Aveek Purohit",
                "Michael Ryoo",
                "Vikas Sindhwani",
                "Johnny Lee",
                "Vincent Vanhoucke",
                "Pete Florence"
            ],
            "title": "Socratic models: Composing zero-shot multimodal reasoning with language",
            "venue": "arXiv preprint arXiv:2204.00598,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaohua Zhai",
                "Alexander Kolesnikov",
                "Neil Houlsby",
                "Lucas Beyer"
            ],
            "title": "Scaling vision transformers",
            "venue": "arXiv preprint arXiv:2106.04560,",
            "year": 2022
        },
        {
            "authors": [
                "Longtao Zheng",
                "Rundong Wang",
                "Bo An"
            ],
            "title": "Synapse: Leveraging few-shot exemplars for humanlevel computer control",
            "venue": "arXiv preprint arXiv:2306.07863,",
            "year": 2023
        },
        {
            "authors": [
                "2020 Brown et al",
                "2022 Chowdhery et al",
                "2022b Wei et al",
                "Tay"
            ],
            "title": "reasoning, open-ended text generation (Radford et",
            "venue": "(Radford et al.,",
            "year": 2021
        },
        {
            "authors": [
                "2022 Chen et al",
                "2022 Reed et al",
                "2023 Liu et al",
                "2023 Dai et al",
                "Li"
            ],
            "title": "Several works also have tackled document understanding with (multimodal) transformer models (Xu et al., 2019; Li et al., 2021a;c; Appalaraju et al., 2021; Tang et al., 2022; Wang et al., 2022a;b), including markup languages such as HTML",
            "venue": "question-answering (Lu et al.,",
            "year": 2022
        },
        {
            "authors": [
                "Raffel"
            ],
            "title": "2020) as multimodal transformers, and vision transformer (Dosovitskiy et al., 2020) pre-trained with ImageNet-21K (Deng et al., 2009) as an image encoder for the visual tokens3. We especially use ViT-B16, a small-size transformer with 86 million parameters, which divides an input image into 16\u00d7 16-size patches",
            "venue": "C IMPLEMENTATION DETAILS We adopt the encoder-decoder",
            "year": 2009
        },
        {
            "authors": [
                "Liu"
            ],
            "title": "2018)) and Base-size model (about 7.3% parameters) already achieves 55.7%, surpassing previous SL state-of-the-art (48.4% by Gur et al. (2022)). This surprising efficiency might come from the sufficient inductive bias and alignment with the user intentions in instruction-finetuned LLMs, and WebGUM could fully leverage them for web automation",
            "year": 2022
        },
        {
            "authors": [
                "Gur"
            ],
            "title": "During the evaluation on MiniWoB++, we ignore the time limit due to the computational constraints",
            "year": 2022
        },
        {
            "authors": [
                "hery"
            ],
            "title": "2022) with one-shot prompt and reasoning annotations), while our model only has 3 billion parameters. IL and IL plus RL-finetuning baselines use BART (Lewis et al., 2019) model for the search policy, and BERT (Devlin et al., 2019) model for the click policy. The better performance of WebGUM proves the hypothesis that the ability of multi-step reasoning in instruction-finetuned language models works as a prior for decision making problems",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Web navigation is a class of sequential decision making problems where agents interact with web interfaces following user instructions (Shi et al., 2017; Liu et al., 2018; Gur et al., 2019). Common web navigation tasks include, for example, form filling (Diaz et al., 2013), information retrieval (Nogueira & Cho, 2016; Adolphs et al., 2022), or sending emails via a sequence of interactions with computer interface such as click or type (Figure 1). Recently, there has been a growing interest in developing agents to automate these actions and free humans from repetitive interactions (Mazumder & Riva, 2020; Li et al., 2020; Shvo et al., 2021).\nMost prior works studied web navigation problems as online RL to learn the optimal action distribution with task-specific models from scratch (Liu et al., 2018; Gur et al., 2019; Jia et al., 2019; Humphreys et al., 2022). However, online RL requires massive trials-and-errors and is often infeasible in practice since the failure in web navigation would result in undesirable consequences; for instance, wrong password may lead to account freeze, and sending email to the wrong person could be problematic in a business scene. In contrast, offline training from the static dataset enables safe development of web agents, but the performance has been sub-optimal compared to those online RL counterparts (Humphreys et al., 2022; Gur et al., 2022). Furthermore, many of the prior works was unable to leverage rich out-of-domain data for generalization, as they usually use specialized models to explicitly handle the hierarchical structures of document object model (DOM) and their dependencies, for example, with LSTM (Gur et al., 2019; 2021), self-attention (Liu et al., 2018), or GNN (Jia et al., 2019). And many of them only output a fixed set of categorical actions (Humphreys et al., 2022), which is unfavorable for truly open-ended web navigation in the real world.\n*Work done as Student Researcher at Google.\nRecently, foundation models (Bommasani et al., 2021), especially large language models (LLM) (Brown et al., 2020; Chowdhery et al., 2022), have demonstrated superior performance in commonsense, symbolic, arithmetic, and multi-step logical reasoning (Wei et al., 2022b;c; Kojima et al., 2022). These models enable transformative generalization and are capable of solving wide ranges of interactive decision making problems in the wild, including but not limited to task planning in robotics (Huang et al., 2022a;b; Shah et al., 2022; Ahn et al., 2022), board game (Meta Fundamental AI Research Diplomacy Team et al., 2022), web-based retrieval and browser crawling (Nakano et al., 2021; Yao et al., 2022b; Zaheer et al., 2022).\nIn this work, we leverage pre-trained vision and language foundation models and introduce a competitive offline learning recipe for autonomous web agents: First, we hypothesize that grounded spatial understanding is important for web navigation (Humphreys et al., 2022; Toyama et al., 2021) and thus enables our agent to observe both HTML and screenshots by combining a language model and a ViT (Dosovitskiy et al., 2020), from semantically rich multimodal tokens that perceive local and temporal information. Second, we observe that web navigation tasks are by nature instructionfollowing and thus base the language model on an instruction-tuned LLM (Wei et al., 2022a; Chung et al., 2022; Ouyang et al., 2022; Iyer et al., 2022) instead of self-supervisedly pre-trained LLMs (Raffel et al., 2020; Brown et al., 2020) as in Gur et al. (2022). Third, we collect a large multimodal corpus, with both HTML and screenshots, to finetune the language model and ViT jointly. Fourth, our model outputs action in free-form text. These four key pieces together give us a multimodal web agent, which we call Web navigation via Grounded Understanding Models or WebGUM in short. As shown in Figure 1, our model takes in a command for a web-based task via a natural language instruction (e.g., in an email client, Find Gisele\u2019s email and forward it to Siana, please.) and uses multimodal observations of the computer interface to complete the task via a sequence of computer actions.\nOn MiniWoB++ (Shi et al., 2017; Liu et al., 2018), a simulated web navigation environment benchmark, WebGUM outperforms previous best offline approaches trained with HTML inputs (Gur et al., 2022) by 45.8%, and even the best existing online RL approaches (Humphreys et al., 2022), despite being trained fully offline with much fewer experiences. WebGUM also shows better performance than humans and private-LLM-based agents (Kim et al., 2023; Sun et al., 2023). We perform extensive ablations and analysis in Section 5 to demonstrate WebGUM\u2019s advantages in (1) temporal and local multimodal perception, (2) dataset and model size scaling, (3) better HTML understanding, and (4) ability of multi-step reasoning. WebGUM grounds vision and HTML understanding on the computer interface, which is critical for solving multi-step tasks with dynamic page transitions or tasks that require visual contexts, such as booking flights (+50%), shape recognition (+22%), or crawling social media (+21%). Using instruction-finetuned language models (Chung et al., 2022), compared to using vanilla models (Raffel et al., 2020), improves the success rate on MiniWoB++ by 25%, and is especially adept at handling the unknown composition of the tasks or out-of-distribution HTML inputs synthesized with realistic perturbations. On the WebShop benchmark (Yao et al., 2022a), we demonstrate that the capability of multi-step reasoning (Wei et al., 2022c) in language models enables better performance than existing state-of-the-art few-shot PaLM-540B (Yao et al., 2022b; Chowdhery et al., 2022), while our model only has 3 billion parameters. WebGUM exhibits strong positive transfer to the real-world action prediction tasks on the Mind2Web while surpassing\nGPT-4. Finally, we collect 347K multimodal expert demonstrations on MiniWoB++, 38 times larger than the existing unimodal dataset (Liu et al., 2018), and make these publicly available for future research 1. We believe that incorporating foundation models for efficient offline training is a scalable approach towards real-world web automation where online interactions are prohibitively costly."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Web Navigation Among many proposed benchmarks for autonomous web navigation (Toyama et al., 2021; Burns et al., 2022; Yao et al., 2022a), one of the most inclusive and representative benchmark to test the capability of autonomous agents is MiniWoB++ (Shi et al., 2017; Liu et al., 2018), which consists of a set of simulated websites with various user instructions from primitive tasks to complex multi-step decision making tasks, such as sending emails or booking flights. Prior works have tried to solve this benchmark using a variety of techniques; Liu et al. (2018) and Gur et al. (2019; 2021) leverage the guidance during online RL from high-level workflow (Liu et al., 2018) or curriculum learning (Gur et al., 2019; 2021), which should be, however, designed per task, and then would not be scalable methods. Other approaches have employed supervised learning (SL) with a large million-scale dataset and following RL-finetuning (Humphreys et al., 2022), or SL with LLM-based agents (Gur et al., 2022). Offline SL agents often suffer from sub-optimal behavior, and online RL with tremendous exploratory experiences has been critical for proficient navigation on the web (Humphreys et al., 2022), which is, however, difficult to conduct in real websites as there is typically no reward signal and interactions are prohibitively costly. As shown in Appendix I, many of these approaches depend on task-specific hierarchical structures of DOM (Jia et al., 2019; He et al., 2020), tailored architectures to encode their dependencies such as LSTM (Gur et al., 2019; 2021), self-attention (Liu et al., 2018), or GNN (Jia et al., 2019), and task-dependent categorical output space (Humphreys et al., 2022), which could not handle open-ended multi-task settings similar to real world, or incorporate pre-trained models. In contrast, we remove such web-specific architectures and convert web navigation into visual question-answering format (text, image\u2192 text), which allows us to leverage pre-trained foundation models (Chung et al., 2022; Dosovitskiy et al., 2020) as rich prior knowledge on the web, and then to learn the capable agents even with offline training.\nLarge Language Models for Web Navigation Concurrently, private-LLM-based agents, such as InstructGPT (text-davinci-003) (Ouyang et al., 2022) and GPT-3.5-turbo, have achieved competitive performance to RL-fintuned models and humans by leveraging a handful of few-shot demonstrations with self-improvement (Kim et al., 2023), code generation (Sun et al., 2023), and structured prompts (Zheng et al., 2023). In contrast, WebGUM focuses on multimodality and finetuning with domain-specific data. With those, we show very competitive performance compared to PaLM-540B with only 3 billion parameters. WebGUM can also handle long HTML observation tasks, such as book-flight or choose-date-hard, where agents that rely on in-context few-shot learning tend to run out of input tokens. In addition, our models do not requires ad-hoc prompt engineering.\n1https://console.cloud.google.com/storage/browser/gresearch/webllm\nIn Appendix B, We discuss additional related works on multimodal large-scale models and foundation models for decision making."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "We formulate autonomous web navigation as a deterministic sequential decision making problem; composed of a state space S, action space A, deterministic transition function T : S \u00d7 A \u2212\u2192 S, instruction space G, reward function (or episodic success criteria) r : S \u00d7 G \u00d7 A \u2212\u2192 {0, 1}. At each time step t, the agent follows a parameterized policy conditioned on previous states and actions \u03c0 : S \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 S\ufe38 \ufe37\ufe37 \ufe38\n\u00d7t\n\u00d7A\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 A\ufe38 \ufe37\ufe37 \ufe38 \u00d7t \u00d7G \u2192 A, and transits to the next state: st+1 = T (st, at). This\nprocess continues until the agent reaches the terminal state (e.g. Submit button is clicked) or the max time step is exceeded. An episode is treated as a success if given instruction g is satisfied (i.e. r(st, g, at) = 1), and as a failure if the agent takes a invalid action or reaches a wrong terminal state.\nIn autonomous web navigation, the state st \u2208 S is a web page consisting of the raw HTML as a text sequence and a screenshot as an image. Following prior works (Shi et al., 2017; Liu et al., 2018; Gur et al., 2019; 2021), we assume the constraint action space: function(selector, text). function is either click or type, selector is an integer index that can uniquely specify the element, and text is a text input for type function.\nFigure 1 presents an example episode of MiniWoB (Shi et al., 2017), which involves multi-step decision making. To meet the given instruction, the agent clicks an email from the proper sender and types the correct receiver to forward that email. MiniWoB also has primitive behavioral tasks such as clicking buttons or entering texts. For the examples of WebShop (Yao et al., 2022a), see Appendix L."
        },
        {
            "heading": "4 WEBGUM",
            "text": ""
        },
        {
            "heading": "4.1 MULTIMODAL TRANSFORMER MODELS WITH TEMPORAL AND LOCAL PERCEPTION",
            "text": "In this work, we follow Gur et al. (2022) to use T5 (Raffel et al., 2020), an encoder-decoder architecture, for HTML-based web navigation, as its bi-directional nature could be a good fit for the tree structure of HTML and the architecture has been shown to scale well. We combine T5 with a vision transformer (ViT) (Dosovitskiy et al., 2020) for multimodality as illustrated in Figure 2. Specifically, we use the ViT to map image observations (screenshots) into image tokens. The ViT is pre-trained on ImageNet-21K classification (Deng et al., 2009). The T5 encoder then consumes both visual and HTML tokens in a unified manner, and the decoder predicts actions in text. See Appendix C for more implementation details.\nEncoding Temporal and Local Visual Tokens For language models to be aware of task temporal information and local scene recognition, the encoder considers multimodal tokens extracted from a history of patched screenshots (H = 2 steps). Temporal visual tokens contribute to predict the consistent actions in a multi-step tasks. To better extract spatial and semantic information across the local parts of websites, our ViT encodes one local token per patch rather than global one per image (i.e. CLS-token). We divide an input image into 16\u00d7 16 patches \u2013 giving a total of 14\u00d7 14 (number of patches)\u00d7 2 (temporal window) = 392 visual tokens. We crop the screenshots of MiniWoB++ to remove the yellow instruction part, and the image size becomes 160 \u00d7 160. We pad cropped images with white pixels to fit them into 224 \u00d7 224; the default input size for ViT."
        },
        {
            "heading": "4.2 INSTRUCTION-FINETUNED LARGE LANGUAGE MODELS",
            "text": "We base our language model on Flan-T5 (Chung et al., 2022), an instruction-finetuned T5, as opposed to using a vanilla pre-trained T5 as in Gur et al. (2022). Flan-T5 is finetuned with largescale instruction-following format problems and chain-of-thought examples across a variety of domains, including reasoning or programming. Considering that web navigation is inherently an instruction-following task, we hypothesize that carefully trained instruction-finetuned models could generalize well to enhance the alignment with user instruction and zero-shot reasoning in the webnavigation, interactive decision making context. For the same reason, we also hypothesize that these high-performing instruction-finetuned models enable better sample efficiency and downstream performance, and thus are well-suited for offline learning. We further finetune the Flan-T5 language model and the ViT vision encoder jointly (Figure 2) on a large corpus of instruction-following multimodal web navigation data, which we describe in Section 4.3. In Section 5, we empirically demonstrate that this instruction-finetuned recipe improves HTML comprehension, multi-step reasoning and decision making significantly."
        },
        {
            "heading": "4.3 LARGE-SCALE DATA COLLECTION WITH LANGUAGE MODEL AGENTS",
            "text": "Recent successes of foundation models are largely powered by internet-scale data (Brown et al., 2020; Radford et al., 2021; Chen et al., 2022; Wang et al., 2023). While large amount of data is critical, for web navigation domain, there is only a small public dataset for MiniWoB++, consisting of 12K episodes of human demonstration (Liu et al., 2018). Moreover, the dataset only consists of DOM observations and lacks any visual features, which might limit the fine spatial perception of the elements on the page. A large-scale multimodal dataset, including screenshots of websites, is required to build a better navigation policy at scale.\nTo collect a huge amount of multimodal behavioral dataset on MiniWoB++, we leverage the finetunedLLM policy from Gur et al. (2022), instead of human demonstrators (Liu et al., 2018; Humphreys et al., 2022). This significantly reduces the cost to construct a new dataset by leveraging the prior success of autonomous agents. We first rollout a LLM policy with 100 episodes per task, which results in a 2.8K successful episodes. Then, we finetune Flan-T5-XL models with this small dataset and run those with 10,000 episodes per task. Lastly, we collect additional 54K demonstrations\nwith Synapse (Zheng et al., 2023), a private-LLM-based agents with prompting, for the tasks where the finetuned-LLM may not complete well. Such efforts result in a multi-task dataset with 401K (347+54K) episodes including HTML and screenshots at each step. See Appendix F for more details."
        },
        {
            "heading": "5 RESULTS",
            "text": "We test our method on MiniWoB++ (Shi et al., 2017; Liu et al., 2018) with 100 evaluation episodes per task, taking the average success rate over 56 tasks taken from Gur et al. (2022). Table 1 shows that WebGUM, with a small 2.8K dataset and Base-size model (310M parameters), significantly outperforms previous offline methods for web navigation (Humphreys et al., 2022; Gur et al., 2022). While they used 2.4 million episodes or 3 billion parameters, WebGUM could improve the data and parameter efficiency to achieve superior performance in offline regime, which is realized by the problem simplification of web navigation in order to leverage temporal-local visual perception and instruction-finetuned LLMs as strong inductive bias on web environments. In addition, scaling dataset and model size, WebGUM achieves 94.2% success rate2, exceeding the previous best offline model, WebN-T5 (Gur et al., 2022), by over 45.8% and even surpassing the online RL-finetuned SoTA, CC-Net (Humphreys et al., 2022) (+0.7%), despite our fully offline training and much fewer data. Moreover, WebGUM surpasses humans and recent LLM-based agents, such as RCI (Kim et al., 2023) and AdaPlanner (Sun et al., 2023), even with GPT-4 (OpenAI, 2023). The per-task comparison and error analysis (Appendix G, L) imply that there is room for improvement in complex reasoning tasks requiring memory such as guess-number.\nIn the following sections, we perform extensive and precise ablations of WebGUM to clearly identify the source of improvement. Especially, we will demonstrate the contribution of (1) temporal and local multimodal perception (Section 5.1), architectures and pre-trained models, and (2) dataset and model size scaling (Section 5.2). We will also point out (3) better HTML comprehension (Section 5.3) and (4) capability of multi-step reasoning (Section 5.4) from instruction-finetuned LLMs. Furthermore, we prove that WebGUM can be transferable to the real-world tasks (Section 5.5)."
        },
        {
            "heading": "5.1 TEMPORAL AND LOCAL VISUAL PERCEPTION FOR GROUNDED WEB NAVIGATION",
            "text": "To verify the importance of image modality, we design three ablations: (i) input replacement, (ii) removing visual perception tokens, and (iii) employing different pre-trained ViT. We first replace image observations with completely white images, and with randomly sampled MiniWoB++ screenshots taken in the initial states at test time. For visual token and pre-trained ViT ablations, we prepare various pre-trained weights with ImageNet-21K (IN) + AugReg (Steiner et al., 2022), JFT-300M (Sun et al., 2017), or JFT-3B (Zhai et al., 2022), and with self-supervised objectives such as CLIP (Radford et al., 2021), MAE (He et al., 2021), or DINO (Caron et al., 2021), and then finetune Base-size models as a proxy of larger-size models (Hoffmann et al., 2022) to reduce the computational costs.\n2Videos are available at https://sites.google.com/view/mm-webnav/\nIn Figure 3 (left), the performance of the model with white images is comparable to the unimodal model. Presumably because the model with randomly-taken images may accidentally contain the images from the target task, WebGUM (random) slightly surpasses WebGUM (white). These results prove WebGUM successfully obtains grounded vision and HTML understanding by leveraging temporal and local fine perception. In the visual token ablation, Figure 4 (left) shows that combining both temporal and local visual tokens (66.1%) improves the performance than temporal (64.2%) or local tokens only (64.0%). Interestingly, the effects of different pre-trained ViT are marginal, compared to visual tokens, which highlights our contribution on designing suitable architecture for multimodal web navigation.\nWe also compare per-task performance gaps caused by adding vision modality to language models. Figure 3 (right) presents top-10 absolute performance improvement, suggesting WebGUM leverages visual inputs for multi-step tasks with dynamic page transitions (e.g. book-flight; +50%) or tasks requiring visual context understanding (e.g. click-shape; +22%) (see Appendix G and L)."
        },
        {
            "heading": "5.2 SCALING EFFECT IN DATASET AND MODEL SIZE",
            "text": "In this section, we show the importance of scaling up the dataset and model size in WebGUM, similar to the observations in the language and vision domain (Shoeybi et al., 2019; Kaplan et al., 2020; Rae et al., 2021; Wei et al., 2022b; Chowdhery et al., 2022). To investigate data scaling, we prepare three dataset: minimal 2.8K demonstrations, 347K demonstrations, and its 20%-size demonstrations (68K), and then finetune Flan-T5-Base with them. Figure 4 (middle) proves that increasing dataset size leads to the improvement of success rate. Because multimodal models benefit from the scaling more, the larger dataset size might be more crucial in multimodal models, which also supports our attempts to construct large-scale multimodal dataset for web navigation. Notably, Base-size WebGUM with 2.8K episodes already achieves 55.7%/66.1%, surpassing previous best SL models (49.8%/55.6% we trained with 347K episodes). This surprising data efficiency comes from the sufficient inductive bias and alignment with the user intentions in instruction-finetuned LLMs.\nIn addition to dataset size, Figure 4 (right) shows that the performance of WebGUM improves as the number of parameters in T5 model increases from Base (220M) to XXL (11B). These results also reveal that scaling the models might be more important than the dataset; the low-capacity model may cap the performance at a lower level. In contrast, decoder-only Flan-PaLM-8B only\nachieves 72.8% success, comparable to WebGUM-Large (770M), which emphasizes the advantage of encoder-decoder models in web navigation. See Appendix D for further details."
        },
        {
            "heading": "5.3 BETTER HTML COMPREHENSION FROM INSTRUCTION-FINETUNED LLMS",
            "text": "We have demonstrated that instruction-finetuned LLMs outperforms vanilla LLMs in web navigation. To analyze the effect of instruction-finetuning more precisely, we here focus on the capability of HTML understanding. Since instruction-finetuned LLMs perform well on many NLP tasks with content comprehension (Chung et al., 2022; Iyer et al., 2022), web navigation should also benefit from them. As a test bed for HTML comprehension, we investigate (1) generalization to unseen compositions of known tasks, and (2) robustness to the realistic input perturbations, which are also important challenges for the web agents to be deployed on the real-world internet. We also provide the base language model comparison on a standard HTML comprehension benchmark, WebSRC (Chen et al., 2021d) in Appendix E, where Flan-T5 achieves better EM/F1 scores than T5 after finetuning.\nFor the compositional tasks, we pick up 4 click-\u201csomething\u201d (link, button, checkboxes, dialog) tasks and make 6 combinations of these by naively stitching with 2 or 3 tasks (e.g. Figure 5). See Appendix H for further details. The results show that WebGUM with HTML and image inputs outperforms prior finetuned-LLM (Gur et al., 2022) and Synapse (Zheng et al., 2023), a SoTA LLM agent in MiniWoB++, which implies WebGUM has obtained better reading skills for web navigation and could transfer them to handle unseen HTML in compositional tasks robustly.\nTo test the robustness against input corruptions, we test three different realistic perturbations; adding extra HTML at the top or bottom of the original HTML, and adding attributes of coordinates (left, right, top, bottom; they are unrelated to solving the tasks) in each element of HTML at test time. These perturbations often happen in the real world due to the renewal or API changes, not to mention unknown websites, but rule-based pre-processing may not fully cover them. The results show that while all the methods are affected by the input corruptions to some extent, WebGUM, with both HTML and HTML plus image modalities, achieves significantly better performances than Gur et al. (2022). Notably, WebGUM outperforms prior finetuned LLM (+ 56.2% in multimodal and +33.4% in unimodal models) even when extra distracted attributes are added to HTML. They support our hypothesis: instruction-finetuning imporves HTML comprehension in LLMs, which enables the downstream agents to deal with out-of-distribution inputs or tasks robustly."
        },
        {
            "heading": "5.4 ABILITY OF MULTI-STEP REASONING AS A PRIOR FOR INTERACTIVE DECISION MAKING",
            "text": "Another notable feature in instruction-finetuned LLMs is an ability of multi-step reasoning (Chung et al., 2022). We hypothesize this reasoning capability would play an important role as a prior for interactive decision making. To decouple the evaluation of reasoning capability from visual page perception, HTML understanding, and the benchmark simulator (MiniWoB++), we extensively evaluate our WebGUM on WebShop (Yao et al., 2022a), another online-shopping website simulator with a large amount of real-world product data. Because it requires complex multi-step decisions considering previous contexts for item comparison, WebShop is suitable for investigating the capability of multi-step reasoning from\ninstruction-finetuned LLM in depth (Yao et al., 2022a;b). WebShop provides a user instruction that describes the features of item (e.g. I need a long clip-in hair extension which is natural looking, and price lower than 20.00 dollars). The agents should search, compare and choose a proper product that matches the given instruction. The performance score is evaluated by the percentage of required attributes covered by the chosen product, and if the product meets all the requirements, that episode is labeled a success. See Appendix K for further details.\nTable 2 shows that WebGUM achieves 45.0% success, significantly outperforming not only simple baselines, such as supervised imitation learning (IL), IL plus RL-finetuing and WebN-T5 (by more than 15%), but also recent prompt-based LLM agents, including ReAct (Yao et al., 2022b) (i.e. PaLM-540B (Chowdhery et al., 2022) with one-shot prompt and reasoning annotations), while our\nmodel only has 3 billion parameters. Due to the consistent reasoning and enhanced alignment with user intentions, WebGUM could compare the products with backtracking, and choose proper options (see Appendix L). Our results imply that ability of multi-step reasoning in Flan-T5 works as strong and transferable prior knowledge for downstream decision making."
        },
        {
            "heading": "5.5 STRONG TRANSFER TO REAL-WORLD ACTION PREDICTION",
            "text": "Lastly, we demonstrate the applicability of WebGUM to real-world problems. We test WebGUM on Mind2Web (Deng et al., 2023), a real-world demonstration dataset with about 2K instructions on 137 websites. In the action prediction tasks, we transfer WebGUM finetuned for MiniWoB++ with 401K dataset into real-world Mind2Web by further finetuning with the training set. WebGUM takes top-50 relevant HTML snippet candidates, instructions, and action history as inputs and outputs next actions by predicting the element id, operations (e.g. click, type), and values. Table 3 reveals that WebGUM, transferred from MiniWoB, achieves superior performance to MindAct-Large/XL and even GPT-4 in all the categories (cross-task/website/domain). Because both MindAct and WebGUM are based on Flan-T5, these results support that WebGUM exhibits strong positive transfer to real-world tasks."
        },
        {
            "heading": "6 DISCUSSION AND LIMITATION",
            "text": "Throughout the paper, we present an effective and practical methodology to simplify web navigation into offline training in order to leverage the inductive bias of web environments in instruction-finetuned LLMs. While WebGUM exhibits positive transferability to real-world problems in Mind2Web, we leave it as future work to scale multimodal foundation models into the deployment for real-world web navigation (Gur et al., 2023).\nWe collect and release a multimodal expert dataset with 347K episodes on MiniWoB++. However, this is still far from internet-scale dataset that is necessary for generalist models. Collecting behavioral data at scale by iterative data-collection and deployment (Ghosh et al., 2021; Matsushima et al., 2021; Li et al., 2022a) might be a key for practical interactive agents. Since our approach \u2013 taking raw HTML and screenshots as inputs and predicting parsable actions in text \u2013 only has minimal assumptions which constraint model architectures, it might be applicable to any advanced LLMs or open-ended situations. While WebGUM could deal with out-of-distribution compositional and perturbed tasks in a robust manner, human-level broader generalization to the diverse real websites or instructions is still a hard problem to be resolved."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "We develop Web navigation via Grounded Understanding Models (WebGUM), learning an instructionfollowing visual language foundation model for web navigation. WebGUM significantly improves the success rate on MiniWoB, compared to previous offline-trained SoTA from 48.4% to 94.2%. Our detailed ablations show that temporal and local visual tokens capture dynamic transition and visual context of the page, and that instruction-finetuned language models significantly improves web navigation performance due to the better HTML comprehension and capability of multi-step reasoning. Multi-step reasoning enables more robust generalization to out-of-distribution tasks, and outperforms PaLM-540B in WebShop. WebGUM also demonstrates strong positive transfer to real-world action prediction tasks in Mind2Web. Furthermore, we scale the existing MiniWoB dataset into multimodal 347K expert demonstrations, about 38 times larger than before. We believe that our work is an significant step towards building more capable and scalable models for autonomous web navigation."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "HF was supported by JSPS KAKENHI Grant Number JP22J21582. We thank Yusuke Iwasawa, Mustafa Safdari, Austin Huang, Heiga Zen for helpful feedback on this work, and Shunyu Yao for setting up WebShop experiments."
        },
        {
            "heading": "A BROADER IMPACTS",
            "text": "While WebGUM is evaluated only in realistic web simulators (Shi et al., 2017; Liu et al., 2018; Yao et al., 2022a), we should carefully conduct it if we deploy the autonomous web agent on the real-world Internet because of security and safety reasons. For instance, the wrong password may cause an account freeze, and emailing the wrong person is problematic in a business scene. Training with online RL may often be infeasible for this reason, while we demonstrate an alternative approach; data-driven, fully offline training by leveraging inductive bias in foundation models. Autonomous agents, well-grounded with the user\u2019s intention, should be helpful in our daily lives by reducing our burden on computer tasks. Because a part of our training corpus (54K) includes the demonstrations taken from the output of LLMs (Anil et al., 2023), we will exclude those from the dataset release and it will result in 347K episodes."
        },
        {
            "heading": "B EXTENDED RELATED WORKS",
            "text": "Foundation Models for Decision Making Recently, the ability of multi-step reasoning and inductive bias in foundation models have been leveraged to solve text-based interactive tasks via sequential decisions considering few-shot in-context examples (Ahn et al., 2022; Huang et al., 2022a;b; Zeng et al., 2022; Yao et al., 2022b; Meta Fundamental AI Research Diplomacy Team et al., 2022). Even in continuous control (Chen et al., 2021a; Janner et al., 2021; Furuta et al., 2022b; Brohan et al., 2022) or computer games (Reed et al., 2022; Lee et al., 2022b; Fan et al., 2022), high-capacity transformer models are trained with a large amount of diverse dataset via multi-task behavioral distillation (Chen et al., 2021c; Gu et al., 2021a; DeepMind Interactive Agents Team et al., 2021; Furuta et al., 2022a; Shridhar et al., 2022; Jiang et al., 2022). To build autonomous web navigation agents, we also leverage pre-trained LLM (Raffel et al., 2020; Chung et al., 2022), by finetuning with massively-curated multimodal demonstrations, and we point out that the better content comprehension and multi-step reasoning abilities, obtained through instruction-finetuning of LLM (Chung et al., 2022), are essential for the notable performance on downstream decision making aligned with human instructions.\nMultimodal Large-scale Models Large language models have demonstrated extraordinary emergent abilities on a variety of NLP tasks, such as commonsense question answering, arithmetic, logical reasoning, open-ended text generation (Radford et al., 2019; Brown et al., 2020; Chowdhery et al., 2022; Wei et al., 2022b; Tay et al., 2022), or code completion (Chen et al., 2021b; Austin et al., 2021; Li et al., 2022b). In addition, some works have investigated vision-and-language understanding to improve the accuracy of common vision-based tasks such as open-ended image/object classification (Radford et al., 2021; Gu et al., 2021b; Kamath et al., 2021), image captioning, or visual question-answering (Lu et al., 2022; Alayrac et al., 2022; Chen et al., 2022; Reed et al., 2022; Liu et al., 2023; Dai et al., 2023; Li et al., 2023). Several works also have tackled document understanding with (multimodal) transformer models (Xu et al., 2019; Li et al., 2021a;c; Appalaraju et al., 2021; Tang et al., 2022; Wang et al., 2022a;b), including markup languages such as HTML (Aghajanyan et al., 2021; 2022; Li et al., 2021b; Lee et al., 2022a) for summarization of the documents or question answering on the contents. Despite the great efforts on document understanding, these works are less connected to interactive decision making problems. Our model obtains not only a grounded understanding of websites in a multimodal manner but also the ability to decide the optimal actions to achieve given instructions in web navigation, helping multi-step decisions and visual context understanding.\nC IMPLEMENTATION DETAILS\nWe adopt the encoder-decoder models proposed by Raffel et al. (2020) as multimodal transformers, and vision transformer (Dosovitskiy et al., 2020) pre-trained with ImageNet-21K (Deng et al., 2009) as an image encoder for the visual tokens3. We especially use ViT-B16, a small-size transformer with 86 million parameters, which divides an input image into 16\u00d7 16-size patches. We use publicly\n3https://github.com/google-research/scenic\navailable checkpoints of T5 (Raffel et al., 2020)4, Flan-T5 (Chung et al., 2022)5, and T5-XL finetuned with MiniWoB++ demonstrations (Gur et al., 2022)6 for the experiments. To construct the training pipeline, we leverage SeqIO (Roberts et al., 2022) library, and use SentencePiece (Kudo & Richardson, 2018) vocabulary with 32K tokens from C4 dataset (Raffel et al., 2020) for text tokenization. The batch size for training is 128, and input sequence length is set to 4096 tokens. Due to the huge computational requirements, we run one seed to train each model throughout the paper (Humphreys et al., 2022; Gur et al., 2022). We use cloud TPU-v4, which has a 32 GiB HBM memory space for the experiments. Base-size models require 256 cores and XL-size models do 512 cores, which takes 1-2 days for finetuning."
        },
        {
            "heading": "D DETAILS ON DATASET AND MODEL SIZE SCALING",
            "text": "We here present how critical it is to scale up the dataset and model size in WebGUM. For the dataset size ablation, we use Flan-T5-Base and ViT-B16. As for both HTML and multimodal models, we could observe the scaling effects in web navigation: the larger the dataset (Table 4) and model (Table 5) size are, the higher the success rates are. Surprisingly, our approach even with only 2.8K HTML episodes (about 25% of the previous one curated by Liu et al. (2018)) and Base-size model (about 7.3% parameters) already achieves 55.7%, surpassing previous SL state-of-the-art (48.4% by Gur et al. (2022)). This surprising efficiency might come from the sufficient inductive bias and alignment with the user intentions in instruction-finetuned LLMs, and WebGUM could fully leverage them for web automation problems. The margin of improvement might be smaller than expected due to the limited capacity of transformer to obtain the grounded understanding of natural language instructions, HTML, and screenshots. In fact, the results also reveal that scaling the models might be more important than the dataset; the low-capacity model may cap the performance at a lower level.\nPre-Trained Models # of Params Modality Success Rate\n4https://github.com/google-research/t5x/blob/main/docs/models.md# t5-11-checkpoints\n5https://github.com/google-research/t5x/blob/main/docs/models.md# flan-t5-checkpoints\n6https://console.cloud.google.com/storage/browser/gresearch/webllm/ webn_t5_3b"
        },
        {
            "heading": "E WEBSRC",
            "text": "We extensively evaluate the capability of HTML comprehension in instruction-finetuned LLMs with WebSRC (Chen et al., 2021d) where the models are asked to solve contextual QA problems understanding a given HTML and its structure. Those problems are curated from real websites to include key-value extraction, entity comparison, and table understanding problems. The answer formats are either text span in HTML or binary (yes/no). Because the context length is insufficient for raw HTML, we preprocess context HTML by extracting a snippet that includes the answers in advance. We finetune both T5-XL and Flan-T5-XL with the training dataset. Table 6 shows that Flan-T5 records better HTML comprehension performance than T5, which may accelerates the web navigation performance on MiniWoB++ and Mind2Web."
        },
        {
            "heading": "F DATASET DETAILS",
            "text": "To construct a large-scale multimodal behavioral dataset on MiniWoB++, we leverage a public finetuned-LLM policy (Gur et al., 2022) trained with multi-task human demonstration dataset (Liu et al., 2018)7 as a demonstrator. We run such LLM policies with 10,000 episodes per task and only keep successful trajectories to maintain the quality of dataset, following Humphreys et al. (2022). Lastly, we collect additional 54K demonstrations with Synapse (Zheng et al., 2023)8, a private-LLMbased agents with prompting, for the tasks where finetuned-LLM may not complete well such as click-scroll-list and enter-time, and also write a scripted policy for book-flight. We use PaLM 2 (Anil et al., 2023) as a base LLM for Synapse. Such efforts result in a multi-task dataset with 401K (347K+54K) episodes including HTML and screenshots at each time step. Table 7 shows the details of our multimodal dataset (347K), consisting of HTML, screenshots, actions, and instructions at each time step.\n7https://github.com/stanfordnlp/miniwob-plusplus-demos 8https://github.com/ltzheng/synapse"
        },
        {
            "heading": "G PER-TASK PERFORMANCE OF MINIWOB++",
            "text": "In this section, we present per-task success rate on MiniWoB++ (Table 8) and absolute performance improvement by adding image modality to HTML input for WebGUM (Figure 7).\nAs for Table 8, we refer to Gur et al. (2022) and Zheng et al. (2023) for the baseline performances. We use 56 tasks as benchmark, while removing some duplicated tasks (e.g. \u201c-nodelay\u201d tasks) from 62 tasks adopted in Gur et al. (2022). During the evaluation on MiniWoB++, we ignore the time limit due to the computational constraints.\nFigure 7 presents full results of the absolute performance improvement, subtracting the success rates: (Success Rate of WebGUM(HTML+Image)) - (Success Rate of WebGUM(HTML)). The results suggest WebGUM leverages visual inputs for multi-step tasks with dynamic page transitions (e.g. book-flight or search-engine) or the tasks that require global contexts of the page (e.g. tic-tac-toe or click-shape). See Appendix L for the visualization.\nsocial-media-some click-menu use-autocomplete click-button click-button-sequence click-checkboxes click-checkboxes-soft click-checkboxes-transfer click-collapsible click-color click-dialog click-dialog-2 click-link click-option click-pie click-scroll-list click-tab click-tab-2-hard click-test click-test-2 click-widget email-inbox-forward-nl email-inbox-forward-nl-turk enter-date enter-password enter-text enter-text-dynamic enter-time focus-text focus-text-2 grid-coordinate login-user multi-layouts multi-orderings navigate-tree social-media click-collapsible-2 email-inbox email-inbox-nl-turk login-user-popup choose-date click-checkboxes-large click-tab-2 count-shape choose-date-easy search-engine use-spinner choose-date-medium tic-tac-toe guess-number identify-shape social-media-all click-shape book-flight choose-list click-shades\n20\n0\n20\n40\n60\n80\n100\nPe rfo\nrm an\nce Im\npr ov\nem en\nt ( %\n)"
        },
        {
            "heading": "H COMPOSITIONAL EVALUATION ON MINIWOB++",
            "text": "For the compositional evaluation, we pick up 4 click-\u201csomething\u201d (link, button, checkboxes, dialog) tasks and make some combinations of those by naively stitching with 2 or 3 tasks. Then, we prepare the following 6 combinational tasks,\n\u2022 click-button_click-checkboxes \u2022 click-button_click-dialog \u2022 click-button_click-link \u2022 click-link_click-button \u2022 click-link_click-button_click-dialog \u2022 click-link_click-dialog\nThese tasks should be resolved in order of the name: for instance, in click-link_click-button_click-dialog task, the agent should click the proper link, click the proper button, click the proper dialog, and then the task results in success. In click-button_click-link task, the agent should click the proper button, and then click the proper link. The instructions for compositional tasks are also simply combined among original task instructions in order of the name. This evaluation could test the ability to transfer primitive skills to control computers to solve unseen tasks. Table 9 shows the per-task average success rate among 6 combinations above. WebGUM can solve the compositional tasks much better than baselines (Gur et al., 2022; Zheng et al., 2023) ."
        },
        {
            "heading": "I COMPARISON AGAINST PRIOR WEB NAVIGATION AGENTS",
            "text": "J INPUT PERTURBATION EVALUATION ON MINIWOB++"
        },
        {
            "heading": "K EVALUATION ON WEBSHOP",
            "text": "In addition to MiniWoB++, we extensively evaluate our WebGUM on WebShop (Yao et al., 2022a) benchmark, another online-shopping websites simulator with a large amount of real-world product data. WebShop provides user instruction that describes the feature of items (e.g. I need a long clip-in hair extension which is natural looking, and price lower than 20.00 dollars). The agents should search, compare and choose a proper product that matches the given instruction. Since WebShop requires complex multi-step reasoning considering previous contexts for comparison (Yao et al., 2022a;b), we can test the capability of instruction-finetuned LLM in decision making tasks in depth. The performance score is evaluated by the percentage of required attributes covered by the chosen product (from 0 to 100), and if the product meets all the requirements, that episode is labeled a success.\nBecause WebShop does not have API to get the screenshot of rendered websites, we focus on WebGUM with text inputs, parsed from noisy HTML in the real world.9 We convert the actions from raw texts (e.g. search[a long clip-in hair extension] or click[<item id>]) to dictionary-like format (e.g. {\"action\": \"search\", \"ref\": \"a long clip-in hair extension\"} or {\"action\": \"click\", \"ref\": \"<item id>\"}), as we use in MiniWoB++, to improve the prediction accuracy. We finetune Flan-T5-XL with about 1K human demonstrations curated by Yao et al. (2022a)10, using only high-score demonstrations. The score threshold is score \u2265 50 and we have 840 episodes in total (Table 12). We construct the model input with action history, instruction, and text observation, the same as MiniWoB++ experiments. We evaluate our method with 500 user instructions in the test set.\nTable 11 shows that WebGUM achieves 45.0% success, significantly outperforming not only simple baselines, such as supervised imitation learning (IL) and IL plus RL-finetuing (by more than 15%), but also recent prompt-based LLM agents, including ReAct (Yao et al., 2022b) (i.e. PaLM-540B (Chowdhery et al., 2022) with one-shot prompt and reasoning annotations), while our model only has 3 billion parameters. IL and IL plus RL-finetuning baselines use BART (Lewis et al., 2019) model for the search policy, and BERT (Devlin et al., 2019) model for the click policy. The better performance of WebGUM proves the hypothesis that the ability of multi-step reasoning in instruction-finetuned language models works as a prior for decision making problems.\nThreshold # of Episodes Score Success Rate\n9WebShop just provides visual features of item pictures when the agents reach the product page. These features are extracted by ResNet-50 (He et al., 2016), rather than raw images or screenshots of the website. Some baseline agents (IL and IL+RL) incorporate such embeddings.\n10https://github.com/princeton-nlp/WebShop/tree/master/baseline_models/ data"
        },
        {
            "heading": "L EXAMPLE EPISODES OF WEBGUM",
            "text": ""
        }
    ],
    "title": "FINETUNED FOUNDATION MODELS",
    "year": 2024
}