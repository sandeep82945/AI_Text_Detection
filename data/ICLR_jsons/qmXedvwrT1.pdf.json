{
    "abstractText": "Diffusion models excel at generating photo-realistic images but come with significant computational costs in both training and sampling. While various techniques address these computational challenges, a less-explored issue is designing an efficient and adaptable network backbone for iterative refinement. Current options like U-Net and Vision Transformer often rely on resource-intensive deep networks and lack the flexibility needed for generating images at variable resolutions or with a smaller network than used in training. This study introduces LEGO bricks, which seamlessly integrate Local-feature Enrichment and Global-content Orchestration. These bricks can be stacked to create a test-time reconfigurable diffusion backbone, allowing selective skipping of bricks to reduce sampling costs and generate higher-resolution images than the training data. LEGO bricks enrich local regions with an MLP and transform them using a Transformer block while maintaining a consistent full-resolution image across all bricks. Experimental results demonstrate that LEGO bricks enhance training efficiency, expedite convergence, and facilitate variable-resolution image generation while maintaining strong generative performance. Moreover, LEGO significantly reduces sampling time compared to other methods, establishing it as a valuable enhancement for diffusion models.",
    "authors": [],
    "id": "SP:fd388b20a8c893f931ef6a508b5d491007254d97",
    "references": [
        {
            "authors": [
                "Michal Aharon",
                "Michael Elad",
                "Alfred Bruckstein"
            ],
            "title": "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation",
            "venue": "IEEE Transactions on signal processing,",
            "year": 2006
        },
        {
            "authors": [
                "Shinei Arakawa",
                "Hideki Tsunashima",
                "Daichi Horita",
                "Keitaro Tanaka",
                "Shigeo Morishima"
            ],
            "title": "Memory efficient diffusion probabilistic models via patch-based generation",
            "venue": "arXiv preprint arXiv:2304.07087,",
            "year": 2023
        },
        {
            "authors": [
                "Arindam Banerjee",
                "Srujana Merugu",
                "Inderjit S Dhillon",
                "Joydeep Ghosh",
                "John Lafferty"
            ],
            "title": "Clustering with Bregman divergences",
            "venue": "Journal of machine learning research,",
            "year": 2005
        },
        {
            "authors": [
                "Fan Bao",
                "Chongxuan Li",
                "Yue Cao",
                "Jun Zhu"
            ],
            "title": "All are worth words: A ViT backbone for score-based diffusion models",
            "venue": "arXiv preprint arXiv:2209.12152,",
            "year": 2022
        },
        {
            "authors": [
                "Fan Bao",
                "Shen Nie",
                "Kaiwen Xue",
                "Chongxuan Li",
                "Shi Pu",
                "Yaole Wang",
                "Gang Yue",
                "Yue Cao",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "One transformer fits all distributions in multi-modal diffusion at scale",
            "venue": "arXiv preprint arXiv:2303.06555,",
            "year": 2023
        },
        {
            "authors": [
                "Omer Bar-Tal",
                "Lior Yariv",
                "Yaron Lipman",
                "Tali Dekel"
            ],
            "title": "Multidiffusion: Fusing diffusion paths for controlled image generation",
            "venue": "arXiv preprint arXiv:2302.08113,",
            "year": 2023
        },
        {
            "authors": [
                "Yoshua Bengio",
                "Samy Bengio"
            ],
            "title": "Modeling high-dimensional discrete data with multi-layer neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 1999
        },
        {
            "authors": [
                "David M Blei",
                "Alp Kucukelbir",
                "Jon D McAuliffe"
            ],
            "title": "Variational inference: A review for statisticians",
            "venue": "Journal of the American statistical Association,",
            "year": 2017
        },
        {
            "authors": [
                "Jooyoung Choi",
                "Sungwon Kim",
                "Yonghyun Jeong",
                "Youngjune Gwon",
                "Sungroh Yoon"
            ],
            "title": "Ilvr: Conditioning method for denoising diffusion probabilistic models",
            "venue": "arXiv preprint arXiv:2108.02938,",
            "year": 2021
        },
        {
            "authors": [
                "Hyungjin Chung",
                "Byeongsu Sim",
                "Dohoon Ryu",
                "Jong Chul Ye"
            ],
            "title": "Improving diffusion models for inverse problems using manifold constraints",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "ImageNet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alex Nichol"
            ],
            "title": "Diffusion models beat GANs on image synthesis",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Xin Ding",
                "Yongwei Wang",
                "Zuheng Xu",
                "William J Welch",
                "Z Jane Wang"
            ],
            "title": "Continuous conditional generative adversarial networks: Novel empirical losses and label input mechanisms",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Zheng Ding",
                "Mengqi Zhang",
                "Jiajun Wu",
                "Zhuowen Tu"
            ],
            "title": "Patched denoising diffusion models for high-resolution image synthesis",
            "venue": "arXiv preprint arXiv:2308.01316,",
            "year": 2023
        },
        {
            "authors": [
                "Laurent Dinh",
                "David Krueger",
                "Yoshua Bengio"
            ],
            "title": "NICE: Non-linear independent components estimation",
            "venue": "International Conference in Learning Representations Workshop Track,",
            "year": 2015
        },
        {
            "authors": [
                "Laurent Dinh",
                "Jascha Sohl-Dickstein",
                "Samy Bengio"
            ],
            "title": "Density estimation using real NVP",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Bradley Efron"
            ],
            "title": "Tweedie\u2019s formula and selection bias",
            "venue": "Journal of the American Statistical Association,",
            "year": 2011
        },
        {
            "authors": [
                "Patrick Esser",
                "Robin Rombach",
                "Bjorn Ommer"
            ],
            "title": "Taming transformers for high-resolution image synthesis",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Shanghua Gao",
                "Pan Zhou",
                "Ming-Ming Cheng",
                "Shuicheng Yan"
            ],
            "title": "Masked diffusion transformer is a strong image synthesizer",
            "venue": "arXiv preprint arXiv:2303.14389,",
            "year": 2023
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "In Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Shuyang Gu",
                "Dong Chen",
                "Jianmin Bao",
                "Fang Wen",
                "Bo Zhang",
                "Dongdong Chen",
                "Lu Yuan",
                "Baining Guo"
            ],
            "title": "Vector quantized diffusion model for text-to-image synthesis",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising Diffusion Probabilistic Models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Chitwan Saharia",
                "William Chan",
                "David J Fleet",
                "Mohammad Norouzi",
                "Tim Salimans"
            ],
            "title": "Cascaded diffusion models for high fidelity image generation",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2022
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine"
            ],
            "title": "Elucidating the design space of diffusionbased generative models",
            "venue": "In Proc. NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Gwanghyun Kim",
                "Jong Chul Ye"
            ],
            "title": "DiffusionCLIP: Text-guided image manipulation using diffusion models",
            "venue": "arXiv preprint arXiv:2110.02711,",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational Bayes",
            "venue": "In International Conference on Learning Representations,",
            "year": 2014
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Tim Salimans",
                "Ben Poole",
                "Jonathan Ho"
            ],
            "title": "Variational diffusion models",
            "venue": "arXiv preprint arXiv:2107.00630,",
            "year": 2021
        },
        {
            "authors": [
                "Durk P Kingma",
                "Prafulla Dhariwal"
            ],
            "title": "Glow: Generative flow with invertible 1x1 convolutions",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Zhifeng Kong",
                "Wei Ping"
            ],
            "title": "On fast sampling of diffusion probabilistic models",
            "venue": "arXiv preprint arXiv:2106.00132,",
            "year": 2021
        },
        {
            "authors": [
                "Tuomas Kynk\u00e4\u00e4nniemi",
                "Tero Karras",
                "Samuli Laine",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Improved precision and recall metric for assessing generative models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Haoying Li",
                "Yifan Yang",
                "Meng Chang",
                "Shiqi Chen",
                "Huajun Feng",
                "Zhihai Xu",
                "Qi Li",
                "Yueting Chen"
            ],
            "title": "SRDiff: Single image super-resolution with diffusion probabilistic models. Neurocomputing, 2022a",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Li",
                "John Thickstun",
                "Ishaan Gulrajani",
                "Percy S Liang",
                "Tatsunori B Hashimoto"
            ],
            "title": "DiffusionLM improves controllable text generation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ziwei Liu",
                "Ping Luo",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "David G Lowe"
            ],
            "title": "Distinctive image features from scale-invariant keypoints",
            "venue": "International journal of computer vision,",
            "year": 2004
        },
        {
            "authors": [
                "Cheng Lu",
                "Yuhao Zhou",
                "Fan Bao",
                "Jianfei Chen",
                "Chongxuan Li",
                "Jun Zhu"
            ],
            "title": "DPM-Solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps",
            "venue": "arXiv preprint arXiv:2206.00927,",
            "year": 2022
        },
        {
            "authors": [
                "Troy Luhman",
                "Eric Luhman"
            ],
            "title": "Improving diffusion model efficiency through patching",
            "venue": "arXiv preprint arXiv:2207.04316,",
            "year": 2022
        },
        {
            "authors": [
                "Calvin Luo"
            ],
            "title": "Understanding diffusion models: A unified perspective",
            "venue": "arXiv preprint arXiv:2208.11970,",
            "year": 2022
        },
        {
            "authors": [
                "Julien Mairal",
                "Michael Elad",
                "Guillermo Sapiro"
            ],
            "title": "Sparse representation for color image restoration",
            "venue": "IEEE Transactions on image processing,",
            "year": 2007
        },
        {
            "authors": [
                "Chenlin Meng",
                "Yang Song",
                "Jiaming Song",
                "Jiajun Wu",
                "Jun-Yan Zhu",
                "Stefano Ermon"
            ],
            "title": "Sdedit: Image synthesis and editing with stochastic differential equations",
            "venue": "arXiv preprint arXiv:2108.01073,",
            "year": 2021
        },
        {
            "authors": [
                "Chenlin Meng",
                "Ruiqi Gao",
                "Diederik P Kingma",
                "Stefano Ermon",
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "On distillation of guided diffusion models",
            "venue": "arXiv preprint arXiv:2210.03142,",
            "year": 2022
        },
        {
            "authors": [
                "Krystian Mikolajczyk",
                "Cordelia Schmid"
            ],
            "title": "A performance evaluation of local descriptors",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2005
        },
        {
            "authors": [
                "Alex Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "arXiv preprint arXiv:2102.09672,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob McGrew",
                "Ilya Sutskever",
                "Mark Chen"
            ],
            "title": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models",
            "venue": "arXiv preprint arXiv:2112.10741,",
            "year": 2021
        },
        {
            "authors": [
                "Kushagra Pandey",
                "Avideep Mukherjee",
                "Piyush Rai",
                "Abhishek Kumar"
            ],
            "title": "DiffuseVAE: Efficient, controllable and high-fidelity generation from low-dimensional latents",
            "venue": "arXiv preprint arXiv:2201.00308,",
            "year": 2022
        },
        {
            "authors": [
                "William Peebles",
                "Saining Xie"
            ],
            "title": "Scalable diffusion models with transformers",
            "venue": "arXiv preprint arXiv:2212.09748,",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical textconditional image generation with CLIP latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Danilo Jimenez Rezende",
                "Shakir Mohamed",
                "Daan Wierstra"
            ],
            "title": "Stochastic backpropagation and approximate inference in deep generative models",
            "venue": "In Proceedings of the 31st International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Herbert E Robbins"
            ],
            "title": "An empirical Bayes approach to statistics",
            "venue": "In Breakthroughs in Statistics: Foundations and basic theory,",
            "year": 1992
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "O. Ronneberger",
                "P.Fischer",
                "T. Brox"
            ],
            "title": "U-Net: Convolutional networks for biomedical image segmentation",
            "venue": "URL http://lmb.informatik.uni-freiburg. de/Publications/2015/RFB15a. (available on arXiv:1505.04597 [cs.CV])",
            "year": 2015
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily Denton",
                "Seyed Kamyar Seyed Ghasemipour",
                "Raphael Gontijo-Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans",
                "Jonathan Ho",
                "David J. Fleet",
                "Mohammad Norouzi"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tim Salimans",
                "Jonathan Ho"
            ],
            "title": "Progressive distillation for fast sampling of diffusion models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Robin San-Roman",
                "Eliya Nachmani",
                "Lior Wolf"
            ],
            "title": "Noise estimation for generative diffusion models",
            "venue": "arXiv preprint arXiv:2104.02600,",
            "year": 2021
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep Unsupervised Learning Using Nonequilibrium Thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "arXiv preprint arXiv:2010.02502,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative Modeling by Estimating Gradients of the Data Distribution",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Improved Techniques for Training Score-Based Generative Models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Herve Jegou"
            ],
            "title": "DeiT III: Revenge of the ViT",
            "venue": "arXiv preprint arXiv:2204.07118,",
            "year": 2022
        },
        {
            "authors": [
                "Tinne Tuytelaars",
                "Krystian Mikolajczyk"
            ],
            "title": "Local invariant feature detectors: a survey",
            "venue": "Foundations and trends\u00ae in computer graphics and vision,",
            "year": 2008
        },
        {
            "authors": [
                "Benigno Uria",
                "Iain Murray",
                "Hugo Larochelle"
            ],
            "title": "RNADE: The real-valued neural autoregressive density-estimator",
            "venue": "In Proceedings of the 26th International Conference on Neural Information Processing Systems-Volume",
            "year": 2013
        },
        {
            "authors": [
                "Benigno Uria",
                "Marc-Alexandre C\u00f4t\u00e9",
                "Karol Gregor",
                "Iain Murray",
                "Hugo Larochelle"
            ],
            "title": "Neural autoregressive distribution estimation",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2016
        },
        {
            "authors": [
                "Arash Vahdat",
                "Karsten Kreis",
                "Jan Kautz"
            ],
            "title": "Score-based generative modeling in latent space",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "A\u00e4ron Van Den Oord",
                "Nal Kalchbrenner",
                "Koray Kavukcuoglu"
            ],
            "title": "Pixel recurrent neural networks",
            "venue": "In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48,",
            "year": 2016
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Zhendong Wang",
                "Huangjie Zheng",
                "Pengcheng He",
                "Weizhu Chen",
                "Mingyuan Zhou"
            ],
            "title": "DiffusionGAN: Training GANs with diffusion",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Zhendong Wang",
                "Yifan Jiang",
                "Huangjie Zheng",
                "Peihao Wang",
                "Pengcheng He",
                "Zhangyang Wang",
                "Weizhu Chen",
                "Mingyuan Zhou"
            ],
            "title": "Patch diffusion: Faster and more data-efficient training of diffusion models",
            "venue": "arXiv preprint arXiv:2304.12526,",
            "year": 2023
        },
        {
            "authors": [
                "Zhisheng Xiao",
                "Karsten Kreis",
                "Arash Vahdat"
            ],
            "title": "Tackling the generative learning trilemma with denoising diffusion GANs",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Xiulong Yang",
                "Sheng-Min Shih",
                "Yinlin Fu",
                "Xiaoting Zhao",
                "Shihao Ji"
            ],
            "title": "Your ViT is secretly a hybrid discriminative-generative diffusion model",
            "venue": "arXiv preprint arXiv:2208.07791,",
            "year": 2022
        },
        {
            "authors": [
                "Qinsheng Zhang",
                "Yongxin Chen"
            ],
            "title": "Fast sampling of diffusion models with exponential integrator",
            "venue": "arXiv preprint arXiv:2204.13902,",
            "year": 2022
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "year": 2018
        },
        {
            "authors": [
                "Hongkai Zheng",
                "Weili Nie",
                "Arash Vahdat",
                "Kamyar Azizzadenesheli",
                "Anima Anandkumar"
            ],
            "title": "Fast sampling of diffusion models via operator learning",
            "venue": "arXiv preprint arXiv:2211.13449,",
            "year": 2022
        },
        {
            "authors": [
                "Hongkai Zheng",
                "Weili Nie",
                "Arash Vahdat",
                "Anima Anandkumar"
            ],
            "title": "Fast training of diffusion models with masked transformers",
            "venue": "arXiv preprint arXiv:2306.09305,",
            "year": 2023
        },
        {
            "authors": [
                "Huangjie Zheng",
                "Pengcheng He",
                "Weizhu Chen",
                "Mingyuan Zhou"
            ],
            "title": "Truncated diffusion probabilistic models and diffusion-based adversarial auto-encoders",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Mingyuan Zhou",
                "Haojun Chen",
                "John Paisley",
                "Lu Ren",
                "Guillermo Sapiro",
                "Lawrence Carin"
            ],
            "title": "Nonparametric Bayesian dictionary learning for sparse image representations",
            "venue": "Advances in neural information processing systems,",
            "year": 2009
        },
        {
            "authors": [
                "Mingyuan Zhou",
                "Tianqi Chen",
                "Zhendong Wang",
                "Huangjie Zheng"
            ],
            "title": "Beta diffusion",
            "venue": "In Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Song et al",
                "Zhang",
                "Chen",
                "Lu"
            ],
            "title": "2022), and employing auxiliary models to improve training (Rombach et al., 2022",
            "venue": "(Kong & Ping,",
            "year": 2021
        },
        {
            "authors": [
                "Saharia"
            ],
            "title": "2022), and robust super-resolution and editing models (Meng et al., 2021",
            "venue": "(Goodfellow et al.,",
            "year": 2014
        },
        {
            "authors": [
                "Van Den Oord"
            ],
            "title": "models offer a versatile framework that is not bound by the traditional constraints associated with autoregressive models (Bengio",
            "venue": "Uria et al.,",
            "year": 1999
        },
        {
            "authors": [
                "Peebles",
                "Xie"
            ],
            "title": "2022), we use the vanilla DDPM (Ho et al., 2020) loss, where \u03bbt = 1, and use the linear diffusion schedule. The EDM preconditioning can be regarded as an SDE model optimized using the latter equation, where the output is parameterized with",
            "year": 2020
        },
        {
            "authors": [
                "Zhang"
            ],
            "title": "2018), to assess the similarity between sections of panoramic generations and images from our training set. For each image, we use either a grid-based cropping or a random cropping process. Our analysis involved 100 generated images from various categories. The results are presented in Table 7, including both the average and the standard deviation of the LPIPS distances. Notably, when employing the random",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Diffusion models, also known as score-based generative models, have gained significant traction in various domains thanks to their proven effectiveness in generating high-dimensional data and offering simple implementation (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; 2020; Ho et al., 2020; Song et al., 2021). Decomposing data generation into simple denoising tasks across evolving timesteps with varying noise levels, they have played a pivotal role in driving progress in a wide range of fields, with a particular emphasis on their contributions to image generation (Dhariwal & Nichol, 2021; Nichol et al., 2021; Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022).\nDespite these advancements, diffusion models continue to face a significant challenge: their substantial computational requirements, not only during training but also during sampling. In sampling, diffusion models typically demand a large number of functional evaluations (NFE) to simulate the reverse diffusion process. To tackle this challenge, various strategies have been proposed, including methods aimed at expediting sampling through approximated reverse steps (Song et al., 2020; Zhang & Chen, 2022; Lu et al., 2022; Karras et al., 2022), as well as approaches that integrate diffusion with other generative models (Zheng et al., 2022b; Pandey et al., 2022).\nAnother clear limitation is that the network used in sampling must generally be the same as that used in training, unless progressive distillation techniques are applied (Salimans & Ho, 2022; Song et al., 2023). With or without distillation, the network used for sampling typically lacks reconfigurability and would require retraining if a smaller memory or computational footprint is desired.\nBeside the computing-intensive and rigid sampling process, training diffusion models also entails a considerable number of iterations to attain convergence to an acceptable checkpoint. This requirement arises from the model\u2019s need to learn how to predict the mean of clean images conditioned on noisy inputs, which exhibit varying levels of noise. This concept can be concluded from either the Tweedie\u2019s formula (Robbins, 1992; Efron, 2011), as discussed in Luo (2022) and Chung et al. (2022), or the Bregman divergence (Banerjee et al., 2005), as illustrated in Zhou et al. (2023).\nThis mechanism imposes a significant computational burden, particularly when dealing with larger and higher dimensional datasets. To mitigate the training cost, commonly-used approaches include training the diffusion model in a lower-dimensional space and then recovering the high-dimensional data using techniques such as pre-trained auto-encoders (Rombach et al., 2022; Li et al., 2022b) or cascaded super-resolution models (Ho et al., 2022; Saharia et al., 2022). An alternative strategy involves training diffusion models at the patch-level input (Luhman & Luhman, 2022), requiring additional methods to ensure generated samples maintain a coherent global structure. These methods may include incorporating full-size images periodically (Wang et al., 2023), using encoded or downsampled semantic features (Ding et al., 2023; Arakawa et al., 2023), or employing masked transformer prediction (Zheng et al., 2023; Gao et al., 2023).\nDespite numerous efforts to expedite diffusion models, a significant but relatively less-explored challenge remains in designing an efficient and flexible network backbone for iterative refinement during both training and sampling. Historically, the dominant choice for a backbone has been based on U-Net (Ronneberger et al., 2015), but recently, an alternative backbone based on the Vision Transformer (ViT) (Dosovitskiy et al., 2021) has emerged as a compelling option. However, it\u2019s important to note that both U-Net and ViT models still require the inclusion of deep networks with a substantial number of computation-intensive convolutional layers or Transformer blocks to achieve satisfactory performance. Moreover, it\u2019s important to highlight that neither of these options readily allows for network reconfiguration during sampling, and generating images at higher resolutions than the training images frequently poses a significant challenge.\nOur primary objective is to introduce the \u201cLEGO brick,\u201d a fundamental network unit that seamlessly integrates two essential components: Local-feature Enrichment and Global-content Orchestration. These bricks can be vertically stacked to form the reconfigurable backbone of a diffusion model that introduces spatial refinement within each timestep. This versatile backbone not only enables selective skipping of LEGO bricks for reduced sampling costs but also capably generates images at resolutions significantly higher than the training set.\nA LEGO diffusion model is assembled by stacking a series of LEGO bricks, each with different input sizes. Each brick processes local patches whose dimensions are determined by its individual input size, all aimed at refining the spatial information of the full-resolution image. In the process of \u201cLocal-feature Enrichment,\u201d a specific LEGO brick takes a patch that matches its input size\n(e.g., a 16 \u00d7 16 noisy patch) along with a prediction from the preceding brick. This patch-level input is divided into non-overlapping local receptive fields (e.g., four 8\u00d7 8 patches), and these are represented using local feature vectors projected with a \u201ctoken\u201d embedding layer. Following that, \u201cGlobal-content Orchestration\u201d involves using Transformer blocks (Vaswani et al., 2017; Dosovitskiy et al., 2021), comprising multi-head attention and MLP layers, to re-aggregate these local tokenembedding feature vectors into a spatially refined output that matches the input size. This approach results in an efficient network unit achieved through MLP mixing that emphasizes local regions while maintaining a short attention span (e.g., a sequence of four token-embedding vectors). Importantly, each LEGO brick is trained using sampled input patches rather than entire images, significantly reducing the computational cost associated with the model\u2019s forward pass. This approach not only enables the flexible utilization of LEGO bricks of varying sizes during training but also facilitates a unique reconfigurable architecture during generation. Furthermore, it empowers the model with the capability to generate images at significantly higher resolutions than those present in the training dataset, as illustrated in Figure 1.\nIn summary, the stackable and skippable LEGO bricks possess several noteworthy characteristics in managing computational costs in both training and sampling, offering a flexible and test-time reconfigurable backbone, and facilitating variable-resolution generation. Our experimental evaluations, as shown in Figures 2 and 1, clearly demonstrate that LEGO strikes a good balance between efficiency and performance across challenging image benchmarks. LEGO significantly enhances training efficiency, as evidenced by reduced FLOPs, faster convergence, and shorter training times, all while maintaining a robust generative performance. These advantages extend seamlessly into the sampling phase, where LEGO achieves a noteworthy 60% reduction in sampling time compared to DiT (Peebles & Xie, 2022), while keeping the same NFE. Additionally, LEGO has the capability to generate images at much higher resolutions (e.g., 2048\u00d7 600) than training images (e.g., 256\u00d7 256)."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "While diffusion models have consistently demonstrated impressive performance, they face challenges related to substantial training and inference overheads, especially when applied to large high-resolution image datasets. Much of the research on diffusion models has focused on improving their sampling efficiency. Some studies (Song et al., 2020; Lu et al., 2022; Karras et al., 2022) have refined sampling strategies and utilized sophisticated numerical solvers to enhance efficiency. In parallel, alternative approaches (Salimans & Ho, 2022; Meng et al., 2022; Zheng et al., 2022a; Song et al., 2023) have employed surrogate networks and distillation techniques. However, these solutions primarily target sampling efficiency and have not made substantial progress in reducing the training costs associated with diffusion models.\nTo address the challenge of training efficiency, latent-space diffusion models were introduced to transform high-resolution images into a manageable, low-dimensional latent space (Vahdat et al., 2021; Rombach et al., 2022). Ho et al. (2022) introduced a cascaded diffusion model approach, consisting of a foundational diffusion model for low-resolution images, followed by super-resolution ones. Similarly, Wang et al. (2023) proposed a segmented training paradigm, conducting score matching at a patch level. While this U-Net-based approach exhibits certain resemblances to ours, wherein a set of smaller patches serves as input for the Transformer blocks, notable distinctions\nexist. Recently, Bao et al. (2022; 2023) and Peebles & Xie (2022) explored the use of Transformer architecture for diffusion models instead of the conventional U-Net architecture. Additionally, masked Transformers were utilized to improve the training convergence of diffusion models (Gao et al., 2023; Zheng et al., 2022a). However, that approach, which involves processing both masked and unmasked tokens, substantially increases the training overhead per iteration. In contrast, our method prioritizes efficiency by exclusively utilizing the unmasked tokens during training."
        },
        {
            "heading": "3 LOCAL-FEATURE ENRICHMENT AND GLOBAL-CONTENT ORCHESTRATION",
            "text": ""
        },
        {
            "heading": "3.1 MOTIVATIONS FOR CONSTRUCTING LEGO BRICKS",
            "text": "Both U-Net and ViT, which are two prominent architectural choices for diffusion models, impose a significant computational burden. The primary goal of this paper is to craft an efficient diffusionmodeling architecture that seamlessly integrates the principles of local feature enrichment, progressive spatial refinement, and iterative denoising within a unified framework. Additionally, our objective extends to a noteworthy capability: the selective skipping of LEGO bricks, which operate on different patch sizes at various time steps during the generation process. This unique test-time reconfigurable capability sets our design apart from prior approaches.\nOur envisioned LEGO bricks are intended to possess several advantageous properties: 1) Spatial Efficiency in Training: Within the ensemble, the majority of LEGO bricks are dedicated to producing local patches using computation-light MLP mixing and attention modules. This design choice leads to a significant reduction in computational Floating-Point Operations (FLOPs) and substantially shortens the overall training duration. 2) Spatial Efficiency in Sampling: During sampling, the LEGO bricks can be selectively skipped at each time step without a discernible decline in generation performance. Specifically, when t is large, indicating greater uncertainty in the global spatial structure, more patch-level LEGO bricks can be safely skipped. Conversely, when t is small, signifying a more stable global spatial structure, more full-resolution LEGO bricks can be bypassed. 3) Versatility: LEGO bricks showcase remarkable versatility, accommodating both end-to-end training and sequential training from lower to upper bricks, all while enabling generation at resolutions significantly higher than those employed during training. Furthermore, they readily support the integration of existing pre-trained models as LEGO bricks, enhancing the model\u2019s adaptability and ease of use."
        },
        {
            "heading": "3.2 TECHNICAL PRELIMINARY OF DIFFUSION-BASED GENERATIVE MODELS",
            "text": "Diffusion-based generative models employ a forward diffusion chain to gradually corrupt the data into noise, and an iterative-refinement-based reverse diffusion chain to regenerate the data. We use \u03b1t \u2208 [0, 1], a decreasing sequence, to define the variance schedule {\u03b2t = 1\u2212 \u03b1t\u03b1t\u22121 } T t=1 and denote x0 as a clean image and xt as a noisy image. A standout feature of this construction is that both the forward marginal and the reverse conditional are analytic, following the Gaussian distributions as\nq(xt |x0) = N (xt; \u221a \u03b1tx0, (1\u2212 \u03b1t)I), (1)\nq(xt\u22121 |xt,x0) = N (\u221a\u03b1t\u22121\n1\u2212\u03b1t (1\u2212 \u03b1t \u03b1t\u22121 )x0 +\n(1\u2212\u03b1t\u22121) \u221a \u03b1t (1\u2212\u03b1t) \u221a \u03b1t\u22121 xt, 1\u2212\u03b1t\u22121 1\u2212\u03b1t (1\u2212 \u03b1t \u03b1t\u22121\n)I ) . (2)\nFurthermore, maximizing the ELBO can be conducted by sampling t \u2208 {1, . . . , T} and minimizing Lt = KL(q(xt\u22121 |xt,x0)||q(xt\u22121 |xt, x\u03020 = f\u03b8(xt, t))) = SNRt\u22121\u2212SNRt2 \u2225x0 \u2212 f\u03b8(xt, t)\u2225 2 2, (3) where SNRt := \u03b1t/(1 \u2212 \u03b1t). From Equation 1, we can sample xt via reparameterization as xt = \u221a \u03b1tx0 + \u221a 1\u2212 \u03b1t\u03f5, \u03f5 \u223c N (0, I), and hence we can parameterize x\u03020 as a linear combination of xt and a noise prediction \u03f5\u03b8(xt, t) as\nx\u03020(xt, t; \u03b8) = f\u03b8(xt, t) = xt\u221a \u03b1t \u2212 \u221a 1\u2212\u03b1t\u221a \u03b1t \u03f5\u03b8(xt, t). (4)\nThus we can also express Lt in Equation 3 in terms of noise prediction as Lt = 12 ( SNRt\u22121 SNRt \u2212 1)\u2225\u03f5\u2212 \u03f5\u03b8(xt, t)\u222522. In summary, the network parameter \u03b8 can be trained by predicting either x0 (Karras et al., 2022) or the injected noise \u03f5 in forward diffusion (Ho et al., 2020; Song et al., 2021): \u03b8\u22c6 = argmin\u03b8 Et,xt,\u03f5[\u03bb \u2032 t\u2225x\u03020(xt, t; \u03b8)\u2212 x0\u222522], or \u03b8\u22c6 = argmin\u03b8 Et,xt,\u03f5[\u03bbt\u2225\u03f5\u03b8(xt, t)\u2212 \u03f5\u222522], (5) where \u03bbt, \u03bb\u2032t are both time-dependent weight coefficients, which are often chosen to be different from the ones suggested by the ELBO to enhance image generation quality (Ho et al., 2020)."
        },
        {
            "heading": "3.3 STACKABLE AND SKIPPABLE LEGO BRICKS: TRAINING AND SAMPLING",
            "text": "In our architectural design, we utilize the diffusion loss to train LEGO bricks at various levels of spatial granularity, with each level corresponding to specific spatial refinement steps. We employ Transformer blocks to capture global content at the patch level, aligning them with the specific LEGO brick in operation. Unlike U-Net, which convolves local filters across all pixel locations, our method enriches feature embeddings from non-overlapping local regions and does not include upsampling layers. In contrast to ViT, our approach operates many of its network layers at local patches of various sizes. This enables us to efficiently capture localized information and progressively aggregate it."
        },
        {
            "heading": "3.3.1 LEGO BRICK ENSEMBLE AND VERTICAL SPATIAL REFINEMENT",
            "text": "Diffusion models use their network backbone to iteratively refine image generation over the time dimension. When employing a backbone consisting of K stacked LEGO bricks, we can progressively refine spatial features at each time step along the vertical dimension. When a LEGO brick processes its patches, it independently refines each patch within the same brick. Additionally, we maintain the full-resolution image consistently at both the input and output of all bricks. This not only provides flexibility in choosing both patch sizes and locations for various bricks but also creates a test-time reconfigurable structure that can selectively skip bricks to significantly reduce sampling costs.\nDenote the original image with spatial dimensions H \u00d7W as x. For the kth LEGO brick, which operates on patches of size rh(k)\u00d7 rw(k), where rh(k) \u2264 H and rw(k) \u2264 W , we extract a set of patches of that size from x. To simplify, we assume the brick size rh(k) = rw(k) = rk, both Hrk and W rk are integers, and the image is divided into non-overlapping patches, represented as:\nx (k) (i,j) = x[(i\u2212 1)rk + 1 : irk, (j \u2212 1)rk + 1 : jrk]; i \u2208 {1, ..., H rk }, j \u2208 {1, ..., Wrk }.\nWe also denote m \u2208 [\u22121, 1]H\u00d7W as the normalized coordinates of the image pixels, and similarly, m\n(k) (i,j) as the coordinate matrix of the (i, j) th patch at the kth LEGO brick.\nMathematically, for the kth LEGO brick parameterized by \u03b8k, and denoting z (k) t,(i,j) as the (i, j) th patch extracted at time t from its output (with z(0)t,(i,j) = \u2205), the LEGO operation can be expressed as:\nz (k) t,(i,j) = f\u03b8k(x (k) t,(i,j),m (k) (i,j), z (k\u22121) t,(i,j) , t), (6)\nwhich receives patches at the same spatial locations from both the previous time step and the lower brick as its input. For now, we treat this patch-level operation-based LEGO brick as a black box and will not delve into its details until we finish describing its recursive-based ensemble and training loss. To streamline the discussion, we will omit patch indices and coordinates when appropriate.\nRecursive ensemble of LEGO bricks: The vanilla denoising diffusion step, as in Equation 4, is decomposed into K consecutive LEGO bricks, stacked from the top to the bottom as follows:\nx\u03020(xt, t; \u03b8) = z (K) t , where z (k) t = f\u03b8k(xt, z (k\u22121) t , t) for k = K, . . . , 1, (7)\nwith z(0)t := \u2205, \u03b8 := {\u03b8k}1,K , and z (k) t denoting a grid of refined patches based on the corresponding patches from the output of the lower LEGO brick at time t. We note that since each LEGO brick starts with a full-resolution image and ends with a refined full-resolution image, we have the flexibility to choose the target brick size for each LEGO brick. In our illustration shown in Figure 3, we follow a progressive-growing-based construction, where the patch size of the stacked LEGO bricks monotonically increases when moving from lower to upper bricks. Alternatively, we can impose a monotonically decreasing constraint, corresponding to progressive refinement. In this work, we explore both progressive growth and refinement as two special examples and present results for both cases. The combinatorial optimization of brick sizes for the LEGO bricks in the stack is a topic that warrants further investigation.\nTraining loss: Denoting a noise-corrupted patch at time t as x(k)t , we have the diffusion chains as\nForward : q(x(k)0:T ) = q(x (k) 0 ) \u220fT t=1 N ( x (k) t ; \u221a \u03b1t\u221a \u03b1t\u22121 x (k) t\u22121, 1\u2212 \u03b1t\u03b1t\u22121 ) , (8)\nReverse : p\u03b8(x0:T ) = p(xT ) \u220fT t=1 q(x (k) t\u22121 |x (k) t , x\u0302 (k) 0 = f\u03b8(x (k) t ,x (k\u22121) t , t)), (9)\nDenote \u03f5 as a normal noise used to corrupt the clean image. The clean image patches, denoted as x (k) 0,(i,j), are of size rk \u00d7 rk and correspond to specific spatial locations (i, j). Additionally, we have refined patches, x\u0302(k)0,(i,j), produced by the k th LEGO brick at the same locations, and x\u0302(k\u22121)0,(i,j) from the (k \u2212 1)th LEGO brick. When processing a noisy image xt at time t, we perform upward propagation through the stacked LEGO bricks to progressively refine the full image. This begins with x\u0302(0)0 = \u2205 and proceeds with refined image patches x\u0302(k)0 for k = 1, . . . ,K. As each LEGO brick operates on sampled patches, when the number of patches is limited, there may be instances where x\u0302(k\u22121)0,(i,j) contain missing values. In such cases, we replace these missing values with the corresponding pixels from x0 to ensure the needed x\u0302 (k\u22121) 0,(i,j) is present. Denote \u03bb (k) t as time- and brick-dependent weight coefficients, whose settings are described in Appendix D. With the refined image patches x\u0302(k)0,(i,j), we express the training loss over the K LEGO bricks as\nEkEt,x(k)0 ,\u03f5,(i,j) [\u03bb\n(k) t \u2225x (k) 0,(i,j) \u2212 x\u0302 (k) 0,(i,j)\u2225 2 2], x\u0302 (k) 0,(i,j) := f\u03b8k(x (k) t,(i,j), x\u0302 (k\u22121) 0,(i,j), t). (10)\nThis training scheme enables each LEGO brick to refine local patches while being guided by the noise-corrupted input and the output of the previous brick in the stack."
        },
        {
            "heading": "3.3.2 NETWORK DESIGN WITHIN THE LEGO BRICK",
            "text": "In the design of a LEGO brick, we draw inspiration from the design of the Diffusion Transformer (DiT) proposed in Peebles & Xie (2022). The DiT architecture comprises several key components: Patch Embedding Layer: Initially, the input image is tokenized into a sequence of patch embeddings; DiT Blocks: These are Transformer blocks featuring multi-head attention, zero-initialized adaptive layer norm (adaLN-zero), and MLP layers; Linear layer with adaLN: Positioned on top of the DiT blocks, this linear layer, along with adaLN, converts the sequence of embeddings back into an image. The diffusion-specific conditions, such as time embedding and class conditions, are encoded within the embeddings through adaLN in every block.\nAs illustrated in Figure 3, we employ the DiT blocks, along with the patch-embedding layer and the linear decoder to construct a LEGO brick. The input channel dimension of the patch-embedding layer is expanded to accommodate the channel-wise concatenated input [x(k)t , x\u0302 (k\u22121) 0 ,m\n(k)]. The size of the local receptive fields, denoted as \u2113k \u00d7 \u2113k, is subject to variation to achieve spatial refinement. In this paper, by default, we set it as follows: if the brick size rk \u00d7 rk is smaller than the image resolution, it is set to rk/2 \u00d7 rk/2, and if the brick size is equal to the image resolution, it is set to 2 \u00d7 2. In simpler terms, for the kth LEGO brick, when rk \u00d7 rk is smaller than W \u00d7 H (or W/8\u00d7H/8 for diffusion in the latent space), we partition each of the input patches of size rk \u00d7 rk\ninto four non-overlapping local receptive fields of size rk/2\u00d7 rk/2. These four local receptive fields are further projected by the same MLP to become a sequence of four token embeddings. Afterward, these four token embeddings are processed by the DiT blocks, which have an attention span as short as four, and decoded to produce output patches of size rk \u00d7 rk. We categorize the LEGO bricks as \u2018patch-bricks\u2019 if their brick size is smaller than the image resolution, and as \u2018image-bricks\u2019 otherwise. The patch-bricks have attention spans as short as four, saving memory and computation in both training and generation. While the image-bricks choose longer attention spans and hence do not yield memory or computation savings during training, they can be compressed with fewer DiT blocks compared with the configuration in Peebles & Xie (2022). During generation, LEGO bricks can also be selectively skipped at appropriate time steps without causing clear performance degradation, resulting in substantial savings in generation costs. For simplicity, we maintain a fixed embedding size and vary only the number of DiT blocks in each brick based on rk. Each LEGO brick may require a different capacity depending on rk. For a comprehensive configuration of the LEGO bricks, please refer to Table 5 in Appendix D.\nWe explore two spatial refinement settings and stack LEGO bricks accordingly: 1) Progressive Growth (PG): In this setup, we arrange LEGO bricks in a manner where each subsequent brick has a patch size that\u2019s four times as large as the previous one, i.e., rk = 4rk\u22121. Consequently, the patch of the current break would aggregate the features of four patches output by the previous brick, facilitating global-content orchestration. 2) Progressive Refinement (PR): By contrast, for the PR configuration, we stack LEGO bricks in reverse order compared to PG, with rk = rk\u22121/4. Here, each LEGO brick is responsible for producing a refined generation based on the content provided by the brick below it."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We present a series of experiments designed to assess the efficacy and versatility of using stacked LEGO bricks as the backbone for diffusion models. We defer the details on datasets, training settings, and evaluation metrics into Appendix D. We begin by showcasing results on both small-scale and large-scale datasets to evaluate the image generation performance using the three configurations outlined in Table 5 in Appendix D."
        },
        {
            "heading": "4.1 MAIN RESULTS",
            "text": "Training in pixel space: We begin by comparing LEGO in pixel-space training on CelebA and ImageNet with two Transformer-based state-of-the-art diffusion models (Bao et al., 2022; Peebles & Xie, 2022), which are our most relevant baselines. All images are resized to a resolution of 64\u00d7 64. We conduct experiments with two spatial refinement methods using LEGO, denoted as -PG for the progressive growth variant and -PR for the progressive refinement variant, as explained in Section 3.3.2. In both Tables 1 and 2, when compared to DiT, LEGO slightly increases the number of parameters due to the expansion of the input dimension in a channel-wise manner. However, in terms of FLOPs, we observe that LEGO incurs a significantly lower computational cost compared to both U-ViT and DiT. Despite maintaining such a low computational cost, LEGO still outperforms the competition in terms of FID performance.\nTraining in latent space: We conducted comparisons with state-of-the-art class-conditional generative models trained on ImageNet at resolutions of 256 \u00d7 256 and 512 \u00d7 512, as shown in Table 3. Our results were obtained after training with 512M images (calculated as the number of iterations\n\u00d7 batch size). When considering LEGO without classifier-free guidance (CFG), we observe that it achieves a better FID than the baseline diffusion models, striking a good balance between precision and recall. One potential explanation for this improvement is that the LEGO bricks with small patch sizes enhance patch diversity, preserving both generation fidelity and diversity.\nWith the inclusion of CFG, we notably achieved an FID of 2.05 and the best IS of 289.12 on ImageNet-256\u00d7 256. In the case of using 512\u00d7 512 resolution data, LEGO also achieved the best IS and produced a competitive FID with fewer iterated images. Additionally, as depicted in Figure 2, LEGO demonstrated superior convergence speed and required significantly less training time. For qualitative validation, we provide visualizations of randomly generated samples in Figure 1."
        },
        {
            "heading": "4.2 IMPROVING THE EFFICIENCY OF DIFFUSION MODELING WITH LEGO",
            "text": "The design of LEGO inherently facilitates the sampling process by generating images with selected LEGO bricks. Intuitively, at low-noise timesteps (i.e., when t is small), the global structure of images is already well-defined, and hence the model is desired to prioritize local details. Conversely, when images are noisier, the global-content orchestration becomes crucial to uncover global structure under high uncertainties. Therefore, for improved efficiency, we skip LEGO bricks that emphasize local details during high-noise timesteps and those that construct global structures during low-noise timesteps. To validate this design, we conduct experiments to study the generation performance when LEGO bricks are skipped in different degrees.\nFor LEGO-PG, we designate a timestep tbreak as a breakpoint. When t > tbreak, no bricks are skipped, but when t \u2264 tbreak, the top-level brick, which is the image-brick with a brick size of 64 \u00d7 64, is skipped for the remainder of the sampling process. In contrast, for PR, we set a timestep tbreak as a breakpoint. When t \u2264 T \u2212 tbreak, no bricks are skipped, but when t > T \u2212 tbreak, the top-level brick, which is the patch-break with a size of 4\u00d7 4, is skipped for the remainder of the sampling process. We evaluate the impact of the choice of tbreak on both FID scores and sampling time for LEGO-PG and LEGO-PR. The results in Figure 4 illustrate the FID scores and the time required to generate 50,000 images using 8 NVIDIA A100 GPUs. In LEGO-PG, when we skip the top-level LEGO brick, we observe a notable enhancement in sampling efficiency, primarily because this brick, responsible for processing the entire image, is computationally demanding. Nevertheless, skipping this brick during timesteps with high levels of noise also results in a trade-off with performance, as the model loses access to global content. On the other hand, for LEGO-PR, skipping the top-level patch-brick\nslightly impacts performance when tbreak is large, while the improvement in sampling efficiency is not as substantial as with LEGO-PG. Interestingly, when tbreak is chosen to be close to the halfway point of sampling, performance is preserved for both models, and significant time savings can be achieved during sampling."
        },
        {
            "heading": "4.3 LEVERAGING A PRETRAINED DIFFUSION MODEL AS A LEGO BRICK",
            "text": "Apart from the training efficiency, another advantage of the design of LEGO is any pre-trained diffusion models can be incorporated as a LEGO brick as long as they can predict x\u0302(k)0 . A natural choice is to deploy a pre-trained diffusion model as the first LEGO brick in LEGO-PR. Then, a series of LEGO bricks that refine local details can be built on top of it. To validate this concept, we conducted experiments on CIFAR-10 using LEGO-PR. We deploy an unconditional Variance Preservation (VP) diffusion model (Song et al., 2021) (implemented under the EDM codebase provided by Karras et al. (2022)) to replace the LEGO brick that deals with full images, and build a LEGO-S (4\u00d7 4) brick on the top for refinement. We train the incorporated model in two different ways: 1) VP (frozen) + LEGO-PR: The VP model is fixed and only the other LEGO bricks are trained, and 2) VP (unfrozen) + LEGO-PR: The VP model is fine-tuned along with the training of the other LEGO bricks. Table 4 shows that the combined model consistently improves the base model to obtain a better generation result, demonstrating the versatility of LEGO bricks."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We introduce the LEGO brick, a novel network unit that seamlessly integrates Local-feature Enrichment and Global-content Orchestration. These bricks can be stacked to form the backbone of diffusion models. LEGO bricks are adaptable, operating on local patches of varying sizes while maintaining the full-resolution image across all bricks. During training, they can be flexibly stacked to construct diffusion backbones of diverse capacity, and during testing, they are skippable, enabling efficient image generation with a smaller network than used in training. This design choice not only endows LEGO with exceptional efficiency during both training and sampling but also empowers it to generate images at resolutions significantly higher than training. Our extensive experiments, conducted on challenging image benchmarks such as CelebA and ImageNet, provide strong evidence of LEGO\u2019s ability to strike a compelling balance between computational efficiency and generation quality. LEGO accelerates training, expedites convergence, and consistently maintains, if not enhances, the quality of generated images. This work signifies a noteworthy advancement in addressing the challenges associated with diffusion models, making them more versatile and efficient for generating high-resolution photo-realistic images."
        },
        {
            "heading": "A ADDITIONAL DISCUSSION ON RELATED WORK",
            "text": "Diffusion models have garnered increasing attention from researchers seeking to enhance them in various ways. These efforts include combining variational loss with weighted denoising loss (Dhariwal & Nichol, 2021; Kingma et al., 2021; Karras et al., 2022), accelerating sampling speed using ODE solvers (Kong & Ping, 2021; San-Roman et al., 2021; Song et al., 2020; Zhang & Chen, 2022; Lu et al., 2022), and employing auxiliary models to improve training (Rombach et al., 2022; Li et al., 2022b; Zheng et al., 2022b; Pandey et al., 2022). Additionally, the iterative refinement property of diffusion models has inspired research in various domains, such as stable GAN training (Xiao et al., 2022; Wang et al., 2022), large-scale text-to-image models (Kim & Ye, 2021; Nichol et al., 2021; Gu et al., 2022; Saharia et al., 2022), and robust super-resolution and editing models (Meng et al., 2021; Choi et al., 2021; Li et al., 2022a), among others.\nWith rapid advancements, diffusion models have often outperformed state-of-the-art generative adversarial networks (GANs, (Goodfellow et al., 2014)). GANs typically require adversarial training and are susceptible to issues such as unstable training and mode collapse. In contrast, diffusion models offer a versatile framework that is not bound by the traditional constraints associated with autoregressive models (Bengio & Bengio, 1999; Uria et al., 2013; 2016; Van Den Oord et al., 2016), variational autoencoders (VAEs, (Kingma & Welling, 2014; Rezende et al., 2014)), or flow-based models (Dinh et al., 2015; 2017; Kingma & Dhariwal, 2018; Song et al., 2023).\nA crucial element within diffusion models revolves around the network architecture employed in their iterative refinement-based generative process. The initial breakthrough, which propelled scoreor diffusion-based generative models into the spotlight, can be attributed to the integration of the convolutional operation-based U-Net architecture (Song & Ermon, 2019; Ho et al., 2020). Originally conceived by Ronneberger et al. (2015) for biomedical image segmentation, the U-Net design incorporates both a progressive downsampling path and a progressive upsampling path, enabling it to generate images of the same dimensions as its input. In the context of diffusion models, the U-Net often undergoes further modifications, such as the incorporation of attention blocks, residual blocks, and adaptive normalization layers (Dhariwal & Nichol, 2021).\nWhile the downsampling and upsampling convolutional layers in U-Net allow it to effectively capture local structures, achieving a good understanding of global structures often requires a deep architecture, resulting in high computational complexity during both training and generation. Instead of depending on a large number of convolutional layers, a classical approach in signal and image processing involves the extraction of local feature descriptors and their application in downstream tasks (Lowe, 2004; Mikolajczyk & Schmid, 2005; Tuytelaars & Mikolajczyk, 2008). For instance, techniques like dictionary learning and sparse coding operate on the principle that each local patch can be expressed as a sparse linear combination of dictionary atoms learned from image patches. This learned patch dictionary can then be used to enrich all overlapping patches for various image-processing tasks, such as denoising and inpainting (Aharon et al., 2006; Mairal et al., 2007; Zhou et al., 2009).\nThe emergence of ViT in image classification (Dosovitskiy et al., 2021), which integrates extracting feature embeddings from non-overlapping local patches with the global attention mechanism of the Transformer, marks a resurgence of this classical local-feature-based concept. ViT has not only competed effectively with state-of-the-art convolution-based networks in both image classification (Touvron et al., 2022) and generation (Esser et al., 2021), but has also been adapted to replace the U-Net in diffusion models (Yang et al., 2022; Bao et al., 2022; Peebles & Xie, 2022; Gao et al., 2023), achieving state-of-the-art image generation performance. This underscores the notion that the convolutional architecture can be supplanted by the enrichment of local features and their orchestration through global attention. ViT-based models start by enriching patch-level representations and then utilize a sequence of Transformer blocks to capture dependencies among local patches. They often demand smaller patch sizes to capture fine-grained local details and a large number of attention layers to capture global dependencies. This necessitates lengthy sequence computations and deep networks to achieve optimal results, thereby incurring significant computational costs.\nTo bridge the gap between the convolutional U-Nets and the Transformer architecture, LEGO tackles the diffusion modeling from a view of local-feature enrichment and global-content orchestration. Inspired by convolutional networks, LEGO enrich the local-features in both local and global scale. However, different from a series of downsampling and upsampling stages used in the U-Nets, LEGO leverages varied patch decomposition methods. This distinction in processing strategy results in\ndiverse feature representations. To achieve a global-content orchestration, LEGO deployed a selfattention, whose length varies depending on the \u2019LEGO brick\u2019. In some cases, attention is confined to specific image patches defined by the brick size, while in some other bricks, it extends over the entire image. This selective mechanism allows for more focused and efficient feature aggregation."
        },
        {
            "heading": "B ADDITIONAL DETAILS ON TECHNICAL PRELIMINARY",
            "text": "The reverse diffusion chain can be optimized by maximizing the evidence lower bound (ELBO) (Blei et al., 2017) of a variational autoencoder (Kingma & Welling, 2014), using a hierarchical prior with T stochastic layers (Ho et al., 2020; Song et al., 2020; 2021; Kingma et al., 2021). Let\u2019s represent the data distribution as q(x0) and the generative prior as p(xT ). The forward and reverse diffusion processes discretized into T steps can be expressed as:\nForward : q(x0:T ) = q(x0) \u220fT t=1 q(xt |xt\u22121) = q(x0) \u220fT t=1 N ( xt; \u221a \u03b1t\u221a \u03b1t\u22121 xt\u22121, 1\u2212 \u03b1t\u03b1t\u22121 ) ,\n(11) Reverse : p\u03b8(x0:T ) = p(xT ) \u220fT t=1 p\u03b8(xt\u22121 |xt) = p(xT ) \u220fT\nt=1 q(xt\u22121 |xt, x\u03020 = f\u03b8(xt, t)), (12)\nwhere q(xt\u22121 |xt,x0) is the conditional posterior of the forward diffusion chain that is also used to construct the reverse diffusion chain."
        },
        {
            "heading": "C LIMITATIONS AND FUTURE WORK",
            "text": "The current work exhibits several limitations. Firstly, we have not explored the application of LEGO bricks for text-guided image generation. Secondly, in generating class-conditional images at resolutions much higher than the training data, categories with low diversity may produce results that lack photorealism. Thirdly, we have primarily focused on stacking LEGO bricks in either a progressive-growth or progressive-refinement manner, leaving other stacking strategies unexplored. Fourthly, our selection of which LEGO bricks to skip at different time steps during generation relies on heuristic criteria, and there is room for developing more principled skipping strategies.\nWhile the use of LEGO bricks has led to considerable computational savings in both training and generation, it still requires state-of-the-art GPU resources that are very expensive to run and hence typically beyond the reach of budget-sensitive projects. For example, training LEGO diffusion with 512M images on ImageNet 512\u00d7 512 took 32 NVIDIA A100 GPUs with about 14 days. It\u2019s important to note that like other diffusion-based generative models, LEGO diffusion can potentially be used for harmful purposes when trained on inappropriate image datasets. Addressing this concern is a broader issue in diffusion models, and LEGO bricks do not appear to inherently include mechanisms to mitigate such risks."
        },
        {
            "heading": "D EXPERIMENTAL SETTINGS",
            "text": "Datasets: We conduct experiments using the CelebA dataset (Liu et al., 2015), which comprises 162,770 images of human faces, and the ImageNet dataset (Deng et al., 2009), consisting of 1.3 million natural images categorized into 1,000 classes. For data pre-processing, we follow the convention to first apply a center cropping and then resize it to 64\u00d7 64 (Song et al., 2021) on CelebA; on ImageNet, we pre-process images at three different resolutions: 64\u00d7 64, 256\u00d7 256, and 512\u00d7 512. For images of resolution 64\u00d7 64, we directly train the model in the pixel space. For higher-resolution images, we follow Rombach et al. (2022) to first encode the images into a latent space with a down-sampling factor of 8 and then train a diffusion model on that latent space. Specifically, we employ the EMA checkpoint of autoencoder from Stable Diffusion1 to pre-process the images of size 256\u00d7 256\u00d7 3 into 32\u00d7 32\u00d7 4, and those of size 512\u00d7 512\u00d7 3 into 64\u00d7 64\u00d7 4. Training settings: For our LEGO model backbones, we employ EDM (Karras et al., 2022) preconditioning on pixel-space and iDDPM (Nichol & Dhariwal, 2021; Dhariwal & Nichol, 2021) on\n1https://huggingface.co/stabilityai/sd-vae-ft-mse-original\nlatent-space data, strictly adhering to the diffusion designs prescribed by these respective preconditioning strategies.\nDuring training, we employ the AdamW optimizer (Loshchilov & Hutter, 2018) for all experiments. For DDPM preconditioning, we use a fixed learning rate of 1 \u00d7 10\u22124; for EDM preconditioning, we adopt the proposed learning rate warmup strategy, where the learning rate increases linearly to 1 \u00d7 10\u22124 until iterated with 10k images. The batch size per GPU is set at 64 for all experiments. When trained on CelebA, we finish the training when the model is iterated with 2M images, and when trained on ImageNet, including 64\u00d7 64, 256\u00d7 256, and 512\u00d7 512 resolution, we finish the training when the model is iterated with 512M images.\nEvaluation: For performance evaluation, we primarily use FLOPs to measure the generation cost and generate 50,000 random images to compute the FID score as a performance metric. Additionally, we include the metrics (Ding et al., 2022; Kynk\u00e4\u00e4nniemi et al., 2019) employed by Peebles & Xie (2022) for a comprehensive comparison. In the context of image generation, we employ the Heun sampler when using EDM preconditioning (Karras et al., 2022) and the DDPM sampler when utilizing iDDPM preconditioning (Nichol & Dhariwal, 2021).\nSpecifically, we employed the same suite, namely the ADM Tensorflow evaluation suite implemented in Dhariwal & Nichol (2021)2. We also utilized their pre-extracted features for the reference batch to ensure that our evaluations are rigorous and reliable. Additionally, we tested the evaluation with scripts and reference batch provided in EDM (Karras et al., 2022) 3. We observed that the results from both evaluation suites were consistent, showing no significant differences (up to two decimal places).\nDiffusion settings We deploy our LEGO model backbones with two commonly-used parameterization, namely DDPM (Nichol & Dhariwal, 2021; Dhariwal & Nichol, 2021) and EDM (Karras et al., 2022). Recall the training loss of diffusing models in Equation 5:\n\u03b8\u22c6 = argmin\u03b8 Et,xt,\u03f5[\u03bbt\u2225\u03f5\u03b8(xt, t)\u2212 \u03f5\u222522], or \u03b8\u22c6 = argmin\u03b8 Et,xt,\u03f5[\u03bb\u2032t\u2225x\u03020(xt, t; \u03b8)\u2212 x0\u222522]. The DDPM preconditioning follows the former equation, training the model to predict the noise injected to xt. Following Peebles & Xie (2022), we use the vanilla DDPM (Ho et al., 2020) loss, where \u03bbt = 1, and use the linear diffusion schedule. The EDM preconditioning can be regarded as an SDE model optimized using the latter equation, where the output is parameterized with:\nx\u03020(xt, t; \u03b8) = f\u03b8(xt, t; \u03b8) = cskip (\u03c3)xt + cout (\u03c3)\u03f5\u03b8 (cin (\u03c3)xt; cnoise (\u03c3)) ,\nwhere \u03c3 depends on the noise-level of the current timestep. cskip (\u03c3), cin , cout , and cnoise are 4 hyper-parameters depending on \u03c3. We strictly follow EDM to set these parameters and use the EDM schedule. More details regarding the diffusion settings can be found in Ho et al. (2020) and Karras et al. (2022).\nDuring generation, for DDPM preconditioning, we use the diffusion sampler for 250 steps with a uniform stride, proposed in Nichol & Dhariwal (2021). For EDM preconditioning, we use the 2nd order Heun sampler in Karras et al. (2022). On CelebA, we sample 75 steps with the deterministic sampler. On Imagenet, we sample for 256 steps with the stochastic sampler. The stochasticity-related hyper-parameters are set as: Schurn = 10, Smin = 0.05, Smax = 20, and Snoise = 1.003.\nModel design We provide an overview of the architecture configurations for the LEGO models with different capacities in Tables 5 and 6. Since Table 6 only differs from Table 5 in the brick sizes and local receptive field sizes, we will focus on explaining Table 5 in detail. Table 5 consists of three LEGO bricks, with two of them being patch-level bricks and one being an image-level brick. When using the PG-based spatial refinement approach, we stack the bricks from the bottom to the top following the order: 4\u00d7 4 \u2192 16\u00d7 16 \u2192 64\u00d7 64. In other words, the 4\u00d7 4 patch-level brick is placed at the bottom, followed by the 16\u00d7 16 patch-level brick, and then the 64\u00d7 64 image-level brick. Conversely, for the PR-based spatial refinement approach, we stack the bricks from the bottom to the top following the order: 64\u00d7 64 \u2192 16\u00d7 16 \u2192 4\u00d7 4. Adhering to conventional modeling techniques, we\u2019ve outlined three distinct configurations \u2014 Small (LEGO-S), Large (LEGO-L), and XLarge (LEGO-XL) \u2014 used in our main experiments to cater to varying model capacities. Across\n2https://github.com/openai/guided-diffusion/tree/main/evaluations 3https://github.com/nvlabs/edm\nbricks, the main differences are in token embedding and the number of attention heads, which revolves around the feature size of a local receptive field. Each brick specifies its patch size rk \u00d7 rk, local receptive field size \u2113k \u00d7 \u2113k, and embedding dimension (number of channels) dk. The number of tokens (attention span) is determined as (rk/\u2113k)2. In each brick, the configurations for LEGO-S, LEGO-L, and LEGO-XL also differ in terms of the number of attention heads and the number of DiT blocks used in the current brick to ensure scalability and adaptability across the architecture. To maintain generation coherence, we set the local-receptive-field size of the image-level brick as 2\u00d7 2, which effectively turns it into a DiT with a long attention span, albeit with significantly fewer layers compared to standard DiT models. Moreover, the image-level brick can be safely skipped at appropriate time steps during generation. We find the proposed configuration strikes a favorable balance between computational efficiency and performance, which is comprehensively studied in our ablation studies on the effects of LEGO bricks in Appendix E.2. In training, LEGO patch-brick can operate on sampled patches to further save memory and computation. In our experiments on CelebA and ImageNet datasets, 50% and 75% of all non-overlapping patches are used in the training of patch-bricks, respectively. As noted in Section 3.2, there are instances where x\u0302(k\u22121)0,(i,j) contains missing values as some patches are not sampled in the preceding brick. In such cases, we replace these missing values with the corresponding pixels from x0 to ensure that the required x\u0302 (k\u22121) 0,(i,j) is complete.\nDuring training, there are a couple of viable approaches. One method is to stack all bricks and train them end-to-end. Alternatively, a brick-by-brick training methodology can be deployed. In our practical studies, we did not find clear performance differences within these choices."
        },
        {
            "heading": "E ADDITIONAL EXPERIMENT RESULTS",
            "text": "E.1 SKIPPING LEGO BRICKS IN SPATIAL REFINEMENT: COMPARING PG AND PR STRATEGIES\nHere we provide the qualitative results to support the analysis of the brick skipping mechanism of LEGO shown in section 4.2. Progressive growth (PG) and progressive refinement (PR) offer two distinct spatial refinement strategies in the LEGO model, and leveraging their unique properties to skip corresponding LEGO bricks during sampling presents a non-trivial challenge. We visualize the differences between these strategies in Figures 5 and 6. In PG, the model initially uses patch-bricks to generate patches and then employs an image-brick to re-aggregate these local features. Conversely, PR begins by utilizing the image-brick to establish a global structure and then employs local featureoriented patch-bricks to refine details on top of it. These differences become more evident when t is larger. Based on these observations, we propose that LEGO-PG can skip its top-level image-brick responsible for handling global image information when t is small, while LEGO-PR skips its top-level patch-brick, which is responsible for processing 4\u00d7 4 local patch information, when t is large.\nBelow we show analogous results of Figure 4 on different datasets, illustrated in Figures 7-9.\nE.2 ABLATION STUDIES: ON THE EFFECTS OF LEGO BRICKS\nIn this section, we first study the effects of different LEGO bricks. Note that the patch-bricks target to provide fine-grained details in generation and cost fewer computation resource as they take smaller input resolution and shorter attention span; the image-brick ensures the global structure of the generation and consistency within local regions, but requires more computation cost. Therefore, to find a good trade-off between generative performance and computation cost, we conduct experiments to find the right balance between the patch-bricks and image-brick.\nProportion of layers assigned to the image-brick We first study the trade-off between the training time and the ratio of the depth (number of DiT block layers) of the image-brick to the overall depth of the LEGO model. For simplicity, we consider a LEGO-Diffusion model consisting of a patch-brick with a patch size of 4\u00d7 4 and an image-brick with a patch size of 64\u00d764, both with a receptive-field-size set as 2\u00d7 2. We also let both bricks have the same token embedding dimension, number of attention heads, MLP ratio, etc. Fixing the total depth as 12, we vary the depth assigned to the image-brick from 0 to 12 and show the results in Figure 10. For a LEGODiffusion model consisting of a patch-brick with a patch size of 4 \u00d7 4 and an imagebrick with a patch size of 64 \u00d7 64, both with a receptive field size set as 2\u00d7 2, we\nvisualize how the FID and training time change as the ratio of the depth (number of DiT-based Transformer layers) of the image-brick to that of the patch-brick increases from 0 to 1. Specifically, at the beginning of x-axis, we have a patch-brick with 12 DiT-based Transformer layers and an image-brick with 0 DiT-based Transformer layers, and at the end, we have a patch-brick with 0 DiT-based Transformer layers and an image-brick with 12 DiT-based Transformer layers. We can observe that when we allocate the majority of layers to the patch-brick, the training is efficient but results in a relatively higher FID. As we assign more layers to the image-brick, the FID improves, but the model requires longer training time. Empirically, we find that when the ratio is greater than 50%, the FID improvement becomes less significant, while efficiency decreases at the same time. This observation motivates us to allocate approximately 50% of the layers to the patch-brick.\nMulti-scale local bricks Inspired by the multi-stage design commonly found in convolutional networks, we investigate the impact of using multi-scale patch-bricks on our diffusion model\u2019s performance. To isolate the effect, we first fix the configurations of both patch-and image-bricks, including the local-receptive-field size that is fixed at 2\u00d7 2 and the total number of DiT-block layers is fixed at 12, varying only the resolution of the patches used in training the patch-bricks. Specifically, we experiment with brick sizes of 4 \u00d7 4, 8 \u00d7 8, 16 \u00d7 16, as well as combinations of them. The corresponding FID scores and training times are summarized on the left panel of Figure 11. Our findings indicate that when using a single patch-brick, a 16\u00d716 brick size yields the best performance but incurs higher training time due to the need for longer token embedding sequences. Using a combination of 4\u00d7 4 and 8\u00d7 8 patch-bricks does not significantly improve the FID score, possibly because 8\u00d78 patches do not add substantially more information than 4\u00d74 patches. However, when a 16\u00d7 16 patch-brick is combined with a smaller patch-brick, we see a significant improvement in FID. Notably, the combination of 4\u00d7 4 and 16\u00d7 16 patch-bricks offers the best trade-off between FID and training time. As a result, this combination of patch sizes is employed when modeling 64\u00d7 64 resolutions in our experiments.\nTo further optimize training efficiency, for the 16\u00d7 16 patch-brick, we increase its receptive field size from 2\u00d7 2 to 8\u00d7 8 and investigate how to assign the six DiT block layers between the 4\u00d7 4 patch-brick and the 16\u00d7 16 patch-brick. The results shown on the right panel of Figure 11 reveal that assigning more layers to the 4\u00d7 4 patch-brick in general results in lower FID but longer training time. Finally, we observe that when the depth of the 16\u00d7 16 patch-brick is approximately twice that of\nthe 4\u00d7 4 patch-brick, the model achieves a satisfactory balance between performance and efficiency. These observations have been leveraged in the construction of the LEGO model configurations, which are outlined in Tables 5 and 6. It\u2019s worth noting that additional tuning of configurations, such as employing specific local receptive field sizes, adjusting the embedding dimension of the local receptive field, varying the number of attention heads, and so on for each LEGO brick, holds the potential for further performance improvements. However, we leave these aspects for future exploration and research.\nE.3 ADDITIONAL GENERATION RESULTS\nCrop Grid - 256 Random - 256 Grid - 512 Random - 512\nDiT 0.15 \u00b1 0.07 0.66 \u00b1 0.13 0.55 \u00b1 0.07 0.76 \u00b1 0.06 LEGO-PG 0.14 \u00b1 0.07 0.21 \u00b1 0.17 0.36 \u00b1 0.07 0.37 \u00b1 0.06 LEGO-PR 0.15 \u00b1 0.07 0.25 \u00b1 0.15 0.55 \u00b1 0.07 0.55 \u00b1 0.05\nFigure 13: Paranoma generation of 4K (3840\u00d7 2160) resolution with a mixture of class conditions, produced with class-conditional LEGO-PG, trained on ImageNet (512\u00d7 512).\nFigure 14: Paranoma generation (2048\u00d7512) with class-conditional LEGO-PG, trained on ImageNet (512\u00d7 512). (From top to bottom) Class: Water ouzel (index 20), Alp (index 970), and Valley (index 979).\nFigure 15: Randomly generated images, using LEGO-PG (cfg-scale=4.0)\nFigure 16: Randomly generated images, using LEGO-PG (cfg-scale=1.5)\nFigure 17: Randomly generated images, using LEGO-PR (cfg-scale=4.0)\nFigure 18: Randomly generated images, using LEGO-PR (cfg-scale=1.5)"
        }
    ],
    "year": 2023
}