{
    "abstractText": "Image recognition and generation have long been developed independently of each other. With the recent trend towards general-purpose representation learning, the development of general representations for both recognition and generation tasks is also promoted. However, preliminary attempts mainly focus on generation performance, but are still inferior on recognition tasks. These methods are modeled in the vector-quantized (VQ) space, whereas leading recognition methods use pixels as inputs. Our key insights are twofold: (1) pixels as inputs are crucial for recognition tasks; (2) VQ tokens as reconstruction targets are beneficial for generation tasks. These observations motivate us to propose an Alternating Denoising Diffusion Process (ADDP) that integrates these two spaces within a single representation learning framework. In each denoising step, our method first decodes pixels from previous VQ tokens, then generates new VQ tokens from the decoded pixels. The diffusion process gradually masks out a portion of VQ tokens to construct the training samples. The learned representations can be used to generate diverse high-fidelity images and also demonstrate excellent transfer performance on recognition tasks. Extensive experiments show that our method achieves competitive performance on unconditional generation, ImageNet classification, COCO detection, and ADE20k segmentation. Importantly, our method represents the first successful development of general representations applicable to both generation and dense recognition tasks. Code is released at https://github.com/ChangyaoTian/ADDP.",
    "authors": [
        {
            "affiliations": [],
            "name": "Changyao Tian"
        },
        {
            "affiliations": [],
            "name": "Chenxin Tao"
        },
        {
            "affiliations": [],
            "name": "Jifeng Dai"
        },
        {
            "affiliations": [],
            "name": "Hao Li"
        },
        {
            "affiliations": [],
            "name": "Ziheng Li"
        },
        {
            "affiliations": [],
            "name": "Lewei Lu"
        },
        {
            "affiliations": [],
            "name": "Xiaogang Wang"
        },
        {
            "affiliations": [],
            "name": "Hongsheng Li"
        },
        {
            "affiliations": [],
            "name": "Gao Huang"
        },
        {
            "affiliations": [],
            "name": "Xizhou Zhu"
        },
        {
            "affiliations": [],
            "name": "3B"
        }
    ],
    "id": "SP:6d13a5e8b8d9b169be998dc34db098470e5e69d9",
    "references": [
        {
            "authors": [
                "Mathilde Caron",
                "Ishan Misra",
                "Piotr Bojanowski",
                "Florian Bordes",
                "Pascal Vincent",
                "Armand Joulin",
                "Mike Rabbat",
                "Nicolas Ballas"
            ],
            "title": "Masked siamese networks for label-efficient learning",
            "year": 2022
        },
        {
            "authors": [
                "Mathilde Caron",
                "Ishan Misra",
                "Julien Mairal",
                "Priya Goyal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Unsupervised learning of visual features by contrasting cluster assignments",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Kaiming He"
            ],
            "title": "Exploring simple siamese representation learning",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Guo",
                "Mohammad Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Thomas Dietterich"
            ],
            "title": "Benchmarking neural network robustness to common corruptions and perturbations",
            "venue": "arXiv preprint arXiv:1903.12261,",
            "year": 2019
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Steven Basart",
                "Norman Mu",
                "Saurav Kadavath",
                "Frank Wang",
                "Evan Dorundo",
                "Rahul Desai",
                "Tyler Zhu",
                "Samyak Parajuli",
                "Mike Guo"
            ],
            "title": "The many faces of robustness: A critical analysis of out-of-distribution generalization",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Zhao",
                "Steven Basart",
                "Jacob Steinhardt",
                "Dawn Song"
            ],
            "title": "Natural adversarial examples",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi"
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Yanghao Li",
                "Hanzi Mao",
                "Ross Girshick",
                "Kaiming He"
            ],
            "title": "Exploring plain vision transformer backbones for object detection",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Haohan Wang",
                "Songwei Ge",
                "Zachary Lipton",
                "Eric P Xing"
            ],
            "title": "Learning robust global representations by penalizing local predictive power",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Ross Wightman",
                "Hugo Touvron",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Resnet strikes back: An improved training procedure in timm",
            "venue": "arXiv preprint arXiv:2110.00476,",
            "year": 2021
        },
        {
            "authors": [
                "Tete Xiao",
                "Yingcheng Liu",
                "Bolei Zhou",
                "Yuning Jiang",
                "Jian Sun"
            ],
            "title": "Unified perceptual parsing for scene understanding",
            "venue": "In ECCV,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Image recognition and image generation are both fundamental tasks in the field of computer vision (Bao et al., 2022; He et al., 2022; Wei et al., 2022; Liu et al., 2021; Dhariwal & Nichol, 2021; Ho et al., 2020; Donahue et al., 2017). Recognition tasks aim to perceive and understand the visual world, while generation tasks aim to create new visual data for various applications. Modern recognition algorithms have already surpassed human performance on many benchmarks (Liu et al., 2021; He et al., 2016), and current generative models can synthesize diverse high-fidelity images (Rombach et al., 2022; Esser et al., 2021). However, these two fields have long been developed independently of each other. Recent years have witnessed a significant trend towards general-purpose representation learning. For recognition tasks, researchers have extensively studied general representations that can be adapted to various downstream tasks (Peng et al., 2022; Dong et al., 2023; He et al., 2022; Chen et al., 2022; Tao et al., 2023). Given this unifying trend, it is natural to expect that representations applicable to both recognition and generation tasks could be developed.\nInspired by this, recent works (Li et al., 2022; Yu et al., 2021; Chen et al., 2020a) attempt to learn general representations for both recognition and generation through a specific generative modeling\n\u2217Equal contribution. BCorresponding authors. \u2020The work is done when Chenxin Tao is an intern at SenseTime Research.\nparadigm, i.e., Masked Image Modeling (MIM) (Bao et al., 2022). As shown in Fig. 1, during the generation process, they iteratively recover the image content for a portion of masked regions. Such generation process has been leveraged for high-fidelity image synthesis (Chang et al., 2022). Meanwhile, each recovery step can be regarded as a special case of MIM using different mask ratios, which has also proved to learn expressive representations for image recognition (He et al., 2022). In particular, ViT-VQGAN (Yu et al., 2021) and MAGE (Li et al., 2022) exhibit remarkable generation performance. Nonetheless, their recognition performances fall short. Specifically, they are still limited to classification task, but are not suitable for dense recognition tasks.\nWe notice that ViT-VQGAN (Yu et al., 2021) and MAGE (Li et al., 2022), like many image generation methods, are modeled in the vector-quantized (VQ) space (van den Oord et al., 2017). While current SoTA representation learning methods for recognition, such as MAE (He et al., 2022) and BEiT (Bao et al., 2022), all take raw image pixels as inputs. Such observation motivates us to propose the following arguments: (1) Raw pixels as inputs are crucial for recognition tasks. Pixels preserve spatially sensitive information better than VQ tokens (Shin et al., 2023), which is particularly useful for dense recognition tasks. As shown in Tab. 1, taking pixels as inputs outperforms the VQ tokens counterpart in typical recognition tasks. (2) VQ tokens as reconstruction targets are beneficial for generation tasks. Previous works such as (van den Oord et al., 2017; Rombach et al., 2022) show that compared to generating raw pixels, predicting VQ tokens can help the model eliminate imperceptible image details, mitigating the optimization difficulty and resulting in better image generation quality.\nBased on these observations, a natural question arises: Is it possible to associate the two spaces within a single representation learning framework, allowing the model to perceive in raw pixels and generate in latent visual tokens?\nTo this end, we propose a general representation learning framework that bridges pixel and token spaces via an Alternating Denoising Diffusion Process (ADDP). Specifically, at each step in the alternating denoising process, we first decode pixels from previous VQ tokens, and then generate new VQ tokens from these decoded pixels. For the corresponding diffusion process, we first map the original images into VQ-token space with a pre-trained VQ encoder (Chang et al., 2022), then gradually mask out some VQ tokens. An off-the-shelf VQ decoder is employed for the tokento-pixel decoding, while a learnable encoder-decoder network is introduced for the pixel-to-token generation. The training objective is given by the evidence lower bound (ELBO) of the alternating denoising diffusion process. When applied to image generation, we follow the proposed alternating\ndenoising process to generate images. When applied to image recognition, the learned encoder, which takes raw pixels as inputs, would be fine-tuned on corresponding datasets.\nExtensive experiments demonstrate the superior performance of ADDP on image generation and recognition tasks, including unconditional generation on ImageNet 256 \u00d7 256 (Deng et al., 2009), ImageNet-1k classification, COCO (Lin et al., 2014) detection and ADE20k (Zhou et al., 2019) segmentation. For unconditional generation, ADDP is able to generate high-fidelity images, achieving better performance than previous SoTAs (Li et al., 2022). For recognition tasks, ADDP is competitive with current leading methods tailored for recognition tasks (He et al., 2022; Bao et al., 2022). Specifically, to the best of our knowledge, ADDP is the first approach to develop general representations that are applicable to both generation and dense recognition tasks."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Deep generative models are initially developed for image generation. However, recent works found that models trained for some specific generative tasks, such as Masked Image Modeling (MIM) (He et al., 2022; Bao et al., 2022), can learn expressive representations that can be transferred to various downstream recognition tasks. Such discovery has inspired a series of works that attempt to unify image generation and representation learning.\nDeep Generative Models for Image Generation. Early attempts (e.g., GANs (Goodfellow et al., 2014; Mirza & Osindero, 2014; Denton et al., 2015; Radford et al., 2016; Chen et al., 2016; Arjovsky et al., 2017; Zhu et al., 2017; Karras et al., 2018; Zhang et al., 2019; Brock et al., 2019), VAEs (Kingma & Welling, 2014; Higgins et al., 2017; Vahdat & Kautz, 2020), and autoregressive models (van den Oord et al., 2016a;b; Salimans et al., 2017)) directly decode raw pixels from random distributions. However, VQ-VAE (van den Oord et al., 2017) points out that directly generating raw pixels is challenging and resource-wasteful due to the redundant low-level information in images. In contrast, VQ-VAE proposes a two-stage paradigm: the first stage encodes images into latent representations (i.e., discrete visual tokens), and the second stage learns to generate visual tokens with powerful autoregressive models. These generated visual tokens are then decoded into raw pixels by the decoder learned in the first stage. Such a two-stage latent space paradigm shows superior training efficiency and performance compared to raw-pixel-wise methods and is thus adopted by most state-of-the-art generative models (Razavi et al., 2019; Yu et al., 2021; Esser et al., 2021; Gu et al., 2022; Chang et al., 2022; Ramesh et al., 2022).\nOn the other hand, diffusion models (Ho et al., 2020; Dhariwal & Nichol, 2021; Gu et al., 2022; Ramesh et al., 2022; Chang et al., 2022; Saharia et al., 2022; Chang et al., 2023) have also achieved impressive results in image generation, which can produce high-fidelity images by iteratively refining the generated results. For example, Guided Diffusion (Dhariwal & Nichol, 2021) directly decodes raw pixels with diffusion models, and for the first time achieves better results than GANand VAE-based generative models. Recent works (e.g., Stable Diffusion (LDMs) (Rombach et al., 2022), VQ-Diffusion (Gu et al., 2022) and MaskGIT (Chang et al., 2022)) further combine diffusion models with the two-stage latent space paradigm, achieving superior image quality. Meanwhile, the success of diffusion models has also been extended to text-to-image generation (Gu et al., 2022; Ramesh et al., 2022; Saharia et al., 2022; Chang et al., 2023), image editing (Zhang et al., 2022; Kawar et al., 2022b), image denoising (Kulikov et al., 2022; Kawar et al., 2022a), etc.\nFollowing previous works, ADDP also performs diffusion with latent space to generate images. The key difference is that our method refines raw pixels and latent representations alternately, which can learn unified representation for both recognition and generation tasks with competitive performance.\nGenerative Pre-training for Image Representation Learning. Recent research (Bao et al., 2022; Peng et al., 2022; Dong et al., 2023; He et al., 2022; Chen et al., 2022; Liu et al., 2022; Wei et al., 2022; Fang et al., 2023) suggests that some specific generative modeling tasks (e.g., Masked Image Modeling (MIM)) can learn more expressive and effective representations than previous representation learning methods (e.g., supervised methods (Dosovitskiy et al., 2021; Liu et al., 2021) and self-supervised discriminative methods (He et al., 2020; Chen et al., 2020c;b)). These generative pre-training methods have shown superior performance when transferred to various downstream recognition tasks, such as image classification, object detection, and semantic segmentation. MIM methods learn representations by reconstructing image content from masked images. For example, BEiTs (Bao et al., 2022; Peng et al., 2022; Dong et al., 2023) reconstruct the discrete visual\ntokens corresponding to masked parts. MAEs (He et al., 2022; Chen et al., 2022) directly reconstruct the masked pixels. Some works (Liu et al., 2022; Wei et al., 2022) also attempt to reconstruct the momentum features of the original images. Apart from these MIM methods, Corrupted Image Modeling (CIM) (Fang et al., 2023) learns representations by reconstructing from corrupted images, which avoids the use of artificial mask tokens that never appear in the downstream fine-tuning stage.\nThese generative pre-training methods only focus on the representational expressiveness for image recognition, while fail to preserve the quality of reconstructed images and thus unable to be used for image generation tasks directly. In contrast, ADDP learns general representations that perform well for both image recognition and generation tasks.\nGenerative Modeling for Unifying Representation Learning and Image Generation. Early attempts (Donahue et al., 2017; Donahue & Simonyan, 2019) consider representation learning and image generation as dual problems, thus learning two independent networks (i.e., image encoder and image decoder) to solve both tasks at the same time in a dual-learning paradigm. Inspired by generative representation learning (Devlin et al., 2018; Radford et al., 2018; Howard & Ruder, 2018) in NLP fields, iGPT (Chen et al., 2020a) for the first time unifies these two tasks into a single network by learning from autoregressive image generation, providing good performance for both image recognition and image generation tasks. ViT-VQGAN (Yu et al., 2021) further replaces the raw-pixel inputs and outputs with discrete visual tokens for better performance. Recently, MAGE (Li et al., 2022) proposes to replace the autoregressive decoding process with a diffusion method (i.e., MaskGIT (Chang et al., 2022)), following state-of-the-art practices for image generation.\nDespite these attempts at unifying representation learning and image generation, their recognition performance is still inferior to state-of-the-art representation learning methods, as they perform representation learning either entirely in raw-pixel space or entirely in latent space. In contrast, ADDP exploits both raw-pixel and latent space, learning general representations that yield competitive performance on both image recognition and image generation tasks."
        },
        {
            "heading": "3 METHOD",
            "text": ""
        },
        {
            "heading": "3.1 ALTERNATING DENOISING OF PIXELS AND VQ TOKENS",
            "text": "Previous works (Yu et al., 2021; Li et al., 2022; Dhariwal & Nichol, 2021) mainly perform the denoising process entirely in either continuous raw pixel space or discrete VQ token space. However, given that both raw pixels and VQ tokens are crucial for recognition and generation tasks respectively, we propose to denoise pixels and VQ tokens alternately, as shown in Fig. 2. In each step, we first decode pixels from previously generated VQ tokens, then generate new VQ tokens conditioned on decoded pixels. To associate the two spaces and enable the alternating denoising process, the token-to-pixel decoding and pixel-to-token generation are introduced as follows.\nToken-to-Pixel Decoding is widely used in image generation to restore the generated VQ tokens to visual image pixels (van den Oord et al., 2017; Esser et al., 2021; Chang et al., 2022). The VQ decoder subnetworks in off-the-shelf pre-trained VQ tokenizers (e.g., VQGAN (Esser et al., 2021)) could be directly used to perform such decoding. However, existing VQ decoders can only decode images from the complete VQ sequences, while unable to take partial VQ tokens as inputs. In contrast, our denoising process only generates a portion of VQ tokens at each step. To resolve this\ninconsistency and facilitate the use of off-the-shelf VQ decoders, we propose to pair these reliable tokens with some unreliable ones. Specifically, at step t, given partially generated VQ tokens zt, we further sample additional complementary VQ tokens z\u0304t so as to form the complete VQ sequences for decoding the pixel image xt. In order to distinguish zt and z\u0304t, we term them as reliable tokens and unreliable tokens respectively. Note that z\u0304t will only be used for decoding the pixel image and not kept to the next step. Then, the conditional probability of p(xt|zt, z\u0304t) is defined as\np(xt|zt, z\u0304t) = \u03b4 [ xt = VQ-Decoder ( zt \u2299 (1\u2212mt) + z\u0304t \u2299mt )] , (1)\nwhere \u03b4 denotes the Dirac delta distribution, \u2299 is the element-wise product. mt is a binary mask indicating the unreliable regions derived from zt. mt shares the same spatial size with zt and z\u0304t, where the regions with binary values of 1 are unreliable. Both the reliable VQ tokens zt and unreliable VQ tokens z\u0304t would be predicted by our models, which will be discussed in Sec. 3.3.\nPixel-to-Token Generation has been shown to be effective for recognition tasks in recent representation learning works (Bao et al., 2022; Peng et al., 2022; Dong et al., 2023). Our method introduces a learnable encoder-decoder network for predicting VQ tokens from noisy images to enable representation learning from pixels. As shown in Fig. 4, taking the previously decoded noisy image xt as inputs, the encoder E first extracts representation from xt, and then the unreliable regions (i.e., mt = 1) of the extracted representation E(xt) would be replaced with learnable <MASK> token embedding before feeding into the decoder D. The decoder will predict zt\u22121 and z\u0304t\u22121 of the next step based on these inputs. Given the noisy image xt, the conditional probability of generated reliable VQ tokens zt\u22121 and unreliable VQ tokens z\u0304t\u22121 at the next step t\u2212 1 are given as\np\u03b8(zt\u22121, z\u0304t\u22121|xt) = D ( E(xt)\u2299 (1\u2212mt) + emask \u2299mt ) , (2)\nwhere mt is the same binary mask as in Eq. (1), indicating the unreliable regions at step t. E and D are learnable encoder and decoder subnetworks respectively. emask is a learnable <MASK> token embedding and \u03b8 represents the parameter of the whole network. Since our network takes full images as the inputs, it can adapt various image backbones (e.g., CNNs (He et al., 2016) and ViTs (Dosovitskiy et al., 2021)) as the encoder naturally. Experiments in Sec. 4.2 show that the learned representations from the encoder generalize well for both recognition and generation tasks.\nAlternating Denoising Process is shown in Fig. 2. Starting from an empty sequence with the unreliable mask mT+1 of all 1s, p\u03b8(zT , z\u0304T |\u2205) is predicted by feeding all <MASK> tokens into our decoder D in Eq. (2). After that, at each step t, the noisy images xt are decoded according to Eq. (1) and then used to generate new reliable tokens zt\u22121 and unreliable tokens z\u0304t\u22121 according to Eq. (2). Finally, the synthesized noisy-free images are the refined images x0 at step 0. The joint distribution of all variables in the alternating denoising process is defined as\np\u03b8(z0:T , z\u03040:T , x0:T ) = p\u03b8(zT , z\u0304T |\u2205)\ufe38 \ufe37\ufe37 \ufe38 our model p(x0|z0)\ufe38 \ufe37\ufe37 \ufe38 VQ-Decoder T\u220f t=1 p(xt|zt, z\u0304t)\ufe38 \ufe37\ufe37 \ufe38 VQ-Decoder p\u03b8(zt\u22121, z\u0304t\u22121|xt)\ufe38 \ufe37\ufe37 \ufe38 our model . (3)\n3.2 DIFFUSION PROCESS\n\ud835\udc65\n\ud835\udc5e \ud835\udc67 \ud835\udc65\n\ud835\udc67 \ud835\udc67\nDiffusion Process\n\u2026\n\ud835\udc67\u0305\n\ud835\udc5e \ud835\udc67\u0305 \ud835\udc67\n\ud835\udc67\n\ud835\udc67\u0305\n\ud835\udc5e \ud835\udc67\u0305 \ud835\udc67\n\u2026 \ud835\udc67\n\ud835\udc67\u0305\n\ud835\udc5e \ud835\udc67\u0305 \ud835\udc67\n\ud835\udc67\n\ud835\udc67\u0305\n\ud835\udc5e \ud835\udc67\u0305 \ud835\udc67\n\u2205 \ud835\udc5e \ud835\udc67 \ud835\udc67 \ud835\udc5e \ud835\udc67 \ud835\udc67\n\ud835\udc67\u0305\nFigure 3: Diffusion process.\nFollowing previous denoising diffusion training paradigms (Ho et al., 2020; Gu et al., 2022), we propose a corresponding diffusion process for ADDP as well, as is shown in Fig. 3. Given an image x0, an off-the-shelf pre-trained VQ encoder is used to map x0 to its corresponding VQ\ntokens z0 = VQ-Encoder(x0). Then, the diffusion process gradually masks out some regions of z0 with a Markov chain q(zt|zt\u22121). As mentioned in Sec. 3.1, zt\u22121 corresponds to the reliable VQ tokens at step t \u2212 1, and reliable VQ tokens zt at step t is a portion of zt\u22121. The whole process consists of T + 1 steps in total, where all tokens would be masked out at last.\nFor the unreliable VQ tokens z\u0304t\u22121, a forward process q(z\u0304t\u22121|zt) is designed to produce z\u0304t\u22121 from zt. Instead of zt\u22121, we use the reliable tokens zt as the condition. Empirical results in Tab. 3 demonstrate the advantage of using q(z\u0304t\u22121|zt) over q(z\u0304t\u22121|zt\u22121) .\nThe conditional distribution q(z\u0304t\u22121|zt) is obtained with a token predictor, which receives reliable tokens zt as inputs and predicts unreliable tokens z\u0304t\u22121. Since our goal is to generate the original image with unreliable tokens, the optimal value of q(z\u0304t\u22121|zt) should be q(z0|zt). However, q(z0|zt) is generally intractable, so the token predictor needs to learn from data samples to approximate q(z0|zt). To achieve this, it is trained to predict all tokens z0 from reliable tokens zt only. Finally, as our model is trained to predict unreliable tokens (see Sec. 3.3), the token predictor is only required during training. Therefore, the whole diffusion process is defined as\nq(z0:T , z\u03040:T |x0) = q(z0|x0)\ufe38 \ufe37\ufe37 \ufe38 VQ-Encoder q(z\u0304T |\u2205)\ufe38 \ufe37\ufe37 \ufe38 token predictor T\u220f t=1 q(zt|zt\u22121)\ufe38 \ufe37\ufe37 \ufe38 add mask q(z\u0304t\u22121|zt)\ufe38 \ufe37\ufe37 \ufe38 token predictor . (4)\nFor simplicity, the decoded noisy images x1:T are omitted, since the derivation of the training objective (refer to Appendix) shows that they are unnecessary to be included in the diffusion process."
        },
        {
            "heading": "3.3 LEARNING THE DENOISING PROCESS",
            "text": "Given the proposed alternating denoising diffusion process, ADDP can be optimized through minimizing the evidence lower bound (ELBO) of log p\u03b8(x0), which consists of the following terms:\nLELBO = LVQ + T\u2211 t=1 Lt + LT+1, (5)\nLVQ = Eq(z0,z\u03040,x0) [ \u2212 log p(x0|z0)\ufe38 \ufe37\ufe37 \ufe38\nVQ-Decoder + log q(z0|x0)\ufe38 \ufe37\ufe37 \ufe38 VQ-Encoder\n] ,\nLt =Eq(zt,z\u0304t,z0) [ DKL ( q(zt\u22121|zt, z0) q(z\u0304t\u22121|zt)\ufe38 \ufe37\ufe37 \ufe38\ntoken predictor || p\u03b8(zt\u22121, z\u0304t\u22121|xt = VQ-Decoder(zt, z\u0304t))\ufe38 \ufe37\ufe37 \ufe38 our model\n)] ,\nLT+1 = Eq(z0) [ DKL ( q(zT |z0) q(z\u0304T |\u2205)\ufe38 \ufe37\ufe37 \ufe38\ntoken predictor || p\u03b8(zT , z\u0304T |\u2205)\ufe38 \ufe37\ufe37 \ufe38 our model\n)] ,\nwhere DKL is KL divergence. LVQ corresponds to the training of VQ tokenizer, which is omitted in our training because a pre-trained VQ tokenizer is used. L1:T+1 are used to optimize our model parameters \u03b8. p\u03b8 is given by Eq. (2). Please see Appendix for detailed derivation.\nTraining Target. Like previous MIM methods (He et al., 2022; Bao et al., 2022), we only optimize the loss Lt on masked tokens (i.e., unreliable regions with mt = 1). Following the reparameterization trick in VQ-Diffusion (Gu et al., 2022), Lt can be further simplified as\nLt = Eq(zt,z\u0304t) [ DKL ( q(z0|zt) q(z\u0304t\u22121|zt)\ufe38 \ufe37\ufe37 \ufe38\ntoken predictor || p\u03b8(z0, z\u0304t\u22121|xt = VQ-Decoder(zt, z\u0304t))\ufe38 \ufe37\ufe37 \ufe38 our model\n)] , (6)\nwhich implies that q(z0|zt) and q(z\u0304t\u22121|zt) are the training targets of our model. Since q(z0|zt) is generally intractable, the token predictor q(z\u0304t\u22121|zt) is used as an estimation of q(z0|zt) as aforementioned in Sec. 3.2. Therefore, it is feasible to predict q(z\u0304t\u22121|zt) only. Training Process. As shown in Fig. 4, given an image x0, we first compute the reliable tokens zt and unreliable tokens z\u0304t following the diffusion process in Sec. 3.2. zt and z\u0304t are then combined and fed into the VQ decoder to generate the synthesized image xt. After that, the pixel-to-token generation network takes xt as input and predicts the distribution of z\u0304t\u22121. The training target q(z\u0304t\u22121|zt) can be computed from the token predictor.\nOur model takes the noisy synthesized image xt = VQ-Decoder(zt, z\u0304t) as training inputs. Inappropriate values of unreliable z\u0304t may hurt the quality of synthesized images, degenerating the performance of image generation and image recognition tasks. To alleviate this issue, instead of directly using z\u0304t sampled from q(z\u0304t|zt+1), a mapping function f(q(z\u0304t|zt+1)) is introduced for the unreliable parts in Eq. (1) as\nxt = VQ-Decoder ( zt \u2299 (1\u2212mt) + f(q(z\u0304t|zt+1))\u2299mt ) . (7)\nCandidates for the mapping function f(\u00b7) are designed as {Sampling, ArgMax, WeightedSum}. Sampling is the na\u0131\u0308ve design that sample tokens according to q(z\u0304t|zt+1). ArgMax is to choose the tokens with the largest probability in q(z\u0304t|zt+1). WeightedSum indicates that all VQ embeddings\nPredicted Tokens\nwithin the token codebook would be weighted summed according to q(z\u0304t|zt+1) before fed into the VQ decoder network. Empirically, we find that using WeightedSum mapping produces high-quality images and helps to improve our model performance (see Sec. 4.3).\nIn practice, we also find that feeding the reliable tokens zt into our decoder D as additional information benefits both recognition and generation tasks, as predicting VQ tokens only from images with considerable noise is difficult when the mask ratio is relatively high.\nImage\nRepresentation\nApply to Image Generation. For image generation, we follow the denoising process mentioned in Sec. 3.1 and Fig. 5 (a). Starting from an empty sequence, our model predicts zT and z\u0304T from pure <MASK> token embeddings. At each step, the VQ decoder generates xt+1 from tokens of current step zt+1 and z\u0304t+1. Then, the pixels xt+1 are fed into our encoder-decoder network to predict zt and z\u0304t of the next step. x0 is the final generated noisy-free image.\nApply to Image Recognition. As shown in Fig. 5 (b), ADDP can be applied to various recognition tasks after pre-training as well. The encoder takes raw pixels as inputs directly, while the output representations are then forwarded to different task-specific heads. Sec. 4.2 shows that\nADDP delivers strong performances after fine-tuning on corresponding datasets."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 IMPLEMENTATION DETAILS",
            "text": "Network. The VQ tokenizer is adopted from the off-the-shelf VQGAN (Esser et al., 2021; Chang et al., 2022) model released by MAGE (Li et al., 2022). MAGE-Large is used as the token predictor. Note that the token predictor is used for training only and can be discarded in inference. ViTLarge (Dosovitskiy et al., 2021) is adopted as the encoder of ADDP, and the decoder consists of 8 Transformer (Vaswani et al., 2017) blocks. Training. ADDP is trained on ImageNet-1k (Deng et al., 2009) dataset. The total denoising step is T = 100. For masking strategy, we sample step t from the values of 1, 2, . . . , T , while ensuring the mask ratio distribution close to that of MAGE (Li et al., 2022) for a fair comparison. AdamW (Loshchilov & Hutter, 2017) optimizer with a peak learning rate of 1.5\u00d710\u22123 is used. The model is trained for 800 epochs with 40 warmup epochs and batch size of 4096.\ntraining setting. The results of ADDP are marked in gray . Image Recognition. We evaluate the transfer performance of image classification on ImageNet1k (Deng et al., 2009), object detection on COCO (Lin et al., 2014) and semantic segmentation on ADE20k (Zhou et al., 2019) respectively. The pre-trained encoder is used as backbone and taskspecific heads are appended for different tasks. Image Generation. After training, we use iterative decoding as in MaskGIT (Chang et al., 2022) to iteratively fill in masked tokens and generate images. By default, we recover one token per step.\nPlease refer to Appendix for more implementation details as well as the generated images of ADDP."
        },
        {
            "heading": "4.2 MAIN RESULTS",
            "text": "Tab. 2 compares ADDP with previous methods designed for recognition only, generation only, or both recognition and generation tasks. Unconditional Generation. ADDP surpasses the previous SoTA by 1.5 FID, which validates the effectiveness of our proposed alternating denoising process in generating high-quality images. Image Classification. ADDP achieves comparable fine-tuning performance with methods tailored for recognition. Moreover, compared with the previous SoTA (i.e., MAGE) designed for both recognition and generation, ADDP boosts the performance by 2 points. Such result is consistent with the conclusion drawn from Tab. 1, suggesting pixel inputs are more competent with recognition tasks. Linear Probing. We observe an interesting fact that the linear probing accuracy of ADDP is not aligned with its performance on other tasks, which we assume is caused by the gap between natural images and the noisy synthetic images ADDP takes as training inputs. Visualizations of such noisy synthetic images can be found in appendix. Nevertheless, as highlighted in prior studies (He et al., 2022; Bao et al., 2022), linear probing and fine-tuning results are largely uncorrelated, while the core capability of deep neural networks resides in learning strong non-linear representations. Therefore, our primary emphasis in this work mainly focus on fine-tuning tasks as well. Object Detection and Semantic Segmentation. Benefiting from our alternating denoising diffusion process, ADDP can also be transferred to object detection and semantic segmentation and achieve competitive performance on these tasks. To the best of our knowledge, this is the first work demonstrating that general representations can be learned for both generation and dense prediction tasks. For comparison, we apply MAGE pre-trained ViT-L to these tasks as well, which is trained under the same setting except that it takes VQ tokens as inputs. The results show that our method surpass MAGE by a large margin.\nConditional FID \u2193 IS \u2191 FT \u2191 Probability\nq(z\u0304t\u22121|zt\u22121) 80.75 13.48 83.7 q(z\u0304t\u22121|zt) 23.63 33.09 83.5\nTable 3: Ablation on condition for unreliable tokens.\nMapping Function f(\u00b7) FID \u2193 IS \u2191 FT \u2191\nSampling 38.79 23.44 83.2 ArgMax 26.62 30.03 83.5 WeightedSum 23.63 33.09 83.5\nTable 4: Ablation on mapping function f(\u00b7).\nToken Input FID \u2193 IS \u2191 FT \u2191\nencoder & decoder 20.40 37.20 81.5 decoder-only 23.63 33.09 83.5 None 36.24 21.71 83.1\nTable 5: Ablation on token input strategy.\n50 100 150 200 250 timestep\n10.0 12.5 15.0 17.5 20.0 22.5 25.0 27.5 FI D\nlinear cosine\nFigure 6: Ablation on masking schedule for generation."
        },
        {
            "heading": "4.3 ABLATION STUDY",
            "text": "Ablation Settings. For ablation, we use ViT-Base as encoder and train ADDP for 150 epochs. The FID (Heusel et al., 2017) and IS (Salimans et al., 2016) scores of unconditional generation with 20 inference steps are reported as well as the fine-tuning accuracy (FT) on ImageNet-1k (Deng et al., 2009). Default settings are marked in gray .\nProbability Condition for Unreliable Tokens. As in Sec. 3.2, the prediction of z\u0304t\u22121 is conditioned on zt rather than zt\u22121. Tab. 3 verifies that using zt\u22121 as condition leads to poor performance. Mapping Function for Unreliable Tokens. Proper mapping function choices can help improve the quality of the learned representation. Tab. 4 shows that directly Sampling is inferior in both generation and classification, while WeightedSum can deliver sound performance. Token Input. Directly recovering tokens from pixels with considerable noise may be challenging when the mask ratio is extremely high. Tab. 5 analyzes the effect of feeding tokens to our model, showing that feeding tokens can help enhance the generation ability. However, if tokens are fed directly through the encoder, the classification performance degrades rapidly. Prediction Target. Sec. 3.3 discusses two possible training targets: q(z0|zt) and q(z\u0304t\u22121|zt). While q(z0|zt) can not be computed directly, we instead sample z0 as an estimated target. Results in Tab. 6 show that predicting q(z\u0304t\u22121|zt) is better on both generation and classification tasks. Inference Strategy for Image Generation. Fig. 6 studies the effect of different inference strategies. Previous works (Chang et al., 2022; Li et al., 2022) adopt cosine masking schedule in inference. However, the generation quickly get saturated when inference steps increase. When using linear masking schedule in inference, similar consistent gain can also be observed as T becomes larger."
        },
        {
            "heading": "5 CONCLUSIONS",
            "text": "In this paper, we introduce ADDP, a general representation learning framework that is applicable to both image generation and recognition tasks. Our key insights are twofold: (1) pixels as inputs are crucial for recognition tasks; (2) VQ tokens as reconstruction targets are beneficial for generation tasks. To meet the demands of both fields, we propose an Alternating Diffusion Denoising Process (ADDP) to bridge pixel and token spaces. The network is trained to optimize the evidence lower bound (ELBO). Extensive experiments demonstrate its superiority in both image generation and recognition tasks. ADDP for the first time demonstrates that general representation can be learned for both generation and dense recognition tasks.\nLimitations. ADDP currently relies on a pre-trained VQ Encoder-Decoder, which may constrain generation diversity. Future directions may also include the integration of continuous diffusion processes and scaling to higher resolutions.\nReproducibility Statement. The pseudo code of pre-training and unconditional generation can be found in Appendix. A for better understanding. Additionally, the implementation details of our method are fully discussed in Sec. 4.1 and Appendix. D, including both pre-training and various downstream tasks. For the theoretical part, the detailed derivation of ADDP is presented in Appendix. B. The source code and checkpoints are also released at https://github.com/ ChangyaoTian/ADDP.\nAcknowledgements. The work is partially supported by the National Key R&D Program of China under Grants NO. 2022ZD0161300 and 2022ZD0114900, by the National Natural Science Foundation of China under Grants 62376134, 62022048 and 62276150, and the Guoqiang Institute of Tsinghua University."
        },
        {
            "heading": "A PSUEDO CODE",
            "text": "The whole training and inference algorithms are shown in Alg. 1 and Alg. 2. Here discrete-truncnorm denotes the probability distribution used in Sec. 4.1, and Dtrain denotes the whole training dataset."
        },
        {
            "heading": "A.1 PRE-TRAINING",
            "text": "Algorithm 1 Pre-training 1: repeat 2: sample t \u223c discrete-truncnorm({1, . . . , T}) 3: sample mt,mt+1 randomly 4: sample x0 \u223c Dtrain 5: z0 \u2190 VQ-Encoder(x0) 6: zt, zt+1 \u2190 z0 \u2299 (1\u2212mt), z0 \u2299 (1\u2212mt+1) 7: z\u0304t\u22121, z\u0304t \u2190 Token-Predictor(zt),Token-Predictor(zt+1) \u25b7 q(z\u0304t\u22121|zt), q(z\u0304t|zt+1) 8: xt \u2190 VQ-Decoder(zt \u2299 (1\u2212mt) + f(z\u0304t)\u2299mt) \u25b7 p(xt|zt, z\u0304t), Eq. (7) 9: et \u2190 Encoder(xt) 10: z\u0304predt\u22121 \u2190 Decoder(et \u2299 (1\u2212mt) + emask \u2299mt) \u25b7 p\u03b8(z0, z\u0304t\u22121|xt), Eq. (2) 11: Lt \u2190 CE(z\u0304t\u22121, z\u0304predt\u22121)\u2299mt \u25b7 DKL, Eq. (6) 12: Take gradient descent step on\u2207\u03b8Lt 13: until converged"
        },
        {
            "heading": "A.2 UNCONDITIONAL GENERATION",
            "text": "Algorithm 2 Unconditional Generation 1: zT , z\u0304T \u2190 Decoder(\u2205) \u25b7 p\u03b8(zT , z\u0304T |\u2205), Eq. (2) 2: xT \u2190 VQ-Decoder(zT \u2299 (1\u2212mT ) + f(z\u0304T )\u2299mT ) \u25b7 p(xT |zT , z\u0304T ), Eq. (7) 3: for t = T, . . . , 1 do 4: et \u2190 Encoder(xt) 5: z\u0304predt\u22121 \u2190 Decoder(et \u2299 (1\u2212mt) + emask \u2299mt) \u25b7 p\u03b8(zt\u22121, z\u0304t\u22121|xt), Eq. (2) 6: sample zpredt\u22121 \u223c z\u0304 pred t\u22121\n7: zt\u22121 \u2190 zt \u2299 (1\u2212mt) + zpredt\u22121 \u2299 (mt \u2212mt\u22121) 8: z\u0304t\u22121 \u2190 f(z\u0304predt\u22121) 9: xt\u22121 \u2190 VQ-Decoder(zt\u22121 \u2299 (1\u2212mt\u22121) + z\u0304t\u22121 \u2299mt\u22121) \u25b7 p(xt\u22121|zt\u22121, z\u0304t\u22121), Eq. (7)\n10: end for 11: return x0"
        },
        {
            "heading": "B DERIVATION FOR ALTERNATING DENOISING DIFFUSION PROCESS",
            "text": "This section presents the detailed derivation for our proposed alternating denoising diffusion process.\nDiffusion Process. As shown in Sec. 3.2, the diffusion process is described as\nq(z0:T , z\u03040:T |x0) = q(z0|x0)\ufe38 \ufe37\ufe37 \ufe38 VQ-Encoder q(z\u0304T |\u2205)\ufe38 \ufe37\ufe37 \ufe38 token predictor \u00b7 T\u22121\u220f t=0 q(zt+1|zt)\ufe38 \ufe37\ufe37 \ufe38 add mask q(z\u0304t|zt+1)\ufe38 \ufe37\ufe37 \ufe38 token predictor , (8)\nwhere x0 is the given image, z0:T is the sequence of reliable VQ tokens and z\u03040:T is the sequence of unreliable VQ tokens. The diffusion process contains T +1 steps, and all tokens will be masked out at step T + 1, resulting in \u2205. For simplicity, we here shift the subscripts t one position to the right (i.e. t\u2212 1 \u2192 t). According to Bayes\u2019 Theorem, we have\nq(zt+1|zt) = q(zt|zt+1, z0) q(zt+1|z0) q(zt|z0) . (9)\nSubstituting Eq. 9 into Eq. 8 gives\nq(z0:T , z\u03040:T |x0) = q(z0|x0)\ufe38 \ufe37\ufe37 \ufe38 VQ-Encoder q(z\u0304T |\u2205)\ufe38 \ufe37\ufe37 \ufe38 token predictor q(zT |z0) \u00b7 T\u22121\u220f t=0 q(zt|zt+1, z0) q(z\u0304t|zt+1)\ufe38 \ufe37\ufe37 \ufe38 token predictor , (10)\nAlternating Denoising Process. On the other hand, Sec 3.1 shows that the alternating denoising process is described as\np\u03b8(z0:T , z\u03040:T , x0:T ) = p\u03b8(zT , z\u0304T |\u2205)\ufe38 \ufe37\ufe37 \ufe38 our model p(x0|z0)\ufe38 \ufe37\ufe37 \ufe38 VQ-Decoder \u00b7 T\u22121\u220f t=0 p(xt+1|zt+1, z\u0304t+1)\ufe38 \ufe37\ufe37 \ufe38 VQ-Decoder p\u03b8(zt, z\u0304t|xt+1)\ufe38 \ufe37\ufe37 \ufe38 our model , (11)\nwhere x0:T refers to the sequence of decoded images during denoising.\nEvidence Lower Bound. We can maximize the evidence lower bound (ELBO) as the training objective for the alternating denoising diffusion process. The cross entropy between generated image distribution p\u03b8(x0) and real image distribution q(x0) is computed as\nL =\u2212 Eq(x0) log p\u03b8(x0) =\u2212 Eq(x0) [ log \u222b p\u03b8(x0:T , z0:T , z\u03040:T )dx1:T dz0:T dz\u03040:T ] =\u2212 Eq(x0) [ log \u222b q(z0:T , z\u03040:T |x0) p\u03b8(x0:T , z0:T , z\u03040:T )\nq(z0:T , z\u03040:T |x0) dx1:T dz0:T dz\u03040:T ] \u2264\u2212 Eq(z0:T ,z\u03040:T ,x0) log [\u222b p\u03b8(x0:T , z0:T , z\u03040:T )dx1:T\nq(z0:T , z\u03040:T |x0) ] \ufe38 \ufe37\ufe37 \ufe38\nusing Jensen\u2019s inequality =\u2212 Eq(z0:T ,z\u03040:T ,x0) [ log\np(x0|z0) q(z0|x0) + log p\u03b8(zT , z\u0304T |\u2205) q(zT |z0)q(z\u0304T |\u2205)\n+ T\u22121\u2211 t=0 log \u222b p(xt+1|zt+1, z\u0304t+1)p\u03b8(zt, z\u0304t|xt+1)dxt+1 q(zt|zt+1, z0)q(z\u0304t|zt+1) ] =\u2212 Eq(z0:T ,z\u03040:T ,x0) [ log\np(x0|z0) q(z0|x0) + log p\u03b8(zT , z\u0304T |\u2205) q(zT |z0)q(z\u0304T |\u2205)\n+ T\u22121\u2211 t=0 log p\u03b8(zt, z\u0304t|xt+1 = VQ-Decoder(zt+1, z\u0304t+1))\nq(zt|zt+1, z0)q(z\u0304t|zt+1)\ufe38 \ufe37\ufe37 \ufe38 p(xt+1|zt+1, z\u0304t+1) is deterministic with VQ Decoder\n]\n=LVQ + T\u22121\u2211 t=0 Lt + LT , (12)\nwhere the inequality holds because of Jensen\u2019s inequality. The second last equality holds because we adopt an off-the-shelf VQ decoder to decode pixels from VQ tokens, and such mapping is deterministic. Therefore, the whole objective can be divided into the following terms:\nLELBO = LVQ + T\u22121\u2211 t=0 Lt + LT , (13)\nLVQ = Eq(z0,z\u03040,x0) [ \u2212 log p(x0|z0)\ufe38 \ufe37\ufe37 \ufe38\nVQ-Decoder + log q(z0|x0)\ufe38 \ufe37\ufe37 \ufe38 VQ-Encoder\n] ,\nLt =Eq(zt+1,z\u0304t+1,z0) [ DKL ( q(zt|zt+1, z0) q(z\u0304t|zt+1)\ufe38 \ufe37\ufe37 \ufe38\ntoken predictor || p\u03b8(zt, z\u0304t|xt+1 = VQ-Decoder(zt+1, z\u0304t+1))\ufe38 \ufe37\ufe37 \ufe38 our model\n)] ,\nLT = Eq(z0) [ DKL ( q(zT |z0) q(z\u0304T |\u2205)\ufe38 \ufe37\ufe37 \ufe38\ntoken predictor || p\u03b8(zT , z\u0304T |\u2205)\ufe38 \ufe37\ufe37 \ufe38 our model\n)] ,\nwhere LVQ corresponds to the training of VQ-VAE, and we omit it because we use a pre-trained VQGAN (Esser et al., 2021). L0:T are used to train our model.\nOptimizing the Evidence Lower Bound. Following the reparameterization trick in VQDiffusion (Gu et al., 2022), predicting q(zt|zt+1, z0) can be approximated by predicting the noiseless token z0. Lt can thus be written as:\nLt = Eq(zt+1,z\u0304t+1,z0) [ DKL ( \u03b4(z\u03020 = z0)q(z\u0304t|zt+1) || p\u03b8(z\u03020, z\u0304t|xt+1 = VQ-Decoder(zt+1, z\u0304t+1)) )] = Eq(zt+1,z\u0304t+1)Eq(z0,z\u0304t|zt+1,z\u0304t+1) [ log\nq(z\u0304t|zt+1) p\u03b8(z0, z\u0304t|xt+1 = VQ-Decoder(zt+1, z\u0304t+1)) ] = Eq(zt+1,z\u0304t+1)Eq(z0,z\u0304t|zt+1,z\u0304t+1) [ log\nq(z0|zt+1, z\u0304t+1)q(z\u0304t|zt+1) p\u03b8(z0, z\u0304t|xt+1 = VQ-Decoder(zt+1, z\u0304t+1))\n] + C1,\n(14) where C1 = \u2212Eq(z0,zt+1,z\u0304t+1) [ log q(z0|zt+1, z\u0304t+1) ] is a constant that can be ignored.\nNote that\nq(z0|zt+1, z\u0304t+1) = q(z0, zt+1, z\u0304t+1)\nq(zt+1, z\u0304t+1)\n= q(z\u0304t+1|z0, zt+1)q(z0, zt+1) q(z\u0304t+1|zt+1)q(zt+1) = q(z0|zt+1), (15)\nwhere q(z\u0304t+1|z0, zt+1) = q(z\u0304t+1|zt+1) because the diffusion process is a Markov chain (see Fig. 3). Therefore, we can simplify Lt as\nLt = Eq(zt+1,z\u0304t+1)Eq(z0,z\u0304t|zt+1,z\u0304t+1) [ log\nq(z0|zt+1)q(z\u0304t|zt+1) p\u03b8(z0, z\u0304t|xt+1 = VQ-Decoder(zt+1, z\u0304t+1))\n] ,\n= Eq(zt+1,z\u0304t+1) [ DKL ( q(z0|zt+1) q(z\u0304t|zt+1)\ufe38 \ufe37\ufe37 \ufe38\ntoken predictor || p\u03b8(z0, z\u0304t|xt+1 = VQ-Decoder(zt+1, z\u0304t+1))\ufe38 \ufe37\ufe37 \ufe38 our model\n)] .\n(16)\nEq. 16 shows that q(z0|zt+1) and q(z\u0304t|zt+1) are two optimization targets of our model. While q(z0|zt+1) is generally intractable, q(z\u0304t|zt+1) can serve as a good approximation (see Sec. 3.3). We adopt q(z\u0304t|zt+1) as the training target in practice. In this way, the loss for use is computed as\nLt = Eq(zt+1,z\u0304t+1) [ DKL ( q(z\u0304t|zt+1) || p\u03b8(z\u0304t|xt+1 = VQ-Decoder(zt+1, z\u0304t+1)) )] = Eq(zt+1,z\u0304t+1) [ CE\n( q(z\u0304t|zt+1)\ufe38 \ufe37\ufe37 \ufe38 token predictor , p\u03b8(z\u0304t|xt+1 = VQ-Decoder(zt+1, z\u0304t+1))\ufe38 \ufe37\ufe37 \ufe38 our model )] \u2212 C2, (17)\nwhere C2 = \u2212Eq(z\u0304t,zt+1)[q(z\u0304t|zt+1)] is a constant, and CE refers to cross entropy."
        },
        {
            "heading": "C ADDITIONAL EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "C.1 RESULTS OF VIT-B",
            "text": "We apply ADDP to ViT-B (Dosovitskiy et al., 2021) and report its performance on image generation and recognition tasks in Tab. 7. The training setting is almost the same as ViT-L, except that we train for 1600 epochs. We use the pre-trained MAGE Base model (Li et al., 2022) as the token predictor. Please refer to Sec. D.2 for implementation details.\nUnconditional Generation. We obtain \u223c 2 FID improvement over previous SOTA, demonstrating the powerful generation capacity of ADDP.\nare marked in gray .\nImage Classification. The performance of our method is comparable to those specifically designed for recognition tasks. Similar to our ViT-L model, using ViT-B as the encoder outperforms the previous best model that supports both recognition and generation by 1.4 percentage points. However, we also observe low linear probing performance, which is likely due to the noisy synthetic images as training input, as shown in Fig. 9.\nObject Detection and Semantic Segmentation. ADDP can achieve comparable performance to methods designed for recognition tasks, suggesting that our model can learn general representations suitable for dense prediction tasks."
        },
        {
            "heading": "C.2 RESULTS OF RESNET-50",
            "text": "Given that ADDP takes full images as inputs during pre-training, it is architecture-agnostic and thus can be applied to other network structures such as convolution networks. To further demonstrate this, we use ResNet50 (He et al., 2016) as the image encoder and pretrain it for 300 epochs. The performance on generation and recognition tasks are reported in Tab. 8. More implementation details can be found in Tab. 14 and Tab. 17.\nUnconditional Generation. The unconditional generation performance of our ResNet50 model is comparable to previous methods speciallly designed for generation tasks. To the best of our knowledge, this is the first time that ResNet is used as the image encoder for image generation .\nImage Classification. Our method\u2019s finetuning performance on ImageNet-1k outperforms previous supervised and self-supervised methods using ResNet50 backbone."
        },
        {
            "heading": "C.3 ROBUSTNESS EVALUATION",
            "text": "We evaluate the robustness of our model in Tab. 9. We use the ImageNet-1k finetuned model from Tab. 7, and run inference on different variants of ImageNet validation datasets (Hendrycks & Di-\nour method are marked in gray .\nMethod IN-Atop-1 IN-R top-1 IN-Sketch top-1 IN-C 1-mCE avg\nMSN (Assran et al., 2022) 37.5 50.0 36.3 53.4 44.3 MoCo-v3 (Chen et al., 2021) 32.4 49.8 35.9 55.4 43.4 MAE (He et al., 2022) 35.9 48.3 34.5 48.3 41.8 SiameseIM (Tao et al., 2023) 43.8 52.5 38.3 57.1 47.9 Ours 35.2 54.4 40.9 57.3 47.0\nTable 9: Robustness evaluation with ViT-B backbone.\netterich, 2019; Hendrycks et al., 2021b;a; Wang et al., 2019). The results show that our model can achieve on-par performances with previous best method, i.e., SiameseIM (Tao et al., 2023) that conbines contrastive learning with masked image modeling. We speculate that training with noisy synthetic images may enhance the robustness of our model."
        },
        {
            "heading": "C.4 RELIABLE TOKEN DETERMINATION MECHANISM",
            "text": "In this section, we study the effect of mechanism to determine the reliable tokens during inference. By default, we generate reliable tokens using an iterative decoding strategy following MaskGIT (Chang et al., 2022) during inference. New reliable tokens are determined based on the predicted probability p\u03b8(z\u0304t\u22121|xt) for a given timestep t. Specifically, for each masked location, we first sample a candidate token id and its corresponding probability p from the predicted distribution. Then the confidence score s of each location is calculated by adding a Gumbel noise \u03f5 to the log probability, i.e. s = log(p) + \u03c4\u03f5. Here, \u03c4 is set to be 6.0 \u00d7 tT by default. (please refer to Appendix D.4 for more details). We explore two factors here:\n(1) truncating the probability distribution employed for sampling each candidate token by using nucleus sampling (Holtzman et al., 2019), denoted by top-p;\n(2) varying the value of \u03c4 to adjust the randomness introduced by the Gumbel noise when computing the confidence scores.\nThe results in Tab. 10 and Tab. 11 imply that the default setting is optimal, yielding the lowest FID. However, it\u2019s noteworthy that the IS score benefits from slightly reducing the value of top-p and \u03c4 .\nThis suggests that disregarding tokens with excessively low confidence can enhance the quality of synthesized images."
        },
        {
            "heading": "C.5 EVALUATION ON GENERATION DIVERSITY",
            "text": "We evaluate the generation diversity of our model in Tab. 12. Although the non-VQ model ADM (Dhariwal & Nichol, 2021) without classifier guidance achieves high recall, its FID and IS are significantly worse than ours. Besides that, our method achieves both better FID, IS and recall compared to ADM (Dhariwal & Nichol, 2021) with classifier guidance and MAGE (Li et al., 2022) on unconditional generation.\nD IMPLEMENTATION DETAILS"
        },
        {
            "heading": "D.1 COMPARISON OF INPUTS FOR RECOGNITION TASKS",
            "text": "We conduct a preliminary comparison of inputs for recognition tasks in Tab. 1. Since the off-theshelf VQ tokenizer is trained with resolution 256\u00d7256, we conduct most tasks with input resolution 256 \u00d7 256 as well. Besides that, we also evaluate the performance on COCO (Lin et al., 2014) detection with resolution 1024\u00d7 1024. Image Classification. We train on ImageNet-1k (Deng et al., 2009) from scratch. The training setting mainly follows the one used in MAE (He et al., 2022), except that the input resolution is 256\u00d7 256. Detailed hyper-parameters are listed in Tab. 13. Object Detection. We train on COCO (Lin et al., 2014) dataset, with the backbone initialized from the model pre-trained on ImageNet-1k. The training settings are almost the same as in Tab. 18; only the training epoch is changed to 25. We report the results with input resolution 256\u00d7 256 and 1024\u00d7 1024, respectively. Semantic Segmentation. We train on ADE20k (Zhou et al., 2019) dataset, with the backbone initialized from the model pre-trained on ImageNet-1k. The training settings are almost the same as in Tab. 19; only the input resolution is changed to 256\u00d7 256."
        },
        {
            "heading": "D.2 PRE-TRAINING SETTING",
            "text": "Network Structure. The VQ tokenizer used for ADDP is from the off-the-shelf VQGAN (Esser et al., 2021; Chang et al., 2022) model released by MAGE (Li et al., 2022). We also use its ViT-Base model as the token predictor by default. For our encoder-decoder network, we use different models including ViT-B, ViT-L (Dosovitskiy et al., 2021) and ResNet50 (He et al., 2016) as the encoder, while the decoder is composed of 8 Transformer (Vaswani et al., 2017) blocks with 768 feature dimension (or 1024 for ViT-L). In addition, the decoder takes three independent sets of learnable positional embeddings for pixels, VQ-tokens, and <MASK> tokens inputs, respectively.\nTraining Setting. All the models are trained on ImageNet-1k (Deng et al., 2009) dataset. The total denoising step is T = 100. The values of sampled mask ratios during training are computed following cos(\u03c02 \u00b7 t T ), where t = 1, 2, . . . , T . The corresponding probability densities for these mask ratios are then calculated from the truncated normal distribution used in MAGE (Li et al., 2022) with mean and standard deviation of 0.55 and 0.25, respectively, truncated probabilities between 0.5 and 1.0. Finally, the mask ratio is sampled based on the normalized discrete probability distribution, as is shown in Fig. 12. To further adapt our model to different denoising timesteps during inference, we sample \u2206t from a uniform discrete distribution from 1 to 5 and replace zt+1 with zt+\u2206t. Detailed hyper-parameters are listed in Tab. 14.\nThe relationship between ADDP and MAGE. Similar to BEiT (Bao et al., 2022), where the output of dVAE (Ramesh et al., 2021) is used as the training target, we also take the predicted token distribution of MAGE (Li et al., 2022) as part of ADDP\u2019s training objective. However, we claim that the two methods are inherently different as MAGE is VQ-token based whereas ours is raw-pixel based. Moreover, the results in Tab. 2 and Tab. 7 also demonstrate that ADDP can achieve better performance on both generation and recognition, especially for dense recognition tasks."
        },
        {
            "heading": "D.3 APPLY TO IMAGE RECOGNITION",
            "text": "We use the pre-trained encoder as the backbone and append task-specific heads for different tasks. We mainly follow the transfer setting in MAE (He et al., 2022).\nImage Classification. We train on ImageNet-1k (Deng et al., 2009) dataset. The detailed hyperparameters of finetuning and linear probing for ViT backbone are listed in Tab. 15 and Tab. 16, while the finetuning hyper-parameters for ResNet50 are listed in Tab 17.\nObject Detection. We train on COCO (Lin et al., 2014) dataset. We follow ViTDet (Li et al., 2022) to use Mask R-CNN (He et al., 2017) as the detection head. The detailed hyper-parameters are listed in Tab. 18.\nSemantic Segmentation. We train on ADE20k (Zhou et al., 2019) dataset. We use UperNet (Xiao et al., 2018) as the segmentation head. The detailed hyper-parameters are listed in Tab. 19."
        },
        {
            "heading": "D.4 APPLY TO IMAGE GENERATION",
            "text": "We adopt an iterative decoding strategy following MaskGIT (Chang et al., 2022) when applying to image generation tasks. Given a blank canvas, the decoder first predicts zT and z\u0304T from pure <MASK> token embeddings. Then the VQ decoder generates the initial image xT based on z\u0304T only. After that, we iteratively decode more reliable tokens zt and the corresponding image xt until finally generating the noisy-free image x0.\nDuring the iteration, new reliable tokens zt for each masked location are sampled based on its prediction probability. The confidence score for each sampled token is its probability plus a Gumbel Noise, of which the temperature \u03c4 is set to be 6.0 \u00d7 tT by default. Previously generated reliable tokens zt+1 will always be kept by setting its corresponding score to 1.0 manually.\nAs for generating the next step\u2019s mask mt\u22121, we mask out the last k tokens of zt based on their prediction scores. Here the exact value of k depends on the masking schedule and the total inference steps T . Specifically, we have k = cos(\u03c02 \u00b7 T\u2212t T ) for cosine schedule and k = t T for linear schedule.\nE VISUALIZATION\nUnconditional Generation. We provide more unconditional image samples generated by ADDP in Fig. 7.\nIntermediate Generated Results. We also show some intermediate generated results in Fig. 8. Note that the masked image here only indicates the corresponding positions of reliable tokens zt in each step, whereas in real implementations we feed the entire image into our encoder.\nSynthetic Training Images. Fig. 9, Fig. 10 and Fig. 11 show some synthetic images generated by different mapping functions as training input. Qualitatively, the WeightedSum strategy synthesizes images with better quality than its couterparts and thus achieves better performance in both recognition and generation tasks, as is shown in Tab. 4.\nDistribution of Mask Ratio and Timestep for Pre-training. Fig. 12 shows the discrete distribution of mask ratio and timesteps during pre-training respectively.\nImage Inpainting and Outpainting. ADDP is able to conduct image inpainting and outpainting without further finetuning. Given a masked image, we first generate the initial image xt by filling the masked region with the average pixels of visible areas. Then the mask ratio and the corresponding timestep t is calculated based on the ratio of the masked area to the entire image. We also use VQ tokenizer to encode xt into VQ tokens zt. After that, ADDP can generate the final image by continuing the subsequent alternating denoising process. The final output is composited with the input image via linear blending based on the mask, following MaskGIT (Chang et al., 2022). Some results of image inpainting, outpainting and uncropping (outpainting on a large mask) are shown in Fig. 13, Fig. 14 and Fig. 15.\nIntermediate Generated Results with Token Space Visualization. In Fig. 16, we map the top 3 PCA components of each token embedding (both reliable and unreliable) to RGB values at each intermediate step. Such visualization illustrates that reliable tokens help enhance image sharpness and add more fine-grained details, while unreliable tokens contribute to the refinement of coarsegrained spatial contours. Collectively, these tokens maintain the spatial consistency of the generated image."
        },
        {
            "heading": "APPENDIX REFERENCES",
            "text": "Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent,\nArmand Joulin, Mike Rabbat, and Nicolas Ballas. Masked siamese networks for label-efficient learning. In ECCV, 2022.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. NeurIPS, 2020.\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2021.\nJean-Bastien Grill, Florian Strub, Florent Altche\u0301, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. NeurIPS, 2020.\nKaiming He, Georgia Gkioxari, Piotr Dolla\u0301r, and Ross Girshick. Mask r-cnn. In ICCV, 2017.\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV, 2021a.\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In CVPR, 2021b.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2019.\nYanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. In ECCV, 2022.\nHaohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. NeurIPS, 2019.\nRoss Wightman, Hugo Touvron, and Herve\u0301 Je\u0301gou. Resnet strikes back: An improved training procedure in timm. arXiv preprint arXiv:2110.00476, 2021.\nTete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In ECCV, 2018."
        }
    ],
    "title": "TERNATING DENOISING DIFFUSION PROCESS",
    "year": 2024
}