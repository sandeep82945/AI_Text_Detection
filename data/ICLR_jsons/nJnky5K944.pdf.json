{
    "abstractText": "Existing analyses of the expressive capacity of Transformer models have required excessively deep layers for data memorization, leading to a discrepancy with the Transformers actually used in practice. This is primarily due to the interpretation of the softmax function as an approximation of the hardmax function. By clarifying the connection between the softmax function and the Boltzmann operator, we prove that a single layer of self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence. As a consequence, we show that one-layer and single-head Transformers have a memorization capacity for finite samples, and that Transformers consisting of one self-attention layer with two feed-forward neural networks are universal approximators for continuous permutation equivariant functions on a compact domain.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tokio Kajitsuka"
        },
        {
            "affiliations": [],
            "name": "Issei Sato"
        }
    ],
    "id": "SP:baba195098415426baa17a04a2346e9af3f0214d",
    "references": [
        {
            "authors": [
                "Armen Aghajanyan",
                "Sonal Gupta",
                "Luke Zettlemoyer"
            ],
            "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Kavosh Asadi",
                "Michael L. Littman"
            ],
            "title": "An Alternative Softmax Operator for Reinforcement Learning",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Peter L. Bartlett",
                "Nick Harvey",
                "Christopher Liaw",
                "Abbas Mehrabian"
            ],
            "title": "Nearly-tight vc-dimension and pseudodimension bounds for piecewise linear neural networks",
            "venue": "Journal of Machine Learning Research,",
            "year": 2019
        },
        {
            "authors": [
                "Eric B Baum"
            ],
            "title": "On the capabilities of multilayer perceptrons",
            "venue": "Journal of Complexity,",
            "year": 1988
        },
        {
            "authors": [
                "Mikhail Belkin",
                "Daniel Hsu",
                "Siyuan Ma",
                "Soumik Mandal"
            ],
            "title": "Reconciling modern machinelearning practice and the classical bias\u2013variance trade-off",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2019
        },
        {
            "authors": [
                "Satwik Bhattamishra",
                "Kabir Ahuja",
                "Navin Goyal"
            ],
            "title": "On the Ability and Limitations of Transformers to Recognize Formal Languages",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 7096\u20137116,",
            "year": 2020
        },
        {
            "authors": [
                "Srinadh Bhojanapalli",
                "Chulhee Yun",
                "Ankit Singh Rawat",
                "Sashank Reddi",
                "Sanjiv Kumar"
            ],
            "title": "Low-Rank Bottleneck in Multi-head Attention Models",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Stephen Boyd",
                "Lieven Vandenberghe"
            ],
            "title": "Convex Optimization",
            "year": 2004
        },
        {
            "authors": [
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language Models are Few-Shot Learners",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Sebastien Bubeck",
                "Ronen Eldan",
                "Yin Tat Lee",
                "Dan Mikulincer"
            ],
            "title": "Network size and size of the weights in memorization with two-layers neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Carroll",
                "Dickinson"
            ],
            "title": "Construction of neural nets using the radon transform",
            "venue": "Joint Conference on Neural Networks, pp. 607\u2013611",
            "year": 1989
        },
        {
            "authors": [
                "David Chiang",
                "Peter Cholak"
            ],
            "title": "Overcoming a Theoretical Limitation of Self-Attention",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "David Chiang",
                "Peter Cholak",
                "Anand Pillay"
            ],
            "title": "Tighter Bounds on the Expressivity of Transformer Encoders, May 2023",
            "venue": "URL http://arxiv.org/abs/2301.10743",
            "year": 2023
        },
        {
            "authors": [
                "Krzysztof Marcin Choromanski",
                "Valerii Likhosherstov",
                "David Dohan",
                "Xingyou Song",
                "Andreea Gane",
                "Tamas Sarlos",
                "Peter Hawkins",
                "Jared Quincy Davis",
                "Afroz Mohiuddin",
                "Lukasz Kaiser",
                "David Benjamin Belanger",
                "Lucy J. Colwell",
                "Adrian Weller"
            ],
            "title": "Rethinking Attention with Performers",
            "venue": "URL https://openreview.net/forum?id=Ua6zuk0WRH",
            "year": 2020
        },
        {
            "authors": [
                "G. Cybenko"
            ],
            "title": "Approximation by superpositions of a sigmoidal function",
            "venue": "Mathematics of Control, Signals and Systems,",
            "year": 1989
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, May 2019",
            "venue": "URL http://arxiv. org/abs/1810.04805",
            "year": 2019
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "venue": "URL https://openreview.net/forum?id=YicbFdNTTy",
            "year": 2022
        },
        {
            "authors": [
                "Benjamin L. Edelman",
                "Surbhi Goel",
                "Sham Kakade",
                "Cyril Zhang"
            ],
            "title": "Inductive Biases and Variable Creation in Self-Attention Mechanisms",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Ken-Ichi Funahashi"
            ],
            "title": "On the approximate realization of continuous mappings by neural networks",
            "venue": "Neural Networks,",
            "year": 1989
        },
        {
            "authors": [
                "Iryna Gurevych",
                "Michael Kohler",
                "G\u00f6zde G\u00fcl Sahin"
            ],
            "title": "On the rate of convergence of a classifier based on a Transformer encoder, November 2021",
            "venue": "URL http://arxiv.org/abs/2111",
            "year": 2021
        },
        {
            "authors": [
                "Michael Hahn"
            ],
            "title": "Theoretical Limitations of Self-Attention in Neural Sequence Models. Transactions of the Association for Computational Linguistics, 8:156\u2013171",
            "venue": "ISSN 2307-387X. doi: 10.1162/tacl",
            "year": 2020
        },
        {
            "authors": [
                "Yiding Hao",
                "Dana Angluin",
                "Robert Frank"
            ],
            "title": "Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Moritz Hardt",
                "Tengyu Ma"
            ],
            "title": "Identity Matters in Deep Learning",
            "venue": "URL https: //openreview.net/forum?id=ryxB0Rtxx",
            "year": 2016
        },
        {
            "authors": [
                "Kurt Hornik"
            ],
            "title": "Approximation capabilities of multilayer feedforward networks",
            "venue": "Neural Networks,",
            "year": 1991
        },
        {
            "authors": [
                "Guang-Bin Huang"
            ],
            "title": "Learning capability and storage capacity of two-hidden-layer feedforward networks",
            "venue": "IEEE Transactions on Neural Networks,",
            "year": 2003
        },
        {
            "authors": [
                "Guang-Bin Huang",
                "H.A. Babri"
            ],
            "title": "Upper bounds on the number of hidden neurons in feedforward networks with arbitrary bounded nonlinear activation functions",
            "venue": "IEEE Transactions on Neural Networks,",
            "year": 1998
        },
        {
            "authors": [
                "Haotian Jiang",
                "Qianxiao Li"
            ],
            "title": "Approximation theory of transformer networks for sequence modeling, May 2023",
            "venue": "URL http://arxiv.org/abs/2305.18475",
            "year": 2023
        },
        {
            "authors": [
                "Junghwan Kim",
                "Michelle Kim",
                "Barzan Mozafari"
            ],
            "title": "Provable Memorization Capacity of Transformers",
            "venue": "URL https://openreview.net/forum?id=8JCg5xJCTPR",
            "year": 2023
        },
        {
            "authors": [
                "Anastasis Kratsios",
                "Behnoosh Zamanlooy",
                "Tianlin Liu",
                "Ivan Dokmani\u0107"
            ],
            "title": "Universal Approximation Under Constraints is Possible with Transformers",
            "venue": "URL https: //openreview.net/forum?id=JGO8CvG5S9",
            "year": 2021
        },
        {
            "authors": [
                "Vladislav Lialin",
                "Namrata Shivagunde",
                "Sherin Muckatira",
                "Anna Rumshisky"
            ],
            "title": "Stack More Layers Differently: High-Rank Training Through Low-Rank Updates, July 2023",
            "venue": "URL http: //arxiv.org/abs/2307.05695",
            "year": 2023
        },
        {
            "authors": [
                "Valerii Likhosherstov",
                "Krzysztof Choromanski",
                "Adrian Weller"
            ],
            "title": "On the Expressive Flexibility of Self-Attention Matrices",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Hongzhou Lin",
                "Stefanie Jegelka"
            ],
            "title": "ResNet with one-neuron hidden layers is a Universal Approximator",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Michael Lederman Littman"
            ],
            "title": "Algorithms for Sequential Decision-Making",
            "venue": "PhD thesis,",
            "year": 1996
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach, July 2019",
            "venue": "URL http://arxiv.org/abs/1907.11692. arXiv:1907.11692 [cs]",
            "year": 1907
        },
        {
            "authors": [
                "Zhou Lu",
                "Hongming Pu",
                "Feicheng Wang",
                "Zhiqiang Hu",
                "Liwei Wang"
            ],
            "title": "The Expressive Power of Neural Networks: A View from the Width",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Shengjie Luo",
                "Shanda Li",
                "Shuxin Zheng",
                "Tie-Yan Liu",
                "Liwei Wang",
                "Di He"
            ],
            "title": "Your Transformer May Not be as Powerful as You Expect",
            "year": 2022
        },
        {
            "authors": [
                "Sadegh Mahdavi",
                "Renjie Liao",
                "Christos Thrampoulidis"
            ],
            "title": "Memorization Capacity of MultiHead Attention in Transformers",
            "venue": "URL http://arxiv.org/abs/2306.02010. arXiv:2306.02010 [cs]",
            "year": 2023
        },
        {
            "authors": [
                "William Merrill",
                "Ashish Sabharwal",
                "Noah A. Smith"
            ],
            "title": "Saturated Transformers are Constant-Depth Threshold Circuits",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Preetum Nakkiran",
                "Gal Kaplun",
                "Yamini Bansal",
                "Tristan Yang",
                "Boaz Barak",
                "Ilya Sutskever"
            ],
            "title": "Deep Double Descent: Where Bigger Models and More Data Hurt",
            "venue": "URL https: //openreview.net/forum?id=B1g5sA4twr",
            "year": 2019
        },
        {
            "authors": [
                "Sejun Park",
                "Jaeho Lee",
                "Chulhee Yun",
                "Jinwoo Shin"
            ],
            "title": "Provable Memorization via Deep Neural Networks using Sub-linear Parameters",
            "venue": "In Proceedings of Thirty Fourth Conference on Learning Theory, pp. 3627\u20133661",
            "year": 2021
        },
        {
            "authors": [
                "Shashank Rajput",
                "Kartik Sreenivasan",
                "Dimitris Papailiopoulos",
                "Amin Karbasi"
            ],
            "title": "An Exponential Improvement on the Memorization Capacity of Deep Threshold Networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Emily Reif",
                "Ann Yuan",
                "Martin Wattenberg",
                "Fernanda B Viegas",
                "Andy Coenen",
                "Adam Pearce",
                "Been Kim"
            ],
            "title": "Visualizing and Measuring the Geometry of BERT",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Eduardo D. Sontag"
            ],
            "title": "Shattering All Sets of\u2018k\u2019 Points in\u201cGeneral Position\u201dRequires",
            "venue": "Neural Computation,",
            "year": 1997
        },
        {
            "authors": [
                "Shokichi Takakura",
                "Taiji Suzuki"
            ],
            "title": "Approximation and Estimation Ability of Transformers for Sequence-to-Sequence Functions with Infinite Dimensional Input",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Erik F. Tjong Kim Sang",
                "Fien De Meulder"
            ],
            "title": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition",
            "venue": "In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL",
            "year": 2003
        },
        {
            "authors": [
                "Gal Vardi",
                "Gilad Yehudai",
                "Ohad Shamir"
            ],
            "title": "On the Optimal Memorization Power of ReLU Neural Networks. January 2022",
            "venue": "URL https://openreview.net/forum?id=MkTPtnjeYTV",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is All you Need",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Roman Vershynin"
            ],
            "title": "Memory capacity of neural networks with threshold and rectified linear unit activations",
            "venue": "SIAM Journal on Mathematics of Data Science,",
            "year": 2020
        },
        {
            "authors": [
                "Sinong Wang",
                "Belinda Z. Li",
                "Madian Khabsa",
                "Han Fang",
                "Hao Ma"
            ],
            "title": "Linformer: SelfAttention with Linear Complexity, June 2020",
            "venue": "URL http://arxiv.org/abs/2006",
            "year": 2006
        },
        {
            "authors": [
                "Shunyu Yao",
                "Binghui Peng",
                "Christos Papadimitriou",
                "Karthik Narasimhan"
            ],
            "title": "Self-Attention Networks Can Process Bounded Hierarchical Languages. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
            "venue": "pp. 3770\u20133785,",
            "year": 2021
        },
        {
            "authors": [
                "Chengxuan Ying",
                "Tianle Cai",
                "Shengjie Luo",
                "Shuxin Zheng",
                "Guolin Ke",
                "Di He",
                "Yanming Shen",
                "Tie-Yan Liu"
            ],
            "title": "Do Transformers Really Perform Badly for Graph Representation",
            "venue": "URL https://openreview.net/forum?id=OeWooOxFwDa",
            "year": 2022
        },
        {
            "authors": [
                "Chulhee Yun",
                "Suvrit Sra",
                "Ali Jadbabaie"
            ],
            "title": "Small ReLU networks are powerful memorizers: a tight analysis of memorization capacity",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Chulhee Yun",
                "Srinadh Bhojanapalli",
                "Ankit Singh Rawat",
                "Sashank Reddi",
                "Sanjiv Kumar"
            ],
            "title": "Are Transformers universal approximators of sequence-to-sequence functions? December 2019",
            "venue": "URL https://openreview.net/forum?id=ByxRM0Ntvr",
            "year": 2019
        },
        {
            "authors": [
                "Chulhee Yun",
                "Yin-Wen Chang",
                "Srinadh Bhojanapalli",
                "Ankit Singh Rawat",
                "Sashank Reddi",
                "Sanjiv Kumar"
            ],
            "title": "O(n) Connections are Expressive Enough: Universal Approximability of Sparse Transformers",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Manzil Zaheer",
                "Guru Guruganesh",
                "Kumar Avinava Dubey",
                "Joshua Ainslie",
                "Chris Alberti",
                "Santiago Ontanon",
                "Philip Pham",
                "Anirudh Ravula",
                "Qifan Wang",
                "Li Yang",
                "Amr Ahmed"
            ],
            "title": "Big Bird: Transformers for Longer Sequences",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Chiyuan Zhang",
                "Samy Bengio",
                "Moritz Hardt",
                "Benjamin Recht",
                "Oriol Vinyals"
            ],
            "title": "Understanding deep learning requires rethinking generalization",
            "venue": "URL https: //openreview.net/forum?id=Sy8gdB9xx",
            "year": 2016
        },
        {
            "authors": [
                "Zhang"
            ],
            "title": "Published as a conference paper at ICLR 2024 task of a one-hidden-layer feed-forward neural network. Here we adopt the implementation",
            "year": 2016
        },
        {
            "authors": [
                "Park"
            ],
            "title": "2021), with which we configure weight matrices of a self-attention mechanism",
            "venue": "B TECHNICAL LEMMAS B.1 PROJECTION OF INPUT TOKENS We cite Lemma",
            "year": 2021
        },
        {
            "authors": [
                "Park"
            ],
            "title": "Let d \u2208 N. Then, for any finite subset X \u2282 R, there exists a unit vector v \u2208 R",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The Transformer model has been ubiquitously used in deep learning since its proposal by Vaswani et al. (2017). Its widespread application spans several domains, not only revolutionizing Natural Language Processing (NLP) through models like BERT (Devlin et al., 2019; Liu et al., 2019) and GPT (Brown et al., 2020; Radford et al., a;b) but also making significant advancements in image and graph processing as an alternative to conventional models like convolutional neural networks (CNNs) and graph neural networks (GNNs) (Dosovitskiy et al., 2022; Ying et al., 2022).\nOne of the key reasons behind the success of the Transformer model is its ability to represent a wide range of functions. Various studies investigated this aspect, including the universal approximation theorem for Transformer models and its memorization capacity Yun et al. (2019); Kim et al. (2023); Mahdavi et al. (2023); Edelman et al. (2022); Gurevych et al. (2021); Takakura & Suzuki (2023); Likhosherstov et al. (2023).\nThe main challenge in proving universal approximation theorems for Transformer models lies in the fact that the Transformer needs to account for the context of the entire input sequence. Unlike feedforward neural networks where each input is processed independently, the self-attention mechanism in Transformer models must take into account the dependencies between all elements in each input sequence. In constructive proofs (Edelman et al., 2022; Yun et al., 2019; Kim et al., 2023; Mahdavi et al., 2023; Gurevych et al., 2021; Takakura & Suzuki, 2023), these dependencies are often aggregated into a token-wise quantity, which we call a \u201ccontext id\u201d here, by a self-attention mechanism, and then feed-forward neural networks map each context id to the desired output.\nThe drawback of existing analyses is that they require excessively deep layers (Yun et al., 2019; Kim et al., 2023) or quite a lot of attention heads (Gurevych et al., 2021; Takakura & Suzuki, 2023; Likhosherstov et al., 2023) for data memorization, which leads to a discrepancy with Transformers being deployed in practice. This discrepancy primarily arises from the interpretation of the softmax function as an approximation of the hardmax function. Consequently, to compute the \u201ccontext id\u201d within the self-attention mechanism, the number of required self-attention parameters scales linearly with the length of an input sequence.\nIn this work, we address this gap by closely examining the softmax function itself. First, we show that it is impossible to output the \u201ccontext id\u201d using just one layer of self-attention with the hardmax function. At the same time, we demonstrate that just one layer of single-head and softmax-based self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence. This result implies that the Transformer with one self-attention layer is a universal approximator of continuous permutation equivariant functions by using two feedforward neural networks connected before and after the single-head self-attention mechanism with an arbitrary head size.\nOur contributions are summarized as follows.\n1. We show that one layer self-attention with the hardmax function is not a contextual mapping; that is, one layer hardmax-based Transformer has no memorization capacity.\n2. In contrast, we provide a framework for constructing a context mapping with one-layer and single-head self-attention using the softmax function.\n3. We prove that one layer Transformer has a memorization capacity for finite samples, and Transformers with one-layer and single-head self-attention are universal approximators of continuous permutation equivariant functions."
        },
        {
            "heading": "1.1 RELATED WORKS",
            "text": "Universal approximation theorems. The history of the universal approximation theorem begins around 1990 (Cybenko, 1989; Carroll & Dickinson, 1989; Hornik, 1991; Funahashi, 1989). Recent studies on this topic include analyses of how network width and depth affect the expressive power (Lu et al., 2017), and analyses for specific architectures (Lin & Jegelka, 2018). There have also been analyses of the memorization capacity of models (Baum, 1988; Huang & Babri, 1998). The main focus of the memorization capacity is mainly on the analysis of parameter efficiency for storing finite samples (Huang, 2003; Vershynin, 2020; Park et al., 2021; Vardi et al., 2022; Yun et al., 2018; Bubeck et al., 2020; Hardt & Ma, 2016; Rajput et al., 2021; Zhang et al., 2016). Notably, Zhang et al. (2016) demonstrated that a neural network of the size used in practice can perfectly memorize a randomly labeled data set. Belkin et al. (2019); Nakkiran et al. (2019) pointed out that the minimum number of parameters required to memorize a dataset is related to the double descent threshold.\nExpressive capacity of Transformer. Ever since Vaswani et al. (2017) first proposed the Transformer architecture, there have been various theoretical analyses on its expressive capacity. Yun et al. (2019) proved for the first time the universal approximation theorem for Transformer models, showing that a continuous function on a compact domain can be approximated if the number of Transformer blocks is on the order of the power of n, where n is the length of each input sequence. Later, Kim et al. (2023) showed that 2n self-attention blocks are sufficient for the memorization of finite samples. Since the studies of Yun et al. (2019) and Kim et al. (2023) are closely related to our paper, we discuss the details in more depth in Section 3.2 later. Their results were based on the assumption that the inputs are separated to some extent, which is an assumption we also make in this paper. Alternatively, under the assumption that input sequences are linearly independent, Mahdavi et al. (2023) showed that a one-layer H-head self-attention mechanism can memorize O(Hn) samples. Relatedly, Edelman et al. (2022) demonstrated that the bounded self-attention head is capable of expressing a sparse Boolean function while obtaining an upper bound on the covering number of self-attention. Gurevych et al. (2021) analyzed the theoretical performance of Transformers as a hierarchical composition model. Later, Takakura & Suzuki (2023) extended their result by utilizing a sinusoidal positional encoding and multiple heads, and showed that a one-layer Transformer with an embedding layer is a universal approximator for shift-equivariant \u03b3-smooth functions. Jiang & Li (2023) recently used the Kolmogorov representation theorem to provide a non-constructive proof of the existence of a two-layer Transformer that approximates an arbitrary continuous function on a certain domain. There are variants of universal approximation theorems for Transformers, such as analyses of sparse Transformers (Yun et al., 2020) and constrained universal approximation theorems (Kratsios et al., 2021). Likhosherstov et al. (2023) showed that, given parameters, there exists an input such that self-attention approximates an arbitrary sparse pattern. While Bhojanapalli et al. (2020) proved that Transformers with a small head size, which is typical for multi-head self-attention, cannot express certain positive column-stochastic matrices, Aghajanyan et al. (2021) demonstrated empirically that pre-trained Transformers have a very low intrinsic dimension, and\nReif et al. (2019) visualized context embeddings in BERT. Luo et al. (2022) showed the existence of functions that cannot be approximated by Transformers with relative positional encoding. There is also a series of papers analyzing Transformer\u2019s expressive capabilities from the perspective of formal languages (Hahn, 2020; Bhattamishra et al., 2020; Yao et al., 2021; Hao et al., 2022; Merrill et al., 2022; Chiang & Cholak, 2022; Chiang et al., 2023), where a softmax function in a self-attention mechanism is treated as an averaging or hardmax function. As for a negative result of Transformers\u2019 expressive capacity, Zaheer et al. (2020) demonstrated that there exist some problems which sparse Transformers must require a superlinear number of layers to solve."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "2.1 NOTATION",
            "text": "We use bold lowercase letters to represent vectors and bold uppercase letters to represent matrices. For any vector v \u2208 Ra, we denote by vi the i-the element of v. For any matrix A \u2208 Ra\u00d7b, we denote its i-th row by Ai,:, its k-th column by A:,k and the element at its i-th row and k-th column by Ai,k. For any positive integer m \u2208 N+, [m] represents the set {1, . . . ,m}. For any real numbers a < b, [a, b] represents the interval {x \u2208 R | a \u2264 x \u2264 b}, (\u2212\u221e, a) represents {x \u2208 R | x < a}, and (b,\u221e) represents {x \u2208 R | x > b}. Let \u03c3S [v] and \u03c3H [v] for any input vector v be the softmax function and hardmax function, respectively. Note that when there are multiple indices with maximum values, the hardmax function is defined such that the sum of the values at these indices equals one. By abuse of notation, for any input matrix A, \u03c3S [A] and \u03c3H [A] are defined as column-wise softmax and column-wise hardmax, respectively. We denote the ReLU activation function by \u03c3R. Unlike \u03c3S and \u03c3H , \u03c3R is always an element-wise operator, regardless of whether the input is a vector or a matrix. Let \u2225 \u00b7 \u2225 be the \u21132 norm and \u2225 \u00b7 \u2225p (1 \u2264 p < \u221e) be the \u2113p norm. We define the distance between two functions f1, f2 : Rd\u00d7n \u2192 Rd\u00d7n by\ndp (f1, f2) := (\u222b \u2225f1(X)\u2212 f2(X)\u2225pp dX )1/p . (1)\nIn this paper, n denotes the length of an input sequence, N the number of input sequences, C the number of output classes, and d the embedding dimension. In addition, i, j are basically used for the indices of finite samples and k, l for the indices in each input sequence."
        },
        {
            "heading": "2.2 TRANSFORMER BLOCK",
            "text": "Transformer was first introduced in Vaswani et al. (2017). Here we follow the definitions adopted in Kim et al. (2023): the Transformer block is composed of the self-attention mechanism and the feed-forward neural network, each accompanied by a skip connection. Given an input sequence Z \u2208 Rd\u00d7n, composed of n tokens each with an embedding dimension of size d, a dot-product self-attention mechanism with h heads outputs the following values:\nF (SA)S (Z) = Z + h\u2211\ni=1\nW (O) i\n( W\n(V ) i Z ) \u03c3S [( W (K) i Z )\u22a4 ( W (Q) i Z )] \u2208 Rd\u00d7n, (2)\nwhere W (V )i , W (K) i , W (Q) i \u2208 Rs\u00d7d and W (O) i \u2208 Rd\u00d7s are the weight matrices, and s is the head size. Note that here, as with Yun et al. (2019) and Kim et al. (2023), we adopt the definition of the self-attention mechanism, which excludes layer normalization from the original definition of Vaswani et al. (2017) for the sake of simplicity.\nIn contrast, given an input H \u2208 Rd\u00d7n, the output of feed-forward neural network with a skip connection at index k \u2208 [n] is\nF (FF ) (H):,k = H:,k +W (2)\u03c3R [ W (1)H:,k + b (1) ] + b(2) \u2208 Rd, (3)\nwhere q is the hidden dimension, W (1) \u2208 Rq\u00d7d and W (2) \u2208 Rd\u00d7q are weight matrices, and b(1) \u2208 Rq and b(2) are bias terms. On the basis of the above definition, the Transformer block is represented as a composition of a self-attention mechanism and a feed-forward neural network: for any input sequence Z \u2208 Rd\u00d7n,\ncomposed of n tokens each with an embedding dimension of size d, the Transformer block F : Rd\u00d7n \u2192 Rd\u00d7n outputs\nF (Z) = F (FF ) ( F (SA)S (Z) ) . (4)\nFrom the above definition, we see that the interaction of each token occurs only in the self-attention mechanism."
        },
        {
            "heading": "3 ATTENTION IS A CONTEXTUAL MAPPING",
            "text": ""
        },
        {
            "heading": "3.1 PROBLEM SETTING",
            "text": "Let (X(1),Y (1)), . . . , (X(1),Y (1)) \u2282 Rd\u00d7n \u00d7 [C]d\u00d7n be an N input-output pairs of sequences, each of which consists of a sequence X(i) of n tokens with embedding dimension d, and an output Y (i), where Y (i):,k corresponds to the label of the token X (i) :,k at index k. In addition, we define the\ni-th vocabulary set for i \u2208 [N ] by V(i) = \u22c3\nk\u2208[n] X (i) :,k \u2282 Rd, and the whole vocabulary set V is defined by V = \u22c3\ni\u2208[N ] V(i) \u2282 Rd."
        },
        {
            "heading": "3.2 BACKGROUND",
            "text": "Yun et al. (2019) proved affirmatively one of the most fundamental questions on the expressive capacity of Transformer models, namely, whether the universal approximation theorem for Transformer models holds. Their proof approach is to quantize the input domain and reduce the universal approximation theorem to the memorization analysis of finite samples, i.e., the construction of a model that achieves zero loss for a finite number of training data, which was also analyzed later by Kim et al. (2023). In the analysis of memorization capacity, assumptions are usually made on the inputs in order to perform a meaningful analysis beyond the lower bound of Sontag (1997). Here, as with the assumptions adopted by Yun et al. (2019); Kim et al. (2023), we assume that the input tokens are separated by a certain distance.\nDefinition 1 (Tokenwise Separatedness). Let m \u2208 N and Z(1), . . . ,Z(N) \u2208 Rm\u00d7n be input sequences. Then, Z(1), . . . ,Z(N) are called tokenwise (rmin, rmax, \u03b4)-separated if the following three conditions hold.\n1. For any i \u2208 [N ] and k \u2208 [n], \u2225\u2225\u2225Z(i):,k\u2225\u2225\u2225 > rmin holds.\n2. For any i \u2208 [N ] and k \u2208 [n], \u2225\u2225\u2225Z(i):,k\u2225\u2225\u2225 < rmax holds.\n3. For any i, j \u2208 [N ] and k, l \u2208 [n] with Z(i):,k \u0338= Z (j) :,l , \u2225\u2225\u2225Z(i):,k \u2212Z(j):,l \u2225\u2225\u2225 > \u03b4 holds. Note that we refer to Z(1), . . . ,Z(N) as tokenwise (rmax, \u03f5)-separated instead if the sequences satisfy conditions 2 and 3.\nThe achievement of Yun et al. (2019) was not only to prove the universal approximation theorem for Transformers, but also to clarify the difficulties in the analysis of this kind of expressive capacity of Transformers and elucidated an approach to establishing the proof. Namely, what makes Transformers\u2019 memorization different from that of feed-forward neural networks is that Transformers need to capture the context of each input sequence as a whole, rather than simply associating each token with a label.\nRemarkably, Yun et al. (2019); Kim et al. (2023) formulated this concept as a contextual mapping, which assigns a unique id to a pair of an input sequence and each of their tokens. We define it here using the notion of (r, \u03b4)-separatedness.\nDefinition 2 (Contextual Mapping). Let X(1), . . . ,X(N) \u2208 Rd\u00d7n be input sequences. Then, a map q : Rd\u00d7n \u2192 Rd\u00d7n is called an (r, \u03b4)-contextual mapping if the following two conditions hold:\n1. For any i \u2208 [N ] and k \u2208 [n], \u2225\u2225\u2225q (X(i))\n:,k\n\u2225\u2225\u2225 < r holds.\n2. For any i, j \u2208 [N ] and k, l \u2208 [n] such that V(i) \u0338= V(j) or X(i):,k \u0338= X (j) :,l ,\u2225\u2225\u2225q (X(i))\n:,k \u2212 q\n( X(j) ) :,l \u2225\u2225\u2225 > \u03b4 holds. In particular, q(X(i)) for i \u2208 [N ] is called a context id of X(i).\nIf we have such a contextual mapping, a label sequence can be associated with a unique id for each input sequence using the existing analysis of memorization in feed-forward neural networks.\nThus, the central question is: how to construct a contextual mapping in Transformer models? The only place in Transformer models where interaction between tokens can be taken into account is in the self-attention mechanism; therefore, the self-attention mechanism must be used to construct a contextual mapping. Yun et al. (2019) first constructed a contextual mapping by using |V|d + 1 self-attention layers1, and later Kim et al. (2023) improved it to 2n self-attention layers. However, this is still far from the practical implementation of Transformers, and it remains unclear whether a reasonably-sized Transformer would possess such memorization capacity or if the universal approximation theorem would hold. This leads to the following question.\nHow many self-attention layers are both necessary and sufficient to construct a contextual mapping?\nWe first point out the reason for requiring a significant number of self-attention layers in the construction of contextual mapping in the analyses of Yun et al. (2019); Kim et al. (2023). Their approach entails interpreting the softmax function in the self-attention mechanism as an approximation of the hardmax function, which also hinders a detailed analysis of the specific properties of the softmax function. As evidence of this, we illustrate in Section 3.3 that using a single layer of self-attention with the hardmax function does not suffice to construct a contextual mapping.\nNext, in Section 3.4, we demonstrate that a contextual mapping can be constructed by using only one self-attention layer with the softmax function. This is somewhat surprising because this implies the probability of fully capturing the context of each input sequence only through the attention coefficients computed by the pairwise dot-product of the softmax function and its weighted average."
        },
        {
            "heading": "3.3 SELF-ATTENTION WITH HARDMAX",
            "text": "In previous studies analyzing the memorization capacity of Transformers (Yun et al., 2019; Kim et al., 2023), the softmax function is taken to be an approximation of the hardmax function. However, we show here that the attention block with the hardmax function is not a contextual mapping.\nFirst we define the attention block with the hardmax function: for an input sequence Z \u2208 Rd\u00d7n, the attention with the hardmax function is calculated as\nF (SA)H (Z) = Z + h\u2211\ni=1\nW (O) i\n( W\n(V ) i Z ) \u03c3H [( W (K) i Z )\u22a4 ( W (Q) i Z )] , (5)\nwhere W (V )i , W (K) i , W (Q) i \u2208 Rs\u00d7d and W (O) i \u2208 Rd\u00d7s are the weight matrices\nThe following theorem holds for such a model. The proof is in Appendix A.1.\nTheorem 1. 1-layer multi-head self-attention F (SA)H with the hardmax function cannot be a contextual mapping.\nSince the self-attention mechanism is the only place in Transformer models where interaction between tokens happens, this theorem indicates that one-layer Transformers with hardmax attention do not have a memorization capacity."
        },
        {
            "heading": "3.4 SELF-ATTENTION WITH SOFTMAX",
            "text": "In this subsection, we show that a softmax-based 1-layer attention block with low-rank weight matrices is a contextual mapping for almost all input sequences. This result is consistent with recent\n1To be precise, when the continuous input range is quantized into 1/\u03b4 pieces for some 0 < \u03b4 < 1, they demonstrated that there exists a contextual mapping composed of \u03b4\u2212d self-attention layers.\nempirical evidence that pre-trained Transformers are low-rank (Aghajanyan et al., 2021; Choromanski et al., 2020; Wang et al., 2020; Lialin et al., 2023), and theoretically supports that the low-rank self-attention mechanism is sufficient to fully comprehend the contextual information of an input sequence. It is worth noting that our construction allows for an arbitrary head size. By considering the case of a head size of 1, this particularly indicates that the self-attention mechanism has the ability to compress the information of an input sequence through a scalar value.\nTheorem 2. Let X(1), . . . ,X(N) \u2208 Rd\u00d7n be input sequences with no duplicate word token in each sequence, that is,\nX (i) :,k \u0338= X (i) :,l (6)\nfor any i \u2208 [N ] and k, l \u2208 [n]. Also assume that X(1), . . . ,X(N) are tokenwise (rmin, rmax, \u03f5)separated. Then, there exist weight matrices W (O) \u2208 Rd\u00d7s and W (V ),W (K),W (Q) \u2208 Rs\u00d7d such that the ranks of W (V ),W (K) and W (Q) are all 1, and 1-layer single head attention with softmax, i.e., F (SA)S with h = 1 is an (r, \u03b4)-contextual mapping for the input sequences X(1), . . . ,X(N) \u2208 Rd\u00d7n with r and \u03b4 defined by\nr = rmax + \u03f5\n4 , (7)\n\u03b4 = 2(log n)2\u03f52rmin\nr2max(|V|+ 1)4(2 log n+ 3)\u03c0d exp\n( \u2212 (|V|+ 1)4 (2 log n+ 3)\u03c0dr 2 max\n4\u03f5rmin\n) . (8)\nHere we provide a simple proof sketch. The full proof can be found in Appendix A.2.\nProof Overview. For simplicity, we here assume s = 1. If we have a unique id, i.e., sequence id, corresponding to each input sequence X(i) for i \u2208 [N ], a context id can be constructed from a suitable linear combination of the sequence id and the value of each token. Since this linear combination can be calculated by the output projection matrix W (O) and skip connection, the problem is how to configure weight parameters W (V ),W (K),W (Q) \u2208 R1\u00d7d so that each row of the values\u2019 softmax weighted average, (\nW (V )X(i) ) \u03c3S [( W (K)X(i) )\u22a4 ( W (Q)X(i) )] \u2208 R1\u00d7n, (9)\noutputs the unique sequence id of X(i).\nActually, an even weaker condition is sufficient for an attention block to be a contextual mapping: there is no need to have just one unique sequence id for each input sequence. In fact, it is possible to construct a contextual mapping, provided that for each token v \u2208 V , input sequences in which the token appears can be identified by some v-dependent sequence ids. This condition can be expressed in a mathematical form as follows: what we have to show is to construct weight matrices W (V ),W (K),W (Q) \u2208 R1\u00d7d with some \u03f5 > 0 such that\u2223\u2223\u2223\u2223(W (V )X(i))\u03c3S [(W (K)X(i))\u22a4 (W (Q)X(i):,k)]\n\u2212 ( W (V )X(j) ) \u03c3S [( W (K)X(j) )\u22a4 ( W (Q)X (j) :,l )]\u2223\u2223\u2223\u2223 > \u03f5 (10) holds for any distinct i, j \u2208 [N ] and any k, l \u2208 [n] such that X(i):,k = X (j) :,l and V(i) \u0338= V(j).\nFor simplicity, we choose W (V ) = W (K) = W (Q) = w\u22a4 2 such that the linear operator w \u2208 Rd projects each token to a scalar value while approximately preserving the distance between each pair of tokens: for any pair of tokens va,vb \u2208 V ,\nc\u2225va \u2212 vb\u2225 \u2264 \u2223\u2223w\u22a4va \u2212w\u22a4vb\u2223\u2223 \u2264 \u2225va \u2212 vb\u2225 (11)\n2In our actual proof, there exist unit vectors v,v\u2032 \u2208 Rd such that W (V ),W (K) and W (Q) may be defined by W (V ) = u\u2032\u2032v\u22a4,W (K) = u\u2032v\u22a4 and W (Q) = uv\u2032\u22a4 for arbitrary vectors u,u\u2032,u\u2032\u2032 \u2208 Rs satisfying certain constraints.\nwith some constant 0 < c < 1. Then, by using the assumption t = X(i):,k = X (j) :,l for some token t \u2208 Rd, we have\u2223\u2223w\u22a4t\u2223\u2223 \u00b7 \u2223\u2223\u2223\u2223(w\u22a4X(i))\u03c3S [(w\u22a4X(i))\u22a4 (w\u22a4t)]\u2212 (w\u22a4X(j))\u03c3S [(w\u22a4X(j))\u22a4 (w\u22a4t)]\u2223\u2223\u2223\u2223 \u2265 \u2223\u2223\u2223\u2223(a(i))\u22a4 \u03c3S [a(i)]\u2212 (a(j))\u22a4 \u03c3S [a(j)]\u2223\u2223\u2223\u2223 , (12) where we denote a(i) = ( w\u22a4X(i) )\u22a4 ( w\u22a4t ) \u2208 Rn and a(j) = ( w\u22a4X(j) )\u22a4 ( w\u22a4t ) \u2208 Rn. Therefore, in order to prove that a self-attention block serves as a contextual mapping, we only have to focus on the separability of the function\nboltz : Rn \u2192 R,a 7\u2192 a\u22a4\u03c3S [a], (13) which is known as the Boltzmann operator (Littman, 1996; Asadi & Littman, 2017).\nThe following lemma shows that the Boltzmann operator is a mapping that projects input sequences to scalar values while preserving some distance, and is central to our proof that the self-attention function is a contextual mapping.\nLemma 1. Let a(1), . . . ,a(m) \u2208 Rn be tokenwise (r, \u03b4)-separated vectors with no duplicate element in each vector and \u03b4 > 2 log n+ 3. (14) Then, the outputs of the Boltzmann operator are (r, \u03b4\u2032)-separated, that is,\u2223\u2223\u2223boltz(a(i))\u2223\u2223\u2223 \u2264 r (15)\u2223\u2223\u2223boltz(a(i))\u2212 boltz(a(j))\u2223\u2223\u2223 > \u03b4\u2032 = (log n)2e\u22122r (16) hold for each i, j \u2208 [m] with a(i) \u0338= a(j).\nTaking into account the above arguments, this separability of the Boltzmann operator allows us to construct one self-attention layer to be a contextual mapping.\nRemark 1 (Masked self-attention). In practice, attention matrices are often masked to avoid directing attention to undesired tokens. This is performed, for example, for autoregressive text generation or padding of inputs with different lengths. It is relatively straightforward to extend Theorem 2 to masked self-attention mechanisms. See Appendix C for more details."
        },
        {
            "heading": "4 APPLICATIONS OF CONTEXTUAL MAPPING",
            "text": ""
        },
        {
            "heading": "4.1 MEMORIZATION CAPACITY OF ONE-LAYER TRANSFORMER",
            "text": "As a first application of Theorem 2, we prove that a 1-layer Transformer can completely memorize finite samples, each of which has no duplicate token. This result emphasizes that in contrast to the proof of Kim et al. (2023), which requires 2n self-attention layers for Transformer memorization, one layer of self-attention is actually sufficient. In addition, it is worth noting that the hardmaxbased Transformers do not have a memorization capacity, which is implied straightforwardly from Theorem 1. Corollary 1 (Memorization capacity of one-layer Transformer). Let \u03f5 > 0, rmax > rmin > 0 and (X(1),Y (1)), . . . , (X(N),Y (N)) \u2282 Rd\u00d7n \u00d7 [C]d\u00d7n be sequences of input-output-pairs such that X(1), . . . ,X(N) are tokenwise (rmin, rmax, \u03f5)-separated input sequences with no duplicate token in each sentence and consistently labeled, that is, Y (i):,k = Y (j) :,l holds for any i, j \u2208 [N ] and k, l \u2208 [n] such that V(i) = V(j) and X(i):,k = X (j) :,l .\nThen, there exist 4(s+ d) + d(2nN + d) weight parameters such that for any i \u2208 [N ] F ( X(i) ) = F (FF ) ( F (SA)S ( X(i) )) = Y (i) (17)\nholds.\nRemark 2 (Parameter efficiency). To achieve the memorization with a one-layer Transformer, the one-hidden-layer feed-forward block has to map each context id to the corresponding label. Since the possible number of context ids is at most nN in the worst case, the linear dependency on nN of the number of parameters in Corollary 1 is optimal up to logarithmic factors (Bartlett et al., 2019). It is worth mentioning that this linear dependency can be relaxed to the milder requirement O\u0303 (\u221a nN )\nby allowing for deeper layers in the feed-forward block (Vardi et al., 2022), under the assumption that the size |V| of the vocabulary set is independent of n and N .\nIn addition, it is straightforward to show that a 1-layer Transformer with trainable positional encodings has a memorization capacity for arbitrary input sequences possibly with duplicate tokens. Corollary 2 (Memorization capacity of one-layer Transformer with positional encodings). Let \u03f5 > 0, rmax > rmin > 0 and (X(1),Y (1)), . . . , (X(N),Y (N)) \u2282 Rd\u00d7n\u00d7[C]d\u00d7n be sequences of inputoutput-pairs such that X(1), . . . ,X(N) are tokenwise (rmin, rmax, \u03f5)-separated input sequences and are consistently labeled, that is, Y (i) = Y (j) holds for any i, j \u2208 [N ] such that X(i) = X(j). Then, there exist 4(s + d) + d(2nN + d) weight parameters and positional encodings E \u2208 Rd\u00d7n such that for any i \u2208 [N ],\nF ( X(i) +E ) = F (FF ) ( F (SA)S ( X(i) +E )) = Y (i) (18)\nholds."
        },
        {
            "heading": "4.2 TRANSFORMERS WITH ONE SELF-ATTENTION LAYER ARE UNIVERSAL APPROXIMATORS",
            "text": "As a further application of Theorem 2 we here provide a proof that Transformer with one selfattention layer is a universal approximator. More precisely, let FPE be the set of all permutation equivariant continuous functions that take values on a compact domain in Rd\u00d7n, and let T2 be the set of all two layer Transformers with one-layer and single-head self-attention, that is,\nT2 = { F (FF )2 \u25e6 F (SA) S \u25e6 F (FF ) 1 : R n\u00d7d \u2192 Rn\u00d7d } , (19)\nwhere F (FF )1 ,F (FF ) 2 and F (SA) S are feed-forward neural network layers and a single-head selfattention layer with the softmax function, respectively. Then the following proposition holds (see the definition (1)). Proposition 1 (Transformers with one layer self-attention are universal approximators). Let 1 \u2264 p < \u221e. Then, for any f \u2208 FPE and \u03f5 > 0, there exists a Transformer g \u2208 T2 with one-layer and single-head self-attention such that dp(f, g) < \u03f5. holds.\nTo the best of our knowledge, this is the first universal approximation theorem for two-layer Transformers with a self-attention of realistic size. Takakura & Suzuki (2023) showed that a one-layer Transformer with an embedding layer is capable of approximating shift-equivariant \u03b3-smooth functions. However, their construction requires a high number of self-attention heads and a large head size to flatten an input sequence into outputs of self-attention. Jiang & Li (2023) used the Kolmogorov representation theorem to give a non-constructive proof of the universal approximation theorem of two-layer Transformers with positional encoding for continuous functions on a compact domain. Acknowledging the contributions of prior studies, we would like to emphasize the novelty of our results again, that is, Transformers using a single-head self-attention are universal approximators for continuous permutation equivariant functions on an arbitrary compact domain, thanks to Theorem 2. Our result can be readily extended for continuous but not necessarily permutation equivariant functions on a compact domain by using positional encoding, and at the same time is significant from the perspective of geometric deep learning."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "As shown in Theorem 2, a self-attention mechanism with rank 1 weight matrices already has enough expressive capacity to become a contextual mapping. In particular, its proof leads us to consider the following simplified form of a self-attention mechanism: for any input sequence Z \u2208 Rd\u00d7n,\nF (R1)S (Z) = Z +W (O) ( v\u22a41 Z ) \u03c3S [( v\u22a41 Z )\u22a4 ( v\u22a42 Z )] \u2208 Rd\u00d7n, (20)\nwhere v1,v2 \u2208 Rd and W (O) \u2208 Rd\u00d71 are weight matrices. This architecture corresponds to a common self-attention with the head size s = 1, and value and query matrices having the same weight vector v1. In this section, we test whether Transformers with self-attention layers replaced by equation 20, which we call rank-1 Transformers, actually have the theoretically predicted expressive capacity by using a real-world dataset.\nWe train rank-1 Transformers on a token classification task with the CoNLL-2003 (Tjong Kim Sang & De Meulder, 2003) dataset. The batch size is 32 and the training are conducted over 400 epochs.\nWe train three different depths of rank-1 transformers on the dataset and do not use layer normalization to match the situation with our theoretical analysis.\nFigure 1 shows training accuracies of 1-layer, 3-layer and 6-layer rank-1 Transformers on each task over 400 epochs. It can be seen that the 1-layer rank-1 Transformer is already able to memorise the CoNLL-2003 dataset almost perfectly. On the other hand, while the accuracy curve for the 1- layer rank-1 Transformer shows that the accuracy is still increasing steadily at 400 epochs, reaching 0.9872 at 800 epochs, its rate of increase is much slower than for the 3-layer and 6-layer Transformers.\nFrom this observation, we conjecture that while theoretically 1-layer Transformers already have a memorisation capacity for finite samples, the advantage of deepening layers lies in speeding up the learning of such tasks. Since our analysis is on the expressive capabilities of Transformers, we leave this hypothesis on the optimisation aspect of Transformers as a future work."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "We demonstrated that a contextual mapping can be implemented in one-layer and single-head selfattention with low-rank matrices, by clarifying the connection between a self-attention mechanism and the Boltzmann operator. This particularly indicates that one-layer Transformers have a memorization capacity for finite samples, and that Transformers with one-layer and single-head selfattention are universal approximators for continuous permutation equivariant functions on a compact domain. Our proof of the universal approximation theorem requires one feed-forward neural network layer before the self-attention layer to quantize continuous inputs. We leave it as future work to clarify whether the one-layer Transformers without such a quantization layer are universal approximators or not. We also expect that our analysis of the softmax function will have an impact on the evaluation of Transformer\u2019s expressive capability from the perspective of formal languages."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported by JSPS KAKENHI Grant Number 20H04239 Japan. We would like to thank all the collaborators and anonymous reviewers for constructive discussions."
        },
        {
            "heading": "NOTATION TABLE",
            "text": "Numbers and Arrays a A scalar a A vector A A matrix n The length of an input sequence N The number of input sequences C The number of output classes d Embedding dimension\nX(i) i-th input sequence, consisting of n tokens of embedding dimension d"
        },
        {
            "heading": "Sets",
            "text": "R Set of real numbers N+ Set of positive integers [m] Set of all integers from 1 to m [a, b] Closed interval from a to b\nV(i) i-th vocabulary set"
        },
        {
            "heading": "Indexing",
            "text": "ai Element i of vector a, with indexing starting at 1 Ai,j Element i, j of matrix A A:,i Column i of matrix A Ai,: Row i of matrix A Functions\n\u2225x\u2225 \u21132 norm of x \u2225x\u2225p \u2113p norm of x 1condition is 1 if the condition is true, 0 otherwise\ndp(f1, f2) (\u222b \u2225f1(X)\u2212 f2(X)\u2225pp dX )1/p\n\u03c3S Softmax function \u03c3H Hardmax function \u03c3R ReLU activation function\nF (SA)H Hardmax-based self-attention mechanism with a skip-connection\nF (SA)S Softmax-based self-attention mechanism with a skip-connection F (FF ) Feed-forward neural network with a skip-connection boltz Boltzmann opeartor"
        },
        {
            "heading": "A PROOF OF MAIN RESULTS",
            "text": "First, we introduce the Boltzmann operator, which frequently appears in our proofs. Definition 3. (Boltzmann operator) The Boltzmann operator is defined by\nboltz : Rm \u2192 R, a 7\u2192 a\u22a4\u03c3S [a]. (21) By abuse of notation, we use the same notation boltz for various dimension m \u2208 N+."
        },
        {
            "heading": "A.1 PROOF OF THEOREM 1",
            "text": "Proof. Let v \u2208 Rd be an arbitrary nonzero vector, and consider the situation that all input tokens can be written as V = {\u03b11v, \u03b12v, \u03b13v, \u03b14v} \u2282 Rd (22) for some scalars \u03b11 < \u03b12 < \u03b13 < \u03b14. Then, the attention matrix inside the hardmax function at head i can be expressed as(\nW (K) i va\n\u22a4 )\u22a4 (\nW (Q) i va\n\u22a4 ) = a ( W\n(K) i v\n)\u22a4 ( W\n(Q) i v\n) a\u22a4 (23)\nwith input coefficients a \u2208 {\u03b11, \u03b12, \u03b13, \u03b14}n \u2282 Rn. In particular, when we focus on a certain index, at which the token is, e.g., \u03b12v, the above expression can further be written as(\nW (K) i va\n\u22a4 )\u22a4 (\nW (Q) i \u03b12v\n) = a ( W\n(K) i v\n)\u22a4 ( W\n(Q) i v ) \u03b12\n= ( W\n(K) i v\n)\u22a4 ( W\n(Q) i v ) \u03b12\ufe38 \ufe37\ufe37 \ufe38\n\u2208R\n\u00b7a (24)\nThe right-hand side is a vector a multiplied by some scalar. So it is evident that the maximum value of the vector on the right-hand side is achieved only at the indices where the values of the input sequence va\u22a4 are \u03b11 or \u03b14. This implies that a self-attention with the hardmax function invariably gets distracted by the indices where \u03b11 or \u03b14 are present, thereby overlooking information from other tokens in the input sequence. As a result, no matter how many heads there are, one-layer self-attention with the hardmax function cannot distinguish input sequences, e.g., (\u03b11v, \u03b12v, \u03b14v) and (\u03b11v, \u03b13v, \u03b14v)."
        },
        {
            "heading": "A.2 PROOF OF THEOREM 2",
            "text": "Proof of Theorem 2. Recall that a softmax-based self-attention function F (SA)S : Rd\u00d7n \u2192 Rd\u00d7n with h = 1 is defined as\nF (SA)S (Z) = Z +W (O) ( W (V )Z ) \u03c3S [( W (K)Z )\u22a4 ( W (Q)Z )] , (25)\nwhere W (O) \u2208 Rd\u00d7s and W (V ),W (K),W (Q) \u2208 Rs\u00d7d are weight matrices.\nWe construct a softmax-based self-attention function F (SA)S with the property that\u2225\u2225\u2225\u2225W (O) (W (V )X(i))\u03c3S [(W (K)X(i))\u22a4 (W (Q)X(i):,k)]\u2225\u2225\u2225\u2225 < \u03f54 (26) holds for any input sequence X(i) with i \u2208 [N ] and index k \u2208 [n]. When this property is fulfilled, it is easy to show that\u2225\u2225\u2225\u2225F (SA)S (X(i))\n:,k \u2225\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225X(i):,k\u2225\u2225\u2225+ \u2225\u2225\u2225\u2225W (O) (W (V )X(i))\u03c3S [(W (K)X(i))\u22a4 (W (Q)X(i):,k)]\u2225\u2225\u2225\u2225 < rmax + \u03f5\n4 (27)\nholds for any i \u2208 [N ] and k \u2208 [n], and also\u2225\u2225\u2225\u2225F (SA)S (X(i)) :,k \u2212F (SA)S ( X(j) ) :,l \u2225\u2225\u2225\u2225 \u2265 \u2225\u2225\u2225X(i):,k \u2212X(j):,l \u2225\u2225\u2225\u2212 \u2225\u2225\u2225\u2225W (O) (W (V )X(i))\u03c3S [(W (K)X(i))\u22a4 (W (Q)X(i):,k)]\u2225\u2225\u2225\u2225\n\u2212 \u2225\u2225\u2225\u2225W (O) (W (V )X(j))\u03c3S [(W (K)X(j))\u22a4 (W (Q)X(j):,l )]\u2225\u2225\u2225\u2225\n> \u03f5\u2212 \u03f5 4 \u2212 \u03f5 4 = \u03f5 2 (28)\nfor any i, j \u2208 [N ] and k, l \u2208 [n] such that X(i):,k \u0338= X (j) :,l . So all that remains to prove is to construct a self-attention function F (SA) that has the properties described above and can also distinguish input tokens X(i):,k = X (j) :,l such that V(i) \u0338= V(j).\nLet \u03b4 = 2 log n+ 3 and fix any vectors u,u\u2032 \u2208 Rs with\n\u2223\u2223u\u22a4u\u2032\u2223\u2223 = (|V|+ 1)4 \u03c0d 8 \u03b4 \u03f5rmin . (29)\nThen, according to Lemma 3 with \u03b4 = 2 log n + 3, we see that there exists a unit vector v \u2208 Rd such that \u2223\u2223\u2223\u2223(W (K)va)\u22a4 (W (Q)vc)\u2212 (W (K)vb)\u22a4 (W (Q)vc)\u2223\u2223\u2223\u2223 > \u03b4, (30)\n1\n(|V|+ 1)2\n\u221a 8\n\u03c0d \u2225vc\u2225 \u2264 \u2223\u2223v\u22a4vc\u2223\u2223 \u2264 \u2225vc\u2225 (31) for any va,vb,vc \u2208 V with va \u0338= vb, where W (K) = uv\u22a4 \u2208 Rs\u00d7d and W (Q) = u\u2032v\u22a4 \u2208 Rs\u00d7d.\nFurthermore, we configure W (O) \u2208 Rd\u00d7s and W (V ) \u2208 Rs\u00d7d to be W (V ) = u\u2032\u2032v\u22a4 for any nonzero vector u\u2032\u2032 \u2208 Rs such that \u2225\u2225\u2225W (O)u\u2032\u2032\u2225\u2225\u2225 = \u03f5\n4rmax (32)\nholds. This can be accomplished, e.g., W (O) = u\u2032\u2032\u2032u\u2032\u2032\u22a4 for any vector u\u2032\u2032\u2032 \u2208 Rd which satisfies \u2225u\u2032\u2032\u2032\u2225 = \u03f5/(4rmax \u2225u\u2032\u2032\u22252). In this case, the value of the self-attention without a skip-connection is\nupper-bounded by\u2225\u2225\u2225\u2225W (O) (W (V )X(i))\u03c3S [(W (K)X(i))\u22a4 (W (Q)X(i):,k)]\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225 n\u2211\nk\u2032=1\nskk\u2032W (O) ( W (V )X(i) ) :,k\u2032 \u2225\u2225\u2225\u2225\u2225 with skk\u2032 = \u03c3S [( W (K)X(i) )\u22a4 ( W (Q)X (i) :,k )] k\u2032\n\u2264 n\u2211\nk\u2032=1\nskk\u2032 \u2225\u2225\u2225\u2225W (O) (W (V )X(i)) :,k\u2032 \u2225\u2225\u2225\u2225 \u2264 max\nk\u2032\u2208[n] \u2225\u2225\u2225\u2225W (O) (W (V )X(i)) :,k\u2032 \u2225\u2225\u2225\u2225 (from n\u2211 k\u2032=1 skk\u2032 = 1)\n= max k\u2032\u2208[n] \u2225\u2225\u2225W (O)u\u2032\u2032v\u22a4X(i):,k\u2032\u2225\u2225\u2225 = \u2225\u2225\u2225W (O)u\u2032\u2032\u2225\u2225\u2225 \u00b7 max\nk\u2032\u2208[n] \u2223\u2223\u2223v\u22a4X(i):,k\u2032 \u2223\u2223\u2223 \u2264 \u03f5\n4rmax \u00b7 max k\u2032\u2208[n] \u2225\u2225\u2225X(i):,k\u2032\u2225\u2225\u2225 (from equation 31 and equation 32) (33) < \u03f5\n4 , (34)\nwhich means that equation 27 and equation 28 are satisfied with the weight matrices defined above.\nNow, we see that the weight matrices W (O),W (V ),W (K),W (Q) configured above can distinguish the most subtle pattern of input tokens, i.e. X(i):,k = X (j) :,l with V(i) \u0338= V(j).\nPick up any i, j \u2208 [N ] and k, l \u2208 [n] such that X(i):,k = X (j) :,l and V(i) \u0338= V(j). In addition, we define a(i),a(j) by\na(i) = ( W (K)X(i) )\u22a4 ( W (Q)X\n(i) :,k\n) \u2208 Rn, (35)\na(j) = ( W (K)X(j) )\u22a4 ( W (Q)X\n(j) :,l\n) \u2208 Rn. (36)\nThen, equation 30 and equation 31 imply that a(i) and a(j) are tokenwise (r, \u03b4)-separated, where r is defined by\nr = (|V|+ 1)4 \u03c0d 8 \u03b4r2max \u03f5rmin , (37)\nbecause for any k\u2032 \u2208 [n], we have\u2223\u2223\u2223a(i)k\u2032 \u2223\u2223\u2223 = \u2223\u2223\u2223\u2223(W (K)X(i):,k\u2032)\u22a4 (W (Q)X(i):,k)\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223(v\u22a4X(i):,k\u2032)\u22a4\u2223\u2223\u2223\u2223 \u00b7 \u2223\u2223u\u22a4u\u2032\u22a4\u2223\u2223 \u00b7 \u2223\u2223\u2223(v\u22a4X(i):,k)\u2223\u2223\u2223 \u2264 (|V|+ 1)4 \u03c0d\n8\n\u03b4\n\u03f5rmin r2max (from equation 29 and equation 31), (38)\nand the same upper-bound also holds for a(j).\nSince V(i) \u0338= V(j) and there exists no duplicate token in X(i) and X(j) respectively, it follows from Lemma 1 that \u2223\u2223\u2223boltz(a(i))\u2212 boltz(a(j))\u2223\u2223\u2223 > \u03b4\u2032 = (log n)2e\u22122r, (39) that is, \u2223\u2223\u2223\u2223(a(i))\u22a4 \u03c3S [a(i)]\u2212 (a(j))\u22a4 \u03c3S [a(j)]\u2223\u2223\u2223\u2223 > \u03b4\u2032. (40)\nSince X(i):,k = X (j) :,l by assumption, equation 40 are further expanded as\n\u03b4\u2032 < \u2223\u2223\u2223\u2223(a(i))\u22a4 \u03c3S [a(i)]\u2212 (a(j))\u22a4 \u03c3S [a(j)]\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223(X(i):,k)\u22a4 (W (Q))\u22a4 W (K) (X(i)\u03c3S [a(i)]\u2212X(j)\u03c3S [a(j)])\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223(X(i):,k)\u22a4 vu\u2032\u22a4uv\u22a4 (X(i)\u03c3S [a(i)]\u2212X(j)\u03c3S [a(j)])\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223v\u22a4X(i):,k \u2223\u2223\u2223 \u00b7 \u2223\u2223u\u22a4u\u2032\u2223\u2223 \u00b7 \u2223\u2223\u2223(v\u22a4X(i))\u03c3S [a(i)]\u2212 (v\u22a4X(j))\u03c3S [a(j)]\u2223\u2223\u2223\n\u2264 rmax \u00b7 (|V|+ 1)4 \u03c0d\n8\n\u03b4 \u03f5rmin \u00b7 \u2223\u2223\u2223(v\u22a4X(i))\u03c3S [a(i)]\u2212 (v\u22a4X(j))\u03c3S [a(j)]\u2223\u2223\u2223 , (41)\nwhere the last inequality follows from equation 29 and equation 31.\nTherefore, the gap between the outputs of th self-attention function for X(i) and X(j) are lowerbounded as follows:\u2225\u2225\u2225\u2225F (SA)S (X(i))\n:,k \u2212F (SA)S\n( X(j) ) :,l \u2225\u2225\u2225\u2225 = \u2225\u2225\u2225W (O) (W (V )X(i))\u03c3S [a(i)]\u2212W (O) (W (V )X(j))\u03c3S [a(j)]\u2225\u2225\u2225 (\u2235 X(i):,k = X(j):,l )\n= \u2225\u2225\u2225W (O)u\u2032\u2032\u2225\u2225\u2225 \u00b7 \u2223\u2223\u2223(v\u22a4X(i))\u03c3S [a(i)]\u2212 (v\u22a4X(j))\u03c3S [a(j)]\u2223\u2223\u2223\n> \u03f5 4rmax \u00b7 \u03b4\n\u2032\n(|V|+ 1)4 8\u03f5rmin \u03c0d\u03b4rmax , (42)\nwhere \u03b4 and \u03b4\u2032 are defined respectively as\n\u03b4 = 2 log n+ 3, (43)\n\u03b4\u2032 = (log n)2e\u22122r with r = (|V|+ 1)4 \u03c0d 8 \u03b4r2max \u03f5rmin . (44)\nBy plugging \u03b4 and \u03b4\u2032, the above inequality is simplified as\u2225\u2225\u2225\u2225F (SA)S (X(i)) :,k \u2212F (SA)S ( X(j) ) :,l \u2225\u2225\u2225\u2225 >\n2(log n)2\u03f52rmin r2max(|V|+ 1)4(2 log n+ 3)\u03c0d exp\n( \u2212 (|V|+ 1)4 (2 log n+ 3)\u03c0dr 2 max\n4\u03f5rmin\n) . (45)"
        },
        {
            "heading": "A.3 PROOF OF COROLLARY 1",
            "text": "Proof. According to Theorem 2, we can construct such self-attention to be contextual mapping, that is, there exist weight matrices W (O) \u2208 Rd\u00d7s and W (V ),W (K),W (Q) \u2208 Rs\u00d7d such that F (SA)S with h = 1 is a (r, \u03b4)-contextual mapping for the input sequences X(1), . . . ,X(N) with r and \u03b4 defined by\nr = rmax + \u03f5\n4 , (46)\n\u03b4 = \u03f5rmin log n\nr2max(|V|+ 1)4(2 log n+ 3)\u03c0d \u00b7 exp ( \u2212 (|V|+ 1)4 (2 log n+ 3)\u03c0dr 2 max\n4\u03f5rmin\n) . (47)\nSo what remains to do is to associate each context id with the corresponding output label using a feed-forward neural network F (FF ). Construction of such a network is a typical memorization\ntask of a one-hidden-layer feed-forward neural network. Here we adopt the implementation from Zhang et al. (2016). In this case, since the possible number of context ids is upper-bounded by nN , the required parameters for the FF layer with output dimension d is at most d \u00d7 (2nN + d) (Zhang et al., 2016). As for the self-attention layer, rank-1 weight matrices W (O) \u2208 Rd\u00d7s and W (V ),W (K),W (Q) \u2208 Rs\u00d7d all require s + d parameters each. Thus, the number of parameters of the self-attention layer is 4(s + d). In conclusion, the total number of parameters for one-layer Transformers to memorize the dataset is at most 4(s+ d) + d(2nN + d)."
        },
        {
            "heading": "A.4 PROOF OF COROLLARY 2",
            "text": "Proof. First, we define the positional encoding matrix E \u2208 Rd\u00d7n as follows:\nE =  2rmax 4rmax . . . 2nrmax... ... . . . ... 2rmax 4rmax . . . 2nrmax  . (48) Then, X(1)+E, . . . ,X(N)+E are tokenwise (rmax, (2n+1)rmax, \u03f5)-separated, and each sentence has no duplicate token.\nFrom Theorem 2, there exist weight matrices W (O) \u2208 Rd\u00d7s and W V ,WK ,WQ \u2208 Rs\u00d7d such that F (SA)S with h = 1 is a (r, \u03b4)-contextual mapping for the input sequences X(1)+E, . . . ,X(N)+E with r and \u03b4 defined by\nr = (2n+ 1)rmax + \u03f5\n4 , (49)\n\u03b4 = 2(log n)2\u03f52\n(2n+ 1)2rmax(nN + 1)4(2 log n+ 3)\u03c0d \u00b7 exp ( \u2212(2n+ 1)2 (nN + 1)4 (2 log n+ 3)\u03c0drmax\n4\u03f5\n) , (50)\nbecause the size of the vocabulary set of X(1)+E, . . . ,X(N)+E is at most nN . Hence, hereafter we do the same thing as in the proof of Corollary 1, that is, implementing a feed-forward neural network F (FF ) which associates each context id with the corresponding label. The total number of parameters required to implement this construction can be evaluated in the same manner as in Corollary 1."
        },
        {
            "heading": "A.5 PROOF OF PROPOSITION 1",
            "text": "Proof. We show the propositioin by the same steps as in Yun et al. (2019). Namely,\n1. First, given a permutation equivariant continuous function f \u2208 FPE defined on a compact set, it follows from typical analysis that f can be approximated by a step function with arbitrary precision in terms of p-norm. Therefore, to show a universal approximation theorem, it is sufficient to show that such a step function can be approximated by a Transformer with one self-attention layer.\n2. Second, we use a first feed-forward neural network layer F (FF )1 to quantize the input domain, reducing the problem to memorization of finite samples.\n3. Then, by a similar analysis as in Corollary 1, it can be shown that a combination of the self-attention layer F (SA) and F (FF )2 can memorize the step function almost everywhere, in the sense that quantized input domains corresponding to sentences with duplicate tokens are negligibly small.\nWe hereafter provide rough proofs of the three steps outlined above, because there are multiple ways to construct a model that satisfies the above requirements, and we do not pursue the efficiency of feed-forward neural networks in this paper.\nFirst, without loss of generality, we ignore skip-connections in F (FF )1 and F (FF ) 2 .\n1. Since f is a continuous function on a compact set, f has maximum and minimum values on the domain. By scaling with F (FF )1 and F (FF ) 2 , f is assumed to be normalized without\nloss of generality: for any Z \u2208 Rd\u00d7n \\ [0, 1]d\u00d7n\nf(Z) = 0, (51)\nand for any Z \u2208 [\u22121, 1]d \u00d7n\n\u22121 \u2264 f(Z) \u2264 1. (52) Let D \u2208 N be the granularity of a grid\nGD = {1/D, 2/D, . . . , 1}d\u00d7n \u2282 Rd\u00d7n (53) such that a piece-wise constant approximation\nf(Z) = \u2211\nL\u2208GD\nf (L)1Z\u2208L+[\u22121/D,0)d\u00d7n (54)\nsatisfies\ndp(f, f) < \u03f5/3. (55)\nSuch a D always exists because of uniform continuity of f .\n2. We use F (FF )1 to quantize the input domain into GD. For any small \u03b4 > 0, the following \u03b4-approximated step function can be constructed with one-hidden-layer feed-forward neural network: for any z \u2208 R\n\u03c3R [z/\u03b4]\u2212 \u03c3R [z/\u03b4 \u2212 1] D =  0 z < 0\nz/\u03b4D 0 \u2264 z < \u03b4 1/D \u03b4 \u2264 z . (56)\nBy shifting and stacking this step function, we have an approximated multiple-step function D\u22121\u2211 t=0 \u03c3R [z/\u03b4 \u2212 t/\u03b4D]\u2212 \u03c3R [z/\u03b4 \u2212 1\u2212 t/\u03b4D] D\n\u2248 quantD(z) =  0 z < 0 1/D 0 \u2264 z < 1/D ...\n... 1 1\u2212 1/D \u2264 z\n, (57)\nand subtracting the last step function from it, D\u2211 t=1 \u03c3R [z/\u03b4 \u2212 t/\u03b4D]\u2212 \u03c3R [z/\u03b4 \u2212 1\u2212 t/\u03b4D] D \u2212 (\u03c3R [z/\u03b4 \u2212 1/\u03b4]\u2212 \u03c3R [z/\u03b4 \u2212 1\u2212 1/\u03b4])\n(58)\napproximately quantize [0, 1] into {1/D, . . . , 1}, while it projects R \\ [0, 1] to 0. These operations can be realized by one-hidden-layer neural network, and it is straightforward to approximate its extension quantD to dimension d \u00d7 n, which we denote quantd\u00d7nD : Rd\u00d7n \u2192 Rd\u00d7n. In addition to that, we also add a penalty term, with which we identify whether an input sequence is in [0, 1]d\u00d7n or not. This is defined by\n\u03c3R [(z \u2212 1)/\u03b4]\u2212 \u03c3R [(z \u2212 1)/\u03b4 \u2212 1]\u2212 \u03c3R [\u2212z/\u03b4]\u2212 \u03c3R [\u2212z/\u03b4 \u2212 1]\n\u2248 penalty(z) =  \u22121 z \u2264 0 0 0 < z \u2264 1 \u22121 1 < z , (59)\nwhich can also be implemented by one-hidden-layer feed-forward neural network.\nCombining these components together, the first feed-forward neural network layer F (FF )1 approximates the following function:\nF (FF )1 (Z) = quantd\u00d7nD (Z) + d\u2211\nt=1 n\u2211 k=1 penalty(Zt,k) (60)\nNote that this function quantizes inputs in [0, 1]d\u00d7n with granularity D, while every element of the output is non-positive for inputs outside [0, 1]d\u00d7n. In particular, the norm of the output is upper-bounded by\nmax Z\u2208Rd\u00d7n \u2225\u2225\u2225F (FF )1 (Z):,k\u2225\u2225\u2225 = dn \u00b7 \u221ad (61) for any k \u2208 [n].\n3. Let G\u0303D \u2282 GD be a sub-grid\nG\u0303D = {L \u2208 GD | \u2200k, l \u2208 [n], L:,k \u0338= L:,l} , (62)\nand consider memorization of G\u0303D with its labels given by f(L) for each L \u2208 G\u0303D. Note that the label sets are consistent because f is a permutation equivariant function. Then, Theorem 2 allows us to construct a self-attention F (SA) to be a contextual mapping for such input sequences, because G\u0303D can be regarded as tokenwise (1/D, \u221a d, 1/D)-separated input sequences, each of which has no duplicate token by definition. The idea is that when the granularity D of GD is sufficiently large, the number of cells with duplicate tokens, that is, |GD \\ G\u0303D| is negligible compared to the total number |GD| of cells, and thus the memorization of G\u0303D suffices for universal approximation theorem.\nFrom the way the self-attention F (SA) is constructed, we have\u2225\u2225\u2225F (SA)S (Z):,k \u2212Z:,k\u2225\u2225\u2225 < 1 4 \u221a dD max k\u2032\u2208[n] \u2225Z:,k\u2032\u2225 (63)\nfor any k \u2208 [n] and Z \u2208 Rd\u00d7n. This follows from the fact that X(i) in equation 33 may actually be replaced with any input sequence Z, because v in equation 33 is a unit vector. In particular, combining this upper-bound with equation 61, we have\u2225\u2225\u2225F (SA)S \u25e6 F (FF )1 (Z:,k)\u2212F (FF ) (Z:,k)\u2225\u2225\u2225 < dn4D. (64) Thus, if we take large enough D, every element of the output for Z \u2208 Rd\u00d7n \\ [0, 1]d\u00d7n is upper-bounded by\nF (SA)S \u25e6 F (FF ) 1 (Z)t,k <\n1\n4D (\u2200t \u2208 [d], k \u2208 [n]), (65)\nwhile the output for Z \u2208 [0, 1]d\u00d7n is lower-bounded by\nF (SA)S \u25e6 F (FF ) 1 (Z)t,k >\n3\n4D (\u2200t \u2208 [d], k \u2208 [n]). (66)\nTherefore, what remains to show is construct a feed-forward neural network F (FF )2 which associates the context id of each L \u2208 G\u0303D \u2282 (3/4D,\u221e)d\u00d7n to its corresponding label, while it outputs 0 for any input matrix Z \u2208 (\u2212\u221e, 1/4D)d\u00d7n. This can be accomplished by usual bump-functions. Precisely, construct a bump function of scale R > 0\nbumpR(Z) = f(L)\ndn d\u2211 t=1 n\u2211 k=1 (\u03c3R [R(Zt,k \u2212 Lt,k)\u2212 1]\u2212 \u03c3R [R(Zt,k \u2212 Lt,k)] (67)\n+ \u03c3R [R(Zt,k \u2212 Lt,k) + 1]) (68)\nfor each input sequence L \u2208 G\u0303D and add up these functions to implement F (FF )2 .\nFor large enough R > 0, F (FF )2 maps each input sequence L \u2208 G\u0303D to its labels f(L) and Z \u2208 (\u2212\u221e, 1/4D)d\u00d7n to 0. In addition, the value of F (FF )2 is always bounded: 0 \u2264 F (FF )2 \u2264 1. Thus, by taking sufficiently small \u03b4 > 0, we have\ndp ( F (FF )2 \u25e6 F (SA) S \u25e6 F (FF ) 1 ,F (FF ) 2 \u25e6 F (SA) S \u25e6 F (FF ) 1 ) < \u03f5\n3 . (69) Lastly, there are |GD \\ G\u0303D| = O ( D\u2212d |GD| ) input sequences with duplicate tokens, each corresponding to a cell of area D\u2212d. Thus, by taking sufficiently large D, areas of GD\\G\u0303D becomes negligible and\ndp ( F (FF )2 \u25e6 F (SA) S \u25e6 F (FF ) 1 , f ) < \u03f5\n3 . (70)\nCombining equation 55, equation 69 and equation 70 together, we get the approximation error of the Transformer:\ndp ( F (FF )2 \u25e6 F (SA) S \u25e6 F (FF ) 1 , f ) < \u03f5. (71)"
        },
        {
            "heading": "B TECHNICAL LEMMAS",
            "text": ""
        },
        {
            "heading": "B.1 PROJECTION OF INPUT TOKENS",
            "text": "We cite Lemma 13 in Park et al. (2021), with which we configure weight matrices of a self-attention mechanism. Lemma 2 (Park et al. (2021)). Let d \u2208 N. Then, for any finite subset X \u2282 Rd, there exists a unit vector v \u2208 Rd such that\n1\n|X |2\n\u221a 8\n\u03c0d \u2225x\u2212 x\u2032\u2225 \u2264 \u2223\u2223v\u22a4 (x\u2212 x\u2032)\u2223\u2223 \u2264 \u2225x\u2212 x\u2032\u2225 (72) holds for any x,x\u2032 \u2208 X .\nThe following lemma follows immediately from Lemma 2. We use W (K) and W (Q) in the lemma as low-rank weight matrices.3\nLemma 3. Given (rmin, rmax, \u03f5)-separated finite vocabulary V \u2282 Rd with rmin > 0. Then, for any \u03b4 > 0, there exists a unit vector v \u2208 Rd such that for any vectors u,u\u2032 \u2208 Rs with\u2223\u2223u\u22a4u\u2032\u2223\u2223 = (|V|+ 1)4 \u03c0d\n8\n\u03b4\n\u03f5rmin , (73)\nwe have \u2223\u2223\u2223\u2223(W (K)va)\u22a4 (W (Q)vc)\u2212 (W (K)vb)\u22a4 (W (Q)vc)\u2223\u2223\u2223\u2223 > \u03b4, (74) 1\n(|V|+ 1)2\n\u221a 8\n\u03c0d \u2225vc\u2225 \u2264 \u2223\u2223v\u22a4vc\u2223\u2223 \u2264 \u2225vc\u2225 (75) for any va,vb,vc \u2208 V with va \u0338= vb, where W (K) = uv\u22a4 \u2208 Rs\u00d7d and W (Q) = u\u2032v\u22a4 \u2208 Rs\u00d7d.\nProof. By applying Lemma 2 to V \u222a {0}, we know that there exists a unit vector v \u2208 Rd such that for any va,vb \u2208 V \u222a {0} such that va \u0338= vb, we have\n1\n(|V|+ 1)2\n\u221a 8\n\u03c0d \u2225va \u2212 vb\u2225 \u2264 \u2223\u2223v\u22a4 (va \u2212 vb)\u2223\u2223 \u2264 \u2225va \u2212 vb\u2225 . (76) 3It is easy to show that different unit vectors v,v\u2032 \u2208 Rd may be used for W (K) and W (Q), respectively,\nthat is, W (K) = uv\u22a4 and W (Q) = u\u2032v\u2032\u22a4, as long as v and v\u2032 satisfy equation 72.\nIn particular, this means that for any vc \u2208 V 1\n(|V|+ 1)2\n\u221a 8\n\u03c0d \u2225vc\u2225 \u2264 \u2223\u2223v\u22a4vc\u2223\u2223 \u2264 \u2225vc\u2225 (77) holds. Thus, pick up arbitrary vectors u,u\u2032 \u2208 Rs with\u2223\u2223u\u22a4u\u2032\u2223\u2223 = (|V|+ 1)4 \u03c0d\n8\n\u03b4\n\u03f5rmin , (78)\nand by setting W (K) = uv\u22a4 \u2208 Rs\u00d7d and W (Q) = u\u2032v\u22a4 \u2208 Rs\u00d7d, we have\u2223\u2223\u2223\u2223(W (K)va)\u22a4 (W (Q)vc)\u2212 (W (K)vb)\u22a4 (W (Q)vc)\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223(va \u2212 vb)\u22a4 (W (K))\u22a4 (W (Q)vc)\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223(va \u2212 vb)\u22a4 v\u2223\u2223\u2223 \u00b7 \u2223\u2223u\u22a4u\u2032\u2223\u2223 \u00b7 \u2223\u2223v\u22a4vc\u2223\u2223\n\u2265 1 (|V|+ 1)2\n\u221a 8\n\u03c0d \u2225va \u2212 vb\u2225 \u00b7 (|V|+ 1)4\n\u03c0d\n8\n\u03b4 \u03f5rmin \u00b7 1 (|V|+ 1)2\n\u221a 8\n\u03c0d \u2225vc\u2225\n> \u03b4, (79) where the last inequality follows from (rmin, rmax, \u03f5)-separatedness of V ."
        },
        {
            "heading": "B.2 PROPERTIES OF BOLTZMANN OPERATOR",
            "text": "For any vector a = (a1, . . . , an) \u2208 Rn, let denote p = (p1, . . . , pn) = \u03c3S [a]. In addition, we define the Boltzmann operator, partition function, and entropy as follows.\nboltz(a) = n\u2211 i=1 aipi, (80)\nL(a) = log\n( n\u2211\ni=1\neai ) , (81)\nS(p) = \u2212 n\u2211\ni=1\npi log pi. (82)\nThe following lemma shows that the Boltzmann operator is monotonically decreasing when the maximum and the rest of the arguments are far enough apart. Lemma 4 (Monotonicity). Let n \u2208 N+, i \u2208 [n] and a = (a1, . . . , an) \u2208 Rn. Then, the derivative of the Boltzmann operator boltz(a) = a\u22a4\u03c3S [a] with respect ai is\n\u2202\n\u2202ai boltz(a) = pi(1 + log pi + S(p)). (83)"
        },
        {
            "heading": "In particular, the Boltzmann operator is monotonically decreasing with respect to ai whenever",
            "text": "ai < max\nj\u2208[n] aj \u2212 log n\u2212 1 (84)\nholds.\nProof. Since\nL(a)\u2212 S(p) = log  n\u2211 j=1 eaj + n\u2211 k=1 pk log pk\n= n\u2211 k=1 pk log pk n\u2211 j=1 eaj  =\nn\u2211 k=1 pk log e ak = n\u2211 k=1 pkak = boltz(a), (85)\nwe have \u2202\n\u2202ai boltz(a) =\n\u2202 \u2202ai L(a)\u2212 \u2202 \u2202ai S(p). (86)\nNotice that \u2202pi \u2202aj = \u2202 \u2202aj eai\u2211n k=1 e ak\n=\n\u2202 \u2202aj\neai \u00b7 \u2211n\nk=1 e ak \u2212 eai \u00b7 \u2202\u2202aj \u2211n k=1 e ak\n( \u2211n\nk=1 e ak)\n2\n= \u03b4ije\naj \u00b7 \u2211n\nk=1 e ak \u2212 eai \u00b7 eaj ( \u2211n\nk=1 e ak)\n2 = pj(\u03b4ij \u2212 pi) (87)\nholds for any i, j \u2208 [n]. Here \u03b4ij is the Kronecker delta, that is,\n\u03b4ij = { 0 if i \u0338= j, 1 if i = j.\n(88)\nThus, \u2202L(a) \u2202ai = eai\u2211n j=1 e aj = pi, (89)\n\u2202S(p) \u2202ai = n\u2211 j=1 \u2202S(p) \u2202pj \u00b7 \u2202pj \u2202ai\n= n\u2211 j=1 \u2202 \u2202pj\n( \u2212\nn\u2211 k=1 pk log pk\n) \u00b7 pi(\u03b4ji \u2212 pj)\n= n\u2211 j=1 (\u2212 log pj \u2212 1) \u00b7 pi(\u03b4ji \u2212 pj)\n= \u2212pi n\u2211\nj=1\n[\u03b4ji(log pj + 1)\u2212 pj log pj \u2212 pj ]\n= \u2212pi (log pi + 1 + S(p)\u2212 1) = \u2212pi(log pi + S(p)). (90)\nPlugging equation 89 and equation 90 into equation 86, we have \u2202\n\u2202ai boltz(a) =\n\u2202 \u2202ai L(a)\u2212 \u2202 \u2202ai S(p)\n= pi + pi(log pi + S(p)) = pi(1 + log pi + S(p)). (91)\nIn particular, the derivative of the Boltzmann operator at index i is negative when 1 + S(p) + log pi < 0 \u21d4 1 + S(p) + (ai \u2212 L(a)) < 0\n\u21d4 ai < L(a)\u2212 S(p)\u2212 1. (92) Since maxj\u2208[n] aj \u2264 L(a)(p.72, Boyd & Vandenberghe (2004)) and S(p) \u2264 log n, we have \u2202 \u2202ai boltz(a) < 0 whenever\nai < max j\u2208[n]\naj \u2212 log n\u2212 1 (93)\nholds.\nLemma 5 (Concavity). Let n \u2208 N+, i \u2208 [n] and a = (a1, . . . , an) \u2208 Rn. Then, the Boltzmann operator boltz(a) is concave with respect to ai, that is,\n\u22022\n\u2202a2i boltz(a) < 0 (94)\nholds in a domain where a satisfies ai < max\nj\u2208[n] aj \u2212 log n\u2212 3. (95)\nProof. According to Lemma 4, we have \u2202\n\u2202ai boltz(a) = pi(1 + log pi + S(p)). (96)\nThus, \u22022\n\u2202a2i boltz(a) =\n\u2202\n\u2202ai [pi(1 + log pi + S(p))]\n= \u2202pi \u2202ai \u00b7 (1 + log pi + S(p)) + pi \u00b7 \u2202 \u2202ai (1 + log pi + S(p))\n= pi(1\u2212 pi) \u00b7 (1 + log pi + S(p)) + pi \u00b7 [ pi(1\u2212 pi)\npi \u2212 pi(log pi + S(p)) ] = pi [(1\u2212 2pi)(log pi + S(p) + 1) + 1] , (97)\nwhere we used equation 87 and equation 90.\nHereafter, we show that the right-hand side of the above equality is negative under the assumption that equation 95 holds. First, the fact that maxj\u2208[n] aj \u2264 L(a)(p.72, Boyd & Vandenberghe (2004)) and S(p) \u2264 log n implies\nai < max j\u2208[n]\naj \u2212 log n\u2212 3 < L(a)\u2212 S(p)\u2212 3. (98)\nIt follows from ai \u2212 L(a) = log pi that ai < max\nj\u2208[n] aj \u2212 log n\u2212 3 \u21d2 log pi < \u2212S(p)\u2212 3. (99)\nNext, since the entropy S(p) is always non-negative, we have log pi < \u22123 under equation 95. By using an inequality e\u2212x \u2264 11+x for x > \u22121, this implies\npi < e \u22123 \u2264 1\n1 + 3 =\n1 4 . (100)\nThus, as long as equation 95 holds, we have (1\u2212 2pi)(log pi + S(p) + 1) + 1 < ( 1\u2212 2 \u00b7 1\n4\n) \u00b7 (\u22122) + 1 = 0, (101)\nwhich in turn implies \u2202 2\n\u2202a2i boltz(a) < 0.\nLemma 6. Let n \u2265 2 and a = (a1, . . . , an\u22121, an), b = (b1, . . . bn\u22121, bn) \u2208 Rn be sequences such that the first n\u2212 1 elements of a and b match, that is, ai = bi for all i \u2208 [n\u2212 1]. In addition, if\nmax i\u2208[n\u22121]\nai \u2212 \u03b4 > an > bn (102)\nwith \u03b4 > log n+ 3, the difference boltz(a)\u2212 boltz(b) is lower-bounded by\nboltz(b)\u2212 boltz(a) > (an \u2212 bn)(\u03b4 + an \u2212 bn \u2212 log n\u2212 1) \u00b7 ebn\u2211n i=1 e bi . (103)\nProof. According to the monotonicity (Lemma 4) and concavity (Lemma 5) of the Boltzmann operator, we have\nboltz(b1, . . . , bn\u22121, x) + (an \u2212 x) \u00b7 \u2202\n\u2202x boltz(b1, . . . , bn\u22121, x) > boltz(b1, . . . , bn\u22121, an)\n(104) for any x < an. In particular, by setting x = bn and using equation 83, this implies\nboltz(b)\u2212 boltz(a) > (an \u2212 bn) \u00b7 ( \u2212 \u2202\n\u2202x boltz(b1, . . . , bn\u22121, x) \u2223\u2223\u2223\u2223 x=bn ) = (an \u2212 bn) \u00b7 [\u2212pn(1 + log pn + S(p))]\n> (an \u2212 bn) \u00b7 [ \u2212pn ( 1 + bn \u2212max\ni\u2208[n] bi + log n )] > (an \u2212 bn) \u00b7 pn(\u03b4 + an \u2212 bn \u2212 log n\u2212 1), (105)\nwhere p = (p1, . . . , pn) \u2208 Rn is the softmax function of b, i.e., p = \u03c3S [b]."
        },
        {
            "heading": "B.3 PROOF OF LEMMA 1",
            "text": "Before moving on to the proof, we first illustrate the proof sketch of Lemma 1 using a simple example.\nSince the Boltzmann operator is permutation invariant, without loss of generality we assume the elements of a(i) and a(j) are sorted in descending order, e.g., a(i) = (8, 6, 5) and a(j) = (4, 3, 1). In this case, since the Boltzmann operator boltz can be regarded as an approximation of max, we have boltz(a(i)) \u2248 8 > 4 \u2248 boltz(a(j)), (106) and so the Boltzmann operator can readily separate these two inputs.\nThe subtle case is where the initial tokens of a(i) and a(j) are identical, but the rest of each sequences differs, like a(i) = (8, 6, 5) and a(j) = (8, 3, 1). However, a closer observation reveals that if the first coordinate and the second one of the input a \u2208 Rn are well-separated, then boltz(a) is monotonically decreasing for each coordinate k = 2, . . . , n. In the above example, this intuitively implies\nboltz ( a(j) ) \u2265 boltz ( a(j) ) with a(j) = (8, 3, 3) (107)\nand boltz ( a(i) ) \u2264 boltz ( a(i) ) with a(i) = (8, 6,\u2212\u221e), (108)\nor by abuse of notation, boltz ( a(i) ) \u2264 boltz (8, 6).\nThus, if we can show boltz ( a(i) ) < boltz ( a(j) ) , then boltz ( a(i) ) < boltz ( a(j) ) holds and\nwe know that the Boltzmann operator can also separate this pattern of inputs. Fortunately, this is indeed the case if each element of the inputs is sufficiently separated, because\nboltz ( a(j) ) = 8e8 + 3e3 + 3e3\ne8 + e3 + e3 =\n8e8 + 2 \u00b7 3e3\ne8 + 2 \u00b7 e3\n\u2248 8e 8 + (3 + log 2)e3+log 2\ne8 + \u00b7e3+log 2 = boltz (8, 3 + log 2) , (109)\nand we have boltz (8, 6) < boltz (8, 3 + log 2) by the monotonicity of the Boltzmann operator.\nProof of Lemma 1. We only have to show the case of m = 2, and for notational convenience, a(1) (resp. a(2)) hereafter is denoted by a (resp. b). Also, since the Boltzmann operator is permutation invariant, we assume without loss of generality a1 > \u00b7 \u00b7 \u00b7 > an and b1 > \u00b7 \u00b7 \u00b7 > bn (Since there is no duplicate element in a, ak is strictly greater than al for any k < l. The same holds for b).\nFirst, since the Boltzmann operator can be regarded as weighting averaging, we have\n|boltz(a)| \u2264 max(|a1|, |an|) < r. (110) For \u03b4\u2032-separatedness, if a \u0338= b, w.l.o.g. we assume that there exists k \u2208 {0, . . . , n\u2212 1} such that\n(a1, . . . , ak) = (b1, . . . , bk) and ak+1 > bk+1. (111)\nThen, Lemma 7 implies that we have\n|boltz(a)\u2212 boltz(b)| > (log n)2e\u2212(a1\u2212bk+1). (112) Since a and b are tokenwise (r, \u03b4)-separated, a1 \u2212 bk+1 < 2r holds. Thus, the right-hand side of the above inequality is further lower-bounded by\n|boltz(a)\u2212 boltz(b)| > (log n)2e\u2212(a1\u2212bk+1) > (log n)2e\u22122r. (113)\nLemma 7. Let a, b \u2208 Rn be tokenwise \u03b4-separated vectors in a decreasing order with no duplicate element in each vector and \u03b4 > 2 log n+ 3, that is,\ni > j \u21d2 ai \u2212 aj , bi \u2212 bj > \u03b4, (114) ai \u0338= bj \u21d2 |ai \u2212 bj | > \u03b4 (115)\nfor any i, j \u2208 [n]. In addition, suppose there exists k \u2208 {0, . . . , n\u2212 1} such that\n(a1, . . . , ak) = (b1, . . . , bk) and ak+1 > bk+1. (116)\nThen, the outputs of the Boltzmann operator are (log n)2e\u2212(a1\u2212bk+1)-separated, that is,\n|boltz(a)\u2212 boltz(b)| > (log n)2e\u2212(a1\u2212bk+1) (117)\nholds.\nProof. We show the lemma by dividing it into the following two cases:\n1. k \u2265 1 Let a and b be\na = (a1, a2, . . . , ak, ak+1) \u2208 Rk+1 (118) b = (a1, a2, \u00b7 \u00b7 \u00b7 , ak, bk+1, bk+1, . . . , bk+1) \u2208 Rn. (119)\nThen, by abuse of notation, Lemma 4 implies that boltz (a) < a\u22a4\u03c3S [a] = boltz (a) and boltz (b) > boltz ( b ) , (120)\nand we only have to evaluate the magnitude of the difference boltz ( b ) \u2212 boltz (a).\nLet \u03b3k and \u03bek be\n\u03b3k = k\u2211 l=1 ale al and \u03bek = k\u2211 l=1 eal . (121)\nThen, boltz ( b ) can be decomposed into\nboltz ( b ) =\n\u03b3k + (n\u2212 k)bk+1ebk+1 \u03bek + (n\u2212 k)ebk+1\n= \u03b3k + bk+1e\nbk+1+log(n\u2212k)\n\u03bek + ebk+1+log(n\u2212k)\n= \u03b3k + (bk+1 + log(n\u2212 k)) ebk+1+log(n\u2212k) \u03bek + ebk+1+log(n\u2212k) \u2212 log(n\u2212 k) \u00b7 e bk+1+log(n\u2212k) \u03bek + ebk+1+log(n\u2212k) = boltz (a1, . . . , ak, bk+1 + log(n\u2212 k))\u2212 log(n\u2212 k) \u00b7 ebk+1+log(n\u2212k)\n\u03bek + ebk+1+log(n\u2212k) .\n(122)\nTherefore, the difference boltz ( b ) \u2212 boltz (a) can be written as\nboltz ( b ) \u2212 boltz (a) = boltz (a1, . . . , ak, bk+1 + log(n\u2212 k))\u2212 boltz (a)\n\u2212 log(n\u2212 k) \u00b7 e bk+1+log(n\u2212k)\n\u03bek + ebk+1+log(n\u2212k) . (123)\nAccording to Lemma 6, the first two terms on the right-hand side are evaluated as\nboltz (a1, . . . , ak, bk+1 + log(n\u2212 k))\u2212 boltz (a) > (ak+1 \u2212 bk+1 \u2212 log(n\u2212 k))(\u03b4 + ak+1 \u2212 bk+1 \u2212 log(n\u2212 k)\u2212 log(k + 1)\u2212 1)\n\u00b7 e bk+1+log(n\u2212k)\n\u03bek + ebk+1+log(n\u2212k)\n> (\u03b4 \u2212 log n)(2\u03b4 \u2212 2 log n\u2212 1) \u00b7 e bk+1+log(n\u2212k)\n\u03bek + ebk+1+log(n\u2212k) . (124)\nSince \u03b4 > 2 log n+ 3 by assumption, the above inequality is further lower-bounded by\nboltz (a1, . . . , ak, bk+1 + log(n\u2212 k))\u2212 boltz (a)\n> (\u03b4 \u2212 log n)(2\u03b4 \u2212 2 log n\u2212 1) \u00b7 e bk+1+log(n\u2212k)\n\u03bek + ebk+1+log(n\u2212k)\n> (log n+ 3)(2 log n+ 5) \u00b7 e bk+1+log(n\u2212k)\n\u03bek + ebk+1+log(n\u2212k) . (125)\nPlugging the above inequality into equation 123, we see boltz ( b ) \u2212 boltz (a) = boltz (a1, . . . , ak, bk+1 + log(n\u2212 k))\u2212 boltz (a)\n\u2212 log(n\u2212 k) \u00b7 e bk+1+log(n\u2212k)\n\u03bek + ebk+1+log(n\u2212k)\n> ebk+1+log(n\u2212k)\n\u03bek + ebk+1+log(n\u2212k) [(log n+ 3)(2 log n+ 5)\u2212 log(n\u2212 k)]\n> ebk+1+log(n\u2212k)\n\u03bek + ebk+1+log(n\u2212k) \u00b7 2(log n)2. (126)\nLastly, notice that the following inequality follows from \u03b4-separatedness of a and b and the assumption that a has no duplicate token:\n\u03bek + e bk+1+log(n\u2212k) < k+1\u2211 l=1 eal (\u2235 ak+1 > bk+1 + log(n\u2212 k))\n< ea1 k+1\u2211 l=1 e\u2212(l\u22121)\u03b4 < 2ea1 (\u2235 \u03b4 > log 2). (127)\nBy using this inequality, equation 126 is lower-bounded by\nboltz ( b ) \u2212 boltz (a) > e bk+1+log(n\u2212k)\n\u03bek + ebk+1+log(n\u2212k) \u00b7 2(log n)2\n> ebk+1+log(n\u2212k)\n2ea1 \u00b7 2(log n)2\n> (log n)2e\u2212(a1\u2212bk+1), (128)\nwhich implies boltz(b)\u2212 boltz(a) > (log n)2e\u2212(a1\u2212bk+1).\n2. k = 0\nSince the Boltzmann operator can be regarded as weighted averaging, boltz(b) \u2264 b1 always holds. Thus, it is enough to evaluate how much greater boltz(a) is than b1.\nLet a \u2208 Rn be\na = (a1, a1 \u2212 \u03b4, . . . , a1 \u2212 \u03b4). (129)\nThen, boltz(a) > boltz(a) follows from Lemma 4, and its value is\nboltz (a) = a1e\na1 + (n\u2212 1)(a1 \u2212 \u03b4)ea1\u2212\u03b4\nea1 + (n\u2212 1)ea1\u2212\u03b4\n= a1 + (n\u2212 1)(a1 \u2212 \u03b4)e\u2212\u03b4\n1 + (n\u2212 1)e\u2212\u03b4\n= a1 \u2212 (n\u2212 1)\u03b4e\u2212\u03b4\n1 + (n\u2212 1)e\u2212\u03b4 . (130)\nTherefore, the difference boltz (a)\u2212 boltz (b) is\nboltz (a)\u2212 boltz (b) \u2265 boltz (a)\u2212 b1\n> a1 \u2212 (n\u2212 1)\u03b4e\u2212\u03b4\n1 + (n\u2212 1)e\u2212\u03b4 \u2212 (a1 \u2212 \u03b4)\n= \u03b4 \u2212 (n\u2212 1)\u03b4e \u2212\u03b4\n1 + (n\u2212 1)e\u2212\u03b4\n= \u03b4\n1 + (n\u2212 1)e\u2212\u03b4\n\u2265 log n, (131)\nwhere the last inequality follows from the assumption \u03b4 > 2 log n. Note that the right-hand side is greater than (log n)2e\u2212(a1\u2212b1), because a1\u2212 b1 > log n implies log n \u00b7 e\u2212(a1\u2212b1) < 1."
        },
        {
            "heading": "C EXTENSION TO MASKED SELF-ATTENTION",
            "text": "Masked self-attention mechanisms are formulated as follows: the Softmax part in equation 2 is replaced by\n\u03c3S\n[( W\n(K) l,i Z\n)\u22a4 ( W\n(Q) l,i Z\n) +C ] \u2208 Rd\u00d7n, (132)\nwith some masking matrix C \u2208 Rn\u00d7n whose elements are either 0 or \u2212\u221e. Our main result Theorem 2 can be readily extended to the case where attention masking are conducted. The idea is that\nboltz(a+ c) = (a+ c)\u22a4\u03c3S [a+ c]\n= a\u22a4\u03c3S [a+ c] + c \u22a4\u03c3S [a+ c]\n= a\u22a4\u03c3S [a+ c] (133)\nholds for any masking vector c \u2208 Rn whose elements are either 0 or \u2212\u221e. Thus, in order to ensure that the masked attention is a contextual mapping, it is sufficient to verify boltz(a + c) are wellseparated. The caveat is that the Boltzmann operator now has to distinguish inputs consisting of the same attention a, but different masks c1, c2. However, the separability in this case can also be proved in the same way as in the proof of Lemma 1."
        }
    ],
    "title": "ARE TRANSFORMERS WITH ONE LAYER SELF- ATTENTION USING LOW-RANK WEIGHT MATRICES UNIVERSAL APPROXIMATORS?",
    "year": 2024
}