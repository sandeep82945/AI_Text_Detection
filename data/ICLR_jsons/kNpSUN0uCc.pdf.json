{
    "abstractText": "We propose and theoretically analyze an approach for planning with an approximate model in reinforcement learning that can reduce the adverse impact of model error. If the model is accurate enough, it accelerates the convergence to the true value function too. One of its key components is the MaxEnt Model Correction (MoCo) procedure that corrects the model\u2019s next-state distributions based on a Maximum Entropy density estimation formulation. Based on MoCo, we introduce the Model Correcting Value Iteration (MoCoVI) algorithm, and its sampled-based variant MoCoDyna. We show that MoCoVI and MoCoDyna\u2019s convergence can be much faster than the conventional model-free algorithms. Unlike traditional model-based algorithms, MoCoVI and MoCoDyna effectively utilize an approximate model and still converge to the correct value function.",
    "authors": [],
    "id": "SP:44f776bc27ffab7c88cb7f71a3156bf699ab65f2",
    "references": [
        {
            "authors": [
                "Romina Abachi",
                "Mohammad Ghavamzadeh",
                "Amir-massoud Farahmand"
            ],
            "title": "Policy-aware model learning for policy gradient methods",
            "year": 2003
        },
        {
            "authors": [
                "Romina Abachi",
                "Claas A Voelcker",
                "Animesh Garg",
                "Amir massoud Farahmand"
            ],
            "title": "VIPer: Iterative value-aware model learning on the value improvement path",
            "venue": "In Decision Awareness in Reinforcement Learning Workshop at ICML 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Zaheer Abbas",
                "Samuel Sokota",
                "Erin Talvitie",
                "Martha White"
            ],
            "title": "Selective dyna-style planning under limited model capacity",
            "venue": "Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Yasemin Altun",
                "Alex Smola"
            ],
            "title": "Unifying divergence minimization and statistical inference via convex duality",
            "venue": "In Proceedings of the 19th Annual Conference on Learning Theory,",
            "year": 2006
        },
        {
            "authors": [
                "Bernardo \u00c1vila Pires",
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Policy error bounds for model-based reinforcement learning with factored linear models",
            "venue": "In Conference on Learning Theory (COLT),",
            "year": 2016
        },
        {
            "authors": [
                "D. Bertsekas"
            ],
            "title": "Convex Optimization Theory. Athena Scientific optimization and computation series",
            "venue": "Athena Scientific,",
            "year": 2009
        },
        {
            "authors": [
                "Jonathan M Borwein",
                "Adrian S Lewis"
            ],
            "title": "Duality relationships for entropy-like minimization problems",
            "venue": "SIAM Journal on Control and Optimization,",
            "year": 1991
        },
        {
            "authors": [
                "Zdravko I Botev",
                "Dirk P Kroese"
            ],
            "title": "The generalized cross entropy method, with applications to probability density estimation",
            "venue": "Methodology and Computing in Applied Probability,",
            "year": 2011
        },
        {
            "authors": [
                "S. Brooks",
                "A. Gelman",
                "G. Jones",
                "X.L. Meng"
            ],
            "title": "Handbook of Markov Chain Monte Carlo",
            "year": 2011
        },
        {
            "authors": [
                "S.F. Chen",
                "R. Rosenfeld"
            ],
            "title": "A survey of smoothing techniques for me models",
            "venue": "IEEE Transactions on Speech and Audio Processing,",
            "year": 2000
        },
        {
            "authors": [
                "Stanley F Chen",
                "Ronald Rosenfeld"
            ],
            "title": "A survey of smoothing techniques for me models",
            "venue": "IEEE transactions on Speech and Audio Processing,",
            "year": 2000
        },
        {
            "authors": [
                "Thomas M. Cover",
                "Joy A. Thomas"
            ],
            "title": "Elements of Information Theory 2nd Edition (Wiley Series in Telecommunications and Signal Processing)",
            "venue": "ISBN 0471241954",
            "year": 2006
        },
        {
            "authors": [
                "Will Dabney",
                "Andr\u00e9 Barreto",
                "Mark Rowland",
                "Robert Dadashi",
                "John Quan",
                "Marc G Bellemare",
                "David Silver"
            ],
            "title": "The value-improvement path: Towards better representations for reinforcement learning",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "John N Darroch",
                "Douglas Ratcliff"
            ],
            "title": "Generalized iterative scaling for log-linear models",
            "venue": "The annals of mathematical statistics,",
            "year": 1972
        },
        {
            "authors": [
                "Andr\u00e9e Decarreau",
                "Danielle Hilhorst",
                "Claude Lemar\u00e9chal",
                "Jorge Navaza"
            ],
            "title": "Dual methods in entropy maximization. application to some problems in crystallography",
            "venue": "SIAM Journal on Optimization,",
            "year": 1992
        },
        {
            "authors": [
                "S. Della Pietra",
                "V. Della Pietra",
                "J. Lafferty"
            ],
            "title": "Inducing features of random fields",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 1997
        },
        {
            "authors": [
                "Miroslav Dud\u00edk",
                "Steven J. Phillips",
                "Robert E. Schapire"
            ],
            "title": "Performance guarantees for regularized maximum entropy density estimation",
            "venue": "Proceedings of the 17th Annual Conference on Computational Learning Theory,",
            "year": 2004
        },
        {
            "authors": [
                "Miroslav Dud\u00edk",
                "Steven J. Phillips",
                "Robert E. Schapire"
            ],
            "title": "Performance guarantees for regularized maximum entropy density estimation",
            "venue": "Learning Theory,",
            "year": 2004
        },
        {
            "authors": [
                "Miroslav Dud\u00edk",
                "Steven J Phillips",
                "Robert E Schapire"
            ],
            "title": "Maximum entropy density estimation with generalized regularization and an application to species distribution modeling",
            "year": 2007
        },
        {
            "authors": [
                "Amir-massoud Farahmand"
            ],
            "title": "Iterative value-aware model learning",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS -",
            "year": 2018
        },
        {
            "authors": [
                "Amir-massoud Farahmand",
                "R\u00e9mi Munos",
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Error propagation for approximate policy and value iteration",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS -",
            "year": 2010
        },
        {
            "authors": [
                "Amir-massoud Farahmand",
                "Andr\u00e9 M.S. Barreto",
                "Daniel N. Nikovski"
            ],
            "title": "Value-aware loss function for model-based reinforcement learning",
            "venue": "In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2017
        },
        {
            "authors": [
                "Joshua Goodman"
            ],
            "title": "Exponential priors for maximum entropy models",
            "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL",
            "year": 2004
        },
        {
            "authors": [
                "Taher Jafferjee",
                "Ehsan Imani",
                "Erin Talvitie",
                "Martha White",
                "Micheal Bowling"
            ],
            "title": "Hallucinating value: A pitfall of dyna-style planning with imperfect environment models",
            "venue": "arXiv preprint arXiv:2006.04363,",
            "year": 2006
        },
        {
            "authors": [
                "Michael Janner",
                "Justin Fu",
                "Marvin Zhang",
                "Sergey Levine"
            ],
            "title": "When to trust your model: Model-based policy optimization",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "E.T. Jaynes"
            ],
            "title": "Information theory and statistical mechanics",
            "venue": "Phys. Rev.,",
            "year": 1957
        },
        {
            "authors": [
                "Sham Kakade",
                "John Langford"
            ],
            "title": "Approximately optimal approximate reinforcement learning",
            "venue": "In Proceedings of the Nineteenth International Conference on Machine Learning (ICML),",
            "year": 2002
        },
        {
            "authors": [
                "J.N. Kapur",
                "H.K. Kesavan"
            ],
            "title": "Entropy Optimization Principles and Their Applications, pages 3\u201320",
            "venue": "ISBN 978-94-011-2430-0",
            "year": 1992
        },
        {
            "authors": [
                "Jun\u2019ichi Kazama",
                "Jun\u2019ichi Tsujii"
            ],
            "title": "Evaluation and extension of maximum entropy models with inequality constraints",
            "venue": "In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2003
        },
        {
            "authors": [
                "S. Kullback"
            ],
            "title": "Information Theory and Statistics. Wiley publication in mathematical statistics",
            "venue": "URL https://books.google.ca/books?id=XeRQAAAAMAAJ",
            "year": 1959
        },
        {
            "authors": [
                "Raymond Lau"
            ],
            "title": "Adaptive statistical language modeling",
            "venue": "PhD thesis, Massachusetts Institute of Technology,",
            "year": 1994
        },
        {
            "authors": [
                "Guy Lebanon",
                "John Lafferty"
            ],
            "title": "Boosting and maximum likelihood for exponential models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2001
        },
        {
            "authors": [
                "\u00c2ngelo G. Lovatto",
                "Thiago P. Bueno",
                "Denis D. Mau\u00e1",
                "Leliane N. de Barros"
            ],
            "title": "Decision-aware model learning for actor-critic methods: When theory does not meet practice",
            "venue": "Proceedings on \"I Can\u2019t Believe It\u2019s Not Better!\" at NeurIPS Workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Robert Malouf"
            ],
            "title": "A comparison of algorithms for maximum entropy parameter estimation",
            "venue": "The 6th Conference on Natural Language Learning 2002 (CoNLL-2002),",
            "year": 2002
        },
        {
            "authors": [
                "R\u00e9mi Munos"
            ],
            "title": "Error bounds for approximate policy iteration",
            "venue": "In Proceedings of the 20th International Conference on Machine Learning (ICML),",
            "year": 2003
        },
        {
            "authors": [
                "R\u00e9mi Munos"
            ],
            "title": "Performance bounds in Lp norm for approximate value iteration",
            "venue": "SIAM Journal on Control and Optimization,",
            "year": 2007
        },
        {
            "authors": [
                "Amin Rakhsha",
                "Andrew Wang",
                "Mohammad Ghavamzadeh",
                "Amir-massoud Farahmand"
            ],
            "title": "Operator splitting value iteration",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Bruno Scherrer",
                "Mohammad Ghavamzadeh",
                "Victor Gabillon",
                "Boris Lesner",
                "Matthieu Geist"
            ],
            "title": "Approximate modified policy iteration and its application to the game of tetris",
            "venue": "Journal of Machine Learning Research (JMLR),",
            "year": 2015
        },
        {
            "authors": [
                "J. Shore",
                "R. Johnson"
            ],
            "title": "Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 1980
        },
        {
            "authors": [
                "Richard S. Sutton"
            ],
            "title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming",
            "venue": "In Proceedings of the 7th International Conference on Machine Learning (ICML),",
            "year": 1990
        },
        {
            "authors": [
                "Richard S. Sutton",
                "Andrew G. Barto"
            ],
            "title": "Reinforcement Learning: An Introduction",
            "venue": "The MIT Press, second edition,",
            "year": 2019
        },
        {
            "authors": [
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Algorithms for Reinforcement Learning",
            "venue": "Morgan Claypool Publishers,",
            "year": 2010
        },
        {
            "authors": [
                "Erin J. Talvitie"
            ],
            "title": "Self-correcting models for model-based reinforcement learning",
            "venue": "In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Claas A. Voelcker",
                "Victor Liao",
                "Animesh Garg",
                "Amir-massoud Farahmand"
            ],
            "title": "Value gradient weighted model-based reinforcement learning",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Tong Zhang"
            ],
            "title": "Class-size independent generalization analsysis of some discriminative multi-category classification",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2004
        },
        {
            "authors": [
                "\u225c inff L(f"
            ],
            "title": "\u03bb,\u039b\u2032) from this result by Decarreau et al",
            "venue": "((Decarreau et al.,",
            "year": 1992
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Reinforcement learning (RL) algorithms can be divided into model-free and model-based algorithms based on how they use samples from the environment with dynamics P . Model-free algorithms directly use samples for P to approximately apply the Bellman operator on value functions. At its core, the next-state expectations EX\u2032\u223cP(\u00b7|x,a)[\u03d5(X \u2032)] is estimated for a function \u03d5, such as the value function V , at a state-action pair (x, a). Model-based reinforcement learning (MBRL) algorithms, on the other hand, use samples from the environment to train a world model P\u0302 to approximate P . The world model P\u0302 can be considered an approximate but cheap substitute of the true dynamics P , and is used to solve the task instead of P . The world model P\u0302 often cannot be learned perfectly, and some inaccuracies between P and P\u0302 is inevitable. This error in the model can catastrophically hinder the performance of an MBRL agent, especially in complex environments that learning an accurate model is challenging (Talvitie, 2017; Jafferjee et al., 2020; Abbas et al., 2020). In some of these challenging environments, estimating the next-state expectations accurately might be much easier than learning a model. Motivated by this scenario, we aim to bridge the gap between model-based and model-free algorithms and ask: Can we improve MBRL algorithms by using both the next-state expectations and the approximate model P\u0302? In this paper, we consider a discounted MDP with the true dynamics P , and we suppose that we have access to an approximate model P\u0302 \u2248 P . At this level of abstraction, we do not care about how P\u0302 is obtained \u2013 it may be learned using a conventional Maximum Likelihood Estimate (MLE) or it might be a low-fidelity and fast simulator of the true dynamics P . We further assume that for any function \u03d5 of states, we can obtain the next-state expectations EX\u2032\u223cP(\u00b7|x,a)[\u03d5(X \u2032)] for all states x and actions a. We consider this procedure costly compared to ones involving P\u0302 which will be considered free. We propose the MaxEnt Model Correction (MaxEnt MoCo) algorithm that can be implemented with any planning algorithm that would normally be used for planning and reduce the impact of model error. MaxEnt MoCo first obtains EX\u2032\u223cP(\u00b7|x,a)[\u03d5i(X \u2032)] for all (x, a) and a set of basis functions \u03d5i. The main idea is that whenever the planning algorithm normally uses P\u0302(\u00b7|x, a) for some state-action (x, a), a corrected distribution p\u0304 is calculated and used instead. The distribution p\u0304 is obtained by minimally modifying P\u0302(\u00b7|x, a) so that the next-state expectations EX\u2032\u223cp\u0304[\u03d5i(X \u2032)] based on p\u0304 are (more) consistent with the obtained EX\u2032\u223cP(\u00b7|x,a)[\u03d5i(X \u2032)] through queries. This procedure is known as Maximum Entropy density estimation (Dud\u00edk et al., 2007) \u2013 hence the name MaxEnt MoCo. We\nshow that if the true value function can be well-approximated by a linear combination of the basis functions \u03d5i, the estimated value function by MaxEnt MoCo can be significantly more accurate than the normally computed one using P\u0302 . We also introduce Model Correcting Value Iteration (MoCoVI) (Section 4) and its sample-based variant MoCoDyna (Section 5), which iteratively update the basis functions \u03d5i. These algorithms select their past value functions as the basis functions, and execute MaxEnt MoCo to get a new, more accurate value function. This choice of basis functions proves to be effective. We show that if the model is accurate enough, MoCoVI and MoCoDyna can converge to the true value function, and the convergence can be much faster than a model-free algorithm that doesn\u2019t have access to a model. In this paper, we study the theoretical underpinnings of maximum entropy model correction in RL. We provide theoretical analysis that applies to both finite and continuous MDPs, and to the approximate versions of the algorithms with function approximation."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "In this work, we consider a discounted Markov Decision Process (MDP) defined as M = (X ,A,R,P, \u03b3) (Szepesv\u00e1ri, 2010). We use commonly used definitions and notations, summarized in Appendix B. We briefly mention that we denote the value of a policy \u03c0 by V \u03c0 and the optimal value function by V \u2217. Whenever we need to be explicit about the dependence of the value functions to reward kernelR and the transition kernel P , we use V \u03c0 = V \u03c0(R,P) and V \u2217 = V \u2217(R,P). For any function \u03d5 : X \u2192 R, we define P\u03d5 : X \u00d7A \u2192 R as (P\u03d5)(x, a) \u225c \u222b P(dx\u2032|x, a)\u03d5(x\u2032) for all (x, a) \u2208 X \u00d7 A. We refer to the problem of finding V \u03c0PE for a specific policy \u03c0PE as the Policy Evaluation (PE) problem, and to the problem of finding an optimal policy as the Control problem. In this paper, we assume an approximate model P\u0302 \u2248 P is given. We define V\u0302 \u03c0 and \u03c0\u0302\u2217 in the approximate MDP M\u0302 = (X ,A,R, P\u0302, \u03b3) similar to their counterparts in the true MDP M . We assume the PE and control problems can be solved in M\u0302 as it is a standard part of MBRL algorithms."
        },
        {
            "heading": "2.1 IMPACT OF MODEL ERROR",
            "text": "In MBRL, the agent relies on the approximate model P\u0302 to solve the PE and Control problems (Sutton, 1990). A purely MBRL agent learns value functions and policies only using P\u0302 , which means it effectively solves the approximate MDP M\u0302 = (X ,A,R, P\u0302, \u03b3) instead of the true MDP M . The advantage is that this only requires access to a cost-efficient P\u0302 , hence avoiding costly access to the true dynamics P (e.g., via real-world interaction). However, the model error can dramatically degrade the agent\u2019s performance (Talvitie, 2017; Jafferjee et al., 2020; Abbas et al., 2020). The extent of the performance loss has been theoretically analyzed in prior work (\u00c1vila Pires and Szepesv\u00e1ri, 2016; Talvitie, 2017; Farahmand et al., 2017; Farahmand, 2018). To characterize model errors and their impact mathematically, we define the following error measure for each state-action pair (x, a):\n\u03f5Model(x, a) = \u221a DKL( P(\u00b7|x, a) \u2225 P\u0302(\u00b7|x, a) ). (2.1)\nWe note that the choice of KL divergence for quantifying the model error is a natural one. Indeed, in conventional model learning (see e.g., Janner et al. 2019), a common choice of optimization objective is the maximum likelihood estimation (MLE) loss, which minimizes the empirical estimate of the KL-divergence of the approximate next-state distribution to the ground-truth. The following lemma provides performance guarantees for an MBRL agent as a function of \u03f5Model. Similar bounds have appeared in recent work (\u00c1vila Pires and Szepesv\u00e1ri, 2016; Farahmand, 2018; Rakhsha et al., 2022). Lemma 1. Suppose that P is the true environment dynamics, P\u0302 is an approximation of P , and \u2225\u03f5Model\u2225\u221e = supx,a\u2208X\u00d7A \u03f5Model(x, a) is the worst-case error between them. Let c1 = \u03b3 \u221a 2/(1\u2212 \u03b3). We have \u2225V \u03c0PE \u2212 V\u0302 \u03c0PE\u2225\u221e \u2264 \u03b31\u2212\u03b3 \u2225(P \u03c0PE \u2212 P\u0302\u03c0PE)V \u03c0PE\u2225\u221e \u2264 c1\u2225\u03f5Model\u2225\u221e \u00b7 \u2225V \u03c0PE\u2225\u221e and \u2225V \u2217 \u2212 V \u03c0\u0302\u2217\u2225\u221e \u2264 2c1\u2225\u03f5Model\u2225\u221e 1\u2212c1\u2225\u03f5Model\u2225\u221e \u2225V \u2217\u2225\u221e.\nNote that the model error impacts the PE solution through the term (P\u03c0PE \u2212 P\u0302\u03c0PE)V \u03c0PE . A similar observation can be made for the Control problem. This dependence has been used in designing value-aware losses for model learning (Farahmand et al., 2017; Farahmand, 2018; Voelcker et al., 2022; Abachi et al., 2022) and proves to be useful in our work as well."
        },
        {
            "heading": "2.2 MAXIMUM ENTROPY DENSITY ESTIMATION",
            "text": "Consider a random variable Z defined over a domain Z with unknown distribution p \u2208 M(Z), and a set of basis functions \u03d5i : Z \u2192 R for i = 1, 2, . . . , d. Suppose that the expected values \u03d5\u0304i = Ep[\u03d5i(Z)] of these functions under p are observed. Our goal is to find a distribution q such that Eq[\u03d5i(Z)] matches \u03d5\u0304i. For example, if \u03d51(z) = z and \u03d52(z) = z2, we are interested in finding a q such that its first and second moments are the same as p\u2019s.\nIn general, there are many densities that satisfy these constraints. Maximum entropy (MaxEnt) principle prescribes picking the most uncertain distribution as measured via (relative) entropy that is consistent with these observations (Jaynes, 1957). MaxEnt chooses q\u2217 = argmaxEq [\u03d5i(Z)]=\u03d5\u0304i H(q), where H(q) is the entropy of q, or equivalently, it minimizes the KL divergence (relative entropy) between q and the uniform distribution (or Lebesgue measure) u, i.e., q\u2217 = argminEq [\u03d5i(Z)]=\u03d5\u0304i DKL( q \u2225 u ).\nIn some applications, prior knowledge about the distribution q is available. The MaxEnt principle can then be generalized to select the distribution with the minimum KL divergence to a prior p\u0302:\nq\u2217 = argmin Eq [\u03d5i(Z)]=\u03d5\u0304i DKL( q \u2225 p\u0302 ). (2.2)\nThis is called the Principle of minimum discrimination information or the Principle of Minimum Cross-Entropy (Kullback, 1959; Shore and Johnson, 1980; Kapur and Kesavan, 1992), and can be viewed as minimally correcting the prior p\u0302 to satisfy the constraints given by observations \u03d5\u0304i. In line with prior work, we call density estimation under this framework MaxEnt density estimation whether or not the prior is taken to be the uniform distribution (Dud\u00edk et al., 2004a; 2007).\nWhile the choice of KL divergence is justified in various ways (e.g., the axiomatic approach of Shore and Johnson 1980), the use of other divergences has also been studied in the literature (Altun and Smola, 2006; Botev and Kroese, 2011). Although we focus on KL divergence in this work, in principle, our algorithms can also operate with other divergences provided that solving the analogous optimization problem of the form (2.2) is computationally feasible.\nProblem (2.2) and its variants have been studied in the literature; the solution is a member of the family of Gibbs distributions :\nq\u03bb(A) = \u222b z\u2208A p\u0302(dz) \u00b7 exp ( d\u2211\ni=1\n\u03bbi\u03d5i(z)\u2212 \u039b\u03bb ) , (2.3)\nwhere A \u2286 Z , \u03bb \u2208 Rd, and \u039b\u03bb is the log normalizer, i.e., \u039b\u03bb = log \u222b p\u0302(dz) \u00b7 exp (\u2211d i=1 \u03bbi\u03d5i(z) ) . The dual problem for finding the optimal \u03bb takes the form\n\u03bb\u2217 = argmin \u03bb\u2208Rd log\n\u222b p\u0302(dz) exp ( d\u2211\ni=1\n\u03bbi\u03d5i(z)\n) \u2212\nd\u2211 i=1 \u03bbi\u03d5\u0304i . (2.4)\nIterative scaling (Darroch and Ratcliff, 1972; Della Pietra et al., 1997), gradient descent, Newton and quasi-Newton methods (see Malouf (2002)) have been suggested for solving this problem. After finding \u03bb\u2217, if Var[exp( \u2211 i \u03bbi\u03d5i(Z\u0302))] for Z\u0302 \u223c p\u0302 is small, e.g. when p\u0302 has low stochasticity, \u039b\u2217\u03bb can be estimated with samples from p\u0302. Then, one can sample from q\u2217 by sampling from Z0 \u223c p\u0302 and assign the importance sampling weight exp (\u2211d i=1 \u03bb \u2217 i\u03d5i(Z0)\u2212 \u039b\u03bb\u2217 ) . In general algorithms such Markov Chain Monte Carlo can be used for sampling (Brooks et al., 2011). When the observations \u03d5\u0304i are empirical averages, Maximum entropy density estimation is equivalent to maximum likelihood estimation that uses the family of Gibbs distributions of the form (2.3) (Della Pietra et al., 1997)."
        },
        {
            "heading": "3 MAXIMUM ENTROPY MODEL CORRECTION",
            "text": "As discussed in Section 2.2, MaxEnt density estimation allows us to correct an initial estimated distribution of a random variable using an additional info in the form of the expected values of some functions of it. In this section, we introduce the MaxEnt Model Correction (MaxEnt MoCo) algorithm, which applies this tool to correct the next-state distributions needed for planning from the one in the approximate model P\u0302 towards the true one in P .\nWe assume that for any function \u03d5 : X \u2192 R, we can obtain (an approximation of) P\u03d5. This operation is at the core of many RL algorithms. For instance, each iteration k of Value Iteration (VI) involves obtaining PVk for value function Vk. This procedure can be approximated when samples from P are available with techniques such as stochastic approximation (as in TD Learning) or regression (as in fitted value iteration). Due to its dependence on the true dynamics P , we consider this procedure costly and refer to it as a query. On the other hand, we will ignore the cost of any other calculation that does not involve P , such as calculations and planning with P\u0302 . In Section 3.1, we consider the exact setting where similar to the conventional VI, we can obtain P\u03d5 exactly for any function \u03d5 : X \u2192 R . Then in Section 3.2, we consider the case that some error exists in the obtained P\u03d5, which resembles the setting considered for approximate VI."
        },
        {
            "heading": "3.1 EXACT FORM",
            "text": "In this section, we assume that for any function \u03d5 : X \u2192 R, we can obtain P\u03d5 exactly. We show that in this case, MaxEnt density estimation can be used to achieve planning algorithms with strictly better performance guarantees than Lemma 1. To see the effectiveness of MaxEnt density estimation to improve planning, consider the idealized case where the true value function V \u03c0PE for the PE problem is known to us. Consequently, we can obtain PV \u03c0PE by querying the true dynamics P . Assume that we could perform MaxEnt density estimation (2.2) for every state x and action a. We minimally change P\u0302(\u00b7|x, a) to a new distribution P\u0304(\u00b7|x, a) such that EX\u2032\u223cP\u0304(\u00b7|x,a)[V \u03c0PE(X \u2032)] = (PV \u03c0PE)(x, a).\nWe then use any arbitrary planning algorithm using the new dynamics P\u0304 instead of P\u0302 , which means we solve MDP M\u0304 = (X ,A,R, P\u0304) instead of M\u0302 . Due to the constraint in finding P\u0304 , we have P\u0304V \u03c0PE = PV \u03c0PE , therefore r\u03c0PE + \u03b3P\u0304\u03c0PEV \u03c0PE = r\u03c0PE + \u03b3P\u03c0PEV \u03c0PE = V \u03c0PE . In other words, V \u03c0PE satisfies the Bellman equation in M\u0304 . This means that MaxEnt MoCo completely eliminates the impact of the model error on the agent, and we obtain the true value function V \u03c0PE . The same argument can be made for the Control problem when we know V \u2217 and correction is performed via constraints given by PV \u2217. The true optimal value function V \u2217 satisfies the Bellman optimality equation in M\u0304 , which means V\u0304 \u2217 = V \u2217. The obtained optimal policy \u03c0\u0304\u2217 = \u03c0g(V \u2217, P\u0304) is also equal to \u03c0\u2217 = \u03c0g(V \u2217,P). In practice, the true value functions V \u03c0PE or V \u2217 are unknown \u2013 we are trying to find them after all. In this case, we do the correction procedure with a set of basis functions \u03d51, . . . , \u03d5d with \u03d5i : X \u2192 R. The set of basis functions can be chosen arbitrarily. As shall be clear later, we prefer to choose them such that their span can approximate the true value function V \u03c0PE or V \u2217 well. We emphasize that this is only a criteria for the choice of basis functions suggested by our analysis. The basis functions are not used to approximate or represent value functions by the agent. In this section and Section 3.2, we focus on the properties of model error correction for any given set of functions. In Sections 4 and 5, we will introduce techniques for finding a good set of such functions.\nNow, we introduce the MaxEnt MoCo algorithm. In large or continuous MDPs, it is not feasible to perform MaxEnt density estimation for all x, a. Instead, we take a lazy computation approach and calculate P\u0304(\u00b7|x, a) only when needed. The dynamics P\u0304 : X \u00d7A \u2192M(X ) is never constructed as a function of states and actions by the agent, and it is defined only for the purpose of analysis. First, we obtain P\u03d5i for i = 1, 2, . . . , d through d queries to the true dynamics P . Then, we execute any planning algorithm that can normally be used in MBRL to solve the approximate MDP M\u0302 . The only modification is that whenever the planning algorithm uses the distribution P\u0302(\u00b7|x, a) for some state x and action a, e.g. when simulating rollouts from (x, a), we find a corrected distribution P\u0304(\u00b7|x, a) using MaxEnt density estimation and pass it to the planning algorithm instead of P\u0302(\u00b7|x, a) that would normally be used. The new distribution P\u0304(\u00b7|x, a) is given by\nP\u0304(\u00b7|x, a) \u225c argmin q\u2208M(X ) DKL( q \u2225 P\u0302(\u00b7|x, a) ), (P1)\nsuch that EX\u2032\u223cq[\u03d5i(X \u2032)] = (P\u03d5i)(x, a) (i = 1, 2, . . . , d).\nAs discussed in Section 3, the optimization problem (P1) can be solved through the respective convex dual problem as in (2.4). Also note that the dual problem only has d parameters, which is usually small,1 and solving it only involves P\u0302 that is considered cheap.\n1For a reference, in our experiments d \u2264 3. Even if d is large, specialized algorithms have been developed to efficiently solve the optimization problem (Dud\u00edk et al., 2007).\nWe now analyze the performance of MaxEnt MoCo in PE. Let V\u0304 \u03c0PE be the value function of \u03c0PE in MDP M\u0304 = (X ,A,R, P\u0304, \u03b3). We will show that the error of MaxEnt MoCo depends on how well V \u03c0PE can be approximated with a linear combination of the basis functions. To see this, first note that the constraints in (P1) mean that (P\u0304\u03c0PE \u2212 P\u03c0PE)\u03d5i = 0. Thus, for any w \u2208 Rd we can write the upper bound on \u2225V \u03c0PE \u2212 V\u0304 \u03c0PE\u2225\u221e that is given in Lemma 1 as\n\u03b3 1\u2212 \u03b3 \u2225\u2225(P\u03c0PE \u2212 P\u0304\u03c0PE)V \u03c0PE\u2225\u2225\u221e = \u03b31\u2212 \u03b3 \u2225\u2225\u2225(P\u03c0PE \u2212 P\u0304\u03c0PE)(V \u03c0PE \u2212 d\u2211 i=1 wi\u03d5i) \u2225\u2225\u2225 \u221e\n(3.1)\n\u2264 \u221a 2\u03b3\n1\u2212 \u03b3 sup x,a\n\u221a DKL( P(\u00b7|x, a) \u2225 P\u0304(\u00b7|x, a) ) \u2225\u2225\u2225V \u03c0PE \u2212 d\u2211 i=1 wi\u03d5i \u2225\u2225\u2225 \u221e ,\nwhere the last inequality is proved similar to the proof of the second inequality in Lemma 1. Now, from the general Pythagoras theorem for KL-divergence (see Thm. 11.6.1 of Cover and Thomas 2006), for any (x, a), we have\nDKL( P(\u00b7|x, a) \u2225 P\u0304(\u00b7|x, a) ) \u2264 DKL( P(\u00b7|x, a) \u2225 P\u0302(\u00b7|x, a) ). (3.2)\nThis inequality is of independent interest as it shows that MaxEnt MoCo is reducing the MLE loss of the model. It is worth mentioning that since P\u0304 is not constructed by the agent, this improved MLE loss can go beyond what is possible with the agent\u2019s model class. A feature that is valuable in complex environments that are hard to model. Inequalities (3.2) and (3.1) lead to an upper bound on \u2225V \u03c0PE \u2212 V\u0304 \u03c0PE\u2225\u221e. We have the following proposition: Proposition 1. Suppose that P is the true environment dynamics, P\u0302 is an approximation of P , and \u03f5Model is defined as in (2.1). Let c1 = \u03b3 \u221a 2/(1\u2212 \u03b3) as in Lemma 1. Then,\n\u2225\u2225V \u03c0PE \u2212 V\u0304 \u03c0PE\u2225\u2225\u221e \u2264 c1\u2225\u03f5Model\u2225\u221e infw\u2208Rd \u2225\u2225\u2225V \u03c0PE \u2212 d\u2211\ni=1\nwi\u03d5i \u2225\u2225\u2225 \u221e ,\n\u2225\u2225\u2225V \u2217 \u2212 V \u03c0\u0304\u2217\u2225\u2225\u2225 \u221e \u2264 2c1\u2225\u03f5Model\u2225\u221e 1\u2212 c1\u2225\u03f5Model\u2225\u221e inf w\u2208Rd \u2225\u2225\u2225V \u2217 \u2212 d\u2211 i=1 wi\u03d5i \u2225\u2225\u2225 \u221e .\nThe significance of this result becomes apparent upon comparison with Lemma 1. Whenever the value function can be represented sufficiently well within the span of the basis functions {\u03d5i} used for correcting P\u0302 , the error between the value function V\u0304 of the modified dynamics P\u0304 compared to the true value function V \u03c0PE is significantly smaller than the error of the value function V\u0302 \u03c0PE obtained from P\u0302 \u2014 compare infw\u2208Rd \u2225V \u03c0PE \u2212 \u2211d i=1 wi\u03d5i\u2225\u221e with \u2225V \u03c0PE\u2225\u221e."
        },
        {
            "heading": "3.2 APPROXIMATE FORM",
            "text": "In the previous section, we assumed that the agent can obtain P\u03d5i exactly. This is an unrealistic assumption when we only have access to samples from P such as in the RL setting. Estimating P\u03d5i from samples is a regression problem and has error. We assume that we have access to the approximations \u03c8i : X \u00d7 A \u2192 R of P\u03d5i such that \u03c8i \u2248 P\u03d5i with the error quantified by \u03f5Query. Specifically, for any (x, a), we have \u03f5Query(x, a) = \u2225\u03c8(x, a)\u2212 (P\u03d5)(x, a)\u22252 where \u03d5 : X \u2192 Rd and \u03c8 : X \u00d7A \u2192 Rd are the d-dimensional vectors formed by \u03d5i and \u03c8i functions. When the observations are noisy, MaxEnt density estimation is prone to overfiting (Dud\u00edk et al., 2007). Many techniques have been introduced to alleviate this issue including regularization (Chen and Rosenfeld, 2000a; Lebanon and Lafferty, 2001), introduction of a prior (Goodman, 2004), and constraint relaxation (Kazama and Tsujii, 2003; Dud\u00edk et al., 2004b). In this work, we use \u211322 regularization (Lau, 1994; Chen and Rosenfeld, 2000b; Lebanon and Lafferty, 2001; Zhang, 2004; Dud\u00edk et al., 2007) and leave the study of the other approaches to future work.\nThe regularization is done by adding 14\u03b2 2\u2225\u03bb\u222522 to the objective of the dual problem (2.4). This pushes the dual parameters to remain small. The hyperparameter \u03b2 controls the amount of regularization. Smaller \u03b2 leads a solution closer to the original one. Notice that with extreme regularization when \u03b2 \u2192 \u221e, we get \u03bb = 0, which makes the solution of MaxEnt density estimation the same as the\ninitial density estimate p\u0302. The regularization of the dual problem has an intuitive interpretation in the primal problem. With the regularization, the primal problem (P1) is transformed to\nP\u0304(\u00b7|x, a) \u225c argmin q\nDKL( q \u2225 P\u0302(\u00b7|x, a) ) + 1\n\u03b22 d\u2211 i=1 ( EX\u2032\u223cq[\u03d5i(X \u2032)]\u2212 \u03c8i(x, a) )2 . (P2)\nWe now have introduced a new hyperparameter \u03b2 to MaxEnt MoCo. As \u03b2 \u2192 0, the solution converges to that of the constrained problem (P1), because intuitively, \u03b2 controls how much we trust the noisy observations \u03c8i. Smaller values of \u03b2 means that we care about being consistent with the queries more than staying close to P\u0302 , and larger values of \u03b2 shows the opposite preference. It turns out the impact of the choice of \u03b2 is aligned with this intuition. As \u2225\u03f5Model\u2225\u221e increases or \u2225\u03f5Query\u2225\u221e decreases, we should rely on the queries more and choose a smaller \u03b2. We provide the analysis for a general choice of \u03b2 in the supplementary material, and here focus on when \u03b2 = \u2225\u03f5Query\u2225\u221e/\u2225\u03f5Model\u2225\u221e. Theorem 1. Let c1 = \u03b3 \u221a 2/(1 \u2212 \u03b3), c2 = 3\u03b3 \u221a d/(1 \u2212 \u03b3), and \u03b2 = \u2225\u03f5Query\u2225\u221e/\u2225\u03f5Model\u2225\u221e. For any wmax \u2265 0, we have\n\u2225\u2225V \u03c0PE \u2212 V\u0304 \u03c0PE\u2225\u2225\u221e \u2264 3c1\u2225\u03f5Model\u2225\u221e inf\u2225w\u2225\u221e\u2264wmax \u2225\u2225\u2225V \u03c0PE \u2212 d\u2211\ni=1\nwi\u03d5i \u2225\u2225\u2225 \u221e + c2\u2225\u03f5Query\u2225\u221e \u00b7 wmax,\n\u2225\u2225\u2225V \u2217 \u2212 V \u03c0\u0304\u2217\u2225\u2225\u2225 \u221e \u2264 6c1\u2225\u03f5Model\u2225\u221e 1\u2212 3c1\u2225\u03f5Model\u2225\u221e inf \u2225w\u2225\u221e\u2264wmax \u2225\u2225\u2225V \u2217 \u2212 d\u2211 i=1 wi\u03d5i \u2225\u2225\u2225 \u221e + 2c2\u2225\u03f5Query\u2225\u221e 1\u2212 3c1\u2225\u03f5Model\u2225\u221e \u00b7 wmax.\nThe above theorem shows that the error in the queries contribute an additive term to the final bounds compared to the exact query setting analyzed in Proposition 1. This term scales with wmax, which can be chosen arbitrarily to minimize the upper bound. Larger values of wmax allow a better approximation of V \u03c0PE and V \u2217 in the infimum terms, but amplify the query error \u03f5Query. Thus, if V \u03c0PE (or V \u2217) can be approximated by some weighted sum of the basis functions using smaller weights, wmax can be chosen to be smaller. Unlike the exact case discussed in Proposition 1, the choice of basis functions is important beyond the subspace generated by their span. Therefore, transformations of the basis functions such as centralization, normalization, or orthogonalization might improve the effectiveness of MaxEnt Model Correction.\nOne limitation of the results of Theorem 1 is that they depend on the \u2113\u221e norm of \u03f5Model and \u03f5Query. However, if the functions P\u0302 and \u03c8i are estimated with function approximation, their error is generally controlled in some weighted \u2113p norm. Thus, error analysis of RL algorithms in weighted \u2113p norm is essential and has been the subject of many studies (Munos, 2003; 2007; Farahmand et al., 2010; Scherrer et al., 2015). We do provide this analysis for MaxEnt MoCo, but to keep the main body of the paper short and simple, we defer them to the supplementary material."
        },
        {
            "heading": "4 MODEL CORRECTING VALUE ITERATION",
            "text": "In the previous section, we introduced MaxEnt model correction for a given set of query functions \u03d51, . . . , \u03d5d. We saw that a good set of functions is one that for some w \u2208 Rd, the true value function V \u03c0PE or V \u2217 is well approximated by \u2211 i wi\u03d5i. In this section, we introduce the Model Correcting Value Iteration (MoCoVI) algorithm that iteratively finds increasingly better basis functions. We show that if the model is accurate enough, MoCoVI can utilize the approximate model to converge to the true value function despite the model error, and do so with a better convergence rate than the conventional VI. Since MoCoVI calls the MaxEnt MoCo procedure iteratively, we introduce a notation for it. If P\u0304 is the corrected dynamics based on the set of basis functions \u03a6 and their query results \u03a8, and V\u0304 \u03c0PE , V\u0304 \u2217, \u03c0\u0304\u2217 are the respective V \u03c0PE , V \u2217, \u03c0\u2217 in M\u0304 = (X ,A,R, P\u0304), we define MoCo\u03c0PE\u03b2 (R, P\u0302,\u03a6,\u03a8) \u225c V\u0304 \u03c0PE and MoCo \u2217 \u03b2(R, P\u0302,\u03a6,\u03a8) \u225c (V\u0304 \u2217, \u03c0\u0304\u2217) to be the solution of PE and Control problems obtained with MaxEnt MoCo.\nTo start with, consider the PE problem and assume that we can make exact queries to P . We set \u03d51, . . . , \u03d5d : X \u2192 R to be an arbitrary initial set of basis functions, with query results \u03c8i = P\u03d5i for 1 \u2264 i \u2264 d. We perform the MaxEnt MoCo procedure using \u03d51:d and \u03c81:d to obtain\nV0 = MoCo \u03c0PE \u03b2 (R, P\u0302, \u03d51:d, \u03c81:d). In the next iteration, we set \u03d5d+1 = V0.2 Then, we query P at \u03d5d+1 to obtain \u03c8d+1 = P\u03d5d+1. By executing MaxEnt MoCo with the last d queries, we arrive at V1 = MoCo \u03c0PE \u03b2 (R, P\u0302, \u03d52:d+1, \u03c82:d+1). We can use Proposition 1 to bound the error of V1.\n\u2225V \u03c0PE \u2212 V1\u2225\u221e \u2264 \u03b3 \u221a 2\n1\u2212 \u03b3 \u00b7 \u2225\u03f5Model\u2225\u221e \u00b7 infw\u2208Rd \u2225\u2225\u2225V \u03c0PE \u2212\u2211di=1 wi\u03d51+i\u2225\u2225\u2225\u221e \u2225V \u03c0PE \u2212 V0\u2225\u221e \u00b7 \u2225V \u03c0PE \u2212 V0\u2225\u221e\nAs \u2211d\ni=1 wi\u03d51+i is equal to V0 with the choice of w1:d\u22121 = 0 and wd = 1, the fraction above is less than or equal to 1. Generally, the fraction gets smaller with larger d and better basis function, leading to a more accurate V1. If the model is accurate enough, the new value function V1 is a more accurate approximation of V \u03c0PE than the initial V0. By repeating this procedure we may converge to V \u03c0PE .\nWe now introduce MoCoVI based on the above idea. We start with an initial set of basis functions \u03d51, . . . , \u03d5d and their query results \u03c81, . . . , \u03c8d such that \u03c8i \u2248 P\u03d5i for 1 \u2264 i \u2264 d. At each iteration k \u2265 0, we execute MaxEnt MoCo with \u03d5k+1:k+d and \u03c8k+1:k+d to obtain Vk (and \u03c0k). In the end, we set \u03d5k+d+1 = Vk and query P to get the new query result. That is, for any k \u2265 0{ Vk = MoCo \u03c0PE \u03b2 (R, P\u0302, \u03d5k+1:k+d, \u03c8k+1:k+d) or Vk, \u03c0k = MoCo \u2217 \u03b2(R, P\u0302, \u03d5k+1:k+d, \u03c8k+1:k+d),\n\u03d5k+d+1 = Vk , \u03c8k+d+1 \u2248 P\u03d5k+d+1.\nThe choice of value functions can be motivated from two viewpoints. First, it has been suggested that features learned to represent the past value function may be useful to represent the true value functions as well (Dabney et al., 2021). This suggests that the true value function may be approximated with the span of the past value functions. A property shown to be useful in Theorem 2. Second, this choice means that the corrected transition dynamics P\u0304 at iteration k will satisfy P\u0304Vk\u2212i \u2248 PVk\u2212i for i = 1, 2, . . . , d. This property has been recognized to be valuable for the dynamics that is used for planning in MBRL, and implemented in value-aware model learning losses (Farahmand et al., 2017; Farahmand, 2018; Abachi et al., 2020; Voelcker et al., 2022; Abachi et al., 2022). However, practical implementations of these losses has been shown to be challenging (Voelcker et al., 2022; Lovatto et al., 2020). In comparison, MoCoVI works with any model learning approach and creates this property through MaxEnt density estimation. The next theorem provides convergence result of MoCoVI in supremum norm based on the analysis in Theorem 1. Theorem 2. Let K \u2265 1. Assume \u03f5\u221eQuery(x, a) = \u221a d \u00b7 supi\u22650|(P\u03d5i)(x, a)\u2212 \u03c8i(x, a)| and \u03b2 = \u2225\u03f5Query\u2225\u221e/\u2225\u03f5Model\u2225\u221e. Let c1, c2 be as in Theorem 1 and wmax \u2265 1. Define V target = V \u03c0PE for PE and V target = V \u2217 for Control. Finally, let\n\u03b3\u2032 = 3c1\u2225\u03f5Model\u2225\u221e \u00b7 max 1\u2264k\u2264K\ninf\u2225w\u2225\u221e\u2264wmax \u2225V target \u2212 \u2211d\ni=1 wi\u03d5k+i\u2225\u221e \u2225V target \u2212 Vk\u22121\u2225\u221e .\nWe have\n\u2225V \u03c0PE \u2212 VK\u2225\u221e \u2264 \u03b3 \u2032K\u2225V \u03c0PE \u2212 V0\u2225\u221e +\n1\u2212 \u03b3\u2032K 1\u2212 \u03b3\u2032 c2 \u2225\u2225\u03f5\u221eQuery\u2225\u2225\u221ewmax,\n\u2225V \u2217 \u2212 V \u03c0K\u2225\u221e \u2264 2\u03b3\u2032K\n1\u2212 3c1\u2225\u03f5Model\u2225\u221e \u00b7 \u2225V \u2217 \u2212 V0\u2225\u221e +\n1\u2212 \u03b3\u2032K 1\u2212 \u03b3\u2032 2c2 \u2225\u2225\u03f5\u221eQuery\u2225\u2225\u221e\n1\u2212 3c1\u2225\u03f5Model\u2225\u221e wmax.\nThis result should be compared with the convergence analysis of approximate VI. Notice that both MoCoVI and VI query P once per iteration, which makes this comparison fair. According to Munos (2007), \u2225V \u2217 \u2212 V \u03c0K\u2225\u221e for VI is bounded by 2\u03b3K 1\u2212\u03b3 \u2225V \u2217 \u2212 V0\u2225\u221e + 2\u03b3(1\u2212\u03b3K\u22121) (1\u2212\u03b3)2 \u2225\u03f5Query\u2225\u221e. Here we considered the error in applying the Bellman operator equal to the query error. In VI, the initial error \u2225V \u2217 \u2212 V0\u2225\u221e decreases with the rate O ( \u03b3K ) . In comparison, for MoCoVI, the initial error decreases\nwith the rate O ( \u03b3\u2032k ) . While the convergence rate of VI is tied to the fixed parameter \u03b3 and become undesirable if \u03b3 is close to 1, the rate of MoCoVI improves with more accurate models. Consequently, the convergence rate of MoCoVI can be much faster than VI if the model is accurate enough.\n2According to the discussion after Theorem 1, it might be beneficial to set \u03d5d+1 to some linear transformations of V0 in presence of query error. For the sake of simplicity of the results, we don\u2019t consider such operations.\nAlgorithm 1 MoCoDyna(T, d, c, \u03b2,K)\n1: Initialize \u03d51, . . . , \u03d5d+c, \u03c81, . . . , \u03c8d+c, and P\u0302, r\u0302. 2: for t = 1, 2, . . . do 3: Sample Xt, At, Rt, X \u2032t from the environment. 4: r\u0302, P\u0302 \u2190 Update(r\u0302, P\u0302, Xt, At, Rt, X \u2032t) 5: \u03c81:d+c \u2190 Update(\u03c81:d+c, Xt, At, X \u2032t) 6: Vt \u2190 MoCo\u03c0PE\u03b2 (r\u0302, P\u0302, \u03d51:d, \u03c81:d) or Vt, \u03c0t \u2190 MoCo \u2217 \u03b2(r\u0302, P\u0302, \u03d51:d, \u03c81:d), 7: if t mod K = 0 then 8: Pop \u03d51, \u03c81 9: \u03d5d+c \u2190 BasisCreation(Vt, \u03d51:d+c\u22121) , \u03c8d+c(x, a)\u2190 0\n10: end if 11: end for\nA closely comparable algorithm to MoCoVI is OS-VI (Rakhsha et al., 2022). OS-VI also does solve a new MDP at each iteration, but instead changing the transition dynamics, changes the reward function. The convergence rate of OS-VI, when stated in terms of our \u03f5Model using Pinsker\u2019s inequality, is c1\u2225\u03f5Model\u2225\u221e. In comparison, \u03b3\u2032 can become much smaller if the past value functions can approximate the true value function well or if d is increased. Moreover, OS-VI can diverge if the model is too inaccurate, but even if \u03b3\u2032 > 1, the bound given in Theorem 1 still holds for Vk for all k, which means MoCoVI does not diverge."
        },
        {
            "heading": "5 MODEL CORRECTING DYNA",
            "text": "We extend MoCoVI to the sample-based setting where only samples from the true dynamics P are available. The key challenge is that we can no longer obtain \u03c8k from \u03d5k by a single query. Instead, we should form an estimate of P\u03d5k using the samples. In general, this is a regression task that is studied in supervised learning. In algorithms that a replay buffer of transitions (Xi, Ai, Ri, X \u2032i) N i=1 is stored, the regression can be done with (Xi, Ai) as the input and Vk(X \u2032i) as the target. In this paper, we present a version of the algorithm based on stochastic approximation, but we emphasize that the algorithm can be extended to use function approximation without any fundamental barriers.\nAn overview of MoCoDyna for finite MDPs is given in Algorithm 1. For some integer c \u2265 0, we keep d + c basis functions \u03d51, . . . , \u03d5d+c. As explained later, this set of basis functions is updated similar to MoCoVI: the oldest function is regularly substituted with the current value function. A set of approximate query results \u03c81, . . . , \u03c8d+c for the basis functions is also maintained. That is, we will have \u03c8i \u2248 P\u03d5i for each i via stochastic approximation. At each step, we get a sample (Xt, At, Rt, X \u2032t) from the environment. We update \u03c8i(Xt, At) for i = 1, . . . , d + c by \u03c8i(Xt, At)\u2190 \u03c8i(Xt, At) + 1Ni(Xt,At) (\u03d5i(X \u2032 t)\u2212 \u03c8i(Xt, At)). Here, Ni(Xt, At) is the number of times (Xt, At) has been visited since the function \u03d5i has been added to the set of basis functions. At every step, the agent also updates its approximate model r\u0302, P\u0302 using the new sample (Xt, At, Rt, X \u2032t). At each iteration, MoCoDyna runs the MaxEnt MoCo procedure to obtain the new value function and policy. That is, the agent uses an arbitrary planning algorithm to solve the PE or control problem with rewards r\u0302 and the dynamics obtained by correcting P\u0302 . The correction only uses the d oldest basis functions among the d+ c functions. The reason is that for a basis function \u03d5 that has been added to the set recently, the agent has not had enough samples to form an accurate approximation of P\u03d5i. Finally, every K steps, the agent updates its set of basis functions. The oldest function \u03d51 is removed along with \u03c81. The new basis function \u03d5d+c is chosen such that Vt belongs to span of \u03d51:d+c. In the simplest form, we can set \u03d5d+c = Vt, but as discussed after Theorem 1 some linear transformations might be beneficial. We allow this transformation by defining \u03d5d+c \u2190 BasisCreation(Vt, \u03d51:d+c\u22121)."
        },
        {
            "heading": "6 NUMERICAL EXPERIMENTS",
            "text": "We empirically show the effectiveness of MoCoVI and MoCoDyna to utilize an approximate model. We consider the 6\u00d7 6 grid world environment with four actions introduced by Rakhsha et al. (2022), with \u03b3 = 0.9. We defer the details of the environment to the supplementary material. As shown in Theorem 2, the convergence rate of MoCoVI depends on the model error and d. We introduce error\nto P\u0302 by smoothing the true dynamics P as suggested by Rakhsha et al. (2022): for \u03bb \u2208 [0, 1], the smoothed dynamics P(\u03bb) is P(\u03bb)(\u00b7|x, a) \u225c (1\u2212 \u03bb) \u00b7 P(\u00b7|x, a) + \u03bb \u00b7U ( {x\u2032|P(x\u2032|x, a) > 0} ) , where\nU(S) is the uniform distribution over set S. The parameter \u03bb controls the model error, from no error with \u03bb = 0 to a large error with \u03bb = 1 (uniform transition probability over possible next-states).\nFig. 1 first compares MoCoVI with OS-VI (Rakhsha et al., 2022), VI, and the value function obtained based on the model. We set P\u0302 = P(\u03bb) for \u03bb = 0.1, 0.5 and 1. The plot shows normalized error of Vk against V \u2217, that is, \u2225Vk \u2212 V \u2217\u22251/\u2225V \u2217\u22251. MoCoVI can converge to the true value function in a few iterations even with extreme model errors. The robustness, as expected, is improved with larger values of d. In comparison, OS-VI and VI show a much slower rate than MoCoVI and the value function obtained from P\u0302 suffers from the model error. Fig. 1 then shows the results in the RL setting. We compare MoCoDyna with OS-Dyna (Rakhsha et al., 2022), QLearning, and Dyna. At each step, the algorithms are given a sample (Xt, At, Rt, X \u2032t) where Xt, At are chosen uniformly in random. We use P\u0302 = P(\u03bb)MLE where PMLE is the MLE estimate of dynamics at the moment. For OS-Dyna and QLearning which have a learning rate, for some \u03b1,N > 0, we use the constant learning \u03b1 for t \u2264 N and \u03b1/(t \u2212 N) for t > N to allow both fast initial convergence and stability. The results show a similar pattern as for MoCoVI. MoCoDyna can successfully solve the task with any model error. In fact, MoCo with d = 2, 3 significantly outperforms other algorithms. In comparison, QLearning and OS-Dyna show a slower rate of convergence, and Dyna cannot solve the task due to the model error."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "In this work, we set out to bridge model-based and model-free approaches in RL by devising a costefficient approach to alleviate model errors. We develop the MaxEnt model correction framework, which adopts MaxEnt density estimation to reduce model errors given a small number of queries to the true dynamics. A thorough theoretical analysis indicates that our framework can significantly accelerate the convergence rate of policy evaluation and control algorithms, and ensure convergence to the true value functions despite model errors if said errors are sufficiently small. We also develop a sample-based variant, MoCoDyna, which extends the Dyna framework. Lastly, we confirm the practical relevance of our theoretical findings by benchmarking MoCo-based planning algorithms against their naive counterparts, and showing superior performance both in terms of convergence rate and expected returns. Future work should investigate deep RL applications of the MoCo framework."
        },
        {
            "heading": "A LIST OF APPENDICES",
            "text": "\u2022 Appendix B provides extended background on MDPs.\n\u2022 Appendix C contains the proofs for Section 2.1.\n\u2022 Appendix D provides technical details of Maximum Entropy Density Estimation.\n\u2022 Appendix E contains the proofs for Section 3.1.\n\u2022 Appendix F contains the proofs for Section 3.2.\n\u2022 Appendix G provides the analysis of MaxEnt MoCo in \u2113p norms\n\u2022 Appendix H contains the proofs for \u2113p analysis of MaxEnt MoCo.\n\u2022 Appendix I contains the proofs for Section 4.\n\u2022 Appendix J shows additional empirical results."
        },
        {
            "heading": "B BACKGROUND ON MARKOV DECISION PROCESSES",
            "text": "In this work, we consider a discounted Markov Decision Process (MDP) defined as M = (X ,A,R,P, \u03b3) (Bertsekas and Tsitsiklis, 1996; Szepesv\u00e1ri, 2010; Sutton and Barto, 2019). Here, X is the state space, A is the action space, R : X \u00d7 A \u2192 M(R) is the reward kernel, P : X \u00d7 A \u2192 M(X ) is the transition kernel, and 0 \u2264 \u03b3 < 1 is the discount factor.3 We define r : X \u00d7 A \u2192 R to be the expected reward and assume it is known to the agent. A policy \u03c0 : X \u2192 M(A) is a mapping from states to distributions over actions. We denote the expected rewards and transitions of a policy \u03c0 by r\u03c0 : X \u2192 R and P\u03c0 : X \u2192M(X ), respectively. For any function \u03d5 : X \u2192 R, we define P\u03d5 : X \u00d7A \u2192 R as\n(P\u03d5)(x, a) \u225c \u222b P(dx\u2032|x, a)\u03d5(x\u2032) (\u2200x, a).\nThe value function V \u03c0 = V \u03c0(R,P) of a policy \u03c0 is defined as\nV \u03c0(x) \u225c E [ \u221e\u2211 t=0 \u03b3tRt|X0 = x ] ,\nwhere actions are taken according to \u03c0, and Xt and Rt are the state and reward at step t. The value function of \u03c0 satisfies the Bellman equation: For all x \u2208 X , we have\nV \u03c0(x) = r\u03c0(x) + \u03b3 \u222b P\u03c0(dx\u2032|x)V \u03c0(x\u2032), (B.1)\nor in short, V \u03c0 = r\u03c0 + \u03b3P\u03c0V \u03c0. The optimal value function V \u2217 = V \u2217(R,P) is defined such that V \u2217(x) = max\u03c0 V \u03c0(x) for all states x \u2208 X . Similarly, V \u2217 satisfies the Bellman optimality equation:\nV \u2217(x) = max a\u2208A\n{ r(x, a) + \u03b3 \u222b P(dx\u2032|x, a)V \u2217(x\u2032) } . (B.2)\nWe denote an optimal policy by \u03c0\u2217 = \u03c0\u2217(P,R), for which we have V \u2217 = V \u03c0\u2217 . We refer to the problem of finding V \u03c0PE for a specific policy \u03c0PE as the Policy Evaluation (PE) problem, and to the problem of finding an optimal policy as the Control problem.\nThe greedy policy at state x \u2208 X is\n\u03c0g(x;V )\u2190 argmax a\u2208A\n{ r(x, a) + \u03b3 \u222b P(dy|x, a)V (y) } . (B.3)\nIn this paper, we assume an approximate model P\u0302 \u2248 P is given. We define V\u0302 \u03c0 and \u03c0\u0302\u2217 in the approximate MDP M\u0302 = (X ,A,R, P\u0302, \u03b3) similar to their counterparts in the true MDP M .\n3For a domain S, we denote the space of all distributions over S byM(S)."
        },
        {
            "heading": "C PROOFS FOR SECTION 2.1",
            "text": "In this section, we provide the proof of Lemma 1. Before that, we first show two useful lemmas. Lemma 2. For two transition dynamics P1,P2 and any policy \u03c0 we have\n\u2225P\u03c01 (\u00b7|x)\u2212 P\u03c02 (\u00b7|x)\u22251 \u2264 \u221a 2 \u222b \u03c0(da|x) \u221a DKL( P1(\u00b7|x, a) \u2225 P2(\u00b7|x, a) )\nProof. We have \u2225P\u03c01 (\u00b7|x)\u2212 P\u03c02 (\u00b7|x)\u22251 = \u222b y |P\u03c01 (dy|x)\u2212 P\u03c02 (dy|x)|\n= \u222b y \u2223\u2223\u2223\u2223\u222b a \u03c0(da, x)(P1(dy|x, a)\u2212 P2(dy|x, a)) \u2223\u2223\u2223\u2223\n\u2264 \u222b y \u222b a \u03c0(da, x)|P1(dy|x, a)\u2212 P2(dy|x, a)|\n= \u222b a \u03c0(da, x) \u222b y |P1(dy|x, a)\u2212 P2(dy|x, a)|\n= \u222b a \u03c0(da, x)\u2225P1(\u00b7|x, a)\u2212 P2(\u00b7|x, a)\u22251 (C.1)\n\u2264 \u222b a \u03c0(da, x) \u221a 2DKL( P1(\u00b7|x, a) \u2225 P2(\u00b7|x, a) )\nwhere we used the Pinsker\u2019s inequality.\nLemma 3. For two transition dynamics P1,P2 and any policy \u03c0, Define G\u03c0P1,P2 \u225c (I\u2212 \u03b3P \u03c0 2 )\n\u22121(\u03b3P\u03c01 \u2212 P\u03c02 ) We have \u2225\u2225\u2225G\u03c0P\u03c01 ,P\u03c02 \u2225\u2225\u2225\u221e \u2264 \u03b3 \u221a 2 1\u2212 \u03b3 sup x,a \u221a DKL( P1(\u00b7|x, a) \u2225 P2(\u00b7|x, a) )\nProof. When clear from context, we write G\u03c0 instead of G\u03c0P1,P2 . We have \u2225G\u03c0\u2225\u221e \u2264 \u03b3 \u2225\u2225(I\u2212 \u03b3P\u03c02 )\u22121\u2225\u2225\u221e\u2225P\u03c01 \u2212 P\u03c02 \u2225\u221e\n\u2264 \u03b3 1\u2212 \u03b3 sup x \u2225P\u03c01 (\u00b7|x)\u2212 P\u03c02 (\u00b7|x)\u22251\nwhere we used \u2225\u2225(I\u2212A)\u22121 = 1/(1\u2212 \u2225A\u2225\u221e)\u2225\u2225\u221e for \u2225A\u2225\u221e \u2264 1 and the fact that \u2225P\u03c02 \u2225\u221e = 1. Due to Lemma 2 we have for any x\n\u2225P\u03c01 (\u00b7|x)\u2212 P\u03c02 (\u00b7|x)\u22251 \u2264 \u221a 2 \u222b \u03c0(da|x) \u221a DKL( P1(\u00b7|x, a) \u2225 P2(\u00b7|x, a) )\n\u2264 sup x,a\n\u221a 2DKL( P1(\u00b7|x, a) \u2225 P2(\u00b7|x, a) )\nsubstituting this in the bound for \u2225G\u03c0\u2225\u221e gives the result.\nWe now give the proof of Lemma 1."
        },
        {
            "heading": "Proof of Lemma 1 for PE",
            "text": "Proof. Since V\u0302 \u03c0 = (I\u2212 \u03b3P\u0302\u03c0)\u22121r\u03c0 and r\u03c0 = (I\u2212 \u03b3P\u03c0)V \u03c0 we have\nV \u03c0 \u2212 V\u0302 \u03c0 = (I\u2212 \u03b3P\u0302\u03c0)\u22121(I\u2212 \u03b3P\u0302\u03c0)V \u03c0 \u2212 (I\u2212 \u03b3P\u0302\u03c0)\u22121r\u03c0\n= (I\u2212 \u03b3P\u0302\u03c0)\u22121[(I\u2212 \u03b3P\u0302\u03c0)V \u03c0 \u2212 (I\u2212 \u03b3P\u03c0)V \u03c0] = (I\u2212 \u03b3P\u0302\u03c0)\u22121(\u03b3P\u03c0 \u2212 \u03b3P\u0302\u03c0)V \u03c0\n= G\u03c0P,P\u0302V \u03c0 (C.2)\nThus, \u2225\u2225\u2225V \u03c0 \u2212 V\u0302 \u03c0\u2225\u2225\u2225 \u221e \u2264 \u03b3 \u2225\u2225\u2225(I\u2212 \u03b3P\u0302\u03c0)\u22121\u2225\u2225\u2225 \u221e \u2225\u2225\u2225(P\u03c0 \u2212 P\u0302\u03c0)V \u03c0\u2225\u2225\u2225 \u221e\n\u2264 \u03b3 1\u2212 \u03b3 \u2225\u2225\u2225(P\u03c0 \u2212 P\u0302\u03c0)V \u03c0\u2225\u2225\u2225 \u221e\n\u2264 \u03b3 1\u2212 \u03b3 \u2225\u2225\u2225(P\u03c0 \u2212 P\u0302\u03c0)\u2225\u2225\u2225 \u221e \u2225V \u03c0\u2225\u221e \u2264 \u03b3 \u221a 2\n1\u2212 \u03b3 \u2225\u03f5Model\u2225\u221e\u2225V \u03c0\u2225\u221e\nwhere we followed the proof of Lemma 3 for the last inequality."
        },
        {
            "heading": "Proof of Lemma 1 for Control",
            "text": "Proof. Define r0 = r + (\u03b3P \u2212 \u03b3P\u0302)V \u2217 similar to Rakhsha et al. (2022, Lemma 3)\nV \u03c0 \u2217 (r0, P\u0302) = V \u2217(r,P).\nAssume f \u227c g mean f(x) \u2265 g(x) for any x. We can write\n0 \u227c V \u03c0 \u2217 (r,P)\u2212 V \u03c0\u0302 \u2217 (r,P)\n= V \u03c0 \u2217 (r0, P\u0302)\u2212 V \u03c0\u0302 \u2217 (r,P) = V \u03c0 \u2217 (r0, P\u0302)\u2212 V \u03c0 \u2217 (r, P\u0302) + V \u03c0 \u2217 (r, P\u0302)\u2212 V \u03c0\u0302 \u2217 (r,P) \u227c V \u03c0 \u2217 (r0, P\u0302)\u2212 V \u03c0 \u2217 (r, P\u0302) + V \u03c0\u0302 \u2217 (r, P\u0302)\u2212 V \u03c0\u0302 \u2217 (r,P) = (I\u2212 \u03b3P\u0302\u03c0 \u2217 )\u22121(r\u03c0 \u2217 0 \u2212 r\u03c0 \u2217 ) + V \u03c0\u0302 \u2217 (r, P\u0302)\u2212 V \u03c0\u0302 \u2217 (r,P) = (I\u2212 \u03b3P\u0302\u03c0 \u2217 )\u22121(\u03b3P\u03c0 \u2217 \u2212 \u03b3P\u0302\u03c0 \u2217 )V \u2217 + V \u03c0\u0302 \u2217 (r, P\u0302)\u2212 V \u03c0\u0302 \u2217 (r,P)\n= G\u03c0 \u2217 P,P\u0302V \u2217 + V \u03c0\u0302 \u2217 (r, P\u0302)\u2212 V \u03c0\u0302 \u2217 (r,P) = G\u03c0 \u2217\nP,P\u0302V \u2217 \u2212G\u03c0\u0302\n\u2217 P,P\u0302V \u03c0\u0302\u2217(r,P)\n= G\u03c0 \u2217 P,P\u0302V \u2217 \u2212G\u03c0\u0302 \u2217 P,P\u0302V \u2217 +G\u03c0\u0302 \u2217 P,P\u0302(V \u2217 \u2212 V \u03c0\u0302 \u2217 (r,P)) \u227c \u2223\u2223\u2223G\u03c0\u2217P,P\u0302V \u2217\u2223\u2223\u2223+ \u2223\u2223\u2223G\u03c0\u0302\u2217P,P\u0302V \u2217\u2223\u2223\u2223+ \u2223\u2223\u2223G\u03c0\u0302\u2217P,P\u0302(V \u2217 \u2212 V \u03c0\u0302\u2217(r,P))\u2223\u2223\u2223\nwhere we used (C.2). Comparing the first line with the least, we obtain\u2225\u2225\u2225V \u03c0\u2217 \u2212 V \u03c0\u0302\u2217\u2225\u2225\u2225 \u221e \u2264 \u2225\u2225\u2225G\u03c0\u2217P,P\u0302V \u2217\u2225\u2225\u2225\u221e + \u2225\u2225\u2225G\u03c0\u0302\u2217P,P\u0302V \u2217\u2225\u2225\u2225\u221e + \u2225\u2225\u2225G\u03c0\u0302\u2217P,P\u0302(V \u2217 \u2212 V \u03c0\u0302\u2217)\u2225\u2225\u2225\u221e (C.3)\n\u2264 2c1\u2225\u03f5Model\u2225\u221e\u2225V \u2217\u2225\u221e ++c1\u2225\u03f5Model\u2225\u221e \u2225\u2225\u2225V \u2217 \u2212 V \u03c0\u0302\u2217\u2225\u2225\u2225 \u221e\nwhere we used Lemma 3. Rearranging the terms give the result."
        },
        {
            "heading": "D TECHNICAL DETAILS OF MAXIMUM ENTROPY DENSITY ESTIMATION",
            "text": "In this section we present the technical details of maximum entropy density estimation. This involves the duality methods for solving the optimization algorithms and some useful lemmas regarding their solutions. These problems are well studied in the literature . We do not make any assumptions on whether the environment states space, which will be the domain of the distributions in this section, is finite or continuous, and we make arguments for general measures. Due to this, our assumptions as well as our constraints may differ from the original papers in the literature. In those cases, we prove the results ourselves."
        },
        {
            "heading": "D.1 MAXIMUM ENTROPY DENSITY ESTIMATION WITH EQUALITY CONSTRAINTS",
            "text": "Assume Z is a random variable over domain Z with an unknown distribution p \u2208 M(Z). For a set of functions \u03d51, . . . , \u03d5d : Z \u2192 R the expected values \u03d5\u0304i = EZ\u223cp[\u03d5i(Z)] are given. We will use \u03d5 : Z \u2192 Rd and \u03d5\u0304 \u2208 Rd to refer to the respective vector forms. We also have access to an approximate distribution p\u0302 \u2208 M(Z) such that p\u0302 \u2248 p. The maximum entropy density estimation gives a new approximation q\u2217 that is the solution of the following optimization problem\nmin q\u2208M(Z)\nDKL( q \u2225 p\u0302 ), (D.1)\ns.t. EZ\u223cq[\u03d5i(Z)] = \u03d5\u0304i (1 \u2264 i \u2264 d).\nThe KL-divergence in (D.1) is finite only if q is absolutely continuous w.r.t. p\u0302. In that case, q can be specified with its density w.r.t p\u0302 defined as f \u225c dqdp\u0302 : Z \u2192 R where dq dp\u0302 is the Radon-Nikodym derivative. Let L1(Z, p\u0302) be the L1 space over Z with measure p\u0302. We have f \u2208 L1(Z, p\u0302). Also for any f \u2208 L1(Z, p\u0302) such that f \u2265 0 and \u222b f(z)p\u0302(dz) = 1 we can recover a distribution q \u2208M(Z) as\nq(A) = \u222b A f(z)p\u0302(dz). (D.2)\nConsequently, (D.1) can be written in terms of f . We have DKL( q \u2225 p\u0302 ) = \u222b q(dz) log q(dz)\np(dz)\n=\n\u222b q(dz)\np(dz) log\nq(dz) p(dz) \u00b7 p\u0302(dz)\n= \u222b f(z) log f(z) \u00b7 p\u0302(dz)\nWe can write (D.1) as\nmin f\n\u222b f(z) log f(z) p\u0302(dz), (D.3)\ns.t. \u222b f(z)\u03d5i(z) p\u0302(dz) = \u03d5\u0304i (1 \u2264 i \u2264 d),\u222b f(z) p\u0302(dz) = 1,\nf \u2208 L1(Z, p\u0302). The constraint f \u2265 0 is implicit in the domain of the KL objective. The Lagrangian with dual parameters \u03bb,\u039b\u2032 is\nL(f, \u03bb,\u039b\u2032) = \u222b [ f(z) log f(z)\u2212\nd\u2211 i=1 \u03bbif(z)\u03d5i(z) + \u039b \u2032f(z)\n] p\u0302(dz) +\n\u2211 i \u03bbi\u03d5\u0304i \u2212 \u039b\u2032. (D.4)\nThen, we can obtain the dual objective D\u03d5\u0304(\u03bb,\u039b \u2032) \u225c inff L(f, \u03bb,\u039b\u2032) from this result by Decarreau et al. (1992, Proposition 2.4). Lemma 4 ((Decarreau et al., 1992)). For any fixed \u03bb \u2208 Rd,\u039b\u2032 \u2208 R, the Lagrangian L in (D.4) has a unique minimizer f\u03bb,\u039b\u2032 defined as\nf\u03bb,\u039b\u2032 \u225c exp ( d\u2211 i=1 \u03bbi\u03d5i(z)\u2212 \u039b\u2032 \u2212 1 ) ."
        },
        {
            "heading": "The dual objective is given by",
            "text": "D\u03d5\u0304(\u03bb,\u039b \u2032) = \u2212\n\u222b exp ( d\u2211 i=1 \u03bbi\u03d5i(z)\u2212 \u039b\u2032 \u2212 1 ) p\u0302(dz) + d\u2211 i=1 \u03bbi\u03d5\u0304i \u2212 \u039b\u2032.\nIt is concave, continuously differentiable, and its partial derivatives are \u2202D\u03d5\u0304(\u03bb,\u039b \u2032)\n\u2202\u03bbi = \u03d5\u0304i \u2212\n\u222b f\u03bb,\u039b\u2032(z)\u03d5i(z)p\u0302(dz) ,\n\u2202D\u03d5\u0304(\u03bb,\u039b \u2032)\n\u2202\u039b\u2032 = 1\u2212\n\u222b f\u03bb,\u039b\u2032(z)p\u0302(dz).\nThus, for the dual objective D\u03d5\u0304(\u03bb,\u039b \u2032) we have D\u03d5\u0304(\u03bb,\u039b \u2032) = L(f\u03bb,\u039b\u2032 , \u03bb,\u039b \u2032). We arrive at the following dual problem\nmax \u03bb\u2208Rd,\u039b\u2032\u2208R\nD\u03d5\u0304(\u03bb,\u039b \u2032) =\n[ \u2212 \u222b exp ( d\u2211 i=1 \u03bbi\u03d5i(z)\u2212 \u039b\u2032 \u2212 1 ) p\u0302(dz) + d\u2211 i=1 \u03bbi\u03d5\u0304i \u2212 \u039b\u2032 ] , (D.5)\nThe following result by Borwein and Lewis (1991, Corollary 2.6 and Theorem 4.8) shows the duality of the problems.\nTheorem 3 ((Borwein and Lewis, 1991)). Assume \u03d5i \u2208 L\u221e(Z, p\u0302) for i = 1, . . . , n. Under certain constraint qualification constraints, the value of (D.3) and (D.5) is equal with dual attainment. Furthermore, let \u03bb\u2217,\u039b\u2032\u2217 be dual optimal. The primal optimal solution is f\u03bb\u2217,\u039b\u2032\u2217 .\nWe do not discuss the technical details of the constraint qualification constraints and refer the readers to (Altun and Smola, 2006; Borwein and Lewis, 1991; Decarreau et al., 1992) for a complete discussion.\nFor any \u03bb \u2208 Rd, the optimal value of \u039b\u2032 can be computed. Due to Lemma 4, we have\n\u2202D\u03d5\u0304(\u03bb,\u039b \u2032)\n\u2202\u039b\u2032 = \u2202D\u03b2,\u03d5\u0304(\u03bb,\u039b\n\u2032)\n\u2202\u039b\u2032 = 1\u2212\n\u222b f\u03bb,\u039b\u2032(z)p\u0302(dz) (D.6)\nsolving for the optimal \u039b\u2032 gives the following value\n\u039b\u2032\u03bb \u225c log \u222b exp ( d\u2211 i=1 \u03bbi\u03d5i(z)\u2212 1 ) p\u0302(dz). (D.7)\nNote that since functions \u03d5i are bounded this quantity is finite. Thus, we can just optimize D\u03d5\u0304(\u03bb,\u039b \u2032 \u03bb) over \u03bb. By substitution we get\nD\u03d5\u0304(\u03bb) \u225c D\u03d5\u0304(\u03bb,\u039b \u2032 \u03bb) = d\u2211 i=1 \u03bbi\u03d5\u0304i \u2212 log \u222b exp ( d\u2211 i=1 \u03bbi\u03d5i(z) ) p\u0302(dz) (D.8)\nWe observe that max\u03bbD\u03d5\u0304(\u03bb) is equivalent to (2.4).\nD.2 MAXIMUM ENTROPY DENSITY ESTIMATION WITH \u211322 REGULARIZATION\nWe now study a relaxed form of the maximum entropy density estimation. In this form, instead of imposing strict equality constraints, the mismatch between the expected value EZ\u223cq[\u03d5i(Z)] with \u03d5\u0304i is added to the loss. The benefit of this version is that even if \u03d5\u0304i values are not exactly equal to EZ\u223cp[\u03d5i(Z)], the problem remains feasible. Moreover, we can adjust the weight of this term in the loss based on the accuracy of \u03d5\u0304i values. Specifically, we define the following problem.\nmin q\u2208M(Z)\nDKL( q \u2225 p\u0302 ) + 1\n\u03b22 d\u2211 i=1 ( EZ\u223cq[\u03d5i(Z)]\u2212 \u03d5\u0304i )2 (D.9)\nSimilar to the previous section, we can write the above problem in terms of the density dqdp\u0302 and write (Decarreau et al., 1992)\nmin f,\u03be\n\u222b f(z) log f(z) p\u0302(dz) + 1\n\u03b22 d\u2211 i=1 \u03be2i , (D.10)\ns.t. \u222b f(z)\u03d5i(z) p\u0302(dz)\u2212 \u03d5\u0304i = \u03bei (1 \u2264 i \u2264 d),\u222b f(z) p\u0302(dz) = 1,\nf \u2208 L1(Z, p\u0302) , \u03be \u2208 Rd.\nThe Lagrangian of this problems can be written as\nL\u03b2(f, \u03be, \u03bb,\u039b \u2032) = L(f, \u03bb,\u039b\u2032) +\n1\n\u03b22 d\u2211 i=1 \u03be2i + d\u2211 i=1 \u03bbi\u03bei (D.11)\nThe dual objective is then\nD\u03b2,\u03d5\u0304(\u03bb,\u039b \u2032) = inf\nf,\u03be L\u03b2(f, \u03be, \u03bb,\u039b\n\u2032)\nIt can be observed that f and \u03be can be independently optimized for any fixed \u03bb,\u039b\u2032. Due to Lemma 4, the optimal value of f is f\u03bb,\u039b\u2032 . The optimal value of \u03bei can be calculated as\n\u03be\u03bb = \u2212 1\n2 \u03b22\u03bb (D.12)\nWe arrive at the following dual objective\nD\u03b2,\u03d5\u0304(\u03bb,\u039b \u2032) = L(f\u03bb,\u039b\u2032 , \u03bb,\u039b\n\u2032) + 1\n4 \u03b22 d\u2211 i=1 \u03bb2i + 1 \u03b22 d\u2211 i=1 \u03be\u03bb 2 i + d\u2211 i=1 \u03bbi\u03be\u03bbi, (D.13)\n= L(f\u03bb,\u039b\u2032 , \u03bb,\u039b \u2032) +\n1 4 \u03b22 d\u2211 i=1 \u03bb2i \u2212 1 2 \u03b22 d\u2211 i=1 \u03bb2i ,\n= D\u03d5\u0304(\u03bb,\u039b \u2032)\u2212 1\n4 \u03b22 d\u2211 i=1 \u03bb2i ,\nwhich means we have the dual problem\nmax \u03bb\u2208Rd,\u039b\u2032\u2208R\nD\u03d5\u0304(\u03bb,\u039b \u2032)\u2212 1\n4 \u03b22 d\u2211 i=1 \u03bb2i . (D2)\nWe now show the duality of the problems. Notice how this problem has an extra 14\u03b2 2 \u2211d i=1 \u03bb 2 i compared to (2.4). This is the reason this problem is considered the regularized version of (2.4). Notice that the regularization term also makes the dual loss strongly concave. This makes solving the optimization problem easier.\nTheorem 4. Assume \u03d5i is bounded for i = 1, . . . , n and \u03b2 > 0. The value of (D.10) and (D2) is equal with dual attainment. Furthermore, let \u03bb\u2217,\u039b\u2032\u2217 be dual optimal. The primal optimal solution is f\u03bb\u2217,\u039b\u2032\u2217 .\nProof. First, we show that the some solution \u03bb\u2217,\u039b\u2032\u2217 exists for the dual problem. To see this, first note that for any \u03bb \u2208 Rd, the optimal value of \u039b\u2032 is \u039b\u2032\u03bb defined in (D.6). Now we need to show D\u03b2,\u03d5\u0304(\u03bb,\u039b \u2032 \u03bb) is maximized by some \u03bb \u2217. We have\nD\u03b2,\u03d5\u0304(\u03bb,\u039b \u2032 \u03bb) = D\u03d5\u0304(\u03bb,\u039b \u2032 \u03bb)\u2212\n1 4 \u03b22 d\u2211 i=1 \u03bb2i . (D.14)\nSince D is concave, D\u03d5\u0304(\u03bb,\u039b \u2032 \u03bb) and therefore D\u03b2,\u03d5\u0304(\u03bb,\u039b \u2032 \u03bb) is also concave. Due to Weierstrass\u2019 Theorem (Bertsekas, 2009, Proposition 3.2.1) it suffices to show the set\nS = { \u03bb \u2208 Rd : D\u03b2,\u03d5\u0304(\u03bb,\u039b\u2032\u03bb) \u2265 D\u03b2,\u03d5\u0304(0,\u039b\u20320) }\nis non-empty and bounded. It is trivially non-empty. Assume |\u03d5(z)| \u2264 \u03d5max for any z and 1 \u2264 i \u2264 d. For any \u03bb \u2208 S, we have\nD\u03b2,\u03d5\u0304(0,\u039b \u2032 0) \u2264 D\u03b2,\u03d5\u0304(\u03bb,\u039b\u2032\u03bb)\n= D\u03d5\u0304(\u03bb,\u039b \u2032 \u03bb)\u2212\n1 4 \u03b22 d\u2211 i=1 \u03bb2i\n= \u2212 \u222b exp ( d\u2211 i=1 \u03bbi\u03d5i(z)\u2212 \u039b\u2032\u03bb \u2212 1 ) p\u0302(dz) + d\u2211 i=1 \u03bbi\u03d5\u0304i \u2212 \u039b\u2032\u03bb \u2212 1 4 \u03b22 d\u2211 i=1 \u03bb2i\n= \u22121 + d\u2211\ni=1\n\u03bbi\u03d5\u0304i \u2212 log \u222b exp ( d\u2211 i=1 \u03bbi\u03d5i(z)\u2212 1 ) p\u0302(dz)\u2212 1 4 \u03b22 d\u2211 i=1 \u03bb2i\n\u2264 d\u2225\u03bb\u2225\u221e \u2225\u2225\u03d5\u0304\u2225\u2225\u221e \u2212 log \u222b exp(\u2212 d\u2225\u03bb\u2225\u221e\u03d5max \u2212 1)p\u0302(dz)\u2212 14\u03b22\u2225\u03bb\u22252\u221e\n\u2264 \u22121 4 \u03b22\u2225\u03bb\u22252\u221e + d\u2225\u03bb\u2225\u221e \u2225\u2225\u03d5\u0304\u2225\u2225\u221e + d\u2225\u03bb\u2225\u221e\u03d5max + 1. which enforces an upper bound on \u2225\u03bb\u2225\u221e. This means that some optimal solution \u03bb\u2217,\u039b\u2032\u03bb\u2217 exists. Now we show that f\u03bb\u2217,\u039b\u2032\n\u03bb\u2217 is primal optimal with \u03be\u03bb\u2217 . First, note that the derivative (D.6) is zero for\n\u03bb\u2217,\u039b\u2032\u03bb\u2217 due to the derivation of \u039b \u2032 \u03bb. Thus, f\u03bb\u2217,\u039b\u2032\u03bb\u2217 is feasible. Similarly using Lemma 4 we have from (D.14)\n0 = \u2202D\u03b2,\u03d5\u0304(\u03bb\n\u2217,\u039b\u2032\u03bb\u2217)\n\u2202\u03bbi\n= \u2202D\u03d5\u0304(\u03bb\n\u2217,\u039b\u2032\u03bb\u2217)\n\u2202\u03bbi \u2212 1 2 \u03b22\u03bb\u2217i\n= \u03d5\u0304i \u2212 \u222b f\u03bb\u2217,\u039b\u2032\n\u03bb\u2217 (z)\u03d5i(z)p\u0302(dz) + \u03be\u03bb\u2217 i,\nwhich shows \u03be\u03bb\u2217 is feasible.\nConsider another feasible f, \u03be for (D.10).\u222b f(z) log f(z) p\u0302(dz) + 1\n\u03b22 d\u2211 i=1 \u03be2i = L\u03b2(f, \u03be, \u03bb \u2217,\u039b\u2032\u03bb\u2217)\n\u2265 D\u03b2,\u03d5\u0304(\u03bb\u2217,\u039b\u2032\u03bb\u2217) = L\u03b2(f\u03bb\u2217,\u039b\u2032\n\u03bb\u2217 , \u03be\u03bb\u2217 , \u03bb\n\u2217,\u039b\u2032\u03bb\u2217)\n= \u222b f\u03bb\u2217,\u039b\u2032\n\u03bb\u2217 (z) log f\u03bb\u2217,\u039b\u2032 \u03bb\u2217 (z) p\u0302(dz) +\n1\n\u03b22 d\u2211 i=1 \u03be\u03bb\u2217 2,\nwhich proves the claim.\nSimilar to the exact formulation, we can substitute \u039b\u2032 with \u039b\u2032\u03bb to obtain a loss function based on \u03bb. We arrive at the loss function\nD\u03b2,\u03d5\u0304(\u03bb) \u225c D\u03b2,\u03d5\u0304(\u03bb,\u039b \u2032 \u03bb) = D\u03d5\u0304(\u03bb,\u039b \u2032 \u03bb)\u2212\n1 4 \u03b22 d\u2211 i=1 \u03bb2i\n= D\u03d5\u0304(\u03bb)\u2212 1\n4 \u03b22 d\u2211 i=1 \u03bb2i\n= d\u2211 i=1 \u03bbi\u03d5\u0304i \u2212 log \u222b exp ( d\u2211 i=1 \u03bbi\u03d5i(z) ) p\u0302(dz)\u2212 1 4 \u03b22 d\u2211 i=1 \u03bb2i . (D.15)"
        },
        {
            "heading": "D.3 LEMMAS REGARDING MAXIMUM ENTROPY DENSITY ESTIMATION",
            "text": "Lemma 5 (Dud\u00edk et al. (2007)). Let \u03bb\u22171, \u03bb\u22172 be the minimizers of D\u03b2,\u03d5\u0304(\u03bb) for \u03d5\u0304 (1) and \u03d5\u0304(2), respectively. We have\n\u2225\u03bb\u22171 \u2212 \u03bb\u22172\u22252 \u2264 2 \u03b22 \u00b7 \u2225\u2225\u2225\u03d5\u0304(1) \u2212 \u03d5\u0304(2)\u2225\u2225\u2225 2 .\nProof. Define\ng(\u03bb) \u225c log \u222b exp ( d\u2211 i=1 \u03bbi\u03d5i(z) ) p\u0302(dz).\nSince g(\u03bb) = \u2211d\ni=1 \u03bbi\u03d5\u0304 (1) i \u2212D\u03d5\u0304(1)(\u03bb) and D\u03d5\u0304(1)(\u03bb) is concave, we know that g is convex. Due to\noptimality of \u03bb\u22171, \u03bb \u2217 2 we have\n\u2207D \u03b2,\u03d5\u0304(1) (\u03bb\u22171) = \u2212\u2207g(\u03bb\u22171) + \u03d5\u0304 (1) \u2212 1 2 \u03b22\u03bb\u22171 = 0,\n\u2207D \u03b2,\u03d5\u0304(2) (\u03bb\u22172) = \u2212\u2207g(\u03bb\u22172) + \u03d5\u0304 (2) \u2212 1 2 \u03b22\u03bb\u22172 = 0.\nBy taking the difference we get\n1 2 \u03b22(\u03bb\u22171 \u2212 \u03bb\u22172) = \u2212(\u2207g(\u03bb\u22171)\u2212\u2207g(\u03bb\u22172)) + (\u03d5\u0304 (1) \u2212 \u03d5\u0304(2)).\nMultiplying both sides by (\u03bb\u22171 \u2212 \u03bb\u22172)\u22a4 we get\n1 2 \u03b22\u2225\u03bb\u22171 \u2212 \u03bb\u22172\u2225 2 2 = \u2212\u27e8\u2207g(\u03bb \u2217 1)\u2212\u2207g(\u03bb\u22172), \u03bb\u22171 \u2212 \u03bb\u22172\u27e9+ \u27e8\u03d5\u0304 (1) \u2212 \u03d5\u0304(2), \u03bb\u22171 \u2212 \u03bb\u22172\u27e9.\nDue to the convexity of g, we have\n\u27e8\u2207g(\u03bb\u22171)\u2212\u2207g(\u03bb\u22172), \u03bb\u22171 \u2212 \u03bb\u22172\u27e9 \u2265 0.\nThus, we continue\n1 2 \u03b22\u2225\u03bb\u22171 \u2212 \u03bb\u22172\u2225 2 2 = \u2212\u27e8\u2207g(\u03bb \u2217 1)\u2212\u2207g(\u03bb\u22172), \u03bb\u22171 \u2212 \u03bb\u22172\u27e9+ \u27e8\u03d5\u0304 (1) \u2212 \u03d5\u0304(2), \u03bb\u22171 \u2212 \u03bb\u22172\u27e9\n\u2264 \u27e8\u03d5\u0304(1) \u2212 \u03d5\u0304(2), \u03bb\u22171 \u2212 \u03bb\u22172\u27e9 \u2264 \u2225\u2225\u2225\u03d5\u0304(1) \u2212 \u03d5\u0304(2)\u2225\u2225\u2225\n2 \u2225\u03bb\u22171 \u2212 \u03bb\u22172\u22252,\nwhere we used the Cauchy-Schwartz inequality. Dividing by \u2225\u03d5\u0304(1) \u2212 \u03d5\u0304(2)\u22252 proves the result.\nLemma 6 (Dud\u00edk et al. (2007)). Let \u03bb\u2217 minimize D\u03b2,\u03d5\u0304. Then for any \u03bb, we have\nDKL( p \u2225 q\u03bb\u2217 ) \u2264 DKL( p \u2225 q\u03bb ) + 2 \u03b22 \u2225\u2225EZ\u223cp[\u03d5(Z)]\u2212 \u03d5\u0304\u2225\u222522 + \u03b224 \u2225\u03bb\u222522"
        },
        {
            "heading": "E PROOFS FOR SECTION 3.1",
            "text": "We first show the following lemma: Lemma 7. For any policy \u03c0, we have \u2225\u2225\u2225G\u03c0P,P\u0304\u2225\u2225\u2225\u221e \u2264 c1\u2225\u03f5Model\u2225\u221e .\nProof. Since the feasibility set of Problem (P1) is convex, and P(\u00b7|x, a) belongs to it, we have from Pythagoras theorem for KL-divergence (see Thm. 11.6.1 of Cover and Thomas 2006) that\nDKL( P(\u00b7|x, a) \u2225 P\u0302(\u00b7|x, a) ) \u2265 DKL( P(\u00b7|x, a) \u2225 P\u0304(\u00b7|x, a) ) +DKL( P\u0304(\u00b7|x, a) \u2225 P\u0302(\u00b7|x, a) ) \u2265 DKL( P(\u00b7|x, a) \u2225 P\u0304(\u00b7|x, a) ).\nFrom Lemma 3 we have\u2225\u2225\u2225G\u03c0P,P\u0304\u2225\u2225\u2225\u221e \u2264 c1 supx,a \u221a DKL( P(\u00b7|x, a) \u2225 P\u0304(\u00b7|x, a) )\n\u2264 c1 sup x,a\n\u221a DKL( P(\u00b7|x, a) \u2225 P\u0302(\u00b7|x, a) )\n\u2264 c1\u2225\u03f5Model\u2225\u221e."
        },
        {
            "heading": "Proof of Proposition 1",
            "text": "Proof. Due the constraint in Problem (P1), for any i we have (P \u2212 P\u0304)\u03d5i = 0 and therefore G\u03c0PEP,P\u0304\u03d5i = 0. Thus, using the proof of Lemma 1, for any w \u2208 R\nd\u2225\u2225V \u03c0PE \u2212 V\u0304 \u03c0PE\u2225\u2225\u221e \u2264 \u2225\u2225\u2225G\u03c0PEP,P\u0304V \u03c0PE\u2225\u2225\u2225\u221e =\n\u2225\u2225\u2225\u2225\u2225G\u03c0PEP,P\u0304(V \u03c0PE \u2212\u2211 i wi\u03d5i) \u2225\u2225\u2225\u2225\u2225 \u221e\n= \u2225\u2225\u2225G\u03c0PEP,P\u0304\u2225\u2225\u2225\u221e \u2225\u2225\u2225\u2225\u2225V \u03c0PE \u2212\u2211 i wi\u03d5i \u2225\u2225\u2225\u2225\u2225 \u221e\n= c1\u2225\u03f5Model\u2225\u221e \u2225\u2225\u2225\u2225\u2225V \u03c0PE \u2212\u2211 i wi\u03d5i \u2225\u2225\u2225\u2225\u2225 \u221e .\nSimilarly for control, from (C.3), we have for any w \u2208 Rd\u2225\u2225\u2225V \u03c0\u2217 \u2212 V \u03c0\u0304\u2217\u2225\u2225\u2225 \u221e \u2264 \u2225\u2225\u2225G\u03c0\u2217P,P\u0304V \u2217\u2225\u2225\u2225\u221e + \u2225\u2225\u2225G\u03c0\u0304\u2217P,P\u0304V \u2217\u2225\u2225\u2225\u221e + \u2225\u2225\u2225G\u03c0\u0304\u2217P,P\u0304(V \u2217 \u2212 V \u03c0\u0304\u2217)\u2225\u2225\u2225\u221e\n\u2264 \u2225\u2225\u2225G\u03c0\u2217P,P\u0304(V \u2217 \u2212\u2211\ni\nwi\u03d5i) \u2225\u2225\u2225 \u221e + \u2225\u2225\u2225G\u03c0\u0304\u2217P,P\u0304(V \u2217 \u2212\u2211\ni\nwi\u03d5i) \u2225\u2225\u2225 \u221e\n+ \u2225\u2225\u2225G\u03c0\u0304\u2217P,P\u0304(V \u2217 \u2212 V \u03c0\u0304\u2217)\u2225\u2225\u2225\u221e\n\u2264 2c1 \u2225\u2225\u2225\u03f5Model\u2225\u2225\u2225\n\u221e \u2225\u2225\u2225V \u2217 \u2212\u2211 i wi\u03d5i \u2225\u2225\u2225 \u221e + c1 \u2225\u2225\u2225\u03f5Model\u2225\u2225\u2225 \u221e \u2225\u2225\u2225V \u2217 \u2212 V \u03c0\u0304\u2217\u2225\u2225\u2225 \u221e ,\nwhere we used Lemma 7 in the last inequality. Rearranging the terms yields the result."
        },
        {
            "heading": "F PROOFS FOR SECTION 3.2",
            "text": "In this section, we provide the analysis of MaxEnt MoCo in supremum norm. We will show a sequence of lemmas before providing the result for general \u03b2 and then proof of Theorem 1. Lemma 8. If P\u0304 is the solution of the optimization problem (P2), for any x, a we have\nDKL( P(\u00b7|x, a) \u2225 P\u0304(\u00b7|x, a) ) \u2264 DKL( P(\u00b7|x, a) \u2225 P\u0302(\u00b7|x, a) ) + 2\n\u03b22 \u03f5Query(x, a)\n2.\nProof. For \u03bb \u2208 Rd, define q\u03bb(A) \u225c \u222b A P\u0302(dy|x, a) exp ( d\u2211\ni=1\n\u03bbi\u03d5i(y)\u2212 \u039b\u03bb ) ,\nwhere \u039b\u03bb is the log-normalizer and A \u2286 X . Due to Lemma 6, for any \u03bb we have\nDKL( P(\u00b7|x, a) \u2225 P\u0304(\u00b7|x, a) ) \u2264 DKL( P(\u00b7|x, a) \u2225 q\u03bb(\u00b7|x, a) ) + 2\n\u03b22 d\u2211 i=1 [(P\u03d5i)(x, a)\u2212 \u03c8i(x, a)]2+\n\u03b22\n4 \u2225\u03bb\u222522.\nSince q0 = P\u0302(\u00b7|x, a), substituting \u03bb = 0 gives the result.\nLemma 9. If P\u0304 is the solution of the optimization problem (P2), for any x, a we have\u2225\u2225P(\u00b7|x, a)\u2212 P\u0304(\u00b7|x, a)\u2225\u2225 1 \u2264 \u221a 2\u03f5Model(x, a) + 2\n\u03b2 \u03f5Query(x, a).\nProof. Using Lemma 8 and Pinsker\u2019s inequality, and the fact that \u221a a+ b \u2264 \u221a a+ \u221a b, we write\u2225\u2225P(\u00b7|x, a)\u2212 P\u0304(\u00b7|x, a)\u2225\u2225 1 \u2264 \u221a 2DKL( P(\u00b7|x, a) \u2225 P\u0304(\u00b7|x, a) )\n\u2264 \u221a 2DKL( P(\u00b7|x, a) \u2225 P\u0304(\u00b7|x, a) ) + 4\n\u03b22 \u03f5Query(x, a)2\n\u2264 \u221a 2\u03f5Model(x, a) + 2\n\u03b2 \u03f5Query(x, a).\nLemma 10. For any x, a we have\nd\u2211 i=1 \u2223\u2223(P\u0304\u03d5i)(x, a)\u2212 (P\u03d5i)(x, a)\u2223\u2223 \u2264 \u221ad(2\u03f5Query(x, a) + \u03b2\u03f5Model(x, a)), also for any policy \u03c0\nd\u2211 i=1 \u2223\u2223(P\u0304\u03c0\u03d5i)(x)\u2212 (P\u03c0\u03d5i)(x)\u2223\u2223 \u2264 \u221ad\u222b \u03c0(da|x)(2\u03f5Query(x, a) + \u03b2\u03f5Model(x, a)). Proof. For a more compact presentation of the proof, let p = P(\u00b7|x, a), p\u0302 = P\u0302(\u00b7|x, a), and p\u0304 = P\u0304(\u00b7|x, a). Let \u03d5 : X \u2192 Rd and \u03c8 : X \u00d7A \u2192 Rd be d-dimensional vectors formed by \u03d5i, \u03c8i. For q \u2208M(X ) and f : X \u2192 Rd, we write\nq[f ] \u225c EX\u223cq[f(X)].\nWe write\n\u2225p[\u03d5]\u2212 p\u0304[\u03d5]\u22251 \u2264 \u221a d\u2225p[\u03d5]\u2212 p\u0304[\u03d5]\u22252\n\u2264 \u221a d ( \u2225p[\u03d5]\u2212\u03c8(x, a)\u22252 + \u2225\u03c8(x, a)\u2212 p\u0304[\u03d5]\u22252 ) \u2264 \u221a d ( \u03f5Query(x, a) + \u2225\u03c8(x, a)\u2212 p\u0304[\u03d5]\u22252 ) . (F.1)\nNow note that p\u0304 is the solution of (P2), the value of objective is smaller for p\u0304 than it is for p. We obtain\nDKL( p\u0304 \u2225 p\u0302 ) + 1\n\u03b22 \u2225p\u0304[\u03d5]\u2212\u03c8(x, a)\u222522 \u2264 DKL( p \u2225 p\u0302 ) +\n1\n\u03b22 \u2225p[\u03d5]\u2212\u03c8(x, a)\u222522\n\u2264 \u03f5Model(x, a)2 + 1\n\u03b22 \u03f5Query(x, a)\n2.\nThus,\n\u2225p\u0304[\u03d5]\u2212\u03c8(x, a)\u22252 \u2264 \u221a \u03b22\u03f5Model(x, a)2 + \u03f5Query(x, a)2 \u2212 \u03b22DKL( p\u0304 \u2225 p\u0302 )\n\u2264 \u221a \u03b22\u03f5Model(x, a)2 + \u03f5Query(x, a)2\n\u2264 \u03b2\u03f5Model(x, a) + \u03f5Query(x, a).\nSubstituting in (F.1) we get \u2225p[\u03d5]\u2212 p\u0304[\u03d5]\u22251 \u2264 \u221a d\u2225p[\u03d5]\u2212 p\u0304[\u03d5]\u22252\n\u2264 \u221a d ( \u2225p[\u03d5]\u2212\u03c8(x, a)\u22252 + \u2225\u03c8(x, a)\u2212 p\u0304[\u03d5]\u22252 ) \u2264 \u221a d ( 2\u03f5Query(x, a) + \u03b2\u03f5Model(x, a) ) .\nFor the second part we simply write d\u2211\ni=1 \u2223\u2223(P\u0304\u03c0\u03d5i)(x)\u2212 (P\u03c0\u03d5i)(x)\u2223\u2223 = d\u2211 i=1 \u2223\u2223\u2223\u2223\u222b \u03c0(da|x)[(P\u0304\u03d5i)(x, a)\u2212 (P\u03d5i)(x, a)]\u2223\u2223\u2223\u2223 \u2264\nd\u2211 i=1 \u222b \u03c0(da|x) \u2223\u2223(P\u0304\u03d5i)(x, a)\u2212 (P\u03d5i)(x, a)\u2223\u2223 =\n\u222b d\u2211 i=1 \u03c0(da|x) \u2223\u2223(P\u0304\u03d5i)(x, a)\u2212 (P\u03d5i)(x, a)\u2223\u2223\n\u2264 \u222b d\u2211\ni=1\n\u03c0(da|x) ( 2\u03f5Query(x, a) + \u03b2\u03f5Model(x, a) ) ,\nwhere we used the first part for the second inequality.\nLemma 11. If P\u0304 is the solution of the optimization problem (P2), for any policy \u03c0, w \u2208 Rd and v : X \u2192 R, we have\u2225\u2225\u2225G\u03c0P,P\u0304v\u2225\u2225\u2225\u221e \u2264 \u03b31\u2212 \u03b3(\u221a2\u2225\u03f5Model\u2225\u221e + 2\u03b2 \u2225\u03f5Query\u2225\u221e) \u2225\u2225\u2225\u2225\u2225v \u2212\u2211 i wi\u03d5i \u2225\u2225\u2225\u2225\u2225 \u221e\n+ \u03b3 \u221a d\n1\u2212 \u03b3\n( \u03b2\u2225\u03f5Model\u2225\u221e + 2\u2225\u03f5Query\u2225\u221e ) \u2225w\u2225\u221e\nProof. We have \u2225\u2225\u2225G\u03c0P,P\u0304v\u2225\u2225\u2225\u221e = \u2225\u2225(I\u2212 \u03b3P\u0304\u03c0)\u22121(\u03b3P\u03c0 \u2212 \u03b3P\u0304\u03c0)v\u2225\u2225\u221e (F.2) \u2264\n\u2225\u2225\u2225\u2225\u2225(I\u2212 \u03b3P\u0304\u03c0)\u22121(\u03b3P\u03c0 \u2212 \u03b3P\u0304\u03c0)(v \u2212\u2211 i wi\u03d5i) \u2225\u2225\u2225\u2225\u2225 \u221e\n+ (F.3)\u2225\u2225\u2225\u2225\u2225(I\u2212 \u03b3P\u0304\u03c0)\u22121(\u03b3P\u03c0 \u2212 \u03b3P\u0304\u03c0)(\u2211 i wi\u03d5i) \u2225\u2225\u2225\u2225\u2225 \u221e . (F.4)\nUsing (C.1) in proof of Lemma 2 and Lemma 9 we have\u2225\u2225P\u03c0(\u00b7|x)\u2212 P\u0304\u03c0(\u00b7|x)\u2225\u2225 1 \u2264 \u222b \u03c0(da|x) \u2225\u2225P(\u00b7|x, a)\u2212 P\u0304(\u00b7|x, a)\u2225\u2225 1\n\u2264 \u222b \u03c0(da|x) [\u221a 2\u03f5Model(x, a) + 2\n\u03b2 \u03f5Query(x, a) ] \u2264 \u221a 2\u2225\u03f5Model\u2225\u221e + 2\n\u03b2 \u2225\u03f5Query\u2225\u221e.\nThus, for the first term (F.3), we can write\u2225\u2225\u2225\u2225\u2225(I\u2212 \u03b3P\u0304\u03c0)\u22121(\u03b3P\u03c0 \u2212 \u03b3P\u0304\u03c0)(v \u2212\u2211 i wi\u03d5i) \u2225\u2225\u2225\u2225\u2225 \u221e\n= \u03b3\n1\u2212 \u03b3 sup x\n\u2225\u2225P\u03c0(\u00b7|x)\u2212 P\u0304\u03c0(\u00b7|x)\u2225\u2225 1 \u00b7 \u2225\u2225\u2225\u2225\u2225v \u2212\u2211 i wi\u03d5i \u2225\u2225\u2225\u2225\u2225 \u221e\n\u2264 \u03b3 1\u2212 \u03b3\n[\u221a 2\u2225\u03f5Model\u2225\u221e + 2\n\u03b2 \u2225\u03f5Query\u2225\u221e\n] \u00b7 \u2225\u2225\u2225\u2225\u2225v \u2212\u2211 i wi\u03d5i \u2225\u2225\u2225\u2225\u2225 \u221e .\nNow, for the second term (F.4), we can write\u2225\u2225\u2225\u2225\u2225(I\u2212 \u03b3P\u0304\u03c0)\u22121(\u03b3P\u03c0 \u2212 \u03b3P\u0304\u03c0)(\u2211 i wi\u03d5i) \u2225\u2225\u2225\u2225\u2225 \u221e\n= \u03b3\n1\u2212 \u03b3 \u2225\u2225\u2225\u2225\u2225(P\u03c0 \u2212 P\u0304\u03c0)(\u2211 i wi\u03d5i) \u2225\u2225\u2225\u2225\u2225 \u221e\n= \u03b3\n1\u2212 \u03b3 sup x \u2223\u2223\u2223\u2223\u2223\u2211 i wi[(P\u03c0\u03d5i)(x)\u2212 (P\u0304\u03c0\u03d5i)(x))] \u2223\u2223\u2223\u2223\u2223 = \u03b3\n1\u2212 \u03b3 sup x \u2211 i \u2223\u2223(P\u03c0\u03d5i)(x)\u2212 (P\u0304\u03c0\u03d5i)(x)\u2223\u2223\u2225w\u2225\u221e = \u03b3 \u221a d\n1\u2212 \u03b3\n[ 2\u2225\u03f5Query\u2225\u221e + \u03b2\u2225\u03f5Model\u2225\u221e ] \u2225w\u2225\u221e.\nPutting the bounds for (F.4) and (F.3) finishes the proof.\nTheorem 5. Define the mixed error values\ne1 = \u03b3 1\u2212 \u03b3 \u00b7 (\u221a 2\u2225\u03f5Model\u2225\u221e + 2 \u03b2 \u2225\u03f5Query\u2225\u221e ) , e2 = \u221a d \u00b7 \u03b3 1\u2212 \u03b3 \u00b7 ( \u03b2\u2225\u03f5Model\u2225\u221e + 2\u2225\u03f5Query\u2225\u221e ) .\nThen, for any wmax \u2265 0, we have\u2225\u2225V \u03c0PE \u2212 V\u0304 \u03c0PE\u2225\u2225\u221e \u2264 e1 inf\u2225w\u2225\u221e\u2264wmax \u00b7 \u2225\u2225\u2225V \u03c0PE \u2212 d\u2211\ni=1\nwi \u00b7 \u03d5i \u2225\u2225\u2225 \u221e + e2 \u00b7 wmax,\n\u2225\u2225\u2225V \u2217 \u2212 V \u03c0\u0304\u2217\u2225\u2225\u2225 \u221e \u2264 2e1 1\u2212 e1 inf \u2225w\u2225\u221e\u2264wmax \u00b7 \u2225\u2225\u2225V \u2217 \u2212 d\u2211\ni=1\nwi \u00b7 \u03d5i \u2225\u2225\u2225 \u221e + 2e2 1\u2212 e1 \u00b7 wmax.\nProof. The PE result is a direct consequence of Lemma 11 and (C.2). We have\n\u2225\u2225V \u03c0PE \u2212 V\u0304 \u03c0PE\u2225\u2225\u221e = \u2225\u2225\u2225G\u03c0PEP,P\u0304V \u03c0PE\u2225\u2225\u2225\u221e \u2264 inf\nw\u2208Rd\n[ e1 \u00b7 \u2225\u2225\u2225V \u03c0PE \u2212 d\u2211 i=1 wi \u00b7 \u03d5i \u2225\u2225\u2225 \u221e + e2 \u00b7 \u2225w\u2225\u221e ]\n\u2264 e1 \u00b7 inf \u2225w\u2225\u221e\u2264wmax\n\u00b7 \u2225\u2225\u2225V \u03c0PE \u2212 d\u2211\ni=1\nwi \u00b7 \u03d5i \u2225\u2225\u2225 \u221e + e2 \u00b7 wmax.\nFor control, from (C.3) we have\u2225\u2225\u2225V \u03c0\u2217 \u2212 V \u03c0\u0304\u2217\u2225\u2225\u2225 \u221e \u2264 \u2225\u2225\u2225G\u03c0\u2217P,P\u0302V \u2217\u2225\u2225\u2225\u221e + \u2225\u2225\u2225G\u03c0\u0304\u2217P,P\u0302V \u2217\u2225\u2225\u2225\u221e + \u2225\u2225\u2225G\u03c0\u0304\u2217P,P\u0302(V \u2217 \u2212 V \u03c0\u0304\u2217)\u2225\u2225\u2225\u221e. Choosing w = 0 in Lemma 11 we get\u2225\u2225\u2225G\u03c0\u0304\u2217P,P\u0302(V \u2217 \u2212 V \u03c0\u0304\u2217)\u2225\u2225\u2225\u221e \u2264 e1\u2225\u2225\u2225V \u2217 \u2212 V \u03c0\u0304\u2217\u2225\u2225\u2225\u221e Also for any w we get\u2225\u2225\u2225G\u03c0\u2217P,P\u0302V \u2217\u2225\u2225\u2225\u221e \u2264 e1 \u00b7 \u2225\u2225\u2225V \u2217 \u2212 d\u2211 i=1 wi \u00b7 \u03d5i \u2225\u2225\u2225 \u221e + e2 \u00b7 \u2225w\u2225\u221e\n\u2225\u2225\u2225G\u03c0\u0304\u2217P,P\u0302V \u2217\u2225\u2225\u2225\u221e \u2264 e1 \u00b7 \u2225\u2225\u2225V \u2217 \u2212 d\u2211\ni=1\nwi \u00b7 \u03d5i \u2225\u2225\u2225 \u221e + e2 \u00b7 \u2225w\u2225\u221e.\nThus,\n\u2225\u2225\u2225V \u03c0\u2217 \u2212 V \u03c0\u0304\u2217\u2225\u2225\u2225 \u221e \u2264 2e1 \u00b7 \u2225\u2225\u2225V \u2217 \u2212 d\u2211 i=1 wi \u00b7 \u03d5i \u2225\u2225\u2225 \u221e + 2e2 \u00b7 \u2225w\u2225\u221e + e1 \u2225\u2225\u2225V \u2217 \u2212 V \u03c0\u0304\u2217\u2225\u2225\u2225 \u221e .\nBy rearranging, we get\n\u2225\u2225\u2225V \u03c0\u2217 \u2212 V \u03c0\u0304\u2217\u2225\u2225\u2225 \u221e \u2264 inf w\u2208Rd [ 2e1 1\u2212 e1 \u00b7 \u2225\u2225\u2225V \u2217 \u2212 d\u2211\ni=1\nwi \u00b7 \u03d5i \u2225\u2225\u2225 \u221e + 2e2 1\u2212 e1 \u00b7 \u2225w\u2225\u221e\n]\n\u2264 2e1 1\u2212 e1 inf \u2225w\u2225\u221e\u2264wmax\n\u00b7 \u2225\u2225\u2225V \u2217 \u2212 d\u2211\ni=1\nwi \u00b7 \u03d5i \u2225\u2225\u2225 \u221e + 2e2 1\u2212 e1 \u00b7 wmax.\nProof of Theorem 1 It is the direct consequence of Theorem 5 with choosing \u03b2 = \u2225\u03f5Query\u2225\u221e/\u2225\u03f5Model\u2225\u221e and observing\ne1 = \u03b3(2 +\n\u221a 2)\n1\u2212 \u03b3 \u00b7 \u2225\u03f5Model\u2225\u221e \u2264 3c1\u2225\u03f5Model\u2225\u221e\ne2 = 3 \u221a d \u00b7 \u03b3\n1\u2212 \u03b3 \u00b7 \u2225\u03f5Query\u2225\u221e = c2\u2225\u03f5Query\u2225\u221e.\nG \u2113p ANALYSIS OF MAXENT MOCO\nThe analysis in the Section 3.2 is based on the supremum norm, which can be overly conservative. First, the error in the model and queries are due to the error in a supervised learning problem. Supervised learning algorithms usually provide guarantees in a weighted \u2113p norm rather than the supremum norm. Second, in the given results, the true value function V \u03c0PE and V \u2217 should be approximated with the span of functions \u03d5i according to the supremum norm. This is a strong condition. Usually, there are states in the MDP that are irrelevant to the problem or even unreachable. Finding a good approximation of the value function in such states is not realistic.\nHence, in this section we give performance analysis of our method in terms of a weighted \u2113p norm. We first define some necessary quantities before providing the results. For any function f : X \u2192 R and distribution \u03c1 \u2208M(X ), the norm \u2225f\u2225p,\u03c1 is defined as\n\u2225f\u2225p,\u03c1 \u225c [\u222b |f(x)|p\u03c1(dx) ]1/p .\nLet \u03c0 be an arbitrary policy, and P\u03c0m be the m-step transition kernel under \u03c0. The discounted future-state distribution \u03b7\u03c0 : X \u2192M(X ) is defined as\n\u03b7\u03c0(\u00b7|x) \u225c 1 1\u2212 \u03b3\n\u00b7 \u221e\u2211\nm=0\n\u03b3mP\u03c0m(\u00b7|x).\nDefine \u03c9\u03c0(\u00b7|x) \u225c \u222b \u03b7\u03c0(dz|x)P\u0302\u03c0(\u00b7|z). This is the distribution of our state when making one transition\naccording to P\u0302\u03c0 from an initial state sampled from the discounted future-state distribution \u03b7\u03c0(z|x). Also let \u03f5\u03c0Model : X \u2192 R and \u03f5\u03c0Query : X \u2192 R be defined based on \u03f5Model and \u03f5Query similar the way r\u03c0 is defined based on r. Assume for any i and x, a we have A\u2212B/2 \u2264 \u03d5i(x), \u03c8i(x, a) \u2264 A+B/2 for some values A and B \u2265 0. Let \u03c1 \u2208 M(X ) be some distribution over states. We define two concentration coefficients for \u03c1. Similar coefficients have appeared in \u2113p error propagation results in the literature (Kakade and\nLangford, 2002; Munos, 2003; 2007; Farahmand et al., 2010; Scherrer et al., 2015). Define\nC\u03c01 (\u03c1) 4 = exp\n( B2d\n\u03b22\n)2 \u222b \u03c1(x) \u2225\u2225\u2225\u2225d\u03b7\u03c0(\u00b7|x)d\u03c1 \u2225\u2225\u2225\u22252 \u221e \u2225\u2225\u2225\u2225d\u03c9\u03c0(\u00b7|x)d\u03c1 \u2225\u2225\u2225\u22252 \u221e\nC\u03c02 (\u03c1) 4 =\n1 \u03b3 \u00b7 \u222b \u03c1(x) \u2225\u2225\u2225\u2225d\u03b7\u03c0(\u00b7|x)d\u03c1 \u2225\u2225\u2225\u22254 \u221e\nHere, d\u03b7 \u03c0(\u00b7|x) d\u03c1 and d\u03b7\u03c0(\u00b7|x) d\u03c1 are the Radon-Nikodym derivatives of \u03b7\n\u03c0(\u00b7|x) and \u03b7\u03c0(\u00b7|x) with respect to \u03c1. In the C\u03c01 (\u03c1) defined above, the exponential term forces us to only focus on large values of \u03b2, which is not ideal. This term is appears as an upper bound for \u2225P\u0304\u03c0(\u00b7|x)/P\u0302\u03c0(\u00b7|x)\u2225\u221e. However, similar to more recent studies on approximate value iteration, it is possible to introduce coefficients that depend on the ratio of the expected values with respect to the two distribution instead of their densities. Due to the more involved nature of those definitions, we only include this simple form of results here and provide further discussion in the supplementary material. The next theorem shows the performance guarantees of our method in terms of weighted \u2113p norms. Theorem 6. Define\ne\u03c01 = 2\u03b3\n1\u2212 \u03b3 \u00b7 (C\u03c01 (\u03c1) + C\u03c02 (\u03c1)) \u00b7\n\u221a\u221a 2 \u00b7 \u2225\u03f5\u03c0Model\u22251,\u03c1 + 2 \u03b2 \u00b7 \u2225\u2225\u2225\u03f5\u03c0Query\u2225\u2225\u2225 1,\u03c1 ,\ne\u03c02 = 2\u03b3 \u221a d\n1\u2212 \u03b3 \u00b7 C2(\u03c1)\n( \u03b2\u2225\u03f5\u03c0Model\u22251,\u03c1 + 2 \u2225\u2225\u03f5\u03c0Query\u2225\u22251,\u03c1.) Then\u2225\u2225V \u03c0PE \u2212 V\u0304 \u03c0PE\u2225\u2225\u221e \u2264 2e\u03c0PE11\u2212 2e\u03c0PE1 \u00b7 inf\u2225w\u2225\u221e\u2264wmax \u00b7 \u2225\u2225\u2225\u2225\u2225V \u03c0PE \u2212 d\u2211\ni=1\nwi \u00b7 \u03d5i \u2225\u2225\u2225\u2225\u2225 4,\u03c1 + 2e\u03c0PE2 1\u2212 2e\u03c0PE1 \u00b7 wmax,\nalso if e\u22171 = max\u03c0\u2208{\u03c0\u2217,\u03c0\u0304\u2217} 6e \u03c0 1/(1\u2212 2e\u03c01 ) and e\u22172 = max\u03c0\u2208{\u03c0\u2217,\u03c0\u0304\u2217} 6e\u03c02/(1\u2212 2e\u03c01 ), we have\u2225\u2225\u2225V \u2217 \u2212 V \u03c0\u0304\u2217\u2225\u2225\u2225\n4,\u03c1 \u2264 2e\n\u2217 1\n1\u2212 e\u22171 inf \u2225w\u2225\u221e\u2264wmax \u00b7 \u2225\u2225\u2225\u2225\u2225V \u2217 \u2212 d\u2211\ni=1\nwi \u00b7 \u03d5i \u2225\u2225\u2225\u2225\u2225 4,\u03c1 + 2e\u22172 1\u2212 e\u22171 \u00b7 wmax.\nNotice that the \u03b2 appears in the bound in the same manner as Theorem 5. This will lead to the same dynamics on the choice of \u03b2. We provide the proof of this theorem in Section H.\nH PROOFS FOR \u2113p ANALYSIS OF MAXENT MOCO\nWe first show some useful lemmas towards the proof of Theorem 6. Lemma 12. For m functions f1, f2, . . . , fm : X \u2192 R, we have\n\u2225f1 + \u00b7 \u00b7 \u00b7+ fm\u222544,\u03c1 \u2264 m 3 m\u2211 i=1 \u2225fi\u222544,\u03c1.\nProof. We have \u2225f1 + \u00b7 \u00b7 \u00b7+ fm\u222544,\u03c1 = \u222b \u03c1(dx) (\u2211 i fi(x) )4\n\u2264 \u222b \u03c1(dx) [(\u2211 i 14/3 )3/4(\u2211 i fi(x) 4 )1/4]4\n= m3 \u222b \u03c1(dx) \u2211 i fi(x) 4\n= m3 m\u2211 i=1 \u2225fi\u222544,\u03c1\nLemma 13. For any policy \u03c0, we have G\u03c0P,P\u0304 = G \u03c0 P\u0304,PG \u03c0 P,P\u0304 \u2212G \u03c0 P\u0304,P .\nProof. We write\nG\u03c0P\u0304,PG \u03c0 P,P\u0304 \u2212G \u03c0 P\u0304,P = (I\u2212 \u03b3P\u03c0)\u22121 ( (\u03b3P\u0304\u03c0 \u2212 \u03b3P\u03c0)(I\u2212 \u03b3P\u0304\u03c0)\u22121 + I ) (\u03b3P\u03c0 \u2212 \u03b3P\u0304\u03c0)\n= (I\u2212 \u03b3P\u03c0)\u22121 ( (\u03b3P\u0304\u03c0 \u2212 \u03b3P\u03c0)(I\u2212 \u03b3P\u0304\u03c0)\u22121 + I ) (\u03b3P\u03c0 \u2212 \u03b3P\u0304\u03c0)\n= (I\u2212 \u03b3P\u03c0)\u22121 ( (\u03b3P\u0304\u03c0 \u2212 \u03b3P\u03c0)(I\u2212 \u03b3P\u0304\u03c0)\u22121 + (I\u2212 \u03b3P\u0304\u03c0)(I\u2212 \u03b3P\u0304\u03c0)\u22121 ) (\u03b3P\u03c0 \u2212 \u03b3P\u0304\u03c0)\n= (I\u2212 \u03b3P\u03c0)\u22121(I\u2212 \u03b3P\u03c0)(I\u2212 \u03b3P\u0304\u03c0)\u22121(\u03b3P\u03c0 \u2212 \u03b3P\u0304\u03c0) = (I\u2212 \u03b3P\u0304\u03c0)\u22121(\u03b3P\u03c0 \u2212 \u03b3P\u0304\u03c0) = G\u03c0P,P\u0304 .\nLemma 14. For any w \u2208 Rd we have\u2225\u2225\u2225(P\u03c0 \u2212 P\u0304\u03c0)(\u2211wi\u03d5i)\u2225\u2225\u2225 1,\u03c1 \u2264 \u221a d ( 2 \u2225\u2225\u03f5\u03c0Query\u2225\u22251,\u03c1 + \u03b2\u2225\u03f5\u03c0Model\u22251,\u03c1) \u00b7 \u2225w\u2225\u221e .\nProof. We write\u2225\u2225\u2225(P\u03c0 \u2212 P\u0304\u03c0)(\u2211wi\u03d5i)\u2225\u2225\u2225 1,\u03c1 = \u222b \u03c1(dx) \u2223\u2223\u2223\u2223\u2223\u2211 i wi ( (P\u03c0\u03d5i)(x)\u2212 (P\u0304\u03c0\u03d5i)(x) )\u2223\u2223\u2223\u2223\u2223 \u2264 \u2225w\u2225\u221e \u222b \u03c1(dx)\n\u2211 i \u2223\u2223\u2223((P\u03c0\u03d5i)(x)\u2212 (P\u0304\u03c0\u03d5i)(x))\u2223\u2223\u2223 \u2264 \u2225w\u2225\u221e \u222b \u03c1(dx) [\u221a d \u222b \u03c0(da|x) ( 2\u03f5Query(x, a) + \u03b2\u03f5Model(x, a)\n)] = \u2225w\u2225\u221e \u222b \u03c1(dx) [\u221a d ( 2\u03f5\u03c0Query(x) + \u03b2\u03f5 \u03c0 Model(x)\n)] = \u221a d ( 2 \u2225\u2225\u03f5\u03c0Query\u2225\u22251,\u03c1 + \u03b2\u2225\u03f5\u03c0Model\u22251,\u03c1) \u00b7 \u2225w\u2225\u221e,\nwhere we used Lemma 10.\nLemma 15. Define\nTV\u03c0\u03c1 (P, P\u0304) \u225c \u222b \u03c1(dx) \u2225\u2225P\u03c0(\u00b7|x)\u2212 P\u0304\u03c0(\u00b7|x)\u2225\u2225 1 .\nWe have TV\u03c0\u03c1 (P, P\u0304) \u2264 \u221a 2\u2225\u03f5\u03c0Model\u22251,\u03c1 + 2\n\u03b2 \u2225\u2225\u03f5\u03c0Query\u2225\u22251,\u03c1. Proof. Using (C.1) in proof of Lemma 2 and Lemma 9 we have\u222b\n\u03c1(dx) \u2225\u2225P\u03c0(\u00b7|x)\u2212 P\u0304\u03c0(\u00b7|x)\u2225\u2225 1 \u2264 \u222b \u03c1(dx) \u222b \u03c0(da|x) \u2225\u2225P(\u00b7|x, a)\u2212 P\u0304(\u00b7|x, a)\u2225\u2225 1\n\u2264 \u222b \u03c1(dx) \u222b \u03c0(da|x) [\u221a 2\u03f5Model(x, a) + 2\n\u03b2 \u03f5Query(x, a) ] = \u221a 2\u2225\u03f5\u03c0Model\u22251,\u03c1 + 2\n\u03b2\n\u2225\u2225\u03f5\u03c0Query\u2225\u22251,\u03c1.\nLemma 16. Assume for any i and x, a we have A\u2212B/2 \u2264 Ui(x), Yi(x, a) \u2264 A+B/2 for some values A and B \u2265 0. Then we have\u2225\u2225\u2225\u2225\u2225dP\u0304(\u00b7|x, a)dP\u0302(\u00b7|x, a) \u2225\u2225\u2225\u2225\u2225 \u221e \u2264 exp ( 2B2d \u03b22 ) .\nProof. Assume \u03bb is the dual problem of MaxEnt density estimation resulted in P\u0304(\u00b7|x, a). We have\ndP\u0304(\u00b7|x, a) dP\u0302(\u00b7|x, a) (y) = exp (\u2211 i \u03bbi\u03d5i(y)\u2212 \u039b\u03bb ) .\nWe have by Jensen\u2019s inequality\n\u039b\u03bb = log\n\u222b P\u0304(dy|x, a) exp (\u2211 i \u03bbi\u03d5i(y) )\n\u2265 \u222b P\u0304(dy|x, a) log ( exp (\u2211 i \u03bbi\u03d5i(y) ))\n= \u222b P\u0304(dy|x, a) (\u2211 i \u03bbi\u03d5i(y) ) = \u2211 i \u03bbi \u00b7 EY\u223cP\u0302(\u00b7|x,a)[\u03d5i(Y )].\nThus\ndP\u0304(\u00b7|x, a) dP\u0302(\u00b7|x, a) (y) = exp (\u2211 i \u03bbi\u03d5i(y)\u2212 \u039b\u03bb )\n\u2264 exp (\u2211 i \u03bbi(\u03d5i(y)\u2212 EY\u223cP\u0302(\u00b7|x,a)[\u03d5i(Y )]) ) \u2264 exp(B\u2225\u03bb\u22251).\nNow to bound \u2225\u03bb\u22251, note that for \u03c8\u2032(x, a) = EY\u223cP\u0302(\u00b7|x,a)[\u03d5i(Y )] the solution of (P2) is P\u0302(\u00b7|x, a) that corresponds to dual parameters \u03bb\u2032 = 0. Using to Lemma 5,\n\u2225\u03bb\u22252 = \u2225\u03bb\u2212 \u03bb \u2032\u22252 \u2264\n2\n\u03b22 \u2225\u03c8(x, a)\u2212 \u03c8\u2032(x, a)\u22252 \u2264\n2\n\u03b22\n\u221a dB.\nWe get\ndP\u0304(\u00b7|x, a) dP\u0302(\u00b7|x, a)\n(y) \u2264 exp(B\u2225\u03bb\u22251) \u2264 exp ( B \u221a d\u2225\u03bb\u22252 ) \u2264 exp ( 2B2d\n\u03b22\n) .\nLemma 17. Let e\u03c01 , e\u03c02 be defined as in Theorem 6. For any policy \u03c0, v : X \u2192 R and w \u2208 Rd we have \u2225\u2225\u2225G\u03c0P\u0304,Pv\u2225\u2225\u22254\n4,\u03c1 \u2264 (e\u03c01 )4 \u00b7 \u2225\u2225\u2225\u2225\u2225v \u2212\u2211 i wi\u03d5i \u2225\u2225\u2225\u2225\u2225 4\n4,\u03c1\n+ (e\u03c02 ) 4 \u00b7 \u2225w\u22254\u221e.\nProof. Define\nu \u225c v \u2212 \u2211 i wi\u03d5i\n\u03c9\u0304\u03c0(\u00b7|x) \u225c \u222b \u03b7\u03c0(dz|x)P\u0304\u03c0(\u00b7|z)\n\u2206P(dz|y) \u225c \u2223\u2223P\u03c0(dz|y)\u2212 P\u0304\u03c0(dz|y)\u2223\u2223\nTV\u03c1 \u225c \u222b \u03c1(dx) \u2225\u2225P\u03c0(\u00b7|x)\u2212 P\u0304\u03c0(\u00b7|x)\u2225\u2225 1\nD \u225c (I\u2212 \u03b3P\u03c0)\u22121(\u03b3P\u0304\u03c0 \u2212 \u03b3P\u03c0)(v \u2212 \u2211 i wi\u03d5i)\nE \u225c (I\u2212 \u03b3P\u03c0)\u22121(\u03b3P\u0304\u03c0 \u2212 \u03b3P\u03c0)( \u2211 i wi\u03d5i).\nWe have\nG\u03c0P\u0304,Pv = (I\u2212 \u03b3P \u03c0)\u22121(\u03b3P\u0304\u03c0 \u2212 \u03b3P\u03c0)v = (I\u2212 \u03b3P\u03c0)\u22121(\u03b3P\u0304\u03c0 \u2212 \u03b3P\u03c0)(v \u2212 \u2211 i wi\u03d5i) + (I\u2212 \u03b3P\u03c0)\u22121(\u03b3P\u0304\u03c0 \u2212 \u03b3P\u03c0)( \u2211 i wi\u03d5i)\n= D + E.\nWe bound norm of each term separately. For A, we write using Cauchy-Schwartz inequality\n\u2225D\u222544,\u03c1 = \u222b x \u03c1(dx) [\u222b\u222b y,z 1 1\u2212 \u03b3 \u03b7\u03c0(dy|x) \u00b7 \u03b3(P\u0304\u03c0(dz|y)\u2212 P\u03c0(dz|y)) \u00b7 u(z) ]4 \u2264 \u222b x \u03c1(dx) [\u222b\u222b y,z 1 1\u2212 \u03b3 \u03b7\u03c0(dy|x) \u00b7 \u03b3\u2206P(dz|y) \u00b7 |u(z)|\n]4 = \u03b34\n(1\u2212 \u03b3)4 \u222b x \u03c1(dx) [\u222b\u222b y,z (\u221a \u03c1(dy) \u00b7 \u221a \u2206P(dz|y) )(\u03b7\u03c0(dy|x) \u00b7 |u(z)| \u00b7\u221a\u2206P(dz|y)\u221a \u03c1(dy) )]4\n\u2264 \u03b3 4\n(1\u2212 \u03b3)4 \u222b x \u03c1(dx) (\u222b\u222b y,z \u03c1(dy) \u00b7\u2206P(dz|y) )2 \u00b7 (\u222b\u222b y,z \u03b7\u03c0(dy|x)2 \u00b7 u(z)2 \u00b7\u2206P(dz|y) \u03c1(dy) )2 = \u03b34\n(1\u2212 \u03b3)4 \u00b7 TV2\u03c1 \u00b7 \u222b x \u03c1(dx) (\u222b\u222b y,z \u03b7\u03c0(dy|x)2 \u00b7 u(z)2 \u00b7\u2206P(dz|y) \u03c1(dy) )2 = \u03b34\n(1\u2212 \u03b3)4 \u00b7 TV2\u03c1 \u00b7 \u222b x \u03c1(dx) [\u222b z (\u221a \u03c1(dz) \u00b7 u(z)2 ) \u00b7 (\u222b y \u03b7\u03c0(dy|x)2 \u00b7\u2206P(dz|y)\u221a \u03c1(dz) \u00b7 \u03c1(dy) )]2\n\u2264 \u03b3 4\n(1\u2212 \u03b3)4 \u00b7 TV2\u03c1 \u00b7 \u222b x \u03c1(dx) [\u222b z \u03c1(dz)u(z)4 ]\u222b z (\u222b y \u03b7\u03c0(dy|x)2 \u00b7\u2206P(dz|y)\u221a \u03c1(dz) \u00b7 \u03c1(dy) )2 = \u03b34\n(1\u2212 \u03b3)4 \u00b7 TV2\u03c1 \u00b7 \u2225u\u2225 4 4,\u03c1 \u00b7 \u222b\u222b x,z \u03c1(dx) (\u222b y \u03b7\u03c0(dy|x)2 \u00b7\u2206P(dz|y)\u221a \u03c1(dz) \u00b7 \u03c1(dy) )2 = \u03b34\n(1\u2212 \u03b3)4 \u00b7 TV2\u03c1 \u00b7 \u2225u\u2225 4 4,\u03c1 \u00b7 C.\nFor C we write\nC = \u222b\u222b x,z \u03c1(dx) (\u222b y \u03b7\u03c0(dy|x)2 \u00b7\u2206P(dz|y)\u221a \u03c1(dz) \u00b7 \u03c1(dy) )2\n= \u222b x \u03c1(dx) \u222b z \u03c1(dz) (\u222b y \u03b7\u03c0(dy|x)2\u2206P(dz|y) \u03c1(dy)\u03c1(dz) )2 =\n\u222b x \u03c1(dx) \u2225\u2225\u2225\u2225d\u03b7\u03c0(\u00b7|x)d\u03c1 \u2225\u2225\u2225\u22252 \u221e \u222b z \u03c1(dz) (\u222b y \u03b7\u03c0(dy|x)\u2206P(dz|y) \u03c1(dz) )2 =\n\u222b x \u03c1(dx) \u2225\u2225\u2225\u2225d\u03b7\u03c0(\u00b7|x)d\u03c1 \u2225\u2225\u2225\u22252 \u221e \u222b z \u03c1(dz) (\u222b y \u03b7\u03c0(dy|x)P\u03c0(dz|y) + \u222b y \u03b7\u03c0(dy|x)P\u0304\u03c0(dz|y) \u03c1(dz) )2\n= \u222b x \u03c1(dx) \u2225\u2225\u2225\u2225d\u03b7\u03c0(\u00b7|x)d\u03c1 \u2225\u2225\u2225\u22252 \u221e \u222b z \u03c1(dz) ( \u03b3\u22121\u03b7\u03c0(dz|x) \u03c1(dz) + \u03c9\u0304(dz|y) \u03c1(dz) )2 = 2\n\u222b x \u03c1(dx) \u2225\u2225\u2225\u2225d\u03b7\u03c0(\u00b7|x)d\u03c1 \u2225\u2225\u2225\u22252 \u221e \u222b z \u03c1(dz) [( \u03b3\u22121\u03b7\u03c0(dz|x) \u03c1(dz) )2 + ( \u03c9\u0304(dz|y) \u03c1(dz) )2]\n= 2 \u222b x \u03c1(dx) \u2225\u2225\u2225\u2225d\u03b7\u03c0(\u00b7|x)d\u03c1 \u2225\u2225\u2225\u22252 \u221e \u222b z \u03c1(dz) [ \u03b3\u22121 \u2225\u2225\u2225\u2225d\u03b7\u03c0(\u00b7|x)d\u03c1 \u2225\u2225\u2225\u22252 \u221e + \u2225\u2225\u2225\u2225d\u03c9\u0304(\u00b7|y)d\u03c1 \u2225\u2225\u2225\u22252 \u221e ]\n= 2\u03b3\u22121 \u222b x \u03c1(dx) \u2225\u2225\u2225\u2225d\u03b7\u03c0(\u00b7|x)d\u03c1 \u2225\u2225\u2225\u22254 \u221e + 2 \u222b x \u03c1(dx) \u2225\u2225\u2225\u2225d\u03b7\u03c0(\u00b7|x)d\u03c1 \u2225\u2225\u2225\u22252 \u221e \u2225\u2225\u2225\u2225d\u03c9\u0304(\u00b7|y)d\u03c1 \u2225\u2225\u2225\u22252 \u221e\n\u2264 2\u03b3\u22121 \u222b x \u03c1(dx) \u2225\u2225\u2225\u2225d\u03b7\u03c0(\u00b7|x)d\u03c1 \u2225\u2225\u2225\u22254 \u221e + 2 exp ( 4B2d \u03b22 )\u222b x \u03c1(dx) \u2225\u2225\u2225\u2225d\u03b7\u03c0(\u00b7|x)d\u03c1 \u2225\u2225\u2225\u22252 \u221e \u2225\u2225\u2225\u2225d\u03c9(\u00b7|y)d\u03c1 \u2225\u2225\u2225\u22252 \u221e = 2C\u03c02 (\u03c1) 4 + 2C\u03c01 (\u03c1) 4,\nwhere we used Lemma 16. Also from Lemma 15 we have\nTV\u03c1 \u2264 \u221a 2\u2225\u03f5\u03c0Model\u22251,\u03c1 + 2\n\u03b2 \u2225\u2225\u03f5\u03c0Query\u2225\u22251,\u03c1 This means\n\u2225D\u222544,\u03c1 \u2264 2\u03b34\n(1\u2212 \u03b3)4 \u00b7 (C\u03c02 (\u03c1)4 + C\u03c01 (\u03c1)4) \u00b7\n(\u221a 2\u2225\u03f5\u03c0Model\u22251,\u03c1 + 2\n\u03b2 \u2225\u2225\u03f5\u03c0Query\u2225\u22251,\u03c1)2 \u00b7 \u2225u\u222544,\u03c1 \u2264 2\u03b3 4\n(1\u2212 \u03b3)4 \u00b7 (C\u03c02 (\u03c1) + C\u03c01 (\u03c1))4 \u00b7\n(\u221a 2\u2225\u03f5\u03c0Model\u22251,\u03c1 + 2\n\u03b2 \u2225\u2225\u03f5\u03c0Query\u2225\u22251,\u03c1)2 \u00b7 \u2225u\u222544,\u03c1. Now we bound the E term. Define\nf(x) \u225c \u2223\u2223\u2223EY\u223cP\u03c0(\u00b7|x)[\u2211wi\u03d5i(Y )]\u2212 EY\u223cP\u0304\u03c0(\u00b7|x)[\u2211wi\u03d5i(Y )]\u2223\u2223\u2223.\nUsing Lemma 14, we have \u2225f\u22251,\u03c1 \u2264 \u221a d ( 2 \u2225\u2225\u03f5\u03c0Query\u2225\u22251,\u03c1 + \u03b2\u2225\u03f5\u03c0Model\u22251,\u03c1) \u00b7 \u2225w\u2225\u221e. (H.1)\nWe have\n\u2225E\u222544,\u03c1 \u2264 \u03b34\n(1\u2212 \u03b3)4 \u222b x \u03c1(dx) (\u222b y \u03b7\u03c0(dy|x)f(y) )4\n\u2264 \u03b3 4\n(1\u2212 \u03b3)4 \u222b x \u03c1(dx) \u2225\u2225\u2225\u2225d\u03b7\u03c0(\u00b7|x)d\u03c1 \u2225\u2225\u2225\u22254 \u221e (\u222b y \u03c1(dy|x)f(y) )4\n\u2264 \u03b3 4\n(1\u2212 \u03b3)4 \u2225f\u222541,\u03c1 \u222b x \u03c1(dx) \u2225\u2225\u2225\u2225d\u03b7\u03c0(\u00b7|x)d\u03c1 \u2225\u2225\u2225\u22254 \u221e\n= \u03b34\n(1\u2212 \u03b3)4 \u00b7 \u03b3C\u03c02 (\u03c1)4 \u00b7 d2 ( 2 \u2225\u2225\u03f5\u03c0Query\u2225\u22251,\u03c1 + \u03b2\u2225\u03f5\u03c0Model\u22251,\u03c1)4 \u00b7 \u2225w\u22254\u221e.\nPutting things together using Lemma 12:\u2225\u2225\u2225G\u03c0P\u0304,Pv\u2225\u2225\u22254 4,\u03c1 = \u2225D + E\u222544,\u03c1\n\u2264 8\u2225D\u222544,\u03c1 + 8\u2225E\u2225 4 4,\u03c1\n\u2264 16\u03b3 4\n(1\u2212 \u03b3)4 \u00b7 (C\u03c02 (\u03c1) + C\u03c01 (\u03c1))4 \u00b7\n(\u221a 2\u2225\u03f5\u03c0Model\u22251,\u03c1 + 2\n\u03b2 \u2225\u2225\u03f5\u03c0Query\u2225\u22251,\u03c1)2 \u00b7 \u2225u\u222544,\u03c1 + 8\u03b34\n(1\u2212 \u03b3)4 \u00b7 \u03b3C\u03c02 (\u03c1)4 \u00b7 d2 ( 2 \u2225\u2225\u03f5\u03c0Query\u2225\u22251,\u03c1 + \u03b2\u2225\u03f5\u03c0Model\u22251,\u03c1)4 \u00b7 \u2225w\u22254\u221e\n\u2264 (e\u03c01 )4 \u00b7 \u2225u\u2225 4 4,\u03c1 + (e \u03c0 2 ) 4 \u00b7 \u2225w\u22254\u221e."
        },
        {
            "heading": "Proof of Theorem 6 for PE",
            "text": "Proof. We have\u2225\u2225V \u03c0PE \u2212 V\u0304 \u03c0PE\u2225\u2225 4,\u03c1 = \u2225\u2225\u2225G\u03c0PEP,P\u0304V \u03c0PE\u2225\u2225\u22254,\u03c1\n= \u2225\u2225\u2225G\u03c0PEP\u0304,PG\u03c0PEP,P\u0304V \u03c0PE \u2212G\u03c0PEP\u0304,PV \u03c0PE\u2225\u2225\u22254,\u03c1\n\u2264 23/4 \u2225\u2225\u2225G\u03c0PEP\u0304,PG\u03c0PEP,P\u0304V \u03c0PE\u2225\u2225\u22254,\u03c1 + 23/4\u2225\u2225\u2225G\u03c0PEP\u0304,PV \u03c0PE\u2225\u2225\u22254,\u03c1\n\u2264 2 \u2225\u2225\u2225G\u03c0PEP\u0304,PG\u03c0PEP,P\u0304V \u03c0PE\u2225\u2225\u22254,\u03c1 + 2\u2225\u2225\u2225G\u03c0PEP\u0304,PV \u03c0PE\u2225\u2225\u22254,\u03c1.\nUsing Lemma 17 with w = 0 we have\u2225\u2225\u2225G\u03c0PEP\u0304,PG\u03c0PEP,P\u0304V \u03c0PE\u2225\u2225\u22254,\u03c1 \u2264 e\u03c0PE1 \u2225\u2225\u2225G\u03c0PEP,P\u0304V \u03c0PE\u2225\u2225\u22254,\u03c1 \u2264 e\u03c0PE1 \u2225\u2225V \u03c0PE \u2212 V\u0304 \u03c0PE\u2225\u2225 4,\u03c1 . Also from Lemma 17 we have\u2225\u2225\u2225G\u03c0PEP\u0304,PV \u03c0PE\u2225\u2225\u22254,\u03c1 \u2264 ( (e\u03c0PE1 ) 4 \u2225\u2225\u2225V \u03c0PE \u2212\u2211wi\u03d5i\u2225\u2225\u22254 4,\u03c1 + (e\u03c0PE2 ) 4\u2225w\u22254\u221e\n)1/4 \u2264 e\u03c0PE1 \u2225\u2225\u2225V \u03c0PE \u2212\u2211wi\u03d5i\u2225\u2225\u2225 4,\u03c1 + e\u03c0PE2 \u2225w\u2225\u221e\nwith substitution we get\u2225\u2225V \u03c0PE \u2212 V\u0304 \u03c0PE\u2225\u2225 4,\u03c1 \u2264 2e\u03c0PE1 \u2225\u2225V \u03c0PE \u2212 V\u0304 \u03c0PE\u2225\u2225 4,\u03c1 + 2e\u03c0PE1 \u2225\u2225\u2225V \u03c0PE \u2212\u2211wi\u03d5i\u2225\u2225\u2225 4,\u03c1 + 2e\u03c0PE2 \u2225w\u2225\u221e.\nRearranging the terms give the result."
        },
        {
            "heading": "Proof of Theorem 6 for Control",
            "text": "Proof. From proof of (C.3), we get\u2225\u2225\u2225V \u03c0\u2217 \u2212 V \u03c0\u0304\u2217\u2225\u2225\u2225 4,\u03c1 \u2264 \u2225\u2225\u2225 \u2223\u2223\u2223G\u03c0\u2217P,P\u0302V \u2217\u2223\u2223\u2223+ \u2223\u2223\u2223G\u03c0\u0304\u2217P,P\u0302V \u2217\u2223\u2223\u2223+ \u2223\u2223\u2223G\u03c0\u0304\u2217P,P\u0302(V \u2217 \u2212 V \u03c0\u0304\u2217)\u2223\u2223\u2223 \u2225\u2225\u22254,\u03c1\n\u2264 3 \u2225\u2225\u2225G\u03c0\u2217P,P\u0302V \u2217\u2225\u2225\u22254,\u03c1 + 3\u2225\u2225\u2225G\u03c0\u0304\u2217P,P\u0302V \u2217\u2225\u2225\u22254,\u03c1 + 3\u2225\u2225\u2225G\u03c0\u0304\u2217P,P\u0302(V \u2217 \u2212 V \u03c0\u0304\u2217)\u2225\u2225\u22254,\u03c1.\nFrom Lemma 17 with w = 0 3 \u2225\u2225\u2225G\u03c0\u0304\u2217P,P\u0302(V \u2217 \u2212 V \u03c0\u0304\u2217)\u2225\u2225\u22254,\u03c1 \u2264 e\u22171\u2225\u2225\u2225V \u2217 \u2212 V \u03c0\u0304\u2217\u2225\u2225\u22254,\u03c1.\nAlso for any w\n3 \u2225\u2225\u2225G\u03c0\u2217P,P\u0302V \u2217\u2225\u2225\u22254,\u03c1 \u2264 e\u22171\u2225\u2225\u2225V \u2217 \u2212\u2211wi\u03d5i\u2225\u2225\u22254,\u03c1 + e\u22172\u2225w\u2225\u221e 3 \u2225\u2225\u2225G\u03c0\u0304\u2217P,P\u0302V \u2217\u2225\u2225\u22254,\u03c1 \u2264 e\u22171\u2225\u2225\u2225V \u2217 \u2212\u2211wi\u03d5i\u2225\u2225\u22254,\u03c1 + e\u22172\u2225w\u2225\u221e.\nThus, \u2225\u2225\u2225V \u03c0\u2217 \u2212 V \u03c0\u0304\u2217\u2225\u2225\u2225 4,\u03c1 \u2264 2e\u22171 \u2225\u2225\u2225V \u2217 \u2212\u2211wi\u03d5i\u2225\u2225\u2225 4,\u03c1 + 2e\u22172\u2225w\u2225\u221e + e \u2217 1 \u2225\u2225\u2225V \u2217 \u2212 V \u03c0\u0304\u2217\u2225\u2225\u2225 4,\u03c1 .\nRearranging proves the result."
        },
        {
            "heading": "I PROOFS FOR SECTION 4",
            "text": "Here, we give the proof of Theorem 2 after the following lemma. Lemma 18. If Vk is the value function at iteration k of MoCoVI for control. Let \u03b2, \u03f5\u221eQuery be defined as in Theorem 2. We have\n\u2225Vk \u2212 V \u2217\u2225\u221e \u2264 3c1\u2225\u03f5Model\u2225\u221e inf\u2225w\u2225\u221e\u2264wmax \u2225\u2225\u2225V \u2217 \u2212\u2211wi\u03d5i\u2225\u2225\u2225 \u221e + c2 \u2225\u2225\u03f5\u221eQuery\u2225\u2225\u221ewmax.\nProof. Let P\u0304k be the corrected transition dynamics used to obtain Vk. Let rk = r+ (\u03b3P \u2212 \u03b3P\u0304k)V \u2217. According to Rakhsha et al. (2022), V \u2217 = V \u2217(rk, P\u0304k) = V \u03c0 \u2217 (rk, P\u0304k). Now we have\nV \u2217 \u2212 Vk = V \u2217(rk, P\u0304k)\u2212 V \u03c0k(r, P\u0304k) \u227d V \u03c0k(rk, P\u0304k)\u2212 V \u03c0k(r, P\u0304k) = (I\u2212 \u03b3P\u0304\u03c0k)\u22121(r\u03c0kk \u2212 r \u03c0k)\n= (I\u2212 \u03b3P\u0304\u03c0k)\u22121(\u03b3P\u03c0k \u2212 \u03b3P\u0304\u03c0k)V \u2217 = G\u03c0kP,P\u0304k V \u2217.\nOn the other hand\nV \u2217 \u2212 Vk = V \u03c0 \u2217 (rk, P\u0304k)\u2212 V \u2217(r, P\u0304k)\n\u227c V \u03c0 \u2217 (rk, P\u0304k)\u2212 V \u03c0 \u2217 (r, P\u0304k) = (I\u2212 \u03b3P\u0304\u03c0 \u2217 )\u22121(r\u03c0 \u2217 k \u2212 r\u03c0 \u2217 ) = (I\u2212 \u03b3P\u0304\u03c0 \u2217 )\u22121(\u03b3P\u03c0 \u2217 \u2212 \u03b3P\u0304\u03c0 \u2217 )V \u2217\n= G\u03c0 \u2217 P,P\u0304kV \u2217.\nThus, \u2225V \u2217 \u2212 Vk\u2225\u221e \u2264 max (\u2225\u2225\u2225G\u03c0kP,P\u0304kV \u2217\u2225\u2225\u2225\u221e,\u2225\u2225\u2225G\u03c0\u2217P,P\u0304kV \u2217\u2225\u2225\u2225\u221e)\n\u2264 3c1\u2225\u03f5Model\u2225\u221e inf\u2225w\u2225\u221e\u2264wmax \u2225\u2225\u2225V \u2217 \u2212\u2211wi\u03d5k+i\u2225\u2225\u2225 \u221e + c2 \u2225\u2225\u03f5\u221eQuery\u2225\u2225\u221ewmax,\nwhere the last inequality is from Theorem 1."
        },
        {
            "heading": "Proof of Theorem 2",
            "text": ""
        },
        {
            "heading": "Proof.",
            "text": "For PE, we note that from Theorem 1 we have for any K \u2264 k \u2265 1\n\u2225V \u03c0PE \u2212 Vk\u2225\u221e \u2264 3c1\u2225\u03f5Model\u2225\u221e inf\u2225w\u2225\u221e\u2264wmax \u2225\u2225\u2225V \u03c0PE \u2212\u2211wi\u03d5k+i\u2225\u2225\u2225 \u221e + c2 \u2225\u2225\u03f5\u221eQuery\u2225\u2225\u221ewmax\n\u2264 \u03b3\u2032\u2225V \u03c0PE \u2212 Vk\u22121\u2225\u221e + c2 \u2225\u2225\u03f5\u221eQuery\u2225\u2225\u221ewmax.\nBy induction, we get\n\u2225V \u03c0PE \u2212 VK\u2225\u221e \u2264 \u03b3 \u2032K\u2225V \u03c0PE \u2212 V0\u2225\u221e +\n1\u2212 \u03b3\u2032K 1\u2212 \u03b3\u2032 c2 \u2225\u2225\u03f5\u221eQuery\u2225\u2225\u221ewmax.\nFor control, note that according to Lemma 18, for 1 \u2264 k \u2264 K\n\u2225V \u2217 \u2212 Vk\u2225\u221e \u2264 3c1\u2225\u03f5Model\u2225\u221e inf\u2225w\u2225\u221e\u2264wmax \u2225\u2225\u2225V \u2217 \u2212\u2211wi\u03d5k+i\u2225\u2225\u2225 \u221e + c2 \u2225\u2225\u03f5\u221eQuery\u2225\u2225\u221ewmax\n\u2264 \u03b3\u2032\u2225V \u2217 \u2212 Vk\u22121\u2225\u221e + c2 \u2225\u2225\u03f5\u221eQuery\u2225\u2225\u221ewmax.\nConsequently\n\u2225V \u2217 \u2212 VK\u22121\u2225\u221e \u2264 \u03b3 \u2032K\u22121\u2225V \u2217 \u2212 V0\u2225\u221e +\n1\u2212 \u03b3\u2032K\u22121 1\u2212 \u03b3\u2032 c2 \u2225\u2225\u03f5\u221eQuery\u2225\u2225\u221ewmax.\nFinally, based on Theorem 1,\n\u2225V \u2217 \u2212 V \u03c0K\u2225\u221e \u2264 6c1\u2225\u03f5Model\u2225\u221e\n1\u2212 3c1\u2225\u03f5Model\u2225\u221e inf \u2225w\u2225\u221e\u2264wmax \u2225\u2225\u2225V \u2217 \u2212 d\u2211 i=1 wi\u03d5i+K \u2225\u2225\u2225 \u221e\n+ 2c2\u2225\u03f5Query\u2225\u221e\n1\u2212 3c1\u2225\u03f5Model\u2225\u221e \u00b7 wmax\n\u2264 2 1\u2212 3c1\u2225\u03f5Model\u2225\u221e\n\u03b3\u2032 \u2225\u2225\u2225V \u2217 \u2212 VK\u22121\u2225\u2225\u2225\n\u221e + 2c2\u2225\u03f5Query\u2225\u221e 1\u2212 3c1\u2225\u03f5Model\u2225\u221e \u00b7 wmax\n\u2264 2\u03b3 \u2032K\n1\u2212 3c1\u2225\u03f5Model\u2225\u221e\n\u2225\u2225\u2225V \u2217 \u2212 V0\u2225\u2225\u2225 \u221e + 1\u2212 \u03b3\u2032K 1\u2212 \u03b3\u2032 2c2\u2225\u03f5Query\u2225\u221e 1\u2212 3c1\u2225\u03f5Model\u2225\u221e wmax."
        },
        {
            "heading": "J ADDITIONAL EMPIRICAL DETAILS",
            "text": "We perform our experiments on a 6\u00d7 6 gridworld environment introduced by Rakhsha et al. (2022). The environment is shown in Figure 2. There are 4 actions in the environment: (UP, RIGHT, DOWN, LEFT). When an action is taken, the agent moves towards that direction with probability 0.9. With probability of 0.1 it moves towards another direction at random. If the agent attempts to exit the environment, it stays in place. The middle 4 states of the first, third, and fifth row are cliffs. If the agent falls into a cliff, it stays there permanently and receives reward of \u221232, \u221216, \u22128 every iterations for the first, third, and fifth row cliffs, respectively. The top-right corner is the goal state, which awards reward of 20 once reached. We consider this environment with \u03b3 = 0.9.\nFor MoCoVI, we set the initial basis functions \u03d5i for i = 1, \u00b7 \u00b7 \u00b7 , d constant zero functions. We can set \u03c8i = 0 for i = 1, \u00b7 \u00b7 \u00b7 , d without querying P . This makes the comparison of algorithms fair as MoCoVI is not given extra queries before the first iteration. The convergence of MoCoVI with exact queries and \u03b2 = 0 is shown in Figures 1 and 3 for the control and PE problems.\nFigures 1 and 4 show the performance of MoCoDyna compared to other algorithms in the PE and control problems. As discussed after Theorem 1, it is beneficial to choose basis functions such that the true value function can be approximated with \u2211 i wi\u03d5i for some small weights wi. To achieve this in our implementation, we initialize \u03d51:d+c with an orthonormal set of functions. Also, in line 9 of Algorithm 1, we maintain this property of basis functions by subtracting the projection of the new value function Vt onto the span of the previous d\u2212 1 functions before adding it to the basis functions. We have\n\u03d5d+c \u2190 Vt \u2212 d+c\u22121\u2211 i=c+1 \u27e8\u03d5i, Vt\u27e9 \u00b7 \u03d5i, (J.1)\nand then we normalize \u03d5d+c to have a fixed euclidean norm. The hyperparameters of MoCoDyna for PE and control problems are given in Tables 3 and 4.\nModel Error Reduction. To show that the model correction procedure in MoCoDyna improves the accuracy of the model, we plot the error of original and corrected dynamics in the control problem in Figure 5. The model error is measured by taking the average of \u2225P(\u00b7|x, a) \u2212 P\u0302(\u00b7|x, a)\u22251 or \u2225P(\u00b7|x, a) \u2212 P\u0304(\u00b7|x, a)\u22251 over all x, a. We observe that higher order correction better reduces the error.\nComputation Cost. In Table 1 we provide the average time the calculation of P\u0304 has taken in MoCoDyna in the control problem. This is total time to calculate P\u0304(\u00b7|x, a) for all 144 state-action\npairs in the environment. In our implementation, the dual variables of the optimization problem for all state-action pairs are optimized with a single instance of the BFGS algorithm in SciPy library. Note that in general, different instances of the optimization problem (P2) for a batch of state-action pairs can be solved in parallel to reduce the computation time. Table 2 shows the full run time of the algorithms. It is important to note that in Algorithm 1, apart from reporting the current policy for the purpose of evaluation in line 6, MoCoDyna only needs to plan with P\u0304 every K steps to have Vt in line 9. In our implementation, planning is done every 2000 steps to evaluate the algorithm. Performing the planning only when needed in line 9 would make the algorithm computationally faster."
        }
    ],
    "title": "MAXIMUM ENTROPY MODEL CORRECTION",
    "year": 2023
}