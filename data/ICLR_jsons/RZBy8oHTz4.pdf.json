{
    "abstractText": "Contrastive learning has emerged as a popular paradigm of self-supervised learning that learns representations by encouraging representations of positive pairs to be similar while representations of negative pairs to be far apart. The spectral contrastive loss, in synergy with the notion of positive-pair graphs, offers valuable theoretical insights into the empirical successes of contrastive learning. In this paper, we propose incorporating an additive factor into the term of spectral contrastive loss involving negative pairs. This simple modification can be equivalently viewed as introducing a regularization term that enforces the mean of representations to be zero, which thus is referred to as zero-mean regularization. It intuitively relaxes the orthogonality of representations between negative pairs and implicitly alleviates the adverse effect of wrong connections in the positive-pair graph, leading to better performance and robustness. To clarify this, we thoroughly investigate the role of zero-mean regularized spectral contrastive loss in both unsupervised and supervised scenarios with respect to theoretical analysis and quantitative evaluation. These results highlight the potential of zero-mean regularized spectral contrastive learning to be a promising approach in various tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "NECTIONS IN"
        },
        {
            "affiliations": [],
            "name": "POSITIVE-PAIR GRAPHS"
        },
        {
            "affiliations": [],
            "name": "Xiong Zhou"
        },
        {
            "affiliations": [],
            "name": "Xianming Liu"
        },
        {
            "affiliations": [],
            "name": "Feilong Zhang"
        },
        {
            "affiliations": [],
            "name": "Gang Wu"
        },
        {
            "affiliations": [],
            "name": "Deming Zhai"
        },
        {
            "affiliations": [],
            "name": "Junjun Jiang"
        },
        {
            "affiliations": [],
            "name": "Xiangyang Ji"
        }
    ],
    "id": "SP:06aaeda2ca04c1d5ed50419892ea75fe0a22c994",
    "references": [
        {
            "authors": [
                "Randall Balestriero",
                "Yann LeCun"
            ],
            "title": "Contrastive and non-contrastive self-supervised learning recover global and local spectral embedding methods",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Adrien Bardes",
                "Jean Ponce",
                "Yann"
            ],
            "title": "Lecun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Mikhail Belkin",
                "Partha Niyogi"
            ],
            "title": "Laplacian eigenmaps and spectral techniques for embedding and clustering",
            "venue": "In Neural Information Processing Systems,",
            "year": 2001
        },
        {
            "authors": [
                "Shai Ben-David",
                "John Blitzer",
                "Koby Crammer",
                "Alex Kulesza",
                "Fernando Pereira",
                "Jennifer Wortman Vaughan"
            ],
            "title": "A theory of learning from different domains",
            "venue": "Machine learning,",
            "year": 2010
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Kaiming He"
            ],
            "title": "Exploring simple siamese representation learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Xinlei Chen",
                "Haoqi Fan",
                "Ross Girshick",
                "Kaiming He"
            ],
            "title": "Improved baselines with momentum contrastive learning",
            "venue": "arXiv preprint arXiv:2003.04297,",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Saining Xie",
                "Kaiming He"
            ],
            "title": "An empirical study of training self-supervised vision transformers",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Ching-Yao Chuang",
                "R Devon Hjelm",
                "Xin Wang",
                "Vibhav Vineet",
                "Neel Joshi",
                "Antonio Torralba",
                "Stefanie Jegelka",
                "Yale Song"
            ],
            "title": "Robust contrastive learning against noisy views",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Jiankang Deng",
                "Jia Guo",
                "Niannan Xue",
                "Stefanos Zafeiriou"
            ],
            "title": "Arcface: Additive angular margin loss for deep face recognition",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Carl Eckart",
                "Gale Young"
            ],
            "title": "The approximation of one matrix by another of lower rank",
            "year": 1936
        },
        {
            "authors": [
                "Yaroslav Ganin",
                "Evgeniya Ustinova",
                "Hana Ajakan",
                "Pascal Germain",
                "Hugo Larochelle",
                "Fran\u00e7ois Laviolette",
                "Mario Marchand",
                "Victor Lempitsky"
            ],
            "title": "Domain-adversarial training of neural networks. The journal of machine learning",
            "year": 2030
        },
        {
            "authors": [
                "Aritra Ghosh",
                "Himanshu Kumar",
                "P Shanti Sastry"
            ],
            "title": "Robust loss functions under label noise for deep neural networks",
            "venue": "Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Guo",
                "Mohammad Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "XY Han",
                "Vardan Papyan",
                "David L Donoho"
            ],
            "title": "Neural collapse under mse loss: Proximity to and dynamics on the central path",
            "venue": "In International Conference on Learning Representations,",
            "year": 2024
        },
        {
            "authors": [
                "Jeff Z HaoChen",
                "Tengyu Ma"
            ],
            "title": "A theoretical study of inductive biases in contrastive learning",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Jeff Z HaoChen",
                "Colin Wei",
                "Adrien Gaidon",
                "Tengyu Ma"
            ],
            "title": "Provable guarantees for self-supervised deep learning with spectral contrastive loss",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jeff Z HaoChen",
                "Colin Wei",
                "Ananya Kumar",
                "Tengyu Ma"
            ],
            "title": "Beyond separability: Analyzing the linear transferability of contrastive representations to related subpopulations",
            "venue": "arXiv preprint arXiv:2204.02683,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Identity mappings in deep residual networks",
            "venue": "In Computer Vision\u2013ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Paul W Holland",
                "Kathryn Blackmond Laskey",
                "Samuel Leinhardt"
            ],
            "title": "Stochastic blockmodels: First steps",
            "venue": "Social networks,",
            "year": 1983
        },
        {
            "authors": [
                "Jonathan J. Hull"
            ],
            "title": "A database for handwritten text recognition research",
            "venue": "IEEE Transactions on pattern analysis and machine intelligence,",
            "year": 1994
        },
        {
            "authors": [
                "Prannay Khosla",
                "Piotr Teterwak",
                "Chen Wang",
                "Aaron Sarna",
                "Yonglong Tian",
                "Phillip Isola",
                "Aaron Maschinot",
                "Ce Liu",
                "Dilip Krishnan"
            ],
            "title": "Supervised contrastive learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images. pages 32\u201333",
            "year": 2009
        },
        {
            "authors": [
                "Yann LeCun",
                "L\u00e9on Bottou",
                "Yoshua Bengio",
                "Patrick Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE,",
            "year": 1998
        },
        {
            "authors": [
                "Jason D Lee",
                "Qi Lei",
                "Nikunj Saunshi",
                "JIACHENG ZHUO"
            ],
            "title": "Predicting what you already know helps: Provable self-supervised learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jason D Lee",
                "Qi Lei",
                "Nikunj Saunshi",
                "Jiacheng Zhuo"
            ],
            "title": "Predicting what you already know helps: Provable self-supervised learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Bin Liu",
                "Yue Cao",
                "Yutong Lin",
                "Qi Li",
                "Zheng Zhang",
                "Mingsheng Long",
                "Han Hu"
            ],
            "title": "Negative margin matters: Understanding margin in few-shot classification",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Weiyang Liu",
                "Yandong Wen",
                "Zhiding Yu",
                "Ming Li",
                "Bhiksha Raj",
                "Le Song"
            ],
            "title": "Sphereface: Deep hypersphere embedding for face recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "SGDR: Stochastic gradient descent with warm restarts",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Dustin G Mixon",
                "Hans Parshall",
                "Jianzong Pi"
            ],
            "title": "Neural collapse with unconstrained features",
            "venue": "Sampling Theory, Signal Processing, and Data Analysis,",
            "year": 2022
        },
        {
            "authors": [
                "Yuval Netzer",
                "Tao Wang",
                "Adam Coates",
                "Alessandro Bissacco",
                "Andrew Y Ng"
            ],
            "title": "Reading digits in natural images with unsupervised feature learning",
            "venue": "nips workshop on deep learning & unsupervised feature learning,",
            "year": 2011
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "Vardan Papyan",
                "XY Han",
                "David L Donoho"
            ],
            "title": "Prevalence of neural collapse during the terminal phase of deep learning training",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2020
        },
        {
            "authors": [
                "Giorgio Patrini",
                "Alessandro Rozza",
                "Aditya Krishna Menon",
                "Richard Nock",
                "Lizhen Qu"
            ],
            "title": "Making deep neural networks robust to label noise: A loss correction approach",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Xingchao Peng",
                "Qinxun Bai",
                "Xide Xia",
                "Zijun Huang",
                "Kate Saenko",
                "Bo Wang"
            ],
            "title": "Moment matching for multi-source domain adaptation",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Shibani Santurkar",
                "Dimitris Tsipras",
                "Aleksander Madry"
            ],
            "title": "Breeds: Benchmarks for subpopulation shift",
            "venue": "arXiv preprint arXiv:2008.04859,",
            "year": 2020
        },
        {
            "authors": [
                "Nikunj Saunshi",
                "Orestis Plevrakis",
                "Sanjeev Arora",
                "Mikhail Khodak",
                "Hrishikesh Khandeparkar"
            ],
            "title": "A theoretical analysis of contrastive unsupervised representation learning",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Kendrick Shen",
                "Robbie M Jones",
                "Ananya Kumar",
                "Sang Michael Xie",
                "Jeff Z HaoChen",
                "Tengyu Ma",
                "Percy Liang"
            ],
            "title": "Connect, not collapse: Explaining contrastive learning for unsupervised domain adaptation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Rui Shu",
                "Hung H Bui",
                "Hirokazu Narui",
                "Stefano Ermon"
            ],
            "title": "A dirt-t approach to unsupervised domain adaptation",
            "venue": "In Proc. 6th International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Ravid Shwartz-Ziv",
                "Randall Balestriero",
                "Yann LeCun"
            ],
            "title": "What do we maximize in self-supervised learning",
            "venue": "In First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML",
            "year": 2022
        },
        {
            "authors": [
                "Gilbert W Stewart"
            ],
            "title": "On the perturbation of pseudo-inverses, projections and linear least squares problems",
            "venue": "SIAM review,",
            "year": 1977
        },
        {
            "authors": [
                "Thomas Strohmer",
                "Robert W Heath Jr."
            ],
            "title": "Grassmannian frames with applications to coding and communication",
            "venue": "Applied and computational harmonic analysis,",
            "year": 2003
        },
        {
            "authors": [
                "Yu Sun",
                "Eric Tzeng",
                "Trevor Darrell",
                "Alexei A Efros"
            ],
            "title": "Unsupervised domain adaptation through self-supervision",
            "venue": "arXiv preprint arXiv:1909.11825,",
            "year": 2019
        },
        {
            "authors": [
                "Chenxin Tao",
                "Honghui Wang",
                "Xizhou Zhu",
                "Jiahua Dong",
                "Shiji Song",
                "Gao Huang",
                "Jifeng Dai"
            ],
            "title": "Exploring the equivalence of siamese self-supervised learning via a unified gradient framework",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yuandong Tian",
                "Xinlei Chen",
                "Surya Ganguli"
            ],
            "title": "Understanding self-supervised learning dynamics without contrastive pairs",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Christopher Tosh",
                "Akshay Krishnamurthy",
                "Daniel Hsu"
            ],
            "title": "Contrastive estimation reveals topic posterior information to linear models",
            "venue": "Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Julius Von K\u00fcgelgen",
                "Yash Sharma",
                "Luigi Gresele",
                "Wieland Brendel",
                "Bernhard Sch\u00f6lkopf",
                "Michel Besserve",
                "Francesco Locatello"
            ],
            "title": "Self-supervised learning with data augmentations provably isolates content from style",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hao Wang",
                "Yitong Wang",
                "Zheng Zhou",
                "Xing Ji",
                "Dihong Gong",
                "Jingchao Zhou",
                "Zhifeng Li",
                "Wei Liu"
            ],
            "title": "Cosface: Large margin cosine loss for deep face recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Tongzhou Wang",
                "Phillip Isola"
            ],
            "title": "Understanding contrastive representation learning through alignment and uniformity on the hypersphere",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Chun-Hsiao Yeh",
                "Cheng-Yao Hong",
                "Yen-Chi Hsu",
                "Tyng-Luh Liu",
                "Yubei Chen",
                "Yann LeCun"
            ],
            "title": "Decoupled contrastive learning",
            "venue": "In Computer Vision\u2013ECCV",
            "year": 2022
        },
        {
            "authors": [
                "Jure Zbontar",
                "Li Jing",
                "Ishan Misra",
                "Yann LeCun",
                "St\u00e9phane Deny. Barlow"
            ],
            "title": "twins: Self-supervised learning via redundancy reduction",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jingyi Zhang",
                "Jiaxing Huang",
                "Zichen Tian",
                "Shijian Lu"
            ],
            "title": "Spectral unsupervised domain adaptation for visual recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Zhilu Zhang",
                "Mert Sabuncu"
            ],
            "title": "Generalized cross entropy loss for training deep neural networks with noisy labels",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Xiong Zhou",
                "Xianming Liu",
                "Junjun Jiang",
                "Xin Gao",
                "Xiangyang Ji"
            ],
            "title": "Asymmetric loss functions for learning with noisy labels",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Xiong Zhou",
                "Xianming Liu",
                "Deming Zhai",
                "Junjun Jiang",
                "Xin Gao",
                "Xiangyang Ji"
            ],
            "title": "Learning towards the largest margins",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Xiong Zhou",
                "Xianming Liu",
                "Hanzhang Wang",
                "Deming Zhai",
                "Jiangjunjun",
                "Xiangyang Ji"
            ],
            "title": "On the dynamics under the unhinged loss and beyond",
            "venue": "Journal of Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Xiong Zhou",
                "Xianming Liu",
                "Deming Zhai",
                "Junjun Jiang",
                "Xiangyang Ji"
            ],
            "title": "Asymmetric loss functions for noise-tolerant learning: Theory and applications",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Hao Zhu",
                "Ke Sun",
                "Peter Koniusz"
            ],
            "title": "Contrastive laplacian eigenmaps",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Contrastive learning has emerged as one of the most prominent self-supervised learning paradigms, which offers promising representations that can be adapted to diverse downstream tasks [5, 6, 7, 8, 9, 19, 23, 30, 54, 55]. Contrastive losses serve as the training objectives for contrastive learning, encouraging the learning of representations invariant to data augmentations by maximizing the similarity between features from different distortions of the same images. In addition to unsupervised training, contrastive losses are extended to the fully-supervised setting as an alternative to the classical cross-entropy loss to effectively leverage label information [26]. This extension enables contrastive learning to achieve state-of-the-art performance on various supervised learning tasks.\nResearchers have attempted to offer theoretical understanding of the empirical successes of contrastive learning [1, 45, 49, 50, 52]. Some works provide mathematical analysis, which, however, are under an impractical assumption that two views are somewhat independent conditioned on the label [29, 42, 51]. Instead, spectral contrastive loss (SpeCL) presents solid theoretical foundations without requiring conditional independence but on a more realistic data property that there is continuity of the population data within the same class [19, 20]. The core concept in SpeCL is the positive-pair graph on data, where nodes are augmented samples and the edge between two nodes is weighted as the probability of encountering them as a positive pair. By applying spectral decomposition on the adjacency matrix defined on the population augmentation graph, SpeCL builds the relationship between the eigenvector matrix and the learned representations. SpeCL naturally exhibits sample-\n\u2217Correspondence to: Xianming Liu <csxm@hit.edu.cn>\ncontrastive property, while the covariance regularization term within the equivalent form [20] can be seen as contrastive between the dimensions of the representations, and thus coincides with the dimension-contrastive property. Therefore, SpeCL serves as a bridge between sample-contrastive and dimension-contrastive properties, offering a pathway to unify them. Moreover, SpeCL resembles state-of-the-art contrastive losses, such as Barlow Twins loss [56] and VICReg loss [1, 2].\nAlthough SpeCL has appealing theoretical advantages, there are still two limitations carefully considered in this paper. Firstly, SpeCL intuitively requires the orthogonality of representations between negative pairs. However, relaxing the orthogonality could potentially enhance the discriminativeness of representations. Secondly, the pairwise similarities among the representations learned by SpeCL are determined by the connections within the positive-pair graph. Unfortunately, the presence of false positive pairs, which may be due to noisy views in self-supervised learning [10] or noisy labels in supervised learning [62], can lead to wrong connections (i.e., unfavorable edge weights). Identifying and correcting these wrong connections is scarcely possible since the underlying structure of the graph is unknown. To overcome these limitations, we propose to incorporate an additional factor \u03c4 into the term of SpeCL involving negative pairs, which is simple yet effective to relax the orthogonality and coincides with the motivation of margin-based losses [12, 32, 33, 53, 60]. By algebraic manipulation, it can be found that our scheme is equivalent to regularizing the mean of representations to be zero, thus referred to as zero-mean regularization. Furthermore, we would establish that the introduction of \u03c4 explicitly performs a uniform reduction in all positive-pair weights, which modifies pairwise similarities between positive pairs and implicitly mitigate the adverse effects of wrong connections.\nTo demonstrate the effectiveness of zero-mean regularization in mitigating adverse effects of wrong connections, we provide theoretical investigations in both unsupervised and supervised scenarios: unsupervised domain adaptation (UDA) and supervised learning with noisy labels. For contrastive pretraining based UDA, wrong connections occurs when positive pairs are built from different domains and classes, we prove that the incorporation of \u03c4 tightens the downstream error on target domains in the form of multiplying a factor (1\u2212 \u03c4)2. For supervised learning with noisy labels, false positive pairs originate from the misguidance of label noise. We first establish the supervised version of SpeCL and show that the global minimizer exhibits a geometric structure similar to the recently discovered Neural Collapse [38], and we prove that zero-mean regularization can mitigate label noise by implicitly reducing mislabeled weights in the noise transition matrix.\nThe main contributions of our paper can be summarized as follows:\n\u2022 We propose to incorporate an additional factor into spectral contrastive loss involving negative pairs, which is equivalent to the zero-mean regularization term. We also show that zero-mean regularization implicitly performs a uniform weight reduction in positive pairs.\n\u2022 We investigate the role of zero-mean regularized spectral contrastive loss on spectral contrastive pretraining-based unsupervised domain adaptation and offer theoretical proof that zero-mean regularization can tighten the error bound by multiplying a factor less than one.\n\u2022 We establish the supervised version of spectral contrastive loss and derive the closed-form optimal representations, which resembles the Neural Collapse phenomenon and suggests using class-mean features as classifier. We further prove that zero-mean regularization can mitigate label noise by implicitly reducing mislabeled weights in the noise transition matrix."
        },
        {
            "heading": "2 PRELIMINARY",
            "text": "We consider datasets containing samples drawn from the input space X with the distribution pdata that can be partitioned into r different classes with the label function y(\u00b7) : X \u2192 [r]. The representation f(x) \u2208 Rd is extracted by a mapping f : X \u2192 Rd that is usually characterized by a number of parameterized compositions, where d is the representation dimensionality.\nSpectral Contrastive Loss. The spectral contrastive loss is formulated as [19]: L(f ; \u03c4) = \u22122 \u00b7 E(x,x+)\u223cppos [ f(x)\u22a4f(x+) ] + E(x,x\u2212)\u223cpdata [( f(x)\u22a4f(x\u2212) )2] =Ex,x+ [\u2225\u2225f(x)\u2212 f(x+)\u2225\u22252 2 ] \ufe38 \ufe37\ufe37 \ufe38\nR0(f)\n+ \u2225\u2225Ex [f(x)f(x)\u22a4]\u2212 I\u2225\u22252F\ufe38 \ufe37\ufe37 \ufe38\nR1(f)\n+const, (2.1)\nwhere (x, x+) \u223c ppos is a pair of augmentations of the same data, (x, x\u2212) is a pair of independently random augmented data, and I is the identity matrix. The second equality, as identified in [20], reveals that the spectral contrastive loss has two terms: (i) R0(f), namely the invariance term, measures the \u21132 distance between positive pairs, which can also be expressed as the Dirichlet energy of representations on the positive-pair graph [1]; (ii) R1(f), restricts the representation covariance towards the identity matrix, similar to Barlow Twins loss [56] and VICReg loss [1, 2]. Spectral contrastive loss naturally falls under the category of a sample-contrastive method, which also exhibits the dimension-contrastive characteristic due to the existence of the R1(f) term.\nLimitations. Although spectral contrastive loss achieves promising theoretical and quantitative results, there are still some limitations needed to be carefully considered: (i) SpeCL intuitively requires the orthogonality of representation between negative pairs, while the InfoNCE loss [37] used in SimCLR [6] and MoCo [23] aims to make them opposite. Relaxing the orthogonality constraint would bring SpeCL closer to these prior works and may also enhance the discriminativeness of representations, particularly for supervised contrastive learning in Section 3.3. (ii) The pairwise similarities among the representations acquired through SpeCL are dictated by the relationships within the positive-pair graph. However, rectifying incorrect connections proves to be a formidable task, given that the inherent structure of the graph remains elusive. SpeCL does not incorporate any mechanism to mitigate the detrimental impact of connections stemming from erroneous positive pairs, which frequently arise in various tasks due to the inclusion of noisy views [10], noisy labels [62], and other contributing factors, as illustrated in Figure 1."
        },
        {
            "heading": "3 ZERO-MEAN REGULARIZED SPECTRAL CONTRASTIVE LEARNING",
            "text": "In this section, we introduce in detail our main contribution\u2014zero-mean regularization for SpeCL. We explain its motivation and how it can be applied in SpeCL for alleviating detrimental connections in positive-pair graph on unsupervised domain adaptation and learning with noisy labels. We also provide more clarification in Appendix A."
        },
        {
            "heading": "3.1 ZERO-MEAN REGULARIZATION",
            "text": "We introduce the zero-mean regularization on SpeCL [19], which is simply incorporating an additional factor \u03c4 in the negative part of the original SpeCL. The population objective is formulated as:\nL(f ; \u03c4) = \u22122 \u00b7 E(x,x+)\u223cppos [ f(x)\u22a4f(x+) ] + E(x,x\u2212)\u223cpdata [( f(x)\u22a4f(x\u2212) + \u03c4 )2] (3.1)\nIt can be found that, compared with Equation 2.1, the only modification is the introduction of the additive factor \u03c4 . To more clearly show the role of \u03c4 , we further derive its equivalent form:\nL(f ; \u03c4) = Ex,x+ [\u2225\u2225f(x)\u2212 f(x+)\u2225\u22252\n2 ] \ufe38 \ufe37\ufe37 \ufe38\nR0(f)\n+ \u2225\u2225Ex [f(x)f(x)\u22a4]\u2212 I\u2225\u22252F\ufe38 \ufe37\ufe37 \ufe38\nR1(f)\n+2\u03c4 \u00b7 \u2225Ex [f(x)]\u222522\ufe38 \ufe37\ufe37 \ufe38 R2(f) +const,\n(3.2) where R2(f) regularizes the mean of representations to be zero and thus is referred to as zero-mean regularization, and \u03c4 \u2265 0 is actually the trade-off parameter that controls the regularization strength. Zero-mean regularization can effectively alleviate the limitations of SpeCL: (i) The introduction of \u03c4 is initially motivated by the observation that SpeCL requires orthogonality of representations between negative pairs, which, in the supervised scenario (as described in Section 3.3), is roughly requiring the orthogonality between different classes. However, prior work has demonstrated that the representations learned by cross entropy and mean-squared error during the terminal phase of training usually exhibit the Neural Collapse phenomenon [17, 35, 38, 60, 64], which shows that the representations of different classes are maximally distant and form a simplex Equiangular Tight Frame (ETF)1. Intuitively, the introduction of \u03c4 can encourage the representations of negative pairs to have larger angles and thus makes them more discriminative. (ii) Spectral contrastive learning is equivalent to spectral decomposition on a positive-pair graph. The introduction of \u03c4 implicitly reduces all these positive-pair weights uniformly in Equation 3.3, which does not change the magnitude relationship between positive-pair weights but alters the ratio between them, as illustrated in Figure 1. This explicitly modifies pairwise similarities between representations, which further indicates the benefits in decoupling the class and domain information for contrastive pretraining based unsupervised domain adaptation (cf. Section 3.2) and mitigating label noise by reducing mislabeled weights in the noise transition matrix (cf. Section 3.3.2).\nIn the following, we introduce in detail zero-mean regularization works in both unsupervised and supervised versions of SpeCL. We carefully discuss in theory how \u03c4 is helpful in downstream unsupervised domain adaptation and supervised classification with noisy labels."
        },
        {
            "heading": "3.2 UNSUPERVISED DOMAIN ADAPTATION",
            "text": "In the UDA setting, we have access to labeled data from a source domain and unlabeled data from target domains, and the goal is to achieve good performance on target domains [4, 40, 41]. We consider a multi-way classification problem, where r is the number of classes, p is the number of domains, and n is the number of examples in each class of each domain. The total number of data is N = rpn. For data x \u2208 X , we use dx \u2208 [p] and yx \u2208 [r] to denote its domain and class, respectively.\nContrastive Pre-training based UDA. While conventional UDA methods typically leverage the intuition of learning domain-invariant representations [14, 44, 48, 57], recent research by Shen et al. [43] has demonstrated that a model first pre-trained on both the source and target domains with SpeCL and then fine-tuned using labeled source data, can yield comparable or superior results to strong UDA methods. Furthermore, the authors have theoretically proven that spectral contrastive pre-training enables the learning of representations that vary substantially across domains, while still generalizing to the target domain by disentangling domain and class information, instead of merely satisfying domain invariance. Following the setup of [43], we also consider a stochastic block model [24] for the positive-pair graph, where the probability of existence of an edge between x and x\u2032 is: (1) \u03c1 if dx = dx\u2032 and yx = yx\u2032 , (2) \u03b1 if dx \u0338= dx\u2032 and yx = yx\u2032 , (3) \u03b2 if dx = dx\u2032 and yx \u0338= yx\u2032 , (4) \u03b3 if dx \u0338= dx\u2032 and yx \u0338= yx\u2032 . Let A \u2208 RN\u00d7N be the adjacency matrix of the graph. Let d = p+ r \u2212 1 be the feature dimension 2 and f : X \u2192 Rd be the representation model.\n1An ETF is a collection of vectors V = [v1, v2, ..., vr] \u2208 Rd\u00d7r having equal lengths and equal, maximally separated pair-wise angles. In the setting of r \u2264 d+ 1, we have V \u22a4V = C(I\u2212 1\nr 11\u22a4) [38, 47, 61].\n2The reason we assume d = p+r\u22121 is to facilitate a better derivation and arrive at more relevant conclusions. As depicted in the proof of Theorem 4.1, there are exactly p+ r \u2212 1 nonzero eigenvalues in A\u0303. By assuming d = p+ r \u2212 1, we can easily write the d-rank approximation of A\u0303 according to spectral decomposition. This also reveals that the effective dimension of the feature space is p+ r \u2212 1. In the scenario where d > p+ r \u2212 1, we can directly set the values of excess dimensions to zero without compromising the validity of our conclusion. However, we acknowledge that this assumption may appear restrictive. In cases where d < p + r \u2212 1, the corresponding derivation becomes intractable, making it challenging to obtain the exact form of A\u0303d.\nmax{\u03b1,\u03b2} when \u03c1 > max{\u03b1, \u03b2},\nwhich benefits in terms of decoupling the class and domain information. Furthermore, the connections of pairs from different domains and different classes in the graph are also weakened, i.e., min{\u03b1,\u03b2}\u2212\u03c4\n\u03b3\u2212\u03c4 > min{\u03b1,\u03b2} \u03b3 , which helps to improve discriminativeness across domains and classes.\nThe Uniform Weight Reduction in Positive-Pair Graphs. Let |E| denote the total number of edges, according to the definition of SpeCL in Equation 3.1, we can rewrite the loss function as\n\u2212 2 \u2211 x,x\u2032 Axx\u2032 |E| f(x)\u22a4f(x\u2032) + \u2211 x,x\u2032 1 N2 ( f(x)\u22a4f(x\u2032) + \u03c4 )2 =\n\u2225\u2225\u2225\u2225 N|E| \u00b7 (A\u2212 \u03c4 |E|N2 11\u22a4)\u2212 ( 1\u221aN \u00b7 F)( 1\u221aN \u00b7 F)\u22a4 \u2225\u2225\u2225\u22252 F + const,\n(3.3)\nwhere F \u2208 RN\u00d7k is the matrix which the x-th row contains f(x)\u22a4. We note that the introduction of \u03c4 reduces the positive-pair weights uniformly, i.e,, all the entries in A are subtracted by \u03c4 |E|N2 . As illustrated in Figure 2, subtracting a suitable positive constant from the edge weights preserves their magnitude relationship while altering the ratio between them. Specifically, the weights of pairs belonging to the same domain and class are relatively increased, whereas the weights of pairs from different domains or classes are relatively decreased. This adjustment explicitly modifies pairwise similarities between representations, leading to further benefits in terms of decoupling the class and domain information. Additionally, the connections of pairs from different domains and different classes in the graph are weakened, which enhances discriminativeness across domains and classes. We further provide theoretical analysis that \u03c4 can tighten the downstream error on the target domains.\nIn UDA, we care about the error on target domains. Let S = {x \u2208 X : dx = 1} and T = {x \u2208 X : dx \u0338= 1} be the source and target domains, respectively. Given the labeled source domain data, we learn the linear classifier with a pre-trained representation model f :\nb\u0302 = argmin b\u2208Rd\u00d7r \u2211 x\u2208S ( \u2225b\u22a4f(x)\u2212 y\u20d7x\u222522 + \u03b7\u2225b\u22252F ) , (3.4)\nwhere y\u20d7x = eyx \u2212 1r \u00b7 1 \u2208 R r is the mean-zero one-hot embedding of the label, \u03b7 > 0 is the regularization strength. For data x \u2208 T , we use pred(x) = argmaxi(b\u0302\u22a4f\u0302(x))i as the predictor, where f\u0302 achieves the global minimum of Equation 3.3. Let pred \u2208 RN\u00d7r be the matrix with b\u0302\u22a4f(x) on its x-th row and predT be the matrix by restricting pred to the target domain.\nCompared to Shen et al. [43], we provide a more general form of the error bound when training SpeCL with zero-mean regularization across multiple domains as follows:\nTheorem 3.1. Let \u03b6 > 0 and \u03f5 \u2208 (0, 12 ) be arbitrary constants. In the above stochastic block model, assume \u03c1 > max{\u03b1, \u03b2}, \u03b3 < min{\u03b1, \u03b2}, and \u03c4 < (\u03bb\u03031(0)\u2212 \u03bb\u0303d(0))/\u03bb\u03031(0). Then, there exists \u03be\u0303 \u2208 [1\u2212 \u03f5, 1], such that for any n \u2265 \u2126 ( rp\nmin{\u03b1\u2212\u03b3,\u03b2\u2212\u03b3}2\n) and regularization strength \u03b7 \u2208 ( 0, (\u03b1\u2212\u03b3)\u03f52r\u03c1 ] ,\nwith a high probability 1\u2212 n\u2212\u03b6 , we have\u2225\u2225\u2225predT \u2212 p\u0303redT \u2225\u2225\u2225 F \u2264 O ( \u03bb\u03031(\u03c4)\n\u03b72(\u03bb\u0303d(\u03c4)\u2212 \u03bb\u0303d+1(\u03c4))\n) \u00b7 poly(r, p), (3.5)\nwhere p\u0303redT \u225c E[predT ] is the expectation of the prediction matrix when achieving the minimum of the loss in Equation 3.3 with \u03c4 \u2208 [0, 1], \u03bb\u03031(\u03c4), \u03bb\u0303d(\u03c4) and \u03bb\u0303d+1(\u03c4) are the 1-st, d-th and d+ 1-th eigenvalues of A\u0303 \u225c E [ A\u2212 \u03c4 |E\u0303|N2 11 \u22a4 ] , respectively.\nFurthermore, the target error can be bounded by\nPx\u223cT (pred(x) \u0338= yx) \u2264 O\n( (\u03bb\u03031(\u03c4)) 2\n\u03b74(\u03bb\u0303d(\u03c4)\u2212 \u03bb\u0303d+1(\u03c4))2 \u00b7 n\n) \u00b7 poly(r, p), (3.6)\nwhere poly(r, p) denotes a polynomial function of r and p.\nIn the following, we show that the introduction of \u03c4 through zero-mean regularization can tighten the error bound: Proposition 3.2. In the setting of Theorem 3.1, if \u03c4 \u2264 \u03bb\u03031(0)\u2212\u03bb\u03032(0)\n\u03bb\u03031(0) = n\u00b7min{p\u03b1+r(p\u22121)\u03b3,r\u03b2+p(r\u22121)\u03b3} \u03bb\u03031(0) ,\nthen, with probability at least 1\u2212 n\u2212\u03b6 , we have\nPx\u223cT (pred(x) \u0338= yx) \u2264 O\n( (1\u2212 \u03c4)2(\u03bb\u03031(0))2\n\u03b74(\u03bb\u0303d(0)\u2212 \u03bb\u0303d+1(0))2 \u00b7 n\n) \u00b7 poly(r, p), (3.7)\nwhere \u03bb\u03031(0) = n\u03c1 + n(r \u2212 1)\u03b2 + n(p \u2212 1)\u03b1 + n(p \u2212 1)(r \u2212 1)\u03b3 and \u03bb\u0303d(0) \u2212 \u03bb\u0303d+1(0) = n \u00b7min{r(\u03b2 \u2212 \u03b3), p(\u03b1\u2212 \u03b3)}.\nAs can be seen, the error decreases when the number of samples n increases, and it also is controlled by the difference between the d-th and d+ 1-th eigenvalues. This difference, in turn, depends on the gap between the across-class/across-domain connectivities \u03b2, \u03b1 and the across-both connectivities \u03b3. Furthermore, Proposition 3.2 provides additional insight that the incorporation of \u03c4 can tighten the error bound in the form of multiplying a factor of (1\u2212 \u03c4)2. This theoretical foundation underpins the proposed zero-mean regularization in the UDA task."
        },
        {
            "heading": "3.3 SUPERVISED CLASSIFICATION WITH NOISY LABELS",
            "text": "The primary objective of this subsection is to establish the supervised version of SpeCL with zeromean regularization. We subsequently derive the closed-form optimal representations by minimizing the supervised SpeCL, which leads to a result similar to the neural collapse solution. Furthermore, we demonstrate that zero-mean regularization can mitigate label noise by implicitly reducing mislabeled weights in the noise transition matrix."
        },
        {
            "heading": "3.3.1 SUPERVISED SPECTRAL CONTRASTIVE LOSS",
            "text": "Similar to the supervised contrastive loss [26], we can define the supervised version of SpeCL with the zero-mean regularization for the labeled dataset (X,Y ) as\nLsup(f ; \u03c4) = Lsuppos (f) + Lneg(f ; \u03c4),\nLsuppos (f) = \u2212 2\nrn2 n\u2211 i=1 n\u2211 j=1 r\u2211 c=1 E x\u223cA(xi,c) x+\u223cA(xj,c) [ f(x)\u22a4f(x+) ] , and\nLneg(f ; \u03c4) = 1\nr2n2 n\u2211 i=1 n\u2211 j=1 r\u2211 c=1 r\u2211 k=1 E x\u223cA(xi,c) x\u2212\u223cA(xj,k) [( f(x)\u22a4f(x\u2212) + \u03c4 )2] ,\n(3.8)\nwhere xi,c denotes the i-th example in the class c, n is the number of examples in each class, and A(x) denotes the augmentations of example x.\nRemark. Note that we retain the condition c = k within the negative part Lneg , which is raised for two main reasons. Firstly, this condition aligns with the self-supervised form of spectral contrastive loss in Eq. 7, which facilitates the derivation of the positive and negative parts into a spectral decomposition as evidenced in the proof of Theorem 3.3; Secondly, the inclusion of the constraint c \u0338= k in Lneg would result in that the supervised loss goes to infinity as the similarity f(x)\u22a4f(x+) in Lpos has the potential to become arbitrarily large. By incorporating positive pairs in Lneg3, we can effectively address this issue. This mechanism is analogous to the presence of numerator in the denominator in CE and InfoNCE, serving to ensure training stability.\nThe supervised version Lsup(f ; \u03c4) differs from the self-supervised version L(f ; \u03c4) in that Lsuppos (f) uses samples in the same class as positive pairs, resulting in class-dependent connections in the positive pair graph for supervised spectral contrastive learning.\nTheorem 3.3. The global minimum of the supervised SpeCL Lsup(f ; \u03c4) in Equation 3.8 is uniquely obtained at \u2200i \u2208 [n],\u2200c \u2208 [r],\u2200x \u223c A(xi,c), f(x) = h\u0302c, where H\u0302 = [h\u03021, ..., h\u0302r]\u22a4 is the minimizer of \u2225\u2225(rI\u2212 \u03c411\u22a4)\u2212H\u22a4H\u2225\u22252\nF .\nA more specific conclusion of Theorem 3.3 is that we have H\u0302\u22a4H\u0302 = rI \u2212 \u03c411\u22a4 when r \u2264 d + 1, and when r > d+ 1, H\u0302\u22a4H\u0302 = Pd(rI\u2212 \u03c411\u22a4) where Pd(X) denotes the best d-rank approximation of X . These results provide insights into the structure of the learned representations and highlight the impact of zero-mean regularization on the geometric characteristics of the learned representations. One straightforward observation is that, as \u03c4 increases, the angle between representations of different classes (i.e, \u2220(h\u0302i, h\u0302j) = arccos \u2212\u03c4r\u2212\u03c4 for i \u0338= j) increases, thereby leading to better discriminativeness. When \u03c4 = 1 and r \u2264 d + 1, we have H\u0302\u22a4H\u0302 = (r \u2212 1)(I \u2212 1r11\n\u22a4) that forms the neural collapse solution4 proposed in [38].\nRemark. Resembling the neural collapse phenomenon [38], the optimal minimizer of supervised SpeCL is characterized by two manifestation for last-layer features when r \u2264 d+ 1 and \u03c4 \u2208 [0, 1]: (i) Within-class variability collapse. The within-class variation of features becomes negligible as they collapse to their respective means. Specifically, for all classes c and samples x \u223c A(xi,c), the feature becomes a constant value f(x) = h\u0302c, leading to a reduction in with-class variability; (ii) Convergence to an equiangular frame. The vectors of the class means converge to having equal length, forming equal-sized angles between any pair. This is represented by the equation H\u22a4H = rI\u2212 \u03c411\u22a4. These two manifestation indicates that the zero-regularized SpeCL exhibits a phenomenon similar to neural collapse even though it exclusively employs features without the linear classifier to calculate loss. Regarding the downstream linear classification, this suggests that we may easily utilize class-mean features as the classifier. Compared Table 1 to Table 12, class-mean features as classifier achieve comparable results with the trained classifier on CIFAR-10 and SVHN."
        },
        {
            "heading": "3.3.2 THE IMPLICATION OF MITIGATING LABEL NOISE",
            "text": "As mentioned in Section 3.3, the introduction of \u03c4 can encourage the discriminativeness of different classes. In the following, we will prove that zero-mean regularization can mitigate label noise. Consider the label noise transition matrix W = (wij)r\u00d7r, where wij denotes the probability of flipping the true class j into class i. Then, for each sample xi,yx in the true class yx \u2208 [r], the probability of its label being y\u0302x \u2208 [r] is wyxy\u0302x . To analyze the supervised SpeCL in the presence of label noise, we let Lsupnoise(f ; \u03c4) denote the expected loss on the label noise model, i.e., L sup noise(f ; \u03c4) = EY\u0302 |Y Lsup(f ; \u03c4, (X, Y\u0302 )), where Y\u0302 is the flipped labels of Y . We can derive the results similar to Theorem 3.3 as follows:\nTheorem 3.4. For the noise transition matrix W defined above, the global minimum of Lsupnoise(f ; \u03c4) is uniquely obtained at \u2200i \u2208 [n],\u2200c \u2208 [r],\u2200x \u223c A(xi,c), f(x) = h\u0302c, where H\u0302 = [h\u03021, ..., h\u0302r]\u22a4 is the minimizer of \u2225\u2225(rW \u2212 \u03c411\u22a4)\u2212H\u22a4H\u2225\u22252 F .\n3We have \u22122s \u00b7 f(x)\u22a4f(x+) + [ f(x)\u22a4f(x+) + \u03c4 ]2 = [ f(x)\u22a4f(x+)\u2212 (s\u2212 \u03c4)2 ] + const.\n4Our analysis covers all choices of representation dimension and the number of classes, unlike previous work [17, 35, 49] that only considers cases where the feature dimension is larger than the number of classes.\nThis theorem reveals that the immediate effect of \u03c4 is to implicitly reduce the mislabeled weights (wij , i \u0338= j) in the noise transition matrix, which in turn mitigates label noise. For a better understanding, we provide a more direct view for symmetric label noise. Proposition 3.5. Considering the symmetric label noise [15] in which wcc\u2032 = 1 \u2212 (r \u2212 1)\u03c9 for c = c\u2032, wcc\u2032 = \u03c9 for c \u0338= c\u2032, and \u03c9 < 1r . If \u03c4 \u2265 r\u03c9, let f\u0302 = argminf L sup noise(f ; \u03c4), then 1\u221a 1\u2212r\u03c9 \u00b7 f\u0302\nis also the global minimizer of Lsup ( f ; \u03c4\u2212r\u03c91\u2212r\u03c9 ) .\nProposition 3.5 indicates that, when \u03c4 \u2265 r\u03c9, using Lsupnoise(f ; \u03c4) training on symmetric noisy labels is equivalent to using Lsup ( f ; \u03c4\u2212r\u03c91\u2212r\u03c9 ) training on clean labels. As illustrated in Figure 3, if the uniform\nreduction is \u03c4 = 15%, the re-normalized matrix will be an identify matrix."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we provide extensive experiments on the tasks of contrastive learning, supervised classification, unsupervised domain adaptation, and learning with noisy labels to verify the effectiveness of zero-mean regularization on several benchmark datasets with ResNets [21, 22]. More experimental results and details can be found in the Appendix B.\nResults of Self-Supervised Learning and Supervised Learning. Table 1 reported the linear probing accuracy of self-supervised learned models and the Top-1 accuracy of supervised learned models with spectral contrastive loss in Equation 3.1 and supervised spectral contrastive loss in Equation 3.8. The results show that zero-mean regularization can learn more discriminative representations.\nResults of Unsupervised Domain Adaptation. We evaluate zero-mean regularization across four digits datasets with different domains: SVHN (S) [36], MNIST (M) [28], USPS (U) [25], and MNIST-M (M-M) [14]. We first pre-train models with the modified spectral contrastive loss in Equation 3.1, and then fine-tune a linear classifier composited the models on the source domain.\nTable 2 demonstrates that zero-mean regularization (\u03c4 > 0) can help decoupling class information of representations across domains. Experiments on DomainNet [40] are provided in Appendix B.4.\nResults of Learning with Noisy Labels. To further verify that zero-mean regularization can mitigate label noise. We conduct experiments in the scenario of symmetric label noise, considering baselines CE, Focal loss [31], and GCE [58]. As shown in Table 3, \u03c4 > 0 performs better than SpeCL in all cases, especially for high noise. This also show that contrastive learning can mitigate label noise."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "This paper introduces zero-mean regularization to enhance spectral contrastive loss, which equivalently incorporates an additive factor into the loss component involving negative pairs and thus makes representations of negative pairs opposite. Provable accuracy guarantees are achieved under linear probe evaluation for contrastive learning with restricted model classes. Additionally, zero-mean regularization reduces positive-pair weights uniformly during contrastive pretraining for unsupervised domain adaptation, tightening downstream error on target domains. The supervised version of spectral contrastive loss reveals a structure resembling the neural collapse phenomenon, with larger regularization strength indicating improved discriminativeness. Moreover, zero-mean regularization mitigates label noise by implicitly reducing mislabeled weights in the noise transition matrix. These findings highlight the superiority of zero-mean regularization in enhancing contrastive learning and its potential applications in various domains."
        },
        {
            "heading": "ACKNOWLEDGE",
            "text": "This work was supported in part by National Natural Science Foundation of China under Grants 92270116 and 62071155, and in part by the Fundamental Research Funds for the Central Universities (Grant No.HIT.OCEF.2023043 and HIT.DZJJ.2023075)."
        },
        {
            "heading": "A MORE CLARIFICATION",
            "text": ""
        },
        {
            "heading": "A.1 \u201cWEIGHT REDUCTION IN POSITIVE PAIRS\u201d BEYOND ZERO-MEAN REGULARIZATION",
            "text": "The main idea of zero-mean regularization is to implicitly reduce the weights of wrong connections in the positive-pair graph, as stated throughout the paper. This idea is fully depicted in the unsupervised domain adaptation (see Eq. 3.3) and learning with noisy labels (see Theorem 3.4) in Section 3.3.2. In this paper, we focus on the spectral contrastive loss, since it is theoretically sound and easier to analyze. The final manifestation is accordingly expressed in the concise term of the loss-specific zero-mean regularization.\nWe further attempt to apply the core idea of \u201cweight reduction in positive pairs\u201d to the InfoNCE loss that is widely used in many popular contrastive learning schemes, such as SimCLR [6], MoCo [23], and CPC [37]. Specifically, we modify the InfoNCE loss by uniformly reducing the importance of positive pair as follows:\n\u2212 log (1\u2212 \u03c4)e sim(x,x+) (1\u2212 \u03c4)esim(x,x+) + \u2211k\ni=1 e sim(x,x\u2212i )\n=\u2212 log e sim(x,x+)\u2212\u03c4 \u2032 esim(x,x+)\u2212\u03c4 \u2032 + \u2211k\ni=1 e sim(x,x\u2212i )\n=\u2212 log e sim(x,x+) esim(x,x+) + \u2211k\ni=1 e sim(x,x\u2212i )+\u03c4\n\u2032 ,\n(A.1)\nwhere sim(x1, x2) denotes the similarity between x1 and x2, \u03c4 \u2032 = log 11\u2212\u03c4 > 0 and \u03c4 \u2208 [0, 1). As can be seen, the derived form that adds a margin term is similar to margin-based losses, particularly the negative-margin softmax loss [32]. From this point of view, weight reduction in positive pairs coincides with the motivation of margin-based losses [12, 32, 33, 53, 60] that enlarges the discriminativeness with intuitive decision boundaries.\nTo empirically validate the efficacy of the modified term InfoNCE loss in Equation A.1, we conduct experiments on self-supervised learning and supervised learning in Table 4. In these experiments, we utilize CIFAR-10/-100 and SVHN datasets as benchmarks. We believe that these additional experiments not only provide further evidence of the versatility embedded within the concept of \"weight reduction in wrong positive pairs\" but also highlight its potential to enhance the performance of contrastive learning algorithms on real-world datasets."
        },
        {
            "heading": "Method Self-Supervised Learning Supervised LearningCIFAR-10 CIFAR-100 SVHN CIFAR-10 CIFAR-100 SVHN",
            "text": ""
        },
        {
            "heading": "A.2 DOWNSTREAM ERROR BOUND FOR CONTRASTIVE LEARNING",
            "text": "There are several works focusing on the SpeCL [18, 19, 20], which provide provable accuracy guarantees under linear probe evaluation in context of an implicit m-way partition of X , i.e., they are\ndisjoint non-empty sub-graphs of X such that X = \u222ai\u2208[m]Si (\u2200x \u2208 X , let idx be the index such that x \u2208 Sidx ). Specifically, HaoChen et al. [19] proved the efficacy of SpeCL under the assumption that the representation dimension d exceeds the number of sub-graphs m. Subsequently, HaoChen and Ma [18] extended these results by studying representations learned within a constrained model class F while imposing the condition d = m. In this work, we prove a downstream error bound when m > d as follows:\nTheorem A.1. Let F be a class of functions of X \u2192 Rd and let f\u0302 = argminf\u2208F L(f ; \u03c4) be the minimizer of the SpeCL. Assume that:\n1. (\u03f5-separability). The probability of a positive pair belonging to two different sets is less than \u03f5, that is, Pr(x,x+)\u223cppos(idx \u0338= idx+) \u2264 \u03f5;\n2. (Alignment) The downstream label y(x) is a constant on each Si for i \u2208 [m];\n3. (F-implementable inner-cluster connection larger than \u03b4). For any f \u2208 F and any linear head w \u2208 Rd, let function g(x) = w\u22a4f(x). For any i \u2208 [m], we have QSi(g) :=\nE (x,x+)\u223cpSipos [(g(x)\u2212g(x+))2]\nE x\u223cpSi\ndata ,x\u2032\u223cpSi data\n[(g(x)\u2212g(x\u2032))2] \u2265 \u03b4;\n4. (Implementability). There exists a function f \u2208 F such that f(x) = vidx for all x \u223c pdata, where {v1, v2, ..., vm} is a set of different vectors that achieve the global minimum of \u2225 \u2211m\ni=1 piviv \u22a4 i \u2212 I\u22252F + 2\u03c4 \u00b7 \u2225 \u2211m i=1 pivi\u222522, and pi = Prx\u223cpdata(x \u2208 Si).\nLet pmin = mini\u2208[m] Prx\u223cpdata(x \u2208 Si) and \u2206 = maxi,j \u2225vi \u2212 vj\u222522. For m > d, if \u03f5\u2206 < 1, then there exists a linear head W \u2208 Rd\u00d7d which achieves the following downstream error\nEx\u223cpdata [\u2225\u2225\u2225Wf\u0302(x)\u2212 vy(x)\u2225\u2225\u22252\n2\n] \u2264 \u03f5\u2206(1 + \u221a \u03f5\u2206)pmin\n2\u03b4(pmin \u2212 \u03f5) . (A.2)\nRemark. Theorem A.1, we assume that the model class F is implementable and capable of expressing some vectors V = [v1, ..., vm] that achieve the global minimum of \u2225 \u2211m i=1 piviv \u22a4 i \u2212\nI\u22252F + 2\u03c4 \u00b7 \u2225 \u2211m\ni=1 pivi\u222522 (see Lemma A.3 in the appendix for the specific form of V ). In [18], these vectors are concretely specified as one-hot vectors, which actually corresponds to the case of \u03c4 = 0. Otherwise, our error bound under linear probing is characterized as Ex\u223cpdataI(y(x) \u0338= argminc \u2225Wf\u0302(x) \u2212 vc\u222522) and can be further bounded by the error Ex\u223cpdata [\u2225Wf\u0302(x) \u2212 vy(x)\u222522]. Moreover, the right-hand side in Equation A.2 can be further tightened once we choose vectors that minimize \u2206 = maxi \u0338=j \u2225vi \u2212 vj\u222522. Importantly, Theorem A.1 also reveals a trade-off between inter-partition separability and intrapartition compactness. Intuitively, good inter-partition separability involves maximizing the distance between any two distinct partitions, i.e., maxV mini \u0338=j \u2225vi \u2212 vj\u222522, while the notion of intra-partition compactness anticipates the reduction in \u2206 = maxi,j \u2225vi \u2212 vj\u222522, contributing to a more stringent bound on the right-hand side of Eq. 3.3. The intricate interdependence between maxV mini \u0338=j \u2225vi \u2212 vj\u222522 and minV maxi \u0338=j \u2225vi\u2212vj\u222522 elucidates the nuanced nature of the trade-off in the understanding of self-supervised learning."
        },
        {
            "heading": "A.3 COMPARISON TO CONTRASTIVE LAPLACIAN EIGENMAPS",
            "text": "While there are similarities between Zero-mean regularized spectral loss (Zero-SpeCL) in Eq. 3.2 and contrastive Laplacian eigenmaps (COLES) [63], there are distinct differences that are crucial to highlight:\n\u2022 Different Negative Components. The contrastive objective (to be maximized) can generally formulated as the objective J(f) = E(x,x+)\u223cpposs(f(x), f(x+)) + \u03b7E(x,x\u2212)\u223cpdata s\u0303(f(x), f(x\u2212)). While the positive components of Zero-SpeCL and COLES are the same, their negative components differ. The negative component of COLES is s\u0303(f(x), f(x\u2212)) = \u2212f(x)\u22a4f(x\u2212), while Zero-SpeCL is s\u0303(f(x), f(x\u2212)) = \u2212(f(x)\u22a4f(x\u2212) + \u03c4)2.\n\u2022 Different Reasons for Covariance Term. Both Zero-SpeCL and COLES introduce the term \u2225Exf(x)f(x)\u22a4 \u2212 I\u22252F . The motivation of COLES is to softly satisfy the constraint F\u22a4F = I that removes an arbitrary scaling factor in the embeddings [3]. In addition, it also helps avoid collapsed solutions, since without the constraint F\u22a4F = I , the derived graph Dirichlet energy \u2212Tr(F\u22a4\u2206WF ) will be minimized when all representations collapse to a constant vector. In contrast, the covariance term R1(f) = \u2225Exf(x)f(x)\u22a4 \u2212 I\u22252F in Zero-SpeCL appears as a part of an equivalent form in Eq. 3.2.\n\u2022 Different Overall Objective. COLES represents a constrained graph Dirichlet energy minimization problem (contrastive Laplacian eigenmaps), i.e., minF\u22a4F=I \u2212Tr(F\u22a4\u2206WF ). On the other hand, Zero-SpeCL depicts the low-rank matrix approximation problem minF |A\u2212 F\u22a4F |2F (as shown in Eq. 3.3).\nA.4 THE ROLE OF \u03c4\nIn this paper, we propose setting \u03c4 \u2208 [0, 1] to balance the regularization strength of zero-mean regularization. Although in Section 3, we do not explicitly constrain \u03c4 \u2264 1, theoretical and empirical evidence supports the necessity of such a constraint:\n\u2022 In Theorem 3.1, we assume that \u03c4 < \u03bb\u03031(0)\u2212\u03bb\u0303d(0) \u03bb\u03031(0)\n< 1 to ensure that the introduction of \u03c4 will not alter the value of the d-th eigenvalue in Eq. (C.23). In Proposition 3.2, we assume that \u03c4 \u2264 \u03bb\u03031(0)\u2212\u03bb\u03032(0)\n\u03bb\u03031(0) < \u03bb\u03031(0)\u2212\u03bb\u0303d(0) \u03bb\u03031(0) < 1 to guarantee that the first eigenvalue of\nA\u0303\u2032 is \u03bb\u03031(\u03c4) = (1 \u2212 \u03c4)\u03bb\u03031(0) (as can be seen, \u03c4 \u2264 1 is essential to avoid the presence of negative eigenvalues of A\u0303\u2032) in Eq. (C.32), thus facilitating a more intuitive comparison with the bound at \u03c4 = 0. While it is possible to draw conclusions within the range of [ \u03bb\u03031(0)\u2212\u03bb\u0303d(0)\n\u03bb\u03031(0) , 1], it requires a more nuanced comparison of the order of these eigenvalues,\npredictably resulting in an obviously different bound than at \u03c4 = 0.\n\u2022 In Theorem 3.3, we prove that the global minimum of the supervised spectral contrastive loss is achieved at H\u0302 = argminH \u2225(rI\u2212 \u03c411\u22a4)\u2212H\u22a4H\u22252F . When r \u2264 d+1, \u2225(rI\u2212 \u03c411\u22a4)\u2212 H\u22a4H\u22252F has a minimum of zero only for \u03c4 \u2208 [0, 1], corresponding to H\u0302\u22a4H\u0302 = rI \u2212 \u03c411\u22a4, where the angle between representations of different classes is arccos \u2212\u03c4r\u2212\u03c4 > 90\n\u25e6. However, for \u03c4 > 1, there exists no H\u0302 that satisfies H\u0302\u22a4H\u0302 = rI \u2212 \u03c411\u22a4, as it would lead to a paradoxical situation where 0 \u2264 1\u22a4H\u0302\u22a4H\u03021 = 1\u22a4(rI \u2212 \u03c411\u22a4)1 = r2(1\u2212 \u03c4) < 0.\n\u2022 We conducted experiments for \u03c4 > 1 as presented in Tables 9 and 11 of the Appendix. As observed, consistently better results are obtained with \u03c4 \u2208 [0, 1], while excessively large \u03c4 values hinder overall performance. Thus, we suggest limiting \u03c4 to the range of [0, 1] to balance the regularization and discriminative capabilities."
        },
        {
            "heading": "B MORE EXPERIMENTAL DETAILS AND RESULTS",
            "text": "In this section, we present the training details and experimental results on several tasks, including self-supervised contrastive learning, supervised contrastive learning, unsupervised domain adaptation, and learning with noisy labels."
        },
        {
            "heading": "B.1 DATASETS",
            "text": "Small Datasets. In our experiments, we consider several commonly-used datasets, namely CIFAR10/-100 [27], SVHN (S) [36], MNIST (M) [28], USPS (U) [25], and MNIST-M (M-M) [14]. For the tasks of self-supervised contrastive learning and supervised classification, we utilize CIFAR-10/-100 and SVHN datasets. These datasets are also employed for the evaluation of learning with noisy labels. To investigate unsupervised domain adaptation, we consider four domains of digit datasets: SVHN, MNIST, USPS, and MNIST-M. For small datasets, each image is resized to a standardized size of 32\u00d7 32 with 3 color channels to ensure uniformity in input sizes.\nImageNet. We also include a subset of the ImageNet dataset called ImageNet-100. ImageNet ILSVRC2012 [11] consists of approximately 1.2 million training images and 50,000 validation images, divided into 1,000 classes. However, due to the limited computing resources and time, we focus specifically on the first 100 classes of ImageNet, forming the ImageNet-100 dataset. Each image in this subset is resized to dimensions of 64\u00d7 64 with 3 color channels.\nDomainNet. For unsupervised domain adaptation, we employ the DomainNet dataset [40]. DomainNet is a large-scale dataset designed specifically for unsupervised domain adaptation tasks. It contains approximately 600,000 images distributed among 345 classes across six domains. These six domains included in DomainNet are clipart, painting, real, sketch, infograph, and quickdraw. In our experiments, we focus on four domains from the cleaned version of DomainNet, including clipart, painting, real, and sketch. Each image in these domains is resized to dimensions of 64\u00d7 64 with 3 color channels to ensure consistency in input sizes."
        },
        {
            "heading": "B.2 TRAINING DETAILS.",
            "text": "Data Augmentation. We adopt the same image augmentation settings as used in the SimSiam method [7]. These augmentation techniques include RandomResizedCrop, ColorJitter, RandomGrayscale, GaussianBlur, and RandomHorizontalFlip.\nNetwork Architectures. For smaller datasets such as CIFAR-10/-100 and digit datasets (SVHN, MNIST, USPS, MNIST-M), we employ the PreAct-ResNet-18 architecture [22] as the backbone network. The projection layers consist of a 2-layer MLP with a hidden dimension of 2048 and an output dimension of 1024. For larger-scale datasets such as ImageNet and DomainNet, we utilize the ResNet-50 architecture [21] as the backbone network. The projection layers in this case also consist of a 2-layer MLP, with both the hidden dimension and the output dimension set to 4096.\nOptimization Details. In this paper, we train these self-supervised (or supervised) models for a total of 200 epochs. The training process utilizes the SGD optimizer with a momentum of 0.9, a learning rate of 0.1, and a weight decay of 5e-4. To dynamically adjust the learning rate throughout the 200 epochs, we use the cosine decay learning rate schedule [34]. For datasets with smaller sizes, we set the batch size to 512 on 1 GPU, while for ImageNet-100 and DomainNet, the batch size was set to 1536 on 8 GPUs. All experiments were implemented using PyTorch and executed on NVIDIA GTX 2080Ti GPUs.\nImplementation of Spectral Contrastive Loss. The pseudocode for the spectral contrastive loss, as well as its supervised version in PyTorch-style, is provided in Table 7. It is worth noting that, similar to other methods such as SimSiam [7], BYOL [16], SimCLR [6], and others, we perform \u21132-normalization on the final output to compute the loss. Additionally, followed HaoChen et al. [19], we introduce an extra hyper-parameter \u00b5 (the default value of \u00b5 is 10) to balance the positive and negative components of the loss."
        },
        {
            "heading": "B.3 EXPERIMENTS ON CONTRASTIVE LEARNING AND SUPERVISED LEARNING",
            "text": "The empirical objective of spectral contrastive loss with zero-mean regularization used in this paper is formulated as\nLssl(f ; \u03c4) = \u2212 2 n n\u2211 i=1 E x\u223cA(xi) x+\u223cA(xi) [f(x)\u22a4f(x+)] + 1 n2 n\u2211 i=1 n\u2211 j=1 E x\u223cA(xi) x\u2212\u223cA(xj) [((f(x)\u22a4f(x\u2212)) + \u03c4)2],\n(B.1) where xi denotes the i-th example in the input space, and A(x) denotes the augmentations of example x.\nThe supervised version of spectral contrastive loss with the zero-mean regularization for the labeled dataset (X , Y ) as\nLsup(f ; \u03c4) = Lsuppos (f) + Lneg(f ; \u03c4),\nLsuppos (f) = \u2212 2\nrn2 n\u2211 i=1 n\u2211 j=1 r\u2211 c=1 E x\u223cA(xi,c) x+\u223cA(xj,c) [ f(x)\u22a4f(x+) ] , and\nLneg(f ; \u03c4) = 1\nr2n2 n\u2211 i=1 n\u2211 j=1 r\u2211 c=1 r\u2211 k=1 E x\u223cA(xi,c) x\u2212\u223cA(xj,k) [( f(x)\u22a4f(x\u2212) + \u03c4 )2] ,\n(B.2)\nwhere xi,c denotes the i-th example in the class c, n is the number of examples in each class, and A(x) denotes the augmentations of example x.\nExperimental Details. We conduct experiments on self-supervised and supervised spectral contrastive learning using PreAct-ResNet-18 models pre-trained on CIFAR-10, CIFAR-100, SVHN, and a subset of ImageNet comprising the first 100 classes. The training process involves training the networks for 200 epochs. To evaluate the performance of these models in linear classification, we employ an independent linear classifier on the fixed representations obtained during contrastive\npre-training. This classifier is trained using labeled data. The reported results correspond to the top-1 accuracy achieved by these models.\nExperimental Results. Table 5 and Table 6 present the Top-1 accuracy results for self-supervised learned models and supervised learned models, respectively. These results clearly demonstrates that zero-mean regularization consistently outperforms the Baseline (models without zero-mean regularization) across most scenarios. Notably, the best performance is consistently achieved when utilizing zero-mean regularization in all cases. Similar conclusions are also reflected in Table 8 which offers the results of self-supervised learning on ImageNet and DomainNet. Moreover, Table 9 reports the results obtained using a fixed value of \u00b5 = 10 and different values of \u03c4 , which shows that excessively large values of \u03c4 can hinder the improvement achieved by zero-mean regularization. Therefore, we suggest limiting the range of \u03c4 to [0, 1]."
        },
        {
            "heading": "B.4 EXPERIMENTS ON UNSUPERVISED DOMAIN ADAPTATION",
            "text": "We conduct empirical validation of unsupervised domain adaptation using spectral contrastive pretraining on four digit datasets: SVHN (S) [36], MNIST (M) [28], USPS (U) [25], and MNIST-M (M-M) [14]. To perform the pretraining, we utilize the spectral contrastive loss with zero-mean regularization on the PreAct-ResNet18 architecture. The models are pretrained over all the domains for 200 epochs. Subsequently, we train these pretrained models using a linear classifier on the source domains for 10 epochs. The learning rate is set to 0.005, and the weight decay is set to 1e-5. The results of these experiments are reported in Table 2.\nFor the experiments on the DomainNet dataset, we follow a similar approach. We pretrain the ResNet-50 architecture over four domains of DomainNet for 200 epochs. Then, we train a linear classifier on the source domains for 100 epochs with base size 2048. The learning rate is set to 0.1, and the weight decay is set to 1e-5. The results of these experiments are provided in Table 10.\nExperimental Results. As depicted in Table 10, the results demonstrate that unsupervised domain adaptation (UDA) utilizing spectral contrastive pretraining yields superior performance compared to the standard empirical risk minimization (ERM) approach. This indicates that spectral contrastive pretraining effectively decouples class information across different domains, leading to improved adaptation. Furthermore, the results indicate that utilizing zero-mean regularization with different values of \u03c4 (0.1, 0.2, and 0.5) outperforms the baselines. This suggests that zero-mean regularization enables the learning of more discriminative representations, resulting in enhanced performance in the unsupervised domain adaptation task."
        },
        {
            "heading": "B.5 EXPERIMENTS ON LEARNING WITH NOISY LABELS",
            "text": "For the experiments on learning with noisy labels, we propose to employ a mixed spectral contrastive loss to mitigate label noise, i.e., using Lssl + 0.5Lsupnoise. The self-supervised part only aligns positive pairs (two random augmentations from the same sample), while the supervised part utilizes positive pairs according to noisy labels.\nNoise generation. The noisy labels are generated following standard approaches in works [39, 59]. For symmetric label noise, we corrupt the training labels by flipping labels in each class randomly to incorrect labels to incorrect labels in other classes with probability \u03b7 \u2208 {0.2, 0.4, 0.6, 0.8}.\nExperimental Results. Table 11 presents the Top-1 validation accuracy in percentage (mean\u00b1std of the last five epochs) for various levels of symmetric label noise. As can be seen, the presence of \u03c4 \u2264 1 consistently improves upon the baseline (\u03c4 = 0) across all scenarios."
        },
        {
            "heading": "B.6 EXPERIMENTS ON \u201cCLASS-MEAN FEATURES AS CLASSIFIER\u201d",
            "text": "To demonstrate the feasibility of employing class-mean features as the classifier following pre-training the representation model f with zero-regularized spectral contrastive losses in the self-supervised or supervised scenarios, we derive the class means {h\u0302c}rc=1 directly from the average features of each class in training set. We subsequently evaluate its performance using the class prediction rule argmaxc h\u0302\u22a4f(x) for each test sample x. As observed in Table 12, our method, termed as \u201cclass-mean Features as classifiers\u201d, achieves comparable performance to the conventional linear probing method which trains a linear classifier CE over tens of epochs."
        },
        {
            "heading": "Method Self-Supervised Learning Supervised LearningCIFAR-10 CIFAR-100 SVHN CIFAR-10 CIFAR-100 SVHN",
            "text": ""
        },
        {
            "heading": "C PROOF OF THEOREMS, LEMMAS AND PROPOSITIONS",
            "text": ""
        },
        {
            "heading": "C.1 PROOF OF THEOREM A.1",
            "text": "Theorem A.1 Let F be a class of functions of X \u2192 Rd and let f\u0302 = argminf\u2208F L(f ; \u03c4) be the minimizer of the spectral contrastive loss. Assume that:\n1. (\u03f5-separability). The probability of a positive pair belonging to two different sets is less than \u03f5, that is, Pr(x,x+)\u223cppos(idx \u0338= idx+) \u2264 \u03f5;\n2. (Alignment) The downstream label y(x) is a constant on each Si for i \u2208 [m]; 3. (F-implementable inner-cluster connection larger than \u03b4). For any f \u2208 F and any linear\nhead w \u2208 Rd, let function g(x) = w\u22a4f(x). For any i \u2208 [m], we have QSi(g) := E (x,x+)\u223cpSipos [(g(x)\u2212g(x+))2] E x\u223cpSi\ndata ,x\u2032\u223cpSi data\n[(g(x)\u2212g(x\u2032))2] \u2265 \u03b4;\n4. (Implementability). There exists a function f \u2208 F such that f(x) = vidx for all x \u223c pdata, where {v1, v2, ..., vm} is a set of different vectors that achieve the global minimum of \u2225 \u2211m\ni=1 piviv \u22a4 i \u2212 I\u22252F + 2\u03c4 \u00b7 \u2225 \u2211m i=1 pivi\u222522, and pi = Prx\u223cpdata(x \u2208 Si).\nLet pmin = mini\u2208[m] Prx\u223cpdata(x \u2208 Si) and \u2206 = maxi,j \u2225vi \u2212 vj\u222522. For m > d, if \u03f5\u2206 < 1, then there exists a linear head W \u2208 Rd\u00d7d which achieves the following downstream error\nEx\u223cpdata [\u2225\u2225\u2225Wf\u0302(x)\u2212 vy(x)\u2225\u2225\u22252\n2\n] \u2264 \u03f5\u2206(1 + \u221a \u03f5\u2206)pmin\n2\u03b4(pmin \u2212 \u03f5) . (C.1)\nProof. Let f\u2217 be the function f\u2217(x) = vidx . According to Lemma C.2, if m > d, the minimum of \u2225 \u2211m\ni=1 piviv \u22a4 i \u2212 I\u22252F + 2\u03c4 \u00b7 \u2225 \u2211m i=1 pivi\u222522 is 0 when \u2211m i=1 piviv \u22a4 i = I and \u2211m i=1 pivi = 0, so we\nknow that R1(f\u2217) = R2(f\u2217) = 0. For the invariance term with respect to f\u2217, we have R0(f\u2217) = Ex,x+ [ \u2225f\u2217(x)\u2212 f\u2217(x+)\u222522 ] \u2264 max\ni,j \u2225vi \u2212 vj\u222522 Pr (x,x+)\u223cppos (idx \u0338= idx+) \u2264 \u03f5\u2206. (C.2)\nDefine matrix M = Ex\u223cpdata [f\u0302(x)f\u0302(x)\u22a4], since f\u0302 = argminf\u2208F L(f ; \u03c4) is the minimizer of contrastive loss within the functional class, we have R0(f\u0302) +R1(f\u0302) + 2\u03c4R2(f\u0302) \u2264 R0(f\u2217), and further obtain\n\u2225M \u2212 I\u22252F = R1(f\u0302) \u2264 R0(f\u2217) \u2264 \u03f5\u2206 < 1. (C.3)\nThus, M is a full rank matrix, and we can define function f\u0303(x) = M\u22121/2f\u0302(x) and have that\nEx\u223cpdata [f\u0303(x)f\u0303(x)\u22a4] = Ex\u223cpdata [M\u22121/2f\u0302(x)f\u0302(x)\u22a4M\u22121/2] = I. (C.4)\nLet Q = Ex\u223cpdata [f\u0303(x)f\u2217(x)\u22a4] and \u03c0f (x) = f\u0303(x)\u2212Qf\u2217(x). Since R1(f\u2217) = 0, we know that\nEx\u223cpdata [\u03c0f (x)f\u2217(x)\u22a4] = Ex\u223cpdata [f\u0303(x)f\u2217(x)\u22a4]\u2212QEx\u223cpdata [f\u2217(x)f\u2217(x)\u22a4] = 0. (C.5)\nUsing the first and third assumptions in Theorem 3.1, we have E(x,x+)\u223cppos [\u2225\u2225\u03c0f (x)\u2212 \u03c0f (x+)\u2225\u222522]\n\u2265 \u2211 i\u2208[m] (pi \u2212 \u03f5) \u00b7 E(x,x+)\u223cpposi [\u2225\u2225\u03c0f (x)\u2212 \u03c0f (x+)\u2225\u222522]\n\u2265\u03b4 \u00b7 \u2211 i\u2208[m] (pi \u2212 \u03f5) \u00b7 Ex,x\u2032\u223cpdatai [ \u2225\u03c0f (x)\u2212 \u03c0f (x\u2032)\u2225 2 2 ] =\u03b4 \u00b7\n\u2211 i\u2208[m] ( 1\u2212 \u03f5 pi ) piEx,x\u2032\u223cpdatai [ \u2225\u03c0f (x)\u2212 \u03c0f (x\u2032)\u2225 2 2 ] \u2265\u03b4 \u00b7 ( 1\u2212 \u03f5\npmin ) \u2211 i\u2208[m] piEx,x\u2032\u223cpdatai [ \u2225\u03c0f (x)\u2212 \u03c0f (x\u2032)\u2225 2 2 ] =2\u03b4 \u00b7 ( 1\u2212 \u03f5\npmin\n) \u00b7 Ex\u223cpdata [ \u2225\u03c0f (x)\u222522 ]\n(C.6)\nOn the other hand, we have E(x,x+)\u223cppos [\u2225\u2225\u03c0f (x)\u2212 \u03c0f (x+)\u2225\u2225]\n\u2264E(x,x+)\u223cppos [\u2225\u2225\u2225f\u0303(x)\u2212 f\u0303(x+)\u2225\u2225\u2225]\n\u2264\u2225M\u22121\u2225spec \u00b7 E(x,x+)\u223cppos [\u2225\u2225\u2225f\u0302(x)\u2212 f\u0302(x+)\u2225\u2225\u2225] \u2264(1 + \u221a \u03f5\u2206) \u00b7 \u03f5\u2206.\n(C.7)\nCombining Equation C.6 and C.7 we have\nEx\u223cpdata [ \u2225\u03c0f (x)\u222522 ] \u2264 \u03f5\u2206(1 + \u221a \u03f5\u2206)pmin\n2\u03b4(pmin \u2212 \u03f5) (C.8)\nBy Lemma C.1, we know that there exists a matrix U \u2208 Rd\u00d7d such that\nEx\u223cpdata [\u2225f\u2217(x)\u2212 UM\u22121/2f\u0302(x)\u222522] \u2264 \u03f5\u2206(1 +\n\u221a \u03f5\u2206)pmin\n2\u03b4(pmin \u2212 \u03f5) . (C.9)\nThus, if we define matrix W = UM\u22121/2, then we have\nEx\u223cpdata [ \u2225vidx \u2212Wf\u0302(x)\u222522 ] \u2264 \u03f5\u2206(1 + \u221a \u03f5\u2206)pmin\n2\u03b4(pmin \u2212 \u03f5) , (C.10)\nwhich finishes the proof when d < m.\nLemma C.1. Suppose f : X \u2192 Rd and g : X \u2192 Rd are two functions defined on X such that\nEx\u223cpdata [f(x)f(x)\u22a4] = Ex\u223cpdata [g(x)g(x)\u22a4] = I. (C.11)\nDefine the projection of f onto g\u2019s orthogonal subspace as\n\u03c0f (x) = f(x)\u2212 Ex\u2032\u223cpdata [f(x\u2032)g(x\u2032)\u22a4]g(x). (C.12)\nThen, there exist matrix U \u2208 Rd\u00d7d such that\nEx\u223cpdata [\u2225g(x)\u2212 Uf(x)\u222522] = Ex\u223cpdata [\u2225\u03c0f (x)\u222522]. (C.13)\nProof. Please see the proof in HaoChen and Ma [18, Lemma B.1]. Lemma C.2. For the fixed weights p1, ..., pm > 0 satisfying \u2211m\ni=1 pi = 1 and \u03c4 \u2208 [0, 1], the global minimum of \u2225 \u2211m i=1 piviv \u22a4 i \u2212 Id\u22252F + 2\u03c4 \u00b7 \u2225 \u2211m i=1 pivi\u222522 (where d is the feature dimensionality) is\n(a) 0 when m > d, which is obtained if and only if V P [V ; 1m]\u22a4 = [Id; 0];\n(b) d\u2212m+ 2\u03c4 \u2212 \u03c42 when m \u2264 d, which is obtained if and only if V \u22a4V = P\u22121 \u2212 \u03c41m1\u22a4m,\nwhere V = [v1, ..., vm] and P = diag (p1, p2, ..., pm). Proof. Obviously, we have \u2225 \u2211m\ni=1 piviv \u22a4 i \u2212 Id\u22252F + 2\u03c4 \u00b7 \u2225 \u2211m i=1 pivi\u222522 \u2265 0, where the equality\nholds if and only if \u2211m\ni=1 piviv \u22a4 i = Id and \u03c4 \u00b7 \u2211m i=1 pivi = 0. However, if m \u2264 d, it is almost\nimpossible to achieve this condition since there are fewer unknowns than equations. Thus, we will analyze the global minimizer while considering the relation between m and d.\nIf m > d, there exist different vectors that satisfy \u2211m\ni=1 piviv \u22a4 i = Id and \u2211m i=1 pivi = 0, that is,\nV P [V \u22a4; 1\u22a4m] = [Id; 0].\nIf m \u2264 d, we have\u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i\u2208[m] piviv \u22a4 i \u2212 Id \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n+ 2\u03c4 \u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i\u2208[m] pivi \u2225\u2225\u2225\u2225\u2225\u2225 2\n2\n=Tr  \u2211\ni\u2208[m]\npiviv \u22a4 i \u2212 Id \u22a4\u2211 i\u2208[m] piviv \u22a4 i \u2212 Id  + 2\u03c4 \u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i\u2208[m] pivi \u2225\u2225\u2225\u2225\u2225\u2225 2\n2\n=Tr \u2211 i\u2208[m] \u2211 j\u2208[m] pipjviv \u22a4 i vjv \u22a4 j \u2212 2 \u2211 i\u2208[m] piviv \u22a4 i + Id + 2\u03c4 \u2211 i\u2208[m] \u2211 j\u2208[m] pipjv \u22a4 i vj\n= \u2211 i\u2208[m] \u2211 j\u2208[m] pipj(v \u22a4 i vj) 2 \u2212 2 \u2211 i\u2208[m] piv \u22a4 i vi + d+ 2\u03c4 \u2211 i\u2208[m] \u2211 j\u2208[m] pipjv \u22a4 i vj\n= \u2211 i\u2208[m] \u2211 j\u2208[m] pipj(v \u22a4 i vj + \u03c4) 2 \u2212 \u03c42 \u2212 2 \u2211 i\u2208[m] piv \u22a4 i vi + d\n=d+ \u2211 i \u0338=j pipj(v \u22a4 i vj + \u03c4) 2 \u2212 \u03c42 + \u2211 i\u2208[m] p2i (\u2225vi\u222522 + \u03c4)2 \u2212 2 \u2211 i\u2208[m] pi\u2225vi\u222522\n=d\u2212 \u03c42 + \u2211 i \u0338=j pipj(v \u22a4 i vj + \u03c4) 2 + \u2211 i\u2208[m] ( pi\u2225vi\u222522 + \u03c4pi \u2212 1 )2 + 2\u03c4 \u2212m\n=d\u2212m+ 2\u03c4 \u2212 \u03c42 + \u2211 i \u0338=j pipj(v \u22a4 i vj + \u03c4) 2 + \u2211 i\u2208[m] ( pi\u2225vi\u222522 + \u03c4pi \u2212 1 )2 .\n(C.14)\nThus, the global minimum of \u2225 \u2211m\ni=1 piviv \u22a4 i \u2212 Id\u22252F +2\u03c4 \u00b7 \u2225 \u2211m i=1 pivi\u222522 is d\u2212m+2\u03c4 \u2212 \u03c42, which\nis obtained at\nv\u22a4i vj = \u2212\u03c4, \u2200i, j \u2208 [m] and i \u0338= j, pi\u2225vi\u222522 + \u03c4pi \u2212 1 = 0, \u2200i \u2208 [m], (C.15)\nthat is, V \u22a4V = P\u22121 \u2212 \u03c41m1\u22a4m, and further \u2225 \u2211\ni\u2208[m] piviv\n\u22a4 i \u2212 Id\u22252F = d\u2212m+ \u03c42, and \u2225 \u2211 i\u2208[m] pivi\u222522 = 1\u2212 \u03c4, (C.16)\nand this equation requires that \u03c4 \u2264 infz\u2208Rm z \u22a4Pz\nz\u22a41m1\u22a4mz = 1."
        },
        {
            "heading": "C.2 PROOF OF THEOREM 3.1 AND PROPOSITION 3.2",
            "text": "Theorem 3.1 Let \u03b6 > 0 and \u03f5 \u2208 (0, 12 ) be arbitrary constants. In the above stochastic block model, assume \u03c1 > max{\u03b1, \u03b2}, \u03b3 < min{\u03b1, \u03b2} and \u03c4 < \u03bb\u03031(0)\u2212\u03bb\u0303d(0)\n\u03bb\u03031(0) . Then, there exists \u03be\u0303 \u2208 [1 \u2212 \u03f5, 1], such that for any n \u2265 \u2126 (\nrp min{\u03b1\u2212\u03b3,\u03b2\u2212\u03b3}2\n) and regularization strength \u03b7 \u2208 ( 0, (\u03b1\u2212\u03b3)\u03f52r\u03c1 ] , with a high\nprobability 1\u2212 n\u2212\u03b6 , we have\u2225\u2225\u2225predT \u2212 p\u0303redT \u2225\u2225\u2225 F \u2264 O ( \u03bb\u03031(\u03c4)\n\u03b72(\u03bb\u0303d(\u03c4)\u2212 \u03bb\u0303d+1(\u03c4))\n) \u00b7 poly(r, p), (C.17)\nwhere p\u0303redT \u225c E[predT ] is the expectation of the prediction matrix when achieving the minimum of the loss in Eq. 7 with \u03c4 \u2208 [0, 1], \u03bb\u03031(\u03c4), \u03bb\u0303d(\u03c4) and \u03bb\u0303d+1(\u03c4) are the 1-st, d-th and d+1-th eigenvalues of A\u0303 \u225c E [ A\u2212 \u03c4 |E\u0303|N2 11 \u22a4 ] , respectively.\nFurthermore, the target error can be bounded by\nPx\u223cT (pred(x) \u0338= yx) \u2264 O\n( (\u03bb\u03031(\u03c4)) 2\n\u03b74(\u03bb\u0303d(\u03c4)\u2212 \u03bb\u0303d+1(\u03c4))2 \u00b7 n\n) \u00b7 poly(r, p), (C.18)\nwhere poly(r, p) denotes a polynomial function of r and p.\nProof. Let Ad \u2208 RN\u00d7N be the rank-k approximation of the adjacency matrix A, which contains the top-d components of A\u2019s SVD decomposition. We use Ad,(T ,S) to denote the matrix by restricting Ad to the rows corresponding to the source and the columns corresponding to the target. We use Ad,(S,S) to denote the matrix by restricting Ad to the rows and columns corresponding to the source.\nLet Y \u2208 RN\u00d7r be the label matrix where on the x-th row it contains the target of the label y\u20d7x. Let YS \u2208 R|S|\u00d7r and TT \u2208 R|T |\u00d7r be the matrices by restricting Y to the source and target domains, respectively.\nWe can rewrite the spectral contrastive loss as \u2212 2 \u2211 x,x\u2032 Axx\u2032 |E| f(x)\u22a4f(x\u2032) + \u2211 x,x\u2032 1 N2 ( f(x)\u22a4f(x\u2032) + \u03c4 )2 =\n\u2225\u2225\u2225\u2225 N|E| \u00b7 (A\u2212 \u03c4 |E|N2 11\u22a4)\u2212 ( 1\u221aN \u00b7 F)( 1\u221aN \u00b7 F)\u22a4 \u2225\u2225\u2225\u22252 F + const.\n(C.19)\nLet A\u2032 = A \u2212 \u03c4 |E|N2 11 \u22a4, A\u0303 = E[A], |E\u0303| = 1\u22a4N A\u03031N be the expectation of number of edges in the graph A, A\u0303\u2032 = E[A\u2032] = E [ A\u2212 \u03c4 |E\u0303|N2 11 \u22a4 ] , and \u03bb\u0303i(\u03c4) be the i-th eigenvalue of A\u0303\u2032. By Lemma C.3, we have that the prediction on the target domain is\npredT = A \u2032 d,(T ,S) ( A\u2032d,(S,S) +\n|E| N2\n\u00b7 \u03b7 \u00b7 |S| \u00b7 I )\u2020 YS . (C.20)\nWe define the ideal prediction as\np\u0303redT = A\u0303 \u2032 d,(T ,S) ( A\u0303\u2032d,(S,S) +\n|E\u0303| N2 \u00b7 \u03b7 \u00b7 |S| \u00b7 I\n)\u2020 YS . (C.21)\nWe will bound the difference between predT and p\u0303redT . For every class c \u2208 [r], define the following error vector\nvc \u225c A\u2032d,(T ,S) ( A\u2032d,(S,S) +\n|E| N2\n\u00b7 \u03b7 \u00b7 |S| \u00b7 I )\u2020 Y cS \u2212 A\u0303\u2032d,(T ,S) ( A\u0303\u2032d,(S,S) +\n|E| N2\n\u00b7 \u03b7 \u00b7 |S| \u00b7 I )\u2020\nY cS ,\nwhere Y cS is the c-th column of YS .\nUsing the perturbation bound for the pseudoinverse matrix [46], we have\n\u2225vc\u2225 \u2264\u2225A\u2032d,(T ,S) \u2212 A\u0303 \u2032 d,(T ,S)\u2225 \u00b7 \u2225\u2225\u2225\u2225\u2225 ( A\u2032d,(S,S) + |E| N2 \u00b7 \u03b7 \u00b7 |S| \u00b7 I )\u2020\u2225\u2225\u2225\u2225\u2225 \u00b7 \u2225Y cS\u2225\n+ \u2225A\u2032d,(T ,S)\u2225 \u00b7 \u2225\u2225\u2225\u2225\u2225 ( A\u2032d,(S,S) + |E| N2 \u00b7 \u03b7 \u00b7 |S| \u00b7 I )\u2020 \u2212 ( A\u0303\u2032d,(S,S) + |E| N2 \u00b7 \u03b7 \u00b7 |S| \u00b7 I )\u2020\u2225\u2225\u2225\u2225\u2225 \u00b7 \u2225Y cS\u2225\n\u2264\n( n2\n\u03b7|E| \u00b7 |S| +\n1 + \u221a 5 2 \u00b7 (\nn2\n\u03b7|E| \u00b7 |S|\n)2 \u00b7 \u2225A\u2032d\u2225 ) \u00b7 \u2225Y cS\u2225 \u00b7 \u2225A\u2032d \u2212 A\u0303\u2032d\u2225,\n(C.22)\nwhere the first inequality is induced by the triangle inequality5, the second inequality in Eq. B.22 is based the facts that \u2225A\u2032d,(T ,S)\u2225 \u2264 \u2225A \u2032 d\u2225, \u2225A\u2032d,(T ,S) \u2212 A\u0303d,(T ,S)\u2225 \u2264 \u2225A \u2032 d \u2212 A\u0303\u2032d\u2225, \u2225(A\u2032d,(S,S) + \u03b7|E|\u00b7|S| N2 \u00b7 I) \u2020\u2225 \u2264 N 2 \u03b7|E|\u00b7|S| , and the upper bound for the difference between two pseudoinverse matrix \u2225B\u2020 \u2212A\u2020\u2225 \u2264 1+ \u221a 5\n2 max{\u2225A \u2020\u2225, \u2225B\u2020\u2225} \u00b7 \u2225B \u2212A\u2225 [46, Theorem 3.3].\nBy Lemma C.6, there exists constant C = C(\u03b6) such that \u2225A\u2032 \u2212 A\u0303\u2032\u2225 \u2264 C \u221a N with probability at least 1\u2212N\u22122\u03b6 for any N > \u2126(1/\u03c1). From now on, we assume this high probability event happens.\nFollow some notations in the proof of Lemma C.4, we have \u03bb\u0303d(\u03c4) = min{\u03bbv, \u03bbw} and \u03bb\u0303d+1(\u03c4) = \u03bbz , which is based on the facts that the eigenvalue corresponding to the eigenvector 1p \u2297 1r \u2297 1n of A\u0303\u2032 satisfies\nn\u03c1+ n(r \u2212 1)\u03b2 + n(p\u2212 1)\u03b1+ n(p\u2212 1)(r \u2212 1)\u03b3 \u2212 \u03c4 |E\u0303|N = (1\u2212 \u03c4)\u03bb\u03031(0) > \u03bb\u0303d(0) (C.23)\nsince \u03bb\u03031(0) = n\u03c1+n(r\u2212 1)\u03b2+n(p\u2212 1)\u03b1+n(p\u2212 1)(r\u2212 1)\u03b3, |E\u0303| = 1\u22a4N A\u03031N = (\u03b2\u2212 \u03b3)pr2n2 + (\u03b1\u2212 \u03b3)p2rn2 + (\u03c1\u2212 \u03b2 \u2212 \u03b1+ \u03b3)prn2 + \u03b3p2r2n2 = N\u03bb\u03031(0), and \u03c4 < \u03bb\u03031(0)\u2212\u03bb\u0303d(0)\u03bb\u03031(0) .\nLet \u2206 \u225c \u03bb\u0303d(\u03c4)\u2212\u03bb\u0303d+1(\u03c4)n = min{r(\u03b2 \u2212 \u03b3), p(\u03b1 \u2212 \u03b3)}. If our choice of N further satisfies N \u2265( 2rmC\n\u2206 )2 , we have \u2225A\u2032 \u2212 A\u0303\u2032\u2225 \u2264 12 (\u03bb\u0303d(\u03c4)\u2212 \u03bb\u0303d+1(\u03c4)), so from Lemma C.5, we have\n\u2225A\u2032d \u2212 A\u0303\u2032d\u2225 \u2264 O\n( \u03bb\u03031(\u03c4)\n\u03bb\u0303d(\u03c4)\u2212 \u03bb\u0303d+1(\u03c4) \u00b7 \u2225A\u2032 \u2212 A\u0303\u2032\u2225\n) \u2264 O ( \u03bb\u03031(\u03c4)\n\u03bb\u0303d(\u03c4)\u2212 \u03bb\u0303d+1(\u03c4) \u00b7 \u221a N\n) . (C.24)\nBy Hoeffding\u2019s inequality, with probability at least 1\u2212 2e\u22122N2 we assume ||E| \u2212 |E\u0303|| \u2264 N . From now on, we assume this high-probability event happens. The total failure probability so far is N\u22122\u03b6 + 2e\u22122N 2 \u2264 N\u2212\u03b6 . By the definition of graph, we have |E\u0303| \u2265 \u03c1N 2\nrp . If our choice of N further\nsatisfies N \u2265 2rp\u03c1 , we have |E| \u2265 \u03c1N2 2rp , thus\n|E| \u00b7 |S| N2 \u2265 \u03c1N 2rp2 . (C.25)\nSubstituting Equation C.24 and Equation C.25 into Equation C.22 gives:\n\u2225vc\u2225 \u2264 O\n( \u03bb\u03031(\u03c4)\n\u03b72(\u03bb\u0303d(\u03c4)\u2212 \u03bb\u0303d+1(\u03c4)) \u221a N\n) \u00b7 poly(r, p) \u00b7 \u2225Y cS\u2225. (C.26)\nSumming over all classes c and noticing that \u2225Y cS\u2225 \u2264 \u221a N leads to\n\u2225v\u2225F = \u221a\u221a\u221a\u221a r\u2211 c=1 \u2225vc\u22252 \u2264 r\u2211 c=1 \u2225vc\u2225 \u2264 O ( \u03bb\u03031(\u03c4) \u03b72(\u03bb\u0303d(\u03c4)\u2212 \u03bb\u0303d+1(\u03c4)) ) \u00b7 poly(r, p). (C.27)\nMoreover, let \u03be = |E|N2 \u00b7 \u03b7 \u00b7 |S| and \u03be\u0303 = |E\u0303| N2 \u00b7 \u03b7 \u00b7 |S|, we have\u2223\u2223\u2223\u2223 \u03bbw\u03bbw + p\u03be \u2212 \u03bbw\u03bbw + p\u03be\u0303 \u2223\u2223\u2223\u2223 \u2264 p\u03bbw \u00b7 |\u03be \u2212 \u03be\u0303| = \u03b7||E| \u2212 |E\u0303||\u03bbwN \u2264 poly(r, p). (C.28) By Lemma C.4, we have\u2225\u2225\u2225\u2225\u2225\u2225A\u0303\u2032d,(T ,S) ( A\u0303\u2032d,(S,S) + |E| N2 \u00b7 \u03b7 \u00b7 |S| \u00b7 I )\u2020 YS \u2212 A\u0303\u2032d,(T ,S) ( A\u0303\u2032d,(S,S) + |E\u0303| N2 \u00b7 \u03b7 \u00b7 |S| \u00b7 I )\u2020 YS \u2225\u2225\u2225\u2225\u2225\u2225 F\n= \u2223\u2223\u2223\u2223 \u03bbw\u03bbw + p\u03be \u2212 \u03bbw\u03bbw + p\u03be\u0303 \u2223\u2223\u2223\u2223 \u00b7 \u2225YT \u2225F \u2264 p\u03bbw \u00b7 |\u03be \u2212 \u03be\u0303| \u00b7 \u2225YT \u2225F \u2264 1\u221aN poly(r, p).\n(C.29) 5That is, \u2225AB \u2212 CD\u2225 = \u2225AB \u2212AD +AD \u2212 CD\u2225 \u2264 \u2225A\u2225\u2225B \u2212D\u2225+ \u2225A\u2212 C\u2225\u2225D\u2225.\nCombining Equation C.27 and Equation C.29, we have\u2225\u2225\u2225predT \u2212 p\u0303redT \u2225\u2225\u2225 F \u2264 O ( \u03bb\u03031(\u03c4)\n\u03b72(\u03bb\u0303d(\u03c4)\u2212 \u03bb\u0303d+1(\u03c4))\n) \u00b7 poly(r, p), (C.30)\nNotice that the x-th row of p\u0303redT is \u03bbw \u03bbw+p\u03b6\u0303 \u00b7 yyx . Since \u03bbw = n(\u03c1\u2212 \u03b2 + (p\u2212 1)\u03b1\u2212 (p\u2212 1)\u03b3) \u2265 1 2np(\u03b1\u2212 \u03b3), and \u03be\u0303 = |E\u0303| N2 \u00b7 \u03b7 \u00b7 |S| \u2264 \u03b7rn\u03c1, we have \u03bbw \u03bbw+p\u03b6\u0303 \u2265 1 1+ 2r\u03c1\u03b7 p(\u03b1\u2212\u03b3) \u2265 1\u2212 \u03f5, where the second inequality follows the assumption on \u03b7.\nSince predT is incorrect on the x-th row only if its difference from the x-th row of p\u0303redT has larger norm than \u2126(1 \u2212 \u03f5), we know the final total error on the target domains is bounded by O ( (\u03bb\u03031(\u03c4)) 2\n\u03b74(\u03bb\u0303d(\u03c4)\u2212\u03bb\u0303d+1(\u03c4))2\u00b7n\n) \u00b7 poly(r, p).\nCollecting all the requirements of N , this bound holds so long as N \u2265 \u2126 ((\nrp min{\u03b1\u2212\u03b3,\u03b2\u2212\u03b3)\n)2) ,\nwhich is equivalent to n \u2265 \u2126 (\nrp min{\u03b1\u2212\u03b3,\u03b2\u2212\u03b3)\n) .\nProposition 3.2 In the setting of Theorem 3.1, if \u03c4 \u2264 \u03bb\u03031(0)\u2212\u03bb\u03032(0) \u03bb\u03031(0) = n\u00b7min{p\u03b1+r(p\u22121)\u03b3,r\u03b2+p(r\u22121)\u03b3} \u03bb\u03031(0) , then, with probability at least 1\u2212 n\u2212\u03b6 , we have\nPx\u223cT (pred(x) \u0338= yx) \u2264 O\n( (1\u2212 \u03c4)2(\u03bb\u03031(0))2\n\u03b74(\u03bb\u0303d(0)\u2212 \u03bb\u0303d+1(0))2 \u00b7 n\n) \u00b7 poly(r, p), (C.31)\nwhere \u03bb\u03031(0) = n\u03c1 + n(r \u2212 1)\u03b2 + n(p \u2212 1)\u03b1 + n(p \u2212 1)(r \u2212 1)\u03b3 and \u03bb\u0303d(0) \u2212 \u03bb\u0303d+1(0) = n \u00b7min{r(\u03b2 \u2212 \u03b3), p(\u03b1\u2212 \u03b3)}.\nProof. According the proof of Theorem 3.1, we know that when \u03c4 < \u03bb\u03031(0)\u2212\u03bb\u03032(0) \u03bb\u03031(0) , the first eigenvalue of A\u0303\u2032 will be\n\u03bb\u03031(\u03c4) = n\u03c1+ n(r \u2212 1)\u03b2 + n(p\u2212 1)\u03b1+ n(p\u2212 1)(r \u2212 1)\u03b3 \u2212 \u03c4 |E\u0303|N = (1\u2212 \u03c4)\u03bb\u03031(0), (C.32)\nand \u03bb\u0303d(\u03c4) = \u03bb\u0303d(0) = n(\u03c1\u2212\u03b2\u2212\u03b1+\u03b3)+n \u00b7min{p(\u03b1\u2212\u03b3), r(\u03b2\u2212\u03b3)}, and \u03bb\u0303d+1(\u03c4) = \u03bb\u0303d+1(0) = n(\u03c1\u2212 \u03b2 \u2212 \u03b1+ \u03b3). Substituting these results into Theorem 4.1 finishes the proof.\nLemma C.3. For f\u0302 that achieves the minimum of spectral contrastive loss \u22122 \u2211\nx,x\u2032 Ax,x\u2032 C f(x) \u22a4f(x\u2032) + \u2211 x,x\u2032 1 N2 (f(x) \u22a4f(x\u2032))2 (where C > 0 is any constant), let\nb\u0302 = argminb\u2208Rd\u00d7r \u2211\nx\u2208S ( \u2225b\u22a4f\u0302(x)\u2212 y\u20d7x\u222522 + \u03b7\u2225b\u22252F ) be the linear head learned on the source\ndomain S . Let pred \u2208 RN\u00d7r be the matrix with b\u0302\u22a4f\u0302(x) on its x-th row and predT be the matrix by restricting pred to the target domain T , then we have\npredT = Ad,(T ,S) ( Ad,(S,S) + C\nN2 \u00b7 \u03b7 \u00b7 |S| \u00b7 I\n)\u2020 YS , (C.33)\nwhere (\u00b7)\u2020 is the Moore-Penrose inverse, |S| is the number of data in the source domain, and Ad is the rank-d approximation of A.\nProof. We can rewrite the spectral contrastive loss as\n\u2212 2 \u2211 x,x\u2032 Ax,x\u2032 C f(x)\u22a4f(x\u2032) + \u2211 x,x\u2032 1 N2 (f(x)\u22a4f(x\u2032))2\n= \u2225\u2225\u2225\u2225\u2225NC \u00b7A\u2212 ( 1\u221a N \u00b7 F )\u22a4( 1\u221a N \u00b7 F )\u2225\u2225\u2225\u2225\u2225 2\nF\n+ const,\n(C.34)\nwhere F \u2208 RN\u00d7d is the matrix which the x-th row contains f(x)\u22a4. According to the Eckart-YoungMirsky theorem, the minimizer of the above loss function is F = N\u221a\nC Sd where Sd \u2208 RN\u00d7d is a\nmatrix such that Ak = S\u22a4d Sd.\nLet Sd,S \u2208 R|S|\u00d7d be the matrix obtained by restricting Sd to the rows corresponding to the source data, and Sd,T be the matrix obtained by restricting Sd to the rows corresponding to the target data. The head learned on the source domain can be expressed as\nb\u0302 =argmin b\u2208Rd\u00d7r \u2211 x\u2208S ( \u2225b\u22a4f(x)\u2212 y\u20d7x\u222522 + \u03b7\u2225b\u22252F ) = \u221a C\nN \u00b7 S\u22a4d,S\n( Sd,SS \u22a4 d,S + C\nN2 \u00b7 \u03b7 \u00b7 |S| \u00b7 I\n)\u2020 YS .\n(C.35)\nTherefore, the prediction on the target domain predT is\npredT = FT b\u0302 = Sd,T S \u22a4 d,S ( Sd,SS \u22a4 d,S + C\nN2 \u00b7 \u03b7 \u00b7 |S| \u00b7 I\n)\u2020 YS\n= Ad,(T ,S) ( Ad,(S,S) + C\nN2 \u00b7 \u03b7 \u00b7 |S| \u00b7 I\n)\u2020 YS .\n(C.36)\nThis finishes the proof.\nLemma C.4. Let A\u0303 = E[A] \u2212 \u03c411\u22a4, where the adjacency matrix A is drawn from the stochastic block model in Section 4.1. Then, for any \u03be > 0, if \u03c1 > max{\u03b1, \u03b2}, min{\u03b1, \u03b2} > \u03b3 and \u03c4 < p\u03b1+r\u03b2+(pr\u2212p\u2212r)\u03b3\npr , we have\nA\u0303d,(T ,S) ( A\u0303d,(S,S) + \u03beI )\u2020 YS =\n\u03bbw \u03bbw + p\u03be \u00b7 YT , (C.37)\nwhere d = r + p\u2212 1 and \u03bbw \u225c n\u03c1\u2212 n\u03b2 + n(p\u2212 1)\u03b1\u2212 n(p\u2212 1)\u03b3. Furthermore, if we use \u03bb\u0303i to denote the i-th largest eigenvalue of A\u0303, we have \u03bb\u0303d \u2212 \u03bb\u0303d+1 = n \u00b7 min{r(\u03b2 \u2212 \u03b3), p(\u03b1 \u2212 \u03b3)} and \u03bb\u03031 \u2264 np \u00b7 r\u03c1.\nProof. By the definition of the stochastic block model, every entry A\u0303xx\u2032 is in the set of {\u03c1\u2212 \u03c4, \u03b1\u2212 \u03c4, \u03b2 \u2212 \u03c4, \u03b3 \u2212 \u03c4}, depending on whether x and x\u2032 belong to the same domain/class. We can index every node x as (dx, yx, idx), where idx \u2208 [n] is the index of x within domain dx and class yx. For any integer i \u2265 1, we use 1i to denote the i-dimensional all-one vectors, and 1i = 1i/\u22251i\u2225 be its normalized unit vector. We use Si\u22121 to denote the i-dimensional unit-norm sphere.\nIt can be verified that A\u0303 can be decomposed into the sum of several matrix Kronecker products:\nA\u0303 =(\u03b2 \u2212 \u03b3) \u00b7 Ip \u2297 (1r1\u22a4r )\u2297 (1n1\u22a4n ) + (\u03b1\u2212 \u03b3) \u00b7 (1p1\u22a4p )\u2297 Ir \u2297 (1n1\u22a4n ) + (\u03c1\u2212 \u03b2 \u2212 \u03b1+ \u03b3) \u00b7 Ip \u2297 Ir \u2297 (1n1\u22a4n ) + \u03b3 \u00b7 (1p1\u22a4p )\u2297 (1r1\u22a4r )\u2297 (1n1\u22a4n ) \u2212 \u03c4 \u00b7 (1p1\u22a4p )\u2297 (1r1\u22a4r )\u2297 (1n1\u22a4n ).\n(C.38)\nAs a result, A\u0303 has the following four sets of eigenvectors with non-zero eigenvalues:\n\u2022 1p \u2297 1r \u2297 1n. The corresponding eigenvalue is \u03bbu \u225c n\u03c1+n(r\u2212 1)\u03b2+n(p\u2212 1)\u03b1+n(p\u2212 1)(r \u2212 1)\u03b3 \u2212 npr\u03c4 .\n\u2022 u \u2297 1r \u2297 1n, where u \u2208 Sp\u22121 and u\u22a41p = 0. The corresponding eigenvalue is \u03bbv \u225c n\u03c1\u2212 n\u03b1+ n(r \u2212 1)\u03b2 \u2212 n(r \u2212 1)\u03b3.\n\u2022 1p \u2297 v \u2297 1n, where v \u2208 Sr\u22121 and v\u22a41r = 0. The corresponding eigenvalue is \u03bbw \u225c n\u03c1\u2212 n\u03b2 + n(p\u2212 1)\u03b1\u2212 n(p\u2212 1)\u03b3.\n\u2022 u \u2297 v \u2297 1n, where u \u2208 Sp\u22121, v \u2208 Sr\u22121 and u\u22a41p = 0, v\u22a41r = 0. The corresponding eigenvalue is \u03bbz \u225c n\u03c1\u2212 n\u03b2 \u2212 n\u03b1+ n\u03b3.\nSince \u03c1 > max{\u03b2, \u03b1}, min{\u03b1, \u03b2} > \u03b3 and \u03c4 < p\u03b1+r\u03b2+(pr\u2212p\u2212r)\u03b3pr , we know that\nmin{\u03bbu, \u03bbv, \u03bbw, } > \u03bbz, (C.39)\nand all these eigenvalues are positive. When d = r + p\u2212 1, A\u0303d will contain exactly the first three sets of eigenvectors, since they correspond to the top-d eigenvalues. This suggests that we can write A\u0303d as follows\nA\u0303d = \u03bbu \u00b71p1 \u22a4 p \u22971r1 \u22a4 r \u22971n1 \u22a4 n +\u03bbv \u00b7(Ip\u22121p1 \u22a4 p )\u22971r1 \u22a4 r \u22971n1 \u22a4 n +\u03bbw1p1 \u22a4 p \u2297(Ir\u22121r1 \u22a4 r )\u22971n1 \u22a4 n .\nRestricting to the source domain, we have\nA\u0303d,(S,S) = \u03bbu + (p\u2212 1)\u03bbv\np \u00b7 1r1\n\u22a4 r \u2297 1n1 \u22a4 n + \u03bbw p \u00b7 (Ir \u2212 1r1 \u22a4 r )\u2297 1n1 \u22a4 n . (C.40)\nBy the definition of pseudoinverse, we have( A\u0303d,(S,S) + \u03beI )\u2020 = ( \u03bbu+(p\u22121)\u03bbv p + \u03be )\u22121 \u00b7 1r1 \u22a4 r \u2297 1n1 \u22a4 n + ( \u03bbw p + \u03be )\u22121 \u00b7 (Ir \u2212 1r1 \u22a4 r )\u2297 1n1 \u22a4 n .\nNotice that YS satisfies (1r1 \u22a4 r \u2297 1n1 \u22a4 n )YS = 0 and ((Ir \u2212 1r1 \u22a4 r )\u2297 1n1 \u22a4 n )YS = YS , we have\n(A\u0303d,(S,S) + \u03beI) \u2020YS = ( \u03bbw p + \u03be )\u22121 YS . (C.41)\nWe can also write A\u0303d,(X ,S) in the form of kronecker products as follows:\nA\u0303d,(X ,S) = \u03bbu p \u00b71p\u22971r1 \u22a4 r \u22971n1 \u22a4 n +\u03bbv(e1\u2212 1p1p)\u22971r1 \u22a4 r \u22971n1 \u22a4 n + \u03bbw p 1p\u2297 (Ir \u22121r1 \u22a4 r )\u22971n1 \u22a4 n .\nAgain, using the fact that (1r1 \u22a4 r \u2297 1n1 \u22a4 n )YS = 0 and ((Ir \u2212 1r1 \u22a4 r )\u2297 1n1 \u22a4 n )YS = YS , we have\nA\u0303d,(X ,S) ( A\u0303d,(S,S) + \u03beI )\u2020 YS =\n\u03bbw \u03bbw + p\u03be 1p \u2297 YS . (C.42)\nFinally, noticing that 1p \u2297 YS = Y finishes the proof.\nLemma C.5. Let Ad and A\u0303d be the rank-d approximations of A and A\u0303, respectively. Let \u03bb\u0303i be the i-th largest eigenvalue of A\u0303, \u2225 \u00b7 \u2225 be the operator norm of a matrix or \u21132-norm of a vector. Then when \u2225A\u2212 A\u0303\u2225 < \u03bb\u0303d \u2212 \u03bb\u0303d+1, we have\n\u2225Ad \u2212 A\u0303d\u2225 \u2264 ( 1 +\n2\u2225A\u2212 A\u0303\u2225+ 2\u2225A\u0303\u2225 (\u03bb\u0303d \u2212 \u03bb\u0303d+1)\u2212 \u2225A\u2212 A\u0303\u2225\n) \u00b7 \u2225A\u2212 A\u0303\u2225. (C.43)\nProof. Please see the proof in Shen et al. [43, Lemma 3].\nLemma C.6. Let A be the adjacency matrix of a random graph on N nodes in which edges occur independently. Let E[A] = A\u0303 be the expectation adjacency matrix and assume that N maxi,j A\u0303ij \u2265 logN . Then, for any \u03be > 0, there exists a constant C = C(\u03be) such that\n\u2225A\u2212 P\u2225 \u2264 C \u221a N (C.44)\nwith probability at least 1\u2212N\u22122\u03be."
        },
        {
            "heading": "C.3 PROOF OF THEOREM 3.3",
            "text": "Theorem 3.3 The global minimum of the supervised spectral contrastive loss Lsup(f ; \u03c4) in Equation 3.8 is uniquely obtained at f(x) = h\u0302c, for any i, c \u2208 [r] and x \u223c A(xi,c), where H\u0302 = [h\u03021, ..., h\u0302r] is the minimizer of \u2225\u2225(rI\u2212 \u03c411\u22a4)\u2212H\u22a4H\u2225\u22252 F\n. More specifically, if r \u2264 d+ 1, H\u0302 satisfies H\u0302\u22a4H\u0302 = rI\u2212 \u03c411\u22a4; if r > d+1, we have H\u0302\u22a4H\u0302 = Pd(rI\u2212 \u03c411\u22a4), where Pd(X) denotes the best d-rank approximation of X .\nProof. Let hi,c = Ex\u223cA(xi,c)[f(x)], hc = 1 n \u2211n i=1 hi,c, and H = [h1, h2, ..., hr].\nFor the positive part Lsuppos , we have\nLsuppos = \u2212 2\nrn2 n\u2211 i=1 n\u2211 j=1 r\u2211 c=1 ( Ex\u223cA(xi,c)[f(x)] )\u22a4 (Ex+\u223cA(xj,c)[f(x+)])\n= \u2212 2 rn2 r\u2211 c=1\n( n\u2211\ni=1\nEx\u223cA(xi,c)[f(x)] )\u22a4 n\u2211 j=1 Ex+\u223cA(xj,c)[f(x +)]  = \u2212 2\nrn2 r\u2211 c=1\n( n\u2211\ni=1\nhi,c )\u22a4 n\u2211 j=1 hj,c  = \u22122\nr r\u2211 c=1 \u2225hc\u222522\n(C.45)\nFor the negative part Lneg , we have\nLneg = 1\nr2n2 n\u2211 i=1 n\u2211 j=1 r\u2211 c=1 r\u2211 k=1 E x\u223cA(xi,c) x\u2212\u223cA(xj,k) [( f(x)\u22a4f(x\u2212) + \u03c4 )2]\n\u2265 1 r2 r\u2211 c=1 r\u2211 k=1  1 n2 n\u2211 i=1 n\u2211 j=1 E x\u223cA(xi,c) x\u2212\u223cA(xj,k) [ f(x)\u22a4f(x\u2212) + \u03c4 ]2\n= 1\nr2 r\u2211 c=1 r\u2211 k=1 ( h\u22a4c hk + \u03c4 )2 ,\n(C.46)\nwhere the equality holds if and only if f(x)\u22a4f(x\u2212) is a constant for any i, j \u2208 [r], x \u223c A(xi,c), and x\u2212 \u223c A(xj,k). Combining Equation C.45 and Equation C.46, we have\nLsup(f ; \u03c4) \u2265 \u22122 r r\u2211 c=1 \u2225hc\u222522 + 1 r2 r\u2211 c=1 r\u2211 k=1 (h\u22a4c hk + \u03c4) 2\n= 1 r2 \u2225\u2225(rI\u2212 \u03c411\u22a4)\u2212H\u22a4H\u2225\u22252 F + const\n(C.47)\nAccording to Eckart-Young-Mirsky theorem [13], the global minimizer H\u0302 of\u2225\u2225(rI\u2212 \u03c411\u22a4)\u2212H\u22a4H\u2225\u22252 F is the best d-rank approximation of rI\u2212 \u03c411\u22a4.\nMore specifically, if r \u2264 d + 1, H\u0302 satisfies H\u0302\u22a4H\u0302 = rI \u2212 \u03c411\u22a4; if r > d + 1, we have H\u0302\u22a4H\u0302 = Pd(rI\u2212 \u03c411\u22a4), where Pd(X) denotes the best d-rank approximation of X ."
        },
        {
            "heading": "C.4 PROOF OF THEOREM 3.4",
            "text": "Theorem 3.4 For the noise transition matrix W defined above, the global minimum of Lsupnoise(f ; \u03c4) is uniquely obtained at \u2200i \u2208 [n],\u2200c \u2208 [r],\u2200x \u223c A(xi,c), f(x) = h\u0302c, where H\u0302 = [h\u03021, ..., h\u0302r]\u22a4 is the minimizer of \u2225\u2225(rW \u2212 \u03c411\u22a4)\u2212H\u22a4H\u2225\u22252 F .\nProof. According to the definition of the label noise model, we know that\nLsupnoise(f ; \u03c4) = L sup noise,pos(f ; \u03c4) + L sup noise,neg(f ; \u03c4) (C.48)\nwhere\nLsupnoise,pos(f ; \u03c4) = \u2212 2\ncn2 n\u2211 i=1 n\u2211 j=1 r\u2211 c=1 ( Ex\u223cA(xi,c)[f(x)] )\u22a4( r\u2211 c\u2032=1 wcc\u2032Ex+\u223cA(xj,c\u2032 )f(x +) )\n= \u22122 r \u2211 c=1 h\u22a4c\n( r\u2211\nc\u2032=1\nwcc\u2032h \u2032 c\n) = \u22122\nr r\u2211 c=1 r\u2211 c\u2032=1 wcc\u2032h \u22a4 c hc\u2032\n(C.49) and\nLsupnoise,neg(f ; \u03c4) = 1\nr2n2 n\u2211 i=1 n\u2211 j=1 r\u2211 c=1 r\u2211 k=1 r\u2211 c\u2032=1 wkc\u2032E x\u223cA(xi,c) x\u2212\u223cA(xj,c\u2032 ) [( f(x)\u22a4f(x\u2212) + \u03c4 )2] = 1\nrn2 n\u2211 i=1 n\u2211 j=1 r\u2211 c=1 r\u2211 c\u2032=1 r\u2211 k=1 wkc\u2032E x\u223cA(xi,c) x\u2212\u223cA(xj,c\u2032 ) [( f(x)\u22a4f(x\u2212) + \u03c4 )2] = 1\nrn2 n\u2211 i=1 n\u2211 j=1 r\u2211 c=1 r\u2211 c\u2032=1 E x\u223cA(xi,c) x\u2212\u223cA(xj,c\u2032 ) [( f(x)\u22a4f(x\u2212) + \u03c4 )2] \u2265 1\nr2 r\u2211 c=1 r\u2211 c\u2032=1 (h\u22a4c hc\u2032 + \u03c4) 2,\n(C.50)\nwhere the second equality is based on the fact that \u2211r\nk=1 wkc\u2032 = 1. We further have\nLsupnoise(f ; \u03c4) \u2265 1 r2 \u2225\u2225(rW \u2212 \u03c411\u22a4)\u2212H\u22a4H\u2225\u22252 F + const. (C.51)\nThis finishes the proof."
        },
        {
            "heading": "C.5 PROOF OF PROPOSITION 3.5",
            "text": "Proposition 3.5 Considering the symmetric label noise [15] in which wcc\u2032 = 1\u2212 (r\u22121)\u03c9 for c = c\u2032, wcc\u2032 = \u03c9 for c \u0338= c\u2032, and \u03c9 < 1r . If \u03c4 \u2265 r\u03c9, let f\u0302 = argminf L sup noise(f ; \u03c4), then 1\u221a 1\u2212r\u03c9 \u00b7 f\u0302 is also\nthe global minimizer of Lsup ( f ; \u03c4\u2212r\u03c91\u2212r\u03c9 ) .\nProof. The symmetric label noise model means that W = (1\u2212 r\u03c9)I + \u03c911\u22a4, according to Theorem 4.4, we know that \u2200i \u2208 [n], \u2200c \u2208 [r], \u2200x \u223c A(xi,c), f\u0302(x) = h\u0302c, where H\u0302 = [h\u03021, ..., h\u0302r] is the minimizer of \u2225\u2225(rW \u2212 \u03c411\u22a4)\u2212H\u22a4H\u2225\u22252\nF = \u2225\u2225(r(1\u2212 r\u03c9)I\u2212 (\u03c4 \u2212 r\u03c9)11\u22a4)\u2212H\u22a4H\u2225\u22252\nF =(1\u2212 r\u03c9)2 \u2225\u2225\u2225\u2225(rI\u2212 \u03c4\u2212r\u03c91\u2212r\u03c9 11\u22a4)\u2212 ( 1\u221a1\u2212r\u03c9 \u00b7H)\u22a4 ( 1\u221a1\u2212r\u03c9 \u00b7H) \u2225\u2225\u2225\u22252 F ,\n(C.52)\nthus, 1\u221a 1\u2212r\u03c9 \u00b7 f\u0302(x) is also the minimizer of L sup(f ; \u03c4\u2212r\u03c91\u2212r\u03c9 ) in accordance with Theorem 3.3."
        }
    ],
    "title": "ZERO-MEAN REGULARIZED SPECTRAL CONTRASTIVE LEARNING: IMPLICITLY MITIGATING WRONG CON-",
    "year": 2024
}