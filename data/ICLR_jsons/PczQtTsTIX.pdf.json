{
    "abstractText": "Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce CrossQ: A lightweight algorithm for continuous control tasks that makes careful use of Batch Normalization and removes target networks to surpass the current state-of-the-art in sample efficiency while maintaining a low UTD ratio of 1. Notably, CrossQ does not rely on advanced bias-reduction schemes used in current methods. CrossQ\u2019s contributions are threefold: (1) it matches or surpasses current state-of-the-art methods in terms of sample efficiency, (2) it substantially reduces the computational cost compared to REDQ and DroQ, (3) it is easy to implement, requiring just a few lines of code on top of SAC.",
    "authors": [
        {
            "affiliations": [],
            "name": "Aditya Bhatt"
        },
        {
            "affiliations": [],
            "name": "Daniel Palenicek"
        },
        {
            "affiliations": [],
            "name": "Boris Belousov"
        },
        {
            "affiliations": [],
            "name": "Artemij Amiranashvili"
        },
        {
            "affiliations": [],
            "name": "Thomas Brox"
        },
        {
            "affiliations": [],
            "name": "Jan Peters"
        }
    ],
    "id": "SP:f41537c5edf4501e652c702f1f0b756e1620e784",
    "references": [
        {
            "authors": [
                "Rishabh Agarwal",
                "Max Schwarzer",
                "Pablo Samuel Castro",
                "Aaron Courville",
                "Marc G Bellemare"
            ],
            "title": "Deep reinforcement learning at the edge of the statistical precipice",
            "venue": "In Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Xinyue Chen",
                "Che Wang",
                "Zijian Zhou",
                "Keith Ross"
            ],
            "title": "Randomized ensembled double Q-learning: Learning fast without a model",
            "venue": "In International conference on learning representations,",
            "year": 2021
        },
        {
            "authors": [
                "Jianqing Fan",
                "Zhaoran Wang",
                "Yuchen Xie",
                "Zhuoran Yang"
            ],
            "title": "A theoretical analysis of deep Qlearning",
            "venue": "In Learning for dynamics and control,",
            "year": 2020
        },
        {
            "authors": [
                "Scott Fujimoto",
                "Herke Hoof",
                "David Meger"
            ],
            "title": "Addressing function approximation error in actorcritic methods",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Kristian Hartikainen",
                "George Tucker",
                "Sehoon Ha",
                "Jie Tan",
                "Vikash Kumar",
                "Henry Zhu",
                "Abhishek Gupta",
                "Pieter Abbeel"
            ],
            "title": "Soft actor-critic algorithms and applications",
            "venue": "arXiv preprint arXiv:1812.05905,",
            "year": 2018
        },
        {
            "authors": [
                "Hado Hasselt"
            ],
            "title": "Double Q-learning",
            "venue": "In Advances in neural information processing systems,",
            "year": 2010
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Tairan He",
                "Yuge Zhang",
                "Kan Ren",
                "Minghuan Liu",
                "Che Wang",
                "Weinan Zhang",
                "Yuqing Yang",
                "Dongsheng Li"
            ],
            "title": "Reinforcement learning with automated auxiliary loss search",
            "venue": "In Advances in neural information processing systems,",
            "year": 2022
        },
        {
            "authors": [
                "Takuya Hiraoka",
                "Takahisa Imagawa",
                "Taisei Hashimoto",
                "Takashi Onishi",
                "Yoshimasa Tsuruoka"
            ],
            "title": "Dropout q-functions for doubly efficient reinforcement learning",
            "venue": "In International conference on learning representations,",
            "year": 2021
        },
        {
            "authors": [
                "Sergey Ioffe"
            ],
            "title": "Batch renormalization: Towards reducing minibatch dependence in batch-normalized models",
            "venue": "In Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Michael Janner",
                "Justin Fu",
                "Marvin Zhang",
                "Sergey Levine"
            ],
            "title": "When to trust your model: Modelbased policy optimization",
            "venue": "In Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Seungchan Kim",
                "Kavosh Asadi",
                "Michael L. Littman",
                "George Dimitri Konidaris"
            ],
            "title": "Deepmellow: Removing the need for a target network in deep Q-learning",
            "venue": "In International joint conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In International conference on learning representations,",
            "year": 2015
        },
        {
            "authors": [
                "Misha Laskin",
                "Kimin Lee",
                "Adam Stooke",
                "Lerrel Pinto",
                "Pieter Abbeel",
                "Aravind Srinivas"
            ],
            "title": "Reinforcement learning with augmented data",
            "venue": "In Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Qiyang Li",
                "Aviral Kumar",
                "Ilya Kostrikov",
                "Sergey Levine"
            ],
            "title": "Efficient deep reinforcement learning requires regulating overfitting",
            "venue": "In International conference on learning representations,",
            "year": 2022
        },
        {
            "authors": [
                "Timothy P Lillicrap",
                "Jonathan J Hunt",
                "Alexander Pritzel",
                "Nicolas Heess",
                "Tom Erez",
                "Yuval Tassa",
                "David Silver",
                "Daan Wierstra"
            ],
            "title": "Continuous control with deep reinforcement learning",
            "venue": "In International conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "Hyesu Lim",
                "Byeonggeun Kim",
                "Jaegul Choo",
                "Sungha Choi"
            ],
            "title": "TTN: A domain-shift aware batch normalization in test-time adaptation",
            "venue": "In International conference on learning representations,",
            "year": 2023
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Andrei A Rusu",
                "Joel Veness",
                "Marc G Bellemare",
                "Alex Graves",
                "Martin Riedmiller",
                "Andreas K Fidjeland",
                "Georg Ostrovski"
            ],
            "title": "Human-level control through deep reinforcement learning",
            "venue": "Nature, 518(7540):529\u2013533,",
            "year": 2015
        },
        {
            "authors": [
                "M. Morales"
            ],
            "title": "Grokking deep reinforcement learning",
            "venue": "Manning Publications,",
            "year": 2020
        },
        {
            "authors": [
                "Evgenii Nikishin",
                "Max Schwarzer",
                "Pierluca D\u2019Oro",
                "Pierre-Luc Bacon",
                "Aaron Courville"
            ],
            "title": "The primacy bias in deep reinforcement learning",
            "venue": "In International conference on machine learning,",
            "year": 2022
        },
        {
            "authors": [
                "Kei Ota",
                "Tomoaki Oiki",
                "Devesh Jha",
                "Toshisada Mariyama",
                "Daniel Nikovski"
            ],
            "title": "Can increasing input dimensionality improve deep reinforcement learning",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Kei Ota",
                "Devesh K Jha",
                "Asako Kanezaki"
            ],
            "title": "Training larger networks for deep reinforcement learning",
            "venue": "arXiv preprint arXiv:2102.07920,",
            "year": 2021
        },
        {
            "authors": [
                "Quang Pham",
                "Chenghao Liu",
                "Steven HOI"
            ],
            "title": "Continual normalization: Rethinking batch normalization for online continual learning",
            "venue": "In International conference on learning representations,",
            "year": 2022
        },
        {
            "authors": [
                "Matthias Plappert",
                "Marcin Andrychowicz",
                "Alex Ray",
                "Bob McGrew",
                "Bowen Baker",
                "Glenn Powell",
                "Jonas Schneider",
                "Josh Tobin",
                "Maciek Chociej",
                "Peter Welinder"
            ],
            "title": "Multi-goal reinforcement learning: Challenging robotics environments and request for research",
            "venue": "arXiv preprint arXiv:1802.09464,",
            "year": 2018
        },
        {
            "authors": [
                "Martin L Puterman"
            ],
            "title": "Markov decision processes: Discrete stochastic dynamic programming",
            "year": 2014
        },
        {
            "authors": [
                "Antonin Raffin",
                "Ashley Hill",
                "Adam Gleave",
                "Anssi Kanervisto",
                "Maximilian Ernestus",
                "Noah Dormann"
            ],
            "title": "Stable-baselines3: Reliable reinforcement learning implementations",
            "venue": "Journal of machine learning research,",
            "year": 2021
        },
        {
            "authors": [
                "Shibani Santurkar",
                "Dimitris Tsipras",
                "Andrew Ilyas",
                "Aleksander Madry"
            ],
            "title": "How does batch normalization help optimization? In Advances in neural information processing systems, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Max Schwarzer",
                "Ankesh Anand",
                "Rishab Goel",
                "R Devon Hjelm",
                "Aaron Courville",
                "Philip Bachman"
            ],
            "title": "Data-efficient reinforcement learning with self-predictive representations",
            "venue": "In International conference on learning representations,",
            "year": 2021
        },
        {
            "authors": [
                "Saurabh Singh",
                "Abhinav Shrivastava"
            ],
            "title": "Evalnorm: Estimating batch normalization statistics for evaluation",
            "venue": "In International conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Laura Smith",
                "Ilya Kostrikov",
                "Sergey Levine"
            ],
            "title": "A walk in the park: Learning to walk in 20 minutes with model-free reinforcement learning",
            "venue": "arXiv preprint arXiv:2208.07860,",
            "year": 2022
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "title": "Dropout: A simple way to prevent neural networks from overfitting",
            "venue": "Journal of machine learning research,",
            "year": 1929
        },
        {
            "authors": [
                "Cecilia Summers",
                "Michael J. Dinneen"
            ],
            "title": "Four things everyone should know to improve batch normalization",
            "venue": "In International conference on learning representations,",
            "year": 2020
        },
        {
            "authors": [
                "Emanuel Todorov",
                "Tom Erez",
                "Yuval Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "In International conference on intelligent robots and systems,",
            "year": 2012
        },
        {
            "authors": [
                "Ge Yang",
                "Anurag Ajay",
                "Pulkit Agrawal"
            ],
            "title": "Overcoming the spectral bias of neural value approximation",
            "venue": "In International conference on learning representations,",
            "year": 2021
        },
        {
            "authors": [
                "Denis Yarats",
                "Rob Fergus",
                "Alessandro Lazaric",
                "Lerrel Pinto"
            ],
            "title": "Mastering visual continuous control: Improved data-augmented reinforcement learning",
            "venue": "In International conference on learning representations,",
            "year": 2021
        },
        {
            "authors": [
                "Brian D Ziebart",
                "Andrew L Maas",
                "J Andrew Bagnell",
                "Anind K Dey"
            ],
            "title": "Maximum entropy inverse reinforcement learning",
            "venue": "In AAAI conference on artificial intelligence,",
            "year": 2008
        }
    ],
    "sections": [
        {
            "text": "1 INTRODUCTION\nSample efficiency is a crucial concern when applying Deep Reinforcement Learning (Deep RL) methods on real physical systems. One of the first successful applications of Deep RL to a challenging problem of quadruped locomotion was achieved using Soft ActorCritic (SAC, Haarnoja et al. (2018a)), allowing a robot dog to learn to walk within 2h of experience (Haarnoja et al., 2018b). Subsequently, it was noted that the critic in SAC may be underfitted, as only a single gradient update step on the network parameters is performed for each environment step. Therefore, Randomized Ensembled Double Q-Learning (REDQ, Chen et al. (2021)) was proposed, which increased this number of gradient steps, termed update-to-data (UTD) ratio. In addition, Dropout Q functions (DroQ, Hiraoka et al. (2021)) improved the computational efficiency of REDQ while maintaining the same sample efficiency by replacing its ensemble of critics with dropout. This enabled learning quadruped locomotion in a mere 20min (Smith et al., 2022). Thus, REDQ and DroQ represent the state-of-the-art in terms of sample efficiency in Deep RL for continuous control.\nImportantly, both REDQ and DroQ showed that naively increasing the UTD ratio of SAC does not perform well due to the critic networks\u2019 Q value estimation bias. Therefore, ensembling techniques were introduced for bias reduction (explicit ensemble in REDQ and implicit\nensemble via dropout in DroQ), which allowed increasing the UTD to 20 critic updates per environment step. Higher UTD ratios improve sample efficiency by paying the price of increased computational cost, which manifests in higher wallclock time and energy consumption. It is, therefore, desirable to seek alternative methods that achieve the same or better sample efficiency at a lower computational cost, e.g., by using lower UTDs.\nIt turns out that even UTD = 1 can perform surprisingly well if other algorithmic components are adjusted appropriately. In this paper, we introduce CrossQ, a lightweight algorithm that achieves superior performance by removing much of the algorithmic design complexity that was added over the years, culminating in the current state-of-the-art methods. First, it removes target networks, an ingredient widely believed to slow down training in exchange for stability (Mnih et al., 2015; Lillicrap et al., 2016; Kim et al., 2019; Fan et al., 2020). Second, we find that Batch Normalization variants (Ioffe & Szegedy (2015); Ioffe (2017)), when applied in a particular manner, effectively stabilize training and significantly improve sample efficiency. This contradicts others\u2019 observations that it hurts the learning performance in Deep RL, e.g. Hiraoka et al. (2021). Third, CrossQ uses wider critic layers, motivated by prior research on the ease of optimization of wider networks (Ota et al., 2021). In addition to the first two improvements, wider networks enable even higher returns.\nContributions. (1) We present the CrossQ algorithm, which matches or surpasses the current state-of-the-art for model-free off-policy RL for continuous control environments with state observations in sample efficiency while being multiple times more computationally efficient; (2) By removing target networks, we are able to successfully accelerate off-policy Deep RL with BatchNorm; (3) We provide empirical investigations and hypotheses for CrossQ\u2019s success. CrossQ\u2019s changes mainly pertain to the deep network architecture of SAC; therefore, our study is chiefly empirical: through a series of ablations, we isolate and study the contributions of each part. We find that CrossQ matches or surpasses the state-of-the-art algorithms in sample efficiency while being up to 4\u00d7 faster in terms of wallclock time without requiring critic ensembles, target networks, or high UTD ratios. We provide the CrossQ source code at github.com/adityab/CrossQ."
        },
        {
            "heading": "2 BACKGROUND",
            "text": ""
        },
        {
            "heading": "2.1 OFF-POLICY REINFORCEMENT LEARNING AND SOFT ACTOR-CRITIC",
            "text": "We consider a discrete-time Markov Decision Process (MDP, Puterman (2014)), defined by the tuple \u27e8S,A,P,R, \u03c1, \u03b3\u27e9 with state space S, action space A, transition probability st+1 \u223c P(\u00b7|st,at), reward function rt = R(st,at), initial state distribution s0 \u223c \u03c1 and discount factor \u03b3 \u2208 [0, 1). RL describes the problem of an agent learning an optimal policy \u03c0 for a given MDP. At each time step t, the agent receives a state st and interacts with the environment according to its policy \u03c0. We focus on the Maximum Entropy RL setting (Ziebart et al., 2008), where the agent\u2019s objective is to find the optimal policy \u03c0\u2217, which maximizes the expected cumulative reward while keeping the entropy H high; argmax\u03c0\u2217 Es0\u223c\u03c1 [ \u2211\u221e t=0 \u03b3\nt(rt \u2212 \u03b1H(\u03c0( \u00b7 |st)))] . The action-value function is defined by Q(s,a) = E\u03c0,P [ \u2211\u221e t=0 \u03b3\nt(rt \u2212 \u03b1 log \u03c0(at|st))|s0 = s,a0 = a] and describes the expected reward when taking action a in state s. Soft Actor-Critic (SAC, (Haarnoja et al., 2018a)) is a popular algorithm that solves the MaxEnt RL problem. SAC parametrizes the Q function and policy as neural networks and trains two independent versions of the Q function, using the minimum of their estimates to compute the regression targets for Temporal Difference (TD) learning. This clipped double-Q trick, originally proposed by Fujimoto et al. (2018) in TD3, helps in reducing the potentially destabilizing overestimation bias inherent in approximate Q-learning (Hasselt, 2010)."
        },
        {
            "heading": "2.2 HIGH UPDATE-TO-DATA RATIOS, REDQ, AND DROQ",
            "text": "Despite its popularity among practitioners and as a foundation for other more complex algorithms, SAC leaves much room for improvement in terms of sample efficiency. Notably, SAC performs exactly one gradient-based optimization step per environment interaction. SAC\u2019s UTD = 1 setting is analogous to simply training for fewer epochs in supervised learning. Therefore, in recent years, gains in sample efficiency within RL have been achieved through increasing the UTD ratio (Janner et al., 2019; Chen et al., 2021; Hiraoka et al., 2021; Nikishin et al., 2022). Different algorithms, however, substantially vary in their approaches to achieving high UTD ratios. Janner et al. (2019)\nuses a model to generate synthetic data, which allows for more overall gradient steps. Nikishin et al. (2022) adopt a simpler approach: they increase the number of gradient steps while periodically resetting the policy and critic networks to fight premature convergence to local minima. We now briefly outline the two high-UTD methods to which we compare CrossQ.\nREDQ. Chen et al. (2021) find that merely raising SAC\u2019s UTD ratio hurts performance. They attribute this to the accumulation of the learned Q functions\u2019 estimation bias over multiple update steps\u2014despite the clipped double-Q trick\u2014which destabilizes learning. To remedy this bias more strongly, they increase the number of Q networks from two to an ensemble of 10. Their method, called REDQ, permits stable training at high UTD ratios up to 20.\nDroQ. Hiraoka et al. (2021) note that REDQ\u2019s ensemble size, along with its high UTD ratio, makes training computationally expensive. They instead propose using a smaller ensemble of Q functions equipped with Dropout (Srivastava et al., 2014), along with Layer Normalization (Ba et al., 2016) to stabilize training in response to the noise introduced by Dropout. Called DroQ, their method is computationally cheaper than REDQ, yet still expensive due to its UTD ratio of 20.\n3 THE CROSSQ ALGORITHM\nIn this paper, we challenge this current trend of high UTD ratios and demonstrate that we can achieve competitive sample efficiency at a much lower computational cost with a UTD = 1 method. CrossQ is our new state-of-the-art off-policy actor-critic algorithm. Based on SAC, it uses purely networkarchitectural engineering insights from deep learning to accelerate training. As a result, it(((((hhhhhcrosses out much of the algorithmic design complexity that was added over the years and which led to the current state-of-the-art methods. In doing so, we present a much simpler yet more efficient algorithm. In the following paragraphs, we introduce the three design choices that constitute CrossQ."
        },
        {
            "heading": "3.1 DESIGN CHOICE 1: REMOVING TARGET NETWORKS",
            "text": "Mnih et al. (2015) originally introduced target networks to stabilize the training of value-based offpolicy RL methods, and today, most algorithms require them (Lillicrap et al., 2016; Fujimoto et al., 2018; Haarnoja et al., 2018a). SAC updates the critics\u2019 target networks with Polyak Averaging\n\u03b8\u25e6 \u2190 (1\u2212 \u03c4)\u03b8\u25e6 + \u03c4\u03b8, (1) where \u03b8\u25e6 are the target network parameters, and \u03b8 are those of the trained critic. Here \u03c4 is the target network smoothing coefficient; with a high \u03c4 = 1 (equivalent to cutting out the target network), SAC training can diverge, leading to explosive growth in \u03b8 and the Q predictions. Target networks stabilize training by explicitly delaying value function updates, arguably slowing down online learning (Plappert et al., 2018; Kim et al., 2019; Morales, 2020).\nRecently, Yang et al. (2021) found that critics with Random Fourier Features can be trained without target networks, suggesting that the choice of layer activations affects the stability of training. Our experiments in Section 4.4 uncover an even simpler possibility: using bounded activation functions or feature normalizers is sufficient to prevent critic divergence in the absence of target networks, whereas the common choice of relu without normalization diverges. While others have used normalizers in Deep RL before, we are the first to identify that they make target networks redundant. Our next design choice exploits this insight to obtain an even greater boost.\n3.2 DESIGN CHOICE 2: USING BATCH NORMALIZATION\nBatchNorm has not yet seen wide adoption in value-based offpolicy RL methods, despite its success and widespread use in supervised learning (He et al., 2016; Santurkar et al., 2018), attempts at doing so have fared poorly. Lillicrap et al. (2016) use BatchNorm layers on the state-only representation layers in the DDPG critic but find that it does not help significantly. Others use BatchNorm in decoupled feature extractors for Deep RL networks (Ota et al., 2020; 2021), but not in critic networks. Hiraoka et al. (2021) report that using BatchNorm in critics causes training to fail in DroQ.\nWe find using BatchNorm carefully, when additionally removing target networks, performs surprisingly well, trains stably, and is, in fact, algorithmically simpler than current methods.\nFirst, we explain why BatchNorm needs to be used carefully. Within the critic loss [Q\u03b8(S,A) \u2212 (r + \u03b3Q\u03b8\u25e6(S\u2032,A\u2032))]2, predictions are made for two differently distributed batches of state-action pairs; (S,A) and (S\u2032,A\u2032), where A\u2032 \u223c \u03c0\u03d5(S\u2032) is sampled from the current policy, while A originates from old behavior policies.\nJust like the target network, the BatchNorm parameters are updated\nby Polyak Averaging from the live network (Equation 1). The BatchNorm running statistics of the live network, which were estimated from batches of (s,a) pairs, will clearly not have seen samples (s\u2032, \u03c0\u03d5(s\u2032)) and will further not match their statistics. In other words, the state-action inputs evaluated by the target network will be out-of-distribution, given its mismatched BatchNorm running statistics. It is well known that the prediction quality of BatchNorm-equipped networks degrades in the face of such test-time distribution shifts (Pham et al., 2022; Lim et al., 2023).\nRemoving the target network provides an elegant solution. With the target network removed, we can concatenate both batches and feed them through the Q network in a single forward pass, as illustrated in Figure 3 and shown in code in Figure 2. This simple trick ensures that BatchNorm\u2019s normalization moments arise from the union of both batches, corresponding to a 50/50 mixture of their respective distributions. Such normalization layers do not perceive the (s\u2032, \u03c0\u03d5(s\u2032)) batch as being out-of-distribution. This small change to SAC allows the safe use of BatchNorm and greatly accelerates training. We are not the only ones to identify this way of using BatchNorm to tackle the distribution mismatch; other works in supervised learning, e.g., Test-Time Adaptation (Lim et al.,\n2023), EvalNorm (Singh & Shrivastava, 2019), and Four Things Everyone Should Know to Improve Batch Normalization (Summers & Dinneen, 2020) also use mixed moments to bridge this gap.\nIn practice, CrossQ\u2019s actor and critic networks use Batch Renormalization (BRN, Ioffe (2017)), an improved version of the original BN (Ioffe & Szegedy, 2015) that is robust to long-term training instabilities originating from minibatch noise. BRN performs batch normalization using the less noisy running statistics after a warm-up period, instead of noisy minibatch estimates as in BN. In the rest of this paper, all discussions with \u201cBatchNorm\u201d apply equally to both versions unless explicitly disambiguated by BN or BRN."
        },
        {
            "heading": "3.3 DESIGN CHOICE 3: WIDER CRITIC NETWORKS",
            "text": "Following Ota et al. (2021), we find that wider critic network layers in CrossQ lead to even faster learning. As we show in our ablations in Section 4.4, most performance gains originate from the first two design choices; however, wider critic networks further boost the performance, helping to match or outperform REDQ and DroQ sample efficiency.\nWe want to stress again that CrossQ, a UTD = 1 method, does not use bias-reducing ensembles, high UTD ratios or target networks. Despite this, it achieves its competitive sample efficiency at a fraction of the compute cost of REDQ and DroQ (see Figures 5 and 6). Note that our proposed changes can just as well be combined with other off-policy TD-learning methods, such as TD3, as shown in our experiments in Section 4.1."
        },
        {
            "heading": "4 EXPERIMENTS AND ANALYSIS",
            "text": "We conduct experiments to provide empirical evidence for CrossQ\u2019s performance, and investigate:\n1. Sample efficiency of CrossQ compared to REDQ and DroQ; 2. Computational efficiency in terms of wallclock time and performed gradient step; 3. Effects of the proposed design choices on the performance via Q function bias evaluations;\nAnd conduct further ablation studies for the above design choices. We evaluate across a wide range of continuous-control MuJoCo (Todorov et al., 2012) environments, with 10 random seeds each. Following Janner et al. (2019); Chen et al. (2021) and Hiraoka et al. (2021), we evaluate on the same four Hopper, Walker2d, Ant, and Humanoid tasks, as well as two additional tasks: HalfCheetah and the more challenging HumanoidStandup from Gymnasium (Towers et al., 2023). We adapted the JAX version of stable-baselines (Raffin et al., 2021) for our experiments.\n4.1 SAMPLE EFFICIENCY OF CROSSQ\nFigure 5 compares our proposed CrossQ algorithm with REDQ, DroQ, SAC and TD3 in terms of their sample efficiency, i.e., average episode return at a given number of environment interactions. As a proof of concept, we also present CrossQ (TD3), a version of CrossQ which uses TD3 instead of SAC as the base algorithm. We perform periodic evaluations during training to obtain the episodic reward. From these, we report the mean and standard deviations over 10 random seeds. All subsequent experiments in this paper follow the same protocol.\nThis experiment shows that CrossQ matches or outperforms the best baseline in all the presented environments except on Ant, where REDQ performs better in the early training stage, but CrossQ eventually matches it. On Hopper, Walker, and HalfCheetah, the learning curves of CrossQ and REDQ overlap, and there is no significant difference. On the harder Humanoid and HumanoidStandup tasks, CrossQ and CrossQ (TD3) both substantially surpass all baselines.\n4.2 COMPUTATIONAL EFFICIENCY OF CROSSQ\nFigure 6 compares the computational efficiency of CrossQ to the baselines. This metric is where CrossQ makes the biggest leap forward. CrossQ requires 20\u00d7 fewer gradient steps than REDQ and DroQ, which results in roughly 4\u00d7 faster wallclock speeds (Table 2). Especially on the more challenging Humanoid and HumanoidStandup tasks the speedup is the most pronounced. In our view, this is a noteworthy feature. On the one hand, it opens the possibility of training agents in a truly online and data-efficient manner, such as in real-time robot learning. On the other hand, with large computing budgets CrossQ can allow the training of even larger models for longer than what is currently feasible, because of its computational efficiency stemming from its low UTD = 1."
        },
        {
            "heading": "4.3 EVALUATING Q FUNCTION ESTIMATION BIAS",
            "text": "All methods we consider in this paper are based on SAC and, thus, include the clipped doubleQ trick to reduce Q function overestimation bias (Fujimoto et al., 2018). Chen et al. (2021) and Hiraoka et al. (2021) stress the importance of keeping this bias even lower to achieve their high performances and intentionally design REDQ and DroQ to additionally reduce bias with explicit and implicit ensembling. In contrast, CrossQ outperforms both baselines without any ensembling. Could CrossQ\u2019s high performance be attributed to implicitly reducing the bias as a side effect of our design choices? Using the same evaluation protocol as Chen et al. (2021), we compare the\nnormalized Q prediction biases in Figure 4.3. Due to space constraints, here we show Hopper and Ant and place the rest of the environments in Figure 17 in the Appendix.\nWe find that REDQ and DroQ indeed have lower bias than SAC and significantly lower bias than SAC with UTD = 20. The results for CrossQ are mixed: while its bias trend exhibits a lower mean and variance than SAC, in some environments, its bias is higher than DroQ, and in others, it is lower or comparable. REDQ achieves comparable or worse returns than CrossQ while maintaining the least bias. As CrossQ performs better despite having\u2014perhaps paradoxically\u2014generally higher Q estimation bias, we conclude that the relationship between performance and estimation bias is complex, and one does not seem to have clear implications on the other."
        },
        {
            "heading": "4.4 ABLATIONS",
            "text": "We conduct ablation studies to better understand the impact of different design choices in CrossQ."
        },
        {
            "heading": "4.4.1 DISENTANGLING THE EFFECTS OF TARGET NETWORKS AND BATCHNORM",
            "text": "CrossQ changes SAC in three ways; of these, two explicitly aim to accelerate optimization: the removal of target networks, and the introduction of BatchNorm. Unfortunately, SAC without target networks diverges; therefore, to study the contribution of the first change, we need a way to compare SAC\u2014divergence-free\u2014with and without target networks. Fortunately, we find that such a way exists: according to our supplementary experiments in Appendix A.6, simply using bounded activation functions in the critic appears to prevent divergence. This is a purely empirical observation and an in-depth study regarding the influence of activations and normalizers on the stability of Deep RL is beyond the scope of this paper. In this specific ablation, we use tanh activations instead of relu, solely as a tool to make the intended comparison possible.\nFigure 8 shows the results of our experiment. The performance of SAC without target networks supports the common intuition that target networks indeed slow down learning to a small extent. We find that the combination of BatchNorm and Target Networks performs inconsistently, failing to learn anything in half of the environments. Lastly, the configuration of BatchNorm without target networks\u2014and the closest to CrossQ\u2014achieves the best aggregate performance, with the boost being significantly bigger than that from removing target networks alone. In summary, even though removing target networks may slightly improve performance in some environments, it is the combination of removing target networks and adding BatchNorm that accelerates learning the most.\n4.4.2 ABLATING THE DIFFERENT DESIGN CHOICES AND HYPERPARAMETERS\nIn this subsection, we examine the contributions of the different CrossQ design choices to show their importance. Figure 9 shows aggregated ablations of these components and various hyperparameters, while Figure 10 ablates the BatchNorm layer itself.\nHyperparameters. CrossQ uses the best hyperparameters obtained from a series of grid searches. Of these, only three are different from SAC\u2019s default values. First, we find that reducing the \u03b21 momentum for the Adam optimizer (Kingma & Ba, 2015) from 0.9 to 0.5 as well the policy delay of 3 have the smallest impact on the performance. However, since fewer actor gradient steps reduce compute, this setting is favorable. Second, reducing the critic network\u2019s width to 256\u2014the same small size as SAC\u2014 reduces performance and yet still significantly outperforms SAC. This suggests that practitioners may be able to make use of a larger compute budget, i.e., train efficiently across a range of different network sizes, by scaling up layer widths according to the\navailable hardware resources. Third, as expected, removing the BRN layers proves to be detrimental and results in the worst overall performance. A natural question that comes to mind is whether other normalization strategies in the critic, such as Layer Normalization (LayerNorm, Ba et al. (2016)), would also give the same results. However, in our ablation, we find that replacing BatchNorm with LayerNorm degrades CrossQ\u2019s performance significantly, roughly to the level of the SAC baseline. Lastly, SAC does not benefit from simply widening critic layers to 2048. And naively adding BRN to SAC while keeping the target networks proves detrimental. This finding is in line with our diagnosis of mismatched statistics being detrimental to the training.\nBatch Normalization Layers. In Figure 10, we ablate the BatchNorm versions (BN (Ioffe & Szegedy, 2015) and BRN (Ioffe, 2017)) and their internal moving-average momentums. Compared to CrossQ\u2019s optimal combination\u2014BRN with momentum 0.99\u2014all variants have similar sample efficiency in the early stages of training (1M steps). When using BN, we sometimes observe sudden performance collapses later in training; we attribute these to BN\u2019s unique approach of using noisy minibatch estimates of normalization moments. BRN\u2019s improved approach of using the less noisy moving-averages makes these collapses less likely; further noise-reduction via higher momentums eliminates these collapses entirely. Additionally, we find that using BatchNorm only in the critic (instead of both the actor and the critic) is sufficient to drive the strong performance of CrossQ; however, including it in both networks performs slightly better."
        },
        {
            "heading": "5 CONCLUSION & FUTURE WORK",
            "text": "We introduced CrossQ, a new off-policy RL algorithm that matches or exceeds the performance of REDQ and DroQ\u2014the current state-of-the-art on continuous control environments with state observations\u2014in terms of sample efficiency while being multiple times more computationally efficient. To the best of our knowledge, CrossQ is the first method to successfully use BatchNorm to greatly accelerate off-policy actor-critic RL. Through benchmarks and ablations, we confirmed that target networks do indeed slow down training and showed a way to remove them without sacrificing training stability. We also showed that BatchNorm has the same accelerating effect on training in Deep RL as it does in supervised deep learning. The combined effect of removing target networks and adding BatchNorm is what makes CrossQ so efficient. We investigated the relationship between the Q estimation bias and the learning performance of CrossQ, but did not identify a straightforward dependence. This indicates that the relationship between the Q estimation bias and the agent performance is more complex than previously thought.\nIn future work, it would be interesting to analyze the Q estimation bias more extensively, similar to Li et al. (2022). Furthermore, a deeper theoretical analysis of the used BatchNorm approach in the context of RL would be valuable, akin to the works in supervised learning, e.g., Summers & Dinneen (2020). Although the wider critic networks do provide an additional performance boost, they increase the computation cost, which could potentially be reduced. Finally, while our work focuses on the standard continuous control benchmarking environments, a logical extension would be applying CrossQ to a real robot system and using visual observations in addition to the robot state. Techniques from image-based RL, such as state augmentation (Laskin et al., 2020; Yarats et al., 2021) and auxiliary losses (Schwarzer et al., 2021; He et al., 2022), also aim to learn efficiently from limited data. We believe some of these ideas could potentially be applied to CrossQ."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We acknowledge the grant \u201cEinrichtung eines Labors des Deutschen Forschungszentrum fu\u0308r Ku\u0308nstliche Intelligenz (DFKI) an der Technischen Universita\u0308t Darmstadt\u201d of the Hessisches Ministerium fu\u0308r Wissenschaft und Kunst. This research was also supported by the Research Clusters \u201cThe Adaptive Mind\u201d and \u201cThird Wave of AI\u201d, funded by the Excellence Program of the Hessian Ministry of Higher Education, Science, Research and the Arts, Hessian.AI and by the German Research Foundation (DFG): 417962828."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 DEEPMIND CONTROL SUITE EXPERIMENTS",
            "text": "Figure 11 presents an additional set of experiments performed on the DeepMind Control Suite (Tassa et al., 2018). The experiments shown here are an extension to the experiments shown in Figure 5 in the main paper and have been moved to the Appendix due to space constraints. For the presented tasks, we lowered the learning rate to 8 \u00d7 10\u22124 for all algorithms, and set the CrossQ policy delay to 1. All other hyperparameters remained the same as for the main paper."
        },
        {
            "heading": "A.2 HYPERPARAMETERS",
            "text": "Experiment hyperparameters, used in the main paper. We adapted most hyperparameters that are commonly used in other works (Haarnoja et al., 2018b; Chen et al., 2021; Hiraoka et al., 2021). The Moving-Average Momentum corresponds to 1 minus the Moving-Average Update Rate as defined in both BatchNorm papers (Ioffe & Szegedy, 2015; Ioffe, 2017)."
        },
        {
            "heading": "A.3 WALLCLOCK TIME MEASUREMENT",
            "text": "Wallclock times were measured by timing and averaging over four seeds each and represent pure training times, without the overhead of synchronous evaluation and logging, until reaching 5 \u00d7 106 environment steps. The times are recorded on an Nvidia RTX 3090 Turbo with an AMD EPYC 7453 CPU.\nA.4 EVOLVING ACTION DISTRIBUTIONS"
        },
        {
            "heading": "A.4.1 ABLATING THE DIFFERENT DESIGN CHOICES AND HYPERPARAMETERS",
            "text": "Figure 13 depicts in detail the CrossQ and SAC ablations, previously shown in aggregate form by Figure 9."
        },
        {
            "heading": "A.5 REDQ AND DROQ ABLATIONS",
            "text": "Figures 14 and 15 show REDQ and DroQ ablations on 5 seeds each. They show both baselines with the CrossQ hyperparameters: wider critic networks as well as \u03b21 = 0.5. Neither baseline benefits from the added changes. In most cases, the performance is unchanged, while in some cases, it deteriorates. The dashed black line shows CrossQ as a reference."
        },
        {
            "heading": "A.6 EFFECT OF ACTIVATIONS AND NORMALIZERS ON LEARNING STABILITY",
            "text": "Figure 8 depicts a small exploratory experiment in which we remove target networks from SAC, and train it with different activation functions and feature normalizers. We do this only to explore whether the boundedness of activations has an influence on training stability. We learn from this experiment that SAC with tanh activations trains without divergence, allowing us to conduct the study in Section 4.4.1. We also observe that at least two feature normalization schemes (on top of the unbounded relu activations) permit divergence-free optimization.\nFor vectors x, relu over max(x) denotes a simple normalization scheme using an underlying unbounded activation: relu(x)/max(x), with the maximum computed over the entire feature vector. layernormed relu simply denotes LayerNorm applied after the relu activations. Both of these schemes prevent divergence. Using LayerNorm before the relu activations also prevent divergence, and is already explored in the ablations in Figure 13. None of these normalizers perform as strongly as BatchNorm.\nA thorough theoretical or experimental study of how activations and normalizers affect the stability of Deep RL is beyond the scope of this paper. We hope, however, that our observations help inform future research directions for those interested in this topic."
        },
        {
            "heading": "A.7 NORMALIZED Q BIAS PLOTS",
            "text": "Figure 17 shows the results of the Q function bias analysis for all environments."
        }
    ],
    "title": "CROSSQ: BATCH NORMALIZATION",
    "year": 2024
}