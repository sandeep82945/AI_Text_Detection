{
    "abstractText": "We focus on the self-supervised discovery of manipulation concepts that can be adapted and reassembled to address various robotic tasks. We propose that the decision to conceptualize a physical procedure should not depend on how we name it (semantics) but rather on the significance of the informativeness in its representation regarding the low-level physical state and state changes. We model manipulation concepts \u2013 discrete symbols \u2013 as generative and discriminative goals and derive metrics that can autonomously link them to meaningful sub-trajectories from noisy, unlabeled demonstrations. Specifically, we employ a trainable codebook containing encodings (concepts) capable of synthesizing the end-state of a sub-trajectory given the current state \u2013 generative informativeness. Moreover, the encoding corresponding to a particular sub-trajectory should differentiate the state within and outside it and confidently predict the subsequent action based on the gradient of its discriminative score \u2013 discriminative informativeness. These metrics, which do not rely on human annotation, can be seamlessly integrated into a VQ-VAE framework, enabling the partitioning of demonstrations into semantically consistent sub-trajectories, fulfilling the purpose of discovering manipulation concepts and the corresponding sub-goal (key) states. We evaluate the effectiveness of the learned concepts by training policies that utilize them as guidance, demonstrating superior performance compared to other baselines. Additionally, our discovered manipulation concepts compare favorably to human-annotated ones while saving much manual effort. Our code is available at: https://zrllrz.github.io/InfoCon /",
    "authors": [
        {
            "affiliations": [],
            "name": "DISCRIMINATIVE INFORMATIVENESS"
        },
        {
            "affiliations": [],
            "name": "Ruizhe Liu"
        },
        {
            "affiliations": [],
            "name": "Qian Luo"
        },
        {
            "affiliations": [],
            "name": "Yanchao Yang"
        }
    ],
    "id": "SP:dd240628c4355bf39f74eff17385226153db846c",
    "references": [
        {
            "authors": [
                "Anurag Ajay",
                "Yilun Du",
                "Abhi Gupta",
                "Joshua Tenenbaum",
                "Tommi Jaakkola",
                "Pulkit Agrawal"
            ],
            "title": "Is conditional generative modeling all you need for decision-making",
            "venue": "arXiv preprint arXiv:2211.15657,",
            "year": 2022
        },
        {
            "authors": [
                "Brenna D Argall",
                "Sonia Chernova",
                "Manuela Veloso",
                "Brett Browning"
            ],
            "title": "A survey of robot learning from demonstration",
            "venue": "Robotics and autonomous systems,",
            "year": 2009
        },
        {
            "authors": [
                "Anthony Brohan",
                "Yevgen Chebotar",
                "Chelsea Finn",
                "Karol Hausman",
                "Alexander Herzog",
                "Daniel Ho",
                "Julian Ibarz",
                "Alex Irpan",
                "Eric Jang",
                "Ryan Julian"
            ],
            "title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "venue": "In Conference on Robot Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Edoardo Caldarelli",
                "Philippe Wenk",
                "Stefan Bauer",
                "Andreas Krause"
            ],
            "title": "Adaptive gaussian process change point detection",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Lili Chen",
                "Kevin Lu",
                "Aravind Rajeswaran",
                "Kimin Lee",
                "Aditya Grover",
                "Misha Laskin",
                "Pieter Abbeel",
                "Aravind Srinivas",
                "Igor Mordatch"
            ],
            "title": "Decision transformer: Reinforcement learning via sequence modeling",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zhoujun Cheng",
                "Tianbao Xie",
                "Peng Shi",
                "Chengzu Li",
                "Rahul Nadkarni",
                "Yushi Hu",
                "Caiming Xiong",
                "Dragomir Radev",
                "Mari Ostendorf",
                "Luke Zettlemoyer"
            ],
            "title": "Binding language models in symbolic languages",
            "venue": "arXiv preprint arXiv:2210.02875,",
            "year": 2022
        },
        {
            "authors": [
                "Cheng Chi",
                "Siyuan Feng",
                "Yilun Du",
                "Zhenjia Xu",
                "Eric Cousineau",
                "Benjamin Burchfiel",
                "Shuran Song"
            ],
            "title": "Diffusion policy: Visuomotor policy learning via action diffusion",
            "venue": "arXiv preprint arXiv:2303.04137,",
            "year": 2023
        },
        {
            "authors": [
                "Norman Di Palo",
                "Arunkumar Byravan",
                "Leonard Hasenclever",
                "Markus Wulfmeier",
                "Nicolas Heess",
                "Martin Riedmiller"
            ],
            "title": "Towards a unified agent with foundation models",
            "venue": "In Workshop on Reincarnating Reinforcement Learning at ICLR 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Bin Fang",
                "Shidong Jia",
                "Di Guo",
                "Muhua Xu",
                "Shuhuan Wen",
                "Fuchun Sun"
            ],
            "title": "Survey of imitation learning for robotic manipulation",
            "venue": "International Journal of Intelligent Robotics and Applications,",
            "year": 2019
        },
        {
            "authors": [
                "Karol Gregor",
                "Danilo Jimenez Rezende",
                "Daan Wierstra"
            ],
            "title": "Variational intrinsic control",
            "venue": "arXiv preprint arXiv:1611.07507,",
            "year": 2016
        },
        {
            "authors": [
                "Jiayuan Gu",
                "Fanbo Xiang",
                "Xuanlin Li",
                "Zhan Ling",
                "Xiqiang Liu",
                "Tongzhou Mu",
                "Yihe Tang",
                "Stone Tao",
                "Xinyue Wei",
                "Yunchao Yao"
            ],
            "title": "Maniskill2: A unified benchmark for generalizable manipulation skills",
            "venue": "arXiv preprint arXiv:2302.04659,",
            "year": 2023
        },
        {
            "authors": [
                "Karol Hausman",
                "Jost Tobias Springenberg",
                "Ziyu Wang",
                "Nicolas Heess",
                "Martin Riedmiller"
            ],
            "title": "Learning an embedding space for transferable robot skills",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "R Devon Hjelm",
                "Alex Fedorov",
                "Samuel Lavoie-Marchildon",
                "Karan Grewal",
                "Phil Bachman",
                "Adam Trischler",
                "Yoshua Bengio"
            ],
            "title": "Learning deep representations by mutual information estimation and maximization",
            "venue": "arXiv preprint arXiv:1808.06670,",
            "year": 2018
        },
        {
            "authors": [
                "Wenlong Huang",
                "Fei Xia",
                "Dhruv Shah",
                "Danny Driess",
                "Andy Zeng",
                "Yao Lu",
                "Pete Florence",
                "Igor Mordatch",
                "Sergey Levine",
                "Karol Hausman"
            ],
            "title": "Grounded decoding: Guiding text generation with grounded models for robot control",
            "venue": "arXiv preprint arXiv:2303.00855,",
            "year": 2023
        },
        {
            "authors": [
                "Matthias Hutsebaut-Buysse",
                "Kevin Mets",
                "Steven Latr\u00e9"
            ],
            "title": "Hierarchical reinforcement learning: A survey and open research challenges",
            "venue": "Machine Learning and Knowledge Extraction,",
            "year": 2022
        },
        {
            "authors": [
                "Dinesh Jayaraman",
                "Frederik Ebert",
                "Alexei A Efros",
                "Sergey Levine"
            ],
            "title": "Time-agnostic prediction: Predicting predictable video frames",
            "venue": "arXiv preprint arXiv:1808.07784,",
            "year": 2018
        },
        {
            "authors": [
                "Zhiwei Jia",
                "Fangchen Liu",
                "Vineet Thumuluri",
                "Linghao Chen",
                "Zhiao Huang",
                "Hao Su"
            ],
            "title": "Chain-ofthought predictive control",
            "venue": "arXiv preprint arXiv:2304.00776,",
            "year": 2023
        },
        {
            "authors": [
                "Miguel L\u00e1zaro-Gredilla",
                "Dianhuan Lin",
                "J Swaroop Guntupalli",
                "Dileep George"
            ],
            "title": "Beyond imitation: Zero-shot task transfer on robots by learning concepts as cognitive programs",
            "venue": "Science Robotics,",
            "year": 2019
        },
        {
            "authors": [
                "Junnan Li",
                "Caiming Xiong",
                "Steven CH Hoi"
            ],
            "title": "Mopro: Webly supervised learning with momentum prototypes",
            "venue": "arXiv preprint arXiv:2009.07995,",
            "year": 2020
        },
        {
            "authors": [
                "Klaus Libertus",
                "Dominic A Violi"
            ],
            "title": "Sit to talk: Relation between motor skills and language development in infancy",
            "venue": "Frontiers in psychology,",
            "year": 2016
        },
        {
            "authors": [
                "Fangchen Liu",
                "Hao Liu",
                "Aditya Grover",
                "Pieter Abbeel"
            ],
            "title": "Masked autoencoding for scalable and generalizable decision making",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Andrew S. Morgan",
                "Walter G. Bircher",
                "Aaron M. Dollar"
            ],
            "title": "Towards generalized manipulation learning through grasp mechanics-based features and self-supervision",
            "venue": "IEEE Transactions on Robotics,",
            "year": 2021
        },
        {
            "authors": [
                "Tongzhou Mu",
                "Zhan Ling",
                "Fanbo Xiang",
                "Derek Yang",
                "Xuanlin Li",
                "Stone Tao",
                "Zhiao Huang",
                "Zhiwei Jia",
                "Hao Su"
            ],
            "title": "Maniskill: Generalizable manipulation skill benchmark with large-scale demonstrations",
            "venue": "arXiv preprint arXiv:2107.14483,",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Neitz",
                "Giambattista Parascandolo",
                "Stefan Bauer",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Adaptive skip intervals: Temporal abstraction for recurrent dynamical models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Tim Pearce",
                "Tabish Rashid",
                "Anssi Kanervisto",
                "Dave Bignell",
                "Mingfei Sun",
                "Raluca Georgescu",
                "Sergio Valcarcel Macua",
                "Shan Zheng Tan",
                "Ida Momennejad",
                "Katja Hofmann"
            ],
            "title": "Imitating human behaviour with diffusion models",
            "venue": "arXiv preprint arXiv:2301.10677,",
            "year": 2023
        },
        {
            "authors": [
                "Karl Pertsch",
                "Oleh Rybkin",
                "Jingyun Yang",
                "Shenghao Zhou",
                "Konstantinos Derpanis",
                "Kostas Daniilidis",
                "Joseph Lim",
                "Andrew Jaegle"
            ],
            "title": "Keyframing the future: Keyframe discovery for visual prediction and planning",
            "venue": "In Learning for Dynamics and Control,",
            "year": 2020
        },
        {
            "authors": [
                "Yuzhe Qin",
                "Hao Su",
                "Xiaolong Wang"
            ],
            "title": "From one hand to multiple hands: Imitation learning for dexterous manipulation from single-camera teleoperation",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Rouhollah Rahmatizadeh",
                "Pooya Abolghasemi",
                "Ladislau B\u00f6l\u00f6ni",
                "Sergey Levine"
            ],
            "title": "Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration",
            "venue": "IEEE international conference on robotics and automation (ICRA),",
            "year": 2018
        },
        {
            "authors": [
                "Aurko Roy",
                "Ashish Vaswani",
                "Arvind Neelakantan",
                "Niki Parmar"
            ],
            "title": "Theory and experiments on vector quantized autoencoders",
            "venue": "arXiv preprint arXiv:1805.11063,",
            "year": 2018
        },
        {
            "authors": [
                "Nur Muhammad Shafiullah",
                "Zichen Cui",
                "Ariuntuya Arty Altanzaya",
                "Lerrel Pinto"
            ],
            "title": "Behavior transformers: Cloning k modes with one stone",
            "venue": "Advances in neural information processing systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tanmay Shankar",
                "Abhinav Gupta"
            ],
            "title": "Learning robot skills with temporal variational inference",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Lin Shao",
                "Toki Migimatsu",
                "Qiang Zhang",
                "Karen Yang",
                "Jeannette Bohg"
            ],
            "title": "Concept2robot: Learning manipulation concepts from instructions and human demonstrations",
            "venue": "The International Journal of Robotics Research,",
            "year": 2021
        },
        {
            "authors": [
                "Lucy Xiaoyang Shi",
                "Archit Sharma",
                "Tony Z Zhao",
                "Chelsea Finn"
            ],
            "title": "Waypoint-based imitation learning for robotic manipulation",
            "venue": "arXiv preprint arXiv:2307.14326,",
            "year": 2023
        },
        {
            "authors": [
                "Mohit Shridhar",
                "Lucas Manuelli",
                "Dieter Fox"
            ],
            "title": "Cliport: What and where pathways for robotic manipulation",
            "venue": "In Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Ishika Singh",
                "Valts Blukis",
                "Arsalan Mousavian",
                "Ankit Goyal",
                "Danfei Xu",
                "Jonathan Tremblay",
                "Dieter Fox",
                "Jesse Thomason",
                "Animesh Garg"
            ],
            "title": "Progprompt: Generating situated robot task plans using large language models",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2023
        },
        {
            "authors": [
                "Vladimir M Sloutsky"
            ],
            "title": "From perceptual categories to concepts: What develops",
            "venue": "Cognitive science,",
            "year": 2010
        },
        {
            "authors": [
                "Daniel Tanneberg",
                "Kai Ploeger",
                "Elmar Rueckert",
                "Jan Peters"
            ],
            "title": "Skid raw: Skill discovery from raw trajectories",
            "venue": "IEEE robotics and automation letters,",
            "year": 2021
        },
        {
            "authors": [
                "Faraz Torabi",
                "Garrett Warnell",
                "Peter Stone"
            ],
            "title": "Behavioral cloning from observation, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Aaron Van Den Oord",
                "Oriol Vinyals"
            ],
            "title": "Neural discrete representation learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jan Ole von Hartz",
                "Eugenio Chisari",
                "Tim Welschehold",
                "Abhinav Valada"
            ],
            "title": "Self-supervised learning of multi-object keypoints for robotic manipulation",
            "venue": "arXiv preprint arXiv:2205.08316,",
            "year": 2022
        },
        {
            "authors": [
                "Eric A Walle",
                "Joseph J Campos"
            ],
            "title": "Infant language development is related to the acquisition of walking",
            "venue": "Developmental psychology,",
            "year": 2014
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yijia Weng",
                "Kaichun Mo",
                "Ruoxi Shi",
                "Yanchao Yang",
                "Leonidas Guibas"
            ],
            "title": "Towards learning geometric eigen-lengths crucial for fitting tasks",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Danfei Xu",
                "Suraj Nair",
                "Yuke Zhu",
                "Julian Gao",
                "Animesh Garg",
                "Li Fei-Fei",
                "Silvio Savarese"
            ],
            "title": "Neural task programming: Learning to generalize across hierarchical tasks",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2018
        },
        {
            "authors": [
                "Mengda Xu",
                "Zhenjia Xu",
                "Cheng Chi",
                "Manuela Veloso",
                "Shuran Song"
            ],
            "title": "Xskill: Cross embodiment skill discovery",
            "venue": "arXiv preprint arXiv:2307.09955,",
            "year": 2023
        },
        {
            "authors": [
                "Mengyuan Yan",
                "Yilin Zhu",
                "Ning Jin",
                "Jeannette Bohg"
            ],
            "title": "Self-supervised learning of state estimation for manipulating deformable linear objects",
            "venue": "IEEE robotics and automation letters,",
            "year": 2020
        },
        {
            "authors": [
                "Mengjiao Sherry Yang",
                "Dale Schuurmans",
                "Pieter Abbeel",
                "Ofir Nachum"
            ],
            "title": "Chain of thought imitation with procedure cloning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik Narasimhan",
                "Yuan Cao"
            ],
            "title": "React: Synergizing reasoning and acting in language models",
            "venue": "arXiv preprint arXiv:2210.03629,",
            "year": 2022
        },
        {
            "authors": [
                "Martina Zambelli",
                "Yusuf Aytar",
                "Francesco Visin",
                "Yuxiang Zhou",
                "Raia Hadsell"
            ],
            "title": "Learning rich touch representations through cross-modal self-supervision",
            "venue": "Proceedings of the 2020 Conference on Robot Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Andy Zeng",
                "Pete Florence",
                "Jonathan Tompson",
                "Stefan Welker",
                "Jonathan Chien",
                "Maria Attarian",
                "Travis Armstrong",
                "Ivan Krasin",
                "Dan Duong",
                "Vikas Sindhwani"
            ],
            "title": "Transporter networks: Rearranging the visual world for robotic manipulation",
            "venue": "In Conference on Robot Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Tianhao Zhang",
                "Zoe McCarthy",
                "Owen Jow",
                "Dennis Lee",
                "Xi Chen",
                "Ken Goldberg",
                "Pieter Abbeel"
            ],
            "title": "Deep imitation learning for complex manipulation tasks from virtual reality teleoperation",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2018
        },
        {
            "authors": [
                "Yifeng Zhu",
                "Peter Stone",
                "Yuke Zhu"
            ],
            "title": "Bottom-up skill discovery from unsegmented demonstrations for long-horizon robot manipulation",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Conceptual development is of core importance to the emergence of human-like intelligence, ranging from primary perceptual grouping to sophisticated scientific terminologies (Sloutsky, 2010). We focus on embodied tasks that require interaction with the physical environment and seek the manipulation concepts that are critical for learning efficient and generalizable action policies (La\u0301zaroGredilla et al., 2019; Shao et al., 2021). Recently, Large Language Models (LLMs) have enabled many interesting robotic applications with their reasoning capabilities that can break complex embodied tasks into short-horizon interactions or manipulation concepts (Singh et al., 2023). However, LLMs are trained with an internet-scale corpus, representing a vast amount of linguistic knowledge of manipulations, but lack embodied experiences that ground these manipulation concepts to specific physical states (Brohan et al., 2023; Huang et al., 2023). In contrast, it is worth noting that in human development, infants initially acquire physical skills, e.g., crawling, grasping, and walking, before delving into the complexities of language, which grant the manipulation concepts described in language with groundingness in the first place (Walle & Campos, 2014; Libertus & Violi, 2016).\nDrawing inspiration from the human developmental process, we aim to equip embodied agents with manipulation concepts intrinsically connected to physical states without relying on additional grounding techniques. Specifically, we seek algorithms that allow agents to abstract or identify\n\u2217Work done as a research assistant at HKU.\nmanipulation concepts from their embodied experiences, such as demonstration trajectories, without humans specifying and annotating the involved concepts. The discovery process serves a dual purpose. Firstly, it generates a set of discrete symbols that hold semantic meaning, with some potentially representing concepts like \u201cgrasping a block\u201d or \u201caligning a block with a hole on the wall.\u201d Furthermore, the discovery process should explicitly establish correspondences between the concepts and physical states, thereby achieving the grounding for low-level actions while minimizing annotation efforts.\nTherefore, we propose to model manipulation concepts as generative and discriminative goals. Specifically, as a generative goal, a manipulation concept should help predict the goal state even though it has not been achieved yet. For example, when given the manipulation concept \u201cgrasp the block,\u201d one can already (roughly) synthesize how the scene looks when the robotic gripper grasps the block. We formulate it as the informativeness between the manipulation concept and the synthesized ending state when the interaction implied by the manipulation concept is accomplished, which we name generative informativeness. On the other hand, given a manipulation concept as a discriminative goal, one could tell if the current state is within the process of achieving the goal state implied by the concept. For example, with the concept \u201cplace the cup under the faucet,\u201d one would assign low compatibility to the state of \u201cpouring water to the mug\u201d compared to the correct one. Similarly, we can formulate it as the informativeness between the manipulation concept and the binary variable, indicating whether the state falls within the manipulation process, which we call discriminative informativeness. Moreover, as a discriminative goal, a manipulation concept should inform the following action. If an action incurs a higher discriminative score (compatibility), then it should be executed to accomplish the task. Accordingly, we formulate it as the informativeness between the gradient of the discriminative function (conditioned on the manipulation concept) and the next action in the demonstration.\nWith the proposed metrics, we can train a VQ-VAE (Van Den Oord et al., 2017) architecture to learn the discrete representations of potential manipulation concepts from the demonstration, as well as the assignment (grounding) between the learned concepts and the physical states, even though no concept descriptions are available in any form. We examine the quality of the learned manipulation concepts through the automatically acquired grounding, and verify that these concepts do come with semantic meaning in terms of human linguistics Fig 1. We further demonstrate the usefulness of the learned manipulation concepts by using the grounded states as guidance to train manipulation policies. Experimental results show that the discovered manipulation concepts from unannotated demonstrations enable policies that surpass the state-of-the-art counterparts, while achieving comparable performance with respect to the oracle trained with human labels, which verifies the effectiveness of the proposed metrics and training for self-supervised manipulation concept discovery."
        },
        {
            "heading": "2 METHOD",
            "text": "Problem Setup We aim to characterize the multi-step nature of low-level manipulation tasks. More explicitly, we assume access to a set of pre-collected demonstrations or trajectories, i.e.,\nD = {\u03c4i}Ni=1 and \u03c4i = {(sit, ait)} T (i) t=1 , which is a sequence of state-action pairs. Our goal is to partition each trajectory into semantically meaningful segments (continuous in time), while maintaining consistency between trajectories of the same task without resorting to human annotations. We call what governs the actions within a particular trajectory segment the manipulation concept. And the \u201csemantic meaning\u201d of the manipulation concept lies in its generative and discriminative informativeness, which we discuss in Sec. 2.1. We further name the state at the boundary of two segments as the key state (Jia et al., 2023). We examine how the learned manipulation concepts align with human semantics, as well as the quality of the partitions, by evaluating the effectiveness of the derived key states on physical manipulation benchmarks. We adopt the CoTPC Jia et al. (2023) framework to train concept-guided policies across different benchmarks. Next, we elaborate on the proposed partitioning metrics."
        },
        {
            "heading": "2.1 MANIPULATION CONCEPT AS GENERATIVE AND DISCRIMINATIVE GOALS",
            "text": "Suppose a task can be described by K manipulation concepts {\u03b1k}Kk=1 with \u03b1k \u2208 RM . Or equivalently, a trajectory \u03c4 = {(st, at)}Tt=1 from this task could be divided into K segments {\u03b2k}Kk=1, with each \u03b2 a continuous short-horizon trajectory and \u03c4 = \u222aKk=1\u03b2k. Note that we do not endow \u03b1k with any specific linguistic descriptions at this moment (e.g., \u201cpick up a cup\u201d or \u201cplace the cup under the faucet\u201d). Instead, we treat them as symbols grounded in the segments with consistency across different trajectories of the same task. In other words, we assume the existence of a partitioning function (neural network) \u03a6 such that {\u03b2k}Kk=1 = \u03a6(\u03c4). We detail the structure of \u03a6 later and enable the training of \u03a6 by proposing the following criteria.\nAs a generative goal, a manipulation concept \u03b1k shall, given the current state, inform the end state of \u03b2k, i.e., the key state stk with tk = \u2211k j=1 |\u03b2j |, where | \u00b7 | is the length of a (sub)sequence. For example, with the manipulation concept that resembles \u201cgrasp the mug handle,\u201d one can imagine the picture depicting when the handle is firmly grasped. We formalize this metric using Shannon\u2019s mutual information: Lgen(\u03b1) = I(\u03b1; skey|s), skey \u2208 \u222a\u03c4i{stk(\u03c4i)}k. (1) We name the above the generative informativeness since knowing the manipulation concept can help confidently synthesize the imminent key state after s. Please note that the manipulation concept, key state, and state are random variables depending on the trajectory, which is omitted for simplicity.\nAs a discriminative goal, a manipulation concept \u03b1k should tell whether the current state is within the process described by \u03b1k or not. For example, in the task to get water to drink, if \u03b1k resembles \u201capproach the faucet,\u201d then the state before a cup is grasped or the state \u201cdrinking water\u201d should have low compatibility with \u03b1k in contrast to the states within the corresponding sub-trajectory of approaching the faucet. Thus, the knowledge of the manipulation concept \u03b1k helps distinguish the states governed by it from the other states. We characterize this phenomenon by instantiating a compatibility function: C : \u03b1\u00d7 s\u2192 [0, 1], (2) such that values close to 1 imply high compatibility of the state s with the process described by the manipulation concept \u03b1. We can also write C\u03b1(\u00b7) to reflect the fact that the manipulation concept\nindexes a discrimination function, hence the role of \u03b1 as a discriminative goal and we call Eq. 2 the discriminative informativeness of the manipulation concept.\nFurthermore, we propose that the gradient of the compatibility function C\u03b1 should be informative of the next action a. We believe, as a densely defined function, it is beneficial that C\u03b1 not only indicates \u201cwhat\u201d the agent is doing but also \u201chow\u201d to do it by modulating the fluctuations of the compatibility function around s. Still, considering the concept of \u201cfilling the cup with water,\u201d the action of putting the cup under the faucet shall be assigned higher compatibility than the action of moving the cup toward a microwave. This illustrates that the compatibility function shall inform state changes (induced by actions) via the changes in its value, depicted through its gradient. Accordingly, we formulate this characteristic of C\u03b1 as:\nLdis(\u2207C) = I(\u2207C;a|\u03b1), (3)\nwhere\u2207C = \u2202C \u03b1\n\u2202s , given the manipulation concept \u03b1. Consequently, we name Eq. 3 the actionable\ninformativeness of the manipulation concept as a discriminative goal. We illustrate the idea of a manipulation concept as both generative and discriminative goals in Fig. 2, which shows different aspects during a manipulation process as represented by the proposed informativeness criteria.\nIn summary, we propose that a manipulation concept \u03b1 should 1). inform the imminent key (physical) state given the current state as a generative goal; 2). indicate whether the current state is within the process governed by itself as a discriminative goal, as well as 3). inform the action with the gradient of the instantiated discriminative function C\u03b1. Please note that these metrics do not count on human supervision. In other words, as long as we have a network \u03a6 that takes in a manipulation trajectory and outputs a set of continuous segments, we can train \u03a6 using the proposed metrics to discover manipulation concepts (consistent partitions) shared across trajectories of different tasks. Next, we operationalize the self-supervised manipulation concept discovery by specifying the training architecture and the objectives."
        },
        {
            "heading": "2.2 SELF-SUPERVISED MANIPULATION CONCEPT DISCOVERY",
            "text": "Network Structure of \u03a6 We adapt the basic structure proposed in VQ-VAE (Van Den Oord et al., 2017) to accommodate the sequential nature of a state-action sequence and the need for continuous sub-trajectories. Specifically, we employ a transformer encoder \u03d5 that maps state st to a latent zt in an autoregressive manner:\nzt = \u03d5(st|{(sj , aj)}t\u22121j=1). (4)\nThis autoregressive design alleviates the ambiguity in predicting the latent by supplying rich history information. Moreover, it helps smooth out noise in zt to facilitate the partitioning of the trajectory into continuous sub-trajectories. To further enhance the segmentation continuity, we devise a positional encoding scheme for time t and apply it to the latent zt. The proposed positional encoding can capture the fact that nearby latents have a good chance of being assigned to the same manipulation concept without causing over-smoothing. Please refer to Sec. A.1 for more details. In the following, we abuse zt for the latent appended with the proposed positional encoding.\nTrajectory Partitioning with Manipulation Concepts To further process {zt}Tt=1 and derive the partitioning, we instantiate K trainable vectors, which serve as the manipulation concepts {\u03b1k}Kk=1. We then assign the state or latent zt to the concept that shares the largest similarity. More explicitly, denote \u03b7 as the concept assignment function, then we have:\n\u03b7(zt) = argmax k p(zt ) \u03b1k) = argmax k exp(\u27e8zt, \u03b1k\u27e9/\u03c4)\u2211K k=1 exp(\u27e8zt, \u03b1k\u27e9/\u03c4) , (5)\nwhere p(zt ) \u03b1k) is the probability of assigning latent zt to the concept \u03b1k, and \u27e8\u00b7, \u00b7\u27e9 is the cosine similarity between two vectors. This assignment process allows us to group the states into segments {\u03b2k}Kk=1, i.e., binding states corresponding to the same concept, and serves as a ground to elaborate the training objectives. Note that we also need to ensure the gradient flow during training, thus, we use a technique similar to the gradient preserving proposed in VQ-VAE (Van Den Oord et al., 2017). Please see Sec. A.5 for more details.\nTraining Objectives With the derived sub-trajectories, we first locate the key state for every single state. Let \u03b1it be the predicted concept \u2013 for trajectory i at time step t \u2013 from {\u03b1k}Kk=1, and let si t(\u03b1it) be the (to be achieved) key state of sit from the trajectory \u03c4i = {(sit, ait)} T (i) t=1 . Then s i t(\u03b1it)\nis determined by setting t(\u03b1it) = argminu{u \u2265 t, \u03b1iu \u0338= \u03b1iu+1}. Next, we detail the loss terms. Generative Goal Loss. To maximize the generative informativeness (Eq. 1), we employ a reconstruction loss that is related to maximizing the mutual information (Hjelm et al., 2018). We instantiate a network \u03b8g that predicts the key state from the current conditioned on the associated manipulation concept \u03b1it:\nLgen(\u03b1, \u03d5; \u03b8g) = 1 N N\u2211 i=1 1 T (i) T (i)\u2211 t=1 \u2225\u03b8g(sit;\u03b1it|{(siu, aiu, \u03b1iu)}t\u22121u=1)\u2212 sit(\u03b1it)\u2225 2 2, (6)\nwith \u2225 \u00b7 \u22252 the L-2 norm. This term encourages \u03b1it to be informative of the to-be-achieved key state by minimizing the prediction error.\nDiscriminative Goal Loss. According to the defining term (Eq. 2) of a manipulation concept as a discriminative goal, we instantiate a network C(\u00b7) as a hyper-classifier, which can decode manipulation concepts from {\u03b1k}Kk=1 into compatibility functions representing the discriminative goal of these concepts. More specifically, C\u03b1it should assign high scores to states in a trajectory \u03c4 i that are governed by \u03b1it. We formulate it as a binary classification and employ the cross entropy loss to maximize the discriminative informativeness (Eq. 2):\nLdisc (\u03b1, \u03d5; C) = \u2212 1\nK K\u2211 k=1 \u27e8 1 |{\u03b1it = \u03b1k}| \u2211 \u03b1it=\u03b1k log C\u03b1k(sit)+ 1 |{\u03b1it \u0338= \u03b1k}| \u2211 \u03b1it \u0338=\u03b1k log(1\u2212C\u03b1k(sit))\u27e9.\n(7) Note that the quantity inside \u27e8\u00b7\u27e9 is the classification error for a manipulation concept\u2019s classifier but is normalized to account for the imbalance between the positive and negative samples. Please see Sec. A.7 for details of the hyper-classifier C. To derive the loss for maximizing the actionable informativeness (Eq. 3), we instantiate a policy network \u03c0, which minimizes the prediction error of the action conditioned on the manipulation concept:\nLdisa (\u03b1, \u03d5; C, \u03c0) = 1\nN N\u2211 i=1 1 T (i) T (i)\u2211 t=1 \u2225\u03c0(sit, \u2202C\u03b1it \u2202s \u2223\u2223\u2223 s=sit ;hit)\u2212 ait\u222522, (8)\nwhere hit = {(siu, aiu, \u2202C\u03b1iu \u2202s \u2223\u2223\u2223 s=siu\n)}t\u22121u=1 is the history, and we use L-2 norm for the discrepancy. Please note that the policy employed here is mainly for the purpose of manipulation concept dis-\ncovery, which is different from the policy in downstream manipulation tasks (Please see Sec. 3.1 for more details). By minimizing the above quantity, the information in the partial derivative about the next action should be maximized. Further, following Van Den Oord et al. (2017), we add a term that encourages more confident assignments by Eq. 5, i.e.,\nLent(\u03b1, \u03d5) = \u2212 1 K K\u2211 k=1\n1 |{\u03b1it = \u03b1k}| \u2211\n\u03b1it=\u03b1k\nlog(p(zit ) \u03b1k)), (9)\nwhich is similar to minimizing the entropy of the assignment probability function p(zt ) \u03b1k).\nFinally, the total training loss for self-supervised manipulation concept discovery with the proposed generative and discriminative informativeness metrics can be summarized as:\nL = Lgen(\u03b1, \u03d5; \u03b8g) + Ldisa (\u03b1, \u03d5; C, \u03c0) + \u03bb(Ldisc (\u03b1, \u03d5; C) + Lent(\u03b1, \u03d5)), (10) with \u03bb balancing the importance between the prediction and the classification terms. More implementation details can be found in Sec. A and Sec. B, and we study the effectiveness of each component in the experiments."
        },
        {
            "heading": "3 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "3.1 EXPERIMENTAL SETTINGS",
            "text": "We evaluate the effectiveness of InfoCon and the derived key states on four robot manipulation tasks from ManiSkill2 (Gu et al., 2023), an extended version of ManiSkill (Mu et al., 2021): P&P Cube: To pick up a cube, move it to a specific location, and keep it still for a short while. Stack Cube: To pick up a cube and put it on the top of another cube. Turn Faucet: To turn different faucets for enough angle values. Peg Insertion: To pick up a cuboid-shaped peg and insert it into a hole in a given object. We visualize the manipulation tasks with the manually defined and grounded key states from CoTPC (Jia et al., 2023) in Fig. 4.\nTraining details. Following the setup of CoTPC, we choose the same sets of trajectories (without ground truth key states) for the four tasks above for training and evaluation. Specifically, we collect 500 training trajectories for each task, 100 evaluation trajectories for P&P Cube and Stack Cube, 100 evaluation trajectories for seen faucets, 400 trajectories for unseen faucets, and 400 evaluation trajectories for Peg Insertion. With the pipeline shown in Fig. 3, we train the model and use the state encoder (Eq. 4) along with the concept assignment module (Eq. 5) to perform the partitioning and grounding of all trajectories. We choose the last state (of a sub-trajectory) as the key state of the corresponding manipulation concept. After collecting the discovered key states, we train a CoTPC policy optimizing the action prediction with the key states as guidance. We leave out the key state prediction in the policy training when the key state does not appear in the trajectory (e.g., concepts discovered for one task may not appear in another). Please see Sec. A and Sec. B for more details.\nEvaluation metrics. The efficacy of our approach is mainly assessed by the task success rate of policies that are trained utilizing the key states identified via InfoCon. Additionally, for curiosity, we introduce a metric for a rough understanding of whether there are similarities between the predicted key states and the ground truth key states (designed by humans). Suppose the time steps of ground truth key states are {tgtk }K \u2032\nk=1, and the time steps of predicted key states are {tj}Kj=1 (here K \u2032 is not always equal to K). We propose the Human Intuition Similarity, or HIS, as in the following:\nHIS({tj}Kj=1, {t gt k }\nK\u2032 k=1) = K\u2032\u2211 k=1 (targminj (tj\u2265t gt k ) \u2212 tgtk ) (11)\nThe HIS metric quantifies the temporal distance between a given ground truth key state and the nearest subsequent predicted key state. Thus, the HIS metric measures similarity without being affected by the number of discovered key states."
        },
        {
            "heading": "3.2 MAIN RESULTS",
            "text": "Baselines. We first consider several popular baselines without Chain-of-Thought key state prediction: Vanilla BC (Torabi et al., 2018), Decision Transformer (Chen et al., 2021), Behavior Transformer (Shafiullah et al., 2022), MaskDP (Liu et al. 2022, use ground truth key states differently), Decision Diffuser (Ajay et al., 2022). We further consider the baselines with different key state labeling techniques (all these baselines are trained with the CoTPC framework; see Sec. B for more training details): (1) Last State only includes the very last state of the trajectory as the key state. (2) AWE (Shi et al., 2023) (3) LLM+CLIP (Di Palo et al., 2023) first uses Large Language Model (LLM) to generate subgoals for a task, then uses CLIP to target the image frame scoring highest regarding each subgoal, which will serve as the key states. (4) GT Key States uses ground truth key states, which is the oracle as in (Jia et al., 2023). Here, labeling methods (1), (3), and (4) can be regarded as zero-shot, which are dependent on human semantics, while methods (2) and InfoCon are intrinsically linked to the inherent properties of the data.\nPerformance of InfoCon. We report the success rate of all baselines in Tab. 1 (additional results of the methods without Chain-of-Thought key state prediction, except Decision Transformer, can be found in Sec. C since their performance is much worse than Decision Transformer according to Jia et al. 2023). Also, as reported in Jia et al. (2023), some of the methods above, which do not use Chain-of-Thought key state prediction strategies, cannot even achieve reasonably good results in seen environments during training (overfitting). Thus, we follow CoTPC to comprehensively compare different baselines and report results on the seen and unseen environment seeds. As observed in Tab. 1, the policies trained with our discovered manipulation key states can achieve competitive performance compared with CoTPC policies using ground-truth key states and other key state grounding methods. Notably, the generalization of InfoCon is also evidenced by delivering top success rates across different tasks. We further provide human intuition similarity of key states grounded by LLM+CLIP, discovered by AWE and InfoCon in Tab. 2. We find that the HIS score seems to have a weak correlation with the policy performance presented in Tab. 1. Therefore, we treat this as a signal that the effectiveness of the manipulation concepts may not align well with what human semantics endorse, which justifies the need to develop discovery and grounding methods that can learn from unannotated trajectories.\n1Policies with key states labeled by AWE and LLM+CLIP are trained by us. Other results are from (Jia et al., 2023).\nVisualization of key states. We visualize the discovered manipulation concepts and grounded key states with InfoCon in Fig. 5. As shown, the identified key states consist of states similar to the ground-truth key states from Jia et al. (2023) (Fig. 4) as well as more fine-grained ones. We provide additional visualizations in Sec. D. We also perform an experiment to assign discovered concepts with semantically meaningful descriptions for the Peg-Insertion task in Sec. E."
        },
        {
            "heading": "3.3 ABLATION STUDY",
            "text": "We perform an ablation study to investigate the characteristics of both generative and discriminative goals. Keeping other hyper-parameters constant, we separately omit the loss terms associated with the discriminative goal (Eq. 7 and Eq. 8) and the loss terms related to the generative goal (Eq. 10). Our evaluation considers two primary metrics: the success rate of CoTPC policies on the four manipulation tasks and the Human Intuition Similarity (HIS, Eq. 11) on those tasks.\nThe success rates of different variants are reported in Tab. 3. When only employing the generative loss or the discriminative losses, the policies tend to be worse than those trained with states discovered by both types of losses 2. While some models might perform well in seen environments, they often fail to deliver comparable results in unseen scenarios. This reaffirms the pivotal role of proper key state identification and underscores the indispensable synergy between the generative and discriminative informativeness losses.\nThe ablations with HIS are reported in Tab. 4. When employing only the generative loss, the identified key states exhibit reduced similarity to human intuition. Interestingly, the effect on intuition similarity appears less prominent when relying solely on the discriminative goal loss. We postulate that this might be attributed to the noise in the manipulation concept assignment during the initial training phase. Particularly, at the beginning of training, the groundings of key states are not always accurate. The key states may not faithfully represent goal states, rendering them suboptimal as prediction targets in the generative goal loss (Eq. 6). Practically, during the initial training phase, we can temporally turn off the optimization of the generative goal loss (Eq. 6) and let the discriminative goal loss run for a few epochs. More details on this part can be found in Sec. B.2."
        },
        {
            "heading": "4 RELATED WORK",
            "text": "Behavior Cloning in Manipulation Tasks. Training robots through demonstration-driven behavior cloning continues to be a key research area (Argall et al., 2009). Within this realm, various techniques have evolved to harness the potential of imitation learning (Zhang et al., 2018; Rahmatizadeh et al., 2018; Fang et al., 2019). Approaches range from employing clear-cut strategies (Zeng et al., 2021; Shridhar et al., 2022; Qin et al., 2022) and more subtle methods to adopting models based on diffusion principles (Chi et al., 2023; Pearce et al., 2023). Some methodologies manage to sidestep the necessity of task-specific labels throughout training and discover robot skills (Shankar & Gupta, 2020; Tanneberg et al., 2021; Xu et al., 2023). Different from prior research, our work prioritizes the autonomous extraction and understanding of manipulation concepts from unlabeled demonstrations, taking a step beyond mere replication toward genuine conceptual understanding.\nHierarchical Planning and Manipulation Concept Discovery. Hierarchical planning is crucial in interaction and manipulation scenarios, as it underscores the need for blending broad strategies with specific, detailed actions to achieve optimal results (Hutsebaut-Buysse et al., 2022; Yang et al., 2022; Xu et al., 2018; Jia et al., 2023). Recent years, the Chain of Thought (CoT) prompting methodology, as depicted in (Wei et al., 2022; Cheng et al., 2022; Yao et al., 2022), further emphasizes the value of breaking down intricate tasks into a succession of more straightforward sub-tasks. By doing so, the complexity of the policies is significantly reduced, paving the way for simpler learning processes and resulting in more reliable policies. However, most hierarchical planning methods lean heavily on human semantic intuition. This can manifest as direct human input or systems mimicking human reasoning like LLMs (Di Palo et al., 2023). Some studies (Zambelli et al., 2021; von Hartz et al., 2022; Sermanet et al., 2018; Morgan et al., 2021; Yan et al., 2020; Weng et al., 2023) employ self-supervised discovery of manipulation concepts to circumvent such manual semantic interventions, leveraging mutual information (Hausman et al., 2018; Gregor et al., 2016), significant points of time (Neitz et al., 2018; Jayaraman et al., 2018; Pertsch et al., 2020; Zhu et al., 2022; Caldarelli et al., 2022), and geometry (Shi et al., 2023; Morgan et al., 2021; Zhu et al., 2022) or physics (Yan et al., 2020) constraints. Recent method AWE (Shi et al., 2023) has been proposed to mitigate the compounding error problem inherent to behavioral cloning and automatically determine waypoints based on trajectory segments that can be approximated by linear motion. However, the interpretability of discovered manipulation skills in these methods remains inadequate. Our proposed InfoCon broadens the scope beyond mere planning or trajectory partitioning, with a deeper understanding and abstraction of underlying manipulation concepts. InfoCon ensures not just task completion but also a richer conceptual grasp of the task itself."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We propose InfoCon, a self-supervised learning method capable of discovering task-specific manipulation concepts based on generative and discriminative informativeness. The concepts discovered by InfoCon share similarities with human intuition and are able to be utilized for training manipulation policies with comparable performance to oracles. Our work provides an idea of constructing embodied agents discovering concepts themselves other than struggling with the grounding of concepts that are manually specified. We hope it is received as an attempt to develop an automatic mechanism for discovering useful abstract concepts. In the future, we will consider exploring the possibility of methods for discovering relationships between manipulation concepts and forming structures of discovered concepts."
        },
        {
            "heading": "6 ACKNOWLEDGMENT",
            "text": "This work is supported by the HKU-100 Award, donation from the Musketeers Foundation, the Microsoft Accelerate Foundation Models Research Program, and in part by the JC STEM Lab of Robotics for Soft Materials funded by The Hong Kong Jockey Club Charities Trust. We also like to thank Qihang Fang for helping with the human motion concept discovery experiments."
        },
        {
            "heading": "A MORE DETAILS OF ARCHITECTURE",
            "text": ""
        },
        {
            "heading": "A.1 TIME-STEP EMBEDDING.",
            "text": "For the feature z extracted (Eq. 4) from state s at time-step t, we always normalize z to proceed the latter concept assignment (Eq. 5). In Eucild Space, z is on a high dimensional unit ball. In order to maintain this \u201cspherical\u201d characteristic and embed z using time-steps, we design the time-step embedding method below:\nz \u2190 [sin (2t/T \u2212 1)\u03c0 2 + 2A , z \u00b7 cos (2t/T \u2212 1)\u03c0 2 + 2A ]\n1 \u2264 t \u2264 T (12)\nwhere A > 0 and T is the total number of time-steps. We give an intuition explanation at Fig. 6. This embedding operation will influence more on z with time step t close to the beginning and the end of the manipulation trajectory, making it focus more on the absolute time-step (original weight in z will be relatively small), while for z with time-steps in the middle, they will receive weaker influence from this embedding operation. Reflecting on our intuition, when learning a manipulation task, the information of absolute passing time is informative at the beginning and the end, but in the middle we will focus more on relative time-step order instead of absolute current time.\nIn Eq. 12 A is a positive hyper parameter controlling the usage of area of the whole spherical surface. When A = 0, the whole spherical surface will be used for time step embedding, but when the unified time-step tT of z is 0 or 1, the embedded vector will always be (\u22121, 0\u20d7) or (1, 0\u20d7), which will annihilate the feature of original vector. So we use coefficient A to avoid this. The inclusion of A will still maintain the characteristic of focusing more on absolute time at the beginning or the end.\n2+2A , y cos (2t/T\u22121)\u03c0 2+2A ).\nThis embedding method can also restrict the upper bound of cosine similarity between two feature vector when time step is different:\n\u27e8[sin t1, z1 cos t1], [sin t2, z2 cos t2]\u27e9 =sin t1 sin t2 + \u27e8z1, z2\u27e9 cos t1 cos t2 \u2264 sin t1 sin t2 + cos t1 cos t2 =cos(t1 \u2212 t2)\n(13)\nHere t1, t2 \u2208 [\u2212\u03c02 , \u03c0 2 ] when using the embedding method in Eq. 12. \u27e8\u00b7, \u00b7\u27e9 is cos similarity (since the vectors here are all unit vectors, it is same as inner product). Larger difference between t1 and t2 leads to smaller value of cos(t1 \u2212 t2), which means smaller cosine similarity."
        },
        {
            "heading": "A.2 WHY VQ-VAE?",
            "text": "When discovering key states, we need to give every state in the trajectory a label and decide the key states based on the labels. VQ-VAE naturally provides a process of assigning symbols for inputs, which is suitable for partition and segmentation. The remaining task involves assigning meanings to the vectors in the codebook using self-supervised learning, which is achieved through the proposed generative and discriminative goal losses."
        },
        {
            "heading": "A.3 DETAILS OF CONCEPT",
            "text": "The concepts {\u03b1k}Kk=1 is detailed model as two vectors, one for generative goal and one for discriminative goal: {(\u03b1k, pk)}Kk=1 \u03b1k will be used for concept assignment (Eq. 5) and prediction of states that achieve the goal in generative goal loss (Eq. 6), and pk is the compressed parameters for compatibility function (Eq. 2). pk is able to be transformed into a simple MLP network using hyper-network (David et al. 2016, detailed design in Sec. A.7). See Sec. A.4, Sec. A.5 also for more details of usage and update of the parameters in concepts during training."
        },
        {
            "heading": "A.4 UPDATE OF PROTOTYPES",
            "text": "We use EMA moving (Li et al., 2020) to update the prototype of features representing concepts. when training and \u03b1k is assigned with a set of extracted features (Eq. 4). We will update \u03b1k using the average of the extracted features:\nz\u0304 = Normalize( 1 |{z : \u03b7(z) = k}| \u2211\n\u03b7(z)=k\nz)\n\u03b1k \u2190 Normalize(cema\u03b1k + (1\u2212 cema)z\u0304) (14)\nHere 0 < cema < 1. Notice that we did not update \u03b1k based on each single feature z, since we find it is inefficient when training. Our experiments use cema = 0.9."
        },
        {
            "heading": "A.5 PRESERVE GRADIENT FLOW",
            "text": "When assigning concepts (Eq. 5), the gradient cannot naturally propagate back to the encoder (Eq. 4). We use technique similar to VQ-VAE to achieve this. If we have a set of concept features {\u03b1k}Kk=1, and the extracted feature from current state is z. We calculate the probability of choosing a certain concept using cosine similarity:\np(z ) \u03b1k) = exp(\u27e8z, \u03b1k\u27e9/\u03c4)\u2211K k=1 exp(\u27e8z, \u03b1k\u27e9/\u03c4) , (15)\nSo we can use the soft version to preserve gradient:\n\u03b1soft = K\u2211 k=1 p(z ) \u03b1k) SG(\u03b1k) , \u03b1\u03b7(z) = \u03b1 soft + SG(\u03b1\u03b7(z) \u2212 \u03b1soft)\npsoft = K\u2211 k=1 p(z ) \u03b1k) SG(pk) , p\u03b7(z) = p soft + SG(p\u03b7(z) \u2212 psoft)\n(16)\nWhere \u03b7(z) is the same concept assignment function as in Eq. 5, SG(\u00b7) is the stop gradient operation, and the definition of \u03b1k and pk is same as concept vectors in Sec. A.4. We only hope the gradient to adjust the selection of concept, so we stop the gradient of prototype \u03b1k and compressed parameters pk. \u03b1k will be updated using the gradient from generative goal loss (Eq. 6), EMA (Sec. A.4), and reconstruction regularization at next Sec. A.6 .pk will be updated using the gradient from discriminative goal loss (Eq. 7 and Eq. 8)."
        },
        {
            "heading": "A.6 RECONSTRUCTION REGULARIZATION",
            "text": "To prevent over smoothing of discovered concepts (always one concept) similar to VQ-VAE, we add in a reconstruction process from extracted features \u03b1 to original states: s\u0302it = \u03d5 \u22121(\u03b1it|{\u03b1ij} t\u22121 j=1). (Here \u03b1it is the assigned concept from {\u03b1k}Kk=1 to state sit in \u03c4i = {(sit, ait)} T (i) t=1 from data). Since the maximum number of concepts is an hyper-parameter and we would not choose a very large number, we do not hope the reconstruction process to be trained well enough. The training loss for reconstruction:\nLrec(\u03d5\u22121) = 1 N N\u2211 i=1 1 T (i) T (i)\u2211 t=1 \u2225sit \u2212 \u03d5\u22121(\u03b1it|{\u03b1ij}t\u22121j=1)\u2225 (17)\nWe \u201cpretrain\u201d InfoCon with only the reconstruction loss above and the classification entropy loss (Eq. 9) before using generative and discriminative goal loss:\nLpre = Lrec(\u03d5\u22121) + \u03bbLent(\u03b1, \u03d5) (18)\nDuring experiments, we find that the above loss can be used to warm up the concept discovery VQ-VAE structure via pertaining to provide a good initialization. The initialization can then help achieve better convergence when the proposed generative and discriminative losses are employed for self-supervised concept discovery. To show the effectiveness of the initialization above, we provide counts of activated manipulation concepts (within the codebook of the VQ-VAE) with and without the usage of this initialization when discovering key concepts. Our observation is that the above initialization can help discover more fined-grained manipulation concepts, as shown by the number of activated concepts. Detailed results are in Tab. 5 below.\nNotice that there are also other methods that can alleviate over-smoothing in VQ-VAE (Roy et al., 2018), but these methods focus on the geometry clustering characteristic of VQ-VAE, which may conflict with our design of generative goal and discriminative goal. On the other hand, regularization with construction is empirically reasonable according to self-supervised learning."
        },
        {
            "heading": "A.7 HYPER-NETWORK",
            "text": "We provide details of our hyper-network for compatibility function (Eq. 2) at Fig. 7."
        },
        {
            "heading": "B TRAINING DETAILS",
            "text": ""
        },
        {
            "heading": "B.1 PSEDUO CODE",
            "text": "See 1 for the training scheme of our method."
        },
        {
            "heading": "B.2 HYPER PARAMETERS",
            "text": "InfoCon. We use the structure of Transformer used in (Jia et al., 2023), which refers the design of (Brown et al., 2020). The state encoder (Eq. 4) and state re-constructor in A.6 both use a 4-layer causal Transformer. The goal based policy in Eq. 8 use a 1-layer causal Transformer. The predictor for generative goal in Eq. 6 use a 2-layer causal Transformer. For hyper-network in A.7, we also use only one hidden layer in the generated goal function. The number of concepts is fixed, maximum number of 10 manipulation concepts for all the tasks. The temperature \u03c4 in Eq. 5 and Fig. 7 is 0.1. A in Eq. 12 is 0.2. All the size of hidden features output by Transformers and concept features {\u03b1k}Kk=1 is 128. When training, the coefficient \u03bb, \u03bbrec in Eq. 18, Eq. 10 is 0.001, 0.1, and we will\ndefer optimization of Lgen until half of iteration for training is done. We pretrain InfoCon according to Eq. 18 for 1\u00d7104 iteration with base learning rate 1.0\u00d710\u22124. Then we train the InfoCon for each of the task with 1.6\u00d7106 iterations based on the pretrained model with base learning rate 1.0\u00d710\u22124. After labeling the original data with key states using trained InfoCon models, we train our CoTPC policies for 1.8 \u00d7 106 iterations with base learning rate 5.0 \u00d7 10\u22124. For the three training stages, we all use AdamW optimizer and warm-up cosine annealing scheduler which linearly increases learning rate from 0.1 of base learning rate to the base learning rate for 1000 iteration, and then decreases learning rate from base learning rate to 0.1 of base learning rate. The weight decay is always 1.0 \u00d7 10\u22123, and batch size is 256. For practice, we would only use a segment of 60 states (along with actions) for every item (trajectory) in the batch.\nBaselines. Here we only give some essential implementation details of the two baseline methods: CoTPC with LLM sub-goal and CLIP detection of key states, CoTPC with key states discovered by AWE Shi et al. (2023). Notice that after the discover of key states, the training of CoTPC policies are same as description in B.2.\n\u2022 CoTPC (LLM+CLIP). We discover that the CLIP scores of different key states in a manipulation trajectory are close to each other. Thus, it is hard for us to set a reasonable threshold to decide whether some of the states have already achieved the sub-goal. for each text description of the key states, we will use the average of the minimum and maximum value of CLIP scores to decide the threshold, and select the state with minimum temporal step and is larger than this threshold to be the key states of this sub-goal. (Notice that it is not reasonable to select the maximum score, since the states after achieving the goal still have the feature in the description of key states. Like \u201cgrasp the cube\u201d, after this sub-goal is achieved, most of the states after it are also suitable for this description in tasks like P&P Cube and Stack Cube.).\n\u2022 CoTPC (AWE). In AWE research, they set thresholds for end condition of the dynamic programming of finding way-points with different manipulation tasks. Here we modify the method so that it can discover a fix number (here we choose 10 to align with InfoCon) of key states for all trajectories, which is more suitable since current implementation of CoTPC needs to fix the number of key states.\nAlgorithm 1 InfoCon Input: demo trajectories D\u03c4 = {\u03c4i = (sit, ait) T (i) t=1 }Ni=1\nModules: state encoder \u03d5, achieved state predictor \u03b8g, compatilbility function hyper-network C, discriminative goal policy \u03c0 concepts {\u03b1k}Kk=1 Output: trained state encoder \u03d5\nfor i = 1, 2, ..., N, t = 1, 2, ..., T (i) do zit = \u03d5(s i t|{(sij , aij)} t\u22121 j=1)\nfor k = 1, 2, . . . ,K do\np(zit ) \u03b1k) = exp(\u27e8zit,SG(\u03b1k)\u27e9/\u03c4)\u2211K k=1 exp(\u27e8zit,SG(\u03b1k)\u27e9/\u03c4)\nend for \u03b1it = \u03b1argmaxk p(zit )\u03b1k)\nend for for k = 1, 2, ...,K do\nz\u0304k = Normalize( 1 |\u03b1it:\u03b1it=\u03b1k| \u2211 \u03b1it=\u03b1k zit)\n\u03b1k \u2190 Normalize(cema\u03b1k + (1\u2212 cema)z\u0304k) end for for i = 1, 2, ..., N, t = 1, 2, ..., T (i) do\nfor k = 1, 2, . . . ,K do\np(zit ) \u03b1k) = exp(\u27e8zit,SG(\u03b1k)\u27e9/\u03c4)\u2211K k=1 exp(\u27e8zit,SG(\u03b1k)\u27e9/\u03c4)\n\u25b7 Recalculate with updated \u03b1k.\nend for sikt = s i argminu{u\u2265t,\u03b1iu \u0338=\u03b1iu+1}\n\u25b7 Last state of sub-trajectory as key state end for Calculate Lgen(\u03b1, \u03d5; \u03b8g), Ldisc (\u03b1, \u03d5; C), Ldisa (\u03b1, \u03d5; C, \u03c0), Lent(\u03b1, \u03d5), Lrec(\u03d5\u22121) \u25b7 Eq. 6, Eq. 7, Eq. 8, Eq. 9, Eq. 17 L = Lgen + Ldisa + \u03bb(Ldisc + Lent) + \u03bbrecLrec Back propagation from L"
        },
        {
            "heading": "C MORE EXPERIMENT RESULTS",
            "text": ""
        },
        {
            "heading": "C.1 OVER-FITTING ISSUE",
            "text": "In Table 1, a noticeable discrepancy is observed in the performance of policies between seen and unseen environments, indicative of an over-fitting issue. Employing the CoTPC with key states of InfoCon, we conducted experiments to evaluate the influence of scaling the training dataset on overfitting. The results of these experiments are presented in Tab. 6. From these results, it is evident that the impact of augmenting the training data on mitigating over-fitting varies across different tasks."
        },
        {
            "heading": "SR(%) P&P Cube Stack Cube Turn Faucet Peg Insertion",
            "text": ""
        },
        {
            "heading": "C.2 MORE BASELINE METHODS",
            "text": "We provide the results of baseline methods without the usage of concepts or semantic information in seen environment in Tab. 7. The data is borrowed from (Jia et al., 2023). According to this source, Decision Transformer exhibits the best performance in unseen environments among all methods evaluated. Consequently, the performance of other methods in unseen environments is not provided, except for that of Decision Transformer."
        },
        {
            "heading": "C.3 CONCEPTS AND CONCEPT-DRIVEN POLICIES",
            "text": "While our experiments highlight the strength of InfoCon in identifying key states, integrating this with policy generation is an exciting and unexplored area. We see a valuable opportunity for future work in developing a method that not only discovers key states using InfoCon but also generates a policy similar to CoTPC.\nConsidering the selection of policy, there are not many methods that focus on making use of key states in their decision-making policies with a tailored design. Here are some methods that are relevant to our knowledge:\n\u2022 MaskDP (Liu et al., 2022). This method has a weakness: appending the end state into the input sequence is needed. The agent must know the exact key states based on the initial state, which makes it constrained to achieving a very specific goal and is not applicable to our problem setting, where the end goal state is not provided.\n\u2022 Modified Decision Transformer. This is a method we found in the work of AWE (Shi et al., 2023), in which they adapt the original decision transformer Chen et al., 2021 to let it leverage key state prediction during policy learning.\n\u2022 CoTPC (Jia et al., 2023), which is the method we mainly employ in our work.\nWe trained the Modified Decision Transformer and saw that policies based on InfoCon also have better performance compared with the Decision Transformer without key states or using groundtruth (GT) key states. The results are in the table below."
        },
        {
            "heading": "D MORE VISUALIZATION RESULTS",
            "text": "We provide more visualization results of key states labeled out by InfoCon at Fig.10, 11, 12, 13, 14, 15"
        },
        {
            "heading": "E ALIGNMENT WITH HUMAN SEMANTICS",
            "text": "A variety of methods have been devised to identify elements similar to key states (Neitz et al., 2018; Jayaraman et al., 2018; Pertsch et al., 2020; Zhu et al., 2022; Caldarelli et al., 2022). Our approach surpasses them by ensuring that the identified key states stem from well-defined concepts. Specifically, in our framework, these concepts pertain to the modeling of goal (Eq. 1,Eq. 2,Eq. 3). Furthermore, we demonstrate that the concepts encapsulated within the key states of InfoCon share similarity with intricate human semantics, as substantiated by the evaluation of the Human Intuition Score (HIS, Eq. 11).\nIn this section, we present further attempts and results to align the self-discovered manipulation concepts by InfoCon with human semantics for potential enhancement in the explainability when facing human-robot interaction. We perform the analysis with the robotic task of \u201cPeg-Insertion.\u201d The major goal is to check whether we can assign reasonable and linguistically meaningful names or descriptions to the discovered concepts or key states.\nBefore we perform some qualitative analysis, we first determine the manipulation concepts discovered for this task by checking the activated concepts (codes) among all the codes in the codebook (in total 10) of the VQ-VAE employed. Namely, all the trajectories of the task \u201cPeg-Insertion\u201d are pushed through the VQ-VAE, the activated codes will be recorded for each of them, and then the activation rate for each code (concept) is computed.\nTab. 9 provides the activation rate of each manipulation concept related to task Peg Insertion using the concept discovery VQ-VAE trained by InfoCon.\nThe table above indicates that nearly every trajectory performing task Peg Insertion has the same set of key states activated, which means that the key states (concepts) involved in accomplishing Peg Insertion are the discovered concepts with numbering: #0, #1, #2, #4, #5, #6, #7, and #8, in total 8 vectors in the code book of VQ-VAE in InfoCon. (#3 and #9 are probably noise, given their low activation rate).\nNext, we use a sequence to illustrate the description we assign to each of the discovered concepts, which help align with human semantics. The concept names are the following:\n1 . The gripper is positioned above the peg (discovered concept #7). 2 . The gripper is aligned with the peg and ready to grasp (discovered concept #5). 3 . The peg is grasped (discovered concept #0). 4 . The peg is grasped and lifted (discovered concept #1). 5 . The peg is aligned with the hole distantly (discovered concept #4). 6 . The peg is aligned with the hole closely (discovered concept #6). 7 . The peg is inserted half-way into the hole (discovered concept #8). 8 . The peg is fully inserted (discovered concept #2).\nPlease check Fig. 8 for visuals corresponding to these discovered concepts. The above verifies that humans can still assign descriptions to the discovered concepts by InfoCon. At the same time, we can see the potential of InfoCon to discover meaningful concepts that can be aligned with human\nsemantics. Moreover, it shows that InfoCon has the capability of discovering more fine-grained concepts or key states human annotators (in CoTPC) have ignored or are unaware of. These further demonstrate the effectiveness of the proposed InfoCon for automatically discovering meaningful manipulation concepts.\nF INFOCON WITH REAL-WORLD DATA\nInfoCon can be applied in various tasks involving sequential events (from manipulation, to sound, video, natural language, log of computer systems, cache behavior of memory, and changing of climate and weather). These processes can all be regarded as a trajectory with meaningful partitions in achieving specific goals. Here we provide a visualization of the results of applying InfoCon on human pose sequences estimated from real-world videos. We can see that it also separates a video of pose motion into different key states, which can be mapped to different \u201chuman motion concepts\u201d."
        }
    ],
    "year": 2024
}