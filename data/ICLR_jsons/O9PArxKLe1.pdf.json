{
    "abstractText": "Untrustworthy users can misuse image generators to synthesize high-quality deepfakes and engage in unethical activities. Watermarking deters misuse by marking generated content with a hidden message, enabling its detection using a secret watermarking key. A core security property of watermarking is robustness, which states that an attacker can only evade detection by substantially degrading image quality. Assessing robustness requires designing an adaptive attack for the specific watermarking algorithm. When evaluating watermarking algorithms and their (adaptive) attacks, it is challenging to determine whether an adaptive attack is optimal, i.e., the best possible attack. We solve this problem by defining an objective function and then approach adaptive attacks as an optimization problem. The core idea of our adaptive attacks is to replicate secret watermarking keys locally by creating surrogate keys that are differentiable and can be used to optimize the attack\u2019s parameters. We demonstrate for Stable Diffusion models that such an attacker can break all five surveyed watermarking methods at no visible degradation in image quality. Optimizing our attacks is efficient and requires less than 1 GPU hour to reduce the detection accuracy to 6.3% or less. Our findings emphasize the need for more rigorous robustness testing against adaptive, learnable attackers.",
    "authors": [
        {
            "affiliations": [],
            "name": "IMAGE WATERMARKS"
        },
        {
            "affiliations": [],
            "name": "Nils Lukas"
        },
        {
            "affiliations": [],
            "name": "Abdulrahman Diaa"
        },
        {
            "affiliations": [],
            "name": "Lucas Fenaux"
        },
        {
            "affiliations": [],
            "name": "Florian Kerschbaum"
        }
    ],
    "id": "SP:b07bed0d94e1aec3e0e9700f62a9d3c2ba38c83c",
    "references": [
        {
            "authors": [
                "Yossi Adi",
                "Carsten Baum",
                "Moustapha Cisse",
                "Benny Pinkas",
                "Joseph Keshet"
            ],
            "title": "Turning your weakness into a strength: Watermarking deep neural networks by backdooring",
            "venue": "In 27th USENIX Security Symposium (USENIX Security",
            "year": 2018
        },
        {
            "authors": [
                "Mohamed Akrout",
                "B\u00e1lint Gyepesi",
                "P\u00e9ter Holl\u00f3",
                "Adrienn Po\u00f3r",
                "Bl\u00e1ga Kincs\u0151",
                "Stephen Solis",
                "Katrina Cirone",
                "Jeremy Kawahara",
                "Dekker Slade",
                "Latif Abid"
            ],
            "title": "Diffusion-based data augmentation for skin disease classification: Impact across original medical datasets to fully synthetic images",
            "venue": "arXiv preprint arXiv:2301.04802,",
            "year": 2023
        },
        {
            "authors": [
                "Arpit Bansal",
                "Ping-yeh Chiang",
                "Michael J Curry",
                "Rajiv Jain",
                "Curtis Wigington",
                "Varun Manjunatha",
                "John P Dickerson",
                "Tom Goldstein"
            ],
            "title": "Certified neural network watermarks with randomized smoothing",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Clark Barrett",
                "Brad Boyd",
                "Ellie Burzstein",
                "Nicholas Carlini",
                "Brad Chen",
                "Jihye Choi",
                "Amrita Roy Chowdhury",
                "Mihai Christodorescu",
                "Anupam Datta",
                "Soheil Feizi"
            ],
            "title": "Identifying and mitigating the security risks of generative ai",
            "venue": "arXiv preprint arXiv:2308.14840,",
            "year": 2023
        },
        {
            "authors": [
                "Dan Boneh",
                "Andrew J Grotto",
                "Patrick McDaniel",
                "Nicolas Papernot"
            ],
            "title": "How relevant is the turing test in the age of sophisbots",
            "venue": "IEEE Security & Privacy,",
            "year": 2019
        },
        {
            "authors": [
                "Tim Brooks",
                "Aleksander Holynski",
                "Alexei A Efros"
            ],
            "title": "Instructpix2pix: Learning to follow image editing instructions",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Nicholas Carlini",
                "David Wagner"
            ],
            "title": "Adversarial examples are not easily detected: Bypassing ten detection methods",
            "venue": "In Proceedings of the 10th ACM workshop on artificial intelligence and security,",
            "year": 2017
        },
        {
            "authors": [
                "Ingemar Cox",
                "Matthew Miller",
                "Jeffrey Bloom",
                "Jessica Fridrich",
                "Ton Kalker"
            ],
            "title": "Digital watermarking and steganography",
            "venue": "Morgan kaufmann,",
            "year": 2007
        },
        {
            "authors": [
                "Yingqian Cui",
                "Jie Ren",
                "Han Xu",
                "Pengfei He",
                "Hui Liu",
                "Lichao Sun",
                "Jiliang Tang"
            ],
            "title": "Diffusionshield: A watermark for copyright protection against generative diffusion models",
            "venue": "arXiv preprint arXiv:2306.04642,",
            "year": 2023
        },
        {
            "authors": [
                "Boris Dayma",
                "Suraj Patil",
                "Pedro Cuenca",
                "Khalid Saifullah",
                "Tanishq Abraham",
                "Ph\u00fac Le Khac",
                "Luke Melas",
                "Ritobrata Ghosh"
            ],
            "title": "URL https://github.com/ borisdayma/dalle-mini",
            "venue": "Dall e mini,",
            "year": 2021
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Federal Register"
            ],
            "title": "Safe, secure, and trustworthy development and use of artificial intelligence. https://www.federalregister.gov/documents/2023/11/01/2023-24283/ safe-secure-and-trustworthy-development-and-use-of-artificialintelligence, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Pierre Fernandez",
                "Guillaume Couairon",
                "Herv\u00e9 J\u00e9gou",
                "Matthijs Douze",
                "Teddy Furon"
            ],
            "title": "The stable signature: Rooting watermarks in latent diffusion models",
            "venue": "arXiv preprint arXiv:2303.15435,",
            "year": 2023
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "Communications of the ACM,",
            "year": 2020
        },
        {
            "authors": [
                "Sven Gowal",
                "Pushmeet Kohli"
            ],
            "title": "Identifying ai-generated images with synthid, 2023",
            "venue": "URL https://www.deepmind.com/blog/identifying-ai-generated-imageswith-synthid",
            "year": 2023
        },
        {
            "authors": [
                "Alexei Grinbaum",
                "Laurynas Adomaitis"
            ],
            "title": "The ethical need for watermarks in machine-generated language",
            "venue": "arXiv preprint arXiv:2209.03118,",
            "year": 2022
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Zhengyuan Jiang",
                "Jinghuai Zhang",
                "Neil Zhenqiang Gong"
            ],
            "title": "Evading watermark based detection of ai-generated content",
            "venue": "arXiv preprint arXiv:2305.03807,",
            "year": 2023
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In Computer Vision\u2013ECCV",
            "year": 2014
        },
        {
            "authors": [
                "Cheng Lu",
                "Yuhao Zhou",
                "Fan Bao",
                "Jianfei Chen",
                "Chongxuan Li",
                "Jun Zhu"
            ],
            "title": "Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models",
            "venue": "arXiv preprint arXiv:2211.01095,",
            "year": 2022
        },
        {
            "authors": [
                "Nils Lukas",
                "Florian Kerschbaum"
            ],
            "title": "Ptw: Pivotal tuning watermarking for pre-trained image generators",
            "venue": "32nd USENIX Security Symposium,",
            "year": 2023
        },
        {
            "authors": [
                "Nils Lukas",
                "Edward Jiang",
                "Xinda Li",
                "Florian Kerschbaum. Sok"
            ],
            "title": "How robust is image classification deep neural network watermarking",
            "venue": "IEEE Symposium on Security and Privacy (SP),",
            "year": 2022
        },
        {
            "authors": [
                "Yisroel Mirsky",
                "Wenke Lee"
            ],
            "title": "The creation and detection of deepfakes: A survey",
            "venue": "ACM Computing Surveys (CSUR),",
            "year": 2021
        },
        {
            "authors": [
                "Sen Peng",
                "Yufei Chen",
                "Cong Wang",
                "Xiaohua Jia"
            ],
            "title": "Protecting the intellectual property of diffusion models by the watermark diffusion process",
            "venue": "arXiv preprint arXiv:2306.03436,",
            "year": 2023
        },
        {
            "authors": [
                "Renana Peres",
                "Martin Schreier",
                "David Schweidel",
                "Alina Sorescu"
            ],
            "title": "On chatgpt and beyond: How generative artificial intelligence may affect research, teaching, and practice",
            "venue": "International Journal of Research in Marketing,",
            "year": 2023
        },
        {
            "authors": [
                "Dustin Podell",
                "Zion English",
                "Kyle Lacey",
                "Andreas Blattmann",
                "Tim Dockhorn",
                "Jonas M\u00fcller",
                "Joe Penna",
                "Robin Rombach"
            ],
            "title": "Sdxl: improving latent diffusion models for high-resolution image synthesis",
            "year": 1952
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L Denton",
                "Kamyar Ghasemipour",
                "Raphael Gontijo Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Romain Beaumont",
                "Richard Vencu",
                "Cade Gordon",
                "Ross Wightman",
                "Mehdi Cherti",
                "Theo Coombes",
                "Aarush Katta",
                "Clayton Mullis",
                "Mitchell Wortsman"
            ],
            "title": "Laion-5b: An open large-scale dataset for training next generation image-text models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Yusuke Uchida",
                "Yuki Nagai",
                "Shigeyuki Sakazawa",
                "Shin\u2019ichi Satoh"
            ],
            "title": "Embedding watermarks into deep neural networks",
            "venue": "In Proceedings of the 2017 ACM on international conference on multimedia retrieval,",
            "year": 2017
        },
        {
            "authors": [
                "Yuxin Wen",
                "John Kirchenbauer",
                "Jonas Geiping",
                "Tom Goldstein"
            ],
            "title": "Tree-ring watermarks: Fingerprints for diffusion images that are invisible and robust",
            "venue": "arXiv preprint arXiv:2305.20030,",
            "year": 2023
        },
        {
            "authors": [
                "Ning Yu",
                "Vladislav Skripniuk",
                "Dingfan Chen",
                "Larry Davis",
                "Mario Fritz"
            ],
            "title": "Responsible disclosure of generative models using scalable fingerprinting",
            "venue": "arXiv preprint arXiv:2012.08726,",
            "year": 2020
        },
        {
            "authors": [
                "Ning Yu",
                "Vladislav Skripniuk",
                "Sahar Abdelnabi",
                "Mario Fritz"
            ],
            "title": "Artificial fingerprinting for generative models: Rooting deepfake attribution in training data",
            "venue": "In Proceedings of the IEEE/CVF International conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Kevin Alex Zhang",
                "Lei Xu",
                "Alfredo Cuesta-Infante",
                "Kalyan Veeramachaneni"
            ],
            "title": "Robust invisible video watermarking with attention",
            "venue": "arXiv preprint arXiv:1909.01285,",
            "year": 2019
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Yunqing Zhao",
                "Tianyu Pang",
                "Chao Du",
                "Xiao Yang",
                "Ngai-Man Cheung",
                "Min Lin"
            ],
            "title": "A recipe for watermarking diffusion models",
            "venue": "arXiv preprint arXiv:2303.10137,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deepfakes are images synthesized using deep image generators that can be difficult to distinguish from real images. While deepfakes can serve many beneficial purposes if used ethically, for example, in medical imaging (Akrout et al., 2023) or education (Peres et al., 2023), they also have the potential to be misused and erode trust in digital media. Deepfakes have already been used in disinformation campaigns (Boneh et al., 2019; Barrett et al., 2023) and social engineering attacks (Mirsky & Lee, 2021), highlighting the need for methods that control the misuse of deep image generators.\nWatermarking offers a solution to controlling misuse by embedding hidden messages into all generated images that are later detectable using a secret watermarking key. Images detected as deepfakes can be flagged by social media platforms or news agencies, which can mitigate potential harm (Grinbaum & Adomaitis, 2022). Providers of large image generators such as Google have announced the deployment of their own watermarking methods (Gowal & Kohli, 2023) to enable the detection of deepfakes and promote the ethical use of their models, which was also declared as one of the main goals in the US government\u2019s \u201cAI Executive Order\u201d (Federal Register, 2023).\nA core security property of watermarking is robustness, which states that an attacker can evade detection only by substantially degrading the image\u2019s quality. While several watermarking methods have been proposed for image generators (Wen et al., 2023; Zhao et al., 2023; Fernandez et al., 2023), none of them are certifiably robust (Bansal et al., 2022) and instead, robustness is tested empirically using a limited set of known attacks. Claimed security properties of previous watermarking methods have been broken by novel attacks (Lukas et al., 2022), and no comprehensive method exists to validate robustness, which causes difficulty in trusting the deployment of watermarking in practice. We propose testing the robustness of watermarking by defining robustness using objective\n*Equal Contribution\nfunction and approaching adaptive attacks as an optimization problem. Adaptive attacks are specific to the watermarking algorithm used by the defender but have no access to the secret watermarking key. Knowledge of the watermarking algorithm enables the attacker to consider a range of surrogate keys similar to the defender\u2019s key. This also presents a challenge for optimization since the attacker only has imperfect information about the optimization problem. Adaptive attackers had previously been shown to break the robustness of watermarking for image classifiers (Lukas et al., 2022), but attacks had to be handcrafted against each watermarking method. Finding attack parameters through an optimization process can be challenging when the watermarking method is not easily optimizable, for instance, when it is not differentiable. Our attacks leverage optimization by approximating watermark verification through a differentiable process. Figure 1 shows that our adaptive attacker can prepare their attacks before the provider deploys their watermark. We show that adaptive, learnable attackers, whose parameters can be optimized efficiently, can evade watermark detection for 1 billion parameter Stable Diffusion models at a negligible degradation in image quality."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "Latent Diffusion Models (LDMs) are state-of-the-art generative models for image synthesis (Rombach et al., 2022). Compared to Diffusion Models (Sohl-Dickstein et al., 2015), LDMs operate in a latent space using fixed, pre-trained autoencoder consisting of an image encoder E and a decoder D. LDMs use a forward and reverse diffusion process across T steps. In the forward pass, real data point x0 is encoded into a latent point z0 = E(x0) and is progressively corrupted into noise via Gaussian perturbations. Specifically,\nq(zt|zt\u22121) = N ( zt; \u221a 1\u2212 \u03b2tzt\u22121, \u03b2tI ) , t \u2208 {0, 1, . . . , T \u2212 1}, (1)\nwhere \u03b2t is the scheduled variance. In the reverse process, a neural network f\u03b8 guides the denoising, taking zt and time-step t as inputs to predict zt\u22121 as f\u03b8(xt, t). The model is trained to minimize the mean squared error between the predicted and actual zt\u22121. The outcome is a latent z\u03020 resembling z0 that can be decoded into x\u03020 = D(z0). Synthesis in LDMs can be conditioned with textual prompts."
        },
        {
            "heading": "2.1 WATERMARKING",
            "text": "Watermarking embeds a hidden signal into a medium, such as images, using a secret watermarking key that is later extractable using the same secret key. Watermarking can be characterized by the medium used by the defender to verify the presence of the hidden signal. White-box and black-\nbox watermarking methods assume access to the model\u2019s parameters or query access via an API, respectively, and have been used primarily for Intellectual Property protection (Uchida et al., 2017)1.\nNo-box watermarking (Lukas & Kerschbaum, 2023) assumes a more restrictive setting where the defender only knows the generated content but does not know the query used to generate the image. This type of watermarking has been used to control misuse by having the ability to detect any image generated by the provided image generator (Gowal & Kohli, 2023). Given a generator\u2019s parameters \u03b8G, a no-box watermarking method defines the following three procedures.\n\u2022 \u03c4 \u2190 KEYGEN(\u03b8G): A randomized function to generate a watermarking key \u03c4 .\n\u2022 \u03b8\u2217G \u2190 EMBED(\u03b8G, \u03c4,m): For a generator \u03b8G, a watermarking key \u03c4 and a message m, return parameters \u03b8\u2217G of a watermarked generator 2 that only generates watermarked images.\n\u2022 p \u2190 VERIFY(x, \u03c4,m): This function (i) extracts a message m\u2032 from x using \u03c4 and (ii) returns the p-value to reject the null hypothesis that m and m\u2032 match by random chance.\nA watermarking method is a set of algorithms that specify (KEYGEN, EMBED, VERIFY). A watermark is a hidden signal in an image that can be mapped to a message m using a secret key \u03c4 . The key refers to secret random bits of information used in the randomized verification algorithm to detect a message. Adaptive attackers know the watermarking method but not the key message pair. Carlini & Wagner (2017) first studied adaptive attacks in the context of adversarial attacks.\nIn this paper, we denote the similarity between two messages by their L1-norm difference. We use more meaningful similarity measures whenM allows it, such as the Bit-Error-Rate (BER) when the messages consist of bits. A watermark is retained in an image if the verification procedure returns p < 0.01, following Wen et al. (2023). Adi et al. (2018) specify the requirements for trustworthy watermarking, and we focus on two properties: Effectiveness and robustness. Effectiveness states that a watermarked generator has a high image quality while retaining the watermark, and robustness means that a watermark is retained in an image unless the image\u2019s quality is substantially degraded. We refer to Lukas & Kerschbaum (2023) for security games encoding effectiveness and robustness."
        },
        {
            "heading": "2.2 WATERMARKING FOR IMAGE GENERATORS",
            "text": "Several works propose no-box watermarking methods to prevent misuse for two types of image generators: Generative Adversarial Networks (GANs) (Goodfellow et al., 2020) and Latent Diffusion Models (LDMs) (Rombach et al., 2022). We distinguish between post-hoc watermarking methods that apply an imperceptible modification to an image and semantic watermarks that modify the output distribution of an image generator and are truly \u201cinvisible\u201d (Wen et al., 2023).\nFor post-hoc watermarking, traditional methods hide messages using the Discrete Wavelet Transform (DWT) and Discrete Wavelet Transform with Singular Value Decomposition (DWTSVD) (Cox et al., 2007) and are currently used for Stable Diffusion. RivaGAN (Zhang et al., 2019) watermarks by training a deep neural network adversarially to stamp a pattern on an image. Yu et al. (2020; 2021) propose two methods that modify the generator\u2019s training procedure but require expensive re-training from scratch. Lukas & Kerschbaum (2023) propose a watermarking method for GANs that can be embedded into a pre-trained generator. Zhao et al. (2023) propose a general method to watermark diffusion models (WDM) that uses a method similar to Yu et al. (2020), which trains an autoencoder to stamp a watermark on all training data before also re-training the generator from scratch. Fernandez et al. (2023) pre-train an autoencoder to encode hidden messages into the training data and embed the watermark by fine-tuning the decoder D component of the LDM. Wen et al. (2023) are the first to propose a semantic watermarking method for LDMs they call TreeRings Watermarks (TRW). The idea is to mark the initial noise xT with a detectable, tree-ring-like pattern m in the frequency domain before generating an image. During detection, they leverage the property of LDM\u2019s that the diffusion process is invertible, which allows mapping an image back to its original noise. The verification extracts a message m\u2032 by spectral analysis and tests whether the same tree-ring patterns m are retained in the frequency domain of the reconstructed noise.\n1Uchida et al. (2017) study watermarking image classifiers. Our categorization is independent of the task. 2Embedding can alter the entire generation process, including adding pre- and post-processors.\nSurveyed Watermarking Methods. In this paper, we evaluate the robustness of five watermarking methods: TRW, WDM, DWT, DWT-SVD, and RivaGAN. DWT, DWT-SVD, and RivaGAN are default choices when using StabilityAI\u2019s Stable Diffusion repository and WDM and TRW are two recently proposed methods for Stable Diffusion models. However, WDM requires re-training a Stable Diffusion model from scratch, which can require 150-1000 GPU days (Dhariwal & Nichol, 2021) and is not replicable with limited resources. For this reason, instead of using the autoencoder on the input data, we apply their autoencoder as a post-processor after generating images."
        },
        {
            "heading": "3 THREAT MODEL",
            "text": "We consider a provider capable of training large image generators who make their generators accessible to many users via a black-box API, such as OpenAI with DALL\u00b7E. Users can query the generator by including a textual prompt that controls the content of the generated image. We consider an attack by an untrustworthy user who wants to misuse the provided generator without detection.\nProvider\u2019s Capabilities and Goals (Model Capabilities) The provider fully controls image generation, including the ability to post-process generated images. (Watermark Verification) In a no-box setting, the defender must verify their watermark using a single generated image. The defender aims for an effective watermark that preserves generator quality while preventing the attacker from evading detection without significant image quality degradation.\nAttacker\u2019s Capabilities. (Model Capabilities) The user has black-box query access to the provider\u2019s watermarked model and also has white-box access to less capable, open-source surrogate generators, such as Stable Diffusion on Huggingface. We assume the surrogate model\u2019s image quality is inferior to the provided model; otherwise, there would be no need to use the watermarked model. Our attacker does not require access to image generators from other providers, but, of course, such access may imply access to surrogate models as our attack does require. (Data Access) The attacker has unrestricted access to real-world image and caption data available online, such as LAION-5B (Schuhmann et al., 2022). (Resources) Computational resources are limited, preventing the attacker from training their own image generator from scratch. (Queries) The provider charges the attacker per image query, limiting the number of queries they can make. The attacker can generate images either unconditionally or with textual prompts. (Adaptive) The attacker knows the watermarking method but lacks access to the secret watermarking key \u03c4 and chosen message m.\nAttacker\u2019s Goal. The attacker wants to use the provided, watermarked generator to synthesize images (i) without a watermark that (ii) have a high quality. We measure quality using a perceptual similarity function Q : X \u00d7X \u2192 R between the generated, watermarked image and a perturbed image after the attacker evades watermark detection. We require that the defender verifies the presence of a watermark correctly with a p-value of at least p < 0.01, same as Wen et al. (2023)."
        },
        {
            "heading": "4 CONCEPTUAL APPROACH",
            "text": "As described in Section 2, a watermarking method defines three procedures (KeyGen, Embed, Verify). The provider generates a secret watermarking key \u03c4 \u2190 KEYGEN(\u03b8G) that allows them to watermark their generator so that all its generated images retain the watermark. To embed a watermark, the provider chooses a message (we sample a message m \u223c M uniformly at random) and modifies their generator\u2019s parameters \u03b8\u2217G \u2190 EMBED(\u03b8G, \u03c4,m). Any image generated by \u03b8\u2217G should retain the watermark. For any x \u2190 GENERATE(\u03b8\u2217G) we call a watermark effective if (i) the watermark is retained, i.e., VERIFY(x, \u03c4,m) < 0.01 and (ii) the watermarked images have a high perceptual quality. The attacker generates images x \u2190 GENERATE(\u03b8\u2217G) and applies an image-toimage transformation, A : X \u2192 X with parameters \u03b8A to evade watermark detection by perturbing x\u0302\u2190 A(x). Finally, the defender verifies the presence of their watermark in x\u0302, as shown in Figure 1. Let W (\u03b8G, \u03c4 \u2032,m) = EMBED(\u03b8G, \u03c4 \u2032,m) be the watermarked generator after embedding with key \u03c4 \u2032 and message m and GW = GENERATE(W (\u03b8G, \u03c4 \u2032,m)) denotes the generation of an image using the watermarked generator parameters. For any high-quality GW , the attacker\u2019s objective becomes:\nmax \u03b8A E \u03c4 \u2032\u2190KEYGEN(\u03b8G)\nm\u2208M\n[VERIFY(A(GW ), \u03c4 \u2032,m) +Q(A(GW ), GW )] (2)\nThis objective seeks to maximize (i) the expectation of successful watermark evasion over all potential watermarking keys \u03c4 \u2032 and messages m (since the attacker does not know which key-message pair was chosen) and (ii) the perceptual similarity of the images before and after the attack. Note that in this paper, we define image quality as the perceptual similarity to the watermarked image before the attack. There are two obstacles for an attacker to optimize this objective: (1) The attacker has imperfect information about the optimization problem and must substitute the defender\u2019s image generator with a less capable, open-source surrogate generator. When KEYGEN depends on \u03b8G, then the distribution of keys differs, and the attack\u2019s effectiveness must transfer to keys generated using \u03b8G. (2) The optimization problem might be hard to approximate, even when perfect information is available, e.g., when the watermark verification procedure is not differentiable."
        },
        {
            "heading": "4.1 MAKING WATERMARKING KEYS DIFFERENTIABLE",
            "text": "We overcome the two aforementioned limitations by (1) giving the attacker access to a similar (but less capable) surrogate generator \u03b8\u0302G, enabling them to generate surrogate watermarking keys, and (2) by creating a method GKEYGEN(\u03b8\u0302G) that creates a surrogate watermarking key \u03b8K through which we can backpropagate gradients. A simple but computationally expensive method of creating differentiable keys \u03b8D is using Algorithm 1 to train a watermark extraction neural network with parameters \u03b8D to predict the message m from an image.\nAlgorithm 1 GKEYGEN: A Simple Method to Generate Differentiable Keys\nRequire: Surrogate generator \u03b8\u0302G, Watermarking method (KeyGen, Embed, Verify), N steps 1: \u03c4 \u2190 KEYGEN(\u03b8\u0302G) \u25b7 The surrogate key 2: for j \u2190 1 to N do 3: m \u223cM \u25b7 Sample a random message 4: \u03b8\u0302\u2217G \u2190 EMBED(\u03b8\u0302G, \u03c4,m) \u25b7 Embed the watermark 5: x\u2190 GENERATE(\u03b8\u0302\u2217G) 6: m\u2032 \u2190 EXTRACT(x; \u03b8D) 7: g\u03b8D \u2190 \u2207\u03b8D ||m\u2212m\u2032||1 \u25b7 Compute gradients using distance between messages 8: \u03b8D \u2190 \u03b8D \u2212 Adam(\u03b8D, g\u03b8D ) 9: return \u03b8D \u25b7 The surrogate key\nAlgorithm 1 generates a surrogate key (line 1) to embed a watermark into the surrogate generator and use it to generate watermarked images (lines 3-5). The attacker extracts the message (line 6) and updates the parameters of the (differentiable) watermark decoder using an Adam optimizer (Kingma & Ba, 2014). The attacker subsequently uses the decoder\u2019s parameters \u03b8D as inputs to VERIFY. Our adaptive attacker must invoke Algorithm 1 only for the non-differentiable watermarks DCT and DCT-SVD (Cox et al., 2007). The remaining three watermarking methods TRW (Wen et al., 2023), WDM (Zhao et al., 2023) and RivaGAN (Zhang et al., 2019) do not require invoking GKEYGEN. In our work, we tune the parameters \u03b8D of a ResNet-50 decoder (see Appendix A.3 for details)."
        },
        {
            "heading": "4.2 LEVERAGING OPTIMIZATION AGAINST WATERMARKS",
            "text": "Equation (2) requires finding attack parameters \u03b8A against any watermarking key \u03c4 \u2032 \u2190 KEYGEN(\u03b8G), which can be computationally expensive if the attacker has to invocate GKEYGEN many times. We find empirically that generating many keys is unnecessary, and the attacker can find effective attacks using only a single surrogate watermarking key \u03b8D \u2190 GKEYGEN(\u03b8\u0302G). We propose two learnable attacks A1,A2 whose parameters \u03b8A1 , \u03b8A2 can be optimized efficiently. The first attack, called Adversarial Noising, finds adversarial examples given an image x using the surrogate key as a reward model. The second attack called Adversarial Compression, first fine-tunes the parameters of a pre-trained autoencoder in a preparation stage and uses the optimized parameters during an attack. The availability of a pre-trained autoencoder is a realistic assumption if the attacker has access to a surrogate Stable Diffusion generator, as the autoencoder is a detachable component of any Stable Diffusion generator. Access to a surrogate generator implies the availability of a pre-trained autoencoder at no additional cost in computational resources for the attacker.\nAlgorithm 2 Adversarial Noising\nRequire: surrogate \u03b8\u0302G, budget \u03f5, image x 1: \u03b8A \u2190 0 \u25b7 adversarial perturbation 2: \u03b8D \u2190 GKEYGEN(\u03b8\u0302G) 3: m\u2190 EXTRACT(x; \u03b8D) 4: for j \u2190 1 to N do 5: m\u2032 \u2190 EXTRACT(x+ \u03b8A, \u03b8D) 6: g\u03b8A \u2190 \u2212\u2207\u03b8A ||m\u2212m\u2032||1 7: \u03b8A \u2190 P\u03f5(\u03b8A \u2212 Adam(\u03b8A, g\u03b8A))\nreturn x+ \u03b8A\nAlgorithm 3 Adversarial Compression\nRequire: surrogate \u03b8\u0302G, strength \u03b1, image x 1: \u03b8A \u2190 [\u03b8E , \u03b8D] \u25b7 Compressor parameters 2: \u03b8D \u2190 GKEYGEN(\u03b8\u0302G) \u25b7 surrogate key 3: for j \u2190 1 to N do 4: m \u223cM 5: \u03b8\u0302\u2217G \u2190 EMBED(\u03b8\u0302G, \u03b8D,m) 6: x\u2190 GENERATE(\u03b8\u0302\u2217G) 7: x\u2032 \u2190 D(E(x; \u03b8A)) \u25b7 compression 8: m\u2032 \u2190 EXTRACT(x\u2032, \u03b8D) 9: g\u03b8A \u2190 \u2207\u03b4(LLPIPS(x\u2032, x)\u2212\u03b1||m\u2212m\u2032||1) 10: \u03b8A \u2190 \u03b8A \u2212 Adam(\u03b8A, g\u03b8A) return D(E(x; \u03b8A))\nAdversarial Noising. Algorithm 2 shows the pseudocode of our adversarial noising attack. Given a surrogate generator \u03b8\u0302G, a budget \u03f5 \u2208 R+ for the maximum allowed noise perturbation, and a watermarked image x generated using the provider\u2019s watermarked model, the attacker wants to compute a perturbation within an \u03f5-ball of the L\u221e norm that evades watermark detection. The attacker generates a local surrogate watermarking key (line 2) and extracts a message m from x (line 3). Then, the attacker computes the adversarial perturbation by maximizing the distance to the initially extracted message m while clipping the perturbation into an \u03f5-ball using P\u03f5 (line 7).\nAdversarial Compression. Algorithm 3 shows the pseudocode of our adversarial compression attack. After generating a surrogate watermarking key (line 2), the attacker generates images containing a random message (lines 4-6) and uses their encoder-decoder pair to compress the images (line 7). The attacker iteratively updates their model\u2019s parameters by (i) minimizing a quality loss, which we set to the LPIPS metric (Zhang et al., 2018), and (ii) maximizing the distance between the extracted and embedded messages (line 9). The output \u03b8A of the optimization loop between lines 3 and 10 only needs to be run once, and the weights \u03b8A can be re-used in subsequent attacks.\nWe highlight that the attacker optimizes an approximation of Equation (2) since they only have access to a surrogate generator \u03b8\u0302G, but not the provider\u2019s generator \u03b8G. This may lead to a generalization gap of the attack at inference time. Even if an attacker can find optimal attack parameters \u03b8A that optimizes Equation (2) using \u03b8\u0302G, the attacker cannot test whether their attack remains effective when the defender uses a different model \u03b8G to generate watermarking keys."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "Image Generators. We experiment with Stable Diffusion, an open-source, state-of-the-art latent diffusion model. The defender deploys a Stable Diffusion-v2.0 model3 trained for 1.4m steps in total on a subset of LAION-5B (Schuhmann et al., 2022). The attacker uses a less capable Stable Diffusion-v1.14 checkpoint, trained for 431k steps in total on LAION-2B and LAION-HD. All experiments were conducted on NVIDIA A100 GPUs.\nImages are generated using a DPM solver (Lu et al., 2022) with 20 inference steps and a default guidance scale of 7.5. We create three different watermarked generators for each surveyed watermarking method by randomly sampling a watermarking key \u03c4 \u2190 KEYGEN(\u03b8G) and a message m \u223c M, used to embed a watermark. Appendix A.1 contains descriptions of the watermarking keys. All reported values represent the mean value over three independently generated secret keys.\nQuantitative Analysis. Similar to Wen et al. (2023), we report the True Positive Rate when the False Positive Rate is fixed to 1%, called the TPR@1%FPR. Appendix A.2 describes statistical tests used in the verification procedure of each watermarking method to derive p-values. We report the\n3https://huggingface.co/stabilityai/stable-diffusion-2-base 4https://huggingface.co/CompVis/stable-diffusion-v1-1\nFre\u0301chet Inception Distance (FID) (Heusel et al., 2017), which measures the similarity between real and generated images. Additionally, we report the CLIP score (Radford et al., 2021) that measures the similarity of a prompt to an image. We generate 1k images to evaluate TPR@1%FPR and 5k images to evaluate FID and CLIP score on the training dataset of MS-COCO-2017 (Lin et al., 2014)."
        },
        {
            "heading": "5.1 EVALUATING ROBUSTNESS",
            "text": "Figure 2 shows a scatter plot of the effectiveness of our attacks against all surveyed watermarking methods. We evaluate adaptive and non-adaptive attacks. Similar to Wen et al. (2023), for the nonadaptive attacks, we use Blurring, JPEG Compression, Cropping, Gaussian noise, Jittering, Quantization, and Rotation but find these attacks to be ineffective at removing the watermark. Figure 2 highlights Pareto optimal attacks for pairs of (i) watermark detection accuracies and (ii) perceptual distances. We find that only adaptive attacks evade watermark detection and preserve image quality.\nTable 1 summarizes the best attacks from Figure 2 when we set the lowest acceptable detection accuracy to 10%. When multiple attacks achieve a detection accuracy lower than 10%, we pick the attack with the lowest perceptual distance to the watermarked image. We observe that adversarial compression is an effective attack against all watermarking methods. TRW is also evaded by adversarial compression, but adversarial noising at \u03f5 = 2/255 preserves a higher image quality."
        },
        {
            "heading": "5.2 IMAGE QUALITY AFTER AN ATTACK",
            "text": "Figure 3 shows the perceptual quality after using our adaptive attacks. We show a cutout of the top left image patch with high contrasts on the bottom right to visualize noise artifacts potentially introduced by our attacks. We observe that, unlike adversarial noising, the compression attack introduces no new visible artifacts (see also Appendix A.4 for more visualizations).\nThe FID and CLIP scores of the watermarked images and the images after using adversarial noising and adversarial compression remain unchanged (see Table 2 in the Appendix). We calculate the quality using the best attack configuration from Figure 2 when the detection accuracy is less than 10%. Adversarial Noising is ineffective at removing WDM and RivaGAN for \u03f5 \u2264 10/255.\nPublished as a conference paper at ICLR 2024\n125"
        },
        {
            "heading": "5.3 ABLATION STUDY",
            "text": "Figure 4 shows ablation studies for our adaptive attacks over the (i) maximum perturbation budget and (ii) the number of compressions applied during the attack. TRW and DWT-SVD are highly vulnerable to adversarial noising, whereas RivaGAN and WDM are substantially more robust to these types of attacks. We believe this is because keys generated by RivaGAN and WDM are sufficiently randomized, which makes our attack (that uses only a single surrogate key) less effective unless the surrogate key uses similar channels as the secret key to hide the watermark. Adversarial compression without optimization of the parameters \u03b8A is ineffective at evading watermark detection against all methods except DWT. After optimization, adversarial compression evades detection from all watermarking methods with only a single compression."
        },
        {
            "heading": "6 DISCUSSION & RELATED WORK",
            "text": "Attack Scalability. The presented findings clearly indicate that even with a less capable surrogate generator, an adaptive attacker can remove all surveyed watermarks with minimal quality degradation. Our attackers generate a single surrogate key and are able to evade watermark verification, which indicates a design flaw since the key seems to have little impact. If KEYGEN were sufficiently randomized, breaking robustness should not be possible using a single key, even if the provided and surrogate generators are the same. An interesting question emerging from our study relates to the maximum difference between the watermarked and surrogate generators for the attacks to remain effective. We used a best-effort approach, by using two public checkpoints with the largest reported quality differences: Stable Diffusion v1.1 and v2. More research is needed to study the impact on the effectiveness of our attacks (i) using different models (ii) or limiting the attacker\u2019s knowledge of the method\u2019s public parameters. Our attacks evaluate the best parameters suggested by the authors.\nTypes of Learnable Attacks. Measuring the robustness against different types of learnable attacks is crucial in assessing the trustworthiness of watermarking. We explored (i) Adversarial Examples, which rely solely on the surrogate key, and (ii) Adversarial Compression, which additionally requires the availability of a pre-trained autoencoder. We believe this requirement is satisfied in practice, given that (i) training autoencoders is computationally less demanding than training Stable Diffusion, and many pre-trained autoencoders have already been made publicly available (Podell et al., 2023). Although autoencoders enhance an attacker\u2019s ability to modify images, our study did not extend to other learnable attacks such as inpainting (Rombach et al., 2022) or more potent image editing methods (Brooks et al., 2023) which could further enhance an attack\u2019s effectiveness.\nEnhancing Robustness using Adaptive and Learnable Attacks. Relying on non-adaptive attacks for evaluating a watermark\u2019s robustness is inadequate as it underestimates the attacker\u2019s capabilities. To claim robustness, the defender could (i) provide a certification of robustness (Bansal et al., 2022), or (ii) showcase empirically that their watermark withstands strong attacks. The issue is that we lack strong attackers. Although Lukas et al. (2022) demonstrated that adaptive attackers can break watermarks for image classifiers, their attacks were handcrafted and did not scale. Instead, we propose a better method of empirically testing robustness by proposing adaptive learnable attackers that require only the specification of a type of learnable attack, followed by an optimization procedure to find parameters that minimize an objective function. We believe that any watermarking method proposed in the future should evaluate robustness using our attacks and expect that future watermarking methods can enhance their robustness by incorporating our attacks.\nLimitations. Our attacks are based on the availability of the watermarking algorithm and an opensource surrogate generator to replicate keys. While providers like Stable Diffusion openly share their models, and replicas of OpenAI\u2019s DALL\u00b7E models are publicly available (Dayma et al., 2021), not all providers release information about their models. To the best of our knowledge, Google has not released their generators (Saharia et al., 2022), but efforts to replicate are ongoing5. Providers like Midjourney, who keep their image generation algorithms undisclosed, prevent adaptive attackers altogether but may be vulnerable to these attacks by anyone to whom this information is released.\nOutlook. An adaptive attacker can instantiate more effective versions of their attacks with knowledge of the watermarking method\u2019s algorithmic descriptions (KEYGEN, EMBED, VERIFY). Our attacks require no interaction with the provider. Robustness against adaptive attacks extends to robustness against non-adaptive attacks, which makes studying the former interesting. Adaptive attacks will remain a useful tool for studying a watermarking method\u2019s robustness, even if the watermarking method is kept secret. Previous works did not consider such attacks, and we show that future watermarking methods must consider them if they claim robustness empirically."
        },
        {
            "heading": "6.1 RELATED WORK",
            "text": "Jiang et al. (2023) propose attacks that use (indirect) access to the secret watermarking key via access to the provider\u2019s VERIFY method. Our attacks require no access to the provider\u2019s secret watermarking key, as our attacks optimize over any key message pair (see Equation (2)). These threats are related since both undermine robustness but are orthogonal due to different threat models. Peng et al. (2023); Cui et al. (2023) propose black-box watermarking methods that protect the Intellectual Property of Diffusion Models. We focus on no-box verifiable watermarking methods that control misuse. Lukas & Kerschbaum (2023); Yu et al. (2020; 2021) propose watermarking methods but only evaluate GANs. We focus on watermarking methods for pre-trained Stable Diffusion models with much higher output diversity and image quality (Dhariwal & Nichol, 2021)."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "We propose testing the robustness of watermarking through adaptive, learnable attacks. Our empirical analysis shows that such attackers can evade watermark detection against all five surveyed image watermarks. Adversarial noising evades TRW (Wen et al., 2023) with \u03f5 = 2/255 but needs to add visible noise to evade the remaining four watermarking methods. Adversarial compression evades all five watermarking methods using only a single compression. We encourage using these adaptive attacks to test the robustness of watermarking methods in the future more comprehensively.\n5https://github.com/lucidrains/imagen-pytorch"
        },
        {
            "heading": "8 ETHICS STATEMENT",
            "text": "The attacks we provide target academic systems, and the engineering efforts to attack real systems are substantial. We make it harder by not releasing our code publicly. We will, however, release our code, including pre-trained checkpoints, upon carefully considering each request. Currently, there are no known security impacts of our attacks since users cannot yet rely on the provider\u2019s use of watermarking. The use of watermarking is experimental and occurs at the provider\u2019s own risk, and our research aims to improve the trustworthiness of image watermarking by evaluating it more comprehensively."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 PARAMETERS FOR WATERMARKING METHODS\nTree Ring Watermark (TRW) (Wen et al., 2023): We evaluate the Tree-RingRings method, which the authors state \u201cdelivers the best average performance while offering the model owner the flexibility of multiple different random keys\u201d. Using the author\u2019s implementation6, we generate and verify watermarks using 20 inference steps, where we use no knowledge of the prompt during verification and keep the remaining default parameters chosen by the authors.\nWatermark Diffusion Model (WDM) (Zhao et al., 2023): As stated in the paper, instead of stamping the model\u2019s training data to embed a watermark, we apply the pre-trained encoder to a generated image as a post-processing step. We choose messages with n = 40 bits and use the encoder architecture proposed by (Yu et al., 2021), followed by a ResNet-50 decoder. Each call to KEYGEN(\u03b8\u0302G), where \u03b8\u0302G is the surrogate generator, trains a new autoencoder from scratch.\nDWT, DWT-SVD (Cox et al., 2007) and RivaGAN (Zhang et al., 2019). We use 32-bit messages and keep the default parameters set in the implementation used by the Stable Diffusion models7.\nA.2 STATISTICAL TESTS\nMatching Bits. WDM (Zhao et al., 2023), DWT, DWT-SVD (Cox et al., 2007) and RivaGAN (Zhang et al., 2019) encode messages m \u2208 M by bits and our goal is to verify whether\n6https://github.com/YuxinWenRick/tree-ring-watermark 7https://github.com/ShieldMnt/invisible-watermark\nmessage m \u2208 M is present in x \u2208 X using key \u03c4 . We extract m\u2032 from x and want to reject the following null hypothesis.\nH0 : m and m\u2032 match by random chance.\nFor a given pair of bit-strings of length n, if we denote the number of matching bits as k, the expected number of matches by random chance follows a binomial distribution with parameters n and expected value 0.5. The p-value for observing at least k matches is given by:\np = 1\u2212 CDF(k \u2212 1;n, 0.5) (3)\nWhere CDF represents the cumulative distribution function of the binomial distribution.\nMatching Latents. TRW (Wen et al., 2023) leverages the forward diffusion process of the diffusion model to reverse an image x to its initial noise representation xT . This transformation is represented by m\u2032 = F(xT ), where F denotes a Fourier transform. The authors find that reversed real images and their representations in the Fourier domain are expected to follow a Gaussian distribution. The watermark verification process aims to reject the following null hypothesis:\nH0 : y originates from a Gaussian distribution N(0, \u03c32IC)\nHere, y is a subset of m\u2032 based on a watermarking mask chosen by the provider, which determines the relevant coefficients. The test statistic, \u03b7, denotes the normalized sum-of-squares difference between the original embedded message m and the extracted message m\u2032, which can be complexvalued due to the Fourier transform. Specifically,\n\u03b7 = 1\n\u03c32 \u2211 i |mi \u2212m\u2032i|2 (4)\nAnd,\np = Pr ( \u03c72|M |,\u03bb \u2264 \u03b7 | H0 ) = \u03a6\u03c72(\u03b7) (5)\nWhere \u03a6\u03c72 represents the cumulative distribution function of the noncentral \u03c72 distribution. We refer to Wen et al. (2023) for more detailed descriptions of these statistical tests.\nA.3 DETAILS ON GKEYGEN\nThis section provides more details on Algorithm 1. VERIFY consists of a sub-procedure EXTRACT, which maps an image to a message using the secret key, as stated in Section 2.1. The space of messages is specific to the watermarking method. We consider two message spaces M: multibit messages and messages in the Fourier space. All surveyed methods except TRW are multi-bit watermarks, for which we use the categorical cross-entropy to measure similarity, and for TRW, we use the mean absolute error as a similarity measure between messages (line 7 of Algorithm 1).\nAs stated in the main paper, we instantiate GKEYGEN only for the DWT and DWT-SVD watermarking methods. We train a ResNet-50 decoder \u03b8D in Algorithm 1 to predict a bit vector of the same length as the message and calculate gradients during training using the cross-entropy loss. Attacking TRW, WDM, and RivaGAN only requires invoking KEYGEN, as the keys are the parameters of (differentiable) decoders. We train these keys from scratch for WDM and RivaGAN. For TRW, the key generation does not require any training, as the key only specifies elements in the Fourier space that encode the message. The forward diffusion process used in VERIFY is already differentiable.\nA.4 QUALITATIVE ANALYSIS OF WATERMARKING TECHNIQUES\nWe refer to Figure 5 for examples of non-watermarked, watermarked, and attacked images using the attacks summarized in Table 1. We show three images for each of the five surveyed watermarking methods: an image without a watermark, one with a watermark, and the watermarked image after an evasion attack. We show the prompt that was used to generate these images and label each image with the p-value with which the expected message was detected in the image using the secret watermarking key and the VERIFY procedure.\nA.5 QUALITY EVALUATION\nTable 2 shows the FID and CLIPScore of all five surveyed watermarking methods without a watermark (first row), with a watermark (second row), after our adaptive noising attack (third row) and after our adversarial compression attack (fourth row). All results are reported as the mean value over three independent runs using three different secret watermarking keys. We observe that the degradation in FID and CLIPScores is statistically insignificant, as seen in Figure 5.\nA.6 ATTACK EFFICIENCY\nFrom a computational perspective, generating a surrogate watermarking key with methods such as RivaGAN or WDM is the most expensive operation, as it requires training a watermark encoderdecoder pair from scratch. Generating a key for these two methods takes around 4 GPU hours each on a single A100 GPU, which is still negligible considering the total training time of the diffusion model, which takes approximately 150-1000 GPU days (Dhariwal & Nichol, 2021). The optimization of Adversarial noising takes less than 1 second per sample, and tuning the adversarial compressor\u2019s parameters takes less than 10 minutes on a single A100 GPU.\nA.7 DOUBLE-TAIL DETECTION\nJiang et al. (2023) propose a more robust statistical test that uses two-tailed detection for multibit messages. The idea is to test for the presence of a watermark with message m or message 1 \u2212m (all bits flipped). We implemented the double-tail detection described by Jiang et al. (2023) and adjusted the statistical test in VERIFY to use double-tail detection on the same images used in Figure 4. Table 3 summarizes the resulting TPR@1%FPR with single or double-tail detection. Since TRW (Wen et al., 2023) is not a multi-bit watermarking method, we omit its results.\nTable 3 shows that double-tail detection increases the robustness of DWT and DWT-SVD against adversarial noising, which is the same effect that Jiang et al. (2023) find in their paper. We find that Adversarial Compression remains effective against all attacks in the presence of double-tail detection. Figure 4 shows that adversarial noising is highly effective against TRW but is ineffective against the remaining methods because an attacker has to add visible noise (see Figure 3). An attacker would always use the Adversarial Compression attack in a real-world attack.\nWatermarkedNo Watermark\nAttacked\nt r r t r r\ntt"
        }
    ],
    "year": 2024
}