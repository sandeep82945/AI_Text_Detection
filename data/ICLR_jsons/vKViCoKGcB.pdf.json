{
    "abstractText": "Data attribution seeks to trace model outputs back to training data. With the recent development of diffusion models, data attribution has become a desired module to properly assign valuations for high-quality or copyrighted training samples, ensuring that data contributors are fairly compensated or credited. Several theoretically motivated methods have been proposed to implement data attribution, in an effort to improve the trade-off between computational scalability and effectiveness. In this work, we conduct extensive experiments and ablation studies on attributing diffusion models, specifically focusing on DDPMs trained on CIFAR-10 and CelebA, as well as a Stable Diffusion model LoRA-finetuned on ArtBench. Intriguingly, we report counter-intuitive observations that theoretically unjustified design choices for attribution empirically outperform previous baselines by a large margin, in terms of both linear datamodeling score and counterfactual evaluation. Our work presents a significantly more efficient approach for attributing diffusion models, while the unexpected findings suggest that at least in non-convex settings, constructions guided by theoretical assumptions may lead to inferior attribution performance. The code is available at https://github.com/sail-sg/D-TRAK.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiaosen Zheng"
        },
        {
            "affiliations": [],
            "name": "Tianyu Pang"
        },
        {
            "affiliations": [],
            "name": "Chao Du"
        },
        {
            "affiliations": [],
            "name": "Jing Jiang"
        },
        {
            "affiliations": [],
            "name": "Min Lin"
        }
    ],
    "id": "SP:ce7572a0d61e409dbddabd478b57e99679207ab1",
    "references": [
        {
            "authors": [
                "Anish Athalye",
                "Nicholas Carlini",
                "David Wagner"
            ],
            "title": "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2018
        },
        {
            "authors": [
                "Fan Bao",
                "Chongxuan Li",
                "Jun Zhu",
                "Bo Zhang"
            ],
            "title": "Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Elnaz Barshan",
                "Marc-Etienne Brunet",
                "Gintare Karolina Dziugaite"
            ],
            "title": "Relatif: Identifying explanatory training samples via relative influence",
            "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2020
        },
        {
            "authors": [
                "Samyadeep Basu",
                "Philip Pope",
                "Soheil Feizi"
            ],
            "title": "Influence functions in deep learning are fragile",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Lucas Bourtoule",
                "Varun Chandrasekaran",
                "Christopher A Choquette-Choo",
                "Hengrui Jia",
                "Adelin Travers",
                "Baiwu Zhang",
                "David Lie",
                "Nicolas Papernot"
            ],
            "title": "Machine unlearning",
            "venue": "IEEE Symposium on Security and Privacy (SP),",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Brophy",
                "Zayd Hammoudeh",
                "Daniel Lowd"
            ],
            "title": "Adapting and evaluating influence-estimation methods for gradient-boosted decision trees",
            "venue": "Journal of Machine Learning Research (JMLR),",
            "year": 2023
        },
        {
            "authors": [
                "Nicholas Carlini",
                "David Wagner"
            ],
            "title": "Towards evaluating the robustness of neural networks",
            "venue": "In IEEE Symposium on Security and Privacy (S&P),",
            "year": 2017
        },
        {
            "authors": [
                "Guillaume Charpiat",
                "Nicolas Girard",
                "Loris Felardos",
                "Yuliya Tarabalka"
            ],
            "title": "Input similarity from the neural network perspective",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Yuanyuan Chen",
                "Boyang Li",
                "Han Yu",
                "Pengcheng Wu",
                "Chunyan Miao"
            ],
            "title": "Hydra: Hypergradient data relevance analysis for interpreting deep neural networks",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "R Dennis Cook",
                "Sanford Weisberg"
            ],
            "title": "Residuals and influence in regression",
            "year": 1982
        },
        {
            "authors": [
                "Francesco Croce",
                "Matthias Hein"
            ],
            "title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Zheng Dai",
                "David K Gifford"
            ],
            "title": "Training data attribution for diffusion models",
            "venue": "arXiv preprint arXiv:2306.02174,",
            "year": 2023
        },
        {
            "authors": [
                "Vitaly Feldman",
                "Chiyuan Zhang"
            ],
            "title": "What neural networks memorize and why: Discovering the long tail via influence estimation",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Kristian Georgiev",
                "Joshua Vendrow",
                "Hadi Salman",
                "Sung Min Park",
                "Aleksander Madry"
            ],
            "title": "The journey, not the destination: How data guides diffusion models",
            "venue": "In Workshop on Challenges in Deployable Generative AI at International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Amirata Ghorbani",
                "James Zou"
            ],
            "title": "Data shapley: Equitable valuation of data for machine learning",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "Roger Grosse",
                "Juhan Bae",
                "Cem Anil",
                "Nelson Elhage",
                "Alex Tamkin",
                "Amirhossein Tajdini",
                "Benoit Steiner",
                "Dustin Li",
                "Esin Durmus",
                "Ethan Perez"
            ],
            "title": "Studying large language model generalization with influence functions",
            "venue": "arXiv preprint arXiv:2308.03296,",
            "year": 2023
        },
        {
            "authors": [
                "Xiangming Gu",
                "Chao Du",
                "Tianyu Pang",
                "Chongxuan Li",
                "Min Lin",
                "Ye Wang"
            ],
            "title": "On memorization in diffusion models",
            "venue": "arXiv preprint arXiv:2310.02664,",
            "year": 2023
        },
        {
            "authors": [
                "Zayd Hammoudeh",
                "Daniel Lowd"
            ],
            "title": "Identifying a training-set attack\u2019s target using renormalized influence estimation",
            "venue": "In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security,",
            "year": 2022
        },
        {
            "authors": [
                "Zayd Hammoudeh",
                "Daniel Lowd"
            ],
            "title": "Training data influence analysis and estimation: A survey",
            "venue": "arXiv preprint arXiv:2212.04612,",
            "year": 2022
        },
        {
            "authors": [
                "Frank R Hampel",
                "Elvezio M Ronchetti",
                "Peter J Rousseeuw",
                "Werner A Stahel"
            ],
            "title": "Robust statistics: the approach based on influence functions",
            "year": 2011
        },
        {
            "authors": [
                "Trevor Hastie"
            ],
            "title": "Ridge regularization: An essential concept in data science",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "arXiv preprint arXiv:2207.12598,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Sara Hooker",
                "Dumitru Erhan",
                "Pieter-Jan Kindermans",
                "Been Kim"
            ],
            "title": "A benchmark for interpretability methods in deep neural networks",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Ilyas",
                "Sung Min Park",
                "Logan Engstrom",
                "Guillaume Leclerc",
                "Aleksander Madry"
            ],
            "title": "Datamodels: Predicting predictions from training data",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Ruoxi Jia",
                "Fan Wu",
                "Xuehui Sun",
                "Jiacen Xu",
                "David Dao",
                "Bhavya Kailkhura",
                "Ce Zhang",
                "Bo Li",
                "Dawn Song"
            ],
            "title": "Scalability vs. utility: Do we have to sacrifice one for the other in data importance quantification",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Roger W Johnson"
            ],
            "title": "An introduction to the bootstrap",
            "venue": "Teaching statistics,",
            "year": 2001
        },
        {
            "authors": [
                "William B. Johnson",
                "Joram Lindenstrauss"
            ],
            "title": "Extensions of lipschitz mappings into hilbert space",
            "venue": "Contemporary mathematics,",
            "year": 1984
        },
        {
            "authors": [
                "Rajiv Khanna",
                "Been Kim",
                "Joydeep Ghosh",
                "Sanmi Koyejo"
            ],
            "title": "Interpreting black box predictions using fisher kernels",
            "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2019
        },
        {
            "authors": [
                "Pang Wei Koh",
                "Percy Liang"
            ],
            "title": "Understanding black-box predictions via influence functions",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2017
        },
        {
            "authors": [
                "Pang Wei W Koh",
                "Kai-Siang Ang",
                "Hubert Teo",
                "Percy S Liang"
            ],
            "title": "On the accuracy of influence functions for measuring group effects",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Shuming Kong",
                "Yanyan Shen",
                "Linpeng Huang"
            ],
            "title": "Resolving training biases via influence-based data relabeling",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Zhifeng Kong",
                "Kamalika Chaudhuri"
            ],
            "title": "Understanding instance-based interpretability of variational auto-encoders",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Technical report,",
            "year": 2009
        },
        {
            "authors": [
                "Peiyuan Liao",
                "Xiuyu Li",
                "Xihui Liu",
                "Kurt Keutzer"
            ],
            "title": "The artbench dataset: Benchmarking generative models with artworks",
            "venue": "arXiv preprint arXiv:2206.11404,",
            "year": 2022
        },
        {
            "authors": [
                "Jinkun Lin",
                "Anqi Zhang",
                "Mathias L\u00e9cuyer",
                "Jinyang Li",
                "Aurojit Panda",
                "Siddhartha Sen"
            ],
            "title": "Measuring the effect of training data on deep learning predictions via randomized experiments",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Zhuoming Liu",
                "Hao Ding",
                "Huaping Zhong",
                "Weijia Li",
                "Jifeng Dai",
                "Conghui He"
            ],
            "title": "Influence selection for active learning",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Ziwei Liu",
                "Ping Luo",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "James Martens"
            ],
            "title": "New insights and perspectives on the natural gradient method",
            "venue": "The Journal of Machine Learning Research (JMLR),",
            "year": 2020
        },
        {
            "authors": [
                "Sung Min Park",
                "Kristian Georgiev",
                "Andrew Ilyas",
                "Guillaume Leclerc",
                "Aleksander Madry"
            ],
            "title": "Trak: Attributing model behavior at scale",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Garima Pruthi",
                "Frederick Liu",
                "Satyen Kale",
                "Mukund Sundararajan"
            ],
            "title": "Estimating training data influence by tracing gradient descent",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin"
            ],
            "title": "Model-agnostic interpretability of machine learning",
            "venue": "arXiv preprint arXiv:1606.05386,",
            "year": 2016
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Andrea Schioppa",
                "Polina Zablotskaia",
                "David Vilar",
                "Artem Sokolov"
            ],
            "title": "Scaling up influence functions",
            "venue": "In AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2022
        },
        {
            "authors": [
                "Lloyd S Shapley"
            ],
            "title": "A value for n-person games",
            "venue": "Contributions to the Theory of Games II.,",
            "year": 1953
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Charles Spearman"
            ],
            "title": "The proof and measurement of association between two things",
            "venue": "The American journal of psychology,",
            "year": 1987
        },
        {
            "authors": [
                "Naoyuki Terashita",
                "Hiroki Ohashi",
                "Yuichi Nonaka",
                "Takashi Kanemaru"
            ],
            "title": "Influence estimation for generative adversarial networks",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Stefano Teso",
                "Andrea Bontempelli",
                "Fausto Giunchiglia",
                "Andrea Passerini"
            ],
            "title": "Interactive label cleaning with example-based explanations",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Ting",
                "Eric Brochu"
            ],
            "title": "Optimal subsampling with influence functions",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2018
        },
        {
            "authors": [
                "Aaron Van Den Oord",
                "Oriol Vinyals"
            ],
            "title": "Neural discrete representation learning",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2017
        },
        {
            "authors": [
                "Sheng-Yu Wang",
                "Alexei A Efros",
                "Jun-Yan Zhu",
                "Richard Zhang"
            ],
            "title": "Evaluating data attribution for text-to-image models",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2023
        },
        {
            "authors": [
                "Chih-Kuan Yeh",
                "Joon Kim",
                "Ian En-Hsu Yen",
                "Pradeep K Ravikumar"
            ],
            "title": "Representer point selection for explaining deep neural networks",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2018
        },
        {
            "authors": [
                "Lvmin Zhang",
                "Anyi Rao",
                "Maneesh Agrawala"
            ],
            "title": "Adding conditional control to text-to-image diffusion models",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2023
        },
        {
            "authors": [
                "Xiaosen Zheng",
                "Jing Jiang"
            ],
            "title": "An empirical study of memorization in nlp",
            "venue": "In Association for Computational Linguistics (ACL),",
            "year": 2022
        },
        {
            "authors": [
                "Raw pixel"
            ],
            "title": "This is a naive similarity-based attribution method that simply uses the raw image as the representation and then computes the dot product or cosine similarity between the sample of interest and each training sample as the attribution score. Especially, for ArtBench, which uses latent diffusion models (Rombach et al., 2022), we represent the image using the VAE encodings",
            "year": 2017
        },
        {
            "authors": [
                "Barshan"
            ],
            "title": "2020) propose the \u03b8-relative influence functions estimator, which normalizes Koh & Liang (2017) influence functions\u2019 estimator by HVP magnitude. We adapt this method to our setting after applying TRAK\u2019s scalability",
            "year": 2017
        },
        {
            "authors": [
                "Journey TRAK. Georgiev"
            ],
            "title": "2023) focus on attributing noisy images xt, while we attribute the finally generated image x. We adapt their method to our setting by averaging the attributions over the generation timesteps as follows \u03c4(x,D)n",
            "year": 2023
        },
        {
            "authors": [
                "Park"
            ],
            "title": "L\u221e-norm, will make the attribution task fail as indicated by near-zero LDS scores. One potential explanation is that these functions are not well approximated by the linear functions of training samples",
            "year": 2023
        },
        {
            "authors": [
                "Journey TRAK Georgiev"
            ],
            "title": "2023) is designed to conduct data attribution at a particular sampling",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Training data plays a pivotal role in determining the behavior of machine learning models. To this end, the goal of data attribution is to precisely indicate the importance of each training data in relation to the model outputs of interest. Data attribution has been extensively utilized to interpret model predictions (Koh & Liang, 2017; Yeh et al., 2018; Ilyas et al., 2022), detect poisoning attacks or noisy labels (Hammoudeh & Lowd, 2022a; Lin et al., 2022), curate data (Khanna et al., 2019; Jia et al., 2021; Liu et al., 2021), debug model behavior (Kong et al., 2022), and understand conventional generative models such as GANs and VAEs (Kong & Chaudhuri, 2021; Terashita et al., 2021).\nOn the other hand, diffusion models have made promising progress on generative tasks (Ho et al., 2020; Song et al., 2021b), and they have gained popularity alongside open-sourced large diffusion models such as Stable Diffusion (Rombach et al., 2022). Numerous applications employ customized variants of Stable Diffusion that are personalized via LoRA (Hu et al., 2022) or ControlNet (Zhang et al., 2023). However, the emerging success and potent ability of diffusion models raise legal and ethical concerns, particularly in domains such as artistic creation where data contributors (e.g., artists) seek fair compensation or credit. In this regard, data attribution acts as an essential module to properly assign valuations for high-quality or copyrighted training samples.\nAlong the research routine of implementing different methods for data attribution, there is a recurring trade-off between computational scalability and effectiveness (Koh & Liang, 2017; Ghorbani & Zou, 2019; Feldman & Zhang, 2020; Pruthi et al., 2020; Ilyas et al., 2022; Schioppa et al., 2022), especially in non-convex settings. Recently, Park et al. (2023) develop an attribution method called TRAK that is both effective and computationally tractable for large-scale models. Georgiev et al. (2023) makes additional use of TRAK on diffusion models to attribute newly generated images to training data.\nIn this work, we conduct comprehensive experiments and ablation studies on attributing diffusion models. In addition to taking TRAK as a strong baseline, we evaluate several prevalent attribution\n\u2217Work done during an internship at Sea AI Lab. \u2020Corresponding authors.\napproaches. Intriguingly, we report counter-intuitive observations that after integrating theoretically unjustified design choices into TRAK (the resulting method is named diffusion-TRAK, D-TRAK), our D-TRAK method consistently outperforms previous baselines including TRAK, in terms of both linear datamodeling score (Park et al., 2023) and counterfactual evaluation (Hooker et al., 2019; Ilyas et al., 2022). Furthermore, D-TRAK has a number of empirical advantages such as insensitivity to checkpoint selection and fewer timestep requirements, as described in Section 4.\nAlthough D-TRAK is empirically appealing for attributing diffusion models, it is challenging to provide a satisfactory theoretical explanation for questions such as \u201cwhy D-TRAK performs better than TRAK?\u201d or \u201care there better design choices than D-TRAK?\u201d. Therefore, the unanticipated results reported in this paper suggest that, at least in non-convex settings, theoretically motivated (under simplified assumptions) constructions are not necessarily superior design choices for practical attribution problems, and that the mechanism of data attribution requires a deeper understanding."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "This section provides a concise overview of diffusion models, the definition of data attribution, the evaluation metrics associated with it, and advanced methods for attribution."
        },
        {
            "heading": "2.1 DIFFUSION MODELS",
            "text": "Our research primarily focuses on discrete-time diffusion models, specifically denoising diffusion probabilistic models (DDPMs) (Ho et al., 2020) and latent diffusion models (LDMs) that serve as the foundation of Stable Diffusion (Rombach et al., 2022). Below we briefly recap the notations of DDPMs, where we consider a random variable x \u2208 X and define a forward diffusion process on x as x1:T \u225c x1, \u00b7 \u00b7 \u00b7 ,xT with T \u2208 N+. The data distribution is x \u223c q(x) and the Markov transition probability from xt\u22121 to xt is q(xt|xt\u22121) \u225c N (xt| \u221a 1\u2212 \u03b2txt\u22121, \u03b2tI), where x0 = x and \u03b21, \u00b7 \u00b7 \u00b7 , \u03b2T correspond to a variance schedule. A notable property of DDPMs is that they can sample xt at an arbitrary timestep t directly from x, since there is q(xt|x) = N (xt| \u221a \u03b1tx, (1\u2212\u03b1t)I),\nwhere \u03b1t \u225c 1\u2212 \u03b2t and \u03b1t \u225c \u220ft\ni=1 \u03b1i. Sohl-Dickstein et al. (2015) show that when \u03b2t are small, the reverse diffusion process can also be modeled by Gaussian conditionals.\nSpecifically, for the DDPMs framework, the reverse transition probability from xt to xt\u22121 is written as p\u03b8(xt\u22121|xt) = N (xt\u22121|\u00b5\u03b8(xt, t), \u03c32t I), where \u03b8 \u2208 Rd is the model parameters and \u03c3t are time dependent constants that can be predefined or analytically computed (Bao et al., 2022). Instead of directly modeling the data prediction \u00b5\u03b8, DDPMs choose to model the noise prediction \u03f5\u03b8 based on the parameterization \u00b5\u03b8(xt, t) = 1\u221a\u03b1t ( xt \u2212 \u03b2t\u221a1\u2212\u03b1t \u03f5\u03b8(xt, t) ) . The training objective of \u03f5\u03b8(xt, t) can be derived from optimizing the variational bound of negative log-likelihood formulated as follows:\nLELBO(x; \u03b8) = E\u03f5,t [\n\u03b22t 2\u03c32t\u03b1t(1\u2212 \u03b1t) \u2225\u2225\u03f5\u2212 \u03f5\u03b8(\u221a\u03b1tx+\u221a1\u2212 \u03b1t\u03f5, t)\u2225\u222522] , (1) where \u03f5 \u223c N (\u03f5|0, I) and t \u223c U([1, T ]) denotes the discrete uniform distribution between 1 and T . Let D \u225c {xn}Nn=1 be a training dataset that xn \u223c q(x), then the empirical training objective on D can be written as LELBO(D; \u03b8) = 1N \u2211 xn\u2208D LELBO(xn, \u03b8). To benefit sample quality, DDPMs apply a simplified training objective that corresponds to a weighted variational bound and is formulated as\nLSimple(x; \u03b8) = E\u03f5,t [\u2225\u2225\u03f5\u2212 \u03f5\u03b8(\u221a\u03b1tx+\u221a1\u2212 \u03b1t\u03f5, t)\u2225\u222522] , (2)\nwhere the empirical objective on D is similarly written as LSimple(D; \u03b8) = 1N \u2211 xn\u2208D LSimple(xn, \u03b8)."
        },
        {
            "heading": "2.2 DATA ATTRIBUTION AND EVALUATION METRICS",
            "text": "Data attribution refers to the goal of tracing model outputs back to training data. We follow Park et al. (2023) and recap the formal definition of data attribution as below:\nDefinition 1 (Data attribution). Consider an ordered training set of samples D \u225c {xn}Nn=1 and a model output function F(x; \u03b8). A data attribution method \u03c4(x,D) is a function \u03c4 : X \u00d7XN \u2192 RN that, for any sample x \u2208 X and a training set D, assigns a score to each training input xn \u2208 D indicating its importance to the model output F(x; \u03b8\u2217(D)), where \u03b8\u2217(D) = argmin\u03b8 L(D; \u03b8).1\n1We apply bold symbols of F and L to highlight the model output function and training objective in the definition of data attribution, respectively, to distinguish them from the functions used in specific attribution methods.\nThere have been various metrics to evaluate data attribution methods, including leave-one-out influences (Koh & Liang, 2017; Koh et al., 2019; Basu et al., 2021), Shapley values (Ghorbani & Zou, 2019), and performance on auxiliary tasks (Jia et al., 2021; Hammoudeh & Lowd, 2022a). However, these metrics may pose computational challenges in large-scale scenarios or be influenced by the specific characteristics of the auxiliary task. In light of this, Park et al. (2023) propose the linear datamodeling score (LDS), which considers the sum of attributions as an additive proxy for F , as a new metric for evaluating data attribution methods. In accordance with Definition 1, we define the attribution-based output prediction of the model output F(x; \u03b8\u2217(D\u2032)) as\ng\u03c4 (x,D\u2032;D) \u225c \u2211\nxn\u2208D\u2032 \u03c4(x,D)n, (3)\nwhere D\u2032 is a subset of D as D\u2032 \u2282 D. Then the LDS metric can be constructed as follows: Definition 2 (Linear datamodeling score). Considering a training set D, a model output function F(x; \u03b8), and a corresponding data attribution method \u03c4 . Let {Dm}Mm=1 be M randomly sampled subsets of the training dataset D that Dm \u2282 D, each of size \u03b1 \u00b7N for some \u03b1 \u2208 (0, 1). The linear datamodeling score (LDS) of a data attribution \u03c4 for a specific sample x \u2208 X is given by\nLDS(\u03c4,x) \u225c \u03c1 ({F(x; \u03b8\u2217(Dm)) : m \u2208 [M ]}, {g\u03c4 (x,Dm;D) : m \u2208 [M ]}) , (4)\nwhere \u03c1 denotes Spearman rank correlation (Spearman, 1987).\nTo counter the randomness of the training mechanism (the process of approximating \u03b8\u2217(Dm)), for every subset Dm, we train three models with different random seeds and average the model output function. We also consider the counterfactual evaluation to study the utility of different attribution methods, following common practice (Hooker et al., 2019; Feldman & Zhang, 2020; Ilyas et al., 2022; Park et al., 2023; Brophy et al., 2023; Georgiev et al., 2023). We compare the pixel-wise \u21132-distance and CLIP cosine similarity of generated images, using the models trained before/after the exclusion of the highest-ranking positive influencers identified by different attribution methods."
        },
        {
            "heading": "2.3 ATTRIBUTION METHODS",
            "text": "The primary interface employed by attribution methods is the score \u03c4(x,D), which is calculated for each training input to indicate its importance to the output of interest. From robust statistics (Cook & Weisberg, 1982; Hampel et al., 2011), influence functions are a classical concept that approximates how much an infinitesimally up-weighting of a training sample xn \u2208 D affects the model output function F(x; \u03b8\u2217), measured on an sample of interest x. In the convex setting, Koh & Liang (2017) show that the attributing score of influence function can be computed as \u03c4IF(x,D)n = \u2207\u03b8F(x; \u03b8\u2217)\u22a4 \u00b7 H\u22121\u03b8\u2217 \u00b7 \u2207\u03b8L(xn; \u03b8\u2217) for n \u2208 [N ], where H\u03b8\u2217 = \u22072\u03b8L(D; \u03b8\u2217) is the Hessian matrix at the optimal parameters \u03b8\u2217. Previous work has shown that computing the inverse of the Hessian matrix exhibits numerical instability, particularly when dealing with deep models (Basu et al., 2021; Pruthi et al., 2020). In more recent approaches, the Hessian matrix is substituted with the Fisher information matrix (Ting & Brochu, 2018; Barshan et al., 2020; Teso et al., 2021; Grosse et al., 2023). In contrast, retrainingbased methodologies demonstrate greater efficacy in accurately assigning predictions to training data, albeit necessitating the training of numerous models, ranging from thousands to tens of thousands, to achieve desired effectiveness (Ghorbani & Zou, 2019; Feldman & Zhang, 2020; Ilyas et al., 2022).\nTracing with the randomly-projected after kernel (TRAK). In a more recent study, Park et al. (2023) develop an approach known as TRAK, which aims to enhance the efficiency and scalability of attributing discriminative classifiers. In the TRAK algorithm, a total of S subsets denoted as {Ds}Ss=1 are initially sampled from the training dataset D, where each subset has a fixed size of \u03b2 \u00b7 N for \u03b2 \u2208 (0, 1].2 On each subset Ds, a model is trained to obtain the parameters \u03b8\u2217s \u2208 Rd and a random projection matrix Ps is sampled from N (0, 1)d\u00d7k (typically there is k \u226a d). Then TRAK constructs the projected gradient matrices \u03a6sTRAK and the weighting terms QsTRAK as\n\u03a6sTRAK = [ \u03d5s(x1); \u00b7 \u00b7 \u00b7 ;\u03d5s(xN ) ]\u22a4 , where \u03d5s(x) = P\u22a4s \u2207\u03b8F(x; \u03b8\u2217s);\nQsTRAK = diag ( Qs(x1), \u00b7 \u00b7 \u00b7 , Qs(xN ) ) , where Qs(x) =\n\u2202L \u2202F (x; \u03b8 \u2217 s).\n(5)\n2The values of S and \u03b2 in TRAK are different from M and \u03b1 that applied for computing LDS in Definition 2.\nFinally, the attribution score \u03c4TRAK(x,D) of TRAK is computed by\n\u03c4TRAK(x,D) =\n[ 1\nS S\u2211 s=1 \u03d5s(x) \u22a4 ( \u03a6sTRAK \u22a4\u03a6sTRAK )\u22121 \u03a6sTRAK \u22a4\n][ 1\nS S\u2211 s=1 QsTRAK\n] . (6)\nIn the discriminative classification cases, Park et al. (2023) design the model output function to be F(x; \u03b8) = log(exp(L(x; \u03b8))\u2212 1) according to the mechanism of logistic regression."
        },
        {
            "heading": "3 DATA ATTRIBUTION ON DIFFUSION MODELS",
            "text": "In the context of generative models, such as diffusion models, it is crucial to accurately attribute the generated images to the corresponding training images. This attribution serves the purpose of appropriately assigning credits or valuations, as well as safeguarding copyright (Ghorbani & Zou, 2019; Dai & Gifford, 2023; Wang et al., 2023; Georgiev et al., 2023). Although the study of data attribution has primarily focused on discriminative classification problems, the Definition 1 of data attribution and its evaluation metric LDS in Definition 2 can also be applied to generative cases."
        },
        {
            "heading": "3.1 DIFFUSION-TRAK",
            "text": "In our initial trials, we attempt to adopt TRAK for attributing images generated by DDPMs, drawing inspiration from the implementation described by Georgiev et al. (2023).3 To be specific, given a DDPM trained by minimizing L(D; \u03b8) = LSimple(D; \u03b8), there is \u03b8\u2217(D) = argmin\u03b8 LSimple(D; \u03b8) and we set the model output function to be F(x; \u03b8) = L(x; \u03b8) = LSimple(x, \u03b8). In this particular configuration, when we compute the attribution score of TRAK in Eq. (6), the weighting terms QsTRAK = \u2202L\u2202F = I become identity matrices, and the function \u03d5s is constructed as \u03d5s(x) = P\u22a4s \u2207\u03b8LSimple(x, \u03b8\u2217s). Counter-intuitive observations. Intuitively, it seems reasonable to consider TRAK with \u03d5s(x) = P\u22a4s \u2207\u03b8LSimple(x, \u03b8\u2217s) as a suitable approach for attributing DDPMs. This is particularly applicable in the context of our initial trials, where both L and F are LSimple. Nevertheless, we fortuitously observe that replacing \u03d5s(x) with alternative functions can result in higher values of LDS. Specifically, we study a generalization of the TRAK formula, which we refer to as diffusion-TRAK (D-TRAK):\n\u03c4D-TRAK(x,D) =\n[ 1\nS S\u2211 s=1 \u03d5s(x) \u22a4 ( \u03a6sD-TRAK \u22a4\u03a6sD-TRAK )\u22121 \u03a6sD-TRAK \u22a4 ] , (7)\nwhere \u03a6sD-TRAK = [ \u03d5s(x1); \u00b7 \u00b7 \u00b7 ;\u03d5s(xN ) ]\u22a4 are the projected gradient matrices. In contrast to the TRAK formula in Eq. (6), D-TRAK allows \u03d5s to be constructed from alternative functions, rather than relying on F as \u03d5s(x) = P\u22a4s \u2207\u03b8F(x; \u03b8\u2217s). The weighting terms are eliminated (i.e., QsD-TRAK = I)\n3Georgiev et al. (2023) focus on attributing noisy images xt, while we attribute the finally generated image x.\nunder the assumption that L and F are the same. In addition to LSimple and LELBO, we define LSquare, LAvg, and Lp-norm to be the alternative functions constructing \u03d5s in D-TRAK, formulated as\nLSquare(x,\u03b8)=Et,\u03f5 [ \u2225\u03f5\u03b8(xt, t)\u222522 ] ; LAvg(x,\u03b8)=Et,\u03f5[Avg (\u03f5\u03b8(xt, t))]; Lp-norm(x,\u03b8)=Et,\u03f5 [ \u2225\u03f5\u03b8(xt, t)\u2225p ] ,\nwhere xt = \u221a \u03b1tx+ \u221a 1\u2212 \u03b1t\u03f5 and Avg(\u00b7) : X \u2192 R is the average pooling operation. We instantiate p = 1, 2,\u221e for Lp-norm. Our preliminary results are concluded in Table 1, where we train a DDPM with T = 1000 on CIFAR-2 (a subset consisting of two classes from CIFAR-10). We compute the values of LDS on the validation set (original test images) and generation set (newly generated images) w.r.t. the training samples. Regarding the trade-off between computational demand and efficiency, we consider different numbers of timesteps (e.g., 10, 100, and 1000) sampled from t \u223c U([1, T ]) to approximate the expectation of Et, where these timesteps are selected to be evenly spaced within the interval [1, T ] (by the arange operation). As can be seen from Table 1, D-TRAK constructed from LSquare, LAvg, L2-norm, and L1-norm consistently outperform TRAK by a large margin.\n3.2 INTERPOLATION BETWEEN LSIMPLE AND LSQUARE The counter-intuitive results in Table 1 pose a challenge in terms of theoretical explanation. It is noteworthy that several alternative functions, namely LSquare, LAvg, L2-norm, and L1-norm, consistently outperform the seemingly reasonable choice of LSimple. To take a closer look on how these phenomena occur, we take LSquare as an object of study, and expand the gradients of LSimple and LSquare as\n\u2207\u03b8LSimple = Et,\u03f5 [ 2 \u00b7 (\u03f5\u03b8 \u2212 \u03f5)\u22a4 \u2207\u03b8\u03f5\u03b8 ] and \u2207\u03b8LSquare = Et,\u03f5 [ 2 \u00b7 \u03f5\u22a4\u03b8 \u2207\u03b8\u03f5\u03b8 ] , (8)\nwhere we omit the dependence on x and \u03f5 for the simplicity of notations. We can find that \u2207\u03b8LSimple and \u2207\u03b8LSquare share the same term of \u2207\u03b8\u03f5\u03b8, and the difference is that they product \u2207\u03b8\u03f5\u03b8 with 2 \u00b7 (\u03f5\u03b8 \u2212 \u03f5)\u22a4 and 2 \u00b7 \u03f5\u22a4\u03b8 , respectively. We deduce that the information of \u2207\u03b8\u03f5\u03b8 is better retained in the norm-based losses. Hence, we perform interpolation on these two loss functions and subsequently utilize the resulting function to construct \u03d5s in D-TRAK:\n\u03d5s(x) = P\u22a4s \u2207\u03b8 [\u03b7LSquare+(1\u2212\u03b7) (LSimple\u2212LSquare)] (x, \u03b8\u2217s) = Et,\u03f5 [ 2 \u00b7 (\u03b7\u03f5\u03b8\u2212(1\u2212\u03b7) \u03f5)\u22a4 \u2207\u03b8\u03f5\u03b8 ] ,\nwhere \u03b7 \u2208 [0, 1] is the interpolation hyperparameter. When \u03b7 = 0.5, there is \u03d5s(x) = 12P \u22a4 s \u2207\u03b8LSimple corresponding to TRAK (the constant factor 12 does not affect LDS); when \u03b7 = 1, there is \u03d5 s(x) = P\u22a4s \u2207\u03b8LSquare corresponding to D-TRAK (LSquare) shown in Table 1. Full results of the LDS values w.r.t. various interpolation values \u03b7 are presented in Figure 1. It can be seen that TRAK (i.e., \u03b7 = 0.5) has the poorest performance compared to other interpolations. Moreover, as the value of \u03b7 diverges further from 0.5, approaching either 0 or 1, the corresponding LDS values increase.\nIn Appendix B, we conduct additional ablation studies on the effects of different implementation details. Our findings indicate that at least for the DDPM trained on CIFAR-2, D-TRAK (LSquare) consistently achieves superior performance compared to TRAK. This empirical evidence motivates us to replicate these unexpected findings across various diffusion models and datasets."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we perform comparative analyses between D-TRAK (LSquare) and existing data attribution methods across various settings. The primary metrics employed for evaluating attribution are LDS and counterfactual generations. Additionally, we visualize the attributions for manual inspection. We show that our D-TRAK achieves significantly greater efficacy and computational efficiency."
        },
        {
            "heading": "4.1 DATASETS",
            "text": "Our experiments are conducted on three datasets including CIFAR (32\u00d732), CelebA (64\u00d764), and ArtBench (256\u00d7256). More details of datasets can be found in Appendix A.1. In addition to the 1,000 held-out validation samples, we created a set of 1,000 generated images for each of the aforementioned datasets. Notably, calculating LDS necessitates retraining a large number of models, which constitutes the majority of the computational cost. In contrast, several attribution methods, such as TRAK and D-TRAK, are computationally efficient and scalable to larger datasets."
        },
        {
            "heading": "4.2 BASIC SETUPS OF DIFFUSION MODELS",
            "text": "On CIFAR, we adhere to the original implementation of the unconditional DDPMs (Ho et al., 2020), where the model architecture has 35.7M parameters (i.e., d = 35.7\u00d7 106 for \u03b8 \u2208 Rd). The maximum timestep is T = 1000, and we choose the linear variance schedule for the forward diffusion process as \u03b21 = 10\u22124 to \u03b2T = 0.02. We set the dropout rate to 0.1, employ the AdamW (Loshchilov & Hutter, 2019) optimizer with weight decay of 10\u22126, and augment the data with random horizontal flips. A DDPM is trained for 200 epochs with a 128 batch size, using a cosine annealing learning rate schedule with a 0.1 fraction warmup and an initial learning rate of 10\u22124. During inference, new images are generated utilizing the 50-step DDIM solver (Song et al., 2021a).\nOn CelebA, we use an unconditional DDPM implementation that is similar to the one used for CIFAR after being adapted to 64\u00d764 resolution by slightly modifying the U-Net architecture, which has 118.8M parameters. Other hyper-parameters are identical to those employed on CIFAR.\nOn ArtBench, we fine-tune the Stable Diffusion model (Rombach et al., 2022) using LoRA (Hu et al., 2022) with the rank set to 128, which consists of 25.5M parameters. To fit the resolution ratio of ArtBench, we use a Stable Diffusion checkpoint that has been adapted from 512\u00d7512 to 256\u00d7256 resolution. We train the model in a class-conditional manner where the textual prompt is simply set as \u201ca {class} painting\u201d such as \u201ca romanticism painting\u201d. We also set the dropout rate to 0.1, employ the AdamW optimizer with weight decay of 10\u22126, and augment the data with random horizontal flips. We train the model for 100 epochs under a batch size of 64, using a cosine annealing learning rate schedule with 0.1 fraction warmup and an initial learning rate of 3\u00d710\u22124. At the inference phase, we sample new images using the 50-step DDIM solver and a guidance scale of 7.5 (Ho & Salimans, 2022). More implementation details on training and generation of diffusion models are in Appendix A.1."
        },
        {
            "heading": "4.3 EVALUATING LDS FOR ATTRIBUTION METHODS",
            "text": "We set up the LDS benchmarks as described in Appendix A.2. In the following experiments, we report the mean and standard deviation of the LDS scores based on bootstrapping (Johnson, 2001) corresponding to the random re-sampling from the subsets. Regarding the design choices of attribution methods, as shown in Figure 2, for D-TRAK, the best LDS scores are obtained at the final checkpoint. However, for TRAK, using the final checkpoint is not the best choice. Finding which checkpoint yields the best LDS score requires computing the attributions for many times, which means much more computational cost. In practice, we might also only get access to the final model. On ArtBench-2, though TRAK obtains comparable LDS scores at the best checkpoint using 1000 timesteps, we believe that even ignoring the cost of searching the best checkpoint, computing the gradients for 1000 times per sample is unrealistic. In summary, when computing gradients, we use\nthe final model checkpoint as default following Koh & Liang (2017); Park et al. (2023); Grosse et al. (2023). We consider using 10 and 100 timesteps selected to be evenly spaced within the interval [1, T ]. For example, the selected timesteps are {1, 101, 201, ..., 901} when the number of timesteps is 10. For each timestep, we sample one standard Gaussian noise. The projection dimension is k = 32768.\nFollowing (Hammoudeh & Lowd, 2022b), we consider the attribution baselines that can be applied to our settings. We filter out methods that are intractable on our settings like Leave-One-Out (Cook & Weisberg, 1982) and Shapely Value (Ghorbani & Zou, 2019). We opt out those methods that are inapplicable to DDPMs, which are generally designed for specific tasks and models like Representer Point (Yeh et al., 2018). The baselines could be grouped into retraining-free and retraining-based methods. We further group the retraining-free methods into similarity-based, gradient-based (without kernel) and gradient-based (with kernel) methods. For TRAK and D-TRAK, the retraining-free setting means S = 1 and \u03b2 = 1.0, where we compute the gradients reusing the model we want to interpret. More implementation details of baselines can be found in Appendix A.3.\nAs shown in Tables 2, 3, 6, the similarity-based baselines including Raw pixel and CLIP similarity, and those gradient-based (without kernel) methods such as Gradient, TracInCP and GAS, yield poor LDS scores. However, gradient-based (with kernel) methods like TRAK and D-TRAK perform consistently better than other methods. Methods normalizing the attributions based on the magnitude of gradients including Relative Influence and Renormalized Influence have similar performance\ncompared to TRAK. The Journey TRAK yields low LDS scores as expected because it is designed for attributing noisy images xt along the sampling trajectory originally, while we attribute the finally generated image x. On the validation set, with 10/100 timesteps, D-TRAK achieves improvements of +15.37%/+10.15%, +11.55%/+8.67% and +15.35%/+5.1% on CIFAR-2, CelebA and ArtBench-2, respectively. On the generation set, with 10/100 timesteps, D-TRAK exhibits gains of +13.04%/+9.8%, +9.82%/+6.76%, and +16.38%/+6.51% in the LDS scores for the above three datasets. Regarding CIFAR-10 and ArtBench-5, overall the LDS scores are lower, which is expected because attributing to a larger training set might be more difficult. Nonetheless, D-TRAK still performs significantly better than TRAK. It is also interesting to see that across all the settings, D-TRAK with 10 timesteps computation budget outperforms TRAK with 100 timesteps computation budget. In Tables 7 and 8, we compare D-TRAK with the retraining-based methods including TRAK (ensemble), Empirical Influence and Datamodel, on CIFAR-2, CelebA and ArtBench-2. D-TRAK performs consistently better. More implementation details and empirical results of LDS evaluation can be found in Appendix C.2."
        },
        {
            "heading": "4.4 COUNTERFACTUAL EVALUATION",
            "text": "To evaluate the faithfulness of D-TRAK more intuitively, on CIFAR-2, CelebA, and ArtBench-2, we measure the pixel-wise \u21132-dist.4 and CLIP cosine similarity between images generated by the models trained before/after the exclusion of the top-1000 positive influencers identified by different attribution methods. For both D-TRAK and TRAK, we consider 100 timesteps, sample one standard Gaussian noise, and set k = 32768. We consider a control baseline called Random, which randomly removes 1000 training samples before retraining. As shown in Figure 3 and 11, when examining the median pixel-wise \u21132 distance resulting from removing-and-retraining, D-TRAK yields 8.97, 15.07, and 187.61 for CIFAR-2, CelebA, and ArtBench-2, respectively, in contrast to TRAK\u2019s values of 5.90, 11.02 and 168.37. D-TRAK obtains median similarities of 0.881, 0.896 and 0.769 for the above three datasets respectively, which are notably lower than TRAK\u2019s values of 0.943, 0.942 and 0.840, highlighting the effectiveness of our method. We manually compare the original generated samples to those generated from the same random seed with the re-trained models.5 As shown in Figure 4, our results suggest our method can better identify images that have a larger impact on the target image."
        },
        {
            "heading": "5 DISCUSSION",
            "text": "In this work, we empirically demonstrate that when the model output function of interest F is the same as the training objective L, the gradients of \u2207\u03b8L may not be a good design choice for attributing L itself. Although this deduction seems counter-intuitive, we find potentially related phenomena observed in previous research. For examples, Park et al. (2023) have reported that using \u2207\u03b8L in TRAK leads to worse performance than using \u2207\u03b8 log(exp(L(x; \u03b8))\u2212 1) on classification problems, even if the LDS score is computed by setting F = L. However, the TRAK paper has not extensively investigated this observation, and the utilization of the design involving \u2207\u03b8 log(exp(L(x; \u03b8)) \u2212 1) is based on theoretical justifications. Furthermore, existing literature on adversarial attacks has also identified the presence of gradient obfuscation when utilizing gradients derived from \u2207xL (Athalye et al., 2018). Conversely, employing gradients from alternative functions has been shown to enhance the efficiency of adversarial attacks (Carlini & Wagner, 2017; Croce & Hein, 2020). The aforementioned phenomena have not been adequately explained in a formal manner. We advocate that further investigation be conducted to gain a deeper understanding of these counter-intuitive observations and to enhance the development of data attribution methods.\n4On ArtBench-2, we measure the \u21132-dist. of VQ-VAE encoding for generated images (Rombach et al., 2022). 5We also visualize top attributions for sampled target samples in Appendix D.2."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This research is supported by the Ministry of Education, Singapore, under its Academic Research Fund Tier 2 (Proposal ID: T2EP20222-0047). Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not reflect the views of the Ministry of Education, Singapore."
        },
        {
            "heading": "A.1 DATASETS AND MODELS",
            "text": "CIFAR (32\u00d732). The CIFAR-10 dataset (Krizhevsky et al., 2009) contains 50,000 training samples.10 We randomly sample 1,000 validation samples from CIFAR-10\u2019s test set for LDS evaluation. To reduce computation, we also construct a CIFAR-2 dataset as a subset of CIFAR-10, which consists of 5,000 training samples randomly sampled from CIFAR-10\u2019s training samples corresponding to the \u201cautomobile\u201d and \u201chorse\u201d classes, and 1,000 validation samples randomly sampled from CIFAR-10\u2019s test set corresponding to the same two classes.\nOn CIFAR, we adhere to the original implementation of unconditional DDPMs (Ho et al., 2020), where the model architecture has 35.7M parameters (i.e., d = 35.7 \u00d7 106 for \u03b8 \u2208 Rd).11 The maximum timestep is T = 1000, and we choose the linear variance schedule for the forward diffusion process as \u03b21 = 10\u22124 to \u03b2T = 0.02. We set the dropout rate to 0.1, employ the AdamW (Loshchilov & Hutter, 2019) optimizer with weight decay of 10\u22126, and augment the data with random horizontal flips. A DDPM is trained for 200 epochs with a 128 batch size, using a cosine annealing learning rate schedule with a 0.1 fraction warmup and an initial learning rate of 10\u22124. During inference, new images are generated utilizing the 50-step DDIM solver (Song et al., 2021a).\nCelebA (64\u00d764). We sample a subset of 5,000 training samples and 1,000 validation samples from the original training set and test set of CelebA (Liu et al., 2015),12 respectively, and first center crop the images to 140\u00d7140 according to Song et al. (2021b) and then resize them to 64\u00d764. On CelebA, we use an unconditional DDPM implementation that is similar to the one used for CIFAR after being adapted to 64\u00d764 resolution by slightly modifying the U-Net architecture, which has 118.8M parameters.13 Other hyper-parameters are identical to those employed on CIFAR.\nArtBench (256\u00d7256). ArtBench (Liao et al., 2022) is a dataset for artwork generation.14 It includes 60,000 artwork images for 10 distinct artistic styles, with 5,000 training images and 1,000 testing images per style. We construct ArtBench-2 as a subset of ArtBench, consisting of 5,000 training/1,000 validation samples sampled from 10,000 training/2,000 test samples of the \u201cpost-impressionism\u201d and \u201cukiyo-e\u201d classes. We also test our method on ArtBench-5 as a subset of ArtBench, consisting of 12,500 training/1,000 validation samples sampled from 25,000 training/5,000 test samples of the \u201cpost-impressionism\u201d, \u201cukiyo-e\u201d, \u201cromanticism\u201d, \u201crenaissance\u201d and \u201cbaroque\u201d classes.\nOn ArtBench, we fine-tune the Stable Diffusion model (Rombach et al., 2022) using LoRA (Hu et al., 2022) with the rank set to 128, which consists of 25.5M parameters.15 To fit the resolution ratio of ArtBench, we use a Stable Diffusion checkpoint that has been adapted from 512\u00d7512 to 256\u00d7256 resolution.16 We train the model in a class-conditional manner where the textual prompt is simply set as \u201ca {class} painting\u201d such as \u201ca romanticism painting\u201d. We also set the dropout rate to 0.1, employ\n6https://github.com/huggingface/diffusers/tree/v0.16.1 7https://pytorch.org/tutorials/intermediate/per_sample_grads.html 8https://github.com/MadryLab/trak/releases/tag/v0.1.3 9https://pypi.org/project/fast-jl/0.1.3\n10https://huggingface.co/datasets/cifar10 11https://huggingface.co/google/ddpm-cifar10-32 12https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html 13https://github.com/huggingface/diffusers/blob/v0.16.1/examples/\nunconditional_image_generation/train_unconditional.py 14https://github.com/liaopeiyuan/artbench 15https://github.com/huggingface/diffusers/blob/v0.16.1/examples/text_ to_image/train_text_to_image_lora.py 16https://huggingface.co/lambdalabs/miniSD-diffusers\nthe AdamW optimizer with weight decay of 10\u22126, and augment the data with random horizontal flips. We train the model for 100 epochs under a batch size of 64, using a cosine annealing learning rate schedule with 0.1 fraction warmup and an initial learning rate of 3\u00d710\u22124. At the inference phase, we sample new images using the 50-step DDIM solver and a guidance scale of 7.5 (Ho & Salimans, 2022)."
        },
        {
            "heading": "A.2 LDS EVALUATION SETUP",
            "text": "To conduct the LDS evaluation, we sample M = 64 different random subsets of the training set D, and train three models with different random seeds on each one of these subsets. Each subset sampled to 50% of the size of the training set, that is, we set \u03b1 = 0.5. We then compute the linear datamodeling score for each sample of interest as the Spearman rank correlation between the model output and the attribution-based output prediction of the model output as described in Eq. (3). Especially, when computing LSimple(x; \u03b8) as described in Eq. (2) for any sample of interest from either the validation set or generation set, we consider all the 1000 timesteps selected to be evenly spaced within the interval [1, T ] to approximate the expectation Et. For each timestep, we sample three standard Gaussian noises \u03f5 \u223c N (\u03f5|0, I) to approximate the expectation E\u03f5. Finally, we average the LDS across samples of interest from the validation set and generation set respectively."
        },
        {
            "heading": "A.3 BASELINES",
            "text": "In this paper, we majorly focus on conducting data attribution in a post-hoc manner, which refers to the application of attribution methods after model training. Post-hoc attribution methods do not add extra restrictions to how we train models and are thus preferred in practice (Ribeiro et al., 2016).\nFollowing (Hammoudeh & Lowd, 2022b), we consider the attribution baselines that can be applied to our settings. We filter out intractable methods on our settings like Leave-One-Out (Cook & Weisberg, 1982) and Shapely Value (Shapley et al., 1953; Ghorbani & Zou, 2019). We exclude those methods that are inapplicable to DDPMs, which are generally designed for specific tasks and models like Representer Point (Yeh et al., 2018). We also omit HYDRA (Chen et al., 2021), which is closely related to TracInCP Pruthi et al. (2020) and can be viewed as trading (incremental) speed for lower precision compared to TracInCP as described by Hammoudeh & Lowd (2022b).\nWe also noticed two concurrent works that do not apply to our settings. Dai & Gifford (2023) propose to conduct training data attribution on diffusion models based on machine unlearning (Bourtoule et al., 2021). However, their method is restricted to the diffusion models that are trained by a specially designed machine unlearning training process thus is not post-hoc and could not be applied to the common settings. Wang et al. (2023) claim that analyzing training influences in a post-hoc manner using existing influence estimation methods is currently intractable and instead propose a method that also requires training/tuning the pretrained text-to-image model in a specifically designed process, which is called \"customization\" in their paper.\nRegarding methods that use the Hessian matrix as the kernel including Influence Functions (Koh & Liang, 2017), Relative Influence (Barshan et al., 2020) and Renormalized Influence (Hammoudeh & Lowd, 2022a), previous work has shown that computing the inverse of the Hessian matrix exhibits numerical instability in practical scenarios, often resulting in either divergence or high computational costs, particularly when dealing with deep models (Basu et al., 2021; Pruthi et al., 2020). In more recent approaches, the Hessian matrix is substituted with the Fisher information matrix (FIM) (Ting & Brochu, 2018; Barshan et al., 2020; Teso et al., 2021; Grosse et al., 2023). Martens (2020) discuss the relationship between Hessian and FIM more thoroughly.\nRecently, Park et al. (2023) developed an estimator that leverages a kernel matrix that is similar to the FIM based on linearizing the model and further employs the classical random projection to speed up the hessian-based influence functions (Koh & Liang, 2017), as the estimators are intractable otherwise. We summarize these two modifications as TRAK\u2019s scalability optimizations and also apply them to Relative Influence and Renormalized Influence.\nWe divided the baselines into two categories: retraining-free and retraining-based methods. The retraining-free methods are further classified into three types: similarity-based, gradient-based (without kernel) and gradient-based (with kernel) methods. Raw pixel and CLIP similarity (Radford et al., 2021) are two similarity-based methods. Gradient (Charpiat et al., 2019), TracInCP (Pruthi et al., 2020), and GAS (Hammoudeh & Lowd, 2022a) are gradient-based (without kernel) meth-\nods. TRAK (Park et al., 2023), Relative Influence Barshan et al. (2020) and Renormalized Influence(Hammoudeh & Lowd, 2022a), and Journey TRAK (Georgiev et al., 2023) are gradient-based (with kernel) methods. TRAK (ensemble), Empirical Influence (Feldman & Zhang, 2020), and Datamodel (Ilyas et al., 2022) are retraining-based methods.\nWe next provide definition and implementation details of the baselines used in Section 4.\nRaw pixel. This is a naive similarity-based attribution method that simply uses the raw image as the representation and then computes the dot product or cosine similarity between the sample of interest and each training sample as the attribution score. Especially, for ArtBench, which uses latent diffusion models (Rombach et al., 2022), we represent the image using the VAE encodings (Van Den Oord et al., 2017) of the raw image.\nCLIP Similarity. This is another similarity-based attribution method. We encode each sample into an embedding using CLIP (Radford et al., 2021) and then compute the dot product or cosine similarity between the target sample and each training sample as the attribution score.\nGradient. This is a gradient-based influence estimator from Charpiat et al. (2019), which computes the dot product or cosine similarity using the gradient representations of the sample of interest and each training sample, as the attribution score as follows\n\u03c4(x,D)n = P\u22a4\u2207\u03b8LSimple(x; \u03b8\u2217)\u22a4 \u00b7 P\u22a4\u2207\u03b8LSimple(xn; \u03b8\u2217);\n\u03c4(x,D)n = P\u22a4\u2207\u03b8LSimple(x; \u03b8\u2217)\u22a4 \u00b7 P\u22a4\u2207\u03b8LSimple(xn; \u03b8\u2217) \u2225P\u22a4\u2207\u03b8LSimple(x; \u03b8\u2217)\u2225\u2225P\u22a4\u2207\u03b8LSimple(xn; \u03b8\u2217)\u2225 .\nTracInCP. We use the TracInCP estimator from Pruthi et al. (2020), implemented as\n\u03c4(x,D)n = 1\nC \u03a3Cc=1P\u22a4c \u2207\u03b8LSimple(x; \u03b8c)\u22a4 \u00b7 P\u22a4c \u2207\u03b8LSimple(xn; \u03b8c),\nwhere C is the number of model checkpoints selected from the training trajectory evenly and \u03b8c is the corresponding checkpoint. We use four checkpoints from the training trajectory. For example, for CIFAR-2, we take the checkpoints at epochs {50, 100, 150, 200}. GAS. This is a \"renormalized\" version of the TracInCP based on using the cosine similarity instead of raw dot products (Hammoudeh & Lowd, 2022a).\nTRAK. As discussed in Section 3, we adapt the TRAK (Park et al., 2023) to the diffusion setting as described in Eq. (6), the retraining-free TRAK is implemented as\n\u03a6TRAK = [ \u03d5(x1); \u00b7 \u00b7 \u00b7 ;\u03d5(xN ) ]\u22a4 , where \u03d5(x) = P\u22a4\u2207\u03b8LSimple(x; \u03b8\u2217);\n\u03c4(x,D)n = P\u22a4\u2207\u03b8LSimple(x; \u03b8\u2217)\u22a4 \u00b7 ( \u03a6TRAK \u22a4\u03a6TRAK + \u03bbI )\u22121 \u00b7 P\u22a4\u2207\u03b8LSimple(xn; \u03b8\u2217),\nwhere the \u03bbI serves for numerical stability and regularization effect. We explore the effect of this term in Appendix B. The ensemble version of TRAK is implemented as\n\u03a6sTRAK = [ \u03d5s(x1); \u00b7 \u00b7 \u00b7 ;\u03d5s(xN ) ]\u22a4 , where \u03d5s(x) = P\u22a4s \u2207\u03b8LSimple(x; \u03b8\u2217s);\n\u03c4(x,D)n = 1\nS \u03a3Ss=1P\u22a4s \u2207\u03b8LSimple(x; \u03b8\u2217s)\u22a4 \u00b7\n( \u03a6sTRAK \u22a4\u03a6sTRAK + \u03bbI )\u22121 \u00b7 P\u22a4s \u2207\u03b8LSimple(xn; \u03b8\u2217s).\nRelative Influence. Barshan et al. (2020) propose the \u03b8-relative influence functions estimator, which normalizes Koh & Liang (2017) influence functions\u2019 estimator by HVP magnitude. We adapt this method to our setting after applying TRAK\u2019s scalability optimizations, which is formulated as\n\u03c4(x,D)n = P\u22a4\u2207\u03b8LSimple(x; \u03b8\u2217)\u22a4 \u00b7\n( \u03a6TRAK \u22a4\u03a6TRAK + \u03bbI )\u22121 \u00b7 P\u22a4\u2207\u03b8LSimple(xn; \u03b8\u2217)\n\u2225 ( \u03a6TRAK \u22a4\u03a6TRAK + \u03bbI )\u22121 \u00b7 P\u22a4\u2207\u03b8LSimple(xn; \u03b8\u2217)\u2225 .\nRenormalized Influence. Hammoudeh & Lowd (2022a) also propose to renormalize the influence by the magnitude of the training sample\u2019s gradients. We adapt this method to our setting after applying\nTRAK\u2019s scalability optimizations, which is formulated as\n\u03c4(x,D)n = P\u22a4\u2207\u03b8LSimple(x; \u03b8\u2217)\u22a4 \u00b7\n( \u03a6TRAK \u22a4\u03a6TRAK + \u03bbI )\u22121 \u00b7 P\u22a4\u2207\u03b8LSimple(xn; \u03b8\u2217) \u2225P\u22a4\u2207\u03b8LSimple(xn; \u03b8\u2217)\u2225 .\nJourney TRAK. Georgiev et al. (2023) focus on attributing noisy images xt, while we attribute the finally generated image x. We adapt their method to our setting by averaging the attributions over the generation timesteps as follows\n\u03c4(x,D)n = 1\nT \u2032 \u03a3T\n\u2032 t=1P\u22a4\u2207\u03b8LtSimple(xt; \u03b8\u2217)\u22a4 \u00b7 ( \u03a6TRAK \u22a4\u03a6TRAK + \u03bbI )\u22121 \u00b7 P\u22a4\u2207\u03b8LSimple(xn; \u03b8\u2217),\nwhere T \u2032 is inference steps set as 50 and xt is the noisy generated image along the sampling trajectory.\nEmpirical Influence. We use the downsampling-based approximation to leave-one-out influences as used by Feldman & Zhang (2020), using up to S = 512 models trained on different random 50% subsets of the full training set, which is a difference-in-means estimator given by:\n\u03c4(x,D)n = EDs\u220bxnLSimple(x; \u03b8\u2217s)\u2212 EDs \u0338\u220bxnLSimple(x; \u03b8\u2217s).\nEspecially, when computing LSimple(x; \u03b8) as described in Eq. (2) for any sample of interest from either the validation set or generation set, we consider all the 1000 timesteps selected to be evenly spaced within the interval [1, T ] to approximate the expectation Et. For each timestep, we sample one standard Gaussian noise \u03f5 \u223c N (\u03f5|0, I) to approximate the expectation E\u03f5. Datamdoel. We use the regression-based estimator from Ilyas et al. (2022), using up to S = 512 mdoels trained on different random 50% subsets of the full training set."
        },
        {
            "heading": "A.4 TIME MEASUREMENTS",
            "text": "We evaluate the computational cost of attribution methods using two factors: the total number of timesteps used and the total number of trained models used. These factors are hardware and implementation-agnostic. We note the number of timesteps used as K. Unlike classification models, we need to compute the gradients of multiple timesteps per sample for diffusion models. For retraining-free methods, the time it takes to compute attribution scores will be dominated by the time of computing the gradients for different timesteps.\nTRAIN_TIME: The time to train one model (from scratch or fine-tuning).\nGRAD_TIME: The time to compute gradients of one model for each sample at each timestep in the entire dataset including train, validation, and generation sets.\nPROJ_TIME: The time to project gradients of one model for each sample in the entire dataset including training, validation, and generation sets. Note that for each sample, we first average the gradient vectors over multiple timesteps and then conduct the random projection.\nWe approximate the total compute time for each method as follows.\nTracInCP and GAS: C\u00d7 (K\u00d7 GRAD_TIME + PROJ_TIME) TRAK and D-TRAK: K\u00d7 GRAD_TIME + PROJ_TIME TRAK and D-TRAK (ensemble): S\u00d7 (TRAIN_TIME + K\u00d7 GRAD_TIME + PROJ_TIME) Empirical Influence and Datamodel: S\u00d7 TRAIN_TIME"
        },
        {
            "heading": "B ABLATION STUDIES",
            "text": "To double-check if the counter-intuitive observations in Table 1 and Figure 1 are caused by certain implementation details, we perform more ablation studies to compare the performance between TRAK and D-TRAK (LSquare). In this section, we choose the CIFAR-2 as our major setting, we also consider CelebA and ArtBench-2 in some experiments. The details of these settings can be found in Appendix A.1. We set up the corresponding LDS benchmarks as described in Appendix A.2.\nWe focus on the retraining-free setting, which means S = 1 and \u03b2 = 1.0, where we compute the gradients reusing the model we want to interpret. When computing gradients for attribution methods, we consider various number of timesteps where the timesteps are selected to be evenly spaced within the interval [1, T ] by the arange operation, which we term as uniform. More concretely, the selected timesteps are {1, 101, 201, \u00b7 \u00b7 \u00b7 , 901} when the number of timesteps is 10. For each timestep, we sample one standard Gaussian noise.\nF Selection. Recap the LDS metric in Definition 2, we need to specify the model output function before conducting the LDS evaluation. This introduces a design choice for discussion. Given a DDPM trained by minimizing L(D; \u03b8) = LSimple(D; \u03b8), there is \u03b8\u2217(D) = argmin\u03b8 LSimple(D; \u03b8) and setting the model output function to be F(x; \u03b8) = L(x; \u03b8) = LSimple(x, \u03b8) should be a natural choice. Nevertheless, we further conduct a comprehensive study by constructing different LDS benchmarks that use different model output functions such as LELBO, LSquared, L1-norm, and L\u221e-norm and then evaluate different constructions of \u03d5s as what we did in Table 1. As shown in Table 4, we first observe that setting the model output function as one of LSquared, L1-norm, and L\u221e-norm, will make the attribution task fail as indicated by near-zero LDS scores. One potential explanation is that these functions are not well approximated by the linear functions of training samples as suggested by Park et al. (2023). Using either LSimple, LELBO yields reasonable LDS scores, rank different constructions of \u03d5s in a similar order and have similar LDS scores for P\u22a4s \u2207\u03b8LSimple. However, setting the model output function to be LSimple(x, \u03b8) yields higher LDS scores for P\u22a4s \u2207\u03b8LSquare\u201430.81% and 22.62%\non validation and generation set respectively, thus we set F(x; \u03b8) = LSimple(x, \u03b8) as the default choice in the following experiments.\nValue of k. When we compute the attribution scores, we use random projection to reduce the dimensionality of the gradients. Intuitively, as the resulting dimension k increases, the associated projection better preserves inner products, but is also more expensive to compute Johnson & Lindenstrauss (1984). We thus investigate how the projection dimension k selection affects the attribution performance. Figure 5 shows that the dimension is increased, and the LDS scores of both TRAK and our method initially increase and gradually saturate, as expected. In the following experiments, we set k = 32768 as the default choice.\nNumber of noises. When computing the gradients, for every timestep, we can sample multiple noises and then average the resulting gradients for better performance at the cost of computation. However, as shown in Table 5, increasing the number of noises is less effective than increasing the number of timesteps. So we set using one noise per timestep when computing the gradients as default.\nTimestep selection. As we mentioned previously, we set the time selection strategy as uniform. We also explore another potential strategy cumulative. For example, when the number of timesteps is set as 10, we will select {1, 2, 3, \u00b7 \u00b7 \u00b7 , 10} for cumulative. As shown in Table 5, for D-TRAK, the sampling strategy has little influence, while for TRAK, the uniform is significantly better. Thus we set the sampling strategy as uniform as default in the following experiments for a fair comparison.\nCheckpoint selection. Motivated by Pruthi et al. (2020), we study the effect of using different checkpoints of a model to compute the gradients. As shown in Figures 6, 7 and 8, for D-TRAK, the best LDS scores are obtained at the final checkpoint. However, for TRAK, using the final checkpoint is not the best choice. Finding which checkpoint yields the best LDS score requires computing the attributions many times, which means much more computational cost. Additionally, in practice, we might only get access to the final model.\nNumber of timesteps. When computing the attribution scores, considering the gradients from more timesteps might increase the performance. That is, there is a trade-off between efficacy and computational efficiency. We consider the extreme case that we compute gradients for all the 1000 timesteps, which is computationally expensive. As shown in Figures 6, 7 and 8, the more timestep we consider, the better LDS we can get. However, for our method, setting the number of timesteps as 100 even 10 could get performance similar to 1000, while being much faster.\nValue of \u03bb. In our initial experiments, we noticed that adding \u03bbI to the kernel before inverting the kernel matrix, is necessary for obtaining better LDS scores. This term may serve for numerical stability and regularization effect as suggested by Hastie (2020). Traditionally the \u03bb is small and close to zero, however, in our case, we found that we need to set a relatively larger \u03bb, which might be because the kernel is not divided by the number of training samples N following Park et al. (2023). We search the \u03bb from {1e-2, 2e-2, 5e-2, 1e-1, 2e-1, 5e-1,\u00b7 \u00b7 \u00b7 , 1e6, 2e6, 5e6}. As shown in Figures 9 and 10, setting a proper \u03bb, which adjusts the regularization level, has a significant influence on the attribution performance. In our experiments, we search \u03bb on the LDS benchmarks directly and report the peak LDS scores for both TRAK and D-TRAK for a fair comparison. In practice, we can search the proper \u03bb value by manually examining the attribution visualizations or evaluating the LDS scores on a smaller scale LDS benchmark."
        },
        {
            "heading": "C ADDITIONAL EXPERIMENT RESULTS",
            "text": "In this section, we provide additional experiment results of LDS evaluation (on both retraining-free methods and retraining-based methods) and counterfactual evaluation."
        },
        {
            "heading": "C.1 LDS EVALUATION (RETRAINING-FREE METHODS)",
            "text": "As shown in Table 6, on the validation set, with 10/100 timesteps, D-TRAK achieves improvements of +11.55%/+8.67% on CelebA. On the generation set, with 10/100 timesteps, D-TRAK exhibits gains of +9.82%/+6.76% in the LDS scores. These results highlight the efficacy of D-TRAK and underscore its capacity for enhancing LDS scores."
        },
        {
            "heading": "C.2 LDS EVALUATION (RETRAINING-BASED METHODS)",
            "text": "As shown in Tables 7 and 8, we compare D-TRAK with those retraining-based methods as references on CIFAR-2, CelebA, and ArtBench-2. Under this setting, we need to retrain multiple models on different random subsets of the full training set and compute gradients multiple times, where each subset has a fixed size of \u03b2 \u00b7 N . We set \u03b2 = 0.5. At the cost of computation, using the gradients computed by independently trained models improves the LDS scores for both TRAK and D-TRAK. Considering 8 models, D-TRAK still performs better than TRAK on various settings.\nWe also compare D-TRAK with both Empirical Influence and Datamodel. These two methods are computationally expensive so we train at most 512 models due to our computation limits. Overall, these two methods perform similarly on our settings. With larger S, which translates into retraining\nmore models and thus more computation needed, the LDS scores grow gradually. However, using 512 models, the LDS scores obtained by these two methods are still substantially lower than D-TRAK, even TRAK."
        },
        {
            "heading": "C.3 COUNTERFACTUAL EVALUATION",
            "text": "In the context of CelebA, the counterfactual evaluation of D-TRAK against TRAK, as presented in Figure 11, demonstrates that D-TRAK can better identify influential images that have a larger impact on the generated images. When examining the median pixel-wise \u21132 distance resulting from removing-and-retraining, D-TRAK yields 15.07, in contrast to TRAK\u2019s values of 11.02. A reverse trend is observed when evaluating the median CLIP cosine similarity, where D-TRAK obtains lower similarities of 0.896, which are notably lower than TRAK\u2019s 0.942.\nD VISUALIZATION\nWe provide more visualization results, including counterfactual visualization, proponents and opponents visualization, as well as self-influence (memorization) visualization."
        },
        {
            "heading": "D.1 COUNTERFACTUAL VISUALIZATION",
            "text": "In the context of CIFAR-2, CelebA, and ArtBench-2 datasets, we compare the original generated samples to those generated from the same random seed with the models trained after the exclusion of the top 1,000 positive influencers identified by different attribution methods including Random, TRAK, and D-TRAK. For both D-TRAK and TRAK, we consider 100 timesteps selected to be evenly spaced within the interval [1, T ], sample one standard Gaussian noise, and set k = 32768 when computing the gradients. In terms of the LDS(%) performance on the generation sets of CIFAR-2, CelebA, and ArtBench-2, D-TRAK and TRAK obtain 25.67% versus 15.87%, 21.47% versus 14.71%, and 26.53% versus 20.02% respectively, as reported previously.\nAs shown in Figure 12, our results suggest that our method D-TRAK can better identify influential images that have a significant impact on the target image. Take the first row of the left column in\nFigure 12 as an example, the model retrained after removing training samples based on Random and TRAK still generates a automobile image, while the one corresponding to D-TRAK generates an image looks similar to a mixture of automobile and horse. It is also interesting to see that for CIFAR-2 and CelebA, randomly removing 1,000 training samples results in little change in the generated images based on manual perception."
        },
        {
            "heading": "D.2 PROPONENTS AND OPPONENTS VISUALIZATION",
            "text": "Following Pruthi et al. (2020), we term training samples that have a positive influence score as proponents and samples that have a negative value of influence score as opponents. For samples of interest, we visualize the training samples corresponding to the most positive and negative attribution scores. The visualizations on CIFAR-2, CIFAR-10, CelebA, ArtBench-2 and ArtBench-5 are shown in Figures 15, 16, 17, 18 and 19, respectively. We manually check the proponents and opponents. We observe that D-TRAK consistently finds proponents visually more similar to the target samples."
        },
        {
            "heading": "D.3 SELF-INFLUENCE (MEMORIZATION) VISUALIZATION",
            "text": "Motivated by Feldman & Zhang (2020); Zheng & Jiang (2022); Gu et al. (2023), we also visualize the training samples that have the highest self-influence scores on CIFAR-2 and ArtBench-2 as shown in Figure 20. The self-influence of xn is computed as \u03c4D-TRAK(xn,D)n. When computing the self-influence scores, we consider 100 timesteps selected to be evenly spaced within the interval [1, T ], and set k = 32768. The identified highly self-influenced samples usually look atypical or unique, while low self-influence ones have similar samples in the training set. Especially, on CIFAR-2, highly self-influenced samples are visually high-contrast."
        },
        {
            "heading": "E COUNTERFACTUAL EVALUATION AT A PARTICULAR STEP",
            "text": "Journey TRAK Georgiev et al. (2023) is designed to conduct data attribution at a particular sampling timestep and thus produce different attribution scores for each timestep. However, our method D-TRAK is designed to conduct data attribution for the final generated images and thus produce only one attribution for the entire diffusion trajectory. To conduct a counterfactual evaluation between these two methods, we adapt D-TRAK to Journey TRAK\u2019s setting by sharing the same attribution scores for all timesteps. We also apply this adaptation to the Random and TRAK baselines and include them in this evaluation.\nWe consider timestep 400 and 300. Then we compute attribution scores on 60 generated samples using different attribution methods. For Random, TRAK, and D-TRAK, we still use the retraining-free settings described in Section 4.4 and share the same attribution scores for both timestep 400 and 300. For Journey TRAK, we compute the attribution scores specific to each timestep. Especially, for Journey TRAK, we set the number of resampling rounds of the random noise at each timestep as 10 when computing the gradients with respect to Journey TRAK\u2019s model output function. We also consider an ensemble variant of Journey TRAK to strengthen its performance via ensembling the attribution scores computed from 8 models. Given the attribution scores for each sample, we then retrain the model after removing the corresponding top-1000 influencers.\nFor simplicity, we abbreviate Journey TRAK as J-TRAK and the ensemble variant as J-TRAK (8). As shown in Figures 21 and 23, when examining the median pixel-wise \u21132-dist distance resulting from removing-and-retraining, D-TRAK yields 7.39/8.71 and 194.18/195.72 for CIFAR-2 and ArtBench-2 at timestep 400/300 respectively, in contrast to J-TRAK (8)\u2019s values of 6.63/7.01 and 174.76/175.06. D-TRAK obtains median similarities of 0.927/0.899 and 0.773/0.740 for the above two datasets at timestep 400/300 respectively, which are notably lower than J-TRAK (8)\u2019s values of 0.961/0.939 and 0.816/0.805, highlighting the effectiveness of our method.\nWe manually compare the original generated samples to those generated from the same random seed with the re-trained models. As shown in Figures 22 and 24, our results first show that J-TRAK (8) can successfully induce a larger effect on diffusion models\u2019 generation results via removing high-scoring training images specific to a sampling timestep, compared to baselines like Random and TRAK. Second, although D-TRAK is not designed for attributing a particular sampling timestep, our results suggest our method can better identify images that have a larger impact on the target image at a specific timestep via removing high-scoring training images based on attribution scores derived from the entire sampling trajectory. Finally, it is also worth noting that for different timesteps, J-TRAK (8) identifies different influential training examples to remove and thus leads to different generation results. Take the 4th row in both Figures 22 and 24 as an example, for the timestep 400, J-TRAK (8) leads the diffusion model to generate a man with the white beard. However, for the timestep 300, it is a man with brown bread and an extra hat. In contrast, D-TRAK removes the same set of influential training examples for different timesteps and thus produces a consistent generation: a still-life painting for both timestep 400 and 300. The above phenomenon again highlights the differences between Journey TRAK and D-TRAK.\nJourney TRAK may perform better via ensembling more models like 50 models at the cost of computations as suggested in Georgiev et al. (2023). Nonetheless, in this paper, we are more interested in the retraining-free setting."
        }
    ],
    "title": "ON DIFFUSION MODELS",
    "year": 2024
}