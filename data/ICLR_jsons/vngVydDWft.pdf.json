{
    "abstractText": "It has been observed that representations learned by distinct neural networks conceal structural similarities when the models are trained under similar inductive biases. From a geometric perspective, identifying the classes of transformations and the related invariances that connect these representations is fundamental to unlocking applications, such as merging, stitching, and reusing different neural modules. However, estimating task-specific transformations a priori can be challenging and expensive due to several factors (e.g., weights initialization, training hyperparameters, or data modality). To this end, we introduce a versatile method to directly incorporate a set of invariances into the representations, constructing a product space of invariant components on top of the latent representations without requiring prior knowledge about the optimal invariance to infuse. We validate our solution on classification and reconstruction tasks, observing consistent latent similarity and downstream performance improvements in a zero-shot stitching setting. The experimental analysis comprises three modalities (vision, text, and graphs), twelve pretrained foundational models, eight benchmarks, and several architectures trained from scratch.",
    "authors": [],
    "id": "SP:ea4211a0d9aea626edf16541161f0e3cb2ecd29f",
    "references": [
        {
            "authors": [
                "Martin Arjovsky",
                "L\u00e9on Bottou",
                "Ishaan Gulrajani",
                "David Lopez-Paz"
            ],
            "title": "Invariant risk minimization",
            "venue": "arXiv preprint arXiv:1907.02893,",
            "year": 2019
        },
        {
            "authors": [
                "Gregory Benton",
                "Marc Finzi",
                "Pavel Izmailov",
                "Andrew G Wilson"
            ],
            "title": "Learning invariances in neural networks from training data",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Irene Cannistraci",
                "Luca Moschella",
                "Valentino Maiorca",
                "Marco Fumero",
                "Antonio Norelli",
                "Emanuele Rodol\u00e0"
            ],
            "title": "Bootstrapping parallel anchors for relative representations",
            "venue": "The First Tiny Papers Track at ICLR",
            "year": 2023
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V. Le",
                "Christopher D. Manning"
            ],
            "title": "ELECTRA: pre-training text encoders as discriminators rather than generators",
            "venue": "In 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Taco Cohen",
                "Max Welling"
            ],
            "title": "Group equivariant convolutional networks",
            "venue": "In International conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "Taco S Cohen",
                "Mario Geiger",
                "Maurice Weiler"
            ],
            "title": "A general theory of equivariant cnns on homogeneous spaces",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "year": 1911
        },
        {
            "authors": [
                "Keenan Crane",
                "Clarisse Weischedel",
                "Max Wardetzky"
            ],
            "title": "The heat method for distance computation",
            "venue": "Commun. ACM,",
            "year": 2017
        },
        {
            "authors": [
                "MohammadReza Davari",
                "Stefan Horoi",
                "Amine Natik",
                "Guillaume Lajoie",
                "Guy Wolf",
                "Eugene Belilovsky"
            ],
            "title": "Reliability of cka as a similarity measure in deep learning",
            "venue": "arXiv preprint arXiv:2210.16156,",
            "year": 2022
        },
        {
            "authors": [
                "Li Deng"
            ],
            "title": "The mnist database of handwritten digit images for machine learning research",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2012
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "year": 2019
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Alhussein Fawzi",
                "Horst Samulowitz",
                "Deepak Turaga",
                "Pascal Frossard"
            ],
            "title": "Adaptive data augmentation for image classification",
            "venue": "In 2016 IEEE international conference on image processing (ICIP),",
            "year": 2016
        },
        {
            "authors": [
                "Kanchana Vaishnavi Gandikota",
                "Jonas Geiping",
                "Zorah L\u00e4hner",
                "Adam Czapli\u0144ski",
                "Michael Moeller"
            ],
            "title": "Training or architecture? how to incorporate invariance in neural networks",
            "venue": "arXiv preprint arXiv:2106.10044,",
            "year": 2021
        },
        {
            "authors": [
                "Dongyoon Han",
                "Sangdoo Yun",
                "Byeongho Heo",
                "YoungJoon Yoo"
            ],
            "title": "Rethinking channel dimensions for efficient model design",
            "venue": "ArXiv preprint,",
            "year": 2020
        },
        {
            "authors": [
                "Irina Higgins",
                "S\u00e9bastien Racani\u00e8re",
                "Danilo Rezende"
            ],
            "title": "Symmetry-based representations for artificial and biological general intelligence, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Harold Hotelling"
            ],
            "title": "Relations between two sets of variates",
            "venue": "Breakthroughs in statistics: methodology and distribution,",
            "year": 1992
        },
        {
            "authors": [
                "Eduard Hovy",
                "Laurie Gerber",
                "Ulf Hermjakob",
                "Chin-Yew Lin",
                "Deepak Ravichandran"
            ],
            "title": "Toward semantics-based answer pinpointing",
            "venue": "In Proceedings of the First International Conference on Human Language Technology Research,",
            "year": 2001
        },
        {
            "authors": [
                "Alexander Immer",
                "Tycho van der Ouderaa",
                "Gunnar R\u00e4tsch",
                "Vincent Fortuin",
                "Mark van der Wilk"
            ],
            "title": "Invariance learning in deep neural networks with differentiable laplace approximations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Haribabu Kandi",
                "Ayushi Jain",
                "Swetha Velluva Chathoth",
                "Deepak Mishra",
                "Gorthi RK Sai Subrahmanyam"
            ],
            "title": "Incorporating rotational invariance in convolutional neural network architecture",
            "venue": "Pattern Analysis and Applications,",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Max Klabunde",
                "Tobias Schumacher",
                "Markus Strohmaier",
                "Florian Lemmerich"
            ],
            "title": "Similarity of neural network models: A survey of functional and representational measures",
            "venue": "arXiv preprint arXiv:2305.06329,",
            "year": 2023
        },
        {
            "authors": [
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Honglak Lee",
                "Geoffrey Hinton"
            ],
            "title": "Similarity of neural network representations revisited",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut"
            ],
            "title": "Albert: A lite bert for self-supervised learning of language representations",
            "year": 1909
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "ArXiv preprint,",
            "year": 2019
        },
        {
            "authors": [
                "Clare Lyle",
                "Mark van der Wilk",
                "Marta Kwiatkowska",
                "Yarin Gal",
                "Benjamin Bloem-Reddy"
            ],
            "title": "On the benefits of invariance in neural networks",
            "venue": "arXiv preprint arXiv:2005.00178,",
            "year": 2020
        },
        {
            "authors": [
                "Ari Morcos",
                "Maithra Raghu",
                "Samy Bengio"
            ],
            "title": "Insights on representational similarity in neural networks with canonical correlation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Luca Moschella",
                "Valentino Maiorca",
                "Marco Fumero",
                "Antonio Norelli",
                "Francesco Locatello",
                "Emanuele Rodol\u00e0"
            ],
            "title": "Relative representations enable zero-shot latent space communication",
            "venue": "arXiv preprint arXiv:2209.15430,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Maithra Raghu",
                "Justin Gilmer",
                "Jason Yosinski",
                "Jascha Sohl-Dickstein"
            ],
            "title": "Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Matthias Rath",
                "Alexandru Paul Condurache"
            ],
            "title": "Deep neural networks with efficient guaranteed invariances",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Justin Salamon",
                "Juan Pablo Bello"
            ],
            "title": "Deep convolutional neural networks and data augmentation for environmental sound classification",
            "venue": "IEEE Signal processing letters,",
            "year": 2017
        },
        {
            "authors": [
                "Prithviraj Sen",
                "Galileo Namata",
                "Mustafa Bilgic",
                "Lise Getoor",
                "Brian Galligher",
                "Tina Eliassi-Rad"
            ],
            "title": "Collective Classification in Network Data. AIMag",
            "year": 2008
        },
        {
            "authors": [
                "Hang Shao",
                "Abhishek Kumar",
                "P. Thomas Fletcher"
            ],
            "title": "The riemannian geometry of deep generative models, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Joshua B Tenenbaum",
                "Vin de Silva",
                "John C Langford"
            ],
            "title": "A global geometric framework for nonlinear dimensionality reduction",
            "year": 2000
        },
        {
            "authors": [
                "Tycho FA van der Ouderaa",
                "Mark van der Wilk"
            ],
            "title": "Learning invariant weights in neural networks",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Zhen Wang",
                "Xu Shan",
                "Xiangxie Zhang",
                "Jie Yang"
            ],
            "title": "N24News: A new dataset for multimodal news classification",
            "venue": "In Proceedings of the Thirteenth Language Resources and Evaluation Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Daniel E Worrall",
                "Stephan J Garbin",
                "Daniyar Turmukhambetov",
                "Gabriel J Brostow"
            ],
            "title": "Harmonic networks: Deep translation and rotation equivariance",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Han Xiao",
                "Kashif Rasul",
                "Roland Vollgraf"
            ],
            "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
            "venue": "arXiv preprint arXiv:1708.07747,",
            "year": 2017
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Jake Zhao",
                "Yann LeCun"
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems",
            "year": 2015
        },
        {
            "authors": [
                "Yan Zhang",
                "Jonathon Hare",
                "Adam Prugel-Bennett"
            ],
            "title": "Deep set prediction networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yanzhao Zhou",
                "Qixiang Ye",
                "Qiang Qiu",
                "Jianbin Jiao"
            ],
            "title": "Oriented response networks",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Moschella"
            ],
            "title": "Table 12: Text Stitching Performance Cross-Architecture and Cross-Seed. Zero-shot accuracy score for text classification task across different pretrained models, seeds, and datasets. The aggregation function is the Aggregation by sum and results obtained with a linear classifier",
            "year": 2024
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Discovering symmetries and conserved quantities is a core step for extracting meaningful representations from raw data in biological and artificial systems Higgins et al. (2022); Benton et al. (2020); Lyle et al. (2020). Achieving invariance to specific groups of transformations within neural models holds significant utility in a wide range of real-world applications, such as comparing similar latent spaces across multiple training instances, facilitating communication,\nand enabling model reuse Cohen & Welling (2016); Fawzi et al. (2016); Salamon & Bello (2017); Klabunde et al. (2023). These desired invariances can be defined with respect to transformations in the input space Benton et al. (2020); Immer et al. (2022); Cohen & Welling (2016); Cohen et al. (2019), or in relation to the latent space, as explored by Moschella et al. (2022). Such properties can arise implicitly from architectural choices Cohen & Welling (2016); Cohen et al. (2019); Worrall et al. (2017); Zhou et al. (2017) or be explicitly enforced using methods like loss penalties Arjovsky et al. (2019). A recent study introduced the concept of Relative Representation (RR) Moschella et al. (2022). In its original formulation, this framework enforces invariance to angle-preserving transformations of the latent space. This approach enhances communication between latent spaces by projecting them into a shared relative space determined by distances between data\n2Global orthogonal transformations composed with local rescalings\npoints. However, as shown in Figure 1, the transformations relating different neural representations are not always consistent with a single class of transformations, such as the one considered in Moschella et al. (2022). Determining a priori which class of transformations relates distinct latent spaces is challenging due to complex interactions in the data, and multiple nuisance factors that are typically irrelevant but can nevertheless affect the representation (e.g. random initialization, neural architecture, and data modality). To address this challenge, we expand upon the method of RR, presenting a framework to efficiently incorporate a set of invariances into the learned latent space. This is achieved by constructing a product space of invariant components on top of the latent representations of, possibly pretrained, neural models. Each component of this product space is obtained by projecting samples as a function of fixed data points, denoted as anchors. Using different similarity functions for each subspace, we can infuse invariances to specific transformations into each component of the product space. Our main contributions can be summarized as follows:\n\u2022 We show that the class of transformation that relates representations learned by distinct Neural Networks (NNs)\u2013trained on semantically similar data\u2013may vary and depends on multiple factors;\n\u2022 We introduce a framework for infusing multiple invariances into a single latent representation, constructing a product space of invariant components to enhance latent communication;\n\u2022 We validate our findings on stitching tasks across various data modalities, including images, text, and graphs: product of invariances can capture complex transformations of the latent space in a single representation, achieving the best performance without any prior knowledge of the transformation or the factors that may affect it."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Representation Similarity. Several metrics have been proposed to compare latent spaces generated by independent NNs, capturing their inherent similarity up to transformations that correlate the spaces. A classical statistical method is Canonical Correlation Analysis (CCA) Hotelling (1992), which is invariant to linear transformations. While variations of CCA seek to improve robustness through techniques like Singular Value Decomposition (SVD) and Singular Value CCA (SVCCA) Raghu et al. (2017) or to reduce sensitivity to perturbations using methods such as Projection Weighted CCA (PWCCA) Morcos et al. (2018). Closely related to these metrics, the Centered Kernel Alignment (CKA) metric Kornblith et al. (2019) measures the similarity between latent spaces while disregarding orthogonal transformations. However, recent research Davari et al. (2022) demonstrates its sensitivity to transformations that shift a subset of data points in the representation space.\nLearning and Incorporating Invariance and Equivariance into Representations. Invariances in NN models can be enforced through various techniques operating at different levels, including adjustments to model architecture, training constraints, or input manipulation Lyle et al. (2020). Benton et al. (2020) proposes a method to learn invariances and equivariances, Immer et al. (2022) introduces a gradient-based approach that effectively captures inherent invariances in the data. Meanwhile, van der Ouderaa & van der Wilk (2022) enables training of NNs with invariance to specific transformations by learning weight-space equivalents instead of modifying the input data. Other works directly incorporate invariances into the model through specific constraints. Rath & Condurache (2023) enforces a multi-stream architecture to exhibit invariance to various symmetry transformations without relying on data-driven learning, Kandi et al. (2019) propose an improved Convolutional Neural Network (CNN) architecture for better rotation invariance, and Gandikota et al. (2021) introduces a method for designing network architectures that are invariant or equivariant to structured transformations. Finally, Moschella et al. (2022) proposes an alternative representation of the latent space that guarantees invariance to angle-preserving transformation without requiring additional training but only a set of anchors, possibly very small (Cannistraci et al., 2023).\nOur work leverages the RR framework to directly incorporate a set of invariances into the learned latent space, creating a product space of invariant components which, combined, can capture complex transformations of the latent space."
        },
        {
            "heading": "3 INFUSING INVARIANCES",
            "text": "Setting. We consider neural networks as parametric functions F\u03b8 compositions of encoding and decoding maps F\u03b8 = D\u03b82 \u25e6 E\u03b81 , where the encoder E\u03b81 is responsible for computing a latent representation z = E\u03b81(x), x \u2208 X for some domain X , with dim(Z) << dim(X ); and the decoder D\u03b82 is responsible for solving the task at hand (e.g., reconstruction, generation, classification). In the following, we will drop the dependence on parameters \u03b8 for notational convenience. For a single module E (equivalently for D), we indicate with EX if the module E was trained on the domain X . In the upcoming, we will summarize the necessary background to introduce our method.\nBackground. The RR framework Moschella et al. (2022) provides a straightforward approach to represent each sample in the latent space according to its similarity to a set of fixed training samples, denoted as anchors. Representing samples in the latent space as a function of the anchors corresponds to transitioning from an absolute coordinate frame into a relative one defined by the anchors and the similarity function. Given a domain X , an encoding function EX : X \u2192 Z , a set of anchors AX \u2282 X , and a similarity or distance function d : Z \u00d7 Z \u2192 R. The RR for each sample x \u2208 X is:\nRR(z;AX , d) = \u2295\nai\u2208AX\nd(z, EX (ai)) (1)\nwhere z = EX (x), and \u2295\ndenotes row-wise concatenation. In Moschella et al. (2022), d was set as cosine similarity. This choice induces a relative representation invariant to angle-preserving transformations. In this work, our focus is to leverage different choices of the similarity function to induce a set of invariances into the representations to capture complex transformations between latent spaces.\nOverview. When considering different networks F, F \u2032, we are interested in modeling the class of transformations T that relates their latent spaces Z,Z \u2032. T could be something known, e.g., rotations, or nontrivial, complex class of transformations.The two networks could differ by their initialization seeds (i.e., training dynamics), by architectural changes, or even domain changes, i.e., X \u0338= X \u2032, which could affect the latent space in a different way (as observed in Figure 1). The fundamental assumption of this work is that these variations induce changes in the latent representations of the models, but there exists an underlying manifold M where the representations are the same (see Figure 2). Formally: Assumption. Given multiple models F1..Fn we assume that there exists a manifold M which identifies an equivalence class of encoders ET induced by the class of transformation T (e.g. rotations), defined as ET := {E | \u03c0MTE = \u03c0ME, \u2200T \u2208 T }, where \u03c0M represent the projection on M. M is equipped with a metric dM which is preserved under the action of elements of T , i.e. dM(\u03c0Mz, \u03c0Mz \u2032) = dM(\u03c0MT (z), \u03c0MT (z \u2032)), \u2200T \u2208 T\nWhat we look for is a function r which independently projects the latent spaces Z1..Zn into M and is invariant to T , i.e. r(z) = r(Tz), for each T \u2208 T , and for each z \u2208 Z1..Zn. Generalizing the\nframework of Moschella et al. (2022) to arbitrary similarity functions, or distance metrics, gives us a straightforward way to define representations r invariant to specific classes of transformations.\nHowever, dM is typically unknown a priori, and in general, it is challenging to capture T with a single class of transformations (as observed in Figure 1and demonstrated in Section 4.1). To overcome this, in this work, we approximate M with a product space M\u0303 := \u220fN i=1 Mi, where each component is obtained by projecting samples of Z in a relative representation space equipped with a different similarity function di. Each Mi will have properties induced by a similarity function di invariant to a specific, known, class of transformations T\u0303i (e.g. dilations). By combining this set of invariances, we want to recover the representation r such that it approximates well \u03c0M (see Figure 2). We define r formally as the projection from Z to M\u0303: Definition (Product projection). Given a set of latent spaces Z1..Zn, related to one another by an unknown class of transformation T , a set of similarity functions D each one invariant to a specific known class of transformations T\u0303i (e.g. rotations), i.e. RR(z, di) = RR(Tz, di), \u2200T \u2208 T\u0303i. We define the product projection r : Z 7\u2192 M\u0303 as:\nr(z) = \u03d5 \u25e6RR(z;AX , di), \u2200di \u2208 D\nwhere \u03d5 is an aggregation function (e.g. concatenation) responsible for merging the relative spaces induced by each di \u2208 D.\nWe give more details on different strategies on how to implement \u03d5 in section 3.\nDistance-induced invariances. We leverage the RR framework considering the following similarity functions d: Cosine (Cos.), Euclidean (Eucl.), Manhattan (l1), Chebyshev (L\u221e), and Geodesic (Geod.), each one inducing invariances to a specific, known class of transformations. For formal definitions, synthetic examples, and visualizations, please refer to the Appendix A.1.\nAggregation functions. This section summarizes different strategies to construct the product space M\u0303, directly integrating a set of invariances into the representations. Consider a latent space Z image of an encoder E : X 7\u2192 Z , and a set of similarity functions D. For each d \u2208 D, we produce n = |D| relative latent spaces. Every subspace is produced via a similarity function (i.e., Cos., Eucl., l1, or L\u221e), enforcing invariance to a specific class of transformations.\nThese subspaces can be merged using diverse aggregation strategies, corresponding to different choices of \u03d5:\n\u2022 Concatenation (Concat): the subspaces are independently normalized and concatenated, giving to M\u0303 the structure of a cartesian product space.\n\u2022 Aggregation by sum (MLP+Sum): similar to DeepSet Zhang et al. (2019), the subspaces are independently normalized and non-linearly projected. The resulting components are summed.\n\u2022 Self-attention (SelfAttention): the subspaces are independently normalized and aggregated via a self-attention layer.\nWhen not specified, all the results are obtained using the Aggregation by sum strategy. For the implementation details of each strategy, please refer to the Appendix A.2. The product space M yields a robust latent representation, made of invariant components which are combined to capture nontrivial, complex transformations, boosting the performance on downstream tasks."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we perform qualitative and quantitative experiments to analyze the effectiveness of our framework in constructing representations that can capture complex transformations of the latent space. Specifically, Section 4.1 provides empirical motivation, analyzing how the similarity between latent spaces produced by AutoEncoder (AE) and Variational AutoEncoder (VAE) architectures, trained from scratch on various vision datasets may vary, according to the invariance enforced on the representation. Similar findings are presented by examining latent spaces generated by different pretrained models on multiple datasets and modalities (i.e., vision and text). Section 4.2 evaluates\nthe zero-shot stitching performance of our framework across text, vision, and graph modalities; Section 4.3 presents an ablation study on different aggregation functions. Finally, Section 4.4 examines attention weights and their role in selecting the optimal relative subspace."
        },
        {
            "heading": "4.1 LATENT SPACE ANALYSIS",
            "text": ""
        },
        {
            "heading": "TRAINING FROM SCRATCH",
            "text": "Experimental setting. In this experiment, we perform an image reconstruction task on CIFAR\u221210, CIFAR\u2212100 (Krizhevsky et al., 2009), MNIST (Deng, 2012), and FashionMNIST (Xiao et al., 2017) datasets (refer to Table 8 for additional information). Using comparable convolutional architectures, we focus on AEs and VAEs. We consider models with an unflattened latent image bottleneck (referred to as AE and VAE) that preserve the image spatial structure and models with linear projections into and out of a flattened latent space (referred to as Linearized AutoEncoder (LinAE) and Linearized Variational AutoEncoder (LinVAE)). We train until convergence five instances for each combination, using different random seeds. Then, for each model, we project the latent spaces into their relative counterpart with different projection functions, thus infusing different invariances in each representation, and measure the latent space similarity across seeds for each combination.\nResult Analysis. In Figures 3, 11 and 12, we report the Pearson and Spearman cross-seed correlations for various architectures on different datasets. These outcomes illustrate that it is impossible to unify the latent spaces obtained with different initialization by means of a single invariance. For instance, in Figure 3, we observe that the highest similarity across different seeds is achieved using different projection types when considering the VAE or LinVAE architecture, even when keeping fixed all the other parameters. Additionally, dataset variations significantly alter the trends in the behavior of all the architectures. This discovery challenges the assumption in Moschella et al. (2022) that angle-preserving transformations are the primary drivers of correlation among the latent spaces of models trained with different seeds. Please refer to Figures 11 and 12 for additional results."
        },
        {
            "heading": "PRETRAINED MODELS",
            "text": "Experimental setting. In this section, we analyze the similarity of latent spaces produced by pretrained foundational models in both the vision and text domains. For the vision domain, we evaluated five distinct foundational models (either convolutional or transformer-based) using the CIFAR10, CIFAR100, MNIST, and FashionMNIST datasets. Meanwhile, in the text domain, we assessed seven different foundational models using the DBpedia Zhang et al. (2015), Trec (coarse) Hovy et al. (2001), and N24news(Text) Wang et al. (2022) datasets. Refer to Table 7 for specific details on each pretrained model and Table 8 for datasets.\nResult Analysis. In Figure 4, we report the Linear CKA correlations for various models and datasets for vision (left) and text (right) modalities. This analysis highlights the absence of a universally shared transformation class that connects latent spaces of foundation models across distinct conditions. For example, on CIFAR10, the highest similarity is achieved with different projection functions when\nusing different architectures. Indeed, from Figure 4, it is possible to see that similar architectures (i.e., ViT-based models) exhibit similar trends when tested on the same dataset (Figure 13).\nTakeaway. The transformation class that correlates different latent spaces produced by both pretrained and trained-from-scratch models depends on the dataset, architecture, task, and possibly other factors."
        },
        {
            "heading": "4.2 DOWNSTREAM TASK: ZERO-SHOT STITCHING",
            "text": "Experimental setting. In this section, we perform zero-shot stitching classification using text, vision, and graph modalities with various models and datasets. For the Vision and Text domains, we used the same datasets and pretrained models employed in Section 4.1. For the Graph domain, we employed the CORA dataset Sen et al. (2008) and the GCN architecture trained from scratch (refer to Tables 7 and 8 for additional details of models and datasets). Relative decoders are trained with three different seed values, and the resulting representations are transformed into relative representations by projecting the encodings onto 1280 randomly selected but fixed anchors. It is important to highlight, as demonstrated in Table 10 and Figure 9, that varying the number of anchors results in different transformations, indicating that a single projection function cannot encompass the desired invariance. Nevertheless, our method achieve the highest score, regardless of the number of anchors. Refer to the Appendix Table 24 and Figures 3 and 14 for additional experiments on the reconstruction task.\nZero-Shot Model Stitching. The stitching methodology allows combining components of different NNs to obtain a new model. In this paper, we adopted the same setting proposed by Moschella et al. (2022), where each element of the stitched model functions as an autonomous frozen module: the encoder handles data embedding, while the dedicated relative decoder manages the downstream task. Refer to Figure 10 for a visual depiction of the procedure. The approach is termed zero-shot because the stitching procedure is executed without any further training or fine-tuning.\nResults Analysis. Tables 1 and 2 present the performance of various projection functions for different modalities. As previously observed in Section 4.1, the experiments reveal the absence of a single optimal projection function across architectures, modalities, and even within individual datasets. Our proposed framework, which leverages a product space to harness multiple invariances, followed by a trainable aggregation mechanism, consistently achieves superior accuracy across most scenarios. It is important to emphasize that the dimensionality of each independent projection and the aggregated product space remains constant, ensuring fair comparison.Additional stitching results on graphs and text in Appendix Table 15 and 12, moreover, in Tables 16 to 22 we show the performance for each pair of encoder and decoder without averaging over the architectures. The reference end-to-end performance are reported in Tables 25 to 32 to better interpret the performance of the stitched models on the downstream tasks. To this end, we propose an additional evaluation metric named the Stitching Index computed as the ratio between the stitching score and the end-to-end score. It measures how closely the stitching accuracy aligns with the original score, i.e., a stitching score of one indicates there is no drop in performance when stitching modules. Results in Table 3 highlight that our method enables zero-shot stitching without any performance drop in this setting while still ensuring competitive end-to-end performance.\nTakeaway. A product space with invariant components can improve the zero-shot stitching performance without any prior knowledge of the class of transformation that relates different spaces."
        },
        {
            "heading": "4.3 AGGREGATION FUNCTIONS: ABLATION",
            "text": "In this section, we perform an ablation study on the merging strategies presented in Section 3.\nExperimental setup. In these experiments, we perform zero-shot stitching classification on the three modalities using the same datasets and models described in previous sections (refer to Tables 7 and 8 for additional details of models and datasets). The relative decoders are trained using three different seeds, and the accuracy score for the classification task is assessed on each assembled model.\nResult Analysis. Table 4 presents the results of the ablation study conducted using various aggregation methodologies (complete results can be found in Tables 13 to 15). Among the different methodologies, MLP+Sum outperforms the others consistently. This method preprocesses each subspace with an\nindependent MultiLayer Perceptron (MLP), which includes layer normalization, a linear layer, and tanh activation, and then sums the resulting representations. It is essential to highlight that the Concat aggregation method is not directly comparable to the others since it increases the dimensionality of the space linearly with the number of subspaces (refer to Appendix A.3 for more details).\nTakeaway. The aggregation by sum of all the subspaces guarantees consistently the highest performance between merging strategies without increasing the dimensionality of the space."
        },
        {
            "heading": "4.4 SUBSPACE SELECTION",
            "text": "In the preceding sections, we discussed integrating individual and multiple invariances into the representation through various projection functions and appropriate aggregation strategies. In this section, we aim to analyze and understand if tuning only the aggregation strategy at stitching time is a reasonable cost for selecting the optimal subspace. We focus on the Self-attention aggregation, which is a single self-attention layer as described in Section 3, and finetune only the Query, Key, Value (QKV) parameters (i.e., the ones responsible for subspace blending). Each subspace is generated by its own projection function. We remark that stitching-time fine-tuning is exclusive to this experiment.\nExperimental setup. We identify two crucial components within the stitched model: (1) the linear projections associated with QKV in the attention mechanism, which is responsible for selecting and blending subspaces, and (2) the MLP in the classification head following the attention mechanism, which classifies the aggregated embeddings. We examine two distinct approaches: the first approach fine-tunes only the first component (QKV opt), while the second one fine-tunes the second component (MLP opt). All the experiments in this section are conducted on the CIFAR100 dataset, employing the RexNet as encoder and the ViT-B/16 as decoder.\nResult Analysis. Table 5 summarizes downstream classification accuracy for the stitched model using various projection functions and aggregation strategies. Incorporating multiple invariances and aggregating them via Self-attention (fifth row) does not perform well; meanwhile, using the MLP+Sum or the Cosine projection alone is more effective. This is expected, considering the attention mechanism is primarily trained to improve end-to-end performance rather than to maximize compatibility between different spaces. Incorporating the adaptation strategies at stitching time significantly boosts performance, either focusing on the subspace selection and blending (QKV opt) or the classification\nhead (MLP opt). We find that an informed fine-tuning of the parameters responsible for the subspace blending (i.e., only the QKV projections) significantly impacts performances, even more than tuning the whole classifier. Figure 5 illustrates the attention weights averaged over the test dataset, the attention weights of the zero-shot stitched model (left) remain unchanged when fine-tuning only in the classifier (right). Meanwhile, fine-tuning the QKV projections (center) shifts the attention weights to allocate less importance to worse-performing projections (i.e., L\u221e).\nTakeaway. Appropriate subspace selection and aggregation are crucial in enhancing latent communication between neural models."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we introduced a framework to incorporate invariances into neural representations to enhance latent space communication without prior knowledge of the optimal invariance to be enforced. Constructing a product space with invariant components, we showed that it is possible to capture a large class of complex transformations between latent spaces within a single representation, robust to multiple changing factors such as dataset, architecture, and training hyperparameters variations.\nLimitations and Future Works. While our method achieves good results on the tested benchmarks, we observed that selecting the appropriate subspaces can yield equivalent performance using fewer components. Therefore, integrating better subspace selection strategies holds promise for future research. Our framework allows readily incorporating invariances, as similarity functions, into the latent representation. However, this can become limiting when the similarity function cannot be modeled analytically or expressed in closed form. In such cases, an interesting direction would be to learn the similarity functions, paving the way to an even more extensive set of possible invariances to be enforced in the representation."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 DISTANCE-INDUCED INVARIANCES DETAILS",
            "text": ""
        },
        {
            "heading": "A.1.1 DISTANCES DEFINITION",
            "text": "This section provides additional details about the metrics described in section Section 3.\nCosine (Cos.). Given two vectors u, v, the cosine similarity is defined as:\ncos(u, v) = u \u00b7 v\n\u2225u\u2225 \u2225v\u2225 (2)\nEuclidean (Eucl.). Given two vectors u, v, the Euclidean distance is defined as:\nd(u, v) = \u221a\u221a\u221a\u221a n\u2211 i=1 (ui \u2212 vi)2 (3)\nManhattan (L1). Given two vectors u, v, the L1 distance is defined as:\nd(u, v) = n\u2211 i=1 |(ui \u2212 vi)| (4)\nChebyshev (L\u221e). Given two vectors u, v, the L\u221e distance is defined as:\nd(u, v) = max i (|ui \u2212 yi|) = lim p\u2192\u221e\n( n\u2211\ni=1\n|xi \u2212 yi|p ) 1 p , (5)\nand can be approximated in a differentiable way employing high values for p.\nGeodesic distance. Given a manifold M and its parametrization g : Z 7\u2192 X we can represent the Riemannian metric as symmetric, positive definite matrix G(z) defined at each point in Z. G(z) can be obtained as G(z) = Jg(z)TJg(z), where Jg(z) indicates the Jacobian of g evaluated at z. This metric enables us to define an inner product on tangent spaces on M. Considering a smooth curve \u03b3 : [a, b] 7\u2192 Z , this corresponds to a curve on M via g \u25e6 \u03b3(t). Its arc length is defined as:\nL(\u03b3) = \u222b b a \u221a \u03b3\u0307(t)TG\u03b3(t)\u03b3\u0307(t)dt (6)\nA geodesic curve is a curve that locally minimizes the arc length, corresponding to minimizing the following energy functional:\nE(\u03b3) = 1\n2 \u222b b a \u03b3\u0307(t)TG\u03b3(t)\u03b3\u0307(t)dt (7)\nIn Figure 6, we show how geodesic distance is preserved under several classes of transformations, including manifold isometries, i.e., possibly nonlinear transformations that preserve the metric on M. In the synthetic experiment, geodesic distances are computed using the heat method of Crane et al. (2017), and the manifold isometry is calculated using Isomap Tenenbaum et al. (2000). Possible approaches to extend geodesic computation to real cases when dim(Z) > 3 include Shao et al. (2017). We leave this promising direction for future work.\nA.1.2 INFUSED INVARIANCES\nIn Table 6, we summarize the invariances guaranteed by different distance metrics concerning the following standard classes of transformations: Isotropic Scaling (IS), Orthogonal Transformation (OT), Translation (TR), Permutation (PT), Affine Transformation (AT), Linear Transformation (LT), and Manifold Isometry (MIS). Where MIS is an isometric deformation of the manifold that preserves the geodesic distances between points, see Figure 6 for a synthetic example. In general, it is not straightforward to capture the set of invariances induced by a similarity function. For example, the L\u221e distance does not enforce isometry invariance in the representation but, simultaneously, induces an invariance to perturbations in dimensions other than the maximum one. Formalizing and analyzing such types of invariances presents challenges since these transformations cannot be neatly classified into a specific simple class of transformations."
        },
        {
            "heading": "A.2 AGGREGATION FUNCTIONS",
            "text": "In this section, we report the implementation details of the aggregation functions \u03d5 described in Section 3. There are two possible preprocessing strategies applied to each subspace independently:\n\u2022 Normalization layer: an independent LayerNorm for each subspace. \u2022 MLP: a compact, independent, fully connected network defined for each subspace, com-\nprised of LayerNorm, a Linear layer, and a Tanh activation function.\nThese preprocessing modules can be applied before either the (Sum) or (Self-attention) aggregation strategies.\nA.3 IMPLEMENTATION DETAILS\nThis section details the experiments conducted in Section 4. Table 7 contains the full list of pretrained models used, while Table 8 contains dataset information."
        },
        {
            "heading": "A.4 LATENT SPACE ANALYSIS",
            "text": ""
        },
        {
            "heading": "RECONSTRUCTION",
            "text": "This experiment adopts convolution-based AE and VAE. The design variations encompass 2Dbottleneck architectures with a dimensionality of 16\u00d7 7\u00d7 7 and 1D-bottleneck architectures with a dimensionality of 784 = 16 \u00d7 7 \u00d7 7 for fair comparison. The models with 2D bottlenecks are endowed with approximately 50k parameters (AE) and 60k parameters (VAE), while their linearized counterparts possess 1.3 million parameters (LinAE) and 1.9 million parameters (LinVAE). Intriguingly, the variants preserving spatial structure in the latent space demonstrate superior performance\ndespite having significantly fewer parameters. All models undergo training using the Adam optimizer (Kingma & Ba, 2015) with a learning rate set to 1e\u22123. Early stopping is employed based on validation reconstruction error, quantified by mean squared error. For reproducibility, we commit to releasing comprehensive model configurations, specifics, weights, and code upon acceptance. An anonymized version of the codebase is currently accessible at: https://anonymous.4open.science/r/reconstruction0ABD."
        },
        {
            "heading": "A.5 TOOLS & TECHNOLOGIES",
            "text": "We use the following tools in all the experiments presented in this work:\n\u2022 PyTorch Lightning, to ensure reproducible results while also getting a clean and modular codebase;\n\u2022 NN-Template GrokAI (2021), to easily bootstrap the project and enforce best practices;"
        },
        {
            "heading": "A.6 SUBSPACE SELECTION",
            "text": "In Table 9 and Figure 8, we present similar results to the ones reported in Section 4.4, but using a more expressive classifier. This choice establishes a less favorable scenario since we are fine-tuning more parameters in the SelfAttention+MLP opt setting. It illustrates that is still better to fine-tune the QKV rather than the whole classifier."
        },
        {
            "heading": "A.7 ANCHOR SELECTION ANALYSIS",
            "text": "In this section, we present the analysis performed on the anchor choice. In Section 4.2 when referring to random but fixed anchors, we mean that the anchors are randomly sampled uniformly from the training set but with a fixed seed value. Thus, we conducted an analysis on the number of randomly selected anchors for the stitching task using CIFAR100 across three different anchor selection seeds. The results in Table 10 and Figure 9 reveal that varying the number of anchors leads to different transformations, indicating that a single projection function cannot incorporate the desired invariance. In contrast, our method enables the attainment of the highest score regardless of the number of anchors or the seed value employed for the uniform random sampling of anchors."
        },
        {
            "heading": "A.8 STITCHING PROCEDURE",
            "text": "In Figure 10, we illustrate in detail the stitching procedure introduced in Section 4.2."
        },
        {
            "heading": "A.9 ADDITIONAL RESULTS",
            "text": ""
        }
    ],
    "year": 2023
}