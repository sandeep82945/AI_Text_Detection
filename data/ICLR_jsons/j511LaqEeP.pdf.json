{
    "abstractText": "Split conformal prediction has recently sparked great interest due to its ability to provide formally guaranteed uncertainty sets or intervals for predictions made by black-box neural models, ensuring a predefined probability of containing the actual ground truth. While the original formulation assumes data exchangeability, some extensions handle non-exchangeable data, which is often the case in many real-world scenarios. In parallel, some progress has been made in conformal methods that provide statistical guarantees for a broader range of objectives, such as bounding the best F1-score or minimizing the false negative rate in expectation. In this paper, we leverage and extend these two lines of work by proposing nonexchangeable conformal risk control, which allows controlling the expected value of any monotone loss function when the data is not exchangeable. Our framework is flexible, makes very few assumptions, and allows weighting the data based on its relevance for a given test example; a careful choice of weights may result in tighter bounds, making our framework useful in the presence of change points, time series, or other forms of distribution drift. Experiments with both synthetic and real world data show the usefulness of our method.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ant\u00f3nio Farinhas"
        },
        {
            "affiliations": [],
            "name": "Chrysoula Zerva"
        },
        {
            "affiliations": [],
            "name": "Dennis Ulmer"
        },
        {
            "affiliations": [],
            "name": "Andr\u00e9 F. T. Martins"
        }
    ],
    "id": "SP:c3331226054981ab19954826f09d31f9a34ae289",
    "references": [
        {
            "authors": [
                "Anastasios N Angelopoulos",
                "Stephen Bates"
            ],
            "title": "A gentle introduction to conformal prediction and distribution-free uncertainty quantification",
            "venue": "arXiv preprint arXiv:2107.07511,",
            "year": 2021
        },
        {
            "authors": [
                "Anastasios N. Angelopoulos",
                "Stephen Bates",
                "Adam Fisch",
                "Lihua Lei",
                "Tal Schuster"
            ],
            "title": "Conformal risk control, 2023a",
            "year": 2023
        },
        {
            "authors": [
                "Anastasios N. Angelopoulos",
                "Emmanuel J. Candes",
                "Ryan J. Tibshirani"
            ],
            "title": "Conformal pid control for time series prediction, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Rina Foygel Barber",
                "Emmanuel J. Cand\u00e8s",
                "Aaditya Ramdas",
                "Ryan J. Tibshirani"
            ],
            "title": "Conformal prediction beyond exchangeability",
            "venue": "The Annals of Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Stephen Bates",
                "Anastasios Angelopoulos",
                "Lihua Lei",
                "Jitendra Malik",
                "Michael Jordan"
            ],
            "title": "Distribution-free, risk-controlling prediction",
            "venue": "sets. J. ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Jean Bretagnolle",
                "Catherine Huber"
            ],
            "title": "Estimation des densit\u00e9s: risque minimax",
            "venue": "Zeitschrift fu\u0308r Wahrscheinlichkeitstheorie und verwandte Gebiete,",
            "year": 1979
        },
        {
            "authors": [
                "Maxime Cauchois",
                "Suyash Gupta",
                "Alnur Ali",
                "John C Duchi"
            ],
            "title": "Robust validation: Confident predictions even when distributions shift",
            "venue": "arXiv preprint arXiv:2008.04267,",
            "year": 2020
        },
        {
            "authors": [
                "Victor Chernozhukov",
                "Kaspar W\u00fcthrich",
                "Zhu Yinchu"
            ],
            "title": "Exact and robust conformal inference methods for predictive machine learning with dependent data",
            "venue": "Proceedings of the 31st Conference On Learning Theory,",
            "year": 2018
        },
        {
            "authors": [
                "Victor Chernozhukov",
                "Kaspar W\u00fcthrich",
                "Yinchu Zhu"
            ],
            "title": "Distributional conformal prediction",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2021
        },
        {
            "authors": [
                "Victor Chernozhukov",
                "Kaspar W\u00fcthrich",
                "Yinchu Zhu"
            ],
            "title": "An exact and robust conformal inference method for counterfactual and synthetic controls",
            "venue": "Journal of the American Statistical Association,",
            "year": 1920
        },
        {
            "authors": [
                "Imre Csisz\u00e1r",
                "J\u00e1nos K\u00f6rner"
            ],
            "title": "Information theory: coding theorems for discrete memoryless systems",
            "year": 2011
        },
        {
            "authors": [
                "Clara Fannjiang",
                "Stephen Bates",
                "Anastasios N. Angelopoulos",
                "Jennifer Listgarten",
                "Michael I. Jordan"
            ],
            "title": "Conformal prediction under feedback covariate shift for biomolecular design",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2022
        },
        {
            "authors": [
                "Shai Feldman",
                "Liran Ringel",
                "Stephen Bates",
                "Yaniv Romano"
            ],
            "title": "Achieving risk control in online learning settings, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Gammerman",
                "Volodya Vovk",
                "Vladimir Vapnik"
            ],
            "title": "Learning by transduction",
            "venue": "In Gregory F. Cooper and Seraf\u0131\u0301n Moral (eds.), UAI \u201998: Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence,",
            "year": 1998
        },
        {
            "authors": [
                "Isaac Gibbs",
                "Emmanuel Candes"
            ],
            "title": "Adaptive conformal inference under distribution shift",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Isaac Gibbs",
                "Emmanuel Cand\u00e8s"
            ],
            "title": "Conformal inference for online prediction with arbitrary distribution shifts, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Leying Guan"
            ],
            "title": "Localized conformal prediction: a generalized inference framework for conformal prediction. Biometrika, 110(1):33\u201350",
            "venue": "Jul 2022",
            "year": 2022
        },
        {
            "authors": [
                "Michael Harries"
            ],
            "title": "Splice-2 comparative evaluation: Electricity pricing",
            "venue": "Technical report,",
            "year": 1999
        },
        {
            "authors": [
                "Ali Jalali",
                "Hannah Lonsdale",
                "Nhue Do",
                "Jacquelin Peck",
                "Monesha Gupta",
                "Shelby Kutty",
                "Sharon R. Ghazarian",
                "Jeffrey P. Jacobs",
                "Mohamed Rehman",
                "Luis M. Ahumada"
            ],
            "title": "Deep learning for improved risk prediction in surgical outcomes",
            "venue": "Scientific Reports,",
            "year": 2020
        },
        {
            "authors": [
                "Edwin T Jaynes"
            ],
            "title": "Information theory and statistical mechanics",
            "venue": "Physical review,",
            "year": 1957
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih"
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6769\u20136781,",
            "year": 2020
        },
        {
            "authors": [
                "Tom Kwiatkowski",
                "Jennimaria Palomaki",
                "Olivia Redfield",
                "Michael Collins",
                "Ankur Parikh",
                "Chris Alberti",
                "Danielle Epstein",
                "Illia Polosukhin",
                "Jacob Devlin",
                "Kenton Lee",
                "Kristina Toutanova",
                "Llion Jones",
                "Matthew Kelcey",
                "Ming-Wei Chang",
                "Andrew M. Dai",
                "Jakob Uszkoreit",
                "Quoc Le",
                "Slav Petrov"
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Zhen Lin",
                "Shubhendu Trivedi",
                "Jimeng Sun"
            ],
            "title": "Conformal prediction intervals with temporal dependence",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Alfred M\u00fcller"
            ],
            "title": "Integral probability metrics and their generating classes of functions",
            "venue": "Advances in Applied Probability,",
            "year": 1997
        },
        {
            "authors": [
                "Travis E Oliphant"
            ],
            "title": "A guide to NumPy, volume 1",
            "venue": "Trelgol Publishing USA,",
            "year": 2006
        },
        {
            "authors": [
                "Roberto I. Oliveira",
                "Paulo Orenstein",
                "Thiago Ramos",
                "Jo\u00e3o Vitor Romano"
            ],
            "title": "Split conformal prediction for dependent data, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Harris Papadopoulos",
                "Kostas Proedrou",
                "Volodya Vovk",
                "Alex Gammerman"
            ],
            "title": "Inductive confidence machines for regression",
            "venue": "Machine Learning: ECML",
            "year": 2002
        },
        {
            "authors": [
                "F. Pedregosa",
                "G. Varoquaux",
                "A. Gramfort",
                "V. Michel",
                "B. Thirion",
                "O. Grisel",
                "M. Blondel",
                "P. Prettenhofer",
                "R. Weiss",
                "V. Dubourg",
                "J. Vanderplas",
                "A. Passos",
                "D. Cournapeau",
                "M. Brucher",
                "M. Perrot",
                "E. Duchesnay"
            ],
            "title": "Scikit-learn: Machine learning in Python",
            "venue": "Journal of Machine Learning Research,",
            "year": 2011
        },
        {
            "authors": [
                "Aleksandr Podkopaev",
                "Aaditya Ramdas"
            ],
            "title": "Distribution-free uncertainty quantification for classification under label shift",
            "venue": "Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych"
            ],
            "title": "Sentence-BERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
            "year": 2019
        },
        {
            "authors": [
                "Craig Saunders",
                "Alexander Gammerman",
                "Volodya Vovk"
            ],
            "title": "Transduction with confidence and credibility",
            "venue": "In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence,",
            "year": 1999
        },
        {
            "authors": [
                "Filip Schlembach",
                "Evgueni Smirnov",
                "Irena Koprinska"
            ],
            "title": "Conformal multistep-ahead multivariate time-series forecasting",
            "venue": "Proceedings of the Eleventh Symposium on Conformal and Probabilistic Prediction with Applications,",
            "year": 2022
        },
        {
            "authors": [
                "Bernhard Sch\u00f6lkopf",
                "Peter Bartlett",
                "Alex Smola",
                "Robert C Williamson"
            ],
            "title": "Shrinking the tube: a new support vector regression algorithm",
            "venue": "Advances in neural information processing systems,",
            "year": 1998
        },
        {
            "authors": [
                "Tal Schuster",
                "Adam Fisch",
                "Jai Gupta",
                "Mostafa Dehghani",
                "Dara Bahri",
                "Vinh Q. Tran",
                "Yi Tay",
                "Donald Metzler"
            ],
            "title": "Confident adaptive language modeling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Kamile Stankeviciute",
                "Ahmed M Alaa",
                "Mihaela van der Schaar"
            ],
            "title": "Conformal time-series forecasting",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Sophia Sun",
                "Rose Yu"
            ],
            "title": "Copula conformal prediction for multi-step time series forecasting, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Jacopo Teneggi",
                "Matthew Tivnan",
                "Web Stayman",
                "Jeremias Sulam"
            ],
            "title": "How to trust your diffusion model: A convex optimization approach to conformal risk control",
            "venue": "Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Guido Van Rossum",
                "Fred L. Drake"
            ],
            "title": "Python 3 Reference Manual",
            "year": 2009
        },
        {
            "authors": [
                "Vladimir Vapnik"
            ],
            "title": "The nature of statistical learning theory",
            "venue": "Springer science & business media,",
            "year": 1999
        },
        {
            "authors": [
                "Charles R Harris",
                "Anne M. Archibald",
                "Ant\u00f4nio H. Ribeiro",
                "Fabian Pedregosa",
                "Paul van"
            ],
            "title": "Mulbregt, and SciPy 1. 0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python",
            "venue": "Nature Methods,",
            "year": 2020
        },
        {
            "authors": [
                "Vladimir Vovk"
            ],
            "title": "Cross-conformal predictors",
            "venue": "Annals of Mathematics and Artificial Intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Vladimir Vovk",
                "Alex Gammerman",
                "Glenn Shafer"
            ],
            "title": "Algorithmic Learning in a Random World",
            "year": 2005
        },
        {
            "authors": [
                "Volodya Vovk",
                "Alexander Gammerman",
                "Craig Saunders"
            ],
            "title": "Machine-learning applications of algorithmic randomness",
            "venue": "In Proceedings of the Sixteenth International Conference on Machine Learning,",
            "year": 1999
        },
        {
            "authors": [
                "St\u00e9fan van der Walt",
                "S Chris Colbert",
                "Gael Varoquaux"
            ],
            "title": "The NumPy array: a structure for efficient numerical computation",
            "venue": "Computing in Science & Engineering,",
            "year": 2011
        },
        {
            "authors": [
                "Jun Wang",
                "Jiaming Tong",
                "Kaiyuan Tan",
                "Yevgeniy Vorobeychik",
                "Yiannis Kantaros"
            ],
            "title": "Conformal temporal logic planning using large language models: Knowing when to do what and when to ask for help, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Chen Xu",
                "Yao Xie"
            ],
            "title": "Conformal prediction interval for dynamic time-series",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Heng Yang",
                "Marco Pavone"
            ],
            "title": "Object pose estimation with statistical guarantees: Conformal keypoint detection and geometric uncertainty propagation",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Margaux Zaffran",
                "Olivier Feron",
                "Yannig Goude",
                "Julie Josse",
                "Aymeric Dieuleveut"
            ],
            "title": "Adaptive conformal predictions for time series",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Angelopoulos"
            ],
            "title": "|EP [f ]\u2212 EQ[f ]| follows. B PROOF OF THEOREM 1 The proof adapts elements of the proofs from Barber et al",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Split conformal prediction has recently sparked great interest due to its ability to provide formally guaranteed uncertainty sets or intervals for predictions made by black-box neural models, ensuring a predefined probability of containing the actual ground truth. While the original formulation assumes data exchangeability, some extensions handle non-exchangeable data, which is often the case in many real-world scenarios. In parallel, some progress has been made in conformal methods that provide statistical guarantees for a broader range of objectives, such as bounding the best F1-score or minimizing the false negative rate in expectation. In this paper, we leverage and extend these two lines of work by proposing nonexchangeable conformal risk control, which allows controlling the expected value of any monotone loss function when the data is not exchangeable. Our framework is flexible, makes very few assumptions, and allows weighting the data based on its relevance for a given test example; a careful choice of weights may result in tighter bounds, making our framework useful in the presence of change points, time series, or other forms of distribution drift. Experiments with both synthetic and real world data show the usefulness of our method."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "As the use of machine learning systems for automated decision-making becomes more widespread, the demand for these systems to produce reliable and trustworthy predictions has grown significantly. In this context, conformal prediction (Papadopoulos et al., 2002; Vovk et al., 2005) has recently resurfaced as an attractive framework. Instead of providing a single output, this framework creates prediction sets or intervals that inherently account for uncertainty. These sets come with a statistical guarantee known as coverage, which ensures that they contain the ground truth in expectation, thereby providing a formal promise of reliability.\nThe standard formulation of conformal prediction has, however, important limitations. First, it assumes that all data is exchangeable, a condition which is often violated in practice (e.g., when there is correlation over time or space). Second, while the predicted sets/intervals provide guarantees on coverage, they do not bound arbitrary losses, some of which may be more relevant for the situation at hand (e.g., the F1-score or the false negative rate in multilabel classification problems). Several works have been proposed to improve over these two shortcomings, namely through nonexchangeable conformal prediction (Tibshirani et al., 2019; Gibbs & Candes, 2021; Barber et al., 2023) and conformal risk control (Bates et al., 2021; Angelopoulos et al., 2023a, CRC). In this paper, we extend these lines of research and propose non-exchangeable conformal risk control (non-X CRC). Our main contributions are:\n\u2022 We propose a new method for conformal risk control that provides formal guarantees when the data is not exchangeable, while also achieving the same guarantees as existing methods if the data is in fact exchangeable (see Table 1 where we position our work in the literature);\n\u2022 Theorem 1 establishes a new bound on the expected loss (assumed to be monotonic and bounded), allowing weighting the calibration data based on its relevance for a given test example;\n\u2022 We demonstrate the usefulness of our framework on three tasks: multilabel classification on synthetic data by minimizing the false negative rate; monitoring electricity usage by minimizing the \u03bb-insensitive absolute loss; and open-domain question answering by bounding the best F1-score.1\nThroughout the paper, we use the following definition of exchangeable data distribution, which is a weaker assumption than independent and identically distributed (i.i.d.) data.\nDefinition 1 (Exchangeable data distribution). Let X and Y designate input and output spaces. A data distribution in X \u00d7 Y is said to be exchangeable if and only if we have P((X\u03c0(1), Y\u03c0(1)), . . . , (X\u03c0(n), Y\u03c0(n))) = P((X1, Y1), . . . , (Xn, Yn)) for any finite sample {(Xi, Yi)}ni=1 \u2286 X \u00d7 Y and any permutation function \u03c0. Note that if the data distribution is i.i.d., then it is also exchangeable, since P((X1, Y1), . . . , (Xn, Yn)) = \u220fn i=1 P((Xi, Yi))."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "We start by providing background on conformal prediction (Papadopoulos et al., 2002; Vovk et al., 2005) in \u00a72.1. We then discuss recent extensions of the framework\u2014\u00a72.2 discusses the case where the data is non-exchangeable (Barber et al., 2023), which is often the case when models are deployed in practice. Another extension pivots from guaranteeing coverage to instead constraining the expected value of any monotone loss function (Angelopoulos et al., 2023a), useful for tasks in which the natural notion of error is not miscoverage (\u00a72.3)."
        },
        {
            "heading": "2.1 CONFORMAL PREDICTION",
            "text": "Although other methods exist, this paper focuses on split conformal prediction (Papadopoulos et al., 2002; hereinafter referred to simply as conformal prediction). We start with a pretrained model and measure its performance on a calibration set {(Xi, Yi)}ni=1 of paired examples. Under the assumption of exchangeable data {(Xi, Yi)}n+1i=1 , conformal prediction constructs prediction sets with the following coverage guarantee:\nP ( Yn+1 \u2208 C(Xn+1) ) \u2265 1\u2212 \u03b1, (1)\nwhere (Xn+1, Yn+1) is a new data point and \u03b1 a predefined confidence level. This is accomplished through the following steps: Let s(x, y) \u2208 R be a non-conformity score function, where larger scores indicate worse agreement between x and y. We compute the value q\u0302 as the 1/n\u2308(n + 1)(1 \u2212 \u03b1)\u2309 quantile of the calibration scores and construct a prediction set as follows:\nC ( Xn+1 ) = { y : s(Xn+1, y) \u2264 q\u0302 } . (2)\nThis prediction set satisfies the coverage guarantee in Eq. (1), see e.g., Angelopoulos & Bates, 2021, App. D for a proof. While this guarantee helps to ensure a certain reliability of the calibrated model, the assumption of exchangeable data is often not true when models are deployed in practice, e.g., due to distribution drift in time series or correlations between different data points.\n1Our code is available at https://github.com/deep-spin/non-exchangeable-crc."
        },
        {
            "heading": "2.2 NON-EXCHANGEABLE CONFORMAL PREDICTION",
            "text": "Let us now consider prespecified weights {wi}ni=1 \u2208 [0, 1]n and define w\u0303i := wi/(1 + \u2211N\ni=1 wi). We take a look at a generalization of conformal prediction put together by Barber et al. (2023), which provides the following coverage guarantee, also valid when exchangeability is violated:\nP ( Yn+1 \u2208 C(Xn+1) ) \u2265 1\u2212 \u03b1\u2212 n\u2211 i=1 w\u0303idTV(Z,Z i), (3)\nwhere Z := (X1, Y1), . . . , (Xn, Yn), (Xn+1, Yn+1) is a sequence of n calibration examples followed by a test example, Zi denotes Z after swapping (Xi, Yi) with (Xn+1, Yn+1), and dTV(Z,Zi) is the total variation (TV) distance between Z and Zi. This is accomplished by using\nq\u0302 = inf { q : N\u2211 i=1 w\u0303i1 { si \u2264 q } \u2265 1\u2212 \u03b1 } (4)\nto construct prediction sets the same way as in Eq. (2). See Barber et al. (2023, \u00a74) for a proof. It is worth noting that this method recovers standard conformal prediction when {wi}ni=1 = 1. Besides, if the data is exchangeable, then the distribution of Z is equal to the distribution of Zi, and thus using a weighted procedure does not hurt coverage according to Eq. (3), since dTV(Z,Zi) = 0 for all i. Intuitively, the \u201ccloser\u201d to exchangeable the data is, the smaller the last term will be in Eq. (3). By choosing wisely the weights wi\u2014e.g., by setting large weights to calibration points (xi, yi) such that Z and Zi are similarly distributed and smaller weights otherwise\u2014tighter bounds can be obtained. For example, in time series data we may want to place larger weights on more recent observations."
        },
        {
            "heading": "2.3 CONFORMAL RISK CONTROL",
            "text": "Let us now consider an additional parameter \u03bb and construct prediction sets of the form C\u03bb(\u00b7), where larger \u03bb yield larger prediction sets, i.e., \u03bb \u2264 \u03bb\u2032 =\u21d2 C\u03bb(.) \u2286 C\u03bb\u2032(.) (see Angelopoulos & Bates (2021, \u00a74.3) for an example). Let \u2113 be an arbitrary (bounded) loss function that shrinks as C(Xn+1) grows (i.e., that is monotonically nonincreasing with respect to \u03bb). We switch from conformal methods that provide prediction sets that bound the miscoverage P ( Yn+1 /\u2208 C(Xn+1) ) \u2264 \u03b1 to conformal risk control (Angelopoulos et al., 2023a), which provides guarantees of the form\nE [ \u2113(C(Xn+1), Yn+1)\ufe38 \ufe37\ufe37 \ufe38\nLn+1(\u03bb\u0302)\n] \u2264 \u03b1. (5)\nThis is accomplished as follows. Let Li(\u03bb) = \u2113(C\u03bb(Xi), Yi), i = 1, . . . , n + 1, with Li : \u039b \u2192 (\u2212\u221e, B] and \u03bbmax := sup\u039b, be an exchangeable collection of nonincreasing functions of \u03bb. Choosing an optimal \u03bb\u0302 as\n\u03bb\u0302 = inf { \u03bb :\nn\nn+ 1 R\u0302n(\u03bb) +\nB\nn+ 1 \u2264 \u03b1\n} , R\u0302n(\u03bb) = 1\nn n\u2211 i=1 Li(\u03bb), (6)\nyields the guarantee in Eq. (5), see Angelopoulos et al. (2023a, \u00a72) for a proof. When \u2113(C(Xn+1), Yn+1) = 1 { Yn+1 /\u2208 C(Xn+1) } is the miscoverage loss, we recover standard conformal prediction (\u00a72.1). Note that, as required, this loss is nonincreasing. Other nonincreasing losses include the false negative rate, \u03bb-insensitive absolute error, and the best token-level F1-loss, all of which used in our experiments in \u00a74. A limitation of the construction presented in this section is that it relies on the assumption of data exchangeability, which might be violated in practical settings. Our work circumvents this requirement, as we show next."
        },
        {
            "heading": "3 NON-EXCHANGEABLE CONFORMAL RISK CONTROL",
            "text": "Up to this point, we have described how to construct prediction sets/intervals with coverage guarantees for non-exchangeable data, in \u00a72.2, and how to control the expected value of arbitrary monotone loss functions, when the data is exchangeable, in \u00a72.3. Using the same notation as before, we now\npresent our method, non-exchangeable conformal risk control, which puts together these parallel lines of research, providing guarantees of the form:\nE[L(\u03bb\u0302; (Xn+1, Yn+1))] \u2264 \u03b1+ (B \u2212A) n\u2211\ni=1\nw\u0303idTV(Z,Z i), (7)\nwhere we additionally assume A < B < \u221e to be a lower bound on Li : \u039b \u2192 [A,B]. Let us define Nw := \u2211N i=1 wi. Eq. (7) is obtained by choosing an optimal \u03bb\u0302 as\n\u03bb\u0302 = inf { \u03bb :\nNw Nw + 1 R\u0302n(\u03bb) + B Nw + 1 \u2264 \u03b1\n} , R\u0302n(\u03bb) = 1\nNw n\u2211 i=1 wiL(\u03bb; (xi, yi)). (8)\nWe can see how Eq. (7) simultaneously mirrors both Eq. (3) and Eq. (5): for an optimal choice of \u03bb, the expected risk for a new test point is bounded by \u03b1 plus an extra loosening term that depends on the normalized weights {wi}ni=1 and on the total variation distance between Z and Zi. When the data is in fact exchangeable, we have again dTV(Z,Zi) = 0 for all i, and we recover Eq. (5), i.e., our method achieves the same coverage guarantees as standard conformal risk control. Although our theoretical bound in Eq. (7) holds for any choice of weights, this result is only useful when the loosening term is small, i.e., if we choose small weights wi for data points Zi with large total variation distance dTV(Z,Zi). While the true value of this term is typically unknown, in some situations, such as distribution drift in time series, we expect it to decrease with i, motivating the choice of weights that increase with i. The same principle can be applied in other domains (e.g., for spatial data, one may place higher weights to points close in space to the test point). We come back to this point in \u00a73.2.\nThe result in Eq. (7) is valid when the weights are fixed, i.e., data-independent. However, our result still applies in the case of data-dependent weights wi = w(Xi, Xn+1) if we replace\u2211n\ni=1 w\u0303idTV(Z,Z i) by E [\u2211n i=1 w\u0303idTV(Z,Z i|w1, . . . , wn) ]\n(see Barber et al. (2023, \u00a74.5) for more information). We experiment with this approach in \u00a74.3, where wi is a function of the embedding similarity between Xi and Xn+1, showing that the new bound is still useful in practice."
        },
        {
            "heading": "3.1 FORMAL GUARANTEES",
            "text": "Now that we have presented an overview of our method, we proceed to providing a formal proof for the guarantee in Eq. (7). We begin with a lemma, proved in App. A, that establishes a TV bound that extends the one introduced by Barber et al. (2023):\nLemma 1. Let f : S \u2192 [A,B] \u2282 R be a bounded function on a measurable space (S,A) (where A \u2286 2S is a \u03c3-algebra) and let P and Q be two probability measures on (S,A). Then\n|EP [f ]\u2212 EQ[f ]| \u2264 (B \u2212A)dTV(P,Q). (9)\nNote that when f(t) = 1 { t \u2208 V } for some event V \u2208 A, the left-hand side becomes |P (V )\u2212Q(V )| and we recover the bound used in the proof of Barber et al. (2023, \u00a76.2).\nWe now state the main result. The proof technique is similar to that of Barber et al. (2023), but instead of modeling the event of a variable belonging to a \u201cstrange set\u201d, we model expectations of loss functions that depend on a calibration variable. See App. B for the full proof.\nTheorem 1 (Non-exchangeable conformal risk control). Assume that for all (x, y) \u2208 X \u00d7 Y the loss L(\u03bb; (x, y)) is nonincreasing in \u03bb and bounded as A \u2264 L(\u03bb; (x, y)) \u2264 B for any \u03bb. Let\nZ := (X1, Y1), . . . , (Xn, Yn), (Xn+1, Yn+1)\nbe a sequence of n calibration examples followed by a test example, and let w1, . . . , wn \u2208 [0, 1]n be data-independent weights. Define Nw = \u2211n i=1 wi, w\u0303i = wi/(Nw + 1) for i \u2208 [n] and w\u0303n+1 = 1/(Nw + 1). Let \u03b1 \u2208 [A,B] be the maximum tolerable risk, and define\n\u03bb\u0302 = inf { \u03bb :\nNw Nw + 1 R\u0302n(\u03bb) + B Nw + 1 \u2264 \u03b1\n} , (10)\nwhere R\u0302n(\u03bb) is the weighted empirical risk in the calibration set:\nR\u0302n(\u03bb) = 1\nNw n\u2211 i=1 wiL(\u03bb; (xi, yi)). (11)\nThen, we have\nE[L(\u03bb\u0302; (Xn+1, Yn+1))] \u2264 \u03b1+ (B \u2212A) n\u2211\ni=1\nw\u0303idTV(Z,Z i), (12)\nwhere Zi is obtained from Z by swapping (Xi, Yi) and (Xn+1, Yn+1).\nThe next section illustrates how we can make practical use of this result to minimize loss functions beyond the miscoverage loss in the presence of non-exchangeable data distributions."
        },
        {
            "heading": "3.2 HOW TO CHOOSE WEIGHTS",
            "text": "To make practical use of Theorem 1, we need a procedure to choose the weights wi. We next suggest a strategy based on regularized minimization of the coverage gap g(w\u03031, ..., w\u0303n) := (B \u2212 A) \u2211n i=1 w\u0303idTV(Z,Z\ni) via the maximum entropy principle (Jaynes, 1957). Note first that simply minimizing this gap would lead to w\u0303i = 0 for all i \u2208 [n] and w\u0303n+1 = 1, which ignores all the calibration data and leads to an infeasible \u03bb\u0302 in Eq. (6). In general, if all weights wi are too small, this leads to a very large wn+1 and an unreasonably large \u03bb\u0302. On the other extreme, having all weights too large (e.g. wi = 1 for all i, which leads to w\u0303i = 1/(n + 1) for i \u2208 [n + 1]) ignores the non-exchangeability of the data and may lead to a large coverage gap. Therefore, it is necessary to find a good balance between ensuring a small coverage gap but at the same time ensuring that the distribution w\u03031, ..., w\u0303n+1 is not too peaked, i.e., that it has sufficiently high entropy. Since by definition, we must have w\u0303n+1 \u2265 w\u0303i for all i \u2208 [n], this can be formalized as the following regularized minimization problem:\nmin w\u03031,...,w\u0303n+1\n(B \u2212A) n\u2211\ni=1\nw\u0303idTV(Z,Z i)\u2212 \u03b2H(w\u03031, ..., w\u0303n+1)\nsubject to n+1\u2211 i=1 w\u0303i = 1 and 0 \u2264 w\u0303i \u2264 w\u0303n+1 for all i \u2208 [n], (13)\nwhere H(w\u03031, ..., w\u0303n+1) = \u2212 \u2211n+1\ni=1 w\u0303i log w\u0303i is the entropy function and \u03b2 > 0 is a temperature parameter. The solution of this problem is w\u0303i \u221d exp(\u2212\u03b2(B \u2212A)dTV(Z,Zi)) for i \u2208 [n+ 1]. Although in general dTV(Z,Zi) is not known, it is possible in some scenarios to bound or to estimate this quantity: for example, when variables are independent but not identically distributed, it can be shown that dTV(Z,Zi) \u2264 2dTV(Zi, Zn+1) (Barber et al., 2023, Lemma 1); and it is possible to upper bound the total variation distance as a function of the (more tractable and amenable to estimation) Kullback-Leibler divergence, e.g., via Pinsker\u2019s or Bretagnolle-Huber\u2019s inequalities (Bretagnolle & Huber, 1979; Csisza\u0301r & Ko\u0308rner, 2011), which may provide good heuristics. For example, in a time series under a distribution shift scenario bounded with a Lipschitztype condition dTV(Zi, Zn+1) \u2264 \u03f5(n + 1 \u2212 i) for some \u03f5 > 0 (see e.g. (Barber et al., 2023, \u00a74.4)), we could replace dTV(Z,Zi) in Eq. (13) by this upper bound to obtain the maxent solution w\u0303i \u221d exp(\u2212\u03b2\u03f5(n + 1 \u2212 i)) = \u03c1n+1\u2212i, where \u03c1 = exp(\u2212\u03b2\u03f5) \u2208 (0, 1). This exponential decay of the weights was suggested by (Barber et al., 2023); our maximum entropy heuristic provides further justification for that choice. We use this strategy in some of our experiments in \u00a74."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we turn to demonstrating the validity of our theoretical results in three different tasks using different nonincreasing losses: a multilabel classification problem using synthetic time series data, minimizing the false negative rate (\u00a74.1), a problem involving monitoring electricity usage, minimizing the \u03bb-insensitive absolute loss (\u00a74.2), and an open-domain question answering (QA)\ntask, where we control the best token-level F1-score (\u00a74.3). Throughout, we report our method alongside a conformal risk control (CRC) baseline that predicts \u03bb\u0302 following Eq. (6)."
        },
        {
            "heading": "4.1 MULTILABEL CLASSIFICATION IN A TIME SERIES",
            "text": "We start by validating our approach on synthetic data, before moving to real-world data in the following subsections. To this end, we modified the synthetic regression experiment of Barber et al. (2023, \u00a75.1) to turn it into a multilabel classification problem with up to M = 10 different labels. We consider three different setups:\n1. Exchangeable (i.i.d.) data: We sample N = 2000 i.i.d. data points (Xi, Yi) \u2208 RM \u00d7 RM . We sample Xi from a Gaussian distribution, Xi\niid\u223cN (0, IM ), and we set Yi \u223c sign(WXi + b + .1N (0, IM )). The coefficient matrix W is set to the identity matrix IM and the biases to b = \u22120.5, to encourage a sparse set of labels.\n2. Changepoints: We follow setting (1) and sample N = 2000 i.i.d. data points (Xi, Yi), setting Xi\niid\u223cN (0, IM ) and Yi \u223c sign(W (k)Xi+b+.1N (0, IM )), again with b = \u22120.5. We start with the same coefficients W (0) = IM and for every changepoint k > 0 we rotate the coefficients such that W (k)i,j = W (k\u22121) i\u22121,j for i > 1 and W (k) 1,j = W (k\u22121) M,j . Following Barber et al. (2023), we use two changepoints (k = 2) at timesteps 500 and 1500.\n3. Distribution drift: We follow setting (2) and sample N = 2000 i.i.d. data points (Xi, Yi), with Xi\niid\u223cN (0, IM ) and Yi \u223c sign(W (k)Xi+b+ .1N (0, IM )), with b as above. Again, we start with W (0) = IM but now we set W (N) to the last matrix of setting (2). We then compute each intermediate W (k) by linearly interpolating between W (0) and W (N).\nAfter a warmup period of 200 time points, at each time step n = 200, . . . , N \u2212 1 we assign odd indices to the training set, even indices to the calibration set, and we let Xn+1 be the test point. We fit M independent logistic regression models to the training data to obtain predictors for each label; we let fm(Xi) denote the estimated probability of the mth label according to the model. Based on this predictor, we define prediction sets C\u03bb(Xi) := {m \u2208 [M ] : fm(Xi) \u2265 1 \u2212 \u03bb}. We compare standard CRC with non-exchangeable (non-X) CRC, for which we use weights wi = 0.99n+1\u2212i and predict \u03bb\u0302 following Eq. (10). In both cases, we minimize the false negative rate (FNR):2\nL(\u03bb; (Xi, Yi)) = 1\u2212 |Yi \u2229 C\u03bb(Xi)|\n|Yi| . (14)\nNote that this loss is nonincreasing in \u03bb, as required. App. C contains additional experiments considering \u03bb to be the number of active labels and using C\u03bb(Xi) = top-\u03bb(f(Xi)). Fig. 1 shows results averaged across 10 independent trials for \u03b1 = 0.2, summarized in Table 2. We see that the performance of both methods is comparable when the data is i.i.d, with non-X CRC being slightly more conservative. However, when the data is not exchangeable due to the presence of changepoints or distribution drift, our proposed method is considerably better. In particular, after the changepoints in setting (2), non-X CRC is able to achieve the desired risk level more rapidly; in setting (3), the performance of standard CRC gradually drops over time\u2014a problem that can be mitigated by accounting for non-exchangeability introduced by the distribution drift. Importantly, while the average risk is above the predefined threshold for standard CRC for settings (2) and (3) (0.246 and 0.225, respectively), our method achieves the desired risk level on average (0.196 and 0.182, respectively)."
        },
        {
            "heading": "4.2 MONITORING ELECTRICITY USAGE",
            "text": "We use the ELEC2 dataset (Harries, 1999), which tracks electricity transfer between two states in Australia, considering the subset of the data used by Barber et al. (2023), which contains 3444 time points. The data points correspond to the 09:00am - 12:00pm timeframe and we use the price (nswprice, vicprice) and demand (nswdemand, vicdemand) variables as input features,\n2With some abuse of notation, we use Yi \u2286 {1, . . . ,M} to denote the set of gold labels with value +1.\nxi to predict the target transfer values yi. We also consider a randomly permuted version of the dataset such that the exchangeability assumption is satisfied. We use the same definitions and settings of \u00a74.1, but this time we fit a least squares regression model to predict the transfer values, y\u0302i = f(xi), at each time step. For non-X CRC, we use weights wi = 0.99n+1\u2212i and we also experiment with weighted least-squares regression, placing weights ti = wi on each data point (non-X CRC + WLS). For both standard and non-X CRC we control the residual (distance) with respect to the confidence interval C\u03bb(xi) = [f(xi)\u2212 \u03bb, f(xi) + \u03bb], where f(xi) corresponds to the predicted values for transfer. We use the \u03bb-insensitive absolute loss, a loss function commonly used in support vector regression (Scho\u0308lkopf et al., 1998; Vapnik, 1999):\nL(\u03bb; (xi, yi)) = { 0, if |f(xi)\u2212 yi| \u2264 \u03bb, |f(xi)\u2212 yi| \u2212 \u03bb, otherwise.\n(15)\nWe experiment using \u03bb \u2208 [0, 1] with a step of 0.01. Since we are using the normalized ELEC2 dataset, transfer takes values in [0, 1], thus L(\u03bb; (f(xi), yi)) is bounded by B = 1. By definition L(\u03bb; (f(xi), yi)) is nonincreasing with respect to \u03bb.\nFig. 2 shows results for the aforementioned setup. We can observe that in the original setting, both non-exchangeable methods approximate well the desired loss threshold even during the timesteps at which the data suffers from distribution drift. Specifically, as observed by Barber et al. (2023), the electricity transfer values are more noisy during the middle of the time range and we can see that the standard CRC + LS method underestimates the \u03bb\u0302 for these data points resulting in increased loss, above the desired one. With respect to the CRC + WLS setup, we can see that it manages to reach the desired loss with a smaller interval width on average, indicating that fitting the weighted leastsquares model performs better when the data distribution changes, allowing for smaller \u03bb during calibration. For the permuted data that simulates the exchangeable data scenario, we can see that all methods perform similarly, reaching the desired loss, as expected."
        },
        {
            "heading": "4.3 OPEN-DOMAIN QUESTION ANSWERING",
            "text": "We now shift to open-domain QA, a task that consists in answering factoid questions using a large collection of documents. This is done in two stages, following Angelopoulos et al. (2023a): (i) a retriever model (Karpukhin et al., 2020, DPR) selects passages from Wikipedia that might contain the answer to the question, and (ii) a reader model examines the retrieved contexts and extract text sub-spans that serve as candidate answers.3\nGiven a vocabulary V , each Xi \u2208 Z is a question and Yi \u2208 Zk a set of k correct answers, where Z := Vm (we assume that Xi and Yi are sequences composed of up to m tokens). We calibrate the best token-based F1-score of the prediction set 4, taken over all pairs of predictions and answers,\nL(\u03bb; (Xi, Yi)) = 1\u2212max{F1(a, c) : c \u2208 C\u03bb(Xi), a \u2208 Yi}, C\u03bb = {y : f(Xi, y) \u2265 \u03bb}, (16)\nwhich is nonincreasing and upper-bounded by B = 1. We consider a CRC baseline that predicts \u03bb\u0302 following Eq. (6). For non-X CRC, we choose weights {wi}ni=1 by computing the dot product between the embedding representations of {Xi}ni=1 and Xn+1, obtained using a sentence-transformer model (Reimers & Gurevych, 2019) designed for semantic search,5 and predict \u03bb\u0302 following Eq. (10). While in standard CRC \u03bb\u0302 is the same for each test example, this is not the case for non-X CRC.\n3Enumerating all possible answers is intractable, and thus we retrieve the top several hundred candidate answers, extracted from the top 100 passages (which is sufficient to control all risks).\n4This is the same loss used by Angelopoulos et al. (2023a). 5We use the multi-qa-mpnet-base-dot-v1 model available at https://huggingface.co/\nsentence-transformers/multi-qa-mpnet-base-dot-v1.\nWhile Theorem 1 requires the weights to be independent of the test example, we relax this assumption by setting higher weights for questions in a \u201cneighborhood\u201d of Xn+1 (see \u00a73). Intuitively, we could think of a situation where the questions are posed by multiple users, each of which may have a tendency to ask semantically similar questions or from the same domain. In this case, we could choose a priori higher weights for closer domains/users without violating this assumption.\nWe use the Natural Questions dataset (Kwiatkowski et al., 2019; Karpukhin et al., 2020), considering n = 2500 points for calibration and 1110 for evaluation. Following Angelopoulos et al. (2023a), we use \u03b1 = 0.3 and report results over 1000 trials in Fig. 3. While the test risk is similar in both cases (0.30\u00b10.015), the prediction sets of our method are considerably smaller than those of standard CRC (23.0\u00b1 1.47 vs. 24.6\u00b1 1.83, respectively). By choosing appropriate weights we can better estimate the set size needed to obtain the desired risk level, while standard CRC tends to overestimate the set size to reach the same value. We thus obtain better estimates of confidence over the predictions."
        },
        {
            "heading": "5 RELATED WORK",
            "text": "Conformal prediction (Gammerman et al., 1998; Vovk et al., 1999; Saunders et al., 1999) has proven to be a useful tool for obtaining uncertainty sets/intervals for the predictions of machine learning models, having found a variety of extensions and applications over the years. Among these are split conformal prediction (Papadopoulos et al., 2002), which does not require retraining the predictor and instead uses a held-out dataset and cross-conformal prediction (Vovk, 2015), which is a hybrid between split conformal prediction and cross-validation. Some of these methods have recently been applied in tasks such as language modeling (Schuster et al., 2022), molecular design (Fannjiang et al., 2022), pose estimation (Yang & Pavone, 2023), and image denoising (Teneggi et al., 2023).\nIn addition to the works discussed in \u00a72, several extensions to non-exchangeable data have been proposed for time series (Chernozhukov et al., 2018; 2021b; Xu & Xie, 2021; Stankeviciute et al., 2021; Lin et al., 2022; Zaffran et al., 2022; Sun & Yu, 2022; Schlembach et al., 2022; Angelopoulos et al., 2023b), covariate shift (Tibshirani et al., 2019), label shift (Podkopaev & Ramdas, 2021), and others (Cauchois et al., 2020; Gibbs & Candes, 2021; Chernozhukov et al., 2021a; Gibbs & Cande\u0300s, 2022; Oliveira et al., 2022; Guan, 2022). Moreover, there is recent work aiming at controlling arbitrary risks in an online setting (Feldman et al., 2022). The ideas, assumptions, or formal guarantees in these works are different to ours\u2014we refer the reader to the specific papers for further information.\nAngelopoulos et al. (2023a) touch the case of conformal risk control under covariate shift (Proposition 3; without providing any empirical validation), explaining how to generalize the work of Tibshirani et al. (2019) to any monotone risk under the strong assumption that the distribution of Y |X is the same for both the training and test data and that the likelihood ratio between Xtest and Xtrain is known or can be accurately estimated using a large set of test data. This result is orthogonal to ours. Besides, they quantify how unweighted conformal risk control degrades when there is an arbitrary distribution shift. Our work is more general and differs in several significant ways: we allow for an arbitrary design of weights, the bounds can be tighter, and the losses are bounded in [A,B], not necessarily in [0, B]. Specifically, their Proposition 4 is a particular case of our main result (choosing A = 0 and unitary weights), which we use as a baseline in our experiments."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "We have proposed a new method for conformal risk control, which is still valid when the data is not exchangeable (e.g., due to an arbitrary distribution shift) and provides a tighter bound on the expected loss than that of previous work. Our simulated experiments illustrate how non-exchangeable conformal risk control effectively provides prediction sets satisfying the risk requirements in the presence of non-exchangeable data (in particular, in the presence of change points and distribution drift), without sacrificing performance if the data is in fact exchangeable. Additional experiments with real data validate the usefulness of our approach.\nOur work opens up exciting possibilities for research on risk control in challenging settings. For instance, it is an attractive framework for providing guarantees on the predictions of large language models, being of particular interest in tasks involving language generation, medical data (Jalali et al., 2020), or reinforcement learning (Wang et al., 2023), where the i.i.d. assumption does not hold."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We would like to thank Ma\u0301rio Figueiredo, the SARDINE lab team, and the anonymous reviewers for helpful discussions. This work was built on open-source software; we acknowledge Van Rossum & Drake (2009); Oliphant (2006); Virtanen et al. (2020); Walt et al. (2011); Pedregosa et al. (2011), and Paszke et al. (2019). This work was supported by EU\u2019s Horizon Europe Research and Innovation Actions (UTTER, contract 101070631), by the project DECOLLAGE (ERC-2022-CoG 101088763), by the Portuguese Recovery and Resilience Plan through project C645008882-00000055 (Center for Responsible AI), and by Fundac\u0327a\u0303o para a Cie\u0302ncia e Tecnologia through contract UIDB/50008/2020."
        },
        {
            "heading": "A PROOF OF LEMMA 1",
            "text": "The TV distance can be written as an integral probability metric (Mu\u0308ller, 1997):\ndTV(P,Q) = 1\n2 sup\ng: \u2225g\u2225\u221e\u22641\n( EP [g]\u2212 EQ[g] ) . (17)\nNow, we define m = (A + B)/2, v = (B \u2212 A)/2, and f\u0304 = (f \u2212m)/v : S \u2192 [\u22121, 1]. Noticing that for any c \u2208 R, we have EP [f ]\u2212EQ[f ] = EP [f+c]\u2212EQ[f+c], we can evaluate the difference in expectations as\nEP [f ]\u2212 EQ[f ] = v ( EP [f\u0304 ]\u2212 EQ[f\u0304 ] ) (18)\n\u2264 B \u2212A 2\nsup g: \u2225g\u2225\u221e\u22641\n( EP [g]\u2212 EQ[g] ) (19)\n= (B \u2212A) dTV(P,Q). (20)\nRepeating with f\u0304 = (m\u2212f)/v (which is also in [\u22121, 1]), yields a similar upper-bound for EQ[f ]\u2212 EP [f ], from which the result for |EP [f ]\u2212 EQ[f ]| follows."
        },
        {
            "heading": "B PROOF OF THEOREM 1",
            "text": "The proof adapts elements of the proofs from Barber et al. (2023) and Angelopoulos et al. (2023a). Let ZK be obtained from Z by swapping (XK , YK) and (Xn+1, Yn+1), where K is a random variable where P{K = i} = w\u0303i (note that Zn+1 = Z). Let\nR\u0302n+1(\u03bb) = n+1\u2211 i=1 w\u0303iL(\u03bb; (xi, yi)) = NwR\u0302n(\u03bb) + L(\u03bb; (xn+1, yn+1)) Nw + 1 (21)\nbe the weighted empirical risk in the calibration set plus the additional test example. Let us define\n\u03bb\u2217 = inf { \u03bb : R\u0302n+1(\u03bb) \u2264 \u03b1 } . (22)\nGiven the random variable Z, we can think of \u03bb\u2217(Z) as another random variable which is a transformation of Z. Moreover, we define the random variable Fi(Z) = L(\u03bb\u2217(Z); (Xi, Yi)) for i \u2208 [n+1], as well as the vector of random variables F (Z) = [F1(Z), . . . , Fn+1(Z)]. From Lemma 1, we have\nE[Fi(Zi)] \u2264 E[Fi(Z)] + (B \u2212A)dTV(F (Z), F (Zi)), (23)\na bound that we will use later. Writing Li(\u03bb) \u2261 L(\u03bb; (Xi, Yi)) for convenience, we also have, for any \u03bb and for any k \u2208 [n+ 1],\nR\u0302n+1(\u03bb;Z k) = n\u2211 i=1,i\u0338=k w\u0303iLi(\u03bb) + w\u0303kLn+1(\u03bb) + w\u0303n+1Lk(\u03bb)\n= n\u2211 i=1,i\u0338=k w\u0303iLi(\u03bb) + w\u0303k(Lk(\u03bb) + Ln+1(\u03bb)\ufe38 \ufe37\ufe37 \ufe38 \u2264B ) + (w\u0303n+1 \u2212 w\u0303k)\ufe38 \ufe37\ufe37 \ufe38 \u22650 Lk(\u03bb)\ufe38 \ufe37\ufe37 \ufe38 \u2264B\n\u2264 n\u2211\ni=1,i\u0338=k\nw\u0303iLi(\u03bb) + w\u0303k(Lk(\u03bb) +B) + (w\u0303n+1 \u2212 w\u0303k)B\n= n\u2211 i=1 w\u0303iLi(\u03bb) + w\u0303n+1B\n= Nw\nNw + 1 R\u0302n(\u03bb;Z) +\nB\nNw + 1 . (24)\nTherefore, setting \u03bb = \u03bb\u0302 and using Eq. (10), we obtain R\u0302n+1(\u03bb\u0302;Zk) \u2264 NwNw+1 R\u0302n(\u03bb\u0302;Z)+ B Nw+1 \u2264 \u03b1, which, from Eq. (22), implies \u03bb\u2217(Zk) \u2264 \u03bb\u0302(Z). Since the loss L is nonincreasing with \u03bb, we get\nE[Ln+1(\u03bb\u0302(Z);Z)] \u2264 E[Ln+1(\u03bb\u2217(ZK);Z)] = E[LK(\u03bb\u2217(ZK);ZK ]\n= n+1\u2211 i=1 P{K = i}\ufe38 \ufe37\ufe37 \ufe38 =w\u0303i E[Li(\u03bb\u2217(Zi), Zi]\ufe38 \ufe37\ufe37 \ufe38 =E[Fi(Zi)] \u2264 n+1\u2211 i=1 w\u0303i E[Li(\u03bb\u2217(Z), Z]\ufe38 \ufe37\ufe37 \ufe38 =E[Fi(Z)] +(B \u2212A)dTV(F (Z), F (Zi))\n = E\n[ n+1\u2211 i=1 w\u0303iLi(\u03bb \u2217(Z), Z) ] + (B \u2212A) n\u2211 i=1 w\u0303idTV(F (Z), F (Z i))\n= E [ R\u0302n+1(\u03bb \u2217(Z)) ] + (B \u2212A) n\u2211 i=1 w\u0303idTV(F (Z), F (Z i))\n\u2264 \u03b1+ (B \u2212A) n\u2211\ni=1\nw\u0303idTV(F (Z), F (Z i)). (25)\nThe result follows by noting that dTV(F (Z), F (Zi)) \u2264 dTV(Z,Zi). Eq. (25) is actually a tighter bound, similarly to what has been noted by Barber et al., 2023."
        },
        {
            "heading": "C MULTILABEL CLASSIFICATION IN A TIME SERIES",
            "text": "Fig. 4 shows results averaged across 10 independent trials for \u03b1 = 0.2 and setting \u03bb in a slightly different way than that of \u00a74.1. In this case, \u03bb represents the number of active labels and we use C\u03bb(Xi) = top-\u03bb(f(Xi)). The main takeaways remain the same: both methods perform similarly when the data is exchangeable, in setting (1). Accounting for the non-exchangeability introduced by changepoints and distribution drift using our method enables lowering the risk to the desired level in settings (2) and (3)."
        }
    ],
    "title": "NON-EXCHANGEABLE CONFORMAL RISK CONTROL",
    "year": 2024
}