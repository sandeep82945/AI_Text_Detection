{
    "abstractText": "Fairness-aware graph neural networks (GNNs) have gained a surge of attention as they can reduce the bias of predictions on any demographic group (e.g., female) in graph-based applications. Although these methods greatly improve the algorithmic fairness of GNNs, the fairness can be easily corrupted by carefully designed adversarial attacks. In this paper, we investigate the problem of adversarial attacks on fairness of GNNs and propose G-FairAttack, a general framework for attacking various types of fairness-aware GNNs in terms of fairness with an unnoticeable effect on prediction utility. In addition, we propose a fast computation technique to reduce the time complexity of G-FairAttack. The experimental study demonstrates that G-FairAttack successfully corrupts the fairness of different types of GNNs while keeping the attack unnoticeable. Our study on fairness attacks sheds light on potential vulnerabilities in fairness-aware GNNs and guides further research on the robustness of GNNs in terms of fairness.",
    "authors": [
        {
            "affiliations": [],
            "name": "Binchi Zhang"
        },
        {
            "affiliations": [],
            "name": "Yushun Dong"
        },
        {
            "affiliations": [],
            "name": "Chen Chen"
        },
        {
            "affiliations": [],
            "name": "Yada Zhu"
        },
        {
            "affiliations": [],
            "name": "Minnan Luo"
        },
        {
            "affiliations": [],
            "name": "Jundong Li"
        }
    ],
    "id": "SP:ec7ffbc368daad7b1a5e2a27876effd862227ed7",
    "references": [
        {
            "authors": [
                "Chirag Agarwal",
                "Himabindu Lakkaraju",
                "Marinka Zitnik"
            ],
            "title": "Towards a unified framework for fair and stable graph representation learning",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Inayat Ali",
                "Sonia Sabir",
                "Zahid Ullah"
            ],
            "title": "Internet of things security, device authentication and access control: A review",
            "venue": "International Journal of Computer Science and Information Security (IJCSIS),",
            "year": 2016
        },
        {
            "authors": [
                "Aleksandar Bojchevski",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Adversarial attacks on node embeddings via graph poisoning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Avishek Bose",
                "William Hamilton"
            ],
            "title": "Compositional fairness constraints for graph embeddings",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Anirban Chakraborty",
                "Manaar Alam",
                "Vishal Dey",
                "Anupam Chattopadhyay",
                "Debdeep Mukhopadhyay"
            ],
            "title": "Adversarial attacks and defences: A survey",
            "venue": "arXiv preprint arXiv:1810.00069,",
            "year": 2018
        },
        {
            "authors": [
                "Heng Chang",
                "Yu Rong",
                "Tingyang Xu",
                "Wenbing Huang",
                "Honglei Zhang",
                "Peng Cui",
                "Wenwu Zhu",
                "Junzhou Huang"
            ],
            "title": "A restricted black-box adversarial framework towards attacking graph embedding models",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Hongyan Chang",
                "Ta Duy Nguyen",
                "Sasi Kumar Murakonda",
                "Ehsan Kazemi",
                "Reza Shokri"
            ],
            "title": "On adversarial bias and the robustness of fair machine learning",
            "venue": "arXiv preprint arXiv:2006.08669,",
            "year": 2020
        },
        {
            "authors": [
                "Pin-Yu Chen",
                "Huan Zhang",
                "Yash Sharma",
                "Jinfeng Yi",
                "Cho-Jui Hsieh"
            ],
            "title": "Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models",
            "venue": "In Proceedings of the 10th ACM workshop on artificial intelligence and security,",
            "year": 2017
        },
        {
            "authors": [
                "Anshuman Chhabra",
                "Peizhao Li",
                "Prasant Mohapatra",
                "Hongfu Liu"
            ],
            "title": "Robust fair clustering: A novel fairness attack and defense framework",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Edward Choi",
                "Mohammad Taha Bahadori",
                "Le Song",
                "Walter F Stewart",
                "Jimeng Sun"
            ],
            "title": "Gram: graph-based attention model for healthcare representation learning",
            "venue": "In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining,",
            "year": 2017
        },
        {
            "authors": [
                "Enyan Dai",
                "Suhang Wang"
            ],
            "title": "Say no to the discrimination: Learning fair graph neural networks with limited sensitive attribute information",
            "venue": "In Proceedings of the 14th ACM International Conference on Web Search and Data Mining,",
            "year": 2021
        },
        {
            "authors": [
                "Hanjun Dai",
                "Hui Li",
                "Tian Tian",
                "Xin Huang",
                "Lin Wang",
                "Jun Zhu",
                "Le Song"
            ],
            "title": "Adversarial attack on graph structured data",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Yushun Dong",
                "Ninghao Liu",
                "Brian Jalaian",
                "Jundong Li"
            ],
            "title": "Edits: Modeling and mitigating data bias for graph neural networks",
            "venue": "In Proceedings of the ACM Web Conference",
            "year": 2022
        },
        {
            "authors": [
                "Yushun Dong",
                "Song Wang",
                "Yu Wang",
                "Tyler Derr",
                "Jundong Li"
            ],
            "title": "On structural explanation of bias in graph neural networks",
            "venue": "In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Yushun Dong",
                "Jing Ma",
                "Song Wang",
                "Chen Chen",
                "Jundong Li"
            ],
            "title": "Fairness in graph mining: A survey",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2023
        },
        {
            "authors": [
                "Yushun Dong",
                "Song Wang",
                "Jing Ma",
                "Ninghao Liu",
                "Jundong Li"
            ],
            "title": "Interpreting unfairness in graph neural networks via training node attribution",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Yushun Dong",
                "Binchi Zhang",
                "Yiling Yuan",
                "Na Zou",
                "Qi Wang",
                "Jundong Li"
            ],
            "title": "Reliant: Fair knowledge distillation for graph neural networks",
            "venue": "In Proceedings of the 2023 SIAM International Conference on Data Mining (SDM),",
            "year": 2023
        },
        {
            "authors": [
                "Michele Donini",
                "Luca Oneto",
                "Shai Ben-David",
                "John S Shawe-Taylor",
                "Massimiliano Pontil"
            ],
            "title": "Empirical risk minimization under fairness constraints",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Cynthia Dwork",
                "Moritz Hardt",
                "Toniann Pitassi",
                "Omer Reingold",
                "Richard Zemel"
            ],
            "title": "Fairness through awareness",
            "venue": "In Proceedings of the 3rd innovations in theoretical computer science conference,",
            "year": 2012
        },
        {
            "authors": [
                "Wei Fan",
                "Kunpeng Liu",
                "Rui Xie",
                "Hao Liu",
                "Hui Xiong",
                "Yanjie Fu"
            ],
            "title": "Fair graph auto-encoder for unbiased graph representations with wasserstein distance",
            "venue": "IEEE International Conference on Data Mining (ICDM),",
            "year": 2021
        },
        {
            "authors": [
                "Wenqi Fan",
                "Yao Ma",
                "Qing Li",
                "Yuan He",
                "Eric Zhao",
                "Jiliang Tang",
                "Dawei Yin"
            ],
            "title": "Graph neural networks for social recommendation",
            "venue": "In The World Wide Web Conference,",
            "year": 2019
        },
        {
            "authors": [
                "Shangbin Feng",
                "Zhaoxuan Tan",
                "Herun Wan",
                "Ningnan Wang",
                "Zilong Chen",
                "Binchi Zhang",
                "Qinghua Zheng",
                "Wenqian Zhang",
                "Zhenyu Lei",
                "Shujie Yang"
            ],
            "title": "Twibot-22: Towards graph-based twitter bot detection",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Danilo Franco",
                "Nicol\u00f2 Navarin",
                "Michele Donini",
                "Davide Anguita",
                "Luca Oneto"
            ],
            "title": "Deep fair models for complex data: Graphs labeling and explainable face recognition",
            "year": 2022
        },
        {
            "authors": [
                "Xingbo Fu",
                "Chen Chen",
                "Yushun Dong",
                "Anil Vullikanti",
                "Eili Klein",
                "Gregory Madden",
                "Jundong Li"
            ],
            "title": "Spatial-temporal networks for antibiogram pattern prediction",
            "venue": "arXiv preprint arXiv:2305.01761,",
            "year": 2023
        },
        {
            "authors": [
                "Simon Geisler",
                "Tobias Schmidt",
                "Hakan \u015eirin",
                "Daniel Z\u00fcgner",
                "Aleksandar Bojchevski",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Robustness of graph neural networks at scale",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Will Hamilton",
                "Zhitao Ying",
                "Jure Leskovec"
            ],
            "title": "Inductive representation learning on large graphs. Advances in neural information processing",
            "year": 2017
        },
        {
            "authors": [
                "Moritz Hardt",
                "Eric Price",
                "Nati Srebro"
            ],
            "title": "Equality of opportunity in supervised learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Hussain Hussain",
                "Meng Cao",
                "Sandipan Sikdar",
                "Denis Helic",
                "Elisabeth Lex",
                "Markus Strohmaier",
                "Roman Kern"
            ],
            "title": "Adversarial inter-group link injection degrades the fairness of graph neural networks",
            "venue": "IEEE International Conference on Data Mining (ICDM),",
            "year": 2022
        },
        {
            "authors": [
                "Zhimeng Jiang",
                "Xiaotian Han",
                "Chao Fan",
                "Zirui Liu",
                "Na Zou",
                "Ali Mostafavi",
                "Xia Hu"
            ],
            "title": "Fmp: Toward fair graph message passing against topology bias",
            "venue": "arXiv preprint arXiv:2202.04187,",
            "year": 2022
        },
        {
            "authors": [
                "Wei Jin",
                "Yaxing Li",
                "Han Xu",
                "Yiqi Wang",
                "Shuiwang Ji",
                "Charu Aggarwal",
                "Jiliang Tang"
            ],
            "title": "Adversarial attacks and defenses on graphs",
            "venue": "ACM SIGKDD Explorations Newsletter,",
            "year": 2021
        },
        {
            "authors": [
                "Jian Kang",
                "Tiankai Xie",
                "Xintao Wu",
                "Ross Maciejewski",
                "Hanghang Tong"
            ],
            "title": "Infofair: Informationtheoretic intersectional fairness",
            "venue": "IEEE International Conference on Big Data (Big Data),",
            "year": 2022
        },
        {
            "authors": [
                "Jian Kang",
                "Yinglong Xia",
                "Ross Maciejewski",
                "Jiebo Luo",
                "Hanghang Tong"
            ],
            "title": "Deceptive fairness attacks on graphs via meta learning",
            "venue": "arXiv preprint arXiv:2310.15653,",
            "year": 2023
        },
        {
            "authors": [
                "Sanjay Kariyappa",
                "Atul Prakash",
                "Moinuddin K Qureshi"
            ],
            "title": "Maze: Data-free model stealing attack using zeroth-order gradient estimation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2015
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2017
        },
        {
            "authors": [
                "Pang Wei Koh",
                "Jacob Steinhardt",
                "Percy Liang"
            ],
            "title": "Stronger data poisoning attacks break data sanitization defenses",
            "venue": "Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Jure Leskovec",
                "Julian Mcauley"
            ],
            "title": "Learning to discover social circles in ego networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2012
        },
        {
            "authors": [
                "Haoyang Li",
                "Shimin Di",
                "Zijian Li",
                "Lei Chen",
                "Jiannong Cao"
            ],
            "title": "Black-box adversarial attack and defense on graph neural networks",
            "venue": "IEEE 38th International Conference on Data Engineering (ICDE),",
            "year": 2022
        },
        {
            "authors": [
                "Michelle M Li",
                "Kexin Huang",
                "Marinka Zitnik"
            ],
            "title": "Graph representation learning in biomedicine and healthcare",
            "venue": "Nature Biomedical Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Peizhao Li",
                "Yifei Wang",
                "Han Zhao",
                "Pengyu Hong",
                "Hongfu Liu"
            ],
            "title": "On dyadic fairness: Exploring and mitigating bias in graph connections",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Lu Lin",
                "Ethan Blaser",
                "Hongning Wang"
            ],
            "title": "Graph structural attack by perturbing spectral distance",
            "venue": "In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Yi-Ju Lu",
                "Cheng-Te Li"
            ],
            "title": "GCAN: Graph-aware co-attention networks for explainable fake news detection on social media",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Jiaqi Ma",
                "Shuangrui Ding",
                "Qiaozhu Mei"
            ],
            "title": "Towards more practical adversarial attacks on graph neural networks. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Jiaqi Ma",
                "Junwei Deng",
                "Qiaozhu Mei"
            ],
            "title": "Adversarial attack on graph neural networks as an influence maximization problem",
            "venue": "In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Ninareh Mehrabi",
                "Muhammad Naveed",
                "Fred Morstatter",
                "Aram Galstyan"
            ],
            "title": "Exacerbating algorithmic bias through fairness attacks",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Nicolo Navarin",
                "Luca Oneto",
                "Michele Donini"
            ],
            "title": "Learning deep fair graph neural networks",
            "venue": "In European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Emanuel Parzen"
            ],
            "title": "On estimation of a probability density function and mode",
            "venue": "The annals of mathematical statistics,",
            "year": 1962
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, highperformance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Jiezhong Qiu",
                "Jian Tang",
                "Hao Ma",
                "Yuxiao Dong",
                "Kuansan Wang",
                "Jie Tang"
            ],
            "title": "Deepinf: Social influence prediction with deep learning",
            "venue": "In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining,",
            "year": 2018
        },
        {
            "authors": [
                "David Solans",
                "Battista Biggio",
                "Carlos Castillo"
            ],
            "title": "Poisoning attacks on algorithmic fairness",
            "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,",
            "year": 2021
        },
        {
            "authors": [
                "Indro Spinelli",
                "Simone Scardapane",
                "Amir Hussain",
                "Aurelio Uncini"
            ],
            "title": "Fairdrop: Biased edge dropout for enhancing fairness in graph representation learning",
            "venue": "IEEE Transactions on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Lichao Sun",
                "Yingtong Dou",
                "Carl Yang",
                "Kai Zhang",
                "Ji Wang",
                "S Yu Philip",
                "Lifang He",
                "Bo Li"
            ],
            "title": "Adversarial attack and defense on graph data: A survey",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Yiwei Sun",
                "Suhang Wang",
                "Xianfeng Tang",
                "Tsung-Yu Hsieh",
                "Vasant Honavar"
            ],
            "title": "Node injection attacks on graphs via reinforcement learning",
            "year": 1909
        },
        {
            "authors": [
                "Minh-Hao Van",
                "Wei Du",
                "Xintao Wu",
                "Aidong Lu"
            ],
            "title": "Poisoning attacks on fair machine learning",
            "venue": "In International Conference on Database Systems for Advanced Applications,",
            "year": 2022
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio"
            ],
            "title": "Graph attention networks",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Binghui Wang",
                "Neil Zhenqiang Gong"
            ],
            "title": "Attacking graph-based classification via manipulating the graph structure",
            "venue": "In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications",
            "year": 2019
        },
        {
            "authors": [
                "Nan Wang",
                "Lu Lin",
                "Jundong Li",
                "Hongning Wang"
            ],
            "title": "Unbiased graph embedding with biased graph observations",
            "venue": "In Proceedings of the Web Conference",
            "year": 2022
        },
        {
            "authors": [
                "Yu Wang",
                "Yuying Zhao",
                "Yushun Dong",
                "Huiyuan Chen",
                "Jundong Li",
                "Tyler Derr"
            ],
            "title": "Improving fairness in graph neural networks via mitigating sensitive attribute leakage",
            "venue": "In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Huijun Wu",
                "Chen Wang",
                "Yuriy Tyshetskiy",
                "Andrew Docherty",
                "Kai Lu",
                "Liming Zhu"
            ],
            "title": "Adversarial examples for graph data: deep insights into attack and defense",
            "venue": "In Proceedings of the 28th International Joint Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Yinhao Xiao",
                "Yizhen Jia",
                "Chunchi Liu",
                "Xiuzhen Cheng",
                "Jiguo Yu",
                "Weifeng Lv"
            ],
            "title": "Edge computing security: State of the art and challenges",
            "venue": "Proceedings of the IEEE,",
            "year": 2019
        },
        {
            "authors": [
                "Kaidi Xu",
                "Hongge Chen",
                "Sijia Liu",
                "Pin-Yu Chen",
                "Tsui-Wei Weng",
                "Mingyi Hong",
                "Xue Lin"
            ],
            "title": "Topology attack and defense for graph neural networks: an optimization perspective",
            "venue": "In Proceedings of the 28th International Joint Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Rex Ying",
                "Ruining He",
                "Kaifeng Chen",
                "Pong Eksombatchai",
                "William L Hamilton",
                "Jure Leskovec"
            ],
            "title": "Graph convolutional neural networks for web-scale recommender systems",
            "venue": "In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining,",
            "year": 2018
        },
        {
            "authors": [
                "Junliang Yu",
                "Hongzhi Yin",
                "Jundong Li",
                "Qinyong Wang",
                "Nguyen Quoc Viet Hung",
                "Xiangliang Zhang"
            ],
            "title": "Self-supervised multi-channel hypergraph convolutional network for social recommendation",
            "venue": "In Proceedings of the Web Conference",
            "year": 2021
        },
        {
            "authors": [
                "Muhammad Bilal Zafar",
                "Isabel Valera",
                "Manuel Gomez Rogriguez",
                "Krishna P Gummadi"
            ],
            "title": "Fairness constraints: Mechanisms for fair classification",
            "venue": "In Artificial intelligence and statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Ziqian Zeng",
                "Rashidul Islam",
                "Kamrun Naher Keya",
                "James Foulds",
                "Yangqiu Song",
                "Shimei Pan"
            ],
            "title": "Fair representation learning for heterogeneous information networks",
            "venue": "In Proceedings of the International AAAI Conference on Weblogs and Social Media,",
            "year": 2021
        },
        {
            "authors": [
                "He Zhang",
                "Xingliang Yuan",
                "Chuan Zhou",
                "Shirui Pan"
            ],
            "title": "Projective ranking-based gnn evasion attacks",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Z\u00fcgner",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Adversarial attacks on graph neural networks via meta learning",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Z\u00fcgner",
                "Amir Akbarnejad",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Adversarial attacks on neural networks for graph data",
            "venue": "In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining,",
            "year": 2018
        },
        {
            "authors": [
                "Zhang"
            ],
            "title": "deployed, the attacker can launch evasion attacks at any time, which increases the difficulty and cost of defending against evasion attacks",
            "venue": "(Zhang et al.,",
            "year": 2022
        },
        {
            "authors": [
                "Bojchevski",
                "G\u00fcnnemann",
                "Lin"
            ],
            "title": "2022) do not have a lower complexity (less than O(n2)) than our proposed G-FairAttack. The adversarial attacks with an O(n2) complexity",
            "year": 2022
        },
        {
            "authors": [
                "Geisler"
            ],
            "title": "2021) to find the maximum of the attacker\u2019s objective. As G-FairAttack, gradient-based structure attacks flip the edges sequentially",
            "venue": "In the t-th iteration",
            "year": 2021
        },
        {
            "authors": [
                "S \u2022 I(\u0176"
            ],
            "title": "For mutual information loss, except for directly decreasing the mutual information, the adversarial training (Bose & Hamilton, 2019; Dai & Wang, 2021) could also be seen as a specific case of exploiting the mutual information loss according to (Kang et al., 2022). In an adversarial training framework, an adversary is trained to predict S based on \u0176",
            "year": 2022
        },
        {
            "authors": [
                "S \u2022 W (\u0176"
            ],
            "title": "We choose EDITS (Dong et al., 2022a), a model agnostic debiasing framework for GNNs. EDITS finds a debiased adjacency matrix and a debiased node attribute matrix by minimizing the Wasserstein distance of the distributions of node embeddings on different sensitive groups. With the debiased input graph data, the fairness of the GNN backbone is improved. Specifically, we choose a two-layer GCN as the GNN backbone",
            "year": 2022
        },
        {
            "authors": [
                "G-FairAttack",
                "FA-GNN"
            ],
            "title": "The results of attack patterns of G-FairAttack are shown in Table 10. According to the study in (Hussain et al., 2022), injecting edges in DD and EE can increase the statistical parity difference. Based on this guidance, FA-GNN randomly injects edges that belong to group DD to attack the fairness of GNNs. Hence, the attack patterns of FA-GNN for all datasets",
            "venue": "Credit",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Graph Neural Networks (GNNs) have achieved remarkable success across various human-centered applications, e.g., social network analysis (Qiu et al., 2018; Lu & Li, 2020; Feng et al., 2022), recommender systems (Ying et al., 2018; Fan et al., 2019; Yu et al., 2021), and healthcare (Choi et al., 2017; Li et al., 2022b; Fu et al., 2023). Despite that, many recent studies (Dai & Wang, 2021; Wang et al., 2022b; Dong et al., 2022b; 2023b) have shown that GNNs could yield biased predictions for nodes from certain demographic groups (e.g., females). With the increasing significance of GNNs in high-stakes human-centered scenarios, addressing such prediction bias becomes imperative. Consequently, substantial efforts have been devoted to developing fairness-aware GNNs, with the goal of learning fair node representations that can be used to make accurate and fair predictions. Typical strategies to improve the fairness of GNNs include adversarial training (Bose & Hamilton, 2019; Dai & Wang, 2021), regularization (Navarin et al., 2020; Fan et al., 2021; Agarwal et al., 2021; Dong et al., 2023c), and edge rewiring (Spinelli et al., 2021; Li et al., 2021; Dong et al., 2022a).\nAlthough these fairness-aware GNN frameworks can make fair predictions, there remains a high risk of the exhibited fairness being corrupted by malicious attackers (Hussain et al., 2022). Adversarial attacks threaten the fairness of the victim model by perturbing the input graph data, resulting in unfair predictions. Such fairness attacks can severely compromise the reliability of GNNs, even when they have a built-in fairness-enhancing mechanism. For instance, we consider a GNN-based recommender system in social networks. In scenarios regarding commercial competition or personal benefits, a malicious adversary might conduct fairness attacks by hijacking inactive user accounts. The adversary can collect user data from these accounts to train a fairness attack model. According to the fairness attack algorithm, the adversary can then modify the attributes and connections of these compromised accounts. As a result, the GNN-based system is affected by the poisoned input data and makes biased predictions against a specific user group.\nTo protect the fairness of GNN models from adversarial attacks, we should first fully understand the potential ways to attack fairness. To this end, we investigate the problem of adversarial attacks on fairness of GNNs in this paper. Despite the importance of investigating the fairness attack of GNNs, most existing studies (Zu\u0308gner et al., 2018; Zu\u0308gner & Gu\u0308nnemann, 2019; Bojchevski & Gu\u0308nnemann, 2019; Dai et al., 2018; Xu et al., 2019) only focus on attacking the utility of GNNs (e.g., prediction\naccuracy) and neglect the vulnerability of GNNs in the fairness aspect. To study fairness attacks, we follow existing works of attacks on GNNs to formulate the attack as an optimization problem and consider the prevalent gray-box attack setting (Zu\u0308gner et al., 2018; Zu\u0308gner & Gu\u0308nnemann, 2019; Wang & Gong, 2019; Sun et al., 2019) where the attacker cannot access the architecture or parameters of the victim model (Jin et al., 2021; Sun et al., 2022). A common strategy in graybox attacks is to train a surrogate model (Zu\u0308gner et al., 2018; Zu\u0308gner & Gu\u0308nnemann, 2019) to gain more knowledge on the unseen victim model for the attack. Compared with conventional adversarial attacks, our attack on fairness is more difficult for the following two challenges: (1) The design of the surrogate loss function. Previous attacks on prediction utility directly choose the loss function adopted by most victim models, i.e. the cross-entropy (CE) loss as the surrogate loss. However, fairness-aware GNN models are trained based on different loss functions for fairness, e.g., demographic parity loss (Navarin et al., 2020; Zeng et al., 2021; Franco et al., 2022; Jiang et al., 2022), mutual information loss (Kang et al., 2022), and Wasserstein distance loss (Fan et al., 2021; Dong et al., 2022a). Because the victim model is unknown and can be any type of fairness-aware GNN, the attacker should find a proper surrogate loss to represent different types of loss functions of fairness-aware GNNs. (2) The necessity to make such attacks unnoticeable. If the poisoned graph data exhibits any clues of being manipulated, the model owner could recognize them and then take defensive actions (Zu\u0308gner et al., 2018), e.g., abandoning the poisoned input data. Conventionally, attackers restrict the perturbation size to make attacks unnoticeable. In fairness attacks, we argue that a distinct change of prediction utility can also be a strong clue of being manipulated. However, none of the existing works on fairness attacks has considered the unnoticeable utility change.\nIn light of these challenges, we propose a novel fairness attack method on GNNs named GFairAttack, which consists of two parts: a carefully designed surrogate loss function and an optimization method. To tackle the first challenge, we categorize existing fairness-aware GNNs into three types based on their fairness loss terms. Then we propose a novel surrogate loss function to help the surrogate model learn from all types of fairness-aware GNNs with theoretical analysis. To address the second challenge, we propose a novel unnoticeable constraint in utility change to make the fairness attack unnoticeable. Then we propose a non-gradient attack algorithm to solve the constrained optimization problem, which is verified to have a better performance than previous gradient-based methods. Moreover, we propose a fast computation strategy to improve the scalability of G-FairAttack. Our contributions can be summarized as follows.\n\u2022 Attack Setting. We consider a novel unnoticeable constraint on prediction utility change for unnoticeable fairness attacks of GNNs, which can be extended to general fairness attacks.\n\u2022 Objective Design. We propose a novel surrogate loss to help the surrogate model learn from various types of fairness-aware GNNs with theoretical analysis.\n\u2022 Algorithmic Design. To solve the optimization problem with the unnoticeable constraint, we discuss the deficiency of previous gradient-based optimization methods and design a non-gradient attack method. In addition, we propose a fast computation approach to reduce its time complexity.\n\u2022 Experimental Evaluation. We conduct extensive experiments on three real-world datasets with four types of victim models and verify that our proposed G-FairAttack successfully jeopardizes the fairness of various fairness-aware GNNs with an unnoticeable effect on prediction utility."
        },
        {
            "heading": "2 PROBLEM DEFINITION",
            "text": "Notation and Preliminary. We use bold uppercase letters, e.g., X, to denote matrices, and use X[i,:], X[:,j], and X[i,j] to denote the i-th row, the j-th column, and the element at the i-th row and the j-th column, respectively. We use bold lowercase letters, e.g., x, to denote vectors, and use x[i] to denote the i-th element. We use PX(\u00b7) and FX(\u00b7) to denote the probability density function and the cumulative distribution function for a random variable X , respectively. We use G = {V, E ,X} to denote an undirected attributed graph. V = {v1, . . . , vn} and E \u2286 V \u00d7 V denote the node set and the edge set, respectively, where n = |V| is the number of nodes. X \u2208 Rn\u00d7dx denotes the attribute matrix, where dx is the number of node attributes. We use A \u2208 {0, 1}n\u00d7n to denote the adjacency matrix, where A[i,j] = 1 when (i, j) \u2208 E and A[i,j] = 0 otherwise. In the node classification task, some nodes are associated with ground truth labels. We use Vtrain to denote the labeled (training) set, Vtest = V\\Vtrain to denote the unlabeled (test) set, and Y to denote the set of labels. Most GNNs take the adjacency matrix A and the attribute matrix X as the input, and obtain\nthe node embeddings used for node classification tasks. Specifically, we use f\u03b8 : {0, 1}n\u00d7n \u00d7 Rn\u00d7dx \u2192 Rn\u00d7c to denote a GNN model, where \u03b8 collects all learnable model parameters. We use y\u0302 = \u03c3 (f\u03b8(A,X)) to denote soft predictions where \u03c3(\u00b7) is the softmax function. A conventional way to train a GNN model is minimizing a utility loss function L (f\u03b8,A,X,Y,Vtrain) (e.g., CE loss) over the training set. In human-centered scenarios, each individual is usually associated with sensitive attributes (e.g., gender). We use S = {s1, . . . , sn} to denote a sensitive attribute set, where si is the sensitive attribute of the node vi. Based on the sensitive attributes, we can divide the nodes into different sensitive groups, denoted as V1, . . . ,VK , where Vk = {vi|si = k} and K is the number of sensitive groups. Compared with vanilla GNNs, fairness-aware GNNs should not yield discriminatory predictions against individuals from any specific sensitive subgroup (Dong et al., 2023a). Hence, the objective of training fairness-aware GNNs generally contains two aspects; one is minimizing the utility loss, i.e., L (f\u03b8,A,X,Y,Vtrain), the other is minimizing the discrimination between the predictions over different sensitive groups, denoted as Lf (f\u03b8,A,X,Y,S). It is worth noting that L is the CE loss in most cases, but there are various types of Lf for training fair GNNs.\nProblem Formulation. In the fairness attack of GNNs, attackers aim to achieve their goals (e.g., to make the predictions of the victim GNN exhibit more bias) via slightly perturbing the input graph data in certain feasible spaces. Here, we focus on structure perturbation because perturbing the graph structure in the discrete domain presents more challenges, and the attack methods based on structure perturbation can be easily adapted to attribute perturbation. In this paper, for the simplicity of discussion, we focus on two different sensitive groups and prediction classes, but our method can be easily adapted to tasks\nwith multiple sensitive groups and classes. More details about our attack settings are discussed in Appendix B. We formulate our problem of fairness attacks of GNNs as follows. Problem 1. Attacks on Fairness of GNNs with Unnoticeable Effect on Prediction Utility. Let G = {V, E ,X} be an attributed graph with the adjacency matrix A. Let Vtrain and Vtest be the labeled and unlabeled node sets, respectively, Y be the label set corresponding to Vtrain, and S be the sensitive attribute value set of V . Let g\u03b8 be a surrogate GNN model, Ls, L, and Lf be the surrogate loss, utility loss, and fairness loss, respectively. Given the attack budget \u2206 and the utility variation budget \u03f5, the problem of attacks on fairness of GNNs is formulated as follows.\nmax A\u2032\u2208F\nLf (g\u03b8\u2217 ,A\u2032,X,Y,Vtest,S)\ns.t. \u03b8\u2217 = argmin \u03b8 Ls ( g\u03b8, A\u0303,X,Y,S ) , \u2225A\u2032 \u2212A\u2225F \u2264 2\u2206,\n|L(g\u03b8\u2217 ,A,X,Y,Vtrain)\u2212 L(g\u03b8\u2217 ,A\u2032,X,Y,Vtrain)| \u2264 \u03f5,\n(1)\nwhere F is the feasible space of the poisoned graph structure A\u2032, g\u03b8\u2217 is a trained surrogate model.\nIn this problem, the attacker\u2019s objective Lf (g\u03b8\u2217 ,A\u2032,X,Y,Vtest,S) measures the bias of the predictions on test nodes. The surrogate model g\u03b8\u2217 is trained based on a surrogate lossLs(g\u03b8, A\u0303,X,Y,S). The constraints restrict the number of perturbed edges and the absolute value of the utility change over the training set. In this paper, we consider two types of attack scenarios, fairness evasion attack and fairness poisoning attack (Chakraborty et al., 2018). By perturbing the input graph data, fairness evasion attack jeopardizes the fairness of a well-trained fairness-aware GNN model in the test phase, while fairness poisoning attack makes the fairness-aware GNN model trained on such perturbed data render unfair predictions. If A\u0303 = A, then Problem 1 boils down to the fairness evasion attack; and if A\u0303 = A\u2032, then Problem 1 becomes fairness poisoning attack. In this paper, we use demographic parity (Navarin et al., 2020) as the attacker\u2019s objective Lf , the CE loss as utility loss L, and a two-layer linearized GCN (Zu\u0308gner et al., 2018) as the surrogate model g\u03b8. These choices have been verified by many previous works (Zu\u0308gner et al., 2018; Zu\u0308gner & Gu\u0308nnemann, 2019; Xu et al., 2019) to be effective in attacking the prediction utility of GNNs, while can also be changed\nflexibly according to the specific task. An illustration of our problem is shown in Figure 1. We leave the detailed explanation of Figure 1 in Appendix B."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "In this section, we first introduce the design of the surrogate loss Ls, which is the most critical part in Problem 1. Then, we propose a non-gradient optimization method to solve Problem 1 and propose a fast computation technique to reduce the time complexity of our optimization method."
        },
        {
            "heading": "3.1 SURROGATE LOSS DESIGN",
            "text": "Existing Loss on Fairness. The loss functions of all existing fairness-aware GNNs can be divided into two parts: utility loss term (CE loss that is shared in common) and fairness loss term (varies from different methods). Correspondingly, our surrogate loss function is designed as Ls = Lsu + \u03b1Lsf where Lsu is the utility loss term, i.e., the CE loss, Lsf is the fairness loss term we aim to study, and \u03b1 is the weight coefficient. Let Y\u0302 \u2208 [0, 1] be a continuous random variable of the output soft prediction and S be a discrete random variable of the sensitive attribute. Next, we categorize the fairness loss terms into three types. (1). \u2206dp(Y\u0302 , S) = |Pr(Y\u0302 \u2265 12 |S = 0)\u2212Pr(Y\u0302 \u2265 1 2 |S = 1)|: demographic parity (Jiang et al., 2022; Navarin et al., 2020; Zeng et al., 2021; Franco et al., 2022), that reduces the difference of positive rate among different sensitive groups during training. (2). I(Y\u0302 , S) = \u222b 1 0 \u2211 i PY\u0302 ,S(z, i) log PY\u0302 ,S(z,i) PY\u0302 (z)Pr(S=i) dz: mutual information of the output and the sensitive attribute (Dai & Wang, 2021; Bose & Hamilton, 2019; Kang et al., 2022), that reduces the dependence between the model output and the sensitive attribute during training. (3). W (Y\u0302 , S) = \u222b 1 0 |F\u22121 Y\u0302 |S=0 (y) \u2212 F\u22121 Y\u0302 |S=1 (y)|dy: Wasserstein-1 distance of the output on different sensitive groups (Fan et al., 2021; Dong et al., 2022a), that makes the conditional distributions of the model output given different sensitive groups closer during training.\nThe Proposed Surrogate Loss. Next, we introduce our proposed surrogate loss term on fairness LSf , aiming at representing all types of aforementioned fairness loss terms, e.g., \u2206dp(Y\u0302 , S), I(Y\u0302 , S), and W (Y\u0302 , S). In particular, our idea is to find a common upper bound for these fairness loss terms asLSf . Therefore, during the training of the surrogate model, LSf is minimized to a small value, and all types of fairness loss terms become even smaller consequently. In this way, the surrogate model trained by our surrogate loss will be close to that trained by any unknown victim loss, which is consistent with conventional attacks on model utility. Consequently, we argue that such surrogate loss can represent the unknown victim loss. Accordingly, we propose a novel surrogate loss function on fairness TV (Y\u0302 , S) = \u222b 1 0 |PY\u0302 |S=0(z)\u2212 PY\u0302 |S=1(z)|dz, i.e., the total variation of Y\u0302\non different sensitive groups. We can prove that TV (Y\u0302 , S) is a common upper bound of \u2206dp(Y\u0302 , S), I(Y\u0302 , S) (at certain condition), and W (Y\u0302 , S) so it can represent all types of fairness loss functions.\nTheorem 1. We have \u2206dp(Y\u0302 , S) and W (Y\u0302 , S) upper bounded by TV (Y\u0302 , S). Moreover, assuming PY\u0302 (z) \u2265 \u03a0iPr(S = i) holds for any z \u2208 [0, 1], I(Y\u0302 , S) is also upper bounded by TV (Y\u0302 , S).\nThe proof of Theorem 1 is shown in Appendix A.1. We also provide a more practical variant of Theorem 1 with a weaker assumption in Appendix A.1. Combining Lsf (total variation loss) and Lsu (CE loss), our surrogate loss function is Ls = CE(g\u03b8, A\u0303,X,Y) + \u03b1TV (g\u03b8, A\u0303,X,S). Since the probability density function of the underlying random variable Y\u0302 is difficult to calculate, we leverage the kernel density estimation (Parzen, 1962) to compute the total variation loss with the output soft predictions g\u03b8(A\u0303,X). The kernel estimation is computed as P\u0302Y\u0302 |S=i(z) =\n1 h|Vi| \u2211 j\u2208Vi K( z\u2212g\u03b8(A\u0303,X)[j] h ) for i = 0, 1, where K(\u00b7) is a kernel function and h is a positive bandwidth constant. Then we exploit the numerical integration to compute the total variation loss as\nTV (g\u03b8, A\u0303,X,S) = 1\nm m\u2211 j=1 \u2223\u2223\u2223\u2223P\u0302Y\u0302 |S=0( jm ) \u2212 P\u0302Y\u0302 |S=1 ( j m )\u2223\u2223\u2223\u2223 , (2) where m is the number of intervals in the integration. Consequently, we obtain a practical calculation of our proposed surrogate loss Ls."
        },
        {
            "heading": "3.2 OPTIMIZATION",
            "text": "Considering that Problem 1 is a bilevel optimization problem in a discrete domain, it is extremely hard to find the exact solution. Hence, we resort to the greedy strategy and propose a sequential attack method. Our sequential attack flips target edges1 sequentially to obtain the optimal poisoned structure A\u2032. It is worth noting that many attack methods on model utility find the target edge corresponding to the largest element of the gradient of A, namely gradient-based methods (Zu\u0308gner & Gu\u0308nnemann, 2019; Xu et al., 2019; Wu et al., 2019; Geisler et al., 2021). However, we find the efficacy of gradient-based methods is not guaranteed. Proposition 1. Gradient-based methods for optimizing the graph structure are not guaranteed to decrease the objective function.\nWe leave a detailed analysis of the shortcomings of gradient-based methods in Appendix D. In contrast, we propose a non-gradient method based on a scoring function that can provably increase the attacker\u2019s objective. We first consider the unconstrained version of Problem 1. We let A0 = A and train the surrogate model to obtain \u03b80, i.e., solving \u03b80 = argmin\u03b8 Ls(g\u03b8,A,X,Y,S). For t = 1, 2, . . . , T0, we find the maximum score rt in the t-th iteration as\nmax (u,v)\u2208Ct\nrt(u, v) = \u2206Ltf (u, v) = Lf (g\u03b8t , f lip(u,v)At)\u2212 Lf (g\u03b8t ,At), (3)\nwhere Ct is the candidate edge set in the t-th iteration; flip(u,v)A denotes the adjacency matrix after flipping the edge (u, v): flip(u,v)A[i,j] = 1 \u2212A[i,j] if (i, j) = (u, v) or (i, j) = (v, u), and flip(u,v)A[i,j] = A[i,j] otherwise. After solving Equation (3), we denote the solution as (ut, vt). Then we update At as At+1 = flip(ut,vt)At in the t-th iteration. For the fairness poisoning attack, we retrain the surrogate model based on At+1 as \u03b8t+1 = argmin\u03b8 Ls(g\u03b8,At+1) to update the surrogate model. After T0 iterations, we have A\u2217 = AT0 as the solution of Problem 1.\nNext, to handle the first constraint, we let T0 \u2264 \u2206 and have \u2225A\u2217\u2212A\u2225F \u2264 \u2211T0\ni=1\u2225At\u2212At\u22121\u2225F \u2264 2\u2206 consequently. For the second constraint, we aim to make every flipping unnoticeable in terms of model utility, i.e., making |L(At+1) \u2212 L(At)| as small as possible. We notice that this constrained optimization problem can be easily solved by projected gradient descent (PGD) (Nocedal & Wright, 2006) in the continuous domain by solving At+1 = argmin|L(At)\u2212L(A\u2032)|\u2264\u03f5t\u2225A\n\u2032\u2212(At+ \u03b7\u2207Lf (At))\u22252F , where \u03f5t is the budget of the t-th iteration that satisfies \u2211\u2206 t=1 \u03f5t \u2264 \u03f5 and \u03b7 is the learning rate. To solve this problem, we have the following theorem. Theorem 2. The optimal poisoned adjacency matrix At+1 in the t + 1-th iteration given by PGD, i.e., the solution of At+1 = argmin|L(At)\u2212L(A\u2032)|\u2264\u03f5t\u2225A \u2032 \u2212 (At + \u03b7\u2207Lf (At))\u22252F is\nAt+1 =  At + \u03b7\u2207ALf (At), if \u03b7|\u2207AL(At)T\u2207ALf (At)| \u2264 \u03f5t, At + \u03b7\u2207ALf (At) + et\u03f5t \u2212 \u03b7\u2207AL(At)T\u2207ALf (At)\n\u2225\u2207AL(At)\u22252F \u2207AL(At), otherwise,\n(4)\nwhere et = sign ( \u2207AL(At)T\u2207ALf (At) ) .\nThe proof of Theorem 2 is provided in Appendix A.2. It is worth noting that the solution in Equation (4) cannot be leveraged directly in the discrete domain. Hence, we use the differences \u2206Ltf (u, v) = Lf (flip(u,v)At) \u2212 Lf (At) and \u2206Lt(u, v) = L(flip(u,v)At) \u2212 L(At) as zerothorder estimations (Chen et al., 2017; Kariyappa et al., 2021) of \u2207A[u,v]Lf (At) and \u2207A[u,v]L(At), and replace them in Equation (4), respectively. Moreover, we minimize the utility budget as \u03f5t = 0 to constrain the model utility change after the flipping as strictly as possible. Consequently, we adjust the scoring function to find the target edge as\nr\u0303t(u, v) = \u2206Ltf (u, v)\u2212 (pt)Tqt\n\u2225pt\u222522 |\u2206Lt(u, v)|, (5)\nfor (u, v) \u2208 Ct, where pt \u2208 R|Ct| and qt \u2208 R|Ct| are denoted as pt[i] = \u2206L t(Ct[i]) and q t [i] = \u2206Ltf (Ct[i]), respectively. Here C t [i] denotes the i-th edge in C\nt. Equation (5) can also be seen as a balanced attacker\u2019s objective between maximizingLf (At+1)\u2212Lf (At) and minimizing |L(At+1)\u2212 L(At)|. With r\u0303t(u, v), the pseudocode of our proposed attack algorithm is shown in Appendix C.1.\n1Flipping the target edge increases the attacker\u2019s objective the most."
        },
        {
            "heading": "3.3 FAST COMPUTATION",
            "text": "In this subsection, we focus on the efficient implementation of our sequential attack. The most important and costly part of our algorithm is to find the maximum value of r\u0303t(u, v) from Ct. This naturally requires traversing each edge in Ct, which is costly on a large graph. Hence, we develop a fast computation method to reduce the time complexity. Reviewing our model settings, we find that the output of node i, g\u03b8(A,X)[i], can be computed as g\u03b8(A,X)[i] = \u2211 j\u2208Ni(d\u0302[i]d\u0302[j]) \u22121A\u0302[j,:]X\u03b8 = Z[i,:]\u03b8, where d\u0302 is the node degree vector, A\u0302 = A + I, Ni = {j|A\u0302[i,j] = 1}, and Z denotes the aggregated feature matrix which is crucial to the fast computation. We aim to reduce the time complexity from two perspectives: (1) reducing the time complexity of computing the score r\u0303t(u, v) for a specific edge (u, v); and (2) reducing the size of the candidate edge set Ct. From perspective (1), we compute flip(u,v)Zt incrementally based on Zt. Then we have g\u03b8t(flip(u,v)A t,X) = flip(u,v)Z t\u03b8t. Consequently, both \u2206Ltf (u, v) and \u2206Lt(u, v) can be obtained based on g\u03b8t(flip(u,v)At,X). Next, we discuss the computation of flip(u,v)Zt[i,:] into three cases and introduce them separately. We only discuss adding edge (u, v), and leave removing edge (u, v) in Appendix C.1. Case 1: If i \u2208 {u, v}, flip(u,v)Zt[i,:] = d\u0302t[i]\nd\u0302t [i]\n+1 (Zt[i,:] \u2212\nA\u0302t[i,:]X (d\u0302t [i] )2 ) + A\u0302t[i,:]X+X[j,:] (d\u0302t [i] +1)2 + A\u0302t[j,:]X+X[i,:] (d\u0302t [i] +1)(d\u0302t [j] +1) , where j \u2208 {u, v}\\{i}; Case 2: If i \u2208 N tu \u222a N tv\\{u, v},\nflip(u,v)Z t [i,:] = Z t [i,:]\u2212Ii\u2208N tu \u00b7(\nA\u0302t[u,:]X d\u0302t [i] d\u0302t [u] \u2212 A\u0302 t [u,:]X+X[v,:] d\u0302t [i] (d\u0302t [u] +1) )\u2212Ii\u2208N tv \u00b7( A\u0302t[v,:]X d\u0302t [i] d\u0302t [v] \u2212 A\u0302 t [v,:]X+X[u,:] d\u0302t [i] (d\u0302t [v] +1) ), where\nIi\u2208N = 1 if i \u2208 N , and Ii\u2208N = 0 otherwise; Case 3: If i \u0338\u2208 N tu \u222a N tv , flip(u,v)Zt[i,:] = Z t [i,:]. From perspective (2), we only choose the influential edges2 as Ct. Specifically, flipping (u, v) is highly likely to increase Lf only if it results in significant changes to the predictions of a large number of nodes. Based on the aforementioned discussions, flipping (u, v) can only affect the soft prediction of nodes in N tu \u222a N tv (case 1 and 2). Hence, we consider (u, v) as an influential edge if the predictions of nodes in N tu \u222a N tv are easy to be changed. To this end, we propose an importance score \u03c1t(u, v) and collect the edges corresponding to the top a importance scores into Ct. We have \u03c1t(u, v) = \u2211 i\u2208N tu\u222aN tv Mt \u2212 |Zt[i,:]\u03b8 t|, where Mt = maxi |Zt[i,:]\u03b8\nt|. When the value of Mt \u2212 |Zt[i,:]\u03b8\nt| is large, the prediction of node i is easy to be changed. We analyze the time complexity of our fast computation method as follows.\nProposition 2. The overall time complexity of G-FairAttack with the fast computation is O(d\u0304n2 + dxan), where d\u0304 denotes the average degree.\nCompared with computing max(u,v)\u2208Ct r\u0303t(u, v) directly, which has a complexity of O(n4), our fast computation approach distinctly improves the efficiency of the attack. Note that the time complexity of G-FairAttack can be further improved in practice by parallel computation and simpler ranking strategies. The proof of Proposition 2, detailed complexity analysis, and ways of further improving the complexity are provided in Appendix C.2."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we evaluate our proposed G-FairAttack. Specifically, we aim to answer the following questions through our experiments: RQ1: Can our proposed surrogate loss function represent the victim loss functions of different kinds of victim models? RQ2: Can G-FairAttack achieve unnoticeable utility change? RQ3: To what extent does our fast computation approach improve the efficiency of G-FairAttack? Due to the space limitation, we provide more experimental results and discussions on the results in Appendix F."
        },
        {
            "heading": "4.1 EXPERIMENTAL SETTINGS",
            "text": "The evaluation of attack methods includes two stages. In the first stage, we use an attack method to obtain the perturbed graph structure. In the second stage, for the fairness evasion attack, we train a test GNN (i.e., victim model) on the clean graph and compare the output of the test GNN\n2Flipping the influential edges is highly likely to increase the attacker\u2019s objective.\non the clean graph and on the perturbed graph; for the fairness poisoning attack, we train the test GNN on the clean graph to obtain the normal model and train the test GNN on the perturbed graph to obtain the victim model; then, we compare the output of the normal model on the clean graph and the output of the victim model on the perturbed graph. We adopt three prevalent real-world datasets, i.e., Facebook (Leskovec & Mcauley, 2012), Credit (Agarwal et al., 2021), and Pokec (Dai & Wang, 2021; Dong et al., 2022a) to test the effectiveness of G-FairAttack. For attack baselines, we choose a random attack method (Hussain et al., 2022; Zu\u0308gner et al., 2018), a state-of-the-art fairness attack method FA-GNN (Hussain et al., 2022), and two gradient-based methods Gradient Ascent and Metattack (Zu\u0308gner & Gu\u0308nnemann, 2019) (adapted from utility attacks). For test GNNs, we adopt a vanilla graph convolutional network (Kipf & Welling, 2017) and three different types of fairness-aware GNNs, Regularization (Zeng et al., 2021) (Reg) for \u2206dp(Y\u0302 , S), FairGNN (Dai & Wang, 2021) for I(Y\u0302 , S), and EDITS (Dong et al., 2022a) for W (Y\u0302 , S). More details about baselines and datasets are provided in Appendices E.1 and E.2. We choose two mostly adopted fairness metrics, demographic parity \u2206dp (Dwork et al., 2012) and equal opportunity \u2206eo (Hardt et al., 2016), to measure the fairness of test GNNs. Larger values of fairness metrics denote more bias. In addition, we also report the accuracy and AUC score to show the utility of test GNNs."
        },
        {
            "heading": "4.2 EFFECTIVENESS OF ATTACK",
            "text": "We compare G-FairAttack with three attack baselines in the fairness evasion attack and fairness poisoning attack (in Appendix F.1) settings on three prevalent real-world datasets. Specifically, to test the effectiveness of attack methods, we choose four different kinds of victim GNN models to test the degradation of fairness metric values after being attacked. The attack budget \u2206 for Facebook and Pokec is 5%, i.e., we can flip 5% \u00b7 |E| edges at most; for Credit, the budget is 1%. We ran each experiment three times with different random seeds and reported the mean value and the standard deviation. The experimental results are shown in Table 1 and Appendix F. We can observe that: (1). G-FairAttack is the most effective attack method that jeopardizes the fairness of all types of victim GNN models, especially for fairness-aware GNNs. (2). Compared with Gradient Ascent, G-FairAttack adds a fairness loss term (total variation loss) in the surrogate loss. Consequently, G-FairAttack outperforms them in attacking different types of fairness-aware GNNs, which demonstrates that our proposed surrogate loss helps our surrogate model learn from different types of fairness-aware victim models (RQ1). (3). Some baselines outperform G-FairAttack in attacking\nvanilla GCN because the surrogate model of G-FairAttack is trained with the surrogate loss, including a fairness loss term, while the victim GCN is trained without fairness consideration. This problem can be addressed by choosing a smaller value of \u03b1 (weight of Lf ). Despite that, G-FairAttack successfully reduces the fairness of GCN on most benchmarks. (4). Compared with gradient-based attacks (Gradient Ascent), G-FairAttack has less space complexity as it does not need to store a dense adjacency matrix.\n4.3 ABLATION STUDY\nEffectiveness of Surrogate Loss. To further answer RQ1, we compare GFairAttack with two variants with different surrogate losses to demonstrate the effectiveness of our proposed surrogate loss. Our proposed surrogate loss functionLs consists of two parts: utility loss term Lsu (CE loss) and\nfairness loss term Lsf (total variation loss). For the first baseline, we remove the fairness loss term from the surrogate loss and name it G-FairAttack-None. For the second baseline, we substitute the total variation loss with \u2206dp loss, named as G-FairAttack-\u2206dp. In our problem, if the surrogate loss is the same as the victim loss (like white-box attacks), the attack method would definitely perform well. However, we show that G-FairAttack (with our proposed total variation surrogate loss) has the most desirable performance even with a different victim loss from the surrogate loss, which verifies the effectiveness of our surrogate loss. Hence, we choose FairGNN and EDITS as the victim models because their victim loss functions differ from the surrogate loss functions of G-FairAttack-None and G-FairAttack-\u2206dp. We conduct the experiment on Credit dataset in the fairness poisoning attack setting. The results are shown in Table 2. It is demonstrated that all three attack methods successfully increase the value of fairness metrics. Among all three attack methods, G-FairAttack achieves the best performance, which demonstrates that our proposed surrogate loss helps the surrogate model learn knowledge from various types of fairness-aware GNNs.\nEffectiveness of Unnoticeable Constraints. To answer RQ2, we investigate the impact of utility constraints on G-FairAttack. We remove the discrete projected update strategy and the utility constraint in G-FairAttack as GFairAttack-C. We compare the variation of \u2206dp and the utility loss on the training set after the attack by G-FairAttack, G-FairAttack-C, and FA-GNN. We chose two mostly adopted victim models, i.e., GCN and regularization, to test the attack methods on Facebook. We set the utility budget at 5% of the utility loss on clean data. The results are shown in Figure 2. It demonstrates that: (1). Our G-FairAttack distinctly deteriorates the fairness of two victim models while keeping the variation of utility loss unno-\nticeable (< 0.5%). Therefore, G-FairAttack can attack the victim model with unnoticeable utility variation. (2). Removing the utility constraint and the discrete projected update strategy from GFairAttack, the variation of utility loss becomes much larger as the attack budget increases. The results of G-FairAttack-C verify the effectiveness of the fairness constraint and the discrete projected update strategy. (3). G-FairAttack and G-FairAttack-C deteriorate the fairness of victim models to a larger extent than FA-GNN. Because G-FairAttack is a sequential greedy method, it becomes more effective as the attack budget increases. Hence, G-FairAttack is more flexible than FA-GNN."
        },
        {
            "heading": "4.4 PARAMETER STUDY",
            "text": "In this subsection, we aim to answer RQ3 and show the impact of the parameter a (the threshold for fast computation) on the time cost for the attack and the test results for victim models. We conduct\nexperiments under different choices of a. For the simplicity of the following discussion, we take a as the proportion of edges considered in Ct (e.g., a = 1e\u22123 means we only consider the top 0.1% edges with the highest important score). We record the attacker\u2019s objective Lf under different thresholds during the optimization process as Figure 3(a) to show the impact of a of the surrogate model.\nWe also record the fairness metrics of regularization-based victim models attacked by G-FairAttack and the average time of GFairAttack in each iteration while choosing different values of a (in Figure 3(b)) to show the impact of a on the attack performance and efficiency. Figure 3(b) demonstrates the tradeoff between effectiveness and efficiency exists but is very tolerant on the effectiveness side. In the range [5e\u22124, 1e\u22122], the performances of G-FairAttack on the victim model are similar, while the time cost decreases distinctly when a gets smaller (the time cost of a = 5e\u22124 is 95% lower than a = 1e\u22122). Therefore, in practice,\nwe can flexibly choose a proper threshold to fit the efficiency requirement. In conclusion, our fast computation distinctly reduces G-FairAttack\u2019s time cost without compromising the performance."
        },
        {
            "heading": "5 RELATED WORKS",
            "text": "Adversarial Attack of GNNs. To improve the robustness of GNNs, we should first fully understand how to attack GNNs. Consequently, various attacking strategies of GNNs have been proposed in recent years. Zu\u0308gner et al. (2018) first formulated the poisoning attack problem on graph data as a bilevel optimization in the discrete domain and proposed the first poisoning attack method of GNNs with respect to a single target node in a gray-box setting based on the greedy strategy. Following the problem formulation in (Zu\u0308gner et al., 2018), researchers proposed many effective adversarial attack methods in different settings. Zu\u0308gner & Gu\u0308nnemann (2019) proposed the first untargeted poisoning attack method in a gray-box setting based on MAML (Finn et al., 2017). Chang et al. (2020a) proposed an untargeted evasion attack method in a black-box setting by attacking the low-rank approximation of the spectral graph filter. Wu et al. (2019) proposed an untargeted evasion attack method in a white-box setting and a corresponding defense method based on integrated gradients.\nAdversarial Attacks on Fairness. Many recent studies investigated the fairness attack problem on tabular data. Van et al. (2022) proposed an online poisoning attack framework on fairness based on gradient ascent. They adopted the convex relaxation (Zafar et al., 2017; Donini et al., 2018) to make the loss function differentiable. Mehrabi et al. (2021) proposed two types of poisoning attacks on fairness. One incorporates demographic information into the influence attack (Koh et al., 2022), and the other generates poisoned samples within the vicinity of a chosen anchor to skew the decision boundary. Solans et al. (2021) studied the fairness poisoning attack as a bilevel optimization and solved it by KKT relaxation with a novel initialization strategy. Chhabra et al. (2023) proposed a black-box attack and defense method for fair clustering. For graph data, Hussain et al. (2022) is the first to investigate the fairness attack problem, proposing FA-GNN that randomly injects links among different sensitive groups to promote demographic parity. In a concurrent study, Kang et al. (2023) investigated the fairness attack problem as a bilevel optimization and proposed FATE, a metalearning-based fairness attack framework that targets both group fairness and individual fairness."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "Fairness-aware GNNs play a significant role in various human-centered applications. To protect GNNs from fairness attacks, we should fully understand potential ways to attack the fairness of GNNs. In this paper, we propose the first unnoticeable requirement for fairness attacks. We design a novel surrogate loss function to attack various types of fairness-aware GNNs. We also propose a sequential attack algorithm to solve the problem, and a fast computation approach to reduce the time complexity. Extensive experiments on three real-world datasets verify the efficacy of our method."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "Binchi Zhang and Jundong Li are supported by the Commonwealth Cyber Initiative award HV2Q23-003, JP Morgan Chase Faculty Research Award, and Cisco Faculty Research Award. Yada Zhu is supported in part by MIT-IBM Watson AI Lab \u2013 a research collaboration as part of the IBM AI Horizons Network."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "Table of Contents",
            "text": ""
        },
        {
            "heading": "A Proof 15",
            "text": "A.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\nA.2 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17"
        },
        {
            "heading": "B Attack Settings 18",
            "text": ""
        },
        {
            "heading": "C Fast Computation 19",
            "text": "C.1 Fast Computation Sketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nC.2 Complexity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21"
        },
        {
            "heading": "D G-FairAttack vs Gradient-based Methods 22",
            "text": "E Implementation Details 24\nE.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\nE.2 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\nE.3 Experimental Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\nE.4 Required Packages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26"
        },
        {
            "heading": "F Supplementary Experiments 27",
            "text": "F.1 Effectiveness of Attack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\nF.2 Effectiveness of Surrogate Loss . . . . . . . . . . . . . . . . . . . . . . . . . . 27\nF.3 Attack Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\nF.4 Attack Generalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nF.5 Advanced Attack Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31"
        },
        {
            "heading": "G Defense Against Fairness Attacks of GNNs 32",
            "text": "H Broader Impact 34"
        },
        {
            "heading": "A PROOF",
            "text": ""
        },
        {
            "heading": "A.1 PROOF OF THEOREM 1",
            "text": "Theorem 1. We have \u2206dp(Y\u0302 , S) and W (Y\u0302 , S) upper bounded by TV (Y\u0302 , S). Moreover, I(Y\u0302 , S) is also upper bounded by TV (Y\u0302 , S) if \u2200z \u2208 [0, 1], PY\u0302 (z) \u2265 \u03a0iPr(S = i) holds.\nProof. \u2206dp(Y\u0302 , S), I(Y\u0302 , S), W (Y\u0302 , S), and TV (Y\u0302 , S) are all non-negative. We first prove that \u2206dp(Y\u0302 , S) is upper bounded by TV (Y\u0302 , S). Recall that TV (Y\u0302 , S) = \u222b 1 0 |PY\u0302 |S=0(z) \u2212 PY\u0302 |S=1(z)|dz, we have\n\u2206dp(Y\u0302 , S) = \u2223\u2223\u2223\u2223Pr(Y\u0302 \u2265 12 | S = 0 ) \u2212 Pr ( Y\u0302 \u2265 1 2 | S = 1 )\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 \u222b 1\n1 2\nPY\u0302 |S=0(z)\u2212 PY\u0302 |S=1(z)dz \u2223\u2223\u2223\u2223\u2223 \u2264 \u222b 1\n1 2 \u2223\u2223\u2223PY\u0302 |S=0(z)\u2212 PY\u0302 |S=1(z)\u2223\u2223\u2223 dz \u2264 \u222b 1 0\n\u2223\u2223\u2223PY\u0302 |S=0(z)\u2212 PY\u0302 |S=1(z)\u2223\u2223\u2223 dz =TV (Y\u0302 , S).\nNext, we prove that W (Y\u0302 , S) is also upper bounded by TV (Y\u0302 , S).\nW (Y\u0302 , S) = \u222b 1 0 \u2223\u2223\u2223F\u22121 Y\u0302 |S=0 (y)\u2212 F\u22121 Y\u0302 |S=1 (y) \u2223\u2223\u2223 dy\n= \u222b 1 0 \u2223\u2223\u2223FY\u0302 |S=0(z)\u2212 FY\u0302 |S=1(z)\u2223\u2223\u2223 dz. (6) This equation holds because we know that FY\u0302 |S=0(0) = FY\u0302 |S=1(0) = 0 and FY\u0302 |S=0(1) = FY\u0302 |S=1(1) = 1 according to the property of cumulative distribution function and the fact that Y\u0302 \u2208 [0, 1]. Hence y = FY\u0302 |S=0(z) and y = FY\u0302 |S=1(z) form a closed curve in [0, 1] \u00d7 [0, 1] in z-y plane. Consequently, Equation (6) could be seen as computing the area of the closed curve from the y-axis and z-axis separately. Consequently, we have\nW (Y\u0302 , S) = \u222b 1 0 \u2223\u2223\u2223FY\u0302 |S=0(z)\u2212 FY\u0302 |S=1(z)\u2223\u2223\u2223 dz =\n\u222b 1 0 \u2223\u2223\u2223\u2223\u222b x 0 PY\u0302 |S=0(z)dz \u2212 \u222b x 0 PY\u0302 |S=1(z)dz \u2223\u2223\u2223\u2223 dx \u2264 \u222b 1 0 \u222b x 0\n\u2223\u2223\u2223PY\u0302 |S=0(z)dz \u2212 PY\u0302 |S=1(z)\u2223\u2223\u2223 dzdx =\n\u222b x\u2032 0 \u2223\u2223\u2223PY\u0302 |S=0(z)dz \u2212 PY\u0302 |S=1(z)\u2223\u2223\u2223 dz, x\u2032 \u2208 [0, 1] \u2264 \u222b 1 0\n\u2223\u2223\u2223PY\u0302 |S=0(z)dz \u2212 PY\u0302 |S=1(z)\u2223\u2223\u2223 dz =TV (Y\u0302 , S).\nFinally, we prove that I(Y\u0302 , S) is upper bounded by TV (Y\u0302 , S) if \u2200z \u2208 [0, 1], PY\u0302 (z) \u2265 \u03a0iPr(S = i). First, we have\nI(Y\u0302 , S) = \u222b 1 0 \u2211 i PY\u0302 ,S(z, i) log PY\u0302 ,S(z, i) PY\u0302 (z)Pr(S = i) dz\n= \u222b 1 0 \u2211 i Pr(S = i)PY\u0302 |S=i(z) log PY\u0302 |S=i(z) PY\u0302 (z) dz.\nLet Pi = Pr(S = i) for i = 0, 1, then we have P0 + P1 = 1 and PY\u0302 (z) = P0PY\u0302 |S=0(z) + P1PY\u0302 |S=1(z). According to the fact that log x \u2264 x \u2212 1 for x \u2208 (0, 1], we let x = PY\u0302 |S=i(z)\nPY\u0302 (z) and\nhave\nI(Y\u0302 , S) = \u222b 1 0 \u2211 i PiPY\u0302 |S=i(z) log PY\u0302 |S=i(z) PY\u0302 (z) dz\n\u2264 \u222b 1 0 \u2211 i Pi\n( PY\u0302 |S=i(z) )2 PY\u0302 (z) \u2212 PiPY\u0302 |S=i(z)dz\n= \u222b 1 0 \u2211 i Pi PY\u0302 |S=i(z)\n( PY\u0302 |S=i(z)\u2212 PY\u0302 (z) ) PY\u0302 (z) dz\n= \u222b 1 0\nP0P1PY\u0302 |S=0(z) ( PY\u0302 |S=0(z)\u2212 PY\u0302 |S=1(z) ) P0PY\u0302 |S=0(z) + P1PY\u0302 |S=1(z)\n+ P0P1PY\u0302 |S=1(z)\n( PY\u0302 |S=1(z)\u2212 PY\u0302 |S=0(z) ) P0PY\u0302 |S=0(z) + P1PY\u0302 |S=1(z) dz\n= \u222b 1 0 P0P1\n( PY\u0302 |S=0(z)\u2212 PY\u0302 |S=1(z) )2 P0PY\u0302 |S=0(z) + P1PY\u0302 |S=1(z) dz.\n(7)\nGiven that PY\u0302 (z) \u2265 \u03a0iPr(S = i) \u2200z \u2208 [0, 1], we have P0PY\u0302 |S=0(z)+P1PY\u0302 |S=1(z) \u2265 P0P1(P0+ P1) = P0P1. Consequently, we have\nI(Y\u0302 , S) \u2264 \u222b 1 0 ( PY\u0302 |S=0(z)\u2212 PY\u0302 |S=1(z) )2 dz.\nConsidering that the training of fairness-aware GNNs makes the distributions PY\u0302 |S=0(z) and PY\u0302 |S=1(z) closer, we assume that |PY\u0302 |S=0(z) \u2212 PY\u0302 |S=1(z)| \u2264 1 for z \u2208 [0, 1]. To verify this assumption, we conduct numerical experiments on all three adopted datasets. Following our methodology, we use the kernel density estimation to estimate the distribution functions PY\u0302 |S=0(z) and PY\u0302 |S=1(z) and compute the value of |PY\u0302 |S=0(z)\u2212PY\u0302 |S=1(z)| consequently. We record the largest value of |PY\u0302 |S=0(z) \u2212 PY\u0302 |S=1(z)| for z \u2208 [0, 1] and obtain the results as 0.1372 \u00b1 0.0425 for Facebook, 0.0999\u00b1 0.0310 for Pokec z, and 0.0356\u00b1 0.0074 for Credit (mean value and standard deviation under 5 random seeds), which are all far less than 1. Then, we come to\nI(Y\u0302 , S) \u2264 \u222b 1 0 \u2223\u2223\u2223PY\u0302 |S=0(z)\u2212 PY\u0302 |S=1(z)\u2223\u2223\u2223 dz = TV (Y\u0302 , S). (8) In conclusion, we have proved that \u2206dp(Y\u0302 , S) and W (Y\u0302 , S) are upper bounded by TV (Y\u0302 , S). Moreover, I(Y\u0302 , S) is also upper bounded by TV (Y\u0302 , S) if \u2200z \u2208 [0, 1], PY\u0302 (z) \u2265 \u03a0iPr(S = i) holds.\nRemarks on Theorem 1. It is worth noting that I(Y\u0302 , S) \u2264 TV (Y\u0302 , S) stems from the condition of PY\u0302 (z) \u2265 \u03a0iPr(S = i), \u2200z \u2208 [0, 1]. Although we are not able to always ensure the correctness of the condition in practice, we can still obtain from Theorem 1 that (1) the probability of the condition holds grows larger when the number of sensitive groups increases; (2) even in the binary case, the condition is highly likely to hold in practice, considering that Pr(S = 0) \u00b7 Pr(S = 1) \u2264 14 (In a binary case, we have Pr(s = 0) + Pr(s = 1) = 1; hence, Pr(s = 0)Pr(s = 1) = Pr(s = 0)(1\u2212 Pr(s = 0)) \u2264 1/4).\nTo further improve the soundness of our theoretical analysis, we can slightly loosen the condition PY\u0302 (z) \u2265 \u03a0iPr(S = i) \u2200z \u2208 [0, 1] and obtain a new condition\u222b 1 0 ( P0P1\nP0PY\u0302 |S=0(z)+P1PY\u0302 |S=1(z)\n)2 dz \u2264 1. Consider the last step of Equation (7). According to the\nCauchy-Schwartz inequality, we have I(Y\u0302 , S) \u2264 \u222b 1 0 P0P1 ( PY\u0302 |S=0(z)\u2212 PY\u0302 |S=1(z) )2 P0PY\u0302 |S=0(z) + P1PY\u0302 |S=1(z) dz\n\u2264 \u222b 1 0 ( P0P1 P0PY\u0302 |S=0(z) + P1PY\u0302 |S=1(z) )2 dz \u00b7 \u222b 1 0 ( PY\u0302 |S=0(z)\u2212 PY\u0302 |S=1(z) )4 dz  12\n\u2264 (\u222b 1\n0\n( PY\u0302 |S=0(z)\u2212 PY\u0302 |S=1(z) )4 dz ) 1 2\n\u2264 \u221a TV (Y\u0302 , S).\nConsequently, we obtain a variant of Theorem 1 as follows. Theorem A 1. I(Y\u0302 , S) is upper bounded by \u221a TV (Y\u0302 , S), if \u222b 1 0 ( P0P1\nP0PY\u0302 |S=0(z)+P1PY\u0302 |S=1(z)\n)2 dz \u2264"
        },
        {
            "heading": "1 holds.",
            "text": "According to Theorem A 1, we find a looser upper bound for I(Y\u0302 , S) (still dependent on TV (Y\u0302 , S)) based on a weaker condition. In addition, Theorem A 1 is able to support our total variation loss as well, since we still have I(Y\u0302 , S) approaches 0 when TV (Y\u0302 , S) approaches 0 after training. Similar as the assumption |PY\u0302 |S=0(z) \u2212 PY\u0302 |S=1(z)| \u2264 1, we conduct numerical experiments with kernel density estimation for estimating PY\u0302 |S=0(z) and PY\u0302 |S=1(z) and numerical integral for computing\nthe value of \u222b 1 0 ( P0P1\nP0PY\u0302 |S=0(z)+P1PY\u0302 |S=1(z)\n)2 dz. We obtain the results of the integral as 0.8621 \u00b1\n0.1110 for Facebook, 0.4568 \u00b1 0.0666 for Pokec z, and 0.5934 \u00b1 0.0763 for Credit (mean value and standard deviation under 5 random seeds). Experimental results verify the feasibility of the condition in Theorem A 1."
        },
        {
            "heading": "A.2 PROOF OF THEOREM 2",
            "text": "Theorem 2. The optimal poisoned adjacency matrix At+1 in the t + 1-th iteration given by PGD, i.e., the solution of At+1 = argmin|L(At)\u2212L(A\u2032)|\u2264\u03f5t\u2225A \u2032 \u2212 (At + \u03b7\u2207Lf (At))\u22252F is\nAt+1 =  At + \u03b7\u2207ALf (At), if \u03b7|\u2207AL(At)T\u2207ALf (At)| \u2264 \u03f5t, At + \u03b7\u2207ALf (At) + et\u03f5t \u2212 \u03b7\u2207AL(At)T\u2207ALf (At)\n\u2225\u2207AL(At)\u22252F \u2207AL(At), otherwise,\n(9)\nwhere et = sign ( \u2207AL(At)T\u2207ALf (At) ) .\nProof. First, we know that A\u2032 = At is a feasible solution because L(At) \u2212 L(At) = 0 \u2264 \u03f5t. Hence we assume that A\u2032 is close to At. Consequently, we use the first-order Taylor expansion to substitute the constraint |L(A\u2032)\u2212 L(At)| \u2264 \u03f5t as \u2223\u2223\u2207L(At)T (A\u2032 \u2212At)\u2223\u2223 \u2264 \u03f5t. For simplicity, we vectorize the adjacency matrices At and A\u2032 here such that At,A\u2032 \u2208 Rn2 . Next, we let A\u2032 = At + \u03b7\u2207Lf (At) + \u03be, and convert the optimization problem as follows.\nAt+1 = argmin |\u2207L(At)T (\u03b7\u2207Lf (At)+\u03be)|\u2264\u03f5t \u2225\u03be\u222522. (10)\nThen we discuss the new constraint in Equation (10) |\u03b7\u2207L(At)T\u2207Lf (At) +\u2207L(At)T \u03be| \u2264 \u03f5t in different conditions.\n(1). When |\u03b7\u2207L(At)T\u2207Lf (At)| \u2264 \u03f5t, we can easily obtain the optimal solution as \u03be = 0.\n(2). When \u03b7\u2207L(At)T\u2207Lf (At) \u2265 \u03f5t, then we have\n\u2212\u03f5t \u2212 \u03b7\u2207L(At)T\u2207Lf (At) \u2264 \u2207L(At)T \u03be \u2264 \u03f5t \u2212 \u03b7\u2207L(At)T\u2207Lf (At).\nBecause \u2207L(At)T \u03be = \u2225\u2207L(At)\u22252 \u00b7 \u2225\u03be\u22252 \u00b7 cos \u03b8, where \u03b8 is the angle of \u2207L(At) and \u03be. To minimize \u2225\u03be\u22252, we minimize cos \u03b8 as cos \u03b8 = \u22121, i.e., \u03be = \u2212\u2225\u03be\u22252 \u00b7 \u2207L(At)/\u2225\u2207L(At)\u22252, and then have\n\u2212\u03f5t + \u03b7\u2207L(At)T\u2207Lf (At) \u2225\u2207L(At)\u22252 \u2264 \u2225\u03be\u22252 \u2264 \u03f5t + \u03b7\u2207L(At)T\u2207Lf (At) \u2225\u2207L(At)\u22252 .\nTherefore, the solution of Equation (10) is\n\u03be = \u03f5t \u2212 \u03b7\u2207L(At)T\u2207Lf (At)\n\u2225\u2207L(At)\u222522 \u2207L(At).\n(3). When \u03b7\u2207L(At)T\u2207Lf (At) \u2264 \u2212\u03f5t, we also have\n\u2212\u03f5t \u2212 \u03b7\u2207L(At)T\u2207Lf (At) \u2264 \u2207L(At)T \u03be \u2264 \u03f5t \u2212 \u03b7\u2207L(At)T\u2207Lf (At).\nDifferent from condition (2), the left-hand side and right-hand side here are both positive. Similarly, we let cos \u03b8 = 1 and obtain the solution of Equation (10) as\n\u03be = \u2212\u03f5t \u2212 \u03b7\u2207L(At)T\u2207Lf (At)\n\u2225\u2207L(At)\u222522 \u2207L(At).\nCombine the aforementioned three conditions into A\u2032 = At + \u03b7\u2207Lf (At) + \u03be, then we have the solution as follows\nAt+1 =  At + \u03b7\u2207ALf (At), if \u03b7|\u2207AL(At)T\u2207ALf (At)| \u2264 \u03f5t, At + \u03b7\u2207ALf (At) + et\u03f5t \u2212 \u03b7\u2207AL(At)T\u2207ALf (At)\n\u2225\u2207AL(At)\u22252F \u2207AL(At), otherwise,\nwhere et = sign ( \u2207AL(At)T\u2207ALf (At) ) ."
        },
        {
            "heading": "B ATTACK SETTINGS",
            "text": "In this section, we introduce our attack settings in detail from three perspectives, the attacker\u2019s goal, the attacker\u2019s knowledge, and the attacker\u2019s capability.\nAttacker\u2019s Goal. There are two different settings of our problem, the fairness evasion attack, and the fairness poisoning attack. In the fairness evasion attack, the attacker\u2019s goal is to let the victim model make unfair predictions on test nodes, where the victim model is trained with fairness consideration on the clean graph. Note that it is possible for real-world attackers to attack the access control of the databases to modify the input graph data, especially for edge computing systems with a coarse-grained access control (Ali et al., 2016; Xiao et al., 2019). In addition, once the model is deployed, the attacker can launch evasion attacks at any time, which increases the difficulty and cost of defending against evasion attacks (Zhang et al., 2022). Considering the severe impact of evasion attacks, many prevalent existing works Dai et al. (2018); Zu\u0308gner et al. (2018); Zu\u0308gner & Gu\u0308nnemann (2019); Zhang et al. (2022) make great efforts to study evasion attacks. In the fairness poisoning attack, the attacker\u2019s goal is to let the victim model make unfair predictions on test nodes, where the victim model is trained with fairness consideration on the poisoned graph. For both settings, we use commonly used fairness metrics, e.g., demographic parity (Dwork et al., 2012) and equal opportunity (Hardt et al., 2016) to measure the fairness of predictions.\nAttacker\u2019s Knowledge. To make our attack practical in the real world, we set several limitations on the attacker\u2019s knowledge and formulate the attack within a gray-box setting. It is worth noting that our attacker\u2019s knowledge basically follows previous attacks on prediction utility of GNNs (Wu et al., 2019; Xu et al., 2019; Zu\u0308gner & Gu\u0308nnemann, 2019; Chang et al., 2020a; Ma et al., 2020; Li et al., 2022a; Ma et al., 2022; Lin et al., 2022). Specifically, the attacker is able to observe the node attributes X, graph structure A, ground truth labels Y , and sensitive attribute value set S, but cannot\nobserve the victim GNN model f\u03b8. Therefore, the attackers need to exploit a surrogate model g\u03b8 to achieve their goal.\nIt is worth noting that our method can be directly adapted to a white-box setting by replacing the trained surrogate model in the attacker\u2019s objective with the true victim model in Problem 1. In contrast, designing fairness attacks in a black-box attack setting can be extremely challenging. The difference between gray-box attacks and black-box attacks is black-box attackers are not allowed to access the ground truth labels. Different from node embeddings which can be obtained in an unsupervised way, group fairness metrics have to rely on the ground truth labels, which makes existing black-box attacks on graphs difficult to adapt to fairness attacks. Despite the difficulty of black-box fairness attacks, we provide an initial step toward a potential way to extend our framework to a black-box setting. First, the attacker can collect some data following a similar distribution, i.e., if the original graph is a Citeseer citation network, the attacker can collect data from Arxiv; if the original graph is a Facebook social network, the attacker can collect data from Twitter (X). During the data collection (preprocessing), the dimension of collected node features should be aligned with the original graph. Then, the attacker can train a state-of-the-art inductive GNN model on the collected graph data and obtain the predicted labels on the original graph. Finally, the attacker can use the predicted labels as a pseudo label to implement our G-FairAttack on the original graph.\nAttacker\u2019s Capability. Between attacking the graph structure and the node attributes, we only consider the structure attack as the structure perturbation in the discrete domain is more challenging to solve and the structure attack can be easily adapted to obtain the attribute attack. Hence, we consider that attackers can only modify the graph structure A, i.e., adding new edges or cutting existing edges, consistent with many previous attacks on prediction utility of GNNs (Dai et al., 2018; Xu et al., 2019; Zu\u0308gner & Gu\u0308nnemann, 2019; Wang & Gong, 2019; Bojchevski & Gu\u0308nnemann, 2019; Chang et al., 2020a). In addition, the structure perturbation should be unnoticeable. Existing attacks of GNNs (Zu\u0308gner et al., 2018; Zu\u0308gner & Gu\u0308nnemann, 2019; Dai et al., 2018; Bojchevski & Gu\u0308nnemann, 2019) proposed the following unnoticeable constraints: \u2206 edges are changed at most; there are no singleton nodes, i.e., nodes without neighbors after the attack; the degree distributions before and after the attack should be the same with high confidence. We follow these works to ensure the unnoticeability of our attack. More importantly, we propose an extra unnoticeable constraint to ensure the difference of the utility losses before and after the attack is less than \u03f5. This unnoticeable utility constraint makes attacks on fairness difficult to recognize.\nAfter clarifying detailed attack settings, we use Figure 1 to illustrate a toy example of our proposed attack problem. In Figure 1, We use squares to denote the sensitive group 0 and triangles to denote the sensitive group 1. We use blue to label class-0 nodes and orange to label class-1 nodes. We compute the demographic parity and the equal opportunity metrics (larger value means less fair) to evaluate the fairness of the model prediction. By modifying two edges (from left to right), the attacker can let the fairness-aware GNN make unfair predictions while preserving the accuracy."
        },
        {
            "heading": "C FAST COMPUTATION",
            "text": ""
        },
        {
            "heading": "C.1 FAST COMPUTATION SKETCH",
            "text": "The goal of fast computation is to solve the problem (ut, vt)\u2190 argmax(u,v)\u2208Ct r\u0303t(u, v) efficiently. According to the definition of r\u0303t(u, v), the computation of r\u0303t(u, v) depends on two loss functions: the attacker\u2019s objective Lf and the utility loss L. Between them, Lf can be formulated as\nLf (g\u03b8\u2217 ,A,X,Y,Vtest,S) = \u2223\u2223\u2223\u2223\u2223 \u2211 i\u2208Vtest kiI\u22650 ( g\u03b8\u2217 (A,X)[i] )\u2223\u2223\u2223\u2223\u2223 , (11) where ki = 1/ |V0 \u2229 Vtest| if i \u2208 V0 and ki = \u22121/ |V1 \u2229 Vtest| if i \u2208 V1. I\u22650(\u00b7) denotes an indicator function where I\u22650(x) = 1 if x \u2265 0 and I\u22650(x) = 0 otherwise. The other L can be formulated as\nL(g\u03b8\u2217 ,A,X,Y,Vtrain) = \u2212 1 |Vtrain| \u2211\ni\u2208Vtrain\nyi log ( \u03c3(g\u03b8\u2217(A,X)[i]) ) + (1\u2212 yi) log ( 1\u2212 \u03c3(g\u03b8\u2217(A,X)[i]) ) ,\n(12)\nAlgorithm 1 G-FairAttack: A Sequential Attack on Fairness of GNNs. Input: Clean adjacency matrix A, attribute matrix X, attack budget \u2206, utility budget \u03f5. Output: The solution of Problem 1: A\u2217. 1: t\u2190 0, C0 \u2190 E , A0 \u2190 A 2: \u03b80 \u2190 argmin\u03b8 Ls (g\u03b8,A,X,Y,S) 3: while t \u2264 \u2206 and\n\u2223\u2223L(g\u03b8t ,At)\u2212 L(g\u03b80 ,A0)\u2223\u2223 \u2264 \u03f5 do 4: (ut, vt)\u2190 argmax(u,v)\u2208Ct r\u0303t(u, v), according to Algorithm 2. 5: At+1 \u2190 flip(ut,vt)At\n6: \u03b8t+1 \u2190 { \u03b8t, Evasion argmin\u03b8 Ls ( g\u03b8,A t+1,X,Y,S ) , Poisoning 7: Ct+1 \u2190 Ct\\{(ut, vt), (vt, ut)} 8: t\u2190 t+ 1 9: end while\n10: A\u2217 \u2190 At\nwhere \u03c3(\u00b7) denotes the sigmoid function. In the t-th iteration of our sequential attack, to compute the score function r\u0303t(u, v) efficiently, we should reduce the complexity of computing both \u2206Lt(u, v) and \u2206Ltf (u, v). Specifically, our solution is to compute flip(u,v)Zt incrementally and obtain g\u03b8t(flip(u,v)At,X) = flip(u,v)Zt\u03b8t. Then we can compute \u2206Lf (u, v) and \u2206L(u, v) according to Equation (11) and Equation (12). We first review the computation of flip(u,v)Zt when a new edge (u, v) is added.\nCase 1: If i \u2208 {u, v}, we can compute flip(u,v)Zt[i,:] as\nflip(u,v)Z t [i,:] =\nd\u0302t[i]\nd\u0302t[i] + 1 (Zt[i,:] \u2212\nA\u0302t[i,:]X (d\u0302t[i]) 2 ) + A\u0302t[i,:]X+X[j,:] (d\u0302t[i] + 1) 2 + A\u0302t[j,:]X+X[i,:] (d\u0302t[i] + 1)(d\u0302 t [j] + 1) , (13)\nwhere j = v if i = u and j = u otherwise;\nCase 2: If i \u2208 N tu \u222aN tv\\{u, v}, we can compute flip(u,v)Zt[i,:] as\nflip(u,v)Z t [i,:] = Z t [i,:]\u2212Ii\u2208N tu \u00b7(\nA\u0302t[u,:]X\nd\u0302t[i]d\u0302 t [u]\n\u2212 A\u0302t[u,:]X+X[v,:]\nd\u0302t[i](d\u0302 t [u] + 1)\n)\u2212Ii\u2208N tv \u00b7( A\u0302t[v,:]X\nd\u0302t[i]d\u0302 t [v]\n\u2212 A\u0302t[v,:]X+X[u,:]\nd\u0302t[i](d\u0302 t [v] + 1)\n),\n(14) where Ii\u2208N = 1 if i \u2208 N , and Ii\u2208N = 0 otherwise; Case 3: If i \u0338\u2208 N tu \u222aN tv , we have flip(u,v)Zt[i,:] = Z t [i,:].\nNext, we introduce the computation of flip(u,v)Zt when an existing edge (u, v) is removed. Similarly, we divide the computation into three cases as follows.\nCase 1: If i \u2208 {u, v}, we can compute flip(u,v)Zt[i,:] as\nflip(u,v)Z t [i,:] =\nd\u0302t[i]\nd\u0302t[i] \u2212 1 (Zt[i,:] \u2212\nA\u0302t[i,:]X (d\u0302t[i]) 2 \u2212 A\u0302t[j,:]X d\u0302t[i]d\u0302 t [j] ) + A\u0302t[i,:]X\u2212X[j,:] (d\u0302t[i] \u2212 1)2 , (15)\nwhere j = v if i = u and j = u otherwise;\nCase 2: If i \u2208 N tu \u222aN tv\\{u, v}, we can compute flip(u,v)Zt[i,:] as\nflip(u,v)Z t [i,:] = Z t [i,:]\u2212Ii\u2208N tu \u00b7(\nA\u0302t[u,:]X\nd\u0302t[i]d\u0302 t [u] \u2212 A\u0302t[u,:]X\u2212X[v,:] d\u0302t[i](d\u0302 t [u] \u2212 1) )\u2212Ii\u2208N tv \u00b7( A\u0302t[v,:]X d\u0302t[i]d\u0302 t [v] \u2212 A\u0302t[v,:]X\u2212X[u,:] d\u0302t[i](d\u0302 t [v] \u2212 1) ),\n(16) where Ii\u2208N = 1 if i \u2208 N , and Ii\u2208N = 0 otherwise; Case 3: If i \u0338\u2208 N tu \u222aN tv , we have flip(u,v)Zt[i,:] = Z t [i,:].\nThe overall fast computation algorithm is shown in Algorithm 2, where argmax@a\u03c1 t(u, v) is denoted as the set of (u, v) corresponding to the top-a elements of \u03c1t(u, v).\nAlgorithm 2 The fast computation algorithm of (ut, vt).\nInput: Adjacency matrix At, attribute matrix X, output matrix Zt, degree vector d\u0302t, product matrix A\u0302tX, model parameter \u03b8t. Output: Target edge (ut, vt) in Algorithm 1. 1: Ct \u2190 argmax@a\n(u,v)\u2208Et \u03c1t(u, v).\n2: k \u2190 0, pt \u2190 0, qt \u2190 0. 3: for (u, v) \u2208 Ct do 4: flip(u,v)Z\nt \u2190 Zt 5: for i \u2208 N tu \u222aN tv do 6: if i \u2208 {u, v} then 7: Update flip(u,v)Zt[i,:] according to Equation (13) or Equation (15). 8: else 9: Update flip(u,v)Zt[i,:] according to Equation (14) or Equation (16).\n10: end if 11: end for 12: g\u03b8t(flip(u,v)A\nt,X)\u2190 flip(u,v)Zt\u03b8t 13: Compute L(flip(u,v)At,X) and Lf (flip(u,v)At,X) according to Equation (11) and Equa-\ntion (12). 14: pt[k] \u2190 L(flip(u,v)A\nt,X)\u2212 L(At,X) 15: qt[k] \u2190 Lf (flip(u,v)A\nt,X)\u2212 Lf (At,X) 16: k \u2190 k + 1 17: end for 18: r\u0303t \u2190 qt \u2212 (p t)T qt\n\u2225pt\u222522 pt\n19: imax \u2190 argmaxi=0,...,|Ct|\u22121 r\u0303t[i] 20: (ut, vt)\u2190 Ct[imax]"
        },
        {
            "heading": "C.2 COMPLEXITY ANALYSIS",
            "text": "Based on Algorithm 2, we provide a detailed complexity analysis for our proposed G-FairAttack.\nProposition 2. The overall time complexity of G-FairAttack with the fast computation is O(d\u0304n2 + dxan), where d\u0304 denotes the average degree.\nProof. First, to compute Ct, we compute |Zt\u03b8t| and find the maximum Mt in O(dxn). Then we compute \u03c1t(u, v) = \u2211 i\u2208N tu\u222aN tv Mt \u2212 |Zt[i,:]\u03b8 t| for (u, v) \u2208 Et in O(d\u0304n2), and find the top-a elements as Ct in O(n log a). Then, the computation of pt and qt can be divided into the following steps for each edge (u, v) \u2208 Ct.\n1. The computation of flip(u,v)Zt[i,:] for i \u2208 N t u \u222a N tv . We can store and update A\u0302tX, d\u0302t, and Zt\nfor each iteration. Hence, the computation of Equation (13), Equation (14), Equation (15), and Equation (16) only requires O(1). Consequently, the total time complexity of this step is O(d\u0304).\n2. The computation of g\u03b8t(flip(u,v)At,X) = flip(u,v)Zt\u03b8t requires O(ndx).\n3. According to Equation (11) and Equation (12), the computation of the loss functions L(flip(u,v)At,X) and Lf (flip(u,v)At,X) based on g\u03b8t(flip(u,v)At,X) requires O(n).\nConsidering all edges from Ct, the computation of pt and qt requires O(dxan). Finally, we can compute r\u0303t and find (ut, vt) in O(a). Combining all these steps, the complexity of Algorithm 2 is O(d\u0304n2 + dxan) in total.\nWith our fast computation method, the total time complexity of G-FairAttack is O((d\u0304n2+dxan)\u2206). Here, the retraining of the surrogate model \u03b8t+1 = argmin\u03b8 Ls ( g\u03b8,A t+1,X,Y,S )\nin the fairness poisoning attack is neglected because the convergence is not controllable. The overall G-FairAttack\nalgorithm is shown in Algorithm 1. We can obtain the space complexity of G-FairAttack as O(|E|+ dxn).\nNext, we show that our complexity can be further reduced in practice. We list two potential ways as follows.\n1. The first approach is to implement our fast computation algorithm in parallel. As the proof of Proposition 2 shows, the main part of G-FairAttack\u2019s complexity is the computation of \u03c1t(u, v) for (u, v) \u2208 Et, which has O(d\u0304n2) complexity. It is distinct that this computation can be implemented in parallel where Et is partitioned into p subsets, and each subset is fed into one process. By exploiting parallel computation, the overall complexity can be reduced to O(d\u0304n2/p).\n2. Instead of ranking all of the edges in Et by \u03c1t(u, v), we can just randomly sample a edges from Et as Ct. By using random sampling instead of ranking, the overall complexity can be reduced to O(dxan). However, the error of the fast computation might increase without a careful choice of the sampling distribution.\nNote that most existing adversarial attack approaches (Zu\u0308gner et al., 2018; Zu\u0308gner & Gu\u0308nnemann, 2019; Bojchevski & Gu\u0308nnemann, 2019; Lin et al., 2022) do not have a lower complexity (less than O(n2)) than our proposed G-FairAttack. The adversarial attacks with an O(n2) complexity can already fit most commonly used graph datasets. In general, our method is practical for most graph datasets as the existing literature. For extremely large datasets, we can also use the aforementioned strategies to reduce further the time complexity.\nFinally, we would like to make a more detailed comparison of the time complexity with adopted attacking baselines. The random sampling-based baselines, i.e., random and FA-GNN, definitely have lower time complexity (O(\u2206), \u2206 is the attack budget) because they are based on random sampling. Although their complexity is low, the effectiveness of random sampling-based attacks is very limited. For the rest gradient-based attack baselines, i.e., Gradient Ascent and Metattack, their time complexities are O(n2) (Zu\u0308gner & Gu\u0308nnemann, 2019), the same as G-FairAttack. However, gradient-based methods have a larger space complexity compared with G-FairAttack (O(n2) vs. O(dxn+ |E|)).\nD G-FAIRATTACK VS GRADIENT-BASED METHODS\nThe fairness attack of GNNs is a discrete optimization problem, which is highly challenging to solve. Most of the existing adversarial attacks that focus on the prediction utility of GNNs adopt gradient-based methods (Zu\u0308gner & Gu\u0308nnemann, 2019; Wu et al., 2019; Xu et al., 2019; Geisler et al., 2021) to find the maximum of the attacker\u2019s objective. As G-FairAttack, gradient-based structure attacks flip the edges sequentially. In the t-th iteration (t = 1, 2, . . . ), the gradient-based optimization algorithm finds a target edge (ut, vt) based on the gradient of the adjacency matrix \u2207AtL where L is the objective function of the attacker, and flips this target edge to obtain the update adjacency matrix At+1, which is expected to increase the attacker\u2019s objective. In particular, to obtain \u2207AtL, gradient-based methods extend the discrete adjacency matrix At \u2208 {0, 1}n\u00d7n to a continuous domain Rn\u00d7n and compute\u2207AtL \u2208 Rn\u00d7n. Specifically, for poisoning attacks where the problem becomes a bilevel opti-\nmization, existing methods exploit the meta-learning (Zu\u0308gner & Gu\u0308nnemann, 2019) or the convex relaxation (Xu et al., 2019) techniques to remove the inner optimization. After obtaining\u2207AtL, the gradient-based methods should update At in the discrete domain instead of using gradient ascent directly. Specifically, gradient-based methods choose the target edge corresponding to the largest element or use random sampling to find a target element At[u,v].\nAlthough existing attacks of GNNs based on gradient-based methods successfully decrease the prediction utility of victim models, they have two main limitations.\nThe first limitation is that we cannot ensure the loss function after flipping the target edge will increase because the update in the discrete domain brings an uncontrollable error. The intuition of the gradient-based method is that we cannot update the adjacency matrix with gradient ascent because the adjacency matrix is binary and only one edge can be flipped in each time. Hence, we expect that flipping the target edge corresponding to the largest gradient component can lead to the largest increment of the attacker\u2019s objective L(A,X). However, this expectation can be false since the update of the adjacency matrix (flipping one target edge) has a fixed length. Next, we prove Proposition 1.\nProposition 1. Gradient-based methods for optimizing the graph structure are not guaranteed to decrease the objective function.\nProof. Consider the loss function L(A,X) near a specific point A0 where A \u2208 Rn 2\nis a vectorized adjacency matrix. Based on Taylor\u2019s Theorem, we have\nL(A,X) = L(A0,X) +\u2207AL(A0,X)\u22a4(A\u2212A0) +R1(A),\nwhere R1(A) = h1(A)\u2225A \u2212A0\u2225 is the Peano remainder and we have limA\u2192A0 h1(A) = 0. For gradient-based methods, we have A1 = A0+ek where ek is the basis vector at the k-th dimension. Then, we have\nL(A1,X) = L(A0,X) +\u2207AL(A0,X)[k] + h1(A1).\nHere, we know that \u2207AL(A0,X)[k] is the largest positive element of \u2207AL(A0,X), which is a fixed value. Then, we expect that choosing A1 can lead to the fact that L(A1,X) > L(A0,X), i.e., \u2207AL(A0,X)[k] + h1(A1) > 0. However, this inequality is not true without further assumptions when \u2225A1 \u2212A0\u22250 = 1.\nIn comparison, we also show that the error can be controlled in the continuous domain by a careful selection of the learning rate. In the continuous domain, the situation is different because we can make the value of \u2225A1 \u2212 A0\u2225 arbitrarily small by tuning the learning rate \u03b7 where A1 = A0 + \u03b7\u2207AL(A0,X)[k]ek. Note that we have limA\u2192A0 h1(A) = 0 and the value of \u2207AL(A0,X)[k] is fixed. Hence we can choose a proper \u03b7 which makes \u2225A1\u2212A0\u2225 small enough to ensure |h1(A1)| < \u2207AL(A0,X)[k]. Finally, we have \u2207AL(A0,X)[k] + h1(A1) > 0 and L(A1,X) > L(A0,X) consequently. We also provide a two-dimensional case in Figure 4. In the iteration, the optimization starts at (0, 0). The gradient at (0, 0) is [1 0]\u22a4. According to the gradient-based methods, the next point should be at (1, 0). However, the loss after updating does not increase f(1, 0) = f(0, 0). Instead, (0, 1) is a better update for f(0, 1) > f(0, 0) in this iteration.\nThe second limitation is the large space complexity. The computation of \u2207AtL requires storing a dense adjacency matrix with O(n2) space complexity, which is costly at a large scale.\nIn contrast, our proposed G-FairAttack successfully addresses these limitations. First, G-FairAttack can ensure the increase of the attacker\u2019s objective after flipping the target edge since G-FairAttack exploits a ranking-based method to choose the target edge in each iteration. In particular, we choose the target edge as max(u,v)\u2208Ct rt(u, v) = Lf (g\u03b8t , f lip(u,v)At) \u2212 Lf (g\u03b8t ,At), which ensures that flipping the target edge can maximize the increment of attacker\u2019s objective. Second, unlike gradient-based methods, G-FairAttack does not require storing a dense adjacency matrix as no gradient computations are involved. Referring to the discussion in Appendix C.2, the space complexity of G-FairAttack is O(|E| + dxn), much lower than gradient-based methods (O(n2)). Moreover, we conduct an experiment to demonstrate the superiority of G-FairAttack compared with gradient-based optimization methods. Specifically, we compare the effectiveness of G-FairAttack with two gradient-based optimization methods in both fairness evasion attack and fairness poisoning attack settings. If an optimization algorithm leads to a larger increase in the attacker\u2019s objective, it is more effective. For the fairness evasion attack, we choose Gradient Ascent as the baseline method. Given the attack budget \u2206 = 0.5%|E|, we record the value of the attacker objective Lf (g\u03b8\u2217 ,At), i.e., demographic parity based on a surrogate model in the t-th iteration, for t = 0, . . . ,\u2206. The result is shown in Figure 5(a). We observe that the attacker objective keeps increasing during our non-gradient optimization (adopted by G-FairAttack) process and reaches an optimum rapidly, while the gradient-based optimization method (adopted by Gradient Ascent) cannot ensure the increment of the attacker objective during the optimization.\nFor the fairness poisoning attack, we choose Metattack as the baseline method. During the optimization process, the surrogate model g\u03b8t is retrained in each iteration. The convergence of the surrogate model during the retraining is not controllable. To make a fair comparison, we should ensure both optimization methods compute the attacker objective based on the same surrogate model. Hence we fix At and g\u03b8t for both methods and record the increment of the attacker objective\nLf (g\u03b8t ,At+1)\u2212 Lf (g\u03b8t ,At) in the t-th iteration, for t = 0, . . . ,\u2206, where At+1 is obtained based on different optimization methods. In conclusion, we compare the increment of the attacker\u2019s objective caused by a single optimization step of different optimization methods in this case. The result is shown in Figure 5(b). For G-FairAttack, the attacker\u2019s objective increases in 28 iterations, i.e., positive \u2206dp variation, and decreases in 1 iteration. For Metattack, the attacker objective increases in only 5 iterations and decreases in 5 iterations as well. Therefore, a single step of G-FairAttack leads to a larger increase of the attacker\u2019s objective than Metattack given the same initial point At. In conclusion, our proposed G-FairAttack leads to a better solution in the optimization process compared with gradient-based methods in both fairness evasion attacks and fairness poisoning attacks.\nE IMPLEMENTATION DETAILS"
        },
        {
            "heading": "E.1 DATASETS",
            "text": "The statistics of these datasets are shown in Table 3. In the Facebook graph (Leskovec & Mcauley, 2012), the nodes represent user accounts of Facebook, and the edges represent the friendship relations between users. Node attributes are collected from user profiles. The sensitive attributes of user nodes are their genders. The task of Facebook is to predict the education type of the users. In the Credit defaulter graph (Dai & Wang, 2021), the nodes represent credit card users, and the edges represent whether two users are similar or not according to their spending and payment patterns. The sensitive attributes of user nodes are their ages. The task of Credit is to predict whether a user will default on the credit card payment or not. In the Pokec graph (Agarwal et al., 2021), the nodes represent user accounts of a prevalent social network in Slovakia, and the edges represent the friendship relations between users. The sensitive attributes of user nodes are their regions. The task of Pokec is to predict the working fields of the users. It is worth noting that we use a subgraph of Pokec in (Dong et al., 2022a) instead of the original version in (Dai & Wang, 2021). In our experiment implementation, we adopt the PyGDebias library (Dong et al., 2023a) to load these datasets."
        },
        {
            "heading": "E.2 BASELINES",
            "text": "We first introduce the detailed settings of the attack baselines in the first stage of evaluation.\n\u2022 Random (Zu\u0308gner et al., 2018; Hussain et al., 2022): Given the attack budget \u2206, we randomly flip \u2206 edges (removing existing edges or adding new edges) and obtain the attacked graph. It is a random method that fits both the fairness evasion attack setting and the fairness poisoning attack setting.\n\u2022 FA-GNN (Hussain et al., 2022): Given the attack budget \u2206, we randomly link \u2206 pairs of nodes that belong to different classes and different sensitive groups. Specifically, we choose the most effective link strategy, \u201dDD\u201d, in our experiments. It is also a random method that fits both attack settings.\n\u2022 Gradient Ascent: Given the attack budget \u2206, we flip \u2206 edges sequentially in \u2206 iterations. In each iteration, we compute the gradient\u2207A\u2032Lf (g\u03b8\u2217 ,A\u2032) and flip one edge corresponding to the largest element of the gradient, where \u03b8\u2217 = argmin\u03b8 Ls(g\u03b8,A). To make Lf differentiable, we use the soft predictions to substitute the prediction labels in Lf as (Zeng et al., 2021). Here, we choose a two-layer graph convolutional network (Kipf & Welling, 2017) as the surrogate model g\u03b8, and CE loss as the surrogate loss function Ls. Gradient Ascent is an optimization-based method that only fits the fairness evasion attack setting.\n\u2022 Metattack (Zu\u0308gner & Gu\u0308nnemann, 2019): Given the attack budget \u2206, we sequentially flip \u2206 edges in \u2206 iterations. In each iteration, we compute the meta gradient \u2207A\u2032Lf (g\u03b8\u2217(A\u2032),A\u2032) by MAML (Finn et al., 2017) and flip one edge corresponding to the largest element of the meta gradient, where \u03b8\u2217 = argmin\u03b8 Ls(g\u03b8,A\u2032). To make Lf differentiable, we implement the same adaption of Lf as Gradient Ascent. Here, we also choose a two-layer GCN as the surrogate model g\u03b8 and CE loss as the surrogate loss function Ls. Metattack is also an optimization-based method, while it only fits the fairness poisoning attack.\nIt is worth noting that for both Gradient Ascent and Metattack, we should flip the sign of the gradient components for connected node pairs as this yields the gradient for a change in the negative direction (i.e., removing the edge) (Zu\u0308gner & Gu\u0308nnemann, 2019). Hence, we flip the edge corresponding to the largest score\u2207A[u,v]Lf \u00b7 (\u22122 \u00b7A[u,v] + 1) for (u, v) \u2208 E for Gradient Ascent and Metattack. Next, we introduce our adopted test GNNs in the second stage. We choose four types of GNNs, including a vanilla GNN and three types of fairness-aware GNNs.\n\u2022 Vanilla: We choose a two-layer GCN (Kipf & Welling, 2017), which is a mostly adopted GNN model in existing works.\n\u2022 \u2206dp: We choose the output-based regularization method (Navarin et al., 2020; Zeng et al., 2021; Wang et al., 2022a; Dong et al., 2023c). Specifically, we choose a two-layer GCN as the backbone and add a regularization term \u2206dp to the loss function. Moreover, to make the regularization term differentiable, we use the soft prediction to substitute the prediction label in \u2206dp as (Zeng et al., 2021).\n\u2022 I(Y\u0302 , S): For mutual information loss, except for directly decreasing the mutual information, the adversarial training (Bose & Hamilton, 2019; Dai & Wang, 2021) could also be seen as a specific case of exploiting the mutual information loss according to (Kang et al., 2022). In an adversarial training framework, an adversary is trained to predict S based on Y\u0302 . If the prediction is more accurate, it demonstrates that the output Y\u0302 of the GNN contains more information about the sensitive attribute S, i.e., the GNN model is less fair. We choose FairGNN (Dai & Wang, 2021) as the baseline in the mutual information type. FairGNN contains a discriminator to predict the sensitive attribute based on the output of a GNN backbone. The GNN backbone is trained to fool the discriminator for predicting the sensitive attribute. Specifically, we choose a single-layer GCN as the GNN backbone.\n\u2022 W (Y\u0302 , S): We choose EDITS (Dong et al., 2022a), a model agnostic debiasing framework for GNNs. EDITS finds a debiased adjacency matrix and a debiased node attribute matrix by minimizing the Wasserstein distance of the distributions of node embeddings on different sensitive groups. With the debiased input graph data, the fairness of the GNN backbone is improved. Specifically, we choose a two-layer GCN as the GNN backbone."
        },
        {
            "heading": "E.3 EXPERIMENTAL SETTINGS",
            "text": "The implementation of our experiments could be divided into two parts, fairness attack methods, and the test GNN models. For attack methods, we use the source code of FA-GNN (Hussain et al., 2022) and Metattack (Zu\u0308gner & Gu\u0308nnemann, 2019). Other attack methods including G-FairAttack are implemented in PyTorch (Paszke et al., 2019). For test GNN models, we use the source code of GCN (Kipf & Welling, 2017), FairGNN (Dai & Wang, 2021), and EDITS (Dong et al., 2022a).\nWe exploit Adam optimizer (Kingma & Ba, 2015) to optimize the surrogate model, gradient-based attacks, and the test GNNs. All experiments are implemented on an Nvidia RTX A6000 GPU. We provide the hyperparameter settings of G-FairAttack in Table 5, and the hyperparameter settings of test GNNs in Table 4. 3"
        },
        {
            "heading": "E.4 REQUIRED PACKAGES",
            "text": "We list some key packages in Python required for implementing G-FairAttack as follows.\n\u2022 Python == 3.9.13\n\u2022 torch == 1.11.0\n\u2022 torch-geometric == 2.0.4\n\u2022 numpy == 1.21.5\n\u2022 numba == 0.56.3\n\u2022 networkx == 2.8.4\n\u2022 scikit-learn == 1.1.1\n\u2022 scipy == 1.9.1\n\u2022 dgl == 0.9.1\n\u2022 deeprobust == 0.2.5\n3The open-source code is available at https://github.com/zhangbinchi/G-FairAttack."
        },
        {
            "heading": "F SUPPLEMENTARY EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "F.1 EFFECTIVENESS OF ATTACK",
            "text": "In Section 4.2, we discussed the effectiveness of G-FairAttack in attacking the fairness of different types of GNNs. Here, we provide more results of the same experiment as Section 4.2 but in different attack settings and more fairness metrics. Table 6 shows the experimental results of fairness evasion attacks in \u2206eo fairness metric, Table 7 shows the experimental results of fairness poisoning attacks in \u2206dp fairness metric, and Table 8 shows the experimental results of fairness poisoning attacks in \u2206eo fairness metric. From the re-\nsults, we can find that the overall performance of G-FairAttack is better than any other baselines, which highlights the clear superiority of G-FairAttack. In addition, we can find that G-FairAttack seems less desirable in the cases where vanilla GCN serves as the victim model. The reason for this phenomenon is our surrogate loss contains two parts, utility loss term L and fairness loss term Lf , while the real victim loss of vanilla GCN only contains the utility loss term L. However, in the general case (without knowing the type of the victim model), G-FairAttack can outperform all other baselines when the surrogate loss differs from the victim loss (when attacking fairness-aware GNNs). Despite that, the shortage in attacking vanilla GNNs can be solved easily by choosing a smaller hyperparameter \u03b1 (the weight of Lf ). As Table 5 shows, we fix the hyperparameter setting (also fix \u03b1) for all attack methods when attacking different types of victim models because the attacker cannot choose different hyperparameters based on the specific type of the victim model in the gray-box attack setting. By fixing the hyperparameter settings for different victim models, the experimental results demonstrate that our G-FairAttack is victim model-agnostic and easy to use (in terms of hyperparameter tuning). In addition, we can also find that the fairness poisoning attack is a more challenging task (easier to fail), while effective fairness poisoning attack methods induce a more serious deterioration in the fairness of the victim model. Furthermore, we obtain that edge rewiring based methods (EDITS) can effectively defend against some fairness attacks (more details about this conclusion are discussed in Appendix G)."
        },
        {
            "heading": "F.2 EFFECTIVENESS OF SURROGATE LOSS",
            "text": "In Section 4.3, we discussed the effectiveness of our proposed surrogate loss in representing different types of fairness loss in different victim models. Here, we provide more results of the same experiment as Section 4.3 but in different attack settings and datasets. Table 9 shows the experimental results of fairness evasion attacks on the Facebook dataset. As mentioned in Section 4.3, the surrogate loss function in the attack method and the victim loss function in the victim model are different in our attack setting. The attacker does not know the form of the victim loss function due to the gray-box setting. If the surrogate loss is the same as the victim loss, the attack method naturally can have a desirable performance (as white-box attacks).\nHowever, in this experiment, we demonstrate that G-FairAttack (with our proposed surrogate loss) has the most desirable performance when attacking different victim models with a different victim loss from the surrogate loss. Consequently, our experiment verifies that our surrogate loss function is adaptable to attacking different types of victim models."
        },
        {
            "heading": "F.3 ATTACK PATTERNS",
            "text": "In this experiment, we compare the patterns of G-FairAttack with FA-GNN (Hussain et al., 2022). FA-GNN divides edges into four different groups, \u2019EE\u2019, \u2019ED\u2019, \u2019DE\u2019, and \u2019DD\u2019, where edges in EE link two nodes with the same label and the same sensitive attribute, edges in ED link two nodes with the same label and different sensitive attributes, edges in DE link two nodes with different labels and the same sensitive attribute, edges in DD link two nodes with different labels and different sensitive attributes In particular, we record the proportion of poisoned edges in these four groups yielded by\nG-FairAttack and FA-GNN. The results of attack patterns of G-FairAttack are shown in Table 10. According to the study in (Hussain et al., 2022), injecting edges in DD and EE can increase the statistical parity difference. Based on this guidance, FA-GNN randomly injects edges that belong to group DD to attack the fairness of GNNs. Hence, the attack patterns of FA-GNN for all datasets and attack settings are all the same, i.e., 100% for the DD group and 0% for the other groups. However, our proposed G-FairAttack has different patterns of poisoned edges. For the Facebook and the Credit dataset, G-FairAttack poisons more EE edges than other groups. For the Pokec dataset, the proportion of poisoned edges in four groups is balanced. Although we cannot analyze the reason for the apparent difference in this paper, we can argue that G-FairAttack is much harder to defend because it does not have a fixed pattern for all cases, unlike FA-GNN."
        },
        {
            "heading": "F.4 ATTACK GENERALIZATION",
            "text": "Although the generalization capability of adversarial attacks on GNNs with a linearized surrogate model has been verified by previous works (Zu\u0308gner et al., 2018; Zu\u0308gner & Gu\u0308nnemann, 2019), we conduct numerical experiments to verify the generalization capability of G-FairAttack to other GNN architectures. In the experiments, G-FairAttack is still trained with a two-layer linearized GCN as the surrogate model, while we choose two different GNN architectures, GraphSAGE (Hamilton et al., 2017) and GAT (Velic\u030ckovic\u0301 et al., 2018), as the backbone of adopted victim models. Experimental results are shown in Tables 11 to 14. From the results, we can observe that G-FairAttack still successfully reduces the fairness of victim models with different GNN backbones, and G-FairAttack still has the most desirable performance on attacking different types of fairness-aware GNNs. In addition, to verify the generality of G-FairAttack, we conduct experiments on the German Credit dataset (Agarwal et al., 2021). In the German dataset, nodes represent customers of a German bank, and edges\nare generated based on the similarity between credit accounts. The task is to predict whether a customer has a high credit risk or low, with gender as the sensitive attribute. In this experiment, we choose GraphSAGE (Hamilton et al., 2017) as the backbone of the victim models and adopt both fairness evasion and poisoning settings. Experimental results are shown in Tables 16 and 17. We can observe that the superiority of G-FairAttack is preserved on the German dataset. In conclusion, supplementary experiments verify the generalization capability of G-FairAttack in attacking victim models with various GNN architectures and consolidate the universality of G-FairAttack."
        },
        {
            "heading": "F.5 ADVANCED ATTACK BASELINES",
            "text": "We supplement experiments on more previous attacks with fairness-targeted adaptations. We choose two more recent adversarial attacks for GNNs in terms of prediction utility, namely MinMax (Wu et al., 2019) and PRBCD (Geisler et al., 2021). To satisfy our fairness attack settings, we modify the attacker\u2019s objective with the demographic parity loss term (in the same way as adapting gradient ascent attack and Metattack). We implement all attack baselines in a fairness evasion attack setting with the same budget (5%). We compare the performance of these attacks with G-FairAttack based on four different victim models with GraphSAGE as the GNN backbone on the Facebook dataset. Results are shown in Table 15. From the experimental results, we can observe that (1) G-FairAttack\nhas the most desirable performance in attacking different types of (fairness-aware) victim models and (2) G-FairAttack best preserves the prediction utility of victim models over the training set. In conclusion, we obtain that our proposed surrogate loss and constrained optimization technique help G-FairAttack address the two proposed challenges of fairness attacks while a simple adaptation of previous attacks is not effective in solving these challenges."
        },
        {
            "heading": "G DEFENSE AGAINST FAIRNESS ATTACKS OF GNNS",
            "text": "In this paper, the purpose of investigating the fairness attack problem on GNNs is to highlight the vulnerability of GNNs on fairness and to inspire further research on the fairness defense of GNNs. Hence, we would like to discuss the defense against fairness attacks of GNNs. Considering the difficulty in fairness defense, this topic deserves a careful further study, and we only provide some simple insights on defending against fairness attacks of GNNs in this section.\n(1). According to the study in (Hussain et al., 2022), injecting edges that belong to DD and EE groups can increase the statistical parity difference. Hence, a simple fairness defense strategy is to delete edges in DD and EE groups randomly. This strategy makes it possible to remove some poisoned edges in the input graph. However, this method cannot defend against G-FairAttack because G-FairAttack can poison the edges in all groups (EE, ED, DE, and DD).\n(2). In our opinion, preprocessing debiasing frameworks such as EDITS can be a promising paradigm for fairness defense. Next, we explain the reasons in detail. We first review the processes of EDITS framework. EDITS is a preprocessing framework for GNNs (Dong et al., 2022a). First, we feed the clean graph into EDITS framework and obtain a debiased graph by reconnecting some edges and changing the node attributes where we can modify the debiasing extent with a threshold. Then, EDITS runs a vanilla GNN model (without fairness consideration), such as GCN, on the debiased graph. Finally, we find that the output of GCN on the debiased graph is less biased than the output of GCN on the clean graph. As a preprocessing framework, EDITS would flip the edges again to obtain a debiased graph for training after we poison the graph structure by attacking methods. Consequently, we can obtain that EDITS can obtain very similar debiased graphs for any two different poisoned graphs with a strict debiasing threshold, while the accuracy will also decrease as the debiasing threshold becomes stricter because the graph structure has been changed too much. In conclusion, EDITS has a tradeoff between the debiasing effect and the prediction utility. EDITS can be a strong fairness defense method for GNNs by sacrificing the prediction utility.\n(3). A possible defense strategy against fairness attacks of GNNs is to solve a similar optimization problem as Problem 1 while minimizing the attacker\u2019s objective as\nmin A\u2032\u2208F\nLf (g\u03b8\u2217 ,A\u2032,X,Y,Vtest,S)\ns.t. \u03b8\u2217 = argmin \u03b8 Ls (g\u03b8,A\u2032,X,Y,S) , \u2225A\u2032 \u2212A\u2225F \u2264 2\u2206,\nL(g\u03b8\u2217 ,A,X,Y,Vtrain)\u2212 L(g\u03b8\u2217 ,A\u2032,X,Y,Vtrain) \u2264 \u03f5.\n(17)\nThe meaning of this optimization problem is to find the rewired graph structure that minimizes the prediction bias. The prediction bias is computed based on the model trained on the rewired graph. It can be seen as an inverse process of G-FairAttack. As a result, we can rewire the problematic edges that hurt the fairness of the model trained on the rewired graph."
        },
        {
            "heading": "H BROADER IMPACT",
            "text": "Adversarial attacks on fairness can make a significant impact in real-world scenarios (Solans et al., 2021; Mehrabi et al., 2021; Hussain et al., 2022). In particular, fairness attacks can exist in many different real-world scenarios.\n\u2022 For personal benefits, malicious attackers can exploit the fairness attack to affect a GNN model (for determining the salary of an employer or the credit/loan of a user account) into favoring specific demographic groups by predicting higher values of money while disadvantaging other groups.\n\u2022 For commercial competitions, a malicious competitor can attack the fairness of a GNN-based recommender system deployed by a tech company and make its users unsatisfied, especially when defending techniques of GNNs\u2019 utility have been widely studied while defending techniques of GNNs\u2019 fairness remain undeveloped.\n\u2022 For governmental credibility, malicious adversaries can attack models used by a government agency with the goal of making them appear unfair in order to depreciate their value and credibility.\nIn addition, adversarial attack on fairness is widely studied on independent and identically distributed data. Extensive works (Chang et al., 2020b; Solans et al., 2021; Mehrabi et al., 2021; Van et al., 2022; Chhabra et al., 2023) have verified the vulnerability of algorithmic fairness of machine learning models. In this paper, we find the vulnerability of algorithmic fairness also exists in GNNs by proposing a novel adversarial attack on fairness of GNNs. It has the potential risk of being leveraged by malicious attackers with access to the input data of a deployed GNN model. Despite that, our research has a larger positive influence compared with the potential risk. Considering the lack of defense methods on fairness of GNNs, our study highlights the vulnerability of GNNs in terms of fairness and inspires further research on the fairness defense of GNNs. Moreover, we also provide discussions on attack patterns and simple ways to defend against fairness attacks."
        }
    ],
    "title": "GRAPH NEURAL NETWORKS",
    "year": 2024
}