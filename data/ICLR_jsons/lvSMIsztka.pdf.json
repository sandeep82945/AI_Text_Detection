{
    "abstractText": "The family of probabilistic values, axiomatically-grounded and proposed in cooperative game theory, has recently received much attention in data valuation. However, it is often computationally expensive to compute exactly (exponential w.r.t. N , the number of data being valuated). Existing generic estimators cost O( 2 \u03b52 log N \u03b4 ) utility evaluations to achieve an (\u03b5, \u03b4)-approximation under the 2norm, while faster estimators have been developed recently for special cases (e.g., empirically for the Shapley value and theoretically for the Banzhaf value). In this work, based on a connection between probabilistic values and least square regressions, we propose two generic estimators for the whole family of probabilistic values that both costO( \u03b52 log N \u03b4 ) utility evaluations for many probabilistic values, largely extending the scope of this currently best complexity bound. Moreover, we show that each distributional value, proposed by Ghorbani et al. (2020) to alleviate the inconsistency of probabilistic values when using distinct databases, can also be cast as optimizing a similar least square regression. This observation makes it the first-time theoretically-grounded to train value estimators such that the distributional value of each unseen data point can be evaluated in a single forward pass. Our experiments verify the faster convergence of our proposed estimators, and demonstrate the effectiveness at learning distributional values.",
    "authors": [],
    "id": "SP:6930d9f8b758a4b5fd6043bdfe61cccb605be8d5",
    "references": [
        {
            "authors": [
                "III J.F. Banzhaf"
            ],
            "title": "Weighted Voting Doesn\u2019t Work: A Mathematical Analysis",
            "venue": "Rutgers Law Review, vol. 19, pp. 317\u2013343.",
            "year": 1965
        },
        {
            "authors": [
                "A. Charnes",
                "B. Golany",
                "M.S. Keane",
                "J.J. Rousseau"
            ],
            "title": "Extremal Principle Solutions of Games in Characteristic Function Form: Core, Chebychev and Shapley Value Generalizations",
            "venue": "In: Econometrics of Planning and Efficiency, pp. 123\u2013133.",
            "year": 1988
        },
        {
            "authors": [
                "I. Covert",
                "S.-I. Lee"
            ],
            "title": "Improving KernelSHAP: Practical Shapley Value Estimation Using Linear Regression",
            "venue": "In: Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, pp. 3457\u20133465.",
            "year": 2021
        },
        {
            "authors": [
                "I.C. Covert",
                "C. Kim",
                "S.-I. Lee"
            ],
            "title": "Learning to Estimate Shapley Values with Vision Transformers",
            "venue": "In: The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "P. Dubey",
                "A. Neyman",
                "R.J. Weber"
            ],
            "title": "Value Theory without Efficiency",
            "venue": "Mathematics of Operations Research, vol. 6, no. 1, pp. 122\u2013128.",
            "year": 1981
        },
        {
            "authors": [
                "A. Ghorbani",
                "M. Kim",
                "J. Zou"
            ],
            "title": "A Distributional Framework for Data Valuation",
            "venue": "In: International Conference on Machine Learning, pp. 3535\u20133544.",
            "year": 2020
        },
        {
            "authors": [
                "A. Ghorbani",
                "J. Zou"
            ],
            "title": "Data Shapley: Equitable Valuation of Data for Machine Learning",
            "venue": "In: International Conference on Machine Learning, pp. 2242\u20132251.",
            "year": 2019
        },
        {
            "authors": [
                "M. Grabisch",
                "C. Labreuche"
            ],
            "title": "How to Improve Acts: An Alternative Representation of the Importance of Criteria in MCDC",
            "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, vol. 9, no. 02, pp. 145\u2013157.",
            "year": 2001
        },
        {
            "authors": [
                "J.C. Harsanyi"
            ],
            "title": "A Simplified Bargaining Model for the N-Person Cooperative Game",
            "venue": "International Economic Review, vol. 4, no. 2, pp. 194\u2013220.",
            "year": 1963
        },
        {
            "authors": [
                "N. Jethani",
                "M. Sudarshan",
                "I.C. Covert",
                "S.-I. Lee",
                "R. Ranganath"
            ],
            "title": "FastSHAP: Real-Time Shapley Value Estimation",
            "venue": "In: International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "R. Jia",
                "D. Dao",
                "B. Wang",
                "F.A. Hubis",
                "N.M. Gurel",
                "B. Li",
                "C. Zhang",
                "C. Spanos",
                "D. Song"
            ],
            "title": "Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms",
            "venue": "Proceedings of the VLDB Endowment, vol. 12, no. 11.",
            "year": 2019
        },
        {
            "authors": [
                "R Jia"
            ],
            "title": "Towards Efficient Data Valuation Based on the Shapley Value",
            "venue": "In: The 22nd International Conference on Artificial Intelligence and Statistics, pp. 1167\u20131176.",
            "year": 2019
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In: ICLR.",
            "year": 2014
        },
        {
            "authors": [
                "P. Kolpaczki",
                "V. Bengs",
                "E. H\u00fcllermeier"
            ],
            "title": "Approximating the Shapley Value without Marginal Contributions",
            "venue": "arXiv preprint arXiv:2302.00736.",
            "year": 2023
        },
        {
            "authors": [
                "Y. Kwon",
                "J. Zou"
            ],
            "title": "Beta Shapley: A Unified and Noise-reduced Data Valuation Framework for Machine Learning",
            "venue": "In: International Conference on Artificial Intelligence and Statistics, pp. 8780\u20138802.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Kwon",
                "J.Y. Zou"
            ],
            "title": "WeightedSHAP: Analyzing and Improving Shapley Based Feature Attributions",
            "venue": "In: Advances in Neural Information Processing Systems, pp. 34363\u201334376.",
            "year": 2022
        },
        {
            "authors": [
                "Y. LeCun",
                "L. Bottou",
                "Y. Bengio",
                "P. Haffner"
            ],
            "title": "Gradient-Based Learning Applied to Document Recognition",
            "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324.",
            "year": 1998
        },
        {
            "authors": [
                "J. Lin",
                "A. Zhang",
                "M. L\u00e9cuyer",
                "J. Li",
                "A. Panda",
                "S. Sen"
            ],
            "title": "Measuring the Effect of Training Data on Deep Learning Predictions via Randomized Experiments",
            "venue": "In: International Conference on Machine Learning, pp. 13468\u201313504.",
            "year": 2022
        },
        {
            "authors": [
                "S.M. Lundberg",
                "S.-I. Lee"
            ],
            "title": "A Unified Approach to Interpreting Model Predictions",
            "venue": "In: Advances in Neural Information Processing Systems 30.",
            "year": 2017
        },
        {
            "authors": [
                "Marichal",
                "J.-L.",
                "P. Mathonet"
            ],
            "title": "Approximations of Lov\u00e1sz Extensions and Their Induced Interaction Index",
            "venue": "Discrete Applied Mathematics, vol. 156, no. 1, pp. 11\u201324.",
            "year": 2008
        },
        {
            "authors": [
                "N. Moehle",
                "S. Boyd",
                "A. Ang"
            ],
            "title": "Portfolio Performance Attribution via Shapley Value",
            "venue": "Journal of Investment Management, vol. 20, no. 3.",
            "year": 2022
        },
        {
            "authors": [
                "W. Rudin"
            ],
            "title": "Principles of Mathematical Analysis",
            "venue": "McGraw-Hill.",
            "year": 1953
        },
        {
            "authors": [
                "L.M. Ruiz",
                "F. Valenciano",
                "J.M. Zarzuelo"
            ],
            "title": "The Family of Least Square Values for Transferable Utility Games",
            "venue": "Games and Economic Behavior, vol. 24, no. 1-2, pp. 109\u2013130.",
            "year": 1998
        },
        {
            "authors": [
                "L.S. Shapley"
            ],
            "title": "A Value for N-Person Games",
            "venue": "Annals of Mathematics Studies, vol. 28, pp. 307\u2013317.",
            "year": 1953
        },
        {
            "authors": [
                "J. Wang",
                "R. Jia"
            ],
            "title": "Data Banzhaf: A Robust Data Valuation Framework for Machine Learning",
            "venue": "In: Proceedings of The 26th International Conference on Artificial Intelligence and Statistics.",
            "year": 2023
        },
        {
            "authors": [
                "R.J. Weber"
            ],
            "title": "Probabilistic Values for Games",
            "venue": "In: The Shapley Value. Essays in Honor of Lloyd S. Shapley, pp. 101\u2013119.",
            "year": 1977
        },
        {
            "authors": [
                "H. Xiao",
                "K. Rasul",
                "R. Vollgraf"
            ],
            "title": "Fashion-MNIST: A Novel Image Dataset for Benchmarking Machine Learning Algorithms",
            "venue": "arXiv preprint arXiv:1708.07747.",
            "year": 2017
        },
        {
            "authors": [
                "B. Zhang",
                "B. Tian",
                "W. Zheng",
                "J. Zhou",
                "J. Lu"
            ],
            "title": "Exploring Unified Perspective For Fast Shapley Value Estimation",
            "venue": "arXiv preprint arXiv:2311.01010.",
            "year": 2023
        },
        {
            "authors": [
                "J. Zhang",
                "Q. Sun",
                "J. Liu",
                "L. Xiong",
                "J. Pei",
                "K. Ren"
            ],
            "title": "Efficient Sampling Approaches to Shapley Value Approximation",
            "venue": "In: ACM SIGMOD International Conference on Management of Data.",
            "year": 2023
        },
        {
            "authors": [
                "Ruiz"
            ],
            "title": "Definition 1 (Least Square",
            "year": 1998
        },
        {
            "authors": [
                "Ruiz"
            ],
            "title": "Theorem 8) developed a system of axioms that uniquely characterizes the family of least square values. Moreover, its relationship with the family of probabilistic values were also revealed",
            "venue": "Proposition 7 (Ruiz et al. (1998,",
            "year": 1998
        },
        {
            "authors": [
                "Kwon",
                "Zou 2022a",
                "Zou"
            ],
            "title": "2022b). Note that Beta(1, 1) is exactly the Shapley value. For the Banzhaf value, the corresponding \u03bc is the Dirac delta distribution \u03b40.5, which leads to ps = 1",
            "year": 2022
        },
        {
            "authors": [
                "Kwon",
                "Zou"
            ],
            "title": "2022b) showed that other candidates of the Beta Shapley values tend to perform better than the Shapley value in feature attribution. Therefore, one may ask how to cast other probabilistic values into optimization, which is answered by Propositions 1 and 3. Though AME proposed by Lin et al. (2022) provides an alternative way to achieve this goal, it is restricted to a subfamily of semi-values",
            "year": 2022
        },
        {
            "authors": [
                "Dubey"
            ],
            "title": "semi-value corresponds to a probability measure \u03bc on the interval",
            "year": 1981
        },
        {
            "authors": [
                "Dubey"
            ],
            "title": "lar if p is bounded (the so-called continuous semivalues",
            "year": 1981
        }
    ],
    "sections": [
        {
            "text": "2\n\u03f52 log N \u03b4 ) utility evaluations to achieve an (\u03f5, \u03b4)-approximation under the 2- norm, while faster estimators have been developed recently for special cases (e.g., empirically for the Shapley value and theoretically for the Banzhaf value). In this work, based on a connection between probabilistic values and least square regressions, we propose two generic estimators for the whole family of probabilistic values that both costO(N\u03f52 log N \u03b4 ) utility evaluations for many probabilistic values, largely extending the scope of this currently best complexity bound. Moreover, we show that each distributional value, proposed by Ghorbani et al. (2020) to alleviate the inconsistency of probabilistic values when using distinct databases, can also be cast as optimizing a similar least square regression. This observation makes it the first-time theoretically-grounded to train value estimators such that the distributional value of each unseen data point can be evaluated in a single forward pass. Our experiments verify the faster convergence of our proposed estimators, and demonstrate the effectiveness at learning distributional values."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "In cooperative game theory, the family of probabilistic values, to which the well-known Shapley value (Shapley 1953) and Banzhaf value (Banzhaf 1965) belong, is uniquely characterized by the axioms of linearity, dummy, monotonicity and symmetry (Weber 1977, Theorems 5 and 10). The resulting analytic formula of probabilistic values is often deemed essential for data valuation methods that aggregate marginal gains (Kwon and Zou 2022a; Lin et al. 2022; Wang and Jia 2023). Specifically, data valuation aims to impute an importance value to each data point in a given dataset Dv . For example, the importance value could represent the contribution to the performance of a neural network trained on Dv; and it is empirically expected that neural networks retrained without the \u201cmost valuable\u201d data (e.g., data that are assigned with importance values higher than a specified threshold) in Dv will perform significantly worse (Ghorbani and Zou 2019).\nThroughout, we identify a dataset Dv to be valuated with [N ] = {1, 2, . . . , N}, where N = |Dv| is its size. The family of probabilistic values is parameterized by a non-negative vector p \u2208 RN+ such that \u2211N s=1 ( N\u22121 s\u22121 ) ps = 1, and the importance value for the i-th data point is computed by\n\u03d5pi (U) = \u2211\nS\u2286[N ]\\i\nps+1 (U(S \u222a i)\u2212 U(S)) , (1)\nwhere U(S \u222a i) \u2212 U(S) is the so-called marginal gain, with U : 2N \u2192 R being a user-specified utility function. Typically, U(S) is set to be the performance of a chosen model trained on S \u2286 Dv , whereas U(\u2205) could be the baseline given by the initialized model or a random guess. Take classification tasks as an example, U(S) could be the accuracy or cross entropy loss reported on a held-out dataset Du. It is obvious that computing \u03d5p(U) exactly requires evaluating the utility function U exponentially many times w.r.t. N , and hence is intractable. Therefore, there has been much research devoted into developing efficient estimators. To our best knowledge, there are only\ntwo generic estimators employed for the whole family of probabilistic values: sampling lift, see Proposition 6, and its weighted version (Kwon and Zou 2022a).\nLet \u03d5\u0302p(U) be a random estimator for \u03d5p(U). The sampling lift method requires O(N 2\n\u03f52 log N \u03b4 )\nutility evaluations to achieve an (\u03f5, \u03b4)-approximation, i.e., P (\u2225\u03d5\u0302 p(U)\u2212\u03d5p(U)\u22252 \u2225\u03d5p(U)\u22252 \u2265 \u03f5) \u2264 \u03b4 (Wang and Jia 2023, Theorem 4.8). Moreover, many efficient estimators have been proposed for specific probabilistic values. In practice, the Shapley value (Shapley 1953) can be efficiently approximated by solving a constrained least square regression (Covert and Lee 2021; Lundberg and Lee 2017), while faster estimators designed for the Shapley value (Kolpaczki et al. 2023; Zhang et al. 2023b) appeared very recently. Wang and Jia (2023, Theorem 4.9) proved that the estimator based on their proposed maximum sample reuse (MSR) principle only requires O(N\u03f52 log N \u03b4 ) utility evaluations for the Banzhaf value, but they also demonstrated in Appendix C.2 therein that the MSR estimator does not extend to many other probabilistic values, e.g., the family of Beta Shapley values (Kwon and Zou 2022a), to which the Shapley value belongs. We also notice that Lin et al. (2022) discovered a framework of unconstrained least square regressions that applies to a subfamily of probabilistic values (which, however, does not include, e.g., the Shapley value). All in all, it is still an open question on how to efficiently approximate other probabilistic values.\nAnother drawback of the formula (1) is that \u03d5p(U) rests on the underlying databaseDv . If a different database is employed, the recalculated importance values could be very inconsistent with the previous ones. To overcome this issue, Ghorbani et al. (2020) proposed the framework of distributional values, in which the importance value of the i-th data point in Dv is\n\u03d5D,wi (U) = E s w\u223c[m] E S\u223cDs\u22121 [U(S \u222a i)\u2212 U(S)], (2)\nwhere D is a data distribution and w \u2208 Rm is a probability vector. Empirically, D is replaced by a (large) dataset sampled from D (Ghorbani et al. 2020). Clearly, the computational cost for the distributional value of each data point is a big hurdle for its practical deployment.\nIn this work, based on a connection between probabilistic values and least square regressions, as shown in Proposition 2, we develop two generic estimators concerning when only the ranking matters and when the exact probabilistic values are desired, respectively. Moreover, we demonstrate that each distributional value can also be cast into a similar least square regression, which serves as the theoretical ground for training value estimators. In other words, we can employ a trained value estimator to obtain the importance value of any unseen data point in a single forward pass. Our main contributions are summarized as follows:\n1. We propose two generic estimators for all probabilistic values and prove that both require O(N\u03f52 log N \u03b4 ) utility evaluations to achieve (\u03f5, \u03b4)-approximation for many probabilistic values,\nmatching the currently best bound for special cases. See Algorithms 1 and 2, Propositions 1, 4, 3 and 5.\n2. By casting the distributional value into optimizing least square regressions, we design a framework of training value estimators to fit a specific distribution value. Therefore, the selected distributional value can be estimated in a single forward pass using a well-trained value estimator. See Algorithm 3 and Theorem 1.\n3. Our experiments verify the faster convergence of our proposed estimators, and we also show that the distributional value can be well-learned using trainable models.\n4. As a minor side note, we also extend the approximation-without-requiring-marginal (ARM) estimator, designed specifically for the Shapley value (Kolpaczki et al. 2023), to the whole family of probabilistic values. See Proposition 9 in the Appendix."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "Notation LetDv be an available training dataset, each data point of which requires being valuated. For convenience, we write N = |Dv| and identify Dv with [N ] = {1, 2, . . . , N}, i.e., Dv \u2261 [N ]. G = {U : 2[N ] \u2192 R} is a set that contains all possible utility functions. Without ambiguity, for each subset S, its lower-case s is used to define its cardinality |S|, and we write S \u222a i and S\\i instead of S \u222a {i} and S\\{i}, respectively.\nThe sampling lift estimator refers to any approximation algorithm designed according to\n\u03d5pi (U) = ES\u2286[N ]\\i[U(S \u222a i)\u2212 U(S)] (3)\nwhere P (S) = ps+1 (Moehle et al. 2022). Its weighted version is to have N ( N\u22121 s\u22121 ) ps+1(U(S \u222a\ni) \u2212 U(S)) with P (S) = 1N ( N\u22121 s )\u22121 instead, which is employed by Kwon and Zou (2022a). Particularly, they are the same for the Shapley value. As summarized in Proposition 6, the sampling lift requires O(N 2\n\u03f52 log N \u03b4 ) utility evaluations to achieve an (\u03f5, \u03b4)-approximation.\nRecently, Wang and Jia (2023) proposed the maximum sample reuse (MSR) principle which aims to find a distribution PMSR on 2[N ] such that for every i \u2208 [N ],\nPMSR(S | i \u2208 S) = ps and PMSR(S | i \u0338\u2208 S) = ps+1, (4)\nwith which Eq. (1) can be rewritten as \u03d5pi (U) = ES|i\u2208S [U(S)]\u2212ES|i \u0338\u2208S [U(S)]. Specifically, PMSR is uniform over 2[N ] for the Banzhaf value, and Wang and Jia (2023, Theorem 4.9) showed that the resulting estimator only requires O(N\u03f52 log N \u03b4 ) utility evaluations. However, they also demonstrated in Appendix C.2 therein that such MSR estimators do not exist for many other probabilistic values including the Beta Shapley values.\nKolpaczki et al. (2023) proposed an ARM estimator specifically for the Shapley value, but we notice that it can be easily generalized for the whole family of probabilistic values. Therefore, we present the generic formula of ARM in the main paper while leaving its justification to Proposition 9 in the Appendix. The ARM estimator is to approximate by using\n\u03d5pi (U) = ES\u223cP+ARM [U(S) | i \u2208 S]\u2212 ES\u223cP\u2212ARM [U(S) | i \u0338\u2208 S] (5)\nwhere P+ARM (S) \u221d ps for all non-empty subsets S \u2286 [N ] and P \u2212 ARM (S) \u221d ps+1 for all S \u228a [N ].\nAnother recently-proposed faster algorithm specific for the Shapley value is the complement estimator (Zhang et al. 2023b) using the complement formula\n\u03d5Shi (U) = \u2211\nS\u2286[N ] : i\u2208S\npShs+1 (U(S)\u2212 U([N ]\\S)) , (6)\nwhich was discovered half a century ago by Harsanyi (1963, Eq. (4.1)).\nThe SHAP proposed by Lundberg and Lee (2017) and later improved by Covert and Lee (2021) exploits the fact that the Shapley value \u03d5Sh(U) is the unique optimal solution to\nargmin v\u2208RN \u2211 \u2205\u228aS\u228a[N ]\n1\ns(N \u2212 s)\n( N\ns\n)\u22121( U(S)\u2212 U(\u2205)\u2212 \u2211 i\u2208S vi )2 , s.t. \u2211 i\u2208[N ] vi = U([N ])\u2212 U(\u2205),\n(7) which was first discovered by Charnes et al. (1988, Theorem 4).\nLin et al. (2022) developed the average marginal effect (AME) as the unique optimal solution to\nargmin v\u2208RN\nE[(Y \u2212X\u22a4v)2], (8)\nwhere X and Y are random variables whose distribution is induced by i) sampling t \u223c P where P is any probability distribution on the open interval (0, 1), ii) sampling a subset S \u2286 [N ] by including each data point in [N ] with probability t, and iii) setting Y = U(S) and Xi = 1t\u00b7MP if i \u2208 S and \u22121(1\u2212t)MP otherwise. The sufficient condition for the existence of the optimal solution is that MP = Et\u223cP [ 1t(1\u2212t) ] < \u221e. Particularly, the AME if it exists is just a probabilistic value parameterized by ps = \u222b 1 0 ts\u22121(1 \u2212 t)n\u2212sdP (t). If P is uniform, the corresponding p is the one defining the Shapley value, but MP =\u221e. In the next section, we propose two generic estimators based on the least square regressions, Proposition 2, that applies to all probabilistic values, and prove that both of them achieve an (\u03f5, \u03b4)approximation using O(N\u03f52 log N \u03b4 ) utility evaluations in Propositions 4 and 5."
        },
        {
            "heading": "3 MAIN RESULTS",
            "text": "In this section, we first show how to efficiently estimate the ranking induced by any probabilistic value. Then, by introducing a null data point we show how to turn our rank estimator into a bona fide value estimator, while retaining the same efficiency. Lastly, we extend our results to estimate the distributional value of Ghorbani et al. (2020), a recent stable version of the Shapley value."
        },
        {
            "heading": "3.1 WHEN RANKING SUFFICES",
            "text": "Practitioners often need to screen training data before feeding them to a model, to remove outliers, low-quality data or even adversarial examples that are deemed harmful to training. Data valuation is a natural way to achieve this goal, i.e., only data assigned with high importance values could be considered potentially \u201cvaluable.\u201d If one has a good estimate of the proportion of \u201cvaluable\u201d data, then the relative ranking, instead of the more precise and demanding value, suffices. Our first results below pave the way to efficiently estimate the relative ranking underlying any probabilistic value. Proposition 1. Let p \u2208 RN be a vector that defines a probabilistic value. Define\nbp : G \u2192 RN where bpi (U) = \u2211\nS\u228a[N ]\nmps \u00b7 1i\u2208SU(S) for every i \u2208 [N ] and every U \u2208 G, (9)\nand mps = ps + ps+1 for s \u2208 [n \u2212 1]. Then, for every U \u2208 G, bp(U) = \u03d5p(U) + cp(U)1n where cp : G \u2192 R. In other words, bp(U) and \u03d5p(U) produce the same ranking for every U \u2208 G.\nWe emphasize that proposition 1 is closely related to proposition 2 below in the sense that they imply each other immediately; we first noticed a more general version of proposition 2 that applies to all least square values (Ruiz et al. 1998, Definition 5), a broader family that includes all the additive-efficient-normalized probabilistic values (see the appendix for more details). Proposition 2. Suppose a utility function U \u2208 G and a vector p \u2208 RN that defines a probabilistic value are given, for the problem\nargmin u\u2208RN \u2211 \u2205\u228aS\u228a[N ] mps ( U(S)\u2212 \u2211 i\u2208S ui )2 , (10)\nwhere mps = ps+ ps+1 for s = 1, 2, . . . , N \u2212 1, its unique optimal solution u\u2217 meets that \u03d5p(U) = u\u2217 + c1N for some constant c \u2208 R; 1N \u2208 RN is the all-one vector. In other words, \u03d5p(U) and u\u2217 have the same ranking for Dv .\nWe point out that Proposition 2 implies Proposition 1 immediately by noting that b = (\u2212 eeN+1JN+ IN )u\n\u2217 in Eq. (19). For all bpi (U), the weights for S of equal size are the same, which means each utility evaluation U(S) can be used for updating the estimated values of s data. In contrast, the sampling lift estimator spends two utility evaluations to update the estimated value of only one data point. Algorithm 1 summarizes the estimator induced by Propositions 1 and 2, and its convergence analysis is provided in Proposition 4."
        },
        {
            "heading": "3.2 WHEN VALUES ARE DESIRED",
            "text": "We now describe how to obtain each probabilistic value exactly via least square regressions, which also leads to an efficient estimator. To this end, we introduce an extra null data point labeled by N +1, and extend each utility function U \u2208 G into U : 2[N+1] \u2192 R by letting U(S) = U(S \u2229 [N ]) for every S \u2286 [N + 1]. We call the data point N + 1 \u201cnull\u201d since it does not contribute anything to evaluating U . The next result justifies our Algorithm 2, which reduces value approximation to rank approximation in Algorithm 1. Proposition 3. Let U \u2208 G and a vector p \u2208 Rn that defines a probabilistic value be given. The unique optimal solution n\u2217 to the problem\nargmin n\u2208RN+1 \u2211 \u2205\u228aS\u228a[N+1] ps ( U(S)\u2212 \u2211 i\u2208S ni )2 (11)\nsatisfies that for every i \u2208 [N ] \u03d5pi (U) = n \u2217 i \u2212 n\u2217N+1. (12)\nAlgorithm 1: Ranking Approximation Input: A dataset Dv \u2261 [N ] to be valuated, a utility function U \u2208 G, a weight vector\nw \u2208 RN\u22121 given by ws = 1sps + 1 N\u2212sps+1 where ps = ( N\u22121 s\u22121 ) ps and the vector\np \u2208 RN defines a probabilistic value, and the total number T of samples. Output: An unbiased estimate b\u0302 to b\u030c = ( \u2211N\u22121 s=1 ( N\u22121 s\u22121 ) (ps + ps+1)) \u22121bp(U)\n1 Normalize w into a probability vector w\u2190 w/ \u2211N\u22121\ns=1 ws 2 b\u0302\u2190 0N , t\u2190 0N 3 for k = 1, 2, . . . , T do 4 Sample sk \u2208 [N \u2212 1] using the probability vector w 5 Uniformly sample Sk from {R \u2286 [N ] | |R| = sk} 6 for i \u2208 Sk do 7 ti \u2190 ti + 1 8 b\u0302i \u2190 (1\u2212 1ti )b\u0302i + 1 ti U(Sk)\nAlgorithm 2: Value Approximation Input: Dv \u2261 [N ] to be valuated, utility function U \u2208 G, vector p \u2208 RN where ps = ( N\u22121 s\u22121 ) ps and p \u2208 RN defines a probabilistic value, and the total number T of samples. Output: An unbiased estimate \u03d5\u0302 to \u03d5p(U).\n1 Introduce a null data point labeled by N + 1, and extend U to U // U(S) = U(S \u2229 [N ]) 2 Compute w \u2208 RN by letting ws \u2190 N(N+1)s(N+1\u2212s)ps for s = 1, 2, . . . , N 3 Obtain b\u0302 from Algorithm 1 using [N + 1], U , w and T 4 \u03d5\u0302i \u2190 ( \u2211N s=1 N N\u2212s+1ps)(b\u0302i \u2212 b\u0302N+1) for each i \u2208 [N ]\nRemark 1. Proposition 3 enables us to design an efficient estimator for any probabilistic value, as shown in Algorithm 2. Notably, for every non-negative non-zero vector q \u2208 RN , it can always be scaled such that \u2211N s=1 ( N\u22121 s\u22121 ) qs = 1. Since scaling the objective does not change the optimal solution, it implies that the formula (12) induced by any non-negative non-zero vector p in problem (11) always produces some probabilistic value! Meanwhile, such an estimator also enjoys the currently known best complexity bound in terms of (\u03f5, \u03b4)-approximation.\nProposition 4. For any vector p \u2208 RN that defines a probabilistic value and every utility function U \u2208 G, the estimator b\u0302 in Algorithm 1 requires T = O(\u03ba(N)N\u03f52 log N \u03b4 ) samples, which is equal to the number of utility evaluations, to achieve P (\u2225b\u0302\u2212b\u030c\u22252\u2225b\u030c\u22252 \u2265 \u03f5) \u2264 \u03b4.\nProposition 5. For every vector p \u2208 RN that defines a probabilistic value and every utility function U \u2208 G, the estimator \u03d5\u0302 in Algorithm 2 requires T = O( \u03c4(N)N\u03f52 log N \u03b4 ) samples, which is equal to the number of utility evaluations, to achieve P (\u2225\u03d5\u0302\u2212\u03d5 p(U)\u22252\n\u2225\u03d5p(U)\u22252 \u2265 \u03f5) \u2264 \u03b4. Remark 2. Intuitively, \u03ba(N) is the inverse square of the average reuse rate of utility evaluations, see Appendix H. In Appendix I, we prove that for semivalues generated by a probability measure \u00b5 on [0, 1], \u03ba(N) = \u03c4(N) = \u0398(1) if \u222b 1 0 1 t(1\u2212t)d\u00b5(t) < \u221e, and \u03ba(N) = \u03c4(N) = O(log\n2N) if \u00b5 has a bounded density. In particular, for the Beta Shapley value Beta(\u03b1, \u03b2), our estimators have complexityO(N\u03f52 log N \u03b4 ) if \u03b1, \u03b2 > 1 andO( N \u03f52 log( N \u03b4 ) log\n2N) if \u03b1, \u03b2 \u2265 1. In contrast, the sampling lift estimator has complexity O(N 2\n\u03f52 log N \u03b4 )."
        },
        {
            "heading": "3.3 DISTRIBUTIONAL VALUE",
            "text": "We are now ready to combine the previous results with the notion of distributional values of Ghorbani et al. (2020) to establish a framework for training value estimators. Such trained models can be employed to valuate any unseen data point sampled from the same (or close-enough) data distribution in a single forward pass. As counterparts in the field of feature attribution, Covert et al. (2023)\nAlgorithm 3: Training Estimators Input: A database Dv \u2261 (xi, yi)1\u2264i\u2264N , a probability vector w \u2208 Rm with m < N , a utility\nfunction U \u2208 G, a trainable model \u03d5\u03b8 with an additional trainable parameter \u03d5o, the batch size B and the total number T of batches used for training.\n1 Compute d \u2208 Rm by letting ds = N\u2212s+1s ws for every s \u2208 [m] 2 Normalize: C \u2190 \u2211m s=1 ds and d\u2190 d/C 3 for t = 1, 2, . . . , T do 4 loss\u2190 0 5 for j = 1, 2, . . . , B do 6 Sample sj from [m] according to the distribution vector d 7 Sample Sj uniformly from {R \u2286 [N + 1] | |R| = sj}\n8 loss\u2190 loss+ ( U(Sj \u2229 [N ])\u2212 \u03d5o1N+1\u2208Sj \u2212 \u2211 i\u2208Sj\u2229[N ] \u03d5\u03b8(xi, yi) )2 9 loss\u2190 loss/B\n10 Update \u03b8 and \u03d5o using loss Evaluation Phase: C \u00b7 (\u03d5\u03b8(x, y)\u2212 \u03d5o)\nand Jethani et al. (2022) exploited the least square regression (7) together with the additive efficient normalization proposed by Ruiz et al. (1998, Definition 11) to train neural networks that can then predict the Shapley value of any unseen instance in a single forward pass.\nFrom the formula (1) of probabilistic values, it is clear that the importance value of a data point rests on the selected databaseDv . Therefore, probabilistic values could change when the underlying databaseDv changes (e.g., due to adding or removing data points). To deal with this issue, Ghorbani et al. (2020) proposed the distributional Shapley value that substitutes a data distribution D for Dv , as show in Eq. (2). In practice, one could sample a dataset B from D instead to represent the latter. Generally, the distributional values are not equal to the probabilistic values. A simple example is when Dv and B are entirely disjoint. Despite that, if Dv = B, each distributional value reduces to some probabilistic value up to a constant. Theorem 1. Suppose a probability vector w \u2208 Rm, m < N , and a utility function U \u2208 G are given. Introduce a null data point labeled by N + 1 into Dv \u2261 [N ], and define U : 2[N+1] \u2192 R by letting U(S) = U(S \u2229 [N ]) for every S \u2286 [N + 1]. Let d \u2208 Rm be a probability vector that satisfies ds \u221d N\u2212s+1s ws for every s \u2208 [m], and v \u2217 be the unique solution to\nargmin v\u2208RN+1 E s d\u223c[m]\nES\u223cBs ( U(S)\u2212 \u2211 i\u2208S vi )2 , (13)\nwhere B = 1N+1 \u2211 z\u2208[N+1] \u03b4z . There is, for every i \u2208 [N ],\nC \u00b7 (v\u2217i \u2212 v\u2217N+1) = \u03d5 B,w i (U) = Esw\u223c[m]ES\u223cBs\u22121 [U(S \u222a i)\u2212 U(S)] (14)\nwhere C = \u2211m\ns=1 N\u2212s+1 N ws and B = 1 N \u2211 z\u2208[N ] \u03b4z .\nRemark 3. As suggested by Algorithm 2, \u03d5pi = (\u2211N s=1 N N\u2212s+1ps ) (bi \u2212 bN+1) for every i \u2208 [N ]\nwhere b = E[b\u0302]. In the context of the proof of theorem 1, ps = ( N\u22121 s\u22121 ) ps = N+s\u22121 N ws C , which\nimplies \u2211N\ns=1 N N\u2212s+1ps = \u2211m s=1 ws C = 1 C . Therefore, by Eq. (14), \u03d5 B,w i = C \u00b7 \u03d5i = bi \u2212 bN+1 for\nevery i \u2208 [N ]. In other words, b\u0302i \u2212 b\u0302N+1 derived in Algorithm 2 is an unbiased estimator for \u03d5B,wi .\nTheorem 1 provides a concrete way to estimate distributional values using trained models. Precisely, let \u03d5\u03b8(x, y) be a trainable model parameterized by \u03b8; x and y stands for features and label, respectively; we propose learning a value estimator \u03d5\u03b8 based on the specified utility function U \u2208 G through optimizing\nargmin \u03b8,\u03d5o E s d\u223c[m] E S\u223cBs\n( U(S \u2229 [N ])\u2212 \u03d5o1N+1\u2208S \u2212 \u2211 i\u2208S\\N+1 \u03d5\u03b8(xi, yi) )2 . (15)\nGenerally, it is impractical to train a value estimator using the exact distributional values, or regressing with a reasonable approximation as supervised signals, since it is extremely expensive to calculate them exactly. For example, in our experiments where N = 10, 000 and m = 1, 000, each unseen data point requires \u2211999 s=0 ( 10,000 s ) utility evaluations to compute exactly. Plus, for each unseen data point, an estimator using 200, 000 utility evaluations only reached roughly 10\u22125 relative difference, not to mention that each evaluation could be as computationally costly as the procedure of training and testing a model. By comparison, the merit of training value estimators through optimizing the problem (15) is clear: i) we do not have to be concerned with what the exact distributional values are, and ii) Theorem 1 guarantees that each value estimator will be trained towards the exact distributional values. The outline of training such a value estimator is summarized in Algorithm 3."
        },
        {
            "heading": "4 EVALUATION",
            "text": "In this section we perform experiments to i) verify the faster convergence of Algorithms 1 and 2, and ii) demonstrate that a distributional value can be effectively approximated by a value estimator trained using the objective (15). All classification datasets we used are from open resources, which are iris, wind (both are from OpenML), FMNIST (Xiao et al. 2017) and MNIST. The utility function U(S) is designed to be the performance reported on a held-out dataset Du for the chosen model trained on S \u2286 Dv . Without stated explicitly, the performance is measured by classification accuracy. Specifically, we fix the random seed as 2024 for every U so that they are deterministic. Meanwhile, we adopt one-epoch learning for evaluating U(S) (Ghorbani and Zou 2019); U(\u2205) is the performance of the initialized model. Logistic regression is implemented for iris and wind, while LeNet (LeCun et al. 1998) is employed for FMNIST and MNIST. All results are reported with mean and standard deviation."
        },
        {
            "heading": "4.1 FASTER CONVERGENCE OF THE PROPOSED ESTIMATORS",
            "text": "We perform this experiment on all above-mentioned datasets. The learning rate for every U is set to be 1.0 for the datasets iris and wind, whereas it is 0.1 for FMNIST and MNIST. To evaluate the chosen probabilistic values exactly, we set |Dv| = |Du| = 21. All estimators are run 30 times with the random seed for sampling set to be 0, 1, . . . , 29.\nWe compare to the following benchmarks:\n\u2022 those designed specifically for the Shapley value, including complement (Zhang et al. 2023b), SHAP (Lundberg and Lee 2017), SHAP-paired (Covert and Lee 2021), group testing (Jia et al. 2019b);\n\u2022 those work for the entire family of probabilistic values, including the sampling lift, see Proposition 6, and its weighted variant (Kwon and Zou 2022a), as well as the generalized ARM, see Proposition 9;\n\u2022 AME (Lin et al. 2022), restricted to a subfamily of probabilistic values; \u2022 MSR (Wang and Jia 2023), proposed specifically for the Banzhaf value.\nThe probabilistic values we considered are: i) Beta(1, 1), which equals to the Shapley value, ii) Beta(2, 2),1 iii) Beta(4, 1), and iv) the Banzhaf value. Note that AME does not apply to Beta(4, 1) and the Shapley, because both cases lead to MP =\u221e, which means the least square regression that AME solves is not guaranteed to have an optimal solution.\nWe present some of the results in Figure 1, while defer others to the Appendix. These results support that the two proposed estimators are currently among the (empirically) fastest tier in the group of generic estimators. For the Shapley value, however, the more specifically-designed estimators, i.e., SHAP, SHAP-paired and the complement, are faster than every other estimator in some experiment settings. Notice that their faster convergence comes at the cost of using \u0398(N2) memory storage instead of \u0398(N). For the Banzhaf value, the two proposed estimators are as fast as the currently state-of-the-art MSR estimator. Although AME is developed from another form of least\n1This probabilistic value is special: i) it can be derived from the best min-polynomial approximation of the Lov\u00e1sz extension of U (Marichal and Mathonet 2008, Corollary 19), and ii) it is uniquely characterized by a set of axioms proposed in multi-criteria decision making (Grabisch and Labreuche 2001, Theorem 2)\nsquare regression, it does not always converge as fast as ours, e.g., for the Beta(2,2) probabilistic value. Meanwhile, it can be observed that the generalized ARM almost enjoys the same empirical convergence across different experiment settings."
        },
        {
            "heading": "4.2 TRAINING VALUE ESTIMATORS",
            "text": "In this experiment, we demonstrate the effectiveness of training value estimators using Algorithm 3 on FMNIST and MNIST. The corresponding results for MNIST are included in the Appendix. The learning rate for every U is set to be 0.01. Specifically, LeNet is the architecture we employ as a value estimator \u03d5\u03b8, where the input of the softmax layer is taken as the output of \u03d5\u03b8. We use the Adam optimizer (Kingma and Ba 2014) with learning rate 0.001. The batch size B is set to be 10, 000, and we take |Dv| = 10, 000 and |Du| = 500, both of which are from the training set. The number of total batches we generated is 1000. After these batches have all been fed into the value estimators, we permute and reuse them to continue training. In other words, we have 1000 utility evaluations in total per data point for training value estimators. Lastly, the probability vector w \u2208 R1,000 we employ is ws \u221d s\u2212 1 2 for s \u2208 [1, 000].\nTo measure the performance of the trained value estimators, we take another 200 data from the test set, denoted by Dt. In other words, Dt is unseen during the training of value estimators. Without learning, the distributional value of each data point z \u2208 Dt can be approximated using the formula\n\u03d5B,wz = Esw\u223c[m]ES\u223cBs\u22121 [U(S \u222a z)\u2212 U(S)] where B = 1 N \u2211 z\u2208Dv \u03b4z. (16)\nTherefore, we approximate the distributional values of all data in Dt by sampling based on Eq. (16) until the relative difference is smaller than 10\u22125, which eventually leads to over 200, 000 utility evaluations for each data point. Then, we treat this well-approximated distributional values \u03d5 as the ground-truth. Meanwhile, \u03d5\u0302t denotes the corresponding predicted distributional values from the value estimator trained using t batches. The performance curves of the value estimators during training are shown in Figure 2. First of all, the increasing curve indicates that the trained value estimators are able to gradually learn the exact ranking. Secondly, the decreasing curve of the relative difference suggests that, as shown in Algorithm 3, the transform C \u00b7 (\u03d5\u03b8(x, y) \u2212 \u03d5o) used for the trained value estimators to predict distributional values is essential! Interestingly, though the curve for the empirical loss, i.e., 1B \u2211B j=1(U(Sj \u2229 [N ]) \u2212 \u03d5o1N+1\u2208Sj \u2212 \u2211 i\u2208Sj\u2229[N ] \u03d5\u03b8(xi, yi))\n2, remains almost horizontal, the value estimators still get improved over time.\nNext, we examine the efficiency of the trained value estimators by comparing them with the convergence of the approximation based on Eq. (16). Specifically, for each random seed, we take out the\ntrained value estimator having the smallest relative difference, and report the averaged performance as the one we eventually have by training. The comparison is shown in the first two plots of Figure 3. As clearly shown, approximation using 10, 000 utility evaluations is still inferior to the trained value estimators in terms of both the relative difference and the Spearman\u2019s rank correlation coefficient.\nNext, we inspect into the quality of valuation on unseen data. We take the whole test set, which constitutes a dataset Dr of 10, 000 data. Note that these data are unseen in the training phase. Then, we employ the well-trained value estimators to valuate all these data. As suggested by Ghorbani and Zou (2019), it is empirically expected that removing data with highest importance value should result in steepest performance drop. Precisely, we order Dr from high-value to low-value. Then, after removing some fraction of high-value data, we retrain the LeNet on the remaining and test the performance on another dataset D\u0303u of 500 data, which is also separate from Dv and Du used during training. Generally, it is very expensive and almost impractical to valuate this amount of data using any probabilistic value. Nevertheless, if U is defined as the accuracy of the standard k nearest neighbor classifier, which produces sparse marginal gains, the Shapley value can be computed exactly with time complexity \u0398(N logN) (Jia et al. 2019a). Therefore, we employ the KNN-Shapley as our benchmark in the removal experiment. Specifically, the features used by KNN-Shapley are extracted using a well-trained LeNet. As shown by the rightmost plot in Figure 3, our trained value estimators produce the steepest performance drop overall."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we start from the least square regression to develop two generic, faster estimators for the whole family of probabilistic values. One is designed for applications where ranking suffices while the other aims to approximate the exact values. The faster convergence of the two proposed estimators is theoretically guaranteed, and is also verified empirically in our experiments. Besides, we also demonstrate how to cast each distributional value into a least square problem, making it theoretically-grounded to train value estimators so that each unseen data can be valuated in a single forward pass. Notably, our experiments show that a value estimator can be well-trained under our proposed framework. Our work significantly broadens the practicality to deploy value-based data valuation methods on rather large datasets."
        },
        {
            "heading": "A LEMMAS",
            "text": "Lemma 1. The matrix aJn + bIn is invertible if and only if na+ b \u0338= 0 and b \u0338= 0.\nProof. Observe that the eigenvalues of Jn are 0 (with n \u2212 1 independent eigenvectors) and n, and thus the eigenvalues of aJn + bIn are b and na + b. Therefore, aJn + bIn is invertible if and only if b \u0338= 0 and na+ b \u0338= 0.\nLemma 2. Suppose the matrix A = aJn + bIn is invertible, then A\u22121 = \u2212 ab(na+b)Jn + 1 b In.\nProof. A priori is that A\u22121 admits the form of xJn+yIn. Using AA\u22121 = In, we have the equation\n(na+ b)x+ (a+ b)y = 1,\n(na+ b)x+ ay = 0, (17)\nwhich leads to that x = \u2212 ab(na+b) and y = 1 b ."
        },
        {
            "heading": "B PROOFS FOR THE PROPOSED ALGORITHMS",
            "text": "Proposition 1. Let p \u2208 RN be a vector that defines a probabilistic value. Define bp : G \u2192 RN where bpi (U) = \u2211\nS\u228a[N ]\nmps \u00b7 1i\u2208SU(S) for every i \u2208 [N ] and every U \u2208 G, (9)\nand mps = ps + ps+1 for s \u2208 [n \u2212 1]. Then, for every U \u2208 G, bp(U) = \u03d5p(U) + cp(U)1n where cp : G \u2192 R. In other words, bp(U) and \u03d5p(U) produce the same ranking for every U \u2208 G.\nProof.\n\u03d5pi (U) = \u2211\nS\u2286[N ]\\i\nps+1(U(S \u222a i)\u2212 U(S)) = bpi (U)\ufe37 \ufe38\ufe38 \ufe37\u2211 S\u228a[N ] : i\u2208S mpsU(S)+ cp(U)\ufe37 \ufe38\ufe38 \ufe37 pNU([N ])\u2212 \u2211 S\u228a[N ] ps+1U(S)\n(18) for every i \u2208 [N ].\nProposition 2. Suppose a utility function U \u2208 G and a vector p \u2208 RN that defines a probabilistic value are given, for the problem\nargmin u\u2208RN \u2211 \u2205\u228aS\u228a[N ] mps ( U(S)\u2212 \u2211 i\u2208S ui )2 , (10)\nwhere mps = ps+ ps+1 for s = 1, 2, . . . , N \u2212 1, its unique optimal solution u\u2217 meets that \u03d5p(U) = u\u2217 + c1N for some constant c \u2208 R; 1N \u2208 RN is the all-one vector. In other words, \u03d5p(U) and u\u2217 have the same ranking for Dv .\nProof. Since the problem (10) is convex, its optimal solution can be obtained by letting its derivative equal 0, which yields\nAu\u2217 = b\nwhere A = eJN + dIN with e = N\u22121\u2211 s=2 ( N \u2212 2 s\u2212 2 ) mps and d = ( N\u22121\u2211 s=1 ( N \u2212 1 s\u2212 1 ) mps ) \u2212 e,\nand bi = \u2211\nS\u228a[N ] : i\u2208S\nmps \u00b7 U(S) for every i \u2208 [N ].\n(19)\nJN \u2208 RN\u00d7N denotes the all-one matrix, IN \u2208 RN\u00d7N is the identity matrix. Specifically,\nd = mp1 + N\u22121\u2211 s=2 (( N \u2212 2 s\u2212 1 ) + ( N \u2212 2 s\u2212 2 )) mps \u2212 N\u22121\u2211 s=2 ( N \u2212 2 s\u2212 2 ) mps = N\u22121\u2211 s=1 ( N \u2212 2 s\u2212 1 ) mps\n= N\u22121\u2211 s=1 ( N \u2212 2 s\u2212 1 ) ps + N\u22121\u2211 s=1 ( N \u2212 2 s\u2212 1 ) ps+1 = p1 + N\u22121\u2211 s=2 ( N \u2212 2 s\u2212 1 ) ps + N\u22121\u2211 s=2 ( N \u2212 2 s\u2212 2 ) ps + pN\n= N\u2211 s=1 ( N \u2212 1 s\u2212 1 ) ps = 1.\n(20) By lemmas 1 and 2, A\u22121 = \u2212 e(eN+1)JN + IN , there is u \u2217 = b+ r1N for some r \u2208 R.\nProposition 3. Let U \u2208 G and a vector p \u2208 Rn that defines a probabilistic value be given. The unique optimal solution n\u2217 to the problem\nargmin n\u2208RN+1 \u2211 \u2205\u228aS\u228a[N+1] ps ( U(S)\u2212 \u2211 i\u2208S ni )2 (11)\nsatisfies that for every i \u2208 [N ] \u03d5pi (U) = n \u2217 i \u2212 n\u2217N+1. (12)\nProof. Calculating b \u2208 RN+1 based on Eq. (19) yields bi = \u2211\nS\u228a[N+1] : i\u2208S\npsU(S) = pNU([N ]) + \u2211\nS\u228a[N ] : i\u2208S\n(ps + ps+1)U(S) \u2200i \u2208 [N ],\nbN+1 = \u2211\nS\u228a[N+1] : N+1\u2208S\npsU(S) = \u2211\nS\u228a[N ]\nps+1U(S). (21)\nAs provided by Eq. (18), \u03d5pi (v) = bi\u2212bN+1 for every i \u2208 [N ]. On the other hand, the corresponding A \u2208 RN+1\u00d7N+1 based on Eqs. (19) and (20) satisfies A = (\u2211N s=2 ( N\u22121 s\u22122 ) ps ) JN+1 + IN+1. By lemmas 1 and 2, A\u22121 = eJN+1 + IN+1 for some e \u2208 R. Since n\u2217 = A\u22121b, there is n\u2217i = t + bi for every i \u2208 [N + 1] where t \u2208 R, which implies n\u2217i \u2212 n\u2217N+1 = bi \u2212 bN+1 for every i \u2208 [N ].\nTheorem 1. Suppose a probability vector w \u2208 Rm, m < N , and a utility function U \u2208 G are given. Introduce a null data point labeled by N + 1 into Dv \u2261 [N ], and define U : 2[N+1] \u2192 R by letting U(S) = U(S \u2229 [N ]) for every S \u2286 [N + 1]. Let d \u2208 Rm be a probability vector that satisfies ds \u221d N\u2212s+1s ws for every s \u2208 [m], and v \u2217 be the unique solution to\nargmin v\u2208RN+1 E s d\u223c[m]\nES\u223cBs ( U(S)\u2212 \u2211 i\u2208S vi )2 , (13)\nwhere B = 1N+1 \u2211 z\u2208[N+1] \u03b4z . There is, for every i \u2208 [N ],\nC \u00b7 (v\u2217i \u2212 v\u2217N+1) = \u03d5 B,w i (U) = Esw\u223c[m]ES\u223cBs\u22121 [U(S \u222a i)\u2212 U(S)] (14)\nwhere C = \u2211m\ns=1 N\u2212s+1 N ws and B = 1 N \u2211 z\u2208[N ] \u03b4z .\nProof. Since U(S \u222a i)\u2212 U(S) = 0 if i \u2208 S, for each i \u2208 [N ],\n\u03d5B,wi (U) = \u2211\nS\u2286[N ]\\i : |S|<m\nws+1\n( N\ns\n)\u22121 (U(S \u222a i)\u2212 U(S)) = C \u00b7 \u03d5pi (U) (22)\nwhere ps = ( N s\u22121 )\u22121ws C if s \u2264 m and 0 otherwise; here, C = \u2211m s=1 ( N\u22121 s\u22121 )( N s\u22121 )\u22121\nws =\u2211m s=1 N\u2212s+1 N ws. Write ds = \u03b1 N\u2212s+1 s ws for every s \u2208 [N ], where \u03b1 is a scalar that makes d a distribution vector. As proved by proposition 3 and argued by remark 1, \u03d5 = (v\u2217i \u2212 v\u2217N+1)1\u2264i\u2264N\nis some probabilistic value, it is sufficient to show that p exactly defines \u03d5. Let q \u2208 RN be the vector that defines \u03d5, then qs \u221d ds ( N s )\u22121 for every s \u2208 [m] and 0 otherwise, which means the scalar for d is\n\u03b2 = m\u2211 s=1 ( N \u2212 1 s\u2212 1 )( N s )\u22121 ds = \u03b1 \u00b7 C. (23)\nTherefore, for every s \u2208 [m], qs = ( N s )\u22121 ds \u03b2 = ( N s )\u22121N\u2212s+1 s ws C = ps."
        },
        {
            "heading": "C PROOFS FOR CONVERGENCES",
            "text": "Proposition 6. Suppose a probability value \u03d5p and a utility function U \u2208 G are given, any estimator \u03d5\u0302 based on the sampling lift strategy requires O(N 2\n\u03f52 log N \u03b4 ) evaluations of v to achieve\nP (\u2225\u03d5\u0302(U)\u2212\u03d5 p(U)\u22252\n\u2225\u03d5p(U)\u22252 \u2265 \u03f5) \u2264 \u03b4.\nThe sampling lift strategy refers to any estimator designed according to \u03d5pi (U) = \u2211\nS\u2286[N ]\\i\nps+1 (U(S \u222a i)\u2212 U(S)) = ES [U(S \u222a i)\u2212 U(S)] (24)\nfor every i \u2208 [N ] as \u2211N\ns=1 ( N\u22121 s\u22121 ) ps = 1. Precisely, U(S \u222a i) \u2212 U(S) is a random variable\nfollowing a certain distribution, denoted by Pi, on all subsets S \u2286 [N ]\\i. Let T be the total number of samples S = {(S11 , S21 , . . . , SN1 ), (S12 , S22 , . . . , SN2 ), . . . , (S1T , S2T , . . . , SNT )} where each Si = {Si1, Si2, . . . , SiT } is supposed to follow Pi. Then, the estimator based on the sampling lift strategy is \u03d5\u0302i(U) = 1T \u2211T k=1(v(S i k \u222a i)\u2212 v(Sik)). There are two slightly different implementations: i) sampling each Si independently, and ii) sampling Si for some i \u2208 [n], and then reusing all samples in Si for every other j \u2208 [n] by swapping i and j. Nevertheless, the proof below works for both of them. Besides, the proof is adapted from (Wang and Jia 2023, Theorem 4.8) where they stated for the Banzhaf value. Nevertheless, we point out that it applies to all probabilistic values.\nProof. Let r = maxS\u2286[N ] |U(S)|. For simplicity, write \u03d5 = \u03d5p(U) and \u03d5\u0302 = \u03d5\u0302(U). Fix an i \u2208 [N ], since E[\u03d5\u0302i] = \u03d5i, by the Hoeffding\u2019s inequality,\nP (|\u03d5\u0302i \u2212 \u03d5i| \u2265 \u03f5) \u2264 2 exp(\u2212 T\u03f52\n2r2 ). (25)\nThen, P (\u2225\u03d5\u0302\u2212 \u03d5\u22252 \u2265 \u03f5\u2225\u03d5\u22252) \u2264 P ( \u22c3\ni\u2208[N ]\n|\u03d5\u0302i \u2212 \u03d5i| \u2265 \u03f5\u2225\u03d5\u22252\u221a N ) \u2264 2N exp(\u2212T\u03f5 2\u2225\u03d5\u222522 2Nr2 ) (26)\nLetting \u03b4 \u2265 2N exp(\u2212T\u03f5 2\u2225\u03d5\u222522 2Nr2 ) leads to T \u2265 2Nr2 \u03f52\u2225\u03d5\u222522 log 2N\u03b4 . Since U has to be evaluated 2N times for each sample, it requires O(N 2\n\u03f52 log N \u03b4 ) evaluations to achieve an (\u03f5, \u03b4)-approximation.\nProposition 4. For any vector p \u2208 RN that defines a probabilistic value and every utility function U \u2208 G, the estimator b\u0302 in Algorithm 1 requires T = O(\u03ba(N)N\u03f52 log N \u03b4 ) samples, which is equal to the number of utility evaluations, to achieve P (\u2225b\u0302\u2212b\u030c\u22252\u2225b\u030c\u22252 \u2265 \u03f5) \u2264 \u03b4.\nProof. The proof is adapted from (Wang and Jia 2023, Theorem 4.9). Let r = maxS\u2286[N ] |U(S)|, \u03b3(N) = \u2211N\u22121 s=1 ( N\u22121 s\u22121 )(ps+ps+1)\u2211N\u22121\ns=1 ( N s )(ps+ps+1)\n. .\nFor convenience, write S = {S1, S2, . . . , ST } that contains all sampled subsets, and b\u030c =(\u2211N\u22121 s=1 ( N\u22121 s\u22121 ) (ps + ps+1) )\u22121 bp(U), cf. Eq. (9). Plus, write Ti = |{S \u2208 S | i \u2208 S}| for every i \u2208 [N ]. Fix an i \u2208 [N ], define\nbi = 1\n\u03b3(N)T \u2211 S\u2208S : i\u2208S U(S). (27)\nThen, we have\n|b\u0302i \u2212 bi| = \u2223\u2223\u2223\u2223\u2223 ( 1 Ti \u2212 1 \u03b3(N)T ) \u2211 S\u2208S : i\u2208S U(S) \u2223\u2223\u2223\u2223\u2223 \u2264 r\u03b3(N)T |\u03b3(N)T \u2212 Ti|. (28) Note that this inequality |b\u0302i \u2212 bi| \u2264 r\u03b3(N)T |\u03b3(N)T \u2212 Ti| still holds when Ti = 0. Since Ti \u223c binomial(T, \u03b3(N)), by the Hoeffding\u2019s inequality, there is\nP (|Ti \u2212 \u03b3(N)T | \u2265 \u2206) \u2264 2 exp(\u2212 2\u22062\nT ). (29)\nTherefore, |b\u0302i \u2212 bi| < r\u2206\u03b3(N)T provided that |Ti \u2212 \u03b3(N)T | < \u2206. Since bi equals to bi = 1 T \u2211 S\u2208S \u03b2(s,N)U(S) where \u03b2(s,N) = \u03b3\n\u22121(N) if i \u2208 S and 0 otherwise, and E[bi] = b\u030ci, using the Heoffding\u2019s inequality again yields\nP (|bi \u2212 b\u030ci| \u2265 \u03c3) \u2264 2 exp(\u2212 2\u03b32(N)T\u03c32\nr2 ) (30)\nTherefore,\nP (|b\u0302i \u2212 b\u030ci| \u2265 \u03f5) = P (|b\u0302i \u2212 b\u030ci| \u2265 \u03f5 \u2229 |Ti \u2212 \u03b3(N)T | < \u2206) + P (|b\u0302i \u2212 b\u030ci| \u2265 \u03f5 \u2229 |Ti \u2212 \u03b3(N)T | \u2265 \u2206)\n\u2264 P (|b\u0302i \u2212 b\u030ci| \u2265 \u03f5 | |Ti \u2212 \u03b3(N)T | < \u2206) + 2 exp(\u2212 2\u22062\nT )\n\u2264 P (|bi \u2212 b\u030ci| \u2265 \u03f5\u2212 r\u2206\n\u03b3(N)T | |Ti \u2212 \u03b3(N)T | < \u2206) + 2 exp(\u2212\n2\u22062\nT )\n\u2264 P (|bi \u2212 b\u030ci| \u2265 \u03f5\u2212 r\u2206\u03b3(N)T )\n1\u2212 2 exp(\u2212 2\u22062T ) + 2 exp(\u22122\u2206\n2\nT )\n\u2264 2 exp(\u2212\n2\u03b32(N)T(\u03f5\u2212 r\u2206\u03b3(N)T ) 2\nr2 ) 1\u2212 2 exp(\u2212 2\u22062T ) + 2 exp(\u22122\u2206\n2\nT )\n\u2264 3 exp(\u2212 2\u03b32(N)T\n( \u03f5\u2212 r\u2206\u03b3(N)T )2 r2 ) + 2 exp(\u22122\u2206 2 T )\n(31) where 1\u22122 exp(\u2212 2\u2206 2\nT ) \u2265 2 3 when T is sufficiently large. The next step is to determine \u2206 by solving\nthe equation\u2212 2\u03b32(N)T(\u03f5\u2212 r\u2206\u03b3(N)T )\n2\nr2 = \u2212 2\u22062 T , which yields \u2206 = \u03b3(N)T\u03f5 (r+1) . Note that this solution gives\n\u03f5\u2212 r\u2206\u03b3(N)T = \u03f5 r+1 > 0 and 2\u22062 T = 2\u03b32(N)T\u03f52 (r+1)2 . Besides, the inequality 1\u2212 2 exp(\u2212 2\u22062 T ) \u2265 2 3 leads to T \u2265 (r+1) 2 log(6)\n2\u03b32(N)\u03f52 . Eventually, we have\nP (|b\u0302i \u2212 b\u030ci| \u2265 \u03f5) \u2264 5 exp(\u2212 2\u03b32(N)T\u03f52\n(r + 1)2 ) (32)\nprovided that T \u2265 (r+1) 2 log(6)r2\n2\u03b32(N)\u03f52 . Then,\nP (\u2225b\u0302\u2212 b\u030c\u22252 \u2265 \u03f5\u2225b\u030c\u22252) \u2264 P ( \u22c3\ni\u2208[N ]\n|b\u0302i \u2212 b\u030ci| \u2265 \u03f5\u2225b\u030c\u22252\u221a N ) \u2264 5N exp(\u22122\u03b3 2(N)T\u03f52\u2225b\u030c\u222522 (r + 1)2N ). (33)\nSolving 5N exp(\u2212 2\u03b3 2(N)T\u03f52\u2225b\u030c\u222522 (r+1)2N ) \u2264 \u03b4 leads to T \u2265 (r+1)2N\n2\u03b32(N)\u03f52\u2225b\u030c\u222522 log 5N\u03b4 , and thus it requires\nmax( (r + 1)2 log(6)\n2\u03b32(N)\u03f52\u2225b\u030c\u222522 ,\n(r + 1)2N\n2\u03b32(N)\u03f52\u2225b\u030c\u222522 log\n5N\n\u03b4 ) = O(\n\u03ba(N)N\n\u03f52 log\nN\n\u03b4 ) (34)\nevaluations of U to achieve an (\u03f5, \u03b4)-approximation. Here, \u03ba(N) = \u03b3\u22122(N).\nProposition 5. For every vector p \u2208 RN that defines a probabilistic value and every utility function U \u2208 G, the estimator \u03d5\u0302 in Algorithm 2 requires T = O( \u03c4(N)N\u03f52 log N \u03b4 ) samples, which is equal to the number of utility evaluations, to achieve P (\u2225\u03d5\u0302\u2212\u03d5 p(U)\u22252\n\u2225\u03d5p(U)\u22252 \u2265 \u03f5) \u2264 \u03b4.\nProof. For simplicity, write \u03d5 = \u03d5p(U). Let b\u030c = E[b\u0302], by Eq. (32), there is, for every i \u2208 [N + 1],\nP (|b\u0302i \u2212 b\u030ci| \u2265 \u03f5) \u2264 5 exp(\u2212 2\u03b3\u03032(N)T\u03f52\n(r + 1)2 ) (35)\nprovided that T \u2265 (r+1) 2 log(6)\n2\u03b3\u03032(N)\u03f52 . Therefore, for every i \u2208 [N ],\nP (|(b\u0302i \u2212 b\u0302N+1)\u2212 (b\u030ci \u2212 b\u030cN+1)| \u2265 \u03f5) \u2264 P (|b\u0302i \u2212 b\u030ci| \u2265 \u03f5\n2 \u222a |b\u0302N+1 \u2212 b\u030cN+1| \u2265\n\u03f5 2 )\n\u2264 10 exp(\u2212 \u03b3\u0303 2(N)T\u03f52\n4(r + 1)2 ).\n(36)\nlet \u03c7(N) = \u2211N\ns=1 ( N s\u22121 ) ps and \u03b7(N) = \u2211N s=1 ( N+1 s ) ps, note that \u03b3\u0303(N) = \u03c7(N) \u03b7(N) . By proposition 3,\n\u03c7(N)(b\u030ci \u2212 b\u030cN+1) = \u03d5i for every i \u2208 [N ]. So, for every i \u2208 [N ],\nP (|\u03d5\u0302i \u2212 \u03d5i| \u2265 \u03f5) = P (|(b\u0302i \u2212 b\u0302N+1)\u2212 (b\u030ci \u2212 b\u030cN+1)| \u2265 \u03f5 \u03c7(N) ) \u2264 10 exp(\u2212 T\u03f5\n2\n4(r + 1)2\u03b72(N) ).\n(37) Therefore,\nP (\u2225\u03d5\u0302\u2212 \u03d5\u22252 \u2265 \u03f5) \u2264 P ( \u22c3\ni\u2208[N ]\n|\u03d5\u0302i \u2212 \u03d5i| \u2265 \u03f5\u221a N ) \u2264 10N exp(\u2212 T\u03f5 2 4(r + 1)2N\u03b72(N) ), (38)\nand thus P (\u2225\u03d5\u0302\u2212\u03d5\u22252\u2225\u03d5\u22252 \u2265 \u03f5) \u2264 10N exp(\u2212 T\u03f52\u2225\u03d5\u222522 4(r+1)2N\u03b72(N) ). Solving \u03b4 \u2265 10N exp(\u2212 T\u03f52\u2225\u03d5\u222522 4(r+1)2N\u03b72(N) ) yields T \u2265 4(r+1) 2N\u03b72(N)\n\u03f52\u2225\u03d5\u222522 log 10N\u03b4 . To conclude, \u03d5\u0302 requires\nmax( (r + 1)2 log(6) 2\u03b3\u03032(N)\u03f52\u2225\u03d5\u222522 , 4(r + 1)2N\u03b72(N) \u03f52\u2225\u03d5\u222522 log 10N \u03b4 ) = O( \u03c4(N)N \u03f52 log N \u03b4 ) (39)\nevaluations of U to achieve an (\u03f5, \u03b4)-approximation. Here, \u03c4(N) = \u03b72(N)."
        },
        {
            "heading": "D LEAST SQUARE VALUES",
            "text": "In this appendix, we demonstrate how we obtained proposition 2 at the very beginning without knowing proposition 1.\nDefinition 1 (Least Square Values Ruiz et al. (1998, Definition 5)). Suppose a non-negative nonzero vector m \u2208 RN\u22121 is given, a least square value \u03c8m : G \u2192 RN is defined to be, for every U \u2208 G, \u03c8m(U) is the unique optimal solution to the problem\nargmin x\u2208RN \u2211 \u2205\u228aS\u228a[N ] ms\n( U(S)\u2212 U(\u2205)\u2212\n\u2211 i\u2208S xi\n)2\ns.t. n\u2211\ni=1\nxi = U([N ])\u2212 U(\u2205).\n(40)\nSpecifically, Ruiz et al. (1998, Theorem 8) developed a system of axioms that uniquely characterizes the family of least square values. Moreover, its relationship with the family of probabilistic values were also revealed.\nProposition 7 (Ruiz et al. (1998, Theorem 12)). For each vector of weights p \u2208 RN that defines a probabilistic value, define m \u2208 RN\u22121 by letting ms = ps + ps+1, there exists some function c : G \u2192 R such that\n\u03c8mi (U) = \u03d5 p i (U) + c(U) (41)\nfor every i \u2208 [N ] and every U \u2208 G.\nAs proposed by Ruiz et al. (1998, Definition 11), for each probabilistic value \u03d5p, its additive efficient normalization \u03d5 p is defined to be\n\u03d5 p (U) = \u03d5p(U) +\n1\nN (v([N ])\u2212 v(\u2205)\u2212 \u2211 i\u2208[N ] \u03d5pi (U)) \u2200U \u2208 G. (42)\nUsing proposition 7, one can verify that \u03d5 p\nbelongs to the family of least square values. On the other hand, we notice that the constraint of the problem (40) can be removed while having the same ranking.\nProposition 8. Suppose a utility function U \u2208 G and a non-negative non-zero vector m \u2208 RN\u22121 are given, the unique optimal solution u\u2217 to the problem\nargmin u\u2208RN \u2211 \u2205\u228aS\u228a[N ] ms\n( U(S)\u2212\n\u2211 i\u2208S ui\n)2 (43)\nsatisfies that there exists some c \u2208 R such that\nu\u2217i = \u03c8 m i (U) + c (44)\nfor every i \u2208 [N ].\nProof. Since the problem (40) is convex, it can be solved using the KKT condition. Specifically, by introducing a dual variable \u03bb \u2208 R, there is\n\u03d5\u22a4A\u03d5\u2212 2b\u22a4\u03d5+ \u2211\n\u2205\u228aS\u228a[N ] ms(U(S)\u2212 U(\u2205))2 + 2\u03bb(U([N ])\u2212 U(\u2205)\u2212 N\u2211 i=1 \u03d5i)\nwhere Aij =\n{\u2211N\u22121 s=1 ( N\u22121 s\u22121 ) ms, i = j\u2211N\u22121\ns=2 ( N\u22122 s\u22122 ) ms, i \u0338= j\n, and bi = \u2211\nS\u228a[N ] : i\u2208S\nmsU(S) \u2200i \u2208 [N ].\n(45)\nLetting its derivative equal 0 yields\nA\u03d5\u2212 b+ \u03bb1N = 0 and 1\u22a4N\u03d5 = U([N ])\u2212 U(\u2205), (46)\nwhich leads to the optimal solution \u03d5\u2217 = A\u22121 ( b\u2212 1 \u22a4 NA\n\u22121b\u2212 U([N ]) + U(\u2205) 1\u22a4NA \u221211N 1N\n) . (47)\nSpecifically,\nN\u22121\u2211 s=1 ( N \u2212 1 s\u2212 1 ) ms \u2212 N\u22121\u2211 s=2 ( N \u2212 2 s\u2212 2 ) ms\n=m1 + N\u22121\u2211 s=2 (( N \u2212 2 s\u2212 1 ) + ( N \u2212 2 s\u2212 2 )) ms \u2212 N\u22121\u2211 s=2 ( N \u2212 2 s\u2212 2 ) ms = N\u22121\u2211 s=1 ( N \u2212 2 s\u2212 1 ) ms,\n(48)\nby lemma 1, the existence of A\u22121 is guaranteed. On the other hand, letting the derivative of the problem (45) equal 0 gives\nu\u2217 = A\u22121b. (49)\nBy lemma 2, we can obtain \u03d5\u2217 = u\u2217 + c1N (50)\nfor some c \u2208 R.\nTo conclude, proposition 2 is derived from propositions 7 and 8."
        },
        {
            "heading": "E GENERALIZED ARM",
            "text": "As proposed by Kolpaczki et al. (2023), the Shapley value can be rewritten as, for every i \u2208 [N ] and every U \u2208 G,\n\u03d5Shi (U) = ES\u223cP+ARM |i\u2208S [U(S)]\u2212 ES\u223cP\u2212ARM |i \u0338\u2208S [U(S)] (51)\nwhere P+ARM (S) = 1 s(Ns )H for every S \u2208 S+ = {\u2205 \u228a S \u2286 [N ]} and P\u2212ARM (S) =\n1\n(N\u2212s)(Ns )H for every S \u2208 S\u2212 = {S \u228a [N ]}; here, H = \u2211N\ns=1 1 s .\nThe estimator based on this formula is called the approximation-without-requiring-marginal estimator. We found that this methodology can be easily adapted for every other probabilistic value, which is summarized in the below.\nProposition 9. Suppose a utility function U \u2208 G and a vector p \u2208 RN that defines a probabilistic value are given, there is, for every i \u2208 [N ],\n\u03d5pi (U) = ES\u223cP+ARM [U(S) | i \u2208 S]\u2212 ES\u223cP\u2212ARM [U(S) | i \u0338\u2208 S] (52)\nwhere P+ARM (S) \u221d ps for every S \u2208 S+ and P \u2212 ARM (S) \u221d ps+1 for every S \u2208 S\u2212.\nProof. For simplicity, we write p+ and p\u2212 instead of P+ARM and P \u2212 ARM . Precisely, p + s = \u03b1 \u00b7 ps and p\u2212s = \u03b2 \u00b7 ps+1 such that \u2211N s=1 ( N s ) p+s = 1 and \u2211N\u22121 s=0 ( N s ) p\u2212s+1 = 1. Let S\n+ and S\u2212 be the corresponding samples from S+ and S\u2212, respectively. Fix an i \u2208 [N ], observe that,\nP (i \u2208 S+) = N\u2211 s=1 P (i \u2208 S+ | |S+| = s) \u00b7 P (|S+| = s) = N\u2211 s=1 ( N \u2212 1 s\u2212 1 )( N s )\u22121 \u00b7 ( N s ) p+s\n= \u03b1 N\u2211 s=1 ( N \u2212 1 s\u2212 1 ) ps = \u03b1.\n(53) Therefore,\nP (S+ | i \u2208 S+) = P (S +, i \u2208 S+) P (i \u2208 S+) = \u03b1 \u00b7 ps \u03b1 = ps. (54)\nSimilarly,\nP (i \u0338\u2208 S\u2212) = N\u22121\u2211 s=0 P (i \u0338\u2208 S\u2212 | |S\u2212| = s) \u00b7 P (|S\u2212| = s) = N\u22121\u2211 s=0 ( N \u2212 1 s )( N s )\u22121 \u00b7 ( N s ) p\u2212s\n= \u03b2 N\u22121\u2211 s=0 ( N \u2212 1 s ) ps+1 = \u03b2,\n(55) which leads to\nP (S\u2212 | i \u0338\u2208 S\u2212) = P (S \u2212, i \u0338\u2208 S\u2212) P (i \u0338\u2208 S\u2212) = \u03b2 \u00b7 ps+1 \u03b2 = ps+1. (56)"
        },
        {
            "heading": "F PRACTICAL PROBABILISTIC VALUES",
            "text": "As provided by Dubey et al. (1981), each semi-value, which is a subfamily of probabilistic values, can be expressed as \u03d5\u00b5i (UN ) = \u2211\nS\u2286[N ]\\i\n(\u222b 1 0 ts(1\u2212 t)N\u22121\u2212sd\u00b5(t) ) (UN (S \u222a i)\u2212 UN (S)) for every UN and i \u2208 [N ]\n(57)\nwhere \u00b5 is a probability measure on the interval [0, 1]. In other words, ps = \u222b 1 0 ts\u22121(1\u2212t)N\u2212sd\u00b5(t) for every 1 \u2264 s \u2264 N in view of Eq. (1). The subscript of UN is to indicate its domain 2[N ]. Conversely, each probability measure leads to a semi-value through Eq. (57), and this map is oneto-one.\nTo the best of our knowledge, practical semi-values, i.e., those ever studied in the previous references, include the Banzhaf value (Wang and Jia 2023) and the Beta Shapley values with \u03b1, \u03b2 \u2265 1 (Kwon and Zou 2022a; Kwon and Zou 2022b). Note that Beta(1, 1) is exactly the Shapley value. For the Banzhaf value, the corresponding \u00b5 is the Dirac delta distribution \u03b40.5, which leads to ps = 12N\u22121 . For Beta(\u03b1, \u03b2), the corresponding probability density function for \u00b5 is \u221d t\u03b2\u22121(1\u2212 t)\u03b1\u22121, and thus ps = \u0393(\u03b1+\u03b2) \u0393(\u03b1)\u0393(\u03b2) \u00b7 \u0393(\u03b2+s\u22121)\u0393(\u03b1+N\u2212s) \u0393(\u03b1+\u03b2+N\u22121) . Specifically, it is ps = (s\u22121)!(N\u2212s)! N ! for Beta(1, 1), i.e., the Shapley value."
        },
        {
            "heading": "G PRACTICAL ASPECT OF FASTER ESTIMATORS",
            "text": "As far as we know, in feature attribution, FastSHAP is the only framework of training semi-valuebased explainers (Jethani et al. 2022), which is based on the least square regression Eq. (7) specific for the Shapley value. Recently, Kwon and Zou (2022b) showed that other candidates of the Beta Shapley values tend to perform better than the Shapley value in feature attribution. Therefore, one may ask how to cast other probabilistic values into optimization, which is answered by Propositions 1 and 3. Though AME proposed by Lin et al. (2022) provides an alternative way to achieve this goal, it is restricted to a subfamily of semi-values. Besides, as shown in our experiments, the induced estimator of AME, in terms of convergence, does not rival our proposed estimators, which are derived from solving the least square regressions (10) and (11).\nRecall that the optimization problem used by FastSHAP is\nE(x,y)\u2208Z  \u2211 \u2205\u228aS\u228a[d] d\u2212 1( d s ) s(d\u2212 s) ( Ux,y(S)\u2212 Ux,y(\u2205)\u2212 1\u22a4S \u03d5(x, y; \u03b8) )2 (58) where d is the number of features (a counterpart of N ), \u03d5(x, y; \u03b8) is a trainable explainer and Ux,y is the utility function based on the data point (x, y) where x and y represent the features and label, respectively.\nAssume {S}\u2205\u228aS\u228a[d] is ordered so that each utility function Ux,y can be treated as a vector Ux,y \u2208 R2d\u22122. Besides, let W \u2208 R(2d\u22122)\u00d7(2d\u22122) be a diagonal matrix such thatW (S, S) = d\u22121\n(ds)s(d\u2212s) , and\ndefine X \u2208 R(2d\u22122)\u00d7d by letting the S-th row of X is 1S , i.e., 1S(i) = 1 if i \u2208 S and 0 otherwise. Very recently, Zhang et al. (2023a) discovered that\nE(x,y)\u2208Z [ \u2225\u03d5(x, y; \u03b8)\u2212 (X\u22a4WX)\u22121X\u22a4W (Ux,y \u2212 Ux,y(\u2205)1) \u22252X\u22a4WX ] = E(x,y)\u2208Z [ \u2225X\u03d5(x, y; \u03b8) + Ux,y(\u2205)1\u2212Ux,y\u22252W + Cx,y\n] = E(x,y)\u2208Z\n \u2211 \u2205\u228aS\u228a[d] d\u2212 1( d s ) s(d\u2212 s) ( Ux,y(S)\u2212 Ux,y(\u2205)\u2212 1\u22a4S \u03d5(x, y; \u03b8) )2 + Cx,y  (59)\nwhere the convention is \u2225z\u22252A := z\u22a4Az. This relationship reveals that FastSHAP is just to train explainers towards a biased target (X\u22a4WX)\u22121X\u22a4W (Ux,y \u2212 Ux,y(\u2205)1) under the metric induced by X\u22a4WX. Therefore, they instead proposed to train explainers based on\nE(x,y)\u223cZ [\u2225\u03d5(x, y; \u03b8)\u2212 \u03d5unbiased\u22252] (60)\nwhere \u03d5unbiased is any unbiased estimator (though they employed the one they discovered) for the Shapley value. They showed in the experiments that this framework of training explainers is not just simple but effective. Intuitively, faster estimator substituted in the problem (60) would lead to better training. All in all, faster estimators, especially those for the Beta Shapley values, could possibly benefit the research line of training semi-value-based explainers in feature attribution.\nH INTERPRETATION OF \u03ba(N) IN PROPOSITION 4 Recall that \u03ba = \u03b3\u22122(N) where \u03b3(N) = \u2211N\u22121 s=1 ( N\u22121 s\u22121 )(ps+ps+1)\u2211N\u22121\ns=1 ( N s )(ps+ps+1)\nand p is a weight vector that defines\na probabilistic value as in Eq. (1). Our interpretation of \u03ba(N) is based on \u03b3(N). In Algorithm 1, for each non-empty proper subset S, i.e., \u2205 \u228a S \u228a [N ], its probability of being sampled is\nP (S) = ps + ps+1\u2211N\u22121\ns=1 ( N s ) (ps + ps+1) . (61)\nSince \u03b3(N) = \u2211\nS\u228a[N ] : i\u2208S P (S), \u03b3(N) is the probability that the i-th data point appears in a random sample S. Thanks to symmetry, the choice of i is immaterial. Thus,\nN\u03b3(N) = N\u2211 i=1 \u2211 S\u228a[N ] : i\u2208S P (S) = \u2211 S\u228a[N ] N\u2211 i=1 1i\u2208SP (S) = \u2211 \u2205\u228aS\u228a[N ] sP (S) = ES [s], (62)\nwhence follows\n\u03b3(N) = ES [s] N \u2265 1 N . (63)\n(The lower bound is achieved when p1 = 1 and pi = 0 for all i \u2265 2, corresponding to the semivalue \u00b5 = \u03b40, i.e., leave everything else out.)\nLooking into Algorithm 1, for each sampled utility evaluationU(S), it is used to update the estimates of s data. Therefore, \u03b3(N) is just the average rate of reusing utility evaluations, and \u03ba(N) is the inverse square of this average rate. The higher \u03b3(N) is, the more efficient the ranking estimator is. On the flip side, reusing utility evaluations also creates correlation among the estimates. We conclude that for the benefit to outweigh the cost, \u03b3(N) needs to be on the order of \u03c9( 1\u221a\nN ), which\nis easily satisfied as long as the sequence {ps} is not exclusively concentrated around small s. (All practical probabilistic values used in the literature, including our experiments, meet this condition.)"
        },
        {
            "heading": "I ASYMPTOTIC ANALYSIS",
            "text": "The appendix is mainly to analyze the asymptotic behavior (as N \u2192 \u221e) of \u03ba(N) and \u03c4(N) that appear in Propositions 4 and 5. For clarity, we write {pN \u2208 RN}N\u22651 where each pN is nonnegative vector substituting p in Eq. 1. For probabilistic values, generally, there is no restriction on how {pN}N\u22651 are organized. Therefore, to analyze the convergence of our proposed estimators, we focus on semi-values instead. According to Dubey et al. (1981), each semi-value corresponds to a probability measure \u00b5 on the interval [0, 1] such that\npNs = \u222b 1 0 ts\u22121(1\u2212 t)N\u2212sd\u00b5(t) for every 1 \u2264 s \u2264 N. (64)\nLemma 3. Suppose N > 1, and for each probability measure \u00b5 on the interval [0, 1] that corresponds to a semi-value, define\nm\u00b5k = \u222b 1 0 tkd\u00b5(t) and w\u00b5k = \u222b 1 0 (1\u2212 t)kd\u00b5(t) for every k \u2265 0,\nM\u00b5k = k\u2211 j=0 m\u00b5k and W \u00b5 k = k\u2211 j=0 w\u00b5k for every k \u2265 0. (65)\nThen, there is\n\u03ba 1 2 (N) =\nM\u00b5N\u22122 +W \u00b5 N\u22122\nM\u00b5N\u22122 and \u03c4\n1 2 (N) =M\u00b5N\u22121 +W \u00b5 N\u22121. (66)\nProof. Recall that in Proposition 4 \u03ba 1 2 (N) =\n\u2211N\u22121 s=1 ( N s )(p N s +p\nN s+1)\u2211N\u22121\ns=1 ( N\u22121 s\u22121 )(pNs +pNs+1)\n. By Eq. (64), there is pNs +\npNs+1 = p N\u22121 s for every 1 \u2264 s \u2264 N \u2212 1. Notice that\nN\u22121\u2211 s=1 ( N s ) pN\u22121s = N\u22121\u2211 s=1 ( N \u2212 1 s ) pN\u22121s + N\u22121\u2211 s=1 ( N \u2212 1 s\u2212 1 ) pN\u22121s . (67)\nBy Eq.(64),\nN\u22121\u2211 s=1 ( N \u2212 1 s ) pN\u22121s = \u222b 1 0 N\u22121\u2211 s=1 ( N \u2212 1 s ) ts\u22121(1\u2212 t)N\u22121\u2212sd\u00b5(t)\n= \u222b 1 0 1 t N\u22121\u2211 s=1 ( N \u2212 1 s ) ts(1\u2212 t)N\u22121\u2212sd\u00b5(t)\n= \u222b 1 0 1\u2212 (1\u2212 t)N\u22121 t d\u00b5(t) = \u222b 1 0 (1\u2212 (1\u2212 t))( \u2211N\u22122 j=0 (1\u2212 t)j) t d\u00b5(t) =W\u00b5N\u22122 (68)\nNote \u2211N\u22121\ns=1 ( N\u22121 s ) ts\u22121(1\u2212 t)N\u22121\u2212s = \u2211N\u22122 j=0 (1\u2212 t)j still holds for t = 0. Similarly, one can get\nN\u22121\u2211 s=1 ( N \u2212 1 s\u2212 1 ) pN\u22121s =M \u00b5 N\u22122. (69)\nRecall that in Proposition 5 \u03c4 1 2 (N) = \u2211N s=1 ( N+1 s ) pNs , and thus one can get \u03c4 1 2 (N) = M\u00b5N\u22121 + W\u00b5N\u22121 in a similar fashion.\nUsing the monotone convergence theorem, we have\nlim N\u2192\u221e M\u00b5N = lim N\u2192\u221e \u222b 1 0 1\u2212 tN+1 1\u2212 t d\u00b5(t) = \u222b 1 0 lim N\u2192\u221e 1\u2212 tN+1 1\u2212 t d\u00b5(t) = \u222b 1 0 1 1\u2212 t d\u00b5(t), (70)\nlim N\u2192\u221e W\u00b5N = lim N\u2192\u221e \u222b 1 0 1\u2212 (1\u2212 t)N+1 t d\u00b5(t) = \u222b 1 0 1 t d\u00b5(t). (71)\nTherefore, \u03c4(N) \u2208 \u0398(1) iff the above two integrals are finite. In particular, when the probability density function p of \u00b5 satisfies limt\u21920 p(t) ta = limt\u21921 p(t) (1\u2212t)b = 0 for some a, b > 0, see Proposition 10 below. Interestingly, we have\n\u03ba(N) = 1 + W\u00b5N\u22122 M\u00b5N\u22122 \u2264 1 +W\u00b5N\u22122. (72)\nThus, \u03ba(N) = \u0398(1) if the integral Eq. (71) is finite, meaning that \u00b5 cannot put \u201clarge\u201d mass around t = 0. However, this is not necessary as any symmetric probability measure \u00b5 will have \u03ba(N) = 2. Remark 4. For extreme cases \u00b5 = \u03b40 (leave everything else out) and \u00b5 = \u03b41 (leave one out), it is clear that \u03ba(N) \u2208 \u0398(1) for the latter but it becomes \u0398(N2) for the former, in which case our ranking estimator (Algorithm 1) in fact only samples subsets of size 1, and therefore spends one utility evaluation on updating the estimate of each data point. This indicates that our complexity bounds still have some room to improve (at least in some cases).\nCorollary 1. For the Banzhaf value, \u03ba(N), \u03c4(N) \u2208 \u0398(1). In other words, the two proposed estimators require O(N\u03f52 log N \u03b4 ) utility evaluations to achieve an (\u03f5, \u03b4)-approximation for the Banzhaf value.\nProof. For the Banzhaf value, \u00b5 = \u03b40.5 (Dirac delta distribution), and thus limM\u2192\u221eM\u03b40.5N = limN\u2192\u221eW \u03b40.5 N = 2.\nWe now provide some easily verifiable conditions that determine the growth of \u03ba(N) and \u03c4(N). Recall that as long as \u03ba(N), \u03c4(N) = o(N), our estimators are more efficient than the sampling lift estimator. Proposition 10. Assume the probability measure \u00b5 admits a density function p : [0, 1]\u2192 R+ (w.r.t. the Lebesgue measure on [0, 1]). Then,\n1. \u03ba(N), \u03c4(N) \u2208 \u0398(1) if there exist a, b > 0 such that limt\u21920 p(t)ta = limt\u21921 p(t) (1\u2212t)b = 0.\n2. \u03ba(N), \u03c4(N) \u2208 O(log2N) if lim supt\u21920 p(t) < \u221e and lim supt\u21921 p(t) < \u221e, in particular if p is bounded (the so-called continuous semivalues by Dubey et al. 1981).\nProof. We first show that there exist counterexamples if these conditions are violated. For the Shapley value where \u00b5 = U (uniform distribution),mUk = wUk = 1k+1 , and thus \u03c4(N) = \u0398(log\n2N). For the second, looking into Lemma 4, consider \u03b1 = \u03b2 < 1, \u03ba(N), \u03c4(N) \u2208 \u0398(N1\u2212\u03b1).\nSuppose there exist a, b > 0 such that limt\u21920 p(t) ta = limt\u21921 p(t) (1\u2212t)b = 0. Then, there exists some \u03f5, C > 0 such that p(t) \u2264 Cta(1 \u2212 t)b if t < \u03f5 and t > 1 \u2212 \u03f5. Define a positive measure \u00b5\u03f5 by letting \u00b5\u03f5(S) = \u00b5(S \u2229 [\u03f5, 1 \u2212 \u03f5]) for every Borel-measurable subset S \u2286 [0, 1]. Besides, define another positive measure \u03bb by letting \u03bb(S) = \u222b S Cta(1 \u2212 t)bdt for every Borel-measurable subset S \u2286 [0, 1]. Therefore, we have \u00b5 \u2264 \u00b5\u03f5 + \u03bb, which indicates that m\u00b5k \u2264 m \u00b5\u03f5 k + m \u03bb k , and thus M\u00b5k \u2264 M \u00b5\u03f5 k +M \u03bb k . By Lemma 4, M \u03bb k \u2208 \u0398(1). For the other, observe that m \u00b5\u03f5 k \u2264 m \u03b41\u2212\u03f5 k , and therefore M\u00b5\u03f5k \u2264 M \u03b41\u2212\u03f5 k \u2208 \u0398(1). The remaining case W \u00b5 k can be tackled similarly. To conclude, \u03ba(N), \u03c4(N) \u2208 \u0398(1). Suppose lim supt\u21920 p(t) < \u221e and lim supt\u21921 p(t) < \u221e, then there exist \u03f5, C > 0 such that p(t) \u2264 C for every t \u2208 [0, \u03f5) \u222a (1 \u2212 \u03f5, 1]. Define \u00b5\u03f5 as the one in the above. Then, \u00b5 \u2264 \u00b5\u03f5 + C\u03bd where \u03bd denotes the uniform measure. Therefore, w\u00b5k \u2264 w \u00b5\u03f5 k + w C\u03bd k . Since w C\u03bd k = C k+1 , there is WC\u00b5k \u2208 \u0398(logN). Besides, W \u00b5\u03f5 k \u2264 W \u03b4\u03f5 k \u2208 \u0398(1). The remaining case M \u00b5 k can be dealt with similarly. Therefore, the conclusion follows by using Lemma 3.\nNext, we derive a more precise estimate for the Beta Shapley values.\nLemma 4. Let B(\u03b1, \u03b2) be the beta distribution with probability density function\u221d t\u03b1\u22121(1\u2212 t)\u03b2\u22121. Note that if \u00b5 = B(\u03b1, \u03b2), it yields the Beta(\u03b2, \u03b1) (a parameterized Beta Shapley value).\nM B(\u03b1,\u03b2) N \u2208  \u0398(logN) \u03b2 = 1 \u0398(1) \u03b2 > 1\n\u0398(N1\u2212\u03b2) otherwise and W B(\u03b1,\u03b2)N \u2208\n \u0398(logN) \u03b1 = 1 \u0398(1) \u03b1 > 1\n\u0398(N1\u2212\u03b1) otherwise . (73)\nProof. Let \u0393 denote the Gamma function.\nm B(\u03b1,\u03b2) k =\n\u0393(\u03b1+ \u03b2) \u0393(\u03b1)\u0393(\u03b2) \u00b7 \u0393(\u03b1+ k)\u0393(\u03b2) \u0393(\u03b1+ \u03b2 + k) = \u220fk\u22121 j=0 (\u03b1+ j)\u220fk\u22121\nj=0 (\u03b1+ \u03b2 + j) . (74)\nWe write ak \u223c bk if limk\u2192\u221e akbk converges. Since \u0393(x) = limn\u2192\u221e n!nx x(x+1)\u00b7\u00b7\u00b7(x+n) , e.g., see (Rudin 1953, Eq. (95)), we havemB(\u03b1,\u03b2)k \u223c k!k\u03b1 k!k\u03b1+\u03b2 = k\u2212\u03b2 , and thus the conclusion follows. The remaining case can be derived similarly.\nRemark 5. Lemma 4 is useful for obtaining the time complexity of the proposed estimators for the Beta Shapley values parameterized by \u03b1, \u03b2 > 0. If \u03b1, \u03b2 > 1, the two proposed estimators require O(N\u03f52 log N \u03b4 ) utility evaluations to achieve an (\u03f5, \u03b4)-approximation. Note that this is the currently best time complexity considering that the sampling lift estimator costs O(N 2\n\u03f52 log N \u03b4 ) instead. For\nthe Shapley value, i.e., Beta(1, 1), it is O(N\u03f52 log N \u03b4 ) for the ranking-approximation estimator, and O(N\u03f52 log( N \u03b4 ) log\n2N) for the value-approximation estimator. Nevertheless, since the Shapley value satisfies \u2211N i=1 \u03d5 Sh i (U) = U([N ])\u2212U(\u2205), the additive-efficient-normalized ranking-approximation estimator, which approximates the Shapley value, also requires O(N\u03f52 log N \u03b4 ) utility evaluations to achieve an (\u03f5, \u03b4)-approximation, as we show next.\nProposition 11. Specific to the Shapley value, let b\u0302(U) be the ranking-approximation estimator in Algorithm 1, its additive-efficient-normalized version is defined to be b\u0303i(U) = b\u0302i(U) + 1 N (U([N ])\u2212 U(\u2205)\u2212 \u2211N i=1 b\u0302i(U)). Then, b\u0303 requires O( N \u03f52 log N \u03b4 ) utility evaluations to achieve an (\u03f5, \u03b4)-approximation.\nProof. Write b = E[b\u0302] and let \u03d5 \u2208 RN be the corresponding Shapley value. By Proposition 2, we have \u03d5 = b + C1N for some constant C \u2208 R. Since \u2211N i=1 \u03d5i = U([N ]) \u2212 U(\u2205), it leads to that\nC = 1N (U([N ])\u2212 U(\u2205)\u2212 \u2211N\ni=1 b\u0302i(U)). In other words, the additive-efficient-normalized b is the Shapley value.\nAs pointed out by Jethani et al. (2022, Appendix B), the additive-efficient-normalization is just an orthogonal projection, and therefore we have\n\u2225b\u0303\u2212 \u03d5\u22252 \u2264 \u2225b\u0302\u2212 b\u22252, (75)\nwhich suggest P (\u2225b\u0303\u2212 \u03d5\u22252 \u2265 \u03f5) \u2264 P (\u2225b\u0302\u2212 b\u22252 \u2265 \u03f5).\nRemark 6. To our knowledge, in terms of (\u03f5, \u03b4)-approximation, the previously best time complexity is O(N\u03f52 log( N \u03b4 ) logN) achieved by the group testing estimator (Wang and Jia 2023, Theorem C.7). Therefore, our additive-efficient-normalized ranking-approximation estimator provides the currently best time complexity in terms of (\u03f5, \u03b4)-approximation for the Shapley value."
        },
        {
            "heading": "J MORE EXPERIMENT RESULTS",
            "text": "For training value estimators on MNIST, all performance curves are provided in Figure 4. Besides, the comparison with the approximation procedure as well as the removal experiment are contained in Figure 5.\nMore empirical evidence for verifying the faster convergence of Algorithms 1 and 2 are included in Figures 6, 7, 8 and 9. Moreover, we also set U to be the cross entropy loss, and corresponding results are presented by Figures 10, 11, 12, 13 and 14."
        }
    ],
    "year": 2023
}