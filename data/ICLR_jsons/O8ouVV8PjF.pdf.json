{
    "abstractText": "Shapelets and CNN are two typical approaches to model time series. Shapelets aim at finding a set of sub-sequences that extract feature-based interpretable shapes, but may suffer from accuracy and efficiency issues. CNN performs well by encoding sequences with a series of hidden representations, but lacks interpretability. In this paper, we demonstrate that shapelets are essentially equivalent to a specific type of CNN kernel with a squared norm and pooling. Based on this finding, we propose ShapeConv, an interpretable CNN layer with its kernel serving as shapelets to conduct time-series modeling tasks in both supervised and unsupervised settings. By incorporating shaping regularization, we enforce the similarity for maximum interpretability. We also find human knowledge can be easily injected to ShapeConv by adjusting its initialization and model performance is boosted with it. Experiments show that ShapeConv can achieve state-of-the-art performance on time-series benchmarks without sacrificing interpretability and controllability.",
    "authors": [
        {
            "affiliations": [],
            "name": "BEST SHAPELETS"
        },
        {
            "affiliations": [],
            "name": "Eric Qu"
        },
        {
            "affiliations": [],
            "name": "Yansen Wang"
        },
        {
            "affiliations": [],
            "name": "Xufang Luo"
        },
        {
            "affiliations": [],
            "name": "Wenqiang He"
        },
        {
            "affiliations": [],
            "name": "Kan Ren"
        },
        {
            "affiliations": [],
            "name": "Dongsheng Li"
        }
    ],
    "id": "SP:3a2da5716bf9c2f59e2e026b9db3623384fb506f",
    "references": [
        {
            "authors": [
                "N Alvarez",
                "CT Lombroso",
                "C Medina",
                "B Cantlon"
            ],
            "title": "Paroxysmal spike and wave activity in drowsiness in young children: its relationship to febrile convulsions",
            "venue": "Electroencephalography and clinical neurophysiology,",
            "year": 1983
        },
        {
            "authors": [
                "Anthony Bagnall",
                "Jason Lines",
                "Aaron Bostrom",
                "James Large",
                "Eamonn Keogh"
            ],
            "title": "The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances",
            "venue": "Data mining and knowledge discovery,",
            "year": 2017
        },
        {
            "authors": [
                "Anthony Bagnall",
                "Hoang Anh Dau",
                "Jason Lines",
                "Michael Flynn",
                "James Large",
                "Aaron Bostrom",
                "Paul Southam",
                "Eamonn Keogh"
            ],
            "title": "The uea multivariate time series classification archive, 2018",
            "venue": "arXiv preprint arXiv:1811.00075,",
            "year": 2018
        },
        {
            "authors": [
                "Shaojie Bai",
                "J Zico Kolter",
                "Vladlen Koltun"
            ],
            "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
            "venue": "arXiv preprint arXiv:1803.01271,",
            "year": 2018
        },
        {
            "authors": [
                "Kai-Wei Chang",
                "Biplab Deka",
                "Wen-Mei W Hwu",
                "Dan Roth"
            ],
            "title": "Efficient pattern-based time series classification on gpu",
            "venue": "IEEE 12th International Conference on Data Mining,",
            "year": 2012
        },
        {
            "authors": [
                "Yanping Chen",
                "Bing Hu",
                "Eamonn Keogh",
                "Gustavo EAPA Batista"
            ],
            "title": "Dtw-d: time series semisupervised learning from a single example",
            "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2013
        },
        {
            "authors": [
                "Mingyue Cheng",
                "Qi Liu",
                "Zhiding Liu",
                "Zhi Li",
                "Yucong Luo",
                "Enhong Chen"
            ],
            "title": "Formertime: Hierarchical multi-scale representations for multivariate time series classification",
            "venue": "In Proceedings of the ACM Web Conference",
            "year": 2023
        },
        {
            "authors": [
                "Kyunghyun Cho",
                "Bart Van Merri\u00ebnboer",
                "Caglar Gulcehre",
                "Dzmitry Bahdanau",
                "Fethi Bougares",
                "Holger Schwenk",
                "Yoshua Bengio"
            ],
            "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
            "venue": "arXiv preprint arXiv:1406.1078,",
            "year": 2014
        },
        {
            "authors": [
                "Edward Choi",
                "Mohammad Taha Bahadori",
                "Jimeng Sun",
                "Joshua Kulas",
                "Andy Schuetz",
                "Walter Stewart"
            ],
            "title": "Retain: An interpretable predictive model for healthcare using reverse time attention mechanism",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Hoang Anh Dau",
                "Anthony Bagnall",
                "Kaveh Kamgar",
                "Chin-Chia Michael Yeh",
                "Yan Zhu",
                "Shaghayegh Gharghabi",
                "Chotirat Ann Ratanamahatana",
                "Eamonn Keogh"
            ],
            "title": "The ucr time series archive",
            "venue": "IEEE/CAA Journal of Automatica Sinica,",
            "year": 2019
        },
        {
            "authors": [
                "David L Davies",
                "Donald W Bouldin"
            ],
            "title": "A cluster separation measure",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 1979
        },
        {
            "authors": [
                "Angus Dempster",
                "Fran\u00e7ois Petitjean",
                "Geoffrey I Webb"
            ],
            "title": "Rocket: exceptionally fast and accurate time series classification using random convolutional kernels",
            "venue": "Data Mining and Knowledge Discovery,",
            "year": 2020
        },
        {
            "authors": [
                "Emadeldeen Eldele",
                "Mohamed Ragab",
                "Zhenghua Chen",
                "Min Wu",
                "Chee Keong Kwoh",
                "Xiaoli Li",
                "Cuntai Guan"
            ],
            "title": "Time-series representation learning via temporal and contextual contrasting",
            "venue": "In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Vincent Fortuin",
                "Matthias H\u00fcser",
                "Francesco Locatello",
                "Heiko Strathmann",
                "Gunnar R\u00e4tsch"
            ],
            "title": "Somvae: Interpretable discrete representation learning on time series",
            "venue": "arXiv preprint arXiv:1806.02199,",
            "year": 2018
        },
        {
            "authors": [
                "Jean-Yves Franceschi",
                "Aymeric Dieuleveut",
                "Martin Jaggi"
            ],
            "title": "Unsupervised scalable representation learning for multivariate time series",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Josif Grabocka",
                "Nicolas Schilling",
                "Martin Wistuba",
                "Lars Schmidt-Thieme"
            ],
            "title": "Learning time-series shapelets",
            "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2014
        },
        {
            "authors": [
                "Antoine Guillaume",
                "Christel Vrain",
                "Wael Elloumi"
            ],
            "title": "Random dilated shapelet transform: A new approach for time series shapelets",
            "venue": "In Pattern Recognition and Artificial Intelligence: Third International Conference, ICPRAI 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Tian Guo",
                "Tao Lin",
                "Nino Antulov-Fantulin"
            ],
            "title": "Exploring interpretable lstm neural networks over multi-variable data",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "John A Hartigan",
                "Manchek A Wong"
            ],
            "title": "Algorithm as 136: A k-means clustering algorithm",
            "venue": "Journal of the royal statistical society. series c (applied statistics),",
            "year": 1979
        },
        {
            "authors": [
                "Wenqiang He",
                "Mingyue Cheng",
                "Qi Liu",
                "Zhi Li"
            ],
            "title": "Shapewordnet: An interpretable shapelet neural network for physiological signal classification",
            "venue": "In Database Systems for Advanced Applications: 28th International Conference,",
            "year": 2023
        },
        {
            "authors": [
                "Jon Hills",
                "Jason Lines",
                "Edgaras Baranauskas",
                "James Mapp",
                "Anthony Bagnall"
            ],
            "title": "Classification of time series by shapelet transformation",
            "venue": "Data mining and knowledge discovery,",
            "year": 2014
        },
        {
            "authors": [
                "Lu Hou",
                "James Kwok",
                "Jacek Zurada"
            ],
            "title": "Efficient learning of timeseries shapelets",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2016
        },
        {
            "authors": [
                "Hassan Ismail Fawaz",
                "Germain Forestier",
                "Jonathan Weber",
                "Lhassane Idoumghar",
                "Pierre-Alain Muller"
            ],
            "title": "Deep learning for time series classification: a review",
            "venue": "Data mining and knowledge discovery,",
            "year": 2019
        },
        {
            "authors": [
                "Satyapriya Krishna",
                "Tessa Han",
                "Alex Gu",
                "Javin Pombra",
                "Shahin Jabbari",
                "Steven Wu",
                "Himabindu Lakkaraju"
            ],
            "title": "The disagreement problem in explainable machine learning: A practitioner\u2019s perspective",
            "venue": "arXiv preprint arXiv:2202.01602,",
            "year": 2022
        },
        {
            "authors": [
                "Kin Kwan Leung",
                "Clayton Rooke",
                "Jonathan Smith",
                "Saba Zuberi",
                "Maksims Volkovs"
            ],
            "title": "Temporal dependencies in feature importance for time series prediction",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Guozhong Li",
                "Byron Choi",
                "Jianliang Xu",
                "Sourav S Bhowmick",
                "Daphne Ngar-yin Mah",
                "Grace Lai-Hung Wong"
            ],
            "title": "Autoshape: An autoencoder-shapelet approach for time series clustering",
            "venue": "arXiv preprint arXiv:2208.04313,",
            "year": 2022
        },
        {
            "authors": [
                "Tianfu Li",
                "Zhibin Zhao",
                "Chuang Sun",
                "Li Cheng",
                "Xuefeng Chen",
                "Ruqiang Yan",
                "Robert X Gao"
            ],
            "title": "Waveletkernelnet: An interpretable deep neural network for industrial intelligent diagnosis",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ziyue Li",
                "Yuchen Fang",
                "You Li",
                "Kan Ren",
                "Yansen Wang",
                "Xufang Luo",
                "Juanyong Duan",
                "Congrui Huang",
                "Dongsheng Li",
                "Lili Qiu"
            ],
            "title": "Protecting the future: Neonatal seizure detection with spatial-temporal modeling",
            "venue": "arXiv preprint arXiv:2307.05382,",
            "year": 2023
        },
        {
            "authors": [
                "Jason Lines",
                "Luke M Davis",
                "Jon Hills",
                "Anthony Bagnall"
            ],
            "title": "A shapelet transform for time series classification",
            "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2012
        },
        {
            "authors": [
                "Jason Lines",
                "Sarah Taylor",
                "Anthony Bagnall"
            ],
            "title": "Time series classification with hive-cote: The hierarchical vote collective of transformation-based ensembles",
            "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD),",
            "year": 2018
        },
        {
            "authors": [
                "Scott M Lundberg",
                "Su-In Lee"
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Yingtao Luo",
                "Chang Xu",
                "Yang Liu",
                "Weiqing Liu",
                "Shun Zheng",
                "Jiang Bian"
            ],
            "title": "Learning differential operators for interpretable time series modeling",
            "venue": "In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Qianli Ma",
                "Jiawei Zheng",
                "Sen Li",
                "Gary W Cottrell"
            ],
            "title": "Learning representations for time series clustering",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Qianli Ma",
                "Sen Li",
                "Wanqing Zhuang",
                "Jiabing Wang",
                "Delu Zeng"
            ],
            "title": "Self-supervised time series clustering with model-based dynamics",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Qianli Ma",
                "Wanqing Zhuang",
                "Sen Li",
                "Desen Huang",
                "Garrison Cottrell"
            ],
            "title": "Adversarial dynamic shapelet networks",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Naveen Sai Madiraju",
                "Seid M Sadat",
                "Dimitry Fisher",
                "Homa Karimabadi"
            ],
            "title": "Deep temporal clustering: Fully unsupervised learning of time-domain features",
            "venue": "arXiv preprint arXiv:1802.01059,",
            "year": 2018
        },
        {
            "authors": [
                "Lara V Marcuse",
                "Madeline C Fields",
                "Jiyeoun Jenna Yoo"
            ],
            "title": "Rowan\u2019s Primer of EEG",
            "venue": "Elsevier Health Sciences,",
            "year": 2015
        },
        {
            "authors": [
                "Douglas W Marshall",
                "Robin L Brey",
                "Michael W Morse"
            ],
            "title": "Focal and/or lateralized polymorphic delta activity: Association with either\u2019normal\u2019or\u2019nonfocal\u2019computed tomographic scans",
            "venue": "Archives of neurology,",
            "year": 1988
        },
        {
            "authors": [
                "Matthew Middlehurst",
                "Patrick Sch\u00e4fer",
                "Anthony Bagnall"
            ],
            "title": "Bake off redux: a review and experimental evaluation of recent time series classification algorithms",
            "venue": "arXiv preprint arXiv:2304.13029,",
            "year": 2023
        },
        {
            "authors": [
                "Iyad Obeid",
                "Joseph Picone"
            ],
            "title": "The temple university hospital eeg data corpus",
            "venue": "Frontiers in neuroscience,",
            "year": 2016
        },
        {
            "authors": [
                "John Paparrizos",
                "Luis Gravano"
            ],
            "title": "k-shape: Efficient and accurate clustering of time series",
            "venue": "In Proceedings of the 2015 ACM SIGMOD international conference on management of data,",
            "year": 2015
        },
        {
            "authors": [
                "Eric Qu",
                "Xufang Luo",
                "Dongsheng Li"
            ],
            "title": "Data continuity matters: Improving sequence modeling with lipschitz regularizer",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Thanawin Rakthanmanon",
                "Eamonn Keogh"
            ],
            "title": "Fast shapelets: A scalable algorithm for discovering time series shapelets",
            "venue": "In proceedings of the 2013 SIAM International Conference on Data Mining,",
            "year": 2013
        },
        {
            "authors": [
                "Chotirat Ann Ratanamahatana",
                "Eamonn Keogh"
            ],
            "title": "Three myths about dynamic time warping data mining",
            "venue": "In Proceedings of the 2005 SIAM international conference on data mining,",
            "year": 2005
        },
        {
            "authors": [
                "Alejandro Pasos Ruiz",
                "Michael Flynn",
                "James Large",
                "Matthew Middlehurst",
                "Anthony Bagnall"
            ],
            "title": "The great multivariate time series classification bake off: a review and experimental evaluation of recent algorithmic advances",
            "venue": "Data Mining and Knowledge Discovery,",
            "year": 2021
        },
        {
            "authors": [
                "Patrick Sch\u00e4fer",
                "Ulf Leser"
            ],
            "title": "Multivariate time series classification with weasel+ muse",
            "venue": "arXiv preprint arXiv:1711.11343,",
            "year": 2017
        },
        {
            "authors": [
                "Mit Shah",
                "Josif Grabocka",
                "Nicolas Schilling",
                "Martin Wistuba",
                "Lars Schmidt-Thieme"
            ],
            "title": "Learning dtw-shapelets for time-series classification",
            "venue": "In Proceedings of the 3rd IKDD Conference on Data Science,",
            "year": 2016
        },
        {
            "authors": [
                "Siyi Tang",
                "Jared A Dunnmon",
                "Khaled Saab",
                "Xuan Zhang",
                "Qianying Huang",
                "Florian Dubost",
                "Daniel L Rubin",
                "Christopher Lee-Messer"
            ],
            "title": "Self-supervised graph neural networks for improved electroencephalographic seizure analysis",
            "venue": "arXiv preprint arXiv:2104.08336,",
            "year": 2021
        },
        {
            "authors": [
                "Wensi Tang",
                "Guodong Long",
                "Lu Liu",
                "Tianyi Zhou",
                "Michael Blumenstein",
                "Jing Jiang"
            ],
            "title": "Omni-scale cnns: a simple and effective kernel size configuration for time series classification",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Sana Tonekaboni",
                "Shalmali Joshi",
                "Kieran Campbell",
                "David K Duvenaud",
                "Anna Goldenberg"
            ],
            "title": "What went wrong and when? instance-wise feature importance for time-series black-box models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Sana Tonekaboni",
                "Danny Eytan",
                "Anna Goldenberg"
            ],
            "title": "Unsupervised representation learning for time series with temporal neighborhood coding",
            "venue": "arXiv preprint arXiv:2106.00750,",
            "year": 2021
        },
        {
            "authors": [
                "Yichang Wang",
                "R\u00e9mi Emonet",
                "Elisa Fromont",
                "Simon Malinowski",
                "Etienne Menager",
                "Lo\u0131\u0308c Mosser",
                "Romain Tavenard"
            ],
            "title": "Learning interpretable shapelets for time series classification through adversarial regularization",
            "year": 1906
        },
        {
            "authors": [
                "Zhiguang Wang",
                "Weizhong Yan",
                "Tim Oates"
            ],
            "title": "Time series classification from scratch with deep neural networks: A strong baseline",
            "venue": "In 2017 International joint conference on neural networks (IJCNN),",
            "year": 2017
        },
        {
            "authors": [
                "Haixu Wu",
                "Jiehui Xu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Qiao Xiao",
                "Boqian Wu",
                "Yu Zhang",
                "Shiwei Liu",
                "Mykola Pechenizkiy",
                "Elena Mocanu",
                "Decebal Constantin Mocanu"
            ],
            "title": "Dynamic sparse network for time series classification: Learning what to \u201csee",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Lexiang Ye",
                "Eamonn Keogh"
            ],
            "title": "Time series shapelets: a new primitive for data mining",
            "venue": "In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2009
        },
        {
            "authors": [
                "Zhihan Yue",
                "Yujing Wang",
                "Juanyong Duan",
                "Tianmeng Yang",
                "Congrui Huang",
                "Yunhai Tong",
                "Bixiong Xu"
            ],
            "title": "Ts2vec: Towards universal representation of time series",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Jesin Zakaria",
                "Abdullah Mueen",
                "Eamonn Keogh"
            ],
            "title": "Clustering time series using unsupervisedshapelets",
            "venue": "IEEE 12th International Conference on Data Mining,",
            "year": 2012
        },
        {
            "authors": [
                "George Zerveas",
                "Srideepika Jayaraman",
                "Dhaval Patel",
                "Anuradha Bhamidipaty",
                "Carsten Eickhoff"
            ],
            "title": "A transformer-based framework for multivariate time series representation learning",
            "venue": "In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining,",
            "year": 2021
        },
        {
            "authors": [
                "Nan Zhang",
                "Shiliang Sun"
            ],
            "title": "Multiview unsupervised shapelet learning for multivariate time series clustering",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Qin Zhang",
                "Jia Wu",
                "Peng Zhang",
                "Guodong Long",
                "Chengqi Zhang"
            ],
            "title": "Salient subsequence learning for time series clustering",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "MUSE (Sch\u00e4fer",
                "Leser",
                "ROCKET (Dempster"
            ],
            "title": "2020), recent years have also witnessed the rapid advancement of deep learning methods in interpretable time series modeling. Apart from RNN models (Choi et al., 2016; Guo et al., 2019), newly devised CNN models have obtained more attention in this area. (Fortuin et al., 2018) developed a CNN-based SOM-VAE method to learn the topologically interpretable discrete representations of time series in a probabilistic fashion",
            "year": 2018
        },
        {
            "authors": [
                "Luo"
            ],
            "title": "2022) employed convolutional kernels to approximate the partial differential equations on data distribution so as to explain the nonlinear dynamics of their sequential patterns. (Li et al., 2021) designed a wavelet convolution layer to help CNNs discover filters with certain physical meaning, while (Tang et al., 2021b; Xiao et al., 2022) studied the best kernel size for time series modelling. Innovatively, we view convolution kernels from the shapelet perspective and endow shapelet-based",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In the realm of machine learning, interpretable time-series modeling stands as a pivotal endeavor, striving to encode sequences and forecast in a manner that resonates with human comprehension. Among an array of early methods to distill interpretable features from sequences, shapelets (Ye & Keogh, 2009) have garnered significant attention, finding applications in diverse downstream tasks. These shapelet are discriminative sub-sequences culled from the primary time series and the minimal distance between a shapelet and all conceivable sub-sequences of the raw input is ascertained, yielding features that signify a shapelet\u2019s imprint on a sequence. The allure of shapelets lies in their capacity to discern local discriminative patterns inherent in the data. However, conventional shapelets grapple with inefficiencies, attributed to their exhaustive search demands and elevated time complexity.\nAs the new era of deep learning comes, more and more works seek to fit the sequence with a high dimensional non-convex function using deep neural networks such as RNN (Guo et al., 2019), CNN (Franceschi et al., 2019), Transformer (Wu et al., 2021; Qu et al., 2022; Cheng et al., 2023), etc. to model the time series. These deep-learning-based methods have attracted much more attention than shapelets, thanks to their great performance when the number of data is sufficient, but they are more likely to overfit when the signal-to-noise ratio is relatively low and the data are scarce. Also, the representations (often called hidden representations) are almost impossible to interpret and control due to the black-box nature of neural networks. While subsequent research endeavors (Ma et al., 2020b; Li et al., 2022; He et al., 2023) are proposed aiming at fusing the interpretability of shapelets and the promising performance of deep methods, they often fail with striking a harmonious equilibrium between performance and interpretability.\nIn this paper, we aim to seamlessly inject the interpretability of shapelets into the convolutional layer while retaining the advantages and characteristics of both. Despite the apparent disparity between shapelets and deep models in time-series modeling, for the forward process, we first theoretically prove that extracting features with shapelets can be equivalently conducted by passing the input time \u2217Work done during an internship in Microsoft Research Asia. \u2020Corresponding author.\nseries to a specific convolutional layer (1-layer CNN) with a squared norm and pooling. This finding provides us the basis for combining shapelets with deep models.\nTo implement the equivalence, we devise ShapeConv, a CNN layer wherein its kernel functions as shapelets, adeptly and interpretably addressing the time-series modeling challenge. We introduce several ingenious designs to make ShapeConv effective in practice. During the optimization process, due to the difference of candidate space, the subsequences derived via gradient-based techniques might diverge significantly from the sub-sequence prototypes, rendering them less suitable as interpretable shapelets. Hence, ShapeConv incorporates an additional shaping regularization to enforce similarity. Besides, another regularization term is utilized to relieve the issue that the model tends to fall into the local optimal point where kernels are similar but not catching diverse and discriminative features. As for the initialization, we also design separate judicious strategies to make model weights close to different discriminative sub-sequences in data in different tasks, capturing class-specific and cluster-specific information for supervised classification and unsupervised clustering, repsectively.\nIn contrast to traditional shapelet techniques, ShapeConv, being a deep model, facilitates end-to-end optimization. This paves the way for parallel computing for acceleration, effortless stacking with deep modules for improved performance. When compared to learning-based approaches, our kernels provide controllability, facilitated by our strategic initialization that incorporates human expertise to yield more human-comprehensible results. Empirical evaluations underscore ShapeConv\u2019s prowess in both supervised classification and unsupervised clustering tasks and various datasets. It surpasses other learning-based shapelet techniques and contemporary deep models tailored for time-series classification and clustering, all the while preserving interpretability. Furthermore, the infusion of human knowledge amplifies the model\u2019s performance, and there\u2019s a marked reduction in time complexity when compared to earlier shapelet methodologies.\nWe summarize our contribution as 4 folds: (1) We have formally and theoretically proven the equivalence of a specific CNN layer, when combined with square norm and pooling, to the shapelet. (2) Based on the discovered equivalence, we introduce ShapeConv, an interpretable CNN layer with its kernel serving as shapelets. Several regularizations and initializations are accompanied to enforce similarity and diversity, making ShapeConv effective in practice. (3) By treating the CNN kernel as a shapelet, we claim another advantage of facilitating the incorporation of human prior knowledge by initialization. (4) Extensive experiments on real-world datasets validates ShapeConv\u2019s superior performance on interpretable time-series modeling tasks.\n2 BACKGROUND\nShapelets Shapelet (Ye & Keogh, 2009), originally defined as a maximally discriminative sub-sequence in time-series data (shown in Figure 1), is designed to capture inter-class features in terms of small subsequences rather than the full sequence. Shapeletbased methods will usually find a subset of shapelets S\u2217 \u2282 S\u0302 from data to maximize information gain by splitting data with the shapelets as nodes of a decision tree. The candidate shapelet set S\u0302 contains all possible sub-sequences of the original data.\nAfterwards, a shapelet transform (Lines et al., 2012) is introduced to decouple the shapelet discovering step and downstream classifier. This shapelet transform step will extract features for an input signal X based on the distances between shapelets and the input. Specifically, for each selected shapelet s \u2208 S\u2217, we calculate the minimal distance between s and all sub-sequences of X, i.e.,\nds,X =min x\u2208X\u0302 dist(s,x), (1)\nwhere X\u0302 is the set containing all sub-sequences with the same length as s, and dist(s,x) is the distance function (usually the squared Euclidean distance) measuring the similarity between a shapelet and a raw sub-sequence. Using all shapelets in S\u2217, the feature vector with length \u2223S\u2217\u2223 can be obtained for each X, and these features are used for building different kinds of classifiers besides tree-based models. We provide a more vivid example on shapelets and shapelet transform in Appendix B.2.\nBeyond traditional costly shapelet discovering methods, following works extend the candidate set to Rls where ls is the length of shapelet, and try to solve the problem with optimization-based methods.\nConvolutional Neural Networks CNN is one of the most commonly used network structures and its several variants have been used in time-series modeling for years (Ismail Fawaz et al., 2019). A CNN layer is essentially some sliding filters known as convolutional kernels followed by the activation function and pooling layer. Here, we consider a simple case to illustrate the CNN layer, where the input is 2-dimensional signal X \u2208 Rnin\u00d7lx with nin input channels and lx time steps. Applying a 1-D convolution over it can be formulated as:\nY = nin\n\u2211 k=1 Wk \u2217Xk, (2)\nWhere W \u2208 Rnin\u00d7nout\u00d7ls denotes the weights of convolutional kernels. The \u2217 symbol denotes the cross-correlation operator on the k-th input channel between each raw vector of Wk and the input Xk. Note that for most CNN kernels, weights Wk is learnable so this cross-correlation is equivalent to the convolution in terms of optimization. After that, a non-linear activation function and a pooling layer are often applied to extract the aggregated value among its neighbors N(j) for each channel i at location j, which can be written as:\nYpoolij = pool j\u2032\u2208N(j) \u03c3(Yij\u2032 + bi). (3)\nHere, The bias term b is sometimes set to zero for simplicity. The symbol \u03c3 denotes the activation function such as ReLU, Tanh, and pool denotes a pooling function such as max or mean."
        },
        {
            "heading": "3 HOW CAN CNN KERNELS BE THE BEST SHAPELETS?",
            "text": "In this section, we give a comprehensive answer to the question in the title. First, we provide a formal proof to show the equivalence between CNN and shapelets in Sec. 3.1. Then, we introduce ShapeConv, a novel convolutional layer well utilizing the equivalence in Sec. 3.2. Finally, we show how ShapeConv can be used for supervised learning and unsupervised learning in Sec. 3.3 and Sec. 3.4, respectively."
        },
        {
            "heading": "3.1 EQUIVALENCE BETWEEN CNN KERNELS AND SHAPELETS",
            "text": "The core idea is that when the calculation of squared Euclidean distance in the shapelet transform step is expanded, one of the terms is exactly the same as the forward passing in a CNN layer. Therefore, using these shapelets to extract features with squared Euclidean distance for X can be equivalently done by convolving X with nout kernels from a 1-D CNN layer added by squared L2 norm, followed by a maximum pooling. The difference between these two can be easily handled and omitted in practice. We summarize the finding in the following theorem:\nTheorem 3.1 Assume the input X \u2208 Rlx is a 1-dimensional single-variate signal of length lx, and nout shapelets S\u2217 = {s1, s2, ...snout} with length ls are discovered. The feature extracted from X with si and squared Euclidean distance is dsi,X. Then we have\ndsi,X = \u22122 max j\u2208{1,2,...,lx\u2212ls+1} [Yij \u2212N (si,Xj\u2236j+ls\u22121)], (4)\nwhere Y = si \u2217X is the cross-correlation defined in Eq. 2 and N (si,X\u2032) = (\u2225si\u222522 + \u2225X\u2032\u222522)/2 is squared L2 norm term.\nDetailed proof can be found in the Appendix C. Eq. 4 bridges shapelets and the learnable CNN kernel. The left-hand side is the feature extracted by a specific shapelet, and the right-hand side contains the\nmaximum pooling over the convolution between the kernel and the input time series and a squared norm term. The difference between these two is a constant factor -2 which can be absorbed in the learnable parameters.\nThere are two more gaps between the practical use of CNN in Eq. 3 and Theorem 3.1. One is the non-linear activation function. When the activation function \u03c3 is monotonically increasing, the order to apply maximum pooling and activation function can be swapped, i.e., max(\u03c3(\u22c5)) = \u03c3(max(\u22c5)). This fits for most cases in practice with ReLU, Tanh, Sigmoid and their variants as activation functions. Another is the bias term. Since b is often designed to be independent of the position, we can have maxj(Yij + bi) =maxj(Yij) + bi. Now we can rewrite Eq. 3 as:\nYmaxij = max j\u2032\u2208N(j) \u03c3(Yij\u2032 + bi) = \u03c3( max j\u2032\u2208N(j) (Yij\u2032) + bi). (5)\nThis suggests that we can optionally add the bias term and the non-linear activation after the features are extracted with the shapelets to obtain a complete equivalence."
        },
        {
            "heading": "3.2 SHAPECONV: AN INTERPRETABLE CNN LAYER WITH ITS KERNELS SERVING AS SHAPELETS",
            "text": "Motivated by the above established equivalence, we introduce ShapeConv, a novel interpretable CNN layer for time-series data, with its kernels serving as shapelets. Specifically, in addition to the cross-correlation operator and max pooling in the original CNN layer, we add a squared norm term N (si,X\u2032) in Eq. 4 before the pooling function to the CNN layer. Consequently, ShapeConv\u2019s forward pass mirrors the shapelet transform, calculating the minimum distance between a shapelet and all possible raw input sub-sequences, and convolutional kernels in ShapeConv play the same role as shapelets. Although initially univariate, ShapeConv can directly adapt to multivariate data tasks by adjusting the number of input channels while maintaining its other designs, enabling the model to learn multiple kernels/shapelets for each variate.\nShaping Kernels To serve as a good shapelet, ShapeConv\u2019s kernel should be the maximally discriminative thus can provide solid criteria for downstream classification (discriminability), and as human-comprehensible as possible by the minimize the distance from the sub-sequence of the data (interpretability).\nThe discriminability is achieved by optimizing kernel weights via task-specific loss which will be discussed in Sec. 3.3 and Sec. 3.4, respectively. As for the interpretability, while extending the candidate set from S\u2217 to Rls allows the learning-based methods to achieve best classification results, shapelets which look way too different from the input data conversely downgrade the overall interpretability. Therefore, to shape kernels like original data, we first strategically design the initialization method. Depending on whether class labels are available, we suggest different methods for initial kernel-data proximity, elaborated in the following subsections. Besides, we introduce a shape regularizer to keep the kernel similar to data during training. Specifically, we calculate distances between the kernel weight, i.e., shapelet si, and sub-sequences in the input, and the shape regularizer is defined as the minimal distances, i.e.,\nRshape = 1\nnout\nnout\n\u2211 i=1 min x\u2208X\u0302i dist(si,x), (6)\nwhere X\u0302i is the set containing all sub-sequences with same length as si. We opt for squared Euclidean distance here to allow for the reuse of the distance calculated in the shapelet transform step. This approach ensures kernel weights are initialized with original data shapes and remain close during training, yielding interpretable kernels with discriminative shapes.\nIncreasing Diversity Since shapelets in our model are learnable weights with large flexibility, they tend to fall into local optimality where all kernels are similar but not catching diverse and discriminative features. We provide experimental results on this phenomenon in Appendix D.3. Therefore, we further introduce a diversity regularizer following Zhang et al. (2018) to relieve this issue. We first use the pairwise \u21132 distance between different kernels (or shapelets) to construct a distance matrix Ds, where Ds(i, j) = exp(\u2212\u21132(si, sj)), and the diversity regularizer is defined as F -norm of Ds, i.e.,\nRdiv = \u2225Ds\u2225F . (7)\nTo summarize, the above basic designs make ShapeConv a differentiable CNN layer with its kernels serving as shapelets. ShapeConv can keep the advantages of both CNN and shapelets. On the one hand, ShapeConv is a fully differentiable CNN layer, so it can be optimized effectively with lower time complexity than traditional methods. On the other hand, kernels of ShapeConv have good interpretability since they are similar to discriminative sub-sequences in original data."
        },
        {
            "heading": "3.3 SUPERVISED LEARNING",
            "text": "In this section, we show the method to apply ShapeConv in a supervised learning task. Here, we focus on the time-series classification task, which is a typical application for shapelets.\nInitialization The target of initialization is to provide convolutional kernels with a good starting point capturing class-specific sub-sequences in the data. As illustrated in the left part of Figure 2, the main idea is that for every class of data, we split them along the time axis, and calculate the mean of sub-sequences in each split part. Suppose we have nout output channels, corresponding to nout shapelets, with each shapelet of length ls. The input length is lX , and there are ncla classes. First, we assign k = nout/ncla shapelets to each class. Within each class, we divide the time series into k equal parts of length lk = lX/k (along the time axis). For each part, we use a sliding window to find all possible candidates of length ls. The final initialization of the kernel is obtained by calculating the mean of all candidates. This method not only incorporates class information into the kernel but also associates each kernel with a specific region of the time-series data in the dataset. As a result, the model can more effectively capture local information within the initialization region. The initialization is also paralleled on GPU, making it very time efficient.\nClassifier and Loss Function The output of the ShapeConv layer is the shapelet transformed distances, representing features extracted by learned shapelets, and these features are further utilized by the downstream classifier. Here, we append a multi-layer perceptron (MLP) after the ShapeConv layer, to map the shapelet transformed distance to the class labels. This method allows for end-to-end training of the entire model, optimizing both the ShapeConv layer and the classifier simultaneously. The loss function is designed as follows,\nL = Lcls + \u03bbshapeRshape + \u03bbdivRdiv. (8)\nHere, Lcls is the task-specific classification loss, such as cross-entropy. The termRshape is the shape regularizer defined in Eq. 6, and the term Rdiv is the diversity regularizer defined in Eq. 7. The hyperparameter \u03bbshape and \u03bbdiv controls the balance between each terms. Note that ShapeConv is compatible with other classifiers. We discuss this in Appendix D.1 and include this variant in our experiment in Sec. 4.1."
        },
        {
            "heading": "3.4 UNSUPERVISED LEARNING",
            "text": "We now apply ShapeConv to the unsupervised learning task, where ShapeConv needs to capture the most representative sub-sequences in data and perform K-means on the shapelet-transformed distance to cluster unlabelled time series.\nInitialization Initialization in the unsupervised learning setting is more challenging, since no class information is given now. Therefore, we design a pre-clustering method to find out some discriminative sub-sequences. The main idea is dividing the time-series data into portions along the time axis and performing separate clustering within each portion to find initial shapelets, as illustrated in the right part of Figure 2. Suppose we have nout shapelets in the ShapeConv layer, each with a length of ls. The input length is lX . First, all input time series are divided into ncut equal parts along the time axis, each with a length of lcut = lX/ncut. Each part is assigned with k = nout/ncut shapelets. We then sample a large number of subsequences (e.g., 10,000) of length ls from each part and perform KMeans clustering with k centers on them. Finally, the cluster centers are utilized as the initialization of the shapelets. In this approach, the class information is implicitly introduced during the clustering process, and the division into cuts allows the shapelets to focus on different regions of the time series. This enables the model to better capture the local patterns of different classes.\nLoss Function As for the task-specific loss, we employ Davies-Bouldin Index (DBI) (Davies & Bouldin, 1979) to optimize ShapeConv for better clustering results. Overall, DBI loss aims to minimize the intra-cluster distances while maximizing the inter-cluster distances, ensuring that the extracted shapelet transformed distances yield well-separated clusters (Li et al., 2022). The detailed formulation of DBI loss can be found in Appendix D.2. The overall loss function in unsupervised learning is designed as, L = LDBI + \u03bbshapeRshape + \u03bbdivRdiv. (9) Here, LDBI is DBI loss. The term Rshape is the shape regularizer defined in Eq. 6, and the term Rdiv is the diversity regularizer defined in Eq. 7. The hyperparameter \u03bbshape and \u03bbdiv controls the balance between each terms.\nIncorporating Human Knowledge The characteristic of ShapeConv makes it easy to incorporate human knowledge, which means that human experts can \u201ctell\u201d the model what some key subsequences look like, and the model can use these knowledge for improving its performance. On the other hand, in unsupervised learning tasks, the model will first learn to minimize the shapelet transform distance, which may lead it to converge to local minima. If the shapelet is initialized in a non-discriminative region, the model\u2019s performance may be negatively affected. Therefore, we propose using human knowledge for shapelet initialization.\nSpecifically, we first visualize the dataset and ask the human labeler to identify the most discriminative regions. Once these regions are labeled, we calculate the mean of each region and use it as the initialization for the shapelets. Then, the shapelet will tend to converge in the targeted region. As shown in experiments in Section 4.2, this approach makes the model learn high-quality shapelets."
        },
        {
            "heading": "4 EXPERIMENTS AND ANALYSIS",
            "text": ""
        },
        {
            "heading": "4.1 SUPERVISED TIME-SERIES CLASSIFICATION",
            "text": "Settings We evaluate our ShapeConv model on time-series classification tasks using the UCR univariate time-series dataset (Dau et al., 2019) and UAE multivariate times series dataset (Bagnall et al., 2018). Hyperparameters are tuned via grid search based on the validation set performance, and they are reported in Appendix G.2.\nCompared Methods We compare ShapeConv with three kinds of baselines: (1) shapelet-based methods (IGSVM (Hills et al., 2014), FLAG (Hou et al., 2016), LTS (Grabocka et al., 2014), ADSN (Ma et al., 2020b)), (2) common deep learning methods (MLP, CNN, ResNet (Wang et al., 2017)), and (3) state-of-the-art time-series classification models (DTW (Chen et al., 2013), TNC (Tonekaboni et al., 2021), TST (Zerveas et al., 2021), TS-TCC (Eldele et al., 2021), T-Loss (Franceschi et al., 2019), TS2Vec (Yue et al., 2022)). The results for the baseline methods are taken directly from the\noriginal paper. In addition, we also design some variants of ShapeConv for ablation studies, including training without diversity loss (w/o div), training with random initialization (w/o init), and using an SVM classifier (w/ SVM).\nResults The performance of ShapeConv, its variants, and shapelet-based baselines, is evaluated on the 25 UCR datasets and presented in Figure 3 (a). For this experiment, we compared in the 25 subset of the 128 UCR dataset, because the baseline method only reported on this subset. Additionally, the summary of results for state-of-the-art time-series classification models on 125 UCR and 29 UEA datasets are shown in Table 1. The full results are presented in the Appendix G.3 In general, ShapeConv consistently outperforms all other baselines and variants, ranking first on average. These results demonstrate that ShapeConv not only provides interpretability but also excels in performance, making it a competitive choice for time-series classification compared to state-of-the-art methods. Ablation studies tell that the effectiveness of our initialization method and diversity loss contributes to improved performance compared to the variants. Lastly, the choice of downstream classifier, either SVM or MLP, does not significantly impact the performance of the ShapeConv model, indicating its flexibility and robustness in different classification settings.\nAnalysis In this section, we investigate two main research questions (RQs): (1) why does ShapeConv outperform other shapelet-based methods? (2) how are ShapeConv\u2019s interpretability results compared to other shapelet-based methods when they yield similar results?\nIn response to the first RQ, we examine the Herring datasets from UCR (Dau et al., 2019). Learned shapelets with minimum distance from the original data by the model with best validation accuracy are\n1This average accuracy metric is meaningless to some extent, due to datasets of different sizes, class skews, number of classes, default rates, etc. We list these results here just for the comparison with previous works, but we sincerely call for metrics with more practical values here.\nplotted in Figure 4. In the Herring dataset, ShapeConv (test accuracy 75.0) significantly outperforms the LTS method (test accuracy 64.1). In response to the first RQ, we observe that ShapeConv\u2019s shapelets (Figure 4 (a)) cover the most discriminative regions of the time series (the turning points), while LTS\u2019s shapelets (Figure 4 (b)) do not. This indicates that ShapeConv\u2019s learned shapelets are better at distinguishing classes, leading to improved performance.\nWe find that the learned shapelet by ShapeConv is much longer than that by LTS. The result of forcing the shapelet learned by LTS to be longer (Figure 4 (c)) reveals that LTS fails to learn a high-quality long shapelet. We also provide an ablation on the shapelet length in Table 2. It shows ShapeConv\u2019s accuracy increases with shapelet length up to a certain point, while LTS\u2019s accuracy does not benefit from the increased length. This is likely due to optimization issues in the LTS method, which cannot handle long-length shapelets. ShapeConv, on the other hand, can be efficiently computed in parallel, leading to better optimization results and significantly faster training time (about 1000\u00d7 faster). In the GunPoint dataset, both ShapeConv and LTS methods achieve saturated accuracy (100). In response to\nthe second RQ, we utilize this dataset to compare the interpretability of the learned shapelets by ShapeConv (Figure 1) and LTS (Figure 5). It is evident that the shapelet learned by ShapeConv captures the distinguishing features of the class effectively. Here, the shapelet 1 (blue) captures the gesture of reaching for the gun and drawing it out of the holster. The shapelet 2 (red) captures the gesture of putting the gun back to the holster. In contrast, the shapelets learned by LTS do not align well with either of the classes, especially for shapelet 1 in blue. Based on this observation, we conclude that ShapeConv is capable of learning more interpretable shapelets compared to LTS."
        },
        {
            "heading": "4.2 UNSUPERVISED TIME-SERIES CLUSTERING",
            "text": "Settings We evaluate our ShapeConv model on time-series clustering task using 36 UCR univariate time-series datasets (Dau et al., 2019). We first learn shapelets using a ShapeConv layer, then apply KMeans on the shapelet-transformed distance. We use the Normalized Mutual Information (NMI) metric to evaluate the models. Hyperparameters are tuned via grid search based on validation set performance, and they are reported in Appendix G.2.\nCompared Methods We compare ShapeConv with three kinds of baselines: (1) pure clustering methods (KMeans (Hartigan & Wong, 1979) applied to the entire time series), (2) shapelet-based methods (U-Shapelet (Zakaria et al., 2012), AutoShape (Li et al., 2022)), and (3) state-of-the-art time-series clustering models (k-Shape (Paparrizos & Gravano, 2015), DTC (Madiraju et al., 2018), USSL (Zhang et al., 2018), DTCR (Ma et al., 2019), STCN (Ma et al., 2020a)). The results for the baseline methods are taken directly from the original paper. We also design variants of ShapeConv for ablation studies, including training with random initialization (w/o Init), training without DBI Loss (w/o DBI), and using human knowledge to initialize shapelets (w/ Human).\nResults The results of all models on 36 UCR datasets are shown in Figure 3 (b), and details are in Appendix G.4. We compared on the 36 subset of the UCR datasets because the baseline methods only reported on this subset. In general, our ShapeConv outperforms all other baselines and variants, achieving the highest average rank among the compared methods. ShapeConv\u2019s superior performance, particularly against its randomly initialized variant, underscores the importance of proper initialization for effective clustering. Its best performance with human knowledge initialization highlights the model\u2019s ability to incorporate human knowledge to guide the learning process and improve clustering results. Overall, ShapeConv stands out as a potent, interpretable tool for unsupervised time-series clustering, outperforming existing methods while adeptly learning interpretable shapelets and assimilating human knowledge.\nAnalysis of the Initialization We now provide a case study to analyze the effect of initialization for ShapeConv in unsupervised learning tasks. We select the ECG200 dataset from UCR (Dau et al., 2019) for this analysis and results are plotted in Figure 6. First, we observe that in the time-series clustering task, the learned shapelets are close to their initializations. This is because, during the first step of learning shapelets, we solely minimize the shapelet-transformed distance, which tends to optimize within the local region. Therefore, determining the initialization of the shapelets is critical for unsupervised learning.\nIn both the random and sample initialization, one of the shapelets matches the right part of the time series, where the two classes are indistinguishable. In contrast, when using human initialization, we choose the two regions with the most significant differences between the classes (the shaded regions in Figure 6) and use the average of those regions as initialization. Consequently, the shapelets are converged in the these regions, effectively capturing the differences between the classes."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we bring together CNNs and shapelets in time-series modeling by finding the equivalance between them. Upon the findings, we further proposed ShapeConv, an interpretable convolutional kernel with its kernels serving as shapelets accompanied by shaping regularizations, and we apply ShapeConv to both supervised and unsupervised tasks. ShapeConv is designed to maintain the advantages of both CNNs and shapelets, providing excellent performance without sacrificing interpretability and controllability. Our experiments on various benchmark datasets showed that ShapeConv outperforms other shapelet-based methods and state-of-the-art time-series classification and clustering models. Moreover, the incorporation of human knowledge can further enhance the performance of ShapeConv, highlighting its potential in real-world applications where expert knowledge is available."
        },
        {
            "heading": "APPENDIX",
            "text": ""
        },
        {
            "heading": "A RELATED WORK",
            "text": "Learning Shapelets How to find the best shapelets from data has long been an intriguing problem since (Ye & Keogh, 2009) firstly proposed it. Traditional practice is to search the raw datasets with some speed-up strategies, like paralleling computing (Chang et al., 2012), SAX transformation (Rakthanmanon & Keogh, 2013) and procedure simplification through newly designed measurements (Lines et al., 2012; Guillaume et al., 2022; Zakaria et al., 2012). However, despite ingenious techniques, the performance of these methods are quite limited in large-scale real-world scenarios due to their inefficiency and inflexibility. Cutting-edge research mostly focus on learning shapelets via optimization-based methods. (Grabocka et al., 2014) firstly proposed to learn shapelets with gradient descent. (Shah et al., 2016; Lods et al., 2017) extended this idea to learn more discriminative shapelets based on DTW measure. To encourage the interpretability of learned shapelets, (Ma et al., 2020b; Wang et al., 2019) designed adversarial strategies to guide model training. Besides, in (Li et al., 2022; Zhang & Sun, 2022), autoencoder and neighbour graph structure were also leveraged to capture high-quality shapelets in an unsupervised manner. Nevertheless, none of these methods have rigorously shown and made full use of the equivalence between the CNN layer and shapelet to achieve both good interpretability and efficiency like ours.\nInterpretable Time Series Modeling Despite noticeable progress made by feature engineering methods (Ruiz et al., 2021; Bagnall et al., 2017; Middlehurst et al., 2023) like HIVE-COTE (Lines et al., 2018), MUSE (Scha\u0308fer & Leser, 2017) and ROCKET (Dempster et al., 2020), recent years have also witnessed the rapid advancement of deep learning methods in interpretable time series modeling. Apart from RNN models (Choi et al., 2016; Guo et al., 2019), newly devised CNN models have obtained more attention in this area. (Fortuin et al., 2018) developed a CNN-based SOM-VAE method to learn the topologically interpretable discrete representations of time series in a probabilistic fashion. (Luo et al., 2022) employed convolutional kernels to approximate the partial differential equations on data distribution so as to explain the nonlinear dynamics of their sequential patterns. (Li et al., 2021) designed a wavelet convolution layer to help CNNs discover filters with certain physical meaning, while (Tang et al., 2021b; Xiao et al., 2022) studied the best kernel size for time series modelling. Innovatively, we view convolution kernels from the shapelet perspective and endow shapelet-based interpretability to incomprehensible model parameters, making ShapeConv an interpretable model."
        },
        {
            "heading": "B DETAILED EXPLANATIONS ON SHAPELETS AND SHAPELET TRANSFORM",
            "text": "B.1 INTERPRETABILITY AND EXPAINABILITY\nThe interpretability of shapelet comes from its human-comprehensible nature. However, \u201cinterpretability\u201d is sometimes confounded with the concept of \u201cexplainability\u201d, introduced in many post-hoc explainable methods (Tonekaboni et al., 2020; Leung et al., 2021). These explainable methods focus on explaining an already learned, non-interpretable model. They may not reflect the actual behavior of the original model and may disagree with each other (Krishna et al., 2022). Self-interpretable methods such as shapelets do not have these issues as the model itself provides explanations during the learning process. Furthermore, the proposed ShapeConv in our paper whose kernel serves as shapelet inherit the same interpretability of the traditional shapelet, utilizing the prototypical shape information of the data to perform classification.\nB.2 VISUALIZING SHAPELETS AND SHAPELET TRANSFORM BY SHAPECONV\nTo illustrates shapelets and shapelet transform more vividly, we take a real-dataset example named GunPoint, aiming at classifying whether a person is pointing with a finger or a gun (Figure B.1). The sequences in the dataset record the normalized x-axis of the hand position, i.e., how far away the hand is from the main body through time.\nFigure B.2 demonstrates the result. The leftmost two subfigures show the learned shapelet. When we try to align the shapelets with typical samples one with \u201cgun\u201d (Class 1) in blue and one with \u201cfinger\u201d (Class 2) in orange in the middle subfigure, we can immediately find that the most distinguishable shapes underlying data from Class 1 are the small flat stage soon after the beginning and symmetrically\nin the end. These two shapes corresponds to the motion of a hand pulling a gun out of the holster and put it back, and would not exist when a person is pointing with his finger.\nTherefore, when we perform the shapelet transform by calculating the distance between data points and the learned shapelet and draw them in the 2-D feature plane as is shown in the right subfigure, we found the two classes are linear separable as expected. Data points at the left-bottom of the plane from the \u201cgun\u201d class contain the shapelets indicating the gun-related actions, while data from the \u201cfinger\u201d class do not. This illustrate how the model can be discriminative while providing interpretability at the same time.\nIn order to further verify our interpretations, we obtained the frames of videos during the data collection process from the collector and visualized the trajectory of hand movements for the first half of the sequence, as is shown in Figure B.3. For data of the Gun class, we observe clear stops when pulling the gun out of the holster as is highlighted in green circles, and such movements do not exist in the data of the Point class. This observation matches our hypothesis made upon the learned shapelets."
        },
        {
            "heading": "Shapelet-Transformed DataLearned Shapelet",
            "text": ""
        },
        {
            "heading": "C PROOF OF THEOREM 3.1",
            "text": "Theorem 3.1 Assume the input X \u2208 Rlx is a 1-dimensional single-variate signal of length lx, and nout shapelets S\u2217 = {s1, s2, ...snout} with length ls are discovered. The feature extracted from X with si and squared Euclidean distance is dsi,X. Then we have\ndsi,X = \u22122 max j\u2208{1,2,...,lx\u2212ls+1} [Yij \u2212N (si,Xj\u2236j+ls\u22121)], (10)\nwhere Y = si \u2217X is the cross-correlation defined in Eq. 2 and N (si,X\u2032) = (\u2225si\u222522 + \u2225X\u2032\u222522)/2 is squared L2 norm term.\n2Image Source: http://www.timeseriesclassification.com/description.php? Dataset=GunPoint\nProof. According to Eq.1, shapelet transform step extract features using minimal distance and can be expanded as:\ndsi,X =min x\u2208X\u0302 dist(si,x) = min j\u2208{1,2,\u22ef,lx\u2212ls+1} \u2225si \u2212Xj\u2236j+ls\u22121\u222522\n= min j\u2208{1,2,\u22ef,lx\u2212ls+1}\n(\u2225si\u222522 + \u2225Xj\u2236j+ls\u22121\u222522 \u2212 2 ls\n\u2211 k=1 sikXj+k)\n= min j\u2208{1,2,\u22ef,lx\u2212ls+1} [\u2225si\u222522 + \u2225Xj\u2236j+ls\u22121\u222522 \u2212 2(si \u2217X)k],\n= \u22122 max j\u2208{1,2,\u22ef,lx\u2212ls+1} [Yij \u2212N (si,Xj\u2236j+ls\u22121)]\nwith Y and N (si,Xj\u2236j+ls\u22121) defined as in the theorem. \u25fb"
        },
        {
            "heading": "D DETAILS ON MODEL DESIGNS",
            "text": ""
        },
        {
            "heading": "D.1 COMPATIBLE WITH OTHER CLASSIFIERS",
            "text": "ShapeConv can also be used together with traditional classifiers, such as support vector machines (SVMs), decision trees, or random forests. In this case, the whole model cannot be optimized via an end-to-end fashion, so we decompose the shapelet learning step and classification. Specifically, no additional module is appended to the ShapeConv layer, and the output of the layer is directly optimized using the above loss function, but with Lcls term in Eq. 8 removed. Then, the learned features are fed into the chosen classifier for training and prediction. We also include this variant in our experiment in Sec. 4.1."
        },
        {
            "heading": "D.2 DAVIES-BOULDIN INDEX (DBI) LOSS",
            "text": "When the number of cluster is set to k, DBI can be denoted as\nIDBI = 1\nk\nk\n\u2211 i=1 max j=1...k,j/=i ri + rj dij . (11)\nHere, ri is the diameter of cluster i, which is defined as the average distance between each element in cluster i and the center of cluster i. The distance between the center of cluster i and cluster j is di,j . However, this formulation is not tractable for optimization due to the max operator. Thus, following Li et al. (2022), the max operator is replaced and approximated by the following calculation,\nLDBI = 1\nk\nk\n\u2211 i=1\n\u2211kj=1mij \u22c5 e\u03b1mij\n\u2211kj=1 e\u03b1mij , (12)\nwhere mij = ri+rjdij . By numerical verification, \u03b1 = 100 is enough for Eq. 12 to approximate the true maximum.\nD.3 ILLUSTRATION OF DIVERSITY LOSS\nFigure D.1 depicts the trained shapelet both with and without diversity loss. The illustration reveals that without the application of diversity regularization, the shapelets tend to converge to the same local minimum. Employing diversity loss can mitigate this issue."
        },
        {
            "heading": "E SHAPECONV AS FEATURE EXTRACTOR WITH DEEP MODELS",
            "text": "Another advantage brought by our ShapeConv to find shapelets with a special kind of convolutional layer is the flexibility. While the traditional shapelet works suffer from handling large time-series data of long sequence efficiently, ShapeConv turns the extraction of shapelets from the original data into a stackable layer can be combined with more sophisticated deep models and optimized in an end-to-end manner, leaving the possibility to keep the interpretability and effectiveness to the maximum extent.\nTo verify the effectiveness of ShapeConv embedded in a deep model, we apply ShapeConv as the first layer to extract features which are further processed by GRU(Cho et al., 2014), a widely used time-series model for long-term modeling. We then conducted the experiment of the proposed method on the seizure detection task based on electroencephalograph (EEG) data (Obeid & Picone, 2016). The dataset contains 97,859 samples (83,647 for training and 14,212 for testing), and each sample contains 20-channel 30-second EEG signal sampled at 200Hz. The goal of the prediction models is to predict the probability of seizure event within the given EEG signal piece, following (Tang et al., 2021a; Li et al., 2023).\nWe compare our model ShapeConv with the most commonly used deep neural network models GRU (Cho et al., 2014) and TCN (Bai et al., 2018). The empirical results are illustrated in Table E.1. Our model has significantly outperformed the compared baselines, which showed the superiority of the proposed ShapeConv paradigm even embedded in another neural architectures.\nTable E.1: Performance comparison on seizure detection task.\nModel AUROC AUPRC\nGRU Cho et al. (2014) 0.814(0.009) 0.386(0.018) TCN Bai et al. (2018) 0.817(0.004) 0.383(0.010) ShapeConv + GRU 0.837(0.007) 0.414(0.008)\nWe further investigate whether our ShapeConv can preserve its interpretability when stacking with deep models. As clinical practice, the morphology of waveform in EEG describes its overall shape, and is important for both interpreting a tracing and communicating findings, which has been wellstudied and recognized in previous medical research (Marcuse et al., 2015). To this end, we visualized\nEEG Examples from Textbook Learned Shapelet\nWaveform 1\nWaveform 2\nShapelet 1\nShapelet 2\nPolymorphic\nDelta Waves\nParoxysmal\nRhythm Waves\n(a) Comparison between two types of textbook waveform and the two learned shapelets, polymorphic delta waves related to lesions (Marshall et al., 1988) and paroxysmal rhythm waves related to sleep stage (Alvarez et al., 1983). (b) Shapelet transform by the two shapelets. Some data points from Class 1 are mixed with Class 2 need to be separated by other shapelets.\nFigure E.1: Demonstration of the learned shapelets for EEG data.\n(a) (b)\nFigure E.2: Alignment between the learned shapelets and original data.\na few obtained shapelets out of 2,688 shapelets (128 shapelets per variate, 21 variates in total) and excitedly found that some of them accord with some textbook waveform, as is shown in Figure E.1a. By aligning them with the mostly similar part of the original data (Figure E.2), we found these shapelets can match a specific type of seizure status and provides solid classification criteria (Figure E.1b). This showcases the possibility of how our interpretable method can benefit medical practitioners in practice by not only offering an accurate judgement, but also pointing out the area of interests with respect to their expert knowledge.\nIt\u2019s also noteworthy that since the amount of summarized waveform in textbook is limited, some shapelets, while serving as similarly strong indicators, may not be included in existing studies. We believe these shapelets can provide inspirations and boost further research in related area."
        },
        {
            "heading": "F MORE VISUALIZATIONS OF SHAPECONV",
            "text": ""
        },
        {
            "heading": "F.1 MORE VISUALIZATIONS ON LEARNT SHAPELETS",
            "text": "In this section, we provide more visualizations of the learnt shapelets of ShapeConv in different UCR datasets. The results clearly shows that ShapeConv could learn the determining regions of the time series.\nF.2 VISUALIZATIONS WITH SHAP VALUE\nIn this section, we further substantiate our claim regarding the interpretability of our model using the SHAP (SHapley Additive exPlanations) Value (Lundberg & Lee, 2017). The analysis employs the GunPoint dataset from the UCR archive. We examine two variations of our model: the original\n0 50 100 150 200 250 300 0.0\n0.2\n0.4\n0.6\n0.8\n1.0 Class 2 Class 1 Shapelet 1 Shapelet 2\n0 10 20 30 40 50 60 70 0.0\n0.2\n0.4\n0.6\n0.8\n1.0 Class 1 Class 2 Shapelet 1 Shapelet 2\n(a) DodgerLoopWeekend (b) SonyAIBORobotSurface1\n0 5 10 15 20 0.0\n0.2\n0.4\n0.6\n0.8\n1.0 Class 2 Class 1 Shapelet 1 Shapelet 2\n0 20 40 60 80 100 120 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass 1 Class 2\nClass 3 Shapelet 1\nShapelet 2 Shapelet 3\n(c) ItalyPowerDemand (d) BME\nFigure F.1: More visualizations of the shapelets learnt by ShapeConv in UCR datasets.\nShapeConv and a modified version where the term \u03bbshape in the loss function (Equation 8) is set to zero. Setting \u03bbshape to zero eliminates the L2 norm term, effectively transforming the layer into a standard CNN. Consequently, this variant lacks the interpretability feature.\nBoth models underwent training under identical hyperparameters. Post-training, we computed the SHAP Values for each model across the entire test dataset using the expected gradients approach. These values are illustrated in Figure F.2, with the mean SHAP value of each class depicted. The blue and orange lines represent the Gun Class (Class 1) and No Gun Class (Class 2), respectively.\nThe left side of Figure F.2 reveals that the model is particularly sensitive to the left and right turning points. These points symbolize the gesture of drawing the gun out of the holster and putting it back, underscoring the model\u2019s reliance on these regions for decision-making. This observation aligns with our hypothesis about the model\u2019s interpretative capabilities. However, on the right, we first notice that the kernel does not match with the input sequence, indicating the lack of interpretability. Additionally, the model appears to base its decisions predominantly on the left region. This disparity highlights the limitations of the variant without the interpretability term.\nIn conclusion, our findings are twofold: firstly, the integration of Shape Loss successfully enhances interpretability. Secondly, ShapeConv not only encompasses all significant regions identified by the SHAP Value of the baseline CNN, but also surpasses conventional explainability methods like SHAP by capturing the shape of sensitive regions, rather than merely indicating their locations.\n0.0\n0.5\n1.0\nClass 1 Class 2 Shapelet 1 Shapelet 2\n0.1 0.0 0.1\nSHAP Value - Class 1\n0 50 100 150 0.1 0.0 0.1 SHAP Value - Class 2\n0.0\n0.5\n1.0\nClass 1 Class 2 Kernel 1 Kernel 2\n0.25 0.00 0.25 SHAP Value - Class 1\n0 50 100 150 0.25 0.00 0.25 SHAP Value - Class 2\n(a) ShapeConv (b) \u03bbshape = 0\nFigure F.2: Illustration of the trained average SHAP value for different class across all the GunPoint testing dataset. Blue: Gun Class. Orange: No Gun Class. Left: our proposed ShapeConv method. Right: ShapeConv with \u03bbshape = 0."
        },
        {
            "heading": "G DETAILS ON EXPERIMENTS AND ANALYSIS",
            "text": ""
        },
        {
            "heading": "G.1 ENVIRONMENT",
            "text": "All experiments are performed on the PyTorch framework using a 24-cores AMD Epyc 7V13 2.5GHz CPU, 220GB RAM, and an NVIDIA A100 80GB PCIe GPU. The server is provided by the Azure cloud computing platform."
        },
        {
            "heading": "G.2 HYPERPARAMETERS",
            "text": "Supervised Learning The training set is divided into training and validation sets at an 8:2 ratio. Hyperparameters are tuned via grid search based on validation set performance. The number of shapelets is chosen from {1,2,3,4,5} times the number of classes, and the shapelet length is evaluated over {0.1,0.2,\u22ef,0.8} times the time series length. The parameter \u03bbshape is chosen from {0.01,0.1,1,10} and the parameter \u03bbdiv is evaluated over {0.01,0.1,1,10}. Learning rate is chosen from {0.001,0.005,0.01,0.05,0.1}.\nUnsupervised Learning The training set is divided into training and validation sets at an 8:2 ratio. Hyperparameters are tuned via grid search based on validation set performance. The number of shapelets is chosen from {1,2,3,4,5} times the number of classes, and the shapelet length is evaluated over {0.1,0.15,0.2,0.25,\u22ef,0.8} times the time series length. The parameter \u03bbshape is chosen from {0.01,0.1,1,10} and the parameter \u03bbdiv is evaluated over {0.01,0.1,1,10}. Learning rate is chosen from {0.001,0.005,0.01,0.05,0.1}."
        },
        {
            "heading": "G.3 RESULTS OF SUPERVISED LEARNING TASKS",
            "text": "In this section, we present the full results of supervised time-series classification tasks. We compared ShapeConv with (1) shapelet-based methods, common deep learning methods, and ablations (described in Sec. 4.1) across 25 UCR Datasets (Table G.1) (2) RNN-based methods (Tang et al., 2021b) across 56 UCR Datasets (Table G.2) (3) state-of-the-art times series classification methods (described in Sec. 4.1) across 125 UCR datasets (Table G.3) (4) state-of-the-art times series classification methods (described in \u00a74.1) across 30 UEA datasets (Table G.4).\nTable G.1: ShapeConv Compared with Shapelet-Based Methods, Common Deep Learning Methods, and Ablations: Evaluating Testing Accuracy for Supervised Time-Series Classification Tasks Across 25 UCR Datasets. Mean accuracy \u00b1 std over 3 independent experiments with different random seeds is reported.\nDataset MLP CNN ResNet IGSVM FLAG LTS ADSN ShapeC. ShapeC. ShapeC. ShapeC.w/o init w/ SVM w/o div\nAdiac 0.752 0.857 0.826 0.235 0.752 0.519 0.798 0.691\u00b10.007 0.813\u00b10.047 0.852\u00b10.014 0.867\u00b10.020 Beef 0.833 0.750 0.767 0.900 0.833 0.767 0.933 0.849\u00b10.005 0.898\u00b10.031 0.921\u00b10.001 0.936\u00b10.008 Chlorine. 0.872 0.843 0.828 0.571 0.760 0.730 0.880 0.825\u00b10.026 0.904\u00b10.025 0.907\u00b10.008 0.924\u00b10.014 Coffee 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000\u00b10.000 1.000\u00b10.000 1.000\u00b10.000 1.000\u00b10.000 Diatom. 0.964 0.930 0.931 0.931 0.964 0.942 0.987 0.992\u00b10.008 0.991\u00b10.001 0.992\u00b10.005 0.994\u00b10.009 DPLittle 0.701 0.703 0.701 0.666 0.683 0.734 0.727 0.698\u00b10.012 0.706\u00b10.003 0.703\u00b10.000 0.713\u00b10.013 DPMiddle 0.721 0.736 0.723 0.695 0.713 0.741 0.784 0.778\u00b10.018 0.782\u00b10.003 0.789\u00b10.001 0.807\u00b10.039 DPThumb 0.705 0.701 0.705 0.696 0.705 0.752 0.736 0.729\u00b10.010 0.715\u00b10.037 0.738\u00b10.005 0.753\u00b10.015 ECGFiveDays 0.970 0.985 0.955 0.990 0.920 1.000 1.000 1.000\u00b10.000 1.000\u00b10.000 1.000\u00b10.000 1.000\u00b10.000 FaceFour 0.830 0.932 0.932 0.977 0.909 0.943 0.977 0.941\u00b10.023 0.916\u00b10.001 0.942\u00b10.015 0.961\u00b10.007 GunPoint 0.933 1.000 0.993 1.000 0.967 0.996 0.987 0.994\u00b10.016 0.995\u00b10.008 0.993\u00b10.003 0.997\u00b10.016 Herring 0.641 0.681 0.641 0.641 0.641 0.641 0.703 0.703\u00b10.014 0.713\u00b10.005 0.702\u00b10.006 0.724\u00b10.028 ItalyPower. 0.966 0.970 0.960 0.937 0.946 0.958 0.972 0.932\u00b10.027 0.953\u00b10.005 0.961\u00b10.001 0.974\u00b10.007 Lightning7 0.644 0.863 0.836 0.630 0.767 0.790 0.808 0.758\u00b10.003 0.723\u00b10.001 0.748\u00b10.001 0.781\u00b10.004 Medicallmages 0.729 0.792 0.772 0.552 0.714 0.713 0.720 0.682\u00b10.012 0.694\u00b10.000 0.752\u00b10.009 0.774\u00b10.003 MoteStrain 0.869 0.950 0.895 0.887 0.888 0.900 0.906 0.884\u00b10.013 0.886\u00b10.016 0.898\u00b10.002 0.913\u00b10.000 MPLittle 0.703 0.758 0.726 0.707 0.693 0.743 0.758 0.701\u00b10.012 0.733\u00b10.002 0.741\u00b10.008 0.749\u00b10.032 MPMiddle 0.750 0.800 0.775 0.769 0.750 0.775 0.791 0.736\u00b10.020 0.759\u00b10.011 0.779\u00b10.013 0.807\u00b10.020 PPLittle 0.710 0.753 0.761 0.721 0.671 0.710 0.715 0.661\u00b10.013 0.694\u00b10.029 0.676\u00b10.000 0.732\u00b10.001 PPMiddle 0.707 0.784 0.753 0.759 0.738 0.749 0.786 0.717\u00b10.023 0.726\u00b10.010 0.764\u00b10.002 0.791\u00b10.000 PPThumb 0.726 0.745 0.708 0.755 0.674 0.705 0.695 0.685\u00b10.018 0.712\u00b10.010 0.728\u00b10.010 0.731\u00b10.011 Sony. 0.727 0.968 0.985 0.927 0.929 0.910 0.915 0.901\u00b10.026 0.914\u00b10.007 0.903\u00b10.002 0.926\u00b10.010 Symbols 0.853 0.962 0.872 0.846 0.875 0.945 0.963 0.942\u00b10.026 0.968\u00b10.006 0.974\u00b10.007 0.980\u00b10.015 SyntheticC. 0.950 0.990 1.000 0.873 0.997 0.973 1.000 1.000\u00b10.000 1.000\u00b10.000 0.997\u00b10.029 1.000\u00b10.000 Trace 0.820 1.000 1.000 0.980 0.990 1.000 1.000 1.000\u00b10.000 1.000\u00b10.000 1.000\u00b10.000 1.000\u00b10.000 TwoLeadECG 0.853 1.000 1.000 1.000 0.990 1.000 0.986 1.000\u00b10.000 1.000\u00b10.000 1.000\u00b10.000 1.000\u00b10.000 Avg. Acc 80.5 86.4 84.8 79.4 82.6 83.2 86.6 83.8 85.4 86.4 87.8 Avg. Rank 8.6 4.7 6.1 7.8 8.4 6.2 4.2 7.2 5.8 4.6 2.4\nTable G.2: ShapeConv Compared with RNN-Based Methods: Evaluating Testing Accuracy for Supervised Time-Series Classification Tasks Across 56 UCR Datasets. Mean accuracy \u00b1 std over 3 independent experiments with different random seeds is reported.\nDataset RNTK NTK RBF POLY Gaussian Identity GRU OS-CNN ShapeConvRNN RNN\nAdiac 0.766 0.719 0.734 0.778 0.514 0.169 0.606 0.835 0.882\u00b10.009 Arrowhead 0.806 0.834 0.806 0.749 0.480 0.560 0.377 0.838 0.915\u00b10.033 Beef 0.900 0.733 0.833 0.933 0.267 0.467 0.367 0.807 0.941\u00b10.001 Car 0.833 0.788 0.800 0.800 0.233 0.583 0.267 0.933 0.992\u00b10.008 ChlorineConcentration 0.908 0.773 0.864 0.915 0.660 0.558 0.611 0.839 0.924\u00b10.002 Coffee 1.000 1.000 0.929 0.929 1.000 0.429 0.571 1.000 1.000\u00b10.000 Computers 0.592 0.552 0.588 0.564 0.532 0.552 0.588 0.707 0.656\u00b10.009 CricketX 0.605 0.595 0.621 0.626 0.085 0.636 0.264 0.855 0.914\u00b10.009 CricketY 0.639 0.590 0.605 0.597 0.159 0.592 0.362 0.867 0.729\u00b10.020 CricketZ 0.603 0.592 0.621 0.592 0.085 0.579 0.413 0.863 0.764\u00b10.003 DistalPhalanxOutlineC. 0.775 0.775 0.754 0.739 0.699 0.696 0.750 0.766 0.804\u00b10.030 DistalPhalanxTW 0.662 0.698 0.669 0.674 0.676 0.647 0.691 0.664 0.781\u00b10.008 Earthquakes 0.748 0.748 0.748 0.748 0.655 0.770 0.770 0.670 0.784\u00b10.031 ECG200 0.930 0.890 0.890 0.860 0.860 0.720 0.760 0.908 1.000\u00b10.000 ECG5000 0.938 0.940 0.937 0.940 0.884 0.932 0.933 0.940 0.963\u00b10.012 Faceall 0.741 0.833 0.833 0.824 0.537 0.705 0.707 0.845 0.853\u00b10.007 FacesUCR 0.817 0.802 0.803 0.830 0.532 0.753 0.795 0.967 0.957\u00b10.004 FiftyWords 0.686 0.686 0.697 0.688 0.343 0.602 0.653 0.816 0.699\u00b10.024 Fish 0.903 0.840 0.857 0.880 0.280 0.383 0.240 0.987 0.920\u00b10.037 FreezerRegularTrain 0.974 0.944 0.965 0.968 0.761 0.075 0.866 0.997 0.997\u00b10.036 GunPoint 0.980 0.953 0.953 0.940 0.820 0.747 0.807 0.999 1.000\u00b10.000 GunPointAgeSpan 0.965 0.946 0.959 0.940 0.478 0.478 0.956 0.992 1.000\u00b10.000 GunPointMaleVSFemale 0.991 0.997 0.994 0.997 0.687 0.525 0.997 0.999 1.000\u00b10.000 GunPointOldVSYoung 0.987 0.975 0.987 0.946 0.540 0.524 0.984 1.000 1.000\u00b10.000 Ham 0.705 0.716 0.667 0.714 0.533 0.600 0.610 0.704 0.733\u00b10.009 Herring 0.567 0.594 0.594 0.594 0.233 0.594 0.594 0.608 0.750\u00b10.014 InsectEPGRegular 0.996 0.992 0.996 0.968 1.000 1.000 0.984 0.951 1.000\u00b10.000 Lightning2 0.787 0.738 0.705 0.689 0.459 0.705 0.672 0.807 0.819\u00b10.008 Lightning7 0.616 0.603 0.630 0.603 0.233 0.699 0.767 0.793 0.808\u00b10.023 Meat 0.933 0.933 0.933 0.933 0.006 0.550 0.333 0.947 0.950\u00b10.016 MedicalImages 0.745 0.733 0.753 0.746 0.482 0.649 0.691 0.769 0.709\u00b10.028 MiddlePhalanxOutlineC. 0.571 0.571 0.487 0.643 0.763 0.570 0.746 0.814 0.856\u00b10.001 MiddlePhalanxTW 0.578 0.610 0.597 0.604 0.584 0.584 0.591 0.519 0.642\u00b10.002 OliveOil 0.900 0.867 0.867 0.833 0.667 0.400 0.400 0.787 0.833\u00b10.000 plane 0.981 0.962 0.971 0.971 0.962 0.848 0.962 1.000 1.000\u00b10.000 PowerCons 0.972 0.972 0.967 0.917 0.961 0.950 0.994 0.990 0.911\u00b10.004 ProximalPhalanxOutlineC. 0.890 0.880 0.873 0.869 0.828 0.746 0.869 0.908 0.913\u00b10.006 RefrigerationDevices 0.469 0.371 0.365 0.411 0.360 0.509 0.467 0.503 0.613\u00b10.002 ScreenType 0.416 0.432 0.435 0.384 0.400 0.411 0.363 0.526 0.493\u00b10.010 SemgHandSubjectCh2 0.842 0.853 0.861 0.867 0.200 0.367 0.891 0.718 0.981\u00b10.025 SmallKitchenAppliances 0.675 0.384 0.403 0.379 0.602 0.760 0.715 0.721 0.803\u00b10.005 SmoothSubspace 0.960 0.873 0.920 0.867 0.940 0.953 0.927 0.989 1.000\u00b10.000 StarLightCurves 0.959 0.962 0.946 0.944 0.821 0.868 0.962 0.975 0.987\u00b10.014 Strawberry 0.984 0.976 0.970 0.968 0.943 0.754 0.916 0.982 0.919\u00b10.000 SwedishLeaf 0.906 0.910 0.914 0.907 0.592 0.459 0.910 0.971 0.961\u00b10.015 SyntheticControl 0.987 0.967 0.980 0.977 0.927 0.977 0.990 0.999 1.000\u00b10.000 Trace 0.960 0.810 0.760 0.760 0.700 0.710 1.000 1.000 1.000\u00b10.000 TwoPatterns 0.943 0.905 0.913 0.939 0.997 0.999 1.000 1.000 1.000\u00b10.000 UMD 0.917 0.924 0.972 0.910 0.444 0.715 1.000 0.993 1.000\u00b10.000 UWaveGestureLibraryX 0.796 0.787 0.785 0.658 0.560 0.753 0.736 0.822 0.832\u00b10.006 UWaveGestureLibraryY 0.716 0.706 0.704 0.703 0.445 0.652 0.654 0.757 0.754\u00b10.003 UWaveGestureLibraryZ 0.740 0.739 0.729 0.719 0.433 0.678 0.703 0.764 0.805\u00b10.003 WordSynonyms 0.580 0.585 0.611 0.621 0.177 0.458 0.538 0.742 0.784\u00b10.022 Worms 0.571 0.507 0.558 0.507 0.351 0.494 0.416 0.765 0.803\u00b10.021 WormsTwoClass 0.623 0.623 0.610 0.597 0.519 0.468 0.571 0.657 0.815\u00b10.028 Yoga 0.849 0.846 0.846 0.849 0.464 0.767 0.618 0.911 0.772\u00b10.006 Average Accuracy 80.1 77.7 78.2 77.7 56.0 63.1 69.5 84.8 87.0 Average Rank 4.2 5.0 4.9 5.4 7.8 7.2 6.1 2.7 1.8\nTable G.3: ShapeConv Compared with State-Of-The-Art Time-Series Classification Methods: Evaluating Testing Accuracy for Supervised Time-Series Classification Tasks Across 128 UCR Datasets. Mean accuracy \u00b1 std over 3 independent experiments with different random seeds is reported.\nDTW TNC TST TS-TCC T-Loss TS2Vec ROCKET ShapeConv\nAdiac 0.604 0.726 0.550 0.767 0.675 0.775 0.783 0.867\u00b10.000 ArrowHead 0.703 0.703 0.771 0.737 0.766 0.857 0.814 0.903\u00b10.005 Beef 0.633 0.733 0.500 0.600 0.667 0.767 0.833 0.936\u00b10.014 BeetleFly 0.700 0.850 1.000 0.800 0.800 0.900 0.900 1.000\u00b10.000 BirdChicken 0.750 0.750 0.650 0.650 0.850 0.800 0.900 1.000\u00b10.000 Car 0.733 0.683 0.550 0.583 0.833 0.883 0.847 0.974\u00b10.000 CBF 0.997 0.983 0.898 0.998 0.983 1.000 1.000 1.000\u00b10.000 ChlorineConcentration 0.648 0.760 0.562 0.753 0.749 0.832 0.815 0.924\u00b10.003 CinCECGTorso 0.651 0.669 0.508 0.671 0.713 0.827 0.836 0.778\u00b10.007 Coffee 1.000 1.000 0.821 1.000 1.000 1.000 1.000 1.000\u00b10.000 Computers 0.700 0.684 0.696 0.704 0.664 0.660 0.761 0.647\u00b10.000 CricketX 0.754 0.623 0.385 0.731 0.713 0.805 0.819 0.895\u00b10.009 CricketY 0.744 0.597 0.467 0.718 0.728 0.769 0.852 0.726\u00b10.016 CricketZ 0.754 0.682 0.403 0.713 0.708 0.792 0.856 0.773\u00b10.017 DiatomSizeReduction 0.967 0.993 0.961 0.977 0.984 0.987 0.970 0.994\u00b10.005 DistalPhalanxOutlineCorrect 0.717 0.754 0.728 0.754 0.775 0.775 0.770 0.753\u00b10.026 DistalPhalanxOutlineAgeGroup 0.770 0.741 0.741 0.755 0.727 0.727 0.759 0.784\u00b10.017 DistalPhalanxTW 0.590 0.669 0.568 0.676 0.676 0.698 0.719 0.763\u00b10.022 Earthquakes 0.719 0.748 0.748 0.748 0.748 0.748 0.748 0.731\u00b10.010 ECG200 0.770 0.830 0.830 0.880 0.940 0.920 0.906 0.992\u00b10.001 ECG5000 0.924 0.937 0.928 0.941 0.933 0.935 0.947 0.953\u00b10.007 ECGFiveDays 0.768 0.999 0.763 0.878 1.000 1.000 1.000 1.000\u00b10.000 ElectricDevices 0.602 0.700 0.676 0.686 0.707 0.721 0.729 0.743\u00b10.013 FaceAll 0.808 0.766 0.504 0.813 0.786 0.805 0.947 0.827\u00b10.037 FaceFour 0.830 0.659 0.511 0.773 0.920 0.932 0.977 0.961\u00b10.019 FacesUCR 0.905 0.789 0.543 0.863 0.884 0.930 0.961 0.930\u00b10.000 FiftyWords 0.690 0.653 0.525 0.653 0.732 0.774 0.830 0.699\u00b10.009 Fish 0.823 0.817 0.720 0.817 0.891 0.937 0.979 0.917\u00b10.006 FordA 0.555 0.902 0.568 0.930 0.928 0.948 0.944 0.954\u00b10.020 FordB 0.620 0.733 0.507 0.815 0.793 0.807 0.805 0.835\u00b10.022 GunPoint 0.907 0.967 0.827 0.993 0.980 0.987 1.000 0.997\u00b10.002 Ham 0.467 0.752 0.524 0.743 0.724 0.724 0.726 0.733\u00b10.032 HandOutlines 0.881 0.930 0.735 0.724 0.922 0.930 0.942 0.947\u00b10.016 Haptics 0.377 0.474 0.357 0.396 0.490 0.536 0.524 0.580\u00b10.003 Herring 0.531 0.594 0.594 0.594 0.594 0.641 0.692 0.724\u00b10.008 InlineSkate 0.384 0.378 0.287 0.347 0.371 0.415 0.457 0.432\u00b10.023 InsectWingbeatSound 0.355 0.549 0.266 0.415 0.597 0.630 0.657 0.613\u00b10.008 ItalyPowerDemand 0.950 0.928 0.845 0.955 0.954 0.961 0.970 0.974\u00b10.017 LargeKitchenAppliances 0.795 0.776 0.595 0.848 0.789 0.875 0.901 0.917\u00b10.003 Lightning2 0.869 0.869 0.705 0.836 0.869 0.869 0.759 0.819\u00b10.000 Lightning7 0.726 0.767 0.411 0.685 0.795 0.863 0.823 0.781\u00b10.007 Mallat 0.934 0.871 0.713 0.922 0.951 0.915 0.956 0.932\u00b10.010 Meat 0.933 0.917 0.900 0.883 0.950 0.967 0.948 0.943\u00b10.017 MedicalImages 0.737 0.754 0.632 0.747 0.750 0.793 0.799 0.774\u00b10.003 MiddlePhalanxOutlineCorrect 0.698 0.818 0.753 0.818 0.825 0.838 0.838 0.827\u00b10.004 MiddlePhalanxOutlineAgeGroup 0.500 0.643 0.617 0.630 0.656 0.636 0.590 0.669\u00b10.011 MiddlePhalanxTW 0.506 0.571 0.506 0.610 0.591 0.591 0.560 0.637\u00b10.007 MoteStrain 0.835 0.825 0.768 0.843 0.851 0.863 0.915 0.919\u00b10.029 NonInvasiveFetalECGThorax1 0.790 0.898 0.471 0.898 0.878 0.930 0.913 0.913\u00b10.012 NonInvasiveFetalECGThorax2 0.865 0.912 0.832 0.913 0.919 0.940 0.929 0.942\u00b10.013 OliveOil 0.833 0.833 0.800 0.800 0.867 0.900 0.917 0.827\u00b10.015 OSULeaf 0.591 0.723 0.545 0.723 0.760 0.876 0.941 0.905\u00b10.004 PhalangesOutlinesCorrect 0.728 0.787 0.773 0.804 0.784 0.823 0.834 0.813\u00b10.017 Phoneme 0.228 0.180 0.139 0.242 0.276 0.312 0.280 0.204\u00b10.004 Plane 1.000 1.000 0.933 1.000 0.990 1.000 1.000 1.000\u00b10.000 ProximalPhalanxOutlineCorrect 0.784 0.866 0.770 0.873 0.859 0.900 0.899 0.913\u00b10.019 ProximalPhalanxOutlineAgeGroup 0.805 0.854 0.854 0.839 0.844 0.844 0.856 0.869\u00b10.004 ProximalPhalanxTW 0.761 0.810 0.780 0.800 0.771 0.824 0.817 0.831\u00b10.033 RefrigerationDevices 0.464 0.565 0.483 0.563 0.515 0.589 0.537 0.594\u00b10.026 ScreenType 0.397 0.509 0.419 0.419 0.416 0.411 0.485 0.423\u00b10.007 ShapeletSim 0.650 0.589 0.489 0.683 0.672 1.000 1.000 1.000\u00b10.000 ShapesAll 0.768 0.788 0.733 0.773 0.848 0.905 0.907 0.853\u00b10.001 SmallKitchenAppliances 0.643 0.725 0.592 0.691 0.677 0.733 0.818 0.741\u00b10.005 SonyAIBORobotSurface1 0.725 0.804 0.724 0.899 0.902 0.903 0.922 0.962\u00b10.005 SonyAIBORobotSurface2 0.831 0.834 0.745 0.907 0.889 0.890 0.913 0.914\u00b10.018 StarLightCurves 0.907 0.968 0.949 0.967 0.964 0.971 0.981 0.987\u00b10.004 Strawberry 0.941 0.951 0.916 0.965 0.954 0.965 0.981 0.903\u00b10.008 SwedishLeaf 0.792 0.880 0.738 0.923 0.914 0.942 0.964 0.952\u00b10.012 Symbols 0.950 0.885 0.786 0.916 0.963 0.976 0.974 0.980\u00b10.041 SyntheticControl 0.993 1.000 0.490 0.990 0.987 0.997 1.000 1.000\u00b10.000 ToeSegmentation1 0.772 0.864 0.807 0.930 0.939 0.947 0.968 0.957\u00b10.003 ToeSegmentation2 0.838 0.831 0.615 0.877 0.900 0.915 0.924 0.931\u00b10.020 Trace 1.000 1.000 1.000 1.000 0.990 1.000 1.000 1.000\u00b10.000\nTwoLeadECG 0.905 0.993 0.871 0.976 0.999 0.987 0.999 1.000\u00b10.000 TwoPatterns 1.000 1.000 0.466 0.999 0.999 1.000 1.000 1.000\u00b10.000 UWaveGestureLibraryX 0.728 0.781 0.569 0.733 0.785 0.810 0.815 0.805\u00b10.034 UWaveGestureLibraryY 0.634 0.697 0.348 0.641 0.710 0.729 0.744 0.738\u00b10.013 UWaveGestureLibraryZ 0.658 0.721 0.655 0.690 0.757 0.770 0.732 0.792\u00b10.018 UWaveGestureLibraryAll 0.892 0.903 0.475 0.692 0.896 0.934 0.925 0.941\u00b10.028 Wafer 0.980 0.994 0.991 0.994 0.992 0.998 0.998 0.973\u00b10.006 Wine 0.574 0.759 0.500 0.778 0.815 0.889 0.813 0.894\u00b10.018 WordSynonyms 0.649 0.630 0.422 0.531 0.691 0.704 0.753 0.765\u00b10.025 Worms 0.584 0.623 0.455 0.753 0.727 0.701 0.740 0.783\u00b10.002 WormsTwoClass 0.623 0.727 0.584 0.753 0.792 0.805 0.797 0.815\u00b10.011 Yoga 0.837 0.812 0.830 0.791 0.837 0.887 0.910 0.742\u00b10.047 ACSF1 0.640 0.730 0.760 0.730 0.900 0.910 0.886 0.902\u00b10.020 AllGestureWiimoteX 0.716 0.703 0.259 0.697 0.763 0.777 0.790 0.831\u00b10.042 AllGestureWiimoteY 0.729 0.699 0.423 0.741 0.726 0.793 0.773 0.826\u00b10.017 AllGestureWiimoteZ 0.643 0.646 0.447 0.689 0.723 0.770 0.766 0.848\u00b10.002 BME 0.900 0.973 0.760 0.933 0.993 0.993 1.000 1.000\u00b10.000 Chinatown 0.957 0.977 0.936 0.983 0.951 0.968 0.983 0.954\u00b10.016 Crop 0.665 0.738 0.710 0.742 0.722 0.756 0.751 0.703\u00b10.005 EOGHorizontalSignal 0.503 0.442 0.373 0.401 0.605 0.544 0.539 0.609\u00b10.018 EOGVerticalSignal 0.448 0.392 0.298 0.376 0.434 0.503 0.441 0.521\u00b10.013 EthanolLevel 0.276 0.424 0.260 0.486 0.382 0.484 0.583 0.704\u00b10.018 FreezerRegularTrain 0.899 0.991 0.922 0.989 0.956 0.986 0.998 0.993\u00b10.003 FreezerSmallTrain 0.753 0.982 0.920 0.979 0.933 0.894 0.950 0.972\u00b10.012 Fungi 0.839 0.527 0.366 0.753 1.000 0.962 1.000 0.954\u00b10.003 GestureMidAirD1 0.569 0.431 0.208 0.369 0.608 0.631 0.617 0.541\u00b10.026 GestureMidAirD2 0.608 0.362 0.138 0.254 0.546 0.515 0.561 0.585\u00b10.029 GestureMidAirD3 0.323 0.292 0.154 0.177 0.285 0.346 0.315 0.405\u00b10.008 GesturePebbleZ1 0.791 0.378 0.500 0.395 0.919 0.930 0.906 0.871\u00b10.002 GesturePebbleZ2 0.671 0.316 0.380 0.430 0.899 0.873 0.830 0.874\u00b10.030 GunPointAgeSpan 0.918 0.984 0.991 0.994 0.994 0.994 0.997 1.000\u00b10.000 GunPointMaleVersusFemale 0.997 0.994 1.000 0.997 0.997 1.000 0.998 1.000\u00b10.000 GunPointOldVersusYoung 0.838 1.000 1.000 1.000 1.000 1.000 0.991 1.000\u00b10.000 HouseTwenty 0.924 0.782 0.815 0.790 0.933 0.941 0.964 0.953\u00b10.018 InsectEPGRegularTrain 0.872 1.000 1.000 1.000 1.000 1.000 1.000 1.000\u00b10.000 InsectEPGSmallTrain 0.735 1.000 1.000 1.000 1.000 1.000 0.979 1.000\u00b10.000 MelbournePedestrian 0.791 0.942 0.741 0.949 0.944 0.959 0.904 0.926\u00b10.016 MixedShapesRegularTrain 0.842 0.911 0.879 0.855 0.905 0.922 0.921 0.965\u00b10.021 MixedShapesSmallTrain 0.780 0.813 0.828 0.735 0.860 0.881 0.918 0.927\u00b10.011 PickupGestureWiimoteZ 0.660 0.620 0.240 0.600 0.740 0.820 0.830 0.871\u00b10.026 PigAirwayPressure 0.106 0.413 0.120 0.380 0.510 0.683 0.095 0.594\u00b10.005 PigArtPressure 0.245 0.808 0.774 0.524 0.928 0.966 0.954 0.872\u00b10.022 PigCVP 0.154 0.649 0.596 0.615 0.788 0.870 0.934 0.831\u00b10.006 PLAID 0.840 0.495 0.419 0.445 0.555 0.561 0.903 0.904\u00b10.008 PowerCons 0.878 0.933 0.911 0.961 0.900 0.972 0.940 0.901\u00b10.000 Rock 0.600 0.580 0.680 0.600 0.580 0.700 0.900 0.700\u00b10.016 SemgHandGenderCh2 0.802 0.882 0.725 0.837 0.890 0.963 0.927 0.972\u00b10.004 SemgHandMovementCh2 0.584 0.593 0.420 0.613 0.789 0.893 0.645 0.924\u00b10.005 SemgHandSubjectCh2 0.727 0.771 0.484 0.753 0.853 0.951 0.881 0.981\u00b10.002 ShakeGestureWiimoteZ 0.860 0.820 0.760 0.860 0.920 0.940 0.898 0.834\u00b10.001 SmoothSubspace 0.827 0.913 0.827 0.953 0.960 0.993 0.979 1.000\u00b10.000 UMD 0.993 0.993 0.910 0.986 0.993 1.000 0.992 1.000\u00b10.000 DodgerLoopDay 0.500 0.000 0.200 0.000 0.000 0.562 0.573 0.628\u00b10.003 DodgerLoopGame 0.877 0.000 0.696 0.000 0.000 0.841 0.873 0.906\u00b10.006 DodgerLoopWeekend 0.949 0.000 0.732 0.000 0.000 0.964 0.975 0.971\u00b10.022 Average Accuarcy 0.728 0.743 0.639 0.740 0.787 0.836 0.842 0.851 Average Rank 6.102 5.285 7.055 5.191 4.523 2.855 2.590 2.398\nTable G.4: ShapeConv Compared with State-Of-The-Art Time-Series Classification Methods: Evaluating Testing Accuracy for Supervised Multivariate Time-Series Classification Tasks Across 30 UEA Datasets. Mean accuracy \u00b1 std over 3 independent experiments with different random seeds is reported.\nDTW TNC TST TS-TCC T-Loss TS2Vec ShapeConv\nArticularyWordRecognition 0.987 0.973 0.977 0.953 0.943 0.987 0.994\u00b10.001 AtrialFibrillation 0.200 0.133 0.067 0.267 0.133 0.200 0.521\u00b10.015 BasicMotions 0.975 0.975 0.975 1.000 1.000 0.975 0.997\u00b10.016 CharacterTrajectories 0.989 0.967 0.975 0.985 0.993 0.995 0.981\u00b10.018 Cricket 1.000 0.958 1.000 0.917 0.972 0.972 0.998\u00b10.008 DuckDuckGeese 0.600 0.460 0.620 0.380 0.650 0.680 0.648\u00b10.006 EigenWorms 0.618 0.840 0.748 0.779 0.840 0.847 0.802\u00b10.008 Epilepsy 0.964 0.957 0.949 0.957 0.971 0.964 0.972\u00b10.009 ERing 0.133 0.852 0.874 0.904 0.133 0.874 0.774\u00b10.003 EthanolConcentration 0.323 0.297 0.262 0.285 0.205 0.308 0.253\u00b10.001 FaceDetection 0.529 0.536 0.534 0.544 0.513 0.501 0.635\u00b10.025 FingerMovements 0.530 0.470 0.560 0.460 0.580 0.480 0.587\u00b10.029 HandMovementDirection 0.231 0.324 0.243 0.243 0.351 0.338 0.413\u00b10.020 Handwriting 0.286 0.249 0.225 0.498 0.451 0.515 0.527\u00b10.006 Heartbeat 0.717 0.746 0.746 0.751 0.741 0.683 0.784\u00b10.011 JapaneseVowels 0.949 0.978 0.978 0.930 0.989 0.984 0.993\u00b10.022 Libras 0.870 0.817 0.656 0.822 0.883 0.867 0.887\u00b10.002 LSST 0.551 0.595 0.408 0.474 0.509 0.537 0.608\u00b10.023 MotorImagery 0.500 0.500 0.500 0.610 0.580 0.510 0.674\u00b10.002 NATOPS 0.883 0.911 0.850 0.822 0.917 0.928 0.937\u00b10.004 PEMS-SF 0.711 0.699 0.740 0.734 0.676 0.682 0.801\u00b10.005 PenDigits 0.977 0.979 0.560 0.974 0.981 0.989 0.968\u00b10.018 PhonemeSpectra 0.151 0.207 0.085 0.252 0.222 0.233 0.192\u00b10.002 RacketSports 0.803 0.776 0.809 0.816 0.855 0.855 0.863\u00b10.012 SelfRegulationSCP1 0.775 0.799 0.754 0.823 0.843 0.812 0.858\u00b10.003 SelfRegulationSCP2 0.539 0.550 0.550 0.533 0.539 0.578 0.624\u00b10.054 SpokenArabicDigits 0.963 0.934 0.923 0.970 0.905 0.988 0.979\u00b10.020 StandWalkJump 0.200 0.400 0.267 0.333 0.333 0.467 0.587\u00b10.013 UWaveGestureLibrary 0.903 0.759 0.575 0.753 0.875 0.906 0.936\u00b10.002 InsectWingbeat - 0.469 0.105 0.264 0.156 0.466 0.509\u00b10.026 Average Accuracy 0.650 0.670 0.617 0.668 0.658 0.704 0.743 Average Rank 4.717 4.583 5.283 4.333 3.900 3.117 2.067"
        },
        {
            "heading": "G.4 RESULTS OF UNSUPERVISED LEARNING TASKS",
            "text": "In this section, we report the full results of unsupervised time-series clustering tasks across 36 UCR Datasets in Table G.5. We also report the result of unsupervised multivariate time-series clustering tasks compared with Zhang & Sun (2022) across 12 UEA datasets in Table G.6.\nTable G.5: Unsupervised time-series clustering results (NMI on test data) across 36 UCR datasets. Mean accuracy \u00b1 std over 3 independent experiments with different random seeds is reported.\nDataset KMeans k-Shape U-ShapeL DTC USSL DTCR STCN AutoS. ShapeC. ShapeC. ShapeC. ShapeC.w/o Init w/o DBI w/ Human\nArrow 0.4816 0.5240 0.3522 0.5000 0.6322 0.5513 0.5240 0.5624 0.5134 0.6123 0.6064\u00b10.010 0.6445\u00b10.013 Beef 0.2925 0.3338 0.3413 0.2751 0.3338 0.5473 0.5432 0.3799 0.3077 0.3854 0.4039\u00b10.003 0.6188\u00b10.017 BeetleFly 0.0073 0.3456 0.5105 0.3456 0.5310 0.7610 1.0000 0.5310 0.4897 1.0000 1.0000\u00b10.000 1.0000\u00b10.000 BirdChicken 0.0371 0.3456 0.2783 0.0073 0.6190 0.5310 1.0000 0.6352 0.5824 1.0000 1.0000\u00b10.000 1.0000\u00b10.000 Car 0.2540 0.3771 0.3655 0.1892 0.4650 0.5021 0.5701 0.4970 0.3672 0.4690 0.4770\u00b10.006 0.5013\u00b10.013 Chlorine. 0.0129 0.0000 0.0135 0.0013 0.0133 0.0195 0.0760 0.0133 0.0024 0.0368 0.0527\u00b10.007 0.0641\u00b10.014 Coffee 0.5246 1.0000 1.0000 0.5523 1.0000 0.6277 1.0000 1.0000 1.0000 1.0000 1.0000\u00b10.000 1.0000\u00b10.000 Diatom. 0.9300 1.0000 0.4849 0.6863 1.0000 0.9418 1.0000 1.0000 1.0000 1.0000 1.0000\u00b10.000 1.0000\u00b10.000 Dist.ageG 0.1880 0.2911 0.2577 0.3406 0.3846 0.4553 0.5037 0.4400 0.4237 0.4464 0.4786\u00b10.004 0.5291\u00b10.007 Dist.correct 0.0278 0.0527 0.0063 0.0115 0.1026 0.1180 0.2327 0.1333 0.0699 0.0885 0.1074\u00b10.009 0.1836\u00b10.012 ECG200 0.1403 0.3682 0.1323 0.0918 0.3776 0.3691 0.4316 0.3928 0.3002 0.5413 0.5552\u00b10.009 0.6240\u00b10.014 ECGFiveDays 0.0002 0.0002 0.1498 0.0022 0.6502 0.8056 0.3582 0.7835 0.6355 0.8150 0.8246\u00b10.010 0.7669\u00b10.027 GunPoint 0.0126 0.3653 0.3653 0.0194 0.4878 0.4200 0.5537 0.4027 0.3803 0.4248 0.4476\u00b10.003 0.5652\u00b10.025 Ham 0.0093 0.0517 0.0619 0.1016 0.3411 0.0989 0.2382 0.3211 0.1764 0.3859 0.3911\u00b10.001 0.4467\u00b10.005 Herring 0.0013 0.0027 0.1324 0.0143 0.1718 0.2248 0.2002 0.2019 0.1423 0.2293 0.2630\u00b10.012 0.2317\u00b10.014 Lighting2 0.0038 0.2670 0.0144 0.1435 0.3727 0.2289 0.3479 0.3530 0.3040 0.3756 0.4282\u00b10.008 0.4723\u00b10.002 Meat 0.2510 0.2254 0.2716 0.2250 0.9085 0.9653 0.9393 0.9437 0.2901 0.9423 0.9481\u00b10.007 1.0000\u00b10.000 Mid.ageG 0.0219 0.0722 0.1491 0.1390 0.2780 0.4661 0.5109 0.3940 0.1830 0.4214 0.4576\u00b10.003 0.6654\u00b10.013 Mid.correct 0.0024 0.0349 0.0253 0.0079 0.2503 0.1150 0.0921 0.2873 0.1942 0.2109 0.2096\u00b10.026 0.3499\u00b10.002 Mid.TW 0.4134 0.5229 0.4065 0.1156 0.9202 0.5503 0.6169 0.9450 0.7836 0.9161 0.9241\u00b10.004 0.9310\u00b10.006 MoteStrain 0.0551 0.2215 0.0082 0.0094 0.5310 0.4094 0.4063 0.4257 0.2976 0.5406 0.5738\u00b10.001 0.5982\u00b10.014 OSULeaf 0.0208 0.0126 0.0203 0.2201 0.3353 0.2599 0.3544 0.4432 0.2952 0.5131 0.5159\u00b10.017 0.4846\u00b10.018 Plane 0.8598 0.9642 1.0000 0.8678 1.0000 0.9296 0.9615 0.9982 1.0000 1.0000 1.0000\u00b10.000 1.0000\u00b10.000 Prox.ageG 0.0635 0.0110 0.0332 0.4153 0.6813 0.5581 0.6317 0.6930 0.5164 0.6057 0.6453\u00b10.013 0.7292\u00b10.013 Prox.TW 0.0082 0.1577 0.0107 0.6199 1.0000 0.6539 0.7330 0.8947 0.5948 0.8368 0.8284\u00b10.006 0.8117\u00b10.005 Sony. 0.6112 0.7107 0.5803 0.2559 0.5597 0.6634 0.6112 0.6096 0.5423 0.6256 0.6215\u00b10.047 0.6107\u00b10.005 Sony.II 0.5444 0.0110 0.5903 0.4257 0.6858 0.6121 0.5647 0.7020 0.4985 0.6765 0.6767\u00b10.015 0.7343\u00b10.034 SwedishLeaf 0.0168 0.1041 0.3456 0.6187 0.9186 0.6663 0.6106 0.9340 0.5834 0.8592 0.8418\u00b10.006 0.8412\u00b10.010 Symbols 0.7780 0.6366 0.8691 0.7995 0.8821 0.8989 0.8940 0.9147 0.7778 0.9313 0.9250\u00b10.030 0.9118\u00b10.002 ToeSeg.1 0.0022 0.3073 0.3073 0.0188 0.3351 0.3115 0.3671 0.4610 0.2830 0.4700 0.4863\u00b10.001 0.5851\u00b10.007 ToeSeg.2 0.0863 0.0863 0.1519 0.0096 0.4308 0.3249 0.5498 0.4664 0.1293 0.4959 0.5178\u00b10.002 0.6636\u00b10.007 TwoPatterns 0.4696 0.3949 0.2979 0.0119 0.4911 0.4713 0.4110 0.5150 0.3083 0.5030 0.5177\u00b10.004 0.6154\u00b10.019 TwoLeadECG 0.0000 0.0000 0.0529 0.0036 0.5471 0.4614 0.6911 0.5654 0.4220 0.6006 0.6289\u00b10.033 0.7045\u00b10.001 Wafer 0.0010 0.0010 0.0010 0.0008 0.0492 0.0228 0.2089 0.0520 -0.0063 0.0741 0.0802\u00b10.008 0.0477\u00b10.021 Wine 0.0031 0.0119 0.0171 0.0000 0.7511 0.2580 0.5927 0.6045 0.2840 0.6090 0.6328\u00b10.016 0.6710\u00b10.003 WordsS. 0.5435 0.4154 0.3933 0.3498 0.4984 0.5448 0.3947 0.5112 0.3861 0.5884 0.5952\u00b10.003 0.6654\u00b10.001 Avg. Acc. 0.2132 0.2841 0.2777 0.2332 0.5427 0.4818 0.5478 0.5558 0.4183 0.5897 0.6017 0.6464 Avg. Rank 9.5556 8.4028 8.4306 9.6250 4.3611 5.0833 4.1250 3.7083 7.2639 3.0694 2.3750 NA\nTable G.6: Unsupervised time-series clustering results (NMI on test data) across 12 UEA datasets. Mean accuracy \u00b1 std over 3 independent experiments with different random seeds is reported.\nDataset MC2PCA SWIMDFC TCK m-kAVG+ED m-kDBA m-kShape m-kSC DeTSEC NESE MUSLA ShapeConv\nArticularyWordR. 0.934 0.523 0.873 0.834 0.741 0.344 0.843 0.792 0.849 0.838 0.867\u00b10.010 AtrialFibrilation 0.514 0.532 0.191 0.515 0.317 0.116 0.387 0.293 0.346 0.538 0.579\u00b10.015 BasicMotions 0.674 0.510 0.776 0.543 0.639 0.341 0.554 0.800 0.525 1.000 1.000\u00b10.000 Epilepsy 0.173 0.190 0.533 0.409 0.471 0.163 0.381 0.345 0.760 0.601 0.681\u00b10.003 Ering 0.336 0.422 0.399 0.400 0.406 0.268 0.348 0.392 0.378 0.722 0.736\u00b10.004 HandMovementD. 0.067 0.151 0.103 0.168 0.265 0.079 0.151 0.112 0.030 0.398 0.362\u00b10.011 Libras 0.577 0.500 0.620 0.622 0.622 0.447 0.724 0.602 0.542 0.724 0.738\u00b10.004 NATOPS 0.698 0.472 0.679 0.643 0.643 0.339 0.600 0.043 0.314 0.855 0.878\u00b10.007 PEMS-SF 0.011 0.441 0.066 0.491 0.402 0.447 0.474 0.424 0.586 0.614 0.630\u00b10.004 PenDigits 0.713 0.652 0.693 0.738 0.605 0.634 0.738 0.563 0.645 0.826 0.784\u00b10.004 StandWalkJump 0.349 0.483 0.536 0.559 0.466 0.116 0.461 0.555 0.399 0.609 0.586\u00b10.028 UWaveGestureL. 0.570 0.482 0.710 0.713 0.582 0.419 0.758 0.557 0.559 0.728 0.742\u00b10.014 Average Acc. 0.4680 0.4466 0.5150 0.5530 0.5132 0.3095 0.5348 0.4566 0.4943 0.7044 0.7154 Average Rank 7.0833 7.3333 5.8333 4.8333 6.4167 10.0833 5.5833 7.6667 7.2500 2.2917 1.6250"
        }
    ],
    "year": 2024
}