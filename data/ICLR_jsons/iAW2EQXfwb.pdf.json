{
    "abstractText": "Deep reinforcement learning has recently been successfully applied to online procedural content generation in which a policy determines promising game-level segments. However, existing methods can hardly discover diverse level patterns, while the lack of diversity makes the gameplay boring. This paper proposes an ensemble reinforcement learning approach that uses multiple negatively correlated sub-policies to generate different alternative level segments, and stochastically selects one of them following a dynamic selector policy. A novel policy regularisation technique is integrated into the approach to diversify the generated alternatives. In addition, we develop theorems to provide general methodologies for optimising policy regularisation in a Markov decision process. The proposed approach is compared with several state-of-the-art policy ensemble methods and classic methods on a well-known level generation benchmark, with two different reward functions expressing game-design goals from different perspectives. Results show that our approach boosts level diversity notably with competitive performance in terms of the reward. Furthermore, by varying the regularisation coefficient values, the trained generators form a well-spread Pareto front, allowing explicit trade-offs between diversity and rewards of generated levels.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ziqi Wang"
        },
        {
            "affiliations": [],
            "name": "Chengpeng Hu"
        },
        {
            "affiliations": [],
            "name": "Jialin Liu"
        },
        {
            "affiliations": [],
            "name": "Xin Yao"
        }
    ],
    "id": "SP:dfa28a68a9b6ef221d0704f389eba212c9bc5ce0",
    "references": [
        {
            "authors": [
                "Alberto Alvarez",
                "Steve Dahlskog",
                "Jose Font",
                "Julian Togelius"
            ],
            "title": "Empowering quality diversity in Dungeon design with interactive constrained MAP-Elites",
            "venue": "IEEE Conference on Games,",
            "year": 2019
        },
        {
            "authors": [
                "Alba Amato"
            ],
            "title": "Procedural content generation in the game industry",
            "venue": "In Game Dynamics: Best Practices in Procedural and Dynamic Game Content Generation,",
            "year": 2017
        },
        {
            "authors": [
                "Michael Beukman",
                "Christopher W Cleghorn",
                "Steven James"
            ],
            "title": "Procedural content generation using neuroevolution and novelty search for diverse video game levels",
            "venue": "In Genetic and Evolutionary Computation Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Rujikorn Charakorn",
                "Poramate Manoonpong",
                "Nat Dilokthanakul"
            ],
            "title": "Generating diverse cooperative agents by learning incompatible policies",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Richard Cheng",
                "Abhinav Verma",
                "Gabor Orosz",
                "Swarat Chaudhuri",
                "Yisong Yue",
                "Joel Burdick"
            ],
            "title": "Control regularization for reduced variance reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Brandon Cui",
                "Andrei Lupu",
                "Samuel Sokota",
                "Hengyuan Hu",
                "David J Wu",
                "Jakob Nicolaus Foerster"
            ],
            "title": "Adversarial diversity in Hanabi",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Rafael Guerra de Pontes",
                "Herman Martins Gomes",
                "Igor Santa Ritta Seabra"
            ],
            "title": "Particle swarm optimization for procedural content generation in an endless platform",
            "venue": "game. Entertainment Computing,",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Degris",
                "Martha White",
                "Richard Sutton"
            ],
            "title": "Off-policy actor-critic",
            "venue": "In International Conference on Machine Learning,",
            "year": 2012
        },
        {
            "authors": [
                "George C Derringer"
            ],
            "title": "A balancing act-optimizing a products properties",
            "venue": "Quality Progress,",
            "year": 1994
        },
        {
            "authors": [
                "Sam Earle",
                "Maria Edwards",
                "Ahmed Khalifa",
                "Philip Bontrager",
                "Julian Togelius"
            ],
            "title": "Learning controllable content generators",
            "venue": "IEEE Conference on Games,",
            "year": 2021
        },
        {
            "authors": [
                "Jesse Farebrother",
                "Marlos C Machado",
                "Michael Bowling"
            ],
            "title": "Generalization and regularization in DQN",
            "venue": "arXiv preprint arXiv:1810.00123,",
            "year": 2018
        },
        {
            "authors": [
                "Matthew C Fontaine",
                "Ruilin Liu",
                "Ahmed Khalifa",
                "Jignesh Modi",
                "Julian Togelius",
                "Amy K Hoover",
                "Stefanos Nikolaidis"
            ],
            "title": "Illuminating Mario scenes in the latent space of a generative adversarial network",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Jordi Grau-Moya",
                "Felix Leibfried",
                "Peter Vrancx"
            ],
            "title": "Soft Q-learning with mutual-information regularization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Daniele Gravina",
                "Ahmed Khalifa",
                "Antonios Liapis",
                "Julian Togelius",
                "Georgios N Yannakakis"
            ],
            "title": "Procedural content generation through quality diversity",
            "venue": "IEEE Conference on Games,",
            "year": 2019
        },
        {
            "authors": [
                "Stefan Greuter",
                "Jeremy Parker",
                "Nigel Stewart",
                "Geoff Leach"
            ],
            "title": "Real-time procedural generation of \u2018pseudo infinite",
            "venue": "cities. In International Conference on Computer Graphics and Interactive Techniques in Australasia and South East Asia,",
            "year": 2003
        },
        {
            "authors": [
                "Shixiang Gu",
                "Ethan Holly",
                "Timothy Lillicrap",
                "Sergey Levine"
            ],
            "title": "Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates",
            "venue": "In International Conference on Robotics and Automation,",
            "year": 2017
        },
        {
            "authors": [
                "Matthew Guzdial",
                "Nicholas Liao",
                "Jonathan Chen",
                "Shao-Yu Chen",
                "Shukan Shah",
                "Vishwa Shah",
                "Joshua Reno",
                "Gillian Smith",
                "Mark O Riedl"
            ],
            "title": "Friend, collaborator, student, manager: How design of an AI-driven game level editor affects creators",
            "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Matthew Guzdial",
                "Sam Snodgrass",
                "Adam J Summerville"
            ],
            "title": "Procedural Content Generation via Machine Learning: An Overview",
            "year": 2022
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Kristian Hartikainen",
                "George Tucker",
                "Sehoon Ha",
                "Jie Tan",
                "Vikash Kumar",
                "Henry Zhu",
                "Abhishek Gupta",
                "Pieter Abbeel"
            ],
            "title": "Soft actor-critic algorithms and applications",
            "venue": "arXiv preprint arXiv:1812.05905,",
            "year": 2018
        },
        {
            "authors": [
                "Conor F Hayes",
                "Roxana R\u0103dulescu",
                "Eugenio Bargiacchi",
                "Johan K\u00e4llstr\u00f6m",
                "Matthew Macfarlane",
                "Mathieu Reymond",
                "Timothy Verstraeten",
                "Luisa M Zintgraf",
                "Richard Dazeley",
                "Fredrik Heintz"
            ],
            "title": "A practical guide to multi-objective reinforcement learning and planning",
            "venue": "Autonomous Agents and Multi-Agent Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Chengpeng Hu",
                "Yunlong Zhao",
                "Ziqi Wang",
                "Haocheng Du",
                "Jialin Liu"
            ],
            "title": "Game-based platforms for artificial intelligence research",
            "venue": "arXiv preprint arXiv:2304.13269,",
            "year": 2023
        },
        {
            "authors": [
                "Tobias Huber",
                "Silvan Mertes",
                "Stanislava Rangelova",
                "Simon Flutura",
                "Elisabeth Andr\u00e9"
            ],
            "title": "Dynamic difficulty adjustment in virtual reality exergames through experience-driven procedural content generation",
            "venue": "In Symposium Series on Computational Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Martin Jennings-Teats",
                "Gillian Smith",
                "Noah Wardrip-Fruin"
            ],
            "title": "Polymorph: A model for dynamic level generation",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment,",
            "year": 2010
        },
        {
            "authors": [
                "Zehua Jiang",
                "Sam Earle",
                "Michael Green",
                "Julian Togelius"
            ],
            "title": "Learning controllable 3D level generators",
            "venue": "In Proceedings of the 17th International Conference on the Foundations of Digital Games,",
            "year": 2022
        },
        {
            "authors": [
                "Sergey Karakovskiy",
                "Julian Togelius"
            ],
            "title": "The Mario AI benchmark and competitions",
            "venue": "IEEE Transactions on Computational Intelligence and AI in Games,",
            "year": 2012
        },
        {
            "authors": [
                "Ahmed Khalifa",
                "Philip Bontrager",
                "Sam Earle",
                "Julian Togelius"
            ],
            "title": "PCGRL: Procedural content generation via reinforcement learning",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "George Konidaris",
                "Andrew Barto"
            ],
            "title": "Skill discovery in continuous reinforcement learning domains using skill chaining",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2009
        },
        {
            "authors": [
                "Raph Koster"
            ],
            "title": "Theory of fun for game design",
            "venue": "O\u2019Reilly Media, Inc.,",
            "year": 2013
        },
        {
            "authors": [
                "Kimin Lee",
                "Michael Laskin",
                "Aravind Srinivas",
                "Pieter Abbeel"
            ],
            "title": "SUNRISE: A simple unified framework for ensemble learning in deep reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Antonios Liapis",
                "Georgios N Yannakakis",
                "Julian Togelius"
            ],
            "title": "Enhancements to constrained novelty search: Two-population novelty search for generating game content",
            "venue": "In Conference on Genetic and Evolutionary Computation,",
            "year": 2013
        },
        {
            "authors": [
                "Jialin Liu",
                "Sam Snodgrass",
                "Ahmed Khalifa",
                "Sebastian Risi",
                "Georgios N Yannakakis",
                "Julian Togelius"
            ],
            "title": "Deep learning for procedural content generation",
            "venue": "Neural Computing and Applications,",
            "year": 2021
        },
        {
            "authors": [
                "Shengcai Liu",
                "Yu Zhang",
                "Ke Tang",
                "Xin Yao"
            ],
            "title": "How good is neural combinatorial optimization? a systematic evaluation on the traveling salesman problem",
            "venue": "IEEE Computational Intelligence Magazine,",
            "year": 2023
        },
        {
            "authors": [
                "Yong Liu",
                "Xin Yao"
            ],
            "title": "Ensemble learning via negative correlation",
            "venue": "Neural Networks,",
            "year": 1999
        },
        {
            "authors": [
                "Andrei Lupu",
                "Brandon Cui",
                "Hengyuan Hu",
                "Jakob Foerster"
            ],
            "title": "Trajectory diversity for zero-shot coordination",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Athar Mahmoudi-Nejad",
                "Matthew Guzdial",
                "Pierre Boulanger"
            ],
            "title": "Arachnophobia exposure therapy using experience-driven procedural content generation via reinforcement learning (EDPCGRL)",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment,",
            "year": 2021
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Adria Puigdomenech Badia",
                "Mehdi Mirza",
                "Alex Graves",
                "Timothy Lillicrap",
                "Tim Harley",
                "David Silver",
                "Koray Kavukcuoglu"
            ],
            "title": "Asynchronous methods for deep reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Sang-Gyu Nam",
                "Chu-Hsuan Hsueh",
                "Kokolo Ikeda"
            ],
            "title": "Generation of game stages with quality and diversity by reinforcement learning in turn-based RPG",
            "venue": "IEEE Transactions on Games,",
            "year": 2021
        },
        {
            "authors": [
                "Ingram Olkin",
                "Friedrich Pukelsheim"
            ],
            "title": "The distance between two random vectors with given dispersion matrices",
            "venue": "Linear Algebra and its Applications,",
            "year": 1982
        },
        {
            "authors": [
                "Jack Parker-Holder",
                "Aldo Pacchiano",
                "Krzysztof M Choromanski",
                "Stephen J Roberts"
            ],
            "title": "Effective diversity in population based reinforcement learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Shubham Pateria",
                "Budhitama Subagdja",
                "Ah-hwee Tan",
                "Chai Quek"
            ],
            "title": "Hierarchical reinforcement learning: A comprehensive survey",
            "venue": "ACM Computing Surveys,",
            "year": 2021
        },
        {
            "authors": [
                "Mike Preuss",
                "Antonios Liapis",
                "Julian Togelius"
            ],
            "title": "Searching for good and diverse game levels",
            "venue": "In Conference on Computational Intelligence and Games,",
            "year": 2014
        },
        {
            "authors": [
                "Jie Ren",
                "Yewen Li",
                "Zihan Ding",
                "Wei Pan",
                "Hao Dong"
            ],
            "title": "Probabilistic mixture-of-experts for efficient deep reinforcement learning",
            "venue": "arXiv preprint arXiv:2104.09122,",
            "year": 2021
        },
        {
            "authors": [
                "Sebastian Risi",
                "Julian Togelius"
            ],
            "title": "Increasing generality in machine learning through procedural content generation",
            "venue": "Nature Machine Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Rohan Saphal",
                "Balaraman Ravindran",
                "Dheevatsa Mudigere",
                "Sasikant Avancha",
                "Bharat Kaul"
            ],
            "title": "SEERL: Sample efficient ensemble reinforcement learning",
            "venue": "In International Conference on Autonomous Agents and Multi Agent Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Noor Shaker",
                "Georgios N Yannakakis",
                "Julian Togelius",
                "Miguel Nicolau",
                "Michael O\u2019neill"
            ],
            "title": "Evolving personalized content for Super Mario Bros using grammatical evolution",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment,",
            "year": 2012
        },
        {
            "authors": [
                "Hassam Sheikh",
                "Mariano Phielipp",
                "Ladislau Boloni"
            ],
            "title": "Maximizing ensemble diversity in deep reinforcement learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Tianye Shu",
                "Jialin Liu",
                "Georgios N Yannakakis"
            ],
            "title": "Experience-driven PCG via reinforcement learning: A Super Mario Bros study",
            "venue": "IEEE Conference on Games,",
            "year": 2021
        },
        {
            "authors": [
                "David Silver",
                "Guy Lever",
                "Nicolas Heess",
                "Thomas Degris",
                "Daan Wierstra",
                "Martin Riedmiller"
            ],
            "title": "Deterministic policy gradient algorithms",
            "venue": "In International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "David Stammer",
                "Tobias G\u00fcnther",
                "Mike Preuss"
            ],
            "title": "Player-adaptive Spelunky level generation",
            "venue": "In Conference on Computational Intelligence and Games,",
            "year": 2015
        },
        {
            "authors": [
                "Evan Kusuma Susanto",
                "Handayani Tjandrasa"
            ],
            "title": "Applying hindsight experience replay to procedural level generation",
            "venue": "In Indonesia Conference on Computer and Information Technology,",
            "year": 2021
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Richard S Sutton",
                "David McAllester",
                "Satinder Singh",
                "Yishay Mansour"
            ],
            "title": "Policy gradient methods for reinforcement learning with function approximation",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 1999
        },
        {
            "authors": [
                "Bryon Tjanaka",
                "Matthew C Fontaine",
                "Julian Togelius",
                "Stefanos Nikolaidis"
            ],
            "title": "Approximating gradients for differentiable quality diversity in reinforcement learning",
            "venue": "In Genetic and Evolutionary Computation Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Julian Togelius",
                "Georgios N Yannakakis",
                "Kenneth O Stanley",
                "Cameron Browne"
            ],
            "title": "Search-based procedural content generation: A taxonomy and survey",
            "venue": "IEEE Transactions on Computational Intelligence and AI in Games,",
            "year": 2011
        },
        {
            "authors": [
                "Vanessa Volz",
                "Jacob Schrum",
                "Jialin Liu",
                "Simon M Lucas",
                "Adam Smith",
                "Sebastian Risi"
            ],
            "title": "Evolving Mario levels in the latent space of a deep convolutional generative adversarial network",
            "venue": "In Genetic and Evolutionary Computation Conference,",
            "year": 2018
        },
        {
            "authors": [
                "Ziqi Wang",
                "Jialin Liu"
            ],
            "title": "Online game level generation from music",
            "venue": "IEEE Conference on Games, pp. 119\u2013126",
            "year": 2022
        },
        {
            "authors": [
                "Ziqi Wang",
                "Jialin Liu",
                "Georgios N Yannakakis"
            ],
            "title": "The fun facets of Mario: Multifaceted experience-driven PCG via reinforcement learning",
            "venue": "In Proceedings of the 17th International Conference on the Foundations of Digital Games,",
            "year": 2022
        },
        {
            "authors": [
                "Ziqi Wang",
                "Tianye Shu",
                "Jialin Liu"
            ],
            "title": "State space closure: Revisiting endless online level generation via reinforcement learning",
            "venue": "IEEE Transactions on Games, Early Access,",
            "year": 2023
        },
        {
            "authors": [
                "Zhengyu Yang",
                "Kan Ren",
                "Xufang Luo",
                "Minghuan Liu",
                "Weiqing Liu",
                "Jiang Bian",
                "Weinan Zhang",
                "Dongsheng Li"
            ],
            "title": "Towards applicable reinforcement learning: Improving the generalization and sample efficiency with policy ensemble",
            "venue": "In International Joint Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Georgios N Yannakakis",
                "Julian Togelius"
            ],
            "title": "Experience-driven procedural content generation",
            "venue": "IEEE Transactions on Affective Computing,",
            "year": 2011
        },
        {
            "authors": [
                "Qingquan Zhang",
                "Jialin Liu",
                "Zeqi Zhang",
                "Junyi Wen",
                "Bifei Mao",
                "Xin Yao"
            ],
            "title": "Mitigating unfairness via evolutionary multi-objective ensemble learning",
            "venue": "IEEE Transactions on Evolutionary Computation,",
            "year": 2022
        },
        {
            "authors": [
                "Adam Kingma"
            ],
            "title": "Learning rate (all networks and \u03b1) 3.0\u00d7 10\u22124 Hidden layer activation (all networks) ReLU Number of hidden layers (all networks) 2 Size of hidden layer(all networks",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "Deep reinforcement learning has recently been successfully applied to online procedural content generation in which a policy determines promising game-level segments. However, existing methods can hardly discover diverse level patterns, while the lack of diversity makes the gameplay boring. This paper proposes an ensemble reinforcement learning approach that uses multiple negatively correlated sub-policies to generate different alternative level segments, and stochastically selects one of them following a dynamic selector policy. A novel policy regularisation technique is integrated into the approach to diversify the generated alternatives. In addition, we develop theorems to provide general methodologies for optimising policy regularisation in a Markov decision process. The proposed approach is compared with several state-of-the-art policy ensemble methods and classic methods on a well-known level generation benchmark, with two different reward functions expressing game-design goals from different perspectives. Results show that our approach boosts level diversity notably with competitive performance in terms of the reward. Furthermore, by varying the regularisation coefficient values, the trained generators form a well-spread Pareto front, allowing explicit trade-offs between diversity and rewards of generated levels."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Continuously generating new content in a game level in real-time during game-playing, namely online level generation (OLG), is an important demand from the game industry (Amato, 2017). Recent works show that reinforcement learning (RL) is capable of training generators that can offline generate levels to satisfy customised needs using carefully designed reward functions (Khalifa et al., 2020; Huber et al., 2021). Inspired by those works, Shu et al. (2021) propose the experience-driven procedural content generation (PCG) via RL (EDRL) framework, in which an RL policy observes previously generated level segments and determines the following segment in real-time during gameplaying. EDRL is shown to be efficient and effective in generating promising levels (Shu et al., 2021; Wang et al., 2022). Then, Wang et al. (2023) show that levels generated by EDRL can be quite similar, i.e., lacking diversity. Diversity is one of the essential characteristics for levels since similar levels make players bored soon (Koster, 2013; Gravina et al., 2019). Research interests in generating diverse levels have a long history of at least two decades (Greuter et al., 2003; Togelius et al., 2011) and has been rapidly growing over the past few years (Gravina et al., 2019; Liu et al., 2021; Guzdial et al., 2022). However, to the best of our knowledge, no work has tackled the issue of limited diversity of levels online generated by RL policies yet.\n\u2217Corresponding author.\nThere are mainly two limitations in existing deep RL algorithms for learning to online generate diverse game levels. Firstly, existing deep RL algorithms typically use a greedy or unimodal stochastic policy. Such policy has a limited capability of representing complex and diverse decision distribution (Ren et al., 2021), thus it is hard to enable diverse level generation. Secondly, the diversity being concerned in this work is about the variations among the generated levels. In the context of OLG via RL, it is induced by the probability distribution of trajectories from the Markov decision process (MDP). A reward function only evaluates single actions, but is not aware of the entire MDPtrajectory distribution. Therefore, one can hardly formulate a reward function to express diversity.\nTo address the two challenges, this paper proposes an ensemble RL approach which performs a stochastic branching generation process, namely negatively correlated ensemble RL (NCERL). NCERL uses multiple individual actors to generate different alternative level segments. A selector policy is employed to determine the final output segment from the alternatives. Figure 1 shows a diagram of the approach. To diversify the generated alternatives, NCERL incorporates a negative correlation regularisation to increase the distances between the decision distributions determined by each pair of actors. As the regularisation evaluates the decision distributions rather than the action instances, traditional RL methodologies do not directly work for it. To tackle this problem, we derive the regularised versions of the policy iteration (Sutton & Barto, 2018) and policy gradient (Sutton et al., 1999) to provide fundamental methodologies for optimising policy regularisation in an MDP. Those theorems can derive general loss functions to establish regularised off-policy and on-policy deep RL algorithms, respectively. Furthermore, the reward evaluation in OLG tasks usually relies on simulating\na game-playing agent on the generated levels, which is time-consuming. Our work also reduces the time cost of training OLG agents by designing an asynchronous off-policy training framework.\nMain contributions of our work are as follows: (i) we propose an ensemble RL approach with a novel negative correlation regularisation to promote the diversity of levels online generated by RL policies; (ii) regularised versions of policy iteration and policy gradient theorems are derived to illustrate how the policy regularisation can be optimised; (iii) comprehensive experiments show that by using different regularisation coefficient values, our approach produces a wide range of non-dominated policies against state-of-the-art policy ensemble RL methods, in terms of cumulative reward and diversity of generated levels. This makes it possible to make trade-offs based on specific preferences. Code and results are available at https://github.com/PneuC/NCERL-Diverse-PCG."
        },
        {
            "heading": "2 BACKGROUND AND RELATED WORK",
            "text": "Procedural Content Generation via RL PCG has been investigated for decades (Togelius et al., 2011; Yannakakis & Togelius, 2018; Risi & Togelius, 2020; Liu et al., 2021; Guzdial et al., 2022). RL has been applied to a variety of offline PCG tasks, including level generation (Susanto & Tjandrasa, 2021) and map generation (Khalifa et al., 2020; Earle et al., 2021; Jiang et al., 2022), and shown to be powerful on those tasks. However, those works focus on offline content generation, where the content is determined before game-playing. Besides, RL has also been applied to mixedinitiative or interactive PCG (Guzdial et al., 2019), which considers the interactions between human designers and content generators. The focus of this work is generating diverse levels in an online manner. The word \u201conline\u201d not only refers to real-time generation but also implies the consideration of player experience while generating new content (Yannakakis & Togelius, 2011). Therefore, our work is more aligned with player-centered approaches, rather than interactive PCG or mixedinitiative systems (Guzdial et al., 2019).\nRecently, research interest in online PCG via RL has been raised. Compared to traditional methods for online PCG like rule-based methods (Stammer et al., 2015; Jennings-Teats et al., 2010) and\nsearch-based methods (Shaker et al., 2012; de Pontes et al., 2022), the RL-based methods rely on little domain knowledge and are scalable. Shu et al. (2021) introduce the EDRL framework to online generate playable, fun and historically deviated levels. Wang et al. (2022) propose a model to estimate player experience and use EDRL to train generators that optimise the experience of playing the online generated levels. RL has also been applied to specific online PCG scenarios including dynamic difficulty adjustment (Huber et al., 2021), music-driven level generation (Wang & Liu, 2022) and adaptive virtual reality exposure therapy (Mahmoudi-Nejad et al., 2021). In research of OLG via RL, the limited diversity of generated levels has been reported by Wang & Liu (2022) and Wang et al. (2023) but not addressed yet. Our work implements EDRL as the online PCG framework and specifically focuses on the RL part of it to improve diversity.\nMethods for Diverse Game Content Generation In offline PCG, it is applicable to formulate the contribution of an individual to the diversity and generate a set of diverse game content gradually given the formulation. A representative method is novelty search (Preuss et al., 2014; Liapis et al., 2013). Beukman et al. (2022) integrate neuroevolution and novelty search to evolve multiple generators to online generate diverse levels. Another popular method is quality-diversity (QD) search (Gravina et al., 2019; Fontaine et al., 2021), which searches for the best quality content under each possible attribute or behaviour combination. QD search has been applied to offline PCG (Gravina et al., 2019; Fontaine et al., 2021; Guzdial et al., 2022), mixed-initiative PCG (Alvarez et al., 2019) and has also been integrated with RL to train diverse policies (Tjanaka et al., 2022). However, searching for game content is typically slower than generating content via machine learning models, thus existing novelty search and QD search approaches are not likely efficient enough to realise real-time generation. Moreover, the QD method is powerful for problems with some attribute or behaviour descriptors to express diversity as the coverage over the corresponding attribute or behaviour space, while our work does not use such descriptors. Besides, Nam et al. (2021) use RL to generate role-playing game stages and use a diversity-enhancing strategy based on some rules.\nPopulation-based RL Population-based RL is used for a variety of aspects. Saphal et al. (2021) select a diverse subset from a set of trained policies to make decisions. ACE (Zhang & Yao, 2019) combines tree search and policy ensemble for better sample efficiency and value prediction. A number of works consider the population diversity in policy ensemble to encourage the exploration. SUNRISE (Lee et al., 2021) integrates several enhancement techniques into ensemble RL. PMOE (Ren et al., 2021) leverages multimodal ensemble policy to improve the exploration ability of RL agents. Parker-Holder et al. (2020) define population diversity as the determinant of an embedding matrix and show that incorporating such a diversity formulation into the training objective generally improves the reward performance of RL algorithms. Yang et al. (2022) integrate a regularisation loss to enhance the decision diversity of ensemble policy. Diversity of policy population is also concerned in multi-agent RL. For example, Cui et al. (2023) propose adversarial diversity to produce meaningfully diverse policies, Lupu et al. (2021) propose trajectory diversity to enable more robust zero-shot coordination, while Charakorn et al. (2023) propose compatibility gap to train a population of diverse policies. While diversity in policy populations has been explored in the aforementioned works, our work uniquely extends this consideration to the context of PCG, making diversity a primary goal alongside reward. Our ensemble policy uses a selector and multiple individual actors, which is similar to hierarchical RL (Pateria et al., 2021) and skill discovery (Konidaris & Barto, 2009). However, our method features a novel diversity regularisation.\nRegularisation in RL According to (Sheikh et al., 2022), regularisation methods in RL are applied for better exploration (Grau-Moya et al., 2019; Haarnoja et al., 2018a), generalisation (Farebrother et al., 2018) and other aspects that promote overall rewards (Sheikh et al., 2022; Grau-Moya et al., 2019; Cheng et al., 2019; Galashov et al.). The works of (Sheikh et al., 2022; Yang et al., 2022) study a combination of ensemble and regularisation. Different from our approach, the former regularises network parameters rather than policy behaviour, while the latter focuses on discrete action space rather than the continuous action space addressed in this work. The work by Haarnoja et al. (2018a) integrates an entropy regularisation for the decision distribution determined by the policy. Compared with their work, this work extends their theoretical results from entropy regularisation to general regularisation. Moreover, we derive the policy gradient for regularisation, which is a basis of onpolicy deep RL algorithms but is not discussed in the work by Haarnoja et al. (2018a)."
        },
        {
            "heading": "3 NEGATIVELY CORRELATED ENSEMBLE RL",
            "text": "This section introduces the problem formulation, then describes our proposed ensemble approach, called negatively correlated ensemble RL (NCERL). NCERL features a multimodal ensemble policy and a negative correlation regularisation, to address the two aforementioned limitations.\nProblem Formulation According to (Sutton & Barto, 2018), a general MDP M consists of a state space S, an action space A and its dynamics defined by a probability (density) function p(s\u2032, r|s, a) over S \u00d7 R \u00d7 S \u00d7 A, where s \u2208 S and s\u2032 \u2208 S are the current and next state, respectively, r \u2208 R \u2282 R is a reward, and a \u2208 A is the action taken at s. In addition, an initial state distribution is considered, with p0(s) denoting its probability (density) at s for any s \u2208 S . A policy \u03c0 is the decision maker which observes a state s and takes an action a stochastically following a probability (density) of \u03c0(a|s), at each time step. The interaction between \u03c0 and M induces a trajectory \u27e8S0, A0, R0\u27e9, \u00b7 \u00b7 \u00b7 , \u27e8St, At, Rt\u27e9, \u00b7 \u00b7 \u00b7 . Our work uses a decoder trained via generative adversarial networks (Goodfellow et al., 2014; Volz et al., 2018) to map a low-dimensional continuous latent vector to a game level segment. The action is a latent vector. The state is represented by a fixed number of latent vectors of recently generated segments. These latent vectors are concatenated into a single vector. Conventionally, the initial state is a randomly sampled latent vector. If there are not enough segments generated to construct a complete state, zeros will be padded into the vacant entries. The reward function is defined to evaluate the newly generated segment."
        },
        {
            "heading": "3.1 MULTIMODAL ENSEMBLE POLICY",
            "text": "Similar to (Ren et al., 2021), we use a multimodal ensemble policy that makes decisions following Gaussian mixture models. The ensemble policy \u03c0 consists of m sub-policies, namely \u03c01, \u00b7 \u00b7 \u00b7 , \u03c0m, and a weight function \u03b2(\u00b7) : S 7\u2192 Rm. The decision distribution determined by \u03c0 at state s, namely \u03c0(\u00b7|s), can be viewed as an m-component Gaussian mixture model. Each component \u03c0i(\u00b7|s) is the decision distribution determined by \u03c0i at s , while its mixture weight \u03b2i(s) is given by the weight function with \u2211m i=1 \u03b2i(s) = 1 holds. Specifically in this work, the components are i.i.d. spherical Gaussian distributions, and their means and standard deviations are denoted by \u00b5i(s) and \u03c3i(s), i.e., \u03c0i(\u00b7|s) = N (\u00b5i(s),\u03c3i(s)I). The ensemble policy samples a sub-policy \u03c0i based on the weight vector \u03b2(s) first, and then samples an action with the sub-policy \u03c0i, i.e., the final output a \u223c \u03c0i(\u00b7|s). We use m + 1 independent muti-layer perceptrons (MLPs) to model the ensemble policy. Each of the m sub-policies is modelled by an individual actor using an MLP, while the weight function is modelled by a selector using an additional MLP. We regard the m + 1 MLPs as a union model with multiple output heads, though they do not share any common parameters. The union of their parameters is denoted by \u03b8. The ith individual actor outputs two vectors \u00b5\u03b8i (s) and \u03c3 \u03b8 i (s), and the selector outputs an m-dimensional vector \u03b2\u03b8(s) that represents the mixture weights."
        },
        {
            "heading": "3.2 NEGATIVE CORRELATION REGULARISATION FOR DIVERSITY",
            "text": "Inspired by (Liu & Yao, 1999), we propose a negative correlation regularisation to diversify the behaviours of sub-policies. The regularisation calculates the 2-Wasserstein distances (Olkin & Pukelsheim, 1982) \u03c9i,j(s) between each pair of Gaussian decision distributions \u03c0i(\u00b7|s) and \u03c0j(\u00b7|s). 2-Wasserstein distance is chosen because it is widely used and differentiable when both distributions are Gaussian. The formula of the distance measure is detailed in Section C.2. Let \u2308\u00b7\u2309c denote a down-clip function bounding its argument under a constant upper bound c, the formulation of the negative correlation regularisation \u03f1(\u00b7) is\n\u03f1(\u03c0(\u00b7|s)) = \u2211m\ni=1 \u2211m j=1 \u03b2i(s)\u03b2j(s)\u2308\u03c9i,j(s)\u2309\u03c9\u0304, (1)\nwhere \u03c9\u0304 is a clip size parameter. This work arbitrarily sets \u03c9\u0304 = 0.6 \u221a d where d is the dimensionality of the action space. We abbreviate \u03f1(\u03c0(\u00b7|s)) as \u03f1\u03c0(s) in the rest of this paper. Omitting the clipping function, \u03f1\u03c0(s) is the expected Wasserstein distance of two sub-decision distributions stochastically sampled according to \u03b2(s). Two ideas motivate the use of the clipping function: (i) if the distance is already large, continuously maximising the distance does not benefit a lot for diversity of generated levels but harms the rewards; (ii) a few (even two) far-away clusters can have large expected distance but such pattern has limited diversity. Taking the down-clip helps with avoiding this case. \u03f1\u03c0(s) is\nmaximised only if \u03c9i,j(s) \u2265 \u03c9\u0304 and \u03b2i(s) = \u03b2j(s) for all pairs of i and j. This regularisation term is integrated into the MDP, thus the objective to be maximised in NCERL is defined as\nJ\u03c0 = EM,\u03c0 [\u2211\u221e t=0 \u03b3t ( r(St, At) + \u03bb\u03f1 \u03c0(St) )] , (2)\nwhere \u03bb is a regularisation coefficient and \u03b3 is the discount rate. The regularisation follows a different form compared to the reward r(St, At), which is based on the decision distribution rather than a single action and is independent of the actual action taken by the policy. This raises the question of how to optimise the regularised objective J\u03c0? We adapt the traditional RL theorems, policy iteration (Sutton & Barto, 2018) and policy gradient (Sutton et al., 1999) to answer it."
        },
        {
            "heading": "4 POLICY REGULARISATION THEOREMS",
            "text": "To answer the question above, this section derives regularised policy iteration and policyregularisation gradient. All lemmas and theorems are proved in Appendix A.\nValue Functions Similar to the standard one (Sutton & Barto, 2018), the state value for policy regularisation is defined as V \u03c0\u03f1 (s) . = EM,\u03c0 [\u2211\u221e k=0 \u03b3 k\u03f1\u03c0(St+k) | St = s ] . The state-action value, however, is varied from the standard Q-value, defined as\nQ\u03c0\u03f1 (s, a) . = EM,\u03c0 [\u2211\u221e k=1 \u03b3k\u03f1\u03c0(St+k) \u2223\u2223\u2223St = s,At = a] . (3)\nThe counter k starts from 1 rather than 0, because \u03f1\u03c0 is independent on the actual action. We further define a regularised state value function V\u03c0\u03f1 (s) = V \u03c0(s) + \u03bbV \u03c0\u03f1 (s) and a regularised state-action value functionQ\u03c0\u03f1 (s, a) = Q\u03c0(s, a) + \u03bbQ\u03c0\u03f1 (s, a). An optimal policy \u03c0\u2217 is defined as \u2200s \u2208 S,\u2200\u03c0 \u2208 \u03a0, V\u03c0\u2217\u03f1 (s) \u2265 V\u03c0\u03f1 (s), where \u03a0 is the hypothesis policy space. A \u03c0\u2217 maximises J over \u03a0."
        },
        {
            "heading": "4.1 REGULARISED POLICY ITERATION",
            "text": "We now describe the regularised policy iteration for an arbitrary policy regularisation \u03f1\u03c0 . Off-policy regularised deep RL algorithms can be established by approximating this theoretical algorithm. Considering an RL algorithm in a tabular setting, we define a Bellman operator T \u03c0\u03f1 of Q\u03f1-function for all paired s, a \u2208 S \u00d7A to derive the Q\u03c1 of a policy as\nT \u03c0\u03f1 Q\u03f1(s, a)\u2190 Es\u2032\u223cp(\u00b7|s,a),a\u2032\u223c\u03c0(\u00b7|s\u2032) [\u03b3\u03f1\u03c0(s\u2032) + \u03b3Q\u03f1(s\u2032, a\u2032)] . (4) Having this definition, it is guaranteed that applying the operators over S \u00d7 A repeatedly will converge to the true Q\u03f1-function of any policy \u03c0, as formalised below.\nLemma 1 (Q\u03f1-Function Evaluation). By repeatedly applying Qk+1\u03f1 = T \u03c0\u03f1 Qk\u03f1 from an arbitrary Q\u03f1-function Q0\u03f1, the sequence Q 0 \u03f1, \u00b7 \u00b7 \u00b7 , Qk\u03f1, \u00b7 \u00b7 \u00b7 converges to Q\u03c0\u03f1 as k \u2192\u221e.\nThe complete regularised policy evaluation applies standard policy evaluation andQ\u03f1-function evaluation either jointly (by summing Q\u03c0 with \u03bbQ\u03c0\u03f1 directly) or separately.\nTo derive an improved policy \u03c0new from an arbitrary policy \u03c0old assuming Q\u03c0old\u03f1 (s, a) is known for any s, a \u2208 S \u00d7A, we define a greedy regularised policy improvement operator for all s \u2208 S as\n\u03c0new(\u00b7|s)\u2190 argmax\u03c0(\u00b7|s)\u2208\u03a0(\u00b7|s) [ \u03bb\u03f1\u03c0(s) + Ea\u223c\u03c0(\u00b7|s) [ Q\u03c0old\u03f1 (s, a) ] ] . (5)\nWe say a policy \u03c0\u2032 is better than another \u03c0, denoted as \u03c0\u2032 \u227b \u03c0, if \u2200s \u2208 S,V\u03c0\u2032\u03f1 (s) \u2265 V\u03c0\u03f1 (s) and \u2203s \u2208 S,V\u03c0\u2032\u03f1 (s) > V\u03c0\u03f1 (s). Then a lemma of regularised policy improvement is formalised as follows. Lemma 2 (Regularised Policy Improvement). For any \u03c0old \u2208 \u03a0 and its \u03c0new derived via equation 5, it is guaranteed that \u03c0new \u227b \u03c0old if \u03c0old is not optimal.\nThe regularised policy iteration algorithm alters between the regularised policy evaluation and the regularised policy improvement repeatedly. It is proved that given a finite \u03a0, such an algorithm converges to an optimal policy over \u03a0. This convergence guarantee is formulated into Theorem 1. Theorem 1 (Regularised Policy Iteration). Given a finite hypothesis policy space \u03a0, regularised policy iteration converges to an optimal policy over \u03a0 from any \u03c00 \u2208 \u03a0."
        },
        {
            "heading": "4.2 POLICY-REGULARISATION GRADIENT",
            "text": "We derive the gradient for policy regularisation, namely the policy-regularisation gradient (PRG) to provide a theoretical foundation of regularised RL for on-policy algorithms.\nAn improper discounted state distribution d\u03c0 is defined as d\u03c0(s) .= \u2211\u221e t=0 \u03b3 t \u222b S p0(u)P[u\nt\u2192 s, \u03c0] du like in standard policy gradient, where P[s k\u2192 s\u2032, \u03c0] denotes the probability density of transiting to s\u2032 after k steps from s, by applying \u03c0. Consider a policy represented by a parametric model \u03c0\u03b8 where \u03b8 is its parameters. Using \u03f1\u03b8(s) as the abbreviation of \u03f1(\u03c0\u03b8(\u00b7|s)), PRG is formalised as follows. Theorem 2 (Policy-Regularisation Gradient, PRG). The gradient of a policy regularisation objective J\u03b8\u03f1 = EM,\u03c0\u03b8 [ \u2211\u221e t=0 \u03b3\nt\u03f1\u03b8(St)] w.r.t. \u03b8 follows \u2202J\u03b8\u03f1\n\u2202\u03b8 = \u222b S d \u03c0 (s) ( \u2202\u03f1\u03b8(s) \u2202\u03b8 + \u222b A Q \u03c0 \u03f1 (s, a) \u2202\u03c0\u03b8(a|s) \u2202\u03b8 da ) ds = E s\u223cd\u03c0,\na\u223c\u03c0(\u00b7|s)\n[ \u2202\u03f1\u03b8(s)\n\u2202\u03b8 + Q\n\u03c0 \u03f1 (s, a) \u2202 ln\u03c0\u03b8(a|s) \u2202\u03b8\n] . (6)"
        },
        {
            "heading": "5 IMPLEMENTING NCERL WITH ASYNCHRONOUS EVALUATION",
            "text": ""
        },
        {
            "heading": "5.1 IMPLEMENTING NCERL AGENT",
            "text": "We implement NCERL agents based on the soft-actor critic (SAC) (Haarnoja et al., 2018b). An NCERL agent carries two critics for soft Q-function, two critics for Q\u03f1-function and the ensemble policy. All critics use MLPs. With Q\u03f1(s, a;\u03d51) and Q\u03f1(s, a;\u03d52) denoting Q\u03f1-value predictions at state-action pair s, a of two Q\u03f1-critics, where \u03d51 and \u03d52 denote their parameters, each critic j \u2208 {1, 2} is trained to minimise the Bellman residual of the negative correlation regularisation\nL\u03d5 = Es,a,s\u2032\u223cD [( Q\u03f1(s, a;\u03d5j)\u2212 \u03b3V\u03f1(s\u2032; \u03d5\u0304) )2] , (7)\nwhere D is the replay memory. V\u03f1(s\u2032; \u03d5\u0304) is the prediction of V\u03f1-value with target parameters \u03d5\u0304: V\u03f1(s \u2032; \u03d5\u0304) = \u03f1\u03b8(s\u2032) + \u2211m\ni=1 \u03b2\u03b8i (s) min j\u2208{1,2} Q\u03f1(s \u2032, a\u0303\u2032i; \u03d5\u0304j), (8)\nwhere \u03f1\u03b8(s\u2032) is the negative correlation regularisation (cf. equation 1) of the ensemble policy model \u03c0\u03b8 at s\u2032, a\u0303\u2032i is an action sampled from \u03c0 \u03b8 i (\u00b7|s\u2032). The target parameters are updated with the same smoothing technique as in SAC. The Q-critics are trained with SAC, but when computing the soft Q-value target, we take expectation over all individual actors as in equation 8.\nThe ensemble policy model is updated by approximating the regularised policy improvement operator defined in equation 5 with critics and gradient ascent. The loss function is\nL\u03b8 = \u2212Es\u223cD [ \u03bb\u03f1\u03b8(s) + \u2211m i=1 \u03b2\u03b8i (s)E\u03f5i\u223cN [ Q\u03f1(s, f\u03b8(s, \u03f5i)) ]] , (9)\nwhere the reparametrisation trick is used with f\u03b8(s, \u03f5i) = \u00b5\u03b8i (s)+\u03c3 \u03b8 i (s)\u2299 \u03f5i where \u03f5i \u223c N (0, I), and the regularised state-action value is predicted by Q\u03f1(s, a) \u2190 mink\u2208{1,2}Q(s, a;\u03c8k) + \u03bbminj\u2208{1,2}Q\u03f1(s, a;\u03d5j). The gradient of \u2212L\u03b8 is also an approximated estimation of PRG (cf. equation 6 and Appendix B). The agent learns through performing gradient descent using Adam optimiser (Kingma & Ba, 2015) periodically to L\u03d5 and L\u03b8, respectively. For exploration, the agent directly samples an action from the ensemble policy model."
        },
        {
            "heading": "5.2 ASYNCHRONOUS OFF-POLICY TRAINING FRAMEWORK",
            "text": "Reward evaluation in OLG typically requires simulations of gameplay on complete levels to test their playability or simulate player behaviours (Wang et al., 2022). Such a simulation is usually realised by running a game-playing agent to play the level, which is time-consuming. The parallel off-policy training framework devised for OLG in previous work is synchronous (Wang et al., 2022), which causes unnecessary hang-on of CPU processes and GPU. On the other hand, existing asynchronous RL frameworks focus on on-policy algorithms (Mnih et al., 2016) or distributed learners (Gu et al., 2017), which do not apply to this work. Therefore, we design an asynchronous training framework. It separates model training and reward evaluation into two parallel processes, while the reward evaluation uses an asynchronous process pool to compute rewards of complete levels through multi-processing. This framework is also potentially beneficial to other tasks such as neural combinatorial optimisation where the reward evaluation operates on complete trajectories (Liu et al., 2023). Appendix C.1 provides the pseudo-code."
        },
        {
            "heading": "6 EXPERIMENTAL STUDIES",
            "text": "NCERL is evaluated on the well-known Mario level generation benchmark (Karakovskiy & Togelius, 2012), with two different tasks. This section introduces the experiment settings and discusses experimental results. Appendix D.2 and E.1 provide more details and analysis."
        },
        {
            "heading": "6.1 EXPERIMENT SETTING",
            "text": "NCERL instances1 with different numbers of individual actors and regularisation coefficient values are compared to several state-of-the-art policy ensemble algorithms and classic algorithms.\nOnline Level Generation Tasks Algorithm instances are tested on two OLG tasks of a wellknown benchmark (Hu et al., 2023) from recent literature, namely MarioPuzzle (Shu et al., 2021) and MultiFacet (Wang et al., 2022). They are mainly different in terms of the reward functions. MarioPuzzle defines two reward terms, fun and historical deviation. Fun restricts the divergence between new segments and old ones while historical deviation encourages the RL policy to generate novel segments in relation to previously generated ones. MultiFacet defines two reward terms to guide the RL policy to generate segments that introduce new tile patterns and play traces. Both of them use a playability reward, penalising the RL policy for generating unpassable segments. The full reward function in each task is a weighted sum of these reward terms. The two tasks are selected as they are the state-of-the-art for online Mario level generation and their source codes are publicly available online. Formulations and more details of the two tasks are presented in Appendix D.1.\nCompared Algorithms In total, 30 algorithm instances are used to train 5 independent generators each (thus 150 generators in total), which can be categorised into three groups. (i) 24 NCERL instances are trained with all the combinations of the ensemble sizem \u2208 {2, 3, 4, 5} and regularisation coefficient \u03bb \u2208 {0.0, 0.1, 0.2, 0.3, 0.4, 0.5}. (ii) Three state-of-the-art ensemble RL algorithms published within the past three years, including PMOE (Ren et al., 2021), DvD (Parker-Holder et al., 2020) and SUNRISE (Lee et al., 2021) reviewed in Section 2. The algorithm proposed in (Yang et al., 2022) is excluded as it is designed for discrete action space. All of them are trained in our proposed asynchronous framework, using five individual actors following (Parker-Holder et al., 2020; Lee et al., 2021). During the test, one of the sub-policies is randomly selected to make the decision at each step for better diversity. (iii) Three SACs are trained in the standard single-process setting, in the synchronous multi-process framework in (Wang et al., 2022), and in our proposed asynchronous framework, referred to as SAC, EGSAC and ASAC, respectively.\nAll algorithm instances are trained for one million steps. Common hyperparameters shared by all instances are set to the same values. Hyperparameters uniquely belonging to an algorithm are set as reported in its original paper. Appendix D.2 reports all the hyperparameters and their values."
        },
        {
            "heading": "6.2 RESULTS AND DISCUSSION",
            "text": "Performance Criteria All of the trained generators are tested by generating 500 levels of 25 segments each. Cumulative reward and diversity evaluated on the 500 levels generated by the generators are compared. The diversity is measured by the expectation of distance between pairs of generated levels, which is extensively used in PCG (Nam et al., 2021; Earle et al., 2021; Beukman et al., 2022). Additionally, geometric mean (G-mean) (Derringer, 1994) and average ranking (Avg-rank) (Zhang et al., 2022) are used to enable unified comparisons. The average ranking criterion estimates the average of reward rank and diversity rank of an algorithm instance out of all compared ones. Both unified criteria are suitable for combining multiple metrics in different scales. Appendix D.4 formulates the criteria and discusses more details.\nEffectiveness of NCERL Table 1 reports the performances of the tested algorithm instances. On both tasks, NCERL achieves the highest diversity. In terms of the G-mean criterion which unifies reward and diversity, NCERL almost outperforms all the other algorithms with any \u03bb except for PMOE. With \u03bb = 0.3, 0.4 and 0.5, the G-mean score of NCERL is higher than PMOE on both tasks. The superior G-mean scores indicate that NCERL balances reward and diversity better and\n1We refer to an algorithm with specific hyperparameters as an algorithm instance.\noutperforms other compared algorithms in a unified sense. In terms of the other unified criterion, the average ranking, NCERL surpasses all other algorithms except for SUNRISE on the MarioPuzzle task. Compared to SUNRISE, NCERL allows one to make trade-offs between reward and diversity by specifying the regularisation coefficient. With \u03bb = 0.5, NCERL achieves the best diversity score on both tasks. PMOE shows competitive performance in terms of diversity, which is only lower than the NCERL instance s of \u03bb = 0.2 and \u03bb = 0.5 on MarioPuzzle and only lower than the NCERL of \u03bb = 0.5 on MultiFacet, but the reward gained by PMOE is worse than most NCERL instances. The reward gained by NCERL is not superior among all compared ones, but the enhancement to diversity is more significant than the sacrifice of reward, according to the results of the G-mean.\nFigure 2 further shows the learning curves of tested algorithms. On both tasks, the reward of all algorithms ascends over time steps. Meanwhile, except for NCERL and PMOE, the diversity scores of all other algorithms descend over time steps. On the MarioPuzzle task, the G-mean scores of NCERL and PMOE ascend, while on MultiFacet, their G-mean values remain high. The G-mean values of all other algorithms descend on both tasks. As both NCERL and PMOE use multimodal policy, this observation implies that the multimodal policy is key to balancing reward and diversity. Overall, NCERL better balances reward and diversity.\nTo analyse the performance of each independent trial, we illustrate the locations of all trained generators in the rewarddiversity objective space via scatter plots\n(Figure 3). According to Figure 3, the compared algorithms are generally located in regions of low diversity, while NCERLs spread widely and contribute a major portion of the Pareto front. Most of the generators trained by the compared algorithms are dominated by NCERL generators, while the non-dominated ones are generally biased towards the reward. The observations further indicate that NCERL is able to train generators with varied trade-offs between reward and diversity, making it possible to cater to varied preferences.\nThe locations of NCERL generators trained under the same hyperparameters sometimes vary a lot, revealing that NCERL can be instable over trials. Table 1 and Figure 2 also show the standard deviation of NCERL\u2019s performance is generally big. Meanwhile, PMOE also suffers from instability\nacross trials, especially on the MultiFacet task. As both PMOE and NCERL use multimodal policy, future work may investigate techniques to mitigate the instability of training multimodal policy. On the other hand, it is expected to integrate NCERL with multi-objective RL (Hayes et al., 2022), to train a set of non-dominated generators in which the instability may not be a disadvantage.\nVerification of the Asynchronous Framework To verify our proposed asynchronous framework, SAC, EGSAC and ASAC are compared. According to Table 1, the EGSAC, which trains SAC in a synchronous framework (Wang et al., 2022), gets lower rewards and higher diversity scores. ASAC\u2019s performance is very similar to SAC, especially in terms of the cumulative reward. Therefore, it is more reliable to plug base algorithms into our proposed asynchronous framework. Meanwhile, our asynchronous framework is faster than the synchronous framework of EGSAC. We train generators with ASAC, EGSAC and standard SAC for one million time steps each on a computation platform with 64 CPU cores and GTX 2080 GPU to compare their time efficiency. Using 20 processes for evaluation, ASAC costs 4.93h while EGSAC costs 6.06h, i.e., ASAC is 22.9% faster than EGSAC. The standard single-process SAC costs 34.26h, i.e., ASAC speeds up SAC by 596%.\nInfluence of Hypareparamters According to Table 1 and Figure 3, as \u03bb increases, the diversity score generally increases while the reward generally decreases. The influence of varying the ensemble size m does not show clear regularities. We investigate and discuss the influence of ensemble size and regularisation coefficient more comprehensively in Appendix E.1."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "In this paper, we propose a novel negatively correlated ensemble RL approach, to enable online diverse game level generation. The NCERL approach uses a Wasserstein distance-based regularisation to diversify the behaviour of a multimodal ensemble policy. Furthermore, an asynchronous off-policy training framework is designed to train online level generators faster. To show how the regularisation can be optimised in MDP, we derive the regularised RL theorems, which facilitate NCERL. NCERL is shown to be able to generate diverse game levels with competitive performance on the reward. It achieves superior G-mean scores, which indicates that NCERL better balances the reward and diversity. The proposed method and theorems make it possible to further develop multiobjective RL algorithms that consider the diversity of generated levels as an objective, which can train a set of non-dominated generators in one single trial to cater to varied preferences. Because the levels generated by NCERL are diverse, it is likely to enable fresh and interesting gameplay experiences even after numerous levels have been generated and played."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported by the National Key R&D Program of China (Grant No. 2023YFE0106300), the National Natural Science Foundation of China (Grant No. 62250710682), the Research Institute of Trustworthy Autonomous Systems, and the Guangdong Provincial Key Laboratory (Grant No. 2020B121201001)."
        },
        {
            "heading": "A PROOFS",
            "text": "The proofs of Lemma 1, Lemma 2, Theorem 1 and Theorem 2 are provided in this section.\nA.1 BEHAVIOUR REGULARISED POLICY ITERATION\nOur proof of regularised policy iteration follows the similar line of soft policy iteration (Haarnoja et al., 2018a) but considers a general decision distribution regularisation setting rather than the specific maximising entropy setting.\nLemma 1 (Q\u03f1-Function Evaluation). By repeatedly applying Qk+1\u03f1 = T \u03c0\u03f1 Qk\u03f1 from an arbitrary Q\u03f1-function Q0\u03f1, the sequence Q 0 \u03f1, \u00b7 \u00b7 \u00b7 , Qk\u03f1, \u00b7 \u00b7 \u00b7 converges to Q\u03c0\u03f1 as k \u2192\u221e.\nProof. As the policy \u03c0 to be evaluated is fixed at this stage, we can treat \u03b3Ea\u223c\u03c0(\u00b7|s),s\u2032\u223cp(\u00b7|s,a)[\u03f1\u03c0(s\u2032)] as a reward function r\u0302(s, a), and then treat Q\u03c0\u03f1 (s, a) as a Q-function since\nQ\u03c0\u03f1 (s, a) . = EM,\u03c0 [\u2211\u221e k=1 \u03b3k\u03f1\u03c0(St+k) \u2223\u2223\u2223St = s,At = a]\n= EM,\u03c0 [\u2211\u221e\nk=0 \u03b3k (\u03b3\u03f1\u03c0(St+k+1)) \u2223\u2223\u2223St = s,At = a] = EM,\u03c0 [\u2211\u221e k=0 \u03b3kR\u0302t+k\n\u2223\u2223\u2223St = s,At = a] , where the last expression is the same as the definition of standard Q-function. Then we can simply borrow the theoretical results of standard policy evaluation (Sutton & Barto, 2018).\nLemma 2 (Regularised Policy Improvement). For any \u03c0old \u2208 \u03a0 and its \u03c0new derived via equation 5, it is guaranteed that \u03c0new \u227b \u03c0old if \u03c0old is not optimal. Recap of equation 5:\n\u2200s \u2208 S, \u03c0new(\u00b7|s) = argmax \u03c0(\u00b7|s)\u2208\u03a0(\u00b7|s)\n[ \u03bb\u03f1\u03c0(s) + Ea\u223c\u03c0(\u00b7|s) [ Q\u03c0old\u03f1 (s, a) ] ] . (5)\nProof. By taking the operator described in equation 5, it is definite that \u2200s \u2208 S , \u03bb\u03f1\u03c0new(s) + Ea\u223c\u03c0new(\u00b7|s) [ Q\u03c0old\u03f1 (s, a) ] \u2265 \u03bb\u03f1\u03c0old(s) + Ea\u223c\u03c0old(\u00b7|s) [ Q\u03c0old\u03f1 (s, a) ] = V\u03c0old\u03f1 (s). With Q\u03c0\u03f1 (s, a) = r(s, a) + Es\u2032\u223cp(\u00b7|s,a) [\u03b3V\u03c0(s\u2032)], we have:\nV\u03c0old(s) \u2264 \u03bb\u03f1\u03c0new(s) + Ea\u223c\u03c0new(\u00b7|s) [ Q\u03c0old\u03f1 (s, a) ] = \u03bb\u03f1\u03c0new(s) + Ea\u223c\u03c0new(\u00b7|s) [ r(s, a) + Es\u2032\u223cp(\u00b7|s,a) [ \u03b3V\u03c0old\u03f1 (s\u2032)\n] ] = Ea\u223c\u03c0new(\u00b7|s)[\u03bb\u03f1 \u03c0new(s) + r(s, a)] + \u03b3EM,\u03c0new [ V\u03c0old\u03f1 (St+1)\n\u2223\u2223St = s] \u2264 Ea\u223c\u03c0new(\u00b7|s)[\u03bb\u03f1 \u03c0new(s) + r(s, a)] + \u03b3EM,\u03c0new [ \u03bb\u03f1\u03c0new(St+1) + Ea\u2032\u223c\u03c0new(\u00b7|St+1) [ Q\u03c0old\u03f1 (St+1, a\u2032)\n] \u2223\u2223\u2223St = s] = \u22111\nk=0 \u03b3kEM,\u03c0new [\u03bb\u03f1\u03c0new(St+k) +Rt+k| St = s] + \u03b32EM,\u03c0new\n[ V\u03c0old\u03f1 (St+2) \u2223\u2223St = s] ...\n\u2264 EM,\u03c0new [\u2211\u221e\nk=0 \u03b3k(\u03bb\u03f1\u03c0new(St+k) +Rt+k)\n] = V\u03c0new\u03f1 (s).\nHence, \u2200s \u2208 S, V\u03c0old\u03f1 (s) = V\u03c0new\u03f1 (s) or \u03c0new \u227b \u03c0old. If it is the former case, then \u2200s \u2208 S we have:\nV\u03c0new\u03f1 (s) = max \u03c0\u2208\u03a0\n[ \u03bb\u03f1\u03c0(s) + Ea\u223c\u03c0(\u00b7|s)[Q\u03c0new\u03f1 (s, a)] ] = max\n\u03c0\u2208\u03a0\n[ \u03bb\u03f1\u03c0(s) + Ea\u223c\u03c0(\u00b7|s) [ r(s, a) + \u03b3Es\u2032\u223cp(\u00b7|s,a)[V\u03c0new\u03f1 (s\u2032)] ] ] = max\n\u03c0\u2208\u03a0 EM,\u03c0\n[ \u03bb\u03f1\u03c0(St) +Rt + \u03b3V\u03c0new\u03f1 (St+1) \u2223\u2223St = s] = max\n\u03c0\u2208\u03a0 EM,\u03c0 [\u2211\u221e k=0 \u03b3k (\u03bb\u03f1\u03c0(St+k) +Rt+k) \u2223\u2223\u2223St = s] .\nIt indicates that both \u03c0new and \u03c0old are optimal. Therefore, \u03c0new \u227b \u03c0old if \u03c0old is not optimal.\nTheorem 1 (Regularised Policy Iteration). Given a finite stochastic policy space \u03a0, regularised policy iteration converges to an optimal policy over \u03a0 from any \u03c00 \u2208 \u03a0.\nProof. Collecting Lemma 1 and Lemma 2 with the theoretical result of standard Q-value evaluation (Sutton et al., 1999) and the condition that \u03a0 is finite, evidently the policy converges to optimal.\nA.2 STOCHASTIC POLICY GRADIENT FOR BEHAVIOUR REGULARISATION\nOur proof for Theorem 2 draws lessons from the proof of stochastic policy gradient (Sutton et al., 1999) for basic ideas, and deterministic policy gradient (Silver et al., 2014) for dealing with continuous space. We alter the superscript between \u03c0 and \u03b8 and sometimes drop the \u03b8 term in the formulation of policy, such a denotation rule aims at emphasising whether the term requires gradient w.r.t. \u03b8 or not, but all the terms are induced by the parametric model \u03c0\u03b8(\u00b7|\u00b7). We assume that the state space and action space are continuous, and the discounted regularisation objective is considered. Furthermore, we assume the involved functions are continuous over the space and have real number supremum whenever we exchange the orders of derivations and integrals.\nTheorem 2 (Policy-Regularisation Gradient, PRG). The gradient of a policy regularisation objective J\u03b8\u03f1 = EM,\u03c0\u03b8 [ \u2211\u221e t=0 \u03b3 t\u03f1\u03b8(St)] w.r.t. \u03b8 follows\n\u2202J\u03b8\u03f1 \u2202\u03b8 = \u222b S d\u03c0(s) ( \u2202\u03f1\u03b8(s) \u2202\u03b8 + \u222b A Q\u03c0\u03f1 (s, a) \u2202\u03c0\u03b8(a|s) \u2202\u03b8 da ) ds (6)\n= E s\u223cd\u03c0, a\u223c\u03c0(\u00b7|s)\n[ \u2202\u03f1\u03b8(s)\n\u2202\u03b8 +Q\u03c0\u03f1 (s, a) \u2202 ln\u03c0\u03b8(a|s) \u2202\u03b8\n] .\nProof. According to the definition of the value functions of regularisation, we have\nV \u03c0\u03f1 (s) = \u03f1 \u03c0(s) + Ea\u223c\u03c0(\u00b7|s)[Q\u03c0\u03f1 (s, a)] = \u03f1\u03c0(s) + \u222b A \u03c0(a|s)Q\u03c0\u03f1 (s, a) da\nand\nQ\u03c0\u03f1 (s, a) = Es\u2032\u223cp(\u00b7|s,a)[\u03b3V \u03c0\u03f1 (s\u2032)] = \u222b S \u03b3p(s\u2032|s, a)V \u03c0\u03f1 (s\u2032) ds\u2032.\nThen we derive the bootstrap equation of \u2202V \u03b8\u03f1 (s)\n\u2202\u03b8 as follows:\n\u2202V \u03b8\u03f1 (s)\n\u2202\u03b8 =\n\u2202\n\u2202\u03b8\n[ \u03f1\u03b8(s) + \u222b A \u03c0\u03b8(a|s)Q\u03b8\u03f1(s, a) da ] = \u2202\u03f1\u03b8(s)\n\u2202\u03b8 + \u222b A ( \u2202\u03c0\u03b8(a|s) \u2202\u03b8 Q\u03c0\u03f1 (s, a) + \u03c0(a|s) \u2202Q\u03b8\u03f1(s, a) \u2202\u03b8 ) da\n= \u2202\u03f1\u03b8(s)\n\u2202\u03b8 + \u222b A \u2202\u03c0\u03b8(a|s) \u2202\u03b8 Q\u03c0\u03f1 (s, a) da+ \u222b A \u03c0(a|s) \u2202 \u2202\u03b8 \u222b S \u03b3p(s\u2032|s, a)V \u03b8\u03f1 (s\u2032) ds\u2032 da\n= \u2202\u03f1\u03b8(s)\n\u2202\u03b8 + \u222b A \u2202\u03c0\u03b8(a|s) \u2202\u03b8 Q\u03c0\u03f1 (s, a) da+ \u222b S ( \u03b3 \u222b A \u03c0(a|s)p(s\u2032|s, a) da ) \u2202V \u03b8\u03f1 (s \u2032) \u2202\u03b8 ds\u2032\n= \u2202\u03f1\u03b8(s)\n\u2202\u03b8 + \u222b A \u2202\u03c0\u03b8(a|s) \u2202\u03b8 Q\u03c0\u03f1 (s, a) da+ \u222b S \u03b3P[s 1\u2192 s\u2032, \u03c0] \u2202V \u03b8\u03f1 (s \u2032) \u2202\u03b8 ds\u2032.\n(10) Note \u222b S P[s t\u2192u, \u03c0] \u222b S P[u 1\u2192 s\u2032, \u03c0]f(s\u2032) ds\u2032 du = \u222b S P[s t+1\u2212\u2192 s\u2032, \u03c0]f(s\u2032) ds\u2032 is held for any t > 0 as the Markov property is satisfied by any MDP. So we can unroll the result of equation 10 as follows:\n\u2202V \u03b8\u03f1 (s)\n\u2202\u03b8 = \u221e\u2211 t=0 \u222b S \u03b3tP[s t\u2192 s\u2032, \u03c0] ( \u2202\u03f1\u03b8(s\u2032) \u2202\u03b8 + \u222b A \u2202\u03c0\u03b8(a|s\u2032) \u2202\u03b8 Q\u03c0\u03f1 (s \u2032, a) da ) ds\u2032, (11)\nwhere the item of t = 0 is an improper integral that represents \u2202\u03f1\u03b8(s)\n\u2202\u03b8 + \u222b A \u2202\u03c0\u03b8(a|s) \u2202\u03b8 Q\u03c0\u03f1 (s, a) da.\nAs J\u03b8\u03f1 = EM,\u03c0 [\u2211\u221e\nt=0 \u03b3t\u03f1\u03b8(St)\n] = \u222b S p0(s)V \u03b8 \u03f1 (s) ds, together with equation 11 we have\n\u2202J\u03b8\u03f1 \u2202\u03b8 = \u2202 \u2202\u03b8 \u222b S p0(s)V \u03b8 \u03f1 (s) ds = \u222b S p0(s) \u2202V \u03b8\u03f1 (s) \u2202\u03b8 ds\n= \u222b S p0(s) \u221e\u2211 t=0 (\u222b S \u03b3tP[s t\u2192 s\u2032, \u03c0] ( \u2202\u03f1\u03b8(s\u2032) \u2202\u03b8 + \u222b A \u2202\u03c0\u03b8(a|s\u2032) \u2202\u03b8 Q\u03c0\u03f1 (s \u2032, a) da ) ds\u2032 ) ds\n= \u221e\u2211 t=0 \u222b S \u222b S p0(s)\u03b3 tP[s t\u2192 s\u2032, \u03c0] ( \u2202\u03f1\u03b8(s\u2032) \u2202\u03b8 + \u222b A \u2202\u03c0\u03b8(a|s\u2032) \u2202\u03b8 Q\u03c0\u03f1 (s \u2032, a) da ) ds ds\u2032\n= \u222b S ( \u221e\u2211 t=0 \u222b S \u03b3tp0(s)P[s t\u2192 s\u2032, \u03c0] ds )( \u2202\u03f1\u03b8(s\u2032) \u2202\u03b8 + \u222b A \u2202\u03c0\u03b8(a|s\u2032) \u2202\u03b8 Q\u03c0\u03f1 (s \u2032, a) da ) ds\u2032\n= \u222b S d\u03c0(s\u2032) ( \u2202\u03f1\u03b8(s\u2032) \u2202\u03b8 + \u222b A Q\u03c0\u03f1 (s \u2032, a) \u2202\u03c0\u03b8(a|s\u2032) \u2202\u03b8 da ) ds\u2032.\n(12) For the \u222b A Q\u03c0\u03f1 (s \u2032, a) \u2202\u03c0\u03b8(a|s\u2032) \u2202\u03b8 da term, we can apply the log-derivative trick as follows:\n\u222b A Q\u03c0\u03f1 (s \u2032, a) \u2202\u03c0\u03b8(a|s\u2032) \u2202\u03b8 da = \u222b A Q\u03c0\u03f1 (s \u2032, a)\u03c0(a|s\u2032)\u2202\u03c0 \u03b8(a|s\u2032) \u03c0(a|s\u2032)\u2202\u03b8 da\n= \u222b A \u03c0(a|s\u2032)Q\u03c0\u03f1 (s\u2032, a) \u2202 ln\u03c0\u03b8(a|s\u2032) \u2202\u03b8 da\n= Ea\u223c\u03c0(\u00b7|s\u2032) [ Q\u03c0\u03f1 (s \u2032, a) \u2202 ln\u03c0\u03b8(a|s\u2032)\n\u2202\u03b8\n] .\n(13)\nConcluding equation 12 and equation 13, we get equation 6."
        },
        {
            "heading": "B EXPLAINING ENTROPY REGULARISED RL WITH GENERAL POLICY REGULARISATION THEOREMS",
            "text": "The soft policy iteration (Haarnoja et al., 2018b) improves a policy \u03c0old by\n\u03c0new(\u00b7|s) = argmin \u03c0\u2208\u03a0 DKL\n( \u03c0(\u00b7|s) \u2225\u2225\u2225\u2225exp(Q\u03c0oldH (s, \u00b7)/\u03b1)Z\u03c0old(s) ) .\nThis is a specific case of equation 5 with \u03f1\u03c0(s) = H(\u03c0(\u00b7|s)) and \u03bb = \u03b1, where H denotes the entropy, since\nargmin \u03c0\u2208\u03a0 DKL\n( \u03c0(\u00b7|s) \u2225\u2225\u2225\u2225exp(Q\u03c0oldH (s, \u00b7)/\u03bb)Z\u03c0old(s) )\n=argmin \u03c0\u2208\u03a0 [\u222b A \u03c0(a|s) log \u03c0(a|s)Z \u03c0old(s) exp(Q\u03c0oldH (s, a)/\u03bb) da ] =argmin\n\u03c0\u2208\u03a0 [\u222b A \u03c0(a|s) log \u03c0(a|s) da+ \u222b A \u03c0(a|s) log 1 exp(Q\u03c0oldH (s, a)/\u03bb) da+ \u222b A \u03c0(a|s) logZ\u03c0old(s) da ] =argmin\n\u03c0\u2208\u03a0\n[ \u2212H(\u03c0(\u00b7|s))\u2212 1\n\u03bb \u222b A \u03c0(a|s)Q\u03c0oldH (s, a) da ] =argmax\n\u03c0\u2208\u03a0\n[ \u03bb\u03f1\u03c0(s) + Ea\u223c\u03c0(\u00b7|s)[Q\u03c0oldH (s, a)] ] .\nAt the same time, the gradient of actor loss of SAC, can be viewed as a weighted summation of standard SPG and PRG in terms of entropy. The actor loss of SAC (Haarnoja et al., 2018b) is written as\nJ\u03b8 = Es\u223cD,a\u223c\u03c0\u03b8(\u00b7|s) [ \u03b1 log(\u03c0\u03b8(\u00b7|s))\u2212QH(s, a) ] .\nWe borrow idea from (Degris et al., 2012), consider a behavioural policy\u03d6 so that s \u223c d\u03d6 \u2261 s \u223c D, then we have\nJ\u03b8 = Es\u223cD,a\u223c\u03c0\u03b8(\u00b7|s) [ \u03b1 log(\u03c0\u03b8(\u00b7|s))\u2212QH(s, a) ] = Es\u223cd\u03d6 [ \u2212H\u03b8(s)\u2212 \u222b A \u03c0\u03b8(a|s)(Q(s, a) + \u03bbQH(s, a)) da\n] \u2212\u2202J\u03b8 \u2202\u03b8 = Es\u223cd\u03d6 [\u222b\nA\n\u2202\u03c0\u03b8(a|s) \u2202\u03b8 Q(s, a) da ] \ufe38 \ufe37\ufe37 \ufe38\nStandard SPG\n+\u03bbEs\u223cd\u03d6 [ \u2202\u03f1\u03b8(s)\n\u2202\u03b8 + \u222b A \u2202\u03c0\u03b8(a|s) \u2202\u03b8 QH(s, a) da ] \ufe38 \ufe37\ufe37 \ufe38\nPRG for entropy\n.\nSimilarly, by s \u223c d\u03d6 \u2261 s \u223c D, we can explain NCERL as optimising a weighted summation of behavioural SPG and behavioural PRG."
        },
        {
            "heading": "C ADDITIONAL DETAILS",
            "text": "C.1 ASYNCHRONOUS OFF-POLICY TRAINING FRAMEWORK\nWe employ an evaluation pool to enable asynchronous evaluation. The pool contains a capacited queue and multiple processes. Once a task is submitted, the pool assigns the task to a free process if available, otherwise, the task will be stored in the queue temporarily. If the queue is full, the pool will be blocked until any process is finished. Algorithm 1 describes our asynchronous framework.\nAlgorithm 1 Main Procedure of the Asynchronous Off-Policy Training Framework. Require: Number of evaluation workers w, horizon of MDP h, update\ninterval itv 1: Create a multi-processing evaluation pool with w workers 2: credits\u2190 0 3: repeat 4: Sample a no-reward trajectory z\u20d7 via the agent 5: Submit \u03c4 to the evaluation pool 6: T \u2190 collect evaluated trajectories from the pool 7: Update D with T 8: credits\u2190 credits + |T |itv 9: for u : 1\u2192 min{\u230acredits\u230b, \u2308 1.25whitv \u2309} do\n10: B \u2190 sample a batch from D 11: Update agent with B 12: credits\u2190 credits\u2212 1 13: end for 14: until run out of interaction budget 15: Waiting for all remaining tasks finished 16: T \u2190 collect all trajectories from the pool 17: Update D with T 18: for u : 1\u2192 credits + |T |itv do 19: B \u2190 sample a batch from D 20: Update agent with B via the base algorithm 21: end for\nThe coefficient 1.25 in line 9 of Algorithm 1 is arbitrarily set. We just arbitrarily assign a number slightly higher than 1, aiming to submit the simulation tasks uniformly, which reduces the suspends of simulation workers. The coefficient should be larger than 1 so that the credits can be used up.\nC.2 2-WASSERSTEIN DISTANCE\nThe 2-Wasserstein distance for a pair of probability distributions X and Y is defined as\n\u03c9(X,Y ) = ( inf\nZ\u2208Z(X,Y ) E(x,y)\u223cZ\n[ \u2225x\u2212 y\u22252 ]) 12 ,\nwhere Z is a joint distribution of X and Y and Z is the set of all joint distribution of X and Y . In case both X and Y are Gaussian, the 2-Wasserstein distance can be expressed as follows.\n\u03c92(N1,N2) = \u2225\u00b51 \u2212 \u00b52\u222522 +Trace ( \u03a31 +\u03a32 \u2212 2 ( \u03a3 1 2 2 \u03a31\u03a3 1 2 2 ) 1 2 ) ,\nwhereN1,N2 are the two Gaussian distribution to be compared, \u00b51 and \u00b52 are the means ofN1,N2, and \u03a31,\u03a32 are the covariance matrices of N1,N2, respectively."
        },
        {
            "heading": "D ADDITIONAL EXPERIMENT DETAILS",
            "text": "D.1 ONLINE LEVEL GENERATION TASKS\nIn our tasks, the state space is a dn-dimensional continuous vector space, where d is the dimensionality of the latent vector of the action decoder and n is the number of recently generated segments considered in the reward function. A state is a concatenated vector of a fixed number of latent vectors of recently generated segments. If there are not enough segments have been generated (< n) to construct a state, zeros will be padded in the vacant entries. The action space is a d-dimensional continuous vector space. An action is a latent vector which can be decoded into a level segment by the decoder. The decoder is a trained GAN in this work.\nThe reward functions of the two tasks considered in this paper consist of several reward terms. Those reward terms are described and formulated as follows.\nPlayability The work of (Shu et al., 2021) and (Wang et al., 2022) use different formulation of playability. We use the one of (Wang et al., 2022) as it is the most recent one. Formally, the playability reward is\nP(xt) = { 0, if xt\u22121 \u2295 xt is playable, \u22121, otherwise,\nwhere \u2295 represents appending a level segment with another. The playability is judged by the strongest game-playing agent in the Mario-AI-Framework benchmark (Karakovskiy & Togelius, 2012).\nFun The fun reward (Shu et al., 2021) uses four configuration parameters lb, ub, \u03b4 and n. Furthermore, a metric TPKL(\u00b7, \u00b7) is used to measure the dissimilarity of levels. Formally, the fun reward is\nF(xt) =  \u2212(D\u0304(xt)\u2212 lb)2, if D\u0304(xt) < lb, \u2212(D\u0304(xt)\u2212 ub)2, if D\u0304(xt) > ub, 0, otherwise,\nwhere\nD\u0304(xt) = 1\nn+ 1 n\u2211 i=0 TPKL(xt,SW(xt, i\u03b4)),\nwhere SW(xt, i\u03b4) represents the level segment extracted by sliding a window from xt backward with a stride of i\u03b4 tiles.\nHistorical Deviation Historical deviation (Shu et al., 2021) uses two configuration parameters m and n with m > n. Formally, the historical deviation reward is\nH(xt) = 1\nn min X \u2211 x\u2032\u2208X TPKL(xt, x \u2032),\ns.t. X \u2282 {xt\u2212m, \u00b7 \u00b7 \u00b7 , xt\u22121} \u2227 |X| = n.\nLevel Novelty Level novelty (Wang et al., 2022) uses two configuration parameters g and n. Furthermore, a metric TPJS(\u00b7, \u00b7) is used to measure the dissimilarity of levels. Formally, the level novelty reward function is\nL(xt) =\n\u2211n i=1 AC ( TPJS(xt, xt\u2212i); g, ri)\u2211n\ni=1 ri ,\nin which ri = 1\u2212 in+1 and\nAC(u; g, r) = min { r, 1\u2212 |u\u2212 g|\ng\n} .\nGameplay Novelty Gameplay novelty (Wang et al., 2022) shares the same form with the level novelty but replaces the TPJS metric with another metric DG(\u00b7, \u00b7) evaluating the distance between the simulated gameplay trace of two levels or level segments. Let gp(x) be simulated gameplay of arbitrary level segment x, formulation of gameplay novelty is\nG(xt) =\n\u2211n i=1 AC ( DG(gp(xt), gp(xt\u2212i)) ) ; g, ri)\u2211n\ni=1 ri .\nAll the configuration parameters are set as suggested in the corresponding papers and are summarised in Table 2.\nThe two tasks use weighted sums of their proposed reward terms as the final reward function. The observation space varies over different tasks since different reward functions depend on different numbers of latest level segments. Table 3 summarises the information of six tasks tested in this paper.\nThe hyperparameters are listed in Table 4.\nUsing a discounted rate at 0.9 which is not close to 1 is counter-intuitive as it can induce a large bias to the optimal policy in the average reward criterion being considered in OLG. However, we consider the OLG tasks satisfying a perfectible property. In this case, using any \u03b3 > 0 does not bias the optimal policy, so we can use a relatively small \u03b3 which reduces the variance of gradient estimation. The next subsection details the assumed property.\nD.3 PERFECTIBLE PROPERTY AND SETTING OF DISCOUNT FACTOR\nTwo criteria are typically considered in RL, namely the average reward criterion JA(\u03c0) = limh\u2192\u221e EM,\u03c0 [ 1 h \u2211h\u22121 t=0 Rt ] and the discounted reward criterion JD(\u03c0) = EM,\u03c0 [ \u2211\u221e t=0 \u03b3\ntRt], where \u03b3 \u2208 [0, 1] is the discount factor. In principle, the average reward criterion should be considered in OLG. However, we found that in some OLG tasks, relatively small \u03b3 (e.g., 0.9) ensures a superior average reward despite that it may make JD badly approximate JA.\nTo explain the aforementioned phenomenon, we assume that the MDP of some OLG tasks permits perfect policy after investigating the formulations of reward function in OLG (see Appendix D.1 for the reward functions). A perfect policy always gains the maximum reward. An MDP is said to be perfectible if it permits perfect policy. Let S\u03c0 be the set of all the states that possibly appear at any time step given a policy \u03c0, and r\u2217 be the maximum value of reward r(s, a) over S \u00d7 A, the assumption is formalised as follows. Assumption 1 (Perfectible MDP). For the MDP of OLG tasks considered in this work, there exists a perfect policy \u03c0\u25e6 that satisfies \u2200s \u2208 S\u03c0\u25e6 , P[r(s, a) = r\u2217 | a \u223c \u03c0\u25e6(\u00b7|s)] = 1.\nUnder Assumption 1, we have the following proposition. Proposition 1. For a perfectible MDP, any optimal policy in terms of JA and any optimal policy in terms of JD (with 0 < \u03b3 \u2264 1) are all perfect policies.\nThe proof of Proposition 1 is straightforward. If an optimal policy \u03c0\u2217 is not perfect, then there must be \u2203s \u2208 S\u03c0\u25e6 , V \u03c0\u25e6(s) > V \u03c0\u2217(s), in terms of both JA and JD criteria. This is in contrast to that \u03c0\u2217 is optimal. Therefore, we can use a \u03b3 that is not close to 1 to optimise JA without bias of optimal policy in perfectible MDP. Intuitively, smaller \u03b3 can reduce the variance, while larger \u03b3 is likely to induce fewer local optima. One may need to set \u03b3 carefully to enable superior average return in perfectible MDP.\nD.4 PERFORMANCE CRITERIA\nLet n = 500 and h = 25 be the number of levels generated by each generator for the test and the number of segments in each level (h = 25 is used because this is slightly longer than the longest level in the training level set of the GAN.), the performance criteria are described below.\nCumulative Reward Cumulative reward for a generator is calculated as R = \u2211h\nt=1Rt for each level, i.e., each MDP trajectory, then averaged over the 500 levels.\nDiversity Score Diversity score of a generator is calculated as\nD = 2\nn(n\u2212 1) n\u2211 i=1 \u2211 j \u0338=i \u2206(xi, xj)\n, where \u2206(\u00b7, \u00b7) indicates the Hamming distance, i.e., how many different tiles are there between the two levels to be compared; and xi, xj indicate the ith one and the jth one in the n levels.\nGeometric Mean G-Mean for a generator is calculated as G = \u221a RD. It is suitable to combine R and D even though they are in different scales. Because given any scaling coefficients sR > 0 and sD > 0 to rescale the cumulative reward and diversity score, the ratio between any two generators\u2019 G-mean values is constant since\nG\u20321 G\u20322 = \u221a sRR1sDD1\u221a sRR1sDD1 = \u221a R1D1\u221a R1D1 = G1 G2 ,\nwhere the subscripts 1 and 2 indicate the two generators being compared in terms of G-mean.\nAverage Ranking For the average ranking, we first rank the 60 generators (K = 12 algorithm instances being compared\u00d7 T = 5 independent trials) in terms of reward and diversity, respectively, from the highest to the lowest. With kR and kD denoting the ranks of a generator in terms of reward and diversity out of the KT generators, respectively, the average ranking of this generator is calculated as A = 12 (kR + kD)/T ."
        },
        {
            "heading": "E ADDITIONAL EXPERIMENT RESULTS",
            "text": "E.1 INFLUENCE OF HYPAREPARAMTERS\nWe add tables 5 and 6 to show the performance of each independent NCERL generator to better analyse the training results, especially the effect of varying hyperparameters.\nAccording to the table, the generators with larger m seem to perform more stable since there are more bad generators found in the group of m = 2. By increasing \u03bb, the diversity is generally improved while the reward is generally decreased. However, the change in performance is not monotonic. The reason may be summed up as that the regularisation is not identical to the diversity score and it also promotes the exploration, making the effect of \u03bb not fully predictable. Reward and diversity are generally conflicted, but there are examples that a generator performs better than another in terms of both reward and diversity, i.e., dominate another. For example, Trial 2 of m = 2, \u03bb = 0.5 dominates Trial 3 of m = 2, \u03bb = 0.5. Some bad generators fail to gain good rewards while their diversity scores are not superior either (e.g., Trial 5 of m = 2, \u03bb = 0.2, Trial 5 of m = 2, \u03bb = 0.3, Trial 5 of m = 3, \u03bb = 0.3). That means NCERL is not totally stable. Probably the regularisation objective sometimes leads to some local optima during training.\nThe observation of this table is similar to the Table 5, the generators with larger m seem to perform more stable while \u03bb is positively correlated to diversity but negatively correlated to reward. The diversity of those generators is generally smaller than the ones trained on MarioPuzzle. That means the reward function of MultiFacet may not allow super highly-diverse generators.\nE.2 INDIVIDUAL ACTOR SELECTION PROBABILITY\nTo see whether each of the sub-policy is used, we report the selection probability of each sub-policy for two NCERL generators trained with \u03bb = 0.5 and \u03bb = 0.1, in Tables 7 and 8.\nTable 7 shows that the selection probability of the generator trained with \u03bb = 0.5 is adjusted adaptively during the generation process, and all the sub-policies are used within the two generation trials.\nTable 8 shows that some of the selection probability is near zero. Sub-policies 2 and 3 are never used within the two trials. This is because \u03bb is small.\nE.3 GENERATED SAMPLES\nThe following pages show partial examples generated by several trained NCERL generators, with a comparison to the examples generated by a standard SAC. Performance in terms of reward and diversity is reported. Our anonymous code repository2 includes generated examples of all the generators we trained in this work.\n2https://anonymous.4open.science/r/NCERL-Diverse-PCG-4F25/"
        }
    ],
    "title": "ONLINE DIVERSE GAME LEVEL GENERATION",
    "year": 2024
}