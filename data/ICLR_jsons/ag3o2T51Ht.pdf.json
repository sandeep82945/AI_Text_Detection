{
    "abstractText": "Text-to-image generative models can produce photo-realistic images for an extremely broad range of concepts, and their usage has proliferated widely among the general public. Yet, these models have numerous drawbacks, including their potential to generate images featuring sexually explicit content, mirror artistic styles without permission, or even hallucinate (or deepfake) the likenesses of celebrities. Consequently, various methods have been proposed in order to \u201cerase\u201d sensitive concepts from text-to-image models. In this work, we examine seven recently proposed concept erasure methods, and show that targeted concepts are not fully excised from any of these methods. Specifically, we devise an algorithm to learn special input word embeddings that can retrieve \u201cerased\u201d concepts from the sanitized models with no alterations to their weights. Our results highlight the brittleness of post hoc concept erasure methods, and call into question their use in the algorithmic toolkit for AI safety.",
    "authors": [],
    "id": "SP:e96813d395e06d77c982bd16ed00453f544d857d",
    "references": [
        {
            "authors": [
                "Romain Beaumont"
            ],
            "title": "Clip retrieval: Easily compute clip embeddings and build a clip retrieval system with them",
            "year": 2022
        },
        {
            "authors": [
                "Praneeth Bedapudi"
            ],
            "title": "Nudenet: Neural nets for nudity detection and censoring, 2022",
            "venue": "URL https: //github.com/notAI-tech/NudeNet/",
            "year": 2022
        },
        {
            "authors": [
                "Abeba Birhane",
                "Vinay Uday Prabhu",
                "Emmanuel Kahembwe"
            ],
            "title": "Multimodal datasets: misogyny, pornography, and malignant stereotypes",
            "year": 1963
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Jamie Hayes",
                "Milad Nasr",
                "Matthew Jagielski",
                "Vikash Sehwag",
                "Florian Tram\u00e8r",
                "Borja Balle",
                "Daphne Ippolito",
                "Eric Wallace"
            ],
            "title": "Extracting training data from diffusion models",
            "year": 2023
        },
        {
            "authors": [
                "Huiwen Chang",
                "Han Zhang",
                "Jarred Barber",
                "Aaron Maschinot",
                "Jos\u00e9 Lezama",
                "Lu Jiang",
                "Ming-Hsuan Yang",
                "Kevin Murphy",
                "William T. Freeman",
                "Michael Rubinstein",
                "Yuanzhen Li",
                "Dilip Krishnan"
            ],
            "title": "Muse: Text-to-image generation via masked generative transformers",
            "venue": "CoRR, abs/2301.00704,",
            "year": 2023
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Quinn Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Bradley Efron"
            ],
            "title": "Tweedie\u2019s formula and selection bias",
            "venue": "Journal of the American Statistical Association,",
            "year": 2011
        },
        {
            "authors": [
                "Oran Gafni",
                "Adam Polyak",
                "Oron Ashual",
                "Shelly Sheynin",
                "Devi Parikh",
                "Yaniv Taigman"
            ],
            "title": "Makea-scene: Scene-based text-to-image generation with human priors",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Rinon Gal",
                "Yuval Alaluf",
                "Yuval Atzmon",
                "Or Patashnik",
                "Amit Haim Bermano",
                "Gal Chechik",
                "Daniel Cohen-Or"
            ],
            "title": "An image is worth one word: Personalizing text-to-image generation using textual inversion",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Rohit Gandikota",
                "Joanna Materzynska",
                "Jaden Fiotto-Kaufman",
                "David Bau"
            ],
            "title": "Erasing concepts from diffusion models",
            "venue": "In International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Rohit Gandikota",
                "Hadas Orgad",
                "Yonatan Belinkov",
                "Joanna Materzynska",
                "David Bau"
            ],
            "title": "Unified concept editing in diffusion models",
            "venue": "CoRR, abs/2308.14761,",
            "year": 2023
        },
        {
            "authors": [
                "Aditya Golatkar",
                "Alessandro Achille",
                "Stefano Soatto"
            ],
            "title": "Eternal sunshine of the spotless net: Selective forgetting in deep networks",
            "venue": "In Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Aditya Golatkar",
                "Alessandro Achille",
                "Stefano Soatto"
            ],
            "title": "Forgetting outside the box: Scrubbing deep networks of information accessible from input-output observations",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Ligong Han",
                "Yinxiao Li",
                "Han Zhang",
                "Peyman Milanfar",
                "Dimitris Metaxas",
                "Feng Yang"
            ],
            "title": "Svdiff: Compact parameter space for diffusion fine-tuning",
            "venue": "In International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Alvin Heng",
                "Harold Soh"
            ],
            "title": "Selective amnesia: A continual learning approach to forgetting in deep generative models",
            "venue": "CoRR, abs/2305.10120,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "In Advances in Neural Information Processing Systems Workshop,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jeremy Howard"
            ],
            "title": "Imagenette, 2019. URL https://github.com/fastai/imagenette",
            "year": 2019
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A. Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska",
                "Demis Hassabis",
                "Claudia Clopath",
                "Dharshan Kumaran",
                "Raia Hadsell"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2017
        },
        {
            "authors": [
                "Nupur Kumari",
                "Bingliang Zhang",
                "Sheng-Yu Wang",
                "Eli Shechtman",
                "Richard Zhang",
                "Jun-Yan Zhu"
            ],
            "title": "Ablating concepts in text-to-image diffusion models",
            "venue": "In International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Yann LeCun",
                "Corinna Cortes",
                "Christopher J.C. Burges"
            ],
            "title": "Mnist handwritten digit database",
            "venue": "ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist,",
            "year": 2010
        },
        {
            "authors": [
                "Kevin Meng",
                "Arnab Sen Sharma",
                "Alex J. Andonian",
                "Yonatan Belinkov",
                "David Bau"
            ],
            "title": "Mass-editing memory in a transformer",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Pamela Mishkin",
                "Lama Ahmad",
                "Miles Brundage",
                "Gretchen Krueger",
                "Girish Sastry"
            ],
            "title": "Dall\u00b7e 2 preview - risks and limitations, 2022",
            "venue": "URL https://github. com/openai/dalle-2-preview/blob/main/system-card.md](https: //github.com/openai/dalle-2-preview/blob/main/system-card.md)",
            "year": 2022
        },
        {
            "authors": [
                "Hadas Orgad",
                "Bahjat Kawar",
                "Yonatan Belinkov"
            ],
            "title": "Editing implicit assumptions in text-to-image diffusion models",
            "venue": "In International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Yiting Qu",
                "Xinyue Shen",
                "Xinlei He",
                "Michael Backes",
                "Savvas Zannettou",
                "Yang Zhang"
            ],
            "title": "Unsafe diffusion: On the generation of unsafe images and hateful memes from text-to-image models",
            "venue": "In ACM Conference on Computer and Communications Security,",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical textconditional image generation with CLIP latents",
            "year": 2022
        },
        {
            "authors": [
                "Javier Rando",
                "Daniel Paleka",
                "David Lindner",
                "Lennart Heim",
                "Florian Tram\u00e8r"
            ],
            "title": "Red-teaming the stable diffusion safety filter",
            "venue": "In Advances in Neural Information Processing Systems Workshop,",
            "year": 2022
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Nataniel Ruiz",
                "Yuanzhen Li",
                "Varun Jampani",
                "Yael Pritch",
                "Michael Rubinstein",
                "Kfir Aberman"
            ],
            "title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
            "venue": "In Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L. Denton",
                "Seyed Kamyar Seyed Ghasemipour",
                "Raphael Gontijo Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans",
                "Jonathan Ho",
                "David J. Fleet",
                "Mohammad Norouzi"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Patrick Schramowski",
                "Manuel Brack",
                "Bj\u00f6rn Deiseroth",
                "Kristian Kersting"
            ],
            "title": "Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models",
            "venue": "In Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Romain Beaumont",
                "Richard Vencu",
                "Cade Gordon",
                "Ross Wightman",
                "Mehdi Cherti",
                "Theo Coombes",
                "Aarush Katta",
                "Clayton Mullis",
                "Mitchell Wortsman",
                "Patrick Schramowski",
                "Srivatsa Kundurthy",
                "Katherine Crowson",
                "Ludwig Schmidt",
                "Robert Kaczmarczyk",
                "Jenia Jitsev"
            ],
            "title": "LAION-5B: an open large-scale dataset for training next generation image-text models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jing Shi",
                "Wei Xiong",
                "Zhe Lin",
                "Hyun Joon Jung"
            ],
            "title": "Instantbooth: Personalized text-to-image generation without test-time finetuning",
            "venue": "CoRR, abs/2304.03411,",
            "year": 2023
        },
        {
            "authors": [
                "Hanul Shin",
                "Jung Kwon Lee",
                "Jaehong Kim",
                "Jiwon Kim"
            ],
            "title": "Continual learning with deep generative replay",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric A. Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning Workshop,",
            "year": 2015
        },
        {
            "authors": [
                "Gowthami Somepalli",
                "Vasu Singla",
                "Micah Goldblum",
                "Jonas Geiping",
                "Tom Goldstein"
            ],
            "title": "Diffusion art or digital forgery? investigating data replication in diffusion models",
            "venue": "In Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Gowthami Somepalli",
                "Vasu Singla",
                "Micah Goldblum",
                "Jonas Geiping",
                "Tom Goldstein"
            ],
            "title": "Understanding and mitigating copying in diffusion models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Xingqian Xu",
                "Zhangyang Wang",
                "Eric Zhang",
                "Kai Wang",
                "Humphrey Shi"
            ],
            "title": "Versatile diffusion: Text, images and variations all in one diffusion model",
            "year": 2022
        },
        {
            "authors": [
                "Jiahui Yu",
                "Yuanzhong Xu",
                "Jing Yu Koh",
                "Thang Luong",
                "Gunjan Baid",
                "Zirui Wang",
                "Vijay Vasudevan",
                "Alexander Ku",
                "Yinfei Yang",
                "Burcu Karagol Ayan",
                "Ben Hutchinson",
                "Wei Han",
                "Zarana Parekh",
                "Xin Li",
                "Han Zhang",
                "Jason Baldridge",
                "Yonghui Wu"
            ],
            "title": "Scaling autoregressive models for content-rich text-to-image generation",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Eric Zhang",
                "Kai Wang",
                "Xingqian Xu",
                "Zhangyang Wang",
                "Humphrey Shi"
            ],
            "title": "Forget-me-not: Learning to forget in text-to-image diffusion models",
            "year": 2023
        },
        {
            "authors": [
                "Jun-Yan Zhu",
                "Philipp Kr\u00e4henb\u00fchl",
                "Eli Shechtman",
                "Alexei A. Efros"
            ],
            "title": "Generative visual manipulation on the natural image manifold",
            "venue": "In European Conference on Computer Vision,",
            "year": 2016
        },
        {
            "authors": [
                "Angelina Jolie"
            ],
            "title": "In the context of NSFW content, the average number of exposed body parts detected increases from 2.5 to 5 in Figure 11. While CI is not effective in recovering the NSFW concept, we observe that the performance of the erased model on unrelated concepts is affected. C.2 FORGET-ME-NOT (FMN) Concept Erasure Method Details",
            "year": 2023
        },
        {
            "authors": [
                "Zhang"
            ],
            "title": "2023) only provides training scripts for ID concepts",
            "year": 2023
        },
        {
            "authors": [
                "Gal"
            ],
            "title": "Concept Inversion Method",
            "year": 2023
        },
        {
            "authors": [
                "Schuhmann"
            ],
            "title": "Qualitative results of Textual Inversion on 4 film character concepts: TI works significantly worse for concepts not abundant in the LAION 5-B",
            "venue": "Spiderman 2099 and Gwen Stacy compared to Miles Morales and Sonic,",
            "year": 2099
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Motivation. Text-to-image models (Chang et al., 2023; Gafni et al., 2022; Ramesh et al., 2021; 2022; Saharia et al., 2022; Yu et al., 2022; Rombach et al., 2022; Xu et al., 2022) have garnered significant attention due to their exceptional ability to synthesize high-quality images based on text prompts. Such models, most prominently Stable Diffusion (SD) ( Rombach et al. (2022)) and DALL-E 2 ( Ramesh et al. (2022)), have been adopted in a variety of commercial products spanning application realms ranging from digital advertising to graphics to game design. In particular, the opensourcing of Stable Diffusion has democratized the landscape of image generation technology. This shift underlines the growing practical relevance of these models in diverse real-world applications. However, despite their burgeoning popularity, these models come with serious caveats: they have been shown to produce copyrighted, unauthorized, biased, and potentially unsafe content (Mishkin et al., 2022; Rando et al., 2022).\nWhat is the best way to ensure that text-to-image models do not produce sensitive or unsafe concepts? Dataset pre-filtering (AI, 2022) may present the most obvious answer. However, existing filtering procedures are imperfect and may exhibit a large proportion of false negatives. See the extensive studies reported in Birhane et al. (2021) on how LAION-400M, a common dataset used in training text-image models, contains numerous offensive image samples which persist after applying standard NSFW filters.\nEven if perfect data pre-filtering were possible, substantial resources would be required to retrain large models from scratch in response to issues unearthed post-training. As a result, several post hoc concept-erasure methods have emerged of late. Some advocate inference guidance (Schramowski et al., 2023; AUTOMATIC1111, 2022). Others require fine-tuning the weights on an auxiliary subset of training data ( Gandikota et al. (2023a); Heng & Soh (2023); Zhang et al. (2023)). These may be categorized as more practical alternatives to full model-retraining with a stripped-down version of the original training data. Many of these methods are accompanied by public releases of the weights of the \u201csanitized\u201d models. Such concept erasure methods are purported \u201cto permanently remove [targeted concepts] from the weights\u201d; moreover, they are presented as \u201cnot easy to circumvent since [the method] modifies weights\u201d (Gandikota et al., 2023a). An array of results on several test instances across use cases (object removal, artistic style forgetting, avoidance of NSFW content, avoiding likeness of specific people) seem to support the efficacy of these methods.\nOur contributions. Our main contribution in this paper is to show that:\nPost hoc concept erasure in generative models provides a false sense of security.\nWe investigate seven recently announced concept-erasure methods for text-to-image generative models: (i) Erased Stable Diffusion (Gandikota et al., 2023a), (ii) Selective Amnesia (Heng & Soh, 2023), (iii) Forget-me-not (Zhang et al., 2023), (iv) Ablating Concepts (Kumari et al., 2023), (v) Unified Concept Editing (Gandikota et al., 2023b), (vi) Negative Prompt (AUTOMATIC1111, 2022), and (vii) Safe Latent Diffusion (Schramowski et al., 2023). All of these were either published or appeared online in the first 9 months of 2023.\nSomewhat surprisingly, we show that all seven techniques can be circumvented. In all cases, the very same \u201cconcept-erased\u201d models \u2014 with zero extra training or fine-tuning \u2014 may produce the erased concept with a suitably constructed (soft) prompt. Therefore, the seemingly-safe model may still be used to produce sensitive or offensive content. Overall, our results indicate that there may be a fundamental brittleness to post hoc erasure methods, and entirely new approaches for building (and evaluating) safe generative models may be necessary. See Figure 1 for examples.\nTechniques. Our approach stems from the hypothesis that existing concept erasure methods may be, in reality, performing some form of input filtering. More specifically, in these methods, the modified generative models produced by these methods are evaluated on a limited subset of text inputs: the original offending/sensitive text, and related prompts. However, this leaves the model vulnerable to more sophisticated text prompts. In particular, we design individual Concept Inversion (CI) \u201cattack\u201d techniques to discover special word embeddings that can recover erased concepts when fed to the modified model. Through the application of CI, we provide evidence that these unique word embeddings outmaneuver concept erasure methods across various use cases such as facial likeness, artistic style, object-types, and NSFW concepts. Therefore, it is not the case that these concepts have been permanently removed from the model; these still persist, albeit remapped to new embeddings.\nImplications. Our extensive experiments below highlight two key points:\n1. Our results call into question the premise that existing erasure methods (fully) excise concepts from the model. Our results show that this premise is not correct and that the results in these previous works on concept erasure should be scrutinized carefully.\n2. We call for stronger evaluation methodologies for concept erasure methods. Measuring the degree of concept erasure in text-to-image models is tricky, since there are potentially a vast number of prompts that a motivated (and moderately well-equipped) attacker can use as inputs. As a first step to mitigate this issue, we recommend evaluating models in terms of our CI attacks during evaluation, and not merely limited to evaluating over mild variations of the original text prompts.\nOverall, our findings shine a spotlight on the considerable challenges in sanitizing already trained generative AI models (such as Stable Diffusion) and making them safe for wide public use."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "Denoising Diffusion Models. Diffusion models belong to a category of generative models that sample from a distribution via an iterative Markov-based denoising process (Sohl-Dickstein et al., 2015; Ho et al., 2020). The process begins with a sampled Gaussian noise vector, denoted as xT , and undergoes a series of T denoising steps to ultimately restore the final data, referred to as x0. In practical applications, the diffusion model is trained to predict the noise \u03f5t at each timestep, t, utilized to generate the progressively denoised image, xt. Latent diffusion models (LDM) (Rombach et al., 2022) offer improved efficiency by operating in a lower dimensional space learned by an autoencoder. The first component of LDM consists of an encoder E and a decoder D that have been pre-trained on a large collection of images. During the training of LDM, for an image x, the encoder learns to map x into a spatial latent code z = E(x). The decoder maps such latent codes back to the original images such that D(E(x)) \u2248 x. The second component is a diffusion model trained to produce codes within the learned latent space. Given a conditional input c, the LDM is trained using the following objective function:\nL = Ez\u223cE(x),t,c,\u03f5\u223cN (0,1) [ \u2225\u03f5\u2212 \u03f5\u03b8(zt, c, t)\u222522 ] , (1)\nHere zt is the latent code for time t, and \u03f5\u03b8 is the denoising network. At inference time, a random noise tensor is sampled and gradually denoised to produce a latent z0, which is then transformed into an image through the pre-trained decoder such that x\u2032 = D(z0). Ho & Salimans (2022) propose a classifier-free guidance technique is used during inference and requires that the model be jointly trained on both conditional and unconditional denoising. The unconditional and conditional scores are used to create the final latent z0. There, we start with zT \u223c N (0, 1) which is transformed to obtain \u03f5\u0303\u03b8(zt, c, t) = \u03f5\u03b8(zt, t) + \u03b1(\u03f5\u03b8(zt, c, t)\u2212 \u03f5\u03b8(zt, t)) , to get zT\u22121. This process is repeated sequentially until z0 is produced.\nMachine Unlearning. The conventional goal in machine learning is to foster generalization while minimizing reliance on direct memorization. However, contemporary large-scale models possess the capacity for explicit memorization, whether employed intentionally or as an inadvertent byproduct (Carlini et al., 2023; Somepalli et al., 2023b;a). The possibility of such memorization has led to the development of many works in machine unlearning (Golatkar et al., 2020a;b), the core aim of which is to refine the model to behave as though a specific set of training data was never presented.\nMitigating Undesirable Image Generation. Numerous methods have been proposed to discourage the creation of undesirable images by generative models. One initial approach is to exclude certain subsets of the training data. However, this solution can necessitate the retraining of large-scale models from scratch, which can be prohibitive. An alternative put forward by Schramowski et al. (2023); AUTOMATIC1111 (2022) involves manipulating the inference process in a way that steers the final output away from the target concepts. Yet another approach employs classifiers to alter the output (Rando et al., 2022; AI, 2022; Bedapudi, 2022). Since inference guiding methods can be evaded with sufficient access to model parameters (SmithMano, 2022), subsequent works (Gandikota et al., 2023a; Heng & Soh, 2023; Zhang et al., 2023; Kumari et al., 2023; Gandikota et al., 2023b) suggest fine-tuning Stable Diffusion models. Qu et al. (2023) study the capability of generating unsafe images and hateful memes of various text-to-image models. The authors then propose a new classifier that outperforms existing built-in safety checkers of these models.\nDiffusion-based Inversion. Image manipulation with generative networks often requires inversion (Zhu et al., 2016; Xia et al., 2021), the process of finding a latent representation that corresponds to a given image. For diffusion models, (Dhariwal & Nichol, 2021) demonstrate that the DDIM (Song et al., 2021) sampling process can be inverted in a closed-form manner, extracting a latent noise map that will produce a given real image. More recent works (Ruiz et al., 2023; Gal et al., 2023; Shi et al., 2023; Han et al., 2023) try to invert a user-provided concept to a new pseudo-word in the model\u2019s vocabulary. The most relevant approach for our work is Textual Inversion (Gal et al., 2023)) which learns to capture the user-provided concept by representing it through new \u201cwords\u201d in the embedding\nspace of a frozen text-to-image model without changing the model weights. In particular, the authors designate a placeholder string, c\u2217, to represent the new concept the user wishes to learn. They replace the vector associated with the tokenized string with a learned embedding v\u2217, in essence \u201cinjecting\u201d the concept into the model vocabulary. The technique is referred to as Textual Inversion and consists of finding an approximate solution to the following optimization problem:\nv\u2217 = argmin v\nEz\u223cE(x),c\u2217,\u03f5\u223cN (0,1),t [ \u2225\u03f5\u2212 \u03f5\u03b8(zt, c\u2217, t)\u222522 ] ."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "Basic setup and threat model. For the remainder of the paper, we will leverage inversion techniques to design an \u201cattack\u201d on concept-erased models. We assume the adversary has: (1) access to the weights and components of the erased model, (2) knowledge of the erasure method, (3) access to example images with the targeted concept (say via an image search engine), and (4) moderately significant computational power.\nA trivial approach to \u201cun-erase\u201d an erased concept would be via fine-tuning a sanitized model on sufficiently many example images. Therefore, we also assume that: (5) the adversary cannot modify the weights of the erased model.\nTo show that our CI attack is a reliable tool for establishing the existence of concepts in a model, we conduct two experiments to investigate whether Textual Inversion (TI) by itself can generate a concept that the model has not captured during training. If TI can hallucinate totally novel concepts, then even data filtering before training might not be able to avoid producing harmful/copyrighted content. In the first experiment, we compare TI performance on concepts that are better represented in the training data of Stable Diffusion 1.4, versus those that are likely not present. In the second experiment, we conducted a more controlled study by training two diffusion models on MNIST (LeCun et al., 2010) from scratch. We include all the training classes in the first run and exclude one class in the second run. In both experiments, we find that Textual Inversion works significantly worse when the concept is not well represented in the training set of the generative model. See Figure 15 in the Appendix."
        },
        {
            "heading": "4 CIRCUMVENTING CONCEPT ERASURE",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUP",
            "text": "In this section, we examine seven (7) different concept erasure methods. To the best of our knowledge, this list constitutes all the concept erasure methods for Stable Diffusion models published up to September 19, 2023. We design CI procedures tailored to each erasure method that search the space of word embeddings to recreate the (purportedly) erased visual concepts. Importantly, our approach relies solely on the existing components of the post-erasure diffusion models. For these experiments, wherever possible we use the pre-trained models released by the authors unless explicitly stated otherwise; for concepts where erased models were not publicly available, we used public code released as-is by the authors to reproduce their erasure procedure. In each subsection, we start by describing the approach, then show how to attack their approach using Concept Inversion. We interleave these with results, and reflect on their implications. Finally, we show evidence that current concept erasure methods are likely performing input filtering, and demonstrate transferability of the learned word embeddings.\nDue to page limit constraints, we will focus on results for two fine-tuning-based methods (ESD, UCE) as well as two inference-guiding-based methods (NP, SLD). We refer readers to the full set of results for all seven models in Appendix C. Our code is available for reproducibility purposes at https: //anonymous.4open.science/r/circumventing-concept-erasure-BB98/"
        },
        {
            "heading": "4.2 EVALUATION PROTOCOL",
            "text": "For each concept erasure method that we will be discussing below, we initially deploy it to erase 4 concept categories including art style, object, ID, and NSFW content. We use Stable Diffusion 1.4 (SD 1.4) for all our experiments. We assume that the adversary can access a small number of examples of the targeted concept from Google Images; see Appendix for details.\nArt style: We select 6 styles from modern artists and artistic topics that have been reported to have been captured by SD 1.4: the movie series \u201cAjin: Demi Human\u201d, Thomas Kinkade, Tyler Edlin, Van Gogh, Kelly McKernan, and Tyler Edlin. We generate images from the erased models using the prompt \u201cA painting in the style of [artist name]\u201d. After performing CI, we generate images by replacing [artist name] with c\u2217 - the special placeholder string associated with the learned word embedding. In addition to qualitative results, we follow Gandikota et al. (2023a) and conduct a human study to measure the effectiveness of our CI methods. In particular, for each artist, we collect 10 images of art created by those artists from Google Images. We then generate 10 images from the erased model using the standard concept name, and 10 images using CI per style and per concept erasure method. Participants were shown 5 real reference images from the same artist and another image of the same style (either real, from the erased model or from CI). They were then asked to estimate, on a five-point Likert scale, their confidence level that the experimental image has the same style as the reference images. Our study consists of 50 participants, with 96 responses per participant.\nObjects: Following Gandikota et al. (2023a), we investigate the Imagenette (Howard, 2019) dataset which comprises ten easily identifiable classes (cassette player, chain saw, church, etc.) We evaluate CI methods by examining the top-1 predictions of a ResNet-50 Imagenet classifier on 500 generated images. We generate images from the erased models using the prompt \u201cA photo of a [object name]\u201d. For CI, we generate images by replacing [object name] with the special string c\u2217.\nID: Following Heng & Soh (2023), we select \u201cBrad Pitt\u201d and \u201cAngelina Jolie\u201d as identity concepts. We then utilize the GIPHY celebrity detector (Giphy, 2020) for Concept Inversion evaluation. We generate 500 images from the erased models using the prompt \u201cA photo of a [person name]\u201d. For CI, we generate the same number of images by replacing [person name] with the special placeholder string c\u2217.\nNSFW content: Introduced by Schramowski et al. (2023), the I2P dataset comprises 4703 unique prompts with corresponding seeds, which (to date) is the definitive benchmark for measuring the effectiveness of NSFW concept erasure. This process involves generating images using the prompts and seeds and subsequently utilizing NudeNet (Bedapudi, 2022) to classify the images into various nudity classes. The I2P benchmark is effective as its prompts do not necessarily contain words strictly related to nudity. Hence, an effective erasure method on this benchmark requires some degree of robustness to prompt selection. To evaluate each concept erasure method, we first used SD 1.4 to generate 4703 images using the I2P dataset. We used NudeNet to filter out 382 images with detected exposed body parts, on which we performed Concept Inversion. To measure how well the NSFW concept is recovered, we generated another 4703 images using the erased model by using the I2P prompts with the special placeholder string c\u2217 prepended, which are then evaluated by NudeNet."
        },
        {
            "heading": "4.2.1 ERASED STABLE DIFFUSION (ESD)",
            "text": "Concept Erasure Method Details. Gandikota et al. (2023a) fine-tune the pre-trained diffusion U-Net model weights to remove a specific style or concept. The authors reduce the probability of generating an image x based on the likelihood described by the textual description of the concept, i.e. P\u03b8\u2217(x) \u221d P\u03b8(x)P\u03b8(c|x)\u03b7 , where \u03b8\n\u2217 is the updated weights of the diffusion model (U-Net), \u03b8 is the original weights, \u03b7 is a scale power factor, c is the target concept to erase, and P(x) represents\nthe distribution generated by the original model. Based on Tweedie\u2019s formula ( Efron (2011)) and the reparametrization trick ( Ho et al. (2020)), the authors derive a denoising prediction problem as \u03f5\u03b8\u2217(xt, c, t) \u2190 \u03f5(xt, t) \u2212 \u03b7[\u03f5\u03b8(xt, c, t) \u2212 \u03f5\u03b8(xt, t)]. By optimizing this equation, the fine-tuned model\u2019s conditional prediction is steered away from the erased concept when prompted with it. The authors propose two variants of ESD: ESD-x and ESD-u, which finetune the cross-attentions and unconditional layers (non-cross-attention modules) respectively.\nConcept Inversion Method. We employ standard Textual Inversion on fine-tuned Stable Diffusion models from Gandikota et al. (2023a) to learn a new word embedding that corresponds to the concept of the training images. The authors provide pre-trained ESD-x models for all 6 artistic concepts, the pre-trained ESD-u model for NSFW content concept, and training scripts for object concepts. For ID concepts, we train our own ESD-u models prior to CI."
        },
        {
            "heading": "4.2.2 UNIFIED CONCEPT EDITING (UCE)",
            "text": "Concept Erasure Method Details. Latent diffusion models (Rombach et al., 2022) operate on lowdimensional embedding that is modeled with a U-Net generation network. The model incorporates conditioned textual information via embeddings derived from a language model. These embeddings are introduced into the system via cross-attention layers. Inspired by Orgad et al. (2023) and Meng et al. (2023), Gandikota et al. (2023b) edit the U-Net of Stable Diffusion models without training using a closed-form solution conditioned on cross-attention outputs. They update attention weights to induce targeted changes to the keys/values that correspond to specific text embeddings for a set of edited concepts, while minimizing changes to a set of preserved concepts.\nConcept Inversion Method. We employ standard Textual Inversion (Gal et al., 2023) on fine-tuned Stable Diffusion models from UCE (Gandikota et al., 2023b) to learn a new word embedding that corresponds to the (identity) concept of the training images. Gandikota et al. (2023b) provide training scripts to reproduce their art style, object, and NSFW content concepts. For ID concepts, we adapt their publicly posted code to train our own models."
        },
        {
            "heading": "4.2.3 NEGATIVE PROMPT (NP)",
            "text": "Concept Erasure Method Details. Negative Prompt (NP) is a guiding inference technique used in the Stable Diffusion community (AUTOMATIC1111, 2022). Instead of updating the weights of the original model, it replaces the unconditional score with the score estimate conditioned on the erased concept in classifier-free guidance. Gandikota et al. (2023a) illustrate how NP can prevent the image generation of targeted artistic concepts.\nConcept Inversion Method. In our experiments, vanilla Textual Inversion was not able to circumvent NP. We modify the objective function for Textual Inversion to:\nv\u2217 = argmin v\nEz\u223cE(x),c,\u03f5\u223cN (0,1),t [ \u2225(\u03f5\u03b8(zt, t) + \u03b1(\u03f5\u03b8(zt, c, t)\u2212 \u03f5\u03b8(zt, t))\n\u2212 (\u03f5\u03b8(zt, c, t) + \u03b1(\u03f5\u03b8(zt, c\u2217, t)\u2212 \u03f5\u03b8(zt, c, t))\u222522 ] ,\nwhere c is the target concept. Our method learns a word embedding associated with the special string c\u2217 such that the predicted noise from NP equals the true predicted noise using classifier-free guidance."
        },
        {
            "heading": "4.2.4 SAFE LATENT DIFFUSION (SLD)",
            "text": "Concept Erasure Method Details. Safe Latent Diffusion (Schramowski et al., 2023) is an inference guiding method that is a more sophisticated version of NP, where the second unconditional score term is replaced with a safety guidance term. Instead of being constant like NP, this term is dependent on: (1) the timestep, and (2) the distance between the conditional score of the given prompt and the conditional score of the target concept at that timestep. In particular, SLD modifies the score estimates during inference as \u03f5\u03b8(xt, c, t)\u2190 \u03f5\u03b8(xt, t) + \u00b5[\u03f5\u03b8(xt, c, t)\u2212 \u03f5\u03b8(xt, t)\u2212 \u03b3(zt, c, cS)]. We refer the reader to the Appendix C and the original work by Schramowski et al. (2023) for a more detailed explanation of \u03b3(zt, c, cS). Note that SLD does not modify the weights of the original\ndiffusion models, but only adjusts the sampling process. By varying the hyperparameters of the safety guidance term, the authors propose 4 variants of SLD: SLD-Weak, SLD-Medium, SLD-Strong, and SLD-Max. A more potent variant of SLD yields greater success in erasing undesirable concepts.\nConcept Inversion Method. In our experimentation, we encountered an issue akin to the one with Negative Prompt when applying vanilla Textual Inversion. Furthermore, the guidance term of SLD at timestep t depends on that at the previous timestep. This recursive dependency implies that in order to calculate the guidance term within our inversion algorithm, it becomes necessary to keep a record of the preceding terms for optimization purposes. Given the high number of denoising steps involved, such a process could result in significant memory usage, presenting an efficiency problem. To address this issue, we propose a new strategy to perform Concept Inversion. Instead of having a constant safety guidance term, SLD requires storing all the guidance terms from step 1 to step t\u2212 1 to calculate the one at timestep t. Since doing so will be memory-intensive, we instead approximate it by calculating only a subset of guidance terms at evenly spaced timesteps between 1 and t. We can then learn a word embedding that counteracts the influence of the safety guidance term. The pseudocode for our CI scheme can be found in Appendix C."
        },
        {
            "heading": "4.3 RESULTS AND DISCUSSION",
            "text": "On the plus side, our experiments confirm that whenever the target concept is explicitly mentioned in the input prompt, all sevem concept erasure methods are effective. Therefore, these methods can indeed provide protection against obviously-offensive text inputs. We confirm this even for concept categories that the methods did not explore in their corresponding publications.\nHowever, on the minus side, all seven methods can be fully circumvented using our CI attacks. In other words, these erasure methods are only effective against their chosen inputs.\nFor artistic concepts, our human study in Figure 2 shows an average (across all methods) score of 1.31 on Likert rating on images generated by the erased model. This score expectedly increases to 3.85 when Concept Inversion is applied. Figure 3 displays images generated from the erased model and CI side-by-side, which shows that CI is effective in recovering the erased style.\nFor object concepts, the average accuracy across all methods of the pre-trained classifier in predicting the erased concept increases from 13.68 to 68.8 using our attack (Table 1). This is supported by qualitative results shown in Figure 6.\nFor ID concepts, the average accuracy across all methods of the GIPHY detector increases from 12.52 to 67.13 in Table 3.\nFor NSFW concepts, Figure 5 suggests that CI can recover the NSFW concept, which is shown by an increase from 26.2 to 170.93 in the average number of detected exposed body parts.\nAmong the 7 concept erasure methods, the most challenging one to circumvent is SLD. Our Concept Inversion technique manages to counteract the influence of SLD-Weak, SLD-Medium, and even SLDStrong under most circumstances. Among the variants, SLD-Max proves to be the most challenging to circumvent. However, this variant comes with a drawback: it has the potential to entirely transform the visual semantics of the generated images. We provide several more results of variants of SLD in the Appendix C. In our proposed CI scheme for SLD, we observed that more GPU memory can give us better approximations of the safety guidance terms and therefore counteract their influence."
        },
        {
            "heading": "4.4 TRANSFERABILITY AND USEABILITY OF OUR ATTACKS",
            "text": "Intriguingly, we show that the learned special tokens derived from CI can be applied to the original SD 1.4 model to generate images of the erased concept. Figure 7 demonstrates our results. This lends further evidence that current concept erasure methods are merely suppressing the generation of the targeted concept when conditioned on the input embeddings corresponding to the concept text. However, there exist word embeddings that trigger the generation of the erased concept in SD 1.4 that also work on the erased model. This means that some degree of input filtering is occurring. Additionally, we provide evidence that the learned word embeddings through CI are useable in practice. Following Gal et al. (2023) and study the reconstruction effectiveness and editability of these embeddings. In particular, we generate two sets of images using CI with each concept erasure method: first, a set of generated images using the inverted concept; second, a set of generated images of the inverted concept in different scenes. We evaluate using CLIP (Radford et al., 2021) how well the inverted concepts are produced with the erased models, and how transferable are they to different scenes. In both cases, the erased models achieve performance similar to that of the original Stable-Diffusion model, pointing to the models are in principle still being able to produce the seemingly erased concepts."
        },
        {
            "heading": "5 CONCLUSIONS",
            "text": "As text-to-image generative AI models continue to gain popularity and usage among the public, issues surrounding the ability to generate proprietary, sensitive, or unsafe images come to the fore. Numerous methods have been proposed in the recent past that claim to erase target concepts from trained generative models, and ostensibly make them safe(r) for public consumption.\nIn this paper, we take a step back and scrutinize these claims. We show that post-hoc erasure methods have not excised the targeted concepts; fairly straightforward \u201cattack\u201d procedures can be used to design special prompts that regenerate the unsafe outputs.\nAs future work, it might be necessary to fundamentally analyze why the \u201cinput filtering\u201d phenomenon seems to be occurring in all these recent methods, despite the diversity of algorithmic techniques involved in each of them. Such an understanding could facilitate the design of better methods that improve both the effectiveness and robustness of concept erasure."
        },
        {
            "heading": "APPENDIX",
            "text": ""
        },
        {
            "heading": "A LIMITATIONS AND DISCUSSION",
            "text": "The attack scenario described in Section 3 is closer to the white-box setting. However, despite limited practicality, our attacks offers valuable insights into the intrinsic limitations and behaviors of current concept erasure methods. Our experimental results suggest that input-filtering is occurring among concept erasure methods. Let x be the observed data, i.e. the images containing the concept we wish to erase, and \u03b8 be the weights of our conditional generative model. From a likelihood-based perspective, we believe current erasure methods are minimizing the conditional probability P\u03b8(x|y) for only certain inputs y. In order to make the generative model more robust, we may want to directly minimize the marginal probability P\u03b8(x), which is unfortunately intractable to calculate in practice. One possible solution is to perform adversarial training and minimize probability conditioned on the adversarial inputs. Nevertheless, it is also crucial to maintain the overall generative performance of the model on unrelated concepts. For instance, Figure 10 demonstrates that the erased models that are more robust against Concept Inversion can create artifacts or generate images not aligned with the prompt."
        },
        {
            "heading": "B TRAINING DETAILS",
            "text": "For the training of Concept Inversion, we use 6 samples for art style concept, 30 samples for object concept, and 25 samples for ID concept. For all our Concept Inversion experiments, unless mentioned otherwise, we perform training on one A100 GPU with a batch size of 4, and a learning rate of 5e\u2212 03. We optimize the word embedding for 1,000 steps while keeping the weights of the erased models frozen. The CI training procedure is the same across erasure methods and concepts, except for ID concepts we optimize for 5,000 steps, and for SLD we we train for 1,000 steps with batch size 1."
        },
        {
            "heading": "B.1 ERASED STABLE DIFFUSION (ESD)",
            "text": "For artistic concepts and NSFW concept, we used the pre-trained models released by the authors. For object concepts, we trained ESD-u models using the suggested parameters by the authors. For ID concepts, we trained ESD-u models using the parameters for object concepts."
        },
        {
            "heading": "B.2 UNIFIED CONCEPT EDITING (UCE)",
            "text": "For artistic concepts, object concepts, and NSFW concept, we trained the erased models using the training script provided by the author. For ID concepts, we used the same script without the preservation loss term."
        },
        {
            "heading": "B.3 SELECTIVE AMNESIA (SA)",
            "text": "For artistic concepts, generated images with photorealism style as the surrogate dataset. We then applied SA to finetune the cross-attention layers of SD 1.4 for 200 epochs. We used the default parameters for training. For object concepts, we used a similar training procedure as above but used images of kangaroos (a class in ImageNet) as the surrogate dataset. Moreover, we finetuned the unconditional layers (non-cross-attention modules) of SD 1.4. For ID concepts and NSFW concept, we used the training script provided by the authors."
        },
        {
            "heading": "B.4 ABLATING CONCEPTS (AC)",
            "text": "For artistic concepts, we used the training scripts provided by the authors to train the erased models. For object concepts, we also used the training script provided by the authors with the anchor distribution set to images of kangaroos. For ID concepts, we used the training script for object concepts and set the anchor distribution to images of middle-aged people. For NSFW concept, we used the training script for object concepts and set the anchor distribution to images of people wearing clothes."
        },
        {
            "heading": "B.5 FORGET-ME-NOT (FMN)",
            "text": "For artistic concepts, we employed FMN to finetune the cross-attention layers for 5 training steps with a learning rate of 1e\u2212 4. The reference images we used are generated paintings with a photorealism style. For object concepts, we finetuned the unconditional layers (non-cross-attention modules) for 100 epochs with a learning rate of 2e\u2212 06. The reference images we used are generated photos of kangaroos. For ID concepts, we finetuned the unconditional layers for 120 epochs with a learning rate of 2e \u2212 06. The reference images we used are generated photos of middle-aged people. For NSFW concept, we finetuned unconditional layers for 100 epochs with a learning rate of 2e\u2212 06. The reference images we used are generated photos of people wearing clothes. Since the authors do not provide any training details, we tuned the number of epochs and learning rate until we observed effective erasure."
        },
        {
            "heading": "B.6 NEGATIVE PROMPT (NP)",
            "text": "To apply Negative Prompt to SD 1.4, we changed the negative prompt argument to the concept we would like to erase during inference using codebase from Hugging Face."
        },
        {
            "heading": "B.7 SAFE LATENT DIFFUSION (SLD)",
            "text": "We used to authors\u2019 public code to apply SLD to Stable Diffusion 1.4. During CI, we sampled 35 evenly spaced timesteps between 1 and 1000. The range between the smallest and largest timesteps is 90. We set guidance scale sS = 5000, warm-up step \u03b4 = 0, threshold \u03bb = 1.0, momentum scale sm = 0.5, and momentum beta \u03b2m = 0.7. This is the hyperparameters for SLD-Max."
        },
        {
            "heading": "C RESULTS ON OTHER CONCEPT ERASURE METHODS",
            "text": ""
        },
        {
            "heading": "C.1 SELECTIVE AMNESIA (SA)",
            "text": "Concept Erasure Method Details. Heng & Soh (2023) pose concept erasure as a problem of continual learning, taking inspiration from Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) and Generative Replay (Shin et al., 2017). Consider a dataset D that can be partitioned as D = Df \u222a Dr = {(x(n)f , c (n) f )} Nf n=1 \u222a {(x (n) r , c (n) r )}Nrn=1, where Df is the data to forget and Dr is the data to remember. The underlying distribution of D is given by p(x, c) = p(x|c)p(c). In the case of concept erasure, Df contains the images of the concept we would like to erase, and Dr consists of images we want to preserve the model performance. They maximize the following objective function for concept erasure:\nL = \u2212Ep(x|c)p(cf )[log p(x|\u03b8 \u2217, c)]\u2212 \u03bb \u2211 i Fi 2 (\u03b8i \u2212 \u03b8\u2217i )2 + Ep(x|c)p(xr)[log p(x|\u03b8 \u2217, c)], (2)\nwhere F is the Fisher information matrix and the third term is a generative replay term to prevent model degradation on samples that do not contain the erased concept. In practice, the authors optimize Eq. 2 by substituting the likelihood terms with the standard ELBOs. Moreover, they observe that directly minimizing the ELBO can lead to poor results. Hence, they propose to maximize the log-likelihood of a surrogate distribution of the concept to forget, q(x|cf ) \u0338= p(x|cf ). This is done by replacing \u2212Ep(x|c)p(cf )[log p(x|\u03b8\u2217, c)] with Eq(x|c)p(cf )[log p(x|\u03b8\u2217, c)]. Intuitively, this fine-tuning will result in a generative model that will produce images according to the surrogate distribution when conditioned on cf .\nConcept Inversion Method. We employ standard Textual Inversion (Gal et al., 2023) on fine-tuned Stable Diffusion models from Heng & Soh (2023) to learn a new word embedding that corresponds to the concept of the training images. The authors provide training scripts for Brad Pitt, Angelina Jolie, and NSFW content concepts. In particular, they use images of clowns and middle-aged people as the surrogate dataset for ID concepts, and images of people wearing clothes for NSFW content concept. For other concepts, we train our own models by appropriately modifying their training scripts prior to CI.\nExperimental Results. For artistic concepts, our human study in Figure 2 shows an average of 1.12 Likert score on images generated by the erased models. This score is increased to 3.55 when Concept Inversion is applied. Qualitative results in Figure 13 demonstrate that CI can recover the erased style. When evaluated on object concepts, Table 8 indicates that the average accuracy of the pre-trained classifier increases from 10.6 to 40.3. With respect to ID concepts, the accuracy of GIPHY detector increases from 4.8 to 67.1 in Table 6. Figure 8 shows that CI can still generate images of Brad Pitt and Angelina Jolie. In the context of NSFW content, the average number of exposed body parts detected increases from 2.5 to 5 in Figure 11. While CI is not effective in recovering the NSFW concept, we observe that the performance of the erased model on unrelated concepts is affected."
        },
        {
            "heading": "C.2 FORGET-ME-NOT (FMN)",
            "text": "Concept Erasure Method Details. Zhang et al. (2023) propose fine-tuning the cross-attention layers of Stable Diffusion\u2019s U-Net to map the erased concept to that of a set of reference images. The authors first locate the word embeddings associated with the forgetting concept. This can be done by tokenizing a pre-defined prompt or through Textual Inversion. They then compute the attention maps between the input features and these embeddings, and minimize the Frobenius norm of attention maps and backpropagate the network. Algorithm 1 describes the concept erasure process.\nAlgorithm 1 Forget-Me-Not on diffuser Require: Context embeddings C containing the forgetting concept, embedding locationsN of the forgetting\nconcept, reference imagesR of the forgetting concept, diffuser G\u03b8 , diffusion step T . repeat\nt \u223c Uniform([1...T ]); \u03f5 \u223c N (0, I) ri \u223c R; ej \u223c C;nj \u223c N x0 \u2190 ri xt \u2190 \u221a \u03b1tx0 + \u221a 1\u2212 \u03b1t\u03f5 \u25b7 \u03b1t : noise variance schedule x,t\u22121, At \u2190 G\u03b8(xt, ej , t) \u25b7 At : all attention maps L \u2190 \u2211 at\u2208At \u2225a [nj] t \u22252 \u25b7 L : attention resteering loss\nUpdate \u03b8 by descending its stochastic gradient\u2207\u03b8L until Concept forgotten\nConcept Inversion Method. We employ standard Textual Inversion Gal et al. (2023) on fine-tuned Stable Diffusion models from Zhang et al. (2023) to learn a new word embedding that corresponds to the concept of the training images. Zhang et al. (2023) only provides training scripts for ID concepts. Hence, we have to train our models on other concepts using their public code prior to CI.\nExperimental Results. For artistic concepts, our human study in Figure 2 shows an average of 1.43 Likert score on images generated by the erased models. This score is increased to 3.66 when Concept Inversion is applied. Our qualitative results in Figure 13 also shows that CI can reconstruct the erased artistic concept. When evaluated on object concepts, Table 8 indicates that the average accuracy of the pre-trained classifier increases from 3.9 to 44.6. In cases where CI is not as effective (chain saw, church, english springer), we observe that the erased model has poor performance in generating unrelated images. With respect to ID concepts, the accuracy of GIPHY detector increases\nfrom 0.3 to 47.0 in Table 6. Additionally, in the context of NSFW content, the average number of exposed body parts detected increases from 1.88 to 52.13 in Figure 11."
        },
        {
            "heading": "C.3 ABLATING CONCEPTS (AC)",
            "text": "Concept Erasure Method Details. Kumari et al. (2023) perform concept erasure by overwriting the target concept with an anchor weight, which can be a superset or a similar concept. The authors propose two variants to erase the target concept, namely Model-based concept ablation and Noisebased concept ablation. In the former method, the authors fine-tune the pre-trained Stable Diffusion U-Net model by minimizing the following objective function:\nargmin \u03b8\u2217\nEz\u223cE,z\u2217\u223cE(x\u2217),c,c\u2217,\u03f5\u223cN (0,1),t [ wt\u2225\u03f5\u03b8\u2217(zt, c, t).sg()\u2212 \u03f5\u03b8\u2217(z\u2217t , c\u2217, t)\u222522 ] .\nwhere wt is a time-dependent weight, \u03b8\u2217 is initialized with the pre-trained weight, x\u2217 is the (generated) images with the anchor concept c\u2217, and .sg() is the stop-gradient operation. For the second variant, the authors redefine the ground truth text-image pairs as <a target concept text prompt, image of the anchor concept>. The authors then fine-tune the model on the redefined pairs with the standard diffusion training loss. In addition, they add an optional standard diffusion loss term on the anchor concept image and corresponding texts as a regularization as the target text prompt can consist of the anchor concept. In both variants, the authors propose to fine-tune on different parts of Stable Diffusion: (1) cross-attention, (2) embedding: the text embedding the text transformer, (2) full weights: all parameters of U-Net.\nConcept Inversion Method. We employ standard Textual Inversion Gal et al. (2023) on fine-tuned Stable Diffusion models from AC ( Kumari et al. (2023)) to learn a new word embedding that corresponds to the (identity) concept of the training images. Kumari et al. (2023) provide training scripts for art style and object concepts. Consequently, we extend their public code to erase the remaining concepts.\nExperimental Results. For artistic concepts, our human study in Figure 2 shows an average of 1.47 Likert score on images generated by the erased models. This score is increased to 3.84 when Concept Inversion is applied. When evaluated on object concepts, Table 8 indicates that the average accuracy of the pre-trained classifier increases from 0.04 to 45.5. With respect to ID concepts, the accuracy of GIPHY detector increases from 1.9 to 76.6 in Table 6. Additionally, in the context of NSFW content, the average number of exposed body parts detected increases from 12.5 to 252.63 in Figure 11."
        },
        {
            "heading": "C.4 SAFE LATENT DIFFUSION (SLD)",
            "text": "Concept Erasure Method Details. Safe Latent Diffusion Schramowski et al. (2023) is an inference guiding method, in which during inference the score estimates for the x\u2212prediction are modified as:\n\u03f5\u03b8(xt, c, t)\u2190 \u03f5\u03b8(xt, t) + \u00b5[\u03f5\u03b8(xt, c, t)\u2212 \u03f5\u03b8(xt, t)\u2212 \u03b3(zt, c, cS)] (3) Where cS is the concept we would like to steer the generated images away from, \u03b3 is the safety guidance term \u03b3(zt, c, cS) = \u03b2(c, cS ; sS , \u03bb)(\u03f5\u03b8(xt, c, t)\u2212 \u03f5\u03b8(xt, t)), and \u03b2 applies a guidance scale sS element-wise by considering those dimensions of the prompt conditioned estimate that would guide the generation process toward the erased concept: \u03b2(c, cS ; sS , \u03bb) ={\nmax{1, |sS(\u03b8\u03f5(xt, ct)\u2212 \u03f5(xt, cS , t)|}, where \u03f5\u03b8(xt, c, t)\u2296 \u03f5\u03b8(xt, cS , t) \u2264 \u03bb, 0, otherwise.\nLarger threshold \u03bb and sS lead to a more substantial shift away from prompt text and in the opposite direction of the erased concept. Moreover, to facilitate a balance between removal of undesired content from the generated images and minimizing the changes introduced, the authors introduce two modifications. First, they accelerate guidance over time by utilizing a momentum term vt for the safety guidance \u03b3. In particular, \u03b3t, denoting \u03b3(zt, c, cS), is defined as:\n\u03b3t = \u03b2(c, cS ; sS , \u03bb)(\u03f5\u03b8(xt, c, t)\u2212 \u03f5\u03b8(xt, t)) + smvt with sm \u2208 [0, 1] and vt+1 = \u03b6mvt + (1 \u2212 \u03b6m)\u03b3t. Here, v0 = 0 and \u03b2m \u2208 [0, 1), with larger \u03b6m resulting in less volatile changes in momentum. Note that SLD does not modify the weights of the original diffusion models, but only adjusts the sampling process. By varying the hyperparameters, the authors propose 4 variants of SLD: SLD-Weak, SLD-Medium, SLD-Strong, and SLD-Max.\nConcept Inversion Method. The pseudocode for our CI method on SLD is shown in Alg. 2.\nAlgorithm 2 Concept Inversion for SLD Require: Concept placeholder string c\u2217, erased concept c, number of iterations N\nfor i = 1 to N do Sample x0 from training data. Randomly select m \u2265 1 and n \u2264 T \u03f5 \u223c N (0, 1); z0 \u2190 E(x0); \u03b3(z0,m, cS)\u2190 0; vm \u2190 0 for t = m to n by k do \u25b7 iterating t from m to k with an increment of k L \u2190 [ \u2225(\u03f5\u03b8(zt, t) + \u03b1(\u03f5\u03b8(zt, cS , t) \u2212 \u03f5\u03b8(zt, t)) \u2212 (\u03f5\u03b8(xt, t) + \u00b5[\u03f5\u03b8(xt, c\u2217, t) \u2212 \u03f5\u03b8(xt, t) \u2212\n\u03b3(zt, c, cS)])\u222522 ]\nvt+1 \u2190 \u03b6mvt + (1\u2212 \u03b6m)\u03b3t \u03b3t+1 \u2190 \u03b2(c, cS ; sS , \u03bb)(\u03f5\u03b8(xt, c, t)\u2212 \u03f5\u03b8(xt, t)) + smvt Update v\u2217 by descending its stochastic gradient\u2207v\u2217L\nend for end for"
        },
        {
            "heading": "C.5 FAILURE CASES FOR CI",
            "text": "In our experiments, we observed a small handful of cases where CI was not able to recover the erased concepts. However, as shown in Figure 10, these models can have poor image generation quality even on prompts not related to the erased concepts. Hence, this shows that while they are more robust to CI, such models are not very useful in real-world applications."
        },
        {
            "heading": "D DETAILS: CHECKING FOR EXISTENCE OF CONCEPTS",
            "text": "To check for the concepts\u2019 existence in LAION-5B, we utilize CLIP Retrieval ( Beaumont (2022)) to search for images nearest to the concept image in the CLIP image embedding space. Our searches indicate that the images of Sonic and Miles Morales appear a lot with multiple duplicates, while images of Gwen Stacy and Spiderman 2099 appear few to none."
        },
        {
            "heading": "E ADDITIONAL QUALITATIVE RESULTS",
            "text": ""
        },
        {
            "heading": "F ADDITIONAL RESULTS FOR STABLE DIFFUSION 2.0",
            "text": ""
        }
    ],
    "title": "CIRCUMVENTING CONCEPT ERASURE METHODS FOR TEXT-TO-IMAGE GENERATIVE MODELS",
    "year": 2023
}