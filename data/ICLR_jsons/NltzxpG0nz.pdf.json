{
    "abstractText": "Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to \u201ca blindfolded text-based game.\u201d Consequently, LLMbased agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model to address this limitation. Steve-Eye integrates the LLM with a visual encoder to process visual-text inputs and generate multimodal feedback. We adopt a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, enabling our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks, then carry out experiments from a wide range of perspectives to validate our model\u2019s capability to strategically act and plan. Codes and datasets will be released.",
    "authors": [],
    "id": "SP:6d9a604d30a8aa69bfffdbb4a7a8e43311829bac",
    "references": [
        {
            "authors": [
                "Michael Ahn",
                "Anthony Brohan",
                "Noah Brown",
                "Yevgen Chebotar",
                "Omar Cortes",
                "Byron David",
                "Chelsea Finn",
                "Chuyuan Fu",
                "Keerthana Gopalakrishnan",
                "Karol Hausman"
            ],
            "title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "venue": "arXiv preprint arXiv:2204.01691,",
            "year": 2022
        },
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Jeff Donahue",
                "Pauline Luc",
                "Antoine Miech",
                "Iain Barr",
                "Yana Hasson",
                "Karel Lenc",
                "Arthur Mensch",
                "Katherine Millican",
                "Malcolm Reynolds"
            ],
            "title": "Flamingo: a visual language model for few-shot learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Anas Awadalla",
                "Irena Gao",
                "Josh Gardner",
                "Jack Hessel",
                "Yusuf Hanafy",
                "Wanrong Zhu",
                "Kalyani Marathe",
                "Yonatan Bitton",
                "Samir Gadre",
                "Shiori Sagawa"
            ],
            "title": "Openflamingo: An opensource framework for training large autoregressive vision-language models",
            "venue": "arXiv preprint arXiv:2308.01390,",
            "year": 2023
        },
        {
            "authors": [
                "Rishi Bommasani",
                "Drew A Hudson",
                "Ehsan Adeli",
                "Russ Altman",
                "Simran Arora",
                "Sydney von Arx",
                "Michael S Bernstein",
                "Jeannette Bohg",
                "Antoine Bosselut",
                "Emma Brunskill"
            ],
            "title": "On the opportunities and risks of foundation models",
            "venue": "arXiv preprint arXiv:2108.07258,",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Danny Driess",
                "Fei Xia",
                "Mehdi SM Sajjadi",
                "Corey Lynch",
                "Aakanksha Chowdhery",
                "Brian Ichter",
                "Ayzaan Wahid",
                "Jonathan Tompson",
                "Quan Vuong",
                "Tianhe Yu"
            ],
            "title": "Palm-e: An embodied multimodal language model",
            "venue": "arXiv preprint arXiv:2303.03378,",
            "year": 2023
        },
        {
            "authors": [
                "Patrick Esser",
                "Robin Rombach",
                "Bjorn Ommer"
            ],
            "title": "Taming transformers for high-resolution image synthesis",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Fallman"
            ],
            "title": "Design-oriented human-computer interaction",
            "venue": "In Proceedings of the SIGCHI conference on Human factors in computing systems,",
            "year": 2003
        },
        {
            "authors": [
                "Linxi Fan",
                "Guanzhi Wang",
                "Yunfan Jiang",
                "Ajay Mandlekar",
                "Yuncong Yang",
                "Haoyi Zhu",
                "Andrew Tang",
                "De-An Huang",
                "Yuke Zhu",
                "Anima Anandkumar"
            ],
            "title": "Minedojo: Building open-ended embodied agents with internet-scale knowledge",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tanmay Gupta",
                "Aniruddha Kembhavi"
            ],
            "title": "Visual programming: Compositional visual reasoning without training",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "William H Guss",
                "Brandon Houghton",
                "Nicholay Topin",
                "Phillip Wang",
                "Cayden Codel",
                "Manuela Veloso",
                "Ruslan Salakhutdinov"
            ],
            "title": "Minerl: A large-scale dataset of minecraft demonstrations",
            "year": 1907
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Rongjie Huang",
                "Mingze Li",
                "Dongchao Yang",
                "Jiatong Shi",
                "Xuankai Chang",
                "Zhenhui Ye",
                "Yuning Wu",
                "Zhiqing Hong",
                "Jiawei Huang",
                "Jinglin Liu"
            ],
            "title": "Audiogpt: Understanding and generating speech, music, sound, and talking head",
            "venue": "arXiv preprint arXiv:2304.12995,",
            "year": 2023
        },
        {
            "authors": [
                "Shaohan Huang",
                "Li Dong",
                "Wenhui Wang",
                "Yaru Hao",
                "Saksham Singhal",
                "Shuming Ma",
                "Tengchao Lv",
                "Lei Cui",
                "Owais Khan Mohammed",
                "Qiang Liu"
            ],
            "title": "Language is not all you need: Aligning perception with language models",
            "venue": "arXiv preprint arXiv:2302.14045,",
            "year": 2023
        },
        {
            "authors": [
                "Wenlong Huang",
                "Pieter Abbeel",
                "Deepak Pathak",
                "Igor Mordatch"
            ],
            "title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Wenlong Huang",
                "Fei Xia",
                "Ted Xiao",
                "Harris Chan",
                "Jacky Liang",
                "Pete Florence",
                "Andy Zeng",
                "Jonathan Tompson",
                "Igor Mordatch",
                "Yevgen Chebotar"
            ],
            "title": "Inner monologue: Embodied reasoning through planning with language models",
            "venue": "arXiv preprint arXiv:2207.05608,",
            "year": 2022
        },
        {
            "authors": [
                "Eric Kolve",
                "Roozbeh Mottaghi",
                "Winson Han",
                "Eli VanderBilt",
                "Luca Weihs",
                "Alvaro Herrasti",
                "Matt Deitke",
                "Kiana Ehsani",
                "Daniel Gordon",
                "Yuke Zhu"
            ],
            "title": "Ai2-thor: An interactive 3d environment for visual ai",
            "venue": "arXiv preprint arXiv:1712.05474,",
            "year": 2017
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi"
            ],
            "title": "Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597,",
            "year": 2023
        },
        {
            "authors": [
                "Shuang Li",
                "Xavier Puig",
                "Chris Paxton",
                "Yilun Du",
                "Clinton Wang",
                "Linxi Fan",
                "Tao Chen",
                "De-An Huang",
                "Ekin Aky\u00fcrek",
                "Anima Anandkumar"
            ],
            "title": "Pre-trained language models for interactive decisionmaking",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee"
            ],
            "title": "Visual instruction tuning",
            "venue": "arXiv preprint arXiv:2304.08485,",
            "year": 2023
        },
        {
            "authors": [
                "Joon Sung Park",
                "Joseph C O\u2019Brien",
                "Carrie J Cai",
                "Meredith Ringel Morris",
                "Percy Liang",
                "Michael S Bernstein"
            ],
            "title": "Generative agents: Interactive simulacra of human behavior",
            "venue": "arXiv preprint arXiv:2304.03442,",
            "year": 2023
        },
        {
            "authors": [
                "Shishir G Patil",
                "Tianjun Zhang",
                "Xin Wang",
                "Joseph E Gonzalez"
            ],
            "title": "Gorilla: Large language model connected with massive apis",
            "venue": "arXiv preprint arXiv:2305.15334,",
            "year": 2023
        },
        {
            "authors": [
                "Zhiliang Peng",
                "Wenhui Wang",
                "Li Dong",
                "Yaru Hao",
                "Shaohan Huang",
                "Shuming Ma",
                "Furu Wei"
            ],
            "title": "Kosmos-2: Grounding multimodal large language models to the world",
            "venue": "arXiv preprint arXiv:2306.14824,",
            "year": 2023
        },
        {
            "authors": [
                "Jenny Preece",
                "Yvonne Rogers",
                "Helen Sharp",
                "David Benyon",
                "Simon Holland",
                "Tom Carey"
            ],
            "title": "Human-computer interaction",
            "venue": "Addison-Wesley Longman Ltd.,",
            "year": 1994
        },
        {
            "authors": [
                "Xavier Puig",
                "Kevin Ra",
                "Marko Boben",
                "Jiaman Li",
                "Tingwu Wang",
                "Sanja Fidler",
                "Antonio Torralba"
            ],
            "title": "Virtualhome: Simulating household activities via programs",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Manolis Savva",
                "Abhishek Kadian",
                "Oleksandr Maksymets",
                "Yili Zhao",
                "Erik Wijmans",
                "Bhavana Jain",
                "Julian Straub",
                "Jia Liu",
                "Vladlen Koltun",
                "Jitendra Malik"
            ],
            "title": "Habitat: A platform for embodied ai research",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "D\u0131\u0301dac Sur\u0131\u0301s",
                "Sachit Menon",
                "Carl Vondrick"
            ],
            "title": "Vipergpt: Visual inference via python execution for reasoning",
            "venue": "arXiv preprint arXiv:2303.08128,",
            "year": 2023
        },
        {
            "authors": [
                "Vicuna Team"
            ],
            "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90quality",
            "venue": "https://vicuna.lmsys.org/,",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "arXiv preprint arXiv:2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Guanzhi Wang",
                "Yuqi Xie",
                "Yunfan Jiang",
                "Ajay Mandlekar",
                "Chaowei Xiao",
                "Yuke Zhu",
                "Linxi Fan",
                "Anima Anandkumar"
            ],
            "title": "Voyager: An open-ended embodied agent with large language models",
            "venue": "arXiv preprint arXiv:2305.16291,",
            "year": 2023
        },
        {
            "authors": [
                "Zihao Wang",
                "Shaofei Cai",
                "Anji Liu",
                "Xiaojian Ma",
                "Yitao Liang"
            ],
            "title": "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents",
            "venue": "arXiv preprint arXiv:2302.01560,",
            "year": 2023
        },
        {
            "authors": [
                "Haoqi Yuan",
                "Chi Zhang",
                "Hongcheng Wang",
                "Feiyang Xie",
                "Penglin Cai",
                "Hao Dong",
                "Zongqing Lu"
            ],
            "title": "Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks",
            "venue": "arXiv preprint arXiv:2303.16563,",
            "year": 2023
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models",
            "venue": "arXiv preprint arXiv:2205.01068,",
            "year": 2022
        },
        {
            "authors": [
                "Chong Zhou",
                "Chen Change Loy",
                "Bo Dai"
            ],
            "title": "Extract free dense labels from clip",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Xizhou Zhu",
                "Yuntao Chen",
                "Hao Tian",
                "Chenxin Tao",
                "Weijie Su",
                "Chenyu Yang",
                "Gao Huang",
                "Bin Li",
                "Lewei Lu",
                "Xiaogang Wang"
            ],
            "title": "Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory",
            "venue": "arXiv preprint arXiv:2305.17144,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "1 INTRODUCTION\nDeveloping embodied agents that can adapt to the open world has long been a substantial challenge (Kolve et al., 2017; Savva et al., 2019). Recently, the rapid progress of large language models (LLMs) (OpenAI, 2022; Touvron et al., 2023a) has shown their potential to serve as a general-purpose assistant. Driven by these pre-trained LLMs, recently proposed agents (Yuan et al., 2023; Wang et al., 2023a;b; Zhu et al., 2023) have managed to extract world knowledge and reasoning capabilities from LLMs, allowing them to become self-driven. Thereby these agents are capable of generating executable policies or plans for a wide range of skills and tasks in an open-ended world.\nWhile current attempts to integrate LLMs show promise in developing a generic embodied agent, these efforts primarily translate the entire world into text, which overlooks the multifaceted richness of diverse visual reality\nand turns interacting with the environment into something akin to \u201ca blindfolded text-based game.\u201d Consequently, such text-only agents often face difficulties when it comes to effectively and intuitively representing the world. Imagine a situation where you request your agent to shop for a pair\nof shoes. Would you prefer to send the agent a picture of the shoes or provide a lengthy description of the shoes to convey their appearance? Undoubtedly, you would opt for the former choice.\nIn fact, the agent\u2019s reliance on text input/output (I/O) imposes significant limitations on its ability to interact with the world. To illustrate this point, we consider Minecraft (Guss et al., 2019; Fan et al., 2022) as an ideal example. Minecraft, being an expansive sandbox game, offers a vast realm for embodied agents to explore, which requires the acquisition of various basic skills (e.g., crafting logs) and the ability to plan and execute diverse tasks. First, as shown in Figure 1 (a), the LLM-based agent produces uncontrollable outputs. The success of the agent\u2019s responses hinges heavily on careful prompt engineering (Huang et al., 2022b), ensuring that the LLM comprehends the environment and task objectives. Moreover, a universally applicable prompt that suits every LLM and task is an unattainable goal. Therefore, this prompting process is labor-intensive and contradicts our aim of enabling agents to act in a self-driven manner. Second, when compared to visual feedback, language often encounters difficulties in intuitively conveying specific world concepts (e.g., recipes) to users, as illustrated in Figure 1 (b), thereby unavoidably creating obstacles for robust human-computer/AI interaction (Preece et al., 1994; Fallman, 2003).\nUnlike LLMs, humans possess an innate ability to process and generate information through both visual and text channels. This inherent gift significantly enhances our capability to interact with the world. However, the coupling of LLM-based agents with multimodal I/O has been relatively underexplored in an open-ended environment. To fill this gap, we introduce Steve-Eye , a large multimodal model that enables LLM-based embodied agents to engage with the open world via visual-text interfaces. Steve-Eye excels at producing responses that demonstrate a comprehensive grasp of the environment, common-sense reasoning, and executable skill plans. To achieve this, Steve-Eye is equipped with three indispensable functions: (1) multimodal perception; (2) foundational knowledge base; and (3) skill prediction and planning. In this paper, we choose Minecraft as our validation platform considering its vast sandbox world and the high degree of freedom. More environments can also be considered, e.g., Virtual Home (Puig et al., 2018), AI2THOR (Kolve et al., 2017). Due to the space limit, we discuss the exploration of more generic environments in Appendix A.4 and leave it as our future work. Our contributions can be summarized as follows:\nOpen-World Instruction Dataset. We construct an extensive instruction dataset to train SteveEye for the acquisition of three mentioned functions. The instruction data contains not only the agent\u2019s per-step status and environmental features but also the essential knowledge for agents to act and plan. However, collecting such a dataset in an open world can be a costly endeavor, especially when aiming to gather fine-grained and diverse labels. As a result, previous studies (Fan et al., 2022) have often relied on readily available unsupervised data (e.g., video-subtitle pairs) for pretraining. In these approaches, the agent\u2019s comprehension of its status and environment is implicitly learned through self-supervised techniques, while its foundational knowledge is directly derived from general-purpose LLMs. In contrast, our work involves curating multimodal instructional data specifically designed for open-ended embodied agents, by utilizing ChatGPT (OpenAI, 2022).\nLarge Multimodal Model and Training. Steve-Eye combines a visual encoder which converts visual inputs into a sequence of embeddings, along with a pre-trained LLM which empowers embodied agents to engage in skill or task reasoning in an open world. During the training process, we employ a two-stage strategy similar to Liu et al. (2023). This strategy commences with the alignment of multimodal elements between the visual encoder and the large language model, followed by the instruction tuning through our constructed dataset.\nOpen-World Benchmarks. We carry out extensive experiments to demonstrate that our proposed Steve-Eye outperforms LLM-based agents in open-world setups. Specifically, we develop the following benchmarks to evaluate agent performance from a broad range of perspectives: (1) environmental visual captioning (ENV-VC), which assesses an agent\u2019s capacity to perceive and describe its surroundings effectively; (2) foundational knowledge question answering (FK-QA), which evaluates the proficiency in mastering basic knowledge crucial for an agent\u2019s decision-making; (3) skill prediction and planning (SPP), which quantifies an agent\u2019s capability to act and plan strategically."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 OPEN-WORLD EMBODIED AGENTS WITH LLMS",
            "text": "The rapid progress of large language models (Brown et al., 2020; Raffel et al., 2020; Zhang et al., 2022; Chowdhery et al., 2022) has significantly boosted their capacity to encode a wide range of human behaviors within training data (Bommasani et al., 2021). When equipped with narrowly designed prompts, LLM-based agents exhibit the capability to generate executable plans for tasks such as indoor robot manipulation. For instance, SayCan (Ahn et al., 2022) integrates skill affordances with LLMs to yield actionable plans, while Palm-E (Driess et al., 2023) takes a step further by constructing hierarchical agents capable of handling multimodal prompts. This approach has also proven its efficacy in open-world environments (Huang et al., 2022a; Li et al., 2022). In contrast to robot manipulation, agents in the wild require a heightened level of real-time situational awareness and foundational knowledge to execute intricate skill plans across a diverse array of tasks. To simulate human behaviors in such open worlds, Generative Agents (Park et al., 2023) store agents\u2019 experiences and retrieve these memories to generate plans in a text-based sandbox game.\nIn recent years, the 3D sandbox Minecraft has received considerable attention owing to its remarkably flexible game mechanics to serve as a prominent open-world benchmark (e.g., MineRL (Guss et al., 2019) and Minedojo (Fan et al., 2022)). DEPS (Wang et al., 2023b) introduces the descriptor, explainer, and selector for plan generation with the help of LLM. Plan4MC (Yuan et al., 2023) constructs a skill graph and proposes a skill search algorithm to minimize planning errors. Voyager (Wang et al., 2023a) proposes an LLM-powered lifelong learning agent that continually explores the Minecraft world. Similar to (Park et al., 2023), GITM (Zhu et al., 2023) integrates LLMs with text-based memory and knowledge to create generic agents in Minecraft. Among these studies, Voyager (Wang et al., 2023a) and GITM (Zhu et al., 2023) lean entirely on text descriptions of the environment to act and plan, while Plan4MC (Yuan et al., 2023) and DEPS (Wang et al., 2023b) have visual-input skills but still rely on merely text for planning. None of them try to understand the rich visual observation provided natively by Minecraft. In contrast to these works, our work trains a large multimodal model to fill this gap."
        },
        {
            "heading": "2.2 LARGE MULTIMODAL MODELS (LMMS)",
            "text": "In comparison to LLMs, large multimodal models (LMMs) (Awadalla et al., 2023) encompass a broad range of information beyond text modality, which can be categorized into two primary streams. The first category (Gupta & Kembhavi, 2023; Huang et al., 2023a; Patil et al., 2023; Sur\u0131\u0301s et al., 2023) involves hinging on ChatGPT (OpenAI, 2022) or GPT-4 (OpenAI, 2023) to generate in-context responses without parameter tuning. However, these approaches heavily rely on the availability of an LLM\u2019s API and the quality of the designed prompts. The second category comprises end-to-end pre-trained models. Within this category, models such as Huang et al. (2023b); Peng et al. (2023) are trained entirely from scratch. Conversely, some research explores efficient fine-tuning using pre-trained LLMs by incorporating lightweight modality encoders, such as Qformer (Li et al., 2023) or Perceiver (Alayrac et al., 2022). Recently, Liu et al. (2023) propose to explicitly instructiontune a LLM using vision-language instruction data.\nIn this work, we propose Steve-Eye by building upon pre-trained LLMs, aiming to develop an openworld agent powered by a large-scale model with versatile multimodal I/O capabilities."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "In this section, we first provide our instruction-following dataset to develop three key functions for the agent\u2019s open-world interaction in Section 3.1. We then propose our large multimodal agent Steve-Eye in Section 3.2, and clarify details of the training procedure in Section 3.3. We adopt Minecraft as our open-ended platform in this paper to collect data and validate the model, anticipating to explore a broader range of environments for Steve-Eye in future studies.\nTo empower an agent with the self-driven capacity to act and plan in an open world, we posit that the following embodied functions are indispensable: (1) multimodal perception function which offers a detailed description of the agent status and environmental features; (2) foundational knowledge\nbase which imparts an understanding of how the world works and conveys crucial basic knowledge related to skills and tasks; (3) skill prediction and planning which is responsible for generating skill execution feedback (e.g., success or failure) and crafting high-level skill plans for handling more complex and long-horizon tasks. We develop these functions by building the corresponding instruction dataset to pre-train Steve-Eye as follows."
        },
        {
            "heading": "3.1 OPEN-WORLD INSTRUCTION-FOLLOWING DATASET",
            "text": "Multimodal Perception Instructions. Human players can perform actions in Minecraft mainly relying on their visual perception, without any prior hints or imposed game judgments. In order to endow Steve-Eye with the same ability, it is required to provide it with comprehensive visual descriptions of the environment. To achieve this, we use Minedojo (Fan et al., 2022) to obtain Minecraft snapshots which contain a wide array of details within the agent\u2019s surroundings, including environmental features, the agent\u2019s life and food status, inventory items, and equipment, as illustrated in Figure 2. In addition, we leverage MaskCLIP (Zhou et al., 2022) to identify the in-sight objects of these snapshots without supervised annotations. During our data collection process, for each snapshot I and its corresponding description XC , we initiate a three-step approach. Firstly, we prompt ChatGPT to curate a list of 40 instructions as shown in Figure 6 in Appendix A.1.1. Then we enrich snapshot details as dense caption to describe its content, with the assistance of ChatGPT. Finally, we select an instruction XQ randomly from the list and combine it with the snapshot\u2019s caption to create a single-round multimodal description pair (e.g., ### Human: XQ I\\n ### Embodied Agent: XC\\n.). By doing so, we collect 200K instructional pairs for multimodal perception learning.\nFoundational Knowledge Instructions. Embodied agents require a foundation of essential knowledge to facilitate actiontaking and skill planning. In Minecraft, such knowledge should contain item recipes, details of item attributes, their associated numerical value, etc. We access this vital information from Minecraft-Wiki (Fandom, 2023), which comprises an extensive collection of over 9,000 HTML pages. To be specific, we first obtain all item icons from Minecraft-Wiki and generate 200K icon inventory images, as illustrated in Figure 3 (a). Each icon image corresponds to a 4-row table with an associated caption adhering to a standardized template: \u201cThere is a Minecraft inventory with 4 rows. From left to right, they are\n...\u201d. As shown in Figuire 7 in Appendix A.1.2, we curate a set of 20 distinct prompts designed to challenge the model\u2019s ability to recognize items. Subsequently, we further collect all recipe-related information from the Wiki as illustrated in Figure 3 (b), and design similar prompt templates to formulate 10,000 recipe-image instructional pairs. Lastly, we process the Wiki and utilize this corpus to produce 40,000 single-round question-answer pairs. In total, we collect a high-quality dataset with 250K foundational knowledge instructions.\nSkill-related Interaction Instructions. The environmental description and foundational knowledge serve as prerequisites for an agent\u2019s interaction within the open world. However, a successful interaction requires more than these elements alone. It relies upon the mastery of basic skills, such as log, harvesting, and food preparation, as well as high-level skill planning abilities to tackle complex, long-horizon tasks, such as crafting an iron pickaxe. To facilitate this, we gather corresponding training data for skill prediction and planning, which enables our model to provide correct feedback on both basic skills and long-horizon tasks across a spectrum of agent or environmental conditions. Specifically, the data collection process involves two steps. First, we sample skill trajectories based on the pre-trained basic skill policies and collect 200K snapshot pairs with corresponding statuses from these trajectories. Each snapshot pair {I0, It} denotes the 0-th and t-th timestamp of the skill trajectory. Next, we employ Chat-\nGPT to generate question-answer pairs about diverse aspects of skill execution status. These ques-\ntions delve into whether the agent completes the skill, encounters unexpected failures, or seeks explanations for such failures. More details can be found in Appendix A.1.3. Second, we sample 40K task trajectories using the planner in Yuan et al. (2023), each of which can be denoted as T = {s1, s2, ...sT} representing the task is finished via a T-round planning procedure, where si is the skill plan for i-th round. At each round i, we feed our model with its start snapshot and task initialization, and curate instructional questions to inquire about si with reasonable explanation. In this manner, we obtain 200K instructional pairs from task trajectories."
        },
        {
            "heading": "3.2 MODEL ARCHITECTURE",
            "text": "Figure 4 illustrates the overall architecture of our proposed model. Steve-Eye, functioning as a generative model, connects an image-oriented tokenizer fv with the pre-trained LLM backbone \u0398. We adopt the image tokenizer, e.g., VQ-GAN (Esser et al., 2021), to encode the raw images I into token embeddings V = {v1, v2, ..., vn} \u2208 Rn\u00d7d, where n denotes the number of visual tokens and d is the dimensionality of each token. We further utilize a lightweight projection module fl with a trainable projection matrix W . This module maps the visual tokens to the same space with text embeddings, yielding V\u0302 = {v\u03021, v\u03022, ..., v\u0302n} \u2208 Rn\u00d7d\u0302:\nV\u0302 = WV; where V = fv(I). (1)\nTo effectively process visual-language inputs and generate corresponding outputs, our model integrates the visual codebook Cv into the pre-existing language vocabulary Cl. This integration leads to the formation of a unified multimodal codebook, denoted as Cm = Cv \u222a Cl. Additionally, in order to mark the starting and ending points of visual elements in I/O sequences, we introduce two special tokens, namely <vis> and </vis>. The LLM backbone \u0398 of our Steve-Eye is built upon a decoder-only architecture with casual transformers. Our model employs an auto-regressive prediction mechanism, generating responses based on the provided multimodal input tokens. The resulting response is a mixed sequence of visual and textual tokens, represented as Y = {y1, y2, ..., ym}. For each embedding yi, we pass it through a linear layer fp followed by a softmax operation, mapping it into a probability distribution of the multimodal vocabulary. The final prediction for the i-th token zi is determined by selecting the token from the multimodal codebook with the highest score:\nzi = argmax(softmax(fp(yi))). (2)"
        },
        {
            "heading": "3.3 TRAINING",
            "text": "Each instruction-following instance can be formulated as a multi-round conversation {X 1Q,X 1C , ..., XNQ ,XNC }, where each {X iQ,X iC} represents a question-answer interaction between a human and\nthe embodied agent and N indicates the total number of rounds in the conversation. The entire instructional dataset follows this unified template, as demonstrated in Figure 11 in Appendix A.1.3. To efficiently train our model, we employ the negative log-likelihood objective over the prediction tokens with instruction tuning:\nL(\u0398) = \u2212 L\u2211\nj=1\nlogP\u0398(yj |I, y\u03021:j\u22121), (3)\nwhere y and y\u0302 respectively denote the input and target token sequences, with \u0398 representing the model parameters, and L representing the length of the target sequence. The input visual content I may represent an empty image depending on the input instruction. It is worth noting that we constrain the loss computation to only consider the answer tokens XC . This constraint prevents training from becoming excessively straightforward and ensures that the model\u2019s primary focus is on learning to precisely generate coherent responses. Similar to Liu et al. (2023), we adopt a twostage instruction-tuning strategy to train our model:\nTwo-Stage Instruction-Tuning. (1) Multimodal feature alignment: In the first stage, our primary objective is to align visual features with the language token space. In order to strike a balance between efficient tuning and a comprehensive coverage of the world\u2019s concepts, we curate our open-ended instruction dataset to 600K snapshot-text pairs. These pairs are then transformed into instruction-following data as described in Section 3.1. During the feature alignment stage, we maintain the visual encoder and the LLM parameters in a frozen state, exclusively training the projection module. Additionally, this training phase involves fine-tuning token embeddings to accommodate the newly introduced visual codebook and two special tokens <vis> and </vis>. (2) End-to-end instruction tuning: In the second stage, we continue to keep the visual encoder frozen while concurrently training the projection module and LLM. This second stage leverages the entire open-ended instructions and contributes significantly to enhancing the model\u2019s capability of comprehending and effectively responding to complex multimodal instructions."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUP",
            "text": "Implementation Details. In this paper, we use the LLaMA-2 model (Touvron et al., 2023b) as the LLM backbone. Additionally, we use CLIP (Radford et al., 2021) as our visual encoder to achieve the best performance for non-visual generative tasks, and use VQ-GAN (Esser et al., 2021) as the default visual tokenizer for visual generation. The size of visual codebook Cv and language vocabulary is 8192 and 32000, respectively. In addition, we add <vis> and </vis> to the final unified codebook, indicating the starting and ending points of visual content. Similar to Liu et al. (2023), we construct 850K instruction-answer pairs for model training. Note that the model is trained to predict the agent\u2019s answer, and thus only sequence/tokens of answer will be used to compute the loss in the auto-regressive model. We also adopt LoRA (Hu et al., 2021) to reduce the computational cost for efficient tuning. We choose MineDojo (Fan et al., 2022) as the Minecraft platform to collect our instruction data and conduct experiments. Following Yuan et al. (2023), we use the environments of programmatic tasks to train basic policies with RL. These policies are trained to execute corresponding skills and keep fixed in all testing tasks.\nEvaluation Benchmarks. We conduct experiments on three benchmarks to evaluate an agent\u2019s interaction ability in an open world. (1) Environmental visual captioning (ENV-VC): given a snapshot, the model is asked to describe the agent\u2019s current status and environmental features from diverse aspects (e.g., life, food...). We evaluate the prediction\u2019s accuracy of each aspect by extracting corresponding answers from the output description to compare with the groundtruth. (2) Foundational knowledge question answering (FK-QA): to assess the model\u2019s grasp of essential knowledge, we collect a set of 10,000 Minecraft-related questions from different sources, including the Wiki pages, Wiki tables, and Minecraft recipes. The performance is measured by the model\u2019s ability to provide correct answers to these questions. (3) Skill prediction and planning (SPP): we utilize our proposed Steve-Eye to predict whether a skill has been successfully completed and assert its capability to generate executable high-level skill plans for long-horizon tasks."
        },
        {
            "heading": "4.2 ENVIRONMENTAL VISUAL CAPTIONING (ENV-VC)",
            "text": "We introduce this evaluation protocol for asserting Steve-Eye\u2019s multimodal perception function, which serves as an initial stride toward comprehensive evaluation of large multimodal models. Specifically, we collect 20,000 Minecraft snapshots (named ENV-VC test set) using Minedojo and apply the proposed data generation pipeline to create six questions for each snapshot, resulting in a total of 120K questions. These six questions pertain to the prediction of various aspects, including inventory items , equipment , objects in sight , life , food , and the visibility of sky .\nDuring the inference phase, Steve-Eye predicts answers based on these questions and the input snapshot. Experimental results are presented in Table 1 and Table 2. As shown in Table 1, our visual encoder, when combined with multimodal instruction tuning, significantly enables the ability of the text-only language model LLM (Llama-2-7b) to comprehend the contents of the snapshots (Steve-Eye-7b). Notably, Steve-Eye outperforms BLIP-2 by a substantial margin due to the improved reasoning ability enabled by the larger LLM. Furthermore, the visual encoder plays a crucial role in facilitating multimodal understanding. Surprisingly, the model equipped with CLIP (Radford et al., 2021) surpasses the performance of the model using MineCLIP (Fan et al., 2022), achieving over +48.9%, +21.0% and +19.9% improvements in inventory, equipment, and object-in-sight predictions, respectively. We attribute this performance difference to the fact that MineCLIP does not prioritize fine-grained alignment during pre-training, despite being exposed to a diverse range of Minecraft videos. In summary, Steve-Eye\u2019s ability to comprehend visual cues from its surroundings lays the foundation for subsequent interactions with the world.\nTo investigate the effictiveness of various types of instructional data for multimodal perception, we carry out experimental comparisons with diverse data configurations in Table 2. First, our results showcase a significant improvement in the model\u2019s capacity to respond to instructional questions through instruction tuning, which leads to impressive gains of over +50% for inventory, equipment, and object-in-sight prediction. Furthermore, the inclusion of the multimodal perception dataset and icon images in the training data both contribute to a substantial improvement in the model\u2019s overall performance. Ultimately, the best results are achieved when combining all available data sources."
        },
        {
            "heading": "4.3 FOUDATIONAL KNOWLEDGE QUESTION ANSWERING (FK-QA)",
            "text": "Following Team (2022), we establish a question database specialized to assess our model\u2019s proficiency in generating responses pertaining to fundamental Minecraft knowledge. This evaluation is carried out through a validation dataset known as the FK-QA test set, which is further divided into two distinct subsets: TEXT and IMG. In the FK-QA TEXT subset, we generate a collection of 10,000 question-answer pairs curated from various sources, including the Minecraft-Wiki pages, Minecraft-Wiki tables, and Minecraft recipes. Each category comprises 2,000, 5,000, and 3,000 pairs, respectively. Upon receiving a response from Steve-Eye, we feed both the generated response\nand the corresponding groundtruth answer to ChatGPT. ChatGPT will first examine the accuracy of the response as a measure of answer correctness. To minimize variability in error, ChatGPT conducts a further evaluation, considering the response\u2019s accuracy, relevance, and level of detail. This comprehensive evaluation yields an overall score on a scale ranging from 0 to 10, where a higher score signifies superior overall performance. In the FK-QA IMG subset, we shift our focus to visual generation by employing 3,000 recipe images as groundtruth data. Here, our model is tasked with generating visual outputs for each item within the recipe inventory, following a specific order. The visual output is considered correct only if every element of the recipe is accurately generated. We adopt this metric to assert our model\u2019s ability to produce multimodal feedback.\nTable 3 presents both scoring and accuracy results. It\u2019s worthy to note that Llama-2 exhibits consistent performance regardless of the model\u2019s scale, with Llama-2-70b only marginally outperforming the 7b-version by +1.26% in accuracy, meanwhile 13b-version performs even worse than 7b-version on the scoring results. We hypothesize that this phenomenon can be attributed to distinct variations in difficulty levels encountered within our FK-QA test set. Llama-2 fails to answer correctly for the challenging part regardless of its size due to essential knowledge missing. In contrast, Steve-Eye outperforms both Llama-2 and gpt-turbo-3.5, despite its considerably smaller scale. Furthermore, our model exhibits a more substantial improvement in responding to Recipe and Wiki Table questions as compared to Wiki Page questions. This disparity can likely be attributed to the fact that Wiki Page contains a large proportion of invalid questions (e.g., version, history), whereas Recipe and Wiki Table predominantly feature knowledge-related inquiries. Such result further validates the effectiveness of our approach in acquiring foundational knowledge. Unlike text-only LLMs, our model exhibits considerable ability to output visual contents, which achieves 65.13% accuracy on FK-QA IMG using the 13b-version. The multimodal generation ability enables Steve-Eye to better serve as an assistant for potential needed people such as beginners of this game. We show more details and cases in Appendix A.3."
        },
        {
            "heading": "4.4 SKILL PREDICTION AND PLANNING (SPP)",
            "text": "Skill Prediction. Similar to Section 3.1, we collect another 20K snapshot pairs in the form of {I0, It} from skill trajectories (referred to as Skill-Pred test). These pairs are input into our model to query the current execution status of the skill. The execution status can fall into one of three categories: success, failure, and running, with \u201crunning\u201d signifying that the skill is currently in progress.\nAs shown in Table 4, our model exhibits commendable performance in skill status prediction. However, the performance is still far from enough to completely replace the rulebased game judgment adopted by the existing RL-based skill agents. These experiments indicate that, despite the excellent multimodal understanding capabilities of our model in openworld environments in previous experiments, it\nstill falls short in fine-grained reasoning tasks that involve consecutive frames to some extent.\nSkill Planning. Following Yuan et al. (2023), we carry out evaluation on 24 difficult tasks in Minecraft. These tasks can be categorized into three types: cutting trees to craft primary items (7), mining cobblestones to craft advanced items (7), and interacting with mobs to harvest food and materials (10). Each task is tested for 30 episodes, where an episode refers to a multi-round interaction\nprocess. At each round, the model receives the environmental feedback from the last round, plans a skill list based on the current status, and then picks up the top skill to execute. For each task episode, we set a maximum step between [3000, 10000]. In our evaluation, we compare Steve-Eye against two baseline approaches: (1) MineAgent (Fan et al., 2022), which completes tasks without decomposing them into basic skills, and uses PPO and self-imitation learning with CLIP reward, and (2) GPT Assistant, which employs ChatGPT as a high-level planner to generate skill plans by prompting itself with information from the environment and the agent\u2019s status. The results in Table 5 demonstrate that Steve-Eye significantly outperforms both baseline methods. Additionally, we conduct experiments in which Steve-Eye takes over the skill prediction function from the rule-based game judgment in Minecraft. This self-driven variant is referred to as \u2018Steve-Eye-auto.\u2019 Since the model\u2019s skill prediction is not always 100% accurate, Steve-Eye-auto does experience some performance degradation when compared to Steve-Eye. This degradation is more pronounced in longer, complex tasks (e.g., , , ) as opposed to short-term tasks (e.g., , , ). Nevertheless, Steve-Eye-auto still demonstrates significant performance improvements in most tasks, compared to the baselines. For additional details about this benchmark, please refer to Appendix A.2.\nFor better visualization, we provide a qualitative example of Steve-Eye completing the task \u201ccrafting stone axe with wooden pickaxe\u201d as shown in Figure 5."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we explore enabling a large multimodal model to serve as a generative embodied agent in open worlds. We achieve this goal by proposing Steve-Eye, which combines the text-only language model with a visual encoder, allowing for a multimodal I/O interface to interact with the environment. With the help of ChatGPT, we curate questions to generate 850K instruction-following data to facilitate the agent\u2019s multimodal perception fuction, foundational knowledge mastery, as well as the capability of skill prediction and planning. Experiments on three open-world benchmarks verify the advantages of our Steve-Eye over a wide range of perspectives."
        },
        {
            "heading": "A APPENDIX",
            "text": "In this appendix, we offer a detailed introduction of the construction of our open-world instruction dataset, as outlined in Appendix A.1, including (1) multimodal perception instructions, (2) foundational knowledge instructions, (3) skill-related interaction instructions, and (4) template of instructional training data. Furthermore, we delve into the skill planning benchmark and its associated task setups in Appendix A.2. In Appendix A.3 we present qualitative cases that illustrate our model\u2019s ability to provide intuitive visual feedback and serve as an intelligent chatbot with a multimodal input-output interface. Finally, we explore the potential applications of our model in diverse environments, such as Virtual Home (Puig et al., 2018)."
        },
        {
            "heading": "A.1 DATASET",
            "text": ""
        },
        {
            "heading": "A.1.1 MULTIMODAL PERCEPTION INSTRUCTIONS",
            "text": "This dataset contains 200K instructional pairs. Figure 6 illustrates a partial listing of instructional questions employed for describing the content of the Minecraft snapshots. These instructions convey similar meanings, albeit with slight variations in natural language."
        },
        {
            "heading": "A.1.2 FOUNDATIONAL KNOWLEDGE INSTRUCTIONS",
            "text": "The dataset comprises 250K training instances, which is organized into three distinct subsets: 200K icon image instructions, 10K recipe image instructions, and 40K Minecraft-Wiki corpus instructions. For the icon images, we generate questions aimed at prompting the model to recognize and describe item icons within the inventory, as depicted in Figure 7. Similarly, we curate instructional questions for recipe images as shown in Figure 8, with the objective of extracting information on completing specific recipes. In addition, we preprocess the raw Minecraft-Wiki HTML pages by removing irrelevant information (e.g., reference links) and unresolved data, transforming the raw corpus into a formatted, clean Markdown version. Leveraging the capabilities of ChatGPT, we employ this powerful language model to generate 10 questions, each with its corresponding answer, for every page of the cleaned Wiki corpus. This process yields a collection of 40K single-round questionanswer pairs, which can be utilized for instruction tuning."
        },
        {
            "heading": "A.1.3 SKILL-RELATED INTERACTION INSTRUCTIONS",
            "text": "For skill prediction, we utilize the skill policies trained by Yuan et al. (2023) to create a dataset comprising 200K skill trajectories. In each trajectory, we extract timestamps from the initial and t-th points to generate a snapshot pair, denoted as {I0, It}. We then construct questions aimed at determining whether the agent successfully executed the skill or, in the case of failure, identifying the underlying reasons for the unsuccessful attempt. Illustrative examples of these skill prediction questions are provided in Figure 9. We also provide examples with snapshot pairs in Figure 10.\nTemplate of Instructional Training Data. Similar to Liu et al. (2023), we formulate each instructional instance as a multi-round conversation as shown in Figure 11, where Xhead message is a sentence to describe this assistant (e.g., \u201c You are in a chat between a curious human and an artificial intelligence assistant. You should serve as an assistant to give helpful, detailed, and polite answers to the human\u2019s questions.\u201d). The number of rounds relies on the input instruction content. And the input images (denoted as < image >) will only be fed in the first round, while XC may contain visual outputs with two additional tokens < vis > and < /vis >."
        },
        {
            "heading": "A.2 SKILL PLANNING BENCHMARK",
            "text": "To clarify this benchmark, we begin by offering comprehensive task setup details in Table 6. During the evaluation phase, we relocate the agent to a random location at the initiation of every episode, with distances of up to 500 units, ensuring that the agent spawns in an unfamiliar environment. Furthermore, for tasks that involve interacting with mobs, we enforce a maximum spawning distance of 30 units for cows and sheep. Our approach to complete tasks is rooted in a hierarchical framework. Specifically, our model exclusively generates high-level skill plans, delegating the actual skill execution to pre-trained basic skill policies as introduced by Yuan et al. (2023). Notably, we introduce a self-driven variant named \u2019Steve-Eye-auto,\u2019 which serves not only as a planner but also replaces the Minecraft rules to verify the successful execution of skills."
        },
        {
            "heading": "A.3 QUALITATIVE RESULTS OF MULTIMODAL GENERATION",
            "text": ""
        },
        {
            "heading": "A.3.1 RECIPE IMAGE GENERATION",
            "text": "Figure 12a showcases qualitative examples of our evaluation on the FK-QA IMG dataset. Utilizing a visual tokenizer like VG-GAN, our model demonstrates the ability to engage in visual generation, enabling it to provide visual feedback based on its comprehension of textual input. However, as shown in Figure 12b, our model encounters difficulties when generating image content characterized by fine-grained or semantically overlapping elements. These challenges warrant further exploration in our future work."
        },
        {
            "heading": "A.3.2 MULTIMODAL CHATBOT",
            "text": "In Figure 13, We present an overview of Steve-Eye functioning as a chatbot to receive task commands and execute them."
        },
        {
            "heading": "A.4 DISCUSSION OF OPEN-WORLD EXPLORATION",
            "text": "In this paper, we have selected Minecraft as our open-world platform. Nevertheless, it is evident that Steve-Eye can be applied to other open-world environments, such as Virtual Home (Puig et al., 2018) and AI2THOR (Kolve et al., 2017), with minimal manual effort using the same methodology in this paper. These alternative benchmarks, when compared to Minecraft, exhibit a closer alignment with the real world. To some extent, this choice holds greater significance since our ultimate objective is to deploy the agent in the real world. To achieve this goal, we expand the Virtual Home benchmark by introducing a more extensive range of environments (50+ rooms), human-interaction tasks (200+ for each room), as well as diverse categories of actions (20+) and objects (100+), as illustrated in Figure 14. The corresponding validation and further exploration of open-ended embodied agents in a real-world context will be the focus of our future work."
        }
    ],
    "title": "STEVE-EYE: EQUIPPING LLM-BASED EMBOD-",
    "year": 2023
}