{
    "abstractText": "Maintaining legacy software requires many software and systems engineering hours. Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze. Existing conventional program translators guarantee correctness, but are hand-engineered for the source and target programming languages in question. Learned transpilation, i.e. automatic translation of code, offers an alternative to manual re-writing and engineering efforts. Automated symbolic program translation approaches guarantee correctness but struggle to scale to longer programs due to the exponentially large search space. Their rigid rule-based systems also limit their expressivity, so they can only reason about a reduced space of programs. Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness. In this work, we leverage the strengths of LMs and symbolic solvers in a neurosymbolic approach to learned transpilation for assembly code. Assembly code is an appropriate setting for a neurosymbolic approach, since assembly code can be divided into shorter non-branching basic blocks amenable to the use of symbolic methods. GUESS & SKETCH extracts alignment and confidence information from features of the LM then passes it to a symbolic solver to resolve semantic equivalence of the transpilation input and output. We test GUESS & SKETCH on three different test sets of assembly transpilation tasks, varying in difficulty, and show that it successfully transpiles 57.6% more examples than GPT-4 and 39.6% more examples than an engineered transpiler. We also share a training and evaluation dataset for this task.",
    "authors": [],
    "id": "SP:b6699c0469de01326abc52ffe30e6c8f5e3b42c7",
    "references": [
        {
            "authors": [
                "Alfred V. Aho",
                "Monica S. Lam",
                "Ravi Sethi",
                "Jeffrey D. Ullman"
            ],
            "title": "Compilers: Principles, Techniques, and Tools (2nd Edition)",
            "year": 2006
        },
        {
            "authors": [
                "Ehsan K. Ardestani",
                "Jose Renau"
            ],
            "title": "Esesc: A fast multicore simulator using time-based sampling",
            "venue": "IEEE 19th International Symposium on High Performance Computer Architecture (HPCA), pp",
            "year": 2013
        },
        {
            "authors": [
                "Jordi Armengol-Estap\u00e9",
                "Jackson Woodruff",
                "Chris Cummins",
                "Michael F.P. O\u2019Boyle"
            ],
            "title": "Slade: A portable small language model decompiler for optimized assembler, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Fabrice Bellard"
            ],
            "title": "Qemu, a fast and portable dynamic translator",
            "venue": "In Proceedings of the Annual Conference on USENIX Annual Technical Conference,",
            "year": 2005
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Xinyun Chen",
                "Chang Liu",
                "Dawn Song"
            ],
            "title": "Tree-to-tree neural networks for program translation, 2018",
            "venue": "URL https://openreview.net/forum?id=rkxY-sl0W",
            "year": 2018
        },
        {
            "authors": [
                "Leonardo de Moura",
                "Nikolaj Bj\u00f8rner. Z"
            ],
            "title": "An efficient smt solver",
            "year": 2008
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "year": 2019
        },
        {
            "authors": [
                "Zhangyin Feng",
                "Daya Guo",
                "Duyu Tang",
                "Nan Duan",
                "Xiaocheng Feng",
                "Ming Gong",
                "Linjun Shou",
                "Bing Qin",
                "Ting Liu",
                "Daxin Jiang",
                "Ming Zhou"
            ],
            "title": "Codebert: A pre-trained model for programming and natural languages. In Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Daya Guo",
                "Alexey Svyatkovskiy",
                "Jian Yin",
                "Nan Duan",
                "Marc Brockschmidt",
                "Miltiadis Allamanis"
            ],
            "title": "Learning to complete code with sketches",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "John L. Hennessy",
                "David A. Patterson"
            ],
            "title": "Computer Architecture, Fifth Edition: A Quantitative Approach",
            "venue": "USA, 5th edition,",
            "year": 2011
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Jingmei Hu",
                "Eric Lu",
                "David A. Holland",
                "Ming Kawaguchi",
                "Stephen Chong",
                "Margo Seltzer"
            ],
            "title": "Towards porting operating systems with program synthesis",
            "venue": "ACM Trans. Program. Lang. Syst.,",
            "year": 2023
        },
        {
            "authors": [
                "Maor Ivgi",
                "Uri Shaham",
                "Jonathan Berant"
            ],
            "title": "Efficient long-text understanding with short-text models",
            "year": 2022
        },
        {
            "authors": [
                "Daniel D. Johnson",
                "Daniel Tarlow",
                "Christian Walder"
            ],
            "title": "R-u-sure? uncertainty-aware code suggestions by maximizing utility across random user intents",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Nikhil Kandpal",
                "Haikang Deng",
                "Adam Roberts",
                "Eric Wallace",
                "Colin Raffel"
            ],
            "title": "Large language models struggle to learn long-tail knowledge, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Svetoslav Karaivanov",
                "Veselin Raychev",
                "Martin Vechev"
            ],
            "title": "Phrase-based statistical translation of programming languages",
            "venue": "In Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming & Software,",
            "year": 2014
        },
        {
            "authors": [
                "Denis Kocetkov",
                "Raymond Li",
                "Loubna Ben Allal",
                "Jia Li",
                "Chenghao Mou",
                "Carlos Mu\u00f1oz Ferrandis",
                "Yacine Jernite",
                "Margaret Mitchell",
                "Sean Hughes",
                "Thomas Wolf",
                "Dzmitry Bahdanau",
                "Leandro von Werra",
                "Harm de Vries"
            ],
            "title": "The stack: 3 tb of permissively licensed source code",
            "year": 2022
        },
        {
            "authors": [
                "Celine Lee",
                "Justin Gottschlich",
                "Dan Roth"
            ],
            "title": "Toward code generation: A survey and lessons from semantic parsing, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
            "year": 1910
        },
        {
            "authors": [
                "Reddy",
                "Daniel Fried",
                "Dzmitry Bahdanau",
                "Yacine Jernite",
                "Carlos Mu\u00f1oz Ferrandis",
                "Sean Hughes",
                "Thomas Wolf",
                "Arjun Guha",
                "Leandro von Werra",
                "Harm de Vries"
            ],
            "title": "Starcoder: may the source be with you",
            "year": 2023
        },
        {
            "authors": [
                "Antonio Valerio Miceli-Barone",
                "Fazl Barez",
                "Ioannis Konstas",
                "Shay B. Cohen"
            ],
            "title": "The larger they are, the harder they fail: Language models do not recognize identifier swaps in python, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Niels M\u00f6ller",
                "Torbjorn Granlund"
            ],
            "title": "Improved division by invariant integers",
            "venue": "IEEE Transactions on Computers,",
            "year": 2011
        },
        {
            "authors": [
                "Anh Tuan Nguyen",
                "Tung Thanh Nguyen",
                "Tien N. Nguyen"
            ],
            "title": "Lexical statistical machine translation for language migration",
            "venue": "In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,",
            "year": 2013
        },
        {
            "authors": [
                "Maxwell I. Nye",
                "Luke B. Hewitt",
                "Joshua B. Tenenbaum",
                "Armando Solar-Lezama"
            ],
            "title": "Learning to infer program sketches",
            "venue": "CoRR, abs/1902.06349,",
            "year": 2019
        },
        {
            "authors": [
                "Ankur Parikh",
                "Oscar T\u00e4ckstr\u00f6m",
                "Dipanjan Das",
                "Jakob Uszkoreit"
            ],
            "title": "A decomposable attention model for natural language inference",
            "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2016
        },
        {
            "authors": [
                "David A. Patterson",
                "John L. Hennessy"
            ],
            "title": "Computer Architecture: A Quantitative Approach",
            "year": 1990
        },
        {
            "authors": [
                "Alec Radford",
                "Ilya Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "venue": "arxiv,",
            "year": 2018
        },
        {
            "authors": [
                "Baptiste Roziere",
                "Marie-Anne Lachaux",
                "Lowik Chanussot",
                "Guillaume Lample"
            ],
            "title": "Unsupervised translation of programming languages",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Baptiste Roziere",
                "Jie Zhang",
                "Francois Charton",
                "Mark Harman",
                "Gabriel Synnaeve",
                "Guillaume Lample"
            ],
            "title": "Leveraging automated unit tests for unsupervised code translation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Baptiste Rozi\u00e8re",
                "Jonas Gehring",
                "Fabian Gloeckle",
                "Sten Sootla",
                "Itai Gat",
                "Xiaoqing Ellen Tan",
                "Yossi Adi",
                "Jingyu Liu",
                "Tal Remez",
                "J\u00e9r\u00e9my Rapin",
                "Artyom Kozhevnikov",
                "Ivan Evtimov",
                "Joanna Bitton",
                "Manish Bhatt",
                "Cristian Canton Ferrer",
                "Aaron Grattafiori",
                "Wenhan Xiong",
                "Alexandre D\u00e9fossez",
                "Jade Copet",
                "Faisal Azhar",
                "Hugo Touvron",
                "Louis Martin",
                "Nicolas Usunier",
                "Thomas Scialom",
                "Gabriel Synnaeve"
            ],
            "title": "Code llama: Open foundation models for code, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Daniel Sanchez",
                "Christos Kozyrakis"
            ],
            "title": "Zsim: Fast and accurate microarchitectural simulation of thousand-core systems",
            "venue": "In Proceedings of the 40th Annual International Symposium on Computer Architecture,",
            "year": 2013
        },
        {
            "authors": [
                "Armando Solar-Lezama"
            ],
            "title": "The sketching approach to program synthesis",
            "venue": "Programming Languages and Systems,",
            "year": 2009
        },
        {
            "authors": [
                "Armando Solar-Lezama",
                "Liviu Tancau",
                "Rastislav Bodik",
                "Sanjit Seshia",
                "Vijay Saraswat"
            ],
            "title": "Combinatorial sketching for finite programs",
            "venue": "SIGARCH Comput. Archit. News,",
            "year": 2006
        },
        {
            "authors": [
                "Armando Solar-Lezama",
                "Liviu Tancau",
                "Rastislav Bodik",
                "Sanjit Seshia",
                "Vijay Saraswat"
            ],
            "title": "Combinatorial sketching for finite programs",
            "venue": "In Proceedings of the 12th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS XII,",
            "year": 2006
        },
        {
            "authors": [
                "Marc Szafraniec",
                "Baptiste Roziere",
                "Hugh James Leather",
                "Patrick Labatut",
                "Francois Charton",
                "Gabriel Synnaeve"
            ],
            "title": "Code translation with compiler representations",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Emina Torlak",
                "Rastislav Bodik"
            ],
            "title": "Growing solver-aided languages with rosette",
            "venue": "Onward!",
            "year": 2013
        },
        {
            "authors": [
                "Helena Vasconcelos",
                "Gagan Bansal",
                "Adam Fourney",
                "Q. Vera Liao",
                "Jennifer Wortman Vaughan"
            ],
            "title": "Generation probabilities are not enough: Exploring the effectiveness of uncertainty highlighting in ai-powered code completions, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Wenwen Wang",
                "Stephen McCamant",
                "Antonia Zhai",
                "Pen-Chung Yew"
            ],
            "title": "Enhancing cross-isa dbt through automatically learned translation rules",
            "venue": "SIGPLAN Not.,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The increasingly heterogeneous landscape of hardware architectures and their instruction set architectures (ISAs) marks a large and growing need to develop support for cross-ISA software management. This challenge is especially relevant for hardware-specific legacy software which must be rewritten to run on any other hardware. Many high-usage source code files also contain in-lined assembly code, which requires porting to alternate hardware architectures. Automated cross-ISA software support has been of interest in the computer architecture community for decades (Armengol-Estape\u0301 et al., 2023; Wang et al., 2018; Bellard, 2005; Ardestani & Renau, 2013; Sanchez & Kozyrakis, 2013). Emulators, virtual machines, and containerized applications allow users to run software on different host hardware by simulating the architecture of the hardware platform that the software is compiled for. However, this option can be unwieldy and compute-inefficient. Assembly-to-assembly transpilation 1 (Ami; occ, 1989), the process of automatically porting software from one ISA to another, offers a way to generate software that can be natively executed on the new hardware. However, current transpilation tools are engineered for the specific source and target hardware architecture, so they scale poorly as new ISAs are introduced.\nNeural machine learning techniques are a natural fit for transpilation. Assembly program translation pairs can be generated by cross-compiling C or C++ programs using different existing compilers and compiler flags, providing vast amounts of training data. Pairs have the same semantics since they originate from the same high-level program. Assembly code syntax is rigid but simple compared\n1\u201cTranspiler\u201d describes the general code translation task that our method targets, but we note that the focus of this paper is assembly-to-assembly transpilation.\nto natural language and most high-level programming languages, settings that existing language models have been shown to perform well in (Devlin et al., 2019; Feng et al., 2020; Radford & Sutskever, 2018; Lewis et al., 2019; Chen et al., 2021). Evaluation in this setting can also be done automatically by comparing execution of the input code and the resulting code.\nHowever, a key weakness of language models in this setting is their inability to perform long-tail logical reasoning (Kandpal et al., 2022; Miceli-Barone et al., 2023). Assembly code transpilation requires reasoning about the complex semantics of programs. Additionally, specific challenging phenomena, such as differing implementations of mathematical operations on different ISAs, are critical and arise frequently in assembly code.\nMotivated by the symbolic properties of logical reasoning in the problem of transpilation, we propose a neurosymbolic method to transpilation. Purely symbolic methods are built on correctness guarantees, but generally can only handle short programs before encountering computational intractability. Classical synthesis techniques struggle to scale past \u223c 6 lines of assembly code (Hu et al., 2023). Purely neural language modeling approaches are powerful general translators but have critical failure points that cause program breakdown. We argue for the value of a mixed-method, i.e. neurosymbolic, approach that uses probabilistic language models to obtain helpful information for transpilation, then passes such information to an ISA semantics-aware solver to complete the transpilation process.\nOur method, GUESS & SKETCH, uses core properties from the language model to extract symbolic methods for transpilation. During the neural GUESS phase, a trained language model produces candidate translations for a given input, identifies potential errors in the output, and extracts semantically-aligned subsequences from the input and output sequences. Potentially erroneous aligned subsequences are passed to the symbolic SKETCH phase, where the input subsequence is used as a specification to correct the output subsequence.\nWe demonstrate the feasibility of our method by porting assembly programs from ARMv8 to RISCV and vice-versa, but note that our method can generalize to various source and target languages. In order to test our method, we introduce a new benchmark consisting of 3 transpilation problems varying in difficulty and domain. We identify weaknesses in engineered symbolic approaches to the task. We also find that existing neural network approaches, using both fine-tuned and pre-trained offthe-shelf large language models, struggle with transpilation. In contrast, our method combines the strengths of both neural and symbolic approaches and successfully transpiles 57.6% more examples than GPT-4, 39.6% more examples than an engineered transpiler, and 13.2% more examples than the most competitive baseline."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Learned code translation. Code transpilers (or transpilers) translate from one programming language to another. The core challenge in this space is preserving operational semantics across the source and target language, while operating within the strict syntax and vocabulary of both. One approach to this task is to train neural machine translation systems with paired code sequences for the task, such as language model (Lewis et al., 2019) or tree-to-tree neural networks (Chen et al., 2018). Approaches such as Transcoder (Roziere et al., 2020) have also presented an unsupervised approach to neural source code-to-source code translation, in which they only require monolingual training data and take advantage of three training objectives: cross-lingual masked language modeling, denoising auto-encoding, and back-translation. Follow-up works use the LLVM intermediate representation (Roziere et al., 2022) and automatically-generated unit tests (Szafraniec et al., 2023) to further improve this approach. Older statistical approaches have mined parallel code from repositories and generated grammar-based statistical machine translation models (Nguyen et al., 2013; Karaivanov et al., 2014; Koehn et al., 2007). These outputs of these prior learned approaches are the generation directly extracted from the model. GUESS & SKETCH instead incorporates knowledge of the semantics of the source and target languages in a symbolic solver that improves semantic correctness the produced output. Additionally, as far as we are aware, we are the first to present a learned approach for learning assembly translation, a lower-level programming language than higher-level programming languages such as Python, Java, and even C.\nEmulators and engineered transpilers. Executing code on a platform different than the one for which it was created is a long-desired task. Apple\u2019s Rosetta (app) software was designed to ease the transition of applications between hardwares by automatically translating binary executables from the previously supported to the new ISA. Specifically, Rosetta in 2006 supported the transition from PowerPC to Intel x86 processors. Rosetta 2 released in 2020 enabled translation from x86-64 based processors to support by ARM64-based Apple silicon. Emulators and virtualizers allow users to execute code designed for another target hardware by simulating the target hardware ISA atop the host hardware. QEMU (Bellard, 2005) is one popular emulator and virtualizer that can emulate various architectures on certain host architectures. Other assembly transpilers have been written to translate assembly from one language to another, such as from ARM to RISC-V (Schorr et al., 2020). However, these emulators and transpilers take years to develop. GUESS & SKETCH, on the other hand, leverages the translation abilities of a learned model to perform a bulk of the transpilation.\nNeurosymbolic program synthesis. Program synthesis is the task of generating computer programs according to some correctness specification (Lee et al., 2021). In the context of program translation, the correctness specification is the semantics of the input program itself. We discuss here some works that take a combined neural and symbolic approach to the program synthesis task, similar to our own approach. Nye et al. (2019) train an LSTM-based model to generate program sketches from some input specification, then use the generated sketch and specification to search for a satisfying program. Guo et al. (2022) devise a top-down grammar-based method to selectively expand nonterminals in a program syntax tree. The incomplete program tree is converted to a sketch that is passed to the symbolic sketch solver to generate a full program. Unlike these previous works, our method infers the sketch using attributes of a single autoregressive language model. The benefit of our approach is over directly producing the sketch or generating based on a grammar is that we avoid encoding specific sketch and language technicalities into the training process."
        },
        {
            "heading": "3 BACKGROUND",
            "text": ""
        },
        {
            "heading": "3.1 TRANSPILATION",
            "text": "The task of transpilation is to take an input program Px, represented as sequence of tokens x, and produce the semantically-equivalent program Py represented as sequence of tokens y. Let D be the domain of all program inputs. For simplicity we represent programs as functions that map inputs to a deterministic measurable output, either an integer or program failure: P\u2217 : D \u2192 (Z\u222a\u22a5). Semantic equivalence can be measured by checking that for all inputs in D, both programs produce the same execution outputs: x \u2261 y : \u2200d \u2208 D : Px(d) = Py(d). In practice, we test the full programs on a feasible subset of D determined by the objective of the source program. When working with programs, we will also assume we can partition the tokens into Bx nonoverlapping subsequences x = xb1 , . . . , xb|Bx| where each b \u2208 Bx defines a span over x. Subsequences are defined so that they can individually be converted to programs Pxb . Details for identifying such subsequences for assembly and translating them into a program representation conducive for symbolic reasoning in a sketch solver are shared in Appendix A.1."
        },
        {
            "heading": "3.2 GENERATIVE LANGUAGE MODELS",
            "text": "Let (x, y) \u2208 (VL,VL) denote an input and output sequence pair where V is the shared vocabulary of tokens and L is the maximum length. The objective of a (conditional) generative language model is to autoregressively produce the correct output y from input x:\nargmax y\u2208VL \u220f t p(yt|y<t, x)\nModern language models are based on the Transformer architecture (Vaswani et al., 2017). Transformers use attention (Parikh et al., 2016), a routing mechanism that provides a distribution over the input tokens used for predicting the next word. Intuitively, attention learns to indicate which part of the input to weigh more for each output. We can extract the model\u2019s attention between the input\nsequence x and output sequence y as a series of stochastic matrices at each layer mapping every output index to a probability distribution over input indices2: M \u2208 \u2206|y|\u00d7|x|."
        },
        {
            "heading": "3.3 SKETCHING",
            "text": "Sketching (Solar-Lezama, 2009; Solar-Lezama et al., 2006a) is an approach to program synthesis in which a partial program outlines the high-level implementation, then a synthesizer populates the omitted low-level details by ensuring that the resulting code passes some given correctness specification. Partial programs are expressed in a procedural programming language augmented with a single added construct: a symbolic constant expressed as a hole, denoted \u2022. Programs expressed in this form, with holes as placeholders for concrete values, are sketches. In our notation, the partial program sequence is composed of tokens from the vocabulary and an added hole token: S = (V\u222a{\u2022})\u2217. Program sequences x are compiled by a semantics-aware translator into representations Px in the procedural programming language understandable by the solver.\nThe correctness specification is set by source program Px. The goal of the synthesizer is to identify the mapping \u03d5 : S \u2192 V\u2217 that populates the holes of the partial program sequence s to produce the full program sequence \u03d5(s) whose corresponding program is semantically equivalent to the source program: \u2200d \u2208 D : P\u03d5(s)(d) = Px(d). The synthesis engine reduces the resulting programmatic sketch representation to a constraint satisfaction problem solved using counterexample guided inductive synthesis (Solar-Lezama et al., 2006b) to find values for the holes."
        },
        {
            "heading": "4 NEUROSYMBOLIC TRANSPILATION: GUESS & SKETCH",
            "text": "Given an input program Px represented as sequence x \u2208 VL, our goal is to learn to generate a semantically-equivalent output sequence y \u2208 VL which represents program Py: Px \u2261 Py . Programs are comprised of function definitions that are generally independent from one another, so functions are individually translated then stitched back together. See details in Appendix A.\nThe challenge of our neurosymbolic approach is that language models operate on prefixes, performing inference by producing one token at a time, while sketch-based methods reason with partially complete sequences. To meaningfully pass information between the language model and the symbolic solver, we must extract relevant sequence-level information from the language model\n2In encoder-decoder models this comes from cross-attention, for decoder-only models by renormalizing self-attention.\nfor the solver to reason over with. Specifically, the solver needs candidate output translations and their semantic alignment in the input.\nOur method breaks the problem into stages that can be better solved by the complementary strengths of neural and symbolic methods: a probabilistic machine learning language model produces candidate translations, then alignment and confidence information is extracted and passed to a semanticsaware solver to filter the search spaces for a correct solution. The pipeline for the GUESS & SKETCH approach is illustrated in Figure 1."
        },
        {
            "heading": "4.1 GUESS: STRUCTURED CANDIDATES FROM A GENERATIVE MODEL",
            "text": "The GUESS phase produces guesses as tuples. For an input sequence x, GUESS produces tuples composed of: a candidate transpilation y, alignments between subsequences: A \u2208 B|By|x , and potential token-level errors in the prediction: E \u2208 {0, 1}|y|.\nCandidates. To produce candidate sequences we follow a standard generative approach. We first train a generative language model on paired source language and target language program sequences. Once trained, candidate transpilations are produced by querying the model:\ny \u2208 top k y\u2208VL p(y|x) (1)\nAlignment. Since the input and target output sequences are intended to be globally semantically equivalent, we assume output sequences locally align to input sequences. While there is not a one-to-one equivalence between tokens, subsequences of the two programs can be matched. We use this subsequence matching and the transformer attention to determine the alignment used by the sketch system. A sample extracted alignment matrix, along with the truth alignment matrix, is shown in Figure 2.\nAlignment is represented as a vector between subsequences: A. To extract the alignment from the language model, we average the transformer attention matrices connecting x and y at single layer to form a stochastic matrix M \u2208 \u2206|y|\u00d7|x|. We then set the alignment\nAbj = bi for the input subsequence with the highest aggregate attention score. Aggregate attention score is given by norm of the submatrices i.e. \u2200bj\u2032 \u2208 Bx : \u2225Mbj ,bi\u2225 \u2265 \u2225Mbj\u2032 ,bi\u2225.\nGuesses and Errors. The generative model is also used to identify tokens where it is most likely guessing. First we check if the output token j is predicted with probability less than some value \u03b3:\np(yj |y<j , x) < \u03b3 (2)\nThese low-confidence prediction points correlate to long-tail code phenomena, i.e. instances that arise rarely in the data distribution, and are where the model may have made a translation mistake. The second case is if the general model is confident, but the program violates a domain specific heuristic, specifically if the token or its aligned input subsequence reference some entity not described in scope. If either of these conditions are satisfied, the tokens in question are marked as potentially erroneous: E \u2208 {0, 1}|y|."
        },
        {
            "heading": "4.2 SKETCH: REASON OVER ALIGNED CANDIDATES",
            "text": "The SKETCH phase produces a full synthesized transpilation using symbolic program solver methods and information from the GUESS phase. Note that determining full program equivalence is an undecidable problem, so we focus on solving for errors in individual subsequences By .\nCreate the sketch. We create a sketch s for each subsequence b \u2208 By that has an possible error from the first stage. The sketch is created from yb by replacing each position in j \u2208 b that also\nAlgorithm 1 GUESS & SKETCH Pseudocode procedure GUESS & SKETCH(x)\nfor y,A,E \u2208 GUESS(x) do \u25b7 produce candidates, alignments, potential errors for b in By do\nif Py \u2261 Px then return y if Ej for any j \u2208 b then \u25b7 identify potential error\nbx \u2190 Ab \u25b7 get aligned input index s\u2190 PLACE HOLES(yb, E) \u25b7 produce sketch sequence \u03d5\u2190 argmax\u03d5 1(Pxbx \u2261 P\u03d5(s)) \u25b7 solve for solution (synthesizer) if \u03d5 success then\ny \u2190 UPDATE(b, \u03d5(s)) \u25b7 update subseq.\nsatisfies Ej \u0338= 0 with a hole \u2022, i.e. potentially erroneous tokensn . The correctness specification is set by the program represented by the aligned input subsequence xbx where Ab = bx. Correctness specifications must be based on complete semantics, so for input subsequences with out-of-scope references, we extract the definition of the referenced entity from the full program. The retrieved entity definition is used to complete the semantics of the correctness specification.\nA semantics-aware translator lifts the sketch and correctness specifications into their sketch solver programmatic representations Ps and Pxbx , respectively. Details about this translation process for our assembly language experiments are shared in Appendix A.1.\nSolve the sketch. To solve the sketch is to find a mapping \u03d5 that correctly populates all holes of the partial program sequence s to satisfy the correctness specification: \u2200d \u2208 D : Pxbx (d) = P\u03d5(s)(d). If a solution populating all holes of the partial program sequences is found by the sketch solver, it is applied to s and the updated subsequence \u03d5(s) replaces the subsequence in the full program sequence. If the subsequence had an out-of-scope reference, the solver would have also resolved a definition of the referenced entity. The resolved referenced entity definition is also updated in the full program. In cases where a sketching solution cannot be found, GUESS & SKETCH resorts to the original prediction. Since our method always at least defaults to the original generation, the correctness of GUESS & SKETCH is lower-bounded by the correctness of the initial guess. This full process is summarized in Algorithm 1."
        },
        {
            "heading": "5 EXPERIMENTAL SETUP",
            "text": "Dataset Our experiments focus on transpilation between real programs compiled to different ISAs, specifically the ARMv8 and RISC-V assembly languages. ARMv8 and RISC-V are both reduced instruction set architectures (ISAs), and have some similarities in instructions (Hennessy & Patterson, 2011). We construct training and evaluation datasets for this task.\nTraining data is composed of 307,916 ARMv8 and RISC-V assembly file pairs compiled from C code files from The Stack (Kocetkov et al., 2022). All selected source C files can be independently compiled to assembly using the standard C libraries (e.g. stdlib, stdio). The C files are compiled to both ARMv8 and RISC-V target architecture assembly files under the -O0, -O1, -O2, and -O3 optimization flags using crosscompilers aarch64-linux-gnu-gcc and\nriscv64-linux-gu-gcc. The resulting dataset is shared on HuggingFace3.\nInference of the system is evaluated on 3 different test sets, summarized in Table 3. Code is emulated in Docker images with QEMU Bellard (2005). Project Euler is constructed from 45 C implementa-\n3released after anonymization period is complete\ntions of Project Euler mathematical challenge problems4. Benchmarks is 16 C implementations of programs in The Computer Language 23.03 Benchmarks Game5. Unix Commands is 11 C implementations of Basic Unix commands6.\nFor verification, all test sets are cross-compiled to the ARMv8 and RISC-V architectures under the -O0 flag. System performance is measured by execution output match. We sample the top 100 candidate guesses for a given full assembly file.\nSystem We experiment with two different types of generative language models: a smaller transformer encoder-decoder model with a bidirectional encoder and autoregressive decoder based on the BART architecture (Lewis et al., 2019), and a larger transformer decoder-only models pre-trained on code (Li et al., 2023; Rozie\u0300re et al., 2023). The first model class is trained from scratch where the second is pretrained. All language models are trained on one NVIDIA RTX A6000 GPU. The encoder-decoder models are trained for 156 hours total and the pre-trained decoder-only models are fine-tuned for 240 hours total. Pre-trained models are fine-tuned with LoRA (Hu et al., 2022). Details of training are shown in Table 4. All resulting models are shared on Huggingface 7. We use confidence threshold \u03b3 = 0.9, although we found that it was not critical for accuracy. Additional \u03b3 experiments are in Appendix A.\nThe symbolic solver is built with Rosette (Torlak & Bodik, 2013), a programming language for synthesis and verification built on top of the Z3 (de Moura & Bj\u00f8rner, 2008) SMT solver. The input space is restricted to 16-bit bitvectors, consistent with the register sizes of the ARMv8 and RISC-V architectures used.\nBaselines We consider several alternate approaches to assembly transpilation. With Few-shot learning (Brown et al., 2020), we prompt GPT-4 (OpenAI, 2023) with instructions and a couple of examplar input-output assembly pairs to obtain a transpilation for a given input assembly file. See details of the specific prompt in Appendix D.1. Transpilers are manually-engineered transpilers that convert the given source assembly to the given target assembly. These are programmatically written for the specified source-to-target-hardware, so for source-target hardware pairs for which we cannot find a transpiler, we cannot obtain numbers for this baseline. We use the engineered ArmV8to-RISCV64 transpiler written by members of the IBM Research Haifa team 8. We did not find a transpiler from RISC-V to ARMv8. LM only methods, FT StarCoder (Li et al., 2023), FT CodeLLaMA (Rozie\u0300re et al., 2023), Encoder-Decoder (Lewis et al., 2019), are the purely neural approaches to machine translation, in which we train or fine-tune a language model with the paired assembly data. The Encoder-Decoder method is equivalent to just the GUESS method of our approach."
        },
        {
            "heading": "6 RESULTS AND ANALYSIS",
            "text": "Performance of our methods on the test sets are shown in Table 1. GUESS & SKETCH outperforms all alternative approaches with 0.01 significance level 9. The Few-shot approach, even with\n4https://github.com/eagletmt/project-euler-c 5https://benchmarksgame-team.pages.debian.net/benchmarksgame/index.html 6https://github.com/yadu007/Basic-Unix-Commands-Implementation 7shared after anonymization period is complete 8https://github.com/schorrm/arm2riscv 9According to a two sample z-test.\nthe largest existing language model today, GPT-4, cannot successfully perform most transpilations. GUESS & SKETCH even outperforms the engineered Transpiler, which fails to translate programs for which it cannot recognize even one instruction. We run several GUESS-only models, comparing from-scratch training to pre-trained models. Interestingly, the fine-tuned pre-trained large language models perform much worse than even just the trained smaller encoder-decoder model. The best-performing baselines is the Encoder-Decoder approach, which we use for the full GUESS & SKETCH. Further experiments testing the performance gain of GUESS & SKETCH over the EncoderDecoder approach on more test programs are shared in Appendix B, and support the same 10% increase in correct transpilations.\nError Analysis Table 2 classifies assembly transpilation errors under one of several categories, determined by bottleneck failure reason: mathematic, copying, ISA, references, logic, memory, and length. See descriptions of each in Appendix C and examples in Appendix C.1.\nThe encoder-decoder model (GUESS) makes few ISA mistakes, but runs into a number of errors in semantics and out-ofscope references, some of which are resolved by the solver in GUESS & SKETCH. However, unless the semantics of all of its erroneous subsequences are resolved, an incorrect transpi-\nlation is not corrected. That is, even though mathematically erroneous subsequences are being resolved across the examples in the test sets, if the bottleneck problem is not resolved or not all errors are properly aligned and solved, the transpilation still fails.\nInterestingly the other approaches fail to transpile or compile before even reaching semantics. For few-shot, the model generates invalid instructions, despite the prompt including a translation instructions as well as multiple exemplar transpilations. Fine-tuning models generate invalid assembly from pretraining despite the fine-tuning phase. On the other hand, the manually engineered transpiler is unable to process many examples at all.\nFigure 4 shows two example outputs. The left shows a guess that is resolved. The language model output (bottom, left) predicts tokens for the incorrect global memory reference, highlighted in yel-\nlow. According to the model cross-attention, these tokens most align to those of the corresponding fmov instruction in the input assembly (top), highlighted in purple. However, in the predicted full assembly program, no memory location is produced with the double-word IEEE representation for the desired float 5.0e+0. After resolution with GUESS & SKETCH, a correct memory location is generated and the memory reference is updated (bottom, right), highlighted in green. The example on the right shows a problem that GUESS & SKETCH does not resolve. The LM output (bottom, left) predicts tokens for the register values with low confidence, highlighted in red. A correct solution is shown (bottom, right). The register use and logic flow is inconsistent.\nSampling Aside from solving more examples in the test dataset, GUESS & SKETCH also reduces the number of samples needed from the underlying LM. For a set of test examples, they are correctly transpiled using the encoder-decoder approach only after sufficiently many samples. Using GUESS & SKETCH, a handful of these are successfully transpiled with fewer samples. Table 3 shows the average number of samples from the LM used by the encoder-decoder approach and the GUESS & SKETCH approach during evaluation of the Project Euler test set. Examples that achieve a correct transpilation after the kth sample are logged to use k samples, and examples that do not achieve a correct transpilation within 100 samples use 100 samples."
        },
        {
            "heading": "7 LIMITATIONS",
            "text": "While GUESS & SKETCH is significantly more effective than the baseline approaches, there are still several remaining open challenges.\n\u2022 The SKETCH method is dependent on alignment with the source sequence. If GUESS fails to provide an accurate alignment than the sketch may be unable to correct the output issue.\n\u2022 Memory management issues are hard for the sketch solver. These include reasoning about values on the stack at any given point in the program, register choice decisions that are incorrectly propagated during autoregressive generation, and loading memory addresses into the register.\n\u2022 The best performing model is a mid-size encoder-decoder, which is strong at pattern matching, but likely cannot perform programmatic reasoning. Potentially larger code models could better solve some of the symbolic transpilation issues, if instruction hallucinations could be reduced.\n\u2022 GUESS & SKETCH is limited in length by the context length of generative language models. Using convolutional methods such as SLeD (Ivgi et al., 2022) could resolve these mistakes in practice.\n\u2022 We have no formal proof of equivalence, only checking on a small finite set of inputs."
        },
        {
            "heading": "8 CONCLUSION",
            "text": "In this work, we present GUESS & SKETCH, a neurosymbolic approach to assembly-to-assembly transpilation. GUESS & SKETCH extracts alignment and confidence information from a language model to guide a symbolic solver. We demonstrate the efficacy of this approach on three different test sets of assembly programs in the ARMv8 and RISC-V architectures. Future work to build on this approach is to identify and use patterns in the decoder attention of the language model that may be helpful for the solver, such as live variable analysis (Aho et al., 2006) patterns. Other future work may include transpiling to or from higher levels of code optimization and devising a mechanism to reason about more elements of the machine state, such as values on the stack."
        },
        {
            "heading": "B ADDITIONAL EXPERIMENTS",
            "text": "To further test the benefit of GUESS & SKETCH over just the language model approach, we run experiments with more Project Euler examples. We collect solutions to 82 additional unique Project Euler problems implemented in C 10, and compile them to the ARMv8 and RISC-V ISAs under the -O0 optimization flag. The average number of lines in these programs is 246. The results of running the strongest baseline and our method are shown in Table 5. GUESS & SKETCH continues to provide performance gains averaging approximately 10%.\n10https://github.com/LaurentMazare/ProjectEuler/tree/master"
        },
        {
            "heading": "C CATEGORIZATION OF FAILED TRANSPILATIONS",
            "text": "Failed transpilations are categorized under one of several bottleneck failure reasons, listed in order of precedence. Process failures include length and process failure, in which the very process of transpilation fails on the given input. If an example does not encounter process failure, the next category is compilation failures including using the incorrect ISA instructions or global references. If the example successfully compiles, the next category of failures it may encounter is semantic failures including mathematic reasoning, copying, operational logic, and memory mis-management. These categories are further described below.\nLength. Some transpilation methods suffer from long input and output sequences. For example, current attention-based language models generally have a context window limit, so sequences that exceed that context window length will not be able to be processed by the language model.\nProcess failure. Examples that fall under this category are ones where the transpilation process fails when processing the input, such as the rules-based transpiler that breaks down upon receiving an input that it cannot parse.\nIncorrect ISA. In assembly transpilation, the produced sequences must use exactly the instructions and entities available to the hardware in question. Failure examples that fall under this category produce sequences mistakenly use syntax that is incorrect or that actually belongs to a different ISA.\nGlobal references. Assembly programs might make references to entities that are invalid, or otherwise use or define global reference labels incorrectly. In these cases, the program will fail.\nMathematic. Math errors are ones in which the translation process fails to correctly perform the required mathematic reasoning for a translation. Examples include translating code idioms such as different implementations of division (Mo\u0308ller & Granlund, 2011), addition and subtraction of large constants, and translation of float values to their IEEE 754 representations (iee, 1985).\nCopying. Copying errors are ones in which part of the input sequence fails to be copied to the output sequence. Examples include copying of constant strings, constant numeric values, and custom function names.\nIncorrect operation or register logic. The produced assembly sequence may use syntactically valid but semantically incorrect logic. These logical errors involve incorrect register or operation use, and the subsequent propagation of such mistakes.\nMemory mis-management. Assembly code must be able to reason about values in memory and manage memory access. Errors in this category are indicated by attempts to access memory at incorrect or invalid stack or memory locations, which may yield stash smashing, stack overflow, or segmentation faults in the latter, and unexpected values in either case.\nC.1 EXAMPLE ERRONEOUS TRANSPILATIONS\nIn this section, we include more example erroneous transpilations from different methods.\nMistakes from fine-tuned code LLMs. Pre-trained code language models, even after fine-tuning with examples in domain, tend to make more ISA mistakes than do other methods. Figure 6 shows two examples of erroneous generated code from a fine-tuned Starcoder-Base method. Figure 6a\nshows an example of the fine-tuned Starcoder-Base method producing code that is largely correct, but violates syntactic rules of the target hardware (RISC-V) by using added-register offsets for the lbu instructions. The syntax of RISC-V 64 does not allow register value addition for loading unsigned bytes by address. It also only allows subtraction by a specified register value rather than an immediate. Figure 6b shows code that allocates then uses a large stack space, but in doing so actually violates syntactic rules of the target hardware (RISC-V) by using an immediate value outside the legal 12-bit immediate ranges for the addi and sd instructions."
        },
        {
            "heading": "D BASELINE IMPLEMENTATION DETAILS",
            "text": "D.1 PROMPTING GPT-4\nThe prompt used to extract translations from GPT-4 for Arm to RISC-V is as follows. For function translations:\nYou a r e a b l e t o t r a n s l a t e as sembly code from ARMv8 t o RISC\u2212V 6 4 .\nARMv8: main :\\ n . LFB0 :\\ n\\ t . c f i s t a r t p r o c \\n\\ t s t p \\ tx29 , x30 , [ sp , \u221248]!\\ n\\ t .\nc f i d e f c f a o f f s e t 48\\n\\ t . c f i o f f s e t 29 , \u221248\\n\\ t . c f i o f f s e t 30 , \u221240\\n \\ tmov\\ tx29 , sp\\n\\ t a d r p \\ tx0 , : g o t : s t a c k c h k g u a r d \\n\\ t l d r \\ tx0 , [ x0 , # : g o t l o 1 2 : s t a c k c h k g u a r d ]\\ n\\ t l d r \\ tx1 , [ x0 ]\\ n\\ t s t r \\ tx1 , [ sp , 40]\\ n \\ tmov\\ tx1 , 0\\n\\ t a d r p \\ tx0 , . LC0\\n\\ t a d d \\ tx0 , x0 , : l o12 : . LC0\\n\\ t b l \\ t p r i n t f \\n\\ t a d d \\ tx0 , sp , 24\\n\\ tmov\\ tx1 , x0\\n\\ t a d r p \\ tx0 , . LC1\\n\\ t a d d \\ tx0 , x0 , : l o12 : . LC1\\n\\ t b l \\ t i s o c 9 9 s c a n f \\n\\ t l d r \\ tw0 , [ sp , 24]\\ n\\ tmov \\ tw1 , 34953\\n\\ tmovk\\ tw1 , 0x8888 , l s l 16\\n\\ t s m u l l \\ tx1 , w0 , w1\\n\\ t l s r \\ tx1 , x1 , 32\\n\\ t a d d \\ tw1 , w0 , w1\\n\\ t a s r \\ tw1 , w1 , 4\\n\\ t a s r \\ tw0 , w0 , 31\\n \\ t s u b \\ tw1 , w1 , w0\\n\\ tmov\\ tw0 , 1500\\n\\ tmul \\ tw0 , w1 , w0\\n\\ t s t r \\ tw0 , [ sp , 28]\\ n\\ t l d r \\ tw1 , [ sp , 24]\\ n\\ tmov\\ tw0 , 34953\\n\\ tmovk\\ tw0 , 0x8888 , l s l 16\\n\\ t s m u l l \\ tx0 , w1 , w0\\n\\ t l s r \\ tx0 , x0 , 32\\n\\ t a d d \\ tw0 , w1 , w0\\n\\ t a s r \\ tw2 , w0 , 4\\n\\ t a s r \\ tw0 , w1 , 31\\n\\ t s u b \\ tw2 , w2 , w0\\n\\ tmov\\ tw0 , w2\\n\\ t l s l \\ tw0 , w0 , 4\\n\\ t s u b \\ tw0 , w0 , w2\\n\\ t l s l \\ tw0 , w0 , 1\\n\\ t s u b \\ tw2 , w1 , w0\\n\\ tmov\\ tw0 , w2\\n\\ t l s l \\ tw0 , w0 , 2\\n\\ t a d d \\ tw0 , w0 , w2\\n\\ t l s l \\ tw0 , w0 , 3\\n\\ t s t r \\ tw0 , [ sp , 32]\\ n\\ t l d r \\ tw1 , [ sp , 28]\\ n\\ t l d r \\ tw0 , [ sp , 32]\\ n\\ t a d d \\ tw0 , w1 , w0\\n\\ t s t r \\ tw0 , [ sp , 36]\\ n\\ t l d r \\ tw1 , [ sp , 36]\\ n\\ t a d r p \\ tx0 , . LC2\\n\\ t a d d \\ tx0 , x0 , : l o12 : . LC2\\n\\ t b l \\ t p r i n t f \\n\\ tmov\\ tw0 , 0\\n\\ tmov\\ tw1 , w0\\n\\ t a d r p \\ tx0 , : g o t : s t a c k c h k g u a r d \\n\\ t l d r \\ tx0 , [ x0 , # : g o t l o 1 2 : s t a c k c h k g u a r d ]\\ n\\ t l d r \\ tx3 , [ sp , 40]\\ n\\ t l d r \\ tx2 , [ x0 ]\\ n\\ t s u b s \\ tx3 , x3 , x2\\n\\ tmov\\ tx2 , 0\\n\\ t b e q \\ t . L3\\n\\ t b l \\ t s t a c k c h k f a i l \\n . L3 :\\ n\\ tmov\\ tw0 , w1\\n\\ t l d p \\ tx29 , x30 , [ sp ] , 48\\n\\ t . c f i r e s t o r e 30\\n\\ t . c f i r e s t o r e 29\\n\\ t . c f i d e f c f a o f f s e t 0\\n\\ t r e t \\n\\ t . c f i e n d p r o c \\n\nRISC\u2212V 6 4 : main :\\ n\\ t a d d i \\ t s p , sp , \u221248\\n\\ t s d \\ t r a , 4 0 ( sp ) \\n\\ t s d \\ t s 0 , 3 2 ( sp ) \\n\\ t a d d i \\ t s 0 , sp\n, 4 8\\ n\\ t l a \\ t a5 , s t a c k c h k g u a r d \\n\\ t l d \\ t a4 , 0 ( a5 ) \\n\\ t s d \\ t a4 , \u221224( s0 ) \\n\n\\ t l i \\ t a4 , 0\\n\\ t l l a \\ t a0 , . LC0\\n\\ t c a l l \\ t p r i n t f @ p l t \\n\\ t a d d i \\ t a5 , s0 , \u221240\\n\\ tmv\\ t a1 , a5\\n\\ t l l a \\ t a0 , . LC1\\n\\ t c a l l \\ t i s o c 9 9 s c a n f @ p l t \\n\\ t l w \\ t a5 , \u221240( s0 ) \\n\\ tmv\\ t a4 , a5\\n\\ t l i \\ t a5 , 3 0\\ n\\ t d ivw \\ t a5 , a4 , a5\\n\\ t s e x t .w\\ t a4 , a5\\n\\ t l i \\ t a5 ,1500\\ n\\ tmulw\\ t a5 , a4 , a5\\n\\ t sw\\ t a5 , \u221236( s0 ) \\n\\ t l w \\ t a5 , \u221240( s0 ) \\n\\ tmv\\ t a4 , a5\\n\\ t l i \\ t a5 , 3 0\\ n\\ tremw\\ t a5 , a4 , a5\\n\\ t s e x t .w\\ t a5 , a5\\n\\ tmv\\ t a4 , a5\\n\\ tmv\\ t a5 , a4\\n\\ t s l l i w \\ t a5 , a5 , 2\\ n\\ taddw\\ t a5 , a5 , a4\\n\\ t s l l i w \\ t a5 , a5 , 3\\ n\\ t sw\\ t a5 , \u221232( s0 ) \\n\\ t l w \\ t a5 , \u221236( s0 ) \\n\\ tmv\\ t a4 , a5\\n\\ t l w \\ t a5 , \u221232( s0 ) \\n\\ taddw\\ t a5 , a4 , a5\\n\\ t sw\\ t a5 , \u221228( s0 ) \\n\\ t l w \\ t a5 , \u221228( s0 ) \\n\\ tmv\\ t a1 , a5\\n \\ t l l a \\ t a0 , . LC2\\n\\ t c a l l \\ t p r i n t f @ p l t \\n\\ t l i \\ t a5 , 0\\ n\\ tmv\\ t a4 , a5\\n\\ t l a \\ t a5 , s t a c k c h k g u a r d \\n\\ t l d \\ t a3 , \u221224( s0 ) \\n\\ t l d \\ t a5 , 0 ( a5 ) \\n\\ t x o r \\ t a5 , a3 , a5\\n\\ t l i \\ t a3 , 0\\n\\ t b e q \\ t a5 , ze ro , . L3\\n\\ t c a l l \\ t s t a c k c h k f a i l @ p l t \\n . L3 :\\ n\\ tmv\\ t a0 , a4\\n\\ t l d \\ t r a , 4 0 ( sp ) \\n\\ t l d \\ t s 0 , 3 2 ( sp ) \\n\\ t a d d i \\ t s p , sp , 4 8\\ n\\ t j r \\ t r a \\n\nARMv8: main :\\ n . LFB6 :\\ n\\ t . c f i s t a r t p r o c \\n\\ t s t p \\ tx29 , x30 , [ sp , \u221264]!\\ n\\ t .\nc f i d e f c f a o f f s e t 64\\n\\ t . c f i o f f s e t 29 , \u221264\\n\\ t . c f i o f f s e t 30 , \u221256\\n \\ tmov\\ tx29 , sp\\n\\ t a d r p \\ tx0 , : g o t : s t a c k c h k g u a r d \\n\\ t l d r \\ tx0 , [ x0 , # : g o t l o 1 2 : s t a c k c h k g u a r d ]\\ n\\ t l d r \\ tx1 , [ x0 ]\\ n\\ t s t r \\ tx1 , [ sp , 56]\\ n \\ tmov\\ tx1 , 0\\n\\ t a d r p \\ tx0 , . LC0\\n\\ t a d d \\ tx0 , x0 , : l o12 : . LC0\\n\\ t b l \\ t p r i n t f \\n\\ t a d d \\ tx0 , sp , 20\\n\\ tmov\\ tx1 , x0\\n\\ t a d r p \\ tx0 , . LC1\\n\\ t a d d \\ tx0 , x0 , : l o12 : . LC1\\n\\ t b l \\ t i s o c 9 9 s c a n f \\n\\ t l d r \\ tw0 , [ sp , 20]\\ n\\ tmov \\ tw1 , w0\\n\\ t a d r p \\ tx0 , . LC2\\n\\ t a d d \\ tx0 , x0 , : l o12 : . LC2\\n\\ t b l \\ t p r i n t f \\n \\ t a d r p \\ tx0 , . LC3\\n\\ t a d d \\ tx0 , x0 , : l o12 : . LC3\\n\\ t b l \\ t p r i n t f \\n\\ t a d d \\ tx0 ,\nsp , 19\\n\\ tmov\\ tx1 , x0\\n\\ t a d r p \\ tx0 , . LC4\\n\\ t a d d \\ tx0 , x0 , : l o12 : . LC4\\n \\ t b l \\ t i s o c 9 9 s c a n f \\n\\ t l d r b \\ tw0 , [ sp , 19]\\ n\\ tmov\\ tw1 , w0\\n\\ t a d r p \\ tx0 , . LC5\\n\\ t a d d \\ tx0 , x0 , : l o12 : . LC5\\n\\ t b l \\ t p r i n t f \\n\\ t a d r p \\ tx0 , . LC6\\n\\ t a d d \\ tx0 , x0 , : l o12 : . LC6\\n\\ t b l \\ t p r i n t f \\n\\ t a d d \\ tx0 , sp , 24\\n\\ tmov\\ tx1 ,\nx0\\n\\ t a d r p \\ tx0 , . LC7\\n\\ t a d d \\ tx0 , x0 , : l o12 : . LC7\\n\\ t b l \\ t i s o c 9 9 s c a n f \\n\\ t l d r \\ td0 , [ sp , 24]\\ n\\ t a d r p \\ tx0 , . LC8\\n\\ t a d d \\ tx0 , x0 , : l o12 : . LC8\\n\\ t b l \\ t p r i n t f \\n\\ t a d r p \\ tx0 , . LC9\\n\\ t a d d \\ tx0 , x0 , : l o12 : . LC9\\n\\ t b l \\ t p r i n t f \\n\\ t a d r p \\ tx0 , : g o t : s t d i n \\n\\ t l d r \\ tx0 , [ x0 , # : g o t l o 1 2 : s t d i n ]\\ n\\ t l d r \\ tx1 , [ x0 ]\\ n\\ t a d d \\ tx0 , sp , 32\\n\\ tmov\\ tx2 , x1\\n\\ tmov\\ tw1 , 20\\n\\ t b l \\ t f g e t s \\n\\ t a d d \\ tx0 , sp , 32\\n\\ tmov\\ tx1 , x0\\n\\ t a d r p \\ tx0 , . LC10\\n\\ t a d d \\ tx0 , x0 , : l o12 : . LC10\\n\\ t b l \\ t p r i n t f \\n\\ tmov\\ tw0 , 0\\n\\ tmov\\ tw1 , w0\\n\\ t a d r p \\ tx0 , : g o t : s t a c k c h k g u a r d \\n\\ t l d r \\ tx0 , [ x0 , # : g o t l o 1 2 : s t a c k c h k g u a r d ]\\ n\\ t l d r \\ tx3 , [ sp , 56]\\ n\\ t l d r \\ tx2 , [ x0 ]\\ n\\ t s u b s \\ tx3 , x3 , x2\\n\\ tmov\\ tx2 , 0\\n\\ t b e q \\ t . L3\\n\\ t b l \\ t s t a c k c h k f a i l \\n . L3 :\\ n\\ tmov\\ tw0 , w1\\n\\ t l d p \\ tx29 , x30 , [ sp ] , 64\\n\\ t . c f i r e s t o r e 30\\n\\ t . c f i r e s t o r e 29\\n\\ t . c f i d e f c f a o f f s e t 0\\n\\ t r e t \\n\\ t . c f i e n d p r o c \\n\nRISC\u2212V 6 4 : main :\\ n\\ t a d d i \\ t s p , sp , \u221264\\n\\ t s d \\ t r a , 5 6 ( sp ) \\n\\ t s d \\ t s 0 , 4 8 ( sp ) \\n\\ t a d d i \\ t s 0 , sp\n, 6 4\\ n\\ t l a \\ t a5 , s t a c k c h k g u a r d \\n\\ t l d \\ t a4 , 0 ( a5 ) \\n\\ t s d \\ t a4 , \u221224( s0 ) \\n \\ t l i \\ t a4 , 0\\n\\ t l l a \\ t a0 , . LC0\\n\\ t c a l l \\ t p r i n t f @ p l t \\n\\ t a d d i \\ t a5 , s0 , \u221260\\n\\ tmv\\ t a1 , a5\\n\\ t l l a \\ t a0 , . LC1\\n\\ t c a l l \\ t i s o c 9 9 s c a n f @ p l t \\n\\ t l w \\ t a5 , \u221260( s0 ) \\n\\ tmv\\ t a1 , a5\\n\\ t l l a \\ t a0 , . LC2\\n\\ t c a l l \\ t p r i n t f @ p l t \\n\\ t l l a \\ t a0 , . LC3\\ n\\ t c a l l \\ t p r i n t f @ p l t \\n\\ t a d d i \\ t a5 , s0 , \u221261\\n\\ tmv\\ t a1 , a5\\n\\ t l l a \\ t a0 , . LC4\\n \\ t c a l l \\ t i s o c 9 9 s c a n f @ p l t \\n\\ t l b u \\ t a5 , \u221261( s0 ) \\n\\ t s e x t .w\\ t a5 , a5\\n\\ tmv\\ t a1 , a5\\n\\ t l l a \\ t a0 , . LC5\\n\\ t c a l l \\ t p r i n t f @ p l t \\n\\ t l l a \\ t a0 , . LC6\\n\\ t c a l l \\ t p r i n t f @ p l t \\n\\ t a d d i \\ t a5 , s0 , \u221256\\n\\ tmv\\ t a1 , a5\\n\\ t l l a \\ t a0 , . LC7\\n\\ t c a l l \\ t i s o c 9 9 s c a n f @ p l t \\n\\ t f l d \\ t f a 5 , \u221256( s0 ) \\n\\ t fmv . x . d\\ t a1 , f a 5 \\n\\ t l l a \\ t a0 , . LC8\\n\\ t c a l l \\ t p r i n t f @ p l t \\n\\ t l l a \\ t a0 , . LC9\\n\\ t c a l l \\ t p r i n t f @ p l t \\n\\ t l a \\ t a5 , s t d i n \\n\\ t l d \\ t a4 , 0 ( a5 ) \\n\\ t a d d i \\ t a5 , s0 , \u221248\\n\\ tmv\\ t a2 , a4\\n\\ t l i \\ t a1 , 2 0\\ n\\ tmv\\ t a0 , a5\\n\\ t c a l l \\ t f g e t s @ p l t \\n\\ t a d d i \\ t a5 , s0 , \u221248\\n\\ tmv\\ t a1 , a5\\n \\ t l l a \\ t a0 , . LC10\\n\\ t c a l l \\ t p r i n t f @ p l t \\n\\ t l i \\ t a5 , 0\\ n\\ tmv\\ t a4 , a5\\n\\ t l a \\ t a5 , s t a c k c h k g u a r d \\n\\ t l d \\ t a3 , \u221224( s0 ) \\n\\ t l d \\ t a5 , 0 ( a5 ) \\n\\ t x o r \\ t a5 ,\na3 , a5\\n\\ t l i \\ t a3 , 0\\n\\ t b e q \\ t a5 , ze ro , . L3\\n\\ t c a l l \\ t s t a c k c h k f a i l @ p l t \\n . L3 :\\ n\\ tmv\\ t a0 , a4\\n\\ t l d \\ t r a , 5 6 ( sp ) \\n\\ t l d \\ t s 0 , 4 8 ( sp ) \\n\\ t a d d i \\ t s p , sp , 6 4\\ n\\ t j r \\ t r a \\n\nARMv8: b :\\ n\\ t . z e r o \\ t 8 \\n\\ t . g l o b a l \\ t c \\n\\ t . a l i g n \\ t 3 \\n\\ t . t y p e \\ t c , %o b j e c t \\n\\ t . s i z e \\\nt c , 8\\n\nRISC\u2212V 6 4 : b :\\ n\\ t . z e r o \\ t 8 \\n\\ t . g l o b l \\ t c \\n\\ t . a l i g n \\ t 3 \\n\\ t . t y p e \\ t c , @object\\n\\ t . s i z e \\ t c\n, 8\\n\nARMv8: foo :\\ n . LFB0 :\\ n\\ t . c f i s t a r t p r o c \\n\\ t s t p \\ tx29 , x30 , [ sp , \u221216]!\\ n\\ t .\nc f i d e f c f a o f f s e t 16\\n\\ t . c f i o f f s e t 29 , \u221216\\n\\ t . c f i o f f s e t 30 , \u22128\\n\\ tmov\\ tx29 , sp\\n\\ t a d r p \\ tx0 , g l o b a l \\n\\ t a d d \\ tx0 , x0 , : l o12 : g l o b a l \\n\\ t b l \\ t b a r \\n\\ t a d r p \\ tx0 , g l o b a l 2 \\n\\ t a d d \\ tx0 , x0 , : l o12 : g l o b a l 2 \\n\\ t b l \\ t b a r \\ n\\ t a d r p \\ tx0 , : g o t : g l o b a l 3 \\n\\ t l d r \\ tx0 , [ x0 , # : g o t l o 1 2 : g l o b a l 3 ]\\ n\\ t b l \\ t b a r \\n\\ t a d r p \\ tx0 , g l o b a l 5 \\n\\ t a d d \\ tx0 , x0 , : l o12 : g l o b a l 5 \\n\\ t b l \\ t b a r \\n\\ t a d r p \\ tx0 , g l o b a l 6 \\n\\ t a d d \\ tx0 , x0 , : l o12 : g l o b a l 6 \\n\\ t b l \\ t b a r \\ n\\ t nop \\n\\ t l d p \\ tx29 , x30 , [ sp ] , 16\\n\\ t . c f i r e s t o r e 30\\n\\ t . c f i r e s t o r e 29\\n\\ t . c f i d e f c f a o f f s e t 0\\n\\ t r e t \\n\\ t . c f i e n d p r o c \\n\nRISC\u2212V 6 4 : foo :\\ n\\ t a d d i \\ t s p , sp , \u221216\\n\\ t s d \\ t r a , 8 ( sp ) \\n\\ t s d \\ t s 0 , 0 ( sp ) \\n\\ t a d d i \\ t s 0 , sp\n, 1 6\\ n\\ t l l a \\ t a0 , g l o b a l \\n\\ t c a l l \\ t b a r @ p l t \\n\\ t l l a \\ t a0 , g l o b a l 2 \\n\\ t c a l l \\ t b a r @ p l t \\n\\ t l a \\ t a0 , g l o b a l 3 \\n\\ t c a l l \\ t b a r @ p l t \\n\\ t l l a \\ t a0 , g l o b a l 5 \\n\\ t c a l l \\ t b a r @ p l t \\n\\ t l l a \\ t a0 , g l o b a l 6 \\n\\ t c a l l \\ t b a r @ p l t \\n\\ t nop \\n\\ t l d \\ t r a , 8 ( sp ) \\n\\ t l d \\ t s 0 , 0 ( sp ) \\n\\ t a d d i \\ t s p , sp , 1 6\\ n\\ t j r \\ t r a \\n\nARMv8: { i n s e r t i n p u t code t o t r a n s l a t e }\nRISC\u2212V 6 4 :\nFor outer file translations:\nYou a r e a b l e t o t r a n s l a t e as sembly code from ARMv8 t o RISC\u2212V 6 4 .\nARMv8: \\ t . a r c h armv8 \u2212a\\n\\ t . f i l e \\ t \u201d program19928025 . c \u201d\\n\\ t . t e x t \\n\\ t . s e c t i o n \\ t .\nr o d a t a \\n\\ t . a l i g n \\ t 3 \\n . LC0 :\\ n\\ t . s t r i n g \\ t \u201d E n t e r your age : \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC1 :\\ n\\ t . s t r i n g \\ t \u201d%d \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC2 :\\ n\\ t . s t r i n g \\ t \u201dYou a r e % d y e a r s o l d .\\\\ n \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC3 :\\ n\\ t . s t r i n g \\ t \u201d E n t e r your g r a d e : \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC4 :\\ n\\ t . s t r i n g \\ t \u201d%c \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC5 :\\ n\\ t . s t r i n g \\ t \u201d Your g r a d e i s : %c \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC6 :\\ n\\ t . s t r i n g \\ t \u201d E n t e r your gpa : \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC7 :\\ n\\ t . s t r i n g \\ t \u201d% l f \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC8 :\\ n\\ t . s t r i n g \\ t \u201d Your gpa i s : %l f \\\\n \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC9 :\\ n\\ t . s t r i n g \\ t \u201d E n t e r\nyour name : \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC10 :\\ n\\ t . s t r i n g \\ t \u201d Your name i s %s \u201d\\n\\ t . t e x t \\n\\ t . a l i g n \\ t 2 \\n\\ t . g l o b a l \\ tma in \\n\\ t . t y p e \\ tmain , %f u n c t i o n \\n{main } . LFE6 :\\ n\\ t . s i z e \\ tmain , . \u2212 main\\n\\ t . i d e n t \\ t \u201dGCC: ( Ubuntu 11 .3 .0 \u22121 ubuntu1 \u02dc 2 2 . 0 4 ) 1 1 . 3 . 0 \u201d\\ n\\ t . s e c t i o n \\ t . n o t e .GNU\u2212 s t a c k , \u201d \u201d , @progb i t s \\n\nRISC\u2212V 6 4 : \\ t . f i l e \\ t \u201d program19928025 . c \u201d\\n\\ t . o p t i o n p i c \\n\\ t . t e x t \\n\\ t . s e c t i o n \\ t . r o d a t a\n\\n\\ t . a l i g n \\ t 3 \\n . LC0 :\\ n\\ t . s t r i n g \\ t \u201d E n t e r your age : \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC1 :\\ n\\ t . s t r i n g \\ t \u201d%d \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC2 :\\ n\\ t . s t r i n g \\ t \u201dYou a r e %d y e a r s o l d .\\\\ n \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC3 :\\ n\\ t . s t r i n g \\ t \u201d E n t e r your g r a d e : \u201d\\n \\ t . a l i g n \\ t 3 \\n . LC4 :\\ n\\ t . s t r i n g \\ t \u201d%c \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC5 :\\ n\\ t . s t r i n g \\ t \u201d Your g r a d e i s : %c \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC6 :\\ n\\ t . s t r i n g \\ t \u201d E n t e r your gpa : \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC7 :\\ n\\ t . s t r i n g \\ t \u201d% l f \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC8 :\\ n\\ t . s t r i n g \\ t \u201d Your gpa i s : %l f \\\\n \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC9 :\\ n\\ t . s t r i n g \\ t \u201d E n t e r\nyour name : \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC10 :\\ n\\ t . s t r i n g \\ t \u201d Your name i s %s \u201d\\n\\ t . t e x t \\n\\ t . a l i g n \\ t 1 \\n\\ t . g l o b l \\ tma in \\n\\ t . t y p e \\ tmain , @func t ion \\n{main}\\ t . s i z e \\ tmain , . \u2212 main\\n\\ t . i d e n t \\ t \u201dGCC: ( Ubuntu 11 .3 .0 \u22121 ubuntu1 \u02dc 2 2 . 0 4 ) 1 1 . 3 . 0 \u201d\\ n\\ t . s e c t i o n \\ t . n o t e .GNU\u2212 s t a c k , \u201d \u201d , @progb i t s \\n\nARMv8: \\ t . a r c h armv8 \u2212a\\n\\ t . f i l e \\ t \u201d program12490936 . c \u201d\\n\\ t . t e x t \\n\\ t . s e c t i o n \\ t .\nr o d a t a \\n\\ t . a l i g n \\ t 3 \\n . LC0 :\\ n\\ t . s t r i n g \\ t \u201d E n t e r t h e d i s t a n c e t h e van has t r a v e l l e d : \u201d\\ n\\ t . a l i g n \\ t 3 \\n . LC1 :\\ n\\ t . s t r i n g \\ t \u201d%d \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC2 :\\ n\\ t . s t r i n g \\ t \u201dAmount = %d \u201d\\n\\ t . t e x t \\n\\ t . a l i g n \\ t 2 \\n\\ t . g l o b a l \\ tma in \\n\\ t . t y p e \\ tmain , %f u n c t i o n \\n{main } . LFE0 :\\ n\\ t . s i z e \\ tmain , . \u2212 main\\n\\ t .\ni d e n t \\ t \u201dGCC: ( Ubuntu 11 .3 .0 \u22121 ubuntu1 \u02dc 2 2 . 0 4 ) 1 1 . 3 . 0 \u201d\\ n\\ t . s e c t i o n \\ t . n o t e .GNU\u2212 s t a c k , \u201d \u201d , @progb i t s \\n\nRISC\u2212V 6 4 : \\ t . f i l e \\ t \u201d program12490936 . c \u201d\\n\\ t . o p t i o n p i c \\n\\ t . t e x t \\n\\ t . s e c t i o n \\ t . r o d a t a\n\\n\\ t . a l i g n \\ t 3 \\n . LC0 :\\ n\\ t . s t r i n g \\ t \u201d E n t e r t h e d i s t a n c e t h e van has t r a v e l l e d : \u201d\\ n\\ t . a l i g n \\ t 3 \\n . LC1 :\\ n\\ t . s t r i n g \\ t \u201d%d \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC2 :\\ n\\ t . s t r i n g \\ t \u201dAmount = %d \u201d\\n\\ t . t e x t \\n\\ t . a l i g n \\ t 1 \\n\\ t . g l o b l \\ tma in \\n\\ t . t y p e \\ tmain , @func t ion \\n{main}\\ t . s i z e \\ tmain , . \u2212 main\\n\\ t . i d e n t \\ t \u201dGCC: ( Ubuntu 11 .3 .0 \u22121 ubuntu1 \u02dc 2 2 . 0 4 ) 1 1 . 3 . 0 \u201d\\ n\\ t . s e c t i o n \\ t . n o t e .GNU\u2212 s t a c k , \u201d \u201d , @progb i t s \\n\nARMv8: \\ t . a r c h armv8 \u2212a\\n\\ t . f i l e \\ t \u201d program14079072 . c \u201d\\n\\ t . t e x t \\n\\ t . g l o b a l \\ t b \\n\\ t .\nb s s \\n\\ t . a l i g n \\ t 3 \\n\\ t . t y p e \\ tb , %o b j e c t \\n\\ t . s i z e \\ tb , 8\\n{b}{ c}{d}{ e}{ f } . LFE0 :\\ n\\ t . s i z e \\ t f , . \u2212 f \\n\\ t . i d e n t \\ t \u201dGCC: ( Ubuntu 11 .3 .0 \u22121 ubuntu1 \u02dc 2 2 . 0 4 ) 1 1 . 3 . 0 \u201d\\ n\\ t . s e c t i o n \\ t . n o t e .GNU\u2212 s t a c k , \u201d \u201d , @progb i t s \\n\nRISC\u2212V 6 4 : \\ t . f i l e \\ t \u201d program14079072 . c \u201d\\n\\ t . o p t i o n p i c \\n\\ t . t e x t \\n\\ t . g l o b l \\ t b \\n\\ t . b s s\n\\n\\ t . a l i g n \\ t 3 \\n\\ t . t y p e \\ tb , @object\\n\\ t . s i z e \\ tb , 8\\n{b}{ c}{d}{ e}{ f }\\ t . s i z e \\ t f , . \u2212 f \\n\\ t . i d e n t \\ t \u201dGCC: ( Ubuntu 11 .3 .0 \u22121 ubuntu1 \u02dc 2 2 . 0 4 ) 1 1 . 3 . 0 \u201d\\ n\\ t . s e c t i o n \\ t . n o t e .GNU\u2212 s t a c k , \u201d \u201d , @progb i t s \\n\nARMv8: \\ t . a r c h armv8 \u2212a\\n\\ t . f i l e \\ t \u201d program17748089 . c \u201d\\n\\ t . t e x t \\n\\ t . s e c t i o n \\ t .\nr o d a t a \\n\\ t . a l i g n \\ t 3 \\n . LC0 :\\ n\\ t . s t r i n g \\ t \u201d%f \\\\n%f \\\\n%f \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC1 :\\ n\\ t . s t r i n g \\ t \u201d% l f \u201d\\n\\ t . t e x t \\n\\ t . a l i g n \\ t 2 \\n\\ t . g l o b a l \\ tma in \\n\\ t . t y p e \\ tmain , %f u n c t i o n \\n{main } . LFE0 :\\ n\\ t . s i z e \\ tmain , . \u2212 main\\n\\ t . i d e n t \\ t \u201dGCC: ( Ubuntu 11 .3 .0 \u22121 ubuntu1 \u02dc 2 2 . 0 4 ) 1 1 . 3 . 0 \u201d\\ n\\ t . s e c t i o n \\ t . n o t e .GNU\u2212 s t a c k , \u201d \u201d , @progb i t s \\n\nRISC\u2212V 6 4 : \\ t . f i l e \\ t \u201d program17748089 . c \u201d\\n\\ t . o p t i o n p i c \\n\\ t . t e x t \\n\\ t . s e c t i o n \\ t . r o d a t a\n\\n\\ t . a l i g n \\ t 3 \\n . LC0 :\\ n\\ t . s t r i n g \\ t \u201d%f \\\\n%f \\\\n%f \u201d\\n\\ t . a l i g n \\ t 3 \\n . LC1 :\\ n \\ t . s t r i n g \\ t \u201d% l f \u201d\\n\\ t . t e x t \\n\\ t . a l i g n \\ t 1 \\n\\ t . g l o b l \\ tma in \\n\\ t . t y p e \\ tmain , @func t ion \\n{main}\\ t . s i z e \\ tmain , . \u2212 main\\n\\ t . i d e n t \\ t \u201dGCC: ( Ubuntu 11 .3 .0 \u22121 ubuntu1 \u02dc 2 2 . 0 4 ) 1 1 . 3 . 0 \u201d\\ n\\ t . s e c t i o n \\ t . n o t e .GNU\u2212 s t a c k , \u201d \u201d , @progb i t s \\n\nARMv8: { i n s e r t i n p u t code t o t r a n s l a t e }\nRISC\u2212V 6 4 :\nThe reverse direction reverses source and target language specifications accordingly."
        }
    ],
    "title": "GUESS & SKETCH: LANGUAGE MODEL GUIDED TRANSPILATION",
    "year": 2023
}