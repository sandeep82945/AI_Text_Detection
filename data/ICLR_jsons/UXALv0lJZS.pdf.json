{
    "abstractText": "The problem of speech separation, also known as the cocktail party problem, refers to the task of isolating a single speech signal from a mixture of speech signals. Previous work on source separation derived an upper bound for the source separation task in the domain of human speech. This bound is derived for deterministic models. Recent advancements in generative models challenge this bound. We show how the upper bound can be generalized to the case of random generative models. Applying a diffusion model Vocoder that was pretrained to model single-speaker voices on the output of a deterministic separation model leads to state-of-the-art separation results. It is shown that this requires one to combine the output of the separation model with that of the diffusion model. In our method, a linear combination is performed, in the frequency domain, using weights that are inferred by a learned model. We show state-of-the-art results on 2, 3, 5, 10, and 20 speakers on multiple benchmarks. In particular, for two speakers, our method is able to surpass what was previously considered the upper performance bound.",
    "authors": [],
    "id": "SP:d3eb160282f96b8ab48850d1cfe9242d4e48646d",
    "references": [
        {
            "authors": [
                "Miko\u0142aj Bi\u0144kowski",
                "Jeff Donahue",
                "Sander Dieleman",
                "Aidan Clark",
                "Erich Elsen",
                "Norman Casagrande",
                "Luis C Cobo",
                "Karen Simonyan"
            ],
            "title": "High fidelity speech synthesis with adversarial networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Nanxin Chen",
                "Yu Zhang",
                "Heiga Zen",
                "Ron J Weiss",
                "Mohammad Norouzi",
                "William Chan"
            ],
            "title": "Wavegrad: Estimating gradients for waveform generation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Nanxin Chen",
                "Yu Zhang",
                "Heiga Zen",
                "Ron J Weiss",
                "Mohammad Norouzi",
                "Najim Dehak",
                "William Chan"
            ],
            "title": "Wavegrad 2: Iterative refinement for text-to-speech synthesis",
            "venue": "arXiv preprint arXiv:2106.09660,",
            "year": 2021
        },
        {
            "authors": [
                "Thomas M. Cover",
                "Joy A. Thomas"
            ],
            "title": "Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)",
            "year": 2006
        },
        {
            "authors": [
                "Shaked Dovrat",
                "Eliya Nachmani",
                "Lior Wolf"
            ],
            "title": "Many-speakers single channel speech separation with optimal permutation training",
            "venue": "In Annual Conference of the International Speech Communication Association (INTERSPEECH),",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "John R Hershey",
                "Zhuo Chen",
                "Jonathan Le Roux",
                "Shinji Watanabe"
            ],
            "title": "Deep clustering: Discriminative embeddings for segmentation and separation",
            "venue": "In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2016
        },
        {
            "authors": [
                "Won Jang",
                "Dan Lim",
                "Jaesam Yoon",
                "Bongwan Kim",
                "Juntae Kim"
            ],
            "title": "Univnet: A neural vocoder with multi-resolution spectrogram discriminators for high-fidelity waveform generation",
            "venue": "arXiv preprint arXiv:2106.07889,",
            "year": 2021
        },
        {
            "authors": [
                "Vivek Jayaram",
                "John Thickstun"
            ],
            "title": "Source separation with deep generative priors",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Nal Kalchbrenner",
                "Erich Elsen",
                "Karen Simonyan",
                "Seb Noury",
                "Norman Casagrande",
                "Edward Lockhart",
                "Florian Stimberg",
                "Aaron Oord",
                "Sander Dieleman",
                "Koray Kavukcuoglu"
            ],
            "title": "Efficient neural audio synthesis",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "Morten Kolb\u00e6k",
                "Dong Yu",
                "Zheng-Hua Tan",
                "Jesper Jensen"
            ],
            "title": "Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2017
        },
        {
            "authors": [
                "Jungil Kong",
                "Jaehyeon Kim",
                "Jaekyoung Bae"
            ],
            "title": "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Zhifeng Kong",
                "Wei Ping",
                "Jiaji Huang",
                "Kexin Zhao",
                "Bryan Catanzaro"
            ],
            "title": "Diffwave: A versatile diffusion model for audio synthesis",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Le Roux",
                "Scott Wisdom",
                "Hakan Erdogan",
                "John R Hershey"
            ],
            "title": "SDR\u2013half-baked or well done",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Yann LeCun",
                "Corinna Cortes. MNIST handwritten digit database."
            ],
            "title": "URL http://yann",
            "venue": "lecun.com/exdb/mnist/.",
            "year": 2010
        },
        {
            "authors": [
                "Qiang Liu",
                "Jason Lee",
                "Michael Jordan"
            ],
            "title": "A kernelized stein discrepancy for goodness-of-fit tests",
            "venue": "In International conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "G Logeshwari",
                "GS Anandha Mala"
            ],
            "title": "A survey on single channel speech separation",
            "venue": "In International Conference on Advances in Communication, Network, and Computing,",
            "year": 2012
        },
        {
            "authors": [
                "Yi Luo",
                "Nima Mesgarani"
            ],
            "title": "Tasnet: time-domain audio separation network for real-time, singlechannel speech separation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2018
        },
        {
            "authors": [
                "Yi Luo",
                "Nima Mesgarani"
            ],
            "title": "Conv-tasnet: Surpassing ideal time\u2013frequency magnitude masking for speech separation",
            "venue": "IEEE/ACM transactions on audio, speech, and language processing,",
            "year": 2019
        },
        {
            "authors": [
                "Yi Luo",
                "Zhuo Chen",
                "Takuya Yoshioka"
            ],
            "title": "Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation",
            "year": 1910
        },
        {
            "authors": [
                "Shahar Lutati",
                "Eliya Nachmani",
                "Lior Wolf"
            ],
            "title": "Sepit: Approaching a single channel speech separation bound",
            "venue": "Interspeech 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Rainer Martin",
                "Israel Cohen"
            ],
            "title": "Single-channel speech presence probability estimation and noise tracking",
            "venue": "Audio Source Separation and Speech Enhancement,",
            "year": 2018
        },
        {
            "authors": [
                "Eliya Nachmani",
                "Yossi Adi",
                "Lior Wolf"
            ],
            "title": "Voice separation with an unknown number of multiple speakers",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Sander Dieleman",
                "Heiga Zen",
                "Karen Simonyan",
                "Oriol Vinyals",
                "Alex Graves",
                "Nal Kalchbrenner",
                "Andrew Senior",
                "Koray Kavukcuoglu"
            ],
            "title": "Wavenet: A generative model for raw audio",
            "venue": "arXiv preprint arXiv:1609.03499,",
            "year": 2016
        },
        {
            "authors": [
                "Vassil Panayotov",
                "Guoguo Chen",
                "Daniel Povey",
                "Sanjeev Khudanpur"
            ],
            "title": "Librispeech: an asr corpus based on public domain audio books",
            "venue": "In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP),",
            "year": 2015
        },
        {
            "authors": [
                "Robin Scheibler",
                "Youna Ji",
                "Soo-Whan Chung",
                "Jaeuk Byun",
                "Soyeon Choe",
                "Min-Seok Choi"
            ],
            "title": "Diffusion-based generative speech source separation",
            "venue": "arXiv preprint arXiv:2210.17327,",
            "year": 2022
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Cem Subakan",
                "Mirco Ravanelli",
                "Samuele Cornell",
                "Mirko Bronzi",
                "Jianyuan Zhong"
            ],
            "title": "Attention is all you need in speech separation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Zhong-Qiu Wang",
                "Samuele Cornell",
                "Shukjae Choi",
                "Younglo Lee",
                "Byeong-Yeol Kim",
                "Shinji Watanabe"
            ],
            "title": "Tf-gridnet: Integrating full-and sub-band modeling for speech separation",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2023
        },
        {
            "authors": [
                "Yair Weiss",
                "William T Freeman"
            ],
            "title": "What makes a good model of natural images",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2007
        },
        {
            "authors": [
                "Dong Yu",
                "Morten Kolb\u00e6k",
                "Zheng-Hua Tan",
                "Jesper Jensen"
            ],
            "title": "Permutation invariant training of deep models for speaker-independent multi-talker speech separation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2017
        },
        {
            "authors": [
                "Fisher Yu",
                "Ari Seff",
                "Yinda Zhang",
                "Shuran Song",
                "Thomas Funkhouser",
                "Jianxiong Xiao"
            ],
            "title": "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop",
            "venue": "arXiv preprint arXiv:1506.03365,",
            "year": 2015
        },
        {
            "authors": [
                "Liwen Zhang",
                "Ziqiang Shi",
                "Jiqing Han",
                "Anyan Shi",
                "Ding Ma"
            ],
            "title": "Furcanext: End-to-end monaural speech separation with dynamic gated dilated temporal convolutional networks",
            "venue": "In International Conference on Multimedia Modeling,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Existing speech separation methods face limitations in performance and scalability. We demonstrate that combining pretrained generative and source separation models achieves state-of-the-art results across various sources and datasets, with detailed achievable bound analysis.\nHere is a simple algorithm for improving source separation. Given a test mixture m of C speakers, (1) apply a deep neural architecture B to m and obtain multiple approximated sources v\u0304id for i = 1...C, (2) apply a generative diffusion model GM using each of the approximations obtaining v\u0304ig, and (3) apply a shallow convolutional neural network F to v\u0304 i d and v\u0304 i g to obtain mixing weights [\u03b1i, \u03b2i] = F (v\u0304 i d, v\u0304 i g) and combine the two approximations linearly in the frequency domain to obtain the output v\u0304i.\nThe need to combine in the frequency domain arises because the reconstructed phase in each segment can be arbitrary and phase compensation is needed. In other words, since v\u0304d and v\u0304g are inferred by different processes, they may be at different phases.\nIn our experiments, out of the three networks (B,GM,F ), we only train network F , as this training takes place on the same training set used to train network B. The other networks are taken, as is, from published models. Specifically, B is either Gated-LSTM (Nachmani et al., 2020) or SepFormer (Subakan et al., 2021) and GM is the DiffWave network (Kong et al., 2020b), which is trained, in an unsupervised way, on the LibriMix dataset (Panayotov et al., 2015) and WSJ0 (Garofolo, John S. et al., 1993) in order to generate speech signals.\nOur empirical results, presented in Sec. 5, demonstrate that across multiple deep architectures and methods, applying a pretrained generative diffusion model, in the most straightforward way, pushes the envelope of results further by a significant gap. Our goal is to shed light on this phenomenon. We show the following two results: (1) the mutual information between the best combination of v\u0304id and v\u0304ig and the underlying ground truth signal v\ni is bounded by twice the mutual information between the mixture and the ground truth signal, (2) we provide a bound for the signal-to-distortion ratio (the SDR error metric) of such combinations v\u0304i, which depends on the quality of network B and the mutual information between m and each source vi."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Single-channel speech separation is a fundamental problem in speech and audio processing that has been extensively studied over the years (Logeshwari & Mala, 2012; Martin & Cohen, 2018). Recently, deep learning models have been proposed for speech separation, resulting in a significant improvement in performance compared to traditional methods. Hershey et al. (2016) proposed a clustering method that utilizes trained speech embeddings for separation. Yu et al. (2017) proposed the Permutation Invariant Training (PIT) at the frame level for source separation, while (Kolb\u00e6k et al., 2017) extended this approach by proposing the utterance-level Permutation Invariant Training (uPIT). An influential deep learning method for speech separation over the time domain was introduced by Luo & Mesgarani (2018). This method employs three components: an encoder, a separator, and a decoder. Subsequently, in Conv-Tasnet the separator network was replaced with a fully convolutional model, using a block of time-depth separable dilated convolution (Luo & Mesgarani, 2019). ConvTasnet was scaled by training several separator networks in parallel to perform an ensemble (Zhang et al., 2020). Dual Path RNN blocks were used to reorder the encoded representation and process it across different dimensions (Luo et al., 2019). The so-called MulCat blocks were presented by Nachmani et al. (2020) as a way to eliminate the need for a masking sub-network.\nOne of the limitations of current methods is their inability to effectively train neural networks for a large number of speakers, due to the reliance on Permutation Invariant Training (PIT) methods, which have a time complexity of O(C!), where C is the number of speakers. Instead, one can use a permutation-invariant training method that employs the Hungarian algorithm, reducing the time complexity to O(C3) (Dovrat et al., 2021).\nLutati et al. (2022) introduced an upper bound for audio source separation. By dividing the speech signal into short segments of sounds, a known distribution is used to describe the signal. Then, using the relation between mutual information and Cramer Rao lower bound, the authors managed to demonstrate an upper bound for speech separation for any deterministic model. This bound depends on the mutual information between the mixture and the sources. As the number of sources increases, the mutual information decreases, and so does the upper bound. The recent advent of very successful randomized generative models points in a new research direction: while the previous bound is applicable to any deterministic processing, it remains an open question whether it holds for generative models too.\nThe Diffusion Probabilistic Model has been successfully applied to various domains, such as time series and images (Sohl-Dickstein et al., 2015). A major limitation of this model is that it requires a significant number of iterative steps to generate valid data samples. This was addressed by a diffusion generative model based on Langevin dynamics and the score matching method (Song & Ermon, 2019). This model estimates the Stein score function (Liu et al., 2016), which is the gradient of the logarithm of data density, and uses it to generate data points. Generative neural diffusion processes for speech generation were developed based on score matching (Chen et al., 2020; Kong et al., 2020b). These models have achieved state-of-the-art results for speech generation, demonstrating superior performance compared to well-established methods, such as Wavernn (Kalchbrenner et al., 2018), Wavenet (Oord et al., 2016), and GAN-TTS (Bin\u0301kowski et al., 2019).\nJayaram & Thickstun (2020) introduced the use of generative models as priors for the separation of different sources from a mixture. This was tested on mixtures of images such as MNIST (LeCun & Cortes, 2010) and LSUN (Yu et al., 2015) datasets. DiffSep (Scheibler et al., 2022) performs audio separating using a diffusion model. The underlying SDE is an affine transformation of a mixing matrix P . While the authors showed the ability to separate sources from a single channel, their result falls behind deterministic models by a large margin. Although they set the ground for source separation using generative models, neither work presented state-of-the-art performance, nor proposed a bound for generative methods.\nWe are unaware of similar generalization bounds developed for diffusion models. Our method can be applied to other domains, such as image denoising and conditional generation, given suitable data on the underlying distribution of the training data. For example, for image denoising, there are known results from the field of natural image statistics (Weiss & Freeman, 2007) that state that while segmenting the image into patches a Gaussian Scale Mixture is able to model the statistics of the underlying natural images. A similar derivation to what is done in our work for audio would obtain an upper bound for the maximal improvement achievable given the noise prior."
        },
        {
            "heading": "3 ANALYSIS",
            "text": "Let m be a mixture of C speakers, each with a ground truth signal vi, i = 1 . . . C. Let B be a source separation model that returns C signals and GM be a vocoder diffusion model that is trained on the domain of clear, single-speaker voice signals. The method described at the beginning of Sec. 1 can be summarized by the following set of equations, where STFT and iSTFT are the Short-Time Fourier transform and its inverse, respectively.\n[v\u03041d, v\u0304 2 d, . . . , v\u0304 c d] = B(m) (1)\n\u2200i \u2208 [c]  v\u0304ig = GM(v\u0304 i d) V\u0304 ig , V\u0304 i d = STFT(v\u0304 i g),STFT(v\u0304 i d) [\u03b1i, \u03b2i] = F (v\u0304 i d, v\u0304 i g)\nv\u0304i = iSTFT(\u03b1i v\u0304id + \u03b2i v\u0304ig)\n(2)\n(3)\n(4)\n(5)\nEq. 1 applies B to separate the mixed signal. Eq. 2 applies the vocoder to denoise each output of B separately. Eq. 4 computes the combination weights of the output of B and the corresponding output of GM . Finally, the combination is performed by linearly mixing in the spectral domain in Eq. 5. Where is the Hadamard product. As we show in Sec. 5, this simple method empirically surpasses the current state-of-the-art. Evidently, when applied correctly, a vocoder model can suppress the errors that the deterministic source separation model has. Below, we analyze the source of this improvement and derive an upper bound for the level of improvement. Definition 3.1 (Signal-to-Distortion Ratio (SDR)). Let the error be defined as the difference between the source and the estimated source. The Signal-to-Distortion Ratio (SDR) is SDR = 10log10 V ar(v) V ar( ) .\nGiven a mixture of sources, m, the ground truth signal v, and the estimated signal, v\u0304d, (Lutati et al., 2022) have found that\nSDR(v, v\u0304d) \u2264 10log10 ( L\nw \u00b7 V ar(v) \u00b7 I(mr;vr)\n) (6)\nwhere v\u0304d is the estimation of the deterministic backbone network B, L is the length of the signal, w is the segment width, v is the ground truth source, m is the mixture and the subscript r describes the mixture and the ground truth signal in the r-th segment.\nFurthermore, (Lutati et al., 2022) show that the mutual information between the deterministic estimation and the source is upper bounded by the mutual information between the source and the mixture.\nI(vr; v\u0304dr) \u2264 I(vr;mr) (7)\nIn what follows, we first lay down the basis for upper bounding the combination of the generative and the deterministic signal. Next, by modeling the noise that is added to the generated signal, the mutual information between the combination of the signals and the sources is bounded. Finally, by applying the changes to Eq. 6, a new upper bound is found. Lemma 3.2 (Mutual Information Chain Rule). For any random variablesX ,Y ,Z the following chain rule holds I(X;Y, Z) = I(X;Z) + I(X;Y |Z).\nSee proof in the supplementary. Using Lemma. 3.2 for the source, vr, the deterministic estimation, v\u0304dr, and the generative estimation, v\u0304gr, we have,\nI(vr; v\u0304dr, v\u0304gr) = I(vr; v\u0304dr) + I(vr; v\u0304gr|v\u0304dr) (8)\nPlug Eq. 8 to Eq. 7, we have the following bound\nI(vr; v\u0304dr, v\u0304gr) \u2264 I(vr;mr) + I(vr; v\u0304gr|v\u0304dr) (9)\nOur goal is now to bound the maximal mutual information achievable between the source, vr, and the generative signal,v\u0304gr."
        },
        {
            "heading": "3.1 GENERATIVE BOUND",
            "text": "The inherent noise of GM is modeled as additive noise. For any practical GM there is an inherent error in reconstructing a perfect signal from a perfect prior.\nThe pretrained generative model (GM) is optimized to generate samples v\u0304gr that minimize the ELBO loss when given a prior for vr. Denote the observed prior as p(vdr). GM generates v\u0304gr that it is the optimizer for ELBO loss: p(v\u0304gr) = argminp(v\u0304gr)ELBO(p(v\u0304dr), p(v\u0304gr)).\nGiven the aforementioned upper-bound I(vr; v\u0304dr), Eq. 7, it follows that p(v\u0304gr) \u2248 p(v\u0304dr) when I(vr; v\u0304dr) \u2264 I(vr;m). This implies that the v\u0304gr, will sample from the same distribution as v\u0304dr. Using Eq. 9 We have, I(vr; v\u0304gr) is upper bounded by I(vr;mr), and I(vr; v\u0304gr, v\u0304dr).\nI(vr; v\u0304gr) \u2264 I(vr;mr) + I(vr; v\u0304gr, v\u0304dr) \u2264 2 \u00b7 I(vr;mr) (10)\nLet us assume the added noise is Gaussian noise. From (Cover & Thomas, 2006), we have that among all distributions Gaussian noise is the worst-case additive noise in terms of mutual information, that is, normal additive noise will degrade most of the information in the generated signal. As shown below, for the pretrained GM, the worst case coalesces with the best case, where the inequality in Eq. 10 becomes equality. Thus, making Eq. 10 tight.\nFor additive white Gaussian noise (AWGN) it holds that v\u0304gr = x+ n, n \u223c N where x is the desired part of the signal, based on the prior, and n is the normal noise. Pr(n) = exp(\u2212 n 2 2\u03c32 )/ \u221a\n2\u03c0\u03c32, where \u03c32 is the variance of the additive noise. Lutati et al. (2022) show empirically that for short segments of 20[ms] the audio is distributed in the time domain as a Laplace distribution, x \u223c Laplace. Therefore, Pr(x) = exp(\u2212| x\u221a\n2 |)/ \u221a 2.\nComputing explicitly the mutual information we obtain\nI(x; v\u0304g) = \u222b x \u222b v\u0304g Pr(x, v\u0304g) log ( Pr(x, v\u0304g) Pr(x)Pr(v\u0304g) ) dx dv\u0304g , (11)\nwhere Pr(x) is the probability of x, Pr(v\u0304g) is the probability of the generated voice, and Pr(x, v\u0304g) is the joint probability of the signals. The joint probability can be written explicitly as Pr(x, v\u0304g) = Pr(v\u0304g|x)Pr(x). Plugging in the known distributions reads\nPr(x, v\u0304g) = exp(\u2212 12 ( v\u0304g\u2212x \u03c3 ) 2) \u221a 2\u03c0\u03c32 \u00b7 exp(\u2212| x\u221a 2 |) \u221a 2\n(12)\nThe distribution of v\u0304g can be formulated using marginalization of the joint probability, Pr(x, v\u0304g)\nPr(v\u0304g) = \u222b x Pr(x, v\u0304g) dx (13)\nRecall that the generated voice without noise can at best (maximum) obtain the mutual information between the mixture and the sources. Let \u03c1 be the ratio between the mutual information of the source and the generative estimation, and the mutual information of the source and mixture, i.e., \u03c1 = I(vr,v\u0304g)I(vr,mr) . The factor \u03c1 is computed numerically from equation Eq. 11, by integrating the double integral. Figure 1 depicts the ratio \u03c1 as defined above. This ratio is computed for numerous sources. Each trial is with a different color. Note that for low-quality generative models (i.e, \u03c32 \u2265 10\u22122) the ratio degrades at a faster rate when the number of sources is lower. Recall that when the number of sources is larger, the mixture tends to have a normal distribution.\nThus, the addition of another independent noise with a smaller variance has a negligible effect in terms of the mutual information (which is already low). Observe that \u03c1 is approximately one when\n\u03c32 \u2264 10\u22122. Since, for any well-functioning pretrained GM, the error in reconstructing the audio signal is below 10\u22122 (Kong et al., 2020b; Chen et al., 2020; 2021), we obtain that\n\u03c1(\u03c32 \u2264 10\u22122) \u2248 1 \u2200C \u2208 [2, 20] (14)\nTherefore, rewriting Eq. 10 with the definition of \u03c1 reads,\n\u03c1I(vr;mr) = I(vr; v\u0304g) \u2264 I(vr; v\u0304dr) \u2264 I(vr;mr). (15)\nUsing the result of Eq. 14 one obtains,\nI(vr;mr) \u2248 I(vr; v\u0304g) \u2264 I(vr; v\u0304dr) \u2264 I(vr;mr) , (16) which implies that the bound is tight. Also, from the bound, under the reasonable assumptions made, the deterministic approximation and the generative approximations would have similar error magnitudes.\nFinally, updating Eq. 6 with the new bound obtains, SDR(v, v\u0304) \u2264 10log10 ( L\nw \u00b7 V ar(v) \u00b7 I(mr,vr)\n) + 3.0 (17)\nOne can see a maximum addition of 3dB to the upper bound when combining both generative results with deterministic processing. Note that this bound is tight in the sense that the worst-case additive independent noise is taken into account and still the achievable mutual information is at maximum.\nThe upper bound assumes sequential processing of uncorrelated chunks and does not hold for mixed chunks processing. An approach such as TF-GridNet Wang et al. (2023) processes the signal entirely but has limitations such as being unable to deal with non-stationary segments, such as silence. Also, such methods need to be retrained for every signal length and are only able to handle relatively short signals."
        },
        {
            "heading": "4 METHOD",
            "text": "Given a mixture signal m, the deterministic estimation v\u0304d is obtained using the backbone network B. Then, a Mel-Spectrogram is computed over all v\u0304d estimations.\nMel(v\u0304d) = MelSpectrogram(v\u0304d) (18)\nUsing a pretrained vocoder,GM , specifically DiffWave, and the priors obtained from the deterministic backbone B, a generative estimation is obtained v\u0304g .\nv\u0304g = GM(Mel(v\u0304d)) (19)\nSince the generative vocoder is not given any phase information, the generated signal has a phase shift that can vary over time. To combine both estimations, an alignment procedure is employed.\nIn order to align the signals, both estimations are transformed into the frequency domain, where the aligning operation dual is multiplication by a phasor. The transformation is done through a short-time Fourier transform with NFFT frequency bins and K segments.\nDenote the spectrogram of v\u0304id and v\u0304 i g , as V\u0304 i d , V\u0304 i g respectively, V\u0304 i d , V\u0304 i g \u2208 CNFFT\u00d7K . The absolute phase of V\u0304g is of no importance; what is important is its trend over short segments. The phase of V\u0304d and the relative phase between V\u0304d and V\u0304g is concatenated into a 2-channel tensor, \u03c8 \u2208 RC\u00d72\u00d7NFFT\u00d7K , i.e.,\n\u03c8 = Concat(\u2220V\u0304d,\u2220(V\u0304g V\u0304 \u2217d )) , (20) where is the Hadamard product, the star superscript denotes conjugate, and the following definition is used.\nDefinition 4.1. The angle, denoted as \u2220, of a complex number is computed by\n\u2220X = tan\u22121( Im(X)\nRe(X) ) (21)\nwhere X \u2208 C, Im is the imaginary part, and Re is the real part.\nIn addition to the relative phase tensor \u03c8, the magnitude of both V\u0304 id and V\u0304 i g is concatenated into tensor, A, A = Concat(|V\u0304d|, |V\u0304g|) (22)\nThe alignment network that we train F is a dual 6-layer convolutional neural network, with residual connections (He et al., 2016), termed ResNet Head in Fig. 2. The magnitude and phase heads share the same hyperparameters. The processed tensors are combined into the complex factor Q \u2208 CC\u00d72\u00d7NFFT\u00d7K ,\nD1, D2 = F (A,\u03c8) (23)\nwhere D1 and D2, both in RC\u00d72\u00d7NFFT\u00d7K , represent the magnitude and phase, respectively, of a complex tensor Q. This dual representation is used as an effective representation for complex tensors. Q is then computed explicitly by combining the magnitude and phase\nQ = D1 \u00b7 exp(\u2212jD2) (24)\nBelow, we denote the different channels of Q by a subscript i. Define as \u03b1 = Q1 \u2208 CC\u00d71\u00d7NFFT\u00d7K and the second channel as \u03b2 = Q2.\n\u03b1 and \u03b2 are the coefficients used in Eq. 5. Written more explicitly, the weighted sum of V\u0304d and V\u0304g is computed as\nV\u0304 = \u03b1 V\u0304d + \u03b2 V\u0304g (25)\nFinally, the time-domain signal is obtained by employing the inverse short-time Fourier transform.\nv\u0304 = iSTFT (V\u0304 ) (26)\nObjective Function Following (Luo & Mesgarani, 2019; Subakan et al., 2021; Lutati et al., 2022; Scheibler et al., 2022), the objective function is the Scale-Invariant SDR. The scale-invariant SDR is agnostic to the scale of the estimated signal, and also to spurious errors, as presented in (Le Roux et al., 2019). First, project the source onto the estimated signal.\nv\u0303 = < v, v\u0304 > v\n||v||2 (27)\nSecond, compute the normalized error e\u0303 = v\u0304 \u2212 v\u0303 (28)\nThird, the scale-invariant SDR is computed as follows, SI \u2212 SDR(v, v\u0304) = 10log10 ( ||v\u0303||2\n||e\u0303||2\n) (29)\nFor a large number of speakers (C \u2265 10), the Hungarian algorithm obtains better results than the permutation invariant loss, by explicitly assigning the different estimated sources to the ground truth sources (Dovrat et al., 2021). Therefore, network F is trained using the assignment obtained by the Hungarian method for the deterministic voice separation network."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "For all datasets, the same alignment network architecture is employed, with the same hyper-parameters. It is a CNN with six layers, 3\u00d7 3 kernels, and 32,32,64,64,64 hidden channels. For Librimix and WSJ0 datasets the DiffWave was trained separately over the training sets\u2019 sources (no mixing). The pretrained models for separation are taken from the official publication when available and from HuggingFace hub otherwise. The optimization procedure is done with Adam (Kingma & Ba, 2015) optimizer with a learning rate of 1E-3 and a batch size of 3. The setting involved 3 A5000 GPUs.\nDatasets The LibriSpeech dataset (Panayotov et al., 2015) is a large corpus of read English speech, designed for training and evaluating automatic speech recognition systems. It consists of over 1000 hours of audio from audiobooks read by professional and non-professional speakers. The Wall Street Journal (WSJ0) (Garofolo, John S. et al., 1993) dataset is a collection of read English speech samples and is widely used by the research community. The dataset includes a total of 80 hours of audio.\nFollowing previous work (Subakan et al., 2021; Dovrat et al., 2021), mixtures are generated by a random process. The speakers are divided into training speakers and test speakers. A training- or a test sample is created by combining random speakers out of the respective set of speakers, with random SNR values between 0\u2212 5 dB.\nBaseline methods We compare our method to the state-of-the-art methods in voice separation, including DiffSep (Scheibler et al., 2022) that employs diffusion models, SepFormer (Subakan et al., 2021), which is a transformer model, and SepIt (Lutati et al., 2022), which extends the Gated-LSTM method (Nachmani et al., 2020) by running it in an iterative manner.\nAs a generative model our experiments employ a pretrained DiffWave model (Kong et al., 2020b).\nIn addition to running our method, we perform an ablation that is aimed at verifying the need for a learned combination model F . In it, we align (recover the phase) the generative approximation with the deterministic one and then average the two. Another ablation is using the GAN-based model HiFiGAN (Kong et al., 2020a), trained on WSJ0, which is a deterministic generative model (no noise is given to the generator). Since this generative model is deterministic the previous upper bound applies and we verify experimentally that no significant improvement is obtained.\nFirst, the cross-correlation function is used as a measure of similarity. In the frequency domain, the cross-correlation dual is computed using conjugate multiplication.\nR\u0304 = V\u0304d V\u0304 \u2217g , (30)\nwhere R\u0304 is the frequency domain. For each segment, the inverse Fourier transform is conducted, and then the time for which the signal is at maximum is searched.\nt\u2217i = arg max(IFFT (R\u0304[I]) \u2200i \u2208 {1, . . . ,K} , (31)\nwhere t\u2217i is the time delay within the i-th segment, R\u0304[I] is the i-th frequency domain cross correlation segment.\nA complex phase shift corresponding to the time delay found per segment is computed via the following,\nTi = exp(j(2\u03c0ft \u2217 i )) \u2200i \u2208 {1, . . . ,K} , (32)\nwhere Ti is the complex Fourier factor of time delay, and f is the frequency that matches each frequency bin in the spectral transform.\nThe combination of the two estimations now reads,\nV\u0304xcorr = V\u0304d + T V\u0304g (33)\nThe time domain signal is obtained as before, using the inverse short-time Fourier transform.\nv\u0304xcorr = iSTFT (V\u0304xcorr) (34)\nQualitative Results A visualization of v\u0304d and v\u0304g is depicted in Fig. 3. Evidently, both signals have a phase difference, which changes over time. For example, in panel (b), at [0.52,0.56] the phase of the generative estimation tracks the phase of the deterministic model. At the other end, at [0.56,0.64], the phase between the signal is offset by \u03c0. Recall that while v\u0304d is given the full information about the phase, v\u0304g is given only magnitude information and thus the absolute phase is not synched. Therefore, the alignment procedure is necessary.\nThe generated signal, v\u0304g, does not track the deterministic part, v\u0304d, one-to-one, but sometimes fixes sudden errors of other sources that the deterministic model could not handle correctly. This phenomenon is depicted in Fig. 3(c), where the deterministic signal has some bursts. As can be seen, the combined signal is more related to the ground truth signal.\nThe supplementary presents samples of both the deterministic and the generative estimation. While the deterministic estimation presents less background noise, it misses multiple temporal parts, which are filtered out by the consecutive chain of static filters of the deterministic model. The signal produced by the generative model, on the other hand, maintains a considerable amount of background noise but is able to sample parts that were previously omitted. The combination of both estimations yields a signal that is closer to the desired source.\nTo estimate the relative quality of the deterministic and generative estimations, the Mean Square Error between the different estimations and the aligned sources is calculated in short segments of 20[ms]. The histogram depicted in Fig. 4(a) shows that the generative signal v\u0304g has a slight tendency to greater error, but has the same performance overall. This is in agreement with Eq. 16, where we expect that both v\u0304g, and v\u0304d obtain similar performances. This result confirms that a combination of both estimations should improve the overall estimation of sources. A scatter plot of the errors is depicted in Fig. 4(b). The red line indicates a ratio of one to one between the deterministic error and the generative one. As can be seen, the error of the generative estimation is within the same range as that of the deterministic estimation, and there is no estimation that is markedly better than the other.\nSource separation results The voice separation results for all the benchmarks are depicted in Tab. 1. For the WSJ0 dataset, when separating a mixture of two sources, the improvement in terms of SI-SDR is 1.5dB over the current state-of-the-art. Using our method not only improved the SI-SDR beyond all other methods but also broke the previous bound for deterministic models. This\nemphasizes the strength of the diffusion models as holding independent modeling benefits that can be used to improve speech separation. For three sources, separation is improved by 1dB, again obtaining state-of-the-art results, but somewhat lower than the classical bound of deterministic methods. We note that the ablation that removes network F and introduces a heuristic alignment and equal mixing performs considerably better than the deterministic part (the SepFormer baseline) but only slightly better than the iterative SepIt. The deterministic HiFiGAN ablation shows little to no improvement, supporting the divide we make between deterministic and nondeterministic models. However, the non-deterministic GAN-based UnivNet Jang et al. (2021) provides results that are similar to those obtained with DiffWave, further strengthening our claim that non-deterministic methods can improve the performance of deterministic ones.\nFor LibriSpeech we use an available SepFormer model as a baseline in the case of two speakers, and for the other mixtures, an available Gated LSTM model. As can be seen, for all numbers of sources, a major improvement is obtained over the current state of the art. The alignment ablation degrades the separation result and is inferior to the learned method, but is still competitive. Evidently, there is still room for improving separation methods. For five speakers, the result obtained falls 0.3dB short of surpassing the classical upper bound. For other mixtures, the gap is larger.\nSince the output of the diffusion process is conditioned on the random seed used, the option to run the randomized part multiple times is readily available. To explore this, we present the results obtained by simply averaging the output of five runs of the DiffWave with different noise initializations. As can be seen in the table, this leads to a small but consistent improvement, which is, as expected, still lower than the generative upper bound. Another question is the suitability of the method for working with noisy datasets, which is explored in Appendix C. As shown, our method can improve the state of the art method on that benchmark as well."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "A general upper bound for source separation using generative models is proposed. The generalization of the upper bound suggests that for a pretrained generative model, a maximal improvement of 3dB from the previous deterministic bound is achievable. In addition, a simple yet effective method is suggested, where the deterministic signals and the generated signals are combined in the frequency\ndomain. The combination procedure is learned and shown to be superior to the classical alignment method. When tested on various numbers of speakers, the estimation always yields improvement. For two sources, the separation is able to surpass the previous upper bound, demonstrating the need to go beyond a deterministic model.\nThe upper bound derived here is general in the sense that a similar bound would hold when the underlying distributions are replaced to fit other data domains. This is left for future work."
        },
        {
            "heading": "A PROOF OF LEMMA 3.2",
            "text": "The proof of Lemma 3.2 in the main text is below.\nProof. Using the definition of mutual information, and chain rule for entropy\nI(X;Y,Z) = H(Y ;Z)\u2212H(Y ;Z|X) = H(Z) +H(Y |Z)\u2212H(Z|X)\u2212H(Y |X,Z) = (H(Z)\u2212H(Z|X)) + (H(Y |Z)\u2212H(Y |X,Z)) = I(X;Z) + I(X;Y |Z)\n(35)"
        },
        {
            "heading": "B SAMPLE SPECTROGRAMS",
            "text": "Figure 5 presents sample spectrograms. Shown are the source, the deterministic estimation, the estimation obtained by the generative model, and their combination in the frequency domain using the coefficients produced by the function F ."
        },
        {
            "heading": "C ADDITIONAL RESULTS",
            "text": "C.1 RESULTS ON THE WHAM! BENCHMARK\nThe WHAM! dataset is a collection of two-speaker mixtures with background noise, designed to be a challenging benchmark for speech separation systems. The dataset was created by mixing speech from the WSJ0 dataset with noise from the WHAM! noise dataset, which contains recordings of urban environments such as restaurants, cafes, bars, and parks. In Tab. 2 utilizing a pretrained SepFormer model and a pretrained DiffWave, our method is able to surpass current state of the art result.\nC.2 ADDITIONAL ABLATIONS\nTo assert that the additional parameters in the alignment network are not the lead cause for the improvement, five ablations of running the alignment network over the generative output alone or the deterministic output alone are done: (a) without aligning the phase to the source signal, (b) Linearly aligning of the phase, (c) using the alignment network with vd twice, (d) using the alignment network with vd and phase-shifted vd, and (e) retraining the alignment network with only the absolute phase without the difference of the phase and then running as in our original method.\nThe reported results are in Tab. 3. In all single module variances (a-d), the performance of the reconstruction is worse than the reconstruction using the combination of both the deterministic and generative samples. Ablation (e) demonstrates that the phrase difference is an important input to the alignment network."
        }
    ],
    "year": 2023
}