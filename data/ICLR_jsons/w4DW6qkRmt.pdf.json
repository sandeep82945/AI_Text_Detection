{
    "abstractText": "Large language models (LLMs) have made significant advancements in various natural language processing tasks, including question answering (QA) tasks. While incorporating new information with the retrieval of relevant passages is a promising way to improve QA with LLMs, the existing methods often require additional fine-tuning which becomes infeasible with recent LLMs. Augmenting retrieved passages of QA via prompting has the potential to address this limitation, but this direction has been limitedly explored. To this end, we design a simple yet effective framework to enhance open-domain QA (ODQA) with LLMs, based on the summarized retrieval (SURE). SURE helps LLMs predict more accurate answers for a given question, which are well-supported by the summarized retrieval that could be viewed as an explicit rationale extracted from the retrieved passages. Specifically, SURE first constructs summaries of the retrieved passages for each of the multiple answer candidates. Then, SURE confirms the most plausible answer from the candidate set by evaluating the validity and ranking of the generated summaries. Experimental results on diverse ODQA benchmarks demonstrate the superiority of SURE, with improvements of up to 4.4% in exact match (EM) and 3.9% in F1 score over standard prompting approaches. SURE also can be integrated with a broad range of retrieval methods and LLMs. Finally, the generated summaries from SURE show additional advantages to measure the importance of retrieved passages and serve as more preferred rationales by models and humans.",
    "authors": [
        {
            "affiliations": [],
            "name": "SUMMARIZING RETRIEVALS"
        }
    ],
    "id": "SP:e8f133e87b94e5e45e8397215e189e941f2c4b8b",
    "references": [
        {
            "authors": [
                "Jonathan Berant",
                "Andrew Chou",
                "Roy Frostig",
                "Percy Liang"
            ],
            "title": "Semantic parsing on freebase from question-answer pairs",
            "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2013
        },
        {
            "authors": [
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Jordan Hoffmann",
                "Trevor Cai",
                "Eliza Rutherford",
                "Katie Millican",
                "George Bm Van Den Driessche",
                "Jean-Baptiste Lespiau",
                "Bogdan Damoc",
                "Aidan Clark"
            ],
            "title": "Improving language models by retrieving from trillions of tokens",
            "venue": "In Proceedings of the International Conference on Machine Learning (ICML). PMLR,",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Danqi Chen",
                "Adam Fisch",
                "Jason Weston",
                "Antoine Bordes"
            ],
            "title": "Reading wikipedia to answer opendomain questions",
            "venue": "In Annual Meeting of the Association for Computational Linguistics (ACL),",
            "year": 2017
        },
        {
            "authors": [
                "Lingjiao Chen",
                "Matei Zaharia",
                "James Zou"
            ],
            "title": "Frugalgpt: How to use large language models while reducing cost and improving performance",
            "venue": "arXiv preprint arXiv:2305.05176,",
            "year": 2023
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harri Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "venue": "arXiv preprint arXiv:2107.03374,",
            "year": 2021
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Gobinda G Chowdhury"
            ],
            "title": "Introduction to modern information retrieval",
            "venue": "Facet publishing,",
            "year": 2010
        },
        {
            "authors": [
                "Yung-Sung Chuang",
                "Wei Fang",
                "Shang-Wen Li",
                "Wen-tau Yih",
                "James Glass"
            ],
            "title": "Expand, rerank, and retrieve: Query reranking for open-domain question answering",
            "venue": "In Findings of the Association for Computational Linguistics (ACL),",
            "year": 2023
        },
        {
            "authors": [
                "Antonia Creswell",
                "Murray Shanahan",
                "Irina Higgins"
            ],
            "title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Ameet Deshpande",
                "Vishvak Murahari",
                "Tanmay Rajpurohit",
                "Ashwin Kalyan",
                "Karthik Narasimhan"
            ],
            "title": "Toxicity in chatgpt: Analyzing persona-assigned language models",
            "venue": "arXiv preprint arXiv:2304.05335,",
            "year": 2023
        },
        {
            "authors": [
                "Nan Du",
                "Yanping Huang",
                "Andrew M Dai",
                "Simon Tong",
                "Dmitry Lepikhin",
                "Yuanzhong Xu",
                "Maxim Krikun",
                "Yanqi Zhou",
                "Adams Wei Yu",
                "Orhan Firat"
            ],
            "title": "Glam: Efficient scaling of language models with mixture-of-experts",
            "venue": "In Proceedings of the International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Bradley Efron",
                "Robert J Tibshirani"
            ],
            "title": "An introduction to the bootstrap",
            "venue": "CRC press,",
            "year": 1994
        },
        {
            "authors": [
                "Tianyu Gao",
                "Howard Yen",
                "Jiatong Yu",
                "Danqi Chen"
            ],
            "title": "Enabling large language models to generate text with citations",
            "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2023
        },
        {
            "authors": [
                "John Giorgi",
                "Luca Soldaini",
                "Bo Wang",
                "Gary Bader",
                "Kyle Lo",
                "Lucy Lu Wang",
                "Arman Cohan"
            ],
            "title": "Open domain multi-document summarization: A comprehensive study of model brittleness under retrieval",
            "venue": "In Findings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2023
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Mingwei Chang"
            ],
            "title": "Retrieval augmented language model pre-training",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Steven Basart",
                "Andy Zou",
                "Mantas Mazeika",
                "Dawn Song",
                "Jacob Steinhardt"
            ],
            "title": "Measuring massive multitask language understanding",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Xanh Ho",
                "Anh-Khoa Duong Nguyen",
                "Saku Sugawara",
                "Akiko Aizawa"
            ],
            "title": "Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps",
            "venue": "arXiv preprint arXiv:2011.01060,",
            "year": 2020
        },
        {
            "authors": [
                "Gautier Izacard",
                "Mathilde Caron",
                "Lucas Hosseini",
                "Sebastian Riedel",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Edouard Grave"
            ],
            "title": "Unsupervised dense information retrieval with contrastive learning",
            "venue": "In Transactions on Machine Learning Research (TMLR),",
            "year": 2022
        },
        {
            "authors": [
                "Gautier Izacard",
                "Patrick Lewis",
                "Maria Lomeli",
                "Lucas Hosseini",
                "Fabio Petroni",
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Armand Joulin",
                "Sebastian Riedel",
                "Edouard Grave"
            ],
            "title": "Few-shot learning with retrieval augmented language models",
            "venue": "In Journal of Machine Learning Research (JMLR),",
            "year": 2023
        },
        {
            "authors": [
                "Ehsan Kamalloo",
                "Nouha Dziri",
                "Charles LA Clarke",
                "Davood Rafiei"
            ],
            "title": "Evaluating open-domain question answering in the era of large language models",
            "venue": "In Annual Meeting of the Association for Computational Linguistics (ACL),",
            "year": 2023
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih"
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Lee Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "In Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),",
            "year": 2019
        },
        {
            "authors": [
                "Omar Khattab",
                "Christopher Potts",
                "Matei Zaharia"
            ],
            "title": "Baleen: Robust multi-hop reasoning at scale via condensed retrieval",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Boseop Kim",
                "HyoungSeok Kim",
                "Sang-Woo Lee",
                "Gichang Lee",
                "Donghyun Kwak",
                "Jeon Dong Hyeon",
                "Sunghyun Park",
                "Sungju Kim",
                "Seonhoon Kim",
                "Dongpil Seo"
            ],
            "title": "What changes can largescale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers",
            "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2021
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Tom Kwiatkowski",
                "Jennimaria Palomaki",
                "Olivia Redfield",
                "Michael Collins",
                "Ankur Parikh",
                "Chris Alberti",
                "Danielle Epstein",
                "Illia Polosukhin",
                "Jacob Devlin",
                "Kenton Lee"
            ],
            "title": "Natural questions: a benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Angeliki Lazaridou",
                "Elena Gribovskaya",
                "Wojciech Stokowiec",
                "Nikolai Grigorev"
            ],
            "title": "Internetaugmented language models through few-shot prompting for open-domain question answering",
            "venue": "arXiv preprint arXiv:2203.05115,",
            "year": 2022
        },
        {
            "authors": [
                "Ronan Le Bras",
                "Swabha Swayamdipta",
                "Chandra Bhagavatula",
                "Rowan Zellers",
                "Matthew Peters",
                "Ashish Sabharwal",
                "Yejin Choi"
            ],
            "title": "Adversarial filters of dataset biases",
            "venue": "In Proceedings of the International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Kenton Lee",
                "Ming-Wei Chang",
                "Kristina Toutanova"
            ],
            "title": "Latent retrieval for weakly supervised open domain question answering",
            "venue": "In Annual Meeting of the Association for Computational Linguistics (ACL),",
            "year": 2019
        },
        {
            "authors": [
                "Patrick Lewis",
                "Yuxiang Wu",
                "Linqing Liu",
                "Pasquale Minervini",
                "Heinrich K\u00fcttler",
                "Aleksandra Piktus",
                "Pontus Stenetorp",
                "Sebastian Riedel"
            ],
            "title": "Paq: 65 million probably-asked questions and what you can do with them",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Jack Hessel",
                "Youngjae Yu",
                "Xiang Ren",
                "Kai-Wei Chang",
                "Yejin Choi"
            ],
            "title": "Symbolic chain-of-thought distillation: Small models can also",
            "venue": "think\u201d step-by-step. In Annual Meeting of the Association for Computational Linguistics (ACL),",
            "year": 2023
        },
        {
            "authors": [
                "Jiongnan Liu",
                "Jiajie Jin",
                "Zihan Wang",
                "Jiehan Cheng",
                "Zhicheng Dou",
                "Ji-Rong Wen"
            ],
            "title": "Reta-llm: A retrieval-augmented large language model toolkit",
            "venue": "arXiv preprint arXiv:2306.05212,",
            "year": 2023
        },
        {
            "authors": [
                "Nelson F Liu",
                "Kevin Lin",
                "John Hewitt",
                "Ashwin Paranjape",
                "Michele Bevilacqua",
                "Fabio Petroni",
                "Percy Liang"
            ],
            "title": "Lost in the middle: How language models use long contexts",
            "venue": "arXiv preprint arXiv:2307.03172,",
            "year": 2023
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu"
            ],
            "title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment",
            "venue": "arXiv preprint arXiv:2303.16634,",
            "year": 2023
        },
        {
            "authors": [
                "Jianguo Mao",
                "Wenbin Jiang",
                "Xiangdong Wang",
                "Hong Liu",
                "Yu Xia",
                "Yajuan Lyu",
                "Qiaoqiao She"
            ],
            "title": "Explainable question answering based on semantic graph by global differentiable learning and dynamic adaptive reasoning",
            "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2022
        },
        {
            "authors": [
                "Yuning Mao",
                "Pengcheng He",
                "Xiaodong Liu",
                "Yelong Shen",
                "Jianfeng Gao",
                "Jiawei Han",
                "Weizhu Chen"
            ],
            "title": "Reader-guided passage reranking for open-domain question answering",
            "venue": "In Findings of the Association for Computational Linguistics (ACL),",
            "year": 2021
        },
        {
            "authors": [
                "Gr\u00e9goire Mialon",
                "Roberto Dess\u0131",
                "Maria Lomeli",
                "Christoforos Nalmpantis",
                "Ram Pasunuru",
                "Roberta Raileanu",
                "Baptiste Rozi\u00e8re",
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Asli Celikyilmaz"
            ],
            "title": "Augmented language models: a survey",
            "venue": "In Transactions on Machine Learning Research (TMLR),",
            "year": 2023
        },
        {
            "authors": [
                "Sewon Min",
                "Weijia Shi",
                "Mike Lewis",
                "Xilun Chen",
                "Wen-tau Yih",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Nonparametric masked language modeling",
            "venue": "arXiv preprint arXiv:2212.01349,",
            "year": 2022
        },
        {
            "authors": [
                "Sebastian Nagel"
            ],
            "title": "Common crawl news dataset. https://data.commoncrawl.org/ crawl-data/CC-NEWS/index.html, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Jaehyun Nam",
                "Woomin Song",
                "Seong Hyeon Park",
                "Jihoon Tack",
                "Sukmin Yun",
                "Jaehyung Kim",
                "Jinwoo Shin"
            ],
            "title": "Semi-supervised tabular classification via in-context learning of large language models",
            "venue": "In Workshop on Efficient Systems for Foundation Models@",
            "year": 2023
        },
        {
            "authors": [
                "Tri Nguyen",
                "Mir Rosenberg",
                "Xia Song",
                "Jianfeng Gao",
                "Saurabh Tiwary",
                "Rangan Majumder",
                "Li Deng"
            ],
            "title": "Ms marco: A human-generated machine reading comprehension dataset",
            "venue": "arXiv preprint arXiv:1611.09268,",
            "year": 2016
        },
        {
            "authors": [
                "Jianmo Ni",
                "Chen Qu",
                "Jing Lu",
                "Zhuyun Dai",
                "Gustavo Hern\u00e1ndez \u00c1brego",
                "Ji Ma",
                "Vincent Y Zhao",
                "Yi Luan",
                "Keith B Hall",
                "Ming-Wei Chang"
            ],
            "title": "Large dual encoders are generalizable retrievers",
            "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2022
        },
        {
            "authors": [
                "Krishna Pillutla",
                "Swabha Swayamdipta",
                "Rowan Zellers",
                "John Thickstun",
                "Sean Welleck",
                "Yejin Choi",
                "Zaid Harchaoui. Mauve"
            ],
            "title": "Measuring the gap between neural text and human text using divergence frontiers",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Zhen Qin",
                "Rolf Jagerman",
                "Kai Hui",
                "Honglei Zhuang",
                "Junru Wu",
                "Jiaming Shen",
                "Tianqi Liu",
                "Jialu Liu",
                "Donald Metzler",
                "Xuanhui Wang"
            ],
            "title": "Large language models are effective text rankers with pairwise ranking prompting",
            "venue": "arXiv preprint arXiv:2306.17563,",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research (JMLR),",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang"
            ],
            "title": "Squad: 100,000+ questions for machine comprehension of text",
            "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2016
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych"
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bertnetworks",
            "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2019
        },
        {
            "authors": [
                "Stephen Robertson",
                "Hugo Zaragoza"
            ],
            "title": "The probabilistic relevance framework: Bm25 and beyond",
            "venue": "Foundations and Trends\u00ae in Information Retrieval,",
            "year": 2009
        },
        {
            "authors": [
                "Joshua Robinson",
                "Christopher Michael Rytting",
                "David Wingate"
            ],
            "title": "Leveraging large language models for multiple choice question answering",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Shibani Santurkar",
                "Esin Durmus",
                "Faisal Ladhak",
                "Cinoo Lee",
                "Percy Liang",
                "Tatsunori Hashimoto"
            ],
            "title": "Whose opinions do language models reflect",
            "venue": "In Proceedings of the International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Timo Schick",
                "Sahana Udupa",
                "Hinrich Sch\u00fctze"
            ],
            "title": "Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Weijia Shi",
                "Sewon Min",
                "Michihiro Yasunaga",
                "Minjoon Seo",
                "Rich James",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Wen-tau Yih"
            ],
            "title": "Replug: Retrieval-augmented black-box language models",
            "venue": "arXiv preprint arXiv:2301.12652,",
            "year": 2023
        },
        {
            "authors": [
                "Chenglei Si",
                "Zhe Gan",
                "Zhengyuan Yang",
                "Shuohang Wang",
                "Jianfeng Wang",
                "Jordan Boyd-Graber",
                "Lijuan Wang"
            ],
            "title": "Prompting gpt-3 to be reliable",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Ivan Stelmakh",
                "Yi Luan",
                "Bhuwan Dhingra",
                "Ming-Wei Chang"
            ],
            "title": "Asqa: Factoid questions meet long-form answers",
            "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2022
        },
        {
            "authors": [
                "Dan Su",
                "Xiaoguang Li",
                "Jindi Zhang",
                "Lifeng Shang",
                "Xin Jiang",
                "Qun Liu",
                "Pascale Fung"
            ],
            "title": "Read before generate! faithful long form question answering with machine reading. In Findings of the Association for Computational Linguistics (ACL), 2022a",
            "year": 2022
        },
        {
            "authors": [
                "Yixuan Su",
                "Tian Lan",
                "Yan Wang",
                "Dani Yogatama",
                "Lingpeng Kong",
                "Nigel Collier"
            ],
            "title": "A contrastive framework for neural text generation",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Weiwei Sun",
                "Lingyong Yan",
                "Xinyu Ma",
                "Pengjie Ren",
                "Dawei Yin",
                "Zhaochun Ren"
            ],
            "title": "Is chatgpt good at search? investigating large language models as re-ranking agent",
            "venue": "arXiv preprint arXiv:2304.09542,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "arXiv preprint arXiv:2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Harsh Trivedi",
                "Niranjan Balasubramanian",
                "Tushar Khot",
                "Ashish Sabharwal"
            ],
            "title": "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
            "venue": "In Annual Meeting of the Association for Computational Linguistics (ACL),",
            "year": 2023
        },
        {
            "authors": [
                "Ellen M Voorhees"
            ],
            "title": "The trec-8 question answering track report",
            "venue": "In Trec,",
            "year": 1999
        },
        {
            "authors": [
                "Yizhong Wang",
                "Hamish Ivison",
                "Pradeep Dasigi",
                "Jack Hessel",
                "Tushar Khot",
                "Khyathi Raghavi Chandu",
                "David Wadden",
                "Kelsey MacMillan",
                "Noah A Smith",
                "Iz Beltagy"
            ],
            "title": "How far can camels go? exploring the state of instruction tuning on open resources",
            "venue": "arXiv preprint arXiv:2306.04751,",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Sean Welleck",
                "Ilia Kulikov",
                "Stephen Roller",
                "Emily Dinan",
                "Kyunghyun Cho",
                "Jason Weston"
            ],
            "title": "Neural text generation with unlikelihood training",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Yixuan Weng",
                "Minjun Zhu",
                "Shizhu He",
                "Kang Liu",
                "Jun Zhao"
            ],
            "title": "Large language models are reasoners with self-verification",
            "venue": "arXiv preprint arXiv:2212.09561,",
            "year": 2022
        },
        {
            "authors": [
                "Spencer Whitehead",
                "Suzanne Petryk",
                "Vedaad Shakib",
                "Joseph Gonzalez",
                "Trevor Darrell",
                "Anna Rohrbach",
                "Marcus Rohrbach"
            ],
            "title": "Reliable visual question answering: Abstain rather than answer incorrectly",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2022
        },
        {
            "authors": [
                "Dao Xuan-Quy",
                "Le Ngoc-Bich",
                "Phan Xuan-Dung",
                "Ngo Bac-Bien",
                "Vo The-Duy"
            ],
            "title": "Evaluation of chatgpt and microsoft bing ai chat performances on physics exams of vietnamese national high school graduation examination",
            "venue": "arXiv preprint arXiv:2306.04538,",
            "year": 2023
        },
        {
            "authors": [
                "Yiben Yang",
                "Chaitanya Malaviya",
                "Jared Fernandez",
                "Swabha Swayamdipta",
                "Ronan Le Bras",
                "Ji-Ping Wang",
                "Chandra Bhagavatula",
                "Yejin Choi",
                "Doug Downey"
            ],
            "title": "Generative data augmentation for commonsense reasoning",
            "year": 2004
        },
        {
            "authors": [
                "Zhilin Yang",
                "Peng Qi",
                "Saizheng Zhang",
                "Yoshua Bengio",
                "William Cohen",
                "Ruslan Salakhutdinov",
                "Christopher D Manning"
            ],
            "title": "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
            "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2018
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh"
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "In Proceedings of the International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "Yukun Zhu",
                "Ryan Kiros",
                "Rich Zemel",
                "Ruslan Salakhutdinov",
                "Raquel Urtasun",
                "Antonio Torralba",
                "Sanja Fidler"
            ],
            "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2015
        },
        {
            "authors": [
                "Recently",
                "Robinson"
            ],
            "title": "multiple-choice prompts generally elicit much more accurate responses than do cloze prompts, for LLMs with high multiple-choice symbol binding ability like OpenAI",
            "venue": "Codex (Chen et al.,",
            "year": 2021
        },
        {
            "authors": [
                "Shi"
            ],
            "title": "2023), to make it easy to notice the significance of our results. Nevertheless, other factors like response coherence, relevance, and efficiency are important metrics to be considered. To evaluate these aspects, we have conducted additional evaluations with LLMs approaches. Specifically, we measured two additional metrics: (1) MAUVE (Pillutla et al., 2021",
            "venue": "LLM-acc (Kamalloo et al.,",
            "year": 2023
        },
        {
            "authors": [
                "Gao"
            ],
            "title": "Methods / Metrics ROUGE-L STR-EM MAUVE Base",
            "venue": "GTR (Ni et al., 2022),",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Large language models (LLMs) (Brown et al., 2020; Touvron et al., 2023b) have significantly accelerated progress in natural language processing (NLP) and have become a core technology in various real-world applications used by millions of users, such as coding assistants (Chen et al., 2021), search engines (Xuan-Quy et al., 2023), and chatbots (Kim et al., 2021; OpenAI, 2022). However, LLMs often suffer from limitations, such as non-factual but seemingly plausible generation, referred to as hallucinations (Welleck et al., 2020), and difficulty in integrating up-to-date knowledge, as their learned knowledge is limited by the training corpus encoded in their parameters (Guu et al., 2020). This problem is particularly critical for question answering (QA) (Kwiatkowski et al., 2019), one of the most frequently encountered applications for LLMs.\nIncorporating new information through the retrieval of relevant knowledge for a given query (e.g., a question from users) is widely explored to improve the accuracy of QA systems, called opendomain QA (ODQA) (Karpukhin et al., 2020), and shows promise in addressing the aforementioned limitations of LLMs (Mialon et al., 2023). Constructing these retrieval-augmented LLMs typically involves additional fine-tuning (Borgeaud et al., 2022; Izacard et al., 2023), but it becomes infeasible due to the increase in scale and the recent nature of black-box API (OpenAI, 2023). Consequently, retrieval augmentation via prompting, i.e., giving specific instruction as the input to obtain the desired outputs by LLM, becomes an attractive direction from its simplicity and efficiency (Shi et al., 2023). However, na\u0131\u0308ve prompting could be limited in fully exploiting the retrieved contexts, since LLMs are simply instructed to use the retrieved information, instead of being explicitly trained to use it; for example, Liu et al. (2023b) recently observed that LLMs struggle to handle long input contexts when they are na\u0131\u0308vely appended. Despite its importance, how to improve retrieval-augmented LLMs via prompting has been under-explored. Therefore, to improve ODQA via LLMs, we aim\nto develop a simple yet effective framework based on prompting, that could be easily applicable to various LLMs and retrieval methods.\nContribution. We propose a framework based on Summarized Retrieval (SURE), to improve ODQA performance of retrieval-augmented LLMs. At a high level, SURE helps LLMs predict more grounded answers, which are well-supported by the summarization of retrieved passages that could be viewed as an explicit rationale extracted from the retrieved passages. To be specific, SURE first constructs the multiple summarizations of retrieved passages conditioned on each of a few possible answer candidates. It enables LLMs to focus on the specific contexts relevant to the given candidate, and hence provides more discriminative viewpoints for the given question. Then, using the generated summarizations, SURE confirms the most plausible answer among candidates by measuring the corresponding summaries\u2019 validity to support the given candidate and ranking of relative informativeness to answer the question. Remarkably, all the procedures of SURE are conducted via zero-shot prompting. Consequently, SURE is widely applicable when LLMs are only accessible with black-box API, even without query-relevant few-shot examples.\nThrough the experiments on four different QA datasets, we demonstrate the effectiveness of SURE for improving the zero-shot ODQA performance of retrieval-augmented LLMs. For example, we observe that the augmentation of 10 relevant passages effectively improves QA accuracy (up to 8.2% with Contriver (Izacard et al., 2022)) of ChatGPT (OpenAI, 2022), and the gain is significantly enlarged with SURE (up to 12.8%), as shown in Figure 1. Overall, SURE with ChatGPT and BM25 (Robertson et al., 2009) exhibited 4.4%/3.9% exact match (EM)/F1 score improvements compared to the standard prompting in average on four ODQA datasets. In addition, SURE is well generalized to different configurations of various retrieval methods and LLMs. More interestingly, we observe that the generated summarization by SURE could be further utilized to evaluate the importance of the retrieved passages, and also verify that it has a higher model/human preference as a rationale for the given\nprediction, compared to the generic summarization of retrieved passages. Overall, these results highlight the effectiveness of SURE, to improve ODQA systems based on LLMs, not only in terms of accuracy but also of additional advantages that can improve the user experience. We, therefore, hope that the proposed framework could be beneficial in various real-world applications."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Open-domain question answering. Open-domain question answering (ODQA) (Voorhees et al., 1999) is a task that requires responding to factual questions using external knowledge sources (Zhu et al., 2015; Nagel, 2016). Recently, there has been significant research interest in ODQA systems, under a framework known as the retriever-and-read system (Chen et al., 2017). The role of retriever is to extract the relevant pieces of information from the given knowledge sources. For the retriever, there are two different popular methods: one is a lexical-based retriever, e.g., TF-IDF or BM25 (Robertson et al., 2009), and the other is a sentence embedding-based retriever such as DPR (Karpukhin et al., 2020) or Contriver (Izacard et al., 2022). On the other hand, the reader is responsible for aggregating and reasoning with the retrieved information to generate answers. Usually, recent transformer-based language models (LMs) such as BERT (Kenton & Toutanova, 2019) or T5 (Raffel et al., 2020) are widely adopted for the reader after fine-tuning. In contrast, LLMs exhibit comparable performance or outperform in QA without fine-tuning (Kamalloo et al., 2023; Shi et al., 2023), which indicates a potential to serve as a universal QA system (Xuan-Quy et al., 2023).\nRetrieval-augmented language models. Similar to enhancing QA systems with retriever in ODQA, augmenting LMs with relevant information retrieved from external knowledge sources has been\ndemonstrated as an effective way to improve the performance of LMs on various NLP tasks (Guu et al., 2020; Lazaridou et al., 2022; Min et al., 2022; Liu et al., 2023a), by reducing hallucination of LLMs and leveraging external knowledge which is not seen during pre-training. To construct such retrieval-augmented LMs, the standard approach is conducting additional fine-tuning to learn how to incorporate the retrieved information (Guu et al., 2020; Borgeaud et al., 2022; Izacard et al., 2023). However, when considering the recent nature of LLMs with increasing scale and providing black-box API only, such a direction becomes less attractive. One promising direction to address this challenge is investigating a better prompting (Brown et al., 2020), which incorporates the retrieved information as additional inputs in a sophisticated way. However, this direction has been only limitedly explored. Appending the retrieval (Si et al., 2023; Trivedi et al., 2023) is a common practice for prompting, but Liu et al. (2023b) recently revealed its limitation in utilizing the retrieved information. Aggregating the predictions from each retrieved passage has been also explored (Lazaridou et al., 2022; Shi et al., 2023), but LLMs can\u2019t see a full context of retrieved information in this case.\nOn the other hand, the summarization of retrieved passages has been considered in open-domain context; for example, there are recent works that propose to learn a module to selectively use the retrieved information in sentence- (Khattab et al., 2021; Su et al., 2022a) or passage-level (Mao et al., 2021; Chuang et al., 2023). In addition, Su et al. (2022a); Giorgi et al. (2023) form a new task that combines both passage retrieval and summarization for a given query, and Gao et al. (2023) considers summarization of information for prompting. However, these works require a large annotated dataset to extract the information specified to answer the question or construct the generic summarization which focuses on preserving the retrieved information within reduced texts."
        },
        {
            "heading": "3 SUMMARIZED RETRIEVAL FOR QUESTION ANSWERING",
            "text": ""
        },
        {
            "heading": "3.1 OVERVIEW AND PROBLEM DESCRIPTION",
            "text": "Overview. In this section, we present our framework, coined Summarized Retrieval (SURE) to enhance ODQA performance of LLMs, by proposing an improved way to incorporate retrieved passages for the prediction. Our main idea is to construct multiple summaries of the retrieved passages conditioned with each of a few answer candidates, and predict the most plausible candidate as the answer after evaluating the validity and relative informativeness of summaries. In Sections 3.2 and 3.3, we present the details to generate the summarizations and evaluate them.\nProblem description. Open-domain question answering (ODQA) is an extension of QA tasks that answer questions that require background knowledge by leveraging an external database. In order to answer the given question q, the ODQA system typically follows retrieve-and-read framework (Chen et al., 2017; Lee et al., 2019), where the retriever finds the informative passages C+N from the whole corpus C, and the reader exploits the retrieved passages to decide the answer a, which can be formulated as follows:\nC+N = Retriever(q, C,N) and a\u0302 = Reader(q, C + N ), (1)\nwhere N is the number of retrieved passages and a\u0302 is the predicted answer.\nIn this work, we focus on improving a prompting method for an LLM-based ODQA system. Specifically, we adopt the existing retriever method, e.g., BM25 (Robertson et al., 2009) or Contriever (Izacard et al., 2022), with the dataset-specific corpus. For the reader method, we use LLMs, denoted byM, such as ChatGPT (Sun et al., 2023) or LLaMA-2 (Touvron et al., 2023b), by incorporating the retrieved passages via prompting (Brown et al., 2020) without additional training. For example, with a prompt p(q, C+N ) = \u201cReading passages C + N , answer to question q\u201d,\nthe prediction a\u0302 is obtained fromM, i.e., a\u0302 =M ( p(q, C+N ) ) ."
        },
        {
            "heading": "3.2 CONDITIONAL SUMMARIZATION OF RETRIEVED PASSAGES",
            "text": "To better exploit the retrieved passages with LLMs, SURE first summarizes them conditioned on each of a few potential answer candidates. This conditional summarization of retrieved passages would include the specific contexts supporting a given answer candidate, compared to the generic summarization focusing on the wide coverage for the retrieved passages. Specifically, SURE first generates answer candidates and then conducts conditional summarization.\nCandidates generation. Given a question q, retrieved passages C+N , and LLMM, we first generate K answer candidates y\u0303 = [y\u03031, . . . , y\u0303K ] using a prompt pcan designed for candidate generation from q and C+N :\ny\u0303 =M ( pcan(q, C + N ) ) . (2)\nIn Figure 2, one can observe the example of generated candidates. It is noticeable that the previous works utilized stochastic decoding to generate multiple answer candidates (Lazaridou et al., 2022; Weng et al., 2022). However, we empirically observe that explicitly prompting an LLM to generate K potential candidates outputs more diverse and high-quality candidates.\nCandidate-conditioned summarization. Next, we conditionally summarize the retrieved passages C+N focusing on including the relevant contexts to validate each candidate y\u0303k \u2208 y\u0303 as an answer to q:\nsk =M ( psum(q, C + N , yk) ) for k = 1, . . . ,K (3)\nwhere psum is a prompt to obtain the conditional summarization sk from q, C+N , and y\u0303k. We present some examples of the generated summarizations in Figure 2, and more examples are in Appendix B. Remarkably, the generated summarizations effectively reduce the given passages by focusing on extracting the candidate-relevant contexts (e.g., 1035 words of retrieved passages \u2192 93 words of summarization). Also, we verify that the contexts of the generated summarization are specialized on a given answer candidate; when we measure TF-IDF (Chowdhury, 2010) based text similarity between two candidates and two conditional summarizations from each candidate (e.g., summarization #1 is generated to support an-\nswer candidate #1) on Natural Question dataset (Kwiatkowski et al., 2019) in Figure 3, the summarization exhibits a higher similarity with the corresponding candidate than the other candidate."
        },
        {
            "heading": "3.3 SELECTIVE PREDICTION VIA VERIFICATION OF SUMMARIZATIONS",
            "text": "Then, using the generated summarizations, SURE confirms the most plausible answer among the candidate set for the prediction. Our key intuition is that the quality (e.g., factuality, logicality, and\nAlgorithm 1 SURE algorithm 1: Input: Large language modelM, question q, N retrieved passages C+N , candidate number K 2: Answer Candidate Generation: y\u0303 =M ( pcan(q, C + N ) ) , y\u0303 = [y\u03031, . . . , y\u0303K ]\n3: Conditional Summarization: sk =M ( psum(q, C + N , yk) ) for k = 1, . . . ,K 4: Instance-wise Validation: v(sk)\u2190 Eq. 4 withM (pval(q, sk)) 5: Pair-wise Ranking: r(sk, SK), rpair(sk, si)\u2190 Eq. 5 withM (prank(q, sk, si)) 6: Output: Prediction a\u0302 = y\u0303k\u2217 , k\u2217 = argmaxk v(sk) + r(sk, SK)\nreadability) of the generated summarizations would vary depending on the plausibility of answer candidates, so as more plausible the answer, the corresponding summarization also will be more plausible. Then, LLMs can find the most plausible summarization among these multiple summarizations if a proper evaluation way is given. To this end, we propose to evaluate the generated summarizations with instance-wise validity and pair-wise ranking among them.\nInstance-wise validity. First, we evaluate the validity of each summarization sk whether it is not a degenerated case as the provided passages are not enough to support y\u0303k, or it properly supports the given answer candidate y\u0303k, rather than the other candidate y\u0303i, i \u0338= k.1 To be specific, we measure a validity vk of each summarization sk using a prompt pval designed for the validation:\nv(sk) = 1, whenM (pval(q, yk, sk)) = True or v(sk) = 0, else. (4)\nPair-wise ranking. In addition, we evaluate how the given summarization sk is relatively informative to answer the question q, among all summaries SK = {sk}Kk=1. To this end, we measure a ranking rk using a pair-wise ranking prompts (Qin et al., 2023; Sun et al., 2023):\nr(sk, SK) = K\u2211 i \u0338=k rpair(sk, si), rpair(sk, si) =  1, M (prank(q, sk, si)) = sk 0, M (prank(q, sk, si)) = si 0.5, else , (5)\nwhere prank is a prompt to determine which is relatively more informative one to answer the question by comparing two summaries. To prevent the order bias of LLMs (Zhao et al., 2021), we query the same pair of summaries twice by changing their order at the prompt prank.\nFinally, SURE makes a final prediction a\u0302 by incorporating both v(sk) and r(sk, SK):\na\u0302 = y\u0303k\u2217 , k \u2217 = argmax\nk v(sk) + r(sk, SK), (6)\ni.e., both validity and ranking scores are equally contributed. Algorithm 1 summarizes SURE and Figure 2 presents the specific example of QA procedure via SURE. We also highlight that the common prompts are shared across different datasets and LLMs, and the used prompts pcan, psum, pval, prank are presented in Appendix A."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we design our experiments to investigate the following questions:\n\u25e6 Does SURE improve the accuracy of LLMs on various ODQA datasets? (Table 1) \u25e6 Is SURE generalizable across various retrieval methods and LLMs? (Table 2) \u25e6 What is the effect of each component in SURE? (Table 3) \u25e6 Is SURE\u2019s summarization a good rationale for the answer? (Table 4 & Figure 4)"
        },
        {
            "heading": "4.1 SETUPS",
            "text": "Evaluation datasets. For all experiments, we measure zero-shot QA accuracy with the four different ODQA datasets: (1) Natural Questions (NQ) (Kwiatkowski et al., 2019), (2) WebQuestions (WebQ) (Berant et al., 2013), (3) 2WikiMulti-hopQA (2Wiki) (Ho et al., 2020), and (4) HotpotQA (Yang\n1We present such failure cases in Appendix D.\net al., 2018). For NQ and WebQ, we use their original test splits and 21M English Wikipedia dump (Karpukhin et al., 2020) as the source passages for the retrieval. For 2Wiki and HotpotQA, we use the subsampled splits released by Trivedi et al. (2023), along with the corresponding corpus for each data. For the experiments with LLaMA2-chat (Table 2) and more analyses (Section 4.3), we took 500 randomly subsampled examples of NQ and WebQ datasets for efficient experiments considering limited computing resources, and denoted these datasets NQ\u2217 and WebQ\u2217, respectively. As evaluation metrics, we calculate the exact match (EM) and F1 score. The EM accuracy is the ratio of correct answers in the test dataset, where a given prediction is considered correct if it coincides with one of the gold answers. The F1 score measures the overlap between bags of tokens in the prediction and the gold answer. We normalize the predictions and answers (i.e., case-folded, and punctuation) to compute the metrics, following the implementation of Rajpurkar et al. (2016).\nBaselines. We compare SURE with the following baselines. (1) No retrieval answers the question with LLMs without the retrieved passages (i.e., closed-book setup). (2) Base appends the retrieved passages as additional inputs of LLMs via prompting. (3) Line of works for better exploitation of retrieved passages with LLMs: Rerank (Lazaridou et al., 2022) and RePlug adopt an ensemble strategy that makes predictions based on each passage and then aggregates them with specific voting methods. Specifically, Rerank and RePlug utilize TF-IDF and sentence embedding from Contriever, respectively. (4) Adapt the works that incorporate intermediate reasoning steps for improved reasoning with LLMs, as summarizing could be viewed as a specific type of reasoning: Selection-inference (Creswell et al., 2023) measures the ranking of the passages, and conducts interactive answering by adding the passages one by one starting from higher ranked ones. Chain-of-thoughts (Kojima et al., 2022): we add zero-shot Chain-of-thoughts prompting (Wei et al., 2022) into the prompt of Base. Self-verification (Weng et al., 2022) generates answer candidates based on random sampling, then selects the most plausible one by verifying its reasoning with the question from conditional masking.\nImplementation details. For the experiments, we use three recent state-of-the-art LLMs: ChatGPT (gpt-3.5-turbo-0301) (OpenAI, 2022), GPT-4 (gpt-4-0613) (OpenAI, 2023), and LLaMA2-chat-70B (Touvron et al., 2023b). We use a temperature of 0.0 when calling the API or greedy decoding for LLaMA, to remove the effect of random sampling (Sun et al., 2023). For the retrieval methods, we use three different approaches: BM25 (Robertson et al., 2009), DPR-multi (DPR) (Karpukhin et al., 2020), and Contriever (Izacard et al., 2022). We use the implementations in Elasticsearch for BM25, and BEIR for DPR and Contriever, respectively.2 In the case of SURE, we use the same prompts across the different datasets, and they are presented in Appendix A. Also, we use a fixed value of K = 2 during the experiments since we observe that the improvements by increasing K are limited, as shown in Appendix B. When there are multiple candidates with equal plausibility (Eq. 6), then SURE selects the one generated earlier in Eq. 2."
        },
        {
            "heading": "4.2 MAIN RESULTS",
            "text": "Table 1 summarizes the experimental results on four different ODQA datasets, under ChatGPT with N = 10 retrieved passages using BM25. First, augmenting the retrieved passages with prompting is effective in improving ODQA accuracies of LLMs. For example, the average EM across four ODQA\n2https://www.elastic.co/, https://github.com/beir-cellar/beir\ndatasets is increased as 25.6 to 26.8. Somewhat surprisingly, we observe that Base outperforms other sophisticated baselines overall; this inefficiency of previous methods might be a result of a more challenging yet practical experimental setup. For example, we assume the zero-shot QA rather than few-shot setups, and also consider general black-box APIs for LLMs which do not provide the output probability. In contrast, one can observe that SURE successfully improves QA accuracy of LLMs by effectively exploiting the retrieved passages. In particular, SURE exhibits 4.4%/3.9% absolute EM/F1 improvements in the average, compared to na\u0131\u0308vely appending the retrieved passages.\nWe further demonstrate the compatibility of SURE across various LLMs and retrieval methods. Specifically, in addition to ChatGPT and BM25 considered in Table 1, we run experiments on three different LLMs (GPT-4, and LLaMA2-chat) and two different retrieval methods (DPR and Contriever). In Table 2, we compare EM metric of SURE with the baseline that simply appends the retrieved passages. Here, ODQA performance significantly depends on the retrieval methods and types of LLMs; for example, using Contriever instead of BM25 makes 2.6% average EM improvements, and using GPT-4 instead of ChatGPT makes 2.8% average EM improvements, respectively. Overall, one can observe that SURE consistently improves ODQA accuracy regardless of types of LLMs and retrieval methods, with 4.7% average EM improvements. More interestingly, SURE successfully improves average EM scores of LLaMA2-chat as 7.9%, a state-of-the-art open-sourced LLM, which further indicates the practical usefulness of SURE as a simple yet effective solution for ODQA for the open source research community. The F1 results are presented in Appendix B.1."
        },
        {
            "heading": "4.3 ADDITIONAL ANALYSES",
            "text": "In this section, we conduct additional analyses of SURE. We conduct experiments using ChatGPT as an LLM, BM25 as a retriever, NQ\u2217 and WebQ\u2217 as datasets.\nAblation and more analysis of SURE. First, we compare the following methods for the ablation of SURE: (1) Base: appends the retrieved passages to inputs, (2) + Conditional summarizations: additionally appends all the conditional summarizations, (3) + Pair-wise ranking: selects the summarization with only ranking (Eq. 4), and (4) + Instance-wise validity: selects the summarization with both ranking and validity, i.e., SURE. In addition, we consider two different methods to further analyze where the effectiveness of SURE comes from. (5) MCQ prompt: composes Multiple Choice Questions by generating the answer candidates via prompting (Eq. 2) and using them as possible choices for prediction by appending them to input prompt (Robinson et al., 2023) (more details in Appendix A.7), (6) Sum-and-pred (Gen): instead of conditional summarization, it generates generic summarization and predicts the answer based on it. We present the results in Table 3.\nFirst, constructing conditional summarizations improves performance as they can extract specialized contexts for a given question and its answer candidates. Next, incorporating the evaluation on the instance-wise validity of each summarization significantly improves the performance compared to only considering the ranking among summarizations, as it enables more precise selection by adding the assessment regarding the relevance and coherence of the summarization in relation to the given question and prediction pair. Also, a simple aggregation of generated answer candidates in the prompt shows improvement, which indicates the effectiveness of our generated candidates. However, this method becomes inefficient when the given question requires more complex reasoning to answer. Lastly, using generic summarization is effective in improving ODQA with LLMs by providing concentrated and brief contexts and addressing the difficulty from the long context (Liu et al., 2023b). However, the gain is significantly limited compared to SURE, which demonstrates that the key components of SURE are conditional summarization and comparison, rather than simply providing compressed contexts.\nDifferent number of retrieval. Next, we investigate the effect of the number of retrieved passages (N ). Increasing N is one of the most intuitive ways to improve the performance of retrieve-andread system by providing more extensive information (Karpukhin et al., 2020), and hence it is natural to expect that similar positive results could be observed with retrieval-augmented LLMs. However, on the other hand, its effectiveness could be limited as LLMs could fail to handle long input contexts (Liu et al., 2023b). To verify the effect of different N on retrieval-augmented LLMs using prompting, we measure EM of ChatGPT and BM25 with varied N . In Figure 4(a), we present the results of Base and SURE on NQ\u2217 and WebQ\u2217. First, we observe that the accuracy of retrievalaugmented LLMs significantly depends on N ; when a small number of retrieved passages is only available, the performance of Base could be even behind the performance without retrieval, as it restricts the prediction within the limited contexts. As N increases, its performance is increased and takes benefit from the retrieval system. With SURE, the accuracy of LLMs could be improved even with the small number of retrievals (N = 5), and it achieves better accuracy with larger N .\nEffectiveness for finding important passages. In previous experiments, we mainly focus on demonstrating the effectiveness of SURE for improving QA accuracy. While the accurate answer is the most important feature of the QA system, providing the proper rationale for the answer is another important feature, especially in LLM-based systems for reliable usage by users such as search engines. One of the standard approaches for this is explicitly enumerating the most relevant"
        },
        {
            "heading": "Generic Summarization",
            "text": "retrieved passages based on the specific scoring method, which is often called Re-ranking (Nguyen et al., 2016; Izacard et al., 2023). To explore the advantages of SURE in this aspect, we measure QA accuracy of ChatGPT augmented with the one passage considered to be most relevant with a specific reranking method within N = 10 originally retrieved passages with BM25. To extract such a reranking method for SURE, we use the cosine similarity between the sentence embeddings (Reimers & Gurevych, 2019) of the generated summarization and the retrieved passages, denoted by Sent-encoder (SURE). Then, we compare it with the following baselines for reranking: (1) BM25: original retrieval score, i.e., no reranking, (2) Sent-encoder (q): sentence-encoder-based reranking using the similarity between retrieved passages and question (Nguyen et al., 2016), (3) LLMrerank: LLM-based reranking (Sun et al., 2023), and (4) Sent-encoder (Gen): sentence-encoderbased reranking using the similarity between retrieved passages and generic summarization. The results are presented in Table 4. Here, we observe that all the reranking methods are effective compared to no reranking. In addition, LLM-based reranking shows a higher accuracy, while SURE\u2019s similarity-based reranking outperforms all the baselines, demonstrating the superiority of SURE.\nQualitative evaluation as rationale to answer. Lastly, we explore the additional benefits of SURE, which offers rationales to support the prediction. Specifically, we compare the summarization from SURE with the generic summarization, which is also generated by LLMs but with no constraint of supporting specific answer candidates. To separately consider the quality as rationale with the accuracy of prediction, we only compare the samples correctly predicted by both SURE and Generic summarization used in Table 3; for example, it results in 84 remaining samples in the case of NQ\u2217. We first evaluate using GPT-4, which has been demonstrated to have a high correlation with humans (Liu et al., 2023c). We present the results in Figure 4(b). Here, one can observe that the summarization via SURE is more preferred by GPT-4; for example, Generic summarization wins 30.3% while SURE wins 37.4% on average. It is also worth noting that the average length of both summarizations is similar (Generic: 600 vs SURE\u2019s: 570 average characters on NQ), therefore the bias of GPT to prefer the longer response (Wang et al., 2023) might limitedly affect the result. Next, we ask human evaluators which summarization is more informative and plausible to support the given question-answer pair on 84 samples of NQ\u2217. This result is presented in Figure 4(c). Here, we also observe a higher preference for SURE\u2019s summarization (Generic: 26.9% vs SURE: 43.4%). Overall, these results reveal the potential of SURE toward a better ODQA system by providing a high-quality rationale for the answer. Details on human evaluations are presented in Appendix C."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we proposed SURE, a simple yet effective framework to improve ODQA accuracy of LLMs. Our key idea is to ensure the correctness of predicted answers by constructing the summaries of the retrieved passages for the potential answer candidates and evaluating their validity and ranking. Our experiments demonstrate that SURE significantly improves ODQA performance of various retrieval-augmented LLMs, and also has additional advantages for measuring the importance of passages and providing the rationale for prediction. From these advantages, we believe our framework can contribute to various real-world applications and provide a better experience to users."
        },
        {
            "heading": "ETHICS STATEMENT",
            "text": "We strongly believe that SURE can provide a strong positive impact in real-world applications related to QA, e.g., search engines or chatbots. Since SURE can provide the summarization that supports the corresponding prediction specifically, it can significantly improve the explainability (Mao et al., 2022) and reliability (Whitehead et al., 2022) of QA systems which are more important when they are constructed using black-box LLMs. Moreover, considering the success of LLMs in various applications more than QA (Izacard et al., 2023; Nam et al., 2023), we expect the advantages of this framework to better exploit the retrieved passages with LLMs will be beneficial to them.\nIn contrast, there also exists some potential negative impacts when developing a system with the multiple usages of LLMs, as it could be costly (Chen et al., 2023) and generate sensitive (Santurkar et al., 2023) and malicious (Deshpande et al., 2023) text outputs. Since the summarization from SURE is constructed based on the provided passages, one should consider their quality to prevent undesirable outputs. On the other hand, incorporating the additional filtering could be a strong solution (Le Bras et al., 2020; Schick et al., 2021). To reduce the cost, substituting specific steps of SURE, e.g., measuring validity, with trainable small LMs could be an effective way, similar to Yang et al. (2020); Lewis et al. (2021); Li et al. (2023)."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "We provide implementation details (e.g., design of prompts, used APIs, and retrieval methods) and experiment setups (e.g., datasets and metrics) in Section 4 and Appendix A. In addition, we will release source codes near future."
        },
        {
            "heading": "A DESIGNED PROMPTS FOR EXPERIMENTS",
            "text": "In this section, we present the specific prompts used for the experiments in Section 4."
        },
        {
            "heading": "A.1 ANSWER CANDIDATES GENERATION",
            "text": "In Listing 1, we present the prompt pcan which is used to generate K answer candidates y\u0303 = [y\u03031, . . . , y\u0303K ] from the given question and N retrieved passages (Eq. 2). Here, we present the case of K = 2.\nListing 1 Prompt for answer candidates generation.\nf''' Below are N passages related to the question at the end. After reading\nthe passages, provide two correct candidates for the answer to the question at the end. Each answer should be in the form: (a) xx, (b) yy, and should not exceed 3 words for each candidate. \u21aa\u2192 \u21aa\u2192 \u21aa\u2192\nPassage #1 Title: {Passage #1 Title} Passage #1 Text: {Passage #1 Text}\n...\nPassage #N Title: {Passage #N Title} Passage #N Text: {Passage #N Text}\nQuestion: {Question}\nAnswer: '''"
        },
        {
            "heading": "A.2 CONDITIONAL SUMMARIZATION",
            "text": "In Listing 2, we present the prompt psum which is used to generate conditional summarization sk of retrieved passages to validate each candidate y\u0303k as an answer to the question (Eq. 3).\nListing 2 Prompt for conditional summarization.\nf''' Passage #1 Title: {Passage #1 Title} Passage #1 Text: {Passage #1 Text}\n...\nPassage #N Title: {Passage #N Title} Passage #N Text: {Passage #N Text}\nYour job is to act as a professional writer. You will write a good-quality passage that can support the given prediction about the question only based on the information in the provided supporting passages. \u21aa\u2192 \u21aa\u2192 \u21aa\u2192\nNow, let's start. After you write, please write [DONE] to indicate you are done. Do not write a prefix (e.g., \"Response:\") while writing a passage. \u21aa\u2192 \u21aa\u2192\nQuestion: {Question} Choices: {(a) Choice 1 (b) Choice 2} Prediction: {(a) Choice 1 (or (b) Choice 2)} Passage: '''\nA.3 INSTANCE-WISE VALIDATION\nIn Listing 3, we present the prompt pval which is used to evaluate the validity of each summarization sk whether it is not a degenerated case as the provided passages are not enough to support y\u0303k, or it properly supports the given answer candidate y\u0303k, rather than the other candidate y\u0303i, i \u0338= k (Eq. 4).\nListing 3 Prompt for instance-wise validation.\nf''' Question: {Question}\nPrediction: {Prediction}\nPassage: {Passage}\nDoes the passage correctly support the prediction? Choices: [True, False]. Answer:\u21aa\u2192 '''"
        },
        {
            "heading": "A.4 PAIR-WISE RANKING",
            "text": "In Listing 4, we present the prompt prank which is used to evaluate how the given summarization sk is relatively informative to answer the question q, among all summaries SK = {sk}Kk=1 (Eq. 5).\nListing 4 Prompt for pair-wise ranking.\nf''' Question: Given the following passages, determine which one provides a\nmore informative answer to the subsequent question.\u21aa\u2192\nPassage 1: {Passage 1}\nPassage 2: {Passage 2}\nTarget Question: {Question}\nYour Task: Identify which passage (Passage 1 or Passage 2) is more relevant and\ninformative to answer the question at hand. Choices: [Passage 1, Passage 2]. \u21aa\u2192 \u21aa\u2192\nAnswer: '''"
        },
        {
            "heading": "A.5 BASELINE PREDICTION",
            "text": "In Listing 5, we present the prompt that is used to append the retrieved passages of the question to give it as inputs of LLMs. The result with this prompt is denoted by Base, in Section 4. The same prompt is used for no retrieval by assuming N = 0, i.e., the only question is given to LLMs with instruction.\nListing 5 Prompt for baseline prediction.\nf''' Below are N passages related to the question at the end. After reading\nthe passages, answer to the question at the end. Answer should not exceed 3 words. \u21aa\u2192 \u21aa\u2192\nPassage #1 Title: {Passage #1 Title} Passage #1 Text: {Passage #1 Text}\n...\nPassage #N Title: {Passage #N Title} Passage #N Text: {Passage #N Text}\nQuestion: {Question}\nAnswer: '''"
        },
        {
            "heading": "A.6 PROMPTS FOR GENERAL SUMMARIZATION",
            "text": "In Listing 6, we present the prompt that is used to construct generic summarization used in Section 4.3. One can observe that conditioning part is removed, compared to psum.\nListing 6 Prompt for generic summarization.\nf''' Passage #1 Title: {Passage #1 Title} Passage #1 Text: {Passage #1 Text}\n...\nPassage #N Title: {Passage #N Title} Passage #N Text: {Passage #N Text}\nYour job is to act as a professional writer. You will write a good-quality passage that can support the prediction about the question only based on the information in the provided supporting passages. \u21aa\u2192 \u21aa\u2192 \u21aa\u2192\nNow, let's start. After you write, please write [DONE] to indicate you are done. Do not write a prefix (e.g., \"Response:\") while writing a passage. \u21aa\u2192 \u21aa\u2192\nQuestion: {Question} Passage: '''"
        },
        {
            "heading": "A.7 PROMPTS FOR MCQ PROMPT",
            "text": "Recently, Robinson et al. (2023) demonstrated that multiple-choice prompts generally elicit much more accurate responses than do cloze prompts, for LLMs with high multiple-choice symbol binding ability like OpenAI Codex (Chen et al., 2021). Motivated by this, we consider MCQ prompt in Listing 7 and use it in Table 3, to evaluate the effectiveness of selecting the answer from the construction and verification of the conditional summarizations rather than direct prompting, under the same answer candidates from Eq. 2. One can observe that the conditioning with multiple choices part is added, compared to baseline prompting in Listing 5.\nListing 7 Prompt for MCQ prompt.\nf''' Below are N passages related to the question at the end. After reading\nthe passages, answer to the question at the end. Answer should not exceed 3 words. \u21aa\u2192 \u21aa\u2192\nPassage #1 Title: {Passage #1 Title} Passage #1 Text: {Passage #1 Text}\n...\nPassage #N Title: {Passage #N Title} Passage #N Text: {Passage #N Text}\nQuestion: {Question} Choices: {(a) Choice 1 (b) Choice 2} Answer: '''"
        },
        {
            "heading": "A.8 DESIGN PRINCIPLES FOR PROMPT",
            "text": "Before finalizing the prompts used in the experiments, we examined several prompt designs and chose the best-performing one. Here, we\u2019d like to share two key observations from this process. First, precise and detailed instructions are crucial. As each component of the proposed framework operates in a zero-shot manner, its output greatly relies on the provided instruction. For example, in answer candidate generation (Eq. 2), the current prompt, outlined in Listing 1, consistently outperforms the initially considered simple prompt (Task description: give two candidates for the answer to the following question (e.g., (a) xx, (b) yy)). Second, proper input arguments are essential. For instance, along with the target candidate, providing all candidates as additional input enhanced the quality of conditional summarization. This is because it further specifies which contexts of retrieval should be the focus. However, including this information, or even the retrieval passages, disrupted the verification step by interrupting the focus on the summarizations."
        },
        {
            "heading": "B ADDITIONAL QUANTITATIVE RESULTS",
            "text": ""
        },
        {
            "heading": "B.1 MORE RESULTS FOR SURE UNDER DIFFERENT CONFIGURATIONS",
            "text": "In Table 5, we present F1 scores with different configurations of various LLMs and retrieval methods. Similar to the result in Table 2, it is observed that SURE consistently improves ODQA accuracy regardless of types of LLMs and retrieval methods, with 3.2% average F1 improvement on average."
        },
        {
            "heading": "B.2 LIMITED ACHIEVABLE IMPROVEMENT WITH MORE CANDIDATES",
            "text": "As we denoted in Section 4.1, we use a fixed value of K = 2 for all the experiments. This is due to our initial observation that the room for improvement by increasing K is not large compared to the additional costs. To investigate this, we first assume the method, denoted Oracle, which takes the maximum of EM and F1 among the multiple candidates, e.g.,, if one candidate is true and the\nother is wrong, then Oracle consider it as true. As one can see in Table 6, increasing K = 3 from K = 2 limitedly improves the accuracy (e.g., 0.9% in EM), compared to the remaining room for improvement by better selection with small K; for example, there is 9.0% gap between SURE and Oracle, in terms of EM. Therefore, in this work, we keep K = 2 but we remark that SURE is able to be extended with K > 2. Also, as there is still remaining room for improvement, we hope that future work could reduce such a gap."
        },
        {
            "heading": "B.3 ADDITIONAL EVALUATION WITH LLMS",
            "text": "In Section 4, we considered EM/F1 scores are the common metrics for the considered ODQA datasets, following the previous works (Chowdhery et al., 2022; Touvron et al., 2023a; Izacard et al., 2023; Shi et al., 2023), to make it easy to notice the significance of our results. Nevertheless, other factors like response coherence, relevance, and efficiency are important metrics to be considered.\nTo evaluate these aspects, we have conducted additional evaluations with LLMs approaches. Specifically, we measured two additional metrics: (1) MAUVE (Pillutla et al., 2021) and (2) LLM-acc (Kamalloo et al., 2023). MAUVE is a recently proposed metric to compare the two distributions of the text generation model and human-written text using divergence frontiers. MAUVE (scale of 0 to 100, higher is better) is known for correlating highly with human judgments, and is frequently used to evaluate LMs\u2019 responses (Su et al., 2022b; Gao et al., 2023). LLM-acc assesses the accuracy (%) of LLMs\u2019 responses to questions, using the prompting of LLMs instead of term overlap like EM/F1. We used the official code from the authors, only changing LLMs to ChatGPT. We measured this metric on NQ\u2217, WebQ\u2217, 2Wiki, and HotpotQA datasets, and the results are presented in Table 7.\nHere, it is observed that the proposed method also makes significant improvements compared to the baseline under these two additional evaluations with LLMs approaches. Along with the results in Section 4, this result further validates that our framework enables LLMs to provide better answers to the given question."
        },
        {
            "heading": "B.4 EXPERIMENTS ON LONG-FORM QUESTION ANSWERING",
            "text": "While we mainly conduct the experiments with QA datasets that have short answers in Section 4, our approach has the potential to be applicable beyond short-answer datasets. To verify this, we have conducted additional experiments on long-form question answering tasks to validate our approach\u2019s applicability. Specifically, we used ASQA dataset (Stelmakh et al., 2022; Gao et al., 2023) which consists of factoid questions and the corresponding long-form answers; for example, the answers of ASQA dataset have an average length of 71.8 words, while the answers of NQ dataset have 2.6 words. Following the setups in Gao et al. (2023), we compared the base prompting method with\nretrieval and name (ours) on 948 test examples, using ChatGPT (GPT-3.5-turbo-0301) with 5 retrieved passages via GTR (Ni et al., 2022) for the experiments. For the evaluation, we measure ROUGE-L and String Exact Match (STR-EM) for correctness, and MAUVE (Pillutla et al., 2021) for fluency and coherence, following the previous works (Stelmakh et al., 2022; Gao et al., 2023).\nThe results are presented in Table 8. One can observe that our proposed framework consistently improves the performance of retrieval-augmented LLMs for long-form QA tasks. However, we acknowledge that there is still room for improvement, particularly in finding better prompt designs, given that our current designs are based on performance on short-answer datasets. We hope future research will explore this direction, extending the benefits of our framework to broader QA scenarios with LLMs."
        },
        {
            "heading": "B.5 EXPERIMENTAL WITH FEW-SHOT EXAMPLES",
            "text": "Here, we conduct additional experiments on NQ\u2217 and WebQ\u2217, using 1-shot and 5-shot examples from training datasets during prediction. We compare the average EM/F1 of base prompting with retrieval and SURE, across four different random seeds used for sample selection. In Listing 8, we present the prompt that is used to generate K answer candidates in the case where few-shot examples are given. Here, we present the case of K = 2. Note that if few-shot examples are provided, only the prompt for generating answer candidates is modified. Also, in Listing 9, we present the prompt for the base prompting. Table 9 shows that adding few-shot examples improves QA accuracy for both the baseline and name. Specifically, we observed that name\u2019s gain primarily results from generating more accurate answer candidates. These findings suggest that our proposed method could be effective in scenarios beyond the zero-shot setup considered. Therefore, we believe that our work could contribute to broader ODQA scenarios in the future.\nListing 8 Prompt for answer candidates generation with few-shot examples.\nf''' Below are N passages related to the question at the end. We also provide\nthe answers for various questions. After reading the passages and question-answer pairs, provide two correct candidates for the answer to the question at the end. Each answer should be in the form: (a) xx, (b) yy, and should not exceed 3 words for each candidate. \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192\nPassage #1 Title: {Passage #1 Title} Passage #1 Text: {Passage #1 Text}\n...\nPassage #N Title: {Passage #N Title} Passage #N Text: {Passage #N Text}\nQuestion: {Example question #1} Answer: {Example answer #1}\n...\nQuestion: {Example question #shot} Answer: {Example answer #shot}\nQuestion: {Query question} Provide two correct candidates for the answer: '''"
        },
        {
            "heading": "C HUMAN EVALUATION OF GENERATED SUMMARIZATION",
            "text": "In this section, we provide details on the human preference evaluation of generated summarizations in Figure 4(c). First, we generate summarizations with a generic method (Listing 6) and with our proposed SURE (Listing 2). To separately consider the quality as rationale with the accuracy of prediction, we only compare the samples correctly predicted by both SURE and generic summarization; it results in 84 examples from the NQ\u2217. Then, using the prompt in Listing 11, we conduct human evaluation. Specifically, we hired seven NLP experts off-line for our human evaluation experiment. Unlike asking GPT-4 with Listing 10, we ask human evaluators to answer as a tie if it is hard to determine.\nListing 9 Base prompt with few-shot examples.\nf''' Below are N passages related to the question at the end. We also provide\nthe answers for various questions. After reading the passages and question-answer pairs, answer to the question at the end. Answer should not exceed 3 words. \u21aa\u2192 \u21aa\u2192 \u21aa\u2192\nPassage #1 Title: {Passage #1 Title} Passage #1 Text: {Passage #1 Text}\n...\nPassage #N Title: {Passage #N Title} Passage #N Text: {Passage #N Text}\nQuestion: {Example question #1} Answer: {Example answer #1}\n...\nQuestion: {Example question #shot} Answer: {Example answer #shot}\nQuestion: {Query question} Answer: '''\nListing 10 Prompt for GPT-based evaluation.\nf''' Question: Given the following summaries for the target question,\ndetermine which one is more informative and plausible as rationale to support a given target question-answer pair. \u21aa\u2192 \u21aa\u2192\nSummary 1: {Summary 1}\nSummary 2: {Summary 2}\nTarget Question: {Question}\nTarget Answer: {Answer}\nYour Task: Identify which summary (Summary 1 or Summary 2) is more informative and\nplausible as rationale to support a given answer at hand. Choices: [Summary 1, Summary 2]. \u21aa\u2192 \u21aa\u2192\nAnswer: '''"
        },
        {
            "heading": "D ADDITIONAL QUALITATIVE RESULTS",
            "text": "In this section, we present more qualitative results with SURE. All the examples are from NQ\u2217, and ChatGPT with BM25 (N = 10) is commonly used.\nListing 11 Template for human evaluation.\nf''' Given the following summaries for the target question, determine which\none is more informative and plausible as rationale to support a given target question-answer pair. \u21aa\u2192 \u21aa\u2192\nTarget Question: {Question}\nTarget Answer: {Answer}\nSummary 1: {Summary 1}\nSummary 2: {Summary 2}\nChoices: [Summary 1, Tie, Summary 2]\nYour choice: '''"
        },
        {
            "heading": "D.1 MORE EXAMPLES OF QUALITATIVE COMPARISON BETWEEN SURE\u2019S SUMMARIZATION AND GENERIC SUMMARIZATION",
            "text": "In Figures 6, 7, and 8, we present more examples for qualitative comparison between the candidateconditioned summarization by SURE and generic summarization. Innecessary and tedious sentences irrelevant to the answer are highlighted with red."
        },
        {
            "heading": "D.2 QUALITATIVE EXAMPLES OF VERIFICATION WITH INSTANCE-WISE VALIDITY",
            "text": "To qualitatively show which samples are considered as invalid by LLMs, we present the examples that exhibit v(sk) = 0 asM (pval(q, yk, sk)) = False in Figure 9. Here, we highlight the sentences with green if they include the relevant context with the given candidate. In addition, we highlight the sentences with red if they induce a different candidate as an answer or do not support the candidate. For example, in the second example with a question (Who is the actor that plays Saul on \u2018\u2018Grace and Frankie\u2019\u2019?), one can observe that the generated summarization concludes that the given candidate (Mark Saul) is incorrect; consequently, LLMs evaluates its validity as supporting summarization for the given candidate as false."
        },
        {
            "heading": "D.3 QUALITATIVE EXAMPLES OF VERIFICATION WITH PAIR-WISE RANKING",
            "text": "In Figure 10, we present examples of verification by pair-wise ranking. Here, we highlight with green for the summarization that gets a higher ranking. In contrast, we highlight with red for the summarization that gets a lower ranking. We also highlight the relevant texts with the same colors, respectively."
        },
        {
            "heading": "E DISCUSSION ON COST AND QUALITY GAIN",
            "text": "While SURE significantly improves QA system of LLMs, one can be concerned about its cost as it requires multiple inferences of LLMs. However, we note that the improvement of SURE is not just a simple consequence of more cost. Compared to other cost-increasing methods for accuracy improvement, SURE significantly outperforms them, i.e., SURE is an even more efficient way to increase performance. For instance, increasing the number of retrieved passages is one of the most straightforward methods for this goal. But, in this case, SURE with 10 passages outperforms the base prompting with 50 passages, even with a lower total cost, as presented in Table 10. In addition, we note that other baseline approaches such as chain-of-thought or self-verification (considered in Table 1) also require more cost than base prompting, but they fail to successfully improve the performance.\nOn the other hand, one can reduce the overall cost by using cheaper LLMs for specific components, thanks to the modularity of SURE. Remarkably, SURE is compatible with the recent state-of-the-art open LLMs (see Tables 2 and 5) and hence this advantage is more noticeable. To give an intuition, we conduct the new experiments by using ChatGPT for the answer candidate generation and summarization, and LLaMA for the succeeding verification steps. As shown in the 4th row of Table 10, this hybrid approach of different LLMs with SURE successfully reduces the cost while keeping the effectiveness for improving the accuracy; for WebQ*, this approach even outperforms the expensive one. This result is from the effectiveness of LLaMA in WebQ* and indicates the potential of such a hybrid method.\nLastly, we further remark that most of SURE\u2019s cost is currently from re-reading retrieved passages for conditional summarizations (e.g., 38% of the total cost for SURE with 10 passages). This is due to current APIs not providing recycling options for previous inputs. If recycling becomes available, SURE\u2019s cost could be significantly reduced."
        },
        {
            "heading": "F LIMITATION AND FUTURE WORK",
            "text": "In this work, we primarily focused on zero-shot setup for the experiments, which is a commonly encountered scenario in the real world, e.g., search engine. But, similar to the previous works (Chowdhery et al., 2022; Touvron et al., 2023a), incorporating data-specific few-shot examples is also an interesting future direction to further improve QA accuracy of LLMs with SURE. Another interesting direction is extending the applied task beyond QA, such as language modeling (Guu et al., 2020) or language understanding tasks (Hendrycks et al., 2021)."
        },
        {
            "heading": "G EXPERIMENTAL RESULTS WITH CONFIDENCE INTERVAL",
            "text": "In this section, we present confidence intervals for our main tables (Tables 1 and 2). To achieve this, we apply bootstrapping (Efron & Tibshirani, 1994), a popular technique for statistical inference that involves random sampling with replacement. We report 95% confidence intervals obtained through 1,000 iterations of bootstrapping. The confidence intervals for the EM and F1 metrics of each main table can be found in Tables 11, 12, 13, and 14.\nThe reliability of the results is reasonably robust, with the 95% confidence interval having only about a 10% variance from the reported value. Specifically, in the EM metric of the NQ dataset, our SuRe has the lowest confidence interval value at 32.0, compared to the maximum value of 29.1 for the no retrieval baseline and 30.0 for the best competitor. This demonstrates that the advantage of SuRe over prior works is statistically significant."
        },
        {
            "heading": "Generic Summarization",
            "text": ""
        },
        {
            "heading": "Passage Title: Puerto Rico's History Before U.S. Occupation",
            "text": ""
        },
        {
            "heading": "Generic Summarization",
            "text": ""
        },
        {
            "heading": "Generic Summarization",
            "text": ""
        },
        {
            "heading": "Generic Summarization",
            "text": ""
        },
        {
            "heading": "Passage Title: John Quincy Adams",
            "text": ""
        },
        {
            "heading": "Generic Summarization",
            "text": ""
        },
        {
            "heading": "He also references their previous collaboration on the song. Therefore, it can be concluded that",
            "text": ""
        },
        {
            "heading": "Generic Summarization",
            "text": ""
        },
        {
            "heading": "Candidate-conditioned Summarization 2 (Candidate: Grameen Bank)",
            "text": ""
        },
        {
            "heading": "Candidate-conditioned Summarization 2 (Candidate: Sascha Bopp)",
            "text": ""
        }
    ],
    "year": 2023
}