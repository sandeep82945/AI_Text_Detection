{
    "abstractText": "This paper investigates discrepancies in how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. Recent works have found that the generalization error of a trained network typically increases with the intrinsic dimension (ddata) of its training set. Yet, the steepness of this relationship varies significantly between medical (radiological) and natural imaging domains, with no existing theoretical explanation. We address this gap in knowledge by establishing and empirically validating a generalization scaling law with respect to ddata, and propose that the substantial scaling discrepancy between the two considered domains may be at least partially attributed to the higher intrinsic \u201clabel sharpness\u201d (KF ) of medical imaging datasets, a metric which we propose. Next, we demonstrate an additional benefit of measuring the label sharpness of a training set: it is negatively correlated with the trained model\u2019s adversarial robustness, which notably leads to models for medical images having a substantially higher vulnerability to adversarial attack. Finally, we extend our ddata formalism to the related metric of learned representation intrinsic dimension (drepr), derive a generalization scaling law with respect to drepr, and show that ddata serves as an upper bound for drepr. Our theoretical results are supported by thorough experiments with six models and eleven natural and medical imaging datasets over a range of training set sizes. Our findings offer insights into the influence of intrinsic dataset properties on generalization, representation learning, and robustness in deep neural networks.1",
    "authors": [
        {
            "affiliations": [],
            "name": "MEDICAL IMAGES"
        },
        {
            "affiliations": [],
            "name": "Nicholas Konz"
        },
        {
            "affiliations": [],
            "name": "Maciej A. Mazurowski"
        }
    ],
    "id": "SP:a7e156cf0269fe79b45c4caf925f6cc1c3ddf21c",
    "references": [
        {
            "authors": [
                "Rayna Andreeva",
                "Katharina Limbeck",
                "Bastian Rieck",
                "Rik Sarkar"
            ],
            "title": "Metric space magnitude and generalisation in neural networks",
            "venue": "arXiv preprint arXiv:2305.05611,",
            "year": 2023
        },
        {
            "authors": [
                "Alessio Ansuini",
                "Alessandro Laio",
                "Jakob H Macke",
                "Davide Zoccolan"
            ],
            "title": "Intrinsic dimension of data representations in deep neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yasaman Bahri",
                "Ethan Dyer",
                "Jared Kaplan",
                "Jaehoon Lee",
                "Utkarsh Sharma"
            ],
            "title": "Explaining neural scaling laws",
            "venue": "arXiv preprint arXiv:2102.06701,",
            "year": 2021
        },
        {
            "authors": [
                "Tolga Birdal",
                "Aaron Lou",
                "Leonidas J Guibas",
                "Umut Simsekli"
            ],
            "title": "Intrinsic dimension, persistent homology and generalization in neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Bradley CA Brown",
                "Anthony L. Caterini",
                "Brendan Leigh Ross",
                "Jesse C Cresswell",
                "Gabriel Loaiza-Ganem"
            ],
            "title": "Verifying the union of manifolds hypothesis for image data",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Ethan Caballero",
                "Kshitij Gupta",
                "Irina Rish",
                "David Krueger"
            ],
            "title": "Broken neural scaling laws",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Li Deng"
            ],
            "title": "The mnist database of handwritten digit images for machine learning research [best of the web",
            "venue": "IEEE signal processing magazine,",
            "year": 2012
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Elena Facco",
                "Maria d\u2019Errico",
                "Alex Rodriguez",
                "Alessandro Laio"
            ],
            "title": "Estimating the intrinsic dimension of datasets by a minimal neighborhood information",
            "venue": "Scientific reports,",
            "year": 2017
        },
        {
            "authors": [
                "Mahyar Fazlyab",
                "Alexander Robey",
                "Hamed Hassani",
                "Manfred Morari",
                "George Pappas"
            ],
            "title": "Efficient and accurate estimation of lipschitz constants for deep neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Charles Fefferman",
                "Sanjoy Mitter",
                "Hariharan Narayanan"
            ],
            "title": "Testing the manifold hypothesis",
            "venue": "Journal of the American Mathematical Society,",
            "year": 2016
        },
        {
            "authors": [
                "Adam E Flanders",
                "Luciano M Prevedello",
                "George Shih",
                "Safwan S Halabi",
                "Jayashree KalpathyCramer",
                "Robyn Ball",
                "John T Mongan",
                "Anouk Stein",
                "Felipe C Kitamura",
                "Matthew P Lungren"
            ],
            "title": "Construction of a machine learning dataset through collaboration: the rsna 2019 brain ct hemorrhage",
            "year": 1902
        },
        {
            "authors": [
                "Sixue Gong",
                "Vishnu Naresh Boddeti",
                "Anil K Jain"
            ],
            "title": "On the intrinsic dimensionality of image representations",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Jordan Hoffmann",
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Elena Buchatskaya",
                "Trevor Cai",
                "Eliza Rutherford",
                "Diego de Las Casas",
                "Lisa Anne Hendricks",
                "Johannes Welbl",
                "Aidan Clark"
            ],
            "title": "Training compute-optimal large language models",
            "venue": "arXiv preprint arXiv:2203.15556,",
            "year": 2022
        },
        {
            "authors": [
                "Jeremy Irvin",
                "Pranav Rajpurkar",
                "Michael Ko",
                "Yifan Yu",
                "Silviana Ciurea-Ilcus",
                "Chris Chute",
                "Henrik Marklund",
                "Behzad Haghgoo",
                "Robyn Ball",
                "Katie Shpanskaya"
            ],
            "title": "Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "year": 2001
        },
        {
            "authors": [
                "Guy Katz",
                "Clark Barrett",
                "David L Dill",
                "Kyle Julian",
                "Mykel J Kochenderfer"
            ],
            "title": "Reluplex: An efficient smt solver for verifying deep neural networks",
            "venue": "In Computer Aided Verification: 29th International Conference,",
            "year": 2017
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Daisuke Komura",
                "Shumpei Ishikawa"
            ],
            "title": "Machine learning methods for histopathological image analysis",
            "venue": "Computational and structural biotechnology journal,",
            "year": 2018
        },
        {
            "authors": [
                "Nicholas Konz",
                "Hanxue Gu",
                "Haoyu Dong",
                "Maciej A Mazurowski"
            ],
            "title": "The intrinsic manifolds of radiological images and their role in deep learning",
            "venue": "Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2022: 25th International Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Henry Kvinge",
                "Davis Brown",
                "Charles Godfrey"
            ],
            "title": "Exploring the representation manifolds of stable diffusion through the lens of intrinsic dimension",
            "venue": "ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models,",
            "year": 2023
        },
        {
            "authors": [
                "Elizaveta Levina",
                "Peter Bickel"
            ],
            "title": "Maximum likelihood estimation of intrinsic dimension",
            "venue": "Advances in neural information processing systems,",
            "year": 2004
        },
        {
            "authors": [
                "Zhuang Liu",
                "Hanzi Mao",
                "Chao-Yuan Wu",
                "Christoph Feichtenhofer",
                "Trevor Darrell",
                "Saining Xie. A convnet for the"
            ],
            "title": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp",
            "venue": "11976\u201311986, 2022.",
            "year": 2020
        },
        {
            "authors": [
                "Xingjun Ma",
                "Yuhao Niu",
                "Lin Gu",
                "Yisen Wang",
                "Yitian Zhao",
                "James Bailey",
                "Feng Lu"
            ],
            "title": "Understanding adversarial attacks on deep learning based medical image analysis systems",
            "venue": "Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "David JC MacKay",
                "Zoubin Ghahramani"
            ],
            "title": "Comments on\u2019maximum likelihood estimation of intrinsic dimension\u2019by",
            "venue": "e. levina and p. bickel (2004)",
            "year": 2005
        },
        {
            "authors": [
                "Bjoern H Menze",
                "Andras Jakab",
                "Stefan Bauer",
                "Jayashree Kalpathy-Cramer",
                "Keyvan Farahani",
                "Justin Kirby",
                "Yuliya Burren",
                "Nicole Porz",
                "Johannes Slotboom",
                "Roland Wiest"
            ],
            "title": "The multimodal brain tumor image segmentation benchmark (brats)",
            "venue": "IEEE transactions on medical imaging,",
            "year": 1993
        },
        {
            "authors": [
                "Yuval Netzer",
                "Tao Wang",
                "Adam Coates",
                "Alessandro Bissacco",
                "Bo Wu",
                "Andrew Y Ng"
            ],
            "title": "Reading digits in natural images with unsupervised feature learning",
            "year": 2011
        },
        {
            "authors": [
                "Phil Pope",
                "Chen Zhu",
                "Ahmed Abdelkader",
                "Micah Goldblum",
                "Tom Goldstein"
            ],
            "title": "The intrinsic dimension of images and its impact on learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Mark Pritt",
                "Gary Chern"
            ],
            "title": "Satellite image classification with deep learning",
            "year": 2017
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jeremy Irvin",
                "Aarti Bagul",
                "Daisy Ding",
                "Tony Duan",
                "Hershel Mehta",
                "Brandon Yang",
                "Kaylie Zhu",
                "Dillon Laird",
                "Robyn L Ball"
            ],
            "title": "Mura: Large dataset for abnormality detection in musculoskeletal radiographs",
            "venue": "arXiv preprint arXiv:1712.06957,",
            "year": 2017
        },
        {
            "authors": [
                "Ashirbani Saha",
                "Michael R Harowicz",
                "Lars J Grimm",
                "Connie E Kim",
                "Sujata V Ghate",
                "Ruth Walsh",
                "Maciej"
            ],
            "title": "A Mazurowski. A machine learning approach to radiogenomics of breast cancer: a study of 922 subjects and 529 dce-mri features",
            "venue": "British journal of cancer,",
            "year": 2018
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Geoffrey A Sonn",
                "Shyam Natarajan",
                "Daniel JA Margolis",
                "Malu MacAiran",
                "Patricia Lieu",
                "Jiaoti Huang",
                "Frederick J Dorey",
                "Leonard S Marks"
            ],
            "title": "Targeted biopsy in the detection of prostate cancer using an office based magnetic resonance ultrasound fusion device",
            "venue": "The Journal of urology,",
            "year": 2013
        },
        {
            "authors": [
                "Naftali Tishby",
                "Noga Zaslavsky"
            ],
            "title": "Deep learning and the information bottleneck principle",
            "venue": "In 2015 ieee information theory workshop (itw),",
            "year": 2015
        },
        {
            "authors": [
                "Aleksei Tiulpin",
                "J\u00e9r\u00f4me Thevenot",
                "Esa Rahtu",
                "Petri Lehenkari",
                "Simo Saarakkala"
            ],
            "title": "Automatic Knee Osteoarthritis Diagnosis from Plain Radiographs: A Deep Learning-Based Approach",
            "venue": "Scientific Reports,",
            "year": 2322
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "arXiv preprint arXiv:2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Yusuke Tsuzuku",
                "Issei Sato",
                "Masashi Sugiyama"
            ],
            "title": "Lipschitz-margin training: Scalable certification of perturbation invariance for deep neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Bohang Zhang",
                "Tianle Cai",
                "Zhou Lu",
                "Di He",
                "Liwei Wang"
            ],
            "title": "Towards certifying l-infinity robustness using neural networks with l-inf-dist neurons",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "There has been recent attention towards how a neural network\u2019s ability to generalize to test data relates to the intrinsic dimension ddata of its training dataset, i.e., the dataset\u2019s inherent \u201ccomplexity\u201d or the minimum degrees of freedom needed to represent it without substantial information loss (Gong et al., 2019). Recent works have found that generalization error typically increases with ddata, empirically (Pope et al., 2020) or theoretically (Bahri et al., 2021). Such \u201cscaling laws\u201d with respect to intrinsic dataset properties are attractive because they may describe neural network behavior in generality, for different models and/or datasets, allowing for better understanding and predictability of the behavior, capabilities, and challenges of deep learning. However, a recent study (Konz et al., 2022) showed that generalization scaling behavior differs drastically depending on the input image type, e.g., natural or medical images, showing the non-universality of the scaling law and motivating us to consider its relationship to properties of the dataset and imaging domain.2\nIn this work, we provide theoretical and empirical findings on how measurable intrinsic properties of an image dataset can affect the behavior of a neural network trained on it. We show that certain\n1Code link: https://github.com/mazurowski-lab/intrinsic-properties. 2Here we take \u201cmedical\u201d images to refer to radiology images (e.g., x-ray, MRI), the most common type.\ndataset properties that differ between imaging domains can lead to discrepancies in behaviors such as generalization ability and adversarial robustness. Our contributions are summarized as follows.\nFirst, we introduce the novel measure of the intrinsic label sharpness (KF ) of a dataset (defined in Section 3.2). The label sharpness essentially measures how similar images in the dataset can be to each other while still having different labels, and we find that it usually differs noticeably between natural and medical image datasets. We then derive and test a neural network generalization scaling law with respect to dataset intrinsic dimension ddata, which includes KF . Our experiments support the derived scaling behavior within each of these two domains, and show a distinct difference in the scaling rate between them. According to our scaling law and likelihood analysis of observed generalization data (Appendix C.1), this may be due to the measured KF being typically higher for medical datasets.\nNext, we show how a model\u2019s adversarial robustness relates to its training set\u2019s KF , and show that over a range of attacks, robustness decreases with higher KF . Indeed, medical image datasets, which have higher KF , are typically more susceptible to adversarial attack than natural image datasets. Finally, we extend our ddata formalism to derive and test a generalization scaling law with respect to the intrinsic dimension of the model\u2019s learned representations, drepr, and reconcile the ddata and drepr scaling laws to show that ddata serves as an approximate upper bound for drepr. We also provide many additional results in the supplementary material, such as a likelihood analysis of our proposed scaling law given observed generalization data (Appendix C.1), the evaluation of a new dataset in a third domain (Appendix C.2), an example of a practical application of our findings (Appendix C.3), and more.\nAll theoretical results are validated with thorough experiments on six CNN architectures and eleven datasets from natural and medical imaging domains over a range of training set sizes. We hope that our work initiates further study into how network behavior differs between imaging domains."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": "We are interested in the scaling of the generalization ability of supervised convolutional neural networks with respect to intrinsic properties of the training set. Other works have also explored generalization scaling with respect to parameter count or training set size for vision or other modalities (Caballero et al., 2023; Kaplan et al., 2020; Hoffmann et al., 2022; Touvron et al., 2023). Note that we model the intrinsic dimension to be constant throughout the dataset\u2019s manifold as in Pope et al. (2020); Bahri et al. (2021) for simplicity, as opposed to the recent work of Brown et al. (2023), which we find to be suitable for interpretable scaling laws and dataset properties.\nSimilar to dataset intrinsic dimension scaling (Pope et al., 2020; Bahri et al., 2021; Konz et al., 2022), recent works have also found a monotonic relationship between a network\u2019s generalization error and the intrinsic dimension of both the learned hidden layer representations (Ansuini et al., 2019), or some measure of intrinsic dimensionality of the trained model itself (Birdal et al., 2021; Andreeva et al., 2023). In this work, we focus on the former, as the latter model dimensionality measures are typically completely different mathematical objects than the intrinsic dimension of the manifolds of data or representations. Similarly, Kvinge et al. (2023) found a correlation between prompt perplexity and representation intrinsic dimension in Stable Diffusion models."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "We consider a binary classification dataset D of points x \u2208 Rn with target labels y = F(x) defined by some unknown function F : Rn \u2192 {0, 1}, split into a training set Dtrain of size N and test set Dtest. The manifold hypothesis (Fefferman et al., 2016) assumes that the input data x lies approximately on some ddata-dimensional manifold Mddata \u2282 Rn, with ddata \u226a n. More technically, Mddata is a metric space such that for all x \u2208 Mddata , there exists some neighborhood Ux of x such that Ux is homeomorphic to Rddata , defined by the standard L2 distance metric || \u00b7 ||. As in Bahri et al. (2021), we consider over-parameterized (number of parameters \u226b N ) models f(x) : Rn \u2192 {0, 1}, that are \u201cwell-trained\u201d and learn to interpolate all training data: f(x) = F(x) for all x \u2208 Dtrain. We use a non-negative loss function L, such that L = 0 when f(x) = F(x). Note that we write L as the expected loss over a set of test set points. We assume that F , f and\nL are Lipschitz/smooth on Mddata with respective constants KF , Kf and KL. Note that we use the term \u201cLipschitz constant\u201d of a function to refer to the smallest value that satisfies the Lipschitz inequality.3 We focus on binary classification as in Pope et al. (2020); Konz et al. (2022), but we note that our results extend naturally to the multi-class case (see Appendix A.1 for more details)."
        },
        {
            "heading": "3.1 ESTIMATING DATASET INTRINSIC DIMENSION",
            "text": "Here we introduce two common intrinsic dimension estimators for high-dimensional datasets that we use in our experiments, which have been used in prior works on image datasets (Pope et al., 2020; Konz et al., 2022) and learned representations (Ansuini et al., 2019; Gong et al., 2019).\nMLE: The MLE (maximum likelihood estimation) intrinsic dimension estimator (Levina & Bickel, 2004; MacKay & Ghahramani, 2005) works by assuming that the number of datapoints enclosed within some \u03f5-ball about some point on Mddata scales not as O(\u03f5n), but O(\u03f5ddata), and then solving for ddata with MLE after modeling the data as sampled from a Poisson process. This results in\nd\u0302data = [ 1 N(k\u22121) \u2211N i=1 \u2211k\u22121 j=1 log Tk(xi) Tj(xi) ]\u22121 , where Tj(x) is the L2 distance from x to its jth nearest neighbor and k is a hyperparameter; we set k = 20 as in Pope et al. (2020); Konz et al. (2022). TwoNN: TwoNN (Facco et al., 2017) is a similar approach that instead relies on the ratio of the first- and second-nearest neighbor distances. We default to using the MLE method for ddata estimation as Pope et al. (2020) found it to be more reliable for image data than TwoNN, but we still evaluate with TwoNN for all experiments. Note that these estimators do not use datapoint labels."
        },
        {
            "heading": "3.2 ESTIMATING DATASET LABEL SHARPNESS",
            "text": "Another property of interest is an empirical estimate for the \u201clabel sharpness\u201d of a dataset, KF . This measures the extent to which images in the dataset can resemble each other while still having different labels. Formally, KF is the Lipschitz constant of the ground truth labeling function F , i.e., the smallest positive KF that satisfies KF ||x1 \u2212 x2|| \u2265 |F(x1) \u2212 F(x2)| = |y1 \u2212 y2| for all x1, x2 \u223c Mddata , where yi = F(xi) \u2208 {0, 1} is the target label for xi. We estimate this as\nK\u0302F := max j,k ( |yj \u2212 yk| ||xj \u2212 xk|| ) , (1)\ncomputed over all M2 pairings ((xj , yj), (xk, yk)) of some M evenly class-balanced random samples {(xi, yi)}Mi=1 from the dataset D. We use M = 1000 in practice, which we found more than sufficient for a converging estimate, and it takes <1 sec. to compute K\u0302F . We minimize the effect of trivial dataset-specific factors on K\u0302F by linearly normalizing all images to the same range (Sec. 4), and we note that both K\u0302F and ddata are invariant to image resolution and channel count (Appendix B.1). As the natural image datasets have multiple possible combinations of classes for the binary classification task, we report K\u0302F averaged over 25 runs of randomly chosen class pairings."
        },
        {
            "heading": "4 DATASETS, MODELS AND TRAINING",
            "text": "Medical Image Datasets. We conducted our experiments on seven public medical image (radiology) datasets from diverse modalities and anatomies for different binary classification tasks. These are (1) brain MRI glioma detection (BraTS, Menze et al. (2014)); (2) breast MRI cancer detection (DBC, Saha et al. (2018)); (3) prostate MRI cancer risk scoring (Prostate MRI, Sonn et al. (2013)); (4) brain CT hemorrhage detection (RSNA-IH-CT, Flanders et al. (2020)); (5) chest X-ray pleural effusion detection (CheXpert, Irvin et al. (2019)); (6) musculoskeletal X-ray abnormality detection (MURA, Rajpurkar et al. (2017)); and (7) knee X-ray osteoarthritis detection (OAI, Tiulpin et al. (2018)). All dataset preparation and task definition details are provided in Appendix G.\nNatural Image Datasets. We also perform our experiments using four common \u201cnatural\u201d image classification datasets: ImageNet (Deng et al., 2009), CIFAR10 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and MNIST (Deng, 2012).\n3A subtlety here is that our Lipschitz assumptions only involve pairs of datapoints sampled from the true data manifold Mddata ; adversarially-perturbed images (Goodfellow et al., 2015) are not included.\nFor each dataset, we create training sets of size N \u2208 {500, 750, 1000, 1250, 1500, 1750}, along with a test set of 750 examples. These splits are randomly sampled with even class-balancing from their respective base datasets. For the natural image datasets we choose two random classes (different for each experiment) to define the binary classification task, and all results are averaged over five runs using different class pairs.4 Images are resized to 224\u00d7 224 and normalized linearly to [0, 1].\nModels and training. We evaluate six models total: ResNet-18, -34 and -50 (He et al., 2016), and VGG-13, -16 and -19 (Simonyan & Zisserman, 2015). Each model f is trained on each dataset for its respective binary classification task with Adam (Kingma & Ba, 2015) until the model fully fits to the training set, for each training set size N described previously. We provide all training and implementation details in Appendix F, and our code can be found at https://github.com/ mazurowski-lab/intrinsic-properties."
        },
        {
            "heading": "5 THE RELATIONSHIP OF GENERALIZATION WITH DATASET INTRINSIC DIMENSION AND LABEL SHARPNESS",
            "text": "In Fig. 1 we show the average measured intrinsic dimension ddata and label sharpness K\u0302F of each dataset we study. While both natural and medical datasets can range in ddata, we note that medical datasets typically have much higher K\u0302F than natural image datasets, which we will propose may explain differences in generalization ability scaling rates between the two imaging domains. We emphasize that ddata and KF are model-independent properties of a dataset itself. We will now describe how network generalization ability scales with ddata and KF ."
        },
        {
            "heading": "5.1 BOUNDING GENERALIZATION ABILITY WITH DATASET INTRINSIC DIMENSION",
            "text": "A result which we will use throughout is that on average, given some N datapoints sampled i.i.d. from a d-dimensional manifold, the distance between the nearest neighbor x\u0302 of some datapoint x scales as Ex ||x \u2212 x\u0302|| = O(N\u22121/ddata) (Levina & Bickel, 2004). As such, the nearestneighbor distance of some test point to the training set decreases as the training set grows larger by O(N\u22121/ddata). It can then be shown that the loss on the test set/generalization error scales as O(KL max(Kf ,KF )N\u22121/ddata) on average; this is summarized in the following theorem. Theorem 1 (Generalization Error and Dataset Intrinsic Dim. Scaling Law (Bahri et al., 2021)). Let L, f and F be Lipschitz on Mddata with respective constants KL, Kf and KF . Further let Dtrain be a training set of size N sampled i.i.d. from Mddata , with f(x) = F(x) for all x \u2208 Dtrain. Then, L = O(KL max(Kf ,KF )N\u22121/ddata).\n4N = 1750 is the upper limit of N that all datasets could satisfy, given the smaller size of medical image datasets and ImageNet\u2019s typical example count per class. In Appendix C.4 we evaluate much higher N for datasets that allow for it.\nWe note that the KF term is typically treated as an unknown constant in the literature (Bahri et al., 2021); instead, we propose to estimate it with the empirical label sharpness K\u0302F (Sec. 3.2). We will next show that Kf \u2243 KF for large N (common for deep models), which allows us to approximate Theorem 1 as L \u2243 O(KLKFN\u22121/ddata), a scaling law independent of the trained model f . Intuitively, this means that the Lipschitz smoothness of f molds to the smoothness of the label distribution as the training set grows larger and test points typically become closer to training points. Theorem 2 (Approximating Kf with KF ). Kf converges to KF in probability as N \u2192 \u221e.\nWe show the full proof in Appendix A.2 due to space constraints. This result is also desirable because computing an estimate for Kf , the Lipschitz constant of the model f , either using Eq. (1) or with other techniques (Fazlyab et al., 2019), depends on the choice of model f , and may require many forward passes. Estimating KF (Eq. (1) is far more tractable, as it is an intrinsic property of the dataset itself which is relatively fast to compute.\nNext, note that the Lipschitz constant KL is a property of the loss function, which we take as fixed a priori, and so does not vary between datasets or models. As such, KL can be factored out of the scaling law of interest, such that we can simply consider L \u2243 O(KFN\u22121/ddata), i.e.,\nlogL \u2272 \u2212 1 ddata logN + logKF + a (2)\nfor some constant a. In the following section, we will demonstrate how the prediction of Eq. (2) may explain recent empirical results in the literature where the rate of this generalization scaling law differed drastically between natural and medical datasets, via the measured differences in the typical label sharpness K\u0302F of datasets in these two domains."
        },
        {
            "heading": "5.2 GENERALIZATION DISCREPANCIES BETWEEN IMAGING DOMAINS",
            "text": "Consider the result from Eq. (2) that the test loss/generalization error scales approximately as L \u221d KFN\n\u22121/ddata on average. From this, we hypothesize that a higher label sharpness KF will result in the test loss curve that grows faster with respect to ddata.\nIn Fig. 2 we evaluate the generalization error (log test loss) scaling of all models trained on each natural and medical image dataset with respect to the training set intrinsic dimension ddata, for all evaluated training set sizes N . We also show the scaling of test accuracy in Appendix E.1.\nWe see that within an imaging domain (natural or medical), model generalization error typically increases with ddata, as predicted, similar to prior results (Pope et al., 2020; Konz et al., 2022); in particular, approximately logL \u221d \u22121/ddata + const., aligning with Eq. (2). However, we also see that the generalization error scaling is much sharper for models trained on medical data than natural data; models trained on datasets with similar ddata and of the same size N tend to perform much worse if the data is medical images. A similarly large gap appears for the scaling of test accuracy (Appendix E.1). We posit that this difference is explained by medical datasets typically having much higher label sharpness (K\u0302F \u223c 2.5\u00d710\u22124) than natural images (K\u0302F \u223c 1\u00d710\u22124) (Fig. 1) , as KF is the only term in Eq. (2) that differs between two models with the same training set intrinsic dimension ddata and size N . Moreover, in Appendix C.1 we show that accounting for KF increases the likelihood of the posited scaling law given the observed generalization data. However, we note that there could certainly be other factors causing the discrepancy which are not accounted for.\nIntuitively, the difference in dataset label sharpness KF between these imaging domains is reasonable, as KF describes how similar a dataset\u2019s images can be while still having different labels (Sec. 3.2). For natural image classification, images from different classes are typically quite visually distinct. However, in many medical imaging tasks, a change in class can be due to a small change or abnormality in the image, resulting in a higher dataset KF ; for example, the presence of a small breast tumor will change the label of a breast MRI from healthy to cancer."
        },
        {
            "heading": "6 ADVERSARIAL ROBUSTNESS AND TRAINING SET LABEL SHARPNESS",
            "text": "In this section we present another advantage of obtaining the sharpness of the dataset label distribution (KF ): it is negatively correlated with the adversarial robustness of a neural network. Given\nsome test point x0 \u2208 Mddata with true label y = F(x0), the general goal of an adversarial attack is to find some x\u0303 that appears similar to x0 \u2014 i.e., ||x\u0303 \u2212 x0||\u221e is small \u2014 that results in a different, seemingly erroneous network prediction for x\u0303. Formally, the robustness radius of the trained network f at x0 is defined by\nR(f, x0) := inf x\u0303\n{||x\u0303\u2212 x0||\u221e : f(x\u0303) \u0338= y} , (3)\nwhere x0 \u2208 Mddata (Zhang et al., 2021). This describes the largest region around x0 where f is robust to adversarial attacks. We define the expected robust radius of f as R\u0302(f) := Ex0\u223cMddata R(f, x0).\nTheorem 3 (Adversarial Robustness and Label Sharpness Scaling Law). Let f be Kf -Lipschitz on Rn. For a sufficiently large training set, the lower bound for the expected robustness radius of f scales as R\u0302(f) \u2243 \u2126 (1/KF ).\nProof. This follows from Prop. 1 of Tsuzuku et al. (2018) \u2014 see Appendix A.4 for all details.\nWhile it is very difficult to estimate robustness radii of neural networks in practice (Katz et al., 2017), we can instead measure the average loss penalty of f due to attack, Ex0\u223cDtest(L(x\u0303)\u2212 L(x0)), over a test set Dtest of points sampled from Mddata , and see if it correlates negatively with K\u0302F (Eq. (1)) for different models and datasets. As the expected robustness radius decreases, so should the loss penalty become steeper. We use FGSM (Goodfellow et al., 2015) attacks with L\u221e budgets of \u03f5 \u2208 {1/255, 2/255, 4/225, 8/255} to obtain x\u0303.\nIn Fig. 3 we plot the test loss penalty with respect to K\u0302F for all models and training set sizes for \u03f5 = 2/255, and show the Pearson correlation r between these quantities for each model, for all \u03f5, in Table 1 (per-domain correlations are provided in Appendix E.3). (We provide the plots for the other \u03f5 values, as well as for the test accuracy penalty, in Appendix E.3). Here we average results over the different training set sizes N due to the lack of dependence of Theorem 3 on N .\nAs expected, the loss penalty is typically worse for models trained on datasets with higher KF , implying a smaller expected robustness radius. We see that medical datasets, which typically have higher KF than natural datasets (Fig. 1), are indeed typically more susceptible to attack, as was found in Ma et al. (2021). In Appendix D.1 we show example clean\nand attacked images for each medical image dataset for \u03f5 = 2/255. A clinical practitioner may not notice any difference between the clean and attacked images upon first look,5 yet the attack makes model predictions completely unreliable. This indicates that adversarially-robust models may be needed for medical image analysis scenarios where potential attacks may be a concern."
        },
        {
            "heading": "7 CONNECTING REPRESENTATION INTRINSIC DIMENSION TO DATASET INTRINSIC DIMENSION AND GENERALIZATION",
            "text": "The scaling of network generalization ability with dataset intrinsic dimension ddata (Sec. 5.1) motivates us to study the same behavior in the space of the network\u2019s learned hidden representations for the dataset. In particular, we follow (Ansuini et al., 2019; Gong et al., 2019) and assume that an encoder in a neural network maps input images to some drepr-dimensional manifold of representations (for a given layer), with drepr \u226a n. As in the empirical work of Ansuini et al. (2019), we consider the intrinsic dimensionality of the representations of the final hidden layer of f . Recall that the test loss can be bounded above as L = O(KL max(Kf ,KF )N\u22121/ddata) (Thm. 1). A similar analysis can be used to derive a loss scaling law for drepr, as follows.\nTheorem 4 (Generalization Error and Learned Representation Intrinsic Dimension Scaling Law). L \u2243 O(KLN\u22121/drepr), where KL is the Lipschitz constant for L.\n5That being said, the precise physical interpretation of intensity values in certain medical imaging modalities, such as Hounsfield units for CT, may reveal the attack upon close inspection.\nWe reserve the proof for Appendix A.3 due to length constraints, but the key is to split f into a composition of an encoder and a final layer and analyze the test loss in terms of the encoder\u2019s outputted representations. Similarly to Eq. (2), KL is fixed for all experiments, such that we can simplify this result to L \u2243 O(N\u22121/drepr), i.e.,\nlogL \u2272 \u2212 1 drepr logN + b (4)\nfor some constant b. This equation is of the same form as the loss scaling law based on the dataset intrinsic dimension ddata of Thm. 1. This helps provide theoretical justification for prior empirical results of L increasing with drepr (Ansuini et al. (2019), as well as for it being similar in form to the scaling of L with ddata (Fig. (2)).\nIn Fig. 4 we evaluate the scaling of log test loss with the drepr of the training set (Eq. (4)), for each model, dataset, and training set size as in Sec. 5.1. The estimates of drepr are made using TwoNN on the final hidden layer representations computed from the training set for the given model, as in Ansuini et al. (2019). We also show the scaling of test accuracy in Appendix E.1, as well as results from using the MLE estimator to compute drepr.\nWe see that generalization error typically increases with drepr, in a similar shape as the ddata scaling (Fig. 2). The similarity of these curves may be explained by drepr \u2272 ddata, or other potential factors unaccounted for. The former arises if the loss bounds of Theorems 1 and 4 are taken as estimates:\nTheorem 5 (Bounding of Representation Intrinsic Dim. with Dataset Intrinsic Dim.). Let Theorems 1 and 4 be taken as estimates, i.e., L \u2248 KL max(Kf ,KF )N\u22121/ddata and L \u2248 KLN\u22121/drepr . Then, drepr \u2272 ddata.\nProof. This centers on equating the two scaling laws and using a property of the Lipschitz constant of classification networks\u2013 see Appendix A.5 for the full proof.\nIn other words, the intrinsic dimension of the training dataset serves as an upper bound for the intrinsic dimension of the final hidden layer\u2019s learned representations. While a rough estimate, we found this to usually be the case in practice, shown in Fig. 5 for all models, datasets and training\nsizes. Here, drepr = ddata is shown as a dashed line, and we use the same estimator (MLE, Sec. 3.1) for ddata and drepr for consistency (similar results using TwoNN are shown in Appendix E.2).\nIntuitively, we would expect drepr to be bounded by ddata, as ddata encapsulates all raw dataset information, while learned representations prioritize task-related information and discard irrelevant details (Tishby & Zaslavsky, 2015), resulting in drepr \u2272 ddata. Future work could investigate how this relationship varies for networks trained on different tasks, including supervised (e.g., segmentation, detection) and self-supervised or unsupervised learning, where drepr might approach ddata.\nDISCUSSION AND CONCLUSIONS\nIn this paper, we explored how the generalization ability and adversarial robustness of a neural network relate to the intrinsic properties of its training set, such as intrinsic dimension (ddata) and label sharpness (KF ). We chose radiological and natural image domains as prominent examples, but our approach was quite general; indeed, in Appendix C.2 we evaluate our hypotheses on a skin lesion image dataset, a domain that shares similarities with both natural images and radiological images, and intriguingly find that properties of the dataset and models trained on it often lie in between these two domains. It would be interesting to study these relationships in still other imaging domains such as satellite imaging (Pritt & Chern, 2017), histopathology (Komura & Ishikawa, 2018), and others. Additionally, this analysis could be extended to other tasks (e.g., multi-class classification or semantic segmentation), newer model architectures such as ConvNeXt (Liu et al., 2022), nonconvolutional models such as MLPs or vision transformers (Dosovitskiy et al., 2021), or even natural language models.\nOur findings may provide practical uses beyond merely a better theoretical understanding of these phenomena. For example, we provide a short example of using the network generalization dependence on label sharpness to rank the predicted learning difficulty of different tasks for the same dataset in Appendix C.3. Additionally, the minimum number of annotations needed for an unlabeled training set of images could be inferred given the measured ddata of the dataset and some desired test loss (Eq. (2)), which depends on the imaging domain of the dataset (Fig. 2).6 This is especially relevant to medical images, where creating quality annotations can be expensive and time-consuming. Additionally, Sec. 6 demonstrates the importance of using adversarially robust models or training techniques for more vulnerable domains. Finally, the relation of learned representation intrinsic dimension to generalization ability (Sec. 7) and dataset intrinsic dimension (Theorem 5) could inform the minimum parameter count of network bottleneck layers.\nA limitation of our study is that despite our best efforts, it is difficult to definitively say if training set label sharpness (KF ) causes the observed generalization scaling discrepancy between natural and medical image models (Sec. 5.1, Fig. 2). We attempted to rule out alternatives via our formal analysis and by constraining many factors in our experiments (e.g., model, loss, training and test set sizes, data sampling strategy, etc.). Additionally, we found that accounting for KF in the generalization scaling law increases the likelihood of the law given our observed data (Appendix C.1). Altogether, our results tell us that KF constitutes an important difference between natural and medical image datasets, but other potential factors unaccounted for should still be considered.\nOur findings provide insights into how neural network behavior varies within and between the two crucial domains of natural and medical images, enhancing our understanding of the dependence of generalization ability, representation learning, and adversarial robustness on intrinsic measurable properties of the training set.\n6Note that doing so in practice by fitting the scaling law model to existing (L, N , ddata) results would require first evaluating a wider range of N due to the logarithmic dependence of Eq. (2) on N .\nAUTHOR CONTRIBUTIONS\nN.K. wrote the paper, derived the mathematical results, ran the experiments, and created the visualizations. M.A.M. helped revise the paper, the presentation of the results, and the key takeaways."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The authors would like to thank Hanxue Gu and Haoyu Dong for helpful discussion and inspiration."
        }
    ],
    "title": "GENERALIZATION: UNRAVELING LEARNING DIFFER-",
    "year": 2024
}