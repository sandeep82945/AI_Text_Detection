{
    "abstractText": "Pruning is one of the mainstream methods to compress over-parameterized neural networks, resulting in significant practical benefits. Recently, another line of work has explored the direction of fusion, i.e. merging, independently trained neural networks. Here, we seek to marry the two approaches in a bid to combine their advantages into a single approach, which we term \u2018Intra-Fusion\u2019. Specifically, we implicitly utilize the pruning criteria to result in more informed fusion. Agnostic to the choice of a specific neuron-importance metric, Intra-Fusion can typically prune an additional considerable amount of the parameters while retaining the same accuracy as other standard pruning approaches. Additionally, we explore how fusion can be added to the pruning process to significantly decrease the training time while maintaining competitive performance. We benchmark our results for various networks on commonly used datasets such as CIFAR10, CIFAR100, and ImageNet. More broadly, we hope that the proposed approach invigorates exploration into a fresh alternative to the predominant compression approaches.",
    "authors": [],
    "id": "SP:8e1b0694284a02b8159d239d792910d5b3ed0a5e",
    "references": [
        {
            "authors": [
                "Samuel K Ainsworth",
                "Jonathan Hayase",
                "Siddhartha Srinivasa"
            ],
            "title": "Git re-basin: Merging models modulo permutation symmetries",
            "venue": "arXiv preprint arXiv:2209.04836,",
            "year": 2022
        },
        {
            "authors": [
                "Davis Blalock",
                "Jose Javier Gonzalez Ortiz",
                "Jonathan Frankle",
                "John Guttag"
            ],
            "title": "What is the state of neural network pruning",
            "venue": "Proceedings of machine learning and systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tim Dettmers",
                "Mike Lewis",
                "Younes Belkada",
                "Luke Zettlemoyer"
            ],
            "title": "Llm. int8 (): 8-bit matrix multiplication for transformers at scale",
            "venue": "arXiv preprint arXiv:2208.07339,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin"
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "venue": "arXiv preprint arXiv:1803.03635,",
            "year": 2018
        },
        {
            "authors": [
                "Elias Frantar",
                "Dan Alistarh"
            ],
            "title": "Sparsegpt: Massive language models can be accurately pruned in one-shot",
            "year": 2023
        },
        {
            "authors": [
                "Charlie Frogner",
                "Chiyuan Zhang",
                "Hossein Mobahi",
                "Mauricio Araya",
                "Tomaso A Poggio"
            ],
            "title": "Learning with a wasserstein loss",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Timur Garipov",
                "Pavel Izmailov",
                "Dmitrii Podoprikhin",
                "Dmitry P Vetrov",
                "Andrew G Wilson"
            ],
            "title": "Loss surfaces, mode connectivity, and fast ensembling of dnns",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Jianping Gou",
                "Baosheng Yu",
                "Stephen J Maybank",
                "Dacheng Tao"
            ],
            "title": "Knowledge distillation: A survey",
            "venue": "International Journal of Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Song Han",
                "Jeff Pool",
                "John Tran",
                "William J. Dally"
            ],
            "title": "Learning both weights and connections for efficient neural networks",
            "year": 2015
        },
        {
            "authors": [
                "Babak Hassibi",
                "David G. Stork",
                "Gregory J. Wolff"
            ],
            "title": "Optimal brain surgeon and general network pruning",
            "venue": "IEEE International Conference on Neural Networks,",
            "year": 1993
        },
        {
            "authors": [
                "Yang He",
                "Ping Liu",
                "Ziwei Wang",
                "Zhilan Hu",
                "Yi Yang"
            ],
            "title": "Filter pruning via geometric median for deep convolutional neural networks acceleration",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Yihui He",
                "Xiangyu Zhang",
                "Jian Sun"
            ],
            "title": "Channel pruning for accelerating very deep neural networks",
            "venue": "IEEE International Conference on Computer Vision (ICCV),",
            "year": 2017
        },
        {
            "authors": [
                "Richard M. Heiberger",
                "Erich Neuwirth"
            ],
            "title": "One-Way ANOVA, pp. 165\u2013191",
            "venue": "ISBN 978-1-4419-0052-4",
            "year": 2009
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531,",
            "year": 2015
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate",
            "venue": "shift. CoRR,",
            "year": 2015
        },
        {
            "authors": [
                "Benoit Jacob",
                "Skirmantas Kligys",
                "Bo Chen",
                "Menglong Zhu",
                "Matthew Tang",
                "Andrew Howard",
                "Hartwig Adam",
                "Dmitry"
            ],
            "title": "Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "William H. Kruskal",
                "Wilson Allen Wallis"
            ],
            "title": "Use of ranks in one-criterion variance analysis",
            "venue": "Journal of the American Statistical Association,",
            "year": 1952
        },
        {
            "authors": [
                "Matt Kusner",
                "Yu Sun",
                "Nicholas Kolkin",
                "Kilian Weinberger"
            ],
            "title": "From word embeddings to document distances",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Yann LeCun",
                "John Denker",
                "Sara Solla"
            ],
            "title": "Optimal brain damage",
            "venue": "Advances in neural information processing systems,",
            "year": 1989
        },
        {
            "authors": [
                "Jaeho Lee",
                "Sejun Park",
                "Sangwoo Mo",
                "Sungsoo Ahn",
                "Jinwoo Shin"
            ],
            "title": "Layer-adaptive sparsity for the magnitude-based",
            "year": 2021
        },
        {
            "authors": [
                "Yixuan Li",
                "Jason Yosinski",
                "Jeff Clune",
                "Hod Lipson",
                "John Hopcroft"
            ],
            "title": "Convergent learning: Do different neural networks learn the same representations",
            "venue": "arXiv preprint arXiv:1511.07543,",
            "year": 2015
        },
        {
            "authors": [
                "Stuart P. Lloyd"
            ],
            "title": "Least squares quantization in pcm",
            "venue": "IEEE Trans. Inf. Theory,",
            "year": 1982
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In Artificial intelligence and statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Pavlo Molchanov",
                "Arun Mallya",
                "Stephen Tyree",
                "Iuri Frosio",
                "Jan Kautz"
            ],
            "title": "Importance estimation for neural network pruning",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Sidak Pal Singh",
                "Dan Alistarh"
            ],
            "title": "Woodfisher: Efficient second-order approximation for neural network compression",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Sidak Pal Singh",
                "Martin Jaggi"
            ],
            "title": "Model fusion via optimal transport",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Sui",
                "Miao Yin",
                "Yi Xie",
                "Huy Phan",
                "Saman Aliari Zonouz",
                "Bo Yuan"
            ],
            "title": "Chip: Channel independence-based pruning for compact neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "C\u00e9dric Villani"
            ],
            "title": "Optimal transport: old and new, volume 338",
            "year": 2009
        },
        {
            "authors": [
                "Chaoqi Wang",
                "Roger B. Grosse",
                "Sanja Fidler",
                "Guodong Zhang"
            ],
            "title": "Eigendamage: Structured pruning in the kronecker-factored eigenbasis",
            "venue": "CoRR, abs/1905.05934,",
            "year": 2019
        },
        {
            "authors": [
                "Hongyi Wang",
                "Mikhail Yurochkin",
                "Yuekai Sun",
                "Dimitris Papailiopoulos",
                "Yasaman Khazaeni"
            ],
            "title": "Federated learning with matched averaging",
            "venue": "arXiv preprint arXiv:2002.06440,",
            "year": 2020
        },
        {
            "authors": [
                "Guangxuan Xiao",
                "Ji Lin",
                "Mickael Seznec",
                "Julien Demouth",
                "Song Han"
            ],
            "title": "Smoothquant: Accurate and efficient post-training quantization for large language models",
            "venue": "arXiv preprint arXiv:2211.10438,",
            "year": 2022
        },
        {
            "authors": [
                "Zhewei Yao",
                "Reza Yazdani Aminabadi",
                "Minjia Zhang",
                "Xiaoxia Wu",
                "Conglong Li",
                "Yuxiong He"
            ],
            "title": "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ruichi Yu",
                "Ang Li",
                "Chun-Fu Chen",
                "Jui-Hsin Lai",
                "Vlad I. Morariu",
                "Xintong Han",
                "Mingfei Gao",
                "Ching-Yung Lin",
                "Larry S. Davis"
            ],
            "title": "Nisp: Pruning networks using neuron importance score propagation",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Xiyu Yu",
                "Tongliang Liu",
                "Xinchao Wang",
                "Dacheng Tao"
            ],
            "title": "On compressing deep models by low rank and sparse decomposition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Mikhail Yurochkin",
                "Mayank Agarwal",
                "Soumya Ghosh",
                "Kristjan Greenewald",
                "Nghia Hoang",
                "Yasaman Khazaeni"
            ],
            "title": "Bayesian nonparametric federated learning of neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Chen Zhang",
                "Yu Xie",
                "Hang Bai",
                "Bin Yu",
                "Weihong Li",
                "Yuan Gao"
            ],
            "title": "A survey on federated learning",
            "venue": "Knowledge-Based Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Aojun Zhou",
                "Yukun Ma",
                "Junnan Zhu",
                "Jianbo Liu",
                "Zhijie Zhang",
                "Kun Yuan",
                "Wenxiu Sun",
                "Hongsheng Li"
            ],
            "title": "Learning n: m fine-grained structured sparse neural networks from scratch",
            "venue": "arXiv preprint arXiv:2102.04010,",
            "year": 2021
        },
        {
            "authors": [
                "Da-Wei Zhou",
                "Han-Jia Ye",
                "De-Chuan Zhan"
            ],
            "title": "Co-transport for class-incremental learning",
            "venue": "In Proceedings of the 29th ACM International Conference on Multimedia, MM",
            "year": 2021
        },
        {
            "authors": [
                "Maohua Zhu",
                "Tao Zhang",
                "Zhenyu Gu",
                "Yuan Xie"
            ],
            "title": "Sparse tensor core: Algorithm and hardware co-design for vector-wise sparse neural networks on modern gpus",
            "venue": "In Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Alongside the massive progress in the past few years, modern over-parameterized neural networks have also brought another thing onto the table. That is, of course, their massive size. Consequently, as part of the community keeps training bigger networks, another community has been working, often in the background, to ensure that these bulky networks can be made compact to actually be deployed (Hassibi et al., 1993). Techniques to reduce the size of these networks and speed-up inference come in various forms, such as pruning \u2014 which can itself be unstructured (Han et al., 2015), semi-structured (Zhou et al., 2021a), or structured (Wang et al., 2019; Frantar & Alistarh, 2023); quantization (Dettmers et al., 2022; Yao et al., 2022; Xiao et al., 2022); knowledge distillation (Hinton et al., 2015; Gou et al., 2021); low-rank decomposition (Yu et al., 2017); hardware co-design (Zhu et al., 2019), to list a few.\nHowever, despite the apparent conceptual simplicity of these techniques, compressing neural networks, in practice, is not as straightforward as simply doing one or two traditional post-processing steps (Blalock et al., 2020). The process involves a crucial element\u2014fine-tuning or retraining, on the original dataset or a subset\u2014extending over several additional epochs. While such additional fine-tuning may not seem too much of an issue for some networks, but for others like large language models even a single epoch might be excessively expensive. Hence, this makes the question of investigating the direction of \u2018fine-tuning-free\u2019 compression methods or even \u2018data-free\u2019 compression methods all the more pertinent.\nBesides, since it is almost a given that the training pipeline for taking any interesting network from scratch to deployment will include some form of compression, an overlooked aspect is whether any improvements can be introduced in this joint space of training and pruning in the conventional strategy. For instance, a fine example is the Lottery Ticket Hypothesis (Frankle & Carbin, 2018), which suggests the presence of sparse sub-networks that can be trained from the outset while obviating the need for subsequent pruning. Presently, however, this is more of an existence result, since the sub-networks are obtained retrospectively, i.e., having trained dense networks from scratch. Another prominent example is that of Federated learning (McMahan et al., 2017; Zhang et al., 2021), which has made the community rethink the process of training large models in a distributed manner by carrying local model updates and then aggregating these model parameters directly.\nIn fact, stemming from the practical interest in federated learning, but also theoretical questions of mode connectivity (Garipov et al., 2018), another novel line of work has lately explored the possibility of fusing (the parameters of) independently trained networks (potentially of different sizes). A notable work in this direction, from which we are heavily inspired, is that of OTFusion (Singh & Jaggi, 2020). More specifically, amongst other things, the authors also demonstrate the idea of fusing a network with a lesser version of itself, say a pruned version, in a bid to help recover the performance drop in the past. However, as their demonstration was merely a proof-of-concept and inherently limited in scope, this exciting idea of self-recovery has remained in a nascent stage.\nOur focus in this paper is, therefore, to unite these two lines of work, namely pruning and fusion, in a more cohesive manner. By unifying these two concepts, we sim to expand the horizon of the conventional pruning paradigm \u2014 across the trifold axes of:\n(i) Intra-Fusion: While most research on pruning has focused on devising more meaningful neuron importance metrics, the overlying procedure has remained largely the same: Keep the most important neurons, discard the rest. In contrast, Intra-Fusion leverages Optimal Transport to additionally inform the process of model compression with the neurons that otherwise would have been discarded (see Section 3). (ii) Data-Free pruning: Pruning neural networks generally leads to immediate drops in accuracy, hence requiring an extensive fine-tuning step to be usable in a practical setting. Accuracy drops between simpler and more sophisticated neuron importance metrics do not differ substantially. We argue that this is largely an artifact of the underlying pruning procedure and show that, by using Intra-Fusion, a significant amount of accuracy can be recovered without the need for any datapoints. (iii) Split-Data Training: Models that are to be pruned after training are most often fine-tuned for many epochs to regain sufficient performance. Via the presented \u2018PaF\u2019 and \u2018FaP\u2019 approaches, we factorize this process through the combination of model pruning and model fusion. By splitting the training dataset into multiple parts, over which models are trained concurrently, we achieve significant training time speedups."
        },
        {
            "heading": "2 BACKGROUND",
            "text": ""
        },
        {
            "heading": "2.1 PRUNING",
            "text": "Pruning techniques (LeCun et al., 1989) can broadly be classified into structured (Wang et al., 2019; Fang et al., 2023), and unstructured pruning (Han et al., 2015; Singh & Alistarh, 2020). While Unstructured pruning involves zeroing out individual weights and leaves the network structure unaltered ,. Our work exclusively deals with structured pruning, which aims to remove entire sets of parameters or neurons and thereby directly alters the network structure. The reason being that , (a) the structured pruning procedure directly translates into a speedup in the inference time \u2014 unlikein unstructured pruning, which requires the aid of specialized hardware accelerators to extract some (typically reduced) levels of speedup; (b) structured pruning eases the storage and memory footprintof the network; while unstructured pruning methods yield no such gains (but, in fact, also necessitate the maintenance of binary masks during the course of training).\nStructured pruning. A very common and successful approach to prune networks structurally is to capture a neuron\u2019s importance by its \u2113p-norm, where p is the order of the norm. Despite its simplicity, \u2113p-norm pruning can achieve state-of-the-art performance by cleverly incorporating the dependencies within the network (Fang et al., 2023). However, other works such as (He et al., 2019) have shown that the \u201csmaller-norm-less-important assumption\u201d does not always hold. Instead of removing neurons with a small norm, redundant filters can be found by exploiting relationships between neurons of the same layer. Namely, they consider neurons close to the geometric mean to be redundant as they represent information abundant in the layer. Instead of evaluating importance based on the weight itself, other methods focus on the activations of the neurons. The importance of a neuron is thus measured by evaluating the reconstruction error of the current layer (He et al., 2017) or of the final response layer (Yu et al., 2018).\nEvidently, research on structured pruning has largely focused on devising more meaningful importance measures, while the overlying procedure has remained the same, repeating the mantra: Keep the most important neurons, discard the rest. This work challenges the above mantra by recycling or restoring information from all neurons to create more accurate compressed networks."
        },
        {
            "heading": "2.2 OPTIMAL TRANSPORT & MODEL FUSION",
            "text": "Optimal Transport (OT). OT (Villani et al., 2009) is a mathematical framework that provides a rigorous and geometrically interpretable way to compare probability distributions. The OT problem aims to find the most economical way to \u201ctransport\u201d mass from one distribution defined over a space X to another supported over the space Y , where the cost is determined by a function c. It achieves this by lifting the metric in the ground space (i.e., X ,Y) to obtain a metric in the space of distributions. In the case of discrete probability measures, OT reduces to the well-known transportation problem in linear programming, which has the following form:\nOT(\u00b5, \u03bd;C) := min \u27e8T,C\u27e9F s.t., T1m = \u03b1, T\u22a41n = \u03b2 and T \u2208 R(n\u00d7m)+ .\nHere, \u00b5 := \u2211n i=1 \u03b1i \u00b7 \u03b4(xi) and \u03bd := \u2211m j=1 bj \u00b7 \u03b4(yj), with \u2211n i=1 \u03b1i = \u2211m\nj=1 \u03b2j = 1 describe two probability measures, where \u03b4(\u00b7) denotes the dirac delta function. Further, T denotes the transport map whose row and columns should sum to the marginals \u03b1 and \u03b2. Besides, C denotes the ground cost, of moving a unit mass from point xi to yj , and for instance, in the Euclidean case, it can be C(xi, yj) = \u2225xi \u2212 yj\u22252. Optimal Transport has found applications in many fields, especially in machine learning (Kusner et al., 2015; Frogner et al., 2015; Arjovsky et al., 2017; Zhou et al., 2021b) and we will consider this in regards to fusion. Model Fusion. The key idea behind model fusion is to combine the capabilities of two parent networks into a single-child network. Singh & Jaggi (2020) introduces Optimal Transport (OT) for model fusion, which is the fusion technique we are using throughout this paper. We will refer to this method as OTFusion. The main idea behind this work is to combine multiple independently trained neural networks, after accounting for the permutation symmetries that exist within the layers. Finding the permutation symmetries is then framed as an Optimal Transport problem between the neurons of the given networks. An important thing to note is that Singh & Jaggi (2020) primarily use uniform distributions in place of \u03b1 and \u03b2; however as we will see later, we further exploit this in our proposed method.\nBesides OTFusion, other works are also inherently based on a similar formulation (Li et al., 2015; Yurochkin et al., 2019; Wang et al., 2020; Ainsworth et al., 2022), though not always geared towards the same end. Our focus will nevertheless be on OTFusion, as we directly build atop their exploration of pruning and fusion."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "Intra-Fusion, as a \u201cmeta-pruning\u201d approach, is an attempt to take a step back from the search for meaningful importance metrics, and reconsider the way these found importance metrics are leveraged to come up with a sparse model representation. Instead of simply discarding the least important neurons (as done in the \u201cconventional pruning\u201d approach, see Algorithm 1), Intra-Fusion leverages a modified version of OTFusion to incorporate the discarded neurons into the \u201csurviving\u201d ones (see Algorithm 2). We refer to our algorithm as a new \u201dmeta\u201d approach to pruning, since it challenges the overlying framework that defines how an agnostic importance metric is integrated. It does not compete with any importance metrics. Instead it provides an alternative methodology on how these metrics are used to compress a network.\nThis section is dedicated to the inner workings of Intra-Fusion. Since Intra-Fusion is an alternative to the conventional pruning procedure, we show both meta-pruning approaches side by side (see Algorithm 1 and 2) to highlight the differences between the two. Section 3.1 serves as a high-level overview of the two algorithms, whereas the subsequent sections are more detailed explanations of the individual parts that make up Intra-Fusion, which are also referenced in Algorithm 2."
        },
        {
            "heading": "3.1 STRUCTURED PRUNING VS META PRUNING: AN OVERVIEW",
            "text": "Structured Pruning: Group-by-Group. As described in Section 2.1, the increasing complexity of neural networks pose a challenge for structured pruning. Removing neurons from layers individually can lead to broken networks, as neurons in a layer might not only be dependent on the previous layer, but also on those that lie even further back.\nIn Figure 1, we show an example of this: assume we are given the beginning of a network in Figure 1a, where layer three represents the arrival of a residual connection. When iterating through\nData: IMPORTANCE i1\u00d7n, group G with layergroup cardinality n, and target layergroup cardinality m((((\n((, and group G. Result: group Gnew with layergroup cardinality m Algorithm 1: Conventional Pruning t\u2190 mth highest scalar in i ; Gnew \u2190 \u2205 ; for layer \u2208 G do\nlayernew \u2190 layer without neurons j where i[j] < t ; layernew \u2190 neuron j \u2208 layer \u2227 i[j] \u2265 t Gnew = Gnew \u222a {layernew} ;\nend\nAlgorithm 2: Intra-Fusion Y\u2190 GETTARGET(G) ; 3.2.1 X\u2190 G ; 3.2.1 Cn\u00d7m \u2190 COMPUTECOST(X,Y) ; 3.2.2 \u03bd1\u00d7m \u2190 GETTARGETDISTR(m, i) ; 3.2.3 \u00b51\u00d7n \u2190 GETSOURCEDISTR(n, i) ; 3.2.3 Tn\u00d7m \u2190 OT(\u00b5,\u03bd,C) ; 3.2.4 Tm\u00d7n \u2190 diag( 1\u00b5 )\u00d7 T\n\u22a4 ; 3.2.4 Gnew \u2190 \u2205 ; for layer \u2208 G do\nGnew = Gnew \u222a {T \u00d7 layer} ; 3.2.4 end\n(a) The beginning of an example network (b) Broken network: individually pruned layers (c) The network as a set of neuron pairings (d) Structurally pruning complete group\nFigure 1: Structural pruning by considering groups\nthe network layer-by-layer, while pruning, the network can easily be broken (see Figure 1b). As multiple layers can be arranged in such a dependency, pruning of multiple layers has to be done jointly (in our example layers one to three). We call a set of layers that have to be pruned in unison a group G. In a group, not all neurons are dependent on one another. We can identify \u201cpairings of neurons\u201d that have to be handled jointly (see the different colored pairings in Figure 1c). The number of neuron pairings in a group we term \u201cgroup cardinality\u201d (in our example this would be three).\nStructurally pruning the layernetwork in Figure 1a whilst considering the dependencies could result in the network shown in Figure 1d.\nMeta-Pruning Comparison. The starting point for conventional pruning and for our Intra-Fusion methodology is the same. We are given a group G with initial group cardinality n, that we wish to prune to a target group cardinality m \u2264 n. Moreover, we are given an importance vector i1\u00d7n that assigns an agnostic importance score (e.g. \u21131-norm) to each independent pairing in the group. In conventional pruning (see Algorithm 1), we keep the m most important neuron pairings according to i, and remove the rest to arrive at our new group Gnew with group cardinality m.\nMost research on structured pruning has focused on devising more meaningful importance measures, i.e. i, whereas the overlying procedure detailed in Algorithm 1 has remained practically unaltered. Inspired by OTFusion, we attempt to develop an alternative to the way pruning is conventionally done. Instead of simply discarding the less important pairings in a group (in Figure 1c this would be the blue pairing), we leverage the computed importance metrics to inform the process of fusing these pairings to end up at a lower group cardinality.\nTo end up at a lower group cardinality, we fuse the exisiting n-many pairings to end up at mmany pairings. The matching of the pairings to be fused is found via Optimal Transport (OT), which requires the careful setting of multiple non-trivial hyper-parameter choices. In particular, the discrete \u201csource distribution\u201d of our OT problem is representative of the original network\u2019s group G (group cardinality n), i.e., it is supported on the space of neuron -pairings.\nTo complete the OT problem formulation we additionally need to determine a target of layergroup cardinality m, and a neural similarity measure to quantify the transportation cost. Lastly, we need to ascertain the probabilistic measures encapsulating the mass distribution of both the source and target entities. Solving the just formulated OT problem gives a transportation map T ."
        },
        {
            "heading": "3.2 COMPONENTS OF INTRA-FUSION.",
            "text": "3.2.1 TARGET AND SOURCE SELECTION. Our experiments reveal that a simple, but fruitful option is to simply use Gnew derived by Algorithm 1, i.e. the neurons with the highest importance according to some agnostically defined metric. Another promising approach is to cluster the neurons with K-means (Lloyd, 1982), or Gaussian Mixture Models, with m clusters, and use the respective cluster centroids as the target.\n3.2.2 TRANSPORTATION COST. Once we have determined the source and the target, the next step is to quantify the cost of moving a unit mass from a source neuron to a target neuron. For this we will take a metric that measures how similar or dissimilar neurons are. In case a group contains multiple layers (as shown in Figure 1), we have to find a joint similarity measure for pairings of neurons.\nGiven two vectors a and b, each representative of the weights of a different neural -pairing, we determine similarity by their normalized \u21131-distance. The weights for neural -pairings can be derived via concatenating the weights of the respective neurons and bias terms.\nHowever, architectural components such as Batch Normalization (BN) (Ioffe & Szegedy, 2015) evidently presents themselves as a challenge due to itstheir unique connection with itstheir prior layer. We overcome this challenge by taking advantage of the properties of batch normalization, and simply merge the batchnorm layer into the prior layer whose outputs it acts upon (Jacob et al., 2018),\nwnew = w \u00d7 \u03b3\u221a\n\u03c3 , bnew = (b\u2212 \u00b5)\u00d7 \u03b3\u221a \u03c3 + \u03b2 . (1)\nIn this context, w denotes the weight vector corresponding to the preceding layer. Furthermore, we denote b as its associated bias term. The symbols \u00b5 and \u03c3 represent the running mean and variance, respectively, of the batch normalization layer, while \u03b3 and \u03b2 symbolize the learnable parameters.\n3.2.3 PROBABILITY DISTRIBUTION. We propose two different ways of quantifying probability mass: uniform or importance-informed. A uniform distribution prescribes equal mass to each neuron pairing. In the importance-informed option, the mass is relative to the importance of the neuron pairing(or neuron pairing respectively). In order to transform the importance of a neuron pairing (pairing of neurons) to a probability, one can either divide the importance by the sum of all importances, or use softmax. While a uniform target and importance-informed source distribution (see Figure 2b) can sometimes achieve superior results, a uniform distribution for both source and target (see Figure 2a) is generally the most robust choice. In Appendix C.4 we show that there are no significant differences in accuracy between the choice of source and target distribution.\n3.2.4 DERIVING FUSED NEURONS. Given the cost matrix C, and our probability distributions \u00b5, and \u03bd, we can finally derive the optimal transport map T . Since the columns of this transport map act as the coefficients for the weighted aggregation of corresponding neuron s pairings, it is imperative to ensure that these coefficients collectively sum to unity.\nMoreover, as a final preparatory step before amalgamating matching neuron s pairings, it is necessary to conjoin the batch normalization layer with the layer upon which it operates. This process is elucidated in Equation 1. Subsequently, we establish the values of \u00b5 and \u03b2 as zero, and \u03c3 and \u03b3 as one, thereby preserving the unaltered state of the batch normalization layer\u2019s activation.\nConsequently, we proceed to traverse each layer within the group denoted as G, and rather than removing neurons of diminished significance, we opt to generate fused neurons through a process of matrix multiplication with the transport map T . This methodology culminates in pruned layers that emerge as a product of a nuanced aggregation, where the shared features of these neurons are subject to a weighted summation."
        },
        {
            "heading": "4 EMPIRICAL RESULTS",
            "text": "Here, we seek to illustrate the accuracy gains Intra-Fusion can achieve. Most pruning literature ignores pre-finetuning accuracy, largely due to the significant accuracy drops imposed. In Section 4, we show that this drop is mainly an artefact of the overlying pruning methodology. Namely, by utilizing importance metrics in a more involved way as done in Intra-Fusion, a significant amount of accuracy can be maintained. Lastly, for the sake of completeness, we show how Intra-Fusion stands up in face of fine-tuning.\nTerminology. Since we cut whole neurons and not just individual edges we will distinguish between \u201cneuron sparsity\u201d and \u201cweight sparsity\u201d. We refer to \u201cneuron sparsity\u201d as the number of neuron s pairings that are cut out of a layergroup. Accordingly, we will refer to the number of edges removed from the neural network as \u201cweight sparsity\u201d. Where unspecified, we are referring to neuron -sparsity when talking about \u201csparsity\u201d. See Appendix E.1 for a comprehensive comparison.\nLastly, in this Section we use the term \u201cGroup\u201d. This refers to G as described in Section 3. Moreover, the group indices are ordered such that small indices are closer to the output of the model, e.g. Group 4 is closer to the end of the model than Group 5.\nData-Free: Pruning without fine-tuning. In order to compare the data-free performance of the conventional meta-pruning paradigm (see Algorithm 1), and Intra-Fusion (see Algorithm 2), we compare the test accuracy of a VGG11-BN, ResNet18, ResNet50, on CIFAR-10, CIFAR-100, and ImageNet. Furthermore, the pruning is done on the basis of a group, for importance metrics \u21131, and more sophisticated ones such as Taylor (Molchanov et al., 2019), and LAMP (Lee et al., 2021) and CHIP (Sui et al., 2021). Given the nature of Taylor importance, additional information about parameter gradients is needed when deploying it as a metric. Due to the extensive set of experiments, we are forced to show only a selection in this section and refer to Appendix E.2 for a more comprehensive list of results.\nA snapshot of the results can be seen in Figure 3 and 4. While diverse importance t metrics do not seem to affect pre-finetuning accuracy meaningfully, Intra-Fusion can leverage the an importance metric to substantially increase accuracy (by up to +60% in certain cases) without any additional use of data. To further highlight that the improvements of Intra-Fusion are agnostic to the choice of importance metric, we include results assigning random scores drawn from a uniform distribution as an importance metric (see Appendix C.3).\nAcross different network architectures, there appear to be, in general, two different kinds of groups: volatile and resilient. Volatile groups exhibit strong accuracy losses as the sparsity increases (e.g., see \u201dGroup 6\u201d in Figure 4), whereas resilient groups only experience small ones (e.g., see \u201dGroup 16\u201d in Figure 3). This pattern seems to be agnostic with respect to the importance metric. Nevertheless, Intra-Fusion manages to increase accuracy substantially for both types.\nTake for instance Group 6 from Figure 4, a volatile group. For a neuron sparsity of 40%, the conventionally pruned model has dropped to an accuracy of only 80.2%, whereas the Intra-Fused model is still at a competitive 92.1%. However, even for resilient groups, where the margin of improvement is low, we make improvements (see Group 19 in Fig. 3). These results represent a general trend (see Appendix E.2 for all results). Overall, this shows the benefit of our proposed approach, which can alleviate the performance drop without relying on fine-tuning, or for that matter on any datapoints at all.\nData-Driven: Pruning with fine-tuning. Although fine-tuning may not always be the most convenient in all scenarios, it might be possible in others. In any case, it would be interesting to see whether the performance gains delivered by Intra-Fusion standup in the face of fine-tuning or not. Hence, we carry out a similar experiment as before for both VGG11-BN and ResNet18 trained on CIFAR10; however, this time, fine-tuning after model compression is available.\nTable 1 contains our results (averaged over multiple runs) for this setting. We observe that IntraFusion obtains a consistent gain of up to 1% test accuracy (with standard deviation of 0.13% for all sparsities), across all the considered sparsity levels. While the gains might not seem as stark, we must remark that here we allowed for a significantly long fine-tuning schedule, and that Intra-Fusion converges faster due to the large initial accuracy gains. It is also important to note that the focus of this paper is on data-free pruning.\nTo conclude, our consistent gains show that the boost afforded by Intra-Fusion is complementary to that provided via just fine-tuning the pruned model \u2014 thereby demonstrating the efficacy of our approach."
        },
        {
            "heading": "5 UNDERSTANDING INTRA-FUSION",
            "text": "So far, we have elucidated and juxtaposed Intra-Fusion with the conventional pruning methodology, demonstrating that Intra-Fusion possesses the distinctive capability to enhance accuracy significantly without relying on any data. In this section, we would like to obtain a better understanding intoof the inner workings of Intra-Fusion endeavor aims to understand the benefit of Intra-Fusion over conventional pruning. Output Preservation. A very intuitive explanation for the superior performance of Intra-Fusion is its ability to better preserve the output of the original non-pruned model. Hence, we quantify output divergence by the \u21132-distance to the output of the original model for various groups in the data-free scenario at different sparsities. As can be seen in Figure 5 (and in more detail in Appendix C.1), Intra-Fusion is indeed able to preserve the output better, with the margin growing as the sparsity increases. Thus, it seems that merging akin neurons as we do with Intra-Fusion leads to better output preservation, and subsequent superior performance.\nNeural Landscape. To gather further insights as to how Intra-Fusion differs from the conventional pruning methodology when navigating the weight space, we show an example of a pruned network and the corresponding accuracy landscape that surrounds the models. We vectorize all the parameters before carrying out their linear interpolation for this figure and following the procedure in (Garipov et al., 2018). We specify three models in that space (in our case: original model, default pruned model, and the Intra-Fusion model), one of which serves as the origin, and thus a 2D slice is built. In this slice, we sample a grid of possible networks and evaluate their performance on the test set. It is important to note that not all models in the given 2D slice of the parameter space are pruned (e.g. original model).\nIn Figure 6 we depict how the accuracy varies for different models in the identified 2D slice of the parameter space. Evidently, the Intra-Fusion model ends up at a more convenient part of the accuracy landscape when compared to the regularly pruned model (\u201dDefault\u201d) resulting in a superior performance. To further explore how the accuracy landscape develops through different sparsities and when the whole model is pruned, we include additional figures in Section C.2. It appears that using the less important neurons to inform the process of model pruning is more effective at identifying a favorable spot in the parameter space than simply discarding them."
        },
        {
            "heading": "6 APPLICATION BEYOND PRUNING: FACTORIZING MODEL TRAINING",
            "text": "In an attempt to find further valuable integrations of pruning and fusion, we also explore how fusion can be used to \u201cfactorize\u201d and possibly speed up the training process of models that are supposed to be pruned after training. In an increasingly digitalized world where the amount of available data points is consistently increasing. We are looking for an approach that manages to leverage the fact that also on subsets of the whole dataset, a competitive performance can be achieved. This factorization provides another angle on enhancing the performance and training time of models. andIt could for example be especially interesting as an alternative or enhancement to data parallelism during distributed model training.\nThe model that is created with the standard pruning approach (trained on the whole dataset, then pruned and finally fine-tuned) we call the \u201cwhole-data model\u201d. Like in a real-world setting, the result of the pruning is fine-tuned since this most of the time recovers a lot of performance.\nThe Split-Data Approach. In the approach we want to propose, we are split ting the model training into smaller phases by utilizing pruning and fusion. For this we first split the data set into two subsets a and b on which we then train two individual models modela, modelb in parallel. This leads to a theoretical 2x speedup in the model training time since the convergence on half of the dataset does not take more epochs than on the whole dataset (see Figure 20). These two models are then fused using OT and fine-tuned on the whole data set. To reach the target sparsity we prune the resulting network and fine-tune again. We call this approach FaP (Fuse and Prune, see Figure 7). For completeness we also include the performance of individually pruning and fine-tuning the networks before fusing \u2014 this we call PaF (Prune and Fuse, see Figure 22). For the pruning part of doing FaP and PaF, we use the introduced Intra-Fusion. To give a concrete insight into the relative runtimes of the different approaches, we give a side-by-side comparison of their timelines in Appendix D.1. Relative to the model training time of a VGG11-BN (CIFAR10), we achieve a speedup of 1.81. The overall speedups of the split-data approaches are 1.42 (PaF) and 1.31 (FaP). Importantly, in applications, the time for training T1 of a model on large datasets will typically be much greater than the time for fine-tuning T2 (T1 \u226b T2), and speedups are expected to be much more significant.\nWe delve deeper into the details and performance of this \u201cSplit-Data\u201d concept in Appendix D. Specifically see \u201cPerformance Comparison: After Convergence\u201d (Appendix D.2), \u201cPerformance Comparison: Varying Fine-Tuning\u201d (Appendix D.3) and \u201ck-Fold Split-Data\u201d (Appendix D.4).\nDeploying the Split-Data approach when training Resnet18 on Cifar10, we were able to recover and even slightly improve over the \u201dwhole-data model\u201d performance, while providing a 2xsignificant speedup in the training time of the involved models (see Appendix D.2). For VGG11-BN we achieve similar results at the cost of higher resource requirements (\u201dk-Fold\u201d setting, see Appendix D.4).\nSplit-Data As Alternative To Data Parallelism. Splitting the dataset into different parts that models are trained on individually is not a new idea. Specifically, during distributed model training in cloud infrastructures, this is a common approach called Data-Parallelism. However, the gradients are exchanged among the models after the individual backpropagation steps so all models are effectively updated with gradients computed from the whole dataset. This leads to high network utilization, sensitivity to network latency and wait times between the training steps.\nIn our Split-Data approaches (like PaF and FaP), we completely separate the model training. Each model has its designated part of the training data it is trained on. Only after the models have individually converged are the edge weights communicated and fused. This yields far less communication overhead and is not sensitive to network latency."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "In sum, we perform a detailed investigation of unifying and bridging the paradigms of pruning and fusion through our conceptions of Pruning-and-Fusion as well as Fusion-and-Pruning. Specifically, we showed how our proposed technique of Intra-Fusion provides a consistent gain \u2014 with and without fine-tuning, the latter being also privacy-preserving and highly cost-efficient. We also investigated how fusion can be used to factorize the training process, given that it is subsequently accompanied by pruning, to result in non-trivial speedup in training times. The sparsity obtained via our algorithm is also amenable to actual speedup in inference times, without needing special accelerators. Overall, our work shows the compatibility of bringing together pruning and fusion in the form of meta-pruning, and the potential inherent therein. All in all, this raises the question of rethinking the pre-dominant paradigm and perhaps redefining our approach to obtaining compact models via fusion."
        },
        {
            "heading": "A Experimental Hyperparameters 14",
            "text": "A.1 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 A.2 Intra-Fusion Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nB Implementation 14"
        },
        {
            "heading": "C Understanding Intra-Fusion 14",
            "text": "C.1 Output Preservation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 C.2 Neural Landscape . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 C.3 Agnosticism to Importance Metrics . . . . . . . . . . . . . . . . . . . . . . . . 18 C.4 Ablation Study on Varying Target and Source Distribution . . . . . . . . . . . . 20"
        },
        {
            "heading": "D Application Beyond Pruning: Factorizing Model Training 22",
            "text": "D.1 Runtime Comparison of Split-Data and Whole-Data approach . . . . . . . . . . 22 D.2 Performance Comparison: After Convergence . . . . . . . . . . . . . . . . . . 22 D.3 Performance Comparison: Varying Fine-Tuning . . . . . . . . . . . . . . . . . 23 D.4 k-Fold Split-Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 D.5 Extensions for Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 D.6 Performance of Models Used . . . . . . . . . . . . . . . . . . . . . . . . . . . 26"
        },
        {
            "heading": "E Empirical Results 27",
            "text": "E.1 Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 E.2 Data-Free Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27"
        },
        {
            "heading": "A EXPERIMENTAL HYPERPARAMETERS",
            "text": "A.1 TRAINING\nDuring the original training of the used VGG11-BN and Resnet18 networks, we deploy the training hyperparameters in Table 2. For the fine-tuning of models after pruning we use the hyperparameters in Table 3.\nA.2 INTRA-FUSION SETTINGS\nFor our data-free and data-driven results, we use a homogeneous distribution for both the target and source distribution. Moreover, we use the most important neuron pairings as the target.\nB IMPLEMENTATION As mentioned in Section 3, structural pruning is inherently complex due to the inderdependencies existing within state-of-the-art neural networks. Researchers and engineers have relied on manuallydesigned and model-specific schemes to handle these. Evidently, this is intractable and not scalable, particularly for more complex networks.\nRecently, (Fang et al., 2023) introduced a fully-automatic and general way to structurally prune neural networks, by extracting a dependency graph from the computational graph derived by backpropagation. Thus, in order for Intra-Fusion to be generally applicable across a wide range of models in an automized fashion, we have extended their library to work with Intra-Fusion. This way, IntraFusion can be applied to a wide and diverse set of models without the user having to adapt the code in any way."
        },
        {
            "heading": "C UNDERSTANDING INTRA-FUSION",
            "text": "In the following sections, we want to further expand on Section 5 by providing more extensive results and background information.\nC.1 OUTPUT PRESERVATION\nThe following figures show how well Intra-Fusion is able to preserve the output of the non-pruned model for VGG11 and ResNet18 on CIFAR-10 and CIFAR-100. As before, Intra-Fusion seems to be able to better preserve the output at low to high sparsities.\nC.2 NEURAL LANDSCAPE\nSimilarly, we expand the experiment in Section 5 by showing results for more sparsities and modeldataset combinations."
        },
        {
            "heading": "50 Original",
            "text": "C.3 AGNOSTICISM TO IMPORTANCE METRICS\nAs we have alluded to in Section 3, we argue that the performance improvements of Intra-Fusion are agnostic with respect to the choice of the importance metric. That is, irregardless of the expressiveness of the importance metric, Intra-Fusion is able to achieve a significant gain in accuracy. In order to further highlight this, we compare how Intra-Fusion performs when it is given random scores drawn from a uniform distribution as an importance metric.\nAs can be seen Table 4, Intra-Fusion still achieves significant increases in accuracy even when only random scores are available. We argue that the superiority of Intra-Fusion is inherent to the integration of the less important neuron pairings in the compressed network. Hence, making it truly agnostic to the importance metric used.\nC.4 ABLATION STUDY ON VARYING TARGET AND SOURCE DISTRIBUTION\nIn this section, we intend to shed a light on the potential differences between choosing uniform or an importance-informed distribution for the target and source distribution, as explained in Section 3.2.\nIn our analysis, we trained six ResNet18 models on CIFAR-10, applying \u21131 criterion-based pruning across various groups and sparsity levels. The source and target distributions for the optimal transport (OT) setting were chosen as either uniform or importance-informed. To assess the significance of distribution choices, we computed the p-value using the Kruskal-Wallis H-test (Kruskal & Wallis, 1952) for each group-sparsity combination. This test evaluates whether the population medians of all distributions are equal, serving as a non-parametric alternative to ANOVA (Heiberger & Neuwirth, 2009).\nThe resulting p-values for each group-sparsity pair are visualized in Figure 19. We consider p-values less than or equal to 0.05 as statistically significant. Interestingly, the majority of cases do not exhibit a significant difference. However, for groups 10 and 11, with sparsities ranging from 10% to 50%, notable and statistically significant differences between the distributions emerge.\nIn Table 5, we focus on Group 10 and 11 to discern nuances in performance. The optimal distribution choices are highlighted in green for the best-performing and in red for the least effective. Notably, the uTuS (uniform target, uniform source) and iTuS (importanceinformed target, uniform source) configurations appear superior, while uTiS (uniform target, importance-informed source) performs less favorably. However, it is crucial to emphasize\nthat the variations in accuracy are subtle, seldom exceeding one percentage point.\nIn light of these findings, we deduce that the selection between uniform or importance-informed distributions for both source and target in the optimal transport (OT) context lacks statistically significant impact in the majority of cases. Furthermore, in instances where statistical significance is observed, the differences remain marginal.\nRemark: As mentioned before, the pruning criterion used in this study is based on the \u21131-norm. Evidently, this necessarily affects the importance-informed distribution. Thus, it might be possible that some other, more meaningful importance criterion could yield improvements for the importance-informed distributions. However, in light of the results we expect the differences to be more or less marginal."
        },
        {
            "heading": "D APPLICATION BEYOND PRUNING: FACTORIZING MODEL TRAINING",
            "text": "D.1 RUNTIME COMPARISON OF SPLIT-DATA AND WHOLE-DATA APPROACH\nD.1 Model Convergence During Training With Half the Data\nD.2 PERFORMANCE COMPARISON: AFTER CONVERGENCE\nTo investigate the performance potential of the two approaches, in Figure 23 we show the performance of the PaF and FaP model when the fine-tuning at the intermediate steps is done until convergence. Although there is no clear performance equivalency between the classic whole-data model and the \u201cfactorized\u201d model training (FaP and PaF), it is evident that depending on the model architecture and training dynamics the factorization has the potential to even outperform the standard approach. All experiments are done with an iterative pruning approach to recover a stronger performance at higher sparsities. Since data is available for fine-tuning and due to its superior performance, we use activation-based fusion with a sample size of 200.\nFor reference, we also compare the performance of the PaF and FaP models using regular pruning (instead of Intra-Fusion) in Figure 24.\nD.3 PERFORMANCE COMPARISON: VARYING FINE-TUNING\nSince the performance of the PaF approach depends on the amount of fine-tuning that is available at the intermediate steps, we also explore how the performance difference to the whole-data model\ndevelops with varying amounts of fine-tuning. In this setting, the PaF and FaP models gain a theoreticla 2x speedup in the original training process and take the same time in the post-processing as the whole-data model. In Figure 25 (using Intra-Fusion) and Figure 26 (using conventional pruning) for multiple sparsities, we vary the amount of retraining that is available to the models. This means here we do not get the converged performance of PaF and Fap (only converged at 80 fine-tuning epochs).\nD.4 K-FOLD SPLIT-DATA\nAs an alternative, to simply splitting the dataset into two and using each subset to train a model (that\u2019s what we have done so far), we can generalize to a k-fold style approach.\nD.4.1 GENERATING ADDITIONAL TRAININGSETS\nHere we split the dataset into k equally sized and distinct subsets sp. We now create training datasets di that consist of k/2-many of these subsets sp. By choosing k%2 = 0 we ensure that each di will end up containing 50% of the original dataset. We then take all possible ( k\nk/2\n) -many di and\nindividually train models on them.\nD.4.2 EXTENDING PAF AND FAP\nSince the fusion algorithm naturally extends to fusing more than two models (as also presented by (Singh & Jaggi, 2020)), we can now generalize PaF and FaP to combine pruning and fusion of more than two models - leading to a more effective use of the OT-based fusion approach.\nIt is obvious that the 50/50 split of the dataset that we considered in the previous split-data experiments can also be interpreted as a k-fold style approach with k = 2, yielding ( 2 1 ) = 2 different models (\u201dmodel a\u201d and \u201cmodel b\u201d in Figure 27a).\nTo explore the potential of this approach we also evaluated the performance for the next even choice of k, namely k = 4. This already yields ( 4 2 ) = 6 different models that are be combined in PaF and FaP (see models \u201ca\u201d to \u201cf\u201d in Figure 27b).\nD.4.3 PERFORMANCE COMPARISON ACROSS DIFFERENT K\nThe performance of PaF and FaP based on the k = 2 and k = 4 can be compared in Figure 28. For the VGG11-BN we observe performance improvements of up to 1%. Here is important to note that across all measured sparsities the 4-Fold approach always outperforms the 2-Fold approach and yields a very competitive performance when compared to the benchmark \u201cWhole Data Model\u201d. For the Resnet18 we seem to not make any improvements in performance by extending from two to six combined models.\nD.4.4 CONSEQUENCES FOR MODEL TRAINING SPEEDUP\nThis approach comes with additional requirements for computational resources to enable the parallel training and pruning of the multiple individually trained models. However, besides fusion taking insignificantly longer, it does not require more time than the previously explored split-data approaches and thus yields the same speedup.\nD.5 EXTENSIONS FOR FUTURE WORK\nWe leave it for further research to explore less drastic splits of the dataset. We believe that this will lead to a better fine-tuning/accuracy trade-off - especially at lower sparsities. For example, the dataset could be split into overlapping sets that for example make up 60% or 70% of the original dataset.\nD.6 PERFORMANCE OF MODELS USED\nTo also get a feeling for how uncompetitive the performance of the individual split-data models is (since they each were only trained on one half of the data) before deploying our PaF and FaP approach and fine-tuning on the whole dataset we include the model performance figures (across different seeds) in Tables 6 and 8. For each seed a different split of the dataset is generated which the split-data models are trained upon"
        },
        {
            "heading": "E EMPIRICAL RESULTS",
            "text": "E.1 TERMINOLOGY\nIn Table 9, we show how Neuron -Sparsity (applied to all groups in the model) translates to Weight -Sparsity.\nE.2 DATA-FREE EXPERIMENTS\nHere, we provide a full list of results for the data-free experiments. The indices again indicate how close a group is to the output of the model, i.e. Group 0 is the last group of the network. Moreover, we color-code every entry where the absolute difference between the default and Intra-Fused model is greater than 0.5%."
        },
        {
            "heading": "10 89.11 89.28 +0.17 89.30 89.35 +0.05 89.41 89.42 +0.01",
            "text": ""
        },
        {
            "heading": "10 89.31 89.52 +0.21 89.45 89.53 +0.08 89.22 89.44 +0.22",
            "text": ""
        },
        {
            "heading": "30 89.30 89.31 +0.01 89.27 89.46 +0.19 89.44 89.56 +0.12",
            "text": ""
        },
        {
            "heading": "20 89.38 89.41 +0.03 89.40 89.55 +0.15 89.54 89.49 -0.05",
            "text": ""
        },
        {
            "heading": "10 89.53 89.47 -0.06 89.47 89.46 -0.01 89.53 89.52 -0.01",
            "text": ""
        },
        {
            "heading": "10 94.73 94.84 +0.11 94.52 94.77 +0.25 94.38 94.64 +0.26",
            "text": ""
        },
        {
            "heading": "10 94.25 94.56 +0.31 94.20 94.52 +0.32 93.96 94.46 +0.49",
            "text": ""
        },
        {
            "heading": "30 94.58 94.86 +0.28 94.72 94.75 +0.03 94.49 94.73 +0.25",
            "text": ""
        },
        {
            "heading": "20 94.75 94.87 +0.12 94.87 94.75 -0.12 94.67 94.79 +0.11",
            "text": ""
        },
        {
            "heading": "10 94.87 94.91 +0.04 94.86 94.85 -0.01 94.71 94.84 +0.13",
            "text": ""
        },
        {
            "heading": "50 94.72 94.86 +0.14 94.36 94.86 +0.49 94.38 94.78 +0.40",
            "text": ""
        },
        {
            "heading": "40 94.84 94.82 -0.02 94.65 94.89 +0.24 94.64 94.74 +0.09",
            "text": ""
        },
        {
            "heading": "30 94.86 94.89 +0.04 94.72 94.88 +0.16 94.70 94.83 +0.13",
            "text": ""
        },
        {
            "heading": "20 94.88 94.88 +0.00 94.88 94.87 -0.01 94.80 94.80 +0.00",
            "text": ""
        },
        {
            "heading": "10 94.86 94.93 +0.07 94.90 94.85 -0.05 94.81 94.76 -0.05",
            "text": ""
        },
        {
            "heading": "30 94.75 94.79 +0.04 94.77 94.66 -0.11 94.23 94.55 +0.32",
            "text": ""
        },
        {
            "heading": "20 94.81 94.82 +0.01 94.81 94.78 -0.03 94.46 94.64 +0.17",
            "text": ""
        },
        {
            "heading": "10 94.89 94.91 +0.02 94.88 94.84 -0.04 94.71 94.70 -0.01",
            "text": ""
        },
        {
            "heading": "10 94.91 94.84 -0.07 94.88 94.80 -0.08 94.45 94.73 +0.28",
            "text": ""
        },
        {
            "heading": "30 76.68 77.01 +0.33 72.24 72.40 +0.16 75.62 75.58 -0.04",
            "text": ""
        },
        {
            "heading": "20 76.99 77.26 +0.28 72.45 72.62 +0.17 75.79 75.59 -0.20",
            "text": ""
        },
        {
            "heading": "10 77.41 77.43 +0.02 72.49 72.45 -0.04 75.84 75.97 +0.13",
            "text": ""
        },
        {
            "heading": "20 77.23 77.47 +0.24 72.57 72.33 -0.24 75.64 75.98 +0.34",
            "text": ""
        },
        {
            "heading": "10 77.58 77.46 -0.12 72.59 72.75 +0.16 75.95 76.20 +0.25",
            "text": ""
        },
        {
            "heading": "10 77.31 77.47 +0.16 72.48 72.49 +0.01 76.02 76.09 +0.07",
            "text": ""
        },
        {
            "heading": "60 69.53 69.09 -0.44 69.76 69.57 -0.19 69.01 68.87 -0.14",
            "text": ""
        },
        {
            "heading": "50 74.62 74.66 +0.04 74.79 74.76 -0.03 74.13 74.59 +0.47",
            "text": ""
        },
        {
            "heading": "20 75.57 75.78 +0.22 75.58 75.70 +0.12 75.45 75.81 +0.36",
            "text": ""
        },
        {
            "heading": "10 75.75 75.88 +0.14 75.70 75.77 +0.07 75.73 75.91 +0.18",
            "text": ""
        },
        {
            "heading": "40 74.58 75.00 +0.41 74.86 75.11 +0.25 74.74 75.17 +0.43",
            "text": ""
        },
        {
            "heading": "30 75.01 75.39 +0.38 75.17 75.46 +0.29 75.23 75.49 +0.26",
            "text": ""
        },
        {
            "heading": "20 75.22 75.63 +0.41 75.47 75.64 +0.17 75.47 75.66 +0.18",
            "text": ""
        },
        {
            "heading": "10 75.56 75.72 +0.16 75.80 75.75 -0.04 75.69 75.84 +0.16",
            "text": ""
        },
        {
            "heading": "50 73.95 74.02 +0.07 73.87 74.14 +0.27 73.79 74.11 +0.32",
            "text": ""
        },
        {
            "heading": "20 75.39 75.69 +0.30 75.24 75.61 +0.38 75.36 75.82 +0.47",
            "text": ""
        },
        {
            "heading": "10 75.74 75.85 +0.11 75.58 75.72 +0.14 75.72 75.82 +0.10",
            "text": ""
        },
        {
            "heading": "20 75.03 75.49 +0.46 75.30 75.58 +0.28 75.28 75.66 +0.38",
            "text": ""
        },
        {
            "heading": "10 75.51 75.80 +0.29 75.63 75.79 +0.17 75.61 75.78 +0.17",
            "text": ""
        },
        {
            "heading": "10 75.59 75.84 +0.25 75.74 75.79 +0.04 75.83 75.94 +0.12",
            "text": ""
        },
        {
            "heading": "30 75.07 75.49 +0.43 75.12 75.60 +0.48 75.08 75.54 +0.46",
            "text": ""
        },
        {
            "heading": "20 75.39 75.68 +0.29 75.51 75.79 +0.29 75.48 75.83 +0.35",
            "text": ""
        },
        {
            "heading": "10 75.71 75.83 +0.12 75.75 75.90 +0.15 75.63 75.87 +0.24",
            "text": ""
        },
        {
            "heading": "50 73.87 74.37 +0.50 74.80 74.30 -0.50 74.13 74.37 +0.25",
            "text": ""
        },
        {
            "heading": "20 75.59 75.87 +0.28 75.60 75.77 +0.17 75.55 75.76 +0.22",
            "text": ""
        },
        {
            "heading": "10 75.84 75.94 +0.10 75.68 75.89 +0.21 75.69 75.81 +0.13",
            "text": ""
        },
        {
            "heading": "60 72.92 73.29 +0.38 73.90 73.93 +0.03 73.11 73.46 +0.34",
            "text": ""
        },
        {
            "heading": "50 73.63 74.13 +0.50 74.67 74.42 -0.25 74.05 74.18 +0.12",
            "text": ""
        },
        {
            "heading": "30 75.06 75.51 +0.45 75.34 75.64 +0.30 75.06 75.49 +0.44",
            "text": ""
        },
        {
            "heading": "20 75.40 75.65 +0.25 75.56 75.73 +0.17 75.32 75.67 +0.35",
            "text": ""
        },
        {
            "heading": "10 75.72 75.80 +0.08 75.80 75.76 -0.04 75.53 75.77 +0.24",
            "text": ""
        },
        {
            "heading": "50 74.59 74.77 +0.18 74.87 74.79 -0.08 74.30 74.55 +0.24",
            "text": ""
        },
        {
            "heading": "20 75.62 75.78 +0.15 75.62 75.74 +0.12 75.59 75.81 +0.22",
            "text": ""
        },
        {
            "heading": "10 75.83 75.87 +0.04 75.75 75.83 +0.07 75.75 75.85 +0.10",
            "text": ""
        },
        {
            "heading": "50 73.85 74.16 +0.32 74.26 74.47 +0.21 73.69 73.76 +0.06",
            "text": ""
        },
        {
            "heading": "20 75.41 75.71 +0.30 75.36 75.69 +0.32 75.40 75.59 +0.19",
            "text": ""
        },
        {
            "heading": "10 75.65 75.82 +0.17 75.71 75.79 +0.09 75.67 75.78 +0.11",
            "text": ""
        }
    ],
    "year": 2023
}