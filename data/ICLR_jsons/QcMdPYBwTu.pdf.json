{
    "abstractText": "Graph Neural Networks (GNNs) have become the de facto standard for modeling graph-structured data in various applications. Among them, implicit GNNs have shown a superior ability to effectively capture long-range dependencies in underlying graphs. However, implicit GNNs tend to be computationally expensive and have high memory usage, due to 1) their use of full-batch training; and 2) they require a large number of iterations to solve a fixed-point equation. These compromise the scalability and efficiency of implicit GNNs especially on large graphs. In this paper, we aim to answer the question: how can we efficiently train implicit GNNs to provide effective predictions on large graphs? We propose a new scalable and effective implicit GNN (SEIGNN) with a mini-batch training method and a stochastic solver, which can be trained efficiently on large graphs. Specifically, SEIGNN can more effectively incorporate global and long-range information by introducing coarse-level nodes in the mini-batch training method. It also achieves reduced training time by obtaining unbiased approximate solutions with fewer iterations in the proposed solver. Comprehensive experiments on various large graphs demonstrate that SEIGNN outperforms baselines and achieves higher accuracy with less training time compared with existing implicit GNNs.",
    "authors": [],
    "id": "SP:25f844f06c3c6d36f4c4811b678ddd262e971eb5",
    "references": [
        {
            "authors": [
                "Shaojie Bai",
                "J. Zico Kolter",
                "Vladlen Koltun"
            ],
            "title": "Deep equilibrium models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Shaojie Bai",
                "Vladlen Koltun",
                "J. Zico Kolter"
            ],
            "title": "Multiscale deep equilibrium models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Abraham Berman",
                "Robert J Plemmons"
            ],
            "title": "Nonnegative matrices in the mathematical sciences",
            "year": 1994
        },
        {
            "authors": [
                "Jie Chen",
                "Tengfei Ma",
                "Cao Xiao"
            ],
            "title": "FastGCN: Fast learning with graph convolutional networks via importance sampling",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Ming Chen",
                "Zhewei Wei",
                "Zengfeng Huang",
                "Bolin Ding",
                "Yaliang Li"
            ],
            "title": "Simple and deep graph convolutional networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Xuanqing Liu",
                "Si Si",
                "Yang Li",
                "Samy Bengio",
                "Cho-Jui Hsieh"
            ],
            "title": "Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks",
            "venue": "In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining,",
            "year": 2019
        },
        {
            "authors": [
                "Zhengyang Geng",
                "Xin-Yu Zhang",
                "Shaojie Bai",
                "Yisen Wang",
                "Zhouchen Lin"
            ],
            "title": "On training implicit models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Justin Gilmer",
                "Samuel S. Schoenholz",
                "Patrick F. Riley",
                "Oriol Vinyals",
                "George E. Dahl"
            ],
            "title": "Neural message passing for quantum chemistry",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Fangda Gu",
                "Heng Chang",
                "Wenwu Zhu",
                "Somayeh Sojoudi",
                "Laurent El Ghaoui"
            ],
            "title": "Implicit graph neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Will Hamilton",
                "Zhitao Ying",
                "Jure Leskovec"
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Weihua Hu",
                "Matthias Fey",
                "Marinka Zitnik",
                "Yuxiao Dong",
                "Hongyu Ren",
                "Bowen Liu",
                "Michele Catasta",
                "Jure Leskovec"
            ],
            "title": "Open graph benchmark: Datasets for machine learning on graphs",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Wenbing Huang",
                "Tong Zhang",
                "Yu Rong",
                "Junzhou Huang"
            ],
            "title": "Adaptive sampling towards fast graph representation learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Kenji Kawaguchi"
            ],
            "title": "On the theory of implicit deep learning: Global convergence with implicit layers",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Diederick P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2015
        },
        {
            "authors": [
                "Thomas N Kipf",
                "Max Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "Mingjie Li",
                "Yifei Wang",
                "Yisen Wang",
                "Zhouchen Lin"
            ],
            "title": "Unbiased stochastic proximal solver for graph neural networks with equilibrium states",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Qimai Li",
                "Zhichao Han",
                "Xiao-Ming Wu"
            ],
            "title": "Deeper insights into graph convolutional networks for semi-supervised learning",
            "venue": "In AAAI,",
            "year": 2018
        },
        {
            "authors": [
                "Juncheng Liu",
                "Kenji Kawaguchi",
                "Bryan Hooi",
                "Yiwei Wang",
                "Xiaokui Xiao"
            ],
            "title": "Eignn: Efficient infinite-depth graph neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Juncheng Liu",
                "Bryan Hooi",
                "Kenji Kawaguchi",
                "Xiaokui Xiao"
            ],
            "title": "Mgnni: Multiscale graph neural networks with implicit layers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Junyoung Park",
                "Jinhyun Choo",
                "Jinkyoo Park"
            ],
            "title": "Convergent graph solvers",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Weijing Shi",
                "Raj Rajkumar"
            ],
            "title": "Point-gnn: Graph neural network for 3d object detection in a point cloud",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Aravind Subramanian",
                "Pablo Tamayo",
                "Vamsi K Mootha",
                "Sayan Mukherjee",
                "Benjamin L Ebert",
                "Michael A Gillette",
                "Amanda Paulovich",
                "Scott L Pomeroy",
                "Todd R Golub",
                "Eric S Lander"
            ],
            "title": "Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2005
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio"
            ],
            "title": "Graph Attention Networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Fangping Wan",
                "Lixiang Hong",
                "An Xiao",
                "Tao Jiang",
                "Jianyang Zeng"
            ],
            "title": "Neodti: neural integration of neighbor information from a heterogeneous network for discovering new drug\u2013target",
            "venue": "interactions. Bioinformatics,",
            "year": 2019
        },
        {
            "authors": [
                "Felix Wu",
                "Amauri Souza",
                "Tianyi Zhang",
                "Christopher Fifty",
                "Tao Yu",
                "Kilian Weinberger"
            ],
            "title": "Simplifying graph convolutional networks",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Zonghan Wu",
                "Shirui Pan",
                "Fengwen Chen",
                "Guodong Long",
                "Chengqi Zhang",
                "S Yu Philip"
            ],
            "title": "A comprehensive survey on graph neural networks",
            "venue": "IEEE transactions on neural networks and learning systems,",
            "year": 2020
        },
        {
            "authors": [
                "Keyulu Xu",
                "Chengtao Li",
                "Yonglong Tian",
                "Tomohiro Sonobe",
                "Ken-ichi Kawarabayashi",
                "Stefanie Jegelka"
            ],
            "title": "Representation learning on graphs with jumping knowledge networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Hanqing Zeng",
                "Hongkuan Zhou",
                "Ajitesh Srivastava",
                "Rajgopal Kannan",
                "Viktor Prasanna"
            ],
            "title": "GraphSAINT: Graph sampling based inductive learning method",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Hanqing Zeng",
                "Muhan Zhang",
                "Yinglong Xia",
                "Ajitesh Srivastava",
                "Andrey Malevich",
                "Rajgopal Kannan",
                "Viktor Prasanna",
                "Long Jin",
                "Ren Chen"
            ],
            "title": "Decoupling the depth and scope of graph neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Shijie Zhang",
                "Hongzhi Yin",
                "Tong Chen",
                "Quoc Viet Nguyen Hung",
                "Zi Huang",
                "Lizhen Cui"
            ],
            "title": "Gcnbased user representation learning for unifying robust recommendation and fraudster detection",
            "venue": "In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval,",
            "year": 2020
        },
        {
            "authors": [
                "Zeng"
            ],
            "title": "Flickr is a single-label multi-class classification dataset. The task is to categorize types of images based on the descriptions and common properties of online",
            "year": 2020
        },
        {
            "authors": [
                "Li"
            ],
            "title": "Reddit is a single-label multi-class classification dataset. The task is to predict different communities of online posts. We use the Reddit dataset from Hamilton et al",
            "year": 2023
        },
        {
            "authors": [
                "Hu"
            ],
            "title": "ogbn-products is a single-label multi-class classification dataset which contains an undirected and",
            "year": 2020
        },
        {
            "authors": [
                "Hu"
            ],
            "title": "As we use the same experimental setting on some datasets, we reuse the results of some baselines from Li et al",
            "venue": "We compare SEIGNN with 3 implicit GNNs (i.e., USP (Li et al.,",
            "year": 2023
        },
        {
            "authors": [
                "IGNN (Gu"
            ],
            "title": "2020)) and 6 explicit/traditional GNNs (GCN (Kipf and Welling, 2016), GraphSAGE (Hamilton et al., 2017)",
            "venue": "FastGCN (Chen et al.,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Graph Neural Networks (GNNs) have become the de facto standard for modeling graph-structured data in various applications. Among them, implicit GNNs have shown a superior ability to effectively capture long-range dependencies in underlying graphs. However, implicit GNNs tend to be computationally expensive and have high memory usage, due to 1) their use of full-batch training; and 2) they require a large number of iterations to solve a fixed-point equation. These compromise the scalability and efficiency of implicit GNNs especially on large graphs. In this paper, we aim to answer the question: how can we efficiently train implicit GNNs to provide effective predictions on large graphs? We propose a new scalable and effective implicit GNN (SEIGNN) with a mini-batch training method and a stochastic solver, which can be trained efficiently on large graphs. Specifically, SEIGNN can more effectively incorporate global and long-range information by introducing coarse-level nodes in the mini-batch training method. It also achieves reduced training time by obtaining unbiased approximate solutions with fewer iterations in the proposed solver. Comprehensive experiments on various large graphs demonstrate that SEIGNN outperforms baselines and achieves higher accuracy with less training time compared with existing implicit GNNs."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Recently, Graph Neural Networks (GNNs) have been widely used for modeling graph-structured data in the real world and achieved great success in numerous applications (Wu et al., 2020) including computer vision (Shi and Rajkumar, 2020), recommendation systems (Zhang et al., 2020), and drug discovery (Wan et al., 2019). In general, to utilize both graph topology and node attributes for generating meaningful node representations, modern GNNs iteratively aggregate representations of neighbors of each node with its own representation to update representations, which is termed as the \u201cmessage passing\" mechanism (Gilmer et al., 2017).\nDespite the success achieved by these GNNs on different tasks, they lack the ability to capture long-range dependencies in graphs. The reason is that traditional GNN models can only capture information up to T -hops away with T layers. T cannot be large because a large T causes the over-smoothing problem (Li et al., 2018). To mitigate this limitation of traditional GNNs, another type of GNNs, called implicit GNNs, has been proposed to capture long-range dependencies (Gu et al., 2020; Liu et al., 2022; Li et al., 2023). Implicit GNNs define an implicit layer using a fixedpoint equation for aggregation and generate the equilibrium as node representations by solving the fixed-point equation. These implicit GNNs have superiority in capturing long-range information as they can be treated as a GNN with many layers defined implicitly to aggregate the information from distant nodes in the forward pass. Meanwhile, they enjoy constant memory complexity through implicit differentiation when computing gradients in the backward pass (Bai et al., 2019).\nIn spite of the advantages of implicit GNNs in capturing long-range information, they suffer from scalability issues on large graphs. The first limitation is is their reliance on full-batch training, which involves iteratively aggregating an entire graph to solve the fixed-point equation. This approach incurs significant computational costs and it is sometimes not even feasible on large graphs since massive memory may be required to load a whole graph into GPU during training. For example, MGNNI runs out of GPU memory on the ogbn-products dataset as shown in Table 2.\nAlthough there are several sampling-based methods proposed to enable mini-batch training for traditional GNN models (Hamilton et al., 2017; Zeng et al., 2021; Chiang et al., 2019), it is infeasible to directly apply these methods to implicit GNNs without sacrificing their effectiveness 1. The reason is that these methods split an entire graph into several subgraphs as mini-batches, which prohibits information propagation between different mini-batches. Therefore, directly employing existing mini-batch methods may be harmful to the ability of implicit GNNs to capture long-range information. Besides the limitation of using full-batch training, the second limitation is that implicit GNNs are computationally expensive to train, as they usually require a large number of iterations to iteratively solve a fixed-point equation. This issue can be exacerbated on large graphs.\nMotivated by the aforementioned limitations of previous implicit GNNs, in this paper, we aim to answer the question: how to efficiently train implicit GNNs to provide effective predictions on large graphs? To achieve this, we propose a scalable and effective implicit GNN (SEIGNN) with a mini-batch training method. Specifically, following previous sampling-based methods, our designed mini-batch training method also samples subgraphs, but adds coarse-level nodes representing different graph partitions, while new edges are included considering both coarse nodes and original nodes. In this way, our mini-batch training method avoids full-batch training and encourages information propagation between nodes within different mini-batches, which cannot be achieved by previous implicit GNNs or by directly applying existing mini-batch methods to implicit GNNs. Therefore, with this mini-batch training, SEIGNN can scale up to large graphs without sacrificing the ability to capture global or long-range information.\nMoreover, to reduce the extensive training time of previous implicit GNNs caused by a large number of iterations for obtaining equilibrium, in SEIGNN, we also propose a new stochastic unbiased solver that can solve the fixed-point equation with fewer iterations to obtain approximated equilibrium.\nOur contributions We summarize the contributions of this work as follows:\n\u2022 We first point out the scalability and efficiency limitations of previous implicit GNNs, which are caused by full-batch training and their large number of iterations used for getting equilibrium.\n\u2022 To mitigate the limitations, we propose SEIGNN, a scalable and effective implicit GNN, which incorporates our designed mini-batch training methods with coarse nodes to capture global information; and a new stochastic solver that achieves unbiased estimates of equilibriums. Through these, SEIGNN can be efficiently trained on large graphs while maintaining the ability to capture long-range information.\n\u2022 Comprehensive experiments on 6 datasets show that SEIGNN achieves better accuracies with less training time on large graphs. Additionally, the detailed ablation studies further demonstrate the effectiveness of our methods."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 GRAPH NEURAL NETWORKS",
            "text": "GNNs have been widely used for modeling graph data in different tasks. Modern GNNs (Kipf and Welling, 2016; Hamilton et al., 2017) usually follow the \u201cmessage passing\" mechanism where information is aggregated from the neighbor nodes for each node. Different GNN models have been proposed to utilize skip connection (Xu et al., 2018; Chen et al., 2020), attention mechanism (Velic\u030ckovic\u0301 et al., 2018), and simplified activation (Wu et al., 2019). However, these models usually only involve finite aggregation layers due to the over-smoothing problem (Li et al., 2018), which makes them hardly capture long-range dependencies on graphs."
        },
        {
            "heading": "2.2 IMPLICIT MODELS AND IMPLICIT GRAPH NEURAL NETWORKS",
            "text": "Implicit models / deep equilibrium models (Bai et al., 2019) generally define an equilibrium equation as implicit hidden layers to generate outputs by solving the equation. These models have attracted much attention recently as they can avoid storing hidden states and achieve constant memory consumption by using implicit differentiation. For example, Bai et al. (2019) and Bai et al. (2020)\n1Table 5 empirically shows that directly using these methods with implicit GNNs performs poorly.\npropose the deep equilibrium model (DEQ) and its multiscale variant demonstrating the ability of implicit models in image and text related tasks. Some theoretical works (Kawaguchi, 2021) and (Geng et al., 2021) also explore the convergence analysis and provide a gradient estimate for implicit models to avoid exact gradient computation, respectively.\nInspired by implicit models, several implicit GNN models have been proposed, such as IGNN (Gu et al., 2020), EIGNN (Liu et al., 2021), MGNNI (Liu et al., 2022), CGS (Park et al., 2022). Since these models iteratively aggregate information from neighbors to obtain the fixed-point solution of the equilibrium equation, they can capture long-range information on graphs. However, as these implicit GNNs use full-batch training that iteratively aggregates a full graph to get outputs, they cost a lot of training time and need massive memory for storing a whole graph and the representation of each node in GPU memory. To reduce the computation complexities, a recent work USP (Li et al., 2023) proposes to use mini-batch training as aggregating information on randomly sampled subgraphs with proposed proximal solvers. However, randomly sampling subgraphs as mini-batch might affect the ability of implicit GNNs to capture long-range information."
        },
        {
            "heading": "2.3 GRAPH NEURAL NETWORKS FOR LARGE GRAPHS",
            "text": "To avoid huge computation costs incurred in training GNNs on large graphs, several mini-batch training methods for traditional GNNs have been proposed to scale up GNNs. Cluster-GCN (Chiang et al., 2019), GraphSAINT (Zeng et al., 2020), and Shadow-GNN (Zeng et al., 2021) sample subgraphs as minibatches and train GNNs within different subgraphs to reduce the computation cost. Specifically, Cluster-GCN relies on a graph clustering method to generate subgraphs while Shadow-GNN uses Personalized-PageRank scores to select important nodes to form a subgraph for each target node. GraphSAGE (Hamilton et al., 2017) proposes to use a sampled neighborhood of a node for message aggregation to efficiently generate node representations."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "An undirected graph can be represented as G = (V, E) which consists of the node set V with n nodes and the edge set E . Each node v has a length-d feature xv. The adjacency matrix A \u2208 Rn\u00d7n and the node feature matrix X \u2208 Rd\u00d7n are taken as the input for graph neural networks. Considering unweighted adjacency matrix A, if node i and j are connected, Ai,j = 1, otherwise Ai,j = 0.\nTraditional GNNs and Implicit GNNs Traditional GNNs (Kipf and Welling, 2016; Chen et al., 2020; Hamilton et al., 2017) have a learnable aggregation process that iteratively propagates information from each node to its neighbor nodes. For each layer l, the aggregation step can be defined as follows:\nZ(l+1) = \u03d5(W (l)Z(l)S +\u2126(l)X), (1)\nwhere Z(l) is the hidden states in layer l, S is the normalized adjacency matrix; W (l) and \u2126(l) are trainable weight matrices.\nSimilar to traditional GNNs, implicit GNNs (Gu et al., 2020; Liu et al., 2021; 2022; Park et al., 2022) also have an aggregation process but with tied wight matrices W and \u2126 at each iteration step. The aggregation process in implicit GNNs is generally defined as Z(l+1) = \u03d5(WZ(l)S +\u2126X) at step l. Given such aggregation step, implicit GNNs solve the fixed-point equation Z\u2217 = \u03d5(WZ\u2217S +\u2126X) and obtain the equilibrium Z\u2217 as node representations. To obtain the equilibrium, implicit GNNs usually require a large number of iterations of the equation until convergence, which may demand a significant amount of time. For example, MGNNI (Liu et al., 2022) ensure the convergence by using a damping factor \u03b3 \u2208 [0, 1) and define the aggregation as follows:\nZ(l+1) = \u03b3g(W )Z(l)S + f(X,G), (2)\nwhere g(W ) projects the weight W into a Frobenius norm ball of radius < 1 and f(X,G) is a parameterized transformation. In contrast, IGNN (Gu et al., 2020) enforces \u2225W\u2225\u221e \u2264 \u03ba/\u03bbpf (A) with the Perron-Frobeius (PF) eigenvalue \u03bbpf (Berman and Plemmons, 1994).\nMini-batch sampling for GNNs In general, mini-batch methods for GNNs need to compute the predictions for target nodes in each mini-batch. Previous mini-batch sampling methods for GNNs\ngenerally either sample auxiliary nodes of target nodes to form a mini-batch (Hamilton et al., 2017; Zeng et al., 2021) or directly sample subgraphs from the whole graph and use a subgraph as a mini-batch (Chiang et al., 2019). For example, given a set of target nodes Vout, Shadow-GNN (Zeng et al., 2021) constructs a set of auxiliary nodes Vaux by selecting nodes with top-k personalized PageRank (PPR) scores for each target node v \u2208 Vtgt. To obtain a mini-batch, it forms a subgraph Gs with a node set Vs = Vtgt \u222a Vaux and train GNNs on this subgraph as if it is the full graph."
        },
        {
            "heading": "4 SCALABLE AND EFFECTIVE IMPLICIT GNNS",
            "text": "As mentioned in Sec 1, previous implicit GNNs (Gu et al., 2020; Liu et al., 2022; Park et al., 2022) usually use full-batch training with an entire graph which involves recursively aggregating neighbors to calculate fixed-point solutions for nodes. It needs massive GPU memory and is not feasible for large graphs (e.g., with millions of nodes) since full-batch training requires storing a whole graph and representations of all nodes. Moreover, these works usually cost a lot of time for model training as they require a large number of iterations to solve a fixed-point equation.\nMotivated by these limitations, we propose an implicit graph model SEIGNN which enables minibatch training for implicit GNNs without harming the ability to capture long-range information and accelerates model training with a new unbiased stochastic solver."
        },
        {
            "heading": "4.1 MINI-BATCH TRAINING",
            "text": "For traditional GNNs, there are several sampling techniques proposed to improve training efficiency such as Hamilton et al. (2017); Zeng et al. (2021); Chiang et al. (2019). However, these techniques cannot be directly used to train implicit GNNs without sacrificing the accuracy, since these sampling methods enforce implicit GNNs to lose long\u2013range dependency. To explain, these sampling-based mini-batch methods either sample neighbor nodes or a subgraph to form a training mini-batch. In this way, given a mini-batch, these methods inevitably prohibit information propagation between nodes of the current subgraph and nodes outside the subgraph. Therefore, directly using these methods can affect the advantage of implicit GNNs in capturing long-range dependencies. This is the reason why we cannot trivially use previous methods for implicit GNNs.\nGraph with coarse nodes To solve the above issue, inspired by graph coarsening/partitioning, we propose to use additional coarse nodes representing different partitions to facilitate longrange information propagation for implicit GNNs with minibatch training. First, we conduct graph partitioning to obtain k partitions/subgraphs Gsi ...Gsk , which can represent the coarse information on the graph. For each partition, we create a new coarse node vci to represent the partition Gsi . We construct two types of edges as follows:\n\u2022 Coarse-original edges: if a node on the original graph v \u2208 G also belongs to the i-th partition, i.e., v \u2208 Gsi , an edge e = (v, vci) connecting v and vci is constructed.\n\u2022 Coarse-coarse edges: considering two different partitions Gsi and Gsj , if there exists at least one edge e \u2208 E connecting two different nodes vi \u2208 Gsi and vj \u2208 Gsj , we construct a coarse-coarse edge connecting two corresponding coarse nodes vsi and vsj as e = (vsi , vsj ).\nWith coarse-original edges, a coarse node can act as a summary of nodes in its partitions as all original nodes in this partition are its 1-hop neighbors. This can be treated as local information\nwithin each partition. Coarse-coarse edges propagate the inter-partition information by passing the message contained in a coarse node to another coarse node, which can facilitate long-range information propagation between nodes from different partitions. In addition, coarse-coarse edges also provide coarse-level graph connectivity information as global information of the whole graph.\nMini-batch training After constructing a new graph with additional coarse nodes, we can use existing mini-batch sampling methods, such as Shadow-GNN (Zeng et al., 2021) or GraphSAGE (Hamilton et al., 2017), to construct mini-batches for training on subgraphs. To construct a mini-batch, we first randomly sample target nodes from the node set V , which means that we exclude coarse nodes from being target nodes. After that, to generate the prediction of each target node v, we choose some nodes which are more relevant or important as auxiliary nodes. As in Zeng et al. (2021), we rely on Personalized PageRank (PPR) score, a graph-structure-based metric, to determine the relevant/important nodes with respect to a target node v. Specifically, we select the nodes with top-k PPR scores as auxiliary nodes of the current target node v. Combining all target nodes and their auxiliary nodes as a node set Vs, we obtain the subgraph Gsub with the corresponding edges connecting any two nodes in this node set.\nIn a mini-batch with the subgraph Gsub, modifying the aggregation step in Eq (2), the fixed-point equation of our implicit GNNs can be re-written as:\nZ\u2217sub = \u03c6(Z \u2217 sub, X,G) = \u03b3g(W )Z\u2217subSsub + f(Xsub,Gsub), (3)\nwhere Ssub and Xsub are the adjacency matrix and node feature matrix with only nodes in Gsub."
        },
        {
            "heading": "4.2 ACCELERATE TRAINING WITH NEW SOLVERS",
            "text": "To get the equilibrium of a fixed-point equation as node representations, previous implicit GNNs usually use an original iterative solver which simply iterates the equation (Gu et al., 2020; Liu et al., 2022; Park et al., 2022). However, it requires a large number of iterations to get the equilibrium, which costs a massive time for training. In this section, we aim to reduce the training time by reducing the number of iterations with new solvers to approximate the equilibrium of the forward pass.\nNeumann solver First, we show that we can approximate the equilibrium with the Neumann series 2. As proven in Liu et al. (2021), the equilibrium of Eq (3) can be obtained as follows:\nlim l\u2192\u221e\nvec[Z(l)] = vec[Z\u2217] = (I \u2212 \u03b3[S \u2297 g(W )])\u22121 vec[f(X,G)]. (4)\nUsing the fact of the Neumann series (I \u2212 \u03b3[S \u2297 g(W )])\u22121 = \u2211\u221e\nk=0 \u03b3 k[ST \u2297 g(W )]k and a\nvectorization property, we can have:\nvec[Z\u2217] = \u221e\u2211 k=0 \u03b3k [ ST \u2297 g(W ) ]k vec[f(X,G)] = \u221e\u2211 k=0 \u03b3k vec[g(W )kf(X,G)Sk]. (5)\nThen, removing the vectorization from both sides, the equilibrium can be obtained as a form of infinite sum:\nZ\u2217 = \u221e\u2211 k=0 \u03b3kg(W )kf(X,G)Sk. (6)\nTo get the simplest approximation of the equilibrium, we can directly truncate the Neumann series at a certain step t and define V (t) as the approximation of the equilibrium:\nV (t) = t\u2211 k=0 \u03b3kg(W )kf(X,G)Sk \u2248 Z\u2217. (7)\nStochastic solver However, directly truncating the Neumann series to get the approximated equilibrium with Eq (7) will incur errors that are not going to vanish as the forward pass is biased. Therefore, we further propose a new stochastic solver to get the approximated equilibrium Z\u0302\u2217. We first truncate the series at step t as in Eq (7) and set the approximation Z\u0302 = V (t). At each following step i > t, we sample a Bernoulli random variable bi \u223c Bernoulli(\u03b1), which means P (bi = 1) = \u03b1 and P (bi = 0) = 1\u2212 \u03b1. If bi = 1, we update the current approximation Z\u0302 with an amplifying factor\n1 \u03b1(i\u2212t) as follows:\nZ\u0302(i) = Z\u0302(i\u22121) + \u03b3i 1\n\u03b1(i\u2212t) g(W )if(X,G)Si. (8)\n2Note that we omit the subscript \"sub\" in equations hereafter for better clarity since the equations are applicable for both a subgraph and a whole graph.\nAlgorithm 1: The stochastic solver\u2019s procedure for the equilibrium. Input: The subgraph G, the normalized adjacency matrix S, and the node features X . Output: The approximated equilibrium Z\u0302\u2217.\n1 V (t) = \u2211t\nk=0 \u03b3 kg(W )kf(X,G)Sk;\n2 Z\u0302(t) = V (t); 3 Define a Bernoulli distribution p\u03b1 where p\u03b1[b = 1] = \u03b1 and p\u03b1[b = 0] = 1\u2212 \u03b1; 4 i = t ; 5 Sample bi \u223c p\u03b1; 6 while bi = 1 do 7 i = i+ 1 ; 8 Z\u0302(i) = Z\u0302(i\u22121) + \u03b3i 1\n\u03b1(i\u2212t) g(W )if(X,G)Si ;\n9 Sample bi \u223c p\u03b1; 10 Z\u0302\u2217 = Z\u0302(i) ; 11 return Z\u0302\u2217;\nOtherwise if bi = 0, we cease the process and obtain the final approximation as Z\u0302\u2217 = Z\u0302(i). The procedure of our stochastic solver is illustrated in Algorithm 1.\nUsing the stochastic solver, we can obtain the unbiased approximation of the equilibrium as shown in the following proposition. Proposition 1. The proposed stochastic solver is an unbiased estimator of the equilibrium Z\u2217 of the forward pass, i.e., the expectation of the approximated equilibrium Z\u0302\u2217 is the same as that of the true equilibrium Z\u2217: E[Z\u0302\u2217] = Z\u2217 = \u2211\u221e k=0 \u03b3\nkg(W )kf(X,G)Sk, under the condition that\u2211\u221e k=t+1 \u03b3 kg(W )kf(X,G)Sk 1 \u03b1k\u2212t exists 3.\nWe provide the proof of this proposition in Appendix A.1. Our stochastic solver is an unbiased stochastic solver which can have the same error with fewer iterations in expectation compared with our Neumann solver and the original iterative solver (Gu et al., 2020) used in existing implicit GNNs."
        },
        {
            "heading": "4.3 TRAINING OF SEIGNN",
            "text": "For model training, with a subgraph in a mini-batch, we use our unbiased stochastic solver with Eq (8) to obtain the approximated equilibrium Z\u0302\u2217 for the forward pass. For backward pass, as shown in Liu et al. (2022) and Bai et al. (2019), implicit differentiation is used to compute the gradients by directly differentiating through the equilibrium as:\n\u2202\u2113\n\u2202(\u00b7) =\n\u2202\u2113\n\u2202Z\u0302\u2217\n( I \u2212 J\u03c6(Z\u0302\u2217) )\u22121 \u2202\u03c6(Z\u0302\u2217, X,G) \u2202(\u00b7) , (9)\nwhere Z\u0302\u2217 = \u03c6(Z\u0302\u2217, X,G) is the fixed-point equation and J = \u2202\u03c6(Z\u0302 \u2217,X,G)\n\u2202Z\u0302\u2217 . To avoid expensive computation of calculating ( I \u2212 J\u03c6(Z\u0302\u2217) )\u22121 , we adopt a recently proposed phantom gradient esti-\nmation (Geng et al., 2021) which has the advantages on efficient computation and stable training dynamics. Therefore, with our unbiased stochastic solver and phantom gradient estimation, we can enjoy efficient computation for both forward and backward passes."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we demonstrate the effectiveness and efficiency of SEIGNN compared with both implicit GNNs and representative traditional GNNs on large graph datasets for the node classification task. Specifically, we conduct experiments on 6 commonly used datasets for node classification (i.e., Flickr, Yelp, Reddit, PPI, ogbn-arxiv, and ogbn-products). We provide the descriptions and details of datasets in Appendix B.1.\n3We discuss the case when this condition does not hold in Appendix A.1."
        },
        {
            "heading": "5.1 NODE CLASSIFICATION",
            "text": "Comparison on popular large graphs We first use the various graph datasets with relatively large sizes (i.e., Flickr, Reddit, Yelp, PPI). Their scenarios vary from predicting communities of online posts (Reddit) to classifying protein functions (PPI).\nThe results are shown in Table 1. We can see that SEIGNN generally outperforms all other representative baselines including both implicit GNNs and traditional GNNs on these four datasets. Compared with USP (Li et al., 2023) which uses mini-batch training, SEIGNN achieves better performance by up to 1.5% absolute improvement. This can be attributed to the better ability of SEIGNN to capture global and long-range information by adding coarse nodes during training. IGNN and MGNNI, two implicit GNNs with full-batch training, generally perform worse than USP and SEIGNN, indicating that mini-batch training is more effective on large graphs in terms of performance. In addition, as MGNNI shares a similar aggregation step as SEIGNN, the worse performance of MGNNI compared with SEIGNN verifies the effectiveness of our designs of mini-batch training with the stochastic solver and coarse nodes.\nComparison on OGBN datasets Apart from four large graph datasets, to better examine scalability and effectiveness, we also conduct experiments with two popular OGBN datasets (ogbn-arxiv and ogbn-products). Specifically, ogbn-products is the largest dataset used in this paper, which contains around 2.5 million nodes and 61 million edges.\nTable 2 shows the comparison of accuracies between SEIGNN and other implicit GNNs on OGBN datasets. SEIGNN can still outperform other implicit GNNs by a large margin. Specifically, SEIGNN achieves 5.1% and 2.7% absolute accuracy improvements on ogbn-arxiv and ogbn-products respectively. In addition, we observe that MGNNI would face the out-of-memory (OOM) issue on ogbn-products as MGNNI has to load all nodes into GPU memory for full-batch training. This verifies again the limitation of using full-batch training in previous implicit GNNs.\nEfficiency Comparison In addition to evaluating prediction accuracy, we also provide experimental results regarding the training efficiency of different implicit GNN models. Table 5 demonstrates the comparison of training time per epoch among different models. We can see that our model SEIGNN generally has less training time per epoch compared with existing implicit GNNs. Especially, on\n0 2000 4000 6000 8000 10000\n0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 Va lid at io n Ac cu ra cy\nReddit, SEIGNN only needs 6.21s for an epoch, which is around 8x less compared with USP. Note that, although USP spends the least time training an epoch on Flickr which is relatively small, it requires more time compared with SEIGNN on other large datasets. This may indicate that USP is not efficient enough for relatively large and dense datasets (e.g., Reddit).\nMoreover, for a more holistic view of efficiency comparison, in Figure 2, we show how accuracy would change as training progresses for different models. First of all, our model SEIGNN achieves better eventual accuracies with less total training time compared with other implicit GNNs. Additionally, the accuracy of SEIGNN also increases much faster. These observations confirm the effectiveness and efficiency of our model on large graphs.\nBesides the efficiency comparison, we also provide the comparison of memory usage in Table 9 of Appendix B.3, showing that SEIGNN has significantly less GPU memory usage compared with other implicit GNNs. Lower memory usage leads to higher scalability of our model."
        },
        {
            "heading": "5.2 ABLATION STUDY AND FURTHER EXPERIMENTAL INVESTIGATION",
            "text": "Besides the overall performance and efficiency comparison in the above section, we also conduct detailed ablation studies and further investigations about the effectiveness of two key components in SEIGNN (i.e., mini-batch training with coarse nodes and the unbiased stochastic solver).\nEffectiveness of using coarse nodes and the proposed solver Table 4 demonstrates the results of removing coarse nodes in our designed mini-batch method and replacing the unbiased stochastic solver with the naive Neumann solver. It shows that, without adding coarse nodes in mini-batch training, the accuracies drop significantly, especially on Yelp and ogbn-arxiv. This confirms that adding coarse nodes is helpful for better global information propagation. By replacing our unbiased stochastic solver with the naive Neumann solver, the performance also slightly decreases, which indicates the effectiveness of the unbiased stochastic solver.\nIneffectiveness of directly applying existing mini-batch methods In Section 1 and 4.1, we explain the reason that directly applying existing mini-batch methods for implicit GNNs may affect the performance. Table 5 empirically verifies the ineffectiveness of using existing mini-batch methods by illustrating that trivially using those methods (i.e., ClusterGCN (Chiang et al., 2019) and GraphSAGE with neighbor sampling (Hamilton et al., 2017)) for implicit GNNs provides much worse performances compared to our mini-batch training method with coarse nodes.\nThe improvements are higher for low-degree nodes. As the significant improvement by using coarse nodes is shown in Table 4, we try to provide more insights by further investigating how coarse nodes may specifically improve the performance. Considering nodes with different levels of degrees, we evenly split nodes into 5 groups according to their degrees (1st group contains nodes with the highest level of degrees while the 5th group contains the lowest-degree nodes). Figure 3a shows the average accuracies of different degree groups by comparing the variants using/not using coarse nodes, and Figure 3b demonstrates the relative improvement of degree groups through adding coarse nodes. We can see that 1) nodes with lower degrees tend to have low accuracies for both two variants, and 2) accuracy improvements on nodes with lower degrees are more obvious compared with nodes with higher degrees. These observations suggest that our proposed mini-batch training with coarse nodes is more helpful on nodes with lower degrees. The reason might be that, by enhancing global/long-range information propagation via coarse nodes, low-degree nodes can receive sufficient information compared with the variant not adding coarse nodes.\nAdditionally, we also investigate the compatibility of using coarse nodes for applying other sampling-based mini-batch methods to SEIGNN. Table 6 shows that, on ogbn-arxiv, adding coarse nodes can also improve the performance of two existing mini-batch methods (i.e., ClusterGCN and GraphSAGE with neighbor sampling).\nEfficiency comparison for different solvers Moreover, to verify the effectiveness of our stochastic solver, in Table 7, we provide an experimental comparison of accuracy and total time between our solver and the original solver used in (Liu et al., 2022; Gu et al., 2020). We use different maximum iterations for the original solvers and set maximum iterations as 3 for our solver with the continue probability \u03b1 = 0.5. The results show that our solver can generally achieve better accuracy compared with the original solver while spending much less time. We also observe that the original solver needs more iterations (i.e., 50) to achieve a comparable accuracy as our solver, which leads to excessive time consumption."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we propose a scalable and effective implicit GNN model (SEIGNN) that can be efficiently trained on large graphs. Specifically, SEIGNN contains a mini-batch training method with added coarse nodes and an unbiased stochastic solver to scale up the model to large graphs without losing the ability to capture long-range information. The experiments on several large-graph datasets demonstrate that SEIGNN can achieve superior performance using less training time compared with existing implicit GNNs. Furthermore, the results of ablation studies verify the effectiveness of our mini-batch training method and the unbiased stochastic solver. We also try to provide a deeper analysis of why using coarse nodes in our mini-batch training can improve performance."
        },
        {
            "heading": "A PROOFS",
            "text": "A.1 PROOF OF PROPOSITION 1\nProof. We first define Si \u2208 {0, 1} as a random variable, indicating whether we sample the i-th term or not. By the property of probability, we can decompose the marginal P[St = 1] by\nP(Si = 1) = P[Si = 1|Si\u22121 = 1] \u00b7 P[Si\u22121 = 1] + P[Si = 1|Si\u22121 = 0] \u00b7 P[Si\u22121 = 0].\nHere, P[Si = 1|Si\u22121 = 0] = 0 in our algorithm because we terminate the iteration if Si\u22121 = 0. Thus,\nP[Si = 1] = P[Si = 1|Si\u22121 = 1] \u00b7 P[Si\u22121 = 1].\nMoreover, as the random variable Si is sampled from a Bernoulli distribution Bernoulli(\u03b1), we have P[Si = 1|Si\u22121 = 1] = \u03b1 in our algorithm. Plugging this into the above,\nP[Si = 1] = \u03b1P[Si\u22121 = 1].\nRecursively applying this equation,\nP[Si = 1] = \u03b1i\u2212tP[St = 1].\nSince P[Si = 1] = 1 for all i \u2264 t in our algorithm, we have\nP[Si = 1] = \u03b1i\u2212t.\nConsidering the possibility of reaching step i, the forward pass of our algorithm can be written as:\nZ\u0302(i) = Z\u0302(t) + i\u2211 k=t+1 \u03b3k 1{Sk = 1} \u03b1(k\u2212t) g(W )kf(X,G)Sk,\nwhere 1{Sk = 1} take the value 1 when Sk = 1 (i.e., the algorithm reaches step k), otherwise it takes the value 0.\nAs we continue the algorithm until the termination (i.e., Si = 0), we can write the approximated equilibrium as follows:\nZ\u0302\u2217 = Z\u0302(t) + \u221e\u2211 k=t+1 \u03b3k 1{Sk = 1} \u03b1(k\u2212t) g(W )kXSk.\nBy taking the expectation of the second term, with the definition Ai = \u03b3ig(W )if(X,G)Si and the condition that \u2211\u221e k=t+1 Ak 1 \u03b1k\u2212t exists, we have the following:\nE [ \u221e\u2211 k=t+1 \u03b3k 1{Sk = 1} \u03b1(k\u2212t) g(W )kXSk ] = E [ \u221e\u2211 k=t+1 Ak 1{Sk = 1} \u03b1k\u2212t ]\n= \u221e\u2211 k=t+1 AkE [ 1{Sk = 1} \u03b1k\u2212t ]\n= \u221e\u2211 k=t+1 Ak P[Sk = 1] \u03b1k\u2212t\n= \u221e\u2211 k=t+1 Ak = \u221e\u2211 k=t+1 \u03b3kg(W )kf(X,G)Sk,\nCombining the above expectation of the second term with the deterministic term Z\u0302(t) =\u2211t i=0 \u03b3g(W ) kf(X,G)Sk, we get obtain the expectation E[Z\u0302\u2217] as follows:\nE[Z\u0302\u2217] = Z\u0302(t) + E [ \u221e\u2211 k=t+1 \u03b3k 1{Sk = 1} \u03b1(k\u2212t) g(W )kXSk ]\n= t\u2211 k=0 \u03b3kg(W )kf(X,G)Sk + \u221e\u2211 k=t+1 \u03b3kg(W )kf(X,G)Sk\n= \u221e\u2211 k=0 \u03b3kg(W )kf(X,G)Sk = Z\u2217.\nThis indicates that our proposed stochastic solver is an unbiased estimator of the equilibrium Z\u2217. In the above proof, we have an assumption that \u2211\u221e\nk=t+1 Ak 1 \u03b1k\u2212t exists. In the case where\u2211\u221e\nk=t+1 Ak 1 \u03b1k\u2212t does not exist, we define fn to be the output of a modified version of the Algorithm 1 where we replace the while-loop with the for-loop up to n step: i.e., we forceful terminate the while-loop if it takes more than n steps. Then, by using the same proof steps except that we replace the infinite sum with the finite sum upto n terms, we conclude that for any n, E[fn] = \u2211n k=0 Ak. This implies that our proposed stochastic solver is still an unbiased estimator of the equilibrium Z\u2217 of the forward pass up to the error \u2211\u221e k=n Ak. For any desired error value \u03f5 > 0 (including\nmachine precision), there exists a sufficiently large n such that \u2211\u221e\nk=n Ak \u2264 \u03f5. Thus, the statement in Proposition 1 still holds true up to the machine precision without the condition that \u2211\u221e k=t+1 Ak 1 \u03b1k\u2212t exists. The output of our algorithm is equivalent to fn with n = \u221e. Thus, it is ensured to be unbiased."
        },
        {
            "heading": "B MORE ON EXPERIMENTS",
            "text": "B.1 DATASET STATISTICS AND DESCRIPTIONS\nThe dataset statistics are provided in Table 8. ogbn-products is the largest dataset used in our paper, which contains around 25 million nodes and 61 million edges. Reddit is the densest dataset here with the average node degrees as 50. We follow Li et al. (2023) to use six datasets in our experiments. We provide a detailed description of each dataset as follows:\n\u2022 Flickr is a single-label multi-class classification dataset. The task is to categorize types of images based on the descriptions and common properties of online images. We are using the Flickr dataset as provided in Zeng et al. (2020). Flickr data are collected in the SNAP website 4 from different sources. Flickr contains an undirected graph and a node in the graph represents an image on Flickr. An edge is connected between two nodes if two corresponding images share some common properties (e.g., the same gallery, comments from the same user, etc.). The node features are the 500-dimensional bag-of-word representations of the images. For labels, each image belongs to one of the 7 classes.\n\u2022 Reddit is a single-label multi-class classification dataset. The task is to predict different communities of online posts. We use the Reddit dataset from Hamilton et al. (2017) as in Li et al. (2023). The nodes are online posts and an edge is connected between two posts if the same user comments on both. Word features in posts are 300-dimensional word vectors. Node features are concatenated using 1) the average embedding of the post title, 2) the average embedding of all the post\u2019s comments, 3) the post\u2019s score, 4) the number of comments on the post.\n\u2022 Yelp is a multi-label multi-class classification dataset. The task is to categorize types of businesses based on users and friendships. We use the Yelp dataset provided in Zeng et al. (2020). Yelp contains a single graph. The nodes are users who provide reviews. If two users are friends, an edge between them is connected. The features of each node are added and normalized using several 300-dimensional vectors representing a review word provided by the user.\n4https://snap.stanford.edu/data/web-flickr.html\n\u2022 PPI is a single-label multi-class classification dataset that contains multiple graphs. The task is to classify protein functions based on the interactions of human tissues. PPI dataset has 24 graphs in total and each graph represents a different human tissue. In a graph, nodes represent proteins and edges indicate interactions between proteins. Each node can have up to 121 labels, which are originally collected from Molecular Signatures dataset (Subramanian et al., 2005) by Hamilton et al. (2017). The dataset splits used in our paper are the same as in Hamilton et al. (2017), i.e., 20 graphs for training, 2 graphs for validation, and 2 graphs for testing.\n\u2022 ogbn-arxiv is a single-label classification dataset that contains a directed graph. The task is to predict the 40 subject areas of arXiv CS papers, such as cs.AI, cs.LG, and cs.OS, which are manually labeled by the authors of the paper and the moderators. each node is an arXiv paper and each directed edge indicates that one paper cites another one. The node feature of each node is a 128-dimensional vector obtained by averaging the embeddings of words in the title and abstract. We download ogbn-arxiv dataset from the OGB website 5. The detailed descriptions can be found in Hu et al. (2020).\n\u2022 ogbn-products is a single-label multi-class classification dataset which contains an undirected and unweighted graph. Nodes represent products sold on Amazon, and edges between two products indicate that these two products are purchased together. Node features are generated by extracting bag-of-words features from the description of the product followed by a Principal Component Analysis to reduce the dimension to 100. We download ogbn-products dataset from the OGB website 5. The detailed descriptions can be found in Hu et al. (2020).\nB.2 EXPERIMENTAL SETTING\nFor experimental setup, we mainly follow Li et al. (2023). As we use the same experimental setting on some datasets, we reuse the results of some baselines from Li et al. (2023) and Zeng et al. (2020). We compare SEIGNN with 3 implicit GNNs (i.e., USP (Li et al., 2023), MGNNI (Liu et al., 2022), and IGNN (Gu et al., 2020)) and 6 explicit/traditional GNNs (GCN (Kipf and Welling, 2016), GraphSAGE (Hamilton et al., 2017), FastGCN (Chen et al., 2018), ASGCN (Huang et al., 2018), ClusterGCN (Chiang et al., 2019), and GraphSAINT (Zeng et al., 2020)). The experiments are run with 5 different trials. The averaged accuracy and standard deviation are reported. We mainly run the experiments on an RTX-A5000 GPU with 24GB GPU memory.\nModel Architecture and Hyperparameters For SEIGNN, we use the same structure with a few implicit graph layers and the same number of linear layers as in MGNNI (Liu et al., 2022) and USP (Li et al., 2023). We select the number of implicit graph layers from {2, 3, 4}. We also conduct a hyperparameter search on learning rate {0.01, 0.005, 0.001} and dropout rate {0.0, 0.2, 0.5}. The number of deterministic steps t in our stochastic solver is chosen from {3, 5} and the continuation probability \u03b1 is set to 0.5. The hyperparameter \u03b3 used in an implicit graph layer is set to 0.8. The Adam optimizer (Kingma and Ba, 2015) is used for optimization. The number of partitions for adding coarse nodes in our mini-batch training method is selected from {50, 100, 200}. The number of target nodes in a mini-batch is configured as follows: 8192 for Flickr and PPI, 10240 for ogbn-arxiv, Yelp, and Reddit, and 16384 for ogbn-products.\n5https://ogb.stanford.edu/docs/nodeprop\nB.3 ADDITIONAL EXPERIMENTAL RESULTS\nBesides the overall comparison of accuracy and efficiency, we also investigate the memory usage of different implicit GNNs. Table 9 shows that SEIGNN requires significantly less GPU memory compared with IGNN and MGNNI. In particular, SEIGNN only uses 37% of the memory as IGNN with 3 implicit layers on Reddit. Moreover, we can see that SEIGNN cost less than 10GB GPU memory on ogbn-products while MGNNI and IGNN face the out-of-memory issue using a GPU with 24GB memory."
        }
    ],
    "year": 2023
}