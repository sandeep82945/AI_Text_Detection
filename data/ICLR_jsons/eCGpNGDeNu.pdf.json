{
    "abstractText": "There has been a recent surge of interest in developing generally-capable agents that can adapt to new tasks without additional training in the environment. Learning world models from reward-free exploration is a promising approach, and enables policies to be trained using imagined experience for new tasks. However, achieving a general agent requires robustness across different environments. In this work, we address the novel problem of generating curricula in the reward-free setting to train robust world models. We consider robustness in terms of minimax regret over all environment instantiations and show that the minimax regret can be connected to minimising the maximum error in the world model across environment instances. This result informs our algorithm, WAKER: Weighted Acquisition of Knowledge across Environments for Robustness. WAKER selects environments for data collection based on the estimated error of the world model for each environment. Our experiments demonstrate that WAKER outperforms several baselines, resulting in improved robustness, efficiency, and generalisation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Marc Rigter"
        },
        {
            "affiliations": [],
            "name": "Minqi Jiang"
        }
    ],
    "id": "SP:0c5b3023f6338a22c102014b208252e60f8c8927",
    "references": [
        {
            "authors": [
                "Rishabh Agarwal",
                "Max Schwarzer",
                "Pablo Samuel Castro",
                "Aaron C Courville",
                "Marc Bellemare"
            ],
            "title": "Deep reinforcement learning at the edge of the statistical precipice",
            "venue": "Advances in Neural Information Process Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ilge Akkaya",
                "Marcin Andrychowicz",
                "Maciek Chociej",
                "Mateusz Litwin",
                "Bob McGrew",
                "Arthur Petron",
                "Alex Paino",
                "Matthias Plappert",
                "Glenn Powell",
                "Raphael Ribas"
            ],
            "title": "Solving rubik\u2019s cube with a robot hand",
            "year": 1910
        },
        {
            "authors": [
                "Minoru Asada",
                "Shoichi Noda",
                "Sukoya Tawaratsumida",
                "Koh Hosoda"
            ],
            "title": "Purposive behavior acquisition for a real robot by vision-based reinforcement learning",
            "venue": "Machine learning,",
            "year": 1996
        },
        {
            "authors": [
                "Philip Ball",
                "Jack Parker-Holder",
                "Aldo Pacchiano",
                "Krzysztof Choromanski",
                "Stephen Roberts"
            ],
            "title": "Ready policy one: World building through active learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Marc Bellemare",
                "Sriram Srinivasan",
                "Georg Ostrovski",
                "Tom Schaul",
                "David Saxton",
                "Remi Munos"
            ],
            "title": "Unifying count-based exploration and intrinsic motivation",
            "venue": "Advances in Neural Information Process Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Homanga Bharadhwaj",
                "Mohammad Babaeizadeh",
                "Dumitru Erhan",
                "Sergey Levine"
            ],
            "title": "Information prioritization through empowerment in visual model-based RL",
            "venue": "International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Anthony Brohan",
                "Yevgen Chebotar",
                "Chelsea Finn",
                "Karol Hausman",
                "Alexander Herzog",
                "Daniel Ho",
                "Julian Ibarz",
                "Alex Irpan",
                "Eric Jang",
                "Ryan Julian"
            ],
            "title": "Do as I can, not as I say: Grounding language in robotic affordances",
            "venue": "In Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoyu Chen",
                "Jiachen Hu",
                "Chi Jin",
                "Lihong Li",
                "Liwei Wang"
            ],
            "title": "Understanding domain randomization for sim-to-real transfer",
            "venue": "International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "David A Cohn",
                "Zoubin Ghahramani",
                "Michael I Jordan"
            ],
            "title": "Active learning with statistical models",
            "venue": "Journal of artificial intelligence research,",
            "year": 1996
        },
        {
            "authors": [
                "Edward L Deci",
                "Richard M Ryan"
            ],
            "title": "Intrinsic motivation and self-determination in human",
            "venue": "Science & Business Media,",
            "year": 1985
        },
        {
            "authors": [
                "Michael Dennis",
                "Natasha Jaques",
                "Eugene Vinitsky",
                "Alexandre Bayen",
                "Stuart Russell",
                "Andrew Critch",
                "Sergey Levine"
            ],
            "title": "Emergent complexity and zero-shot transfer via unsupervised environment design",
            "venue": "Advances in Neural Information Process Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Theresa Eimer",
                "Andr\u00e9 Biedenkapp",
                "Frank Hutter",
                "Marius Lindauer"
            ],
            "title": "Self-paced context evaluation for contextual reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Carlos Florensa",
                "David Held",
                "Markus Wulfmeier",
                "Michael Zhang",
                "Pieter Abbeel"
            ],
            "title": "Reverse curriculum generation for reinforcement learning",
            "venue": "In Conference on Robot Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Carlos Florensa",
                "David Held",
                "Xinyang Geng",
                "Pieter Abbeel"
            ],
            "title": "Automatic goal generation for reinforcement learning agents",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Alex Graves",
                "Marc G Bellemare",
                "Jacob Menick",
                "Remi Munos",
                "Koray Kavukcuoglu"
            ],
            "title": "Automated curriculum learning for neural networks",
            "venue": "In international conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "David Ha",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Recurrent world models facilitate policy evolution",
            "venue": "Advances in Neural Information Process Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Nick Haber",
                "Damian Mrowca",
                "Stephanie Wang",
                "Li F Fei-Fei",
                "Daniel L Yamins"
            ],
            "title": "Learning to play with intrinsically-motivated, self-aware agents",
            "venue": "Advances in Neural Information Process Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Danijar Hafner",
                "Timothy Lillicrap",
                "Ian Fischer",
                "Ruben Villegas",
                "David Ha",
                "Honglak Lee",
                "James Davidson"
            ],
            "title": "Learning latent dynamics for planning from pixels",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Danijar Hafner",
                "Timothy Lillicrap",
                "Jimmy Ba",
                "Mohammad Norouzi"
            ],
            "title": "Dream to control: Learning behaviors by latent imagination",
            "venue": "International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Danijar Hafner",
                "Timothy Lillicrap",
                "Mohammad Norouzi",
                "Jimmy Ba"
            ],
            "title": "Mastering Atari with discrete world models",
            "venue": "International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Nick Jakobi"
            ],
            "title": "Evolutionary robotics and the radical envelope-of-noise hypothesis",
            "venue": "Adaptive behavior,",
            "year": 1997
        },
        {
            "authors": [
                "Michael Janner",
                "Justin Fu",
                "Marvin Zhang",
                "Sergey Levine"
            ],
            "title": "When to trust your model: Modelbased policy optimization",
            "venue": "Advances in Neural Information Process Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Minqi Jiang",
                "Michael Dennis",
                "Jack Parker-Holder",
                "Jakob Foerster",
                "Edward Grefenstette",
                "Tim Rockt\u00e4schel"
            ],
            "title": "Replay-guided adversarial environment design",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Minqi Jiang",
                "Edward Grefenstette",
                "Tim Rockt\u00e4schel"
            ],
            "title": "Prioritized level replay",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Minqi Jiang",
                "Michael D Dennis",
                "Jack Parker-Holder",
                "Andrei Lupu",
                "Heinrich Kuttler",
                "Edward Grefenstette",
                "Tim Rockt\u00e4schel",
                "Jakob Nicolaus Foerster"
            ],
            "title": "Grounding aleatoric uncertainty for unsupervised environment design",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Minqi Jiang",
                "Tim Rockt\u00e4schel",
                "Edward Grefenstette"
            ],
            "title": "General intelligence requires rethinking exploration",
            "venue": "arXiv preprint arXiv:2211.07819,",
            "year": 2022
        },
        {
            "authors": [
                "Leslie Pack Kaelbling",
                "Michael L Littman",
                "Anthony R Cassandra"
            ],
            "title": "Planning and acting in partially observable stochastic domains",
            "venue": "Artificial intelligence,",
            "year": 1998
        },
        {
            "authors": [
                "Maximilian Karl",
                "Maximilian Soelch",
                "Justin Bayer",
                "Patrick Van der Smagt"
            ],
            "title": "Deep variational bayes filters: Unsupervised learning of state space models from raw data",
            "venue": "International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Michael Kearns",
                "Satinder Singh"
            ],
            "title": "Near-optimal reinforcement learning in polynomial time",
            "venue": "Machine learning,",
            "year": 2002
        },
        {
            "authors": [
                "Rahul Kidambi",
                "Aravind Rajeswaran",
                "Praneeth Netrapalli",
                "Thorsten Joachims"
            ],
            "title": "MOREL: Modelbased offline reinforcement learning",
            "venue": "Advances in Neural Information Process Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Alexander S Klyubin",
                "Daniel Polani",
                "Chrystopher L Nehaniv"
            ],
            "title": "Empowerment: A universal agent-centric measure of control",
            "venue": "In IEEE Congress on Evolutionary Computation,",
            "year": 2005
        },
        {
            "authors": [
                "Michael Laskin",
                "Denis Yarats",
                "Hao Liu",
                "Kimin Lee",
                "Albert Zhan",
                "Kevin Lu",
                "Catherine Cang",
                "Lerrel Pinto",
                "Pieter Abbeel"
            ],
            "title": "URLB: Unsupervised reinforcement learning benchmark",
            "venue": "arXiv preprint arXiv:2110.15191,",
            "year": 2021
        },
        {
            "authors": [
                "Cong Lu",
                "Philip J Ball",
                "Jack Parker-Holder",
                "Michael A Osborne",
                "Stephen J Roberts"
            ],
            "title": "Revisiting design choices in offline model-based reinforcement learning",
            "venue": "International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Tambet Matiisen",
                "Avital Oliver",
                "Taco Cohen",
                "John Schulman"
            ],
            "title": "Teacher\u2013student curriculum learning",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Bhairav Mehta",
                "Manfred Diaz",
                "Florian Golemo",
                "Christopher J Pal",
                "Liam Paull"
            ],
            "title": "Active domain randomization",
            "venue": "In Conference on Robot Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Russell Mendonca",
                "Oleh Rybkin",
                "Kostas Daniilidis",
                "Danijar Hafner",
                "Deepak Pathak"
            ],
            "title": "Discovering and achieving goals via world models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Sanmit Narvekar",
                "Bei Peng",
                "Matteo Leonetti",
                "Jivko Sinapov",
                "Matthew E Taylor",
                "Peter Stone"
            ],
            "title": "Curriculum learning for reinforcement learning domains: A framework and survey",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Georg Ostrovski",
                "Marc G Bellemare",
                "A\u00e4ron Oord",
                "R\u00e9mi Munos"
            ],
            "title": "Count-based exploration with neural density models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Jack Parker-Holder",
                "Minqi Jiang",
                "Michael Dennis",
                "Mikayel Samvelyan",
                "Jakob Foerster",
                "Edward Grefenstette",
                "Tim Rockt\u00e4schel"
            ],
            "title": "Evolving curricula with regret-based environment design",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Deepak Pathak",
                "Pulkit Agrawal",
                "Alexei A Efros",
                "Trevor Darrell"
            ],
            "title": "Curiosity-driven exploration by self-supervised prediction",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Ken Perlin"
            ],
            "title": "Improving noise",
            "venue": "Proceedings of the Annual Conference on Computer Graphics and Interactive Techniques,",
            "year": 2002
        },
        {
            "authors": [
                "R\u00e9my Portelas",
                "C\u00e9dric Colas",
                "Katja Hofmann",
                "Pierre-Yves Oudeyer"
            ],
            "title": "Teacher algorithms for curriculum learning of deep RL in continuously parameterized environments",
            "venue": "In Conference on Robot Learning,",
            "year": 2020
        },
        {
            "authors": [
                "R\u00e9my Portelas",
                "C\u00e9dric Colas",
                "Lilian Weng",
                "Katja Hofmann",
                "Pierre-yves Oudeyer"
            ],
            "title": "Automatic curriculum learning for deep rl: A short survey",
            "venue": "In IJCAI 2020-International Joint Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Martin L Puterman"
            ],
            "title": "Markov decision processes: Discrete stochastic dynamic programming",
            "year": 2014
        },
        {
            "authors": [
                "Sai Rajeswar",
                "Pietro Mazzaglia",
                "Tim Verbelen",
                "Alexandre Pich\u00e9",
                "Bart Dhoedt",
                "Aaron Courville",
                "Alexandre Lacoste"
            ],
            "title": "Mastering the unsupervised reinforcement learning benchmark from pixels",
            "venue": "International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Alex Ray",
                "Joshua Achiam",
                "Dario Amodei"
            ],
            "title": "Benchmarking safe exploration in deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1910.01708,",
            "year": 1910
        },
        {
            "authors": [
                "Scott Reed",
                "Konrad Zolna",
                "Emilio Parisotto",
                "Sergio Gomez Colmenarejo",
                "Alexander Novikov",
                "Gabriel Barth-Maron",
                "Mai Gimenez",
                "Yury Sulsky",
                "Jackie Kay",
                "Jost Tobias Springenberg"
            ],
            "title": "A generalist agent",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Marc Rigter",
                "Bruno Lacerda",
                "Nick Hawes"
            ],
            "title": "Minimax regret optimisation for robust planning in uncertain Markov decision processes",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Marc Rigter",
                "Bruno Lacerda",
                "Nick Hawes"
            ],
            "title": "RAMBO-RL: Robust adversarial model-based offline reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Marc Rigter",
                "Bruno Lacerda",
                "Nick Hawes"
            ],
            "title": "One risk to rule them all: Addressing distributional shift in offline reinforcement learning via risk-aversion",
            "venue": "arXiv preprint arXiv:2212.00124,",
            "year": 2023
        },
        {
            "authors": [
                "R Tyrrell Rockafellar",
                "Stanislav Uryasev"
            ],
            "title": "Optimization of conditional value-at-risk",
            "venue": "Journal of risk,",
            "year": 2000
        },
        {
            "authors": [
                "Leonard J Savage"
            ],
            "title": "The theory of statistical decision",
            "venue": "Journal of the American Statistical Association,",
            "year": 1951
        },
        {
            "authors": [
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Reinforcement learning in Markovian and non-Markovian environments",
            "venue": "Advances in Neural Information Process Systems,",
            "year": 1990
        },
        {
            "authors": [
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "A possibility for implementing curiosity and boredom in model-building neural controllers",
            "venue": "In Proceedings of the International Conference on Simulation of Adaptive Behavior,",
            "year": 1991
        },
        {
            "authors": [
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010)",
            "venue": "IEEE Transactions on Autonomous Mental Development,",
            "year": 2010
        },
        {
            "authors": [
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Thomas Hubert",
                "Karen Simonyan",
                "Laurent Sifre",
                "Simon Schmitt",
                "Arthur Guez",
                "Edward Lockhart",
                "Demis Hassabis",
                "Thore Graepel"
            ],
            "title": "Mastering Atari, Go, chess and shogi by planning with a learned model",
            "year": 2020
        },
        {
            "authors": [
                "Ramanan Sekar",
                "Oleh Rybkin",
                "Kostas Daniilidis",
                "Pieter Abbeel",
                "Danijar Hafner",
                "Deepak Pathak"
            ],
            "title": "Planning to explore via self-supervised world models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Shyam",
                "Wojciech Ja\u015bkowski",
                "Faustino Gomez"
            ],
            "title": "Model-based active exploration",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "David Silver",
                "Aja Huang",
                "Chris J Maddison",
                "Arthur Guez",
                "Laurent Sifre",
                "George Van Den Driessche",
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Veda Panneershelvam",
                "Marc Lanctot"
            ],
            "title": "Mastering the game of go with deep neural networks and tree",
            "venue": "search. Nature,",
            "year": 2016
        },
        {
            "authors": [
                "Adam Stooke",
                "Anuj Mahajan",
                "Catarina Barros",
                "Charlie Deck",
                "Jakob Bauer",
                "Jakub Sygnowski",
                "Maja Trebacz",
                "Max Jaderberg",
                "Michael Mathieu"
            ],
            "title": "Open-ended learning leads to generally capable agents",
            "venue": "arXiv preprint arXiv:2107.12808,",
            "year": 2021
        },
        {
            "authors": [
                "Richard S Sutton"
            ],
            "title": "Dyna, an integrated architecture for learning, planning, and reacting",
            "venue": "ACM Sigart Bulletin,",
            "year": 1991
        },
        {
            "authors": [
                "Josh Tobin",
                "Rachel Fong",
                "Alex Ray",
                "Jonas Schneider",
                "Wojciech Zaremba",
                "Pieter Abbeel"
            ],
            "title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Igor Babuschkin",
                "Wojciech M Czarnecki",
                "Micha\u00ebl Mathieu",
                "Andrew Dudzik",
                "Junyoung Chung",
                "David H Choi",
                "Richard Powell",
                "Timo Ewalds",
                "Petko Georgiev"
            ],
            "title": "Grandmaster level in starcraft II using multi-agent reinforcement learning",
            "year": 2019
        },
        {
            "authors": [
                "Rui Wang",
                "Joel Lehman",
                "Jeff Clune",
                "Kenneth O Stanley"
            ],
            "title": "Paired open-ended trailblazer (POET): Endlessly generating increasingly complex and diverse learning environments and their solutions",
            "year": 1901
        },
        {
            "authors": [
                "Yingchen Xu",
                "Jack Parker-Holder",
                "Aldo Pacchiano",
                "Philip Ball",
                "Oleh Rybkin",
                "S Roberts",
                "Tim Rockt\u00e4schel",
                "Edward Grefenstette"
            ],
            "title": "Learning general world models in a handful of reward-free deployments",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tianhe Yu",
                "Garrett Thomas",
                "Lantao Yu",
                "Stefano Ermon",
                "James Y Zou",
                "Sergey Levine",
                "Chelsea Finn",
                "Tengyu Ma"
            ],
            "title": "MOPO: Model-based offline policy optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "2020 Ball et al",
                "2021 Mendonca et al",
                "Xu"
            ],
            "title": "Unlike prior methods, WAKER augments action-based exploration by directly exploring over the space of environments via an autocurriculum. To our knowledge, this is the first work to address curricula for reward-free training of a world model. WAKER can thus be viewed as an automatic curriculum learning (ACL) (Graves et al., 2017",
            "venue": "(i.e. in imagination) (Sekar et al.,",
            "year": 2022
        },
        {
            "authors": [
                "las et al",
                "Narvekar"
            ],
            "title": "2020) method for training a world model. ACL methods optimise the order in which tasks or environments are presented during training, to improve the agent\u2019s final performance",
            "venue": "(Asada et al.,",
            "year": 1996
        },
        {
            "authors": [
                "2020 Portelas et al",
                "2021 Eimer et al",
                "2020 Dennis et al",
                "Jiang"
            ],
            "title": "2021a; Matiisen et al., 2019",
            "venue": "Active DR (Mehta et al.,",
            "year": 2020
        },
        {
            "authors": [
                "Sekar et al",
                "Shyam"
            ],
            "title": "2019), b) can be maximally influenced (Bharadhwaj et al., 2022",
            "venue": "Klyubin et al.,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep reinforcement learning (RL) has been successful on a number of challenging domains, such as Go (Silver et al., 2016), and Starcraft (Vinyals et al., 2019). While these domains are difficult, they are narrow in the sense that they require solving a single pre-specified task. More recently, there has been a surge of interest in developing generally-capable agents that can master many tasks and quickly adapt to new tasks without any additional training in the environment (Brohan et al., 2022; Mendonca et al., 2021; Reed et al., 2022; Sekar et al., 2020; Stooke et al., 2021; Jiang et al., 2022b).\nMotivated by the goal of developing generalist agents that are not specialised for a single task, we consider the reward-free setting. In this setting, the agent first accumulates useful information about the environment in an initial, reward-free exploration phase. Afterwards, the agent is presented with specific tasks corresponding to arbitrary reward functions, and must quickly adapt to these tasks by utilising the information previously acquired during the reward-free exploration phase. By separating the learning of useful environment representations into an initial pre-training phase, reward-free learning provides a powerful strategy for data-efficient RL.\nA promising line of work in the reward-free setting involves learning world models (Ha & Schmidhuber, 2018), a form of model-based RL (Sutton, 1991), where the agent learns a predictive model of the environment. In the reward-free setting, the world model is trained without access to a reward function, and instead, is trained using data collected by a suitable exploration policy (Sekar et al., 2020). Once a world model has been trained for an environment, it is possible to train policies entirely within the world model (i.e. \u201cin imagination\u201d) for new tasks corresponding to specific reward functions within that environment (Sekar et al., 2020; Xu et al., 2022; Rajeswar et al., 2023).\nHowever, to realise the vision of a general agent, it is not only necessary for the agent to be able to learn multiple tasks in a single environment: the agent must also be robust to different environments. To enable this, one approach is to apply domain randomisation (DR) (Tobin et al., 2017) to sample different environments uniformly at random to gather a more diverse dataset. However, the amount of data required to learn a suitable world model might vary by environment. Unsupervised Environment Design (UED) (Dennis et al., 2020) aims to generate curricula that present the optimal environments to the agent at each point of training, with the goal of maximising the robustness of\nthe final agent across a wide range of environments. However, existing UED approaches require a task-specific reward function during exploration (Eimer et al., 2021; Matiisen et al., 2019; Portelas et al., 2020; Dennis et al., 2020; Jiang et al., 2021a; Mehta et al., 2020; Parker-Holder et al., 2022; Wang et al., 2019), and therefore cannot be applied in the reward-free setting that we consider. In this work we address the novel problem of generating curricula for training robust agents without access to a reward function during exploration. To distil the knowledge obtained during reward-free exploration, we aim to learn a world model that is robust to downstream tasks and environments.\nWe first analyse the problem of learning a robust world model in the reward-free setting. We then operationalise these insights in the form of novel algorithms for robust, reward-free world model learning. Inspired by past works on UED with known reward functions (Dennis et al., 2020; Jiang et al., 2021a; Parker-Holder et al., 2022), we consider robustness in terms of minimax regret (Savage, 1951). To our knowledge, WAKER is the first work to address automatic curriculum learning for environment selection without access to a reward function. We make the following contributions: a) We formally define the problem of learning a robust world model in the reward-free setting, in terms of minimax regret optimality, b) We extend existing theoretical results for MDPs to prove that this problem is equivalent to minimising the maximum expected error of the world model across all environments under a suitable exploration policy, and finally c) We introduce WAKER, an algorithm for actively sampling environments for exploration during reward-free training based on the estimated error of the world model in each environment. We introduce pixel-based continuous control domains for benchmarking generalisation in the reward-free setting. We evaluate WAKER on these domains, by pairing it with both an instrinsically-motivated exploration policy and a random exploration policy. Our results show that WAKER outperforms several baselines, producing more performant and robust policies that generalise better to out-of-distribution (OOD) environments."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "A reward-free Markov Decision Process (MDP) is defined byM = {S,A, T}, where S is the set of states and A is the set of actions. T : S\u00d7A\u2192 \u2206(S) is the transition function, where \u2206(S) denotes the set of possible distributions over S. For some reward function, R : S\u00d7A\u2192 [0, 1], we writeMR to denote the corresponding (standard) MDP (Puterman, 2014) with reward function R and discount factor \u03b3. A reward-free Partially Observable Markov Decision Process (POMDP) (Kaelbling et al., 1998) is defined by P = {S,A,O, T, I}, where O is the set of observations, and I : S \u2192 \u2206(O) is the observation function. A history is a sequence of observations and actions, h = o0, a0, . . . , ot, at, where oi \u2208 O and ai \u2208 A. We use H to denote the set of all possible histories. Analogous to the MDP case, PR denotes a POMDP with reward function R and discount factor \u03b3. We assume that there are many possible instantiations of the environment. To model an underspecified environment, we consider a reward-free Underspecified POMDP (UPOMDP): U = {S,A,O, T\u0398, I,\u0398} (Dennis et al., 2020). In contrast to a POMDP, the UPOMDP additionally includes a set of free parameters of the environment, \u0398. Furthermore, the transition function depends on the setting of the environment parameters, i.e. T\u0398 : S\u00d7A\u00d7\u0398\u2192 \u2206(S). For each episode, the environment parameters are set to a specific value \u03b8 \u2208 \u0398. Therefore, for each episode the environment can be represented by a standard POMDP P\u03b8 = {S,A,O, T\u03b8, I, \u03b3}, where T\u03b8(s, a) = T\u0398(s, a, \u03b8). World Models Model-based RL algorithms use experience gathered by an agent to learn a model of the environment (Sutton, 1991; Janner et al., 2019). When the observations are high-dimensional, it is beneficial to learn a compact latent representation of the state, and predict the environment dynamics in this latent space. Furthermore, in partially observable environments where the optimal action depends on the history of observations and actions, recurrent neural networks can be used to encode the history into a Markovian representation (Schmidhuber, 1990; Karl et al., 2017). In this work, we consider a world model to be a model that utilises a recurrent module to predict environment dynamics in a Markovian latent space (Ha & Schmidhuber, 2018; Hafner et al., 2021).\nLet the environment be some reward-free POMDP, P . A world model, W , can be thought of as consisting of two parts: W = {q, T\u0302}. The first part is the representation model q : H \u2192 Z, which encodes the history into a compact Markovian latent representation z \u2208 Z. The second part is the latent transition dynamics model, T\u0302 : Z \u00d7A\u2192 \u2206(Z), which predicts the dynamics in latent space. Because the latent dynamics are Markovian, we can think of the world model is approximating the\noriginal reward-free POMDP P with a reward-free MDP in latent space: M\u0302 = (Z,A, T\u0302 ). In this work, we consider policies of the form: \u03c0 : Z \u2192 \u2206(A). This corresponds to policies that are Markovian in latent space, and history-dependent in the original environment.\nMinimax Regret In robust optimisation, there is a set of possible scenarios, each defined by parameters \u03b8 \u2208 \u0398. The goal is to find a solution that achieves strong performance across all scenarios. In the context of reinforcement learning, we can think of each scenario as a possible instantiation of the environment, P\u03b8. For some reward function, R, the expected value of a policy in PR\u03b8 is V (\u03c0,PR\u03b8 ) := E[ \u2211\u221e t=0 \u03b3\ntrt | \u03c0,PR\u03b8 ], where rt are the rewards received by executing \u03c0 in PR\u03b8 . Minimax regret (Savage, 1951) is a commonly used objective for robust policy optimisation in RL (Chen et al., 2022; Dennis et al., 2020; Jiang et al., 2021a; Parker-Holder et al., 2022; Rigter et al., 2021). To define the minimax regret objective, we begin by defining the optimal policy for a given environment and reward function, \u03c0\u2217\u03b8,R = argmax\u03c0 V (\u03c0,PR\u03b8 ). We refer to each PR\u03b8 as a \u201ctask\u201d or, when clear from context, an \u201cenvironment.\u201d The regret for some arbitrary policy \u03c0 in environment PR\u03b8 is\nREGRET(\u03c0,PR\u03b8 ) := V (\u03c0\u2217\u03b8,R,PR\u03b8 )\u2212 V (\u03c0,PR\u03b8 ). (1)\nThe minimax regret objective finds the policy with the lowest regret across all environments:\n\u03c0\u2217regret = argmin \u03c0 max \u03b8\u2208\u0398 REGRET(\u03c0,PR\u03b8 ). (2)\nMinimax regret aims to find a policy that is near-optimal in all environments, and is therefore robust."
        },
        {
            "heading": "3 APPROACH",
            "text": "The minimax regret objective defines how to optimise a policy to be robust to different environments when the task is known. However, our aim in this work is to train a world model such that policies derived from the world model for future tasks are robust to different environments. In this section, we present our approach for gathering data to train a robust world model to achieve this aim.\nIn Section 3.1, we outline how we learn a single world model for many possible environments. In Section 3.2 we define the Reward-Free Minimax Regret objective, which connects minimax regret to world model training by assuming that when a reward function is provided, the optimal policy in the world model for that reward function can be found. We then show that we can optimise an upper bound on this objective by minimising the maximum expected latent dynamics error in the world model across all environments, under a suitable exploration policy. This informs our algorithm for selecting environments to sample data from to train the world model, WAKER: Weighted Acquisition of Knowledge across Environments for Robustness (Section 3.4). WAKER biases sampling towards environments where the world model is predicted to have the greatest errors (Figure 1a)."
        },
        {
            "heading": "3.1 WORLD MODELS FOR UNDERSPECIFIED POMDPS",
            "text": "We utilise a single world model, W = {q, T\u0302}, to model the reward-free UPOMDP. Consider any parameter setting of the UPOMDP, \u03b8 \u2208 \u0398, and the corresponding reward-free POMDP, P\u03b8. For any history in P\u03b8, h \u2208 H\u03b8, the representation model encodes this into a subset of the latent space, i.e. q : H\u03b8 \u2192 Z\u03b8, where Z\u03b8 \u2282 Z. We can then use the latent dynamics model T\u0302 to predict the\nlatent dynamics in Z\u03b8, corresponding to the dynamics of P\u03b8 for any \u03b8 \u2208 \u0398. Thus, we think of the world model as representing the set of reward-free POMDPs in the UPOMDP by a set of reward-free MDPs, each with their own latent state space: M\u0302\u03b8 = {Z\u03b8, A, T\u0302 , \u03b3}. This is illustrated in Figure 1b. Using a single world model across all environment parameter settings is a natural approach as it a) utilises the recurrent module to infer the parameter setting (which may be partially observable), and b) enables generalisation between similar parameter settings. Furthermore, it is sufficient to train a single generalist policy over the entire latent space Z to obtain a policy for all environments."
        },
        {
            "heading": "3.2 REWARD-FREE MINIMAX REGRET: PROBLEM DEFINITION",
            "text": "As discussed in Section 2, we can define robust policy optimisation via the minimax regret objective. However, this definition cannot be directly applied to our setting where we do not know the reward function during exploration, and our goal is to train a world model. In this section, we present the first contribution of this work, the Reward-Free Minimax Regret problem, which adapts the minimax regret objective to the setting of reward-free world model training that we address.\nConsider some world model, W , which as discussed in Section 3.1 represents the possible environments by a set of reward-free MDPs in latent space, {M\u0302\u03b8}\u03b8\u2208\u0398. Assume that after training W we are given some reward function, R. We define \u03c0\u0302\u2217\u03b8,R to be the optimal world model policy for that reward function R and parameter setting \u03b8, i.e.\n\u03c0\u0302\u2217\u03b8,R = argmax \u03c0 V (\u03c0,M\u0302R\u03b8 ). (3)\nThis is the optimal policy according to the MDP defined in the latent space of the world model for parameter setting \u03b8 and reward function R, and does not necessarily correspond to the optimal policy in the real environment. From here onwards, we will make the following assumption.\nAssumption 1 Consider some world model W that defines a set of MDPs in latent space {M\u0302\u03b8}\u03b8\u2208\u0398. Assume that given any reward function R, and parameter setting \u03b8 \u2208 \u0398, we can find \u03c0\u0302\u2217\u03b8,R.\nAssumption 1 is reasonable because we can generate unlimited synthetic training data in the world model for any parameter setting, and we can use this data to find a policy that is near-optimal in the world model using RL. In practice, we cannot expect to find the exact optimal policy within the world model, however Assumption 1 enables an analysis of our problem setting. We now define the Reward-Free Minimax Regret problem.\nProblem 1 (Reward-Free Minimax Regret) Consider some UPOMDP, U , with parameter set \u0398. For world model W , and \u03b8 \u2208 \u0398, let M\u0302R\u03b8 be the latent-space MDP defined by W , that represents real environment P\u03b8 with reward function R. Define the optimal world model policy as \u03c0\u0302\u2217\u03b8,R = argmax\u03c0 V (\u03c0,M\u0302R\u03b8 ). Find the world model, W \u2217, that minimises the maximum regret of the optimal world model policy across all parameter settings and reward functions:\nW \u2217 = argmin W max \u03b8,R REGRET(\u03c0\u0302\u2217\u03b8,R,PR\u03b8 ) (4)\nProblem 1 differs from the standard minimax regret objective in Equation 2 in two ways. First, Problem 1 optimises a world model (not a policy) under the assumption that the optimal world model policy can later be recovered (i.e. Assumption 1). Second, the maximum is over all possible reward functions in addition to parameter settings. This makes Problem 1 suitable for the rewardfree setting: we do not need access to a specific reward function when training the world model."
        },
        {
            "heading": "3.3 THEORETICAL MOTIVATION",
            "text": "Before presenting our algorithm for Problem 1, we first provide the motivation for our approach. We make the assumption that the world model learns a suitable representation model, q, which encodes any sequence of observations and actions in the UPOMDP into a Markovian latent state.\nAssumption 2 Consider the representation model learnt by the world model, q : H\u03b8 \u2192 Z\u03b8, for all \u03b8 \u2208 \u0398. Assume that given the representation model q, there exists a latent transition dynamics function, T , for which the expected reward is the same as the real environment: V (\u03c0,PR\u03b8 ) = V (\u03c0,MR\u03b8 ), whereMR\u03b8 = (Z\u03b8, A,R, T, \u03b3), for any policy \u03c0, reward function R, and parameter setting \u03b8.\nAssumption 2 states that the representation model successfully encodes any sequence of observations and actions into a Markovian latent state. Therefore, there exists a dynamics function T defined over the latent space that exactly models the real environment. Assumption 2 allows us to reason about the inaccuracy of the world model solely in terms of the difference between the learnt latent dynamics function, T\u0302 , and the (unknown) exact latent dynamics, T . For this reason, from here onwards we will solely refer to the latent dynamics function T\u0302 when discussing the world model, under the implicit assumption that a suitable representation model q is learnt according to Assumption 2.\nUsing Assumption 2 we can bound the sub-optimality of the optimal world model policy for any parameter setting \u03b8 according the difference between the learnt latent dynamics function, T\u0302 , and the true latent dynamics, T , in latent MDP M\u0302\u03b8. This is stated formally in Proposition 1.\nProposition 1 Let T\u0302 be the learnt latent dynamics in the world model. Assume the existence of a representation model q that adheres to Assumption 2, and let T be the true latent dynamics according to Assumption 2. Then, for any parameter setting \u03b8 and reward function R, the regret of the optimal world model policy is bounded according to:\nREGRET(\u03c0\u0302\u2217\u03b8,R,PR\u03b8 ) \u2264 2\u03b3 (1\u2212 \u03b3)2 [ E z,a\u223cd(\u03c0\u2217\u03b8,R,M\u0302\u03b8) [ TV ( T\u0302 (\u00b7|z, a), T (\u00b7|z, a) )] + E z,a\u223cd(\u03c0\u0302\u2217\u03b8,R,M\u0302\u03b8) [ TV ( T\u0302 (\u00b7|z, a), T (\u00b7|z, a)\n)]] where d(\u03c0,M) denotes the state-action distribution of \u03c0 in MDP M, and TV(P,Q) is the total variation distance between distributions P and Q.\nProof Sketch: This can be proven by applying a version of the Simulation Lemma (Kearns & Singh, 2002) twice. The full proof is provided in Appendix B.\nProposition 1 tells us that the optimal world model policy will have low regret if T\u0302 is accurate under the latent state-action distribution of both \u03c0\u2217\u03b8,R and \u03c0\u0302 \u2217 \u03b8,R in M\u0302\u03b8. However, during data collection we do not have access to the reward function. Therefore, we do not know these distributions as the stateaction distribution induced by both \u03c0\u2217\u03b8,R, and \u03c0\u0302 \u2217 \u03b8,R depends upon the reward function. To alleviate this issue, we define an exploration policy, \u03c0expl\u03b8 , that maximises the expected error (in terms of total variation distance) of the latent dynamics model:\n\u03c0expl\u03b8 = argmax \u03c0 E z,a\u223cd(\u03c0,M\u0302\u03b8)\n[ TV ( T\u0302 (\u00b7|z, a), T (\u00b7|z, a) )] . (5)\nThis allows us to write an upper bound on the regret that has no dependence on the reward function:\nREGRET(\u03c0\u0302\u2217\u03b8,R,PR\u03b8 ) \u2264 4\u03b3\n(1\u2212 \u03b3)2 E z,a\u223cd(\u03c0expl\u03b8 ,M\u0302\u03b8)\n[ TV ( T\u0302 (\u00b7|z, a), T (\u00b7|z, a) )] for all R. (6)\nTherefore, we can upper bound the objective of Problem 1 by the maximum expected TV error in the latent dynamics function over all parameter settings:\nmax \u03b8,R REGRET(\u03c0\u0302\u2217\u03b8,R,PR\u03b8 ) \u2264 max \u03b8\n4\u03b3\n(1\u2212 \u03b3)2 E z,a\u223cd(\u03c0expl\u03b8 ,M\u0302\u03b8)\n[ TV ( T\u0302 (\u00b7|z, a), T (\u00b7|z, a) )] . (7)\nWe now formally state the Minimax World Model Error problem, which proposes to optimise this upper bound as an approximation to the Reward-Free Minimax Regret objective in Problem 1.\nProblem 2 (Minimax World Model Error) Consider some UPOMDP, U , with parameter set \u0398, and world model latent dynamics function T\u0302 . Let T be the true latent dynamics function according to Assumption 2. Define the world model error for some parameter setting \u03b8 as:\nWORLDMODELERROR(T\u0302 , \u03b8) = E z,a\u223cd(\u03c0expl\u03b8 ,M\u0302\u03b8)\n[ TV ( T\u0302 (\u00b7|z, a), T (\u00b7|z, a) )] (8)\nFind the world model that minimises the maximum world model error across all parameter settings:\nT \u2217 = argmin T\u0302 max \u03b8\u2208\u0398 WORLDMODELERROR(T\u0302 , \u03b8) (9)\nProblem 2 optimises an upper bound on our original objective in Problem 1 (see Equation 7). Problem 2 finds a world model that has low prediction error across all environments under an exploration\npolicy that seeks out the maximum error. This ensures that for any future reward function, the optimal world model policy will be near-optimal for all environments. In the next section, we present our algorithm for selecting environments to gather data for world model training, with the aim of minimising the maximum world model error across all environments, per Problem 2."
        },
        {
            "heading": "3.4 WEIGHTED ACQUISITION OF KNOWLEDGE ACROSS ENVIRONMENTS FOR ROBUSTNESS",
            "text": "Overview In this section, we address how to select environments to collect data for world model training, so that the world model approximately solves Problem 2. We cannot directly evaluate the world model error in Equation 8, as it requires knowing the true latent dynamics function, T . Therefore, following works on offline RL (Lu et al., 2022; Yu et al., 2020; Kidambi et al., 2020), we use the disagreement between an ensemble of neural networks as an estimate of the total variation distance between the learnt and true latent transition dynamics functions in Equation 8. This enables us to generate an estimate of the world model error for each environment. Then, when sampling environments, our algorithm (WAKER) biases sampling towards the environments that are estimated to have the highest error. By gathering more data for the environments with the highest estimated error, WAKER improves the world model on those environments, and therefore WAKER reduces the maximum world model error across environments as required by Problem 2.\nAlgorithm 1 Weighted Acquisition of Knowledge across Environments for Robustness (WAKER) 1: Inputs: UPOMDP with free parameters \u0398; Boltzmann\ntemperature \u03b7; Imagination horizon h; 2: Initialise: data buffer D; error buffer Derror; world model\nW = {q, T\u0302}; exploration policy \u03c0expl\n3: while training world model do 4: if p \u223c U[0,1] < pDR or D.is empty() then 5: \u03b8 \u223c DomainRandomisation(\u0398) 6: else 7: \u03b8 \u223c Boltzmann(Normalize(Derror), \u03b7) 8: \u03c4\u03b8 \u2190 rollout of \u03c0expl in P\u03b8 Collect real traj. for \u03b8 9: Add \u03c4\u03b8 to D 10: Train W on D 11: \u03c0expl, Derror \u2190 Imagine(D, Derror, W , \u03c0expl) 12: function Imagine(D, Derror, W , \u03c0expl) 13: for i = 1, . . . ,K do 14: \u03c4\u03b8 \u2190 D.sample() Sample real trajectory 15: Z\u03c4\u03b8 = {zt} |\u03c4\u03b8| t=0 \u2190 q(\u03c4\u03b8) Encode latent states 16: \u03c4\u0302\u03b8 \u2190 rollout \u03c0expl for h steps from z \u2208 Z\u03c4\u03b8 in W 17: \u03b4\u03b8 \u2190 Error estimate for \u03b8 via Eq. 10 on \u03c4\u0302\u03b8 18: Update Derror with \u03b4\u03b8 for \u03b8 19: Train \u03c0expl on \u03c4\u0302\u03b8 20: return \u03c0expl, Derror WAKER is presented in Algorithm 1, and illustrated in Figure 1a. We train a single exploration policy over the entire latent space to approximately optimise the exploration objective in Equation 5 across all environments. Therefore, we refer to the exploration policy simply as \u03c0expl, dropping the dependence on \u03b8. We maintain a buffer Derror of the error estimate for each parameter setting \u03b8 that we have collected data for. To choose the environment for the next episode of exploration, with probability pDR we sample \u03b8 using domain randomisation (Line 5). This ensures that we will eventually sample all environments. With probability 1 \u2212 pDR, we sample \u03b8 from a Boltzmann distribution, where the input to the Boltzmann distribution is the error estimate for each environment in Derror (Line 7). Once the environment has been selected we sample a trajectory, \u03c4\u03b8, by rolling out \u03c0expl in the environment (Line 8). We add \u03c4\u03b8 to the data buffer D, and perform supervised learning on D (Line 10) to update the world model.\nImagine updates \u03c0expl and Derror using imagined rollouts (Line 11). For each imagined rollout, we first sample real trajectory \u03c4\u03b8 \u2208 D (Line 14). We encode \u03c4\u03b8 into latent states Z\u03c4\u03b8 (Line 15). We then generate an imagined trajectory, \u03c4\u0302\u03b8, by rolling out \u03c0expl using T\u0302 starting from an initial latent state z \u2208 Z\u03c4\u03b8 (Line 16). Thus, \u03c4\u0302\u03b8 corresponds to an imagined trajectory in the environment with parameter setting \u03b8, as illustrated in Figure 1b. We wish to estimate the world model error for environment parameter setting \u03b8 from this imagined rollout. Following previous works (Mendonca et al., 2021; Rigter et al., 2023; Sekar et al., 2020), we learn an ensemble of N latent dynamics models: {T\u0302i}Ni=1. Like (Yu et al., 2020; Kidambi et al., 2020; Lu et al., 2022), we use the disagreement between the ensemble means as an approximation to the world model TV error for parameters \u03b8:\nWORLDMODELERROR(T\u0302 , \u03b8) \u2248 E z,a\u223cd(\u03c0expl\u03b8 ,M\u0302\u03b8)\n[ Var { E[T\u0302i(\u00b7|z, a)] }N i=1 ] (10)\nWe compute the error estimate using the latent states and actions in \u03c4\u0302\u03b8 in Line 17. We use this to update our estimate of the world model error for environment \u03b8 in Derror (Line 18). Optionally, we may also use the imagined rollout to update the exploration policy in Line 19. For the world model architecture, we use DreamerV2 (Hafner et al., 2021). Implementation details are in Appendix E.\nError Estimate Update Function We consider two possibilities for updating the error estimate for each \u03b8 in Derror on Line 18: 1) in WAKER-M, Derror maintains a smoothed average of the magnitude of the error estimate for each \u03b8; 2) In WAKER-R, we update Derror to maintain a smoothed average\nof the rate of reduction in the error estimate for each \u03b8. Thus, WAKER-M biases sampling towards environments that have highest error, while WAKER-R biases sampling towards environments that have the highest rate of reduction in error. More details are in Appendix E.\nExploration Policy We must learn an exploration policy \u03c0expl that approximately maximises the world model error according to Equation 5. By default, we use Plan2Explore (Sekar et al., 2020), and train the exploration policy to maximise the approximation in Equation 10. To test whether our approach is agnostic to the exploration policy used, we also consider a random exploration policy.\nZero-Shot Task Adaptation Once the world model has been trained, we use it to derive a taskspecific policy without any further data collection. For reward function R, we find a single task policy \u03c0\u0302\u2217R that is defined over the entire latent space Z, and therefore all environments. We use the same approach as Sekar et al. (2020): we label the data in D with the associated rewards and use this data to train a reward predictor. Then, we train the task policy in the world model to optimise the expected reward according to the reward predictor. All task policies are trained in this manner.\n4 EXPERIMENTS\nWe seek to answer the following questions: a) Does WAKER enable more robust policies to be trained in the world model? b) Does the performance of WAKER depend on the exploration policy used? c) Does WAKER lead to stronger generalisation to out-of-distribution environments? The code for our experiments is available at github.com/marcrigter/waker.\nMethods Compared To answer question a), we compare WAKERM and WAKER-R with the following baselines: Domain Randomisation (DR), samples uniformly from the default environment distribution; Gradual Expansion (GE) gradually increases the range of environments sampled from the default distribution; Hardest Environment Oracle (HE-Oracle) samples only the single most complex environment; Re-weighting Oracle (RW-Oracle) re-weights the environment distribution to focus predominantly on the most complex environments. Note that the HE-Oracle and RW-Oracle baselines require expert domain knowledge. Detailed descriptions of the baselines can be found in Appendix G.\nFor both WAKER variants, we set pDR = 0.2 and perform limited tuning of the Boltzmann temperature \u03b7. More details are in Appendix F.3. To investigate question b), we pair WAKER and DR with two different exploration policies: Plan2Explore (P2E) (Sekar et al., 2020) or a random exploration policy. For the other baselines we always use the P2E exploration policy.\nDomains and Tasks All domains use image observations. For Terrain Walker and Terrain Hopper we simulate the Walker and Hopper robots from the DMControl Suite (Tassa et al., 2018) on procedurally generated terrain. For each environment, there are two parameters (amplitude and length scale) that control the terrain generation. For each domain, we evaluate a number of downstream tasks (walk, run, stand, flip, walk-back. / hop, hop-back., stand). The Clean Up and Car Clean Up domains are based on SafetyGym (Ray et al., 2019) and consist of blocks that can be pushed and either a point mass (Clean Up) or car robot (Car Clean Up). There are three environment parameters: the environment size, the number of blocks, and the block colours. The downstream tasks (sort, push, sort-reverse) each correspond to different goal locations for each colour of block. Examples of training environments are shown in Figure 2, and more details are in Appendix F.1. We also perform experiments where we train a single world model for both the Clean Up and Terrain Walker domains. Due to space constraints we defer these results to Appendix C.1.\nEvaluation Our aim is to train the world model such that the policies obtained are robust, as measured by minimax regret. However, we cannot directly evaluate regret as it requires knowing the true optimal performance. Therefore, following (Jiang et al., 2021a; Rigter et al., 2023) we evaluate conditional value at risk (CVaR) (Rockafellar et al., 2000) to measure robustness. For confidence level \u03b1, CVaR\u03b1 is the average performance on the worst \u03b1-fraction of runs. We evaluate CVaR0.1 by evaluating each policy on 100 environments sampled uniformly at random, and reporting the aver-\nFigure 3: Robustness evaluation aggregated CIs.\nFigure 4: OOD evaluation aggregated CIs.\nage of the worst 10 runs. We also report the average performance over all 100 runs in Appendix C.4. To answer question c), we additionally evaluate the policies on out-of-distribution (OOD) environments. For the terrain domains, the OOD environments are terrain with a length scale 25% shorter than seen in training (Steep), and terrain containing stairs (Stairs). For the clean up domains, the OOD environments contain one more block than was ever seen in training (Extra Block).\nResults Presentation In Tables 1-2 we present results for task policies obtained from the final world model at the end of six days of training. For each exploration policy, we highlight results within 2% of the best score (provided that non-trivial performance is obtained), and\u00b1 indicates the S.D. over 5 seeds. Learning curves for the performance of policies obtained from snapshots of the world model are in Appendix D.2. To assess statistical significance, we present 95% confidence intervals of the probability that algorithm X obtains improved performance over algorithm Y, computed using the aggregated results across all tasks with the rliable (Agarwal et al., 2021) framework (Figures 3-4).\nResults Table 1 presents the robustness evaluation results. For both exploration policies, WAKERM outperforms DR across almost all tasks. For the Plan2Explore exploration policy, WAKER-R also outperforms DR. Figure 3 shows that these improvements over DR are statistically significant, as the lower bound on the probability of improvement is greater than 0.5. Both WAKER variants result in significant improvements over DR when using Plan2Explore, suggesting that WAKER is more effective when combined with a sophisticated exploration policy. Between WAKER-M and WAKER-R, WAKER-M (which prioritises the most uncertain environments) obtains stronger performance. This is expected from our analysis in Section 3.3, which shows that minimising the maximum world model error across environments improves robustness. Figure 5 illustrates that WAKER focuses sampling on more complex environments: larger environments with more blocks in the clean up domains, and steeper terrain with shorter length scale and greater amplitude in the terrain domains. Plots of how the sampling evolves during training are in Appendix C.3. Regardless of the environment selection method, Plan2Explore leads to stronger performance than random exploration, verifying previous findings (Sekar et al., 2020). The results for average performance in Appendix C.4 show that WAKER achieves improved or similar average performance compared to DR. This demonstrates that WAKER improves robustness at no cost to average performance. The results for training a single world model for both the Clean Up and Terrain Walker environments in Appendix C.1 demonstrate that even when training across highly diverse environments, WAKER also achieves more robust performance in comparison to DR.\nThe robustness results in Table 1 show that GE obtains very similar performance to DR. This is unsurprising, as GE does not bias sampling towards more difficult environments, and only modifies the DR distribution by gradually increasing the range of environments sampled. HE-Oracle obtains poor performance, demonstrating that focussing on the most challenging environment alone is insufficient to obtain a robust world model. This is expected from the analysis in Section 3.3 which shows that to obtain robustness we need the world model to have low error across all environments (not just the most complex one). For Terrain Walker and Terrain Hopper, RW-Oracle is a strong baseline, and obtains similar performance to WAKER. However, for Clean Up and Car Clean Up RW-Oracle\nobtains significantly worse performance than WAKER. This is likely because by focussing sampling environments with four blocks of any colour, RW-Oracle does not sample diversely enough to obtain good robustness. This demonstrates that even with domain knowledge, handcrafting a suitable curriculum is challenging.\nTo assess the quality of the world models, we evaluate the error between the transitions predicted by the world model and real transitions. To evaluate robustness, we compute the error on the worst 10% of trajectories generated by a performant task-policy on randomly sampled environments. In Figure 6, we observe that WAKER leads to lower prediction errors on the worst 10% of trajectories compared to DR. This verifies that by biasing sampling towards environments with higher error estimates, WAKER reduces the worst-case errors in the world model. This suggests that improved world model training leads to the improved policy robustness that we observe in Table 1, as expected from the upper bound in Equation 7.\nTable 2 and Figure 4 present the evaluation on OOD environments, averaged across tasks. Full results for each task are in Appendix C.2. WAKER-M results in a considerable performance improvement over DR for both exploration policies, and WAKER-R significantly improves performance when the exploration policy is Plan2Explore. Thus, WAKER-M again obtains better performance than WAKER-R, but both variants of our algorithm outperform DR. GE obtains similar or slightly worse performance than DR for the OOD environments. For the steep terrain OOD environments, HE-Oracle performs quite well as it focuses on sampling the steepest possible in-distribution terrain. However, HE-Oracle does not perform well on the stairs OOD environments, demonstrating that sampling a range of environments is necessary for strong OOD generalisation. RW-Oracle performs well on the OOD environments across all domains. However, RW-Oracle has the significant drawback that expert domain knowledge is required. These results demonstrate that by actively sampling more uncertain environments for exploration, WAKER leads to world models that are able to generalise more broadly to environments never seen during training, without requiring any expert domain knowledge."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we have proposed the first method for automatic curriculum learning for environment selection in the reward-free setting. Due to space constraints, a discussion of related work can be found in Appendix A. We formalised this problem in the context of learning a robust world model. Building on prior works in robust RL, we considered robustness in terms of minimax regret, and we derived a connection between the maximum regret and the maximum error of the world model dynamics across environments. We operationalised this insight in the form of WAKER, which trains a world model in the reward-free setting by selectively sampling the environment settings that induce the highest latent dynamics error. In several pixel-based continuous control domains, we demonstrated that compared to other baselines that do not require expert domain knowledge, WAKER drastically improves the zero-shot task adaptation capabilities of the world model in terms of robustness. Policies trained for downstream tasks inside the learned world model exhibit significantly improved generalisation to out-of-distribution environments that were never encountered during training. WAKER therefore represents a meaningful step towards developing more generallycapable agents. In future work, we are excited to scale our approach to even more complex domains with many variable parameters. One limitation of our approach is that it relies upon an intrinsically motivated policy to adequately explore the state-action space across a range of environments. This may pose a challenge for scalability to more complex environments. To scale WAKER further, we plan to make use of function approximation to estimate uncertainty throughout large parameter spaces, as opposed to the discrete buffer used in this work. We also plan to investigate using WAKER for reward-free pretraining, followed by task-specific finetuning to overcome the challenge of relying upon intrinsically motivated exploration.\nAcknowledgements This work was supported by a Programme Grant from the Engineering and Physical Sciences Research Council (EP/V000748/1) and a gift from Amazon Web Services. Additionally, this project made use of the Tier 2 HPC facility JADE2, funded by the Engineering and Physical Sciences Research Council (EP/T022205/1)."
        },
        {
            "heading": "A RELATED WORK",
            "text": "Our work focuses on training more robust world-models (Ha & Schmidhuber, 2018; Hafner et al., 2020; 2019; 2021; Janner et al., 2019; Rigter et al., 2022; Schrittwieser et al., 2020) in the rewardfree setting. Past works improve data collection by training an exploration policy within the world model (i.e. in imagination) (Sekar et al., 2020; Ball et al., 2020; Mendonca et al., 2021; Xu et al., 2022). Unlike prior methods, WAKER augments action-based exploration by directly exploring over the space of environments via an autocurriculum. To our knowledge, this is the first work to address curricula for reward-free training of a world model.\nWAKER can thus be viewed as an automatic curriculum learning (ACL) (Graves et al., 2017; Portelas et al., 2021; Narvekar et al., 2020) method for training a world model. ACL methods optimise the order in which tasks or environments are presented during training, to improve the agent\u2019s final performance (Asada et al., 1996; Florensa et al., 2017; 2018; Mendonca et al., 2021). Existing ACL approaches for environment selection require task-specific evaluation metrics to assess the performance or learning potential of the agent in each environment, and select appropriate environments to sample from (Portelas et al., 2020; Eimer et al., 2021; Dennis et al., 2020; Jiang et al., 2021a; Matiisen et al., 2019; Mehta et al., 2020; Parker-Holder et al., 2022; Wang et al., 2019). Therefore, existing ACL approaches are not applicable to the reward-free setting that we address.\nDomain randomisation (DR) (Jakobi, 1997; Tobin et al., 2017) samples environments uniformly, and can be viewed as the simplest approach to ACL. Active DR (Mehta et al., 2020; Akkaya et al., 2019) extends DR by upweighting environments that induce divergent behaviors relative to a reference behaviour. However, Active DR requires access to a task-specific reward function to generate the reference behaviour and therefore is also not applicable to our reward-free setting.\nUnsupervised environment design (UED) (Dennis et al., 2020) refers to a class of methods that generate curricula to produce more robust policies. UED typically frames curriculum learning as a zero-sum game between a teacher and a student, where the teacher actively selects environments to optimise an adversarial objective. When this objective is the student\u2019s regret in each environment, the student provably follows a minimax regret policy at the Nash equilibria of the resulting game, making minimax regret UED methods a principled approach for learning robust policies (Dennis et al., 2020; Jiang et al., 2021a). A simple and effective form of UED is Prioritised Level Replay (PLR) (Jiang et al., 2021b;a), which selectively revisits environments that have higher estimated \u201clearning potential\u201d, as measured by the temporal difference (TD) error. Extensions of PLR improve on its random search (Parker-Holder et al., 2022) and address issues with UED in stochastic settings (Jiang et al., 2022a). However, all existing UED methods require a known reward function during curriculum learning. To our knowledge, ours is the first work to address reward-free UED.\nWAKER uses ideas from intrinsic motivation (Deci & Ryan, 1985; Schmidhuber, 2010) as a bridge between UED and the reward-free setting. By providing agents with intrinsic rewards, such methods enable agents to efficiently explore complex environments in the absence of extrinsic rewards. Intrinsic motivation generalises active learning (Cohn et al., 1996) to sequential decision-making, by rewarding agents for visiting states that: a) have high uncertainty (Haber et al., 2018; Pathak et al., 2017; Schmidhuber, 1991; Sekar et al., 2020; Shyam et al., 2019), b) can be maximally influenced (Bharadhwaj et al., 2022; Klyubin et al., 2005), or c) have rarely been visited (Bellemare et al., 2016; Ostrovski et al., 2017). Our work extends uncertainty-based intrinsic motivation beyond states within a single environment to the space of environment configurations, allowing novelty-related metrics to be used in place of typical UED objectives that require a task-specific reward function. WAKER thus highlights the fundamental connection between autocurricula and exploration."
        },
        {
            "heading": "B PROOF OF PROPOSITION 1",
            "text": "We begin by stating and proving a version of the classic Simulation Lemma from (Kearns & Singh, 2002).\nLemma 1 (Simulation Lemma (Kearns & Singh, 2002)) Consider two infinite horizon MDPs, MR = {S,A,R, T, \u03b3} and M\u0302R = {S,A,R, T\u0302 , \u03b3}, with reward function R : S \u00d7 A \u2192 [0, 1].\nConsider any stochastic policy \u03c0 : S \u2192 \u2206(A). Then:\u2223\u2223\u2223V (\u03c0,M\u0302R)\u2212 V (\u03c0,MR)\u2223\u2223\u2223 \u2264 2\u03b3 (1\u2212 \u03b3)2 E s,a\u223cd(\u03c0,M\u0302R) [ TV ( T\u0302 (\u00b7|s, a), T (\u00b7|s, a) )] (11)\nwhere d(\u03c0,M\u0302R) is the state-action distribution of \u03c0 in M\u0302R, and TV(P,Q) is the total variation distance between two distributions P and Q.\nProof : To simplify the notation of the proof, we will use the notation V \u03c0 = V (\u03c0,MR) and V\u0302 \u03c0 = V (\u03c0,M\u0302R).\nUsing the Bellman equation for V\u0302 \u03c0 and V \u03c0 , we have: V\u0302 \u03c0(s0)\u2212 V \u03c0(s0) = Ea0\u223c\u03c0(s0) [ R(s0, a0) + \u03b3Es\u2032\u223cT\u0302 (s0,a0)V\u0302 \u03c0(s\u2032) ]\n\u2212 Ea0\u223c\u03c0(s0) [ R(s0, a0) + \u03b3Es\u2032\u223cT (s0,a0)V \u03c0(s\u2032) ]\n= \u03b3Ea0\u223c\u03c0(s0) [ Es\u2032\u223cT\u0302 (s0,a0)V\u0302 \u03c0(s\u2032)\u2212 Es\u2032\u223cT (s0,a0)V \u03c0(s\u2032) ] = \u03b3Ea0\u223c\u03c0(s0) [ Es\u2032\u223cT\u0302 (s0,a0)V\u0302 \u03c0(s\u2032)\u2212 Es\u2032\u223cT\u0302 (s0,a0)V \u03c0(s\u2032)\n+ Es\u2032\u223cT\u0302 (s0,a0)V \u03c0(s\u2032)\u2212 Es\u2032\u223cT (s0,a0)V\n\u03c0(s\u2032) ]\n= \u03b3Ea0\u223c\u03c0(s0) [ Es\u2032\u223cT\u0302 (s0,a0)V\u0302 \u03c0(s\u2032)\u2212 Es\u2032\u223cT\u0302 (s0,a0)V \u03c0(s\u2032) ] + \u03b3Ea0\u223c\u03c0(s0) [ Es\u2032\u223cT\u0302 (s0,a0)V \u03c0(s\u2032)\u2212 Es\u2032\u223cT (s0,a0)V \u03c0(s\u2032)\n] (12)\nWe define P\u0302\u03c01 (s1|s0) = \u222b a\u2208A \u03c0(a|s0)T\u0302 (s1|s0, a)da. This allows us to rewrite the first term from the last line as:\n\u03b3Ea0\u223c\u03c0(s0) [ Es\u2032\u223cT\u0302 (s0,a0)V\u0302 \u03c0(s\u2032)\u2212 Es\u2032\u223cT\u0302 (s0,a0)V \u03c0(s\u2032) ] = \u03b3Es1\u223cP\u0302\u03c01 (\u00b7|s0) [ V\u0302 \u03c0(s1)\u2212 V \u03c0(s1) ] We define P\u0302\u03c01 (s1, a1|s0) = \u03c0(a1|s1)P\u0302\u03c01 (s1|s0), and P\u0302\u03c02 (s2|s0) = \u222b s\u2208S \u222b a\u2208A P\u0302 \u03c0 1 (s, a|s0)T\u0302 (s2|s, a)dads. We again apply the Bellman equation:\n\u03b3Es1\u223cP\u0302\u03c01 (\u00b7|s0) [ V\u0302 \u03c0(s1)\u2212 V \u03c0(s1) ] =\n\u03b3Es1\u223cP\u0302\u03c01 (\u00b7|s0) [ \u03b3Ea1\u223c\u03c0(\u00b7|s1) [ Es\u2032\u223cT\u0302 (s1,a1)V\u0302 \u03c0(s\u2032)\u2212 Es\u2032\u223cT (s1,a1)V \u03c0(s\u2032) ]] = \u03b3Es1\u223cP\u0302\u03c01 (\u00b7|s0) [ \u03b3Ea1\u223c\u03c0(\u00b7|s1) [ Es\u2032\u223cT\u0302 (s1,a1)V\u0302 \u03c0(s\u2032)\u2212 Es\u2032\u223cT\u0302 (s1,a1)V \u03c0(s\u2032) ]] +\n\u03b3Es1\u223cP\u0302\u03c01 (\u00b7|s0) [ \u03b3Ea1\u223c\u03c0(\u00b7|s1) [ Es\u2032\u223cT\u0302 (s1,a1)V \u03c0(s\u2032)\u2212 Es\u2032\u223cT (s1,a1)V \u03c0(s\u2032) ]] = \u03b32Es2\u223cP\u0302\u03c02 (\u00b7|s0) [ V\u0302 \u03c0(s2)\u2212V \u03c0(s2) ] +\u03b3Es1,a1\u223cP\u0302\u03c01 (\u00b7,\u00b7|s0) [ Es\u2032\u223cT\u0302 (s1,a1)V \u03c0(s\u2032)\u2212Es\u2032\u223cT (s1,a1)V \u03c0(s\u2032) ] .\n(13)\nCombining Equations 12 and 13 we have V\u0302 \u03c0(s0)\u2212 V \u03c0(s0) = \u03b3Ea0\u223c\u03c0(s0) [ Es\u2032\u223cT\u0302 (s0,a0)V \u03c0(s\u2032)\u2212 Es\u2032\u223cT (s0,a0)V \u03c0(s\u2032) ] +\n\u03b3Es1,a1\u223cP\u0302\u03c01 (\u00b7,\u00b7|s0) [ Es\u2032\u223cT\u0302 (s1,a1)V \u03c0(s\u2032)\u2212 Es\u2032\u223cT (s1,a1)V \u03c0(s\u2032) ] +\n\u03b32Es2\u223cP\u0302\u03c02 (\u00b7|s0) [ V\u0302 \u03c0(s2)\u2212 V \u03c0(s2) ] (14)\nRepeatedly applying the reasoning in Equations 12-14 we have V\u0302 \u03c0(s0)\u2212 V \u03c0(s0) = \u221e\u2211 t=0 \u03b3t+1Es,a\u223cP\u0302\u03c0t (\u00b7,\u00b7|s0) [ Es\u2032\u223cT\u0302 (s,a)V \u03c0(st)\u2212 Es\u2032\u223cT (s,a)V \u03c0(st) ]\n= \u03b3\n1\u2212 \u03b3 E s,a\u223cd(\u03c0,M\u0302R)\n[ Es\u2032\u223cT\u0302 (s,a)V \u03c0(st)\u2212 Es\u2032\u223cT (s,a)V \u03c0(st) ] (15)\nwhere d(\u03c0,M\u0302R) is the state-action distribution of \u03c0 in M\u0302R. By the definition of the reward function (R(s, a) \u2208 [0, 1]), we have that V \u03c0(s) \u2208 [0, 1/(1 \u2212 \u03b3)]. We utilise the inequality that: |Ex\u223cP f(x) \u2212 Ex\u223cQf(x)| \u2264 2maxx |f(x)|TV(P,Q). Then, taking the absolute value of Equation 15 and applying the inequality we have:\u2223\u2223\u2223V\u0302 \u03c0(s0)\u2212 V \u03c0(s0)\u2223\u2223\u2223 \u2264 \u03b3\n1\u2212 \u03b3 E s,a\u223cd(\u03c0,M\u0302R) \u2223\u2223\u2223Es\u2032\u223cT\u0302 (s,a)V \u03c0(st)\u2212 Es\u2032\u223cT (s,a)V \u03c0(st)\u2223\u2223\u2223 \u2264 2\u03b3\n(1\u2212 \u03b3)2 E s,a\u223cd(\u03c0,M\u0302R)\n[ TV ( T\u0302 (\u00b7|s, a), T (\u00b7|s, a) )] \u25a1\n(16)\nNow that we have proven Lemma 1, we return to our original purpose of proving Proposition 1. We begin by restating Proposition 1:\nProposition 1 Let T\u0302 be the learnt latent dynamics in the world model. Assume the existence of a representation model q that adheres to Assumption 2, and let T be the true latent dynamics according to Assumption 2. Then, for any parameter setting \u03b8 and reward function R, the regret of the optimal world model policy is bounded according to:\nREGRET(\u03c0\u0302\u2217\u03b8,R,PR\u03b8 ) \u2264 2\u03b3 (1\u2212 \u03b3)2 [ E z,a\u223cd(\u03c0\u2217\u03b8,R,M\u0302\u03b8) [ TV ( T\u0302 (\u00b7|z, a), T (\u00b7|z, a) )] + E z,a\u223cd(\u03c0\u0302\u2217\u03b8,R,M\u0302\u03b8) [ TV ( T\u0302 (\u00b7|z, a), T (\u00b7|z, a)\n)]] where d(\u03c0,M) denotes the state-action distribution of \u03c0 in MDP M, and TV(P,Q) is the total variation distance between distributions P and Q.\nNow, let us consider the latent MDP learnt by our world model M\u0302\u03b8 = (Z\u03b8, A, T\u0302 , \u03b3) for some parameters \u03b8 \u2208 \u0398, as well as the latent space MDP that exactly models the true environment dynamics according to Assumption 2, M\u03b8 = (Z\u03b8, A, T, \u03b3). Recall that for some reward function, R, the optimal world model policy is defined to be\n\u03c0\u0302\u2217\u03b8,R = argmax \u03c0 V (\u03c0,M\u0302R\u03b8 )\nThe regret of the optimal world model policy for parameter setting \u03b8 and reward function R is REGRET(\u03c0\u0302\u2217\u03b8,R,PR\u03b8 ) = V (\u03c0\u2217\u03b8,R,PR\u03b8 )\u2212 V (\u03c0\u0302\u2217\u03b8,R,PR\u03b8 ) (17)\nBy Assumption 2 we have that V (\u03c0,PR\u03b8 ) = V (\u03c0,MR\u03b8 ) for all \u03c0. This allows us to write the regret as the performance difference inMR\u03b8 rather than PR\u03b8 :\nREGRET(\u03c0\u0302\u2217\u03b8,R,PR\u03b8 ) = V (\u03c0\u2217\u03b8,R,MR\u03b8 )\u2212 V (\u03c0\u0302\u2217\u03b8,R,MR\u03b8 ) (18)\nBy the definition of the optimal world model policy, we have that\nV (\u03c0\u0302\u2217\u03b8,R,M\u0302R\u03b8 )\u2212 V (\u03c0\u2217\u03b8,R,M\u0302R\u03b8 ) \u2265 0 (19)\nAdding together Equations 18 and 19 we have\nREGRET(\u03c0\u0302\u2217\u03b8,R,PR\u03b8 ) \u2264 V (\u03c0\u2217\u03b8,R,MR\u03b8 )\u2212 V (\u03c0\u2217\u03b8,R,M\u0302R\u03b8 ) + V (\u03c0\u0302\u2217\u03b8,R,M\u0302R\u03b8 )\u2212 V (\u03c0\u0302\u2217\u03b8,R,MR\u03b8 ) \u2264 \u2223\u2223\u2223V (\u03c0\u2217\u03b8,R,MR\u03b8 )\u2212 V (\u03c0\u2217\u03b8,R,M\u0302R\u03b8 )\u2223\u2223\u2223+ \u2223\u2223\u2223V (\u03c0\u0302\u2217\u03b8,R,M\u0302R\u03b8 )\u2212 V (\u03c0\u0302\u2217\u03b8,R,MR\u03b8 )\u2223\u2223\u2223\nThen, applying the Lemma 1 to both terms on the right-hand side we have\nREGRET(\u03c0\u0302\u2217\u03b8,R,PR\u03b8 ) \u2264 2\u03b3 (1\u2212 \u03b3)2 [ E z,a\u223cd(\u03c0\u2217\u03b8,R,M\u0302\u03b8) [ TV ( T\u0302 (\u00b7|z, a), T (\u00b7|z, a) )] + E z,a\u223cd(\u03c0\u0302\u2217\u03b8,R,M\u0302\u03b8) [ TV ( T\u0302 (\u00b7|z, a), T (\u00b7|z, a) )]] \u25a1"
        },
        {
            "heading": "C KEY ADDITIONAL RESULTS",
            "text": "In this section, we present the most important additional results that were omitted from the main paper due to space constraints."
        },
        {
            "heading": "C.1 TRAINING A SINGLE WORLD MODEL FOR TWO DOMAINS",
            "text": "In this subsection, we present results for training a single world model on both the Terrain Walker and Clean Up domains. WAKER must choose which domain to sample from (either Terrain Walker or Clean Up) as well as sample the environment parameters for that domain. The domain randomisation (DR) baseline chooses either domain with 50% probability, and then randomly samples the parameters for that domain.\nTo handle the varying dimensionality of the action spaces between the two domains, we pad the action space of the Clean Up domain with additional actions that are unused in that domain. To ensure that WAKER receives well-calibrated ensemble-based uncertainty estimates between these two significantly different domains, we use the following approach. We first normalise the uncertainty estimates for Terrain Walker and Clean Up separately, and then concatenate the uncertainty estimates to create the buffer of uncertainty estimates. We found that this was necessary because the scale of the ensemble-based uncertainty estimates can differ between the two significantly different domains. Obtaining well-calibrated uncertainty estimates without requiring this normalisation process is an orthogonal area of research that is outside the scope of this paper.\nThe results in Table 3 show that WAKER outperforms DR on both the robustness evaluation and the out-of-distribution evaluation when training a single world model on both domains. This highlights the potential for WAKER to be used to train very general world models, to enable agents capable of solving a wide range of tasks in a wide range of domains."
        },
        {
            "heading": "C.2 OUT OF DISTRIBUTION EVALUATION: FULL TASK RESULTS",
            "text": "In Table 2 in the main paper, we presented the out-of-distribution results in terms of averages across the tasks for each domain. In Table 4 we present the full results for each task.\nC.3 ILLUSTRATION OF CURRICULA\nThe plots in Figures 7 and 8 illustrate the parameters sampled throughout world model training for the Terrain Walker and Clean Up domains.\nDomain randomisation (DR) samples parameters uniformly throughout training. WAKER-R biases sampling towards environments where the uncertainty estimate is decreasing the fastest. For Terrain Walker, we see in Figure 7 that initially (0 - 2M steps), WAKER-R samples all parameters equally in a similar fashion to DR. This indicates that during this period, the uncertainty estimate for all environment parameters is decreasing approximately equally. From 2M - 5M steps, WAKER-R selects terrain with a higher amplitude, and shorter length scale (more complex and undulating terrain). This indicates that during this period, the uncertainty for the complex terrain is decreasing more quickly and therefore is sampled more often. Thus, we observe that for Terrain Walker WAKER-R initially focuses equally on all environments, before switching to sampling more complex terrain when simple domains have converged and therefore no longer exhibit a gradient in uncertainty.\nWAKER-M biases sampling towards the domains with the highest magnitude of uncertainty. We do not expect WAKER-M to initially focus on the simplest environments, as it is always biased towards sampling complex environments where uncertainty is high. In Figure 7, we see that for Terrain Walker WAKER-M initially (0 - 0.3M steps) samples the parameters similar to DR. From 0.3M steps onwards, WAKER-M more frequently samples complex terrain with high amplitude and short length scale. Thus, WAKER-M consistently samples complex environments with higher uncertainty. Likewise, in Figure 8 we observe that for Clean Up WAKER-M more frequently samples environments with a larger arena size and more blocks. Thus, in Clean Up WAKER-M also consistently samples complex environments with higher uncertainty.\nFor WAKER-R in Clean Up, we observe that in the early stages (0-1M steps) sampling is heavily focused on the most complex environments with larger arenas and more blocks. This indicates that uncertainty is being reduced more quickly on the more complex environments during this stage of training. As training progresses, WAKER-R gradually places less emphasis on the most complex environments, and samples the environments more evenly."
        },
        {
            "heading": "C.4 AVERAGE PERFORMANCE RESULTS",
            "text": "In Table 1 in the main results in Section 4, we presented the results for the robustness evaluation, where we averaged performance over the 10 worst runs on 100 randomly sampled environments. In Table 5 and Figure 9 we present the results from averaging over all 100 episodes. Note that we do not necessarily expect WAKER to improve over DR for this evaluation, as the DR baseline directly optimises for this exact training distribution."
        },
        {
            "heading": "D FURTHER RESULTS",
            "text": "Here, we present additional results that expand upon the results already presented in the main paper."
        },
        {
            "heading": "D.1 ADDITIONAL WORLD MODEL IMAGE PREDICTION ERRORS",
            "text": "To compare the quality of the world model predictions, we compute the errors between the images predicted by the world model, and the actual next image. For each world model, we collect 200 trajectories in each of 200 randomly sampled environments. For the results in the main paper (Figure 6), the trajectories are collected under performant task policies (for the hop task for Terrain Hopper, and for the sort task for Car Clean Up). For the results in Figures 10 and 11 the trajectories are collected under a uniform random policy.\nAlong each trajectory, we compute the next image predicted by the world model by decoding the mean of the next latent state prediction, using the decoder learned by DreamerV2. We compute the mean squared error between the image prediction and the actual next image. Then, we average the errors along the trajectory to compute the error for the trajectory. We repeat this process along all trajectories to compute the error for each trajectory. Then, we compute CVaR0.1 of these error evaluations by taking the average over the worst 10% of trajectories. These values are plotted for each domain in Figures 6, 10, and 11.\nWe observe that the CVaR of the image prediction errors are generally lower for world models trained with WAKER than DR. The difference in performance is especially large for the Clean Up and Car Clean Up domains. This mirrors the large difference in policy performance between WAKER and DR on the Car Clean Up and Clean Up domains. This verifies that WAKER is successfully able to learn world models that are more robust to different environments than DR. For Terrain Walker and Terrain Hopper, the difference in the error evaluations between methods is smaller, but we still observe that WAKER produces smaller errors than DR when using the Plan2Explore exploration policy.\nNote that in Figures 10 and 11 the error evaluation is performed using trajectories generated by a uniform random policy. Therefore, we observe that the world models trained using data collected by a random policy tend to have lower errors relative to world models trained using Plan2Explore as the exploration policy."
        },
        {
            "heading": "D.2 PERFORMANCE ON SNAPSHOTS OF WORLD MODEL TRAINED WITH VARIABLE AMOUNTS OF DATA",
            "text": "In the main results presented in Section 4, we report the performance of policies obtained from the final world model at the end of six days of training. In this section, we present the performance of policies obtained from snapshots of the world model trained with variable amounts of data. These results are presented in Figures 12, 13 and 14. In each of the plots, the x-axis indicates the number of steps of data collected within the environment that was used to train the world model. The values on the y-axis indicate the performance of policies obtained from imagined training within each snapshot of the world model.\nAs expected, we observe that as the amount of data used to train the world model increases, the performance of policies trained within the world model increases. We observe that for most levels of world model training data, and for both exploration policies, the WAKER variants improve upon DR in the robustness and OOD evaluations in Figures 12 and 13. The magnitude of the improvement tends to become larger as the amount of training data from the environment increases. This suggests that WAKER might be especially effective when scaled to larger amounts of training data.\nThe average performance of policies trained in snapshots of the world model is presented in Figure 14. For Terrain Walker and Terrain Hopper, the average performance between WAKER and DR for the same exploration policy is similar across all levels of environment data. For Clean Up and Car Cleanup, we observe that both WAKER variants improve the average performance relative to DR when using Plan2Explore as the exploration policy, for almost all levels of data. For the random exploration policy, we observe that the average performance initially improves more slowly when using WAKER, but eventually surpasses the performance of DR. This initially slower improvement in average performance with WAKER is likely due to WAKER putting increased emphasis on collecting data from more complex environments."
        },
        {
            "heading": "E WAKER IMPLEMENTATION DETAILS",
            "text": "Here, we provide additional details on our implementation of the WAKER algorithm that were omitted from the main paper due to space constraints. The code for our experiments is available at github.com/marc-rigter/waker.\nError Estimate Update and Smoothing In Line 17 of Algorithm 1 we compute the error estimate \u03b4\u03b8 according to the average ensemble disagreement over an imagined trajectory. Each \u03b4\u03b8 value is noisy, so we maintain a smoothed average \u03b4\u03b8 of the \u03b4\u03b8 values. Specifically, we use an exponential moving average (EMA). For each parameter setting \u03b8, when we receive a new \u03b4\u03b8 value, we update the EMA according to: \u03b4\u03b8 \u2190 \u03b1\u03b4\u03b8 + (1\u2212 \u03b1)\u03b4\u03b8, where we set \u03b1 = 0.9999. For WAKER-M, the error buffer Derror contains the smoothed average of the \u03b4\u03b8 values for each parameter setting: Derror = {\u03b4\u03b81 , \u03b4\u03b82 , \u03b4\u03b83 , . . .}. These values are used as input to the Boltzmann distribution used to sample the environment parameters in Line 7 of Algorithm 1.\nFor WAKER-R, we compute the reduction of the \u03b4\u03b8 values for each parameter setting between each interval of 10,000 environment steps: \u2206\u03b8 = \u03b4 old \u03b8 \u2212 \u03b4 new \u03b8 . Because the error estimates change very slowly, we perform a further smoothing of the \u2206\u03b8 values using an exponential moving average: \u2206\u03b8 \u2190 \u03b1\u2206\u2206\u03b8 + (1 \u2212 \u03b1\u2206)\u2206\u03b8, where we set \u03b1\u2206 = 0.95. For WAKER-R, the error estimate buffer contains these \u2206\u03b8 values: Derror = {\u2206\u03b81 ,\u2206\u03b82 ,\u2206\u03b83 , . . .}, and these values determine the environment sampling distribution in Line 7 of Algorithm 1.\nError Estimate Normalisation We normalize the error values in Line 7 of Algorithm 1 to reduce the sensitivity of our approach to the scale of the error estimates, and reduce the need for hyperparameter tuning between domains. For WAKER-R, we divide each \u2206\u03b8 value by the mean absolute value of \u2206\u03b8 across all parameter settings. The rationale for this form of normalisation is that if the rate of reduction of error is similar across all environments, then WAKER-R will sample the environments with approximately equal probability.\nFor WAKER-M, for each \u03b4\u03b8 value, we subtract the mean of \u03b4\u03b8 across all parameter settings, and divide by the standard deviation. This means that regardless of the scale of the error estimates, WAKER-M will always favour the environments with the highest error estimates, as motivated by Problem 2.\nWorld Model Training For the world model, we use the official open-source implementation of DreamerV2 (Hafner et al., 2021) at https://github.com/danijar/dreamerv2. For the world model training we use the default hyperparameters from DreamerV2, with the default batch size of 16 trajectories with 50 steps each. For the ensemble of latent dynamics functions, we use 10 fully-connected neural networks, following the implementation of Plan2Explore (Sekar et al., 2020) in the official DreamerV2 repository. We perform one update for every eight environment steps added to the data buffer.\nIn our implementation of Plan2Explore, each member of the ensemble is trained to predict both the deterministic and stochastic part of the next latent state. This is slightly different to the Plan2Explore implementation in the official Dreamer-v2 codebase, where the ensemble only predicts part of the latent state. We made this modification because the derivation of our algorithm indicates that we should be concerned with the uncertainty over the prediction of the entire latent state.\nPolicy Training For the exploration and task policies, we use the actor-critic implementation from the official DreamerV2 repository. During each update, we sample a batch of 16 trajectories from the data buffer of 50 steps each. For each latent state corresponding to a data sample in the batch, we generate an imagined trajectory starting from that latent state, with a horizon of 15 steps. We then update both the task and exploration policies using dynamics gradients (i.e. backpropagation through the world model) computed on these imagined trajectories. We perform one update to each of the task and exploration policies for every eight environment steps."
        },
        {
            "heading": "F EXPERIMENT DETAILS",
            "text": ""
        },
        {
            "heading": "F.1 DOMAINS AND TASKS",
            "text": "Terrain Walker and Terrain Hopper We simulate the Walker and Hopper robots from the DeepMind Control Suite (Tassa et al., 2018). The observations are images of shape 64 \u00d7 64 \u00d7 3. For each episode, we generate terrain using Perlin noise (Perlin, 2002), a standard technique for procedural terrain generation. The terrain generation is controlled by two parameters, the amplitude, with values in [0, 1], and the length scale, with values in [0.2, 2]. Domain randomisation samples both of these parameters uniformly from their respective ranges. For examples of each domain and the terrain generated, see Figures 15 and 16.\nWe evaluate 5 different downstream tasks for Terrain Walker. The tasks walk, run, stand, flip are from the URLB benchmark (Laskin et al., 2021). We also include walk-backward, which uses the same reward function as walk, except that the robot is rewarded for moving backwards rather than forwards.\nWe evaluate 3 different downstream tasks for Terrain Hopper: hop, stand and hop-backward. Hop and stand are from the Deepmind Control Suite Tassa et al. (2018). Hop-backward is the same as hop, except that the robot is rewarded for moving backwards rather than forwards.\nTerrain Walker and Terrain Hopper: Out-of-Distribution Environments We use two different types of out-of-distribution environments for the terrain environments. The first is Steep, where the length scale of the terrain is 0.15. This is 25% shorter than ever seen during training. This results in terrain with sharper peaks than seen in training. The second is Stairs, where the terrain contains stairs, in contrast to the undulating terrain seen in training. The out-of-distribution environments are shown in Figure 17 for the Walker robot. The out-of-distribution environments for Terrain Hopper are the same, except that we use the Hopper robot.\nClean Up and Car Clean Up These domains are based on SafetyGym (Ray et al., 2019) and consists of a robot and blocks that can be pushed. The observations are images from a top-down view with dimension 64\u00d7 64\u00d7 3. For Clean Up, the robot is a point mass robot. For Car Clean Up the robot is a differential drive car. In both cases, the action space is two-dimensional.\nFor each environment, there are three factors that vary: the size of the environment, the number of blocks, and the colour of the blocks. For the default domain randomisation sampling distribution, the size of the environment is first sampled uniformly from size \u2208 {0, 1, 2, 3, 4}. The number of\nblocks is then sampled uniformly from {0, 1, . . . ,size}. The number of green vs blue blocks is then also sampled uniformly. Examples of the training environments generated for the Clean Up domain are in Figure 18.\nThere are three different tasks for both Clean Up and Car Clean Up: sort, push, and sort-reversed. The tasks vary by the goal location for each colour of block. For sort, each block must be moved to the goal location of the corresponding colour. For sort-reverse, each block must be moved to the goal location of the opposite colour. For push, all blocks must be pushed to the blue goal location, irrespective of the colour of the block.\nWe define the task completion to be the number of blocks in the environment that are in the correct goal region divided by the number of blocks in the environment. If there are no blocks in the environment, then the task completion is 1. The reward function for each task is defined as follows: The agent receives a dense reward for moving any block closer to the desired goal region, and the agent also receives a reward at each time step that is proportional to the task completion.\nIn the main results we report the task completion at the end of each episode, as it is easier to interpret. We observe that the results for the total reward directly correlate to those for the task completion.\nClean Up and Car Clean Up: Out-of-Distribution Environments For the out-of-distribution environments, we place one more block in the environment than was ever seen during training. We set the size of the environment to size = 4, and there are 5 blocks in the environment. The task completion and reward function is defined the same as for the training environments. Examples of the out-of-distribution environments are in Figure 20."
        },
        {
            "heading": "F.2 LENGTH OF TRAINING RUNS",
            "text": "Due to resource limitations, each training run is limited to six days of run time. This corresponds to a total of 7.4 \u00d7 106 environment steps for the Terrain Walker and Terrain Hopper domains, and a total of 8.2 \u00d7 106 environment steps for the Clean Up and Car Clean up domains. The results reported in Section 4 are for task policies trained in imagination in the final world models at the end of these training runs."
        },
        {
            "heading": "F.3 HYPERPARAMETER TUNING",
            "text": "We use the default parameters for DreamerV2 (Hafner et al., 2021) for training the world model.\nFor WAKER, there are two hyperparameters: the probability of sampling uniformly from the default environment distribution, pDR, and the temperature parameter for the Boltzmann environment distribution, \u03b7. In our experiments, we set pDR = 0.2 for all experiments and did not tune this value. We performed limited hyperparameter tuning of the Boltzmann temperature parameter, \u03b7. We ran\nWAKER-M + Plan2Explore and WAKER-R + Plan2Explore for each of three different values of \u03b7 \u2208 {0.5, 1.0, 1.5}. For each algorithm and \u03b7 value, we ran two seeds for 5e6 environment steps on the Clean Up domain. At the end of 5e6 environment steps, we chose the value of \u03b7 that obtained the best performance for each algorithm for CVaR0.1 on the sort task. We then use this value of \u03b7 for WAKER-M and WAKER-R across all experiments, when using both the Plan2Explore and the random exploration policies.\nThe hyperparameters used in our experiments for WAKER are summarised in Table 6."
        },
        {
            "heading": "F.4 CONFIDENCE INTERVAL DETAILS",
            "text": "Figures 3, 4 and 9 present 95% confidence intervals of the probability of improvement, computed using the rliable framework (Agarwal et al., 2021). To compute these values, we first normalise the results for each algorithm and task to between [0, 1] by dividing by the highest value obtained by any algorithm. We then input the normalised scores in the rliable package to compute the confidence intervals.\nA confidence interval where the lower bound on the probability of improvement is greater than 0.5 indicates that the algorithm is a statistically significant improvement over the baseline."
        },
        {
            "heading": "F.5 COMPUTATIONAL RESOURCES",
            "text": "Each world model training run takes 6 days on an NVIDIA V100 GPU. In our experiments, we train 120 world models in total, resulting in our experiments using approximately 720 GPU days."
        },
        {
            "heading": "G BASELINE METHOD DETAILS",
            "text": "In our experiments, we compare the following methods. Note that two of the baselines (HE-Oracle and RW-Oracle) require expert domain knowledge. The other methods (WAKER, GE, and DR) do not require domain knowledge.\nDomain Randomisation (DR) DR samples environments uniformly from the default environment distribution (as described in Appendix F.1).\nHardest Environment Oracle (HE-Oracle) For this baseline, the most complex instance of the environment is always sampled. For the block pushing tasks, the most complex environment is the largest arena, containing two blocks of each colour. For the terrain tasks, the most complex environment is the terrain with the highest possible amplitude (1) and the shortest possible length scale (0.2).\nRe-weighting Oracle (RW-Oracle) RW-Oracle re-weights the environment distribution to focus predominantly on the most complex environments. 20% of the time RW-Oracle samples from the default domain randomisation distribution. The remaining 80% of the time RW-Oracle samples uniformly from the most complex environments. For the terrain environments, the most complex environments are those where the amplitude is within [0.8, 1] and the length scale is within [0.2, 0.4]. For the block pushing environments, the most complex environments are those where the size of the arena is four and there are four blocks of any colour.\nGradual Expansion (GE) 20% of the time, a new environment is sampled from the default domain randomisation distribution. The remaining 80% of the time, GE samples from the default distribution, but the default distribution is restricted to only include environments that have been seen so far. Thus, GE utilises the default domain randomisation distribution to sample new environments to gradually increase the range of environments seen during training."
        }
    ],
    "title": "REWARD-FREE CURRICULA FOR TRAINING ROBUST WORLD MODELS",
    "year": 2024
}