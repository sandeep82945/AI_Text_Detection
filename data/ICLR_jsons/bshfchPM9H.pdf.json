{
    "abstractText": "Natural Language Explanation (NLE) in vision and language tasks aims to provide human-understandable explanations for the associated decision-making process. In practice, one might encounter explanations which lack informativeness or contradict visual-grounded facts, known as implausibility and hallucination problems, respectively. To tackle these challenging issues, we consider the task of visual question answering (VQA) and introduce Rapper, a two-stage Reinforced RationalePrompted Paradigm. By knowledge distillation, the former stage of Rapper infuses rationale-prompting via large language models (LLMs), encouraging the rationales supported by language-based facts. As for the latter stage, a unique Reinforcement Learning from NLE Feedback (RLNF) is introduced for injecting visual facts into NLE generation. Finally, quantitative and qualitative experiments on two VL-NLE benchmarks show that RAPPER surpasses state-of-the-art VQA-NLE methods while providing plausible and faithful NLE.",
    "authors": [],
    "id": "SP:d466fbf01396473ffd6aa4bd6c0b54660b7ce1c9",
    "references": [
        {
            "authors": [
                "Afra Feyza Aky\u00fcrek",
                "Ekin Aky\u00fcrek",
                "Aman Madaan",
                "Ashwin Kalyan",
                "Peter Clark",
                "Derry Wijaya",
                "Niket Tandon"
            ],
            "title": "Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs",
            "venue": "arXiv preprint arXiv:2305.08844,",
            "year": 2023
        },
        {
            "authors": [
                "Peter Anderson",
                "Basura Fernando",
                "Mark Johnson",
                "Stephen Gould"
            ],
            "title": "Spice: Semantic propositional image caption evaluation",
            "venue": "In Computer Vision\u2013ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Peter Anderson",
                "Xiaodong He",
                "Chris Buehler",
                "Damien Teney",
                "Mark Johnson",
                "Stephen Gould",
                "Lei Zhang"
            ],
            "title": "Bottom-up and top-down attention for image captioning and visual question answering",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Yuntao Bai",
                "Saurav Kadavath",
                "Sandipan Kundu",
                "Amanda Askell",
                "Jackson Kernion",
                "Andy Jones",
                "Anna Chen",
                "Anna Goldie",
                "Azalia Mirhoseini",
                "Cameron McKinnon"
            ],
            "title": "Constitutional ai: Harmlessness from ai feedback",
            "venue": "arXiv preprint arXiv:2212.08073,",
            "year": 2022
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie"
            ],
            "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization",
            "year": 2005
        },
        {
            "authors": [
                "Oana-Maria Camburu",
                "Tim Rockt\u00e4schel",
                "Thomas Lukasiewicz",
                "Phil Blunsom"
            ],
            "title": "e-snli: Natural language inference with natural language explanations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Soravit Changpinyo",
                "Doron Kukliansky",
                "Idan Szpektor",
                "Xi Chen",
                "Nan Ding",
                "Radu Soricut"
            ],
            "title": "All you may need for vqa are image captions",
            "venue": "arXiv preprint arXiv:2205.01883,",
            "year": 2022
        },
        {
            "authors": [
                "Yen-Chun Chen",
                "Linjie Li",
                "Licheng Yu",
                "Ahmed El Kholy",
                "Faisal Ahmed",
                "Zhe Gan",
                "Yu Cheng",
                "Jingjing Liu"
            ],
            "title": "Uniter: Universal image-text representation learning",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Mingkai Deng",
                "Jianyu Wang",
                "Cheng-Ping Hsieh",
                "Yihan Wang",
                "Han Guo",
                "Tianmin Shu",
                "Meng Song",
                "Eric Xing",
                "Zhiting Hu"
            ],
            "title": "RLPrompt: Optimizing discrete text prompts with reinforcement learning",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Zhihao Fan",
                "Zhongyu Wei",
                "Siyuan Wang",
                "Yang Liu",
                "Xuan-Jing Huang"
            ],
            "title": "A reinforcement learning framework for natural question generation using bi-discriminators",
            "venue": "In Proceedings of the 27th International Conference on Computational Linguistics,",
            "year": 2018
        },
        {
            "authors": [
                "Zhibin Gou",
                "Zhihong Shao",
                "Yeyun Gong",
                "Yelong Shen",
                "Yujiu Yang",
                "Nan Duan",
                "Weizhu Chen"
            ],
            "title": "Critic: Large language models can self-correct with tool-interactive critiquing",
            "venue": "arXiv preprint arXiv:2305.11738,",
            "year": 2023
        },
        {
            "authors": [
                "Yash Goyal",
                "Tejas Khot",
                "Douglas Summers-Stay",
                "Dhruv Batra",
                "Devi Parikh"
            ],
            "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Jack Hessel",
                "Ari Holtzman",
                "Maxwell Forbes",
                "Ronan Le Bras",
                "Yejin Choi"
            ],
            "title": "Clipscore: A referencefree evaluation metric for image captioning",
            "venue": "arXiv preprint arXiv:2104.08718,",
            "year": 2021
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung"
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Computing Surveys,",
            "year": 2023
        },
        {
            "authors": [
                "Saurav Kadavath",
                "Tom Conerly",
                "Amanda Askell",
                "Tom Henighan",
                "Dawn Drain",
                "Ethan Perez",
                "Nicholas Schiefer",
                "Zac Hatfield-Dodds",
                "Nova DasSarma",
                "Eli Tran-Johnson"
            ],
            "title": "Language models (mostly) know what they know",
            "venue": "arXiv preprint arXiv:2207.05221,",
            "year": 2022
        },
        {
            "authors": [
                "Maxime Kayser",
                "Oana-Maria Camburu",
                "Leonard Salewski",
                "Cornelius Emde",
                "Virginie Do",
                "Zeynep Akata",
                "Thomas Lukasiewicz"
            ],
            "title": "e-vil: A dataset and benchmark for natural language explanations in vision-language tasks",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Yeganeh Kordi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Unifiedqa-v2: Stronger generalization via broader cross-format training",
            "venue": "arXiv preprint arXiv:2202.12359,",
            "year": 2022
        },
        {
            "authors": [
                "Daniel King",
                "Zejiang Shen",
                "Nishant Subramani",
                "Daniel S Weld",
                "Iz Beltagy",
                "Doug Downey"
            ],
            "title": "Don\u2019t say what you don\u2019t know: Improving the consistency of abstractive summarization by constraining beam search",
            "venue": "arXiv preprint arXiv:2203.08436,",
            "year": 2022
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "year": 2022
        },
        {
            "authors": [
                "Satyapriya Krishna",
                "Jiaqi Ma",
                "Dylan Slack",
                "Asma Ghandeharioun",
                "Sameer Singh",
                "Himabindu Lakkaraju"
            ],
            "title": "Post hoc explanations of language models can improve language models",
            "venue": "arXiv preprint arXiv:2305.11426,",
            "year": 2023
        },
        {
            "authors": [
                "Zhibin Lan",
                "Wei Li",
                "Jinsong Su",
                "Xinyan Xiao",
                "Jiachen Liu",
                "Wenhao Wu",
                "Yajuan Lyu"
            ],
            "title": "Factgen: Faithful text generation by factuality-aware pre-training and contrastive ranking fine-tuning",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi"
            ],
            "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597,",
            "year": 2023
        },
        {
            "authors": [
                "Xiujun Li",
                "Xi Yin",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Xiaowei Hu",
                "Lei Zhang",
                "Lijuan Wang",
                "Houdong Hu",
                "Li Dong",
                "Furu Wei"
            ],
            "title": "Oscar: Object-semantics aligned pre-training for vision-language tasks",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Yuang Li",
                "Yu Wu",
                "Jinyu Li",
                "Shujie Liu"
            ],
            "title": "Prompting large language models for zero-shot domain adaptation in speech recognition",
            "venue": "arXiv preprint arXiv:2306.16007,",
            "year": 2023
        },
        {
            "authors": [
                "Chin-Yew Lin"
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "In Text summarization branches out,",
            "year": 2004
        },
        {
            "authors": [
                "Zhihong Lin",
                "Donghao Zhang",
                "Qingyi Tao",
                "Danli Shi",
                "Gholamreza Haffari",
                "Qi Wu",
                "Mingguang He",
                "Zongyuan Ge"
            ],
            "title": "Medical visual question answering: A survey",
            "venue": "Artificial Intelligence in Medicine,",
            "year": 2023
        },
        {
            "authors": [
                "Feng Liu",
                "Tao Xiang",
                "Timothy M Hospedales",
                "Wankou Yang",
                "Changyin Sun"
            ],
            "title": "ivqa: Inverse visual question answering",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Jiacheng Liu",
                "Skyler Hallinan",
                "Ximing Lu",
                "Pengfei He",
                "Sean Welleck",
                "Hannaneh Hajishirzi",
                "Yejin Choi"
            ],
            "title": "Rainier: Reinforced knowledge introspector for commonsense question answering",
            "venue": "arXiv preprint arXiv:2210.03078,",
            "year": 2022
        },
        {
            "authors": [
                "Jiacheng Liu",
                "Alisa Liu",
                "Ximing Lu",
                "Sean Welleck",
                "Peter West",
                "Ronan Le Bras",
                "Yejin Choi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Generated knowledge prompting for commonsense reasoning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "year": 2022
        },
        {
            "authors": [
                "Jiaying Lu",
                "Xin Ye",
                "Yi Ren",
                "Yezhou Yang"
            ],
            "title": "Good, better, best: Textual distractors generation for multiple-choice visual question answering via reinforcement learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ximing Lu",
                "Sean Welleck",
                "Jack Hessel",
                "Liwei Jiang",
                "Lianhui Qin",
                "Peter West",
                "Prithviraj Ammanabrolu",
                "Yejin Choi"
            ],
            "title": "Quark: Controllable text generation with reinforced unlearning",
            "venue": "Advances in neural information processing systems,",
            "year": 2022
        },
        {
            "authors": [
                "Bodhisattwa Prasad Majumder",
                "Oana-Maria Camburu",
                "Thomas Lukasiewicz",
                "Julian McAuley"
            ],
            "title": "Knowledge-grounded self-rationalization via extractive and natural language explanations",
            "venue": "arXiv preprint arXiv:2106.13876,",
            "year": 2021
        },
        {
            "authors": [
                "Ana Marasovi\u0107",
                "Chandra Bhagavatula",
                "Jae sung Park",
                "Ronan Le Bras",
                "Noah A. Smith",
                "Yejin Choi"
            ],
            "title": "Natural language rationales with full-stack visual reasoning: From pixels to semantic frames to commonsense graphs",
            "venue": "In Findings of the Association for Computational Linguistics:",
            "year": 2020
        },
        {
            "authors": [
                "Sewon Min",
                "Xinxi Lyu",
                "Ari Holtzman",
                "Mikel Artetxe",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Rethinking the role of demonstrations: What makes in-context learning work",
            "venue": "arXiv preprint arXiv:2202.12837,",
            "year": 2022
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "Wei-Jing Zhu"
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "In Proceedings of the 40th annual meeting of the Association for Computational Linguistics,",
            "year": 2002
        },
        {
            "authors": [
                "Dong Huk Park",
                "Lisa Anne Hendricks",
                "Zeynep Akata",
                "Anna Rohrbach",
                "Bernt Schiele",
                "Trevor Darrell",
                "Marcus Rohrbach"
            ],
            "title": "Multimodal explanations: Justifying decisions and pointing to the evidence",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Steven J Rennie",
                "Etienne Marcheret",
                "Youssef Mroueh",
                "Jerret Ross",
                "Vaibhava Goel"
            ],
            "title": "Self-critical sequence training for image captioning",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Fawaz Sammani",
                "Tanmoy Mukherjee",
                "Nikos Deligiannis"
            ],
            "title": "Nlx-gpt: A model for natural language explanations in vision and vision-language tasks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Ramprasaath R Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Joe Stacey",
                "Pasquale Minervini",
                "Haim Dubossarsky",
                "Oana-Maria Camburu",
                "Marek Rei"
            ],
            "title": "Logical reasoning for natural language inference using generated facts as atoms",
            "venue": "arXiv preprint arXiv:2305.13214,",
            "year": 2023
        },
        {
            "authors": [
                "Alane Suhr",
                "Mike Lewis",
                "James Yeh",
                "Yoav Artzi"
            ],
            "title": "A corpus of natural language for visual reasoning",
            "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Wei Suo",
                "Mengyang Sun",
                "Weisong Liu",
                "Yiqi Gao",
                "Peng Wang",
                "Yanning Zhang",
                "Qi Wu"
            ],
            "title": "S3c: Semi-supervised vqa natural language explanation via self-critical learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Miles Turpin",
                "Julian Michael",
                "Ethan Perez",
                "Samuel R Bowman"
            ],
            "title": "Language models don\u2019t always say what they think: Unfaithful explanations in chain-of-thought prompting",
            "venue": "arXiv preprint arXiv:2305.04388,",
            "year": 2023
        },
        {
            "authors": [
                "Ramakrishna Vedantam",
                "C Lawrence Zitnick",
                "Devi Parikh"
            ],
            "title": "Cider: Consensus-based image description evaluation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Leandro von Werra",
                "Younes Belkada",
                "Lewis Tunstall",
                "Edward Beeching",
                "Tristan Thrush",
                "Nathan Lambert"
            ],
            "title": "Trl: Transformer reinforcement learning",
            "venue": "https://github.com/lvwerra/trl,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Wang",
                "Kyunghyun Cho",
                "Mike Lewis"
            ],
            "title": "Asking and answering questions to evaluate the factual consistency of summaries",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5008\u20135020,",
            "year": 2020
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Jialin Wu",
                "Raymond J. Mooney"
            ],
            "title": "Faithful multimodal explanation for visual question answering",
            "venue": "ArXiv, abs/1809.02805,",
            "year": 2018
        },
        {
            "authors": [
                "Jialin Wu",
                "Raymond J Mooney"
            ],
            "title": "Faithful multimodal explanation for visual question answering",
            "venue": "arXiv preprint arXiv:1809.02805,",
            "year": 2018
        },
        {
            "authors": [
                "Ning Xie",
                "Farley Lai",
                "Derek Doran",
                "Asim Kadav"
            ],
            "title": "Visual entailment: A novel task for fine-grained image understanding",
            "year": 1901
        },
        {
            "authors": [
                "Cheng-Fu Yang",
                "Yao-Hung Hubert Tsai",
                "Wan-Cyuan Fan",
                "Russ R Salakhutdinov",
                "Louis-Philippe Morency",
                "Frank Wang"
            ],
            "title": "Paraphrasing is all you need for novel object captioning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Zhengyuan Yang",
                "Linjie Li",
                "Jianfeng Wang",
                "Kevin Lin",
                "Ehsan Azarnasab",
                "Faisal Ahmed",
                "Zicheng Liu",
                "Ce Liu",
                "Michael Zeng",
                "Lijuan Wang"
            ],
            "title": "Mm-react: Prompting chatgpt for multimodal reasoning and action",
            "venue": "arXiv preprint arXiv:2303.11381,",
            "year": 2023
        },
        {
            "authors": [
                "Qinghao Ye",
                "Haiyang Xu",
                "Guohai Xu",
                "Jiabo Ye",
                "Ming Yan",
                "Yiyang Zhou",
                "Junyang Wang",
                "Anwen Hu",
                "Pengcheng Shi",
                "Yaya Shi"
            ],
            "title": "mplug-owl: Modularization empowers large language models with multimodality",
            "venue": "arXiv preprint arXiv:2304.14178,",
            "year": 2023
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Hai Zhao",
                "George Karypis",
                "Alex Smola"
            ],
            "title": "Multimodal chain-of-thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2302.00923,",
            "year": 2023
        },
        {
            "authors": [
                "Haiyan Zhao",
                "Hanjie Chen",
                "Fan Yang",
                "Ninghao Liu",
                "Huiqi Deng",
                "Hengyi Cai",
                "Shuaiqiang Wang",
                "Dawei Yin",
                "Mengnan Du"
            ],
            "title": "Explainability for large language models: A survey",
            "venue": "arXiv preprint arXiv:2309.01029,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep learning has achieved remarkable success in vision-language (VL) tasks such as visual reasoning (Suhr et al., 2017), visual question answering (VQA, Goyal et al., 2017), and visual entailment (Xie et al., 2019). Take VQA as an example, while these models exhibit impressive ability in inferring answer descriptions from the given image-question pairs, its decision-making process remains an unsolved problem. As a result, such a black-box manner severely restricts their applicability in certain real-world scenarios (e.g., medical VQA, Lin et al., 2023), where the interpretability of the learning model is crucial for establishing trustworthy systems. To tackle this long-standing challenge, some approaches adopt attention mechanisms (Anderson et al., 2018) or gradient-based activations (Selvaraju et al., 2017), focusing on highlighting image regions which are relevant to the associated prediction. However, such visual explanations might not be desirable for VL tasks (e.g., those beyond classification) due to the lack of reasoning process (Kayser et al., 2021; Sammani et al., 2022). As a result, Natural Language Explanation (NLE) has emerged as a potential alternative, which aims to interpret the underlying reasoning process by natural language descriptions.\nTo extend NLE for vision-language tasks (i.e., VL-NLE), Park et al. (2018) and Kayser et al. (2021) introduced the benchmarks for explaining the decision-making process with NLEs for VQA and visual entailment tasks, respectively. Subsequent VL-NLE works have evolved into two research lines. The first research line (Park et al., 2018; Marasovic\u0301 et al., 2020) focuses on how to improve their pipeline from an architecture perspective for training NLE generators within a fully supervised learning manner. On the other hand, Sammani et al. (2022) and Suo et al. (2023) emphasize the utilization of unlabeled pre-training data to enhance the language models\u2019 NLE capability.\nDespite significant advancements, most existing VL-NLE works require training in a full supervised manner. They might encounter problems where the explanations are irrelevant to the questions or contradictory to the established supporting facts (Majumder et al., 2021). The other potential concern is that the explanation is not related to the visual image (Ji et al., 2023). More specifically, the former problem is referred to as implausibility, while the latter is known as hallucination. Take visual input and question in Fig. 1 as an example, \u201cBecause there is a tower.\u2019 is an implausible explanation since it is irrelevant to question, and \u201cBecause the sun is big.\u201d is a hallucinated one since\nthe sun is not visible in the image. Although these issues have been recently studied in the NLE community (Zhao et al., 2023; Turpin et al., 2023), they remain unexplored in the field of VL-NLE. As a result, generating plausible yet faithful NLEs for elucidating vision-language models continues to pose a crucial challenge.\nRecently, rationale-based prompting techniques have been manifested to improve the capability of Large Language Models (LLMs) on complex reasoning tasks (Wei et al., 2022; Liu et al., 2022b). Such techniques involve elicitation of rationales from LLMs, producing knowledge-riched or factbased intermediate to facilitate the reasoning capability of language model. Thus, these prompting manners are emerging as promising solutions for NLE (Zhao et al., 2023; Krishna et al., 2023). These rationale-prompting paradigms have been further extended to multi-modal regimes such as mm-CoT (Zhang et al., 2023) and mm-ReAct (Yang et al., 2023). However, mm-CoT (Zhang et al., 2023) relies on the ground-truth rationales for training, while mm-ReAct (Yang et al., 2023) have potential hallucinated outputs due to the information loss when converting visual signals into text for ChatGPT API call understanding.\nIn this paper, we propose Reinforced Rationale-Prompted Paradigm (Rapper) for providing accurate answers for VQA with sufficient NLE, which are plausible and faithful. As depicted in Fig. 1(b), our Rapper learns to exploit knowledge learned from LLM and incorporate the corresponding visual content from input images into rationales through two stages. Without observing any ground truth rationale during training, the first stage utilizes a knowledge distillation process to introduce LLM for enriching the rationales with supporting facts, encouraging NLE to be factual and plausible. The subsequent stage of Reinforcement Learning from NLE Feedback (RLNF) further exploits the answer-explanation feedback to enforce the produced rationales associated with both question and visual inputs, allowing faithful NLE.\nWe now summarize the contributions of this work below:\n\u2022 A reinforced rationale-prompted paradigm, Rapper, is proposed for plausible and faithful NLE generation in VQA. This is achieved through two proposed stages: knowledge distillation process from LLM and Reinforcement Learning from NLE Feedback (RLNF).\n\u2022 In Rapper, we first advance LLM and perform knowledge distillation. This results in predicted rationales are based on language-based facts, which prompt the VQA model for plausible NLE.\n\u2022 To align NLE with the visual input, we introduce Reinforcement Learning from NLE Feedback (RLNF) to Rapper, which utilizes the answer-explanation feedback as rewards and prompts the VQA model with predicted rationales for faithful NLE.\n\u2022 Our Rapper achieves new state-of-the-art performance for both VQA-X (Park et al., 2018) and eSNLI-VE (Kayser et al., 2021) on NLE generation. We also demonstrate that Rapper outperforms existing VQA-NLE works with reduced implausibility and hallucination."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Plausible and Faithful Natural Language Explanation Research on plausibility and faithfulness in NLE (Majumder et al., 2021; King et al., 2022; Gou et al., 2023; Stacey et al., 2023) has garnered wide attention, particularly due to the evolution of Large Language Models (LLMs) and chain-ofthought (CoT) prompting techniques (Wei et al., 2022). Notably, the method of integrating external knowledge databases for fact generation or retrieval has been proven effective in enhancing the plausibility and faithfulness of NLEs (Majumder et al., 2021; Stacey et al., 2023). Based on this advancement, some recent approaches, such as the verify-then-correct pipeline by Gou et al. (2023) and novel decoding strategies proposed by Lan et al. (2023) and King et al. (2022), aim to mitigate hallucination in textual outputs. However, these works typically focus on isolated single text modality or rely on static external knowledge databases, limiting its scalability to multimodal data.\nNatural Language Explanation for Vision-Language Tasks Most existing VL-NLE works (Wu & Mooney, 2018a; Park et al., 2018; Marasovic\u0301 et al., 2020; Kayser et al., 2021) generate explanations in a predict-then-explain fashion. Specifically, an answer is first predicted by a pre-trained VL model (e.g., UNITER (Chen et al., 2020) or Oscar (Li et al., 2020)), followed by the generation of the corresponding explanation via a separate language decoder (e.g., GPT2 (Radford et al., 2019)). As the answer and explanation are predicted separately, the explanation often contains irrelevant or contradictory descriptions of the given visual information, struggling to faithfully represent the underlying reasoning process. Recently, NLX-GPT (Sammani et al., 2022) proposes to jointly generate the answer and explanation by a unified sequence-to-sequence model, while S3C (Suo et al., 2023) further enforces the explanation to be consistent with the predicted answer. Although the above approaches have been shown to mitigate the hallucination issue, it is not clear how their NLE is established upon supporting facts or taking the visual input into consideration. Therefore, how to tackle the potential implausibile or hallucinated NLE remains a challenging task.\nReinforcement Learning for Language Models Several research works have explored RL and view it as the key component to enhance models across vision-language tasks such as image captioning (Rennie et al., 2017), novel object captioning (NOC) (Yang et al., 2022), and VQA (Lu et al., 2022a; Fan et al., 2018; Liu et al., 2018). There has been a concentrated effort to align LMs with natural language (NL) feedback (Aky\u00fcrek et al., 2023; Yang et al., 2022; Liu et al., 2022a) as well as non-NL feedback (Bai et al., 2022; Lu et al., 2022b). For example, Liu et al. (2022a) utilizes the probability of the correct answer as a reward to stimulate an auxiliary module to produce beneficial knowledge, thereby enhancing QA-task performance. Similarly, Yang et al. (2022) employs a CIDEr optimization strategy to enhance the caption with sufficiently visual fidelity in the task of novel object captioning. Despite of their effectiveness, their RL framework or NL-feedback approaches cannot be easily applied for VL-NLE tasks."
        },
        {
            "heading": "3 PROPOSED METHOD",
            "text": ""
        },
        {
            "heading": "3.1 PROBLEM FORMULATION",
            "text": "Given a VQA input X = (V,Q) consisting of an input image V and a textual input Q (i.e., question), our goal is to predict the answer A\u0302 and the corresponding explanation E\u0302 (denoted as Y\u0302 = (A\u0302, E\u0302)) via a reasoning module M . In order to encourage M to provide corrent answer with plausible and faithful explanation, we propose a Reinforced Rationale-Prompted Paradigm (Rapper) scheme, which learns an additional rationale generator G to jointly exploit the supporting facts from LLMs and the visual content observed from the conditioned image into rationales. Note that only the ground truth A and E are available during training, not the rationales. As depicted in Fig. 2, the learning of Rapper is decomposed into: (A) Knowledge Distillation from LLM (Sec. 3.2), and (B) Reinforcement learning from NLE Feedback (RLNF) (Sec. 3.3), which trains rationale generator G for providing auxiliary intermediates when predicting Y\u0302 = (A\u0302, E\u0302)."
        },
        {
            "heading": "3.2 PLAUSIBLE NLE GENERATION",
            "text": "Since VL-NLE models typically rely on ground truth answers and explanations for training, it is not clear whether the underlying visual and language knowledge are exploited to support the predicted\noutputs. In the first stage of Rapper, we propose to leverage powerful reasoning capability inherent in LLM for plausible NLE generation. As depicted in Fig. 2(A), we propose to learn a rationale generator G by utilizing knowledge distillation from LLM (e.g., LLaMA-65B (Touvron et al., 2023)). This would have the reasoning module M elaborate the conditioned rationales before answering and explaining and encourage plausible NLE. We now detail this learning stage."
        },
        {
            "heading": "3.2.1 KNOWLEDGE DISTILLATION FOR FACTED-BASED RATIONALE GENERATION",
            "text": "With the recent success of LLMs showing great capability for generating rationale prompts as intermediate reasoning steps and knowledge (Wei et al., 2022; Kojima et al., 2022; Liu et al., 2022b) for reasoning task, we propose to advance the guidance of pre-trained LLMs to acquire such knowledge, so that supporting facts or knowledge can be exploited and serve as rationales for VLNLE. Since no ground-truth rationales are available, we leverage the LLM to produce rationales as pseudo ground truth for training our rationale generator G. Inspired by Liu et al. (2022a;b) and Min et al. (2022), we elicit pseudo rationale rp from LLM with a task-specific set of few-shot demonstrations (see Sec. A.4 for details) as follows:\nRp = {rp | rp \u223c PLLM(y, q)}, (1)\nwhere y is the ground-truth answer-explanation pair, q is question, PLLM denotes the LLM in an autoregressive manner, rp is the sampled pseudo rationale from PLLM, and thus Rp is the set of all rp.\nHowever, the above pseudo rationales may be redundant, noisy or lengthy, which would not be desirable for subsequent NLE tasks (Li et al., 2023b). Thus, we apply a post-processing mechanism to filter pseudo rationales Rp to R\u2032p. To be specific, we apply a round-trip consistency by answering the input question on the pseudo rationales with a pre-trained question-answering (QA) model F 1. The pseudo rationale is retained when the matching score between the ground-truth answer and the\n1In the implementation, we follow (Changpinyo et al., 2022) and use UnifiedQA (Khashabi et al., 2022) as the pre-trained QA model.\nanswer predicted by F exceeds a predetermined threshold \u03c4 . This matching score is quantified with the token-level F1 score (Wang et al., 2020). Thus, the process of collecting the filtered pseudo rationales R\u2032p is formulated as follows:\nR\u2032p = {rp | F1-score(a\u0303, a) \u2265 \u03c4, a\u0303 \u223c PF(Q, rp), rp \u2208 Rp}, (2)\nwhere a is the ground truth answer, a\u0303 is the answer predicted by F based on the pseudo rationale, and PF denotes the pre-trained QA model F in an autoregressive fashion.\nWith the above R\u2032p serving as psuedo ground truth, we are able to train the rationale generator G with the distillation loss LG described below:\nLG = \u2212 T\u2211\nt=1\nlog(pG(r \u2032 p,t|r\u2032p,0:t\u22121, x)), (3)\nwhere r\u2032p \u2208 R\u2032p, T = |r\u2032p|, and x = {v, q} \u2208 X ."
        },
        {
            "heading": "3.2.2 PROMPTING BY FACT-BASED RATIONALE FOR PLAUSIBLE NLE",
            "text": "With rationales R\u2032p better aligned with the facts, we can proceed to the training of the reasoning module M for NLE generation. We note that, since rationales R\u2032p are in the form of natural language, our the reasoning module M (which is also based on visual-language model) would be able to interpret them. Thus, in addition to the image-question pair X as the inputs to the reasoning module M , the derived pseudo rationales R\u2032p are further viewed as input prompts, which provide fact-supporting conditions when training M to perform VL-NLE. As a result, we train M by calculating the reasoning loss LM as follows:\nLM = \u2212 T\u2211\nt=1\nlog(pM (yt|y0:t\u22121, r\u2032p, x)). (4)\nIn the above cross-entropy loss, y = [a; e] \u2208 Y is the concatenation of the ground-truth answer a and explanation e."
        },
        {
            "heading": "3.3 FAITHFUL NLE GENERATION",
            "text": "Although the above knowledge distillation process based on LLM introduces plausibility into our rationale generation, the predicted rationales might not be related to the visual input and thus encounter the hallucination problem. To tackle this issue, we introduce a novel technique of Reinforcement Learning from NLE Feedback (RLNF). This learning strategy is to encourage the rationale generator G to fully exploit multimodal input data, so that the output rationales are not only plausible but also faithful. Once G produces faithful rationales, we can fine-tune the reasoning module M for plausible yet faithful NLE."
        },
        {
            "heading": "3.3.1 RLNF FOR INJECTING VISUAL FACTS",
            "text": "To address the potential hallucination issue, we propose Reinforcement Learning from NLE Feedback (RLNF) by enforcing rationale generator G to derive the visual facts from the input image into rationales. To achieve this, we define a reward function via RL that penalizes the fact-based but hallucinated rationales R\u2032, while rewarding the rationales R that contain both established facts and visual content, as depicted in Fig. 2(B). To achieve this, we design our reward r total to be the addition of answer scores r ans and the explanation score r exp, which are the average predicted probability of the ground-truth answer and CIDEr score (Vedantam et al., 2015), respectively. For the answer score, inspired by and following Kadavath et al. (2022), we maximize the answer score to assess the faithfulness of the predicted explanation. This maximization enforces the rationale generator G to inject more visual content into the rationale because the reasoning module M need more visual clues to correctly answer the question. Therefore, this process transform R\u2032 to R, and simultaneously provide the M with more visual fact-based rationale R to enable the explanation with sufficient faithfulness. On the other hand, the explanation score r exp is (i.e., specifically CIDEr\nAlgorithm 1 Training RAPPER Input: Rationale generator G, reasoning module M , LLM PLLM and pre-trained QA model PF Data: Image-question pairs X = {xi}Ni=1, and answer-explanation pairs Y = {yi}Ni=1\n/* Stage(A): KD for Plausible NLE Generation */ Rp \u2190 Collect pseudo rationales (Eq. equation 1); R\u2032p \u2190 Get filtered pseudo rationales from Rp (Eq. equation 2); \u25b7 Section 3.2.1 G\u2190 Update G with LG (Eq. equation 3); M \u2190 Update M with LM (Eq. equation 4); \u25b7 Section 3.2.2 /* Stage(B): RLNF for Faithful NLE Generation */ G\u2190 Update G with Rtotal (Eq. equation 8); \u25b7 Section 3.3.1 M \u2190 Update M with LM (Eq. equation 10); \u25b7 Section 3.3.2\nOutput: G\u03b8 , M\u03d5\nscore) to maintain the plausibility of NLE after the first training stage. As a result, the reward r total is formulated as follows:\nr total(x, a, e, e\u0302, r) = r ans(a, x, r) + r exp(e, e\u0302), (5) r ans(a, x, r) = Z(PM\u03d5(a | x, r)), (6)\nr exp(e, e\u0302) = Z(CIDEr(e, e\u0302)), (7) where x = {v, q} is the input image-question pair, a denotes the ground-truth answer, e denotes the ground-truth explanation, e\u0302 is the predicted explanation from M , and r \u2208 R is the sampled rationales from G. Notably, Z is an input-specific normalization function that follows Deng et al. (2022) to normalize reward for stabilizing the RL training process.\nRLNF Formulation Our RLNF employs Proximal Policy Optimization (PPO) (Schulman et al., 2017) as the RL algorithm. As the policy model updated, the rationale generator G is to maximize the following reward Rtotal:\nmax{Rtotal(x, a, e, e\u0302, r)}, r \u223c T\u220f\nt=1\nPG(wt|w<t), (8)\nwhere r = {wi}Ti=0, T = |r|, and x = {v, q}. However, we need to ensure the generated rationales are understandable by humans and do not deviate too far from the distilled knowledge. To achieve this, we add a KL penalty term between the learned policy \u03b8 and the initial policy \u03b8init after the knowledge distillation phase. Therefore, the overall reward is defined as:\nRtotal(x, a, e, e\u0302, r) = r total(x, a, e, e\u0302, r)\u2212 \u03b1 log pG(r|x; \u03b8) pG(r|x; \u03b8init) , (9)\nwhere Rtotal(x, a, e, e\u0302, r) is the reward in Eq. 5."
        },
        {
            "heading": "3.3.2 PROMPTING BY VISUAL-FACT-BASED RATIONALE FOR FAITHFUL NLE",
            "text": "Once the rationale generator G is trained with the introduced RLNF, it is encouraged to produce visual fact-based rationales R that are encapsulated with established facts and visual content from visual input. Again, wince R are natural language prompts, they are inherently interpretable by our reasoning module M . Therefore, for the given image-question pairs X , we utilize R as part of input prompts during the reasoning process of M . This ensures the NLEs from M retain plausibility because of the established supporting facts lies in R, together with the enhanced faithfulness because of the derived visual content embedded in R. We optimize M to achieve this with the reasoning loss LM defined as follows:\nLM = \u2212 T\u2211\nt=1\nlog(pM (yt|y0:t\u22121, r, x)), (10)\nwhere r \u2208 R, x = {v, q} \u2208 X , and y = [a; e] \u2208 Y , which is the concatenated ground-truth answer a and explanation e sequence.\nTherefore, through the complete Rapper training process as outlined in Algorithm 1, VL-NLE tasks would be successfully enabled with adequate plausibility and faithfulness."
        },
        {
            "heading": "3.4 INFERENCE",
            "text": "At inference time, for a given input image-question pair x \u2208 X , we first generate rationale r on the fly from the rationale generator G:\nr = {wi | wi \u223c PG(w<i | x); i = 0, . . . , n},\nwhere r = {wi}ni=0 is the sampled rationale, n = |r|, and x = {v, q}. Subsequently, we prompt the reasoning module M by concatenating the predicted rationale r\u0302 with the image-question pair x for outputting the final answer and explanation sequence y\u0302. This can be formulated as:\ny\u0302 = [a\u0302; e\u0302]\n= {zi | zi \u223c PM (z<i | x, r); i = 0, . . . ,m},\nwhere m = |y\u0302|, and y\u0302 = {zi}mi=0 is the concatenated answer and explanation sequence, denoted as [a\u0302; e\u0302]."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 DATASET AND SETUP",
            "text": "We follow (Kayser et al., 2021; Sammani et al., 2022; Suo et al., 2023) and consider two VL-NLE datasets. VQA-X (Park et al., 2018) builds upon VQAv2 dataset (Goyal et al., 2017). It is composed of 32.3K samples, divided into 29K for training, 1.4K for validation, and 1.9K for testing. The dataset OF e-SNLI-VE (Kayser et al., 2021) builds upon e-SNLI dataset (Camburu et al., 2018), consisting of 43K image-hypothesis pairs, divided into 40K for training, 1.4K for validation, and 1.6K for testing.\nRapper is consists of a rationale generator G and a reasoning module M , are both initialized from the pretrained image captioning model (Li et al., 2023a). The LLM for knowledge distillation during stage(A) is LLaMA-65B (Touvron et al., 2023). More implementation details are shown in Sec. A.1."
        },
        {
            "heading": "4.2 EVALUATION METRICS",
            "text": "For NLE evaluation, we use BLEU@N (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005), ROUGE-L (Lin, 2004), CIDEr (Vedantam et al., 2015), and SPICE (Anderson et al., 2016) as the metrics, while using VQA accuracy to evaluate predicted answers. To evaluate the degree of plausibility and faithfulness of explanations, we measure them with CIDEr/SPICE and RefCLIPScore Hessel et al. (2021), respectively.\nPlausibility To quantitatively evaluate explanation plausibility, we employ CIDEr and SPICE scores. CIDEr measures the similarity between the generated explanation and human-written ground truth sentences, capturing human consensus by introducing tf-idf weight (Vedantam et al., 2015). On the other hand, SPICE converts sentences into semantic scene graphs, allowing evaluation to break grammatical constraints and thus closely resembling human judgment (Anderson et al., 2016).\nFaithfulness We adopt RefCLIPScore, which computes the harmonic mean of CLIPScore (Hessel et al., 2021) and maximal reference cosine similarity, thereby encapsulating the correlation between the explanation and its reference. As noted by Hessel et al. (2021), RefCLIPScore surpasses prior metrics in correlating with human judgment for hallucination detection."
        },
        {
            "heading": "4.3 QUANTITATIVE ANALYSIS",
            "text": "NLE evaluation. In Table 1, Table 4, and Table 5, we demonstrate that Rapper outperform previous state-of-the-art methods in NLE-related metrics on both VQA-X and e-SNLIV-VE datasets with\n\"filtered\" and \"unfiltered\" settings. For the filtered setting mentioned in Table 1 considers the explanations that are associated with correct answers, Rapper achieves new SOTA performance across all NLE metrics on both datasets of Rapper to generate better explanations. Note that under the unfilter setting, Rapper still achieve SOTA prefromances in all metrics, as detailed in Table 4 and Table 5 in Appendix A.2.\nPlausibility & faithfulness of NLE. We assess plausibility by using CIDEr and SPICE, and evaluate NLE faithfulness with RefCLIPScore (Hessel et al., 2021). Results in Table 1 indicate Rapper\u2019s superiority in NLE metrics over existing VL-NLE methods, underscoring its superiority in generating plausible explanations.\nIn Table 2, we evaluate NLE faithfulness by comparing Rapper to prior SOTA methods and our stageablated approaches. Rapper\u2019s superior RefCLIPScore indicates fewer hallucinations and increased faithfulness over other VL-NLE works. Although\nthe RefCLIPScore of Rapper (w/o RLNF) is lower due to knowledge distillation introducing hallucinations. Nonetheless, Rapper still successfully to reduce hallucination after the RLNF. This demonstrated the effectiveness of our proposed RLNF to enable the model to generate faithful NLEs.\nAblation on the proposed stages. In top of Table 3, we evaluate our two-stage approach: (A) KD from LLM and (B) RL from NLE Feedback. Compared to the Rapper baseline without KD and\nRLNF, our method enhances explanation plausibility and faithfulness, highlighting the importance of both stages.\nAblation on the filtering mechanism. In bottom of Table 3, our filtering mechanism in knowledge distillation outperforms the baseline Rapper without filtering, highlighting its ability to filter lowquality pseudo rationales (i.e., redundant and noisy, etc.) and improve performance across metrics.\nAblation studies of derived rationales. In Fig. 4, we assess the quality of derived rationale on the VQA-X datasets using VQA accuracy. Rationales come from two stages: KD only R\u2032, KD+RLNF R, and a baseline of None. We test if mPLUG-Owl (Ye et al., 2023), a multimodal large language model, can answer accurately with given a pair (image, question, or x \u2208 (none, R\u2032, R)). Notably, we find that rationale quality improves progressively as we implement the stages we have proposed. This underscores the effectiveness of our designed stages in enhancing rationale quality.\n4.4 QUALITATIVE EVALUATION\nIn Fig.3, we compare NLX-GPT(Sammani et al., 2022), S3C (Suo et al., 2023), and our Rapper on the VQA-X dataset. Rapper consistently produces more plausible explanations. For example, Fig.3(a) highlights ability of Rapper to derive visual facts, such as identifying a single object on the table, surpassing previous methods that might produce hallucinated explanations. Similarly, in Fig.3(b), Rapper offers plausible explanations like recognizing Asian writing, contrasting with the implausible outputs of prior methods. Additional results and ablation studies are in Appendix A.3."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we proposed Rapper, a two-stage Reinforced Rationale-Prompted Paradigm for enabling NLE with sufficient plausible and faithful properties. Our Rapper uniquely distills language-based knowledge from LLM and utilizes RL with natural language feedback from the VQA task, so that the\ndesigned rationale generator is able to produce rationales with the aforementioned desirable properties. By prompting such predicted rationales into the reasoning module, we demonstrated that satisfactory VQA performances can be achieved. Compared to SOTA VQA-NLE methods, possible implausible or hallucinated explanations can be mitigated by our Rapper."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 IMPLEMENTATION DETAILS\nGiven an image-question pair x = (v, q) \u2208 X , the rationale generator G first generate rationales r \u2208 R that contains both rich knowledge and visul-grounded facts. The ground-truth answerexplanation pair y = (a, e) \u2208 Y is available. Subsequently, conditioned on these rationales as well as the image-question pair, the reasoning module is enable to generate the answer a\u0302 \u2208 A as well as NLEs e\u0302 \u2208 E that are sufficiently plausible and faithful. Specifically, our approach follows a series of steps outlined in Algorithm 1. We use 8 V100 GPUs to perform the above training algorithms.\nStage(A): KD from LLM In Sec. 3.2.1, we gather pseudo rationales for each image-question pairs using in-context learning to prompt LLaMA-65B (Touvron et al., 2023). To ensure the quality of these pseudo rationales, we employ UnifiedQA (Khashabi et al., 2022) for filtering, keeping only those rationales whose predicted answers have a token-level F1 score surpassing a threshold \u03c4 (follow Changpinyo et al. (2022) to set it to 0.54 manually).Next, proceeding to Sec. 3.2.2, we train rationale generator G for 10 epochs using the distillation loss LG. The input contains a image and a input textual template, formed by concatenating the question with the filtered pseudo rationale, represented as [Question: {q} Rationale: {r\u2032p}]. The ground-truth label template is [r\u2032p]. Training settings include a total batch size of 128, a learning rate of 3e-5, and a weight decay of 0.95.\nProceeding to Sec. 3.2.2, we train the reasoning module M with the reasoning loss LM for 15 epochs. Similar to the rationale generator, the input template includes the concatenated question and pseudo rationale, which is formulated as [Question: {q} Rationale: {r\u2032p} Answer: {a\u0302; e\u0302}], and the ground truth label template is {a; e}.\nStage(B): RL for NLE feedback In Sec. 3.3.1, we apply RLNF to continue to train the rationale generator G. For the RL experimental settings, we follow von Werra et al. (2020), and use their default PPO hyperparameter setting to train for 10 epochs with a batch size of 128. The rationale generator G and reasoning module M both use greedy search to sample the rationales, answers, and explanations for RL optimization.\nFinally, in Sec. 3.3.2, we continue to train the reasoning module M for 10 epochs with the same loss LM and similar training parameters. The input contains a image and a input template which involves the concatenated question q, our predicted rationale r\u0302, and ground-truth answer e and explanation e, which is formulated as: [Question: {q} Rationale: {r\u0302} Answer: {a; e}], and the ground truth label template is {a; e}. During the training period, the rationale generator G samples the rationales on the fly by beam search decoding with a beam size of 5. During the evaluation period, both G and M generate rationales and answer-explanation pairs by beam search decoding with a beam size of 5.\nA.2 UNFILTERED QUANTATIVE RESULTS\nAs demonstrated in Table 4 and Table 5, RAPPER significantly outperforms existing VL-NLE methods across all metrics on both VQA-X and e-SNLI-VE datasets. It\u2019s notable that Rapper surpasses the second-best results by 11.6 and 12.6 in CIDEr on VQA-X and e-SNLI-VE datasets, representing a relative improvement of 11.1% and 11.6%, respectively.\nA.3 MORE QUALITATIVE RESULTS\nIn Fig. 5, we present some qualitative results comparing NLX-GPT (Sammani et al., 2022) and our Rapper on the e-SNLI-VE dataset. The results demonstrate that Rapper generates more precise answers accompanied by more faithful explanations. For instance, as seen in Fig. 5(b), NLX-GPT mistakenly identifies the rifle in the image as a bow. On the other hand, Rapper accurately answers the question, supported by the visually evident fact contained within the prompting rationale that the man is holding a rifle. As shown in Fig. 6, we present some qualitative results comparing Rapper without RLNF and our Rapper on the VQA-X dataset. Through the comparison, we can observe that RLNF help lower our model\u2019s probability of generating hallucinated rationales. As a result, our model can generate more accurate answers and reasonable explanations with the help of RLNF.\nWe show more qualitative results in Fig. 7.\nA.4 FEW-SHOT DEMONSTRATIONS FOR ELICITING LLAMA TO GENERATE PSEUDO RATIONALES\nIn Fig. 8 and Fig. 9, we show the task-specific few-shot demonstrations for the VQA-X and e-SNLIVE tasks. We use these demonstrations to prompt LLaMA (Touvron et al., 2023) with in-context learning."
        }
    ],
    "title": "RAPPER: REINFORCED RATIONALE-PROMPTED PARADIGM FOR NATURAL LANGUAGE EXPLANATION IN VISUAL QUESTION ANSWERING",
    "year": 2023
}