{
    "abstractText": "Synthesizing novel 3D models that resemble the input example has long been pursued by graphics artists and machine learning researchers. In this paper, we present Sin3DM, a diffusion model that learns the internal patch distribution from a single 3D textured shape and generates high-quality variations with fine geometry and texture details. Training a diffusion model directly in 3D would induce large memory and computational cost. Therefore, we first compress the input into a lower-dimensional latent space and then train a diffusion model on it. Specifically, we encode the input 3D textured shape into triplane feature maps that represent the signed distance and texture fields of the input. The denoising network of our diffusion model has a limited receptive field to avoid overfitting, and uses triplane-aware 2D convolution blocks to improve the result quality. Aside from randomly generating new samples, our model also facilitates applications such as retargeting, outpainting and local editing. Through extensive qualitative and quantitative evaluation, we show that our method outperforms prior methods in generation quality of 3D shapes.",
    "authors": [
        {
            "affiliations": [],
            "name": "TEXTURED SHAPE"
        },
        {
            "affiliations": [],
            "name": "Rundi Wu"
        },
        {
            "affiliations": [],
            "name": "Ruoshi Liu"
        },
        {
            "affiliations": [],
            "name": "Carl Vondrick"
        },
        {
            "affiliations": [],
            "name": "Changxi Zheng"
        }
    ],
    "id": "SP:be500e5abd6175d65223f1a4204fb046fbd5a62d",
    "references": [
        {
            "authors": [
                "Panos Achlioptas",
                "Olga Diamanti",
                "Ioannis Mitliagkas",
                "Leonidas Guibas"
            ],
            "title": "Learning representations and generative models for 3d point clouds",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Ahmad Riazi"
            ],
            "title": "Voronoi card stand",
            "venue": "https://sketchfab.com/3d-models/ voronoi-card-stand-9df4c3437f4c4db7bfaeda35cfc33c5f,",
            "year": 2013
        },
        {
            "authors": [
                "Titas Anciukevi\u010dius",
                "Zexiang Xu",
                "Matthew Fisher",
                "Paul Henderson",
                "Hakan Bilen",
                "Niloy J Mitra",
                "Paul Guerrero"
            ],
            "title": "Renderdiffusion: Image diffusion for 3d reconstruction, inpainting and generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Martin Bokeloh",
                "Michael Wand",
                "Hans-Peter Seidel"
            ],
            "title": "A connection between partial symmetry and inverse procedural modeling",
            "venue": "In ACM SIGGRAPH",
            "year": 2010
        },
        {
            "authors": [
                "Ruojin Cai",
                "Guandao Yang",
                "Hadar Averbuch-Elor",
                "Zekun Hao",
                "Serge Belongie",
                "Noah Snavely",
                "Bharath Hariharan"
            ],
            "title": "Learning gradient fields for shape generation",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Eric R Chan",
                "Connor Z Lin",
                "Matthew A Chan",
                "Koki Nagano",
                "Boxiao Pan",
                "Shalini De Mello",
                "Orazio Gallo",
                "Leonidas Guibas",
                "Jonathan Tremblay",
                "Sameh Khamis"
            ],
            "title": "Efficient geometry-aware 3d generative adversarial networks",
            "venue": "arXiv preprint arXiv:2112.07945,",
            "year": 2021
        },
        {
            "authors": [
                "Zhiqin Chen",
                "Hao Zhang"
            ],
            "title": "Learning implicit fields for generative shape modeling",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Zhiqin Chen",
                "Andrea Tagliasacchi",
                "Hao Zhang"
            ],
            "title": "Bsp-net: Generating compact meshes via binary space partitioning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Zhiqin Chen",
                "Vladimir G Kim",
                "Matthew Fisher",
                "Noam Aigerman",
                "Hao Zhang",
                "Siddhartha Chaudhuri"
            ],
            "title": "Decor-gan: 3d shape detailization by conditional refinement",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Yen-Chi Cheng",
                "Hsin-Ying Lee",
                "Sergey Tulyakov",
                "Alexander Schwing",
                "Liangyan Gui"
            ],
            "title": "Sdfusion: Multimodal 3d shape completion, reconstruction, and generation",
            "venue": "arXiv preprint arXiv:2212.04493,",
            "year": 2022
        },
        {
            "authors": [
                "Matt Deitke",
                "Dustin Schwenk",
                "Jordi Salvador",
                "Luca Weihs",
                "Oscar Michel",
                "Eli VanderBilt",
                "Ludwig Schmidt",
                "Kiana Ehsani",
                "Aniruddha Kembhavi",
                "Ali Farhadi"
            ],
            "title": "Objaverse: A universe of annotated 3d objects",
            "venue": "arXiv preprint arXiv:2212.08051,",
            "year": 2022
        },
        {
            "authors": [
                "Matt Deitke",
                "Ruoshi Liu",
                "Matthew Wallingford",
                "Huong Ngo",
                "Oscar Michel",
                "Aditya Kusupati",
                "Alan Fan",
                "Christian Laforte",
                "Vikram Voleti",
                "Samir Yitzhak Gadre"
            ],
            "title": "Objaverse-xl: A universe of 10m+ 3d objects",
            "venue": "arXiv preprint arXiv:2307.05663,",
            "year": 2023
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Alexei A Efros",
                "Thomas K Leung"
            ],
            "title": "Texture synthesis by non-parametric sampling",
            "venue": "In Proceedings of the seventh IEEE international conference on computer vision,",
            "year": 1999
        },
        {
            "authors": [
                "Sara Fridovich-Keil",
                "Alex Yu",
                "Matthew Tancik",
                "Qinhong Chen",
                "Benjamin Recht",
                "Angjoo Kanazawa"
            ],
            "title": "Plenoxels: Radiance fields without neural networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Funkhouser",
                "Michael Kazhdan",
                "Philip Shilane",
                "Patrick Min",
                "William Kiefer",
                "Ayellet Tal",
                "Szymon Rusinkiewicz",
                "David Dobkin"
            ],
            "title": "Modeling by example",
            "venue": "ACM transactions on graphics (TOG),",
            "year": 2004
        },
        {
            "authors": [
                "Jun Gao",
                "Tianchang Shen",
                "Zian Wang",
                "Wenzheng Chen",
                "Kangxue Yin",
                "Daiqing Li",
                "Or Litany",
                "Zan Gojcic",
                "Sanja Fidler"
            ],
            "title": "Get3d: A generative model of high quality 3d textured shapes learned from images",
            "venue": "Advances In Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Niv Granot",
                "Ben Feinstein",
                "Assaf Shocher",
                "Shai Bagon",
                "Michal Irani"
            ],
            "title": "Drop the gan: In defense of patches nearest neighbors as single image generative models",
            "venue": "arXiv preprint arXiv:2103.15545,",
            "year": 2021
        },
        {
            "authors": [
                "Gal Greshler",
                "Tamar Shaham",
                "Tomer Michaeli"
            ],
            "title": "Catch-a-waveform: Learning to generate audio from a single short example",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Anchit Gupta",
                "Wenhan Xiong",
                "Yixin Nie",
                "Ian Jones",
                "Barlas O\u011fuz"
            ],
            "title": "3dgen: Triplane latent diffusion for textured mesh generation",
            "venue": "arXiv preprint arXiv:2303.05371,",
            "year": 2023
        },
        {
            "authors": [
                "Niv Haim",
                "Ben Feinstein",
                "Niv Granot",
                "Assaf Shocher",
                "Shai Bagon",
                "Tali Dekel",
                "Michal Irani"
            ],
            "title": "Diverse generation from a single video made possible",
            "venue": "arXiv preprint arXiv:2109.08591,",
            "year": 2021
        },
        {
            "authors": [
                "Charles Han",
                "Eric Risser",
                "Ravi Ramamoorthi",
                "Eitan Grinspun"
            ],
            "title": "Multiscale texture synthesis",
            "venue": "In ACM SIGGRAPH",
            "year": 2008
        },
        {
            "authors": [
                "Amir Hertz",
                "Rana Hanocka",
                "Raja Giryes",
                "Daniel Cohen-Or"
            ],
            "title": "Deep geometric texture synthesis",
            "venue": "ACM Trans. Graph.,",
            "year": 2020
        },
        {
            "authors": [
                "Tobias Hinz",
                "Matthew Fisher",
                "Oliver Wang",
                "Stefan Wermter"
            ],
            "title": "Improved techniques for training single-image gans",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "year": 2024
        },
        {
            "authors": [
                "Pradeep Kumar Jayaraman",
                "Joseph G Lambourne",
                "Nishkrit Desai",
                "Karl DD Willis",
                "Aditya Sanghi",
                "Nigel JW Morris"
            ],
            "title": "Solidgen: An autoregressive model for direct b-rep synthesis",
            "venue": "arXiv preprint arXiv:2203.13944,",
            "year": 2022
        },
        {
            "authors": [
                "R. Kenny Jones",
                "Theresa Barton",
                "Xianghao Xu",
                "Kai Wang",
                "Ellen Jiang",
                "Paul Guerrero",
                "Niloy J. Mitra",
                "Daniel Ritchie"
            ],
            "title": "Shapeassembly: Learning to generate programs for 3d shape structure synthesis",
            "venue": "ACM Transactions on Graphics (TOG), Siggraph Asia 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Heewoo Jun",
                "Alex Nichol"
            ],
            "title": "Shap-e: Generating conditional 3d implicit functions",
            "venue": "arXiv preprint arXiv:2305.02463,",
            "year": 2023
        },
        {
            "authors": [
                "Evangelos Kalogerakis",
                "Siddhartha Chaudhuri",
                "Daphne Koller",
                "Vladlen Koltun"
            ],
            "title": "A probabilistic model for component-based shape synthesis",
            "venue": "Acm Transactions on Graphics (TOG),",
            "year": 2012
        },
        {
            "authors": [
                "Animesh Karnewar",
                "Oliver Wang",
                "Tobias Ritschel",
                "Niloy J Mitra"
            ],
            "title": "3ingan: Learning a 3d generative model from images of a self-similar scene",
            "venue": "In 2022 International Conference on 3D Vision",
            "year": 2022
        },
        {
            "authors": [
                "Animesh Karnewar",
                "Andrea Vedaldi",
                "David Novotny",
                "Niloy J Mitra"
            ],
            "title": "Holodiffusion: Training a 3d diffusion model using 2d images",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Vladimir Kulikov",
                "Shahar Yadin",
                "Matan Kleiner",
                "Tomer Michaeli"
            ],
            "title": "Sinddm: A single image denoising diffusion model",
            "venue": "arXiv preprint arXiv:2211.16582,",
            "year": 2022
        },
        {
            "authors": [
                "Samuli Laine",
                "Janne Hellsten",
                "Tero Karras",
                "Yeongho Seol",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Modular primitives for high-performance differentiable rendering",
            "venue": "ACM Transactions on Graphics,",
            "year": 2020
        },
        {
            "authors": [
                "Jun Li",
                "Kai Xu",
                "Siddhartha Chaudhuri",
                "Ersin Yumer",
                "Hao Zhang",
                "Leonidas Guibas"
            ],
            "title": "Grass: Generative recursive autoencoders for shape structures",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2017
        },
        {
            "authors": [
                "Peizhuo Li",
                "Kfir Aberman",
                "Zihan Zhang",
                "Rana Hanocka",
                "Olga Sorkine-Hornung"
            ],
            "title": "Ganimator: Neural motion synthesis from a single sequence",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2022
        },
        {
            "authors": [
                "Ruihui Li",
                "Xianzhi Li",
                "Ka-Hei Hui",
                "Chi-Wing Fu"
            ],
            "title": "Sp-gan: Sphere-guided 3d shape generation and manipulation",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2021
        },
        {
            "authors": [
                "Weiyu Li",
                "Xuelin Chen",
                "Jue Wang",
                "Baoquan Chen"
            ],
            "title": "Patch-based 3d natural scene generation from a single example",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Chen-Hsuan Lin",
                "Jun Gao",
                "Luming Tang",
                "Towaki Takikawa",
                "Xiaohui Zeng",
                "Xun Huang",
                "Karsten Kreis",
                "Sanja Fidler",
                "Ming-Yu Liu",
                "Tsung-Yi Lin"
            ],
            "title": "Magic3d: High-resolution text-to-3d content creation",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Ruoshi Liu",
                "Carl Vondrick"
            ],
            "title": "Humans as light bulbs: 3d human reconstruction from thermal reflection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Ruoshi Liu",
                "Sachit Menon",
                "Chengzhi Mao",
                "Dennis Park",
                "Simon Stent",
                "Carl Vondrick"
            ],
            "title": "Shadows shed light on 3d objects",
            "venue": "arXiv preprint arXiv:2206.08990,",
            "year": 2022
        },
        {
            "authors": [
                "Ruoshi Liu",
                "Rundi Wu",
                "Basile Van Hoorick",
                "Pavel Tokmakov",
                "Sergey Zakharov",
                "Carl Vondrick"
            ],
            "title": "Zero-1-to-3: Zero-shot one image to 3d object",
            "venue": "arXiv preprint arXiv:2303.11328,",
            "year": 2023
        },
        {
            "authors": [
                "William E. Lorensen",
                "Harvey E. Cline"
            ],
            "title": "Marching cubes: A high resolution 3d surface construction algorithm",
            "venue": "SIGGRAPH \u201987,",
            "year": 1987
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Marti David Rial"
            ],
            "title": "High detail sandstone mountain",
            "venue": "https://www.cgtrader.com/items/ 2951688/download-page,",
            "year": 2021
        },
        {
            "authors": [
                "Max Ramirez"
            ],
            "title": "Damaged wall (pbr). https://sketchfab.com/3d-models/ damaged-wall-pbr-009fc4bbc1184fca8fb6d6f15359d835, 2016",
            "venue": "License: CC Attribution",
            "year": 2016
        },
        {
            "authors": [
                "Paul Merrell"
            ],
            "title": "Example-based model synthesis",
            "venue": "In Proceedings of the 2007 symposium on Interactive 3D graphics and games, pp",
            "year": 2007
        },
        {
            "authors": [
                "Lars Mescheder",
                "Michael Oechsle",
                "Michael Niemeyer",
                "Sebastian Nowozin",
                "Andreas Geiger"
            ],
            "title": "Occupancy networks: Learning 3d reconstruction in function space",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Gal Metzer",
                "Elad Richardson",
                "Or Patashnik",
                "Raja Giryes",
                "Daniel Cohen-Or"
            ],
            "title": "Latent-nerf for shape-guided generation of 3d shapes and textures",
            "venue": "arXiv preprint arXiv:2211.07600,",
            "year": 2022
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P. Srinivasan",
                "Matthew Tancik",
                "Jonathan T. Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Kaichun Mo",
                "Paul Guerrero",
                "Li Yi",
                "Hao Su",
                "Peter Wonka",
                "Niloy Mitra",
                "Leonidas J Guibas"
            ],
            "title": "Structurenet: Hierarchical graph networks for 3d shape generation",
            "year": 1908
        },
        {
            "authors": [
                "Charlie Nash",
                "Yaroslav Ganin",
                "SM Ali Eslami",
                "Peter Battaglia"
            ],
            "title": "Polygen: An autoregressive generative model of 3d meshes",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Nichol",
                "Heewoo Jun",
                "Prafulla Dhariwal",
                "Pamela Mishkin",
                "Mark Chen"
            ],
            "title": "Point-e: A system for generating 3d point clouds from complex prompts",
            "venue": "arXiv preprint arXiv:2212.08751,",
            "year": 2022
        },
        {
            "authors": [
                "Yaniv Nikankin",
                "Niv Haim",
                "Michal Irani"
            ],
            "title": "Sinfusion: Training diffusion models on a single image or video",
            "venue": "arXiv preprint arXiv:2211.11743,",
            "year": 2022
        },
        {
            "authors": [
                "Jeong Joon Park",
                "Peter Florence",
                "Julian Straub",
                "Richard Newcombe",
                "Steven Lovegrove"
            ],
            "title": "Deepsdf: Learning continuous signed distance functions for shape representation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Dario Pavllo",
                "Jonas Kohler",
                "Thomas Hofmann",
                "Aurelien Lucchi"
            ],
            "title": "Learning generative models of textured 3d meshes from real-world images",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Pedram Ashoori"
            ],
            "title": "Small town. https://www.cgtrader.com/free-3d-models/ exterior/cityscape/small-town-87b127c8-c991-4063-aa69-e58800686299, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Songyou Peng",
                "Michael Niemeyer",
                "Lars Mescheder",
                "Marc Pollefeys",
                "Andreas Geiger"
            ],
            "title": "Convolutional occupancy networks",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Ben Poole",
                "Ajay Jain",
                "Jonathan T. Barron",
                "Ben Mildenhall"
            ],
            "title": "Dreamfusion: Text-to-3d using 2d diffusion",
            "year": 2022
        },
        {
            "authors": [
                "Sigal Raab",
                "Inbal Leibovitch",
                "Guy Tevet",
                "Moab Arar",
                "Amit H Bermano",
                "Daniel Cohen-Or"
            ],
            "title": "Single motion diffusion",
            "venue": "arXiv preprint arXiv:2302.05905,",
            "year": 2023
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical textconditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Carlos Rodriguez-Pardo",
                "Elena Garces"
            ],
            "title": "Seamlessgan: Self-supervised synthesis of tileable texture maps",
            "venue": "IEEE Transactions on Visualization and Computer Graphics,",
            "year": 2022
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Nataniel Ruiz",
                "Yuanzhen Li",
                "Varun Jampani",
                "Yael Pritch",
                "Michael Rubinstein",
                "Kfir Aberman"
            ],
            "title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
            "venue": "arXiv preprint arXiv:2208.12242,",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L Denton",
                "Kamyar Ghasemipour",
                "Raphael Gontijo Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Romain Beaumont",
                "Richard Vencu",
                "Cade Gordon",
                "Ross Wightman",
                "Mehdi Cherti",
                "Theo Coombes",
                "Aarush Katta",
                "Clayton Mullis",
                "Mitchell Wortsman"
            ],
            "title": "Laion-5b: An open large-scale dataset for training next generation image-text models",
            "venue": "arXiv preprint arXiv:2210.08402,",
            "year": 2022
        },
        {
            "authors": [
                "Tamar Rott Shaham",
                "Tali Dekel",
                "Tomer Michaeli"
            ],
            "title": "Singan: Learning a generative model from a single natural image",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Shahriar Shahrabi"
            ],
            "title": "The vast land. https://sketchfab.com/3d-models/ the-vast-land-733b802f5a4743ef99ad574279d49920, 2021",
            "venue": "License: CC Attribution",
            "year": 2021
        },
        {
            "authors": [
                "Assaf Shocher",
                "Shai Bagon",
                "Phillip Isola",
                "Michal Irani"
            ],
            "title": "Ingan: Capturing and retargeting the",
            "venue": "dna\u201d of a natural image. In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "J Ryan Shue",
                "Eric Ryan Chan",
                "Ryan Po",
                "Zachary Ankner",
                "Jiajun Wu",
                "Gordon Wetzstein"
            ],
            "title": "3d neural field generation using triplane diffusion",
            "venue": "arXiv preprint arXiv:2211.16677,",
            "year": 2022
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Minjung Son",
                "Jeong Joon Park",
                "Leonidas Guibas",
                "Gordon Wetzstein"
            ],
            "title": "Singraf: Learning a 3d generative radiance field for a single scene",
            "venue": "arXiv preprint arXiv:2211.17260,",
            "year": 2022
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "arXiv preprint arXiv:2010.02502,",
            "year": 2020
        },
        {
            "authors": [
                "Haochen Wang",
                "Xiaodan Du",
                "Jiahao Li",
                "Raymond A Yeh",
                "Greg Shakhnarovich"
            ],
            "title": "Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation",
            "venue": "arXiv preprint arXiv:2212.00774,",
            "year": 2022
        },
        {
            "authors": [
                "Tengfei Wang",
                "Bo Zhang",
                "Ting Zhang",
                "Shuyang Gu",
                "Jianmin Bao",
                "Tadas Baltrusaitis",
                "Jingjing Shen",
                "Dong Chen",
                "Fang Wen",
                "Qifeng Chen"
            ],
            "title": "Rodin: A generative model for sculpting 3d digital avatars using diffusion",
            "venue": "arXiv preprint arXiv:2212.06135,",
            "year": 2022
        },
        {
            "authors": [
                "Weilun Wang",
                "Jianmin Bao",
                "Wengang Zhou",
                "Dongdong Chen",
                "Dong Chen",
                "Lu Yuan",
                "Houqiang Li"
            ],
            "title": "Sindiffusion: Learning a diffusion model from a single natural image",
            "venue": "arXiv preprint arXiv:2211.12445,",
            "year": 2022
        },
        {
            "authors": [
                "Jiajun Wu",
                "Chengkai Zhang",
                "Tianfan Xue",
                "Bill Freeman",
                "Josh Tenenbaum"
            ],
            "title": "Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Rundi Wu",
                "Changxi Zheng"
            ],
            "title": "Learning to generate 3d shapes from a single example",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2022
        },
        {
            "authors": [
                "Rundi Wu",
                "Chang Xiao",
                "Changxi Zheng"
            ],
            "title": "Deepcad: A deep generative network for computeraided design models",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Kai Xu",
                "Hao Zhang",
                "Daniel Cohen-Or",
                "Baoquan Chen"
            ],
            "title": "Fit and diverse: Set evolution for inspiring 3d shape galleries",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2012
        },
        {
            "authors": [
                "Guandao Yang",
                "Xun Huang",
                "Zekun Hao",
                "Ming-Yu Liu",
                "Serge Belongie",
                "Bharath Hariharan"
            ],
            "title": "Pointflow: 3d point cloud generation with continuous normalizing flows",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Xiaohui Zeng",
                "Arash Vahdat",
                "Francis Williams",
                "Zan Gojcic",
                "Or Litany",
                "Sanja Fidler",
                "Karsten Kreis"
            ],
            "title": "Lion: Latent point diffusion models for 3d shape generation",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "year": 2018
        },
        {
            "authors": [
                "Zhixing Zhang",
                "Ligong Han",
                "Arnab Ghosh",
                "Dimitris Metaxas",
                "Jian Ren"
            ],
            "title": "Sine: Single image editing with text-to-image diffusion models",
            "venue": "arXiv preprint arXiv:2212.04489,",
            "year": 2022
        },
        {
            "authors": [
                "ZiCheng Zhang",
                "CongYing Han",
                "TianDe Guo"
            ],
            "title": "Exsingan: Learning an explainable generative model from a single image",
            "venue": "arXiv preprint arXiv:2105.07350,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Zhou",
                "Zhen Zhu",
                "Xiang Bai",
                "Dani Lischinski",
                "Daniel Cohen-Or",
                "Hui Huang"
            ],
            "title": "Nonstationary texture synthesis by adversarial expansion",
            "venue": "ACM Trans. Graph.,",
            "year": 2018
        },
        {
            "authors": [
                "Li"
            ],
            "title": "Visual comparison to Sin3DGenLi et al",
            "venue": "In comparison,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Creating novel 3D digital assets is challenging. It requires both technical skills and artistic sensibilities, and is often time-consuming and tedious. This motivates researchers to develop computer algorithms capable of generating new, diverse, and high-quality 3D models automatically. Over the past few years, deep generative models have demonstrated great promise for automatic 3D content creation (Achlioptas et al., 2018; Nash et al., 2020; Gao et al., 2022). More recently, diffusion models have proved particularly efficient for image generation and further pushed the frontier of 3D generation (Gupta et al., 2023; Wang et al., 2022b).\nThese generative models are typically trained on large datasets. However, collecting a large and diverse set of high-quality 3D data, with fine geometry and texture, is significantly more challenging than collecting 2D images. Today, publicly accessible 3D datasets (Chang et al., 2015; Deitke et al.,\n2022) remain orders of magnitude smaller than popular image datasets (Schuhmann et al., 2022), insufficient to train production-quality 3D generative models. In addition, many artistically designed 3D models possess unique structures and textures, which often have no more than one instance to learn from. In such cases, conventional data-driven techniques may fall short.\nIn this work, we present Sin3DM, a diffusion model that only trains on a single 3D textured shape. Once trained, our model is able to synthesize new, diverse and high-quality samples that locally resemble the training example. The outputs from our model can be converted to 3D meshes and UV-mapped textures (or even textures that describe physics-based rendering materials), which can be used directly in modern graphics engine such as Blender (Community, 2018) and Unreal Engine (Epic Games, 2019). We show example results in Fig. 1 and include more in Sec. 4. Our model also facilitates applications such as retargeting, outpainting and local editing.\nWe aim to train a diffusion model on a single 3D textured shape with locally similar patterns. Two key technical considerations must be taken into account. First, we need an expressive and memoryefficient 3D representation. Training a diffusion model simply on 3D grids would induce large memory and computational cost. Second, the receptive field of the diffusion model needs to be small, analogously to the use of patch discriminators in GAN-based approaches (Shaham et al., 2019). A small receptive field forces the model to capture local patch features.\nThe training process of our Sin3DM consists of two stages. We first train an autoencoder to compress the input 3D textured shape into triplane feature maps (Peng et al., 2020), which are three axisaligned 2D feature maps. Together with the decoder, they implicitly represent the signed distance and texture fields of the input. Then we train a diffusion model on the triplane feature maps to learn the distribution of the latent features. Our denoising network is a 2D U-Net with only one-level of depth, whose receptive field is approximately 40% of the feature map size. Furthermore, we enhance the generation quality by incorporating triplane-aware 2D convolution blocks, which consider the relation between triplane feature maps. At inference time, we generate new 3D textured shapes by sampling triplane feature maps using the diffusion model and subsequently decoding them with the triplane decoder.\nTo our best knowledge, Sin3DM is the first diffusion model trained on a single 3D textured shape. We demonstrate generation results on various 3D models of different types. We also compare to prior methods and baselines through quantitative evaluations, and show that our proposed approach achieves better quality."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "3D shape generation Since the pioneering work by (Funkhouser et al., 2004), data-driven methods for 3D shape generation has attracted immense research interest. Early works in this direction follow a synthesis-by-analysis approach (Merrell, 2007; Bokeloh et al., 2010; Kalogerakis et al., 2012; Xu et al., 2012). After the introduction of deep generative networks such as GAN (Goodfellow et al., 2014), researchers start to develop deep generative models for 3D shapes. Most of the existing works primarily focus on generating 3D geometry of various representations, including voxels (Wu et al., 2016; Chen et al., 2021), point clouds (Achlioptas et al., 2018; Yang et al., 2019; Cai et al., 2020; Li et al., 2021), meshes (Nash et al., 2020; Pavllo et al., 2021), implicit fields (Chen & Zhang, 2019; Park et al., 2019; Mescheder et al., 2019; Liu et al., 2022; Liu & Vondrick, 2023), structural primitives (Li et al., 2017; Mo et al., 2019; Jones et al., 2020), and parametric models (Chen et al., 2020; Wu et al., 2021; Jayaraman et al., 2022). Some recent works take a step forward to generate 3D textured shapes (Gupta et al., 2023; Gao et al., 2022; Nichol et al., 2022; Jun & Nichol, 2023). All these methods rely on a large 3D dataset for training. Yet, collecting a high-quality 3D dataset is much more expensive than images, and many artistically designed shapes have unique structures that are hard to learn from a limited collection. Without the need of a large dataset, from merely a single 3D textured shape, our method is able to learn and generate its high-quality variations.\nAnother line of recent works (Poole et al., 2022; Lin et al., 2023; Wang et al., 2022a; Metzer et al., 2022; Liu et al., 2023) use gradient-based optimization to produce individual 3D models by leveraging differentiable rendering techniques (Mildenhall et al., 2020; Laine et al., 2020) and pretrained text-to-image generation models (Rombach et al., 2022). However, these methods have a long inference time due to the required per-sample optimization, and the results often show high saturation artifact. They are unable to generate fine variations of an input 3D example.\nSingle instance generative models The goal of single instance generative models is to learn the internal patch statistics from a single input instance and generate diverse new samples with similar local content. Texture synthesis is an important use case for such models and has been an active area of research for more than a decade Efros & Leung (1999); Han et al. (2008); Zhou et al. (2018); Niklasson et al. (2021); Rodriguez-Pardo & Garces (2022). Beyond stationary textures, the seminal work SinGAN (Shaham et al., 2019) explores this problem by training a hierarchy of patch GANs (Goodfellow et al., 2014) on an pyramid of a nature image. Many follow-up works improve upon it from various perspectives (Hinz et al., 2021; Shocher et al., 2019; Granot et al., 2021; Zhang et al., 2021). Some recent works use a diffusion model for single image generation by limiting the receptive field of the denoising network (Nikankin et al., 2022; Wang et al., 2022c) or constructing a multi-scale diffusion process (Kulikov et al., 2022). Our method is inspired by the above generative models trained on single image.\nThe idea of single image generation has been extended to other data domains, such as videos (Haim et al., 2021; Nikankin et al., 2022), audio (Greshler et al., 2021), character motions (Li et al., 2022; Raab et al., 2023), 3D shapes (Hertz et al., 2020; Wu & Zheng, 2022) and radiance fields (Karnewar et al., 2022; Son et al., 2022; Li et al., 2023). In particular, SSG (Wu & Zheng, 2022) is the most relevant prior work. It uses a multi-scale GAN architecture and trains a voxel pyramid of the input 3D shape. However, it only generates un-textured meshes (i.e., the geometry only) and the geometry quality is limited by the highest training resolution of the voxel grids. By encoding the input 3D textured shape into a neural implicit representation, our method is able to generate textured meshes with high resolutions for both geometry and texture.\nLi et al. (2023) uses a patch matching approach to synthesize 3D scenes from a single example. It specifically focuses on 3D natural scenes represented as grid-based radiance fields (Plenoxels (Fridovich-Keil et al., 2022)), which cannot be easily re-lighted and we found its converted meshes are often broken.\nDiffusion models Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020) are a class of generative models that use a stochastic diffusion process to generate samples matching the real data distribution. Recent works with diffusion models achieved state-of-the-art performance for image generation (Rombach et al., 2022; Dhariwal & Nichol, 2021; Saharia et al., 2022; Ramesh et al., 2022). Following the remarkable success in image domain, researchers start to extend diffusion models to 3D (Zeng et al., 2022; Nichol et al., 2022; Cheng et al., 2022; Shue et al., 2022; Gupta et al., 2023; Anciukevic\u030cius et al., 2023; Karnewar et al., 2023) and obtain better performance than prior GAN-based methods. These works typically train diffusion models on a large scale 3D shape dataset such as ShapeNet (Chang et al., 2015) and Objaverse (Deitke et al., 2022; 2023). In contrast, we explore the diffusion model trained on a single 3D textured shape to capture the patch-level variations."
        },
        {
            "heading": "3 METHOD",
            "text": "Overview Sin3DM learns the internal patch distribution from a single textured 3D shape and generates high-quality variations. The core of our method is a denoising diffusion probabilistic model (DDPM) (Ho et al., 2020). The receptive field of the denoising network is designed to be small, analogously to the use of patch discriminators in GAN-based approaches (Shaham et al., 2019). With that, the trained diffusion model is able to produce patch-level variations while preserving the global structure (Wang et al., 2022c; Nikankin et al., 2022).\nDirectly training a diffusion model on a high resolution 3D volume is computationally demanding. To address this issue, we first compress the input textured 3D mesh into a compact latent space, and then apply diffusion model in this lower-dimensional space (see Fig. 2). Specifically, we encode the geometry and texture of the input mesh into an implicit triplane latent representation (Peng et al., 2020; Chan et al., 2021). Given the encoded triplane latent, we train a diffusion model on it with a denoising network composed of triplane-aware convolution blocks. After training, we can generate a new 3D textured mesh by decoding the triplane latent sampled from the diffusion model."
        },
        {
            "heading": "3.1 TRIPLANE LATENT REPRESENTATION",
            "text": "To train a diffusion model on a high resolution textured 3D mesh, we need a 3D representation that is expressive, compact and memory efficient. With such consideration, we adopt the triplane representation (Peng et al., 2020; Chan et al., 2021) to model the geometry and texture of the input 3D mesh. Specifically, we train an auto-encoder to compress the input into a triplane representation.\nGiven a textured 3D mesh M, we first construct a 3D grid of size H \u00d7 W \u00d7 D to serve as the input to the encoder. At each grid point p, we compute its signed distance d(p) to the mesh surface and truncate the value by a threshold \u03f5d. For points whose absolute signed distance falls within this threshold, we set their texture color c(p) \u2208 R3 to be the same as the color of the nearest point on the mesh surface. For points outside the distance threshold, we assign their color values to be zero. After such process, we get a 3D grid GM \u2208 RH\u00d7W\u00d7D\u00d74 of truncated signed distance and texture values. In our experiments, we set max(H,W,D) = 256.\nNext, we use an encoder \u03c8enc to encode the 3D grid into a triplane latent representation h = \u03c8enc(GM), which consists of three axis-aligned 2D feature maps\nh = (hxy,hxz,hyz), (1)\nwhere hxy \u2208 RC\u00d7H \u2032\u00d7W \u2032 , hxz \u2208 RC\u00d7H \u2032\u00d7D\u2032 and hyz \u2208 RC\u00d7W \u2032\u00d7D\u2032 , with C being the number of channels. H \u2032,W \u2032, D\u2032 are the spatial dimensions of the feature maps. The encoder \u03c8enc is composed of one 3D convolution layer and three average pooling layers for the three axes, as illustrated in Fig. 2.\nThe decoder \u03c8dec consists of three 2D ResNet blocks (\u03c8 xy dec, \u03c8 xz dec, \u03c8 yz dec), and two separate MLP heads (\u03c8geodec , \u03c8 tex dec), for decoding the signed distances and texture colors, respectively. Consider a 3D position p \u2208 R3. The decoder first refines the triplane latent h using 2D ResNet blocks and then gather the triplane features at three projected locations of p,\nfxy = interp(\u03c8 xy dec(hxy), pxy), fxz = interp(\u03c8xzdec(hxz), pxz), fyz = interp(\u03c8 yz dec(hyz), pyz),\n(2)\nwhere interp(\u00b7, q) performs bilinear interpolation of a 2D feature map at position q. The interpolated features are summed and fed into the MLP heads to predict the signed distance d\u0302 and color c\u0302,\nd\u0302(p) = \u03c8geodec (fxy + fxz + fyz), c\u0302(p) = \u03c8texdec(fxy + fxz + fyz). (3)\nWe train the auto-encoder using a reconstruction loss\nL(\u03c8) = Ep\u2208P [|d\u0302(p)\u2212 d(p)|+ |c\u0302(p)\u2212 c(p)|], (4) where P is a point set that contains all the grid points and 5 million points sampled near the mesh surface. d(p) and c(p) are the ground truth signed distance and color at position p, respectively.\nU-Net p\u03b8\nt ht\nTriplaneConv\nGroupNorm + SiLU\nShift and Scale\nTriplaneConv\nGroupNorm + SiLU\nResBlock\n(a) U-Net structure\nP\nP\nE\nE\n2D C onv\nTriplaneConv\nConvXY\nConvXZ\nConvYZ\nP average pooling E expand to 2D\nhxy\nhxz\nhyz\n(b) TriplaneConv block\nFigure 3: Left: denoising network structure. Our denoising network is a fully convolution U-Net composed of four ResBlocks and its bottleneck downsamples the input by 2. Right: triplane-aware convolution block. A TriplaneConv block considers the relation between triplane feature maps. Inside ConvXY, we apply axis-wise average pooling to hxz and hyz , yielding two feature vectors, which are then expanded to the original 2D dimension by replicating along y(or x) axis. The two expanded 2D feature maps are concatenated with hxy and fed into a regular 2D convolution layer.\nNote that a seemingly simpler option is to follow an auto-decoder approach (Park et al., 2019), i.e., optimize the triplane latent h directly without an encoder. However, we found the resulting triplane latent h to be noisy and less structured, making the subsequent diffusion model hard to train. An encoder naturally regularizes the latent space."
        },
        {
            "heading": "3.2 TRIPLANE LATENT DIFFUSION MODEL",
            "text": "After compressing the input into a compact triplane latent representation, we train a denoising diffusion probabilistic model (DDPM) (Ho et al., 2020) to learn the distribution of these latent features.\nAt a high level, a diffusion model is trained to reverse a Markovian forward process. Given a triplane latent h0 = (hxy,hxz,hyz), the forward process q gradually adds Gaussian noise to the triplane features, according to a variance schedule {\u03b2t}Tt=0,\nq(ht |ht\u22121) = N (ht| \u221a 1\u2212 \u03b2tht\u22121, \u03b2tI). (5)\nThe noised data at step t can be directly sampled in a closed form solution ht = \u221a \u03b1\u0304th+ \u221a 1\u2212 \u03b1\u0304t\u03f5, where \u03f5 is random noise drawn from N (0, I) and \u03b1\u0304t := \u220ft s=1 \u03b1s = \u220ft\ns=1(1 \u2212 \u03b2s). With a large enough T , hT is approximately random noise drawn from N (0, I). A denoising network p\u03b8 is trained to reverse the forward process. Due to the simplicity of the data distribution of a single example, instead of predicting the added noise \u03f5, we choose to predict the clean input and thus train with the loss function\nL(\u03b8) = Et\u223c[1,T ]||h0 \u2212 p\u03b8(ht, t)||22. (6)\nDenoising network structure. A straight-forward option for the denoising network is to use the original U-Net structure from (Ho et al., 2020), treating the triplane latent as images. However, this leads to \u201coverfitting\u201d, namely the model can only generate the same triplane latent as the input h0, without any variations. In addition, it does not consider the relations between the three axis-aligned feature maps. Therefore, we design our denoising network to have a limited receptive field to avoid \u201coverfitting\u201d, and use triplane-aware 2D convolution to enhance coherent triplane generation.\nFigure 3a illustrates the architecture of our denoising network. It\u2019s a fully-convolutional U-Net with only one-level of depth, whose receptive field covers roughly 40% region of a triplane feature map of spatial size 128. On the examples used in our experiments, we found that these parameter choices produce consistently plausible results and reasonable variations while keeping the input shape\u2019s global structure. In each ResBlock in our U-Net, we use a triplane-aware convolution block (see Fig. 3b), similar to the one introduced in (Wang et al., 2022b). It introduces cross-plane feature interaction by aggregating features via axis-aligned average pooling. As we will show in Sec. 4.3, it effectively improves the final generation quality."
        },
        {
            "heading": "3.3 GENERATION",
            "text": "At inference time, we generate new 3D textured shape by first sampling new triplane latent using the diffusion model, and then decoding it using the triplane decoder \u03c8dec. Starting from random Gaussian noise hT \u223c N (0, I), we follow the iterative denoising process (Sohl-Dickstein et al., 2015; Ho et al., 2020),\nht\u22121 = \u221a \u03b1\u0304t\u22121\u03b2t 1\u2212 \u03b1\u0304t p\u03b8(ht, t) + \u221a \u03b1t(1\u2212 \u03b1\u0304t\u22121) 1\u2212 \u03b1\u0304t ht + \u03c3t\u03f5 (7)\nuntil t = 1. Here, \u03f5 \u223c N (0, I) for all but the last step (\u03f5 = 0 when t = 1) and \u03c32t = \u03b2t. After obtaining the sampled triplane latent h0, we first decode a signed distance grid at resolution 256, from which we extract the mesh using Marching Cubes (Lorensen & Cline, 1987). To get the 2D texture map, we use xatlas (Young, 2023) to warp the extracted 3D mesh onto a 2D plane and get the 2D texture coordinates for each mesh vertex. The 2D plane is then discritized into a 2024 \u00d7 2048 image. For each pixel that is covered by a warped mesh triangle, we obtain its corresponding 3D location via barycentric interpolation, and query the decoder \u03c8texdec to obtain its RGB color."
        },
        {
            "heading": "3.4 IMPLEMENTATION DETAILS",
            "text": "For all examples tested in the paper, we use the same set of hyperparameters. The input 3D grid has a resolution 256, i.e., max(H,W,D) = 256, and the signed distance threshold \u03f5d is set to 3/256. The encoded triplane latent has a spatial resolution 128, i.e., max(H \u2032,W \u2032, D\u2032) = 128, and the number of channels C = 12.\nWe train the triplane auto-encoder for 25000 iterations using the AdamW optimizer (Loshchilov & Hutter, 2017) with an initial learning rate 5e\u22123 and a batch size of 216. The triplane latent diffusion model has a max time step T = 1000. We train it for 25000 iterations using the AdamW optimizer with an initial learning rate 5e\u22123 and a batch size of 32. With the above settings, the training usually takes 2 \u223c 3 hours on an NVIDIA RTX A6000. Please see the Table 3 for detailed network configurations. We also include the source code in the supplementary materials."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 QUALITATIVE RESULTS",
            "text": "We show a gallery of generated results in Fig. 8 and Fig. 9. We also show some shape retargeting results in Fig. 1 and Fig. 4, where we generate new shapes of different sizes and aspect ratios while keeping the local content. This is achieved by changing the spatial dimension of the sampled Gaussian noise hT . The generated shapes are able to preserve the global structure of the input example, while presenting reasonable local variations in terms of both geometry and texture. In the supplementary materials, we provide a webpage for interactive view of the generated 3D models."
        },
        {
            "heading": "4.2 COMPARISON",
            "text": "We compare our method against SSG (Wu & Zheng, 2022), a GAN-based single 3D shape generative model, and Sin3DGen Li et al. (2023), a recently proposed method that applies patch matching on the Plenoxels representation Fridovich-Keil et al. (2022). Since SSG only generates geometry, we extend it to support texture generation by adding 3 extra dimensions for RGB color at its generator\u2019s output layer. For a fair comparison, we train it on the same sampled 3D grid (resolution 256) that we used as our encoder input. In the Fig. 17 of the appendix, we show comparison to the original SSG on geometry generation only, by removing the texture component (\u03c8texdec) in our model. For Sin3DGen, since it synthesize radiance fields that cannot be easily relighted, we generate its training data with the same lighting condition that we use for evaluation.\nEvaluation Metrics To quantitatively evaluate the quality and diversity for both the geometry and texture of the generated 3D shapes, we adopt the following metrics. For geometry evaluation, we first voxelize the input shape and the generated shapes at resolution 128. Geometry Quality (G-Qual.) is measured against the input 3D shape using SSFID (single shape Fre\u0301chet Inception Distance) (Wu & Zheng, 2022). Geometry Diversity (G-Div.) is measured by calculating the pair-wise IoU distance (1\u2212 IoU) among generated shapes. For texture evaluation, we first render the input 3D model from 8 views at resolution 512. Each generated 3D model is then rendered from the same set of views. For the rendered images under each view, we compute the SIFID (Single Image Fre\u0301chet Inception Distance) (Shaham et al., 2019) against the image from the input model, and also compute the LPIPS metric (Zhang et al., 2018) between those images. Texture Quality (T-Qual.) is defined as the SIFID averaged over different views. Similarly, Texture Diversity (T-Div.) is defined as the averaged LPIPS metric.\nWe select 10 shapes in different categories as our testing examples. For each input example, we generate 50 outputs to calculate the above metrics. Formal definition of the above evaluation metrics are included in the Sec. C of the appendix.\nResults We report the quantitative evaluation results in Table 1, and highlight the visual difference in Fig. 5 and Fig. 14. Compared to the baselines, our method obtains better scores for geometry and texture quality, while having similar scores for diversity. The generated shapes from SSG Wu & Zheng (2022) often have noisy geometry and blurry textures. This is largely because it is limited by the highest voxel grid resolution it trains on. Sin3DGen Li et al. (2023), based on Plenoxels radiance fields, often produces broken mesh surfaces and distorted textures (see Fig. 14). In contrast, our method is able to generate 3D shapes with high quality geometry and fine texture details."
        },
        {
            "heading": "4.3 ABLATION STUDY",
            "text": "We conduct ablation studies to validate several design choices of our method. Specifically, we compare our proposed method with the following variants:\nOurs (w/o triplaneConv), in which we do not use the triplane-aware convolution (Fig. 3b) in the denoising network. Instead, we simply use three separate 2D convolution layers for each plane, without considering the relation between triplane features.\nOurs (w/o encoder), in which we remove the triplane encoder \u03c8enc and fit the triplane latent in an auto-decoder fashion (Park et al., 2019). The resulting triplane latent is less structured, making the subsequent diffusion model hard to train.\nOurs (\u03f5-prediction), in which the diffusion model predicts the added noise \u03f5 instead of the clean input h0 (see Eqn. 6). In single example case, h0 is fixed and therefore easier to predict. Predicting the noise \u03f5 adds extra burden to the diffusion model.\nAs shown in Table 2, all these variants lead to lower scores for geometry and texture quality. The diversity scores increase at the cost of much lower result quality. Please refer to Fig. 12 for visual comparison. We also visually compare results of using different receptive field size in Fig. 13."
        },
        {
            "heading": "4.4 CONTROLLED GENERATION",
            "text": "Aside from randomly generating novel variations of the input 3D textured shape, we can also control the generation results by specifying a region of interest. Let m denote a spatial binary mask indicating the region of interest. Our goal is to generate new samples such that the masked region m stays as close as possible to our specified content y0 while the complementary region (1 \u2212m) are synthesized from random noise. Specifically, when applying the iterative denoising process (Eqn. 7), we replace the masked region with y0. That is, ht\u22121 \u2299 m \u2190 y0 \u2299 m, where \u2299 is element-wise multiplication. To allow smooth transition around the boundaries, we adjust m such that the borders\nbetween 0 and 1 are linearly interpolated. Note that no re-training is needed and all changes are made at inference time.\nWe demonstrate two use cases in Fig. 6 and Fig. 15. 1) Outpainting, which seamlessly extends the input 3D textured shape beyond its boundaries. This is achieved by setting y0 to be the padded triplane latent h0 of the input (padded by zeros). The mask m corresponds to the region occupied by the input. 2) Patch duplication, which copies the a patch of the input to the certain locations of the generated outputs where other parts are synthesized coherently. In the case, we take h0 and copy the corresponding features to get y0. The mask m corresponds to the regions of the copied patches."
        },
        {
            "heading": "4.5 SUPPORTING PBR MATERIAL",
            "text": "PBR (Physics-Based Rendering) materials are commonly used in modern graphics engines, as they provide more realistic rendering results. Our method can be easily extended to support input 3D shape with PBR material. In particular, we consider the material in terms of base color (R3), metallic (R), roughness (R) and normal (R3). Then, the input 3D grid has 9 channels in total (i.e., GM \u2208 RH\u00d7W\u00d7D\u00d79). We add two additional MLP heads in the decoder \u2014 one for predicting metallic and roughness; the other for predicting normal. We demonstrate some examples in Fig. 7 and Fig. 10."
        },
        {
            "heading": "5 DISCUSSION AND FUTURE WORK",
            "text": "In this work, we present Sin3DM, a diffusion model that is trained on a single 3D textured shape with locally similar patterns. To reduce the memory and computational cost, we compress the input into triplane feature maps and then train a diffusion model to learn the distribution of latent features. With a small receptive field and triplane-aware convolutions, our trained model is able to synthesize faithful variations with intricate geometry and texture details.\nWhile the use of triplane representation significantly reduces the memory and computational cost, we empirically observed that the generated variations primarily occur along three axis directions. Our method is also limited in controlling the trade-off between the generation quality and diverse, which is only possible by changing the receptive field size in our diffusion model. Generative models that learn from a single instance lack the ability to leverage prior knowledge from external datasets. Combing large pretrained models with single instance learning, possibly through finetuning (Ruiz et al., 2022; Zhang et al., 2022), is another interesting direction for synthesizing more diverse variations."
        },
        {
            "heading": "A RESULTS GALLERY",
            "text": ""
        },
        {
            "heading": "B NETWORK ARCHITECTURES",
            "text": ""
        },
        {
            "heading": "C EVALUATION METRICS",
            "text": "For G-Qual. and G-Div., we refer readers to (Wu & Zheng, 2022) for the calculation of SSFID and diversity score based on IoU.\nT-Qual. and T-Div. are computed as follows. We uniformly select 8 upper views (elevation angle 45\u25e6) and render the textured meshes in Blender. Let Ii(M) and Ii(Gj) denote the rendered images at the i\u2212th view, of the reference mesh M and the generated mesh Gj , respectively. T-Qual. and T-Div. are then defined as\nT-Qual. = 1\n8 8\u2211 i=1 [ 1 n n\u2211 j=1 SIFID(Ii(M), Ii(Gj))],\nT-Div. = 1\n8 8\u2211 i=1 [ 1 k(k \u2212 1) n\u2211 j=1 n\u2211 k=1 k \u0338=j LPIPS(Ii(Gj), Ii(Gk))], (8)\nwhere we set n = 50. SIFID(Shaham et al., 2019) and LPIPS (Zhang et al., 2018) are distance measures."
        },
        {
            "heading": "D MORE EVALUATIONS",
            "text": "Visual comparison for the ablation study (Sec. 4.3) is shown in Fig. 11 and Fig. 12. Ablation over different receptive field sizes is shown in Fig. 13. We also compare to SSG (Wu & Zheng, 2022) on geometry generation only, by removing the texture component (\u03c8texdec) in our model. In Table 4 and Fig. 17, we show the quantitative and qualitative evaluation results on their 10 testing examples."
        }
    ],
    "year": 2024
}