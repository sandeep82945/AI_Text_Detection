{
    "abstractText": "Neuromorphic computing with spiking neural networks is promising for energyefficient artificial intelligence (AI) applications. However, different from humans who continually learn different tasks in a lifetime, neural network models suffer from catastrophic forgetting. How could neuronal operations solve this problem is an important question for AI and neuroscience. Many previous studies draw inspiration from observed neuroscience phenomena and propose episodic replay or synaptic metaplasticity, but they are not guaranteed to explicitly preserve knowledge for neuron populations. Other works focus on machine learning methods with more mathematical grounding, e.g., orthogonal projection on high dimensional spaces, but there is no neural correspondence for neuromorphic computing. In this work, we develop a new method with neuronal operations based on lateral connections and Hebbian learning, which can protect knowledge by projecting activity traces of neurons into an orthogonal subspace so that synaptic weight update will not interfere with old tasks. We show that Hebbian and anti-Hebbian learning on recurrent lateral connections can effectively extract the principal subspace of neural activities and enable orthogonal projection. This provides new insights into how neural circuits and Hebbian learning can help continual learning, and also how the concept of orthogonal projection can be realized in neuronal systems. Our method is also flexible to utilize arbitrary training methods based on presynaptic activities/traces. Experiments show that our method consistently solves forgetting for spiking neural networks with nearly zero forgetting under various supervised training methods with different error propagation approaches, and outperforms previous approaches under various settings. Our method can pave a solid path for building continual neuromorphic computing systems. The code is available at https://github.com/pkuxmq/HLOP-SNN.",
    "authors": [
        {
            "affiliations": [],
            "name": "SPIKING NEU"
        },
        {
            "affiliations": [],
            "name": "RAL NETWORKS"
        },
        {
            "affiliations": [],
            "name": "Mingqing Xiao"
        },
        {
            "affiliations": [],
            "name": "Qingyan Meng"
        },
        {
            "affiliations": [],
            "name": "Zongpeng Zhang"
        },
        {
            "affiliations": [],
            "name": "Di He"
        },
        {
            "affiliations": [],
            "name": "Zhouchen Lin"
        }
    ],
    "id": "SP:0b99cadc570e7574dde03a6c6bbd84f10d72804d",
    "references": [
        {
            "authors": [
                "Filipp Akopyan",
                "Jun Sawada",
                "Andrew Cassidy",
                "Rodrigo Alvarez-Icaza",
                "John Arthur",
                "Paul Merolla",
                "Nabil Imam",
                "Yutaka Nakamura",
                "Pallab Datta",
                "Gi-Joon Nam"
            ],
            "title": "TrueNorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip",
            "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Rahaf Aljundi",
                "Francesca Babiloni",
                "Mohamed Elhoseiny",
                "Marcus Rohrbach",
                "Tinne Tuytelaars"
            ],
            "title": "Memory aware synapses: Learning what (not) to forget",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Guillaume Bellec",
                "Darjan Salaj",
                "Anand Subramoney",
                "Robert Legenstein",
                "Wolfgang Maass"
            ],
            "title": "Long short-term memory and learning-to-learn in networks of spiking neurons",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Guillaume Bellec",
                "Franz Scherr",
                "Anand Subramoney",
                "Elias Hajek",
                "Darjan Salaj",
                "Robert Legenstein",
                "Wolfgang Maass"
            ],
            "title": "A solution to the learning dilemma for recurrent networks of spiking neurons",
            "venue": "Nature Communications,",
            "year": 2020
        },
        {
            "authors": [
                "Thomas Bohnstingl",
                "Stanis\u0142aw Wo\u017aniak",
                "Angeliki Pantazi",
                "Evangelos Eleftheriou"
            ],
            "title": "Online spatio-temporal learning in deep neural networks",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Natalia Caporale",
                "Yang Dan"
            ],
            "title": "Spike timing-dependent plasticity: a hebbian learning rule",
            "venue": "Annual Review of Neuroscience,",
            "year": 2008
        },
        {
            "authors": [
                "Arslan Chaudhry",
                "Marc\u2019Aurelio Ranzato",
                "Marcus Rohrbach",
                "Mohamed Elhoseiny"
            ],
            "title": "Efficient lifelong learning with a-gem",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Chi-Ning Chou",
                "Mien Brabeeba Wang"
            ],
            "title": "ODE-inspired analysis for the biological version of Oja\u2019s rule in solving streaming PCA",
            "venue": "In Conference on Learning Theory,",
            "year": 2020
        },
        {
            "authors": [
                "Iulia M Comsa",
                "Krzysztof Potempa",
                "Luca Versari",
                "Thomas Fischbacher",
                "Andrea Gesmundo",
                "Jyrki Alakuijala"
            ],
            "title": "Temporal coding in spiking neural networks with alpha synaptic function",
            "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Mike Davies",
                "Narayan Srinivasa",
                "Tsung-Han Lin",
                "Gautham Chinya",
                "Yongqiang Cao",
                "Sri Harsha Choday",
                "Georgios Dimou",
                "Prasad Joshi",
                "Nabil Imam",
                "Shweta Jain"
            ],
            "title": "Loihi: A neuromorphic manycore processor with on-chip learning",
            "venue": "IEEE Micro,",
            "year": 2018
        },
        {
            "authors": [
                "Matthias De Lange",
                "Rahaf Aljundi",
                "Marc Masana",
                "Sarah Parisot",
                "Xu Jia",
                "Ale\u0161 Leonardis",
                "Gregory Slabaugh",
                "Tinne Tuytelaars"
            ],
            "title": "A continual learning survey: Defying forgetting in classification tasks",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Konstantinos I Diamantaras",
                "Sun Yuan Kung"
            ],
            "title": "Principal component neural networks: theory and applications",
            "year": 1996
        },
        {
            "authors": [
                "Wei Fang",
                "Zhaofei Yu",
                "Yanqi Chen",
                "Tiejun Huang",
                "Timoth\u00e9e Masquelier",
                "Yonghong Tian"
            ],
            "title": "Deep residual learning in spiking neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Mehrdad Farajtabar",
                "Navid Azizan",
                "Alex Mott",
                "Ang Li"
            ],
            "title": "Orthogonal gradient descent for continual learning",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Robert M French"
            ],
            "title": "Catastrophic forgetting in connectionist networks",
            "venue": "Trends in Cognitive Sciences,",
            "year": 1999
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Mehdi Mirza",
                "Da Xiao",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "An empirical investigation of catastrophic forgetting in gradient-based neural networks",
            "venue": "arXiv preprint arXiv:1312.6211,",
            "year": 2013
        },
        {
            "authors": [
                "Xu He",
                "Herbert Jaeger"
            ],
            "title": "Overcoming catastrophic interference using conceptor-aided backpropagation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Donald Olding Hebb"
            ],
            "title": "The organization of behavior: A neuropsychological theory",
            "year": 2005
        },
        {
            "authors": [
                "John J Hopfield"
            ],
            "title": "Neural networks and physical systems with emergent collective computational abilities",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 1982
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2017
        },
        {
            "authors": [
                "Dhireesha Kudithipudi",
                "Mario Aguilar-Simon",
                "Jonathan Babb",
                "Maxim Bazhenov",
                "Douglas Blackiston",
                "Josh Bongard",
                "Andrew P Brna",
                "Suraj Chakravarthi Raja",
                "Nick Cheney",
                "Jeff Clune"
            ],
            "title": "Biological underpinnings for lifelong learning machines",
            "venue": "Nature Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Axel Laborieux",
                "Maxence Ernoult",
                "Tifenn Hirtzlin",
                "Damien Querlioz"
            ],
            "title": "Synaptic metaplasticity in binarized neural networks",
            "venue": "Nature Communications,",
            "year": 2021
        },
        {
            "authors": [
                "Jun Haeng Lee",
                "Tobi Delbruck",
                "Michael Pfeiffer"
            ],
            "title": "Training deep spiking neural networks using backpropagation",
            "venue": "Frontiers in Neuroscience,",
            "year": 2016
        },
        {
            "authors": [
                "Yuhang Li",
                "Yufei Guo",
                "Shanghang Zhang",
                "Shikuang Deng",
                "Yongqing Hai",
                "Shi Gu"
            ],
            "title": "Differentiable spike: Rethinking gradient-descent for training spiking neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zhizhong Li",
                "Derek Hoiem"
            ],
            "title": "Learning without forgetting",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Timothy P Lillicrap",
                "Daniel Cownden",
                "Douglas B Tweed",
                "Colin J Akerman"
            ],
            "title": "Random synaptic feedback weights support error backpropagation for deep learning",
            "venue": "Nature Communications,",
            "year": 2016
        },
        {
            "authors": [
                "Sen Lin",
                "Li Yang",
                "Deliang Fan",
                "Junshan Zhang"
            ],
            "title": "Trgp: Trust region gradient projection for continual learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "David Lopez-Paz",
                "Marc\u2019Aurelio Ranzato"
            ],
            "title": "Gradient episodic memory for continual learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Arun Mallya",
                "Svetlana Lazebnik"
            ],
            "title": "Packnet: Adding multiple tasks to a single network by iterative pruning",
            "venue": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Qingyan Meng",
                "Mingqing Xiao",
                "Shen Yan",
                "Yisen Wang",
                "Zhouchen Lin",
                "Zhi-Quan Luo"
            ],
            "title": "Training high-performance low-latency spiking neural networks by differentiation on spike representation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Qingyan Meng",
                "Mingqing Xiao",
                "Shen Yan",
                "Yisen Wang",
                "Zhouchen Lin",
                "Zhi-Quan Luo"
            ],
            "title": "Towards memory-and time-efficient backpropagation for training spiking neural networks",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2023
        },
        {
            "authors": [
                "Hesham Mostafa"
            ],
            "title": "Supervised learning based on temporal coding in spiking neural networks",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Arild N\u00f8kland"
            ],
            "title": "Direct feedback alignment provides learning in deep neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Erkki Oja"
            ],
            "title": "Simplified neuron model as a principal component analyzer",
            "venue": "Journal of Mathematical Biology,",
            "year": 1982
        },
        {
            "authors": [
                "Erkki Oja"
            ],
            "title": "Neural networks, principal components, and subspaces",
            "venue": "International Journal of Neural Systems,",
            "year": 1989
        },
        {
            "authors": [
                "Priyadarshini Panda",
                "Jason M Allred",
                "Shriram Ramanathan",
                "Kaushik Roy"
            ],
            "title": "Asp: Learning to forget with adaptive synaptic plasticity in spiking neural networks",
            "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Cengiz Pehlevan",
                "Tao Hu",
                "Dmitri B Chklovskii"
            ],
            "title": "A hebbian/anti-hebbian neural network for linear subspace learning: A derivation from multidimensional scaling of streaming data",
            "venue": "Neural Computation,",
            "year": 2015
        },
        {
            "authors": [
                "Jing Pei",
                "Lei Deng",
                "Sen Song",
                "Mingguo Zhao",
                "Youhui Zhang",
                "Shuang Wu",
                "Guanrui Wang",
                "Zhe Zou",
                "Zhenzhi Wu",
                "Wei He"
            ],
            "title": "Towards artificial general intelligence with hybrid Tianjic chip",
            "venue": "architecture. Nature,",
            "year": 2019
        },
        {
            "authors": [
                "Arjun Rao",
                "Philipp Plank",
                "Andreas Wild",
                "Wolfgang Maass"
            ],
            "title": "A long short-term memory for ai applications in spike-based neuromorphic hardware",
            "venue": "Nature Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Sylvestre-Alvise Rebuffi",
                "Alexander Kolesnikov",
                "Georg Sperl",
                "Christoph H Lampert"
            ],
            "title": "icarl: Incremental classifier and representation learning",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "David Rolnick",
                "Arun Ahuja",
                "Jonathan Schwarz",
                "Timothy Lillicrap",
                "Gregory Wayne"
            ],
            "title": "Experience replay for continual learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Kaushik Roy",
                "Akhilesh Jaiswal",
                "Priyadarshini Panda"
            ],
            "title": "Towards spike-based machine intelligence with neuromorphic",
            "venue": "computing. Nature,",
            "year": 2019
        },
        {
            "authors": [
                "Gobinda Saha",
                "Isha Garg",
                "Kaushik Roy"
            ],
            "title": "Gradient projection memory for continual learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Joan Serra",
                "Didac Suris",
                "Marius Miron",
                "Alexandros Karatzoglou"
            ],
            "title": "Overcoming catastrophic forgetting with hard attention to the task",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Hanul Shin",
                "Jung Kwon Lee",
                "Jaehong Kim",
                "Jiwon Kim"
            ],
            "title": "Continual learning with deep generative replay",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Sumit Bam Shrestha",
                "Garrick Orchard"
            ],
            "title": "Slayer: spike layer error reassignment in time",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Nicholas Soures",
                "Peter Helfer",
                "Anurag Daram",
                "Tej Pandit",
                "Dhireesha Kudithipudi"
            ],
            "title": "Tacos: task agnostic continual learning in spiking neural networks",
            "venue": "In Theory and Foundation of Continual Learning Workshop at ICML\u20192021,",
            "year": 2021
        },
        {
            "authors": [
                "Timothy Tadros",
                "Giri P Krishnan",
                "Ramyaa Ramyaa",
                "Maxim Bazhenov"
            ],
            "title": "Sleep-like unsupervised replay reduces catastrophic forgetting in artificial neural networks",
            "venue": "Nature Communications,",
            "year": 2022
        },
        {
            "authors": [
                "Gido M van de Ven",
                "Hava T Siegelmann",
                "Andreas S Tolias"
            ],
            "title": "Brain-inspired replay for continual learning with artificial neural networks",
            "venue": "Nature Communications,",
            "year": 2020
        },
        {
            "authors": [
                "Gido M van de Ven",
                "Tinne Tuytelaars",
                "Andreas S Tolias"
            ],
            "title": "Three types of incremental learning",
            "venue": "Nature Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Stanis\u0142aw Wo\u017aniak",
                "Angeliki Pantazi",
                "Thomas Bohnstingl",
                "Evangelos Eleftheriou"
            ],
            "title": "Deep learning incorporating biologically inspired neural dynamics and in-memory computing",
            "venue": "Nature Machine Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Jibin Wu",
                "Yansong Chua",
                "Malu Zhang",
                "Guoqi Li",
                "Haizhou Li",
                "Kay Chen Tan"
            ],
            "title": "A tandem learning rule for effective training and rapid inference of deep spiking neural networks",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yujie Wu",
                "Lei Deng",
                "Guoqi Li",
                "Jun Zhu",
                "Luping Shi"
            ],
            "title": "Spatio-temporal backpropagation for training high-performance spiking neural networks",
            "venue": "Frontiers in Neuroscience,",
            "year": 2018
        },
        {
            "authors": [
                "Yujie Wu",
                "Rong Zhao",
                "Jun Zhu",
                "Feng Chen",
                "Mingkun Xu",
                "Guoqi Li",
                "Sen Song",
                "Lei Deng",
                "Guanrui Wang",
                "Hao Zheng"
            ],
            "title": "Brain-inspired global-local learning incorporated with neuromorphic computing",
            "venue": "Nature Communications,",
            "year": 2022
        },
        {
            "authors": [
                "Mingqing Xiao",
                "Qingyan Meng",
                "Zongpeng Zhang",
                "Yisen Wang",
                "Zhouchen Lin"
            ],
            "title": "Training feedback spiking neural networks by implicit differentiation on the equilibrium state",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Mingqing Xiao",
                "Qingyan Meng",
                "Zongpeng Zhang",
                "Di He",
                "Zhouchen Lin"
            ],
            "title": "Online training through time for spiking neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Mingqing Xiao",
                "Qingyan Meng",
                "Zongpeng Zhang",
                "Yisen Wang",
                "Zhouchen Lin"
            ],
            "title": "SPIDE: A purely spike-based method for training feedback spiking neural networks",
            "venue": "Neural Networks,",
            "year": 2023
        },
        {
            "authors": [
                "Will Xiao",
                "Honglin Chen",
                "Qianli Liao",
                "Tomaso Poggio"
            ],
            "title": "Biologically-plausible learning algorithms can scale to large datasets",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Wei-Yong Yan",
                "Uwe Helmke",
                "John B Moore"
            ],
            "title": "Global analysis of oja\u2019s flow for neural networks",
            "venue": "IEEE Transactions on Neural Networks,",
            "year": 1994
        },
        {
            "authors": [
                "Bojian Yin",
                "Federico Corradi",
                "Sander M Boht\u00e9"
            ],
            "title": "Accurate and efficient time-domain classification with adaptive spiking recurrent neural networks",
            "venue": "Nature Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Jaehong Yoon",
                "Eunho Yang",
                "Jeongtae Lee",
                "Sung Ju Hwang"
            ],
            "title": "Lifelong learning with dynamically expandable networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Guanxiong Zeng",
                "Yang Chen",
                "Bo Cui",
                "Shan Yu"
            ],
            "title": "Continual learning of context-dependent processing in neural networks",
            "venue": "Nature Machine Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Friedemann Zenke",
                "Surya Ganguli"
            ],
            "title": "Superspike: Supervised learning in multilayer spiking neural networks",
            "venue": "Neural Computation,",
            "year": 2018
        },
        {
            "authors": [
                "Friedemann Zenke",
                "Ben Poole",
                "Surya Ganguli"
            ],
            "title": "Continual learning through synaptic intelligence",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Wenrui Zhang",
                "Peng Li"
            ],
            "title": "Spike-train level backpropagation for training deep recurrent spiking neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Hanle Zheng",
                "Yujie Wu",
                "Lei Deng",
                "Yifan Hu",
                "Guoqi Li"
            ],
            "title": "Going deeper with directly-trained larger spiking neural networks",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Shibo Zhou",
                "Xiaohua Li",
                "Ying Chen",
                "Sanjeev T Chandrasekaran",
                "Arindam Sanyal"
            ],
            "title": "Temporalcoded deep spiking neural network with easy training and robust performance",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "\u2207WlL. A"
            ],
            "title": "OTTT OTTT (Xiao et al., 2022) is a training method to improve BPTT for temporally online learning. BPTT requires storing the computational graph unfolded over time and backpropagating through previous time steps, which is inconsistent with biological online learning and learning rules on",
            "year": 2022
        },
        {
            "authors": [
                "romorphic hardware"
            ],
            "title": "Some works introduce eligibility traces to improve BPTT for online training through time (Zenke & Ganguli, 2018",
            "venue": "Bellec et al.,",
            "year": 2020
        },
        {
            "authors": [
                "Saha"
            ],
            "title": "For SNN models, the input and output encodings follow the practice",
            "year": 2021
        },
        {
            "authors": [
                "Pei"
            ],
            "title": "2019), such as their spiking property and synaptic connections with local storage of weights for in-memory computation, for highly energy-efficient and parallel event-driven computation with avoidance of frequent memory transpose (its computation architecture is expected to be different from the commonly used hardware with von Neumann architecture such as CPU or GPU)",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Brain-inspired neuromorphic computing, e.g., biologically plausible spiking neural networks (SNNs), has attracted tremendous attention (Roy et al., 2019) and achieved promising results with deep architectures and gradient-based methods recently (Shrestha & Orchard, 2018; Zheng et al., 2021; Fang et al., 2021; Yin et al., 2021; Meng et al., 2022). The neuromorphic computing systems imitate biological neurons with parallel in-memory and event-driven neuronal computation and can be efficiently deployed on neuromorphic hardware for energy-efficient applications (Akopyan et al., 2015; Davies et al., 2018; Pei et al., 2019; Woz\u0301niak et al., 2020; Rao et al., 2022). SNNs are also believed as one of important approaches toward artificial general intelligence (Pei et al., 2019).\nHowever, neural network models typically suffer from catastrophic forgetting of old tasks when learning new ones (French, 1999; Goodfellow et al., 2013; Kirkpatrick et al., 2017), which is sig-\n\u2217corresponding author\nnificantly different from the biological systems of human brains that can build up knowledge in the whole lifetime. This is because the weights in the network are updated for new tasks and are usually not guaranteed to preserve the knowledge of old tasks during learning. To establish human-like lifelong learning in dynamically changing environments, developing neuromorphic computing systems of SNNs with continual learning (CL) ability is important for artificial intelligence.\nTo deal with this problem, biological underpinnings are investigated (Kudithipudi et al., 2022), and various machine learning methods are proposed. Most works can be categorized into two classes.\nThe first category draws inspiration from observed neuroscience phenomena and proposes methods such as replay (Rebuffi et al., 2017; Shin et al., 2017; Rolnick et al., 2019; van de Ven et al., 2020) or regularization (Kirkpatrick et al., 2017; Zenke et al., 2017; Aljundi et al., 2018; Laborieux et al., 2021). For example, replay methods are inspired by the episodic replay phenomenon during sleep or rest, and regularization is based on the metaplasticity of synapses, i.e., the ease with which a synapse can be modified depends on the history of synaptic modifications and neural activities (Kudithipudi et al., 2022). However, replay methods only implicitly protect knowledge by retraining and require storing a lot of old input samples for frequent replay, and metaplasticity methods only focus on regularization for each individual synapse, which is not guaranteed to preserve knowledge considering the high dimensional transformation of interactive neuron populations. It is also unclear how other biological rules such as Hebbian learning can systematically support continual learning.\nThe second category focuses on developing pure machine learning methods with complex computation, e.g., pursuing better knowledge preservation considering projection on high dimensional spaces (He & Jaeger, 2018; Zeng et al., 2019; Saha et al., 2021), but they require operations such as calculating the inverse of matrices or singular value decomposition (SVD), which may not be implemented by neuronal operations. Considering neuromorphic computing systems of SNNs, it is important to investigate how neural computation can deal with the problem.\nIn this work, we develop a new method, Hebbian learning based orthogonal projection (HLOP), for task- and domain-incremental continual learning of neuromorphic computing systems, which is based on lateral connections as well as Hebbian and anti-Hebbian learning. The core idea is to use lateral neural computation with Hebbian learning to extract principle subspaces of neuronal activities from streaming data and project activity traces for synaptic weight update into an orthogonal subspace so that the learned abilities are not influenced much when learning new knowledge. Such a way of learning is flexible to utilize arbitrary training methods based on presynaptic activity traces.\nOur method can explicitly preserve old knowledge by neural computation without frequent replay, and impose a more systematic constraint for weight update considering neuron populations instead of individual synapses, compared with replay and metaplasticity methods. Meanwhile, our method focuses on knowledge protection during the instantaneous learning of new tasks and is compatible with post-processing methods such as episodic replay. Compared with previous projection methods, our method enables a better, unbiased construction of the projection space based on online learning from streaming large data instead of a small batch of data (Saha et al., 2021), which leads to better performance, and our work is also the first to show how the concept of projection with more mathematical guarantees can be realized with pure neuronal operations.\nExperiments on continual learning of spiking neural networks under various settings and training methods with different error propagation approaches consistently show the superior performance of HLOP with nearly zero forgetting, outperforming previous methods. Our results indicate that lateral circuits can play a substantial role in continual learning, providing new insights into how neural circuits and Hebbian learning can support the advanced abilities of neural systems. With purely neuronal operations, HLOP can also pave paths for continual neuromorphic computing systems."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Many previous works explore continual learning of neural networks. Most methods focus on artificial neural networks (ANNs) and only few replay or metaplasticity/local learning works (Tadros et al., 2022; Soures et al., 2021; Panda et al., 2017; Wu et al., 2022) consider SNNs. Methods for fixed-capacity networks can be mainly categorized as replay, regularization-based, and parameter isolation (De Lange et al., 2021). Replay methods will replay stored samples (Rebuffi et al., 2017; Rolnick et al., 2019) or generated pseudo-samples (Shin et al., 2017; van de Ven et al., 2020) during or after learning a new task to serve as rehearsals or restrictions (Lopez-Paz & Ranzato,\n2017; Chaudhry et al., 2019), which are inspired by the episodic replay during sleep or rest in the brain (Kudithipudi et al., 2022). Regularization-based methods are mostly inspired by the metaplasticity of synapses in neuroscience, and regularize the update of each synaptic weight based on the importance estimated by various methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Aljundi et al., 2018; Laborieux et al., 2021; Soures et al., 2021) or knowledge distillation (Li & Hoiem, 2017). Parameter isolation methods separate parameters for different tasks, e.g., with pruning and mask (Mallya & Lazebnik, 2018) or hard attention (Serra et al., 2018). There are also expansion methods (Rusu et al., 2016; Yoon et al., 2018) to enlarge network capacity for every new task.\nOrthogonal projection methods take a further step of regularization to consider the projection of gradient so that the update of weight is constrained to an orthogonal direction considering the highdimensional linear transformation. Farajtabar et al. (Farajtabar et al., 2020) store previous gradients and project gradients into the direction orthogonal to old ones. He & Jaeger (2018), Zeng et al. (2019) and Saha et al. (2021) project gradients into the orthogonal subspace of the input space for more guarantee and better performance. Lin et al. (2022) improve Saha et al. (2021) with trust region projection considering similarity between tasks. However, these methods only estimate the subspace with a small batch of data due to the large costs of SVD, and there is no neural correspondence. We generalize the thought of projection to neuronal operations under streaming data.\nMeanwhile, continual learning can be classified into three fundamental types: task-incremental, domain-incremental, and class-incremental (van de Ven et al., 2022). Task-CL requires task ID at training and testing, domain-CL does not need task ID, while class-CL requires comparing new tasks with old tasks and inferring context without task ID. Following previous gradient projection methods, we mainly consider task- and domain-incremental settings. As for class-CL, it inevitably requires some kind of replay of old experiences (via samples or other techniques) for good performance since it expects explicit comparison between new classes and old ones (van de Ven et al., 2020). Our method may be combined with replay methods to first update with projection and then learn with replay, or with context-dependent processing modules similar to biological systems (Zeng et al., 2019; Kudithipudi et al., 2022), e.g., task classifiers. This is not the focus of this paper."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "3.1 SPIKING NEURAL NETWORK",
            "text": "We consider SNNs with the commonly used leaky integrate and fire (LIF) neuron model as in previous works (Wu et al., 2018; Xiao et al., 2022). Each spiking neuron maintains a membrane potential u to integrate input spike trains and generates a spike once u reaches a threshold. The dynamics of the membrane potential are \u03c4m dudt = \u2212(u \u2212 urest) + R \u00b7 I(t) for u < Vth, where I is the input current, Vth is the threshold, and R and \u03c4m are resistance and time constant, respectively. When u reaches Vth at time tf , a spike is generated and u is reset to the resting potential u = urest. The output spike train is defined using the Dirac delta function: s(t) = \u2211 tf \u03b4(t\u2212 tf ). Consider the simple\ncurrent model Ii(t) = \u2211\nj wijsj(t) + bi, where i represents the i-th neuron, wij is the weight from neuron j to neuron i and bi is a bias, then the discrete computational form is described as:ui [t+ 1] = \u03bb(ui[t]\u2212 Vthsi[t]) + \u2211 j wijsj [t] + bi,\nsi[t+ 1] = H(ui [t+ 1]\u2212 Vth), (1)\nwhere H(x) is the Heaviside step function, si[t] is the spike train of neuron i at discrete time step t, and \u03bb < 1 is a leaky term based on \u03c4m and the discrete step size. The constant R, \u03c4m, and time step size are absorbed into the weights and bias. The reset operation is implemented by subtraction.\nDifferent from ANNs, the supervised learning of SNNs is hard due to the non-differentiability of spiking neurons, and different methods are proposed to tackle the problem. One kind of training approach is based on the explicit encoding of spike trains, such as (weighted) firing rate (Wu et al., 2021; Xiao et al., 2021; Meng et al., 2022) or spiking time (Mostafa, 2017), and calculates gradients through analytical transformations between the spike representation. Another kind is spike-based approaches by backpropagation through time (BPTT) and surrogate gradients (SG) without the explicit coding scheme (Shrestha & Orchard, 2018; Bellec et al., 2018; Wu et al., 2018; Zheng et al., 2021; Li et al., 2021; Fang et al., 2021; Yin et al., 2021). Some works further improve BPTT for temporally online learning that is more consistent with biological learning and friendly for on-chip training (Bellec et al., 2020; Xiao et al., 2022; Meng et al., 2023). We will consider these three kinds of training methods and more details can be found in Appendix A."
        },
        {
            "heading": "3.2 ORTHOGONAL GRADIENT PROJECTION",
            "text": "Orthogonal gradient projection methods protect knowledge for each neural network layer. For feedforward synaptic weights W \u2208 Rm\u00d7n between two layers, suppose that the presynaptic input vectors xold \u2208 Rn in previous tasks span a subspace X, the goal is to make the updates \u2206WP orthogonal to the subspace X so that \u2200xold \u2208 X,\u2206WPxold = 0. This ensures that the weight update does not interfere with old tasks, since (W +\u2206WP )xold = Wxold. If we calculate a projection matrix P to the subspace orthogonal to the principal subspace of X, then gradients can be projected as \u2206WP = \u2206WP\u22a4. Previous works leverage different methods to calculate P. For example, Zeng et al. (2019) estimate it as P = I \u2212A(A\u22a4A + \u03b1I)\u22121A (where A represents all previous inputs) with the recursive least square algorithm. Saha et al. (2021) instead calculate P = I\u2212M\u22a4M, where M \u2208 Rk\u00d7n denotes the matrix of top k principal components of X calculated by SVD with a small batch of data and M\u22a4M is the projection matrix to the principal subspace of X. However, these methods cannot be implemented by neuronal operations for neuromorphic computing, and they only mainly estimate the projection matrix with a small batch of data, which can be biased.\n(a) Overview"
        },
        {
            "heading": "4 METHODS",
            "text": ""
        },
        {
            "heading": "4.1 HEBBIAN LEARNING BASED ORTHOGONAL PROJECTION",
            "text": "We propose Hebbian learning based orthogonal projection (HLOP) to achieve orthogonal projection with lateral neural connections and Hebbian learning, and the sketch is presented in Fig. 1. The major conception is to leverage Hebbian learning in a lateral circuit to extract the principal subspace of neural activities in current tasks, which enables the lateral circuit to project activity traces (traces recording activation information used for synaptic update) into the orthogonal subspace, as illustrated in Fig. 1(a). To this end, different from the common feedforward networks, our models consider recurrent lateral connections to a set of \u201csubspace neurons\u201d for each layer (Fig. 1(b)). Lateral connections will not influence feedforward propagation and are only for weight updates.\nRecall that the thought of orthogonal projection is to project gradients with P = I \u2212M\u22a4M (Section 3.2). Since existing supervised learning methods calculate the weight update as \u2206W = \u03b4x\u22a4, where \u03b4 is the error signal and x is the activity trace of neurons whose definition depends on training algorithms, we can correspondingly obtain the projected update by \u2206WP = \u03b4 ( x\u2212M\u22a4Mx )\u22a4 . The above calculation only requires linear modification on presynaptic activity traces x which can be implemented by recurrent lateral connections with skew-symmetric synaptic weights, as shown in Fig. 1(b,d). Specifically, the lateral circuit first propagates x to \u201csubspace neurons\u201d with connections H as y = Hx, and then recurrently propagates y with connections \u2212H for the response x\u2212 = \u2212H\u22a4y. The activity trace will be updated as x\u0302 = x+ x\u2212 = x\u2212H\u22a4Hx. So as long as the lateral connections H equal or behave similarly to the principal component matrix M, the neural circuits can implement orthogonal projection.\nActually, H can be updated by Hebbian learning to fulfill the requirements, as shown in Fig. 1(c). Hebbian-type learning has long been regarded as the basic learning method in neural systems (Hebb, 2005) and has shown the ability to extract principal components of streaming inputs (Oja, 1982; 1989). Particularly, Oja\u2019s rule (Oja, 1982) was first proposed to update weights h of input neurons x to one-unit output neuron y as ht = ht\u22121 + \u03b7tyt(xt \u2212 ytht\u22121) so that ht will converge to the principal component of streaming inputs, which can achieve the state-of-the-art convergence rate in the streaming principal component analysis problem (Chou & Wang, 2020). Several methods extend original Oja\u2019s rule to top k principal components (Oja, 1989; Diamantaras & Kung, 1996; Pehlevan et al., 2015), and we focus on the subspace algorithm that updates weights as \u2206H = \u03b7 ( yx\u22a4 \u2212 yy\u22a4H ) , which enables weights to converge to a dominant principal subspace (Yan et al., 1994). We propose to again leverage the recurrent lateral connections for Hebbian learning. The neurons first propagate signals to \u201csubspace neurons\u201d as y = Hx, and \u201csubspace neurons\u201d transmit information through the recurrent connections as x\u2212 = \u2212H\u22a4y. The connection weights H are updated by \u2206H = yx\u22a4+yx\u2212\u22a4 with two-stage Hebbian rules considering presynaptic activities and postsynaptic responses. The skew-symmetric weights share symmetric but opposite update directions, which corresponds to the Hebbian and anti-Hebbian learning, respectively. The above condition is the construction of the first subspace neurons. If we further consider learning new subspace neurons with connections H\u2032 for new tasks with existing consolidated subspace neurons, we can only update the connections to new neurons by the similar Hebbian-type learning with presynaptic activities and integrated postsynaptic responses x\u0303 = x\u2212 + x\u2212\u2032 from both new and consolidated subspace neurons, i.e., \u2206H\u2032 = y\u2032x\u22a4 + y\u2032x\u0303\u22a4, as shown in Fig. 1(c). This enables the extraction of new principal subspaces not included in the existing subspace neurons unbiasedly from streaming large data. More details are in Appendix C.\nHLOP is neuromorphic-friendly as it only requires neuronal operations with neural circuits. At the same time, HLOP may better leverage the asynchronous parallel advantage of neuromorphic computing since the projection and Hebbian learning can be parallel with the forward and error propagation of the feedforward network. The proposed HLOP also has wide adaptability to the combination with training methods based on presynaptic activity traces and other post-processing methods. As HLOP mainly focuses on the constraint of weight update direction during the instantaneous learning of new tasks, it can also be combined with post-processing methods such as episodic replay after the learning to improve knowledge transfer between different tasks."
        },
        {
            "heading": "4.2 COMBINATION WITH SNN TRAINING",
            "text": "The basic thought of HLOP is to modify presynaptic traces, and thus HLOP can be flexibly plugged into arbitrary training methods that are based on the traces, such as various SNN training methods.\nAs introduced in Section 3.1, we will consider different SNN training methods. The definition of activity traces varies for diverse training methods, and HLOP is adapted differently. For methods with explicit coding schemes, activity traces correspond to specified spike representations of spike trains (e.g., weighted firing rates). HLOP acts on the presynaptic spike representation. For BPTT with SG methods, activity traces correspond to spikes at each time step. HLOP can recursively act on presynaptic spike signals at all time steps. For online methods with eligibility traces, activity traces correspond to eligibility traces. HLOP can act on traces at all time steps, which can be implemented by modifying eligibility traces with neuronal responses to lateral circuits.\nWe illustrate the combination between HLOP and eligibility trace based methods in Fig. 2 (more details about other training methods can be found in Appendix A). Such temporarily online methods\nmay better suit on-chip training of SNNs, and HLOP can be integrated with online modification of traces by taking additional neuronal response to lateral circuits into account. In experiments, we will consider DSR (Meng et al., 2022), BPTT with SG (Wu et al., 2018; Shrestha & Orchard, 2018), and OTTT (Xiao et al., 2022) as the representative of the three kinds of methods.\nThe original HLOP leverages linear neurons in lateral circuits. Considering neuromorphic computing systems, this may be supported by some hybrid hardware (Pei et al., 2019). For more general systems, it is important to investigate if HLOP can be implemented by the same spiking neurons as well. To this end, we also propose HLOP with lateral spiking neurons based on the rate coding of high-frequency bursts. As shown in Fig. 2(d), spiking neurons in lateral circuits will generate high-frequency bursts, and the responses are based on spiking rates. We simulate this by quantizing the output of subspace neurons in experiments, i.e., the output y is quantized as y\u0302 = scale \u00d7 [ clamp(y,\u2212scale,scale)\nscale \u00d7Tl] Tl\n, where scale is taken as 20 and Tl is the time steps of lateral spiking neurons as specified in experiments (see Appendix D for more details)."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We conduct experiments for comprehensive evaluation under different training settings, input domain settings, datasets, and network architectures. We consider two metrics to evaluate continual learning: Accuracy and Backward Transfer (BWT). Accuracy measures the performance on one task, while BWT indicates the influence of learning new tasks on the old ones, which is defined as BWTk,i = Accuracyk,i \u2212 Accuracyi,i, where Accuracyk,i is the accuracy on the i-th task after learning k tasks (k \u2265 i). The average accuracy after task k is the average of these k tasks, while the average BWT is the average of previous k \u2212 1 tasks. Experimental details are in Appendix E."
        },
        {
            "heading": "5.1 RESULTS OF DIFFERENT SNN TRAINING METHODS",
            "text": "We first evaluate the effectiveness of HLOP for different SNN training methods on the PMNIST dataset under the online and domain-incremental setting. As shown in Table 1, HLOP significantly improves the results by overcoming catastrophic forgetting under all training methods. The vanilla baseline (i.e., direct training for sequential tasks) suffers from significant forgetting (> 20%) that leads to poor accuracy, while HLOP can achieve almost no forgetting and behave similarly for different training methods. In the following, we leverage DSR as the representative for more settings."
        },
        {
            "heading": "5.2 RESULTS UNDER DIFFERENT DATASETS AND SETTINGS",
            "text": "Then we consider three different settings similar to Saha et al. (2021): (1) continual learning on PMNIST with fully connected networks and single-head classifier (i.e., domain-incremental) under the online setup (i.e., each sample only appears once); (2) continual learning on 10-split CIFAR100 with convolutional network and multi-head classifiers (i.e., task-incremental); (3) continual learning on 5-Datasets (i.e., the sequential learning of CIFAR-10, MNIST, SVHN, FashionMNIST, and notMNIST) with deeper ResNet-18 networks and multi-head classifiers (i.e., task-incremental).\nThese settings cover comprehensive evaluation with similar (CIFAR-100) or distinct (PMNIST, 5- Datasets) input domains, task- and domain-incremental settings, online or multi-epochs training,\nsimple to complex datasets, and shallow fully connected to deeper convolutional architectures. We also consider different error propagation methods including backpropagation (BP), feedback alignment (FA) (Lillicrap et al., 2016), and sign symmetric (SS) (Xiao et al., 2018). Since BP is often regarded as biologically implausible due to the \u201cweight transport\u201d problem (Lillicrap et al., 2016)1, FA and SS can handle the problem as more biologically plausible alternatives by relaxing feedback weights as random weights or only sharing the sign of forward weights (see Appendix B for more details). They differ from BP in the way to propagate error signals while still calculating gradients based on presynaptic activity traces and errors, so they are naturally compatible with HLOP. Since FA does not work well for large convolutional networks (Xiao et al., 2018), we only consider FA in the PMNIST task with fully connected layers and include SS in all settings.\nAs shown in Fig. 3, HLOP consistently solves catastrophic forgetting of SNNs under different settings as well as different error propagation methods. While in the baseline setting the forgetting differs for different error propagation methods, it is similar when HLOP is applied, demonstrating the consistent ability of HLOP to overcome forgetting. Compared with memory replay (MR) methods, HLOP achieves higher accuracy and less forgetting, and HLOP can also be combined with memory replay to further improve performance.\n(a) PMNIST\n-40\n-20\n0\n20\n40\n60\n80\n100\nAverage Acc Average BWT\nMul\ufffdtask Baseline HLOP HLOP (spiking, 40) HLOP (spiking, 100) HLOP (spiking, 200)\n-10\n-8\n-6\n-4\n-2\n0\n0 2 4 6 8 10\nAv er\nag e\nBW T\n(% )\nTask number\nBaseline\nHLOP\nHLOP (spiking, 40)\nHLOP (spiking, 100)\nHLOP (spiking, 200)\n-10\n-8\n-6\n-4\n-2\n0\n2\n0 2 4 6 8 10\nAv er\nag e\nBW T\n(% )\nBaseline\nHLOP\nHLOP (spiking, 400)\nHLOP (spiking, 1000)\nHLOP (spiking, 4000) -20\n0\n20\n40\n60\n80\nMul\ufffdtask Baseline HLOP HLOP (spiking, 400) HLOP (spiking, 1000) HLOP (spiking, 4000) 65 70 75\n80\n85\n0 2 4 6 8 10\nAv er\nag e\nAc c\n(% )\nMul\ufffdtask Baseline HLOP HLOP (spiking, 400) HLOP (spiking, 1000) HLOP (spiking, 4000)\n70\n75\n80\n85\n90\n95\n100\n0 2 4 6 8 10\nAv er\nag e\nAc c\n(% )\nTask number\nMul\ufffdtask Baseline HLOP HLOP (spiking, 40) HLOP (spiking, 100) HLOP (spiking, 200)"
        },
        {
            "heading": "5.3 RESULTS OF HLOP WITH LATERAL SPIKING NEURONS",
            "text": "We verify the effectiveness of HLOP with lateral spiking neurons and compare the results with the original linear HLOP in Fig. 4. As shown in the results, HLOP (spiking) also effectively deals with forgetting. And on PMNIST, HLOP (spiking) can achieve similar performance with a small number of time steps (e.g., 40). It shows that we can effectively leverage spiking neurons to implement the whole neural network model and achieve good performance for continual learning."
        },
        {
            "heading": "5.4 COMPARISON WITH OTHER CONTINUAL LEARNING METHODS",
            "text": "We compare the performance with other representative continual learning methods. We compare HLOP to the baseline, the small memory replay (MR) method, the regularization-based method EWC (Kirkpatrick et al., 2017), the parameter isolation method HAT (Serra et al., 2018), and the gradient projection-based method GPM (Saha et al., 2021). We consider Multitask as an upper bound, in which all tasks are available and learned together. Experiments on task-incremental 20- split miniImageNet are also compared following Saha et al. (2021). We also provide results for the domain-CL setting on 5-Datasets and comparison results between ANNs and SNNs in Appendix G.\n1That means the inverse connection weights between neurons should be exactly symmetric to the forward ones, which is hard to realize for unidirectional synapses in biological systems and neuromorphic hardware.\nAs shown in Table 2, HLOP consistently outperforms previous continual learning approaches under various settings considering both accuracy and backward transfer. The comparison results of accuracy for each task after learning successive ones by different methods on 5-Datasets is illustrated in Fig. 5, showing that HLOP successfully solves forgetting for each task and outperforms previous approaches. Compared with replay and regularization methods, HLOP enables less forgetting and higher accuracy due to the better constraint of weight update direction. Compared with the parameter isolation method, HLOP achieves higher accuracy due to the flexibility of network constraints with high-dimensional subspaces rather than explicit neuronal separation. Compared with the previous gradient projection-based method, HLOP achieves better performance since HLOP has a better construction of projection space from streaming data, especially on the 5-Datasets with distinct input distributions. Moreover, HLOP is based on purely neuronal operations friendly for neuromorphic hardware. These results show the superior performance of HLOP and indicate the potential for building high-performance continual neuromorphic computing systems."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this work, we propose a new method based on lateral connections and Hebbian learning for continual learning of SNNs. Our study demonstrates that lateral neural circuits and Hebbian learning can systematically provide strong continual learning ability by extracting principle subspaces of neural activities and modifying presynaptic activity traces for projection. This sheds light on how neural circuits and biological learning rules can support the advanced abilities of neuromorphic computing systems, which is rarely considered by popular neural network models. We are also the first to show how the popular thought of orthogonal projection can be realized in pure neuronal systems. Since our HLOP method is fully composed of neural circuits with neuronal operations and is effective for SNNs as well as more biologically plausible error propagation methods, it may be applied to neuromorphic computing systems. And the possible parallelism of the lateral connections and Hebbian learning with the forward and error propagation may better leverage its advantages. Although we mainly focus on gradient-based supervised learning for SNNs in this paper, HLOP may also be extended to other learning methods based on presynaptic activity traces. We expect that our approach can pave solid paths for building continual neuromorphic computing systems."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "Z. Lin was supported by National Key R&D Program of China (2022ZD0160302), the NSF China (No. 62276004), and the major key project of PCL, China (No. PCL2021A12). D. He was supported by National Science Foundation of China (NSFC62376007)."
        },
        {
            "heading": "A DETAILS OF SNN TRAINING METHODS AND COMBINATION WITH HLOP",
            "text": "In this section, we provide a detailed introduction to SNN training methods used in our work and the combination of the proposed HLOP with them.\nA.1 DSR\nDSR (Meng et al., 2022) is a training method based on the explicit encoding of spike trains and the analytical transformations between spike representations. For the LIF neuron model, DSR considers the discrete update equations as: u[t] = e\u2212 \u2206t \u03c4 v[t\u2212 1] + ( 1\u2212 e\u2212\u2206t\u03c4 ) I[t],\ns[t] = H(u[t]\u2212 Vth), v[t] = u[t]\u2212 Vths[t],\n(2)\nwhere u[t] is the membrane potential before spiking, s[t] is the output spike, v[t] is membrane potential after spiking and resetting, I[t] is the input current, t is the time step index, H is the Heaviside step function, Vth is the threshold, \u2206t is the discrete step, and \u03c4 is the time constant.\nDSR defines the spike representation of the spike train with T time steps as the weighted firing rate: a[T ] = Vth \u2211T t=1 \u03bb T\u2212ts[t]\u2211T\nt=1 \u03bb T\u2212t\u2206t\n, (3)\nwhere \u03bb = e\u2212 \u2206t \u03c4 . For multi-layer feedforward neural networks, let al[T ] denote the spike representations of neurons at layer l, DSR derives that al[T ] approximates sub-differentiable mappings zl with the analytical closed-form transformation as:\nzl[T ] = clamp ( 1\n\u03c4l Wl\u22121zl\u22121[T ], 0, V lth \u2206t\n) , (4)\nwhere Wl\u22121 is the weight matrix between layer l \u2212 1 and layer l. Then al[T ] can be viewed as following such transformation as well.\nWith the explicit encoding al[T ] of spike trains and the approximate closed-form transformations between al[T ], DSR backpropagates errors and calculates gradients based on al[T ] and their transformations, as shown in Fig. 6(b). The gradients for weights are calculated by \u2202L\n\u2202Wl =\n\u2202L \u2202aN [T ] \u220fN\u2212l\u22122 j=0 \u2202aN\u2212j [T ] \u2202aN\u2212j\u22121[T ] \u2202al+1[T ] \u2202Wl\n, and can be rewritten in the form as \u2207WlL = \u03b4l+1al[T ]\u22a4, where \u03b4l+1 is the backpropagated error signal. For static images, the inputs to SNNs are set as real-valued pixel values at each time step, which can be viewed as the input currents, and the loss is calculated based on the encoding aN [T ] at the last layer. To reduce the representation error, DSR additionally proposes to train the spike threshold and introduce a hyperparameter \u03b1 to adjust the threshold and reset potential. We maintain these techniques following the default setting in the released code. Hyperparameters are set as the default value (Meng et al., 2022): T = 20, Vth = 0.3, \u03c4 = 1.0,\u2206t = 0.05, \u03b1 = 0.3, V bound th = 0.0005, where V bound th is a hyperparameter to bound the trainable threshold.\nWhen applying our proposed HLOP to the DSR method, al[T ] can be viewed as the activity traces and we act on them, as shown in Fig. 6(b). Note that al[T ] is calculated based on spikes at each time step with different coefficients, so it is equivalent to applying HLOP to all spike signals: the calculation of the projection for weight update is the same. Considering Hebbian learning of lateral weights, there could be two choices to update weights: using the weighted firing rate (spike representation) for one calculation or using all spike trains at each time step for multiple calculations (i.e., Hebbian learning is carried out at each time step for multiple times). We verified both methods and their results are similar, as shown in Table 3. So we leverage the weighted firing rate by default for efficiency.\nThe thought to train SNNs with analytical transformations for the explicit encoding of spike trains is also shared by many works (Lee et al., 2016; Zhang & Li, 2019; Wu et al., 2021; Xiao et al., 2021; 2023; Mostafa, 2017; Comsa et al., 2020; Zhou et al., 2021). These works leverage (weighted) firing\nrate / accumulated response for transformations (Lee et al., 2016; Zhang & Li, 2019; Zhou et al., 2021; Wu et al., 2021; Xiao et al., 2021; 2023) or consider the first spiking time as the encoding and derive the analytical transformations (Mostafa, 2017; Comsa et al., 2020; Zhou et al., 2021).\nBy treating the explicit encoding as activity traces, our proposed HLOP can be applied to all these methods similarly to DSR. HLOP provides a general way to support continual learning for training methods based on presynaptic traces, which is orthogonal to specific SNN training approaches.\nA.2 BPTT WITH SG\nBackpropagation through time (BPTT) with surrogate gradients (SG) is a popular way to train SNNs without specific coding schemes (Shrestha & Orchard, 2018; Bellec et al., 2018; Wu et al., 2018; Zheng et al., 2021; Li et al., 2021; Fang et al., 2021; Yin et al., 2021). BPTT unfolds the discrete computational graph of SNNs through time and treats it similarly to recurrent neural networks, as shown in Fig. 6(c). The non-differentiable problem of the spiking function is tackled by leveraging the derivative of a smoothed function as a surrogate. Specifically, for the computation through time:{\nul [t+ 1] = \u03bb(ul[t]\u2212 Vthsl[t]) +Wl\u22121sl\u22121[t] + bl, sl[t+ 1] = H(ul [t+ 1]\u2212 Vth),\n(5)\nthe gradients are calculated as:\n\u2202L\n\u2202Wl = T\u2211 t=1 \u2202L \u2202sl+1[t] \u2202sl+1[t] \u2202ul+1[t]\n( \u2202ul+1[t]\n\u2202Wl + \u2211 \u03c4<t t\u2212\u03c4\u220f i=1 ( \u2202ul+1[t\u2212 i+ 1] \u2202ul+1[t\u2212 i] +\n\u2202ul+1[t\u2212 i+ 1] \u2202sl+1[t\u2212 i] \u2202sl+1[t\u2212 i] \u2202ul+1[t\u2212 i]\n) \u2202ul+1[\u03c4 ]\n\u2202Wl\n) ,\n(6)\nwhere \u2202L \u2202sl+1[t] is the error backpropagated to layer l + 1 at time t, and the non-differentiable terms \u2202sl[t] \u2202ul[t] will be replaced by surrogate derivatives, e.g., derivatives of rectangular or sigmoid func-\ntions (Wu et al., 2018): \u2202s\u2202u = 1 a1 sign ( |u\u2212 Vth| < a12 ) or \u2202s\u2202u = 1 a2 e(Vth\u2212u)/a2 (1+e(Vth\u2212u)/a2 )2 (a1 and a2 are hyperparameters). We leverage the sigmoid-like functions in our experiments and set a2 = 0.25. The gradients can be rewritten in the form as \u2207WlL = \u2211T t=1 \u03b4\u0302\nl+1[t]\u0302sl[t]\u22a4, where \u03b4\u0302l+1[t] is the error for each time step recursively backpropagated through time. For static images, the inputs to SNNs are also set as real-valued pixel values at each time step as the input currents, and the loss is calculated based on the firing rate at the last layer.\nWhen applying our proposed HLOP to the BPTT with SG method, the activity traces are all spikes s\u0302l[t] at each time step, and HLOP acts on all spike signals, as shown in Fig. 6(c). Both projection and Hebbian learning are based on spikes, and the traces of spikes are all projected to formulate the projected gradient of \u2207WlL.\nA.3 OTTT\nOTTT (Xiao et al., 2022) is a training method to improve BPTT for temporally online learning. BPTT requires storing the computational graph unfolded over time and backpropagating through previous time steps, which is inconsistent with biological online learning and learning rules on neuromorphic hardware. Some works introduce eligibility traces to improve BPTT for online training through time (Zenke & Ganguli, 2018; Bellec et al., 2020; Bohnstingl et al., 2022; Xiao et al., 2022), in which OTTT is an efficient and scalable approach with more theoretical guarantees. Specifically, OTTT analyzes the gradients of BPTT and proposes to decouple the temporal dependency by tracking the presynaptic activities and leveraging instantaneous gradients as:\na\u0302l[t] = \u2211 \u03c4\u2264t \u03bbt\u2212\u03c4sl[\u03c4 ], a\u0302l[t+ 1] = \u03bba\u0302l[t] + sl[t+ 1], (7)\ngul+1 [t] =\n( \u2202L[t]\n\u2202sN [t] N\u2212l\u22122\u220f i=0 \u2202sN\u2212i[t] \u2202sN\u2212i\u22121[t] \u2202sl+1[t] \u2202ul+1[t]\n)\u22a4 , (8)\n\u2207WlL = T\u2211\nt=1\ngul+1 [t]a\u0302 l[t]\n\u22a4 , (9)\nwhere L[t] = 1T L ( sL[t],y ) is the instantaneous loss and the total loss L = \u2211T t=1 L[t] is the upper bound of the commonly used firing-rate-based loss when L is a convex function such as crossentropy. With such calculation, OTTT does not require backpropagation through time and enables forward-in-time learning, as shown in Fig. 6(d).\nWhen applying our proposed HLOP to these eligibility traces-based methods, the activity traces correspond to the tracked traces at each time step, and HLOP acts on them, as shown in Fig. 6(d). Note that the tracked traces are calculated based on spike trains, so the projection of traces only requires modifying eligibility traces by lateral signals, i.e., the neuronal traces consider both spike responses for feedforward inputs and responses for lateral circuits. Specifically, the traces are originally accumulated by a\u0302l[t + 1] = \u03bba\u0302l[t] + sl[t + 1] for each neuronal spike, but will change to a\u0302l[t+1] = \u03bba\u0302l[t]+sl[t+1]+sl \u2212 [t+1] when HLOP is applied, considering the response sl\u2212[t+1] for lateral circuits with the input signal sl[t + 1], e.g., sl\u2212[t + 1] = \u2212Hl\u22a4Hlsl[t + 1]. In this way, both projection and Hebbian learning are based on spikes as well. The online training methods are more friendly for neuromorphic online learning, and the easy application of our HLOP with purely neuronal operations to these methods can pave a solid path for continual on-chip training of neuromorphic computing systems."
        },
        {
            "heading": "B DETAILS OF BACKPROPAGATION, FEEDBACK ALIGNMENT, AND SIGN SYMMETRIC",
            "text": "In this section, we provide a detailed introduction to backpropagation (BP), feedback alignment (FA), and sign symmetric (SS) used in our work.\nAs shown in Fig. 7, BP, FA, and SS mainly differ in the way they back-propagate error signals across layers. BP follows the chain rule of gradient calculation to propagate errors. Specifically, for the feedforward calculation y = Wx, the error will be backpropagated by \u2207xL = W\u22a4\u2207yL. This requires symmetric weights for forward and backward connections between neurons.\nFA (Lillicrap et al., 2016) relaxes the symmetric weight requirement and proposes to leverage random feedback weights F to propagate error. They observe that training with random feedback weights can lead to the alignment of forward weights and feedback weights, enabling effective learning. FA replaces the calculation of \u2207xL = W\u22a4\u2207yL for each layer by \u2207xL = F\u2207yL. There are other variants of FA such as direct feedback alignment (DFA) (N\u00f8kland, 2016) etc., and we consider FA in experiments. The feedback weights F are initialized following the same distribution as the forward weights, e.g., the Kaiming uniform distribution.\nSS (Xiao et al., 2018) also relaxes the strict symmetric weight requirement but does not use fully random feedback weights. They consider that it is possible to pass a little information about the sign direction between forward and feedback weights, and this enables SS to scale to large-scale models and tasks. SS replaces the calculation of \u2207xL = W\u22a4\u2207yL for each layer by \u2207xL = s \u00d7 sign(W)\u22a4\u2207yL, where s is a scale factor to prevent gradient explosion. We follow the same setting in Xiao et al. (2018)."
        },
        {
            "heading": "C MORE DETAILS OF HLOP",
            "text": "Our HLOP method adds recurrent lateral connections for each layer of neural network models. For the l-th layer of SNNs, the output spike trains sl[t] of feedforward neurons will also be passed to the lateral subspace neurons with skew-symmetric connection weights Hl and \u2212H\u22a4l . For DSR, the spike trains are mainly treated as the specified spike representation for the calculation of HLOP, and for BPTT with SG and OTTT, spikes or eligibility traces at each time step are considered for HLOP. In the following, we first denote activity traces as xl for simplicity.\nWhen learning the first task, there is no consolidated subspace but only newly expanded subspace neurons with weights H\u2032l. So there will be no projection for the feedforward synaptic update and it is still based on the activity trace xl. The lateral synaptic weight is updated by Hebbian learning as \u2206H\u2032l = y \u2032 lxl \u22a4 + y\u2032lx\u0303 \u22a4 l , where y \u2032 l = H \u2032 lxl, x\u0303l = \u2212H\u2032\u22a4l y\u2032l. Since the Hebbian learning can be viewed as stochastic gradient descent for minimizing an explicit objective function minH\u2032l Exl\n\u2225\u2225xl \u2212H\u2032\u22a4l H\u2032lxl\u2225\u22252 (Pehlevan et al., 2015), inspired by the momentum technique commonly used in gradient-based methods, we also introduce momentum update in practice. We use momentum to track the synaptic update and modify weights based on the momentum. The momentum is taken as 0.9 in practice. After learning each task, the newly expanded subspace neurons will be consolidated.\nWhen learning successive tasks, consolidated subspace neurons with weights Hl will project the activity traces, and new subspace neurons with weights H\u2032l will be constructed with Hebbian learning. Let yl = Hlxl,x\u2212l = \u2212H\u22a4l yl,y\u2032l = H\u2032lxl, and x \u2212\u2032 l = \u2212H\u2032\u22a4l y\u2032l denote signals of recurrent lateral connections of consolidated and new subspaces, respectively. x\u0302l = xl + x\u2212l is the modified activity trace and the feedforward synaptic update is calculated based on the projected activity trace x\u0302l, instead of the original one, as well as the error signal. x\u0303l = x\u2212l + x \u2212\u2032 l is the integrated postsynaptic response to the recurrent connections for Hebbian update. Only the new lateral synaptic weight H\u2032l is updated by Hebbian learning as \u2206H\u2032l = y \u2032 lx \u22a4 l + y \u2032 lx\u0303 \u22a4 l , which can be viewed as minimizing the\nobjective function minH\u2032l Exl \u2225\u2225xl \u2212H\u22a4l Hlxl \u2212H\u2032\u22a4l H\u2032lxl\u2225\u22252 to extract the new principal subspace not included in the existing subspace neurons. The momentum technique is also applied. And in practice, for each mini-batch data, we update lateral synaptic weights by K times to accelerate convergence. We take K = 5 and the learning rate as 0.01 for Hebbian learning in experiments. After learning the new task, the new subspace neurons will be consolidated and the connection weights H\u2032l are concatenated into Hl.\nFor convolutional operations, the shared kernel will linearly act on multiple patches of feature maps. We view the dimension of presynaptic inputs of feedforward neural networks as the dimension of these patches and view them as different input samples. Hebbian learning is similarly applied to the shared lateral connections between these patches and subspace neurons."
        },
        {
            "heading": "D MORE DETAILS OF HLOP WITH LATERAL SPIKING NEURONS",
            "text": "In this section, we provide more details about HLOP with lateral spiking neurons. We consider that the subspace neurons in the lateral circuits can generate high-frequency bursts in a short time and then the neuron model is approximated by the non-leaky integrate and fire (IF) model. The computation of the IF neuron model with input current I is described as:{\nu [t+ 1] = (u[t]\u2212 Vths[t]) + I, s[t+ 1] = H(u [t+ 1]\u2212 Vth),\n(10)\nIt is easy to prove that the scaled output spiking rate of the IF model, i.e., r[T ] = VthT \u2211T t=1 s l[t], is a quantization of the input when I is positive, i.e., r[T ] = VthT \u230a TI Vth\n\u230b with a clipping to the range [0, Vth]. If we initialize the membrane potential as 0 and modify the spiking threshold and reset potential as Vth2 and \u2212 Vth 2 , respectively, we can obtain a more precise quantization as r[T ] = Vth T [ TI Vth\n]. By considering the rate of high-frequency bursts and the response to the burst, we can obtain HLOP with lateral spiking neurons by taking the output y of subspace neurons as the quantization of Hx and the (scaled) response to burst spikes as x\u2212 = \u2212H\u22a4y, as shown in Fig. 8(a).\nOne problem is that the common neuron models only spike for positive inputs, limiting the ability to deal with negative signals. We require ternary spiking in order to imitate the original linear neurons of HLOP, i.e., the spiking function should be:\ns[t+ 1] = T (u[t+ 1], Vth) =  1, u[t+ 1] > Vth\n0, |u[t+ 1]| \u2264 Vth \u2212 1, u[t+ 1] < \u2212Vth . (11)\nOne possible approach is to enable such ternary spiking on hardware. Another way is to realize the ternary output by neuron couples with the common positively spiking neuron model, as introduced in (Xiao et al., 2023). The conceptual illustration of such ternary neuron couples is shown in Fig. 8(c), where two neurons receive opposite inputs and share a small recurrent connection to synchronize resetting. The operation of taking negative can be realized by reconnecting neurons with opposite weights, i.e., the two coupled neurons are oppositely connected to other neurons. Fig. 8(d) illustrates such realization for HLOP modules. This may require weight sharing between connections to neuron couples.\nWith such a design, we can leverage spiking neurons to imitate linear neurons except for some quantization. We simulate our spiking HLOP in experiments by quantizing the output of subspace neurons in experiments. Specifically, the output y is quantized as y\u0302 = scale\u00d7 [ clamp(y,\u2212scale,scale)\nscale \u00d7Tl] Tl\n. Both projection and Hebbian learning are based on y\u0302. Since it is expected to be realized by highfrequency bursts in a short time, the spiking HLOP can have a different time scale than spiking neurons in the feedforward network as shown in Fig. 8(b), and projection and Hebbian learning with rate coding of burst spikes can be carried out for all spikes/eligibility traces in the feedforward network at each discrete time step which has a larger temporal interval."
        },
        {
            "heading": "E EXPERIMENTAL DETAILS",
            "text": "Our experiments are conducted on PMNIST, 10-split CIFAR-100, 5-Datasets, and 20-split miniImageNet. The PMNIST dataset is a variant of the MNIST dataset and each task is a different random\npermutation of the original pixels. The 10 sequential tasks share the same classifier with 10 classes\u2019 output. This single-head classifier setting will require orthogonal projection for the update of the classifier. The 10-split CIFAR-100 dataset splits 100 classes of the CIFAR-100 dataset into 10 tasks with 10 classes per task. 5-Datasets is a sequential of CIFAR-10, MNIST, SVHN, Fashion MNIST, and notMNIST, where each task has 10 classes. The 20-split miniImageNet dataset splits 100 classes of the miniImageNet dataset into 20 tasks with 5 classes per task. These three settings use multihead classifiers for different tasks and do not need projection for classifiers. We normalize the inputs based on the dataset statistics and do not use any data augmentation. We do not split the training and validation set and take the model at the last training epoch as the final model for testing because it would be vague to determine the best model considering both the accuracy and fitness of Hebbian learning.\nAs for the network structures, for experiments on PMNIST, we consider the fully connected networks with two hidden layers 784-800-800-10, and all tasks share the same classifier. For experiments on 10-split CIFAR-100, we consider three convolutional hidden layers whose kernel size is 3 and whose channels are 64, 128, and 256, and different tasks have different final classifiers. A batch normalization (BN) is added after each convolutional operation, which follows the practice in DSR (Meng et al., 2022) and can be absorbed into linear layers after training. Average pooling is added after each spiking layer. For experiments on 5-Datasets, we consider a reduced ResNet-18 network following previous work (Saha et al., 2021), whose base channel is set as 20, and different tasks have different final classifiers. For experiments on 20-split miniImageNet, we also consider the reduced ResNet-18 network, and the first convolution has stride 2 and kernel size 5, similar to Saha et al. (2021). For SNN models, the input and output encodings follow the practice in different training methods (Meng et al., 2022; Xiao et al., 2022) when we use them (BPTT with SG and OTTT follow the same setting as in Xiao et al. (2022) and BPTT with SG uses the rate-based loss), and other details and hyperparameters also follow them, e.g., we take the default 20 time steps for DSR while the default 6 time steps for BPTT with SG and OTTT.\nFor the continual learning setting, we only track the statistics of BN for the first task and will fix them for the successive tasks following the practice in Mallya & Lazebnik (2018), in order to avoid interference with old tasks. The learnable threshold in the DSR method is also learned in the first task and fixed in the following tasks. We follow the common practice to track statistics during training and use stored statistics during testing.\nOur comparison methods include baseline, small memory replay (MR), EWC (Kirkpatrick et al., 2017), HAT (Serra et al., 2018), and GPM (Saha et al., 2021). The baseline method simply trains models for each task sequentially. The MR method randomly stores 50 input samples for each class in current tasks and replays all stored samples for learning with additional 20 epochs after each new task (for miniImageNet, the stored sample size is 5 for each class, corresponding to one percent of data similar to other settings). EWC and HAT are implemented by adapting the official code in Serra et al. (2018), and GPM is implemented by adapting the official implementation. We also consider the Multitask method as an upper bound that simultaneously learns all tasks at once. All methods share the same training setting.\nFor PMNIST, we consider the online setting and train models for 1 epoch by SGD with a batch size of 64 and a learning rate of 0.1. The Multitask setting for PMNIST is slightly different as the batch size of each task is set as 8 to enable a similar number of weight updates under the online setting and the learning rate is taken as 0.01. The hyperparameters for HAT, EWC, and GPM are taken as the default in the code. For HLOP, both hidden layers and the classifier require orthogonal projection, and the number of subspace neurons for the three layers is set as [80, 200, 100] (from bottom to top) at the first task. For each later task, the number of newly expanded subspace neurons is initially [70, 70, 70] for each layer and is reduced by 20 every 3 tasks.\nFor 10-split CIFAR-100, we train all models for 200 epochs by SGD with momentum 0.9. The batch size is 64, and the learning rate is 0.01 with the cosine annealing scheduler to 0 as well as a linear warm-up in the first 5 epochs. The hyperparameters for EWC, HAT, and GPM are taken as the default in the code. For HLOP, the number of subspace neurons is set as [6, 100, 200] for each layer at the first task (around 1/5 as the dimension of presynaptic inputs), and the number of newly expanded subspace neurons for successive tasks is set as [2, 20, 40]. For the combination between HLOP and MR, we do not update lateral connections during replay.\nFor 5-Datasets, we train all models for 100 epochs by SGD with momentum 0.9. The batch size is 64, and the initial learning rate is 0.1 for the first task and 0.01 for successive tasks, with the cosine annealing scheduler to 0. The hyperparameters for EWC, HAT, and GPM are taken as the default in the code. For HLOP, the number of subspace neurons is set as [6, [40, 40], [40, 40], [40, 100, 6], [100, 100], [100, 200, 8], [200, 200], [200, 200, 16], [200, 200]] for each presynaptic layer in ResNet-18 at the first task (around 1/5 as the dimension of presynaptic inputs) and the number of newly expanded neurons is the same.\nFor 20-split miniImageNet, we train all models for 100 epochs by SGD with momentum 0.9. The batch size is 64, and the initial learning rate is 0.1 for the first task and 0.01 for successive tasks, with the cosine annealing scheduler to 0. The learning rate for multitask is 0.01. The hyperparameters for EWC, HAT, and GPM are taken as the default in the code/paper. For HLOP, the number of subspace neurons is set as [24, [90, 90], [90, 90], [90, 180, 10], [180, 180], [180, 360, 20], [360, 360], [360, 720, 40], [720, 720]] for each presynaptic layer in ResNet-18 at the first task (around 1/2 as the dimension of presynaptic inputs) and the number of newly expanded neurons is [2, [6, 6], [6, 6], [6, 12, 1], [12, 12], [12, 24, 2], [24, 24], [24, 48, 4], [48, 48]] (around 1/30) for the first five successive tasks and [0, [2, 2], [2, 2], [2, 4, 0], [4, 4], [4, 8, 0], [8, 8], [8, 16, 0], [16, 16]] (around 1/90) for others. The number is decreasing because there are many shared subspaces between different tasks (miniImageNet has similar input domains) and the considered network capacity is not large enough.\nCurrently, the number of subspace neurons is a hyperparameter for HLOP, which is similar to the threshold hyperparameter in GPM (Saha et al., 2021). It may be determined with some prior knowledge about the task difficulty, e.g., first train the network for a short time to decide the roughly required number (for example, the ratio of the dimension of presynaptic inputs) and then formally perform training with Hebbian learning. It can be interesting future work to study how to automatically and adaptively determine the number with neuronal mechanisms.\nAll code implementations are based on the PyTorch framework and experiments are carried out on one NVIDIA GeForce RTX 3090 GPU. All experiments and dataset split are under the same random seed 2022."
        },
        {
            "heading": "F MORE DISCUSSION",
            "text": "F.1 DISCUSSION ON HEBBIAN LEARNING\nOne major component of our method is Hebbian learning. Hebbian learning is often related to memory modeling, e.g., Hopfield network (Hopfield, 1982) with Hebbian-type learning can serve as associative memory systems. In this work, we provide a new perspective on how Hebbian learning can support advanced learning abilities, which is not in an associative approach. Extraction of the principal subspace of streaming data is to some extent similar to memory but is not in an associative approach, and can well fit into our projection-based continual learning method. This work mainly focuses on Hebbian learning based on neuronal spiking activities, which can be shown to optimize synaptic weights for principal component analysis (Oja, 1982; 1989). In biological systems, spiketiming-dependent plasticity (STDP) is the often observed Hebbian learning considering the time of spikes (Caporale et al., 2008). However, STDP has not been shown to optimize an explicit objective. It is interesting future work to study if the biological STDP rule can be similarly leveraged to systematically promote advanced abilities such as continual learning.\nA basic focus of our work is neuromorphic computing. Neuromorphic computing aims at the computation inspired by the structure and function of the human brain. At the hardware level, neuromorphic chips are designed to imitate biological neurons (Akopyan et al., 2015; Davies et al., 2018; Pei et al., 2019), such as their spiking property and synaptic connections with local storage of weights for in-memory computation, for highly energy-efficient and parallel event-driven computation with avoidance of frequent memory transpose (its computation architecture is expected to be different from the commonly used hardware with von Neumann architecture such as CPU or GPU). At the algorithm level, we are interested in developing methods compatible with some properties and operations of neurons so that they are possible for deployment on the hardware. Also, since neuromorphic hardware is under development considering software-hardware co-design, most algorithms are simulated on common hardware (e.g., GPU) while considering neuromorphic properties. From the perspective of neuromorphic computing, Hebbian learning can be mapped to the computation of\nneurons and synapses and implemented on neuromorphic hardware, so it is more suitable for SNNs than other PCA approximation algorithms.\nF.2 DISCUSSION OF LIMITATIONS/CHALLENGES\nAs for our method, HLOP introduces additional computational costs for learning the lateral weights of each layer during training, which will increase training costs in our current implementation codes. While this process can theoretically be parallel to the normal forward-backward propagation of network training as discussed in Section 4.1, such parallelization may not be easily realized in established deep learning libraries such as PyTorch. So for our implementation of GPU training, the training time would be slightly longer and the evaluation of parallelization is lacking. It can be future work on low-level code optimization or consideration of asynchronously parallel neuromorphic computing hardware. Additionally, HLOP currently requires a manual specification for the number of subspace neurons, which may be improved to automatically and adaptively determine the allocation. For example, the Generalized Hebbian Algorithm can perform Gram-Schmidt orthonormalization on the rows of the weight matrix, which may help to sweep out unnecessary neurons and connections, but it requires some non-local information and may not be directly suitable to our method. It can be interesting for future work to study improvements.\nAs for the evaluation, similar to other gradient projection methods, this work mainly focuses on taskincremental and domain-incremental continual learning settings. There is another class-incremental setting which, as discussed in Section 2, inevitably requires some kind of replay of old experiences for good performance since it expects explicit comparison between new classes and old ones. So following previous works, our evaluation mainly focuses on task- and domain-incremental settings. It can be future work to study if HLOP can be combined with some replay methods or contextdependent processing modules similar to biological systems (e.g., with a task classifier) for better class-incremental tasks."
        },
        {
            "heading": "G ADDITIONAL RESULTS",
            "text": "G.1 DOMAIN-CL ON 5-DATASETS\nTo further show the scalability of our method for the domain-incremental setting on larger datasets and networks apart from PMNIST, we supplement the domain-CL results on 5-Datasets. Compared with the task-incremental settings, it shares the final classifier for all tasks, i.e., considering a singlehead classifier. All models are trained for 10 epochs and MR also performs 10 epochs for additional replay training. As shown in Table 4 (where HAT requires task ID and is not feasible), HLOP significantly outperforms other methods, indicating the superiority of our method.\nG.2 COMPARISON BETWEEN ANNS AND SNNS\nTo further study whether the improvement of our method over others mainly comes from the better combination with SNNs or the continual learning ability, we supplement all the corresponding results of ANNs that replace the spiking neuron by the ReLU activation.\nAs shown in Table 5, the performance of some methods can differ considering ANNs and SNNs. Particularly, on 5-Datasets, HLOP has more improvements for SNNs. It may imply that our method\ncan be better combined with spikes, for example probably because subspaces expanded by spike signals are harder to learn and therefore our Hebbian learning performing streaming PCA has more advantages over GPM that only performs SVD/PCA on a small batch of data which can be biased. Also, our method is promising for ANNs as well. As there is no systematic study on the difference between the continual learning performance of ANNs and SNNs, it can be interesting for future work to study it."
        }
    ],
    "year": 2024
}