{
    "abstractText": "The objective of domain generalization (DG) is to enhance the transferability of the model learned from a source domain to unobserved domains. To prevent overfitting to a specific domain, Sharpness-Aware Minimization (SAM) reduces source domain\u2019s loss sharpness. Although SAM variants have delivered significant improvements in DG, we highlight that there\u2019s still potential for improvement in generalizing to unknown domains through the exploration on data space. This paper introduces an objective rooted in both parameter and data perturbed regions for domain generalization, coined Unknown Domain Inconsistency Minimization (UDIM). UDIM reduces the loss landscape inconsistency between source domain and unknown domains. As unknown domains are inaccessible, these domains are empirically crafted by perturbing instances from the source domain dataset. In particular, by aligning the loss landscape acquired in the source domain to the loss landscape of perturbed domains, we expect to achieve generalization grounded on these flat minima for the unknown domains. Theoretically, we validate that merging SAM optimization with the UDIM objective establishes an upper bound for the true objective of the DG task. In an empirical aspect, UDIM consistently outperforms SAM variants across multiple DG benchmark datasets. Notably, UDIM shows statistically significant improvements in scenarios with more restrictive domain information, underscoring UDIM\u2019s generalization capability in unseen domains. Our code is available at https://github.com/SJShin-AI/UDIM.",
    "authors": [
        {
            "affiliations": [],
            "name": "Seungjae Shin"
        },
        {
            "affiliations": [],
            "name": "HeeSun Bae"
        },
        {
            "affiliations": [],
            "name": "Byeonghu Na"
        },
        {
            "affiliations": [],
            "name": "Yoon-Yeong Kim"
        },
        {
            "affiliations": [],
            "name": "Il-Chul Moon"
        }
    ],
    "id": "SP:55d7b1487b2884a514c3fe0754d147245ffd21be",
    "references": [
        {
            "authors": [
                "Martin Arjovsky",
                "L\u00e9on Bottou",
                "Ishaan Gulrajani",
                "David Lopez-Paz"
            ],
            "title": "Invariant risk minimization",
            "venue": "arXiv preprint arXiv:1907.02893,",
            "year": 2019
        },
        {
            "authors": [
                "Gilles Blanchard",
                "Aniket Anand Deshmukh",
                "\u00dcrun Dogan",
                "Gyemin Lee",
                "Clayton Scott"
            ],
            "title": "Domain generalization by marginal transfer learning",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Liang Chen",
                "Yong Zhang",
                "Yibing Song",
                "Ying Shan",
                "Lingqiao Liu"
            ],
            "title": "Improved test-time adaptation for domain generalization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Liang Chen",
                "Yong Zhang",
                "Yibing Song",
                "Anton van den Hengel",
                "Lingqiao Liu"
            ],
            "title": "Domain generalization via rationale invariance",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Casey Chu",
                "Kentaro Minami",
                "Kenji Fukumizu"
            ],
            "title": "Smoothness and stability in gans",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Gabriela Csurka"
            ],
            "title": "Domain adaptation for visual applications: A comprehensive survey",
            "venue": "arXiv preprint arXiv:1702.05374,",
            "year": 2017
        },
        {
            "authors": [
                "Felix Dangel",
                "Frederik Kunstner",
                "Philipp Hennig"
            ],
            "title": "BackPACK: Packing more into backprop",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Gintare Karolina Dziugaite",
                "Daniel M Roy"
            ],
            "title": "Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data",
            "venue": "arXiv preprint arXiv:1703.11008,",
            "year": 2017
        },
        {
            "authors": [
                "Dan Feldman"
            ],
            "title": "Introduction to core-sets: an updated survey",
            "venue": "arXiv preprint arXiv:2011.09384,",
            "year": 2020
        },
        {
            "authors": [
                "Pierre Foret",
                "Ariel Kleiner",
                "Hossein Mobahi",
                "Behnam Neyshabur"
            ],
            "title": "Sharpness-aware minimization for efficiently improving generalization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Yaroslav Ganin",
                "Evgeniya Ustinova",
                "Hana Ajakan",
                "Pascal Germain",
                "Hugo Larochelle",
                "Fran\u00e7ois Laviolette",
                "Mario Marchand",
                "Victor Lempitsky"
            ],
            "title": "Domain-adversarial training of neural networks. The journal of machine learning",
            "year": 2030
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "arXiv preprint arXiv:1412.6572,",
            "year": 2014
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Thomas Dietterich"
            ],
            "title": "Benchmarking neural network robustness to common corruptions and perturbations",
            "venue": "arXiv preprint arXiv:1903.12261,",
            "year": 2019
        },
        {
            "authors": [
                "Zeyi Huang",
                "Haohan Wang",
                "Eric P Xing",
                "Dong Huang"
            ],
            "title": "Self-challenging improves cross-domain generalization",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "JoonHo Jang",
                "Byeonghu Na",
                "Dong Hyeok Shin",
                "Mingi Ji",
                "Kyungwoo Song",
                "Il chul Moon"
            ],
            "title": "Unknown-aware domain adversarial learning for open-set domain adaptation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Minyoung Kim",
                "Da Li",
                "Shell X Hu",
                "Timothy Hospedales"
            ],
            "title": "Fisher sam: Information geometry and sharpness aware minimisation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Yoon-Yeong Kim",
                "Youngjae Cho",
                "Joonho Jang",
                "Byeonghu Na",
                "Yeongmin Kim",
                "Kyungwoo Song",
                "Wanmo Kang",
                "Il-Chul Moon"
            ],
            "title": "SAAL: Sharpness-aware active learning",
            "venue": "Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "David Krueger",
                "Ethan Caballero",
                "Joern-Henrik Jacobsen",
                "Amy Zhang",
                "Jonathan Binas",
                "Dinghuai Zhang",
                "Remi Le Priol",
                "Aaron Courville"
            ],
            "title": "Out-of-distribution generalization via risk extrapolation (rex)",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jungmin Kwon",
                "Jeongseop Kim",
                "Hyunseo Park",
                "In Kwon Choi"
            ],
            "title": "Asam: Adaptive sharpnessaware minimization for scale-invariant learning of deep neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Beatrice Laurent",
                "Pascal Massart"
            ],
            "title": "Adaptive estimation of a quadratic functional by model selection",
            "venue": "Annals of statistics,",
            "year": 2000
        },
        {
            "authors": [
                "Da Li",
                "Yongxin Yang",
                "Yi-Zhe Song",
                "Timothy M Hospedales"
            ],
            "title": "Deeper, broader and artier domain generalization",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Da Li",
                "Yongxin Yang",
                "Yi-Zhe Song",
                "Timothy Hospedales"
            ],
            "title": "Learning to generalize: Meta-learning for domain generalization",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Haoliang Li",
                "Sinno Jialin Pan",
                "Shiqi Wang",
                "Alex C Kot"
            ],
            "title": "Domain generalization with adversarial feature learning",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Lei Li",
                "Ke Gao",
                "Juan Cao",
                "Ziyao Huang",
                "Yepeng Weng",
                "Xiaoyue Mi",
                "Zhengze Yu",
                "Xiaoya Li",
                "Boyang Xia"
            ],
            "title": "Progressive domain expansion network for single domain generalization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Pan Li",
                "Da Li",
                "Wei Li",
                "Shaogang Gong",
                "Yanwei Fu",
                "Timothy M Hospedales"
            ],
            "title": "A simple feature augmentation for domain generalization",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8886\u20138895,",
            "year": 2021
        },
        {
            "authors": [
                "Ya Li",
                "Xinmei Tian",
                "Mingming Gong",
                "Yajing Liu",
                "Tongliang Liu",
                "Kun Zhang",
                "Dacheng Tao"
            ],
            "title": "Deep domain generalization via conditional invariant adversarial networks",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "arXiv preprint arXiv:1706.06083,",
            "year": 2017
        },
        {
            "authors": [
                "David A McAllester"
            ],
            "title": "Pac-bayesian model averaging",
            "venue": "In Proceedings of the twelfth annual conference on Computational learning theory, pp",
            "year": 1999
        },
        {
            "authors": [
                "Hyeonseob Nam",
                "HyunJae Lee",
                "Jongchan Park",
                "Wonjun Yoon",
                "Donggeun Yoo"
            ],
            "title": "Reducing domain gap by reducing style bias",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Giambattista Parascandolo",
                "Alexander Neitz",
                "ANTONIO ORVIETO",
                "Luigi Gresele",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Learning explanations that are hard to vary",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Giambattista Parascandolo",
                "Alexander Neitz",
                "Antonio Orvieto",
                "Luigi Gresele",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Learning explanations that are hard to vary",
            "venue": "arXiv preprint arXiv:2009.00329,",
            "year": 2020
        },
        {
            "authors": [
                "Xingchao Peng",
                "Qinxun Bai",
                "Xide Xia",
                "Zijun Huang",
                "Kate Saenko",
                "Bo Wang"
            ],
            "title": "Moment matching for multi-source domain adaptation",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Fengchun Qiao",
                "Long Zhao",
                "Xi Peng"
            ],
            "title": "Learning to learn single domain generalization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Fengchun Qiao",
                "Long Zhao",
                "Xi Peng"
            ],
            "title": "Learning to learn single domain generalization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Alexandre Rame",
                "Corentin Dancette",
                "Matthieu Cord"
            ],
            "title": "Fishr: Invariant gradient variances for out-of-distribution generalization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Harsh Rangwani",
                "Sumukh K Aithal",
                "Mayank Mishra",
                "Arihant Jain",
                "Venkatesh Babu Radhakrishnan"
            ],
            "title": "A closer look at smoothness in domain adversarial training",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Shiori Sagawa",
                "Pang Wei Koh",
                "Tatsunori B Hashimoto",
                "Percy Liang"
            ],
            "title": "Distributionally robust neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Nicol N Schraudolph"
            ],
            "title": "Fast curvature matrix-vector products for second-order gradient descent",
            "venue": "Neural computation,",
            "year": 2002
        },
        {
            "authors": [
                "Seungjae Shin",
                "Heesun Bae",
                "Donghyeok Shin",
                "Weonyoung Joo",
                "Il-Chul Moon"
            ],
            "title": "Loss-curvature matching for dataset selection and condensation",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Changjian Shui",
                "Boyu Wang",
                "Christian Gagn\u00e9"
            ],
            "title": "On the benefits of representation regularization in invariance based domain generalization",
            "venue": "Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Baochen Sun",
                "Kate Saenko"
            ],
            "title": "Deep coral: Correlation alignment for deep domain adaptation",
            "venue": "In Computer Vision\u2013ECCV 2016 Workshops:",
            "year": 2016
        },
        {
            "authors": [
                "Hemanth Venkateswara",
                "Jose Eusebio",
                "Shayok Chakraborty",
                "Sethuraman Panchanathan"
            ],
            "title": "Deep hashing network for unsupervised domain adaptation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Yoav Wald",
                "Amir Feder",
                "Daniel Greenfeld",
                "Uri Shalit"
            ],
            "title": "On calibration and out-of-domain generalization",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jindong Wang",
                "Cuiling Lan",
                "Chang Liu",
                "Yidong Ouyang",
                "Tao Qin",
                "Wang Lu",
                "Yiqiang Chen",
                "Wenjun Zeng",
                "Philip Yu"
            ],
            "title": "Generalizing to unseen domains: A survey on domain generalization",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Pengfei Wang",
                "Zhaoxiang Zhang",
                "Zhen Lei",
                "Lei Zhang"
            ],
            "title": "Sharpness-aware gradient matching for domain generalization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Zijian Wang",
                "Yadan Luo",
                "Ruihong Qiu",
                "Zi Huang",
                "Mahsa Baktashmotlagh"
            ],
            "title": "Learning to diversify for single domain generalization",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zijian Wang",
                "Yadan Luo",
                "Ruihong Qiu",
                "Zi Huang",
                "Mahsa Baktashmotlagh"
            ],
            "title": "Learning to diversify for single domain generalization",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Shen Yan",
                "Huan Song",
                "Nanxiang Li",
                "Lincan Zou",
                "Liu Ren"
            ],
            "title": "Improve unsupervised domain adaptation with mixup training",
            "venue": "arXiv preprint arXiv:2001.00677,",
            "year": 2020
        },
        {
            "authors": [
                "Sangdoo Yun",
                "Dongyoon Han",
                "Seong Joon Oh",
                "Sanghyuk Chun",
                "Junsuk Choe",
                "Youngjoon Yoo"
            ],
            "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N Dauphin",
                "David Lopez-Paz"
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Marvin Zhang",
                "Henrik Marklund",
                "Nikita Dhawan",
                "Abhishek Gupta",
                "Sergey Levine",
                "Chelsea Finn"
            ],
            "title": "Adaptive risk minimization: Learning to adapt to domain shift",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Xingxuan Zhang",
                "Renzhe Xu",
                "Han Yu",
                "Yancheng Dong",
                "Pengfei Tian",
                "Peng Cu"
            ],
            "title": "Flatness-aware minimization for domain generalization",
            "venue": "arXiv preprint arXiv:2307.11108,",
            "year": 2023
        },
        {
            "authors": [
                "Xingxuan Zhang",
                "Renzhe Xu",
                "Han Yu",
                "Hao Zou",
                "Peng Cui"
            ],
            "title": "Gradient norm aware minimization seeks first-order flatness and improves generalization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Zhun Zhong",
                "Yuyang Zhao",
                "Gim Hee Lee",
                "Nicu Sebe"
            ],
            "title": "Adversarial style augmentation for domain generalized urban-scene segmentation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Yongxin Yang",
                "Yu Qiao",
                "Tao Xiang"
            ],
            "title": "Domain generalization with mixstyle",
            "venue": "arXiv preprint arXiv:2104.02008,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Ziwei Liu",
                "Yu Qiao",
                "Tao Xiang",
                "Chen Change Loy"
            ],
            "title": "Domain generalization: A survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Zhang"
            ],
            "title": "GENERALIZATION Gradient norm-Aware Minimization",
            "year": 2023
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "Sharpness-Aware Gradient Matching",
            "year": 2023
        },
        {
            "authors": [],
            "title": "2023a) concurrently optimizes both zerothorder and first-order flatness to identify flatter minima. To compute various sharpness metric on the different order, it incurs a higher computational cost",
            "venue": "B PROOFS AND DISCUSSIONS",
            "year": 2023
        },
        {
            "authors": [
                "Rame"
            ],
            "title": "2022) empirically supports that the similarity between Hessian diagonals and gradient variances is over 99.99%. Loss Matching Matching the gradient variances for all parameters of our model, f\u03b8",
            "year": 2022
        },
        {
            "authors": [
                "Dangel"
            ],
            "title": "Experimental settings For PACS and OfficeHome, we trained for total of 5,000 iterations. For DomainNet, we trained for 15,000 iterations. For CIFAR-10, since it usually trains for 100 epochs, we translate it to iterations, which becomes total of 781\u00d7100",
            "year": 2020
        },
        {
            "authors": [
                "IRM (Arjovsky"
            ],
            "title": "2019) tries to learn a data representation such that the optimal classifier matches for all training distributions. Specifically, it minimizes the empirical risk and the regularization term, the multiplication of samples",
            "year": 2019
        },
        {
            "authors": [
                "Mixup (Yan"
            ],
            "title": "2020) is a mixup among domains. Cutmix (Yun et al., 2019) is another skill which is widely used in machine learning community to boost generalization. Specifically, it mixes up parts of inputs randomly by pixel-wise",
            "year": 2019
        },
        {
            "authors": [
                "Mixstyle (Zhou"
            ],
            "title": "2021) mix up the statistics (specifically, mean and standard deviation) of the feature. The mixed feature statistics are applied to the style-normalized",
            "year": 2021
        },
        {
            "authors": [
                "MTL (Blanchard"
            ],
            "title": "2021) considers the exponential moving average (EMA) of features. MLDG (Li et al., 2018a) is a meta learning based method for domain generalization. Specifically, it simulates the domain shift between train and test during training procedure by synthesizing virtual testing domains within each mini-batch",
            "year": 2018
        },
        {
            "authors": [
                "CORAL (Sun",
                "Saenko"
            ],
            "title": "MMD. However, while MMD employs the gaussian kernel to measure the feature discrepancy, CORAL aligns the second-order statistics between different distributions with a nonlinear transformation. This alignment is achieved by matching the correlations of layer activations in deep neural networks",
            "year": 2016
        },
        {
            "authors": [
                "Fishr (Rame"
            ],
            "title": "2022) approximates the hessian as the variance of gradient matrix, and they align the gradient variance of each domain",
            "year": 2022
        },
        {
            "authors": [
                "SAM (Foret"
            ],
            "title": "2020) is an optimization technique to consider the sharpness of loss surface. It first perturbs parameter to its worst direction, gets gradient and update the calculated gradient at the original parameter point",
            "year": 2020
        },
        {
            "authors": [
                "Shui"
            ],
            "title": "2022) introduced novel regularization techniques for the embedding function based on theoretical analysis. Their approach involves establishing an upper bound on the balanced error rate in the test environment, which is obtained from the combination of the balanced error rates in the source environments, the feature-conditional invariance, and the smoothness of the embedding",
            "year": 2022
        },
        {
            "authors": [
                "Zhou"
            ],
            "title": "its dependency on learning signals such as parameters or loss, dividing it into not-learnable domain augmentation Zhang et al",
            "year": 2021
        },
        {
            "authors": [
                "PGD Madry"
            ],
            "title": "2017) do not consider the local parameter curvature in their perturbation process. By integrating perturbations on instances with attention to parameter loss curvature; and parameter perturbation, we facilitate the modeling of inconsistency in unknown domains, as described in Eq 3. Having said that, Kim et al. (2023) also utilize worst-case instance selection on the active learning framework by utilizing the parameter perturbation",
            "year": 2023
        },
        {
            "authors": [
                "Shin"
            ],
            "title": "2023) also utilizes the perturbed parameter region to attain samples which effectively represent whole dataset. From a mathematical perspective, UDIM\u2019s data perturbation involves receiving not only gradients related to the simple cross-entropy loss but also additional gradients concerning the norm of gradient",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Domain Generalization (DG) (Zhou et al., 2022; Wang et al., 2022) focuses on domain shift that arises when training and testing occur across distinct domains, i.e. a domain of real pictures in training, and a separate domain of cartoon images in testing. The objective of DG is to train a model on a given source domain dataset, and generalizes well on other unobserved domains. To address the domain discrepancy between the source domain and other domains, various methods have been proposed: 1) alignment-based methods (Li et al., 2021a; Wald et al., 2021); 2) augmentation-based methods (Qiao et al., 2020a; Zhou et al., 2021); and 3) regularization-based methods (Arjovsky et al., 2019; Krueger et al., 2021; Rame et al., 2022). While these methodologies have demonstrated promising results, they often underperform in settings where given domain information is particularly limited (Wang et al., 2021b; Qiao et al., 2020b). Also, most methods lack theoretical guarantees on the minimization of the target risk at the distribution level.\nIn contrast to the aforementioned methods, Sharpness-aware optimizers, which flatten the loss landscape over a perturbed parameter region, demonstrate promising performances in DG tasks (Zhang et al., 2023b; Wang et al., 2023). By optimizing the perturbed local parameter regions, these approaches relieve the model overfitting to a specific domain, thereby enhancing the adaptability of the model across various domains. Also, this concept has a solid theoretical foundation based on the parameter space analysis with PAC-Bayes theories (McAllester, 1999; Dziugaite & Roy, 2017).\nWhile the perturbation methods based on the parameter space have shown promising improvements in DG tasks, this paper theoretically claims that perturbation rooted in the data space is essential for\n\u2217Equal contribution\nrobust generalization to unobserved domains. Accordingly, this paper introduces an objective that leverages both parameter and data perturbed regions for domain generalization. In implementation, our objective minimizes the loss landscape discrepancy between a source domain and unknown domains, where unknown domains are emulated by perturbing instances from the source domain datasets. Recognizing the loss landscape discrepancy as an Inconsistency score across different domains, we name our objective as Unknown Domain Inconsistency Minimization (UDIM).\nIntroduction of UDIM on the DG framework has two contributions. First, we theoretically prove that the integration of sharpness-aware optimization and UDIM objective becomes the upper bound of population risk for all feasible domains, without introducing unoptimizable terms. Second, we reformulate the UDIM objective into a practically implementable term. This is accomplished from deriving the worst-case perturbations for both parameter space and data space, each in a closed-form expression. Our experiments on various DG benchmark datasets illustrate that UDIM consistently improves the generalization ability of parameter-region based methods. Moreover, we found that these improvements become more significant as domain information becomes more limited."
        },
        {
            "heading": "2 PRELIMINARY",
            "text": ""
        },
        {
            "heading": "2.1 PROBLEM DEFINITION OF DOMAIN GENERALIZATION",
            "text": "This paper investigates the task of domain generalization in the context of multi-class classification (Arjovsky et al., 2019; Sagawa et al., 2019; Nam et al., 2021). We define an input as x \u2208 Rd and its associated class label as y \u2208 {1, .., C}. Let De represent a distribution of e-th domain. E is a set of indices for all domains, and D := {De}e\u2208E denotes the set of distributions for all domains, where every domain shares the same class set. For instance, let\u2019s hypothesize that video streams from autonomous cars are being collected, and the data collection at days and nights will constitute two distinct domains. A sampled dataset from the e-th domain is denoted by De = {(xi, yi)}nei=1 where (xi, yi) \u223c De and ne is the number of data instances of e-th domain. Throughout this paper, let \u03b8 \u2208 \u0398 represents a parameter of trained model f\u03b8, where \u0398 is a set of model parameters. Using De, we define a loss function as LDe(\u03b8) = 1ne \u2211 (xi,yi)\u2208De \u2113(f\u03b8(xi), yi), where we sometimes denote \u2113(f\u03b8(xi), yi) as \u2113(xi, \u03b8). The population risk for domain e is given by LDe(\u03b8) = E(x,y)\u223cDe [\u2113(f\u03b8(x), y)]. Then, the population risk over all domains is defined as LD(\u03b8) = \u2211 e\u2208E p(e)LDe(\u03b8), where p(e) represents the occurrence probability of domain e.\nIn essence, the primary goal of training a model, f\u03b8, is to minimize the population risk, LD(\u03b8). In practical scenarios, we only have access to datasets derived from a subset of all domains. We call these accessible domains and datasets as source domains and source domain datasets, respectively; denoted as DS = {Ds}s\u2208S and DS = {Ds}s\u2208S where S is the set of indexes for source domains. As DS \u0338= D , DS deviates from the distribution D under the sampling bias of DS . As a consequence, a model parameter \u03b8\u2217S = argmin\u03b8LDS (\u03b8), which is trained exclusively on DS , might not be optimal for LD(\u03b8). Accordingly, domain generalization emerges as a pivotal task to optimize \u03b8\u2217 = argmin\u03b8LD(\u03b8) by only utilizing the source domain dataset, DS ."
        },
        {
            "heading": "2.2 VARIANTS OF SHARPNESS-AWARE MINIMIZATION FOR DOMAIN GENERALIZATION",
            "text": "Recently, a new research area has emerged by considering optimization over the parameter space (Foret et al., 2020; Kwon et al., 2021). Several studies have focused on the problem of \u03b8 overfitted to a training dataset (Wang et al., 2023; Zhang et al., 2023b;a). These studies confirmed that optimization on the perturbed parameter region improves the generalization performance of the model. To construct a model that can adapt to unknown domains, it is imperative that an optimized parameter point is not overfitted to the source domain datasets. Accordingly, there were some studies to utilize the parameter perturbation in order to avoid such overfitting, as elaborated below (Table 1).\nAmong variations in Table 1, Sharpness-Aware Minimization (SAM) (Foret et al., 2020) is the most basic form of the parameter perturbation, which regularizes the local region of \u03b8 to be the flat minima on the loss curvature as min\u03b8 max\u2225\u03f5\u22252\u2264\u03c1 LDs(\u03b8+ \u03f5)+ \u2225\u03b8\u22252. Here, \u03f5 is the perturbation vector to \u03b8; and \u03c1 is the maximum size of the perturbation vector. Subsequently, methodologies for regularizing stronger sharpness were introduced (Zhang et al., 2023b; Wang et al., 2023; Zhang et al., 2023a).\nThese approaches exhibited incremental improvements in domain generalization tasks. Check Appendix A for more explanations.\nTable 1: Objectives of SAM variants for DG\nMethod Objective\nSAM (Foret et al., 2020) max \u2225\u03f5\u22252\u2264\u03c1 LDs(\u03b8 + \u03f5)\nGAM (Zhang et al., 2023b) LDs(\u03b8) + \u03c1 max\u2225\u03f5\u22252\u2264\u03c1 \u2225\u2207LDs(\u03b8 + \u03f5)\u2225\nSAGM (Wang et al., 2023) LDs(\u03b8) + LDs(\u03b8 + \u03c1\u2207LDs(\u03b8) / \u2225\u2207LDs(\u03b8)\u2225 \u2212 \u03b1\u2207LDs(\u03b8)) FAM (Zhang et al., 2023a) LDs(\u03b8) + max\u2225\u03f5\u22252\u2264\u03c1 (LDs(\u03b8 + \u03f5)\u2212 LDs(\u03b8)) + \u03c1 max\u2225\u03f5\u22252\u2264\u03c1 \u2225\u2207LDs(\u03b8 + \u03f5)\u2225 We are motivated by this characteristic to extend the parameter perturbation towards data perturbation, which could be effective for the exploration of unknown domains. None of SAM variants proposed data-based perturbation for unknown domain discovery. Fundamentally, SAM variants predominantly focus on identifying the flat minima for the given source domain dataset, DS . In Section 3.1, we highlight that finding flat minima in the source domain cannot theoretically ensure generalization to unknown target domains. Consequently, we demonstrate that generalization should be obtained from unobserved domains, rather than solely the source domain. Since SAM imposes the perturbation radius of \u03c1 on only \u03b8, we hypothesize Ds could be perturbed by additional mechanism to generalize Ds toward D .\nWhile SAM minimizes the loss over \u03c1-ball region of \u03b8, its actual implementation minimizes the maximally perturbed loss w.r.t. \u03b8 + \u03f5\u2217; where the maximal perturbation, \u03f5\u2217, is approximated in a closed-form solution via Taylor expansion as follows:\nmax \u2225\u03f5\u22252\u2264\u03c1 LDs(\u03b8 + \u03f5) \u2248 LDs(\u03b8 + \u03f5\u2217) where \u03f5\u2217 \u2248 \u03c1 \u00b7 sign(\u2207\u03b8LDs(\u03b8)) |\u2207\u03b8LDs(\u03b8)| \u2225\u2207\u03b8LDs(\u03b8)\u222522 . (1)\nThe existence of this closed-form solution of \u03f5\u2217 simplifies the learning procedure of SAM by avoiding min-max game and its subsequent problems, such as oscillation of parameters (Chu et al., 2019). This closed-form solution can also be applied to the perturbation on the data space."
        },
        {
            "heading": "3 METHOD",
            "text": ""
        },
        {
            "heading": "3.1 MOTIVATION : BEYOND THE SOURCE DOMAIN-BASED FLATNESS",
            "text": "Based on the context of domain generalization, Theorem 3.1 derives the relationship between the SAM loss on the source domain dataset, denoted as max\u2225\u03f5\u22252\u2264\u03c1 LDs(\u03b8 + \u03f5), and the generalization loss on an arbitrary unknown domain, LDe(\u03b8), for a model parameter \u03b8 \u2208 \u0398 as follows: Theorem 3.1. (Rangwani et al., 2022) For \u03b8 \u2208 \u0398 and arbitrary domain De \u2208 D , with probability at least 1 \u2212 \u03b4 over realized dataset Ds from Ds with |Ds| = n, the following holds under some technical conditions on LDe(\u03b8), where h0 : R+ \u2192 R+ is a strictly increasing function.\nLDe(\u03b8) \u2264 max\u2225\u03f5\u22252\u2264\u03c1 LDs(\u03b8 + \u03f5) +Df(Ds||De) + h0( \u2225\u03b8\u222522 \u03c12 ) (2)\nTheorem 3.1 provides an upper bound for LDe(\u03b8). The second term of this upper bound, represented asDf(Ds||De), corresponds to the distribution discrepancy between Ds and De. Notably,Df denotes the discrepancy based on f -divergence. When e \u0338= s, De is inaccessible in the context of domain generalization. Accordingly, the SAM optimization on Ds leaves an unoptimizable term in its upper bound, posing challenges for the domain generalization.\nIn a setting that only Ds and model parameter \u03b8 are accessible, generating unseen domain data becomes infeasible. Nontheless, by perturbing Ds towards the direction that is most sensitive given \u03b8, we can emulate the worst-case scenario for an unobserved domain (Sagawa et al., 2019). While parameters trained via the SAM optimizer may exhibit flat regions based on the source domain dataset, Ds, there is no theoretic study on the flat minima under unobserved domains. By identifying the worst-case scenario that maximizes the loss landscape difference between domains, our methodology seeks the generalization across the unknown domains."
        },
        {
            "heading": "3.2 UDIM : UNKNOWN DOMAIN INCONSISTENCY MINIMIZATION",
            "text": "This section proposes an objective based on both parameter and data perturbed regions for domain generalization, coined Unknown Domain Inconsistency Minimization (UDIM). UDIM minimizes\nthe loss landscape discrepancy between the source domain and unknown domains, where unknown domains are empirically realized by perturbing instances from the source domain dataset, Ds.\nLet \u0398\u03b8,\u03c1 = {\u03b8 \u2032 |\u2225\u03b8\u2032 \u2212 \u03b8\u22252 \u2264 \u03c1}, which is \u03c1-ball perturbed region of a specific parameter point \u03b8. When training a parameter \u03b8 on an arbitrary domain dataset De with the SAM optimizer, some regions within \u0398\u03b8,\u03c1 are expected to be optimized as flat regions for source domain. Following the notation of Parascandolo et al. (2020b), define N\u03b3,\u03c1e,\u03b8 := {\u03b8 \u2032 \u2208 \u0398\u03b8,\u03c1 | \u2223\u2223LDe(\u03b8\u2032) \u2212 LDe(\u03b8)\u2223\u2223 \u2264 \u03b3}, which is the region in \u0398\u03b8,\u03c1 where the loss value of \u03b8 deviates by no more than \u03b3. Given small enough values of LDe(\u03b8) and \u03b3, N\u03b3,\u03c1e,\u03b8 could be recognized as a flat minima for e-th domain. We aim to utilize the flat minima of \u03b8 obtained through training on Ds using the SAM optimizer, where we represent s-th domain as our source domain. The goal is to regularize the loss and its corresponding landscape of unknown domains, so that the flat minima of the unknown domain aligns with that of the source domain. By optimizing the domain of worst-case deviation in the loss landscape from Ds, we facilitate regularization across a range of intermediary domains. Eq. 3 formalizes our motivation, which is cross-domain inconsistency score:\nI\u03b3s (\u03b8) = max e\u2208E max \u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8\n|LDe(\u03b8 \u2032 )\u2212 LDs(\u03b8 \u2032 )| (3)\nIn the above equation, the inner maximization seeks the worst-case parameter point \u03b8 \u2032\nthat amplifies the domain-wise loss disparity, while the outer maximization identifies e-th domain that maximizes max\u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 |LDe(\u03b8 \u2032 ) \u2212 LDs(\u03b8 \u2032 )|. Parascandolo et al. (2020a) utilizes an equation similar to Eq. 3, and this equation is also employed to indicate \u03b8 with a loss surface that is invariant to the environment changes. Our methodology differs from Parascandolo et al. (2020a), which simply uses inconsistency as a metric, in that we employ it directly as the objective we aim to optimize. Let\u2019s assume that \u03b8 exhibits sufficiently low loss along with flat loss landscape of Ds. Further, if we can identify \u03b8 with a low value of I\u03b3s (\u03b8), then this \u03b8 would also demonstrate consistently good generalization for unknown domains. This motivation leads to the UDIM\u2019s objective, which specifically targets the reduction of the cross-domain inconsistency score across unobserved domains.\nObjective Formulation of UDIM While we define the cross-domain inconsistency as I\u03b3s (\u03b8), the final formulation of UDIM is the optimization of \u03b8 regarding both source and unknown domain losses from the flat-minima perspective. Eq. 4 is the parameter optimization of our proposal:\nmin \u03b8\n( max\n\u2225\u03f5\u22252\u2264\u03c1 LDs(\u03b8 + \u03f5) + \u03bb1 max e\u2208E max \u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 |LDe(\u03b8\n\u2032 )\u2212 LDs(\u03b8 \u2032 )|+ \u03bb2\u2225\u03b8\u22252 ) (4)\nThe objective of UDIM consists of three components. The first term is a SAM loss on Ds, guiding \u03b8 toward a flat minima in the context of Ds. Concurrently, as \u03b8 provides flattened local landscape, the second term weighted with \u03bb1 is a region-based loss disparity between the worst-case domain e and the source domain. Subsequently, the algorithm seeks to diminish the region-based loss disparity between the worst-case domain and the source domain. Figure 1 (a) illustrates the update of the loss landscape in the parameter-space for each domain, based on the optimization of Eq. 4. As SAM\noptimization on Ds progresses, N \u03b3,\u03c1 s,\u03b8 is expected to be broadened. However, SAM optimization does not imply the minimization of I\u03b3s (\u03b8) (center). Given that the optimization of I\u03b3s (\u03b8) is conducted on N\u03b3,\u03c1s,\u03b8 , we expect the formation of flat minima spanning all domains in the optimal state (rightmost).\nThe optimization of Eq. 4 can also be interpreted as minimizing another form of sharpness in the data space. Let\u2019s suppose all other domains lie within a finite perturbation of the source domain\u2019s dataset. In the context of the data space over the Ds, the optimization can be seen as identifying the worst-case perturbed dataset, De, by evaluating max\u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 |LDe(\u03b8 \u2032 ) \u2212 LDs(\u03b8 \u2032 )|. If we view the perturbation of the Ds, not as discrete choices but as a continuum within the data-space; our optimization can be viewed as minimizing the sharpness of domain-wise inconsistency in the data space. Figure 1 (b) illustrates this interpretation. After such optimization, the resulting parameter \u03b8 can offer the consistent generalization over domains located within the perturbed data space.\nAt this juncture, a crucial consideration is whether the SAM optimization on Ds focuses on minimizing the second term, I\u03b3s (\u03b8), or\nnot. We illustrate the limited capability of SAM variants in minimizing the second term, by an analytical illustration of Figure 1 (a); and by an empirical demonstration of Figure 2. Therefore, UDIM covers the optimization area that SAM does not operate.\nTheoretical Analysis of UDIM Given the definition of \u0398\u03b8,\u03c1 = {\u03b8 \u2032 |\u2225\u03b8\u2032 \u2212 \u03b8\u22252 \u2264 \u03c1}, we introduce \u0398\u03b8,\u03c1\u2032 = argmax\u0398\u03b8,\u03c1\u0302\u2286N\u03b3,\u03c1s,\u03b8 \u03c1\u0302, which is the largest \u03c1 \u2032-ball region around \u03b8 in N\u03b3,\u03c1s,\u03b8 .\n1 Theorem 3.2 introduces the generalization bound of Eq. 4, which is the objective of UDIM. Theorem 3.2 states that Eq. 4 can become the upper bound of LD(\u03b8), which is the population risk over all domains. Theorem 3.2. For \u03b8 \u2208 \u0398 and arbitrary domain e \u2208 E , with probability at least 1\u2212 \u03b4 over realized dataset De from De, the following holds under technical conditions on LDe(\u03b8) and LDe(\u03b8), where h : R+ \u2192 R+ is a strictly increasing function. (Proof in Appendix B.1.)\nLD(\u03b8) \u2264 max \u2225\u03f5\u22252\u2264\u03c1\nLDs(\u03b8 + \u03f5) + (1\u2212 1\n|E| )maxe\u2208E max\u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 |LDe(\u03b8\n\u2032 )\u2212 LDs(\u03b8 \u2032 )|+ h(\u2225\u03b8\u2225 2 2\n\u03c12 ) (5)\nIn such a case, Theorem 3.2 retains the same form of weight decay term as presented in Theorem 3.1. Unlike Theorem 3.1, which contains terms that are inherently unoptimizable, our objective is capable of minimizing every term encompassed in the upper bound of Theorem 3.2 because we do not have an inaccessible term, Df(Ds||De)."
        },
        {
            "heading": "3.3 IMPLEMENTATION OF UDIM",
            "text": "This section reformulates the second term of Eq. 4 into an implementable form. We first formalize the perturbation of Ds to emulate the worst-case domain for max\u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 |LDe(\u03b8 \u2032 )\u2212 LDs(\u03b8 \u2032 )|.\nInconsistency-Aware Domain Perturbation on Ds For clarification, we explain the perturbation process based on an arbitrary input instance x from Ds. Given that the magnitude of the perturbation vector for input x is constrained to \u03c1x, the perturbed input x\u0303 can be expressed as follows:\nx\u0303 = x+ argmax \u03f5x:\u2225\u03f5x\u22252\u2264\u03c1x max \u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8\n( \u2113(x+ \u03f5x, \u03b8 \u2032 )\u2212 \u2113(x, \u03b8\u2032) ) \u2248 x+ argmax\n\u03f5x:\u2225\u03f5x\u22252\u2264\u03c1x max \u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 \u2113(x+ \u03f5x, \u03b8\n\u2032 )\n(6)\n\u2248 1st Taylor x+ argmax \u03f5x:\u2225\u03f5x\u22252\u2264\u03c1x\n( \u2113(x+ \u03f5x, \u03b8) + \u03c1 \u2032\u2225\u2207\u03b8\u2113(x+ \u03f5x, \u03b8)\u22252 )\n(7)\nAssuming N\u03b3,\u03c1s,\u03b8 as flat minima of \u03b8, \u2113(x, \u03b8 \u2032 ) is almost invariant for \u03b8 \u2032 \u2208 N\u03b3,\u03c1s,\u03b8 . We assume that the invariant value of \u2113(x, \u03b8 \u2032 ) does not significantly influence the direction of x\u2019s perturbation because it becomes almost constant in N\u03b3,\u03c1s,\u03b8 . Therefore, we cancel out this term in Eq. 6.\n1For the sake of simplicity, we do not inject the notation of s or \u03b3 in \u0398\u03b8,\u03c1\u2032 .\nAdditionally, as we cannot specify the regional shape of N\u03b3,\u03c1s,\u03b8 , it is infeasible to search maximal point \u03b8\n\u2032 \u2208 N\u03b3,\u03c1s,\u03b8 . As a consequence, we utilize \u0398\u03b8,\u03c1\u2032 , which is the largest \u03c1\u2032-ball region within N\u03b3,\u03c1s,\u03b8 , to approximately search the maximal point \u03b8 \u2032 . It should be noted that \u0398\u03b8,\u03c1\u2032 \u2286 N\u03b3,\u03c1s,\u03b8 \u2286 \u0398\u03b8,\u03c1, where we assume that \u03c1\u2032 gradually approaches \u03c1 during the SAM optimization. Through the firstorder Taylor expansion2 for the maximum point within \u0398\u03c1\u2032 , we can design the aforementioned perturbation loss as Eq. 7. Consequently, the perturbation is carried out in a direction that maximizes the loss and \u03c1\u2032-weighted gradient norm of the original input x.\nInconsistency Minimization on \u03b8 After the perturbation on x \u2208 Ds, we get an inconsistencyaware perturbed dataset, D\u0303s; which approximates the worst-case of unobserved domains. Accordingly, we can formulate the optimization of I\u03b3s (\u03b8) based on \u03b8 as min\u03b8 max\n\u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8\n( LD\u0303s(\u03b8 \u2032 )\u2212LDs(\u03b8 \u2032 ) ) .\nFor the above min-max optimization, we approximate the search for the maximum parameter \u03b8 \u2032\nin a closed-form using second order taylor-expansion, similar to the approach of SAM in Eq. 1.\nmax \u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8\n( LD\u0303s(\u03b8 \u2032 )\u2212 LDs(\u03b8 \u2032 ) ) \u2248LD\u0303s(\u03b8)\u2212 LDs(\u03b8) + \u03c1\n\u2032\u2225\u2207\u03b8LD\u0303s(\u03b8)\u22252 + max \u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8\n1 2 \u03b8 \u2032\u22a4HD\u0303s\u03b8 \u2032\n(8) = ( LD\u0303s(\u03b8)\u2212 LDs(\u03b8) ) + \u03c1\u2032\u2225\u2207\u03b8LD\u0303s(\u03b8)\u22252 + \u03b3maxi \u03bb D\u0303s i /\u03bb Ds i (9)\nFull derivation and approximation procedure of Eq. 8 and Eq. 9 are in Appendix B.2. HD\u0303s in Eq. 8 denotes a Hessian matrix of the perturbed dataset, D\u0303s. Also, \u03bbD\u0303si in Eq. 9 denotes i-th eigenvalue of HD\u0303s . Finally, Eq. 9 becomes the objective with three components: 1) the loss difference between D\u0303s and Ds, 2) the gradient norm of D\u0303s and 3) the maximum eigenvalue ratio between D\u0303s and Ds. Note that \u03bbD\u0303si /\u03bb Ds i is minimized when HD\u0303s becomes equivalent to HDs .\nWhile Eq. 9 is differentiable with respect to \u03b8 and can thus be utilized as a tractable objective, computing the Hessian matrix for an over-parameterized \u03b8 is computationally demanding. In line with Rame et al. (2022), we replace the objective with the Hessian matrix (Eq. 9) with an optimization based on gradient variance. Accordingly, Eq. 10 represents the gradient variance-based objective as an optimization with respect to \u03b8 as follows: (See detailed derivations in Appendix B.3)\nmin \u03b8\n\u03c1\u2032\u2225\u2207\u03b8LD\u0303s(\u03b8)\u22252 + \u2225Var(GD\u0303s)\u2212 Var(GDs)\u22252 (10)\nHere, gi is a per-sample gradient for i-th sample; and GD = {gi}|D|i=1 is a set of per-sample gradient for xi \u2208 D. Accordingly, the variance of GD, which we denote as Var(GD), is calculated as Var(GD) = 1|D|\u22121 \u2211|D| i=1 ( gi \u2212 g\u0304 )2 . Matching the gradient variances between two distinct datasets encapsulates a specific form of loss matching between them (Rame et al., 2022). This allows us to unify loss matching and Hessian matching under a single optimization using Var(GD).\nSummary Our methodology applies the perturbation technique to both input x and parameter \u03b8. Particularly, the perturbation on inputs and parameters is necessary to minimize I\u03b3s (\u03b8) under unknown domains, which is the unique contribution of this work. Ablation study in Section 4.3 supports that UDIM, which is the combination of the Eq. 7 and Eq. 10, yields the best performance compared to various perturbations and optimizations. Algorithm of UDIM is in Appendix C.\nA single iteration of UDIM optimization with SAM loss can be described as the following procedure:\n1. Construction of D\u0303s via Inconsistency-Aware Domain Perturbation on Ds\nD\u0303s = {(x\u0303i, yi) | (xi, yi) \u2208 Ds} where x\u0303i = xi + \u03c1x \u2207xi\n( \u2113(x, \u03b8t) + \u03c1 \u2032\u2225\u2207\u03b8t\u2113(x, \u03b8t)\u22252 )\u2225\u2225\u2207xi(\u2113(x, \u03b8t) + \u03c1\u2032\u2225\u2207\u03b8t\u2113(x, \u03b8t)\u22252)\u2225\u22252 (11)\n2. SAM loss and Inconsistency Minimization on the current parameter \u03b8t \u03b8t+1 = \u03b8t \u2212 \u03b7\u2207\u03b8t (\nmax \u2225\u03f5\u22252\u2264\u03c1\nLDs(\u03b8t + \u03f5) + \u03c1\u2032\u2225\u2207\u03b8tLD\u0303s(\u03b8t)\u22252 + \u2225Var(GD\u0303s)\u2212 Var(GDs)\u22252 + \u03bb2\u2225\u03b8t\u22252 )\n(12)\n2We empirically found out that extending it into second-order does not affect the resulting performance."
        },
        {
            "heading": "4 EXPERIMENT",
            "text": ""
        },
        {
            "heading": "4.1 IMPLEMENTATION",
            "text": "Datasets and Implementation Details We validate the efficacy of our method, UDIM, via experiments across multiple datasets for domain generalization. First, we conducted evaluation on CIFAR10-C (Hendrycks & Dietterich, 2019), a synthetic dataset that emulates various domains by applying several synthetic corruptions to CIFAR-10 (Krizhevsky et al., 2009). Furthermore, we extend our evaluation to real-world datasets with multiple domains, namely PACS (Li et al., 2017), OfficeHome (Venkateswara et al., 2017), and DomainNet (Peng et al., 2019). Since UDIM can operate regardless of the number of source domains, we evaluate UDIM under both scenarios on real-world datasets: 1) when multiple domains are presented in the source (Leave-One-Out Domain Generalization, LOODG); and 2) when a single domain serves as the source (Single Source Domain Generalization, SDG). Unless specified, we report the mean and standard deviation of accuracies from three replications. Appendix D.1 provides information on the dataset and our implementations.\nImplementation of UDIM To leverage a parameter \u03b8 exhibitng flat minima on Ds during the minimization of I\u03b3s (\u03b8), we perform a warm-up training on \u03b8 with the SAM loss on Ds. While keeping the total number of iterations consistent with other methods, we allocate initial iterations for warmup based on the SAM loss. As a result, we expect to optimize Eq. 4 with a sufficiently wide region of N\u03b3,\u03c1s,\u03b8 for sufficiently low \u03b3.\nThe gradient variance, Var(GDs) in Eq. 10, necessitates the costly computation of per-sample gradients with respect to \u03b8. We utilize BackPACK (Dangel et al., 2020), which provides the faster computation of per-sample gradients. Also, we compute the gradient variance only for the classifier parameters, which is an efficient practice to improve the performance with low computational costs (Shin et al., 2023). Appendix D.1 specifies additional hyperparameter settings of UDIM.\nBaselines for Comparison Since our approach, UDIM, is tested under both LOODG and SDG scenarios, we employed methods tailored for each scenario as baselines. These include strategies for robust optimization (Arjovsky et al., 2019; Sagawa et al., 2019) and augmentations for novel domain discovery (Zhang et al., 2018; Yun et al., 2019; Nam et al., 2021). Appendix D.2 enumer-\nates baselines for comparisons. For methods that leverage relationships between source domains, a straightforward application is not feasible in SDG scenarios. In such cases, we treated each batch as if it came from a different domain to measure experimental performance. ERM means the base model trained with standard classification loss. We also utilize the sharpness-based approaches as the baselines. Note that the first term of Eq. 4 (SAM loss on Ds) can be substituted with objectives for similar flatness outcomes, such as SAGM (Wang et al., 2023) and GAM (Zhang et al., 2023b)."
        },
        {
            "heading": "4.2 CLASSIFICATION ACCURACIES ON VARIOUS BENCHMARK DATASETS",
            "text": "To assess the effectiveness of each method under unknown domains, we present the accuracy results on unknown target domains. Table 2 shows the results on CIFAR-10-C. The sharpness-based methods exhibit excellent performances compared to the other lines of methods. This underscores the importance of training that avoids overfitting to a specific domain. By adding UDIM (specifically, the inconsistency term of Eq. 4) to these SAM-based approaches, we consistently observed improved performances compared to the same approaches without UDIM.\nTable 3 shows the results on the PACS dataset. Similar to the results in Table 2, UDIM consistently outperforms the existing baselines across each scenario. This improvement is particularly amplified in the single source scenario. Unlike the Leave-One-Out scenario, the single source scenario is more challenging as it requires generalizing to unknown domains using information from a single domain. These results emphasize the robust generalization capability of UDIM under unknown domains.\nThis section aims to examine the efficacy of UDIM by analyzing the sensitivity of each hyperparameter used in UDIM\u2019s implementation. Additionally, We perform an ablation study by composing each part of the UDIM\u2019s objective in Eq. 12. Unless specified, each experiment is carried out in the Single source domain generalization scenario using the PACS dataset."
        },
        {
            "heading": "4.3 SENSITIVITY ANALYSES AND ABLATION STUDY",
            "text": "Sensitivity Analyses Figure 3 (a) shows the sensitivity of \u03c1 \u2032 , \u03bb1 and \u03c1x, which are main hyper-parameters of UDIM, under the feasible range of value lists. As a default setting of UDIM, we set \u03c1 \u2032 =0.05, \u03bb1=1 and \u03c1x=1. In implementation, we multiply \u03c1x by the unnormalized gradient of x. Therefore, the values presented in the ab-\nlation study have a larger scale. Also, we compare the performances between SAM and UDIM w/ SAM to compare the effectiveness of UDIM objective. Each figure demonstrates that UDIM\u2019s performance remains robust and favorable, invariant to the changes in each hyper-parameter. Figure 3 (b) presents the test accuracies over training iterations while varying the sharpness-based approaches used alongside UDIM. Regardless of which method is used in conjunction, additional performance improvements over the iterations are observed compared to the original SAM variants.\nAblation Studies Figure 4 presents the ablation results of UDIM, which were carried out by replacing a subpart of the UDIM\u2019s objective with alternative candidates and subsequently assessing the performances. We conduct various ablations: \u2019SourceOpt\u2019 represents the optimization method for Ds, \u2019Perturb\u2019 indicates the perturbation method utilized to emulate unknown domains, and \u2019PerturbOpt\u2019 indicates the optimization for the perturbed dataset D\u0303s. Appendix D.3 enumerates each ablation candidate and its implementation. UDIM, depicted by the red bar, consistently outperforms in all ablations, suggesting the effectiveness of our derived objective formulation."
        },
        {
            "heading": "4.4 SHARPNESS ANALYSES",
            "text": "Figure 1 claims that UDIM would reduce the suggested sharpness in both parameter space and data space. To support this claim with experiments, Figure 5 enumerates the sharpness plots for models trained with sharpness-based methods and those trained with UDIM. These figures are obtained by training models in the single source domain setting of the PACS dataset, and each plot is drawn utilizing target domain datasets, which are not utilized for training.\nThe top row of Figure 5 represents the measurement of sharpness in the parameter space by introducing a random perturbation to the parameter \u03b8. Additionally, to examine the sharpness in the perturbed data space of unobserved domains, the bottom side of Figure 5 illustrates sharpness based on the input-perturbed region of the target domain datasets. Current SAM variants struggle to maintain sufficient flatness within both the perturbed parameter space and data space. On the other hand, UDIM effectively preserves flatness in the perturbed parameter space and the data space of unknown domains. Within the region, the model trained using UDIM also exhibits a lower loss value compared to other methods. Through preserving flatness in each space, we confirm that the optimization with UDIM, both in parameter and data space, has practical effectiveness."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We introduce UDIM, a novel approach to minimize the discrepancy in the loss landscape between the source domain and unobserved domains. Combined with SAM variants, UDIM consistently improves generalization performance on unobserved domains. This performance is achieved by perturbing both domain and parameter spaces, where UDIM optimization is conducted based on the iterative update between the dataset and the parameter. Experimental results demonstrate accuracy gains, up to 9.9% in some settings, by adopting UDIM in current sharpness-based approaches."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This research was supported by AI Technology Development for Commonsense Extraction, Reasoning, and Inference from Heterogeneous Data(IITP) funded by the Ministry of Science and ICT(20220-00077)."
        },
        {
            "heading": "A EXPLANATION OF SHARPNESS VARIANTS FOR DOMAIN GENERALIZATION",
            "text": "Gradient norm-Aware Minimization (GAM) Zhang et al. (2023b) introduces first-order flatness, which minimizes a maximal gradient norm within a perturbation radius, to regularize a stronger flatness than SAM. Accordingly, GAM seeks minima with uniformly flat curvature across all directions.\nSharpness-Aware Gradient Matching (SAGM) Wang et al. (2023) minimizes an original loss, the corresponding perturbed loss, and the gap between them. This optimization aims to identify a minima that is both flat and possesses a sufficiently low loss value. Interpreting the given formula, this optimization inherently regularizes the gradient alignment between the original loss and the perturbed loss.\nFlatness-Aware Minimization (FAM) Zhang et al. (2023a) concurrently optimizes both zerothorder and first-order flatness to identify flatter minima. To compute various sharpness metric on the different order, it incurs a higher computational cost."
        },
        {
            "heading": "B PROOFS AND DISCUSSIONS",
            "text": ""
        },
        {
            "heading": "B.1 PROOF FOR THEOREM 3.2",
            "text": "First, we provide some theorem, definition, and assumptions needed to prove the Theorem 3.2. Theorem B.1. (Foret et al., 2020) For any \u03c1 > 0 which satisfies LDe(\u03b8) \u2264 E\u03f5\u223cp(\u03f5)LDe(\u03b8 + \u03f5), with probability at least 1\u2212 \u03b4 over realized dataset De from De with |De| = n, the following holds under some technical conditions on LDe(\u03b8):\nLDe(\u03b8) \u2264 max\u2225\u03f5\u22252\u2264\u03c1 LDe(\u03b8 + \u03f5) + he( \u2225\u03b8\u222522 \u03c12 ),\nwhere he : R+ \u2192 R+ is a strictly increasing function. Definition B.2. Let \u0398\u03b8,\u03c1 = {\u03b8 \u2032 |\u2225\u03b8\u2032\u2212\u03b8\u22252 \u2264 \u03c1} and N\u03b3,\u03c1e,\u03b8 = {\u03b8 \u2032 \u2208 \u0398\u03b8,\u03c1| |LDe(\u03b8 \u2032 )\u2212LDe(\u03b8)| \u2264 \u03b3}.\nAssumption B.3. LD(\u03b8) \u2264 E\u03f5\u223cp(\u03f5)LD(\u03b8 + \u03f5) where p(\u03f5) \u223c N (0, \u03c32I) for some \u03c3 > 0. Assumption B.4. maxe\u2208E LDe(\u03b8 \u2032 ) \u2265 LDs(\u03b8 \u2032 ) for all \u03b8 \u2032 \u2208 N\u03b3,\u03c1s,\u03b8 .\nIn practice, Assumption B.4 is acceptable. Contrary to the dataset De from an unobserved domain e, Ds is a source domain dataset provided to us and hence, available for training. Moreover, the region of N\u03b3,\u03c1s,\u03b8 would be flat with respect to local minima, \u03b8\n\u2217, from the perspective of the source domain dataset. Therefore, the perturbed loss on the source domain dataset, LDs(\u03b8 \u2032 ), would be likely to have a sufficiently low value. Theorem B.5. For \u03b8 \u2208 \u0398 and arbitrary domain e \u2208 E , with probability at least 1\u2212 \u03b4 over realized dataset De from De with |De| = n, the following holds under some technical conditions on LDe(\u03b8).\nLD(\u03b8) \u2264 max \u2225\u03f5\u22252\u2264\u03c1\nLDs(\u03b8 + \u03f5) + (1\u2212 1\n|E| )maxe\u2208E max\u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 |LDe(\u03b8\n\u2032 )\u2212 LDs(\u03b8 \u2032 )|+ h(\u2225\u03b8\u2225 2 2\n\u03c12 ) (13)\nwhere h : R+ \u2192 R+ is a strictly increasing function.\nProof. For the derivation of Theorem 3.2, we assume the case of single source domain generalization, where s represents a single domain in E . It should be noted that the number of available source domain does not affect the validity of this proof because we can consider multiple source domains as a single source domain by Ds = \u222ai\u2208SDi. Based on the definition, LD(\u03b8) = 1|E| \u2211 e\u2208E LDe(\u03b8) = 1|E| ( LDs(\u03b8) + \u2211 e\u2208E,e\u0338=s LDe(\u03b8) ) . From Theorem B.1, we can derive the generalization bound of a source domain s as follows:\nLDs(\u03b8) \u2264 max\u2225\u03f5\u22252\u2264\u03c1 LDs(\u03b8 + \u03f5) + hs( \u2225\u03b8\u222522 \u03c12 ) (14)\nwhere hs : R+ \u2192 R+ is a strictly increasing function. To define the upper bound of LD(\u03b8\u2217), we need to find the upper bound of the remaining term, \u2211 e\u2208E,e\u0338=s LDe(\u03b8). Here, we introduce a parameter set \u0398\u03c1\u2032 := argmax\u0398\u03c1\u0302\u2286N\u03b3,\u03c1s,\u03b8 \u03c1\u0302, which is the largest \u03c1 \u2032-ball region around \u03b8 in N\u03b3,\u03c1s,\u03b8 . Then, we can construct inequality as follows:\nmax \u2225\u03f5\u22252\u2264\u03c1\u2032 LDe(\u03b8 + \u03f5) \u2264 max \u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8\nLDe(\u03b8 \u2032 ) \u2264 max\n\u2225\u03f5\u22252\u2264\u03c1 LDe(\u03b8 + \u03f5) (15)\nThis is because \u0398\u03c1\u2032 \u2282 N\u03b3,\u03c1s,\u03b8 \u2282 \u0398\u03b8,\u03c1. Similar to Foret et al. (2020); Kim et al. (2022), we make use of the following result from Laurent & Massart (2000):\nz \u223c N (0, \u03c32I)\u21d2 \u2225z\u222522 \u2264 k\u03c32 ( 1 + \u221a log n\nk\n)2 with probability at least 1\u2212 1\u221a\nn (16)\nWe set \u03c1 = \u03c3( \u221a k + \u221a log n). Then, it enables us to connect expected perturbed loss and maximum loss as follows:\nE\u03f5\u223cN (0,\u03c32I) [ LDe(\u03b8 + \u03f5) ] \u2264 (1\u2212 1\u221a\nn ) max \u2225\u03f5\u22252\u2264\u03c1 LDe(\u03b8 + \u03f5) + 1\u221a n le,max (17)\nHere, le,max is the maximum loss bound when \u2225z\u222522 \u2265 \u03c12. Also, we introduce \u03c3\u2032 where \u03c1\u2032 = \u03c3\u2032( \u221a k + \u221a log n). Then, similar to Eq. 17, we also derive the equation for \u03c1\u2032 as follows:\nE\u03f5\u223cN (0,(\u03c3\u2032 )2I) [ LDe(\u03b8 + \u03f5) ] \u2264 (1\u2212 1\u221a\nn ) max \u2225\u03f5\u22252\u2264\u03c1\u2032 LDe(\u03b8 + \u03f5) + 1\u221a n l\u2032e,max (18)\n\u2264 (1\u2212 1\u221a n ) max \u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 LDe(\u03b8 \u2032 ) + 1\u221a n l\u2032e,max (19)\nwhere l\u2032e,max is the maximum loss bound when \u2225z\u222522 \u2265 (\u03c1\u2032)2. Then, we have the below derivation by summing up all e \u0338= s and using the fact that le,max \u2264 l\u2032e,max:\n1 (|E| \u2212 1) \u2211\ne\u2208E,e\u0338=s\nE \u03f5\u223cN (0,\u03c3\u20322I) [ LDe(\u03b8 + \u03f5) ] (20)\n\u2264 (1\u2212 1\u221a n )\n1 (|E| \u2212 1) \u2211\ne\u2208E,e \u0338=s\nmax \u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8\nLDe(\u03b8 \u2032 ) + 1\u221a n\n1 (|E| \u2212 1) \u2211\ne\u2208E,e\u0338=s\nl\u2032e,max (21)\n\u2264 (1\u2212 1\u221a n )max e\u2208E max \u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 LDe(\u03b8 \u2032 ) + 1\u221a n max e\u2208E l\u2032e,max (22)\nUsing Assumption B.3 and PAC-Bayesian generalization bound (McAllester, 1999; Dziugaite & Roy, 2017; Foret et al., 2020), we find the upper bound of the sum of target domain losses.\n1 (|E| \u2212 1) \u2211\ne\u2208E,e\u0338=s\nLDe(\u03b8) \u2264 1 (|E| \u2212 1) \u2211\ne\u2208E,e\u0338=s\nE\u03f5\u223cN (0,(\u03c3\u2032)2I)[LDe(\u03b8 + \u03f5)] (23)\n\u2264 1 (|E| \u2212 1) \u2211 e\u2208E,e\u0338=s {E\u03f5\u223cN (0,(\u03c3\u2032)2I)[LDe(\u03b8 + \u03f5)] + he( \u2225\u03b8\u222522 \u03c12 )} (24)\n\u2264 (1\u2212 1\u221a n )max e\u2208E max \u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 LDe(\u03b8 \u2032 ) + 1\u221a n max e\u2208E l\u2032e,max + 1 (|E| \u2212 1) \u2211\ne\u2208E,e\u0338=s\nhe( \u2225\u03b8\u222522 \u03c12 ) (25)\n\u21d2 \u2211\ne\u2208E,e\u0338=s\nLDe(\u03b8) \u2264 (|E| \u2212 1)(1\u2212 1\u221a n )max e\u2208E max \u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 LDe(\u03b8 \u2032 ) + h\u0303( \u2225\u03b8\u222522 \u03c12 ) (26)\nwhere he, h\u0303 are strictly increasing functions. We use the fact that the sum of strictly increasing functions is also a strictly increasing function. By integrating results of Eq. 14 and 26,\n1 |E| ( LDs(\u03b8) + \u2211 e\u2208E,e\u0338=s LDe(\u03b8) ) \u2264 1|E| max\u2225\u03f5\u22252\u2264\u03c1 LDs(\u03b8 + \u03f5)\n+ (1\u2212 1|E| )(1\u2212 1\u221a n )max e\u2208E max \u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 LDe(\u03b8 \u2032 ) + h( \u2225\u03b8\u222522 \u03c12 ) (27)\nwhere h : R+ \u2192 R+ is a strictly increasing function. The first and second terms of RHS in the above inequality are upper bounded by the maximum perturbed loss for source domain and unknown domain inconsistency loss.\n1\n|E| max\u2225\u03f5\u22252\u2264\u03c1 LDs(\u03b8 + \u03f5) + (1\u2212\n1 |E| )(1\u2212 1\u221a n )max e\u2208E max \u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 LDe(\u03b8 \u2032 ) (28)\n\u2264 1|E| max\u2225\u03f5\u22252\u2264\u03c1 LDs(\u03b8 + \u03f5) + (1\u2212 1 |E| )maxe\u2208E max\u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 LDe(\u03b8 \u2032 ) (29)\n= max \u2225\u03f5\u22252\u2264\u03c1\nLDs(\u03b8 + \u03f5) + (1\u2212 1 |E| ) ( max e\u2208E\nmax \u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8\nLDe(\u03b8 \u2032 )\u2212 max\n\u2225\u03f5\u22252\u2264\u03c1 LDs(\u03b8 + \u03f5)\n) (30)\n\u2264 max \u2225\u03f5\u22252\u2264\u03c1\nLDs(\u03b8 + \u03f5) + (1\u2212 1 |E| ) ( max e\u2208E\nmax \u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8\nLDe(\u03b8 \u2032 )\u2212 max\n\u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 LDs(\u03b8\n\u2032 ) )\n(31)\n\u2264 max \u2225\u03f5\u22252\u2264\u03c1\nLDs(\u03b8 + \u03f5) + (1\u2212 1\n|E| )maxe\u2208E max\u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 (LDe(\u03b8\n\u2032 )\u2212 LDs(\u03b8 \u2032 )) (32)\n= max \u2225\u03f5\u22252\u2264\u03c1\nLDs(\u03b8 + \u03f5) + (1\u2212 1\n|E| )maxe\u2208E max\u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 |LDe(\u03b8\n\u2032 )\u2212 LDs(\u03b8 \u2032 )| (33)\nThe last equality comes from the Assumption B.4. To sum up, we can derive the upper bound of the population loss for whole domain using the maximum perturbed loss for source domain and unknown domain inconsistency loss with weight decay term.\nLD(\u03b8) \u2264 max \u2225\u03f5\u22252\u2264\u03c1\nLDs(\u03b8 + \u03f5) + (1\u2212 1\n|E| )maxe\u2208E max\u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 |LDe(\u03b8\n\u2032 )\u2212 LDs(\u03b8 \u2032 )|+ h(\u2225\u03b8\u2225 2 2\n\u03c12 ) (34)"
        },
        {
            "heading": "B.2 DETAILED EXPLANATION ON APPROXIMATION",
            "text": "Here, we show the full derivation of Eq. 8 and 9 as follows.\nmax \u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8\n( LD\u0303s(\u03b8 \u2032 )\u2212 LDs(\u03b8 \u2032 ) ) \u2248 max\n\u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8 LD\u0303s(\u03b8\n\u2032 )\u2212 LDs(\u03b8) + \u03b3 \u2032 (35)\n\u2248 2nd Taylor LD\u0303s(\u03b8)\u2212 LDs(\u03b8) + \u03c1 \u2032\u2225\u2207\u03b8LD\u0303s(\u03b8)\u22252 + max\n\u03b8\u2032\u2208N\u03b3,\u03c1s,\u03b8\n1 2 \u03b8 \u2032\u22a4HD\u0303s\u03b8 \u2032 (36)\n= ( LD\u0303s(\u03b8)\u2212 LDs(\u03b8) ) + \u03c1\u2032\u2225\u2207\u03b8LD\u0303s(\u03b8)\u22252 + \u03b3maxi \u03bb D\u0303s i /\u03bb Ds i (37)"
        },
        {
            "heading": "B.3 DISCUSSION ON HESSIAN MATRIX AND GRADIENT VARIANCE",
            "text": "Hessian Matching This section first discusses how the Hessian matrix matching between two different datasets could be substituted by the gradient variance matching for the respective datasets. It should be noted that we follow the motivation and derivation of Rame et al. (2022), and this section just re-formalize the derivations based on our notations. gi is a per-sample gradient for i-th sample; and GD = {gi}|D|i=1 is a set of per-sample gradient for xi \u2208 D. Accordingly, the variance of GD, which we denote as Var(GD), is calculated as Var(GD) = 1|D|\u22121 \u2211|D| i=1 ( gi\u2212 g\u0304 )2 . We first revisit the Eq. 37, which is our intermediate objective as follows:( LD\u0303s(\u03b8)\u2212 LDs(\u03b8) ) + \u03c1\u2032\u2225\u2207\u03b8LD\u0303s(\u03b8)\u22252 + \u03b3maxi \u03bb D\u0303s i /\u03bb Ds i (38)\nThe last term of Eq. 38, max i \u03bbD\u0303si /\u03bb Ds i , refers to the maximum eigenvalue ratio of the Hessian matrices between two different datasets, D\u0303s and Ds. This ratio is minimized when Hessian matrices of D\u0303s and Ds becomes equivalent. Then, max\ni \u03bbD\u0303si /\u03bb Ds i is approximated to the hessian matrix\nmatching between D\u0303s and Ds as \u2225HD\u0303s \u2212 HDs\u22252. Computing the full Hessian matrix in overparameterized networks is computationally challenging. Therefore, we express the formula using the diagonalized Hessian matrix, denoted as H\u0302Ds , which results in \u2225H\u0302D\u0303s \u2212 H\u0302Ds\u22252.\nLet the Fisher Information Matrix (Rame et al., 2022) as F =\u2211n i=1 Ey\u0302\u223cP\u03b8(\u00b7|xi) [ \u2207\u03b8 log p\u03b8(y\u0302|xi)\u2207\u03b8 log p\u03b8(y\u0302|xi)\u22a4 ] , where p\u03b8(\u00b7|xi) is the density of f\u03b8 on a input instance x. Fisher Information Matrix (FIM) approximates the Hessian H with theoretically probably bounded errors under mild assumptions Schraudolph (2002). Then, diagonalized Hessian matrix matching between D\u0303s and Ds, \u2225H\u0302D\u0303s \u2212 H\u0302Ds\u22252 could be replaced by \u2225F\u0302D\u0303s \u2212 F\u0302Ds\u22252, where F\u0302 also denotes the diagonalized version of F . Empirically, F\u0302 is equivalent to the gradient variance of the trained model, f\u03b8. This finally confirms the validation of our objective, gradient variance difference between D\u0303s and Ds as \u2225Var(GD\u0303s)\u2212Var(GDs)\u22252. Table 2 on the main paper of Rame et al. (2022) empirically supports that the similarity between Hessian diagonals and gradient variances is over 99.99%.\nLoss Matching Matching the gradient variances for all parameters of our model, f\u03b8, incurs significant computational overhead. In this study, we restrict the gradient variance matching to a subset of entire parameters, specificall y selecting the classifier parameters. In this section, we demonstrate that by matching gradient variances of two different datasets based on the classifier parameters, it inherently achieve the loss matching across those datasets.\nFor simplicity in notation, we refer an arbitrary domain index as e. Let xie represent the i-th sample and yie be its corresponding target class label. We denote z i e \u2208 Rd as the features for this i-th sample from domain e. The associated classifier layer W is characterized by weights {wk}dk=1 and bias b. First, we assume the mean squared error as our loss function for our analysis. For the i-th sample, the gradient of the loss with respect to b is given by \u2207b\u2113(f\u03b8(xie), yie) = (f\u03b8(xie) \u2212 yie). Hence, the gradient variance based on the parameter b for domain e is given by: Var(GbDe) = 1 ne \u2211ne i=1(f\u03b8(x i e) \u2212 yie)2, which directly aligns with the mean squared error (MSE) between the predictions and the target labels in domain e. Considering our objective, \u2225Var(GD\u0303s)\u2212Var(GDs)\u22252, the gradient variance matching on b could be recognized as mean squared error loss matching, which is \u2225 1|D\u0303s| \u2211 (xi,yi)\u2208D\u0303s(f\u03b8(x i)\u2212 yi)2 \u2212 1|Ds| \u2211 (xj ,yj)\u2208Ds(f\u03b8(x j)\u2212 yj)2\u22252.\nAnalysis on the remaining term We also investigate the gradient variance matching based on {wk}dk=1 \u2208 W , which are remaining part of the classifier parameter W . The gradients with respect to the wk is derived as\u2207wk\u2113(yie, y\u0302ie) = (y\u0302ie \u2212 yie)zi,ke . Thus, the uncentered gradient variance in wk for domain e is: Var(GwkDe) = 1 ne \u2211ne i=1 ( (y\u0302ie \u2212 yie)zi,ke )2 . Different from the case of b, Var(GwkDe) adds a weight zi,ke on the mean squared error. As z i,k e act as a weight, gradient variance matching on wk still learns toward matching the MSE loss between two different datasets."
        },
        {
            "heading": "C ALGORITHM OF UDIM",
            "text": "Here, we present the algorithm of UDIM as follows.\nAlgorithm 1: Training algorithm of UDIM w/ SAM Input: Source dataset Ds; perturbation threshold for model parameter \u03b8 and data, \u03c1\u2032 and \u03c1x;\nlearning rate \u03b7; warmup ratio for source domain flatness, p; number of total training iterations, N ; Other hyperpameters.\nOutput: Trained model f\u03b8(x) for t = 1, ..., N/p do\nWarmup f\u03b8(x) with SAM optimization as Eq. 1 end for t = N/p, ..., N do\nDefine DB = {(xi, yi)}|B|i=1 i.i.d. sampled from Ds Make D\u0303B = {(x\u0303i, yi)}|B|i=1, with x\u0303i for all i as x\u0303i = xi + \u03c1x \u2207xi ( \u2113(x,\u03b8t)+\u03c1 \u2032\u2225\u2207\u03b8t\u2113(x,\u03b8t)\u22252 )\u2225\u2225\u2207xi(\u2113(x,\u03b8t)+\u03c1\u2032\u2225\u2207\u03b8t\u2113(x,\u03b8t)\u22252)\u2225\u22252\nUpdate f\u03b8(x) by \u03b8t+1 \u2190 \u03b8t \u2212 \u03b7\u2207\u03b8t (\nmax \u2225\u03f5\u22252\u2264\u03c1\nLDB (\u03b8t + \u03f5) + \u03c1\u2032\u2225\u2207\u03b8tLD\u0303B (\u03b8t)\u22252 + \u2225Var(GD\u0303B )\u2212 Var(GDB )\u22252 + \u03bb2\u2225\u03b8t\u22252 )\nend\nWe represent the algorithm of UDIM with SAM as a default setting. It should be noted that our method can be orthogonally utilized with other sharpness-based optimization methods."
        },
        {
            "heading": "D EXPERIMENT",
            "text": "D.1 IMPLEMENTATION DETAILS"
        },
        {
            "heading": "Dataset Explanation",
            "text": "\u2022 PACS (Li et al., 2017) comprises of four domains, which are photos, arts, cartoons and sketches. This dataset contains 9,991 images. It consists of 7 classes.\n\u2022 OfficeHome (Venkateswara et al., 2017) includes four domains, which are art, clipart, product and real. This dataset contains 15,588 images. It consists of 65 classes.\n\u2022 DomainNet (Peng et al., 2019) consists of six domains, which are clipart, infograph, painting, quickdraw, real and sketch. This dataset contains 586,575 images. It consists of 345 classes.\n\u2022 CIFAR-10-C (Hendrycks & Dietterich, 2019) has been utilized to evaluate the robustness of a trained classifier. Images are corrupted from CIFAR10 (Krizhevsky et al., 2009) test dataset under 5 levels of severity. Corruption types include, brightness, contrast, defocusblur, elastic-transform, fog, frost, gaussian-blur, gaussian-noise, glass-blur, impulse-noise, jpeg-compression, motion-blur, pixelate, saturate, shot-noise, snow, spatter, speckle-noise, zoom-blur total of 19 types.\nNetwork Architecture and Optimization We use ResNet-18 (He et al., 2016) for CIFAR-10-C and ResNet-50 (He et al., 2016) for other datasets pretrained on ImageNet (Deng et al., 2009) and use Adam (Kingma & Ba, 2014) optimizer basically. learning rate is set as 3 \u00d7 10\u22125 following Wang et al. (2023). For calculating the gradient related materials, e.g. Hessian, we utilize BackPACK (Dangel et al., 2020) package.\nExperimental settings For PACS and OfficeHome, we trained for total of 5,000 iterations. For DomainNet, we trained for 15,000 iterations. For CIFAR-10, since it usually trains for 100 epochs, we translate it to iterations, which becomes total of 781\u00d7100 = 78, 100 iterations. Unless specified,\nwe use batch size as 32 for PACS, OfficeHome, and DomainNet and 64 for CIFAR-10-C. For other hyperparameters, we follow the experiment settings of Wang et al. (2023) unless specified. Although our method mainly focuses on the domain generalization, our concept could be also effectively utilized for domain adaptation Csurka (2017) and open-set domain adaptation Jang et al. (2022).\nHyperparameter setting of UDIM The main hyperparameters of UDIM is \u03c1, \u03c1 \u2032 , \u03bb1 and \u03c1x. Throughout all experiments, we set \u03c1=0.05 without any hyperparameter tuning. For \u03c1 \u2032 , we used values [0.01, 0.025, 0.05], and in most experiments, the value 0.05 consistently showed good performance. It should be noted that a warm-up using the SAM loss is required before the full UDIM optimization to ensure that \u03c1 \u2032 =0.05 can be utilized validly. For both \u03bb1 and \u03c1x, we used values in the range [1,10] and reported the best performances observed among these results. Our methodology applies perturbations to each instance of the original source domain dataset, effectively doubling the number of unique instances in a single batch compared to experiments for the baselinse. As a result, we utilized half the batch size of other baselines.\nEvaluation Detail For reporting the model performance, model selection criterion is important. We get the test performance whose accuracy for source validation dataset is best. For PACS and OfficeHome, we evaluated every 100 iterations and for DomainNet, we evaluated every 1,000 iterations for Leave-One-Out Source Domain Generalization and every 5,000 iterations for Single Source Domain Generalization."
        },
        {
            "heading": "D.2 BASELINE DESCRIPTION",
            "text": "In this paragraph, we explain baselines that we used for comparison. Specifically, we compare our method with (1) methods whose objectives are mainly related to Leave-One Out Source Domain Generalization, (2) methods which are mainly modeled for Single Source Domain Generalization, and (3) sharpness-aware minimization related methods, as we reported in tables repeatedly.\nIRM (Arjovsky et al., 2019) tries to learn a data representation such that the optimal classifier matches for all training distributions. Specifically, it minimizes the empirical risk and the regularization term, the multiplication of samples\u2019 gradients, to motivate the invariance of the predictor.\nGroupDRO (Sagawa et al., 2019) minimizes the loss by giving different weight to each domain. Weight term for each domain is proportional to the domain\u2019s current loss.\nOrgMixup (Zhang et al., 2018) represents the naive mixup technique which is generally utilized in machine learning community to boost generalization.\nMixup (Yan et al., 2020) is a mixup among domains.\nCutmix (Yun et al., 2019) is another skill which is widely used in machine learning community to boost generalization. Specifically, it mixes up parts of inputs randomly by pixel-wise.\nMixstyle (Zhou et al., 2021) mix up the statistics (specifically, mean and standard deviation) of the feature. The mixed feature statistics are applied to the style-normalized input. We did not consider the domain label.\nMTL (Blanchard et al., 2021) considers the exponential moving average (EMA) of features.\nMLDG (Li et al., 2018a) is a meta learning based method for domain generalization. Specifically, it simulates the domain shift between train and test during training procedure by synthesizing virtual testing domains within each mini-batch. Then it optimizes meta loss using the synthesized dataset.\nMMD (Li et al., 2018b) minimizes the discrepancy of feature distributions in a every domain pairwise manner, while minimizing the empirical risk for source domains.\nCORAL (Sun & Saenko, 2016) is similar to MMD. However, while MMD employs the gaussian kernel to measure the feature discrepancy, CORAL aligns the second-order statistics between different distributions with a nonlinear transformation. This alignment is achieved by matching the correlations of layer activations in deep neural networks.\nSagNet (Nam et al., 2021) disentangles style features from class categories to prevent bias. Specifically, it makes two networks, content network and style network, and trains both networks to be\ninvariant to other counterpart by giving randomized features (updating the content network with randomized styled features and vice versa).\nARM (Zhang et al., 2021) represents adaptive risk minimization. Specifically, it makes an adaptive risk representing context.\nDANN represents Domain Adversarial Neural Networks, and it iteratively trains a discriminator which discriminates domain and a featurizer to learn a feature which becomes invariant to domain information.\nCDANN is class conditional version of DANN.\nVREx (Krueger et al., 2021) controls the discrepancy between domains by minimizing the variance of loss between domains.\nRSC (Huang et al., 2020) challenges the dominant features of training domain (by masking some specific percentage of dominant gradient), so it can focus on label-related domain invariant features.\nFishr (Rame et al., 2022) approximates the hessian as the variance of gradient matrix, and they align the gradient variance of each domain.\nM-ADA (Qiao et al., 2020a) perturbs input data to simulate the unseen domain data, yet with adequate regularization not to make the data be too far from the original one. The adversarial perturbation direction is affected by the wasserstein autoencoder. Note that this method is specifically designed for Single source domain generalization.\nLTD (Wang et al., 2021a) perturbs source domain data with augmentation network, maximize the mutual information between the original feature and the perturbed feature so that the perturbed feature is not too far from the original feature (with contrastive loss), and maximize likelihood of the original feature. Note that this method is also specifically designed for Single source domain generalization.\nSAM (Foret et al., 2020) is an optimization technique to consider the sharpness of loss surface. It first perturbs parameter to its worst direction, gets gradient and update the calculated gradient at the original parameter point.\nSAGM (Wang et al., 2023) minimizes an original loss, the corresponding perturbed loss, and the gap between them. This optimization aims to identify a minima that is both flat and possesses a sufficiently low loss value. Interpreting the given formula, this optimization inherently regularizes the gradient alignment between the original loss and the perturbed loss.\nGAM (Zhang et al., 2023b) introduces first-order flatness, which minimizes a maximal gradient norm within a perturbation radius, to regularize a stronger flatness than SAM. Accordingly, GAM seeks minima with uniformly flat curvature across all directions.\nRIDG (Chen et al., 2023b) presents a new approach in deep neural networks focusing on decisionmaking in the classifier layer, diverging from the traditional emphasis on features. It introduces a \u2019rationale matrix\u2019, derived from the relationship between features and weights, to guide decisions for each input. A novel regularization term is proposed to align each sample\u2019s rationale with the class\u2019s mean, enhancing stability across samples and domains.\nITTA (Chen et al., 2023a) proposes an Improved Test-Time Adaptation (ITTA) method for domain generalization. ITTA uses a learnable consistency loss for the TTT task to better align with the main prediction task and introduces adaptive parameters in the model, recommending updates solely during the test phase. This approach aims to address the issues of auxiliary task selection and parameter updating in test-time training."
        },
        {
            "heading": "D.3 ABLATION",
            "text": "Figure 4 of the main paper presents the ablation results of UDIM, which were carried out by replacing a subpart of the UDIM\u2019s objective with alternative candidates and subsequently assessing the performances. This section enumerates enumerates each ablation candidate and its implementation. We conduct various ablations: \u2019SourceOpt\u2019 represents the optimization method for Ds, \u2019Perturb\u2019 indicates the perturbation method utilized to emulate unknown domains, and \u2019PerturbOpt\u2019 indicates\nthe optimization for the perturbed dataset D\u0303s. It should be noted that each ablation means that only the specific part is substituted, while keeping the other parts of UDIM unchanged.\nSourceOpt The optimization for the source domain dataset in UDIM is originally based on the SAM loss, which is max\u2225\u03f5\u22252\u2264\u03c1 LDs(\u03b8 + \u03f5). To verify the significance of flatness modeling, we replaced the original optimization with simple ERM, which we refer to as the ERM ablation.\nPerturb The perturbation process of UDIM is conducted based on Eq. 11. We substituted the perturbation method in Eq. 11 with traditional adversarial attack techniques, which are the cases of FGSM (Goodfellow et al., 2014) and PGD (Madry et al., 2017), to compare their performance outcomes.\nPerturbOpt Lastly, to observe the ablation for inconsistency minimization between the perturbed domain dataset D\u0303s and Ds, we replaced the optimization in Eq. 10 with both ERM and SAM-based optimizations. Each case in the PerturbOpt ablation is denoted as ERM or SAM."
        },
        {
            "heading": "D.4 ADDITIONAL RESULTS",
            "text": "In this section, we report more results that we did not report in the main paper due to the space issue.\nTable 4 shows the model performance of total baselines (At the table 3 of the main paper, there are only the model performance of some baselines). As we can see in the table, our method, UDIM, consistently improves SAM-based optimization variants and show best performances for each column. We mark - for training failure case (when the model performance is near 1%).\nTable 5 and 7 shows the model performances for OfficeHome (Venkateswara et al., 2017) and DomainNet (Peng et al., 2019) dataset, respectively. Similar to the above tables, UDIM shows per-\nformance improvement over sharpness-aware baselines, and good performances for each column consistently."
        },
        {
            "heading": "D.5 ADDITIONAL ANALYSES",
            "text": "In this section, we additionally provide analysis on 1) Comparison with Shui et al. (2022), which utilzes both 1) data-based perturbation and 2) parameter-based regularization on their own framework. Afterwards, we provide visual inspections on the perturbed domain instances, which are constructed by Eq 11."
        },
        {
            "heading": "D.5.1 ANALYTICAL COMPARISON WITH SHUI ET AL. (2022)",
            "text": "UDIM and Shui et al. (2022) share a similar direction, as both methodologies involve 1) generating virtual samples through distinct data perturbation methods, and 2) implementing their own types of regularization on these samples.\nShui et al. (2022) introduced novel regularization techniques for the embedding function based on theoretical analysis. Their approach involves establishing an upper bound on the balanced error rate in the test environment, which is obtained from the combination of the balanced error rates in the source environments, the feature-conditional invariance, and the smoothness of the embedding function. In particular, they focused on minimizing the smoothness term by reducing the Frobenius norm of the Jacobian matrix with respect to the embedding function. To implement this regularization term over unobserved regions, they use virtual samples generated by a linear combination of samples from each source.\nOn the other hand, UDIM also introduces an upper bound on the generalization loss in an unknown domain. This bound includes the SAM loss on the source domain and a region-based loss disparity between the worst-case domain and the source domain. This disparity is implemented by a gradient variance-based objective outlined in Eq. 10. The objective involves a perturbed dataset constructed by perturbing samples, where the direction of perturbation is determined by the inconsistencies described in Eq. 7."
        },
        {
            "heading": "D.5.2 ANALYTIC COMPARISON WITH DOMAIN AUGMENTATION AND ADVERSARIAL ATTACK",
            "text": "The proposed method, UDIM, involves applying arbitrary perturbation to a given instance to create a new instance, which is similar to domain augmentation and adversarial attacks. However, UDIM has the following differences and advantages compared to them.\nComparison with domain augmentation First, we taxonomize domain augmentation based on its dependency on learning signals such as parameters or loss, dividing it into not-learnable domain augmentation Zhang et al. (2018); Zhou et al. (2021); Li et al. (2021b) and learnable domain augmentation Zhang et al. (2018); Zhou et al. (2021); Li et al. (2021b). Please note that we briefly cite the representative methodologies among wide range of augmentation techniques.\nWhile non-learnable domain augmentations are effective in generating new styles and may generalize well to specific types of domains, it does not guarantee generalization across wide range of\nunseen domains, as discussed in the Theorem 3.2. In contrast, UDIM\u2019s data perturbation method is designed to generate perturbations towards the most vulnerable or worst domain from a parameter space perspective, enabling a reduction of the generalization bound in Eq 5, even in scenarios involving numerous unobserved domains. Additionally, it\u2019s important to note that these domain augmentation techniques could be applied orthogonally to the UDIM framework, for instance, by implementing domain augmentation prior to UDIM\u2019s data perturbation.\nLearnable augmentations, similar to UDIM, determine its augmentation direction based on the current parameter response. However, these methodologies do not link their augmentation with a theoretical analysis to assure minimization of the target objective, which is left-hand side of Eq 5 of our manuscript. UDIM\u2019s data perturbation impacts the generalization bound from a parameter perspective as it takes into account a parameter loss curvature information, rather than just a single parameter point, when determining perturbations.\nComparison with adversarial attack Adversarial attacks also introduce perturbations in the direction most vulnerable to the current parameters, but methodologies like FGSM Goodfellow et al. (2014) and PGD Madry et al. (2017) do not consider the local parameter curvature in their perturbation process. By integrating perturbations on instances with attention to parameter loss curvature; and parameter perturbation, we facilitate the modeling of inconsistency in unknown domains, as described in Eq 3. Having said that, Kim et al. (2023) also utilize worst-case instance selection on the active learning framework by utilizing the parameter perturbation. In the literature of coreset selection Feldman (2020), Shin et al. (2023) also utilizes the perturbed parameter region to attain samples which effectively represent whole dataset.\nFrom a mathematical perspective, UDIM\u2019s data perturbation involves receiving not only gradients related to the simple cross-entropy loss but also additional gradients concerning the norm of gradient, as elaborated in Eq 7.\nCombination of domain augmentation with SAM optimizer We accordingly report the experimental results of models, which combines various domain augmentation techniques with SAM optimization in Table 8. We reported the average test accuracy for each domain in each setting. Applying SAM optimization to data instances of augmented domains led to mixed results: some methodologies improved, others didn\u2019t, but all still under-performed compared to UDIM. We hypothesize that the observed performance decline in certain augmentations combined with the SAM optimizer might stem from an unstable learning process. This instability may arise from attempting to minimize sharpness in the perturbed domain prematurely, before ensuring flatness in the source domain."
        },
        {
            "heading": "Method Leave-One-Out Source Domain Generalization Single Source Domain Generalization",
            "text": "D.5.3 VISUAL INSPECTION ON THE PERTURBED DOMAIN\nIn this section, we aim to illustrate how perturbed instances are created depending on the size of \u03c1, which represents the magnitude of data perturbation. Each plot utilizes the PACS dataset and a model trained under the Single Source Domain Generalization setting.\nFigure 6 displays instances perturbed through the original UDIM methodology. As the perturbation size increases, we can observe a gradual distortion of the image\u2019s semantics, highlighting the importance of selecting an appropriate perturbation size.\nFigure 7 shows the scenario where the data perturbation of the original UDIM method is applied to the input channel, instead of input pixels. This approach of perturbing the channel over pixels has the advantage of maintaining the basic shape and line information of the image, ensuring the preservation of essential visual features."
        }
    ],
    "title": "UNKNOWN DOMAIN INCONSISTENCY MINIMIZATION FOR DOMAIN GENERALIZATION",
    "year": 2024
}