{
    "abstractText": "Real-world visual data exhibit intrinsic hierarchical structures that can be represented effectively in hyperbolic spaces. Hyperbolic neural networks (HNNs) are a promising approach for learning feature representations in such spaces. However, current HNNs in computer vision rely on Euclidean backbones and only project features to the hyperbolic space in the task heads, limiting their ability to fully leverage the benefits of hyperbolic geometry. To address this, we present HCNN, a fully hyperbolic convolutional neural network (CNN) designed for computer vision tasks. Based on the Lorentz model, we generalize fundamental components of CNNs and propose novel formulations of the convolutional layer, batch normalization, and multinomial logistic regression. Experiments on standard vision tasks demonstrate the promising performance of our HCNN framework in both hybrid and fully hyperbolic settings. Overall, we believe our contributions provide a foundation for developing more powerful HNNs that can better represent complex structures found in image data. Our code is publicly available at https://github.com/kschwethelm/HyperbolicCV.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ahmad Bdeir"
        },
        {
            "affiliations": [],
            "name": "Kristian Schwethelm"
        },
        {
            "affiliations": [],
            "name": "Niels Landwehr"
        }
    ],
    "id": "SP:030625ea1ab777fab344134e4a94330fe8ed8a1f",
    "references": [
        {
            "authors": [
                "Mina Ghadimi Atigh",
                "Julian Schoep",
                "Erman Acar",
                "Nanne van Noord",
                "Pascal Mettes"
            ],
            "title": "Hyperbolic image segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Irving Biederman"
            ],
            "title": "Recognition-by-components: a theory of human image understanding",
            "venue": "Psychological review,",
            "year": 1987
        },
        {
            "authors": [
                "Gary B\u00e9cigneul",
                "Octavian-Eugen Ganea"
            ],
            "title": "Riemannian adaptive optimization methods, 2018",
            "venue": "URL https://arxiv.org/abs/1810.00760",
            "year": 2018
        },
        {
            "authors": [
                "James W. Cannon",
                "William J. Floyd",
                "Richard Kenyon",
                "Walter R. Parry"
            ],
            "title": "Hyperbolic Geometry, volume 31",
            "venue": "MSRI Publications,",
            "year": 2006
        },
        {
            "authors": [
                "Ines Chami",
                "Rex Ying",
                "Christopher R\u00e9",
                "Jure Leskovec"
            ],
            "title": "Hyperbolic graph convolutional neural networks, 2019",
            "venue": "URL https://arxiv.org/abs/1910.12933",
            "year": 1910
        },
        {
            "authors": [
                "Weize Chen",
                "Xu Han",
                "Yankai Lin",
                "Hexu Zhao",
                "Zhiyuan Liu",
                "Peng Li",
                "Maosong Sun",
                "Jie Zhou"
            ],
            "title": "URL https://arxiv.org/ abs/2105.14686",
            "venue": "Fully hyperbolic neural networks. CoRR,",
            "year": 2021
        },
        {
            "authors": [
                "Hyunghoon Cho",
                "Benjamin DeMeo",
                "Jian Peng",
                "Bonnie Berger"
            ],
            "title": "Large-margin classification in hyperbolic space",
            "venue": "Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Terrance DeVries",
                "Graham W. Taylor"
            ],
            "title": "Improved regularization of convolutional neural networks with cutout, 2017",
            "venue": "URL https://arxiv.org/abs/1708.04552",
            "year": 2017
        },
        {
            "authors": [
                "Vincent Dumoulin",
                "Francesco Visin"
            ],
            "title": "A guide to convolution arithmetic for deep learning, 2016",
            "venue": "URL https://arxiv.org/abs/1603.07285",
            "year": 2016
        },
        {
            "authors": [
                "Aleksandr Ermolov",
                "Leyla Mirvakhabova",
                "Valentin Khrulkov",
                "Nicu Sebe",
                "Ivan Oseledets"
            ],
            "title": "Hyperbolic vision transformers: Combining improvements in metric learning, 2022",
            "venue": "URL https://arxiv.org/abs/2203.10833",
            "year": 2022
        },
        {
            "authors": [
                "Xiran Fan",
                "Chun-Hao Yang",
                "Baba C. Vemuri"
            ],
            "title": "Nested hyperbolic spaces for dimensionality reduction and hyperbolic nn design",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Pengfei Fang",
                "Mehrtash Harandi",
                "Trung Le",
                "Dinh Phung"
            ],
            "title": "Hyperbolic geometry in computer vision: A survey, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Octavian Ganea",
                "Gary Becigneul",
                "Thomas Hofmann"
            ],
            "title": "Hyperbolic neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Partha Ghosh",
                "Mehdi S.M. Sajjadi",
                "Antonio Vergari",
                "Michael Black",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "From variational to deterministic autoencoders, 2019",
            "year": 1903
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "In International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Jacob Grosek"
            ],
            "title": "Fundamental reflection domains for hyperbolic tesselations",
            "venue": "In Rose-Hulman Undergraduate Mathematics Journal,",
            "year": 2008
        },
        {
            "authors": [
                "Y. Guo",
                "X. Wang",
                "Y. Chen",
                "S.X. Yu"
            ],
            "title": "Clipped hyperbolic classifiers are super-hyperbolic classifiers",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "CoRR, abs/1512.03385,",
            "year": 2015
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "In Proceedings of the 31st International Conference on Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Geoffrey Hinton"
            ],
            "title": "Some demonstrations of the effects of structural descriptions in mental imagery",
            "venue": "Cognitive Science,",
            "year": 1979
        },
        {
            "authors": [
                "Joy Hsu",
                "Jeffrey Gu",
                "Gong-Her Wu",
                "Wah Chiu",
                "Serena Yeung"
            ],
            "title": "Capturing implicit hierarchical structure in 3d biomedical images with self-supervised hyperbolic representations, 2020",
            "venue": "URL https://arxiv.org/abs/2012.01644",
            "year": 2012
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37,",
            "year": 2015
        },
        {
            "authors": [
                "Daniel Kahneman",
                "Anne Treisman",
                "Brian J Gibbs"
            ],
            "title": "The reviewing of object files: Objectspecific integration of information",
            "venue": "Cognitive Psychology,",
            "year": 1992
        },
        {
            "authors": [
                "V. Khrulkov",
                "L. Mirvakhabova",
                "E. Ustinova",
                "I. Oseledets",
                "V. Lempitsky"
            ],
            "title": "Hyperbolic image embeddings",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "URL https: //arxiv.org/abs/1312.6114",
            "year": 2013
        },
        {
            "authors": [
                "Reinmar J. Kobler",
                "Jun-ichiro Hirayama",
                "Motoaki Kawanabe"
            ],
            "title": "Controlling the fr\u00e9chet variance improves batch normalization on the symmetric positive definite manifold",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Max Kochurov",
                "Rasul Karimov",
                "Serge"
            ],
            "title": "Kozlukov. Geoopt: Riemannian optimization in pytorch",
            "year": 2020
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "pp. 32\u201333,",
            "year": 2009
        },
        {
            "authors": [
                "Marc Law",
                "Renjie Liao",
                "Jake Snell",
                "Richard Zemel"
            ],
            "title": "Lorentzian distance learning for hyperbolic representations",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Ya Le",
                "Xuan S. Yang"
            ],
            "title": "Tiny imagenet visual recognition challenge",
            "year": 2015
        },
        {
            "authors": [
                "Guy Lebanon",
                "John Lafferty"
            ],
            "title": "Hyperplane margin classifiers on the multinomial manifold",
            "venue": "In Proceedings of the Twenty-First International Conference on Machine Learning,",
            "year": 2004
        },
        {
            "authors": [
                "Y. Lecun",
                "L. Bottou",
                "Y. Bengio",
                "P. Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE,",
            "year": 1998
        },
        {
            "authors": [
                "Shaoteng Liu",
                "Jingjing Chen",
                "Liangming Pan",
                "Chong-Wah Ngo",
                "Tat-Seng Chua",
                "Yu-Gang Jiang"
            ],
            "title": "Hyperbolic visual embedding learning for zero-shot recognition",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Ziwei Liu",
                "Ping Luo",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "In 2015 IEEE International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "Aaron Lou",
                "Isay Katsman",
                "Qingxuan Jiang",
                "Serge Belongie",
                "Ser-Nam Lim",
                "Christopher De Sa"
            ],
            "title": "Differentiating through the fr\u00e9chet mean, 2020",
            "year": 2003
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Emile Mathieu",
                "Charline Le Lan",
                "Chris J. Maddison",
                "Ryota Tomioka",
                "Yee Whye Teh"
            ],
            "title": "Continuous hierarchical representations with poincar\u00e9 variational auto-encoders, 2019",
            "venue": "URL https:// arxiv.org/abs/1901.06033",
            "year": 1901
        },
        {
            "authors": [
                "Pascal Mettes",
                "Mina Ghadimi Atigh",
                "Martin Keller-Ressel",
                "Jeffrey Gu",
                "Serena Yeung"
            ],
            "title": "Hyperbolic deep learning in computer vision: A survey, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Gal Mishne",
                "Zhengchao Wan",
                "Yusu Wang",
                "Sheng Yang"
            ],
            "title": "The numerical stability of hyperbolic representation learning, 2022",
            "venue": "URL https://arxiv.org/abs/2211.00181",
            "year": 2022
        },
        {
            "authors": [
                "Valter Moretti"
            ],
            "title": "The interplay of the polar decomposition theorem and the lorentz group, 2002. URL https://arxiv.org/abs/math-ph/0211047",
            "year": 2002
        },
        {
            "authors": [
                "Yoshihiro Nagano",
                "Shoichiro Yamaguchi",
                "Yasuhiro Fujita",
                "Masanori Koyama"
            ],
            "title": "A wrapped normal distribution on hyperbolic space for gradient-based learning, 2019",
            "venue": "URL https://arxiv",
            "year": 1902
        },
        {
            "authors": [
                "Maximilian Nickel",
                "Douwe Kiela"
            ],
            "title": "Learning continuous hierarchies in the lorentz model of hyperbolic geometry, 2018",
            "venue": "URL https://arxiv.org/abs/1806.03417",
            "year": 2018
        },
        {
            "authors": [
                "Natalya Fridman Noy",
                "Carole D. Hafner"
            ],
            "title": "The state of the art in ontology design: A survey and comparative review",
            "venue": "AI Magazine,",
            "year": 1997
        },
        {
            "authors": [
                "Ivan Ovinnikov"
            ],
            "title": "Poincar\u00e9 wasserstein autoencoder. 2019",
            "venue": "doi: 10.48550/ARXIV.1901.01427. URL https://arxiv.org/abs/1901.01427",
            "year": 1901
        },
        {
            "authors": [
                "Wei Peng",
                "Tuomas Varanka",
                "Abdelrahman Mostafa",
                "Henglin Shi",
                "Guoying Zhao"
            ],
            "title": "Hyperbolic deep neural networks: A survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Xavier Pennec"
            ],
            "title": "Intrinsic statistics on riemannian manifolds: Basic tools for geometric measurements",
            "venue": "Journal of Mathematical Imaging and Vision,",
            "year": 2006
        },
        {
            "authors": [
                "Eric Qu",
                "Dongmian Zou"
            ],
            "title": "Lorentzian fully hyperbolic generative adversarial network, 2022",
            "venue": "URL https://arxiv.org/abs/2201.12825",
            "year": 2022
        },
        {
            "authors": [
                "John G. Ratcliffe"
            ],
            "title": "Foundations of Hyperbolic Manifolds. Springer, 2 edition",
            "year": 2006
        },
        {
            "authors": [
                "Danilo Jimenez Rezende",
                "Shakir Mohamed",
                "Daan Wierstra"
            ],
            "title": "Stochastic backpropagation and approximate inference in deep generative models, 2014. URL https://arxiv.org/abs/ 1401.4082",
            "year": 2014
        },
        {
            "authors": [
                "Rik Sarkar"
            ],
            "title": "Low distortion delaunay embedding of trees in hyperbolic plane",
            "venue": "Graph Drawing,",
            "year": 2012
        },
        {
            "authors": [
                "Maximilian Seitzer"
            ],
            "title": "pytorch-fid: FID Score for PyTorch",
            "venue": "https://github.com/mseitzer/ pytorch-fid,",
            "year": 2020
        },
        {
            "authors": [
                "Jiexi Yan",
                "Lei Luo",
                "Cheng Deng",
                "Heng Huang"
            ],
            "title": "Unsupervised hyperbolic metric learning",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Zhen Yu",
                "Toan Nguyen",
                "Yaniv Gal",
                "Lie Ju",
                "Shekhar S. Chandra",
                "Lei Zhang",
                "Paul Bonnington",
                "Victoria Mar",
                "Zhiyong Wang",
                "Zongyuan Ge"
            ],
            "title": "Skin lesion recognition with class-hierarchy regularized hyperbolic embeddings, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Yun Yue",
                "Fangzhou Lin",
                "Kazunori D Yamada",
                "Ziming Zhang"
            ],
            "title": "Hyperbolic contrastive learning, 2023",
            "venue": "URL https://arxiv.org/abs/2302.01409",
            "year": 2023
        },
        {
            "authors": [
                "Huanqiu Zhang",
                "P. Dylan Rich",
                "Albert K. Lee",
                "Tatyana O. Sharpee"
            ],
            "title": "Hippocampal spatial representations exhibit a hyperbolic geometry that expands with experience",
            "venue": "Nature Neuroscience,",
            "year": 2023
        },
        {
            "authors": [
                "1Chen"
            ],
            "title": "2021) note that their general formula is not fully hyperbolic, but a relaxation in implementation, while the input and output are still guaranteed to lie in the Lorentz model",
            "year": 2021
        },
        {
            "authors": [
                "Cho"
            ],
            "title": "2019) and utilize the hyperbolic reflection",
            "year": 2019
        },
        {
            "authors": [
                "Cho"
            ],
            "title": "2019) shows that the distance formula for hyperplanes in the unit Lorentz model can be extended easily to the general case by inserting the curvature parameter K at two places",
            "venue": "Comparing Eq",
            "year": 2019
        },
        {
            "authors": [
                "Motivating the use of HNNs by measuring the \u03b4-hyperbolicity of visual datasets was proposed by Khrulkov"
            ],
            "title": "The idea is to first generate image embeddings from a vision model and then quantify the degree of inherent tree-structure",
            "venue": "This is achieved by considering \u03b4-slim triangles and determining the",
            "year": 2020
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "Lorentz fully-connected layers with the uniform distribution U(\u22120.02, 0.02). As the LFC is the backbone of our Lorentz convolutional layer, we test the uniform initialization in HCNNs and compare it to the standard Kaiming initialization (He et al., 2015a) used in most Euclidean CNNs",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Representation learning is a fundamental aspect of deep neural networks, as obtaining an optimal representation of the input data is crucial. While Euclidean geometry has been the traditional choice for representing data due to its intuitive properties, recent research has highlighted the advantages of using hyperbolic geometry as a geometric prior for the feature space of neural networks. Given the exponentially increasing distance to the origin, hyperbolic spaces can be thought of as continuous versions of trees that naturally model tree-like structures, like hierarchies or taxonomies, without spatial distortion and information loss (Nickel & Kiela, 2018; Sarkar, 2012). This is compelling since hierarchies are ubiquitous in knowledge representation (Noy & Hafner, 1997), and even the natural spatial representations in the human brain exhibit a hyperbolic geometry (Zhang et al., 2023).\nLeveraging this better representative capacity, hyperbolic neural networks (HNNs) have demonstrated increased performance over Euclidean models in many natural language processing (NLP) and graph embedding tasks (Peng et al., 2022). However, hierarchical structures have also been shown to exist in images. Mathematically, Khrulkov et al. (2020) have found high \u03b4-hyperbolicity in the final embeddings of image datasets, where the hyperbolicity quantifies the degree of inherent treestructure. Extending their measurement to the whole model reveals high hyperbolicity in intermediate embeddings as well (see Appendix D.1). Intuitively, hierarchies that emerge within and across images can be demonstrated on the level of object localization and object class relationships. A straightforward example of the latter is animal classification hierarchy, where species is the lowest tier, preceded by genus, family, order, etc. Similarly, on a localization level, humans are one example: the nose, eyes, and mouth are positioned on the face, which is a part of the head, and, ultimately, a part of the body. This tree-like localization forms the basis of part-whole relationships and is strongly believed to be how we parse visual scenes (Biederman, 1987; Hinton, 1979; Kahneman et al., 1992).\nIn light of these findings, recent works have begun integrating hyperbolic geometry into vision architectures (Mettes et al., 2023; Fang et al., 2023). Specifically, they rely on the Poincar\u00e9 ball\n\u2217Equal contribution. \u2020Work done while at University of Hildesheim.\nand the Lorentz model as descriptors of hyperbolic space and formalize hyperbolic translations of neural network layers. This is challenging due to ill-defined hyperbolic analogs of, e.g., addition, multiplication, and statistical measures. Currently, most HNN components are only available in the Poincar\u00e9 ball as it supports the gyrovector space with basic vector operations. However, due to its hard numerical constraint, the Poincar\u00e9 ball is more susceptible to numerical instability than the Lorentz model (Mishne et al., 2022), which motivates introducing the missing layers for the Lorentz model. Moreover, HNNs in computer vision have been limited to hybrid architectures that might not fully leverage the advantages of hyperbolic geometry as they rely on Euclidean encoders to learn hyperbolic representations. Until now, hyperbolic encoder architectures are missing in computer vision, although prevalent in NLP and graph applications (Peng et al., 2022).\nIn this work, we present HCNN, a fully hyperbolic framework for vision tasks that can be used to design hyperbolic encoder models. We generalize the ubiquitous convolutional neural network (CNN) architecture to the Lorentz model, extend hyperbolic convolutional layers to 2D, and present novel hyperbolic formulations of batch normalization and multinomial logistic regression. Our methodology is general, and we show that our components can be easily integrated into existing architectures. Our contributions then become three-fold:\n1. We propose hybrid (HECNN) and fully hyperbolic (HCNN) convolutional neural network encoders for image data, introducing the fully hyperbolic setting in computer vision.\n2. We provide missing Lorentzian formulations of the 2D convolutional layer, batch normalization, and multinomial logistic regression.\n3. We empirically demonstrate the performance potential of deeper hyperbolic integrations in experiments on standard vision tasks, including image classification and generation."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Hyperbolic image embeddings Previous research on HNNs in computer vision has mainly focused on combining Euclidean encoders and hyperbolic embeddings. This approach involves projecting Euclidean embeddings onto the hyperbolic space in the task heads and designing task-related objective functions based on hyperbolic geometry. Such simple hybrid architectures have been proven effective in various vision tasks like recognition (Yu et al., 2022; Khrulkov et al., 2020; Liu et al., 2020; Guo et al., 2022), segmentation (Hsu et al., 2020; Atigh et al., 2022), reconstruction/generation (Mathieu et al., 2019; Nagano et al., 2019; Ovinnikov, 2019; Qu & Zou, 2022), and metric learning (Ermolov et al., 2022; Yan et al., 2021; Yue et al., 2023). However, there remains the discussion of whether the single application of hyperbolic geometry in the decoder can fully leverage the present hierarchical information. In contrast, HE/HCNN also learns latent hyperbolic feature representations in the encoder, potentially magnifying these benefits. We also forgo the typically used Poincar\u00e9 ball in favor of the Lorentz model, as it offers better stability and optimization (Mishne et al., 2022). For a complete overview of vision HNNs and motivations, refer to (Mettes et al., 2023; Fang et al., 2023).\nFully hyperbolic neural networks Designing fully hyperbolic neural networks requires generalizing Euclidean network components to hyperbolic geometry. Notably, Ganea et al. (2018) and Shimizu et al. (2020) utilized the Poincar\u00e9 ball and the gyrovector space to generalize various layers, including fully-connected, convolutional, and attention layers, as well as operations like split, concatenation,\nand multinomial logistic regression (MLR). Researchers have also designed components in the Lorentz model (Nickel & Kiela, 2018; Fan et al., 2022; Chen et al., 2021; Qu & Zou, 2022), but crucial components for vision, like the standard convolutional layer and the MLR classifier, are still missing. Among the hyperbolic layer definitions, fully hyperbolic neural networks have been built for various tasks in NLP and graph applications (Peng et al., 2022). However, no hyperbolic encoder architecture has yet been utilized in computer vision. Our work provides formulations for missing components in the Lorentz model, allowing for hyperbolic CNN vision encoders. Concurrently, van Spengler et al. (2023) proposed a fully hyperbolic Poincar\u00e9 CNN.\nNormalization in HNNs There are few attempts at translating standard normalization layers to the hyperbolic setting. To the best of our knowledge, there is only a single viable normalization layer for HNNs, i.e., the general Riemannian batch normalization (Lou et al., 2020). However, this method is not ideal due to the slow iterative computation of the Fr\u00e9chet mean and the arbitrary re-scaling operation that is not based on hyperbolic geometry. The concurrent work on Poincar\u00e9 CNN (van Spengler et al., 2023) only solved the first issue by using the Poincar\u00e9 midpoint. In contrast, we propose an efficient batch normalization algorithm founded in the Lorentz model, which utilizes the Lorentzian centroid (Law et al., 2019) and a mathematically motivated re-scaling operation.\nNumerical stability of HNNs The exponential growth of the Lorentz model\u2019s volume with respect to the radius can introduce numerical instability and rounding errors in floating-point arithmetic. This requires many works to rely on 64-bit precision at the cost of higher memory and runtime requirements. To mitigate this, researchers have introduced feature clipping and Euclidean reparameterizations (Mishne et al., 2022; Guo et al., 2022; Mathieu et al., 2019). We adopt these approaches to run under 32-bit floating point arithmetic and reduce computational cost."
        },
        {
            "heading": "3 BACKGROUND",
            "text": "This section summarizes the mathematical background of hyperbolic geometry (Cannon et al., 2006; Ratcliffe, 2006). The n-dimensional hyperbolic space HnK is a Riemannian manifold (Mn, gKx ) with constant negative curvature K < 0, where Mn and gKx represent the manifold and the Riemannian metric, respectively. There are isometrically equivalent models of hyperbolic geometry. We employ the Lorentz model because of its numerical stability and its simple exponential/logarithmic maps and distance functions. Additionally, we use the Poincar\u00e9 ball for baseline implementations. Both hyperbolic models provide closed-form formulae for manifold operations, including distance measures, exponential/logarithmic maps, and parallel transportation. They are detailed in Appendix A.\nLorentz model The n-dimensional Lorentz model LnK = (Ln, gKx ) models hyperbolic geometry on the upper sheet of a two-sheeted hyperboloid Ln, with origin 0 = [ \u221a\n\u22121/K, 0, \u00b7 \u00b7 \u00b7 , 0]T and embedded in (n+ 1)-dimensional Minkowski space (see Figure 2). Based on the Riemannian metric gKx = diag(\u22121, 1, . . . , 1), the manifold is defined as\nLn := {x \u2208 Rn+1 | \u27e8x,x\u27e9L = 1\nK , xt > 0}, (1)\nwith the Lorentzian inner product\n\u27e8x,y\u27e9L := \u2212xtyt + xTs ys = xTdiag(\u22121, 1, \u00b7 \u00b7 \u00b7 , 1)y. (2)\nWhen describing points in the Lorentz model, we inherit the terminology of special relativity and call the first dimension the time component xt and the remaining dimensions the space component xs, such that x \u2208 LnK = [xt,xs]T and xt = \u221a ||xs||2 \u2212 1/K."
        },
        {
            "heading": "4 FULLY HYPERBOLIC CNN (HCNN)",
            "text": "We aim to give way to building vision models that can fully leverage the advantages of hyperbolic geometry by learning features in hyperbolic spaces. For this, we generalize Euclidean CNN components\nto the Lorentz model, yielding one-to-one replacements that can be integrated into existing architectures. In the following, we first define the cornerstone of HCNNs, i.e., the Lorentz convolutional layer, including its transposed variant. Then, we introduce the Lorentz batch normalization algorithm and the MLR classifier. Finally, we generalize the residual connection and non-linear activation."
        },
        {
            "heading": "4.1 LORENTZ CONVOLUTIONAL LAYER",
            "text": "Hyperbolic feature maps The convolutional layer applies vector operations to an input feature map containing the activations of the previous layer. In Euclidean space, arbitrary numerical values can be combined to form a vector. However, in the Lorentz model, not all possible value combinations represent a point that can be processed with hyperbolic operations (LnK \u2282 Rn+1). We propose using channel-last feature map representations throughout HCNNs and adding the Lorentz model\u2019s time component as an additional channel dimension. This defines a hyperbolic feature map as an ordered set of n-dimensional hyperbolic vectors, where every spatial position contains a vector that can be combined with its neighbors. Additionally, it offers a nice interpretation where an image is an ordered set of color vectors, each describing a pixel.\nFormalization of the convolutional layer We define the convolutional layer as a matrix multiplication between a linearized kernel and a concatenation of the values in its receptive field, following Shimizu et al. (2020). Then, we generalize this definition by replacing the Euclidean operators with their hyperbolic counterparts in the Lorentz model.\nGiven a hyperbolic input feature map x = {xh,w \u2208 LnK}H,Wh,w=1 as an ordered set of n-dimensional hyperbolic feature vectors, each describing image pixels, the features within the receptive field of the kernel K \u2208 Rm\u00d7n\u00d7H\u0303\u00d7W\u0303 are {xh\u2032+\u03b4h\u0303,w\u2032+\u03b4w\u0303 \u2208 LnK} H\u0303,W\u0303 h\u0303,w\u0303=1 , where (h\u2032, w\u2032) denotes the starting position and \u03b4 is the stride parameter. Now, we define the Lorentz convolutional layer as\nyh,w = LFC(HCat({xh\u2032+\u03b4h\u0303,w\u2032+\u03b4w\u0303 \u2208 LnK} H\u0303,W\u0303 h\u0303,w\u0303=1 })), (3)\nwhere HCat denotes the concatenation of hyperbolic vectors, and LFC denotes a Lorentz fullyconnected layer performing the affine transformation and parameterizing the kernel and bias, respectively (see Appendix A). Additionally, we implement padding using origin vectors, the analog of zero vectors in hyperbolic space. The LFC layer is similar to Chen et al. (2021) but does not use normalization as it is done through the hyperbolic batch normalization formulated below.\nExtension to the transposed setting The transposed convolutional layer is usually used in encoderdecoder architectures for up-sampling. A convolutional layer carries out a transposed convolution when the correct local connectivity is established by inserting zeros at certain positions. Specifically, when stride s > 1, then s\u2212 1 zero vectors are inserted between the features. We refer to Dumoulin & Visin (2016) for illustrations. Under this relationship, the Lorentz transposed convolutional layer is a Lorentz convolutional layer with changed connectivity through origin padding."
        },
        {
            "heading": "4.2 LORENTZ BATCH NORMALIZATION",
            "text": "Given a batch B of m features xi, the traditional batch normalization algorithm (Ioffe & Szegedy, 2015) calculates the mean \u00b5B = 1m \u2211m i=1 xi and variance \u03c3 2 B = 1 m \u2211m i=1(xi \u2212 \u00b5B)2 across the batch dimension. Then, the features are re-scaled and re-centered using a parameterized variance \u03b3 and mean \u03b2 as follows\nBN(xi) = \u03b3 \u2299 xi \u2212 \u00b5B\u221a \u03c32B + \u03f5 + \u03b2. (4)\nAt test time, running estimates approximate the batch statistics. They are calculated iteratively during training: \u00b5t = (1\u2212 \u03b7)\u00b5t\u22121+ \u03b7\u00b5B and \u03c32t = (1\u2212 \u03b7)\u03c32t\u22121+ \u03b7\u03c32B, with \u03b7 and t denoting momentum and the current iteration, respectively. We generalize batch normalization to the Lorentz model using the Lorentzian centroid and the parallel transport operation for re-centering, and the Fr\u00e9chet variance and straight geodesics at the origin\u2019s tangent space for re-scaling.\nRe-centering To re-center hyperbolic features, it is necessary to compute a notion of mean. Usually, the Fr\u00e9chet mean is used (Lou et al., 2020), which minimizes the expected squared distance between a set of points in a metric space (Pennec, 2006). Generally, the Fr\u00e9chet mean must be solved iteratively, massively slowing down training. To this end, we propose to use the centroid with respect to the squared Lorentzian distance, which can be calculated efficiently in closed form (Law et al., 2019). The weighted Lorentzian centroid, which solves min\u00b5\u2208LnK \u2211m i=1 \u03bdid 2 L(xi,\u00b5), with xi \u2208 LnK and\n\u03bdi \u2265 0, \u2211m i=1 \u03bdi > 0, is given by\n\u00b5 = \u2211m i=1 \u03bdixi\u221a\n\u2212K |||\u2211mi=1 \u03bdixi||L| . (5) In batch normalization, the mean is not weighted, which gives \u03bdi = 1m . Now, we shift the features from the batch\u2019s mean \u00b5B to the parameterized mean \u03b2 using the parallel transport operation PTK\u00b5B\u2192\u03b2 (x). Parallel transport does not change the variance, as it is defined to preserve the distance between all points. Finally, the running estimate is updated iteratively using the weighted centroid with \u03bd1 = (1\u2212 \u03b7) and \u03bd2 = \u03b7.\nRe-scaling For re-scaling, we rely on the Fr\u00e9chet variance \u03c32 \u2208 R+, defined as the expected squared Lorentzian distance between a point xi and the mean \u00b5, and given by \u03c32 = 1m \u2211m i=1 d 2 L(xi,\u00b5) (Kobler et al., 2022). In order to re-scale the batch, features must be moved along the geodesics connecting them to their centroid, which is generally infeasible to compute. However, geodesics intersecting the origin are very simple, as they can be represented by straight lines in tangent space T0LnK . This is reflected by the equality between the distance of a point to the origin and the length of its corresponding tangent vector (dL(x,0) = || logK0 (x)||). Using this property, we propose to re-scale features by first parallel transporting them towards the origin PTK\u00b5B\u21920 ( logK\u00b5B(x) ) , making the origin the new centroid and straightening the relevant geodesics. Then, a simple multiplication re-scales the features in tangent space. Finally, parallel transporting to \u03b2 \u2208 LnK completes the algorithm and yields the normalized features. The final algorithm is formalized as\nLBN(x) = expK\u03b2 PTK0\u2192\u03b2 \u03b3 \u00b7 PTK\u00b5B\u21920 ( logK\u00b5B(x) ) \u221a \u03c32B + \u03f5  . (6)"
        },
        {
            "heading": "4.3 LORENTZ MLR CLASSIFIER",
            "text": "In this section, we consider the problem of classifying instances that are represented in the Lorentz model. A standard method for multi-class classification is multinomial logistic regression (MLR). Inspired by the generalization of MLR to the Poincar\u00e9 ball (Ganea et al., 2018; Shimizu et al., 2020) based on the distance to margin hyperplanes, we derive a formulation in the Lorentz model.\nHyperplane in the Lorentz model Analogous to Euclidean space, hyperbolic hyperplanes split the manifold into two half-spaces, which can then be used to separate instances into classes. The hyperplane in the Lorentz model is defined by a geodesic that results from the intersection of an n-dimensional hyperplane with the hyperboloid in the ambient space Rn+1 (Cho et al., 2019). Specifically, for p \u2208 LnK and w \u2208 TpLnK , the hyperplane passing through p and perpendicular to w is given by\nHw,p = {x \u2208 LnK | \u27e8w,x\u27e9L = 0}. (7)\nThis formulation comes with the non-convex optimization condition \u27e8w,w\u27e9L > 0, which is undesirable in machine learning. To eliminate this condition, we use the Euclidean reparameterization of Mishne et al. (2022), which we extend to include the curvature parameter K in Appendix B.1. In short, w is parameterized by a vector z \u2208 T0LnK = [0, az/||z||], where a \u2208 R and z \u2208 Rn. As w \u2208 TpLnK , z is parallel transported to p, which gives\nw := PTK0\u2192p (z) = [sinh( \u221a \u2212Ka)||z||, cosh( \u221a \u2212Ka)z]. (8)\nInserting Eq. 8 into Eq. 7, the formula of the Lorentz hyperplane becomes\nH\u0303z,a = {x \u2208 LnK | cosh( \u221a \u2212Ka)\u27e8z,xs\u27e9 \u2212 sinh( \u221a \u2212Ka) ||z|| xt = 0}, (9)\nwhere a and z represent the distance and orientation to the origin, respectively.\nFinally, we need the distance to the hyperplane to quantify the model\u2019s confidence. It is formulated by the following theorem, proven in Appendix B.2.\nTheorem 1 Given a \u2208 R and z \u2208 Rn, the minimum hyperbolic distance from a point x \u2208 LnK to the hyperplane H\u0303z,a defined in Eq. 9 is given by\ndL(x, H\u0303z,a) = 1\u221a \u2212K \u2223\u2223\u2223\u2223\u2223\u2223sinh\u22121 \u221a\u2212K cosh(\u221a\u2212Ka)\u27e8z,xs\u27e9 \u2212 sinh(\u221a\u2212Ka) ||z|| xt\u221a || cosh( \u221a \u2212Ka)z||2 \u2212 (sinh( \u221a \u2212Ka)||z||)2 \u2223\u2223\u2223\u2223\u2223\u2223 . (10) MLR in the Lorentz model Lebanon & Lafferty (2004) formulated the logits of the Euclidean MLR classifier using the distance from instances to hyperplanes describing the class regions. Specifically, given input x \u2208 Rn and C classes, the output probability of class c \u2208 {1, ..., C} can be expressed as\np(y = c | x) \u221d exp(vwc(x)), vwc(x) = sign(\u27e8wc,x\u27e9)||wc||d(x, Hwc), wc \u2208 Rn, (11)\nwhere Hwc is the decision hyperplane of class c.\nWe define the Lorentz MLR without loss of generality by inserting the Lorentzian counterparts into Eq. 11. This yields logits given by the following theorem, proven in Appendix B.3.\nTheorem 2 Given parameters ac \u2208 R and zc \u2208 Rn, the Lorentz MLR\u2019s output logit corresponding to class c and input x \u2208 LnK is given by\nvzc,ac(x) = 1\u221a \u2212K sign(\u03b1)\u03b2 \u2223\u2223\u2223\u2223sinh\u22121(\u221a\u2212K\u03b1\u03b2 )\u2223\u2223\u2223\u2223 , (12)\n\u03b1 = cosh( \u221a \u2212Ka)\u27e8z,xs\u27e9 \u2212 sinh( \u221a \u2212Ka),\n\u03b2 = \u221a || cosh( \u221a \u2212Ka)z||2 \u2212 (sinh( \u221a \u2212Ka)||z||)2."
        },
        {
            "heading": "4.4 LORENTZ RESIDUAL CONNECTION AND ACTIVATION",
            "text": "Residual connection The residual connection is a crucial component when designing deep CNNs. As vector addition is ill-defined in the Lorentz model, we add the vector\u2019s space components and concatenate a corresponding time component. This is possible as a point x \u2208 LnK can be defined by an arbitrary space component xs \u2208 Rn and a time component xt = \u221a ||xs||2 \u2212 1/K. Our method is straightforward and provides the best empirical performance compared to other viable methods for addition we implemented, i.e., tangent space addition (Nickel & Kiela, 2018), parallel transport addition (Chami et al., 2019), M\u00f6bius addition (after projecting to the Poincar\u00e9 ball) (Ganea et al., 2018), and fully-connected layer addition (Chen et al., 2021).\nNon-linear activation Prior works use non-linear activation in tangent space (Fan et al., 2022), which weakens the model\u2019s stability due to frequent logarithmic and exponential maps. We propose a simpler operation for the Lorentz model by applying the activation function to the space component and concatenating a time component. For example, the Lorentz ReLU activation is given by\ny =\n[ \u221a ||ReLU(xs)||2 \u2212 1/K\nReLU(xs)\n] . (13)"
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We evaluate hyperbolic models on image classification and generation tasks and compare them against Euclidean and hybrid HNN counterparts from the literature. To ensure a fair comparison, in every task, we directly translate a Euclidean baseline to the hyperbolic setting by using hyperbolic modules as one-to-one replacements. All experiments are implemented in PyTorch (Paszke et al., 2019), and we optimize hyperbolic models using adaptive Riemannian optimizers (B\u00e9cigneul & Ganea, 2018) provided by Geoopt (Kochurov et al., 2020), with floating-point precision set to 32 bits. We provide detailed experimental configurations in Appendix C and ablation experiments in Appendix D."
        },
        {
            "heading": "5.1 IMAGE CLASSIFICATION",
            "text": "Experimental setup We evaluate image classification performance using ResNet-18 (He et al., 2015b) and three datasets: CIFAR-10 (Krizhevsky, 2009), CIFAR-100 (Krizhevsky, 2009), and Tiny-ImageNet (Le & Yang, 2015). All these datasets exhibit hierarchical class relations and high hyperbolicity (low \u03b4rel), making the use of hyperbolic models well-motivated.\nFor the HCNN, we replace all components in the ResNet architecture with our proposed Lorentz modules. Additionally, we experiment with a novel hybrid approach (HECNN), where we employ our Lorentz decoder and replace only the ResNet encoder blocks with the highest hyperbolicity (\u03b4rel < 0.2), i.e., blocks 1 and 3 (see Appendix D.1). To establish hyperbolic baselines we follow the literature (Atigh et al., 2022; Guo et al., 2022) and implement hybrid HNNs with a Euclidean encoder and a hyperbolic output layer (using both the Poincar\u00e9 MLR (Shimizu et al., 2020) and our novel Lorentz MLR). Additionally, we report classification results for the concurrently developed fully hyperbolic Poincar\u00e9 ResNet (van Spengler et al., 2023). For all models, we adopt the training procedure and hyperparameters of DeVries & Taylor (2017), which have been optimized for Euclidean CNNs and yield a strong Euclidean ResNet baseline.\nMain results Table 1 shows that hyperbolic models using the Lorentz model achieve the highest accuracy across all datasets, outperforming both the Euclidean and Poincar\u00e9 baselines. In contrast, the Poincar\u00e9 HNNs are consistently worse than the Euclidean baseline, aligning with the results of Guo et al. (2022). Notably, only in the case of CIFAR-10, all models exhibit equal performance, which is expected due to the dataset\u2019s simplicity. We also notice that the hybrid encoder model outperforms the fully hyperbolic model, indicating that not all parts of the model benefit from hyperbolic geometry. Overall, our findings suggest that the Lorentz model is better suited for HNNs than the Poincar\u00e9 ball. This may be attributed to the better numerical stability causing fewer inaccuracies (Mishne et al., 2022). Furthermore, we achieve a notable improvement (of up to 1.5%) in the accuracy of current HNNs. This shows the potential of using our HCNN components in advancing HNNs.\nAdversarial robustness Prior works have demonstrated the robustness of hyperbolic models against adversarial attacks (Yue et al., 2023; Guo et al., 2022). We expect better performance for HCNNs/HECNNs due to the bigger effect fully hyperbolic models have on the embedding space as can be seen in Figure 3. We believe the benefit could come from the increased inter-class separation afforded by the distance metric which allows for greater slack in the object classification. To study this, we employ the trained models and attack them using FGSM (Goodfellow et al., 2015) and PGD (Madry et al., 2019) with different perturbations. The results in Table 2 show that our HCNN is more\nrobust, achieving up to 5% higher accuracy. In addition, and contrary to Guo et al. (2022), we observe that hybrid decoder HNNs can be more susceptible to adversarial attacks than Euclidean models."
        },
        {
            "heading": "Low embedding dimensionality",
            "text": "HNNs have shown to be most effective for low-dimensional embeddings (Peng et al., 2022). To this end, we reduce the dimensionality of the final ResNet block and the embeddings and evaluate classification accuracy on CIFAR-100.\nThe results in Figure 3 verify the effectiveness of hyperbolic spaces with low dimensions, where all HNNs outperform the Euclidean models. However, our HCNN and HECNN can leverage this advantage best, suggesting that hyperbolic encoders offer great opportunities for dimensionality reduction and designing smaller mod-\nels with fewer parameters. The high performance of HECNN is unexpected as we hypothesized the fully hyperbolic model to perform best. This implies that hybrid encoder HNNs might make better use of the combined characteristics of both Euclidean and hyperbolic spaces."
        },
        {
            "heading": "5.2 IMAGE GENERATION",
            "text": "Experimental setup Variational autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) have been widely adopted in HNN research to model latent embeddings in hyperbolic spaces (Nagano et al., 2019; Mathieu et al., 2019; Ovinnikov, 2019; Hsu et al., 2020). HNNs have shown to generate more expressive embeddings under lower dimensionalities which would make them a good fit for VAEs. In this experiment, we extend the hyperbolic VAE to the fully hyperbolic setting using our proposed HCNN framework and, for the first time, evaluate its performance on image generation using the standard Fr\u00e9chet Inception Distance (FID) metric (Heusel et al., 2017).\nBuilding on the experimental setting of Ghosh et al. (2019), we test vanilla VAEs and assess generative performance on CIFAR-10 (Krizhevsky, 2009), CIFAR-100 (Krizhevsky, 2009), and CelebA (Liu et al., 2015) datasets. We compare our HCNN-VAE against the Euclidean and two hybrid models. Following prior works, the hybrid models only include a latent hyperbolic distribution and no hyperbolic layers. Specifically, we employ the wrapped normal distributions in the Lorentz model (Nagano et al., 2019) and the Poincar\u00e9 ball (Mathieu et al., 2019), respectively.\nMain results The results in Table 3 show that our HCNN-VAE outperforms all baselines. Likewise, the hybrid models improve performance over the Euclidean model, indicating that learning the latent embeddings in hyperbolic spaces is beneficial. This is likely due to the higher representation capacity of the hyperbolic space, which is crucial in low dimensional settings. However, our HCNN is better at leveraging the advantages of hyperbolic geometry due to its fully hyperbolic architecture. These results suggest that our method is a promising approach for generation and for modeling latent structures in image data.\nAnalysis of latent embeddings The latent embedding space is a crucial component of VAEs as it influences how the data\u2019s features are encoded and used for generating the output. We visually analyze the distribution of latent embeddings inferred by the VAEs. For this, the models are retrained on the MNIST (Lecun et al., 1998) dataset with an embedding dimension dE = 2. Then, the images of the training dataset are passed through the encoder and visualized as shown in Figure 4.\nWe observe the formation of differently shaped clusters that correlate with the ground truth labels. While the embeddings of the Euclidean and hybrid models form many clusters that direct towards the origin, the HCNN-VAE obtains rather curved clusters that maintain a similar distance from the origin. The structures within the HCNN\u2019s latent space can be interpreted as hierarchies where the distance to the origin represents hierarchical levels. As these structures cannot be found for the hybrid model, our results suggest that hybrid HNNs using only a single hyperbolic layer have little impact on the model\u2019s Euclidean characteristics. Conversely, our fully hyperbolic architecture significantly impacts how features are represented and learned, directing the model toward tree-like structures."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this work, we proposed HCNN, a generalization of the convolutional neural network that learns latent feature representations in hyperbolic spaces. To this end, we formalized the necessary modules in the Lorentz model, deriving novel formulations of the convolutional layer, batch normalization, and multinomial logistic regression. We empirically demonstrated that ResNet and VAE models based on our hyperbolic framework achieve better performance on standard vision tasks than Euclidean and hybrid decoder baselines, especially in adversarial and lower dimensional settings. Additionally, we showed that using the Lorentz model in HNNs leads to better stability and performance than the Poincar\u00e9 ball.\nHowever, hyperbolic CNNs are still in their early stages and introduce mathematical complexity and computational overhead. For this, we explored HECNN models with the benefit of targeting only specific parts of the encoder, allowing for faster runtimes and larger models. Moreover, our framework currently relies on generalizations of neural network layers that were designed for Euclidean geometry and might not fully capture the unique properties of hyperbolic geometry. Further research is needed to fully understand the properties of HCNNs and address open questions such as optimization, scalability, and performance on other deep learning problems. We hope our work will inspire future research and development in this exciting and rapidly evolving field."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was performed on the HoreKa supercomputer funded by the Ministry of Science, Research and the Arts Baden-W\u00fcrttemberg and by the Federal Ministry of Education and Research. Ahmad Bdeir and Kristian Schwethelm were funded by the European Union\u2019s Horizon 2020 research and innovation programme under the SustInAfrica grant agreement No 861924. Kristian Schwethelm was also funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - project number 225197905."
        },
        {
            "heading": "A OPERATIONS IN HYPERBOLIC GEOMETRY",
            "text": ""
        },
        {
            "heading": "A.1 LORENTZ AND POINCARE",
            "text": "Poincar\u00e9 ball The n-dimensional Poincar\u00e9 ball BnK = (Bn, gKx ) is defined by Bn = {x \u2208 Rn | \u2212K||x||2 < 1} and the Riemannian metric gKx = (\u03bbKx )2In, where \u03bbKx = 2(1 +K||x||2)\u22121. It describes the hyperbolic space by an open ball of radius \u221a \u22121/K, see Figure 2."
        },
        {
            "heading": "A.2 LORENTZ MODEL",
            "text": "In this section, we describe essential geometrical operations in the Lorentz model. Most of these operations are defined for all Riemannian manifolds and thus introduced for the general case first. However, the closed-form formulae are only given for the Lorentz model. We also provide visual illustrations in Figure 5.\nDistance Distance is defined as the length of the shortest path between a pair of points on a surface. While in Euclidean geometry, this is a straight line, in hyperbolic space, the shortest path is represented by a curved geodesic generalizing the notion of a straight line. In the Lorentz model, the distance is inherited from Minkowski space. Let x,y \u2208 LnK denote two points in the Lorentz model. Then, the length of the connecting geodesic and, thereby, the distance is given by\ndL(x,y) = 1\u221a \u2212K cosh \u22121(K\u27e8x,y\u27e9L), (14)\nand the squared distance (Law et al., 2019) by\nd2L(x,y) = ||x\u2212 y||2L = 2\nK \u2212 2\u27e8x,y\u27e9L. (15)\nWhen calculating the distance of any point x \u2208 LnK to the origin 0, the equations can be simplified to\ndL(x,0) = || logK0 (x)||, (16)\nd2L(x,0) = 2\nK (1 +\n\u221a \u2212Kxt). (17)\nTangent space The space around each point x on a differentiable manifold M can be linearly approximated by the tangent space TxM. It is a first-order approximation bridging the gap to Euclidean space. This helps performing Euclidean operations, but it introduces an approximation error, which generally increases with the distance from the reference point. Let x \u2208 LnK , then the tangent space at point x can be expressed as\nTxLnK := {y \u2208 Rn+1 | \u27e8y,x\u27e9L = 0}. (18)\nExponential and logarithmic maps Exponential and logarithmic maps are mappings between the manifold M and the tangent space TxM with x \u2208 M. The exponential map expKx (z) : TxLnK \u2192 LnK maps a tangent vector z \u2208 TxLnK on the Lorentz manifold by\nexpKx (z) = cosh(\u03b1)x+ sinh(\u03b1) z\n\u03b1 , with \u03b1 =\n\u221a \u2212K||z||L, ||z||L = \u221a \u27e8z, z\u27e9L. (19)\nThe logarithmic map is the inverse mapping and maps a vector y \u2208 LnK to the tangent space of x by\nlogKx (y) = cosh\u22121(\u03b2)\u221a \u03b22 \u2212 1 \u00b7 (y \u2212 \u03b2x), with \u03b2 = K\u27e8x,y\u27e9L. (20)\nIn the special case of working with the tangent space at the origin 0, the exponential map simplifies to\nexpK 0 (z) = 1\u221a \u2212K\n[ cosh( \u221a \u2212K||z||), sinh(\n\u221a \u2212K||z||) z||z||\n] . (21)\nParallel transport The parallel transport operation PTKx\u2192y (v) maps a vector v \u2208 TxM from the tangent space of x \u2208 M to the tangent space of y \u2208 M. It preserves the local geometry around the reference point by moving the points along the geodesic connecting x and y. The formula for the Lorentz model is given by\nPTKx\u2192y (v) = v \u2212 \u27e8logKx (y),v\u27e9L dL(x,y) (logKx (y) + log K y (x)) (22)\n= v + \u27e8y,v\u27e9L\n1 \u2212K \u2212 \u27e8x,y\u27e9L\n(x+ y). (23)\nLorentzian centroid (Law et al., 2019) The weighted centroid with respect to the squared Lorentzian distance, which solves min\u00b5\u2208LnK \u2211m i=1 \u03bdid 2 L(xi,\u00b5), with xi \u2208 LnK and \u03bdi \u2265\n0, \u2211m\ni=1 \u03bdi > 0, is given by\n\u00b5 = \u2211m i=1 \u03bdixi\u221a\n\u2212K |||\u2211mi=1 \u03bdixi||L| . (24) Lorentz average pooling The average pooling layer is implemented by computing the Lorentzian centroid of all hyperbolic features within the receptive field.\nLorentz transformations The set of linear transformations in the Lorentz model are called Lorentz transformations. A transformation matrix A(n+1)\u00d7(n+1) that linearly maps Rn+1 \u2192 Rn+1 is called Lorentz transformation if and only if \u27e8Ax,Ay\u27e9L = \u27e8x,y\u27e9L \u2200 x,y \u2208 Rn+1. The set of matrices forms an orthogonal group O(1, n) called the Lorentz group. As the Lorentz model only uses the upper sheet of the two-sheeted hyperboloid, the transformations under consideration here lie within the positive Lorentz group O+(1, n) = {A \u2208 O(1, n) : a11 > 0}, preserving the sign of the time component xt of x \u2208 LnK . Specifically, here, the Lorentz transformations can be formulated as\nO+(1, n) = {A \u2208 R(n+1)\u00d7(n+1) | \u2200x \u2208 LnK : \u27e8Ax,Ax\u27e9L = 1\nK , (Ax)0 > 0)}. (25)\nAny Lorentz transformation can be decomposed into a Lorentz rotation and Lorentz boost by polar decomposition A = RB (Moretti, 2002). The former rotates points around the time axis, using matrices given by\nR = [ 1 0T\n0 R\u0303\n] , (26)\nwhere 0 is a zero vector, R\u0303 T\nR\u0303 = I, and det(R\u0303) = 1. This shows that the Lorentz rotations for the upper sheet lie in a special orthogonal subgroup SO+(1, n) preserving the orientation, while R\u0303 \u2208 SO(n). On the other side, the Lorentz boost moves points along the spatial axis given a velocity v \u2208 Rn, ||v|| < 1 without rotating them along the time axis. Formally, the boost matrices are given by\nB =\n[ \u03b3 \u2212\u03b3vT\n\u2212\u03b3v I + \u03b321+\u03b3vvT\n] , (27)\nwith \u03b3 = 1\u221a 1\u2212||v||2 . See Figure 6 for illustrations of the Lorentz rotation and Lorentz boost.\nLorentz fully-connected layer Recently, Chen et al. (2021) showed that the linear transformations performed in the tangent space (Ganea et al., 2018; Nickel & Kiela, 2018) can not apply all Lorentz transformations but only a special rotation and no boost. They proposed a direct method in pseudohyperbolic space1, which can apply all Lorentz transformations. Specifically, let x \u2208 LnK denote the input vector and W \u2208 Rm\u00d7n+1, v \u2208 Rn+1 the weight parameters, then the transformation matrix is given by\nfx(M) = fx ([ vT\nW\n]) = [ \u221a ||Wx||2\u22121/K vTx vT\nW\n] . (28)\nAdding other components of fully-connected layers, including normalization, the final definition of the proposed Lorentz fully-connected layer becomes\ny =\n[ \u221a ||\u03d5(Wx,v)||2 \u2212 1/K\n\u03d5(Wx,v)\n] , (29)\nwith operation function\n\u03d5(Wx,v) = \u03bb\u03c3(vTx+ b\u2032) W\u03c8(x) + b\n||W\u03c8(x) + b|| , (30)\nwhere \u03bb > 0 is a learnable scaling parameter and b \u2208 Rn, \u03c8, \u03c3 denote the bias, activation, and sigmoid function, respectively.\n1Chen et al. (2021) note that their general formula is not fully hyperbolic, but a relaxation in implementation, while the input and output are still guaranteed to lie in the Lorentz model.\nIn this work, we simplify the layer definition by removing the internal normalization, as we use batch normalization. This gives following formula for the Lorentz fully connected layer\ny = LFC(x) = [ \u221a\n||\u03c8(Wx+ b)||2 \u2212 1/K \u03c8(Wx+ b)\n] . (31)\nLorentz direct concatenation (Qu & Zou, 2022) Given a set of hyperbolic points {xi \u2208 LnK}Ni=1, the Lorentz direct concatenation is given by\ny = HCat({xi}Ni=1) =\n \u221a\u221a\u221a\u221a N\u2211\ni=1\nx2it + N \u2212 1 K ,xT1s , . . . ,x T Ns T , (32) where y \u2208 LnNK \u2282 RnN+1.\nWrapped normal distribution Nagano et al. (2019) proposed a wrapped normal distribution in the Lorentz model, which offers efficient sampling, great flexibility, and a closed-form density formulation. It can be constructed as follows:\n1. Sample a Euclidean vector v\u0303 from the Normal distribution N (0,\u03a3). 2. Assume the sampled vector lies in the tangent space of the Lorentz model\u2019s origin v =\n[0, v\u0303] \u2208 T0LnK . 3. Parallel transport v from the tangent space of the origin to the tangent space of a new mean\n\u00b5 \u2208 LnK , yielding a tangent vector u \u2208 T\u00b5LnK . 4. Map u to LnK by applying the exponential map, yielding the final sample z \u2208 LnK .\nThe distribution is parameterized by a Euclidean variance \u03a3 \u2208 Rn\u00d7n and a hyperbolic mean \u00b5 \u2208 LnK . This method has shown to work well in hybrid HNN settings. However, in our fully hyperbolic VAE, high Euclidean variances destabilize the model. This is because, usually, the VAE\u2019s prior is set to a standard normal distribution with unit variance v\u0303 \u223c N (0, I). However, for high dimensional spaces, this leads to large values after the exponential map. That is why we propose to scale the prior variance as follows.\nLet v \u2208 T0LnK denote a vector in the tangent space of the origin. Then the space component of the hyperbolic vector z \u2208 LnK resulting from the exponential map is given by\nzs = (exp K 0 (v))s = 1\u221a \u2212K sinh( \u221a \u2212K||v||) v||v|| . (33)\nThis shows that the norm of the space component depends on the sinh function, which grows approximately exponentially with the norm of the tangent vector (||zs|| = 1\u221a\u2212K sinh( \u221a \u2212K||v||)). The norm of the space component is important as it gets used to calculate the time component zt = \u221a ||zs||2 \u2212 1/K, and it indicates how large the values of the hyperbolic points are. Now, assume an n-dimensional vector 1n = (1, ..., 1) \u2208 T0LnK , resembling the diagonal of the covariance matrix. Applying the exponential map to such a vector leads to fast-growing values with respect to the dimensionality n because the norm of the tangent vector increases with n:\n||1n|| = \u221a\u221a\u221a\u221a n\u2211 i=1 12 = \u221a n. (34)\nTo work against this, we propose to clip the norm of the prior variance as follows\n\u03c32 =\n{ s\u221a n : if \u221a n > s\n\u03c32 : otherwise , (35)\nwhere s parameterizes the resulting norm. This achieves a clipped time component with respect to the dimensionality of 1n (see Figure 7). Furthermore, it offers nice interpretability using the Fr\u00e9chet variance. As the distribution has zero mean, the Fr\u00e9chet variance is given by the distance to the origin, which can be calculated by the norm of the tangent vector. This shows that this method controls the Fr\u00e9chet variance. In practice, we empirically found s = 2.5 to be a good value.\nAdditionally, to prevent the HCNN-VAE from predicting relatively high variances, the scaling in Eq. 35 is applied. In this case, the Fr\u00e9chet variance is not predefined, as usually \u03c32 \u0338= 1n. However, it introduces a scaling operation resembling the variance scaling of the prior."
        },
        {
            "heading": "A.3 MAPPING BETWEEN MODELS",
            "text": "Because of the isometry between models of hyperbolic geometry, points in the Lorentz model can be mapped to the Poincar\u00e9 ball by the following diffeomorphism\npLnK\u2192BnK (x) = xs\nxt + 1\u221a \u2212K\n. (36)"
        },
        {
            "heading": "B PROOFS",
            "text": ""
        },
        {
            "heading": "B.1 PROOFS FOR LORENTZ HYPERPLANE",
            "text": "This section contains the proof for the Euclidean reparameterization of the Lorentz hyperplane proposed by Mishne et al. (2022). Unfortunately, the authors only provided proof for the unit Lorentz model, i.e., assuming a curvature of K = \u22121. However, in general, the curvature can be different as K < 0. That is why we reproduce their proof for the general case.\nProof for Eq. 8 Let a \u2208 R, z \u2208 Rn, and z \u2208 T0LnK = [0, az/||z||]. Then, Mishne et al. (2022) parameterize a point in the Lorentz model as follows\np \u2208 LnK := exp0 ( a z\n||z||\n) (37)\n= [ 1\u221a \u2212K cosh(\u03b1), sinh(\u03b1) a z||z|| \u03b1 ] . (38)\nNow, with \u03b1 = \u221a \u2212K||a z||z|| ||L = \u221a \u2212Ka we get\np = [ cosh(\n\u221a \u2212Ka) 1\u221a\u2212K , sinh( \u221a \u2212Ka) a z||z||\u221a \u2212Ka\n] (39)\n= 1\u221a \u2212K\n[ cosh( \u221a \u2212Ka), sinh(\n\u221a \u2212Ka) z||z||\n] . (40)\nThis definition gets used to reparameterize the hyperplane parameter w as follows\nw := PTK0\u2192p (z)\n= z + \u27e8p, z\u27e9L\n1 \u2212K \u2212 \u27e80,p\u27e9L\n(0+ p)\n= [0, z]T + \u27e8p, [0, z]T \u27e9L 1\n\u2212K \u2212 \u27e80,p\u27e9L (0+ p)\n= [0, z]T +\n1\u221a \u2212K sinh(\n\u221a \u2212Ka)||z||\n1 \u2212K + 1 \u2212K cosh(\n\u221a \u2212Ka) \u00b7 1\u221a \u2212K\n[ 1 + cosh( \u221a \u2212Ka), sinh(\n\u221a \u2212Ka) z||z|| ] = [0, z]T + sinh( \u221a \u2212Ka)||z||\n1 + cosh( \u221a \u2212Ka) \u00b7\n[ 1 + cosh( \u221a \u2212Ka), sinh(\n\u221a \u2212Ka) z||z|| ] = [ sinh( \u221a \u2212Ka)||z||, z + sinh 2( \u221a \u2212Ka)\n1 + cosh( \u221a \u2212Ka)z ] = [ sinh( \u221a \u2212Ka)||z||, z + cosh 2( \u221a \u2212Ka)\u2212 1\n1 + cosh( \u221a \u2212Ka) z ] = [sinh( \u221a \u2212Ka)||z||, cosh( \u221a \u2212Ka)z].\nProof for Eq. 9 After inserting Eq. 8 into Eq. 7 and solving the inner product, the hyperplane definition becomes\nH\u0303z,a = {x \u2208 LnK | cosh( \u221a \u2212Ka)\u27e8z,xs\u27e9 \u2212 sinh( \u221a \u2212Ka) ||z|| xt = 0}. (41)"
        },
        {
            "heading": "B.2 PROOF FOR DISTANCE TO LORENTZ HYPERPLANE",
            "text": "Proof for Theorem 1 To proof the distance of a point to hyperplanes in the Lorentz model, we follow the approach of Cho et al. (2019) and utilize the hyperbolic reflection. The idea is, that a hyperplane defines a reflection that interchanges two half-spaces. Therefore, the distance from a point x \u2208 LnK to the hyperplane Hw,p can be calculated by halving the distance to its reflection in the hyperplane x \u2192 yw\ndL(x, Hw,p) = 1\n2 dL(x,yw). (42)\nThe hyperbolic reflection is well-known in the literature (Grosek, 2008) and can be formulated as\nyw = x+ 2\u27e8w,x\u27e9Lw \u27e8w,w\u27e9L , (43)\nwhere w is the perpendicular vector to the hyperplane and \u27e8w,w\u27e9L > 0. Now, inserting Eq. 43 into Eq. 42 we can compute the distance to the hyperplane as follows\ndL(x, Hw,p) = 1\n2 \u221a \u2212K cosh \u22121(K\u27e8x,yw\u27e9L)\n= 1\n2 \u221a \u2212K cosh \u22121(K\u27e8x,x+ 2\u27e8w,x\u27e9Lw\u27e8w,w\u27e9L \u27e9L)\n= 1\n2 \u221a \u2212K cosh \u22121(2K\u27e8x,x\u27e9L +K\u27e8x, \u27e8w,x\u27e9Lw \u27e8w,w\u27e9L \u27e9L)\n= 1\n2 \u221a \u2212K cosh\n\u22121 K 1 K + 2K ( \u27e8w,x\u27e9L\u221a \u27e8w,w\u27e9L )2\n= 1\n2 \u221a \u2212K cosh\n\u22121 1 + 2(\u221a\u2212K \u27e8w,x\u27e9L\u221a \u27e8w,w\u27e9L )2 =\n1\u221a \u2212K \u2223\u2223\u2223\u2223sinh\u22121(\u221a\u2212K \u27e8w,x\u27e9L||w||L )\u2223\u2223\u2223\u2223 ,\nwhich gives the final formula:\ndL(x, Hw,p) = 1\u221a \u2212K \u2223\u2223\u2223\u2223sinh\u22121(\u221a\u2212K \u27e8w,x\u27e9L||w||L )\u2223\u2223\u2223\u2223 . (44)\nComparing Eq. 44 to the equation of Cho et al. (2019) shows that the distance formula for hyperplanes in the unit Lorentz model can be extended easily to the general case by inserting the curvature parameter K at two places.\nFinally, defining w with the aforementioned reparameterization\nw := PTK0\u2192p (z) = [sinh( \u221a \u2212Ka)||z||, cosh( \u221a \u2212Ka)z], (45)\nand solving the inner products, gives our final distance formula\ndL(x, H\u0303z,a) = 1\u221a \u2212K \u2223\u2223\u2223\u2223\u2223\u2223sinh\u22121 \u221a\u2212K cosh(\u221a\u2212Ka)\u27e8z,xs\u27e9 \u2212 sinh(\u221a\u2212Ka) ||z|| xt\u221a || cosh( \u221a \u2212Ka)z||2 \u2212 (sinh( \u221a \u2212Ka)||z||)2 \u2223\u2223\u2223\u2223\u2223\u2223 . (46)"
        },
        {
            "heading": "B.3 PROOF FOR LOGITS IN THE LORENTZ MLR CLASSIFIER",
            "text": "Proof for Theorem 2 Following Lebanon & Lafferty (2004), given input x \u2208 Rn and C classes, the Euclidean MLR logits of class c \u2208 {1, ..., C} can be expressed as\nvwc(x) = sign(\u27e8wc,x\u27e9)||wc||d(x,Hwc), wc \u2208 Rn, (47)\nwhere Hwc is the decision hyperplane of class c.\nReplacing the Euclidean operations with their counterparts in the Lorentz model yields logits of class c for x \u2208 LnK as follows\nvwc,pc(x) = sign(\u27e8wc,x\u27e9L)||wc||LdL(x,Hwc,pc), (48)\nwith wc \u2208 TpcLnK ,pc \u2208 LnK , and \u27e8wc,wc\u27e9L > 0. Inserting Eq. 44 into Eq. 48 gives a general formula without our reparameterization\nvwc,pc(x) = 1\u221a \u2212K sign(\u27e8wc,x\u27e9L)||wc||L \u2223\u2223\u2223\u2223sinh\u22121(\u221a\u2212K \u27e8wc,x\u27e9L||wc||L )\u2223\u2223\u2223\u2223 . (49)\nNow, we reparameterize w with Eq. 45 again, which gives\n\u03b1 := \u27e8wc,x\u27e9L = cosh( \u221a \u2212Ka)\u27e8z,xs\u27e9 \u2212 sinh( \u221a \u2212Ka), (50)\n\u03b2 := ||wc||L = \u221a || cosh( \u221a \u2212Ka)z||2 \u2212 (sinh( \u221a \u2212Ka)||z||)2, (51)\nwith a \u2208 R and z \u2208 Rn. Finally, we obtain the equation in Theorem 2:\nvzc,ac(x) = 1\u221a \u2212K sign(\u03b1)\u03b2 \u2223\u2223\u2223\u2223sinh\u22121(\u221a\u2212K\u03b1\u03b2 )\u2223\u2223\u2223\u2223 . (52)"
        },
        {
            "heading": "C ADDITIONAL EXPERIMENTAL DETAILS",
            "text": ""
        },
        {
            "heading": "C.1 CLASSIFICATION",
            "text": "Datasets For classification, we employ the benchmark datasets CIFAR-10 (Krizhevsky, 2009), CIFAR-100 (Krizhevsky, 2009), and Tiny-ImageNet (Le & Yang, 2015). The CIFAR-10 and CIFAR-100 datasets each contain 60,000 32\u00d7 32 colored images from 10 and 100 different classes, respectively. We use the dataset split implemented in PyTorch, which includes 50,000 training images and 10,000 testing images. Tiny-ImageNet is a small subset of the ImageNet (Deng et al., 2009) dataset, with 100,000 images of 200 classes downsized to 64\u00d764. Here, we use the official validation split for testing our models.\nSettings Table 4 summarizes the hyperparameters we adopt from DeVries & Taylor (2017) to train all classification models. Additionally, we use standard data augmentation methods in training, i.e., random mirroring and cropping. Regarding the feature clipping in hybrid HNNs, we tune the feature clipping parameter r between 0.8 and 5.0 and find that, for most experiments, the best feature clipping parameter is r = 1. Only the Lorentz hybrid ResNet performs best with r = 4 on Tiny-ImageNet, and r = 2 on CIFAR-100 with lower embedding dimensions. Overall, we observe that the hybrid Lorentz ResNet has fewer gradient issues, allowing for higher clipping values. The HCNN-ResNet does not need tuning of any additional hyperparameters."
        },
        {
            "heading": "C.2 GENERATION",
            "text": "Datasets For image generation, we use the aforementioned CIFAR-10 (Krizhevsky, 2009) and CIFAR-100 (Krizhevsky, 2009) datasets again. Additionally, we employ the CelebA (Liu et al., 2015) dataset, which includes colored 64\u00d7 64 images of human faces. Here, we use the PyTorch implementation, containing 162,770 training images, 19,867 validation images, and 19,962 testing images.\nSettings For hyperbolic and Euclidean models, we use the same architecture (see Table 5) and training hyperparameters (see Table 6). We employ a vanilla VAE similar to Ghosh et al. (2019) as the baseline Euclidean architecture (E-VAE). For the hybrid model, we replace the latent distribution of the E-VAE with the hyperbolic wrapped normal distribution in the Lorentz model (Nagano et al., 2019) and the Poincar\u00e9 ball (Mathieu et al., 2019), respectively. Replacing all layers with our proposed hyperbolic counterparts yields the fully hyperbolic model. Here, we include the variance scaling mentioned in Section A.2, as otherwise training fails with NaN errors. Furthermore, we set the curvature K for the Lorentz model to \u22121 and for the Poincar\u00e9 ball to \u22120.1. We evaluate the VAEs by employing two versions of the FID (Heusel et al., 2017) implemented by Seitzer (2020):\n1. The reconstruction FID gives a lower bound on the generation quality. It is calculated by comparing test images with reconstructed validation images. As the CIFAR datasets have no official validation set, we exclude a fixed random portion of 10,000 images from the training set.\n2. The generation FID measures the generation quality by comparing random generations from the models\u2019 latent space with the test set."
        },
        {
            "heading": "D ABLATION EXPERIMENTS",
            "text": "Table 7: \u03b4rel for intermediate embeddings of ResNet-18 trained on CIFAR-100.\nEncoder Section \u03b4rel Initial Conv. 0.26 Block 1 0.19 Block 2 0.23 Block 3 0.18 Block 4 0.21\nMotivating the use of HNNs by measuring the \u03b4-hyperbolicity of visual datasets was proposed by Khrulkov et al. (2020). The idea is to first generate image embeddings from a vision model and then quantify the degree of inherent tree-structure. This is achieved by considering \u03b4-slim triangles and determining the minimal value that satisfies the triangle inequality using the Gromov product. The lower \u03b4 \u2265 0 is, the higher the hyperbolicity of the dataset. Usually, the scale-invariant value \u03b4rel is reported. For further insights refer to Khrulkov et al. (2020).\nWhile Khrulkov et al. (2020) studies the hyperbolicity of the final embeddings, we extend the perspective to intermediate embeddings from the encoder, aiming to provide additional motivation for fully hyperbolic models. For this, we first train a Euclidean ResNet-18 classifier using the settings detailed in Appendix C.1. Then, we run the model on the test dataset and extract the embeddings after each ResNet block to assess their \u03b4-hyperbolicity. The results in Table 7 show that all intermediate embeddings exhibit a high degree of hyperbolicity. Furthermore, we observe a difference in hyperbolicity between blocks. This motivates hybrid hyperbolic encoders (HECNNs) with hyperbolic layer only used where hyperbolicity is the highest."
        },
        {
            "heading": "D.2 HCNN COMPONENTS",
            "text": "In this section, we perform ablation studies to obtain additional insights into our proposed HCNN components. All ablation experiments consider image classification on CIFAR-100 using ResNet-18. We estimate the mean and standard deviation from five runs, and the best performance is highlighted in bold.\nRuntime Currently, two major drawbacks of HNNs are relatively high runtime and memory requirements. This is partly due to custom Pythonic implementations of hyperbolic network components introducing significant computational overhead. To study the overhead in practice and assess the efficiency of our implementations, we use PyTorch\u2019s compile function, which automatically builds a more efficient computation graph. We compare the runtime of our Lorentz ResNets with the Euclidean baseline\nunder compiled and default settings in Table 8 using an RTX 4090 GPU.\nThe results show that hybrid HNNs only add little overhead compared to the significantly slower HCNN. This makes scaling HCNNs challenging and requires special attention in future works. However, we also see that hyperbolic models gain much more performance from the automatic compilation than the Euclidean model. This indicates greater room for improvement in terms of implementation optimizations.\nTable 9: Normalization ablation.\nNormalization Accuracy (%)\nNone 42.24\u00b13.86\nSpace bnorm 77.48\u00b10.25 Tangent bnorm NaN Riemannian bnorm (Lou et al., 2020) NaN LFC norm (Chen et al., 2021) 1.00\u00b10.00\nLBN + LFC norm (Chen et al., 2021) 76.98\u00b10.18 LBN 78.07\u00b10.21\nBatch normalization We investigate the effectiveness of our proposed Lorentz batch normalization (LBN) compared to other possible methods. Specifically, we train HCNN-ResNets with different normalization methods and compare them against a model without normalization. The results are shown in Table 9.\nFirstly, the batch normalization in tangent space and the Riemannian batch normalization (Lou et al., 2020) lead to infinite loss within the first few training iterations. This could be caused by the float32 precision used in this work. Additionally, the normalization within the Lorentz fully-connected layer (LFC) proposed by Chen et al. (2021) (see Section A.2) inhibits learning when not combined with our LBN.\nConversely, using LBN alone improves convergence speed and accuracy significantly, getting approximately 36% higher accuracy than the non-normalized model after 200 epochs. Combining LBN and the LFC normalization leads to worse accuracy and runtime, validating our modified LFC (see Section A.2). Overall, this experiment shows the effectiveness of our LBN and suggests that, currently, there is no viable alternative for HNNs using the Lorentz model. However, a naive implementation operating on the hyperbolic space component can serve as a good initial baseline, although ignoring the properties of hyperbolic geometry.\nNon-linear activation In this work, we propose a method for applying standard activation functions to points in the Lorentz model that is more stable and efficient than the usual tangent space activations. Here, we quantify the improvement by comparing HCNN-ResNets using different ReLU applications. Furthermore, we consider a model without activation functions, as the LFC is non-linear already and might not need such functions.\nThe results in Table 10 show that non-linear activations improve accuracy significantly and are therefore needed in HCNNs. Furthermore, compared to Tangent ReLU, our Lorentz ReLU increases the average accuracy by 0.64%, decreases the runtime of a training epoch by about 16.7%, and is more consistent overall.\nResidual connection In this experiment, we compare the effect of different approaches for residual connections on performance. As mentioned in Section 4.4, vector addition is ill-defined in the Lorentz model. As alternatives, we test tangent space addition (Nickel & Kiela, 2018), parallel transport (PT) addition (Chami et al., 2019), M\u00f6bius addition (after projecting to the Poincar\u00e9 ball) (Ganea et al., 2018), fully-connected (FC) layer addition (Chen et al., 2021), and our proposed space component addition. The results are shown in Table 11.\nIn training, we observe that PT and M\u00f6bius addition make the model very unstable, causing an early failure in the training process. This might be because of the float32 precision again. The tangent space addition performs relatively well, but the needed exponential and logarithmic maps add computational overhead (\u2248 12% higher runtime per training epoch) and some instability that hampers learning. The FC addition and our proposed space addition are very similar and perform best. However, our method is simpler and therefore preferred.\nInitialization Chen et al. (2021) proposed initializing Lorentz fully-connected layers with the uniform distribution U(\u22120.02, 0.02). As the LFC is the backbone of our Lorentz convolutional layer, we test the uniform initialization in HCNNs and compare it to the standard Kaiming initialization (He et al., 2015a) used in most Euclidean CNNs. For this, we employ the same ResNet architecture and initialize Lorentz convolutional layers with these two methods.\nThe results in Table 12 show that the Kaiming initialization is preferred in HCNNs, leading to 0.55% higher accuracy."
        }
    ],
    "title": "FULLY HYPERBOLIC CONVOLUTIONAL NEURAL NETWORKS FOR COMPUTER VISION",
    "year": 2024
}