{
    "abstractText": "The genome sequence contains the blueprint for governing cellular processes. While the availability of genomes has vastly increased over the last decades, experimental annotation of the various functional, non-coding and regulatory elements encoded in the DNA sequence remains both expensive and challenging. This has sparked interest in unsupervised language modeling of genomic DNA, a paradigm that has seen great success for protein sequence data. Although various DNA language models have been proposed, evaluation tasks often differ between individual works, and might not fully recapitulate the fundamental challenges of genome annotation, including the length, scale and sparsity of the data. In this study, we introduce BEND, a Benchmark for DNA language models, featuring a collection of realistic and biologically meaningful downstream tasks defined on the human genome. We find that embeddings from current DNA LMs can approach performance of expert methods on some tasks, but only capture limited information about long-range features. BEND is available at https://github.com/frederikkemarin/BEND.",
    "authors": [
        {
            "affiliations": [],
            "name": "Frederikke I. Marin"
        },
        {
            "affiliations": [],
            "name": "Felix Teufel"
        },
        {
            "affiliations": [],
            "name": "Marc Horlacher"
        },
        {
            "affiliations": [],
            "name": "Dennis Madsen"
        },
        {
            "affiliations": [],
            "name": "Dennis Pultz"
        },
        {
            "affiliations": [],
            "name": "Ole Winther"
        },
        {
            "affiliations": [],
            "name": "Wouter Boomsma"
        }
    ],
    "id": "SP:842d4d0d1a2b8b51ba41a1e2c8944f3d0f6f853f",
    "references": [
        {
            "authors": [
                "Babak Alipanahi",
                "Andrew Delong",
                "Matthew T Weirauch",
                "Brendan J Frey"
            ],
            "title": "Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning",
            "venue": "Nature Biotechnology,",
            "year": 2015
        },
        {
            "authors": [
                "Weizhi An",
                "Yuzhi Guo",
                "Yatao Bian",
                "Hehuan Ma",
                "Jinyu Yang",
                "Chunyuan Li",
                "Junzhou Huang"
            ],
            "title": "MoDNA: motif-oriented pre-training for DNA language model",
            "venue": "In Proceedings of the 13th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics,",
            "year": 2022
        },
        {
            "authors": [
                "Christof Angermueller",
                "Heather J. Lee",
                "Wolf Reik",
                "Oliver Stegle"
            ],
            "title": "DeepCpG: accurate prediction of single-cell DNA methylation states using deep learning",
            "venue": "Genome Biology,",
            "year": 2017
        },
        {
            "authors": [
                "Sanjit Singh Batra",
                "Yun S. Song"
            ],
            "title": "DNA language models are powerful",
            "year": 2011
        },
        {
            "authors": [
                "Bonnie Berger"
            ],
            "title": "Learning the protein language: Evolution",
            "year": 2022
        },
        {
            "authors": [
                "Aaron K. Wong",
                "Olga G. Troyanskaya",
                "Jian Zhou"
            ],
            "title": "A sequence-based global",
            "year": 2039
        },
        {
            "authors": [
                "July"
            ],
            "title": "ISSN 1061-4036, 1546-1718",
            "venue": "doi: 10.1038/s41588-022-01102-2. URL https:",
            "year": 2022
        },
        {
            "authors": [
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of Deep",
            "year": 2023
        },
        {
            "authors": [
                "October"
            ],
            "title": "ISSN 0162-8828, 2160-9292, 1939-3539",
            "venue": "doi: 10.1109/TPAMI.2021.3095381.",
            "year": 2022
        },
        {
            "authors": [
                "Julien Gagneur"
            ],
            "title": "Species-aware DNA language modeling",
            "year": 2023
        },
        {
            "authors": [
                "Hal Daum\u00e9",
                "Kate Crawford"
            ],
            "title": "Datasheets for Datasets. 2018",
            "venue": "doi: 10.48550/ARXIV.1803",
            "year": 2018
        },
        {
            "authors": [
                "J. Michael Cherry",
                "Benjamin C. Hitz"
            ],
            "title": "Data navigation on the ENCODE portal. 2023",
            "year": 2023
        },
        {
            "authors": [
                "July"
            ],
            "title": "ISSN 1549-5469",
            "venue": "doi: 10.1101/gr.200535.115. David R. Kelley, Yakir A. Reshef, Maxwell Bileschi, David Belanger, Cory Y. McLean, and Jasper",
            "year": 2016
        },
        {
            "authors": [
                "Stephen Merity",
                "Nitish Shirish Keskar",
                "Richard Socher"
            ],
            "title": "Regularizing and Optimizing LSTM Language Models. 2017",
            "venue": "doi: 10.48550/ARXIV.1708.02182. URL https://arxiv.org/ abs/1708.02182. Publisher: arXiv Version Number:",
            "year": 2017
        },
        {
            "authors": [
                "Eric Nguyen",
                "Michael Poli",
                "Marjan Faizi",
                "Armin Thomas",
                "Callum Birch-Sykes",
                "Michael Wornow",
                "Aman Patel",
                "Clayton Rabideau",
                "Stefano Massaroli",
                "Yoshua Bengio",
                "Stefano Ermon",
                "Stephen A. Baccus",
                "Chris R\u00e9"
            ],
            "title": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
            "venue": "doi: 10.48550/ARXIV.2306.15794. URL https://arxiv.org/ abs/2306.15794. Publisher: arXiv Version Number:",
            "year": 2023
        },
        {
            "authors": [
                "Michael Poli",
                "Stefano Massaroli",
                "Eric Nguyen",
                "Daniel Y. Fu",
                "Tri Dao",
                "Stephen Baccus",
                "Yoshua Bengio",
                "Stefano Ermon",
                "Christopher R\u00e9"
            ],
            "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models, April 2023",
            "venue": "URL http://arxiv.org/abs/2302.10866",
            "year": 2023
        },
        {
            "authors": [
                "Ofir Press",
                "Noah A. Smith",
                "Mike Lewis"
            ],
            "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation, April 2022",
            "venue": "URL http://arxiv.org/abs/2108",
            "year": 2022
        },
        {
            "authors": [
                "Roshan Rao",
                "Nicholas Bhattacharya",
                "Neil Thomas",
                "Yan Duan",
                "Xi Chen",
                "John Canny",
                "Pieter Abbeel",
                "Yun S. Song"
            ],
            "title": "Evaluating Protein Transfer Learning with TAPE",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Roshan Rao",
                "Joshua Meier",
                "Tom Sercu",
                "Sergey Ovchinnikov",
                "Alexander Rives"
            ],
            "title": "Transformer protein language models are unsupervised structure learners",
            "venue": "URL http://biorxiv.org/lookup/doi/10.1101/2020.12.15.422761",
            "year": 2020
        },
        {
            "authors": [
                "Alexander Rives",
                "Joshua Meier",
                "Tom Sercu",
                "Siddharth Goyal",
                "Zeming Lin",
                "Jason Liu",
                "Demi Guo",
                "Myle Ott",
                "C. Lawrence Zitnick",
                "Jerry Ma",
                "Rob Fergus"
            ],
            "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein",
            "venue": "URL http://biorxiv.org/lookup/doi/10.1101/622803",
            "year": 2019
        },
        {
            "authors": [
                "Marco Salvatore",
                "Marc Horlacher",
                "Annalisa Marsico",
                "Ole Winther",
                "Robin Andersson"
            ],
            "title": "Transfer learning identifies sequence determinants of cell-type specific regulatory element accessibility",
            "venue": "NAR Genomics and Bioinformatics,",
            "year": 2023
        },
        {
            "authors": [
                "Melissa Sanabria",
                "Jonas Hirsch",
                "Anna R. Poetsch"
            ],
            "title": "The human genome\u2019s vocabulary as proposed by the DNA language model GROVER, September 2023",
            "venue": "URL https://www.biorxiv. org/content/10.1101/2023.07.19.549677v2",
            "year": 2023
        },
        {
            "authors": [
                "Nicolas Scalzitti",
                "Anne Jeannin-Girardon",
                "Pierre Collet",
                "Olivier Poch",
                "Julie D. Thompson"
            ],
            "title": "A benchmark study of ab initio gene prediction methods in diverse eukaryotic organisms",
            "venue": "BMC Genomics,",
            "year": 2020
        },
        {
            "authors": [
                "Koren",
                "Matthew Boitano",
                "Paul Peluso",
                "Heng Li",
                "Chen-Shan Chin",
                "Adam M. Phillippy",
                "Richard Durbin",
                "Richard K. Wilson",
                "Paul Flicek",
                "Evan E. Eichler",
                "Deanna M. Church"
            ],
            "title": "Evaluation of GRCh38 and de novo haploid genome assemblies demonstrates the enduring quality of the reference assembly",
            "venue": "Genome Research,",
            "year": 2017
        },
        {
            "authors": [
                "Ritambhara Singh",
                "Jack Lanchantin",
                "Gabriel Robins",
                "Yanjun Qi"
            ],
            "title": "DeepChrome: deeplearning for predicting gene expression from histone modifications",
            "venue": "i639\u2013i648, September 2016",
            "year": 2016
        },
        {
            "authors": [
                "Mario Stanke",
                "Stephan Waack"
            ],
            "title": "Gene prediction with a hidden Markov model and a new intron submodel",
            "venue": "Bioinformatics (Oxford, England),",
            "year": 2003
        },
        {
            "authors": [
                "Felix Teufel",
                "Magn\u00fas Halld\u00f3r G\u0131\u0301slason",
                "Jos\u00e9 Juan Almagro Armenteros",
                "Alexander Rosenberg Johansen",
                "Ole Winther",
                "Henrik Nielsen"
            ],
            "title": "GraphPart: homology partitioning for biological sequence analysis",
            "venue": "NAR Genomics and Bioinformatics,",
            "year": 2023
        },
        {
            "authors": [
                "Jesse Vig",
                "Ali Madani",
                "Lav R. Varshney",
                "Caiming Xiong",
                "Richard Socher",
                "Nazneen Fatema Rajani"
            ],
            "title": "BERTology Meets Biology: Interpreting Attention in Protein Language Models, March 2021",
            "year": 2006
        },
        {
            "authors": [
                "Minghao Xu",
                "Zuobai Zhang",
                "Jiarui Lu",
                "Zhaocheng Zhu",
                "Yangtian Zhang",
                "Ma Chang",
                "Runcheng Liu",
                "Jian Tang"
            ],
            "title": "PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Meng Yang",
                "Haiping Huang",
                "Lichao Huang",
                "Nan Zhang",
                "Jihong Wu",
                "Huanming Yang",
                "Feng Mu"
            ],
            "title": "LOGO, a contextualized pre-trained language model of human genome flexibly adapts to various downstream tasks by fine-tuning",
            "venue": "URL https: //www.researchsquare.com/article/rs-448927/v1",
            "year": 2021
        },
        {
            "authors": [
                "Manzil Zaheer",
                "Guru Guruganesh",
                "Kumar Avinava Dubey",
                "Joshua Ainslie",
                "Chris Alberti",
                "Santiago Ontanon",
                "Philip Pham",
                "Anirudh Ravula",
                "Qifan Wang",
                "Li Yang",
                "Amr Ahmed"
            ],
            "title": "Big Bird: Transformers for Longer Sequences",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jian Zhou",
                "Olga G Troyanskaya"
            ],
            "title": "Predicting effects of noncoding variants with deep learning\u2013based sequence model",
            "venue": "Nature Methods,",
            "year": 2015
        },
        {
            "authors": [
                "Zhihan Zhou",
                "Yanrong Ji",
                "Weijian Li",
                "Pratik Dutta",
                "Ramana Davuluri",
                "Han Liu"
            ],
            "title": "DNABERT2: Efficient Foundation Model and Benchmark For Multi-Species Genome, June 2023",
            "venue": "URL http://arxiv.org/abs/2306.15006",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Within the last two decades, the cost of sequencing whole genomes has significantly decreased, having led to an extraordinary wealth of genomic DNA sequences. This has improved our understanding of genetic variation among human genomes and introduced genomes of hitherto understudied species. However, the generation of experimental data to annotate and understand these genomic sequences has not kept pace.\nAt the same time, Natural Language Processing (NLP) has demonstrated the power of large-scale models to capture signals in sequences by masking and reconstructing them in a self-supervised manner. The success of masked language modeling (MLM) has extended to the biological domain Rao et al. (2019); Bepler & Berger (2021); Madani et al. (2023); Rives et al. (2019), with protein language models (pLMs) now being widely used for prediction tasks on protein sequences. The availability of unlabeled genomic sequences and, in many organisms, limited labeled data appear to make language modeling a natural fit for DNA. DNA language models (LMs) have indeed started to emerge, but while the paradigms of NLP have been easy to transfer to proteins, the same may not be true for modeling genomes, as they present unique challenges: signals can have an extremely long length range, high-signal regions are sparse, and even in those regions the density of signal is lower compared to proteins.\nIn this paper, we present BEND, a Benchmark for DNA Language Models, a collection of realistic and biologically meaningful downstream tasks. BEND aims to provide a standardized set of tasks that measure the ability of LMs to capture the intricacies of genomic data, and to help advance this nascent field. In summary, BEND contributes:\n\u21e4Equal contribution\n\u2022 Seven curated tasks and datasets, probing understanding of different DNA functional elements over a variety of length scales.\n\u2022 Experiments covering DNA LMs from six different sources. To our knowledge, this represents first evaluation of all publicly available self-supervised DNA LMs suitable for the human genome together with appropriate baseline methods.\n\u2022 An adaptable benchmarking framework for preparing embeddings and training lightweight supervised models.\n\u2022 Result: DNA LMs approach expert method performance on some tasks. However, no LM consistently outperforms all others, and reasoning over very long contexts, as e.g. required for finding enhancers, is still challenging.\n\u2022 Result: DNA LMs can learn distinct features in masked language modeling. Some LMs\u2019 embeddings primarily capture information about gene structure, while others focus on noncoding regions."
        },
        {
            "heading": "2 BACKGROUND",
            "text": ""
        },
        {
            "heading": "2.1 EUKARYOTIC DNA ORGANIZATION AND TERMINOLOGY",
            "text": "In order to facilitate understanding how different prediction tasks relate to various aspects of the genome, we briefly discuss the fundamental structure and function of eukaryotic genomic DNA (Figure 1). DNA is a linear polymer of four nucleotide bases, which are represented by the four letters A, C, G and T. It consists of two complementary strands that form a double helix by base pairing the bases A, T, and C, G respectively.\nGenomic DNA is physically organized in a hierarchical manner. The DNA polymer is coiled around histone proteins, which reduces its physical length and plays a role in regulation. A complex of 8 histone proteins together with coiled DNA is called a nucleosome. Nucleosomes further condense to form chromatin fibers, which occur in compact (closed) or loose (open) form. This controls the accessibility of the DNA sequence to the transcriptional machinery, a process tightly regulated by chemical modifications of the histones (Bannister & Kouzarides, 2011). Chromatin can form loops, which allows regions distant in the sequence to be close in physical space. DNA appears in independent modules called chromosomes, which are typically millions of base pairs (bp) in length.\nThe genome contains genes, segments that are transcribed to RNA molecules and potentially translated to proteins. Protein-coding genes are structured as introns and exons. For expression, a gene is first transcribed to a pre-mRNA molecule, and introns are removed via splicing. This combines the exons to one contiguous sequence that encodes the protein. Flanking nucleotides in the RNA that do not code for the protein are called untranslated regions (UTRs) and can have regulatory function. In addition, genes are associated with regulatory regions such as promoters, enhancers, silencers and insulators that modulate their expression. Some elements, such as promoters, may lie in close proximity to the start of the gene, the transcription start site (TSS). Others can appear multiple thousands bp away from the gene, but mediate their effect by physical proximity."
        },
        {
            "heading": "2.2 LANGUAGE MODELING FOR BIOMOLECULAR SEQUENCES: FROM PROTEINS TO DNA",
            "text": "Over the last years, language modeling has achieved breakthroughs in representation learning for protein property and structure prediction, with transformer-based pLMs emerging as powerful foundation models, capable of learning long-range interactions fully unsupervised (Rives et al., 2019; Elnaggar et al., 2022; Lin et al., 2023). The development of pLMs benefitted from the availability of standardized, representative benchmarks, such as TAPE (Rao et al., 2019) and PEER (Xu et al., 2022), as well as long-running protein machine learning tasks with an emphasis on fair benchmarking to measure progress (Kryshtafovych et al., 2021; Zhou et al., 2019).\nWhile LMs have been extremely successful for modeling proteins, key differences between the two types of macromolecules hinder their widespread adoption for DNA. A typical protein consists of 400-500 amino acids, which are represented as tokens from an alphabet of size 20. The analogy of amino acid tokens with word tokens in NLP, as well as the fact that size of inputs to pLMs and NLP models are on the same order of magnitude, made methods developed for NLP directly transferable to protein data, with little to no methodological adaption required (Rao et al., 2020; Elnaggar et al., 2022). The alphabet of DNA is significantly smaller (4 tokens), while at the same time sequences,\nsuch as those of genes, are considerably longer and have no naturally defined border, as e.g. the position of the most distant relevant regulatory element is typically unknown. In contrast, protein sequences are naturally self-contained and, being the final gene product, have a significantly higher information density. Together, sparsity and long sequences pose unique challenges to DNA LMs."
        },
        {
            "heading": "2.3 RELATED WORKS",
            "text": ""
        },
        {
            "heading": "2.3.1 DNA LANGUAGE MODELS",
            "text": "The first available DNA LM was DNABERT (Ji et al., 2021), a 12-layer BERT (Devlin et al., 2018) model trained on sequences of length 512 from the human genome. Sequences were tokenized as k-mers using a sliding window. DNABERT was evaluated by fine-tuning on tasks comprising promoter, transcription factor (TF) binding site and splice site (SS) prediction.\nA growing number of DNA LMs has been proposed since the release of DNABERT. These include the Genomic Pretrained Network (GPN) (Benegas et al., 2023), FloraBERT (Levy et al., 2022), the Nucleotide Transformer (NT) (Dalla-Torre et al., 2023), Species-aware LM (Gankin et al., 2023), GENA-LM (Fishman et al., 2023), DNABERT-2 (Zhou et al., 2023) and HyenaDNA (Poli et al., 2023). With the exception of HyenaDNA, models were trained using the MLM objective, but differ in their model architectures, tokenization strategies and training data.\nGPN uses dilated convolution layers rather than a transformer model. It showed strong performance for zero-shot prediction of variant effects in the A. thaliana genome it was trained on. Qualitative results showed that GPN captures information about gene structure and motifs of binding sites.\nNucleotide Transformer introduced the first large-scale transformer-based DNA LMs. All NT models share the same architecture, but differ in their number of training genomes and model parameters. Models were trained on either the human reference genome, 3,202 different genetically diverse human genomes or a selection of 850 genomes from a range of species. To increase the receptive field of the model, sequences were tokenized as 6-mers, allowing for processing sequences of up to 5,994 bp in length. A second generation of multispecies models released later (NT-V2) extended the input length to 12,282 bp. The NT models were evaluated on tasks comprising promoter, SS, histone modification and enhancer prediction with a context length of up to 600 bp.\nGENA-LM (Fishman et al., 2023) proposed multiple medium-size LMs trained on human and multispecies genomes based on BERT and the BigBird (Zaheer et al., 2020) architecture for long sequences. Byte-Pair Encoding (BPE) was used for tokenization to further increase the receptive field, enabling an input length of about 36,000 bp. Models were evaluated on tasks comprising promoter, SS, enhancer, chromatin profile and polyadenylation site prediction. While covering the same biological phenomena, tasks were defined differently than in NT. Similarly, DNABERT-2 (Zhou et al., 2023) replaced DNABERT\u2019s k-mer tokenizer with BPE and pre-trained on multi-species genomes, while GROVER (Sanabria et al., 2023) adopted BPE for the human genome.\nPredating NT and GENA-LM, FloraBERT (Levy et al., 2022) proposed pre-training on 93 different plant genomes to enable transfer learning for predicting gene expression. However, FloraBERT was\ntrained exclusively on promoter-containing sequences. As this requires features to already be annotated in the genome, it can be considered a departure from the paradigm of fully self-supervised learning. Similarly, Gankin et al. (2023) pre-trained on 3\u2019 UTRs from 1,500 fungal genomes. Species information was made explicit by providing a species label with each sequence to the model.\nHyenaDNA (Nguyen et al., 2023) introduced a collection of autoregressive LMs, trained using the next token prediction objective at single-nucleotide resolution on the human genome. The Hyena LM architecture (Poli et al., 2023) enabled scaling to input lengths of up to 1 million nucleotides. HyenaDNA models were evaluated by fine-tuning on NT\u2019s supervised tasks and the Genomic Benchmarks (Gres\u030cova\u0301 et al., 2023) collection, outperforming NT on the majority of tasks.\nA number of DNA LMs were proposed without making trained models available. These comprise the original BigBird (Zaheer et al., 2020), GeneBERT, which includes the prediction of ATACseq signals in the pre-training stage, MoDNA (An et al., 2022), with a motif prediction task as an additional objective, the BERT-based LOGO (Yang et al., 2021), and Revolution (Cheng et al., 2023), which adopts convolutions with circular padding."
        },
        {
            "heading": "2.3.2 SUPERVISED LEARNING ON DNA",
            "text": "Developing models on genomic DNA sequences for the prediction of properties and understanding of transcriptional regulation has long been a central task of computational genomics research. The availability of large-scale functional genomics data and advancements in deep learning techniques have brought progress in predicting various genomic features directly from DNA sequences. DeepBind (Alipanahi et al., 2015) and DeepSEA (Zhou & Troyanskaya, 2015) were two of the first methods leveraging shallow CNNs for predicting TF binding and chromatin features, respectively. DeepCpG (Angermueller et al., 2017) predicts DNA methylation via a CNN/GRU architecture. Basset (Kelley et al., 2016) and ChromTransfer (Salvatore et al., 2023) model chromatin state in a cell type specific manner by predicting the presence of DNase-I peaks. Using chromatin state as an auxiliary input, DeepChrome (Singh et al., 2016) predicts gene expression via multi-modal learning on DNA sequence and histone mark information.\nRecently, methods for predicting gene expression have leveraged information across thousands of functional genomic tracks by training in a large-scale, multi-task fashion. Basenji (Kelley et al., 2018) and Enformer (Avsec et al., 2021) demonstrated state-of-the-art performance for gene expression prediction from DNA sequence alone, by integrating genomic information across up to 200 kilobases and multi-task training across several genome-wide functional tasks, including DNase-I activity and CAGE signal prediction. Similarly, Sei (Chen et al., 2022) models cis-regulatory TF binding, chromatin accessibility and histone modification profiles across a large range of cell types."
        },
        {
            "heading": "2.3.3 BENCHMARK COLLECTIONS ON DNA",
            "text": "Genomic Benchmarks (Gres\u030cova\u0301 et al., 2023) features balanced classification tasks on DNA sequences with median lengths ranging from 200 to 2,381 bp. The benchmark covers the classification of functional elements and the prediction of a sequence\u2019s origin. The element classification tasks are defined on human DNA, with one task covering D. melanogaster additionally. For each task, only performance of a baseline supervised neural network model was reported.\nDNABERT-2 introduced Genome Understanding Evaluation (GUE), a collection of classification tasks ranging from 70 to 1,000 bp. On the human genome, it includes classification of promoter, SS and TF binding sequences. It covers other species with a TF binding task on mouse, a histone modification task on yeast and a Covid variant classification task on viruses. DNABERT, DNABERT-2 and NT were evaluated. No non-LM task-specific baselines are included in GUE.\nThe authors of NT provide a public leaderboard for their tasks, comprising promoter (human/mouse), enhancer (human), SS (human/multispecies) and histone modification (yeast) prediction with lengths ranging from 300 to 600 bp. NT is compared to DNABERT, DNABERT-2, HyenaDNA and Enformer. No task-specific baselines are included in the leaderboard."
        },
        {
            "heading": "2.3.4 MOTIVATION OF BEND",
            "text": "While existing DNA LMs have reported good performance on the tasks on which they were evaluated, evaluation strategies to date have shown limited consistency across individual works, with GUE constituting the most recent attempt at benchmarking on equal terms. Beyond comparability, it is important to ensure that benchmark tasks reflect the complexity and characteristics of real-world genome analysis. In practice, genomes are vast, and functional regions are sparsely distributed\nthroughout the genome. While there are tasks on DNA that are inherently local, such as classifying functional regions (e.g. classifying TF binding sites), it needs to be recognized that such tasks do not allow us to evaluate a model\u2019s understanding of the genome over longer ranges.\nTherefore, focusing solely on tasks on short sequences, such as distinguishing promoter from nonpromoter sequences, falls short of evaluating the extent to which a model\u2019s representations capture complex features of genomic organization, preventing us from measuring benefits of modeling the genome with larger context windows. For instance, reporting performance on predicting SSs, which can be done on short sequences, does not allow us to evaluate how useful a model would be for gene finding over longer ranges, a common task in genome annotation.\nTo provide a more comprehensive assessment, BEND proposes genomic tasks that rely less on prior knowledge of feature positions and require reasoning over potentially long contexts. The tasks cover a range of length scales, selected to be both biologically relevant and to cover a variety of DNA properties. The tasks explore representations at different resolutions, requiring modelling of DNA at single bp resolution as well as over longer stretches (Table 1). We establish our benchmark on the human genome, as it offers ample experimental data for the derivation of tasks, has a complex organization, and was the focus of most published DNA LMs."
        },
        {
            "heading": "3 TASKS AND DATASETS",
            "text": "We introduce the collection of tasks included in BEND. For each task, we additionally provide a datasheet (Gebru et al., 2018) in section A.1. All tasks are provided in bed format, listing the genome coordinates of samples (A.2). This makes it convenient to include more flanking context without reprocessing the data, should future works find it useful to take more bp into account."
        },
        {
            "heading": "3.1 GENE FINDING",
            "text": "Definition Gene finding is a multiclass problem where each nucleotide is either part of an exon (EF/R), intron (IF/R), a donor (DF/R) or acceptor (AF/R) splice site or a noncoding region (NC). The F/R subscript denotes whether the gene is located on the forward or reverse strand. Biological relevance Annotating genes and identifying coding sequences is a key step in genome annotation and protein discovery. It requires a model to use local context to identify correct reading frames and codon structure, while using longer range signals to propagate the location of SS to distant bp between SS, and correctly annotate them as lying in introns or exons. Introns vary in length from a few hundred to several thousand bp, requiring an LM to understand long-range dependencies. Data GENCODE (Frankish et al., 2021) gene annotations were processed to construct sequences of nucleotide labels y 2 {EF , DF , IF , AF , ER, DR, IR, AR, NC} for each gene. Detailed processing is laid out in A.1.1. Samples were partitioned at 80% identity following AUGUSTUS\u2019 recommendations (Stanke & Waack, 2003). It should be noted that there is a large label imbalance as there is only one donor and acceptor site per intron segment. Metric We compute the multi-class Matthews correlation coefficient (MCC) (Gorodkin, 2004) over all bp. The MCC is used as it is robust to the inherently highly uneven label ratios of this task."
        },
        {
            "heading": "3.2 ENHANCER ANNOTATION",
            "text": "Definition Enhancer annotation is the problem of finding enhancer regions for a given gene. We define enhancer annotation as a binary classification task. Given a sequence of gene-adjacent genomic DNA that contains enhancers, a binary label indicating whether it is part of an enhancer needs to be predicted for each segment of 128bp. Biological relevance Enhancers are short, noncoding segments that contribute to regulating gene expression. They can be located anywhere from a few thousand to a million bp away from their target gene and work by being brought into physical proximity to the gene\u2019s promoter. Their annotation is a highly challenging task that requires detection of long-range interactions. Data Experimentally validated enhancer-gene pairs were taken from CRISPR interference experiments (Fulco et al. (2019); Gasperini et al. (2019) and paired with the main TSS of each gene from Avsec et al. (2021). We extracted a sequence of 100,096 bp centered on the TSS for each gene. Each 128bp were annotated with a binary label y 2 {0, 1} indicating whether the bin is part of an enhancer, yielding a label sequence of length 782. Detailed processing is laid out in A.1.2. Samples were partitioned based on chromosomes. Metric The AUPRC is computed over all labels. As the number of samples is too limited for measuring performance robustly on a single test split, we perform 10-fold cross-validation in order to evaluate performance over all samples."
        },
        {
            "heading": "3.3 CHROMATIN ACCESSIBILITY PREDICTION",
            "text": "Definition Chromatin accessibility prediction is a multilabel task where sequences are classified as being in open or closed chromatin across a range of cell types. Biological relevance Dynamically modulating chromatin accessibility is a key mechanism for the cell type specific regulation of gene expression, as binding of the transcription machinery is highly dependent on the accessibility of DNA elements, including promoters, enhancers and TSS. Data DNase I hypersensitive sites were obtained from ENCODE (ENCODE Project Consortium, 2012) for 125 cell types. Following the preprocessing of Kelley et al. (2016), segments of length 512 bp were labeled with binary vectors y 2 {0, 1}125, with yi = 1 if the chromatin is open for the i\u2019th cell type. Detailed processing is laid out in A.1.3. Samples were partitioned based on chromosomes. Metric The AUROC is computed for each label and averaged."
        },
        {
            "heading": "3.4 HISTONE MODIFICATION PREDICTION",
            "text": "Definition Histone modification prediction is a multilabel task, where the histones which are part of the nucleosomes of a given DNA sequence are labeled with one or more histone marks. Biological relevance Histone proteins are key to the organisation of DNA into chromatin. Modifications of histones modulate chromatin structure and thus contribute to regulating chromatin accessibility and gene expression. Histone modification prediction requires modeling local binding of TFs as well as long-range regulation, such as by distant enhancers. Data Histone ChIP-seq data for 11 histone marks and 19 replicates in the K562 cell line was obtained from ENCODE. Detailed processing is laid out in A.1.4 and follows the methodology of 3.3. Each sample is a sequence of length 512 bp with a label vector y 2 {0, 1}19, such that yi = 1 if a histone bound to this sequence carries mark i. Samples were partitioned based on chromosomes. Metric The AUROC is computed for each label and averaged."
        },
        {
            "heading": "3.5 CPG METHYLATION PREDICTION",
            "text": "Definition CpG methylation prediction is a multilabel classification task, where a given CpG site is either methylated or unmethylated in different cell lines. Biological relevance Methylation of cytosine nucleotides in CpG sites is a prominent form of epigenetic modification and plays a key role in the repression of gene expression. Data Bisulfite sequencing data for 7 human cell lines was obtained from ENCODE. Detailed processing is laid out in A.1.5. Each sample is a sequence of length 512 bp centered on the CpG site with a label vector y 2 {0, 1}7, such that yi = 1 if the C is methylated. Samples were partitioned based on chromosomes. Metric The AUROC is computed for each label and averaged."
        },
        {
            "heading": "3.6 NONCODING VARIANT EFFECTS (EXPRESSION AND DISEASE)",
            "text": "Definition Predicting variant effects is a binary problem, where single-bp mutations are classified as either having an effect or not. We treat classification as a zero-shot task, using the cosine distance\nin embedding space between a variant nucleotide and its reference nucleotide as the prediction score. Biological relevance Single-bp variants in noncoding regions can have functional consequences by altering gene expression levels or causing disease. This task probes the LM\u2019s understanding of local context and potentially the structure of regulatory motifs. We focus on noncoding regions, as coding variant effects can be predicted with high accuracy by modeling the mutation in the resulting protein sequence (Frazer et al., 2021). Data For expression variants, we adapt the DeepSEA dataset (Zhou & Troyanskaya, 2015). For disease-associated variants, we process ClinVar (Landrum et al., 2020). We apply Ensembl VEP (McLaren et al., 2016) to categorize variants by genomic regions into consequence types. Detailed processing is laid out in A.1.6 and A.1.7. Each variant is a genomic position with a mutation x 2 {A, C, G, T} and a label y 2 {0, 1}. The adjacent 512 bp serve as embedding context. Metric We compute the AUROC. Additionally, we report separate AUROCs for the variant consequence types to gain further insight into what genomic features are driving performance."
        },
        {
            "heading": "4 MODELING",
            "text": "Language Models We benchmark available LMs suitable for the human genome (Table 2). Checkpoint selection criteria are laid out in A.6.2. Additionally, we train two simple baseline DNA LMs: An AWD-LSTM (Merity et al., 2017) model trained on three species, and a dilated CNN similar to GPN (Benegas et al., 2023), trained on the human genome. The model differs from GPN in the parameter count and the length of training sequences (A.6.1).\nDownstream model We train a lightweight supervised two-layer CNN model with 64 channels on top of the LM embeddings for each task. LM weights are kept frozen and are not fine-tuned. For LMs with reduced output length due to tokenization, embeddings are upsampled to the original sequence length (A.6.3). For sequence-level tasks, we apply average pooling after the last CNN layer. For the enhancer annotation task, the number of channels was reduced to prevent overfitting. No downstream model is trained for variant effect prediction, as the cosine distance of the LM embeddings directly serves as the zero-shot predictor.\nSupervised baselines For each task, we train two supervised models without pre-training. For a direct comparison of raw and embedded DNA, we train the two-layer CNN on one-hot encoded sequences. For chromatin accessibility, histone modificaton and CpG methylation prediction, we train the Basset model (Kelley et al., 2016), which was specifically designed for modeling genomewide functional genomics data. For gene finding and enhancer annotation, we train the ResNet CNN model on one-hot encoded sequences. For variant effect prediction, no supervised models are trained. For all tasks where Basset is not applicable, we report the performance of a previously published task-specific expert method on the benchmark dataset to put LM performance into context."
        },
        {
            "heading": "5 RESULTS",
            "text": "Gene finding DNA LMs show promising performance for gene finding (Table 3). The two-layer CNN baseline fails to learn, possibly due to its inherent limitation to local context. However, the same CNN is able to achieve varying levels of performance when using LM embeddings, suggesting that embeddings capture some long-range information. NT-MS and NT-V2 outperform all other models by a wide margin, but still do not approach the highly specialized AUGUSTUS (Stanke & Waack, 2003) gene finding model. This highlights that while more specialized downstream models are still needed to accurately predict gene structure, using pre-trained DNA LM embeddings presents a promising avenue to attain good performance. Computing individual performance metrics across all classes (Table A8) reveals that although there is high variance in the performance across all classes, some embeddings capture splice sites fairly considering their low frequency. HyenaDNAlarge, although being the only LM whose context length fully covers the input length of the task, only shows modest performance.\nEnhancer annotation All investigated models perform poorly on this task. Enhancer annotation is an extremely difficult task due to the length scale, sparsity of the signal, and small dataset, which pose challenges for all investigated models. Although the supervised baseline has a large enough receptive field to detect the long-range interaction, the size of the dataset is prohibitive for performance. The performance of Enformer (Avsec et al., 2021) (A.7) is comparable on this task, but it must be noted that this is an unsupervised method that was not trained directly on enhancer data. Rather, it infers their locations from learning to predict other genome annotations. Predicting genespecific enhancers from sequence alone without considering supporting experimental data as input therefore remains a highly difficult problem. While this task already proves to be highly challenging for current models at the given length scales, we note that biology is even more complex, with enhancers potentially being millions of bp away.\nChromatin accessibility DNABERT shows the highest performance, on par with the specialized Basset model (0.85). All other LMs perform worse on this task, offering no advantage over Basset.\nHistone modification NT-MS and DNABERT show the highest performance (0.74), outperforming Basset (0.72). This suggests that LM embeddings can improve performance for histone modification prediction, albeit at marginal levels.\nCpG methylation NT-MS performs best (0.92) on all included cell lines (Table A11), but is outperformed by Basset (0.93). DNABERT, GENA-LM and HyenaDNA-large also perform competitively, indicating that embeddings capture information about CpG island methylation patterns.\nVariant effect prediction DNA LMs show some signal for unsupervised prediction of noncoding variant effects. As the two datasets focus on different genomic regions, we only see limited consistency between the expression and disease variant tasks, with DNABERT and NT-MS performing best respectively. While being worse than the supervised DeepSEA method, DNABERT matches DeepSEA\u2019s unsupervised performance on the expression dataset (AUROC 0.6, A.7). On the disease\ndataset, multiple LMs approach DeepSEA\u2019s Disease Impact Score, with NT-MS outperforming it. When dissecting performance by variant types, we find that the performance of NT-MS is driven by variants affecting splice sites and introns (Table A13). While splice sites can be considered noncoding DNA, they are not the focus of DeepSEA, which models chromatin features, and shows stronger performance in UTRs and up- or downstream regions. Similar to the results on the expression dataset, we find that DNABERT outperforms NT-MS in such regions, suggesting that the two LMs learned distinct sequence features during pre-training. As all other NT models show weaker performance on variants affecting gene structure, this could be a consequence of the model\u2019s large size and multi-species pre-training. However, we do not see similarly strong performance in the multi-species DNABERT-2 and NT-V2."
        },
        {
            "heading": "6 DISCUSSION",
            "text": "We find that currently available DNA LMs already show promising performance on some tasks over fully supervised baselines, but do not offer consistent improvements over all included tasks and can fall short of surpassing specialized existing prediction methods. Overall, we find that NT-MS is a strong default LM, but is in some tasks inferior to the much smaller DNABERT. Interestingly, while both models trained using the MLM objective, we find that they learned distinct genomic features during pre-training. With the pre-training data and the tokenization strategy being the key architectural difference, these choices may deserve more attention in future DNA LMs.\nFor modeling functional genomics data, DNA LMs only show limited utility. In direct comparison to the Basset model trained on the same data, LM embeddings fail to yield consistent improvements in performance when only using a two-layer CNN.\nOn the gene finding task, we observe that NT-MS with a simple two-layer CNN shows promising performance compared to the specialized AUGUSTUS, which was found to be the state of the art in a recent benchmark (Scalzitti et al., 2020). This suggests that future more sophisticated LM-based gene finders might become a method of choice for this problem. The result also indicates that current DNA LMs are capable of modeling long-range dependencies to some extent.\nProbing LMs at even longer ranges in the enhancer annotation task reveals that long-range understanding still needs improvement for sparse problems with limited data. This highlights a key issue facing DNA LMs: Not only is there a need for long-range modeling to improve our understanding of the genome, as demonstrated by Avsec et al. (2021), but it also raises a fundamental question as to whether current LM training objectives will lead to the incorporation of such distant, sparse signals, or whether the local sequence context is all that is required for sequence reconstruction and some level of supervision is needed. Since BEND is not inherently tied to an LM objective, our standardized benchmark may also prove useful for evaluating eventual DNA representation models that follow a different paradigm."
        },
        {
            "heading": "7 LIMITATIONS AND OUTLOOK",
            "text": "As the curation of a comprehensive benchmark task collection requires experimental ground-truth data to be available, and most published models are trained on human data, we focused BEND on the human genome. BEND aims at comparing the effectiveness of different model architectures and training strategies for learning representations from genomic data, under the assumption that other, similarly structured genomes should behave comparably under self-supervision. However, an important question that remains unanswered is whether DNA LMs can aid with generalization across different organisms. In the future, we hope to extend the benchmark to other, diverse organisms, so that generalization power can be tested in a transfer-learning setting, i.e. by training a task on a given organism, and evaluating performance on another.\nIn BEND, we benchmarked to what extent embeddings capture features that can be leveraged by downstream models for prediction. This approach is fully agnostic regarding the underlying LM\u2019s methodology and scales to models of any size. Other works proposed to fine-tune LMs on tasks directly. While this potentially conflates a representation\u2019s content with the inductive bias of a model architecture for a given task, fine-tuning may yield performance gains beyond the results observed in this work (Nguyen et al., 2023; Zhou et al., 2023). Another aspect to be investigated in the future is to dive deeper into how LMs learn features during pre-training, as done previously for protein LMs (Vig et al., 2021)."
        }
    ],
    "title": "BEND: BENCHMARKING DNA LANGUAGE MODELS ON BIOLOGICALLY MEANINGFUL TASKS",
    "year": 2024
}