{
    "abstractText": "We study differentially private (DP) algorithms for recovering clusters in wellclustered graphs, which are graphs whose vertex set can be partitioned into a small number of sets, each inducing a subgraph of high inner conductance and small outer conductance. Such graphs have widespread application as a benchmark in the theoretical analysis of spectral clustering. We provide an efficient (\u03b5,\u03b4)-DP algorithm tailored specifically for such graphs. Our algorithm draws inspiration from the recent work of Chen et al., who developed DP algorithms for recovery of stochastic block models in cases where the graph comprises exactly two nearly-balanced clusters. Our algorithm works for well-clustered graphs with k nearly-balanced clusters, and the misclassification ratio almost matches the one of the best-known non-private algorithms. We conduct experimental evaluations on datasets with known ground truth clusters to substantiate the prowess of our algorithm. We also show that any (pure) \u03b5-DP algorithm would result in substantial error.",
    "authors": [
        {
            "affiliations": [],
            "name": "RITHM FOR"
        },
        {
            "affiliations": [],
            "name": "WELL-CLUSTERED GRAPHS"
        }
    ],
    "id": "SP:bf580bd22dad65b1aef39ef79760059c5b6ff65c",
    "references": [
        {
            "authors": [
                "Sara Ahmadian",
                "Ashkan Norouzi-Fard",
                "Ola Svensson",
                "Justin Ward"
            ],
            "title": "Better guarantees for kmeans and euclidean k-median by primal-dual algorithms",
            "venue": "SIAM Journal on Computing,",
            "year": 2019
        },
        {
            "authors": [
                "Charles J Alpert",
                "So-Zen Yao"
            ],
            "title": "Spectral partitioning: The more eigenvectors, the better",
            "venue": "In Proceedings of the 32nd annual ACM/IEEE design automation conference,",
            "year": 1995
        },
        {
            "authors": [
                "Reid Andersen",
                "Fan Chung",
                "Kevin Lang"
            ],
            "title": "Local graph partitioning using pagerank vectors",
            "venue": "47th Annual IEEE Symposium on Foundations of Computer Science",
            "year": 2006
        },
        {
            "authors": [
                "Raman Arora",
                "Jalaj Upadhyay"
            ],
            "title": "On differentially private graph sparsification and applications",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Mikhail Belkin",
                "Partha Niyogi"
            ],
            "title": "Laplacian eigenmaps and spectral techniques for embedding and clustering",
            "venue": "Advances in neural information processing systems,",
            "year": 2001
        },
        {
            "authors": [
                "Mark Bun",
                "Marek Elias",
                "Janardhan Kulkarni"
            ],
            "title": "Differentially private correlation clustering",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Hongjie Chen",
                "Vincent Cohen-Addad",
                "Tommaso d\u2019Orsi",
                "Alessandro Epasto",
                "Jacob Imola",
                "David Steurer",
                "Stefan Tiegel"
            ],
            "title": "Private estimation algorithms for stochastic block models and mixture models",
            "venue": "arXiv preprint arXiv:2301.04822,",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Chiplunkar",
                "Michael Kapralov",
                "Sanjeev Khanna",
                "Aida Mousavifar",
                "Yuval Peres"
            ],
            "title": "Testing graph clusterability: Algorithms and lower bounds",
            "venue": "IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS),",
            "year": 2018
        },
        {
            "authors": [
                "Vincent Cohen-Addad",
                "Chenglin Fan",
                "Silvio Lattanzi",
                "Slobodan Mitrovic",
                "Ashkan Norouzi-Fard",
                "Nikos Parotsidis",
                "Jakub M Tarnawski"
            ],
            "title": "Near-optimal correlation clustering with privacy",
            "year": 2022
        },
        {
            "authors": [
                "Mihai Cucuringu",
                "Ioannis Koutis",
                "Sanjay Chawla",
                "Gary Miller",
                "Richard Peng"
            ],
            "title": "Simple and scalable constrained clustering: a generalized spectral method",
            "venue": "In Artificial Intelligence and Statistics,",
            "year": 2016
        },
        {
            "authors": [
                "Yuyang Cui",
                "Huaming Wu",
                "Yongting Zhang",
                "Yonggang Gao",
                "Xiang Wu"
            ],
            "title": "A spectral clustering algorithm based on differential privacy preservation",
            "venue": "In International Conference on Algorithms and Architectures for Parallel Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Artur Czumaj",
                "Pan Peng",
                "Christian Sohler"
            ],
            "title": "Testing cluster structure of graphs",
            "venue": "In Proceedings of the forty-seventh annual ACM symposium on Theory of Computing,",
            "year": 2015
        },
        {
            "authors": [
                "Chandler Davis",
                "William Morton Kahan"
            ],
            "title": "The rotation of eigenvectors by a perturbation",
            "venue": "iii. SIAM Journal on Numerical Analysis,",
            "year": 1970
        },
        {
            "authors": [
                "Tamal K Dey",
                "Pan Peng",
                "Alfred Rossi",
                "Anastasios Sidiropoulos"
            ],
            "title": "Spectral concentration and greedy k-clustering",
            "venue": "Computational Geometry,",
            "year": 2019
        },
        {
            "authors": [
                "Cynthia Dwork",
                "Frank McSherry",
                "Kobbi Nissim",
                "Adam Smith"
            ],
            "title": "Calibrating noise to sensitivity in private data analysis",
            "venue": "In Theory of Cryptography: Third Theory of Cryptography Conference,",
            "year": 2006
        },
        {
            "authors": [
                "Shayan Oveis Gharan",
                "Luca Trevisan"
            ],
            "title": "Approximating the expansion profile and almost optimal local graph clustering",
            "venue": "In 2012 IEEE 53rd Annual Symposium on Foundations of Computer Science,",
            "year": 2012
        },
        {
            "authors": [
                "Shayan Oveis Gharan",
                "Luca Trevisan"
            ],
            "title": "Partitioning into expanders",
            "venue": "In Proceedings of the twentyfifth annual ACM-SIAM symposium on Discrete algorithms,",
            "year": 2014
        },
        {
            "authors": [
                "Badih Ghazi",
                "Ravi Kumar",
                "Pasin Manurangsi"
            ],
            "title": "Differentially private clustering: Tight approximation ratios",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Grzegorz Gluch",
                "Michael Kapralov",
                "Silvio Lattanzi",
                "Aida Mousavifar",
                "Christian Sohler"
            ],
            "title": "Spectral clustering oracles in sublinear time",
            "venue": "In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA),",
            "year": 2021
        },
        {
            "authors": [
                "Olivier Gu\u00e9don",
                "Roman Vershynin"
            ],
            "title": "Community detection in sparse networks via grothendieck\u2019s inequality",
            "venue": "Probability Theory and Related Fields,",
            "year": 2016
        },
        {
            "authors": [
                "Jonathan Hehir",
                "Aleksandra Slavkovic",
                "Xiaoyue Niu"
            ],
            "title": "Consistent spectral clustering of network block models under local differential privacy",
            "venue": "Journal of Privacy and Confidentiality,",
            "year": 2022
        },
        {
            "authors": [
                "Zhiyi Huang",
                "Jinyan Liu"
            ],
            "title": "Optimal differentially private algorithms for k-means clustering",
            "venue": "In Proceedings of the 37th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Ravi Kannan",
                "Santosh Vempala",
                "Adrian Vetta"
            ],
            "title": "On clusterings: Good, bad and spectral",
            "venue": "Journal of the ACM (JACM),",
            "year": 2004
        },
        {
            "authors": [
                "Tapas Kanungo",
                "David M Mount",
                "Nathan S Netanyahu",
                "Christine D Piatko",
                "Ruth Silverman",
                "Angela Y Wu"
            ],
            "title": "A local search approximation algorithm for k-means clustering",
            "venue": "In Proceedings of the eighteenth annual symposium on Computational geometry,",
            "year": 2002
        },
        {
            "authors": [
                "Pavel Kolev",
                "Kurt Mehlhorn"
            ],
            "title": "Approximate spectral clustering: Efficiency and guarantees",
            "venue": "arXiv preprint arXiv:1509.09188,",
            "year": 2015
        },
        {
            "authors": [
                "James R Lee",
                "Shayan Oveis Gharan",
                "Luca Trevisan"
            ],
            "title": "Multiway spectral partitioning and higherorder cheeger inequalities",
            "venue": "Journal of the ACM (JACM),",
            "year": 2014
        },
        {
            "authors": [
                "Daogao Liu"
            ],
            "title": "Better private algorithms for correlation clustering",
            "venue": "arXiv preprint arXiv:2202.10747,",
            "year": 2022
        },
        {
            "authors": [
                "Rong Liu",
                "Hao Zhang"
            ],
            "title": "Segmentation of 3d meshes through spectral clustering",
            "venue": "In 12th Pacific Conference on Computer Graphics and Applications,",
            "year": 2004
        },
        {
            "authors": [
                "Jitendra Malik",
                "Serge Belongie",
                "Thomas Leung",
                "Jianbo Shi"
            ],
            "title": "Contour and texture analysis for image segmentation",
            "venue": "International journal of computer vision,",
            "year": 2001
        },
        {
            "authors": [
                "Tomohiko Mizutani"
            ],
            "title": "Improved analysis of spectral algorithm for clustering",
            "venue": "Optimization Letters,",
            "year": 2021
        },
        {
            "authors": [
                "Mohamed S Mohamed",
                "Dung Nguyen",
                "Anil Vullikanti",
                "Ravi Tandon"
            ],
            "title": "Differentially private community detection for stochastic block models",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Ng",
                "Michael Jordan",
                "Yair Weiss"
            ],
            "title": "On spectral clustering: Analysis and an algorithm",
            "venue": "Advances in neural information processing systems,",
            "year": 2001
        },
        {
            "authors": [
                "Kobbi Nissim",
                "Sofya Raskhodnikova",
                "Adam Smith"
            ],
            "title": "Smooth sensitivity and sampling in private data analysis",
            "venue": "In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing,",
            "year": 2007
        },
        {
            "authors": [
                "Pan Peng"
            ],
            "title": "Robust clustering oracle and local reconstructor of cluster structure of graphs",
            "venue": "In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms,",
            "year": 2020
        },
        {
            "authors": [
                "Richard Peng",
                "He Sun",
                "Luca Zanetti"
            ],
            "title": "Partitioning well-clustered graphs: Spectral clustering works",
            "venue": "In Conference on learning theory,",
            "year": 2015
        },
        {
            "authors": [
                "Mohamed Seif",
                "Andrea J Goldsmith",
                "H Vincent Poor"
            ],
            "title": "Differentially private community detection over stochastic block models with graph sketching",
            "venue": "57th Annual Conference on Information Sciences and Systems (CISS),",
            "year": 2023
        },
        {
            "authors": [
                "Moshe Shechner",
                "Or Sheffet",
                "Uri Stemmer"
            ],
            "title": "Private k-means clustering with stability assumptions",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Jianbo Shi",
                "Jitendra Malik"
            ],
            "title": "Normalized cuts and image segmentation",
            "venue": "IEEE Transactions on pattern analysis and machine intelligence,",
            "year": 2000
        },
        {
            "authors": [
                "Daniel A Spielman",
                "Shang-Hua Teng"
            ],
            "title": "Spectral partitioning works: Planar graphs and finite element meshes",
            "venue": "In Proceedings of 37th conference on foundations of computer science,",
            "year": 1996
        },
        {
            "authors": [
                "Daniel A. Spielman",
                "Shang-Hua Teng"
            ],
            "title": "Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems",
            "venue": "Proceedings of the 36th Annual ACM Symposium on Theory of Computing,",
            "year": 2004
        },
        {
            "authors": [
                "Kadim Ta\u015fdemir"
            ],
            "title": "Vector quantization based approximate spectral clustering of large datasets",
            "venue": "Pattern Recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Ulrike Von Luxburg"
            ],
            "title": "A tutorial on spectral clustering",
            "venue": "Statistics and computing,",
            "year": 2007
        },
        {
            "authors": [
                "Lijun Wang",
                "Ming Dong"
            ],
            "title": "Multi-level low-rank approximation-based spectral clustering for image segmentation",
            "venue": "Pattern Recognition Letters,",
            "year": 2012
        },
        {
            "authors": [
                "Yining Wang",
                "Yu-Xiang Wang",
                "Aarti Singh"
            ],
            "title": "Differentially private subspace clustering",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Yue Wang",
                "Xintao Wu",
                "Leting Wu"
            ],
            "title": "Differential privacy preserving spectral graph analysis",
            "venue": "Proceedings, Part II",
            "year": 2013
        },
        {
            "authors": [
                "Scott White",
                "Padhraic Smyth"
            ],
            "title": "A spectral clustering approach to finding communities in graphs",
            "venue": "In Proceedings of the 2005 SIAM international conference on data mining,",
            "year": 2005
        },
        {
            "authors": [
                "Lihi Zelnik-Manor",
                "Pietro Perona"
            ],
            "title": "Self-tuning spectral clustering",
            "venue": "Advances in neural information processing systems,",
            "year": 2004
        },
        {
            "authors": [
                "Zeyuan Allen Zhu",
                "Silvio Lattanzi",
                "Vahab Mirrokni"
            ],
            "title": "A local algorithm for finding wellconnected clusters",
            "venue": "In International Conference on Machine Learning,",
            "year": 2013
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "Consider the set S = {p \u2208 {\u22121, 1} | 1\u22a4p = 0} equipped with the semimetric err",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Graph Clustering is a fundamental task in unsupervised machine learning and combinatorial optimization, relevant to various domains of computer science and their diverse practical applications. The goal of Graph Clustering is to partition the vertex set of a graph into distinct groups (or clusters) so that similar vertices are grouped in the same cluster while dissimilar vertices are assigned to different clusters.\nThere exist numerous notions of similarity and measures of evaluating the quality of graph clusterings, with conductance being one of the most extensively studied (see e.g. (Kannan et al., 2004; Von Luxburg, 2007; Gharan & Trevisan, 2012)). Formally, let G = (V,E) be an undirected graph. For any vertex u \u2208 V , its degree is denoted by degG(u), and for any set S \u2286 V , its volume is volG(S) = \u2211 u\u2208S degG(u). For any two subsets S, T \u2282 V , we define E(S, T ) to be the set of edges between S and T . For any nonempty subset C \u2282 V , the outer conductance and inner conductance are defined by\n\u03a6out(G,C) = |E(C, V \\ C)|\nvolG(C) , \u03a6in(G,C) = min\nS\u2286C volG(S)\u2264 volG(C)\n2\n\u03a6out(C, S)\nIntuitively, if a vertex set C has low outer conductance, then it has relatively few connections to the outside, and if it has high inner conductance, then it is well connected inside. Based on this intuition, Gharan & Trevisan (2014) introduced the following notion of well-clustered graphs. A k-partition of a graph G = (V,E) is a family of k disjoint vertex subsets C1, . . . , Ck of V such that the union \u222aki=1Ci = V . If there is some constant c \u2208 (0, 1] such that for every i \u2208 [k], volG(Ci) \u2265 cvolG(G)k = 2cm k is satisfied, we call the k-partition {Ci}i\u2208[k] c-balanced. Definition 1.1 (Well-clustered graph). Given parameters k \u2265 1, \u03d5in, \u03d5out \u2208 [0, 1], a graph G = (V,E) is called (k, \u03d5in, \u03d5out)-clusterable if there exists a k-partition {Ci}i\u2208[k] of V such that for all i \u2208 [k], \u03a6in(G,Ci) \u2265 \u03d5in and \u03a6out(G,Ci) \u2264 \u03d5out. Furthermore, if {Ci}i\u2208[k] is c-balanced, G = (V,E) is called c-balanced (k, \u03d5in, \u03d5out)-clusterable.\nIn the above, if a k-partition {Ci}i\u2208[k] satisfies the above two conditions on the inner and outer conductances, then we call the partition a ground truth partition of G. Gharan & Trevisan (2014)\ngive a simple polynomial-time spectral algorithm to approximately find such a partitioning. Since then, a plethora of works has focused on extracting the cluster structure of such graphs with spectral methods (see Section 1.1). For example, Czumaj et al. (2015); Peng et al. (2015); Chiplunkar et al. (2018); Peng (2020); Gluch et al. (2021) used well-clustered graphs as a theoretical arena to gain a better understanding of why spectral clustering is successful. They showed that variants of the widely used spectral clustering give a good approximation of the optimal clustering in a wellclustered graph.\nIn this paper, we study differentially private (DP) algorithms for recovering clusters in well-clustered graphs. DP algorithms aim to enable statistical analyses of sensitive information on individuals while providing strong guarantees that no information of any individual is leaked (Dwork et al., 2006). Given the success of spectral methods for clustering graphs in non-private settings, surprisingly little is known about differentially private spectral clustering. Finding ways to leverage these methods in a privacy-preserving way gradually bridges differential privacy and an area of research with many deep structural results and a strong toolkit for graphs. In this line of research, we obtain the following result.\nTheorem 1. Let G = (V,E) be a c-balanced (k, \u03d5in, \u03d5out)-clusterable graph with its ground truth partition {Ci}i\u2208[k], where \u03d5out\u03d52in = O(k \u22124). Then, there exists an algorithm that outputs a k-partition {C\u0302i}ki=1 such that\nvolG(C\u0302i\u25b3C\u03c3(i)) = O ( k4\nc2 \u00b7 \u03d5out \u03d52in\n) \u00b7 volG(C\u03c3(i)), for any i \u2264 k\nwith probability 1 \u2212 exp(\u2212\u2126(n)) as long as m \u2265 n \u00b7 \u03d5 4 in\n\u03d52out \u00b7 log(2/\u03b4)\u03f52 , where \u03c3 is a permutation over\n[k] := {1, . . . , k}. Moreover, the algorithm is (\u03f5, \u03b4)-differentially private for any input graph with respect to edge privacy and runs in polynomial time.\nIn the non-private setting, the best-known efficient algorithms achieve O(k3 \u03d5out \u03d52in ) misclassification ratio under the assumption that1 \u03d5out \u03d52in\n= O(k\u22123) (Peng et al., 2015). Thus, our private algorithm almost matches the best-known non-private one in terms of approximate accuracy or utility.\nOur private mechanism is inspired by the recent work of Chen et al. (2023). We design a simple Semi-Definite Program (SDP) and run spectral clustering on a noisy solution. To analyze our algorithm, we extend the notion of strong convexity and prove the stability of the SDP. This allows us to show that the solution of the SDP has small sensitivity. In differential privacy, sensitivity is a measure of how much a function\u2019s value changes for small, but arbitrary and possibly adversarial changes in the data. For the non-private SDP solution, we show that applying classical privacy mechanisms and spectral clustering yields a differentially private clustering algorithm. Based on the analysis by Peng et al. (2015), we prove that our differentially private clustering algorithm achieves an approximate accuracy that nearly matches the non-private version. Furthermore, we remark that Chen et al. (2023) give a DP algorithm for recovery of stochastic block model (SBM) in cases where the graph comprises exactly two nearly-balanced clusters. Our algorithm achieves a similar approximation accuracy to their weak recovery and supports k clusters.\nTo complement our results, we conduct an experimental evaluation on datasets with known ground truth clusters to substantiate the prowess of our algorithm. Furthermore, we show that any (pure) \u03f5-DP algorithm entails substantial error in its output (see Appendix D).\nTheorem 2 (informal). Any algorithm for the cluster recovery of well-clustered graphs with failure probability \u03b7 and misclassification rate \u03b6 cannot satisfy \u03f5-DP for \u03f5 < 2 ln(1/(9e\u03b6)/d on d-regular graphs."
        },
        {
            "heading": "1.1 RELATED WORK",
            "text": "Our results combine spectral clustering and differential privacy. After Dwork et al. (2006) systematically introduced the notion of differential privacy, various clustering objectives have been studied\n1Strictly speaking, Peng et al. (2015) stated their result in terms of a so-called quantity \u03a5, which can be lower bounded by \u03d52in/\u03d5out by higher Cheeger inequality (Lee et al., 2014).\nin this regime. Differentially private metric clustering (e.g., k-clustering) was studied in Nissim et al. (2007); Wang et al. (2015); Huang & Liu (2018); Shechner et al. (2020); Ghazi et al. (2020). Correlation Clustering with differential privacy has been the subject of more recent works, e.g., Bun et al. (2021); Liu (2022); Cohen-Addad et al. (2022). In machine learning, the SBM and mixture models are used to model and approximate properties of real graphs. Differentially private recovery of these graphs or their properties has been studied in Hehir et al. (2022); Mohamed et al. (2022); Chen et al. (2023); Seif et al. (2023).\nSpectral methods for graphs have a long track of research that started before differential privacy was widely studied. One of the first theoretical analyses of these methods appeared in Spielman & Teng (1996), followed by several others, e.g., Von Luxburg (2007); Gharan & Trevisan (2014); Peng et al. (2015); Kolev & Mehlhorn (2015); Dey et al. (2019); Mizutani (2021). Indeed, there has been a long line of research aiming at designing efficient algorithms for extracting clusters with small outer conductance (and with high inner conductance) in a graph (Spielman & Teng, 2004; Andersen et al., 2006; Gharan & Trevisan, 2012; Zhu et al., 2013). Privacy-preserving spectral methods have been studied rather recently (Wang et al. (2013); Arora & Upadhyay (2019); Cui et al. (2021)).\nFor well-clustered graphs, spectral clustering is the most commonly used and effective algorithm, which operates in the following two steps: (1) construct a spectral embedding, mapping vertices into a k-dimensional real space. (2) use k-means or other algorithms to cluster the points in Euclidean space. Spectral clustering has been applied in many fields and has achieved significant results (Alpert & Yao, 1995; Shi & Malik, 2000; Ng et al., 2001; Malik et al., 2001; Belkin & Niyogi, 2001; Liu & Zhang, 2004; Zelnik-Manor & Perona, 2004; White & Smyth, 2005; Von Luxburg, 2007; Wang & Dong, 2012; Tas\u0327demir, 2012; Cucuringu et al., 2016)."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "We use bold symbols to represent vectors and matrices. For vectors u,v, we denote by \u27e8u,v\u27e9 the inner product, \u27e8u,v\u27e9 = u\u22a4v = \u2211 i uivi. For a vector u, we denote by \u2225u\u22251 = \u2211 i|ui|, \u2225u\u22252 =\u221a\u2211\ni u 2 i its \u21131 norm and \u21132 norm, respectively. For two matrices A and B, define \u27e8A,B\u27e9 =\u2211\ni,j AijBij . Denote by \u2225A\u22252 the spectral norm of A. Denote by \u2225A\u2225F the Frobenius norm of A. A matrix A is said to be positive semidefinite if there is a matrix V such that A = V \u22a4V , denoted as A \u2ab0 0. Note that Aij = vi \u00b7 vj , where vi is the i-th column of V and A is known as the Gram matrix of these vectors vi, i \u2208 [n]. For n \u2265 1, let [n] = {1, . . . , n}. Let Diag(a1, a2, \u00b7 \u00b7 \u00b7 , an) be the diagonal matrix with a1, a2, \u00b7 \u00b7 \u00b7 , an on the diagonal. We denote N ( 0, \u03c32 )m\u00d7n as the distribution over Gaussian matrices with m\u00d7 n entries, each having a standard deviation \u03c3. In this paper, we assume that G = (V,E) is an undirected graph with |V | = n vertices, |E| = m edges. For a nonempty subset S \u2282 V , we define G[S] to be the induced subgraph on S and we denote by G{S} the subgraph G[S], where self-loops are added to vertices v \u2208 S such that their degrees in G and G{S} are the same. For graph G = (V,E), let DG = Diag(degG(v1),degG(v2), \u00b7 \u00b7 \u00b7 ,degG(vn)). We denote by AG the adjacency matrix and by LG the Laplacian matrix where LG = DG \u2212AG.\nDifferential Privacy We consider edge-privacy and call two graphs G = (V,E) and G\u2032 = (V \u2032, E\u2032) neighboring if it holds that V = V \u2032 and |(E \\ E\u2032) \u222a (E\u2032 \\ E)| \u2264 1. The definition of differential privacy is as follows: Definition 2.1 (Differential privacy). A randomized algorithm M is (\u03f5, \u03b4)-differentially private if for all neighboring graphs G and G\u2032 and all subsets of outputs S, Pr[M(G) \u2208 S] \u2264 e\u2212\u03f5 \u00b7 Pr[M(G\u2032) \u2208 S] + \u03b4, where the probability is over the randomness of the algorithm.\nDP mechanisms typically achieve privacy by adding noise, where the magnitude of the noise depends on the sensitivity of the function. Definition 2.2 (Sensitivity of a function). Let f : D \u2192 Rd with domain D be a function. The \u21131-sensitivity and \u21132-sensitivity of f are defined respectively as \u2206f,1 := max\nY ,Y \u2032\u2208D Y ,Y \u2032 are neighboring\n\u2225f(Y )\u2212 f(Y \u2032)\u22251 \u2206f,2 := max Y ,Y \u2032\u2208D\nY ,Y \u2032 are neighboring\n\u2225f(Y )\u2212 f(Y \u2032)\u22252 .\nGaussian mechanism is one of the most widely used mechanisms in differential privacy.\nLemma 2.3 (Gaussian mechanism). Let f : D \u2192 Rd with domain D be an arbitrary d-dimensional function. For 0 < \u03f5 , \u03b4 \u2264 1, the algorithm that adds noise scaled to N ( 0,\n\u22062f,2\u00b72 log(2/\u03b4) \u03f52\n) to each of\nthe d components of f is (\u03f5, \u03b4)-DP.\nk-means and spectral clustering Given a set of n points F1, . . . ,Fn \u2208 Rk, the objective of kmeans problem is to find a k-partition of these points, C = {C1, C2, . . . , Ck}, such that the sum of squared distances between each data point and its assigned cluster center (i.e., the average of all points in the cluster) is minimized. This optimization problem can be formally expressed as\nargminC \u2211k i=1 \u2211 u\u2208Ci \u2225\u2225\u2225u\u2212 1|Ci| \u2211x\u2208Ci x\u2225\u2225\u222522. It is known there exist polynomial time algorithms that approximate the optimum of the k-means within a constant factor (see e.g. Kanungo et al. (2002); Ahmadian et al. (2019)).\nPeng et al. (2015) proved the approximation ratio of spectral clustering when eigenvectors satisfy certain properties. They showed the following lemma, whose proof is sketched in Appendix B.\nLemma 2.4 (Peng et al. (2015)). Let G = (V,E) be a graph and k \u2208 N. Let F : V \u2192 Rk be the embedding defined by F (u) = 1\u221a\ndegG(u) \u00b7 (f1(u), \u00b7 \u00b7 \u00b7 ,fk(u))\u22a4, where {fi}ki=1 is a set\nof orthogonal bases in Rn. Let {Si}ki=1 be a k-partition of G, and {g\u0304i}ki=1 be the normalized indicator vectors of the clusters {Si}ki=1, where g\u0304i(u) = \u221a degG(u) volG(Si)\nif u \u2208 Si, and g\u0304i(u) = 0 otherwise. Suppose there is a threshold \u03b8 \u2264 15k , such that for each i \u2208 [k], there exists a linear combination of the eigenvectors g\u03041, \u00b7 \u00b7 \u00b7 , g\u0304k with coefficients \u03b2(i)j : g\u0302i = \u03b2 (i) 1 g\u03041 + \u00b7 \u00b7 \u00b7+ \u03b2 (i) k g\u0304k, and for each i \u2208 [k], \u2225fi \u2212 g\u0302i\u2225 \u2264 \u03b8. Let KMEANS be any algorithm for the k-means problem in Rk with approximation ratio APT. Let {Ai}ki=1 be a k-partition obtained by invoking KMEANS on the input set {F (u)}u\u2208V . Then, there exists a permutation \u03c3 on {1, . . . , k} such that volG(Ai\u25b3S\u03c3(i)) = O(APT \u00b7 k2 \u00b7 \u03b82)volG(S\u03c3(i)) holds for every i \u2208 [k]."
        },
        {
            "heading": "3 STABILITY OF GENERALIZED STRONGLY CONVEX OPTIMIZATION",
            "text": "The following is a generalization of a result in Chen et al. (2023) whose proof is deferred to Appendix C. It shows that if the objective function of an SDP is generalized strongly convex for some diagonal matrices D1 and D2, then the \u21132-sensitivity of the scaled solution can be bounded by the \u21131-sensitivity of the objective function.\nLemma 3.1 (Stability of strongly-convex optimization). Let Y be a set of databases. Let K(Y) be a family of closed convex subsets of Rm parametrized by Y \u2208 Y and let F(Y) be a family of functions fY : K(Y ) \u2192 R , parametrized by Y \u2208 Y , such that:\n1. for adjacent databases Y, Y \u2032 \u2208 Y , and X \u2208 K(Y ) there exist X\u2032 \u2208 K(Y \u2032) \u2229 K(Y ) satisfying |fY (X)\u2212 fY \u2032(X\u2032)| \u2264 \u03b1 and |fY \u2032(X\u2032)\u2212 fY (X\u2032)| \u2264 \u03b1 .\n2. fY is (\u03ba,D1,D2)-strongly convex in X \u2208 K(Y ) for some diagonal matrices D1 and D2.\nThen for Y, Y \u2032 \u2208 Y , X\u0302 := argminX\u2208K(Y ) fY (X) and X\u0302 \u2032 := argminX\u2032\u2208K(Y \u2032) fY \u2032(X\u2032) , it holds\u2225\u2225\u2225D1X\u0302D2 \u2212D1X\u0302\u2032D2\u2225\u2225\u22252 2 \u2264 12\u03b1 \u03ba ."
        },
        {
            "heading": "4 PRIVATE CLUSTERING FOR WELL-CLUSTERED GRAPHS",
            "text": "In this section, we present our DP algorithm for a well-clustered graph and prove Theorem 1."
        },
        {
            "heading": "4.1 THE ALGORITHM",
            "text": "For a c-balanced (k, \u03d5in, \u03d5out)-clusterable graph G = (V,E) with a ground truth partition {Ci}i\u2208[k], we set b = 1m2 \u2211 i,j\u2208[k],i\u0338=j volG(Ci)volG(Cj) = 1 \u2212 1 2m2 \u2211 i\u2208[k] volG(Ci)\n2. Specifically, if all clusters have the same volume, then b = k\u22121k .\nWe make use of the following SDP to extract the cluster structure of G. Note that our SDP assumes the knowledge of the parameter b, which has also been used in previous work (e.g., Gue\u0301don & Vershynin (2016)).\nmin \u2211\n(u,v)\u2208E\n\u2225u\u0304\u2212 v\u0304\u22252 + 2 \u03bbm \u2211 u,v\u2208V \u27e8u\u0304, v\u0304\u27e92 degG(u) degG(v)\ns.t. \u2211\nu,v\u2208V\n( \u2225u\u0304\u2212 v\u0304\u22252 degG(u) degG(v) ) \u2265 2bm2\n\u27e8u\u0304, v\u0304\u27e9 \u2265 0, for all u, v \u2208 V \u2225u\u0304\u22252 = 1, for all u \u2208 V\n(1)\nLet X be 1n times the Gram matrix of these vector v\u03041, v\u03042, \u00b7 \u00b7 \u00b7 , v\u0304n (i.e., Xi,j = 1 n \u00b7 v\u0304i \u00b7 v\u0304j), LKV be the Laplacian of the complete graph on set V . Let X \u2265 0 denote that all entries of X are non-negative. It is easy to see that the above SDP can be equivalently written as follows:\nmin \u27e8LG,X\u27e9+ n\n\u03bbm \u2225D1/2G XD 1/2 G \u2225 2 F\ns.t. \u27e8DGLKV DG,X\u27e9 \u2265 bm2\nn\nX \u2ab0 0,X \u2265 0,Xii = 1\nn ,\u2200i\n(2)\nDefine a domain D as D := { X \u2208 Rn\u00d7n \u2223\u2223\u2223\u2223 \u27e8DGLKV DG,X\u27e9 \u2265 bm2n ,X \u2ab0 0 ,X \u2265 0 ,Xii = 1n ,\u2200i } .\nThen the optimal solution of this SDP can be expressed as\nX\u0302 := argmin X\u2208D\n\u27e8LG,X\u27e9+ n\n\u03bbm \u2225D1/2G XD 1/2 G \u2225 2 F.\nNow we are ready to describe our algorithm whose pseudo-code is given in Algorithm 1. That is, we first solve the aforementioned SDP to obtain a solution X1 and then we add Gaussian noise to a scaled version of X1, denoted by X2. We then find the first k eigenvectors of X2 and obtain the corresponding spectral embedding {F (u)}u\u2208V and then apply the approximation algorithm KMEANS on the embedding and output the final k partition (of the vertex set V ).\nAlgorithm 1: PrivateClustering(G{V }) Input: Graph G = (V,E), \u03b5, \u03b4,; Output: A k-partition {C\u0302i}i\u2208[k]; 1 X1 := argminX\u2208D\u27e8LG,X\u27e9+ n\u03bbm\u2225D 1/2 G XD 1/2 G \u22252F.\n2 X2 := nD 1/2 G X1D 1/2 G +W , where W \u223c N ( 0, 24 (\u03bb+ 3)m \u00b7 log(2/\u03b4)\u03f52 )n\u00d7n .\n3 Let f1,f2, ...,fk be the k eigenvectors of X2 corresponding to the first k smallest eigenvalues. 4 Let F : V (G) \u2192 Rk, where F (u) = degG(u)\u22121/2(f1(u),f2(u), ...,fk(u))\u22a4. 5 Apply KMEANS(F (u), u \u2208 V ) and let C\u03021, . . . , C\u0302k be the output sets."
        },
        {
            "heading": "4.2 PROOF OF THEOREM 1",
            "text": "Privacy of the algorithm\nLemma 4.1 (Stability). Let f(G,X) = \u27e8LG,X\u27e9 + n\u03bbm \u2225\u2225\u2225D1/2G XD1/2G \u2225\u2225\u22252\nF , and let g(G) =\nnD 1/2 G (argminX\u2208D f(G,X))D 1/2 G . The \u21132-sensitivity \u2206g,2 \u2264\n\u221a 24 (\u03bb+ 3)m.\nProof. For two adjacent graphs G,G\u2032, we have \u2225LG \u2212 LG\u2032\u22251 \u2264 4. And according to the range of X that X \u2ab0 0,Xii = 1n , we have maxi,j Xij \u2264 1 n . Thus, \u2225\u27e8LG,X\u27e9 \u2212 \u27e8LG\u2032 ,X\u27e9\u22251 \u2264 4 n . Without loss of generality, let G\u2032 have one more edge (u\u2217, v\u2217) than G. In this case,\u2225\u2225\u2225\u2225\u2225\u2225\u2225D1/2G XD1/2G \u2225\u2225\u22252 F \u2212 \u2225\u2225\u2225D1/2G\u2032 XD1/2G\u2032 \u2225\u2225\u22252F \u2225\u2225\u2225\u2225 1 = \u2211 u,v X2uv |degG(u) degG(v)\u2212 degG\u2032(u) degG\u2032(v)|\n\u2264 4 n2 \u2211 u/\u2208{u\u2217,v\u2217} degG(u) + 4 n2 (degG(u \u2217) + degG(v \u2217) + 1)\n\u2264 4 n2 \u2211 u degG(u)\u2212 4 n2 (degG(u \u2217) + degG(v \u2217)) + 4 n2 (degG(u \u2217) + degG(v \u2217) + 1)\n\u2264 8m+ 2 n2 \u2264 12m n2\nSo f has \u21131-sensitivity 4n + 12m \u03bbnm with respect to G.\nNext, we prove that f(G,X) is ( 2n\u03bbm ,D 1/2 G ,D 1/2 G )-strongly convex with respect to X . The gradient \u2207f(G,X) = LG + 2n\u03bbmD 1/2 G XD 1/2 G . Let X,X \u2032 \u2208 K then\nf(G,X\u2032) = \u27e8LG,X\u2032\u27e9+ n\n\u03bbm \u2225\u2225\u2225D1/2G X\u2032D1/2G \u2225\u2225\u22252 F\n= \u27e8LG,X\u2032\u27e9+ n\n\u03bbm \u2225\u2225\u2225D1/2G X\u2032D1/2G \u2212D1/2G XD1/2G \u2225\u2225\u22252 F\n\u2212 n \u03bbm \u2225\u2225\u2225D1/2G XD1/2G \u2225\u2225\u22252 F + 2n \u03bbm \u27e8D1/2G XD 1/2 G ,D 1/2 G X \u2032D 1/2 G \u27e9\n= \u27e8LG,X\u27e9+ \u27e8LG,X\u2032 \u2212X\u27e9+ n\n\u03bbm \u2225\u2225\u2225D1/2G XD1/2G \u2225\u2225\u22252 F\n+ 2n\n\u03bbm \u27e8D1/2G XD 1/2 G ,D 1/2 G X \u2032D 1/2 G \u2212D 1/2 G XD 1/2 G \u27e9+\nn\n\u03bbm \u2225\u2225\u2225D1/2G X\u2032D1/2G \u2212D1/2G XD1/2G \u2225\u2225\u22252 F\n\u2265 f(G,X) + \u27e8LG,X\u2032 \u2212X\u27e9+ 2n\n\u03bbm \u27e8D1/2G XD 1/2 G ,X \u2032 \u2212X\u27e9+ n \u03bbm \u2225\u2225\u2225D1/2G X\u2032D1/2G \u2212D1/2G XD1/2G \u2225\u2225\u22252 F\n= f(G,X) + \u27e8\u2207f(G,X),X\u2032 \u2212X\u27e9+ n \u03bbm \u2225\u2225\u2225D1/2G X\u2032D1/2G \u2212D1/2G XD1/2G \u2225\u2225\u22252 F\nThat is f(G,X) is ( 2n\u03bbm ,D 1/2 G ,D 1/2 G )-strongly convex with respect to X .\nBy Lemma 3.1, \u2225\u2225\u2225 g(G)n \u2212 g(G\u2032)n \u2225\u2225\u22252\n2 \u2264 24(\u03bb+3)mn2 . So the \u21132-sensitivity \u2206g,2 \u2264\n\u221a 24 (\u03bb+ 3)m.\nLemma 4.2 (Privacy). The Algorithm 1 is (\u03f5, \u03b4)-DP. Proof. Consider X2 in the algorithm as a function of G. According to Lemma 4.1, we can get that the \u21132-sensitivity of X2 id not greater than \u221a 24(\u03bb+ 3)m. Combining with Lemma 2.3, we get that the algorithm achieves (\u03f5, \u03b4)-DP.\nUtility of the algorithm Lemma 4.3 (Utility). Suppose that \u03d5out\n\u03d52in = O(c2k\u22124), \u03bb \u2208 [\u2126(k4c\u22122\u03d5\u22122in ), O( mc 2\u03f52 nk4 log(2/\u03b4) )]. For any\nc-balanced (k, \u03d5in, \u03d5out)-clusterable graph, let {C\u0302i}ki=1 be a k-partition obtained by the Algorithm 1 (which invokes a k-means algorithm KMEANS with an approximation ratio APT). Then, there exists a permutations \u03c3 on {1, . . . , k} such that\nvolG(C\u0302i\u25b3C\u03c3(i)) \u2264 O ( APT \u00b7 k 4 c2 \u00b7 ( \u03d5out + 1 \u03bb \u03d52in + \u03bb n m log(2/\u03b4) \u03f52 )) volG(C\u03c3(i))\nwith probability 1\u2212 exp(\u2212\u2126(n)).\nNote that Theorem 1 follows by choosing a constant-approximation k-means algorithm KMEANS (so that APT = O(1)) and setting \u03bb = \u221a m\u03f52\nn log(2/\u03b4) and letting m \u2265 n \u00b7 \u03d54in \u03d52out \u00b7 log(2/\u03b4)\u03f52 .\nTo prove Lemma 4.3, we need the following lemma. Lemma 4.4. Consider the SDP (1), if the input graph G = (V,E) is a c-balanced (k, \u03d5in, \u03d5out)clusterable graph, the solution satisfies \u2211 u,v\u2208Ci ( \u2225u\u0304\u2212 v\u0304\u22252 degG(u) degG(v) ) \u2264 mvolG(Ci) \u00b7 \u03d5out + 16 \u03bb \u03d52in , \u2200i \u2208 [k]\n\u2211 u\u2208Ci,v\u2208Cj ,i\u0338=j ( \u2225u\u0304\u2212 v\u0304\u22252 degG(u) degG(v) ) \u2265 m2 ( 2b\u2212 \u03d5out + 16 \u03bb \u03d52in ) .\nProof. As G = (V,E) is a c-balanced (k, \u03d5in, \u03d5out)-clusterable graph, there exists a c-balanced partitioning {Ci}i\u2208[k] of V and, for all i \u2208 [k], \u03a6in(G,Ci) \u2265 \u03d5in and \u03a6out(G,Ci) \u2264 \u03d5out.\nFor 1 \u2264 i \u2264 k, we have \u03a6out(G,Ci) = |E(Ci,V\u2212Ci)|volG(Ci) \u2264 \u03d5out. Summing it yields \u2211k\ni=1 |E(Ci, V \u2212 Ci)| \u2264 \u03d5out \u00b7 \u2211k i=1 volG(Ci) = \u03d5out \u00b7 volG(V ) = \u03d5out\u00b7m 2 . So the number of edges between clusters in G is not greater than \u03d5out\u00b7m4 .\nNow let us consider a feasible solution of the SDP (1). For every vertex u in the \u2113-th cluster, assign unit vector v\u2113 to u\u0304. We can let v1,v2, . . . ,vk be a set of orthogonal bases, as k \u2264 n. For this feasible solution, the value of the objective function is not greater than \u03d5outm4 \u00b7 2+ 2 \u03bbm \u00b7 \u2211 j\u2208[k] volG(Cj)\n2 \u2264 \u03d5outm\n4 \u00b7 2 + 2 \u03bbm \u00b7 volG(G) 2 = 12\u03d5outm+ 8m \u03bb . Thus, for any i \u2208 [k], we have \u2211\n(u,v)\u2208G{Ci}\u2225u\u0304 \u2212 v\u0304\u2225 2 \u2264 12\u03d5outm + 8m \u03bb . Let \u00b5 be the second\neigenvalue of the Laplacian matrix LG{Ci}, we have\n\u00b5 = volG(Ci) min {u\u0304}u\u2208V\n\u2211 (u,v)\u2208G{Ci} ( \u2225u\u0304\u2212 v\u0304\u22252 )\u2211 u,v\u2208Ci (\u2225u\u0304\u2212 v\u0304\u2225 2) degG(u) degG(v) \u2265 \u03d5 2 in 2\nSo \u2211\nu,v\u2208Ci\n( \u2225u\u0304\u2212 v\u0304\u22252 degG(u) degG(v) ) \u2264 volG(Ci) \u00b7 1 2\u03d5outm+ 8m \u03bb\n1 2\u03d5 2 in\n= volG(Ci) \u00b7 \u03d5outm+\n16m \u03bb\n\u03d52in =\nmvolG(Ci) \u00b7 \u03d5out+\n16 \u03bb\n\u03d52in , for all i \u2208 [k]. By the definition of the SDP (1), we know that \u2211 u,v\u2208V ( \u2225u\u0304\u2212 v\u0304\u22252 degG(u) degG(v) ) \u2265 2bm2.\nThus, \u2211 u\u2208Ci,v\u2208Cj ,i\u0338=j ( \u2225u\u0304\u2212 v\u0304\u22252 degG(u) degG(v) ) \u2265 2bm2\u2212 \u2211 i\u2208[k] ( mvolG(Ci) \u00b7 \u03d5out+ 16 \u03bb\n\u03d52in\n) \u2265\n2bm2 \u2212m \u00b7 \u03d5out+ 16 \u03bb\n\u03d52in\n\u2211 i\u2208[k] volG(Ci) = m 2 ( 2b\u2212 \u03d5out+ 16 \u03bb\n\u03d52in\n) .\nLemma 4.5. Consider the setting in Theorem 1 and Algorithm 1, with probability 1\u2212 exp(\u2212\u2126(n)),\n\u2225X2 \u2212Z\u2225 \u2264 2m \u00b7 \u221a\u03d5out + 16\u03bb \u03d52in + 3 \u221a 6(\u03bb+ 3)n m log(2/\u03b4) \u03f52  where Z is a matrix defined as: Zuv = \u221a degG(u) degG(v), if u, v are in same cluster; Zuv = 0 if u, v are in different clusters.\nProof. According to Lemma 4.4, for u, v in same cluster Ci :\n\u2211 u,v\u2208Ci ( n \u221a degG(u) degG(v)[X1]uv \u2212Zuv )2 = \u2211 u,v\u2208Ci ( (n[X1]uv \u2212 1)2 degG(u) degG(v) ) \u2264 2\n\u2211 u,v\u2208Ci (|n[X1]uv \u2212 1|degG(u) degG(v)) = \u2211 u,v\u2208Ci ( \u2225u\u0304\u2212 v\u0304\u22252 degG(u) degG(v) ) \u2264 mvolG(Ci) \u00b7 \u03d5out + 16 \u03bb\n\u03d52in\nFor u, v in different clusters :\u2211 u\u2208Ci,v\u2208Cj\ni \u0338=j\n( n \u221a degG(u) degG(v)[X1]uv \u2212Zuv )2 = \u2211 u\u2208Ci,v\u2208Cj\ni \u0338=j\n( (n[X1]uv) 2 degG(u) degG(v) )\n\u2264 \u2211\nu\u2208Ci,v\u2208Cj i \u0338=j\n(|n[X1]uv|degG(u) degG(v)) = 1\n2 \u2211 u\u2208Ci,v\u2208Cj\ni \u0338=j\n( 2\u2212 \u2225u\u0304\u2212 v\u0304\u22252 ) degG(u) degG(v)\n= bm2 \u2212 1 2 \u2211 u\u2208Ci,v\u2208Cj\ni \u0338=j\n( \u2225u\u0304\u2212 v\u0304\u22252 degG(u) degG(v) ) \u2264 m 2\n2 \u00b7 \u03d5out +\n16 \u03bb\n\u03d52in\nCombing these, we have: \u2225\u2225\u2225nD1/2G X1D1/2G \u2212Z\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225nD1/2G X1D1/2G \u2212Z\u2225\u2225\u2225\nF\n= \u221a\u2211 u,v ( n \u221a degG(u) degG(v)[X1]uv \u2212Zuv )2 \u2264 2m \u00b7 \u221a \u03d5out+ 16 \u03bb\n\u03d52in .\nAlgorithm 1 uses W \u223c N ( 0, 24 (\u03bb+ 3)m \u00b7 log(2/\u03b4)\u03f52 )n\u00d7n , and we choose t = \u221a n in Lemma A.1,\nso with probability 1 \u2212 exp(\u2212\u2126(n)), \u2225X2 \u2212Z\u2225 \u2264 \u2225W \u2225 + \u2225\u2225\u2225nD1/2G X1D1/2G \u2212Z\u2225\u2225\u2225 \u2264 3\u221an \u00b7\u221a\n24 (\u03bb+ 3)m \u00b7 log(2/\u03b4)\u03f52 + 2m \u00b7 \u221a \u03d5out+ 16 \u03bb\n\u03d52in = 2m \u00b7\n(\u221a \u03d5out+ 16 \u03bb\n\u03d52in + 3\n\u221a 6(\u03bb+3)n\nm log(2/\u03b4) \u03f52\n) .\nProof of Lemma 4.3. Let \u03b3 = \u221a \u03d5out+ 16 \u03bb\n\u03d52in + 3\n\u221a 6(\u03bb+3)n\nm log(2/\u03b4) \u03f52 . According to Lemma 4.5, with\nprobability 1\u2212 exp(\u2212\u2126(n)), it holds that \u2225X2 \u2212Z\u2225 \u2264 2\u03b3m. We denote the eigenvalues of the matrix X2 by \u00b51 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u00b5n, with their corresponding orthonormal eigenvectors f1, \u00b7 \u00b7 \u00b7 ,fn. We denote the eigenvalues of the matrix Z by \u03bd1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bdn, with their corresponding orthonormal eigenvectors g1, \u00b7 \u00b7 \u00b7 , gn. Let Y = [f1, \u00b7 \u00b7 \u00b7 ,fn],Q = [g1, \u00b7 \u00b7 \u00b7 , gn], and let A = Diag(\u00b51, \u00b7 \u00b7 \u00b7 , \u00b5n),\u039b = Diag(\u03bd1, \u00b7 \u00b7 \u00b7 , \u03bdn). Then X2 = Y AY \u22a4,Z = Q\u039bQ\u22a4 are the eigen-decompositions of X2,Z, respectively. As {f1, \u00b7 \u00b7 \u00b7 ,fn} is a set of orthogonal bases in Rn, for every i \u2208 [n], fi is the linear combination of eigenvector g1, \u00b7 \u00b7 \u00b7 , gn. We write fi as fi = \u03b2 (i) 1 g1 + \u00b7 \u00b7 \u00b7 + \u03b2 (i) n gn, where \u03b2 (i) j \u2208 R. By the definition of Z, we know that Z is composed of k rank-1 matrices of sizes |C1|, |C2|, \u00b7 \u00b7 \u00b7 , |Ck|, respectively. Let these matrices be M1,M2, \u00b7 \u00b7 \u00b7 ,Mk such that Mi \u2208 R|Ci|\u00d7|Ci|, for each i \u2208 [k]. For each j \u2208 [k], note that the eigenvalues of Mj are volG(Cj), 0, \u00b7 \u00b7 \u00b7 , 0, where the multiplicities of 0 is |Cj | \u2212 1. So we have \u03bdk+1 = \u00b7 \u00b7 \u00b7 = \u03bdn = 0, and \u03bd1, \u00b7 \u00b7 \u00b7 , \u03bdk are equal to volG(C1), \u00b7 \u00b7 \u00b7 , volG(Ck), respectively, and \u03bdk = mini\u2208[k] volG(Ci) \u2265 2cmk .\nBy Lemma A.2, we have \u00b5k \u2265 \u03bdk \u2212 \u2225X2 \u2212Z\u2225 \u2265 2m ( c k \u2212 \u03b3 ) .\nWe apply Theorem A.3 with H = X2, E0 = Y[k], E1 = Y\u2212[k], A0 = A[k], A1 = A\u2212[k], and H\u0303 = Z, F0 = Q[k], F1 = Q\u2212[k], \u039b0 = \u039b[k], \u039b1 = \u039b\u2212[k], \u03b7 = |\u00b5k \u2212 \u03bdk+1| = |\u00b5k| \u2265 2m ( c k \u2212 \u03b3 ) . Therefore, by Theorem A.3 we have\n\u2225Q\u22a4\u2212[k]Y[k]\u2225 = \u2225F \u22a4 1 E0\u2225 \u2264 \u2225F\u22a41 (Z \u2212X2)E0\u2225 \u03b7 \u2264 \u2225X2 \u2212Z\u2225 \u03b7 \u2264 \u03b3k c\u2212 \u03b3k .\nThus we have \u2211n\nj=k+1 ( \u03b2 (i) j )2 = \u2225Q\u22a4\u2212[k]fi\u2225 2 2 \u2264 \u2225Q\u22a4\u2212[k]Y[k]\u2225 2 \u2264 \u03b3 2k2 (c\u2212\u03b3k)2 , for all i \u2208 [k].\nFor all i \u2208 [k], let g\u0302i = \u03b2(i)1 g1 + \u00b7 \u00b7 \u00b7+ \u03b2 (i) k gn, then \u2225fi \u2212 g\u0302i\u222522 = \u2211n j=k+1 ( \u03b2 (i) j )2 \u2264 \u03b3 2k2 (c\u2212\u03b3k)2 .\nNote that our Algorithm 1 invokes KMEANS algorithm on the input set the input set {F (u)}u\u2208V , where F (u) = degG(u) \u22121/2(f1(u),f2(u), ...,fk(u)) \u22a4. By Lemma 2.4, we know that the\noutput partition {C\u0302i}i\u2208[k] satisfies: volG(C\u0302i\u25b3C\u03c3(i)) \u2264 APT \u00b7 k2 \u03b3 2k2 (c\u2212\u03b3k)2 volG(C\u03c3(i)) = O ( APT \u00b7 k 4 c2 \u00b7 ( \u03d5out+ 1 \u03bb \u03d52in + \u03bb nm log(2/\u03b4) \u03f52 )) volG(C\u03c3(i)), for some permutation \u03c3 on {1, . . . , k}."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "To evaluate the empirical trade-off between privacy and utility of our algorithm, we perform exemplary experiments on synthetic datasets sampled from SBMs. As a baseline, we compare our algorithm to an approach based on randomized response as described in Mohamed et al. (2022). The algorithm based on randomized response generates a noisy output by flipping each bit of the adjacency matrix with some probability pRR (for undirected graphs, it flips a single coin for opposing directed edges). It was shown that randomized response is \u03f5-DP for pRR \u2265 1/(1 + e\u03f5). Since randomized response is an \u03f5-DP algorithm, we are interested in the utility improvement that we can gain by using (\u03f5, \u03b4)-DP when the utility of randomized response becomes insufficient. Our hypothesis is that when the noise added to the adjacency matrix by randomized response is balancing with its signal, the output of Algorithm 1 still has significant utility.\nImplementation. We model the SDP (2) in CVXPY 1.3.2 and use SCS 3.2.3 as SDP solver. We use NumPy 1.23.5 for numerical computation and scikit-learn 1.3 for an implementation of k-means++. For our algorithm, we use b = (k\u22121)/k and \u03bb = c \u00b7 \u221a m\u03f52\nn log(2/\u03b4) , where c is a trade-off constant that we fix for each SBM parameterization before sampling the input datasets. For randomized response, we replace the objective with argminX\u2208D\u27e8LG,X\u27e9, i.e., we remove the regularizer term, and set pRR = 1/(1 + e \u03f5).\nDatasets and setup. We sample datasets from a stochastic block model SBM(n, k, p, q) with k blocks, each of size n/k, intra-cluster edge probability p and inter-cluster edge probability q. We consider the sampled graphs as instances of well-clustered graphs. For our experiments, we use \u03f5 = 1 and \u03b4 = 1/n2. Since pRR = 1/(1 + e) \u2248 0.27, we choose small values of p, q so that |p \u2212 q| = 0.2. For each parameterization, we sample 10 SBM graphs and run each algorithm 100 times to boost the statistical stability of the evaluation. We report the median over the repetitions, over the datasets, of the adjusted and the normalized mutual information between the ground truth and the clustering reported by the algorithm.\nEvaluation. The evaluation of the comparison between RR+SDP and Algorithm 1 is shown in Table 1. From the results, we see that the noisy adjacency matrix output by randomized response obfuscates most of the signal from the original adjacency matrix so that the solution of the SDP has low utility. On the other hand, we see that the regularizer term in the SDP and the noise added to the solution recovered significantly more information from the ground truth: Using Algorithm 1 instead of randomized response can lead to (\u03f5, \u03b4)-differentially private solution with significantly improved quality, and consistently did so in our experiments."
        },
        {
            "heading": "A USEFUL TOOLS",
            "text": "The spectral norm of a Gaussian matrix has a high probability upper bound. The following lemma illustrates this fact.\nLemma A.1 (Concentration of spectral norm of Gaussian matrices). Let W \u223c N (0, 1)m\u00d7n. Then for any t, we have\nPr (\u221a m\u2212 \u221a n\u2212 t \u2264 \u03c3min(W ) \u2264 \u03c3max(W ) \u2264 \u221a m+ \u221a n+ t ) \u2265 1\u2212 2 exp ( \u2212 t 2\n2\n) ,\nwhere \u03c3min(\u00b7) and \u03c3max(\u00b7) denote the minimum and the maximum singular values of a matrix, respectively.\nLet W \u2032 be an n-by-n symmetric matrix with independent entries sampled from N (0, \u03c32). Then by the above fact, \u2225bsW \u2032\u22252 \u2264 3\u03c3 \u221a n with probability at least 1\u2212 exp(\u2212\u2126(n)).\nWhen a matrix undergoes a slight perturbation under some conditions, its eigenvalues and eigenvectors do not experience significant changes. Lemma A.2 and Lemma A.3 illustrate this.\nLemma A.2 (Weyl\u2019s inequality). Let A and B be symmetric matrices. Let R = A \u2212 B. Let \u03b11 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b1n be the eigenvalues of A. Let \u03b21 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b2n be the eigenvalues of B. Then for each i \u2208 [n],\n|\u03b1i \u2212 \u03b2i| \u2264 \u2225R\u22252 .\nLemma A.3 (Davis-Kahan sin(\u03b8)-Theorem (Davis & Kahan, 1970)). Let H = E0A0E\u22a40 + E1A1E \u22a4 1 and H\u0303 = F0\u039b0F \u22a4 0 + F1\u039b1F \u22a4 1 be symmetric real-valued matrices with E0,E1 and F0,F1 orthogonal. If the eigenvalues of A0 are contained in an interval (a, b), and the eigenvalues of \u039b1 are excluded from the interval (a\u2212 \u03b7, b+ \u03b7)for some \u03b7 > 0, then for any unitarily invariant norm \u2225.\u2225\n\u2225F\u22a41 E0\u2225 \u2264 \u2225F\u22a41 (H\u0303 \u2212H)E0\u2225\n\u03b7 ."
        },
        {
            "heading": "B DEFERRED PROOFS FROM SECTION 2",
            "text": "In this section, we give a sketch of the proof of Lemma 2.4 restarted below.\nLemma B.1 (Peng et al. (2015)). Let G = (V,E) be a graph and k \u2208 N. Let F : V \u2192 Rk be the embedding defined by F (u) = 1\u221a\ndegG(u) \u00b7 (f1(u), \u00b7 \u00b7 \u00b7 ,fk(u))\u22a4, where {fi}ki=1 is a set\nof orthogonal bases in Rn. Let {Si}ki=1 be a k-partition of G, and {g\u0304i}ki=1 be the normalized indicator vectors of the clusters {Si}ki=1, where g\u0304i(u) = \u221a degG(u) volG(Si)\nif u \u2208 Si, and g\u0304i(u) = 0 otherwise. Suppose there is a threshold \u03b8 \u2264 15k , such that for each i \u2208 [k], there exists a linear combination of the eigenvectors g\u03041, \u00b7 \u00b7 \u00b7 , g\u0304k with coefficients \u03b2(i)j : g\u0302i = \u03b2 (i) 1 g\u03041 + \u00b7 \u00b7 \u00b7+ \u03b2 (i) k g\u0304k, and for each i \u2208 [k], \u2225fi \u2212 g\u0302i\u2225 \u2264 \u03b8. Let KMEANS be any algorithm for the k-means problem in Rk with approximation ratio APT. Let {Ai}ki=1 be a k-partition obtained by invoking KMEANS on the input set {F (u)}u\u2208V . Then, there exists a permutation \u03c3 on {1, . . . , k} such that volG(Ai\u25b3S\u03c3(i)) = O(APT \u00b7 k2 \u00b7 \u03b82)volG(S\u03c3(i)) holds for every i \u2208 [k].\nProof sketch. The following five properties given in Lemma B.2 are five key components proven in Peng et al. (2015). They start by demonstrating the first property based on the existing conditions. Then, they provide k centers for spectral embeddings and sequentially prove that all embedded points concentrate around their corresponding centers, the magnitudes of center vectors, and the distances. Finally, based on these properties, they establish the fifth one. With this fifth property, we can directly employ a proof by contradiction to derive our conclusion.\nLemma B.2. Consider the setting in in Lemma 2.4, let \u03b6 \u225c 1 10 \u221a k , and let p(i) \u225c\n1\u221a vol(Si)\n( \u03b2 (1) i , . . . , \u03b2 (k) i )\u22a4 for i \u2208 [k], the following statements hold:\n1. For any \u2113 \u0338= j, there exists i \u2208 [k] such that\u2223\u2223\u2223\u03b2(i)\u2113 \u2212 \u03b2(i)j \u2223\u2223\u2223 \u2265 \u03b6 \u225c 1 10 \u221a k\n2. All embedded points are concentrated around p(i):\nk\u2211 i=1 \u2211 u\u2208Si deg(u) \u2225\u2225\u2225F (u)\u2212 p(i)\u2225\u2225\u22252 2 \u2264 k\u03b82.\n3. For every i \u2208 [k] that\n99\n100vol(Si) \u2264 \u2225\u2225\u2225p(i)\u2225\u2225\u22252 \u2264 101 100vol(Si) .\n4. For every i \u0338= j, i \u2208 [k], it holds that\u2225\u2225\u2225p(i) \u2212 p(j)\u2225\u2225\u22252 2 \u2265 \u03b6 2 10min {vol(Si), vol(Sj)} ,\n5. Suppose that, for every permutations \u03c3 on {1, . . . , k}, there exists i such that vol ( Ai\u25b3S\u03c3(i) ) \u2265 2\u03f5vol ( S\u03c3(i) ) for \u03f5 \u2265 105\u00b7k2\u03b82, then COST(A1, . . . , Ak) \u2265 10\u22124\u00b7\u03f5/k."
        },
        {
            "heading": "C DEFERRED PROOFS FROM SECTION 3",
            "text": "Lemma C.1 (See e.g. (Chen et al., 2023)). Let f : Rm \u2192 R be a convex function. Let K \u2286 Rm be a convex set. Then y\u2217 \u2208 K is a minimizer of f over K if and only if there exists a subgradient g \u2208 \u2202f(y\u2217) such that \u27e8y \u2212 y\u2217, g\u27e9 \u2265 0 \u2200y \u2208 K. Definition C.2 (Generalized strongly convex function). Let K \u2286 Rm\u00d7n be a convex set, f : K \u2192 R be a function, and D1,D2 be diagonal matrices. The function f is called (\u03ba,D1,D2)-strongly convex if the following inequality holds for all X,X\u2032 \u2208 K:\nf(X\u2032) \u2265 f(X) + \u27e8X\u2032 \u2212X,\u2207f(X)\u27e9+ \u03ba 2 \u2225D1X\u2032D2 \u2212D1XD2\u2225 2\nLemma C.3 (Pythagorean theorem from strong convexity). Let K \u2286 Rm\u00d7n be a convex set, f : K \u2192 R be a function. Suppose f is (\u03ba,D1,D2)-strongly convex for some diagonal matrices D1 and D2. Let X\u2217 \u2208 K be a minimizer of f . Then for any X \u2208 K, one has\n\u2225D1XD2 \u2212D1X\u2217D2\u22252 \u2264 2\n\u03ba (f(X)\u2212 f(X\u2217)).\nProof. By the definition of (\u03ba,D1,D2)-strongly convexity,\nf(X) \u2265 f(X\u2217) + \u27e8X \u2212X\u2217,\u2207f(X\u2217)\u27e9+ \u03ba 2 \u2225D1XD2 \u2212D1X\u2217D2\u22252 .\nBy Lemma C.1, \u27e8X \u2212X\u2217,\u2207f(X\u2217)\u27e9 \u2265 0. Then the result follows.\nProof of Lemma 3.1. The proof follows from Lemma C.3 and the proof of Lemma 4.1 in (Chen et al., 2023)."
        },
        {
            "heading": "D LOWER BOUND",
            "text": "In this section, we show that approximation algorithms for recovering the clusters of well-clustered, sparse graphs cannot satisfy pure \u03f5-DP for small error. Given a cluster membership vector u \u2208 {\u22121, 1}n of a graph G = (V,E) that assigns each vertex to one of two clusters (labeled \u22121 and 1), and given a ground truth vector uG \u2208 {\u22121, 1}n, we define the misclassification rate err(u,uG) = errG(u) = (n\u2212min(\u27e8u,uG\u27e9, \u27e8\u2212u,uG\u27e9) /(2n). In other words, the misclassification rate is the minimum number of assignment changes that are required to turn uG into one of u and \u2212u. It is known that err is a semimetric. Lemma D.1 (Lemma 5.25, Chen et al. (2023)). For any u,v \u2208 {\u22121, 1}n, err is a semimetric. Theorem 3. For \u03d5in, \u03d5out \u2208 [0, 1], let G be a (2, \u03d5in, \u03d5out)-clusterable graph, and let \u03b7 < 1/2. Then, any approximate algorithm with failure probability \u03b7 and misclassification rate \u03b6 cannot satisfy \u03f5-DP for \u03f5 < 2 ln(1/(9e\u03b6)/d on d-regular graphs.\nProof. We follow a packing argument by Chen et al. (2023). Consider the set S = {p \u2208 {\u22121, 1}n | 1\u22a4p = 0} equipped with the semimetric err. Let G be a balanced, d-regular, (2, \u03d5in, \u03d5out)-clusterable graph with cluster membership vector xG \u2208 S. Let B = B(xG, 8\u03b6). Let P = {xG,x1, . . . ,xp} be a maximal 2\u03b6-packing of B. For every xi \u2208 P , there exists a (2, \u03d5in, \u03d5out)-clusterable graph with cluster membership vector xi and errG(xi) \u2264 6\u03b6dn: Since xi is balanced, the number of vertices j such that (xi)j = \u22121 and (xG)j = 1 is equal to the number\nof vertices j\u2032 such that (xi)j\u2032 = 1 and (xG)j\u2032 = \u22121. Consider a perfect matching between these two sets of vertices and, for each matched pair, swap their neighbors in G to obtain Hxi .\nBy the maximality of P , we have that B(xG, 6\u03b6) \\ \u222ai(B(xi, 4\u03b6)) = \u2205. Otherwise, we could extend P by xp+1, where xp+1 is an element of the non-empty difference. This would contradict the maximality of P . Therefore, it follows that\np\u2211 i=1 |B(xi, 4\u03b6)| \u2265 |B(xG, 6\u03b6)| \u21d2 p \u2265 |B(xG, 6\u03b6)| |B(xG, 4\u03b6)|\n\u2265 ( n/2 3\u03b6n )2( n/2 2\u03b6n )2 \u2265 ( 16\u03b6 )6\u03b6n( e4\u03b6 )4\u03b6n \u2265 ( 2 18e\u03b6 )2\u03b6n .\nBy group privacy and an averaging argument, there exists i \u2208 [p] so that\nPr[A(Hxi) \u2286 B(xi, \u03b6)] \u2264 exp(\u03f5\u03b6nd) Pr[A(GxG) \u2286 B(xi, \u03b6)] \u2264 exp(\u03f5\u03b6nd) \u03b7\np .\nNow, assume that there is an \u03b6-approximate, \u03f5-DP algorithm with failure probability \u03b7. By assumption, 1\u2212 \u03b7 \u2264 Pr[A(Hxi) \u2286 B(xi, \u03b6)]. Rearranging, we obtain\n2 ln (\n1 9e\u03b6 ) d \u2264 2\u03b6n ln ( 2 18e\u03b6 ) \u03b6dn \u2264 ln(1\u2212 \u03b7) + ln ( ( 218e\u03b6 ) 2\u03b6n ) \u2212 ln(\u03b7) \u03b6dn \u2264 \u03f5."
        }
    ],
    "title": "A DIFFERENTIALLY PRIVATE CLUSTERING ALGO-",
    "year": 2023
}