{
    "abstractText": "How does scaling the number of parameters in large language models (LLMs) affect their core capabilities? We study two natural scaling techniques \u2014 weight pruning and simply training a smaller or larger model, which we refer to as dense scaling \u2014 and their effects on two core capabilities of LLMs: (a) recalling facts presented during pre-training and (b) processing information presented in-context during inference. By curating a suite of tasks that help disentangle these two capabilities, we find a striking difference in how these two abilities evolve due to scaling. Reducing the model size by more than 30% (via either scaling approach) significantly decreases the ability to recall facts seen in pre-training. Yet, a 60\u201370% reduction largely preserves the various ways the model can process in-context information, ranging from retrieving answers from a long context to learning parameterized functions from in-context exemplars. The fact that both dense scaling and weight pruning exhibit this behavior suggests that scaling model size has an inherently disparate effect on fact recall and in-context learning.",
    "authors": [],
    "id": "SP:abbf7a32254be48c544892cd5cd103fd99460058",
    "references": [
        {
            "authors": [
                "Ekin Aky\u00fcrek",
                "Dale Schuurmans",
                "Jacob Andreas",
                "Tengyu Ma",
                "Denny Zhou"
            ],
            "title": "What learning algorithm is in-context learning? investigations with linear models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Berant",
                "Andrew Chou",
                "Roy Frostig",
                "Percy Liang"
            ],
            "title": "Semantic parsing on Freebase from question-answer pairs",
            "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2013
        },
        {
            "authors": [
                "Stella Biderman",
                "USVSN Sai Prashanth",
                "Lintang Sutawika",
                "Hailey Schoelkopf",
                "Quentin Anthony",
                "Shivanshu Purohit",
                "Edward Raff"
            ],
            "title": "Emergent and predictable memorization in large language models, 2023a",
            "year": 2023
        },
        {
            "authors": [
                "Stella Biderman",
                "Hailey Schoelkopf",
                "Quentin Anthony",
                "Herbie Bradley",
                "Kyle O\u2019Brien",
                "Eric Hallahan",
                "Mohammad Aflah Khan",
                "Shivanshu Purohit",
                "USVSN Sai Prashanth",
                "Edward Raff",
                "Aviya Skowron",
                "Lintang Sutawika",
                "Oskar van der Wal"
            ],
            "title": "Pythia: A suite for analyzing large language models across training and scaling, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Steven Bills",
                "Nick Cammarata",
                "Dan Mossing",
                "Henk Tillman",
                "Leo Gao",
                "Gabriel Goh",
                "Ilya Sutskever",
                "Jan Leike",
                "Jeff Wu",
                "William Saunders"
            ],
            "title": "Language models can explain neurons in language models",
            "venue": "https://openaipublic.blob.core.windows.net/neuron-explainer/ paper/index.html,",
            "year": 2023
        },
        {
            "authors": [
                "Laurent Sifre"
            ],
            "title": "Improving language models by retrieving from trillions of tokens",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners, 2020b",
            "year": 2020
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg",
                "Harsha Nori",
                "Hamid Palangi",
                "Marco Tulio Ribeiro",
                "Yi Zhang"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Stephanie C.Y. Chan",
                "Ishita Dasgupta",
                "Junkyung Kim",
                "Dharshan Kumaran",
                "Andrew K. Lampinen",
                "Felix Hill"
            ],
            "title": "Transformers generalize differently from information stored in context vs in weights",
            "venue": "2022a. doi: 10.48550/arXiv.2210.05675. URL https://doi.org/10.48550/",
            "year": 2022
        },
        {
            "authors": [
                "Stephanie C.Y. Chan",
                "Adam Santoro",
                "Andrew K. Lampinen",
                "Jane X. Wang",
                "Aaditya Singh",
                "Pierre H. Richemond",
                "Jay McClelland",
                "Felix Hill"
            ],
            "title": "Data distributional properties drive emergent incontext learning in transformers, 2022b",
            "year": 2022
        },
        {
            "authors": [
                "Lingjiao Chen",
                "Matei Zaharia",
                "James Zou"
            ],
            "title": "Frugalgpt: How to use large language models while reducing cost and improving performance, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Damai Dai",
                "Li Dong",
                "Y. Hao",
                "Zhifang Sui",
                "Furu Wei"
            ],
            "title": "Knowledge neurons in pretrained",
            "venue": "transformers. ArXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Tim Dettmers",
                "Ruslan Svirschevski",
                "Vage Egiazarian",
                "Denis Kuznedelev",
                "Elias Frantar",
                "Saleh Ashkboos",
                "Alexander Borzunov",
                "Torsten Hoefler",
                "Dan Alistarh"
            ],
            "title": "Spqr: A sparse-quantized representation for near-lossless llm weight compression, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "year": 2019
        },
        {
            "authors": [
                "David Dohan",
                "Winnie Xu",
                "Aitor Lewkowycz",
                "Jacob Austin",
                "David Bieber",
                "Raphael Gontijo Lopes",
                "Yuhuai Wu",
                "Henryk Michalewski",
                "Rif A. Saurous",
                "Jascha Sohl-dickstein",
                "Kevin Murphy",
                "Charles Sutton"
            ],
            "title": "Language model cascades, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Xin Dong",
                "Shangyu Chen",
                "Sinno Pan"
            ],
            "title": "Learning to prune deep neural networks via layer-wise optimal brain surgeon",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Xin Dong",
                "Shangyu Chen",
                "Sinno Pan"
            ],
            "title": "Learning to prune deep neural networks via layer-wise optimal brain surgeon",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin"
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Gintare Karolina Dziugaite",
                "Daniel Roy",
                "Michael Carbin"
            ],
            "title": "Pruning neural networks at initialization: Why are we missing the mark",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Elias Frantar",
                "Dan Alistarh"
            ],
            "title": "Sparsegpt: Massive language models can be accurately pruned in one-shot",
            "year": 2023
        },
        {
            "authors": [
                "Elias Frantar",
                "Eldar Kurtic",
                "Dan Alistarh"
            ],
            "title": "M-fac: Efficient matrix-free approximations of secondorder information",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Elias Frantar",
                "Saleh Ashkboos",
                "Torsten Hoefler",
                "Dan Alistarh"
            ],
            "title": "Gptq: Accurate post-training quantization for generative pre-trained transformers, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Leo Gao",
                "Jonathan Tow",
                "Stella Biderman",
                "Sid Black",
                "Anthony DiPofi",
                "Charles Foster",
                "Laurence Golding",
                "Jeffrey Hsu",
                "Kyle McDonell",
                "Niklas Muennighoff",
                "Jason Phang",
                "Laria Reynolds",
                "Eric Tang",
                "Anish Thite",
                "Ben Wang",
                "Kevin Wang",
                "Andy Zou"
            ],
            "title": "A framework for few-shot language model evaluation",
            "venue": "URL https://doi.org/10.5281/zenodo.5371628",
            "year": 2021
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Ming-Wei Chang"
            ],
            "title": "Realm: Retrievalaugmented language model pre-training",
            "venue": "In Proc. International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Song Han",
                "Huizi Mao",
                "William J. Dally"
            ],
            "title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding",
            "venue": "In International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "B. Hassibi",
                "D.G. Stork",
                "G.J. Wolff"
            ],
            "title": "Optimal brain surgeon and general network pruning",
            "venue": "In IEEE International Conference on Neural Networks,",
            "year": 1993
        },
        {
            "authors": [
                "Jordan Hoffmann",
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Elena Buchatskaya",
                "Trevor Cai",
                "Eliza Rutherford",
                "Diego de Las Casas",
                "Lisa Anne Hendricks",
                "Johannes Welbl",
                "Aidan Clark",
                "Tom Hennigan",
                "Eric Noland",
                "Katie Millican",
                "George van den Driessche",
                "Bogdan Damoc",
                "Aurelia Guy",
                "Simon Osindero",
                "Karen Simonyan",
                "Erich Elsen",
                "Jack W. Rae",
                "Oriol Vinyals",
                "Laurent Sifre"
            ],
            "title": "Training compute-optimal large language models, 2022b",
            "year": 2022
        },
        {
            "authors": [
                "Sara Hooker",
                "Aaron C. Courville",
                "Yann N. Dauphin",
                "Andrea Frome"
            ],
            "title": "What do compressed deep neural networks forget",
            "year": 2019
        },
        {
            "authors": [
                "Sara Hooker",
                "Nyalleng Moorosi",
                "Gregory Clark",
                "Samy Bengio",
                "Emily Denton"
            ],
            "title": "Characterising bias in compressed models, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Ajay Jaiswal",
                "Zhe Gan",
                "Xianzhi Du",
                "Bowen Zhang",
                "Zhangyang Wang",
                "Yinfei Yang"
            ],
            "title": "Compressing llms: The truth is rarely pure and never simple, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Ting Jiang",
                "Deqing Wang",
                "Fuzhen Zhuang"
            ],
            "title": "Pruning pre-trained language models without fine-tuning, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Tian Jin",
                "Michael Carbin",
                "Dan Roy",
                "Jonathan Frankle",
                "Gintare Karolina Dziugaite"
            ],
            "title": "Pruning\u2019s effect on generalization through the lens of training and regularization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Mandar Joshi",
                "Eunsol Choi",
                "Daniel S. Weld",
                "Luke Zettlemoyer"
            ],
            "title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
            "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2017
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B. Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "venue": "CoRR, abs/2001.08361,",
            "year": 2020
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B. Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models, 2020b",
            "year": 2020
        },
        {
            "authors": [
                "Eldar Kurtic",
                "Elias Frantar",
                "Dan Alistarh"
            ],
            "title": "Ziplm: Hardware-aware structured pruning of language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Tom Kwiatkowski",
                "Jennimaria Palomaki",
                "Olivia Redfield",
                "Michael Collins",
                "Ankur Parikh",
                "Chris Alberti",
                "Danielle Epstein",
                "Illia Polosukhin",
                "Matthew Kelcey",
                "Jacob Devlin",
                "Kenton Lee",
                "Kristina N. Toutanova",
                "Llion Jones",
                "Ming-Wei Chang",
                "Andrew Dai",
                "Jakob Uszkoreit",
                "Quoc Le",
                "Slav Petrov"
            ],
            "title": "Natural questions: a benchmark for question answering research",
            "venue": "Transactions of the Association of Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Yann LeCun",
                "John Denker",
                "Sara Solla"
            ],
            "title": "Optimal brain damage",
            "venue": "In Conference on Neural Information Processing Systems,",
            "year": 1990
        },
        {
            "authors": [
                "Daliang Li",
                "Ankit Singh Rawat",
                "Manzil Zaheer",
                "Xin Wang",
                "Michal Lukasik",
                "Andreas Veit",
                "Felix X. Yu",
                "Sanjiv Kumar"
            ],
            "title": "Large language models with controllable working memory",
            "year": 2022
        },
        {
            "authors": [
                "Nathan Kim",
                "Neel Guha",
                "Niladri Chatterji",
                "Omar Khattab",
                "Peter Henderson",
                "Qian Huang",
                "Ryan Chi",
                "Sang Michael Xie",
                "Shibani Santurkar",
                "Surya Ganguli",
                "Tatsunori Hashimoto",
                "Thomas Icard",
                "Tianyi Zhang",
                "Vishrav Chaudhary",
                "William Wang",
                "Xuechen Li",
                "Yifan Mai",
                "Yuhui Zhang",
                "Yuta Koreeda"
            ],
            "title": "Holistic evaluation of language models, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Lucas Liebenwein",
                "Cenk Baykal",
                "Brandon Carter",
                "David Gifford",
                "Daniela Rus"
            ],
            "title": "Lost in pruning: The effects of pruning neural networks beyond test accuracy",
            "venue": "In Conference on Machine Learning and Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tiedong Liu",
                "Bryan Kian Hsiang Low"
            ],
            "title": "Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Zhuang Liu",
                "Jianguo Li",
                "Zhiqiang Shen",
                "Gao Huang",
                "Shoumeng Yan",
                "Changshui Zhang"
            ],
            "title": "Learning efficient convolutional networks through network slimming",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2017
        },
        {
            "authors": [
                "Shayne Longpre",
                "Kartik Perisetla",
                "Anthony Chen",
                "Nikhil Ramesh",
                "Chris DuBois",
                "Sameer Singh"
            ],
            "title": "Entity-based knowledge conflicts in question answering",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Xinyin Ma",
                "Gongfan Fang",
                "Xinchao Wang"
            ],
            "title": "Llm-pruner: On the structural pruning of large language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Kevin Meng",
                "David Bau",
                "Alex Andonian",
                "Yonatan Belinkov"
            ],
            "title": "Locating and editing factual associations in gpt",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Pavlo Molchanov",
                "Stephen Tyree",
                "Tero Karras",
                "Timo Aila",
                "Jan Kautz"
            ],
            "title": "Pruning convolutional neural networks for resource efficient inference",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Michael C Mozer",
                "Paul Smolensky"
            ],
            "title": "Skeletonization: A technique for trimming the fat from a network via relevance assessment",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 1988
        },
        {
            "authors": [
                "Ella Neeman",
                "Roee Aharoni",
                "Or Honovich",
                "Leshem Choshen",
                "Idan Szpektor",
                "Omri Abend"
            ],
            "title": "Disentqa: Disentangling parametric and contextual knowledge with counterfactual question answering, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Alex Renda",
                "Jonathan Frankle",
                "Michael Carbin"
            ],
            "title": "Comparing rewinding and fine-tuning in neural network pruning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Michael Santacroce",
                "Zixin Wen",
                "Yelong Shen",
                "Yuanzhi Li"
            ],
            "title": "What matters in the structured pruning of generative language",
            "year": 2023
        },
        {
            "authors": [
                "Yair Lakretz",
                "Yangqiu Song",
                "Yasaman Bahri",
                "Yejin Choi",
                "Yichi Yang",
                "Yiding Hao",
                "Yifu Chen",
                "Yonatan Belinkov",
                "Yu Hou",
                "Yufang Hou",
                "Yuntao Bai",
                "Zachary Seid",
                "Zhuoye Zhao",
                "Zijian Wang",
                "Zijie J. Wang",
                "Zirui Wang",
                "Ziyi Wu"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Mingjie Sun",
                "Zhuang Liu",
                "Anna Bair",
                "J. Zico Kolter"
            ],
            "title": "A simple and effective pruning approach for large language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Chaofan Tao",
                "Lu Hou",
                "Haoli Bai",
                "Jiansheng Wei",
                "Xin Jiang",
                "Qun Liu",
                "Ping Luo",
                "Ngai Wong"
            ],
            "title": "Structured pruning for efficient generative pre-trained language models. In Findings of the Association for Computational Linguistics: ACL 2023",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar",
                "Aurelien Rodriguez",
                "Armand Joulin",
                "Edouard Grave",
                "Guillaume Lample"
            ],
            "title": "Llama: Open and efficient foundation language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Johannes von Oswald",
                "Eyvind Niklasson",
                "Ettore Randazzo",
                "Jo\u00e3o Sacramento",
                "Alexander Mordvintsev",
                "Andrey Zhmoginov",
                "Max Vladymyrov"
            ],
            "title": "Transformers learn in-context by gradient descent, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Ro Wang",
                "Alexandre Variengien",
                "Arthur Conmy",
                "Buck Shlegeris",
                "Jacob Steinhardt"
            ],
            "title": "Interpretability in the wild: a circuit for indirect object identification in GPT-2 small",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Percy Liang",
                "Jeff Dean",
                "William Fedus"
            ],
            "title": "Emergent abilities of large language models",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Lu Yin",
                "Shiwei Liu",
                "Ajay Jaiswal",
                "Souvik Kundu",
                "Zhangyang Wang"
            ],
            "title": "Junk dna hypothesis: A task-centric angle of llm pre-trained weights through sparsity, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin",
                "Todor Mihaylov",
                "Myle Ott",
                "Sam Shleifer",
                "Kurt Shuster",
                "Daniel Simig",
                "Punit Singh Koura",
                "Anjali Sridhar",
                "Tianlu Wang",
                "Luke Zettlemoyer"
            ],
            "title": "Opt: Open pre-trained transformer language models, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Frantar"
            ],
            "title": "2023). C CONSTRUCTING PARAMETERIZED FUNCTIONS IN ICL EVALUATION While it may be unreasonable to expect the model to infer any arbitrary f , we focus on three natural classes of functions from which we pick f : the class of linear, 2-layered neural network, and decision tree models",
            "year": 2023
        },
        {
            "authors": [
                "A: House"
            ],
            "title": "Context: \u201dRoses Are Red (My Love)\u201d is a popular song composed by Al Byron and Paul Evans. It was recorded by Bobby Vinton and was his first hit",
            "venue": "The song was released in April",
            "year": 1962
        },
        {
            "authors": [
                "Robert Morgan"
            ],
            "title": "Chinese lyrics written by Suyin under title name of, with Ruby Records in 1967. In 1962, an answer song, entitled \u201dLong As The Rose Is Red\u201d, was recorded by Florraine Darlin. The song spent seven weeks on the Billboard Hot 100, reaching No. 62, while reaching No. 15 on Billboards Easy Listening chart",
            "venue": "It was released by Epic Records",
            "year": 1962
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Scaling up the size of LLMs is known to yield impressive performance gains on various tasks (Kaplan et al., 2020a; Hoffmann et al., 2022b; Brown et al., 2020b; Wei et al., 2022). On the other hand, to deploy language models sustainably, it is also critical to scale them down while preserving their end utility. Naturally, scaling, in both directions, has gained interest in a recent wave of research on language models (Kaplan et al. (2020a); Hoffmann et al. (2022b); Frantar & Alistarh (2023); Jiang et al. (2022); Kurtic et al. (2023); Santacroce et al. (2023)). Much of this work evaluates size\u2013performance tradeoffs of scaling through aggregate performance metrics such as perplexity or downstream accuracy on existing benchmarks.\nHowever, we argue that there must exist subtle but important effects of scaling that cannot be captured by standard metrics alone. Indeed, work on image classification already hints at this. Pruning image models, for example, can introduce biases (Hooker et al., 2019) or disproportionate effects on certain subsets of the data (Jin et al., 2022) \u2014 these are effects that simply do not reflect in the overall accuracy of the model. Taking inspiration from this, our goal is to identify the subtler effects of scaling (up or down) LLMs in terms of the various capabilities that underpin their success in practice.\nOur approach. In this work, we study the effects of scaling the number of parameters in an LLM on two fundamentally dichotomous capabilities as put forth by Chan et al. (2022b;a): the ability to process information stored in weights (such as facts seen during pre-training) and the ability to process information stored in context (such as hints given in the context). It is, however, challenging to isolate how well a model has learned these abilities by simply measuring performance on an arbitrary downstream task \u2014 after all, most tasks require both abilities to varying extents to succeed. Instead, we carefully curate a set of downstream tasks that help tease apart the performance of any given model on the two abilities (as explained shortly). We then evaluate model performance in terms of these two abilities under two fundamentally different types of scaling: pruning and dense scaling. For the former, we consider recent scalable pruning methods that prune and update the remaining weights in one-shot (Frantar & Alistarh, 2023; Sun et al., 2023); for the latter, we use (separately-trained) dense models with increased/reduced width and depth.\nOur curated suite of benchmarks involve four classes of tasks, defining a gradation of reliance on information stored in-weights and in-context. The first three are Q&A tasks: (1) a closed book Q&A that necessarily requires fact recall from pre-training for which the model needs to process information from the weights, (2) an open book Q&A for which it suffices to copy information from the context and (3) an overriding Q&A, where the information provided in-context overrides facts seen during pre-training (Li et al., 2022; Longpre et al., 2021). Finally, to evaluate more sophisticated context-processing abilities, we consider testing the model on its ability to (4) learn parameterized functions (e.g., linear classifiers) using input\u2013output pairs presented in-context. Overall, we consider open-book, overriding Q&A and learning tasks as tasks that require increasing levels of in-context learning (ICL) abilities (See Section 3.1 for more discussion). We summarize these tasks in Table 1.\nWe find surprisingly dichotomous effects of down-scaling on fact recall and in-context learning (ICL) abilities. For fact recall, minimal pruning significantly degrades performance \u2013 specifically, removing more than 30% of weights leads to significant (> 5%, relative) accuracy degradation on fact recall related tasks (Figure 1, left). This is surprising, given that prior pruning literature demonstrates significant down-scaling without noticeable change to accuracy: pioneering work (Dong et al., 2017b) removes \u223c 45% of the weights in a ResNet50 model without noticeable accuracy degradation. 1 In stark contrast, ICL performance withstands significant pruning. Specifically, even after pruning 60% of weights, the relative accuracy decrease on ICL tasks is less than 5% (Figure 1, right). Our results challenges the hypothesis presented in the seminal work of Brown et al. (2020a), that scaling up the number of parameters strongly benefits the ICL ability of LLMs.2 Furthermore, fact recall and ICL shows the same striking sensitivity and insensitivity to dense scaling, respectively. Our work reveals the disparate effects of scaling on fact recall and ICL, which notably holds for two substantially different types of scaling: pruning and dense scaling. This motivates several future work:\nImprove inference efficiency. Our work reveals that scaling down model size alone has little impact on tasks demanding processing information in the LLM\u2019s context. Practitioners may thus use our findings to identify scenarios where decisions could be routed to a smaller model instead of a larger one without hurting task performance (Chen et al., 2023; Dohan et al., 2022).\nImprove LLM systems. Our work shows the utility of feeding external supportive information, like documents from similarity search (Borgeaud et al., 2022; Guu et al., 2020), in question\u2013answering tasks to improve the compute cost and task accuracy trade-off for down-scaled models.\n1Please refer to Section 4.1, Figure 2.(a) of Dong et al. (2017b). Notably, Dong et al. (2017b) obtained these results similarly by pruning without retraining.\n2\u201cSince in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale.\u201d (Brown et al., 2020a)\nImprove LLM Interpretability. We find a remarkably small set of weights responsible for ICL. This underscores the potential of pruning as a tool for isolating neurons responsible for LLM capabilities.\nContributions.\n1. We curate a set of benchmarks for assessing the disparate effects of down-scaling on core capabilities of LLMs, focusing on fact recall and ICL.\n2. Using these benchmarks, we evaluate two scalable weight pruning algorithms for LLMs. We investigate an extensive set of models comprising six base LLMs, with sizes reaching up to 33 billion parameters. We find that:\n\u2022 even moderate levels of pruning (> 30%) hurt fact recall. However, when the evidence required to solve a question-answering task is provided in context, the model\u2019s ability to correctly answer questions survives to higher levels of sparsity;\n\u2022 in contrast, in-context learning withstands aggressive pruning (up to 60\u201370%). 3. Similarly for dense scaling, we find the same disparate patterns as above for fact recall and ICL\nabilities relative to model size. This underscores that the disparate effects on ICL and fact recall are not exclusive to pruning but are broader characteristics of scaling in general."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "LLM scaling. Studies on scaling laws (Hoffmann et al., 2022a; Kaplan et al., 2020b) suggest a predictable relationship between the quality of language models (i.e., perplexity) and the size of the pre-training corpus and model. Pursuant to this, many have scaled up and discovered remarkable capabilities of language models. Brown et al. (2020b) discover that LLMs can perform in-context learning (ICL) effectively: the model learns to perform tasks based on a few examples of input\u2013output demonstrations in the model\u2019s context. Other studies (Devlin et al., 2019; Wei et al., 2022; Liang et al., 2022; Srivastava et al., 2022; Gao et al., 2021; Ganguli et al., 2022; Bubeck et al., 2023; Biderman et al., 2023a) benchmark LLMs across many tasks and metrics to assess the utility of scaling. Our work differs in two ways: while prior work (Kaplan et al., 2020b) studies joint scaling of both pre-training corpus size and model size, we focus on scaling model size alone. Furthermore, instead of measuring an arbitrary notion of task performance, we focus on the foundational capabilities of fact recall and ICL. These capabilities underpin the success of many real world applications of LLMs.\nIn-weight versus in-context learning. LLMs utilize information both stored in weights and present in context (Chan et al., 2022a). Recent research illustrates the similarities and differences between these learning approaches: Von Oswald et al. (2022) demonstrated that in-context learning (ICL) can implement an algorithm akin to gradient descent, commonly associated with in-weight learning. Akyu\u0308rek et al. (2023) revealed that ICL resembles various in-weight learning algorithms, depending on model size. Chan et al. (2022b;a) study how how the properties of the data distribution disparately affect the two abilities in the model. Our work identifies yet another crucial difference: scaling model size has distinct impacts on in-weight learning versus in-context learning abilities.\nNeural network pruning. Pruning removes unimportant parameters in a model. The origin of pruning traces back to LeCun et al. (1990); Hassibi et al. (1993), with a focus on reducing the computational footprint. More recently, with the advent of deep learning, pruning research has seen a resurgence (Renda et al., 2020; Han et al., 2016; Liu et al., 2017; Frankle et al., 2021; Molchanov\net al., 2017; Dong et al., 2017a; Ma et al., 2023; Tao et al., 2023; Kurtic et al., 2023; Dettmers et al., 2023). Though pruning traditionally focuses on preserving aggregate metrics such as accuracy, the versatility of LLMs calls for a different approach to assessing pruned models. Frantar & Alistarh (2023); Sun et al. (2023) did a commendable job to assess model size and task accuracy trade-off. Our work continues to expand our toolkits for empirical assessment, proposing to evaluate pruning\u2019s effect on fact recall and ICL. Our investigation extends a growing line of studies on effects of pruning beyond aggregate metrics such as accuracy and perplexity. Hooker et al. (2019; 2020) show that pruning may harm under-represented categories of examples; Liebenwein et al. (2021) suggest that the pruned models are less accurate than the dense one when predicting out-of-distribution examples. Jin et al. (2022) demonstrate that pruning mitigates the harmful effects of noisy labels on generalization.\nConcurrent to our work, Yin et al. (2023); Jaiswal et al. (2023) remarked on the pronounced negative effect of pruning on knowledge-intensive tasks. These works extensively evaluate tasks that require abilities of fact recall and in-context information retrieval. Our work additionally curates a gradation of tasks that further disentangle these abilities \u2014 specifically via overriding QA and parameterized ICL tasks, which are not covered in these works."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "Pruning algorithms. We investigate pruning as one possible technique to (down-)scale LLMs. Few pruning algorithms currently scale to LLMs. We use SparseGPT (Frantar & Alistarh, 2023) in the main text and Wanda (Sun et al., 2023) in Appendix F. Both are one-shot pruning algorithms that scale to LLMs and outperform magnitude pruning (i.e., pruning the smallest magnitude weights), without computationally intensive re-training (Frantar & Alistarh, 2023). SparseGPT prunes each layer of the language model by minimizing the \u21132-distance between the outputs of the original dense layer and the pruned layer. It computes these outputs based on a small training dataset. Please refer to Appendix O for implementation details of SparseGPT and Wanda pruning algorithms.\nFollowing standard practice (Frantar & Alistarh, 2023; Frankle & Carbin, 2019), we only prune fully-connected layers. Since attention and feed forward modules consist of mostly fully-connected layers, parameters in fully-connected layers account for > 97.5% parameters for all models we examine. We do not prune embedding layers, language modeling heads and normalization layers.\nModels. We evaluate 6 models from 3 families: OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023) and Pythia (Biderman et al., 2023b). We focus on OPT and LLaMA in our main text and present Pythia results in Appendix G. Pythia family models show consistent results as LLaMA and OPT family models. From the OPT family, we evaluate the two largest models that fit in our hardware setup \u2013 OPT-13B and OPT-30B, with 13 and 30 billion parameters, respectively. From the LLaMA family, we evaluate LLaMA-13B and LLaMA-33B, with 13 and 33 billion parameters, respectively.\nA notable difference between OPT and LLaMA families is the ratio of training data to model parameters. Zhang et al. (2022) train the OPT family of models with 180 billion tokens, yielding approximately 14 and 5.5 tokens per parameter, respectively, for our two considered OPT models. Touvron et al. (2023) train the LLaMA-13B model with 1 trillion tokens (77 tokens/parameter) and the LLaMA-33B model with 1.4 trillion tokens (42 tokens/parameter)."
        },
        {
            "heading": "3.1 EVALUATION",
            "text": "We evaluate models on two complementary abilities: the ability to call on information seen during pre-training (and then stored in the model\u2019s weights) and the ability to call on information presented in-context. Unfortunately, it is difficult to construct tasks that strictly isolate one of these two abilities. On the one hand, for every conceivable natural language task, the model must rely on the semantic representations of language learned from pre-training and stored in the weights. On the other, every conceivable task necessitates processing all of the context to understand the instructions in the first place. To help disentangle the two (overly) broad abilities, we focus on two well-defined instantiations of these abilities: (a) the ability to recall facts from pre-training to answer questions and, complementing that (b) the ability to learn patterns from context.\nFact recall. We isolate (a) by evaluating the model on recalling facts from training data not provided in-context. We do this using closed-book QA benchmarks.\nIn-context learning. To evaluate (b), we consider a gradation of tasks that require the increasingly advanced ways of learning from in-context information. Broadly, we consider a series of in-context learning tasks: tasks where the context contains examplars of (query, answer) pairs in some form, followed by a test query that the model has to answer.\nFirst, as a simple ICL task, we consider an Open-book QA (Section 4) counterpart of our QA task. Here the context is augmented with supporting evidence that directly helps answer the question. This can be thought of as a simple ICL task where the test query (\u201cwho is the author of the novel The Eagle Has Landed?\u201d) is simply already present as an in-context examplar within the evidence (\u201cThe author of The Eagle Has Landed is...\u201d), along with its ground truth answer (e.g., \u201c...Jack Higgins\u201d). Note that in addition to the aforementioned evidence\u2013question\u2013answer triplet, we further aid the model with an independent one-shot triplet to demonstrate how to format its answer.\nIn the above case, the answer provided in-context may also have been present in pre-training data; thus the model may opt to ignore the context, and recall facts from pre-training, and still succeed at the task. Thus, to more rigorously isolate the two mechanisms, we next evaluate on the Overriding QA task (Section 4), where the in-context evidence contradicts a fact present in training data. We present a one-shot evidence\u2013question\u2013answer triplet, where the answer is based on the contextual evidence rather than on pre-training facts. Thus, to solve the task correctly, it is clear that the model must retrieve from the context rather than recall pre-training facts.\nNext, we investigate ICL abilities more sophisticated than copying an answer present in an in-context exemplar. To this end, we are interested in tasks where the test query is not already seen as an in-context exemplar. As one such example, consider English\u2013French translation task (Brown et al., 2020b) where the examplars are pairs of English and French sentences, and the test query is a new English sentence. However, such conventional instances of ICL tasks require significant background knowledge of languages and the world, not available in the context.\nTo ensure we disentangle the role of background knowledge from ICL, we consider tasks where the query\u2013answer mapping is given by a parameterized function. Arguably, learning this mapping, would require advanced context-processing mechanisms that go beyond information-retrieval mechanisms required for the Open-book and Overriding QA tasks.\nMetrics. We reproduce the effects of down-scaling on perplexity as in Frantar & Alistarh (2023); Sun et al. (2023) in Appendix B. However, our main metric is exact match accuracy (of the answer to each task) because it is a direct measure of the model\u2019s ability to satisfactorily perform a task.\nWe present details of our hardware/software configurations in Appendix J."
        },
        {
            "heading": "4 EFFECT OF PRUNING ON QUESTION ANSWERING TASKS",
            "text": "The ability of LLMs to answer questions underpins many applications of LLMs. Within a questionanswering evaluation framework, we examine the effect of pruning on its ability to recall facts learnt during pre-training. We also test a simple form of ICL capability \u2013 extracting answers from the provided context, where the answer is unlikely available in pretraining data. Finally, we interpolate between these two capabilities by providing supporting evidence in context when querying information is likely also available in pre-training data. Our results expose that, with contextual information in question-answering tasks, the model\u2019s ability to answer questions withstands more aggressive pruning than without contextual information. With no supporting evidence in context, accuracy rapidly drops as sparsity increases.\nDatasets. We use these datasets: (a) TriviaQA. Joshi et al. (2017) developed the TriviaQA dataset with questions and supporting evidence. We use its Wikipedia validation partition consisting of 7993 questions. (b) WebQuestions. Berant et al. (2013) collected question-answer pairs from the Freebase knowledge database. We use its test set consisting of 2032 questions. (c) NaturalQuestions. Kwiatkowski et al. (2019) compiled the NaturalQuestions dataset from Google search queries. We sampled a 7700-question subset of its validation partition (the same size as the following dataset derived from it), to make our evaluation computationally feasible. (d) DissentQA. Neeman et al. (2022) constructed the DissentQA dataset from the NaturalQuestions dataset. It contains pairs of questions and evidence for a made-up answer that is different from the factual one. It assesses"
        },
        {
            "heading": "Op n-book",
            "text": "whether the model can override its memory formed during pre-training with new context. We use its validation partition consisting of 7700 questions.\nEvaluation setup. Using the aforementioned datasets, we evaluate the ability of pruned models on the following task setup: (i) Close-book. We feed the question without any supporting information to the model. We use the Wikipedia partition of the TriviaQA dataset without the supporting evidence and the WebQA dataset for this setup. (ii) Open-book. We feed the question with supporting evidence to the model in its context. It is important to note that this evaluation setup only evaluates whether the answer is right; it is however agnostic to the mechanism by which the answer is retrieved: the model may either generate its answer using the supporting evidence, or by recalling facts from pre-training. To create this dataset, we use a subset of the TriviaQA dataset whose context can fit within the maximum sequence length of the model, consisting of 881 questions. We denote this filtered subset as TriviaQA(F). Additionally, we use the NaturalQuestions dataset for this setup with factual contexts. (iii) Overriding. We present the question accompanied by evidence that deviates from the facts presented during pre-training. Given that the anticipated made-up answers are randomized and different from the factual ones, the model cannot depend on memorization from its pre-training data to generate responses. This evaluation framework rigorously assesses the model\u2019s ability to override its pre-training data with new, context-specific information. We use the DisentQA dataset with synthetic contexts for this setup. Examples of the setups are shown in Appendix H.\nTo summarize: across these setups, our prompts have three parts to them: first, (1) an example context\u2013question\u2013answer triplet (or question\u2013answer pair in close-book setting) for demonstration, (2) the supporting evidence for a test question (except in the close-book setup), and (3) a test question to answer. Answers are the model\u2019s prompt completions produced by greedy decoding. We report the percentage of answers that exactly match ground truth.\nClose-book versus open-book results. Left 2 plots of Figure 2 shows close-book and open-book results. Notably, for all the models, the pruned model maintains performance on open-book tasks until much higher sparsity levels compared to close-book tasks. In particular, when we accept a relative decrease of 5% from pruning in the mean accuracy over four models, the highest achievable sparsities for closed-book tasks are 30% on TriviaQA and 40% on WebQA.\nIn contrast, maintaining the same acceptable performance drop, the highest acceptable sparsity levels for open-book tasks are 50% on TriviaQA(F) and 60% on NaturalQuestions. Our results suggest that while pruning hurts the model\u2019s ability to recall information from its pre-training data at moderate\nsparsity (30\u201340%), one possible remedy is to provide the model with relevant evidence in context. The model\u2019s ability to answer questions with relevant evidence in its context remains largely intact to higher sparsity levels (50\u201360%).\nOverriding results. The rightmost plot of Figure 2 demonstrates a consistent trend for the overriding tasks. On the DisentQA dataset with overriding context, the highest achievable sparsity is 70% allowing for the same 5% accuracy drop as before. Recall that to solve the overriding task, the model must rely on in-context information rather than in-weights information from the pre-training data. Thus, our observations here further substantiate that the model\u2019s ability to extracting information from its context remains largely intact at higher sparsity levels.\nAccuracy improvements. We note that, surprisingly, on open-book and overriding tasks, there can be small accuracy boosts from pruning. For example, the pruned OPT-30B sees a 9.7% accuracy improvement on DissentQA dataset in overriding task setup. The pruned LLaMa-13B sees 3.5% accuracy improvement on NaturalQuestions in the open-book task setup. Though prior work often reports accuracy improvements with pruning (Frankle & Carbin, 2019; Renda et al., 2020), we are, to the best of our knowledge, the first to observe such improvement in the QA setup.\nTakeaways. Our findings suggest that scaling via pruning has a much higher impact on the ability to retrieve facts from pre-training than on the ability to retrieve information from the context. Pruning may even improve model\u2019s ability to answer questions when its context includes the necessary information to answer the question."
        },
        {
            "heading": "5 MORE SOPHISTICATED IN-CONTEXT LEARNING",
            "text": "Section 4 demonstrates that moderate pruning preserves question answering task accuracy when relevant information is available in context. In this section, we show that even with more sophisticated ICL tasks than previously studied, moderately pruned models maintain their ICL abilities.\nTypical complex ICL tasks require extensive knowledge about language and the world gathered from pre-training. However, as Section 4 shows, pruning likely impairs such knowledge. So, to isolate LLMs\u2019 ICL ability from factual recall abilities, we choose a set of tasks where the goal is to learn a parameterized function (e.g., a linear model) through input-output examples present in-context.\nMethod. We evaluate LLMs in their ability to learn multiclass classifiers of the form f : RD \u2192 {0, 1, \u00b7 \u00b7 \u00b7K \u2212 1} from in-context examples, where D refers to the input dimensionality and K the number of label categories. To generate a sequence consisting of a context, query and an answer, we first pick a random parameterized function f from one of three function classes: 2-way linear classifiers, 2-way neural network classifiers with 2 layers and 4-way decision tree classifiers (Appendix C contains details for constructing f ). Then, in the context, we provide N exemplars of the form (x, f(x)), followed by a novel query xquery. The goal of the model is to correctly predict the answer as f(xquery). Please refer to Appendix H for an example of the full prompt.\n0 10 20 30 Param Count (B)\n0\n20\n40 60 Ex ac t M at ch (% )\nLinear Classification\n0 10 20 30 Param Count (B)\n0\n20\n40\n60\nEx ac\nt M at\nch (%\n)\n2-layer Neural Net\n0 10 20 30 Param Count (B)\n0\n10\n20\n30\n40\nEx ac\nt M at\nch (%\n)\nDecision Tree\nPruned OPT-30B Pruned OPT-13B Dense\n0 10 20 30 Param Count (B)\n0\n10\n20\n30 40 Ex ac t M at ch (% )\nClose-book TriviaQA WebQA\n0 10 20 30 Param Count (B)\n0\n20\n40\nEx ac\nt M at\nch (%\n)\nOpen-book & Counterfactual\nTriviaQA(F) NatQ DisentQA\n0 10 20 30 Param Count (B)\n20\n30\n40\n50\n60\n70\nEx ac\nt M at\nch (%\n)\nICL\nlinear nn dt\nFigure 4: Like pruning, moderate dense down-scaling preserves ICL while harming fact recall.\nAll task instances consist of N total in-context examples split evenly across all possible labels. We draw all random numbers uniformly from integers between -10 and 10 inclusive. We set D=4, N=32. We record additional methodological details in Appendix D.\nPruning results. Figure 19 shows the accuracy of pruned models versus sparsity. We can remove a substantial number of parameters from the models without affecting their ICL ability. Specifically, the average accuracy of the pruned models on linear, 2-layer NN, decision tree classification tasks is within 5% of the dense models up to 60%, 60% and 70% sparsity, respectively.\nTask difficulty. One trivial explanation for our observation is that perhaps these tasks are so easy that they are solvable even by a severely pruned model. To eliminate this hypothesis, in Appendix E, we increase the input dimension (D) of the linear classification task. Even as we increase this to a significant extent (say, to an extent that the task is almost, but not totally, unlearnable), we find that the ICL performance is maintained under aggressive pruning.\nConclusion. Our results suggest that when pruning LLMs moderately (i.e., to < 70% sparsity), the model\u2019s ICL ability stays largely intact. Thus, unlike fact recall tasks, one can prune models significantly more while maintaining ICL performance."
        },
        {
            "heading": "6 DENSE SCALING",
            "text": "For vision models, pruned models behave similarly to smaller dense models trained with similar hyperparameters (Jin et al., 2022). Here, we examine the effects of dense scaling (i.e., choosing from a suite of independently trained models of various sizes) on fact recall vs. ICL abilities of LLMs.\nMethod. With the same benchmarks as in Section 4 and Appendix N, we evaluate the accuracy of dense models with different parameter count, but the same pre-training data size. The OPT family of models provides an ideal collection of dense models for our evaluation: Zhang et al. (2022) trained all OPT models with the same number of tokens (300b).\nResults. Figure 4 shows that fact recall ability is highly sensitive to dense scaling \u2013 e.g., focusing on scaling downwards, moving from the 30B model to the next largest 13B model leads to more than 5% relative task accuracy degradation on close-book TriviaQA and WebQA task. However, open-book/overriding QA accuracy is much less sensitive to dense scaling. Specifically, maintaining the same 5% acceptable relative accuracy degradation with respect to the 30B models, one may replace a 30B model on TriviaQA(F) and NaturalQuestions dataset with a 6.7B model. Figure 4 also shows that ICL ability is similarly robust to dense scaling.\nConclusion. Like pruning, changing a dense model\u2019s size more readily affects its ability to retrieve facts from pre-training than to process information from context. We hypothesize that this effect stems from scaling in general \u2013 be it pruning or using a dense model of different size."
        },
        {
            "heading": "7 PRUNING ATTENTION AND FEED FORWARD LAYERS",
            "text": "A growing line of work (Dai et al., 2021; Meng et al., 2022) suggests that feed forward (FFW) layers and the attention layers are responsibly for distinct capabilities considered in this paper \u2014 namely,\nfact recall and in-context learning. To test this, we exclusively prune either type of layers and examine how the model\u2019s abilities deteriorate on fact recall and ICL tasks.\nMethod. We prune a LLaMA-13B model. We either exclusively prune attention layers or exclusively prune FFW layers and observe their effects on accuracy of TriviaQA task (fact recall) and linear classification task (ICL). We plotted the task accuracy as a function of module sparsity. Module sparsity refers to the fraction of parameters pruned from a specific type of layers (either attention or FFW), with respect to the total number of parameters of that type.\nResults. Figure 5 shows that while attention and FFW layers appear equally important for ICL, FFW layers appear more important for fact recall. For example, pruning 60% FFW layers lead to 14% more accuracy degradation than pruning 60% of attention layers.\nConclusion. Attention and FFW layers show similar importance for ICL; and FFW layers show greater importance for knowledge recall."
        },
        {
            "heading": "8 CLOSING DISCUSSION",
            "text": "We study the effects of scaling model size via pruning and dense scaling on two core abilities of LLMs that underpin their practical success: the ability to recall facts and the ability to perform ICL. In both cases, we find similar disparate effects on the two abilities. Moderate pruning (> 30% sparsity) harms fact recall, and yet the ability to learn from a few input-output examples from context withstands aggressive pruning (up to 60\u201370% sparsity). The same disparity arises when changing the width and depth of dense (independently-trained) models.\nWhat could explain this disparity? We conjecture that the number of parameters required to store a set of facts must scale in proportion to the number of independent facts. On the other hand, in-context learning of many kinds may be accomplished by a smaller set of parameters that act as a universal gradient descent module (von Oswald et al., 2022) that can be applied for any task. Verifying these hypotheses theoretically is an important direction for future work. Our findings also invite various research directions for practice:\nPruning & interpretability. Our findings suggest, remarkably, a relatively small fraction of weights are responsible for ICL performance. This observation could prove to be valuable for enhancing the interpretability of LLMs, reviving a decades-old motivation behind work in pruning (Mozer & Smolensky, 1988).3 In particular, pruning may help better localize the weights responsible for in-context ability, and complement recent approaches (Wang et al., 2023; Bills et al., 2023).\nMemory augmentation. Our observations advocate for memory augmentation as a promising way to improve the trade-off between computational cost and task accuracy. Memory augmentation techniques present helpful facts to the model by augmenting them directly in the context. Thus, rather than having the LLM to rely on fact recall from pre-training \u2014 an ability that is degraded under downscaling \u2014 we can delegate fact-retrieval to a separate retrieval model (Borgeaud et al., 2022; Guu et al., 2020), and allow the LLM to focus on retrieving the fact from the context\u2014which is an ability that is preserved under down-scaling.\nLimitations. We address the limitations of our work in Appendix A.\n3\u201dThis skeletonization technique can be used ... to understand the behavior of networks in terms of minimal \u2019rules.\u2019\u201d (Mozer & Smolensky, 1988)"
        },
        {
            "heading": "A LIMITATION.",
            "text": "Overall, our study provides a valuable first step towards evaluating pruning algorithms for LLMs, as well as identifying limitations of a particular type of pruning approaches. We have validated our claims on a large number of benchmarks (a total of 8 tasks and 6 models). Since our work is empirical in nature, our observations may not generalize to the full spectrum of tasks and large language models (LLMs). In this study, we focused on evaluating pruning algorithms that scale to large-scale LLMs. This was a deliberate decision, as we believe that scalability is essential for any pruning algorithm to be widely adopted in the real world. More sophisticated one-shot/iterative pruning algorithms exist (Renda et al., 2020). They typically require re-training \u2013 redoing the training of these foundation models for every sparsity level we examine. The cost of such an experiment is at least in the millions of dollars therefore beyond our means."
        },
        {
            "heading": "B PRUNING PERPLEXITY EVAL",
            "text": "In Table 2 we present the full range of pruning\u2019s effect on the perplexity of models we consider in this work. We obtain perplexity results by running the pruned model on a randomly sampled subset of C4 validation set, following the precedent of Frantar et al. (2023)."
        },
        {
            "heading": "C CONSTRUCTING PARAMETERIZED FUNCTIONS IN ICL EVALUATION",
            "text": "While it may be unreasonable to expect the model to infer any arbitrary f , we focus on three natural classes of functions from which we pick f : the class of linear, 2-layered neural network, and decision tree models. These classes are defined as follows: (a) Linear. For each task instance, we generate a distinct random hyperplane in the D-dimensional input space as the decision boundary. We label each example as positive/negative depending on the side of the decision boundary it falls on. (b) 2-layer NN. For each task instance, we generate a 2-layer neural network mapping a D-dimensional input vector x to a binary label y with the following form: W2\u03c3(W1x), where W1, W2 are D \u00d7D, D \u00d7 2 matrices correspondingly. We draw elements within these two matrices from independent, Gaussian distributions with zero-mean and unit variance. We use ReLU as the activation function \u03c3. (c) Decision tree. For each task instance, we construct a full depth-2 binary decision tree, with each of its four leaf nodes representing one possible label. This maps D-dimensional input vectors to the said labels. We assign each non-leaf node an index from 0 to D \u2212 1. We evaluate the tree by traversing from the root, comparing the input vector\u2019s value at each node\u2019s associated index to determine the next step: if negative, we go left; if zero or positive, we go right. Reaching a leaf node marks the end of traversal, and the corresponding label represents the evaluation result."
        },
        {
            "heading": "D ADDITIONAL DETAILS FOR ICL EVALUATION METHODOLOGY",
            "text": "In this section, we provide additional details for our ICL task evaluation methodology.\nIn-context example label distribution. When generating the ICL evaluation datasets, we programmatically enforce that the labels for the N in-context demonstrations are split exactly evenly between K possible labels.\nEvaluation label distribution. Each evaluation input has an equal probability of receiving one of the K possible classification labels. Therefore, evaluation label distribution should be even in expectation. However, we do not programmatically enforce an even distribution of evaluation labels.\nSize of evaluation dataset. We generate 2048 independent task instances for each function class. Each task instance represents a unique parameterization of the classification function to learn.\nPrompting technique. We do not use any advanced prompting technique when solving ICL tasks. The LLMs only generate answer tokens corresponding directly to classification labels. We do not generate any other tokens during evaluation.\nTokenizer. The OPT models use a byte-pair encoding tokenizer whereas the LLaMA models use a sentence piece tokenizer. Notably, LLaMA tokenizers split each digit into an individual token. This feature of LLaMA tokenizers boosts LLaMA performance on arithmetic tasks (Liu & Low, 2023).\nE INCREASING INPUT DIMENSION OF ICL EVALUATION\nIn this section, we further evaluate the effect of pruning on ICL tasks with different input dimensions.\nMethod. In Appendix N, we evaluate pruning\u2019s effect on learning linear classification tasks incontext. The task is to classify 4-dimensional input vectors. Here, we increase the input vector dimensions to 8 and 16, and observe whether our observation about pruning\u2019s effect generalize to larger input dimenions.\nOPT-13B OPT-30B LLaMA-13B LLaMA-33B\nResults. Figure 7 presents the relationship between task accuracy and sparsity for linear classification task. We increase the input dimension from 4 to 8 and 16. We observe that tolerating the same 5% relative drop in the average accuracy among all four models with respect to the dense model, we can prune to 60%, 80% sparsity on linear classification task with 4, 8 dimensional inputs respectively. Dense models perform close to chance on 16-dimensional inputs, thus we refrain from drawing conclusions based on these results.\nTriviaQA(CloseBook)\nTriviaQA(OpenBook)\nLinear Classification\nConclusion. Even with higher dimensional inputs, model\u2019s ICL ability remains resilient to aggressive pruning (>= 60% sparsity)."
        },
        {
            "heading": "F ADDITIONAL PRUNING ALGORITHM EVALUATION",
            "text": "In our main paper, we focus on a single pruning algorithm SparseGPT (Frantar & Alistarh, 2023). In this section, we study whether our observation that scaling down LLM size affects fact recall more readily than ICL generalize to another pruning algorithm.\nMethod. We repeat key experiments with another pruning algorithm called Wanda (Sun et al., 2023). Notably, unlike SparseGPT, Wanda does not update the remaining weights after weights removal. The author of Wanda shows that at 50% sparsity, Wanda achieves accuracy that is competitive with SparseGPT.\nResults. We observe the same disparate effects from pruning on fact recall versus ICL: while moderate pruning hurts fact recall, ICL survives to higher sparsity. Specifically, accepting the same 5% relative drop in accuracy as we did for SparseGPT results, one may remove 30%, 40% and 50% weights on TriviaQA(Closebook), TriviaQA(Openbook) and Linear Classification tasks. Unsurprisingly, given that Wanda is computationally less expensive, it underperforms SparseGPT at high sparsities on ICL tasks.\nConclusion. With our experiments repeated with another pruning algorithm, we show that our observation is general."
        },
        {
            "heading": "G PYTHIA MODELS EVALUATION",
            "text": "In this section, we study whether our observation that scaling down LLM size affects fact recall more readily than ICL generalize to another model family: Pythia (Biderman et al., 2023b).\nMethod. We repeat key experiments with Pythia model family. With SparseGPT, we pruned two variants of the Pythia-12B model, the original and a \u201cdeduped\u201d variant where the model is trained on a deduplicated dataset. They\u2019re the most capable models in the Pythia family.\nResults. ?? shows consistent disparate effect of pruning on fact recall versus ICL. Specifically, accepting the same 5% accuracy drop as in all our results, one can remove 20%, 30% and 50% weights for TriviaQA(Closebook), TriviaQA(Openbook) and in-context linear classification tasks.\nConclusion. Our observation is robust to the choice of model families."
        },
        {
            "heading": "H EXAMPLE PROMPTS",
            "text": "In this section, we present example prompts for Q&A tasks in Section 4 and ICL on algorithmic tasks in Appendix N.\nH.1 QUESTION-ANSWERING WITHOUT CONTEXT.\nOur prompt follows the precedent of Touvron et al. (2023). We show our prompt for Q/A tasks without context in Figure 10.\nH.2 QUESTION-ANSWERING WITH CONTEXT.\nWe show an example prompt we use to evaluate Q/A task with the necessary information to answer the question available in-context in Figure 11.\nH.3 ICL ON ALGORITHMIC TASK.\nWe present the prompt we use to evaluate ICL on linear classification task in Figure 12. Evaluations for ICL on check-sorting, check-even uses analogous prompts."
        },
        {
            "heading": "I ANALYSIS ON ACCURACY IMPROVEMENT",
            "text": "On many tasks, such as ICL on algorithmic tasks in Appendix N, we observe accuracy improvement from pruning. In this section, we provide further analysis on this phenomenon.\nMethod. We focus on the ICL task of check-sorting and check-even from Appendix N, where the pruned LLaMA models see significant accuracy improvement. We analyze the prediction of the pruned model, in relation to the following two answers: 1). Answer without context. We compute the classification output with the highest log likelihood, without all the in-context examples. The prompt thus looks like \u201d[1, 0, 3, -5]=\u201d without any other information such as in-context training examples. The LLM thus classifies this input array exclusively based on likelihood of a classification label as a continuation of the prompt. We denote this classification outcome as the answer without context. 2). Ground-truth answer.\nResults. In Figure 13, we present the percentage of time when the prediction of the pruned model matches the answer without context versus the ground truth. We observe that accuracy improvement often mirrors a departure of the model\u2019s prediction from answer without context (the solid line dropping whilst the dashed line raising). This suggest that for some pruned models (for example LLaMA-30B at 60% sparsity), the presence of in-context examples changes the prediction of the LLM more than the dense models.\nConclusion. We hypothesize, without testing, that pruning may improve task accuracy by enhancing the effect of contextual information on its prediction."
        },
        {
            "heading": "J DETAILS ON COMPUTING SOFTWARE/HARDWARE",
            "text": "We perform pruning on 80GB version of A100 GPUs, using the implementation of SparseGPT(Frantar & Alistarh, 2023). We perform our evaluations using TPU v3 running PyTorch (Paszke et al., 2019) with XLA backend. Our research project consumes approximately 3 TPU-month worth of computation.\nOur computing software/hardware necessitates the use of the bfloat16 numerical format. We assess its impact on our experimental results in Appendix K."
        },
        {
            "heading": "K CHARACTERIZING ERRORS FROM NUMERICAL PRECISION",
            "text": "All our experiments run on TPU v3. The TPU hardware does not support float16 (IEEE 754 half precision floating point format), which are the formats used for training the OPT/LLaMA models. We thus use a closely related floating point format called bfloat16 (Brain Float) instead to evaluate these models. Both bfloat 16 and float16 are floating point numbers requiring 16 bits to represent, the key difference is that bfloat16 format can represent numbers in a slightly wider range of values, but with slightly reduced precision compared with float16. For this reason, we expect our results to contain small systematic errors. In this section, we characterize the difference between our results in bfloat16 and standard results in float16.\nPerplexity and accuracy results. In Figure 14, we plot the perplexity and accuracy in next token prediction in bfloat16 on TPU v3 and float16 on A100. We observe that the average accuracy difference is 1.0%, 0.89%, 0.70% and 0.68% for OPT-13B, OPT-30B, LLaMA-13B and LLaMA-33B. The perplexity grows to very large numbers at high sparsity, so we report the average perplexity difference for sparsity level less than or equal to 50%. The average perplexity difference is: 0.86, 0.69, 0.19, 0.13. We note that 1). the difference in accuracy and perplexity due to numerical precision is systematic. The accuracy of next token prediction raises by a similar amount across all sparsity levels. 2). the difference in next token prediction accuracy is small (\u00a11%).\nOPT. OPT models see higher difference in accuracy and perplexity. This is because its GPU implementation uses mixed precision arithmetic, where the attention score computes in float32. Our compiler and runtime, however, does not support mixed precision arithmetic, we therefore compute entirely within bfloat16.\nL BEAM SEARCH + TOLERANT ANSWER MATCHING"
        },
        {
            "heading": "M MORE TASKS",
            "text": "0 20 40 60 80 Sparsity (%)\n0\n20\n40\n60 80 Ac cu ra cy (%\n) Lambada\nllama-13b llama-30b\nopt-13b opt-30b\nFigure 16: Lambada\n0.0 0.2 0.4 0.6 Sparsity (%)\n10\n20\n30\nBL EU\nS co\nre\nTranslation (WMT14 EN-FR)\nLLaMA 13B LLaMA 30B\nFigure 17: Translation\nN IN-CONTEXT LEARNING ON ALGORITHMIC TASKS\nMethod. We use the following ICL task and for each task, generate a total of N task instances. Each task instance consists of a set of in-context examples along with a test input. a). Check Sorting. The task is to determine, whether a D-element input array is sorted. We label each example as positive/negative depending on whether the input array is sorted or not.b). Check Contains Even Number. The task is to determine, whether a D-element input array contains an even number. We label each example as positive/negative depending on whether the input array contains an even number. We present the pseudo code solution to these algorithmic tasks in Figure 18.\nAll task instances consist of K positive in-context examples and K negative in-context examples. We draw all random numbers from integers between -10 to 10 inclusive. We set N=2048, D=4, K=16.\nSparsity results. Figure 19 shows the accuracy of pruned models versus sparsity. We can remove substantial number of parameters from the models without affecting their ICL ability. Specifically, the average accuracy of the pruned models on check sorting and check contains even number is within 5% of the dense models up to 60% and 70% sparsity respectively.\nConclusion. Our results suggest that when pruning to moderate sparsity levels (60-70%), the model\u2019s ICL ability stays largely intact. Compared with pruning on fact recall tasks, one can prune to much higher sparsity levels while maintaining or even improving ICL performance.\n0 25 50 75 Sparsity (%)\n30\n40\n50\n60 70 Ex ac t M at ch (% )\nLinear Classification\n0 25 50 75 Sparsity (%)\n30\n40\n50\n60\n70\nEx ac\nt M at\nch (%\n)\nCheck Sorting\n0 25 50 75 Sparsity (%)\n30\n40\n50\n60\n70\nEx ac\nt M at\nch (%\n)\nContains Even\nOPT-13B OPT-30B LLaMA-13B LLaMA-33B\n0 25 50 75 Sparsity (%)\n30\n40\n50\n60\n70\nEx ac\nt M at\nch (%\n)\nLinear Classification\n0 25 50 75 Sparsity (%)\n30\n40\n50\n60 70 Ex ac t M at ch (% )\nCheck Sorting\n0 25 50 75 Sparsity (%)\n30\n40\n50\n60\n70\nEx ac\nt M at\nch (%\n)\nContains Even\nOPT-13B OPT-30B LLaMA-13B LLaMA-33B\nFigure 19: ICL withstands aggresive pruning (60-70% spasity). Accepting a relative average (over 4 models) accuracy drop of 5% w.r.t. the dense models, we can prune to 60% and 70% on check sorting and check contains even number tasks, respectively."
        },
        {
            "heading": "O PRUNING ALGORITHMS DESCRIPTION",
            "text": "In this section, we present implementation details of pruning algorithms we evaluate in this work.\nO.1 SPARSEGPT.\nThe SparseGPT algorithm prunes weights in a language model layer by layer. For each layer\u2019s weight Wl, where l is the layer index, it produces a binary mask Ml and a updated set of weights W\u0302l to minimize the layerwise reconstruction loss \u2225WlX \u2212 (M \u2299 W\u0302l)X\u222522, where \u2299 denotes element-wise multiplication, and X correspond to inputs to layer l. Such inputs are computed using a small set of calibration data, randomly drawn from the training dataset. Note that rows in Wl do not interact with one another, SparseGPT thus prunes each row independently.\nWeight selection and repair. SparseGPT builds on optimal brain surgeon (OBS) (Hassibi et al., 1993; Frantar et al., 2021), a classical approach in weight pruning. For a given weight row w, upon removing an element at position m, OBS suggests the optimal weight update \u03b4m to compensate for the effects of removed weights, and characterize the associated reconstruction error \u03f5m as follows:\n\u03b4m = \u2212 wm\n[H\u22121]mm \u00b7H\u22121:,m, \u03f5m = w2m [H\u22121]mm . (1)\nThe SparseGPT thus prunes rows of Wl independently and in parallel. Within each row, the algorithm prunes weights in vectors of size B = 128. The algorithm selects weights for removal based on the lowest reconstruction error \u03f5 After removing weights, SparseGPT iteratively applies the OBS weight update. An important difference between SparseGPT and OBS is that while OBS adjusts all remaining weights to compensate for removed weights, SparseGPT only updates a subset of remaining weights to reduce the computational complexity associated with computing the Hessian inverse. This complexity reduction enables SparseGPT to scale to very large language models (175B). See pseudocode below for details about the exact subset of remaining weights updated by SparseGPT.\nPseudocode The following pseudocode is a simplified version of SparseGPT largely based on Algorithm. 1 of Frantar & Alistarh (2023). This algorithm prunes the layer weight matrix W to p% unstructured sparsity given inverse Hessian H\u22121 = (XXT )\u22121, weight vector size B.\nAlgorithm 1 The SparseGPT algorithm. 1: M \u2190 1drow\u00d7dcol \u25b7 binary pruning mask 2: E \u2190 0drow\u00d7B \u25b7 block pruning errors 3: for i = 0, B, 2B, . . . do 4: M:,j:(j+Bs) \u2190 mask of (1\u2212 p%) weights wc in W:,j:(j+Bs) with largest w2c/[H\u22121]cc 5: # Apply the OBS update to a subset of remaining weights. 6: for j = i, . . . , i+B \u2212 1 do 7: E:,j\u2212i \u2190W:,j/[H\u22121]jj 8: E:,j\u2212i \u2190 (1\u2212M:,j) \u00b7 E:,j\u2212i 9: W:,j:(i+B) \u2190W:,j:(i+B) \u2212 E:,j\u2212i \u00b7 [H\u22121]j:(i+B) 10: end for 11: W:,i:(i+B) \u2190W:,i:(i+B) \u2212 E \u00b7H\u22121i:(i+B),(i+B) 12: end for 13: W \u2190W \u00b7M \u25b7 set pruned weights to 0\nO.2 WANDA\nWanda(Sun et al., 2023) similarly prunes a language model layer by layer. Within each layer, it independently prunes individual rows in the weight matrix. The inputs to a layer, denoted as X , have the shape (N,L,Cin). Here, N , L, and Cin refer to the batch size, sequence length, and input hidden dimension size, respectively. The inputs to the layer W derive from a small set of calibration data. The weight matrix to prune has shape (Cin, Cout). Cin and Cout refer to the input and output hidden dimension sizes. Wanda scores each weight Wi,j using the following equation:\nSij = |Wij | \u00b7 \u2225Xj\u22252 ,\nwhere |.| denotes the absolute value function. Wanda removes p% of the weights with the lowest scores in each row Wi. Notably, unlike SparseGPT, there is no weight update after weights removal."
        },
        {
            "heading": "P STRUCTURED PRUNING (LLM-PRUNER) RESULTS",
            "text": "In this section, we test our findings using structured pruning algorithm LLM-Pruner (Ma et al., 2023).\nConfiguration. We use LLaMA7B/13B instead of 13B/30B in the main paper because LLM-Pruner appears to consume significantly more GPU memory than SparseGPT, and 30B model does not fit within our hardware setup.\nFollowing the recommended setup of Ma et al. (2023) in their code repository, we skip the first 4 layers and last 2 layers for all LLaMA models we test. Following Frantar & Alistarh (2023), we sample calibration data of maximum sequence length (2048 tokens) from C4 dataset. The number of samples in the calibration data is configured to maximum allowed by our system memory size (167 GB CPU memory + 80GB GPU memory), which is 2 for 7B and 1 for 13B LLaMA model.\nSince Ma et al. (2023) focuses on the low-sparsity regime (0-20%) in the paper, we similarly focus on this regime of sparsity. Specifically, we prune each of the prunable layers (which excludes 4 initial and 2 final layers) to 5%, 10%, 15%, 20%, and 25% sparsity. We refer to this sparsity ratio as local sparsity, since it refers to the sparsity ratio within prunable layers only. In Figure 20, we report global sparsity to remain consistent with our main paper \u2013 global sparsity refers to the number of remaining parameters divided by the total number of parameters of the full model.\nResults. Figure 20 shows that our main conclusion that fact recall deteriorates faster than ICL under down-scaling remains valid with the structured pruning algorithm. Specifically, averaging over two models, we can prune to 0%, 5% and 20% local sparsity on Closebook (TriviaQA), Openbook (TriviaQA(F)) and ICL linear classification tasks, respectively, without dropping more than 5% relative accuracy."
        }
    ],
    "year": 2023
}