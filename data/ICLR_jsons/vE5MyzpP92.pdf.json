{
    "abstractText": "Existing losses used in deep metric learning (DML) for image retrieval often lead to highly non-uniform intra-class and inter-class representation structures across test classes and data distributions. When combined with the common practice of using a fixed threshold to declare a match, this gives rise to significant performance variations in terms of false accept rate (FAR) and false reject rate (FRR) across test classes and data distributions. We define this issue in DML as threshold inconsistency. In real-world applications, such inconsistency often complicates the threshold selection process when deploying commercial image retrieval systems. To measure this inconsistency, we propose a novel variance-based metric called Operating-Point-Inconsistency-Score (OPIS) that quantifies the variance in the operating characteristics across classes. Using the OPIS metric, we find that achieving high accuracy levels in a DML model does not automatically guarantee threshold consistency. In fact, our investigation reveals a Pareto frontier in the high-accuracy regime, where existing methods to improve accuracy often lead to degradation in threshold consistency. To address this trade-off, we introduce the Threshold-Consistent Margin (TCM) loss, a simple yet effective regularization technique that promotes uniformity in representation structures across classes by selectively penalizing hard sample pairs. Extensive experiments demonstrate TCM\u2019s effectiveness in enhancing threshold consistency while preserving accuracy, simplifying the threshold selection process in practical DML settings.",
    "authors": [
        {
            "affiliations": [],
            "name": "Qin Zhang"
        },
        {
            "affiliations": [],
            "name": "Linghan Xu"
        },
        {
            "affiliations": [],
            "name": "Qingming Tang"
        },
        {
            "affiliations": [],
            "name": "Jun Fang"
        },
        {
            "affiliations": [],
            "name": "Ying Nian Wu"
        },
        {
            "affiliations": [],
            "name": "Joe Tighe"
        },
        {
            "affiliations": [],
            "name": "Yifan Xing"
        }
    ],
    "id": "SP:eb3a517bd2e1b6aaa6bb5d2473b17582560bb6e8",
    "references": [
        {
            "authors": [
                "Xiang An",
                "Jiankang Deng",
                "Kaicheng Yang",
                "Jiawei Li",
                "Ziyong Feng",
                "Jia Guo",
                "Jing Yang",
                "Tongliang Liu"
            ],
            "title": "Unicom: Universal and compact representation learning for image retrieval",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "L.C. Andrews"
            ],
            "title": "Special Functions of Mathematics for Engineers. Online access with subscription: SPIE Digital Library",
            "venue": "URL https://books.google.com/books?id=2CAqsF-RebgC",
            "year": 1998
        },
        {
            "authors": [
                "Rina Foygel Barber",
                "Emmanuel J Candes",
                "Aaditya Ramdas",
                "Ryan J Tibshirani"
            ],
            "title": "Conformal prediction beyond exchangeability",
            "venue": "The Annals of Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Abhijit Bendale",
                "Terrance Boult"
            ],
            "title": "Towards open world recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 1893
        },
        {
            "authors": [
                "Abhijit Bendale",
                "Terrance E Boult"
            ],
            "title": "Towards open set deep networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Andrew Brown",
                "Weidi Xie",
                "Vicky Kalogeiton",
                "Andrew Zisserman"
            ],
            "title": "Smooth-ap: Smoothing the path towards large-scale image retrieval",
            "venue": "CoRR, abs/2007.12163,",
            "year": 2020
        },
        {
            "authors": [
                "Kwan Ho Ryan Chan",
                "Yaodong Yu",
                "Chong You",
                "Haozhi Qi",
                "John Wright",
                "Yi Ma"
            ],
            "title": "Redunet: A white-box deep network from the principle of maximizing rate reduction",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Yen-Chi Chen"
            ],
            "title": "A tutorial on kernel density estimation and recent advances, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Jiankang Deng",
                "Jia Guo",
                "Niannan Xue",
                "Stefanos Zafeiriou"
            ],
            "title": "Arcface: Additive angular margin loss for deep face recognition",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Jiankang Deng",
                "Jia Guo",
                "Tongliang Liu",
                "Mingming Gong",
                "Stefanos Zafeiriou"
            ],
            "title": "Sub-center arcface: Boosting face recognition by large-scale noisy web faces",
            "venue": "Computer Vision \u2013 ECCV",
            "year": 2020
        },
        {
            "authors": [
                "Qi Dong",
                "Shaogang Gong",
                "Xiatian Zhu"
            ],
            "title": "Class rectification hard mining for imbalanced deep learning",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Yueqi Duan",
                "Jiwen Lu",
                "Jie Zhou"
            ],
            "title": "Uniformface: Learning deep equidistributed representation for face recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Natalie Dullerud",
                "Karsten Roth",
                "Kimia Hamidieh",
                "Nicolas Papernot",
                "Marzyeh Ghassemi"
            ],
            "title": "Is fairness only metric deep? evaluating and addressing subgroup gaps in deep metric learning",
            "venue": "arXiv preprint arXiv:2203.12748,",
            "year": 2022
        },
        {
            "authors": [
                "Chen Fang",
                "Ye Xu",
                "Daniel N Rockmore"
            ],
            "title": "Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp. 1657\u20131664,",
            "year": 2013
        },
        {
            "authors": [
                "Isaac Gibbs",
                "Emmanuel Candes"
            ],
            "title": "Adaptive conformal inference under distribution shift",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q. Weinberger"
            ],
            "title": "On calibration of modern neural networks. CoRR, abs/1706.04599, 2017a. URL http://arxiv.org/abs/1706.04599",
            "year": 2017
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q. Weinberger"
            ],
            "title": "On calibration of modern neural networks",
            "venue": "Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Raia Hadsell",
                "Sumit Chopra",
                "Yann LeCun"
            ],
            "title": "Dimensionality reduction by learning an invariant mapping",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906),",
            "year": 2006
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Ramya Hebbalaguppe",
                "Jatin Prakash",
                "Neelabh Madan",
                "Chetan Arora"
            ],
            "title": "A stitch in time saves nine: A train-time regularizing loss for improved neural network calibration",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Grant Van Horn",
                "Oisin Mac Aodha",
                "Yang Song",
                "Alexander Shepard",
                "Hartwig Adam",
                "Pietro Perona",
                "Serge J. Belongie"
            ],
            "title": "The inaturalist challenge",
            "venue": "dataset. CoRR,",
            "year": 2017
        },
        {
            "authors": [
                "Christina Ilvento"
            ],
            "title": "Metric learning for individual fairness",
            "venue": "arXiv preprint arXiv:1906.00250,",
            "year": 2019
        },
        {
            "authors": [
                "Shichao Kan",
                "Zhiquan He",
                "Yigang Cen",
                "Yang Li",
                "Vladimir Mladenovic",
                "Zhihai He"
            ],
            "title": "Contrastive bayesian analysis for deep metric learning",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Sungyeon Kim",
                "Dongwon Kim",
                "Minsu Cho",
                "Suha Kwak"
            ],
            "title": "Proxy anchor loss for deep metric learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Sungyeon Kim",
                "Boseung Jeong",
                "Suha Kwak"
            ],
            "title": "Hier: Metric learning beyond class labels via hierarchical regularization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Krause",
                "Michael Stark",
                "Jia Deng",
                "Li Fei-Fei"
            ],
            "title": "3d object representations for fine-grained categorization",
            "venue": "IEEE International Conference on Computer Vision Workshops,",
            "year": 2013
        },
        {
            "authors": [
                "Yann LeCun",
                "L\u00e9on Bottou",
                "Yoshua Bengio",
                "Patrick Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE,",
            "year": 1998
        },
        {
            "authors": [
                "Gongbo Liang",
                "Yu Zhang",
                "Xiaoqin Wang",
                "Nathan Jacobs"
            ],
            "title": "Improved trainable calibration method for neural networks on medical imaging classification",
            "venue": "In British Machine Vision Conference (BMVC),",
            "year": 2020
        },
        {
            "authors": [
                "Hao Liu",
                "Xiangyu Zhu",
                "Zhen Lei",
                "Stan Z. Li"
            ],
            "title": "Adaptiveface: Adaptive margin and sampling for face recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Jiaheng Liu",
                "Zhipeng Yu",
                "Haoyu Qin",
                "Yichao Wu",
                "Ding Liang",
                "Gangming Zhao",
                "Ke Xu"
            ],
            "title": "Oneface: one threshold for all",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Yair Movshovitz-Attias",
                "Alexander Toshev",
                "Thomas K. Leung",
                "Sergey Ioffe",
                "Saurabh Singh"
            ],
            "title": "No fuss distance metric learning using proxies",
            "venue": "CoRR, abs/1703.07464,",
            "year": 2017
        },
        {
            "authors": [
                "Jishnu Mukhoti",
                "Viveka Kulharia",
                "Amartya Sanyal",
                "Stuart Golodetz",
                "Philip Torr",
                "Puneet Dokania"
            ],
            "title": "Calibrating deep neural networks using focal loss",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Rafael M\u00fcller",
                "Simon Kornblith",
                "Geoffrey E Hinton"
            ],
            "title": "When does label smoothing help",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Mahdi Pakdaman Naeini",
                "Gregory Cooper",
                "Milos Hauskrecht"
            ],
            "title": "Obtaining well calibrated probabilities using bayesian binning",
            "venue": "In Twenty-Ninth AAAI Conference on Artificial Intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Jeremy Nixon",
                "Michael W Dusenberry",
                "Linchuan Zhang",
                "Ghassen Jerfel",
                "Dustin Tran"
            ],
            "title": "Measuring calibration in deep learning",
            "venue": "In CVPR Workshops,",
            "year": 2019
        },
        {
            "authors": [
                "Hyun Oh Song",
                "Yu Xiang",
                "Stefanie Jegelka",
                "Silvio Savarese"
            ],
            "title": "Deep metric learning via lifted structured feature embedding",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Maxime Oquab",
                "Timoth\u00e9e Darcet",
                "Th\u00e9o Moutakanni",
                "Huy Vo",
                "Marc Szafraniec",
                "Vasil Khalidov",
                "Pierre Fernandez",
                "Daniel Haziza",
                "Francisco Massa",
                "Alaaeldin El-Nouby"
            ],
            "title": "Dinov2: Learning robust visual features without supervision",
            "venue": "arXiv preprint arXiv:2304.07193,",
            "year": 2023
        },
        {
            "authors": [
                "Yash Patel",
                "Giorgos Tolias",
                "Jiri Matas"
            ],
            "title": "Recall@k surrogate loss with large batches and similarity mixup",
            "year": 2022
        },
        {
            "authors": [
                "Gabriel Pereyra",
                "George Tucker",
                "Jan Chorowski",
                "Lukasz Kaiser",
                "Geoffrey Hinton"
            ],
            "title": "Regularizing neural networks by penalizing confident output distributions",
            "venue": "In 5th International Conference on Learning Representations (ICLR",
            "year": 2017
        },
        {
            "authors": [
                "John Platt"
            ],
            "title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods",
            "venue": "Advances in large margin classifiers,",
            "year": 1999
        },
        {
            "authors": [
                "Qi Qian",
                "Lei Shang",
                "Baigui Sun",
                "Juhua Hu",
                "Hao Li",
                "Rong Jin"
            ],
            "title": "Softtriple loss: Deep metric learning without triplet sampling",
            "venue": "CoRR, abs/1909.05235,",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Elias Ramzi",
                "Nicolas THOME",
                "Cl\u00e9ment Rambour",
                "Nicolas Audebert",
                "Xavier Bitot"
            ],
            "title": "Robust and decomposable average precision for image retrieval",
            "venue": "In Thirty-Fifth Conference on Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Joshua Robinson",
                "Ching-Yao Chuang",
                "Suvrit Sra",
                "Stefanie Jegelka"
            ],
            "title": "Contrastive learning with hard negative samples",
            "venue": "arXiv preprint arXiv:2010.04592,",
            "year": 2020
        },
        {
            "authors": [
                "Yaniv Romano",
                "Matteo Sesia",
                "Emmanuel Candes"
            ],
            "title": "Classification with valid and adaptive coverage",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Karsten Roth",
                "Timo Milbich",
                "Samarth Sinha",
                "Prateek Gupta",
                "Bjorn Ommer",
                "Joseph Paul Cohen"
            ],
            "title": "Revisiting training strategies and generalization performance in deep metric learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Karsten Roth",
                "Oriol Vinyals",
                "Zeynep Akata"
            ],
            "title": "Non-isotropy regularization for proxy-based deep metric learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Yutaka Sasaki"
            ],
            "title": "The truth of the f-measure",
            "venue": "Teach tutor mater,",
            "year": 2007
        },
        {
            "authors": [
                "Walter J Scheirer",
                "Anderson de Rezende Rocha",
                "Archana Sapkota",
                "Terrance E Boult"
            ],
            "title": "Toward open set recognition",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2012
        },
        {
            "authors": [
                "Florian Schroff",
                "Dmitry Kalenichenko",
                "James Philbin"
            ],
            "title": "Facenet: A unified embedding for face recognition and clustering",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Florian Schroff",
                "Dmitry Kalenichenko",
                "James Philbin"
            ],
            "title": "Facenet: A unified embedding for face recognition and clustering",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Hyun Oh Song",
                "Yu Xiang",
                "Stefanie Jegelka",
                "Silvio Savarese"
            ],
            "title": "Deep metric learning via lifted structured feature embedding",
            "venue": "CoRR, abs/1511.06452,",
            "year": 2015
        },
        {
            "authors": [
                "Eu Wern Teh",
                "Terrance DeVries",
                "Graham W. Taylor"
            ],
            "title": "Proxynca++: Revisiting and revitalizing proxy neighborhood component analysis",
            "venue": "CoRR, abs/2004.01113,",
            "year": 2020
        },
        {
            "authors": [
                "Ryan J Tibshirani",
                "Rina Foygel Barber",
                "Emmanuel Candes",
                "Aaditya Ramdas"
            ],
            "title": "Conformal prediction under covariate shift",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "A\u00e4ron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "CoRR, abs/1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "Andreas Veit",
                "Kimberly Wilber"
            ],
            "title": "Improving calibration in deep metric learning with crossexample softmax",
            "venue": "arXiv preprint arXiv:2011.08824,",
            "year": 2020
        },
        {
            "authors": [
                "Catherine Wah",
                "Steve Branson",
                "Peter Welinder",
                "Pietro Perona",
                "Serge J. Belongie"
            ],
            "title": "The caltechucsd birds-200-2011 dataset",
            "year": 2011
        },
        {
            "authors": [
                "Hao Wang",
                "Yitong Wang",
                "Zheng Zhou",
                "Xing Ji",
                "Dihong Gong",
                "Jingchao Zhou",
                "Zhifeng Li",
                "Wei Liu"
            ],
            "title": "Cosface: Large margin cosine loss for deep face recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Xun Wang",
                "Xintong Han",
                "Weilin Huang",
                "Dengke Dong",
                "Matthew R Scott"
            ],
            "title": "Multi-similarity loss with general pair weighting for deep metric learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Ross Wightman"
            ],
            "title": "Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Chao-Yuan Wu",
                "R. Manmatha",
                "Alexander J. Smola",
                "Philipp Kr\u00e4henb\u00fchl"
            ],
            "title": "Sampling matters in deep embedding learning",
            "venue": "CoRR, abs/1706.07567,",
            "year": 2017
        },
        {
            "authors": [
                "Zhenlin Xu",
                "Yi Zhu",
                "Tiffany Deng",
                "Abhay Mittal",
                "Yanbei Chen",
                "Manchen Wang",
                "Paolo Favaro",
                "Joseph Tighe",
                "Davide Modolo"
            ],
            "title": "Challenges of zero-shot recognition with vision-language models: Granularity and correctness",
            "venue": "arXiv preprint arXiv:2306.16048,",
            "year": 2023
        },
        {
            "authors": [
                "Hong Xuan",
                "Abby Stylianou",
                "Xiaotong Liu",
                "Robert Pless"
            ],
            "title": "Hard negative examples are hard, but useful",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Nikolaos-Antonios Ypsilantis",
                "Kaifeng Chen",
                "Bingyi Cao",
                "M\u00e1rio Lipovsk\u1ef3",
                "Pelin DoganSch\u00f6nberger",
                "Grzegorz Makosa",
                "Boris Bluntschli",
                "Mojtaba Seyedhosseini",
                "Ond\u0159ej Chum",
                "Andr\u00e9 Araujo"
            ],
            "title": "Towards universal image embeddings: A large-scale dataset and challenge for generic image representations",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Yaodong Yu",
                "Kwan Ho Ryan Chan",
                "Chong You",
                "Chaobing Song",
                "Yi Ma"
            ],
            "title": "Learning diverse and discriminative representations via the principle of maximal coding rate reduction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Bianca Zadrozny",
                "Charles Peter Elkan"
            ],
            "title": "Transforming classifier scores into accurate multiclass probability estimates",
            "venue": "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2002
        },
        {
            "authors": [
                "Kai Zhao",
                "Jingyi Xu",
                "Ming-Ming Cheng"
            ],
            "title": "Regularface: Deep face recognition via exclusive regularization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Patel"
            ],
            "title": "and variances in relative values will naturally increase when absolute metric values get small. This behavior is attributed to the general tendency for relative ranks to become more unstable as absolute metric values decrease \u2013 a phenomenon observed across numerous evaluation metrics including recall@k as shown in Brown et al",
            "year": 2020
        },
        {
            "authors": [
                "2016 Oh Song et al",
                "2017 Wu et al",
                "Wang"
            ],
            "title": "2019) has consistently demonstrated that hard negative pairs generally convey more information than hard positive pairs. As shown in Table 8, when comparing RS@K+TCM and RS@K+ROADMAP at a batch size of 4000, using ROADMAP as a regularization results in a significant reduction in accuracy, aligning with our earlier analysis",
            "year": 2024
        },
        {
            "authors": [
                "Patel"
            ],
            "title": "Si for the positive samples against xi, Recall@k Surrogate The Recall@k Surrogate loss mirrors Smooth-AP\u2019s approach but approximates the Recall@k metric rather than the average precision metric",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep metric learning (DML) has shown success in various open-world recognition and retrieval tasks (Schroff et al., 2015a; Wu et al., 2017; Deng et al., 2019; Wang et al., 2018). Nevertheless, the common DML losses, such as contrastive loss (van den Oord et al., 2018; Chen et al., 2020), pairwise loss (Brown et al., 2020; Patel et al., 2022) and proxy-based losses (Kim et al., 2020; Movshovitz-Attias et al., 2017; Qian et al., 2019; Deng et al., 2019), often yield highly varied intraclass and inter-class representation structures across classes (Liu et al., 2019; Duan et al., 2019; Zhao et al., 2019). Hence, even if an embedding model has strong separability, distinct classes may still require varying thresholds to uphold a consistent operating point in terms of false reject rate (FRR) or false acceptance rate (FAR). This challenge is particularly important in real-world image retrieval systems, where a threshold-based retrieval criterion is preferred over a top-k approach due to its ability to identify negative queries without matches in the gallery. However, selecting the right threshold is difficult, especially when systems must cater to diverse use-cases. For instance, in clothing image retrieval for online shopping, the similarity between two T-shirts can be significantly different from that between two coats. A threshold that works well for coats may lead to poor relevancy and give many false positives in the retrieved images for T-shirts, as shown in Figure 1. These difficulties are more pronounced in the open-world scenarios (Scheirer et al., 2012; Bendale & Boult, 2015; 2016), where the test classes may include entirely new classes not seen during training.\nWe define the phenomenon in DML, where different test classes and distributions require varying distance thresholds to achieve a similar retrieval or recognition accuracy, as threshold inconsistency. In commercial environments, particularly under the practical evaluation and deployment\n*Equal contribution.\nsetting with one fixed threshold for diverse user groups (Liu et al., 2022), the significance of threshold inconsistency cannot be overstated. Accurate quantification of this inconsistency is essential for detecting potential biases in the chosen threshold. To this end, we introduce a novel evaluation metric, named Operating-Point-Inconsistency-Score (OPIS), which quantifies the variance in the operating characteristics across classes within a target performance range. Using OPIS, we observe an accuracy-threshold consistency Pareto frontier in the high accuracy regime, where methods to improve accuracy often result in a degradation in threshold consistency, as shown in Figure 3. This highlights that achieving high accuracy does not inherently guarantee threshold consistency.\nOne solution to this problem is using posthoc calibration methods (Platt et al., 1999; Zadrozny & Elkan, 2002; Guo et al., 2017a), which adjust a trained model\u2019s distance thresholds to align with specific operating points in FAR or FRR. However, in real-world settings, these methods can be inefficient and lack robustness, as they involve constructing separate calibration datasets and may require prior knowledge about the test distribution for effective calibration (Naeini et al., 2015; Guo et al., 2017a). Moreover, they do not address the threshold inconsistency problem unless customized calibration is done for each user. Another option is employing conformal prediction (Romano et al., 2020; Gibbs & Candes, 2021), which guarantees confidence probability coverage and can handle complex data distributions as well as covariate and label shifts. However, conformal prediction inherently assumes a closed-world setting, where training and test samples share the same label space. In contrast, real-world image retrieval systems typically operate in an open-world environment, presenting a more complex and realistic setting with unknown classes at test time.\nGiven these challenges, an essential question arises: Can we train an embedding model for openworld image retrieval that sustains a consistent distance threshold across diverse data distributions, thus avoiding the complexities of posthoc threshold calibration? This objective falls within the scope of calibration-aware training. In closed-set classification, the goal of calibration-aware training is to align predicted confidence probabilities with empirical correctness of the model (Guo et al., 2017a; M\u00fcller et al., 2019; Mukhoti et al., 2020). However, our focus lies on what we term as thresholdconsistent DML, a paradigm that trains an embedding model with reduced threshold inconsistencies, such that a universal distance threshold can be applied to different test distributions to attain a similar level of FAR or FRR. This differentiation is crucial because in DML the output similarity score does not strictly reflect the empirical correctness of the model (Xu et al., 2023) and may exhibit strong variations across test data distributions. To address the unique challenges of threshold inconsistency in DML, we propose a simple yet effective regularization technique called Threshold-Consistent Margin (TCM) loss. Through experiments on four standard image retrieval benchmarks, we validate the efficacy of the TCM regularization in improving threshold consistency while maintaining accuracy. To summarize, our contributions are as follows:\n\u2022 We propose a novel variance-based metric, named Operating-Point-Inconsistency-Score (OPIS), to quantify the threshold inconsistency of a DML model. Notably, OPIS does not need a separate hold-out dataset besides the test set, enhancing flexibility in evaluation.\n\u2022 We observe an accuracy-threshold consistency Pareto frontier in the high accuracy regime. This finding underscores that achieving high model accuracy in DML does not automatically guarantee threshold consistency, necessitating dedicated solutions.\n\u2022 We introduce the Threshold-Consistent Margin (TCM) loss, a simple yet effective regularization technique, that can be combined with any base losses and backbone architecture to improve threshold consistency in DML. Our approach outperforms SOTA methods across various standard image retrieval benchmarks, demonstrating substantial improvements in threshold consistency while maintaining or even enhancing accuracy."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": "DML losses for image retrieval Advancements in DML losses for image retrieval have focused on improving accuracy, scalability and generalization Brown et al. (2020); Patel et al. (2022); Deng et al. (2020); Kim et al. (2023); Roth et al. (2020); Kan et al. (2022); Ypsilantis et al. (2023). The pioneering work of the Smooth-AP loss (Brown et al., 2020) optimizes a smoothed approximation for the average precision. Similarly, the Recall@k Surrogate loss (Patel et al., 2022) approximates the recall@k metric. Leveraging vision-transformer backbones and large batch sizes, Recall@k Surrogate has achieved remarkable performance in several image retrieval benchmarks. However, these pairwise methods are inefficient when dealing with a large number of classes. To reduce the computational complexity, proxy-based methods such as ProxyAnchor (Kim et al., 2020), ProxyNCA (Movshovitz-Attias et al., 2017), SoftTriple (Qian et al., 2019), ArcFace (Deng et al., 2019), and HIER (Kim et al., 2023) are employed, where sample representations are compared against class prototypes. Despite high accuracy, these methods still face challenges in biases and fairness (Fang et al., 2013; Ilvento, 2019; Dullerud et al., 2022) and display inconsistencies in distance thresholds when applied in real-world scenarios (Liu et al., 2022).\nEvaluation Metrics for threshold consistency (inconsistency) In closed-set classification, threshold consistency is usually evaluated through calibration metrics, such as Expected Calibration Error (ECE) (Naeini et al., 2015), Maximum Calibration Error (MCE) (Guo et al., 2017b) and Adaptive ECE (Nixon et al., 2019). These metrics gauge how well a model\u2019s predictions match actual correctness. However, directly applying them to evaluate threshold consistency in DML (e.g., by replacing confidence probability with similarity measures) is not straightforward. A key hurdle is that DML uses distance measurements to represent semantic similarities, and these distances can vary widely across different classes due to the intrinsic non-bijectiveness of semantic similarity in the data (Roth et al., 2022). In the context of DML, OneFace (Liu et al., 2022) introduced the calibration threshold for face recognition systems, which corresponds to the distance threshold at a given FAR of a separate calibration dataset. They further propose the One-Threshold-for-All (OTA) evaluation protocol to measure the difference in the accuracy performance across datasets at this calibration threshold as an indicator for threshold consistency. However, this approach requires a dedicated calibration dataset, which can be difficult to acquire in practice. To our knowledge, there is no widely accepted and straightforward metric for threshold consistency in DML.\nCalibration-aware training vs Posthoc threshold calibration Calibration-aware training has been well studied in closet-set classification, where the goal is to align predicted probabilities with empirical correctness (Guo et al., 2017a; M\u00fcller et al., 2019; Mukhoti et al., 2020). Common approaches use a regularizer to guide the model in generating more calibrated predictions (Pereyra et al., 2017; Liang et al., 2020; Hebbalaguppe et al., 2022). Yet, threshold-consistent training for DML differs from calibration-aware training. Instead of aligning model output with empirical correctness, threshold-consistent DML seeks to maintain a consistent distance threshold across classes and data distributions. In face recognition, Liu et al. (2022) introduces the Threshold Consistency Penalty to improve threshold consistency among various face domains. The method divides mini-batch data into 8 domains and computes each domain threshold using a large set of negative pairs from a feature queue. It then adjusts the loss contribution from each sample based on the ratio of its domain threshold to the in-batch calibration threshold. However, this method is designed for face recognition \u2013 a more constrained scenario. In contrast, our target is general image retrieval tasks which can involve significantly more domains, making it impractical to construct negative pairs for all domains. Besides train-time methods, another approach is posthoc threshold calibration, such as Platt calibration (Platt et al., 1999), isotonic regression (Zadrozny & Elkan, 2002) and temperature scaling (Guo et al., 2017a), which seeks to calibrate the operating point of a trained model using\nhold-out calibration datasets. However, it cannot solve threshold inconsistency unless customized calibration is conducted for each user. Another category of posthoc calibration method is conformal prediction (Tibshirani et al., 2019; Romano et al., 2020; Gibbs & Candes, 2021; Barber et al., 2023), which can be applied beyond the setting of exchangeable data even when the training and test data are drawn from different distributions. However, conformal prediction relies on a closed-set setting where the training and test data share the same label space, which does not apply to open-world image retrieval. Thus, in this work, we focus on developing a threshold-consistent training technique tailored for DML, with the goal of simplifying the posthoc calibration process in practical settings."
        },
        {
            "heading": "3 THRESHOLD INCONSISTENCY IN DEEP METRIC LEARNING",
            "text": "Visualizing threshold inconsistency in image retrieval We visually illustrate the issue of threshold inconsistency in DML using image retrieval datasets. First, we borrow the widely-used F - score (Sasaki et al., 2007) to define the utility score, incorporating both sides of the accuracy metric (e.g. precision and recall, or specificity and sensitivity). Specifically, we denote one side as \u03d5 and the other side as \u03c8, and define the utility score, denoted as U, as follows:\nU(d) = (1 + c2) \u00b7 \u03d5(d) \u00b7 \u03c8(d)\nc2\u03d5(d) + \u03c8(d) (1)\nwhere d is the distance threshold (d \u2208 [0, 2] for hyperspherical embeddings), and c is the relative importance of \u03c8 over \u03d5 (c = 1 if not specified). Without loss of generality, we let \u03d5 be specificity (same as TNR or 1\u2212 FAR) and \u03c8 be sensitivity (same as recall or 1\u2212 FRR).1\nIn Figure 2, we present the accuracy utility-distance threshold curves for the test classes using models trained on the iNaturalist-2018 (Horn et al., 2017) and Cars-196 (Krause et al., 2013) datasets. In the left column of each subfigure, we observe considerable variations in the operating characteristics among distinct classes for models trained with the popular Smooth-AP loss. These variations make it difficult to select a single distance threshold that works well across the entire spectrum of test distributions. However, while we will elaborate on in later sections, incorporating our proposed TCM regularization during training visibly improves the threshold consistency across classes, as evidenced by the more aligned utility curves compared to those without the TCM regularization.\nOPIS for overall threshold inconsistency To quantify threshold inconsistency in DML, we introduce a variance-based metric, Operating-Point-Inconsistency Score (OPIS). Unlike the OTA evaluation proposed in Liu et al. (2022), OPIS does not require a separate calibration dataset. It quantifies the variance in the operating characteristics across test classes in a predefined calibration range of distance thresholds. This calibration range, denoted as [dmin, dmax], is typically determined based on the target performance metric operating ranges (e.g., a <FAR< b, where a, b are pre-determined error constraints). Formally, the OPIS metric can be expressed as follows:\nOPIS =\n\u2211T i=1 \u222b dmax dmin\n||Ui(d)\u2212 U\u0304(d)||2 dd T \u00b7 (dmax \u2212 dmin)\n(2)\n1We employ the specificity and sensitivity pair because they are particularly relevant for visual recognition applications and are not sensitive to changes in test data composition.\nwhere i = 1, 2, ..., T is the index for the test classes, Ui(d) is the accuracy utility for class i, and U\u0304(d) is the average utility for the entire test dataset.\n\u03f5-OPIS for utility divide between groups The overall OPIS metric does not emphasize on the outlier classes. For applications where outlier threshold consistency is essential, we also provide a more fine-grained metric that focuses on the utility disparity between the best and worst sub-groups. First, we define the utility of the \u03b5 percentile of best-performing classes as follows:\nU\u03b5best(d) = \u03d5\u03b5best(d) \u00b7 \u03c8\u03b5best(d) \u03d5\u03b5best(d) + \u03c8\u03b5best(d)\n(3)\nwhere \u03d5\u03b5best(d), \u03c8\u03b5best(d) are the expected accuracy metrics for the entirety of the \u03b5 percentile of the best-performing classes. By replacing \u03b5best with \u03b5worst, the same can be defined for U\u03b5worst(d). Then, we define the \u03b5-OPIS metric as the following:\n\u03b5-OPIS = 1\ndmax \u2212 dmin \u222b dmax dmin ||U\u03b5worst(d)\u2212U\u03b5best(d)|| 2 dd (4)\nBy definition, the \u03b5-OPIS metric is maximized at \u03b5 \u2192 0, and eventually becomes zero when \u03b5 \u2192 100% as the best-performing set and worst-performing set become identical.\nHigh accuracy \u0338= High threshold consistency In Figure 3, we employ the OPIS metric to examine the relations between threshold inconsistency and recognition error in embedding models trained with various DML losses, backbones and batch sizes. Notably, we observe distinct behaviors across different accuracy regimes. In the low-accuracy regime, located in the right of the plot, we notice a simultaneous improvement of accuracy and threshold consistency. This aligns with the established notion that improving model discriminability helps threshold consistency by strengthening the association between samples and their corresponding class centroids. However, as the error decreases, a trade-off surfaces in the high-accuracy regime. Here, the reduction in error is correlated with increased threshold inconsistency, leading to the formation of a Pareto frontier.\nThe trade-off between recognition error and threshold inconsistency highlights that achieving high accuracy alone does not automatically guarantee threshold consistency. In this context, introducing the proposed OPIS metric as an additional evaluation criterion alongside recall@k is crucial for threshold-based commer-\ncial DML applications, where the ability to identify negative queries without matching classes in the gallery is of importance. To explain further, we compare OPIS with the widely-used accuracy metric, recall@k. These two metrics evaluate different aspects of a model and can be used complementarily: recall@k focuses on top-k relevancy (retrieving top-k similar samples as the query from a collection), and OPIS measures the inconsistency in threshold-relevancy (retrieving similar examples above a threshold from a collection). Moreover, unlike recall@k that solely gauges recall, OPIS evaluates both the FAR and FRR (=recall), offering a more holistic error assessment."
        },
        {
            "heading": "4 TOWARDS THRESHOLD-CONSISTENT DEEP METRIC LEARNING",
            "text": "To tackle the threshold inconsistency problem, we introduce the Threshold-Consistent Margin (TCM) loss. TCM specifically penalizes hard positive and hard negative sample pairs near the decision boundaries outlined by a pair of cosine margins. This strategy is in line with several studies (Dong et al., 2017; Xuan et al., 2020; Robinson et al., 2020) that emphasize hard mining for\nextracting more informative samples. Let S+ and S\u2212 be the sets of cosine similarity scores for positive and negative pairs in a mini-batch, respectively, the TCM loss is formulated as follows:\nLTCM = \u03bb + \u00b7\n\u2211 s\u2208S+(m\n+ \u2212 s) \u00b7 1s\u2264m+\u2211 s\u2208S+ 1s\u2264m+\n+ \u03bb\u2212 \u00b7 \u2211\ns\u2208S\u2212(s\u2212m\u2212) \u00b7 1s\u2265m\u2212\u2211 s\u2208S\u2212 1s\u2265m\u2212\n(5)\nwhere 1condition = 1 if the condition is true, and 0 otherwise. \u03bb+ and \u03bb\u2212 are the weights assigned to the positive and negative regularizations, respectively. The TCM regularizer can be combined with any base loss Lbase, resulting in the final objective function:\nLfinal = Lbase + LTCM (6) Design justification: representation structures Several works have shown a strong correlation between model accuracy and representation structures (Yu et al., 2020; Chan et al., 2022). Indeed, SOTA DML losses are designed to optimize this relationship by encouraging intra-class compactness and inter-class discrimination. However, when considering threshold consistency, the focus shifts towards achieving consistent performance in FAR and FRR in the calibration range, with an emphasis on local representation structures near the distance threshold. In this context, the TCM regularization serves as a \u201clocal inspector\" by selectively adjusting hard samples to prevent over separateness and excessive compactness in the vicinity of the margin boundaries. This strategy also aligns with previous work that found excessive feature compression actually hurts DML generalization (Roth et al., 2020). Since the margin constraints are applied globally, this helps encourage more equidistant distribution of class centroids and more uniform representation compactness across different classes in the embedding space.\nHard mining strategy TCM regularizes on hard samples, distinguishing it from techniques that encourage similarity consistency by minimizing marginal variance (Kan et al., 2022). Specifically, TCM\u2019s hard mining strategy is different from the semi-hard negative mining strategy (Schroff et al., 2015b) and its variants (Oh Song et al., 2016; Wu et al., 2017; Wang et al., 2019), as TCM\u2019s hard mining is based on the absolute cosine similarity values, rather than their relative differences. Meanwhile, TCM also differs from ROADMAP (Ramzi et al., 2021) in that TCM utilizes hard positive and negative counts, whereas ROADMAP uses the total positive and negative counts. This makes TCM well-suited for scenarios involving large batch sizes (as is the standard in DML) and significant imbalances between the counts of positive and negative pairs2.\nConnection to the calibration range TCM is implicitly connected to the calibration range of the OPIS metric through the two cosine margins. Since cosine similarity is bijective with the L2 distance for hyperspherical embeddings, these margin constraints ensure that the model\u2019s intra-class and inter-class representation structures adhere to the desired distance threshold range, which is [ \u221a 2\u2212 2m+, \u221a 2\u2212 2m\u2212]. However, due to the inevitable distributional shift between the training and testing datasets, the selection of the margin constraints requires some hyper-parameter tuning and cannot be directly estimated from the calibration range. In Figure 6, we give guidance on how to select the margins, with details discussed in the ablation of TCM margin hyperparameters.\nTCM vs Margin-based Softmax loss TCM has distinguishing characteristics when compared to margin-based softmax losses (Deng et al., 2019; Qian et al., 2019), as illustrated in Figure 4(b).\n2An detailed comparison between TCM and the method of Ramzi et al. (2021) is given in appendix A.2.2\nFirst, TCM is designed as a regularizer that operates in conjunction with a base loss. It specifically applies to hard sample pairs that are located near the margin boundaries. Second, TCM employs two cosine margins to regularize the intra-class and inter-class distance distributions simultaneously. This allows TCM to capture both hard positive and hard negative examples, resulting in more hard pairs within a mini-batch. Secondly, the TCM loss is specifically applied to the hard pairs, contrasting with Arcface, which is applied to all pairs. Last, TCM is a sample-level pair-wise loss, which better models the relationships between individual samples compared to proxy-based methods.\nVisualization of TCM effect We visualize the effect of the TCM regularization on representation structures across the 10 classes in the MNIST dataset of handwritten digits (LeCun et al., 1998) by training a shallow CNN using the Arcface loss (Deng et al., 2019). For clearer visualization, we use two-dimensional features and employ kernel-density estimation (Chen, 2017) to model the probability density function for the embeddings of each class. As shown in Figure 5, compared to using ArcFace (Deng et al., 2019) only, the incorporation of TCM (ArcFace+TCM) enhances the separation between digits 2 and 5 (lower middle), 0 and 8 (lower right), and 4 and 9 (upper left). This observation supports our claims about TCM\u2019s ability in refining the representation structures for improved threshold consistency."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "5.1 DATASETS AND IMPLEMENTATION DETAILS",
            "text": "Datasets For training and evaluation, we use four commonly-used image retrieval benchmarks, namely iNaturalist-2018 (Horn et al., 2017), Stanford Online Product (Song et al., 2015), CUB-2002011 (Wah et al., 2011) and Cars-196 (Krause et al., 2013). These benchmarks cover a diverse set of data domains including natural species, online catalog images, birds, and cars. As in previous works (Brown et al., 2020; Patel et al., 2022; An et al., 2023), the iNaturalist and Stanford Online Product datasets use an open-world train-test-split, where the training classes are disjoint from the ones in testing. For CUB and Cars, we use shared train-test classes to make fair comparisons with prior DML methods3. The details to each dataset can be found in Table 1.\nEvaluation metrics We measure model accuracy using the recall@k metric and assess threshold inconsistency using the OPIS and \u03f5-OPIS metrics as defined earlier. Similar to previous works (Veit & Wilber, 2020; Liu et al., 2022), we estimate threshold inconsistency by comparing normalized features of image pairs in 1:1 comparisons. In the case of the iNaturalist-2018 and Stanford Online Product datasets, given the large number of classes, we only sample positive pairs exhaustively and randomly sample negative pairs with a fixed negative-to-positive ratio of 10-to-1 for each class. All positive and negative pairs in the CUB and Cars datasets are exhaustively sampled.\nImplementation details We use two backbone architectures, namely ResNet (He et al., 2016) and Vision Transformer (Dosovitskiy et al., 2020), both pretrained on ImageNet4. Since the original papers do not report OPIS, we train both baseline models (without TCM) and TCM-regularized models using the same configuration. The hyperparameters for each base loss are taken from the original papers. For TCM, we set \u03bb+ = \u03bb\u2212 = 1. For OPIS, the calibration range is set to 1e-2 \u2264 FAR \u2264 1e-1 for all benchmarks. The margin parameters (m+, m\u2212) are tuned using grid search on 10% of the training data for each benchmark. We adopt the same optimization schemes as specified\n3We also provide results for CUB and Cars in the open-world setting in Appendix A.2.5. 4For ResNet, we follow Brown et al. (2020) and use ImageNet-pretrained backbones. For ViTs, we follow Patel et al. (2022) and use ImageNet-21k pretrained backbones released by timm library (Wightman, 2019).\nin the original papers for each base loss. During training, mini-batches are generated by randomly sampling 4 images per class following previous works (Brown et al., 2020; Patel et al., 2022)."
        },
        {
            "heading": "5.2 ABLATION AND COMPLEXITY ANALYSIS",
            "text": "Unless stated otherwise, all ablation studies are conducted using the iNaturalist-2018 dataset. Owing to space constraints, further ablations can be found in the appendix.\nEffect of TCM margins We examine the impact of the cosine margins m+, m\u2212 on accuracy and OPIS. As shown in Figure 6, adding TCM consistently enhances threshold consistency compared to the baseline Smooth-AP loss across all combinations of margins, with up to 50% of reduction\nin OPIS. Regarding accuracy, we observe that the negative margin (m\u2212) has a greater influence than the positive margin (m+), which aligns with previous works (Dong et al., 2017; Xuan et al., 2020; Robinson et al., 2020). However, when the negative margin becomes excessively stringent, such as m\u2212 = 0.25, the accuracy drops below the baseline. We hypothesize that an overly restrictive negative margin may interfere with the base loss, leading to decreased accuracy. For ImageNet-pretrained backbones, the recommended values for m+ and m\u2212 are around 0.9 and 0.5, respectively.\nCompatibility with various base DML losses We select the most representative DML losses for each method category, including proxy-based methods (Movshovitz-Attias et al., 2017; Deng et al., 2020) and pairwise-based methods (Brown et al., 2020; Patel et al., 2022). Notably, the Recall@k surrogate loss (Patel et al., 2022) represents the SOTA loss for fine-grained image retrieval tasks. We run experiments using these base losses with and without the TCM regularization. As shown in Table 2, there is a consistent improvement in both accuracy (> 1.0% increase in recall@1) and threshold consistency (up to 60.7% in relative reduction) when TCM regularization is applied in conjunction with different high-performing base losses.\nCompatibility with different architectures We investigate the compatibility of TCM regularization with different backbone architectures including ResNet variants and Vision Transformers. As shown in Table 3, we observe significant improvements in threshold consistency across backbone architectures when TCM is incorporated. On accuracy, ResNet models exhibit more notable improvements in accuracy (> 1.5%) compared to Vision Transformers, which see a < 1.0% boost.\nTime Complexity In a mini-batch with size n, the complexity of TCM is O(n2) as it compares every sample with all samples in the mini-batch. For image retrieval benchmarks where the number of training classes K is significantly greater than the batch size n, i.e., K \u226b n, this complexity is comparable to most pair-based losses (O(n2)) and proxy-based losses (O(nK)). In Table 4, we provide time complexities for the loss computation, the forward and backward passes and the overall\ntime per epoch. The results suggest that adding TCM regularization results in a negligible (< 1.5%) increment in the overall training time per epoch."
        },
        {
            "heading": "5.3 IMAGE RETRIEVAL EXPERIMENT",
            "text": "The results for supervised fine-tuning for image retrieval benchmarks with and without the TCM regularizer are summarized in Table 5. As is shown, our TCM loss is effective in improving threshold consistency (measured by OPIS and \u03f5-OPIS, the lower the better), by up to 77.3%, compared to the various baseline losses considered. Meanwhile, adding TCM regularization consistently improves accuracy across almost all benchmarks, base losses and backbone architectures. While we notice a slight decrease in recall@1 on the two smaller datasets (as marked in red), namely CUB and Cars, these are at the same magnitude as non-significant variations due to random initialization during training. It\u2019s worth highlighting that on iNaturalist-2018, arguably the largest public image retrieval benchmark, adding our TCM regularization is shown to out-perform SOTA DML loss, recall@k surrogate, reducing the OPIS threshold inconsistency score from 0.37\u00d710\u22123 to 0.17\u00d710\u22123, while improving the recall@1 accuracy metrics from 83.9% to 84.8%."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this work, we comprehensively study the issue of threshold inconsistency in deep metric learning. We introduce a novel variance-based metric named Operating-Point-Inconsistency-Score (OPIS) to quantify threshold inconsistency among different classes. Distinct from the One-Threshold-for-All evaluation protocol proposed by Liu et al. (2022), a key advantage of OPIS is its elimination of the need for a separate calibration dataset. As a result, OPIS can be easily utilized alongside existing accuracy metrics, providing an added dimension for evaluating the threshold robustness of trained DML models. With the OPIS metric, we find that achieving high accuracy in a DML model does not necessarily guarantee threshold consistency. To address this issue, we propose the ThresholdConsistent Margin loss (TCM), a simple and versatile regularization technique that can be integrated with any base loss and backbone architecture to improve the model\u2019s threshold consistency during training. TCM is designed to enforce more uniform intra-class compactness and inter-class separability across diverse classes in the embedding space. By incorporating TCM, we demonstrate state-of-the-art performance in both threshold consistency and accuracy across various image retrieval benchmarks. We hope that our work serves as a catalyst to encourage more explorations in developing threshold-consistent DML solutions for practical open-world scenarios. Limitations of OPIS The OPIS and \u03f5-OPIS metrics necessitate a sufficient number of samples per class to ensure statistical significance, making them unsuitable for few-shot evaluation scenarios. Limitations of TCM Like other inductive deep learning methods, TCM can fail when there\u2019s a significant distribution shift between the training and test sets or when strong label noise is present."
        },
        {
            "heading": "A APPENDIX",
            "text": "In the appendix, we offer in-depth theoretical analyses of our proposed OPIS metric (refer to Appendix A.1), conduct additional ablation studies for the TCM regularization (see Appendix A.2), and provide more implementation details (see Appendix A.3)."
        },
        {
            "heading": "A.1 OPERATING-POINT-INCONSISTENCY SCORE",
            "text": "In this section, we provide theoretical analyses for the utility score and the OPIS metric, grounded in Gaussian assumptions. Leveraging these assumptions, we establish upper and lower bounds for OPIS as benchmark values."
        },
        {
            "heading": "A.1.1 UTILITY SCORE ANALYSIS: A GAUSSIAN MODEL",
            "text": "For a specific class, we assume that its L2 distance distributions between the embeddings of positive sample pairs and between the embeddings of negative sample pairs follow a Gaussian distribution. We represent the L2 distance distribution for positive pairs as d \u223c N (\u00b5pos, \u03c3pos) and that for negative pairs as d \u223c N (\u00b5neg, \u03c3neg), with the constraint 0 < \u00b5pos < \u00b5neg < 2 for hyperspherical embeddings. Given a distance threshold d, we can estimate the fractions of true positive (TP), false negative (FN), false positive (FP) and true negative (TN) as follows:\nPredicted Positive Negative\nA ct\nua l Positive TP = 1+erf(tpos)2 FN = 1\u2212erf(tpos) 2\nNegative FP = 1+erf(tneg)2 TN = 1\u2212erf(tneg) 2\nHere, erf(t) represents the Gaussian error function (Andrews, 1998), and tpos = d\u2212\u00b5pos\u221a 2\u03c3pos\nand tneg = d\u2212\u00b5neg\u221a 2\u03c3neg\n. Thus we can express the accuracy metrics for precision, recall, specificity, and sensitivity as follows:\nprecision = 1 + erf(tpos)\n2 + erf(tpos) + erf(tneg) , recall =\n1\n2\n( 1 + erf(tpos) ) (7)\nspecificity = 1\n2\n( 1\u2212 erf(tneg) ) , sensitivity = 1\n2\n( 1 + erf(tpos) ) (8)\nPlugging these into the utility score Uroc defined by the specificity and sensitivity pair yields:\nUroc(d) = (1 + erf(tpos)) \u00b7 (1\u2212 erf(tneg))\n2 + erf(tpos)\u2212 erf(tneg) (9)\nSimilarly, the utility score defined based on the precision and recall pair, denoted as Upr (equivalent to F1 score), can be expressed as the following:\nUpr(d) = 2 \u00b7 (1 + erf(tpos))\n4 + erf(tpos) + erf(tneg) (10)\nFigure 7 gives typical utility score curves. These curves are derived from representative values (indicated in the subtitles) for the mean and standard deviation of the intra-class and inter-class L2 distance distributions of embeddings generated using the Gaussian distribution model. Notably, the left part of the Upr curve and the Uroc curve are nearly indistinguishable. This similarity arises because erf(tneg) \u2192 \u22121 when tneg \u2264 \u22122, leading to Upr(d) \u2248 Uroc(d) \u2248 23+erf(tpos) when d < 1.0. Meanwhile, both Uroc and Upr utility score curves exhibit a concave shape, indicating a maximum utility attainable at an optimal distance threshold."
        },
        {
            "heading": "A.1.2 LOWER AND UPPER BOUNDS OF OPIS",
            "text": "Lower bound The OPIS threshold inconsistency metric measures the variance in the utility curves across different test classes. Consequently, its theoretical lower bound is zero, a value achieved when utility curves for all test classes are perfectly aligned within the calibration range. However, it\u2019s essential to recognize that achieving a zero-variance is an idealistic scenario, unrealistic for realworld datasets. From our evaluations on public image retrieval datasets, the lower bound of OPIS for prevailing DML losses coupled with ViT backbones is around 1e-5.\nAbsolute upper bound The upper bound of OPIS depends on the degree of dispersion present in the utility score curves, making it largely dataset-specific. Here, we provide an absolute upper bound. Let\u2019s consider an extreme case where a fraction (= \u03b1) of the test classes are perfectly accurate within the calibration range, i.e., Ucorrect(d) = 1, d \u2208 [dmin, dmax], while the remainder are entirely inaccurate, i.e., Uwrong(d) = 0, d \u2208 [dmin, dmax]. This leads to an average utility of U\u0304(d) = \u03b1 and an overall OPIS score of 2\u03b1(1\u2212 \u03b1). This OPIS score has an absolute upper bound of 0.5, which is achieved at \u03b1 = 50%.\nRealistic upper bound Here we consider a more practical, realistic upper bound for OPIS. Assuming a fraction (= \u03b1) of the classes have perfect utility score in the calibration range, i.e., Ucorrect(d) = 1, d \u2208 [dmin, dmax], and the rest are perfectly \u201crandom\", meaning their pairwise L2 distances are randomly drawn from a uniform distribution with a probability of p(d = x) = 12 , 0 \u2264 x \u2264 2. Given these assumptions, the sensitivity and specificity for a random class are described as:\nsensitivity(d) = d 2 , specificity(d) = 1\u2212 d 2 (11)\nIncorporating these assumptions into the utility score definition, the utility curves for a \u201crandom\" class can be expressed as follows5:\nUroc(d) = d(1\u2212 d\n2 ) (12)\nThus, the average utility score, represented by U\u0304roc(d), can be written as the following:\nU\u0304roc(d) = (1\u2212 \u03b1)(1\u2212 d\n2 ) + \u03b1 (13)\n5We focus on Uroc(d), the definition used in the main paper, to establish the realistic upper bound of OPIS.\nWe can then determine the upper bound of OPIS using the equation below:\nOPIS = (1\u2212 \u03b1) \u00b7\n\u222b dmax dmin ||U\u0304(d)\u2212 d(1\u2212 d2 )|| 2 dd\ndmax \u2212 dmin + \u03b1 \u00b7\n\u222b dmax dmin\n||1\u2212 U\u0304(d)||2 dd dmax \u2212 dmin\n= \u03b1(1\u2212 \u03b1) \u00b7 ( 1 + \u222b dmax dmin\n(d\u2212 1)2 dd dmax \u2212 dmin ) = \u03b1(1\u2212 \u03b1)\n3 \u00b7 ( (dmax) 2 + (dmin) 2 + dmax \u00b7 dmin\n) (14)\nThis realistic upper bound \u03b1(1\u2212\u03b1)3 \u00b7 ( (dmax) 2 + (dmin) 2 + dmax \u00b7 dmin ) is a function of \u03b1 and the calibration range. To illustrate, consider a scenario where \u03b1 = 10% and [dmin, dmax] = [0.8, 1.2]. In this case, the computed OPIS upper bound stands at 9.12e-2."
        },
        {
            "heading": "A.1.3 SENSITIVITY OF OPIS TO CALIBRATION RANGES",
            "text": "In Figure 8, we show the variation of the OPIS metric across different FAR ranges \u2013 each representing a distinct calibration range. Notably, while the absolute values of the OPIS metric might fluctuate across different calibration ranges, the relative rank orderings mostly stay consistent. The only deviation observed is a rank flip between positions 5 and 6 when transitioning from the calibration range of 0.001 < FAR < 0.05 to 0.01 < FAR < 0.1; meanwhile, ranks 1, 2, 3 and 4 persistently remain stable. It\u2019s worth noting that at high FAR, the magnitude of OPIS decreases and variances in relative values will naturally increase when absolute metric values get small. This behavior is attributed to the general tendency for relative ranks to become more unstable as absolute metric values decrease \u2013 a phenomenon observed across numerous evaluation metrics including recall@k as shown in Brown et al. (2020); Patel et al. (2022); Kim et al. (2020); Movshovitz-Attias et al. (2017); Teh et al. (2020). Given these observations, we consider OPIS to be largely unaffected by the calibration range."
        },
        {
            "heading": "A.2 ADDITIONAL ABLATION STUDIES FOR TCM",
            "text": "In this section, we present comprehensive ablation studies, including sensitivity analyses, comparison between TCM and other losses, TCM design variations, and an investigation into the compatibility of TCM with self-supervised pretraining. We also give results for the CUB and Cars datasets in an open-world setting."
        },
        {
            "heading": "A.2.1 SENSITIVITY OF TCM TO RANDOM SEEDS",
            "text": "Table 6 gives the Recall@k and OPIS evaluation results for ResNet50 models trained using the Smooth-AP base loss with the TCM regularization for different random seeds. As demonstrated, the improvements in accuracy and calibration consistency due to the TCM regularization remain stable across varied random seeds, exhibiting a small variation of 0.1 for recall@k and 4e-6 for OPIS."
        },
        {
            "heading": "A.2.2 COMPARISON WITH OTHER REGULARIZATIONS",
            "text": "In Table 7, we demonstrate the superiority of our proposed TCM loss over other DML losses, including the ArcFace loss (Deng et al., 2019), the Triplet loss (Schroff et al., 2015a), and the contrastive loss (Hadsell et al., 2006), when used as a regularizer. It\u2019s worth noting that these losses can function as stand-alone DML losses, whereas TCM is specifically designed as a regularizer. As indicated in the table, although adding contrastive loss as the regularizer leads to the best threshold consistency, it also causes some degradation in recall@1. In contrast, our TCM regularization concurrently enhances both accuracy and threshold consistency, resulting in a 1.7% boost in recall@1 and a relative OPIS improvement of 48%.\nWe also compare our TCM loss with the calibration loss introduced in ROADMAP (Ramzi et al., 2021), which aims to address the decomposability gap for average precision during training. Their approach, akin to ours, involves the imposition of constraints to regulate pairwise similarity scores. However, a notable difference lies in the denominator\u2019s composition: they use the total counts of positive and negative pairs, whereas we employ hard positive and negative counts. An apparent limitation of their design is that in scenarios with a large negative-to-positive ratio, the importance of the negative term diminishes. Such scenarios often arise with large batch sizes (which is the standard of DML), such as the case in Patel et al. (2022) where a batch size of 4000 is employed. This limitation can have adverse effect on the model performance as a body of research (Schroff et al.,\n2015b; Oh Song et al., 2016; Wu et al., 2017; Wang et al., 2019) has consistently demonstrated that hard negative pairs generally convey more information than hard positive pairs. As shown in Table 8, when comparing RS@K+TCM and RS@K+ROADMAP at a batch size of 4000, using ROADMAP as a regularization results in a significant reduction in accuracy, aligning with our earlier analysis."
        },
        {
            "heading": "A.2.3 COMPARISON OF TCM AND ITS DESIGN VARIANTS",
            "text": "We examine the impact of various components within TCM, namely the indicator function, the positive term, and the negative term, to provide deeper insights into the design of the TCM regularization. Specifically, we consider three design variations, each omitting just one of its three core components: the positive term, the negative term, and the indicator functions. These TCM design alternatives are described as follows:\nLTCM\u2212 = 1 |Sm\u2212 | \u2211 s\u2208S\u2212 (s\u2212m\u2212) \u00b7 1s\u2265m\u2212 (15)\nLTCM+ = 1 |Sm+ | \u2211 s\u2208S+ (m+ \u2212 s) \u00b7 1s\u2264m+ (16)\nLTCM\u2032 = 1 |Sm+ | \u2211 s\u2208S+ (m+ \u2212 s) + 1 |Sm\u2212 | \u2211 s\u2208S\u2212 (s\u2212m\u2212) (17)\nwhere LTCM+ denotes the variant using only the positive term, LTCM\u2212 represents the variant with just the negative term, and LTCM\u2032 stands for the variant without the indicator functions.\nIn Figure 9, we give the progression of accuracy across training epochs. The accuracy and OPIS corresponding to the epoch with the peak accuracy are detailed in Table 9. In general, excluding any of the three components of TCM leads to a marked decrease in recall@1 of up to 12%. Moreover, in scenarios where only the positive term is employed or when TCM operates without the indicator function, we observe that the model stops improving its accuracy after the first epoch. When relying solely on TCM\u2019s negative term, we observe an initial dip in recall@1, which shows signs of recovery after 20 epochs. Nevertheless, the eventual recall@1, even post-recovery, lags significantly behind that achieved with the full TCM regularization with all three components included. In terms of threshold consistency, our observations suggest that penalizing the TCM positive term, the negative term, or both simultaneously can all improve the OPIS metric. Notably, regularizing solely with the positive TCM term (LTCM+ ) yields the best OPIS result. However, omitting the indicator function results in minimal improvements in OPIS, aligning closely with the baseline outcome observed\nwithout any regularization. This showcases the importance of the indicator function as an effective quantile estimation mechanism to filter out the hard sample pairs.\nWe also visualize the intra-class and inter-class L2 distance distributions in Figure 10 for models trained with TCM regularization and the other four alternative designs. The results reveal that a regularizer focusing solely on penalizing hard negative pairs (RS@K+TCM\u2212) causes the inter-class embeddings to become overly dispersed, compromising the desired intra-class compactness. Con-\nversely, when only hard positive pairs are penalized (RS@K+TCM+), it results in the collapse of intra-class embeddings, leaving the inter-class distribution unchecked. On the other hand, in the absence of the indicator function, the inter-class distribution manifests as multi-modal with dual peaks with significantly increased overlap between the inter-class and intra-class L2 distance distributions. This phenomenon may be attributed to the failure to distinguish hard negative pairs from the easy ones. To conclude, to maintain accuracy on par with the baseline (without regularization), all three components of TCM are indispensable."
        },
        {
            "heading": "A.2.4 COMPATIBILITY OF TCM WITH SELF-SUPERVISED PRETRAINING",
            "text": "We examine the compatibility of our proposed TCM regularization with self-supervised pretraining methods, including CLIP (Radford et al., 2021), DINOv2 (Oquab et al., 2023), and Unicom (An et al., 2023). In alignment with the approach in Unicom An et al. (2023), we adopt the ArcFace loss (Deng et al., 2019) for this supervised fine-tuning. The results are presented in Table 10.\nAs shown in the table, incorporating TCM regularization in models initialized through various selfsupervised pretraining methods consistently delivers improvements in the OPIS metrics, observing gains of up to 10%. However, the relative improvement in OPIS doesn\u2019t quite reach the levels achieved with supervised pretraining as reported in the main paper. This discrepancy is likely caused by the distinctive learning objectives of self-supervised pretraining and its use of completely different pretraining datasets compared to ImageNet. As for accuracy, adding TCM is shown to significantly boost recall@1 by up to 4.5% when paired with CLIP pretraining. Yet, for approaches such as DINOv2 and Unicom, the enhancement in recall@1 is marginal."
        },
        {
            "heading": "A.2.5 OPEN-WORLD SETTING RESULTS FOR CUB AND CARS",
            "text": "In the main paper, we utilize shared train-test classes in the closed-set setting for the CUB and Cars datasets to ensure fair comparisons with previous works. However, in this section, we present results for these two datasets under the open-world setting, where training and testing classes are disjoint, to align with our central focus on open-world scenarios. Specifically, we partition the datasets based on the class indices: for CUB, classes 1-100 serve as the training set while the rest constitute the test set; similarly, for Cars, classes 1-98 are used for training, with the remaining classes reserved for testing. In Table 11, we present evaluation results for the CUB and Cars datasets under the openworld setting. The results demonstrate that integrating TCM leads to a notable relative improvement of 18% in OPIS for the Cars-196 dataset and a substantial 45% enhancement for the CUB dataset, while preserving and improving the accuracy compared to the baseline without regularization.\nA.3 IMPLEMENTATION DETAILS"
        },
        {
            "heading": "A.3.1 CONNECTIONS BETWEEN CALIBRATION RANGE AND COSINE MARGINS",
            "text": "We clarify the connection between the calibration range and the cosine margins introduced in TCM. First, calibration range is determined by the FAR and FRR requirement specified by the usecase. For a well-trained embedding model, optimal performance is typically achieved within an intermediate range of distance thresholds. Thus, the left bound of the calibration range is determined by the maximum FRR, as defined by the use-case. On the flip side, the right bound is determined by the maximum FPR set by the same use-case. The margins m+ and m\u2212 of the TCM regularization are defined in the cosine space. Given hyperspherical embeddings, there is a bijection between the cosine similarity and the L2 distance. In the absence of a distribution shift between the training and testing datasets, the cosine margins can be adeptly pinpointed as m+ = \u221a\n2\u2212dmin2 2 and m \u2212 =\u221a 2\u2212dmax2\n2 to align the representation structures to the desired calibration range. However, while the calibration range can serve as a useful reference, due to the inevitable distribution shifts between training and testing data, the selection of the margin constraints requires some hyper-parameter tuning and cannot be directly estimated from the calibration range."
        },
        {
            "heading": "A.3.2 ELABORATION ON FIG.5(B) OF THE MAIN PAPER",
            "text": "In Fig.5(b) of the main paper, \u03b81 and \u03b82 represent the angular arc lengths of the red and blue classes, respectively. Applying TCM regularization promotes intra-class compactness, targeting a condition where cos(\u03b8) = m+, with \u03b8 representing the angular span of the given class in the embedding. Thus training with TCM regularization would encourage \u03b81 \u2190 arccos(m+) and \u03b82 \u2190 arccos(m+).\nA.3.3 IMPLEMENTATION DETAILS FOR OPIS METRIC AND TCM REGULARIZATION\nIn the following, we provide a brief description of our implementation for both the OPIS metric and the TCM regularization in pseudo-code format.\nAlgorithm 1 Computation for OPIS metric Require: Pairwise evaluation protocol for test embeddings with a total of T classes, calibration range\n[dmin, dmax] with a grid number N . 1: Initialize OPIS\u2190 0 2: for j = 1 to N do \u25b7 Iterate over calibration grid 3: d\u2190 dmin + dmax\u2212dminn \u00b7 j 4: for i in T do \u25b7 Iterate over test classes 5: Gather all L2 distances for class i 6: Compute specificity (\u03d5) and sensitivity (\u03c8) at d 7: Ui(d)\u2190 2\u00b7\u03d5(d)\u00b7\u03c8(d)\u03d5(d)+\u03c8(d) 8: Compute average utility score U\u0304(d) across all classes\n9: OPIS\u2190 OPIS + \u2211T i=1 ||Ui(d)\u2212U\u0304(d)|| 2 2\nT \u00b7(dmax\u2212dmin) return OPIS\nIn Algorithm 1, the number of grids in the calibration range, denoted as N , should be selected in accordance with the maximum sharpness observed in the utility-distance curves within the calibration range. It is recommended to maintainN \u2265 10. The calibration range itself is determined by the desired False Acceptance Rate (FAR) range of the entire dataset, as specified by the user case. Additionally, it is bound by the least achievable FAR of the dataset. In Algorithm 2, |Sm+ | represents the number of hard positive pairs in S+ with similarity below m+ and |Sm\u2212 | represents the number of hard negative pairs in S\u2212 with similarity above m\u2212.\nAlgorithm 2 Training with TCM regularization Require: m+, m\u2212, \u03bb+, \u03bb\u2212, base loss Lbase.\n1: for number of epochs do 2: for k steps do \u25b7 In each training iteration 3: LTCM \u2190 0 4: Sample mini-batch data {(x1, y1), ..., (xm, ym)} 5: Extract embeddings (denoted by f ) for each x 6: Compute cosine similarities for the entire mini-batch data. Let S+ be the set of positive pair\nsimilarities and S\u2212 be the set of negative pair similarities 7: Compute |Sm + | and |Sm \u2212 | 8: for s in S+ do 9: LTCM \u2190 LTCM + \u03bb+ \u00b7\n(m+\u2212s)\u00b7 1s\u2264m+ |Sm+ |\n\u25b7 Sum over all hard positive pairs\n10: for s in S\u2212 do 11: LTCM \u2190 LTCM + \u03bb\u2212 \u00b7\n(s\u2212m\u2212)\u00b7 1s\u2265m\u2212 |Sm\u2212 |\n\u25b7 Sum over all hard negative pairs\n12: Loverall \u2190 Lbase + LTCM 13: Update model using Loverall"
        },
        {
            "heading": "A.3.4 DETAILS ON THE BASE DML LOSSES EMPLOYED",
            "text": "Smooth-AP Smooth-AP (Brown et al., 2020) is a pair-based DML loss that optimizes a smoothed approximation of average precision (AP) using a sigmoid function. It is expressed as:\nLSAP = 1\nn n\u2211 i=1 (1\u2212 1 |S+i | \u2211 j\u2208S+i\n1 + \u2211\nk\u2208S+i Gjk,i 1 + \u2211\nk\u2208Si Gjk,i ) (18)\nHere, Gjk,i = (1 + e\u03bb(sij\u2212sik))\u22121 is a softmax function incorporating a temperature parameter, \u03bb, to rank similarity scores of mini-batch samples against an anchor sample xi. Si represents the set of cosine similarity scores for the entire mini-batch against xi, while S+i denotes the subset of Si for the positive samples against xi,\nRecall@k Surrogate The Recall@k Surrogate loss mirrors Smooth-AP\u2019s approach but approximates the Recall@k metric rather than the average precision metric. Its detailed formulation is available in Patel et al. (2022).\nArcFace ArcFace (Deng et al., 2019) introduces additive margin penalties based on the angle, \u03b8, between mini-batch samples and prototype representations. It aims to contrast samples against class prototypes, and is formulated as:\nLArcFace = 1\nn n\u2211 i=1 log(1 + n\u2211 j=1,j \u0338=i e\u03bb(sij\u2212sii+\u03b4)) (19)\nwhere sij is the cosine similarity with inter-class margin penalty between the representation of an image with class i and the prototypical representation for another image whose class is j, with m1,m2 denoting different angular margins:\nsij = { cos(m1\u03b8ij +m2) for i = j cos \u03b8ij , for i \u0338= j\n(20)"
        }
    ],
    "title": "THRESHOLD-CONSISTENT MARGIN LOSS FOR OPEN- WORLD DEEP METRIC LEARNING",
    "year": 2024
}