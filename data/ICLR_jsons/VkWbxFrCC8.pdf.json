{
    "abstractText": "COMpression with Bayesian Implicit NEural Representations (COMBINER) is a recent data compression method that addresses a key inefficiency of previous Implicit Neural Representation (INR)-based approaches: it avoids quantization and enables direct optimization of the rate-distortion performance. However, COMBINER still has significant limitations: 1) it uses factorized priors and posterior approximations that lack flexibility; 2) it cannot effectively adapt to local deviations from global patterns in the data; and 3) its performance can be susceptible to modeling choices and the variational parameters\u2019 initializations. Our proposed method, Robust and Enhanced COMBINER (RECOMBINER), addresses these issues by 1) enriching the variational approximation while retaining a low computational cost via a linear reparameterization of the INR weights, 2) augmenting our INRs with learnable positional encodings that enable them to adapt to local details and 3) splitting high-resolution data into patches to increase robustness and utilizing expressive hierarchical priors to capture dependency across patches. We conduct extensive experiments across several data modalities, showcasing that RECOMBINER achieves competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on lowresolution images at low bitrates. Our PyTorch implementation is available at https://github.com/cambridge-mlg/RECOMBINER/.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiajun He"
        },
        {
            "affiliations": [],
            "name": "Gergely Flamich"
        },
        {
            "affiliations": [],
            "name": "Zongyu Guo"
        },
        {
            "affiliations": [],
            "name": "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
        }
    ],
    "id": "SP:d18e6c8d90e43facf7628f8a62860b3431561b17",
    "references": [
        {
            "authors": [
                "Eirikur Agustsson",
                "Radu Timofte"
            ],
            "title": "Ntire 2017 challenge on single image super-resolution: Dataset and study",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,",
            "year": 2017
        },
        {
            "authors": [
                "Johannes Ball\u00e9",
                "Philip A Chou",
                "David Minnen",
                "Saurabh Singh",
                "Nick Johnston",
                "Eirikur Agustsson",
                "Sung Jin Hwang",
                "George Toderici"
            ],
            "title": "Nonlinear transform coding",
            "venue": "IEEE Journal of Selected Topics in Signal Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Johannes Ball\u00e9",
                "David Minnen",
                "Saurabh Singh",
                "Sung Jin Hwang",
                "Nick Johnston"
            ],
            "title": "Variational image compression with a scale hyperprior",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Charles Blundell",
                "Julien Cornebise",
                "Koray Kavukcuoglu",
                "Daan Wierstra"
            ],
            "title": "Weight uncertainty in neural network",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Hao Chen",
                "Matthew Gwilliam",
                "Ser-Nam Lim",
                "Abhinav Shrivastava"
            ],
            "title": "Hnerf: A hybrid neural representation for videos",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Zhengxue Cheng",
                "Heming Sun",
                "Masaru Takeuchi",
                "Jiro Katto"
            ],
            "title": "Learned image compression with discretized gaussian mixture likelihoods and attention modules",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Emilien Dupont",
                "Adam Golinski",
                "Milad Alizadeh",
                "Yee Whye Teh",
                "Arnaud Doucet"
            ],
            "title": "Coin: Compression with implicit neural representations. In Neural Compression: From Information Theory to Applications\u2013Workshop",
            "year": 2021
        },
        {
            "authors": [
                "Michael Dusenberry",
                "Ghassen Jerfel",
                "Yeming Wen",
                "Yian Ma",
                "Jasper Snoek",
                "Katherine Heller",
                "Balaji Lakshminarayanan",
                "Dustin Tran"
            ],
            "title": "Efficient and scalable bayesian neural nets with rank-1 factors",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Gergely Flamich"
            ],
            "title": "Greedy Poisson rejection sampling",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Gergely Flamich",
                "Marton Havasi",
                "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
            ],
            "title": "Compressing images by encoding their latent representations with relative entropy coding",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Gergely Flamich",
                "Stratis Markou",
                "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
            ],
            "title": "Fast relative entropy coding with A* coding",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Zongyu Guo",
                "Zhizheng Zhang",
                "Runsen Feng",
                "Zhibo Chen"
            ],
            "title": "Causal contextual prediction for learned image compression",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology,",
            "year": 2021
        },
        {
            "authors": [
                "Zongyu Guo",
                "Gergely Flamich",
                "Jiajun He",
                "Zhibo Chen",
                "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
            ],
            "title": "Compression with Bayesian implicit neural representations",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Marton Havasi",
                "Robert Peharz",
                "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
            ],
            "title": "Minimal random code learning: Getting bits back from compressed model parameters",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Dailan He",
                "Ziming Yang",
                "Weikun Peng",
                "Rui Ma",
                "Hongwei Qin",
                "Yan Wang"
            ],
            "title": "Elic: Efficient learned image compression with unevenly grouped space-channel contextual adaptive coding",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Edward J Hu",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Pavel Izmailov",
                "Sharad Vikram",
                "Matthew D Hoffman",
                "Andrew Gordon Gordon Wilson"
            ],
            "title": "What are Bayesian neural network posteriors really like",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Chiyu Jiang",
                "Avneesh Sud",
                "Ameesh Makadia",
                "Jingwei Huang",
                "Matthias Nie\u00dfner",
                "Thomas Funkhouser"
            ],
            "title": "Local implicit grid representations for 3d scenes",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Hyunbin Kim",
                "Milot Mirdita",
                "Martin Steinegger"
            ],
            "title": "Foldcomp: a library and format for compressing and indexing large protein structure",
            "venue": "sets. Bioinformatics,",
            "year": 2023
        },
        {
            "authors": [
                "Subin Kim",
                "Sihyun Yu",
                "Jaeho Lee",
                "Jinwoo Shin"
            ],
            "title": "Scalable neural video representations with learnable positional features",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Durk P Kingma",
                "Tim Salimans",
                "Max Welling"
            ],
            "title": "Variational dropout and the local reparameterization trick",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Eastman Kodak"
            ],
            "title": "Kodak Lossless True Color Image Suite (PhotoCD PCD0992)",
            "venue": "http://r0k. us/graphics/kodak/,",
            "year": 1993
        },
        {
            "authors": [
                "Daphne Koller",
                "Nir Friedman"
            ],
            "title": "Probabilistic graphical models: principles and techniques",
            "venue": "MIT press,",
            "year": 2009
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny",
            "year": 2009
        },
        {
            "authors": [
                "Th\u00e9o Ladune",
                "Pierrick Philippe",
                "F\u00e9lix Henry",
                "Gordon Clare",
                "Thomas Leguay"
            ],
            "title": "Cool-chic: Coordinate-based low complexity hierarchical image codec",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Jinming Liu",
                "Heming Sun",
                "Jiro Katto"
            ],
            "title": "Learned image compression with mixed transformercnn architectures",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Fabian Mentzer",
                "George D Toderici",
                "Michael Tschannen",
                "Eirikur Agustsson"
            ],
            "title": "High-fidelity generative image compression",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "David Minnen",
                "Johannes Ball\u00e9",
                "George D Toderici"
            ],
            "title": "Joint autoregressive and hierarchical priors for learned image compression",
            "venue": "In Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Thomas M\u00fcller",
                "Alex Evans",
                "Christoph Schied",
                "Alexander Keller"
            ],
            "title": "Instant neural graphics primitives with a multiresolution hash encoding",
            "venue": "ACM Transactions on Graphics,",
            "year": 2022
        },
        {
            "authors": [
                "Deniz Oktay",
                "Johannes Ball\u00e9",
                "Saurabh Singh",
                "Abhinav Shrivastava"
            ],
            "title": "Scalable model compression by entropy penalized reparameterization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Vassil Panayotov",
                "Guoguo Chen",
                "Daniel Povey",
                "Sanjeev Khudanpur"
            ],
            "title": "Librispeech: An asr corpus based on public domain audio books",
            "venue": "In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2015
        },
        {
            "authors": [
                "Ethan Perez",
                "Florian Strub",
                "Harm De Vries",
                "Vincent Dumoulin",
                "Aaron Courville"
            ],
            "title": "Film: Visual reasoning with a general conditioning layer",
            "venue": "In AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Jonathan Richard Schwarz",
                "Yee Whye Teh"
            ],
            "title": "Meta-learning sparse compression",
            "venue": "networks. Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Richard Schwarz",
                "Jihoon Tack",
                "Yee Whye Teh",
                "Jaeho Lee",
                "Jinwoo Shin"
            ],
            "title": "Modalityagnostic variational compression of implicit neural representations",
            "venue": "In International conference on machine learning,",
            "year": 2023
        },
        {
            "authors": [
                "Vincent Sitzmann",
                "Julien N.P. Martel",
                "Alexander W. Bergman",
                "David B. Lindell",
                "Gordon Wetzstein"
            ],
            "title": "Implicit neural representations with periodic activation functions",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Casper Kaae S\u00f8nderby",
                "Tapani Raiko",
                "Lars Maal\u00f8e",
                "S\u00f8ren Kaae S\u00f8nderby",
                "Ole Winther"
            ],
            "title": "Ladder variational autoencoders",
            "venue": "In Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Khurram Soomro",
                "Amir Roshan Zamir",
                "Mubarak Shah"
            ],
            "title": "UCF101: A dataset of 101 human actions classes from videos in the wild",
            "venue": "arXiv preprint arXiv:1212.0402,",
            "year": 2012
        },
        {
            "authors": [
                "Luke Staniscia",
                "Yun William Yu"
            ],
            "title": "Image-centric compression of protein structures improves space savings",
            "venue": "BMC Bioinformatics,",
            "year": 2023
        },
        {
            "authors": [
                "Kenneth O Stanley"
            ],
            "title": "Compositional pattern producing networks: A novel abstraction of development",
            "venue": "Genetic programming and evolvable machines,",
            "year": 2007
        },
        {
            "authors": [
                "Matthew Tancik",
                "Pratul Srinivasan",
                "Ben Mildenhall",
                "Sara Fridovich-Keil",
                "Nithin Raghavan",
                "Utkarsh Singhal",
                "Ravi Ramamoorthi",
                "Jonathan Barron",
                "Ren Ng"
            ],
            "title": "Fourier features let networks learn high frequency functions in low dimensional domains",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Suramya Tomar"
            ],
            "title": "Converting video formats with FFmpeg",
            "venue": "Linux Journal,",
            "year": 2006
        },
        {
            "authors": [
                "Brian Trippe",
                "Richard Turner"
            ],
            "title": "Overpruning in variational Bayesian neural networks",
            "venue": "In Advances in Approximate Bayesian Inference workshop at NIPS",
            "year": 2017
        },
        {
            "authors": [
                "Chengxin Zhang",
                "Anna Marie Pyle"
            ],
            "title": "PDC: a highly compact file format to store protein 3D coordinates",
            "venue": "Database (Oxford),",
            "year": 2023
        },
        {
            "authors": [
                "Yinhao Zhu",
                "Yang Yang",
                "Taco Cohen"
            ],
            "title": "Transformer-based transform coding",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Cheng"
            ],
            "title": "2020) on CIFAR-10 and Kodak, and MP3 and COIN++ on the full test set of LibriSpeech, are taken from the COIN++\u2019s GitHub repo4. Statistics for VC-INR and MSCN",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Advances in deep learning recently enabled a new data compression technique impossible with classical approaches: we train a neural network to memorize the data (Stanley, 2007) and then encode the network\u2019s weights instead. These networks are called the implicit neural representation (INR) of the data, and differ from neural networks used elsewhere in three significant ways. First, they treat data as a signal that maps from coordinates to values, such as mapping pX,Y q pixel coordinates to pR,G,Bq color triplets in the case of an image. Second, their architecture consists of many fewer layers and units than usual and tends to utilize SIREN activations (Sitzmann et al., 2020). Third, we aim to overfit them to the data as much as possible.\nUnfortunately, most INR-based data compression methods cannot directly and jointly optimize ratedistortion, which results in a wasteful allocation of bits leading to suboptimal coding performance. COMpression with Bayesian Implicit NEural Representations (COMBINER; Guo et al., 2023) addresses this issue by picking a variational Gaussian mean-field Bayesian neural network (Blundell et al., 2015) as the INR of the data. This choice enables joint rate-distortion optimization via maximizing the INR\u2019s \u03b2-evidence lower bound (\u03b2-ELBO), where \u03b2 controls the rate-distortion trade-off.\n\u02daequal contribution.\nFinally, the authors encode a weight sample from the INR\u2019s variational weight posterior to represent the data using relative entropy coding (REC; Havasi et al., 2018; Flamich et al., 2020).\nAlthough COMBINER performs strongly among INR-based approaches, it falls short of the stateof-the-art codecs on well-established data modalities both in terms of performance and robustness. In this paper, we identify several issues that lead to this discrepancy: 1) COMBINER employs a fully-factorized Gaussian variational posterior over the INR weights, which tends to underfit the data (Dusenberry et al., 2020), going directly against our goal of overfitting; 2) Overfitting small INRs used by COMBINER is challenging, especially at low bitrates: a small change to any weight can significantly affect the reconstruction at every coordinate, hence optimization by stochastic gradient descent becomes unstable and yields suboptimal results. 3) Overfitting becomes more problematic on high-resolution signals. As highlighted by Guo et al. (2023), the method is sensitive to model choices and the variational parameters\u2019 initialization and requires considerable effort to tune.\nWe tackle these problems by proposing several non-trivial extensions to COMBINER, which significantly improve the rate-distortion performance and robustness to modeling choices. Hence, we dub our method robust and enhanced COMBINER (RECOMBINER). Concretely, our contributions are:\n\u2022 We propose a simple yet effective learned reparameterization for neural network weights specifically tailored for INR-based compression, yielding more expressive variational posteriors while matching the computational cost of standard mean-field variational inference.\n\u2022 We augment our INR with learnable positional encodings whose parameters only have a local influence on the reconstructed signal, thus allowing deviations from the global patterns captured by the network weights, facilitating overfitting the INR with gradient descent.\n\u2022 We split high-resolution data into patches to improve robustness to modeling choices and the variational parameters\u2019 initialization. Moreover, we propose an expressive hierarchical Bayesian model to capture the dependencies across patches to enhance performance.\n\u2022 We conduct extensive experiments to verify the effectiveness of our proposed extensions across several data modalities, including image, audio, video and protein structure data. In particular, we show that RECOMBINER achieves better rate-distortion performance than VAE-based approaches on low-resolution images at low bitrates."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "This section reviews the essential parts of Guo et al. (2023)\u2019s compression with Bayesian implicit neural representations (COMBINER), as it provides the basis for our method.\nVariational Bayesian Implicit Neural Representations: We assume the data we wish to compress can be represented as a continuous function f : RI \u00d1 RO from I-dimensional coordinates to Odimensional signal values. Then, our goal is to approximate f with a small neural network gp\u00a8 | wq\nwith weights w. Given L hidden layers in the network, we write w \u201c rwr1s, . . . ,wrLss, which represents the concatenation of the L weight matrices wr1s, . . .wrLs, each flattened into a rowvector. Guo et al. (2023) propose using variational Bayesian neural networks (BNN; Blundell et al., 2015) that place a prior pw and a variational posterior qw on the weights. Furthermore, they use Fourier embeddings \u03b3pxq for the input data (Tancik et al., 2020) and sine activations at the hidden layers (Sitzmann et al., 2020). To infer the implicit neural representation (INR) for some data D, we treat D as a dataset of coordinate-value pairs tpxi,yiquDi\u201c1, e.g. for an image, xi can be an pX,Y q pixel coordinate and yi the corresponding pR,G,Bq triplet. Next, we pick a distortion metric \u2206 (e.g., mean squared error) and a trade-off parameter \u03b2 to define the \u03b2-rate-distortion objective:\nLpD, qw, pw, \u03b2q \u201c \u03b2 \u00a8 DKLrqw}pws ` 1\nD\nD \u00ff i\u201c1 Eqw r\u2206pyi, gpxi | wqs , (1)\nwhere DKLrqw}pws denotes the Kullback-Leibler divergence of qw from pw, and as we explain below, it represents the compression rate of a single weight sample w \u201e qw. Note that Equation (1) corresponds to a negative \u03b2-evidence lower bound under mild assumptions on \u2206.\nWe infer the optimal posterior by computing q\u02daw \u201c argminqwPQ LpD, qw, pw, \u03b2q over an appropriate variational family Q. Guo et al. (2023) set Q to be the family of factorized Gaussian distributions. Training COMBINER: Once we selected a network architecture g for our INRs, a crucial element of COMBINER is to select a good prior on the weights pw. Given a training set tD1, . . . ,DMu and an initial guess for pw, Guo et al. (2023) propose the following iterative scheme to select the optimal prior: 1) Fix pw and infer the variational INR posteriors q\u02daw,m for each datum Dm by minimizng Equation (1); 2) Fix the q\u02daw,ms and update the prior parameters pw based on the parameters of the posteriors. When the qw are Gaussian, Guo et al. (2023) derive analytic formulae for updating the prior parameters. To avoid overloading the notion of training, we refer to learning pw and the other model parameters as training, and to learning qw as inferring the INR.\nCompressing data with COMBINER: Once we picked the INR architecture g and found the optimal prior pw, we can use COMBINER to compress new data D in two steps: 1) We first infer the variational INR posterior qw for D by optimizing Equation (1), after which 2) we encode an approximate sample from qw using relative entropy coding (REC), whose expected coding cost is approximately DKLrqw}pws (Havasi et al., 2018; Flamich et al., 2020). Following Guo et al. (2023), we used depth-limited global-bound A\u02da coding (Flamich et al., 2022), to which we will refer as just A\u02da coding. Unfortunately, applying A\u02da coding to encode a sample from qw is infeasible in practice, as the time complexity of the algorithm grows as \u2126pexppDKLrqw}pwsqq. Hence, Guo et al. (2023) suggest breaking up the problem into smaller ones. First, they draw a uniformly random permutation \u03b1 on dimpwq elements, and use it to permute the dimensions of w as \u03b1pwq \u201c rw\u03b1p1q, . . . ,w\u03b1pdimpwqqs. Then, they partition \u03b1pwq into smaller blocks, and compress the blocks sequentially. Permuting the weight vector ensures that the KL divergences are spread approximately evenly across the blocks. As an additional technical note, between compressing each block, we run a few steps of finetuning the posterior of the weights that are yet to be compressed, see Guo et al. (2023) for more details."
        },
        {
            "heading": "3 METHODS",
            "text": "In this section, we propose several extensions to Guo et al. (2023)\u2019s framework that significantly improve its robustness and performance: 1) we introduce a linear reparemeterization for the INR\u2019s weights which yields a richer variational posterior family; 2) we augment the INR\u2019s input with learned positional encodings to capture local features in the data and to assist overfitting; 3) we scale our method to high-resolution image compression by dividing the images into patches and introducing an expressive hierarchical Bayesian model over the patch-INRs, and 4) we introduce minor modifications to the training procedure and adaptively select \u03b2 to achieve the desired coding budget. Contributions 1) and 2) are depicted in Figure 1, while 3) is shown in Figure 2."
        },
        {
            "heading": "3.1 LINEAR REPARAMETERIZATION FOR THE NETWORK PARAMETERS",
            "text": "A significant limitation of the factorized Gaussian variational posterior used by COMBINER is that it posits dimension-wise independent weights. This assumption is known to be unrealistic (Izmailov et al., 2021) and to underfit the data (Dusenberry et al., 2020), which goes directly against our goal of overfitting the data. On the other hand, using a full-covariance Gaussian posterior approximation would increase the INR\u2019s training and coding time significantly, even for small network architectures.\nHence, we propose a solution that lies in-between: at a high level, we learn a linearly-transformed factorized Gaussian approximation that closely matches the full-covariance Gaussian posterior on average over the training data. Formally, for each layer l \u201c 1, . . . , L, we model the weights as wrls \u201c hrlswArls, where the Arls are square matrices, and we place a factorized Gaussian prior and variational posterior on hrlsw instead. We learn each Arls during the training stage, after which we fix them and only infer factorized posteriors q\nh rls w\nwhen compressing new data. To simplify notation,\nwe collect the Arls in a block-diagonal matrix A \u201c diagpAr1s, . . . ,ArLsq and the hrlsw in a single row-vector hw \u201c rhr1sw , . . . ,hrLsw s, so that now the weights are given by w \u201c hwA. We found this layer-wise weight reparameterization as efficient as using a joint one for the entire weight vector w. Hence, we use the layer-wise approach, as it is more parameter and compute-efficient.\nThis simple yet expressive variational approximation has a couple of advantages. First, it provides an expressive full-covariance prior and posterior while requiring much less training and coding time. Specifically, the KL divergence required by Equation (1) is still between factorized Gaussians and we do not need to optimize the full covariance matrices of the posteriors during coding. Second, this parameterization has scale redundancy: for any c P R we have hwA \u201c p1{c \u00a8 hwqpc \u00a8 Aq. Hence, if we initialize hw suboptimally during training, A can still learn to compensate for it, making our method more robust. Finally, note that this reparameterization is specifically tailored for INR-based compression and would usually not be feasible in other BNN use-cases, since we learn A while inferring multiple variational posteriors simultaneously."
        },
        {
            "heading": "3.2 LEARNED POSITIONAL ENCODINGS",
            "text": "A challenge for overfitting INRs, especially at low bitrates is their global representation of the data, in the sense that each of their weights influences the reconstruction at every coordinate. To mitigate this issue, we extend our INRs to take a learned positional input zi at each coordinate xi: gpxi, zi | wq. However, it is usually wasteful to introduce a vector for each coordinate in practice. Instead, we use a lower-dimensional row-vector representation hz, that we reshape and upsample with a learnable function \u03d5. In the case of a W \u02c6 H image with F -dimensional positional encodings, we could pick hz such that dimphzq ! F \u00a8 W \u00a8 H , then reshape and upsample it to be F \u02c6 W \u02c6 H by picking \u03d5 to be some small convolutional network. Then, we set zi \u201c \u03d5phzqxi to be the positional encoding at location xi. We placed a factorized Gaussian prior and variational posterior on hz. Hereafter, we refer to hz as the latent positional encodings, \u03d5phzq and zi as the upsampled positional encodings."
        },
        {
            "heading": "3.3 SCALING TO HIGH-RESOLUTION DATA WITH PATCHES",
            "text": "With considerable effort, Guo et al. (2023) successfully scaled COMBINER to high-resolution images by significantly increasing the number of INR parameters. However, they note that the training procedure was very sensitive to hyperparameters, including the initialization of variational parameters and model size selection. Unfortunately, improving the robustness of large INRs using the weight reparameterization we describe in Section 3.1 is also impractical, because the size of the transformation matrix A grows quadratically in the number of weights. Therefore, we split high-resolution data into patches and infer a separate small INR for each patch, in line with other INR-based works as well (Dupont et al., 2022; Schwarz & Teh, 2022; Schwarz et al., 2023). However, the patches\u2019 INRs are independent by default, hence we re-introduce information sharing between the patch-INRs\u2019 weights via a hierarchical model for hw. Finally, we take advantage of the patch structure to parallelize data compression and reduce the encoding time in RECOMBINER, as discussed at the end of this section.\nRECOMBINER\u2019s hierarchical Bayesian model: We posit a global representation for the weights hw, from which each patch-INR can deviate. Thus, assuming that the data D is split into P patches, for each patch \u03c0 P 1, . . . , P , we need to define the conditional distributions of patch representations hp\u03c0qw | hw. However, since we wish to model deviations from the global representation, it is natural to decompose the patch representation as hp\u03c0qw \u201c \u2206hp\u03c0qw ` hw, and specify the conditional distribution of the differences \u2206hp\u03c0qw | hw instead, without any loss of generality. In this paper, we place a factorized Gaussian prior and variational posterior on the joint distribution of the global\nrepresentation and the deviations, given by the following product of P ` 1 Gaussian measures:\np hw,\u2206h p1:P q w\n\u201c N p\u00b5w,diagp\u03c3wqq \u02c6 P \u017a\n\u03c0\u201c1 N p\u00b5p\u03c0q\u2206 ,diagp\u03c3 p\u03c0q \u2206 qq (2)\nq hw,\u2206h p1:P q w\n\u201c N p\u03bdw,diagp\u03c1wqq \u02c6 P \u017a\n\u03c0\u201c1 N p\u03bdp\u03c0q\u2206 ,diagp\u03c1 p\u03c0q \u2206 qq, (3)\nwhere 1 : P is the slice notation, i.e. \u2206hp1:P qw \u201c \u2206hp1qw , . . . ,\u2206hpP qw . Importantly, while the posterior approximation in Equation (3) assumes that the global representation and the differences are independent, hw and h p\u03c0q w remain correlated. Note that optimizing Equation (1) requires us to compute DKLrqhp1:P qw }php1:P qw s. Unfortunately, due to the complex dependence between the h p\u03c0q w s, this calculation is infeasible. Instead, we can minimize an upper bound to it by observing that DKLrqhp1:P qw }php1:P qw s \u010f DKLrqhp1:P qw }php1:P qw s ` DKLrqhw|hp1:P qw }phw|hp1:P qw s\n\u201c DKLrqhw,hp1:P qw }phw,hp1:P qw s \u201c DKLrqhw,\u2206hp1:P qw }phw,\u2206hp1:P qw s. (4)\nHence, when training the patch-INRs, we replace the KL term in Equation (1) with the divergence in Equation (4), which is between factorized Gaussian distributions and cheap to compute. Finally, we remark that we can view hw as side information also prevalent in other neural compression codecs (Balle\u0301 et al., 2018), or auxiliary latent variables enabling factorization (Koller & Friedman, 2009).\nWhile Equations (2) and (3) describe a two-level hierarchical model, we can easily extend the hierarchical structure by breaking up patches further into sub-patches and adding extra levels to the probabilistic model. For our experiments on high-resolution audio, images, and video, we found that a three-level hierarchical model worked best, with global weight representation hw, second/grouplevel representations hp1:Gqw and third/patch-level representations hp1:P qw , illustrated in Figure 2a. Empirically, a hierarchical model for hz did not yield significant gains, thus we only use it for hw.\nCompressing high-resolution data with RECOMBINER: An advantage of patching is that we can compress and fine-tune INRs and latent positional encodings of all patches in parallel. Unfortunately, compressing P patches in parallel using COMBINER\u2019s procedure is suboptimal, since the information content between patches might vary significantly. However, by carefully permuting the weights across the patches\u2019 representations we can 1) adaptively allocate bits to each patch to compensate for the differences in their information content and 2) enforce the same coding budget across each parallel thread to ensure consistent coding times. Concretely, we stack representations of each patch in a matrix at each level of the hierarchical model. For example, in our three-level model we set\nHp0q\u03c0,: \u201c rhp\u03c0qw ,hp\u03c0qz s, Hp1qg,: \u201c h pgq w , H p2q \u201c hw, (5) where we use slice notation to denote the ith row as Hi,: and the jth column as H:,j . Furthermore, let Sn denote the set of permutations on n elements. Now, at each level \u2113, assume Hp\u2113q has C\u2113 columns and R\u2113 rows. We sample a single within-row permutation \u03ba uniformly from SC\u2113 and for each column of Hp\u2113q we sample an across-rows permutation \u03b1j uniformly from SR\u2113 elements. Then, we permute Hp\u2113q as \u0102Hp\u2113qi,j \u201c H p\u2113q \u03b1jpiq,\u03bapjq. Finally, we split the H\np\u2113qs into blocks row-wise, and encode and fine-tune each row in parallel. We illustrate the above procedure in Figure 2b."
        },
        {
            "heading": "3.4 EXTENDED TRAINING PROCEDURE",
            "text": "In this section, we describe the ways in which RECOMBINER\u2019s training procedure deviates from COMBINER\u2019s. To begin, we collect the RECOMBINER\u2019s representations into one vector. For nonpatching cases we set h \u201c rhw,hzs, and for the patch case using the three-level hierarchical model we set h \u201c vecprHp0q,Hp1q,Hp2qsq. For simplicity, we denote the factorized Gaussian prior and variational posterior over h as ph \u201c N p\u00b5,diagp\u03c3qq and qh \u201c N p\u03bd,diagp\u03c1qq, where \u00b5 and \u03bd are the means and \u03c3 and \u03c1 are the diagonals of covariances of the prior and the posterior, respectively.\nTraining RECOMBINER: Our objective for the training stage is to obtain the model parameters A, \u03d5,\u00b5,\u03c3 given a training dataset tD1, . . . ,DMu and a coding budget C. 1 In their work, Guo et al. (2023) control the coding budget implicitly by manually setting different values for \u03b2 in Equation (1). In this paper, we adopt an explicit approach and tune \u03b2 dynamically based on our desired coding budget of C bits. More precisely, after every iteration, we calculate the average KL divergence of the training examples, i.e., \u03b4\u0304 \u201c 1M \u0159M m\u201c1 DKLrqh,m||phs. If \u03b4\u0304 \u0105 C, we update \u03b2 by \u03b2 \u00d0 \u03b2\u02c6p1`\u03c4Cq; if \u03b4\u0304 \u0103 C \u00b4 \u03f5C , we update \u03b2 by \u03b2 \u00d0 \u03b2{p1 ` \u03c4Cq. Here \u03f5C is a threshold parameter to stabilize the training process and prevent overly frequent updates to \u03b2, and \u03c4C is the adjustment step size. Unless otherwise stated, we set \u03c4C \u201c 0.5 in our experiments. Empirically, we find the value of \u03b2 stabilizes after 30 to 50 iterations. We present the pseudocode of this prior learning algorithm in Algorithm 1. Then, our training step is a three-step coordinate descent process analogous to Guo et al. (2023)\u2019s:\n1. Optimize variational parameters, linear transformation and upsampling network: Fix the prior ph, and optimize Equation (1) or its modified version from Section 3.3 via gradient descent. Note, that L is a function of the linear transform A and upsampling network parameters \u03d5 too:\nt\u03bdm,\u03c1muMm\u201c1 ,A, \u03d5 \u00d0 argmin t\u03bdm,\u03c1muMm\u201c1,A,\u03d5\n#\n1\nM\nM \u00ff\nm\u201c1 LpDm, qh,m, ph,A, \u03d5, \u03b2q\n+\n. (6)\n2. Update prior: Update the prior parameters by the closed-form solution:\n\u00b5 \u00d0 1 M\nM \u00ff\nm\u201c1 \u03bdm, \u03c3 \u00d0\n1\nM\nM \u00ff\nm\u201c1\n\u201d p\u03bdm \u00b4 \u00b5q2 ` \u03c1m \u0131 . (7)\n3. Update \u03b2: Set \u03b2 \u00d0 \u03b2 \u02c6 p1 ` \u03c4Cq or \u03b2 \u00d0 \u03b2{p1 ` \u03c4Cq based on the procedure described above.\nNote that unlike other INR-based methods (Dupont et al., 2022; Schwarz & Teh, 2022; Schwarz et al., 2023) our training procedure is remarkably stable, as we illustrate in Appendix D.4."
        },
        {
            "heading": "4 RELATED WORKS",
            "text": "Nonlinear transform coding: Currently, the dominant paradigm in neural compression is nonlinear transform coding (NTC; Balle\u0301 et al., 2020) usually implemented using variational autoencoders (VAE). NTC has achieved impressive performance in terms of both objective metrics (Cheng et al., 2020; He et al., 2022) and perceptual quality (Mentzer et al., 2020), mainly due to their expressive learned non-linear transforms (Balle\u0301 et al., 2020; Zhu et al., 2021; Liu et al., 2023) and elaborate entropy models (Balle\u0301 et al., 2018; Minnen et al., 2018; Guo et al., 2021).\nCompressing INRs can also be viewed as a form of NTC: we use gradent descent to transform data into an INR. The idea to quantize INR weights and entropy code them was first proposed by Dupont et al. (2021), whose method has since been extended significantly (Dupont et al., 2022; Schwarz & Teh, 2022; Schwarz et al., 2023). The current state-of-the-art INR-based method, VCINR (Schwarz et al., 2023), achieves impressive results across several data modalities, albeit at the cost of significantly higher complexity and still falling short of autoencoder-based NTC methods on images. Our method, following COMBINER (Guo et al., 2023), differs from all of the above methods, as it uses REC to encode our variational INRs, instead of quantization and entropy coding.\nLinear weight reparameterization: Similar to our proposal in Section 3.1, Oktay et al. (2019) learn an affine reparameterization of the weights of large neural networks. They demonstrate that scalar quantization in the transformed space leads to significant gains in compression performance. However, since they are performing one-shot model compression, their linear transformations have\n1As a slight abuse of notation, we use \u03d5 to denote both the upsampling function and its parameters.\nvery few parameters as they need to transmit them alongside the quantized weights, limiting their expressivity. On the other hand, RECOMBINER learns the linear transform during training after which it is fixed and shared between communicating parties, thus it does not cause any communication overhead. Therefore, our linear transformation can be significantly more expressive.\nPositional encodings: Some recent works have demonstrated that learning positional features is beneficial for fitting INRs (Jiang et al., 2020; Kim et al., 2022; Mu\u0308ller et al., 2022; Ladune et al., 2023). Sharing a similar motivation, our method essentially incorporates implicit representations with explicit ones, forming a hybrid INR framework (Chen et al., 2023)."
        },
        {
            "heading": "5 EXPERIMENTAL RESULTS",
            "text": "In this section, we evaluate RECOMBINER on image, audio, video, and 3D protein structure data and demonstrate that it achieves strong performance across all modalities. We also perform extensive ablation studies on the CIFAR-10 and Kodak datasets which demonstrate RECOMBINER\u2019s robustness and the effectiveness of each of our proposed solutions. For all experiments, we use a 4-layer, 32- hidden unit SIREN network (Sitzmann et al., 2020) as the INR architecture unless otherwise stated, and a small 3-layer convolution network as the upsampling network \u03d5, as shown in Figure 6 in the appendix. See Appendix C for the detailed description of our experimental setup."
        },
        {
            "heading": "5.1 DATA COMPRESSION ACROSS MODALITIES",
            "text": "Image: We evaluate RECOMBINER on the CIFAR-10 (Krizhevsky et al., 2009) and Kodak (Kodak, 1993) image datasets, and show its rate-distortion (RD) performance in Figure 3a, and compare it against recent INR and VAE-based methods, as well as VTM (JVET, 2020)2, BPG (Bellard, 2014) and JPEG2000. RECOMBINER displays remarkable performance on CIFAR-10, especially at low bitrates, outperforming even VAE-based codecs. On Kodak, it outperforms most INR-based codecs and is competitive with the more complex VC-INR method of Schwarz et al. (2023). Finally, while RECOMBINER still falls behind VAE-based codecs, it significantly reduces the performance gap.\nAudio: Following the experimental set-up of Guo et al. (2023), we evaluate our method on the LibriSpeech (Panayotov et al., 2015) dataset. In Figure 3b, we depict RECOMBINER\u2019s RD curve on the full test set, alongside the curves of VC-INR, COIN++, and MP3. We can see RECOMBINER outperforms both COIN++ and MP3 and matches with VC-INR. Since Guo et al. (2023) only tested COMBINER on 24 test clips, we do not include COMBINER in this plot but put an extra comparison in Figure 13 in Appendix F, where we can also see that RECOMBINER clearly outperforms COMBINER.\nVideo: We evaluate RECOMBINER on UCF-101 action recognition dataset (Soomro et al., 2012), following Schwarz et al. (2023)\u2019s experimental setup. However, as they do not report their traintest split and due to the time-consuming encoding process of our approach, we only benchmark our method against H.264 and H.265 on 16 randomly selected video clips. Figure 3c shows RECOMBINER achieves comparable performance to the classic domain-specific codecs H.264 and H.265, especially at lower bitrates. However, there is still a gap between our approach and H.264 and H.265 when they are configured to prioritize quality. Figure 3e shows a non-cherry-picked video compressed with RECOMBINER at two different bitrates and its reconstruction errors.\n3D Protein Structure: To further illustrate the applicability of our approach, we use it to compress the 3D coordinates of C\u03b1 atoms in protein fragments. We take domain-specific lossy codecs as baselines, including Foldcomp (Kim et al., 2023), PDC (Zhang & Pyle, 2023) and PIC (Staniscia & Yu, 2023). Surprisingly, as shown in Figure 3d, RECOMBINER\u2019s performance is competitive with highly domain-specific codecs. Furthermore, it allows us to tune its rate-distortion performance, whereas the baselines only support a certain compression rate. Since the experimental resolution of 3D structures is typically between 1-3 A\u030a (RCSB Protein Data Bank, 2000), RECOMBINER could help with reducing the increasing storage demand for protein structures without losing key information. Figure 3f shows non-cherry-picked examples compressed with our method."
        },
        {
            "heading": "5.2 EFFECTIVENESS OF OUR SOLUTIONS, ABLATION STUDIES AND RUNTIME ANALYSIS",
            "text": "This section showcases RECOMBINER\u2019s robustness to model size and the effectiveness of each component. Appendix D.1 provides additional visualizations for a deeper understanding of our methods.\nPositional encodings facilitate local deviations: Figure 4 compares images obtained by RECOMBINER with and without positional encodings at matching bitrates and PSNRs. As we can see, positional encodings preserve intricate details in fine-textured regions while preventing noisy artifacts in other regions of the patches, making RECOMBINER\u2019s reconstructions more visually pleasing.\n2 https://vcgit.hhi.fraunhofer.de/jvet/VVCSoftware_VTM/-/tree/VTM-12.0?ref_type=tags\nRECOMBINER is more robust to model size: Using the same INR architecture, Figure 5a shows COMBINER and RECOMBINER\u2019s RD curves as we vary the number of hidden units. RECOMBINER displays minimal performance variation and also consistently outperforms COMBINER. Based on Figure 7 in Appendix D, this phenomenon is likely due to RECOMBINER\u2019s linear weight reparameterization allowing it to more flexibly prune its weight representations.\nAblation study: In Figures 5b and 5c, we ablate our linear reparameterization, positional encodings, hierarchical model, and permutation strategy on CIFAR-10 and Kodak, with five key takeaways:\n1. Linear weight reparameterization consistently improves performance on both datasets, yielding up to 4dB gain on CIFAR-10 at high bitrates and over 0.5 dB gain on Kodak in PSNR.\n2. Learnable positional encodings provide more substantial advantages at lower bitrates. On CIFAR-10, the encodings contribute up to 0.5 dB gain when the bitrate falls below 2 bpp. On Kodak, the encodings provide noteworthy gains of 2 dB at low bitrates and 1 dB at high bitrates.\n3. Surprisingly, the hierarchical model without positional encodings can degrade performance. We hypothesize that this is because directly applying the hierarchical model poses challenges in optimizing Equation (1). A potential solution is to warm up the rate penalty \u03b2 level by level akin to what is done in hierarchical VAEs (S\u00f8nderby et al., 2016), which we leave for further work.\n4. However, positional encodings appear to consistently alleviate this optimization difficulty, yielding 0.5 dB gain when used with hierarchical models.\n5. Our proposed permutation strategy provides significant gains of 0.5 dB at low bitrates and more than 1.5 dB at higher bitrates.\nRuntime Analysis: We list RECOMBINER\u2019s encoding and decoding times in Appendix D.5. Unfortunately, our approach exhibits a long encoding time, similar to COMBINER. However, our decoding process is still remarkably fast, matching the speed of COIN and COMBINER, even on CPUs."
        },
        {
            "heading": "6 CONCLUSIONS AND LIMITATIONS",
            "text": "In this paper, we propose RECOMBINER, a new codec based on several non-trivial extensions to COMBINER, encompassing the linear reparameterization for the network weights, learnable positional encodings, and expressive hierarchical Bayesian models for high-resolution signals. Experiments demonstrate that our proposed method sets a new state-of-the-art on low-resolution images at low bitrates, and consistently delivers strong results across other data modalities.\nA major limitation of our work is the encoding time complexity and tackling it should be of primary concern in future work. A possible avenue for solving this issue is to reduce the number of parameters to optimize over and switch from inference over weights to modulations using, e.g. FiLM layers (Perez et al., 2018), as is done in other INR-based works. A second limitation is that while compressing with patches enables parallelization and higher robustness, it is suboptimal as it leads to block artifacts, as can be seen in Figure 4. Third, as Guo et al. (2023) demonstrate, the approximate samples given by A\u02da coding significantly impact the methods performance, e.g. by requiring more fine-tuning. An interesting question is whether an exact REC algorithm could be adapted to solve this issue, such as the recently developed greedy Poisson rejection sampler (Flamich, 2023)."
        },
        {
            "heading": "7 ACKNOWLEDGEMENTS",
            "text": "The authors would like to thank Runsen Feng for helping us ensure that our baseline for our experiments on video compression is correctly set up. GF acknowledges funding from DeepMind. ZG acknowledges funding from the Outstanding PhD Student Program at the University of Science and Technology of China."
        },
        {
            "heading": "A NOTATIONS",
            "text": "We summarize the notations used in this paper in Table 1:"
        },
        {
            "heading": "B RECOMBINER\u2019S TRAINING ALGORITHMS",
            "text": "We describe the algorithm to train RECOMBINER in Algorithm 1.\nAlgorithm 1 Training RECOMBINER: the prior, the linear transform A and upsampling network \u03d5 Require: Training data tD1, ...,DMu; desired bitrate C.\nInitialize: qh,m \u201c N p\u03bdm,diag p\u03c1mqq for every training instance Dm. Initialize: ph \u201c N p\u00b5,diag p\u03c3qq. Initialize: A, \u03d5. repeat until convergence\n# Step 1: Optimize posteriors, linear reparameterization matrix, and upsampling network t\u03bdm,\u03c1muMm\u201c1 ,A, \u03d5 \u00d0 argmint\u03bdm,\u03c1muMm\u201c1,A,\u03d5 ! 1 M \u0159M m\u201c1 LpDm, qh,m, ph,A, \u03d5, \u03b2q ) . \u0179 Optimize by Equation (6) # Step 2: Update prior \u00b5 \u00d0 1M \u0159M m\u201c1 \u03bdm, \u03c3 \u00d0 1 M \u0159M m\u201c1 \u201d p\u03bdm \u00b4 \u00b5q2 ` \u03c1m \u0131 . \u0179 Update by Equation (7)\n# Step 3: Update \u03b2 \u03b4\u0304 \u201c 1M \u0159M m\u201c1 DKLrqh,m||phs. \u0179 Calculate the average training KL if \u03b4\u0304 \u0105 C then \u03b2 \u00d0 \u03b2 \u02c6 p1 ` \u03c4Cq \u0179 Increase \u03b2 if budget is exceeded end if if \u03b4\u0304 \u0103 C \u00b4 \u03f5C then\n\u03b2 \u00d0 \u03b2{p1 ` \u03c4Cq \u0179 Decrease \u03b2 if budget is not fully occupied end if\nend repeat Return: ph \u201c N p\u00b5,diag p\u03c3qq, A, \u03d5."
        },
        {
            "heading": "C SUPPLEMENTARY EXPERIMENTAL DETAILS",
            "text": ""
        },
        {
            "heading": "C.1 DATASETS AND MORE DETAILS ON EXPERIMENTS",
            "text": "In this section, we describe the dataset and our experimental settings. We depict the upsampling network we used in Figure 6 and summarize the hyperparameters for each modality in Table 2. Besides, we present details for the baselines in Appendix C.2.\nNote, that as the proposed linear reparameterization yields a full-covariance Gaussian posterior over the weights in the INR, the local reparameterization trick (Kingma et al., 2015) is not applicable in RECOMBINER. Therefore, in the above experiments, when inferring the posteriors of a test signal, we employ a Monte Carlo estimator with 5 samples to estimate the expectation in \u03b2-ELBO in Equation (1). While during the training stage, we still use 1 sample. In Appendix D.3, we provide an analysis of the sample size\u2019s influence. It is worth noting that using just 1 sample during inferring does not significantly deteriorate performance, and therefore we have the flexibility to reduce the sample size when prioritizing encoding time, with marginal performance impact.\nCIFAR-10: CIFAR-10 is a set of low-resolution images with a size of 32 \u02c6 32. It has a training set of 50,000 images and a test set of 10,000 images. We randomly select 15,000 images from the training set for the training stage and evaluate RD performance on all test images. we use SIREN network (Sitzmann et al., 2020) with 4 layers and 32 hidden units as the INR architecture.\nKodak: Kodak dataset is a commonly used image compression benchmark, containing 24 images with resolutions of either 768 \u02c6 512 or 512 \u02c6 768. In our experiments, we split each images into 96 patches with size 64 \u02c6 64. Lacking a standard training set, we randomly select and crop 83 images with the same size (splitting into 7,968 patches) from the DIV2K dataset (Agustsson & Timofte, 2017) as the training set. We compress each Kodak image in 64 \u02c6 64 patches. For each patch, we use the same INR setup as that for CIFAR-10, i.e., SIREN network (Sitzmann et al., 2020) with 4 layers and 32 hidden units. Besides, we apply a three-level hierarchical Bayesian model to Kodak\npatches. The lowest level has 96 patches. Every 16 (4 \u02c6 4) patches are grouped together in the second level, and in total there are 6 groups. The highest level consists of a global representation for the entire image.\nAudio: LibriSpeech (Panayotov et al., 2015) is a speech dataset recorded at a 16kHz sampling rate. We follow the experiment settings by Guo et al. (2023), taking the first 3 seconds of every recording, corresponding to 48,000 audio samples. We compress each audio clip with 60 patches, each of which has 800 audio samples. For each patch, we use the same INR architecture as CIFAR10 except the output of the network has only one dimension. We train RECOMBINER on 197 training instances (corresponding to 11,820 patches) and evaluate it on the test set split by Guo et al. (2023), consisting of 24 instances. We also apply a three-level hierarchical model. The lowest level consists of 60 patches. Every 4 patches are grouped together in the second level, and in total there are 60{4 \u201c 16 groups. The highest level consists of a global representation for the entire signal. Video: UCF-101 (Soomro et al., 2012) is a dataset of human actions. It consists of 101 action classes, over 13k clips, and 27 hours of video data. We follow Schwarz et al. (2023) center-cropping each video clip to 240\u02c6240\u02c624 and then resizing them to 128\u02c6128\u02c624. Then we compress each clip with 16\u02c616\u02c624 patches. We train RECOMBINER on 75 video clips (4,800 patches) and evaluate it on 16 randomly selected clips. For each patch, we still use the INR with 4 layers and 32 hidden units. We also apply the three-level hierarchical model. The lowest level consists of 64 patches. Every 16 4 \u02c6 4 patches are grouped together in the second level, and in total, there are 4 groups. The highest level consists of a global representation for the entire clip. 3D Protein structure: We evaluate RECOMBINER on the Saccharomyces cerevisiae proteome from the AlphaFold DB v43. To standardize the dataset, for each protein, we take the C\u03b1 atom of the first 96 residues (i.e., amino acids) as the target data to be compressed. The input coordinates are the indices of the C\u03b1 atoms (varying between 1-96, and normalized between 0-1) and the outputs of INRs are their corresponding 3D coordinates. We randomly select 1,000 structures as the test set and others as the training set. We still use the same INR architecture as CIFAR-10, i.e., SIREN network with 4 layers and 32 hidden units in each layer. We use the standard MSE as the distortion measure. Note that our method can also be extended to take the fact that the 3D structure is rotation and translation invariant into account by using different losses."
        },
        {
            "heading": "C.2 BASELINE SETTINGS",
            "text": "The baseline performances, including JPEG2000, BPG, COIN, COIN++, Balle\u0301 et al. (2018) and Cheng et al. (2020) on CIFAR-10 and Kodak, and MP3 and COIN++ on the full test set of LibriSpeech, are taken from the COIN++\u2019s GitHub repo4. Statistics for VC-INR and MSCN are pro-\n3https://ftp.ebi.ac.uk/pub/databases/alphafold/v4/UP000002311_559292_"
        },
        {
            "heading": "YEAST_v4.tar",
            "text": "4https://github.com/EmilienDupont/coinpp"
        },
        {
            "heading": "Patching",
            "text": ""
        },
        {
            "heading": "Positional Encodings",
            "text": ""
        },
        {
            "heading": "Training Stage",
            "text": "vided by the authors in the paper. We also include a comparison of RECOMBINER and COMBINER on 24 test audio clips since the authors of COMBINER did not test on the full test set. For this comparison, the performances of COMBINER and MP3 on 24 test audio clips are provided by the authors of COMBINER.\nBelow, we describe details about the baseline of the video and protein structure compression.\nC.2.1 VIDEO BASELINES\nVideo compression baselines are implemented by ffmpeg (Tomar, 2006), with the following commands.\nH.264 (best speed):\nffmpeg.exe -i INPUT.avi -c:v libx264 -preset ultrafast -crf $CRF"
        },
        {
            "heading": "OUTPUT.mkv",
            "text": "H.265 (best speed):\nffmpeg.exe -i INPUT.avi -c:v libx265 -preset ultrafast -crf $CRF OUTPUT.mkv\nH.264 (best quality):\nffmpeg.exe -i INPUT.avi -c:v libx264 -preset veryslow -crf $CRF"
        },
        {
            "heading": "OUTPUT.mkv",
            "text": "H.265 (best quality):\nffmpeg.exe -i INPUT.avi -c:v libx265 -preset veryslow -crf $CRF"
        },
        {
            "heading": "OUTPUT.mkv",
            "text": "The argument $CRF varies in 15 20 25 30 35 40."
        },
        {
            "heading": "C.2.2 PROTEIN BASELINES",
            "text": "Softwares implementing PIC, PDC and Foldcomp are available at https://github. com/lukestaniscia/PIC, https://github.com/kad-ecoli/pdc and https:// github.com/steineggerlab/foldcomp.\nPIC first employs a lossy mapping, converting the 3D coordinates of atoms to an image, and then lossless compresses the image in PNG format. We use the PNG image size to calculate the bitrate.\nAs for PDC and Foldcomp, since they directly operate on PDB files containing other information like the headers, sequences, B factor, etc., we cannot use the file size directly. Therefore, we use their theoretical bitrates as our baseline. Below we present how we calculate their theoretical bitrates.\nPDC uses three 4-byte integers to save the coordinates of the first C\u03b1 atom, and three 1-byte integers for coordinate differences of all remaining C\u03b1 atoms. Therefore, in theory, for a 96-residue length protein, each C\u03b1 atom is assigned with p8 \u02c6 3 \u02c6 95 ` 4 \u02c6 8 \u02c6 3 \u02c6 1q{96 bits. Foldcomp compresses the quantized dihedral/bond angles for each residue. Every residue needs 59 bits. Besides, Foldcomp saves uncompressed coordinates for every 25 residues as anchors, which requires 36 bytes. Therefore, the theoretical number of bits assigned to each C\u03b1 is given by p36 \u02c6 8 ` 59 \u02c6 25q{25. However, since Foldcomp is designed to encode all backbone atoms (C, N, C\u03b1) instead of merely C\u03b1, it is unfair to compare in this way. We thus also report its performance on all backbone atoms for reference."
        },
        {
            "heading": "C.3 ABLATION STUDY SETTINGS",
            "text": "In this section, we describe the details settings for ablation studies in Figures 5b and 5c.\nExperiments without Linear Reparameterization: We simply set w \u201c hw without the linear matrix A. Besides, since in this case, w follows mean-field Gaussian, we use the local reparameterization trick with 1 sample to reduce the variance during both training and inferring.\nExperiments without Positional Encodings: Recall that the inputs of INRs in RECOMBINER is the concatenation of Fourier transformed coordinates \u03b3pxiq and the upsampled positional encodings at the corresponding position zi \u201c \u03d5phzqxi . In the experiments without positional encodings, we only input the Fourier transformed coordinates to the INR. To keep the INR size consistent, we also increase the dimension of the Fourier transformation, so that dimp\u03b31pxiqq \u00d0 dimp\u03b3pxiqq ` dimpziq. Also, we no longer need to train the upsampling network \u03d5.\nExperiments without Hierarchical Model: We assume all patch-INRs are independent and simply assign independent mean-field Gaussian priors and posteriors over hp\u03c0qw for each patch.\nExperiments without Random Permutation across patches: Recall in RECOMBINER, for each level in the hierarchical model, we stack the representations together into a matrix, where each row is one representation. We then (a) apply the same permutation over all rows. This is the same as COMBINER and is to ensure KL is distributed uniformly across the entire representation for each patch. Then (b) for each column, we apply its own permutation to encourage KL to be distributed uniformly across patches. In the ablation study, we do not only apply the permutation in (b) but still perform the permutation in (a)."
        },
        {
            "heading": "D SUPPLEMENTARY EXPERIMENTS AND RESULTS",
            "text": ""
        },
        {
            "heading": "D.1 METHODS VISUALIZATION",
            "text": "In this section, we bring insights into our methods by visualizations. Recall that each signal is represented by hZ and hw together in RECOMBINER. We visualize the information contained in each of them. Besides, we visualize the linear transform A to understand how it improves performances.\nPositional encodings: We take kodim03 at 0.488 bpp as an example, and visualize 4 channels of its upsampled positional encodings \u03d5phzq in Fig 7a. Interestingly, before fed into the INR, the positional\nencodings already present a pattern of the image. This is an indication of how the learnable positional encodings help with the fitting. When the target signal is intricate, and there is a strict bitrate constraint, the INR capacity is insufficient for learning the complex mapping from coordinates to signal values directly. On the other hand, when combined with positional encodings, INR simply needs to extract, combine, and enhance this information, instead of \u201ccreating\u201d information from scratch. This aligns with the findings of the ablation study, which indicate that learnable positional encodings have a more significant impact on CIFAR-10 at low bitrates and the Kodak dataset, but a small effect on CIFAR-10 at high bitrates.\nInformation contained in hw: To visualize the information contained in hw, we also take kodim03 at 0.488 bpp as an example. We reconstruct the image using hw for this image but mask out hZ by the prior mean. The image reconstructed in this way is shown in Fig 7b.\nFrom the figure, we can clearly see hw mostly captures the color specific to each patch, in comparison to the positional encodings containing information more about edges and shapes. Moreover, interestingly, we can see patches close to each other share similar patterns, indicating the redundancy between patches. This explains why employing the hierarchical model provides substantial gains, especially when applying it together with positional encodings.\nLinear Transform A: To interpret how the linear reparameterization works, we take the Kodak dataset as an example, and visualize A for the second layer (i.e., Ar2s) at 0.074 and 0.972 bpp in Fig 7c and 7d. Note that this layer has 32 hidden units and thus Ar2s has a shape of 1056 \u02c6 1056. We only take a subset of 150 \u02c6 150 in order to have a clearer visualization. Recall w \u201c hwA, and thus rows correspond to dimensions in hw and columns correspond to dimensions in w.\nIt can be seen that when the bitrate is high, many rows in A are active, enabling a flexible model. Conversely, at lower bitrates, many rows become 0, effectively pruning out corresponding dimensions. This explains clearly how A contributes to improve the performance: first, A greatly promotes parameter sharing. For instance, at low bitrates, merely 10 percent of the parameters get involved in constructing the entire network. Second, the pruning in hw is more efficient than that in w directly. The predecessor of RECOMBINER, i.e., COMBINER, utilizes standard Bayesian neural networks. It controls its bitrates by pruning or activating the hidden units. When a unit is pruned, the entire column in the weight matrix will be pruned out (Trippe & Turner, 2017). In other words, in COMBINER, the pruning in w is always conducted in chunks, which highly limits the flexibility of the network. On the contrary, in our approach, the linear reparameterization enables a direct pruning or activating of each dimension in hw individually, ensuring the flexibility of INR while effectively managing the rate.\nAnother interesting observation is the matrix A essentially learns a low-rank pattern without manual tuning. This is in comparison with VC-INR (Schwarz et al., 2023) where the low-rank pattern is explicitly enforced by manually setting the LoRA-style (Hu et al., 2021) modulation."
        },
        {
            "heading": "D.2 EFFECTIVENESS OF RANDOM PERMUTATION",
            "text": "In this section, we provide an example illustrating the effectiveness of random permutation across patches. Specifically, we encode kodim23 at 0.074 bpp, both with and without random permutation, and visualize their residual images in Figure 8. We can see that, without permutation, the residuals for complex patches are significantly larger than simpler patches. This is due to the fact that, in RECOMBINER, the bits allocated to each patch are merely determined by the number of blocks, which is shared across all the patches. On the other hand, after the permutation, we can see a more balanced distribution of residuals across patches: complex patches achieve better reconstructions, whereas simple patches\u2019 performances only degrade marginally. This is because, after the permutation across patches, each block can have different patches\u2019 representations, enabling an adaptive allocation of bits across patches. Overall, random permutation yields a 1.00 dB gain on this image.\nD.3 INFLUENCE OF SAMPLE SIZE\nAs discussed in Appendix C.1, in our experiments, we use 5 samples to estimate the expectation in the \u03b2-ELBO in Equation (1), when inferring the posterior of a test datum. Here, we provide the RD\ncurve using 1, 5 and 10 samples, on 500 randomly selected Cifar-10 test images and kodim03 as examples, to illustrate the influence of different choices of sample sizes.\nAs shown in Figure 9, the sample size mainly impacts the performance at high bitrates. Besides, further increasing the sample size to 10 only brings a minor improvement. Therefore, we choose 5 samples in our experiments to balance between encoding time and performance. It is also worth noting that using just 1 sample does not significantly reduce the performance. Therefore, we have the flexibility of choosing smaller sample sizes when prioritizing encoding time, with minor performance impacts."
        },
        {
            "heading": "D.4 ROBUSTNESS DURING TRAINING",
            "text": "Different from previous INR-based codecs based on MAML (Finn et al., 2017) including COIN++ (Dupont et al., 2022), MSCN (Schwarz & Teh, 2022) and VC-INR (Schwarz et al., 2023), our proposed RECOMBINER does not require nested gradient descent and thus features higher stability during training period.\nTo demonstrate this advantage, we present a visualization of the average \u03b2-ELBO during training on CIFAR-10 across three bitrates in Figure 10. We can see that the training curves exhibit an initial dip followed by a consistent increase. The dip at the beginning is a result of our adjustment of \u03b2 during training (Step 3 in Algorithm 1). Importantly, this adjustment does not impact training robustness; and we can see that \u03b2 is quickly adjusted, and the training proceeds smoothly."
        },
        {
            "heading": "D.5 CODING TIME",
            "text": "In this section, we provide details regarding the encoding and decoding time of RECOMBINER. The encoding speed is measured on a single NVIDIA A100-SXM-80GB GPU. On CIFAR-10 and protein structures, we compress signals in batch, with a batch size of 500 images and 1,000 structures, respectively. On Kodak, audio, and video datasets, we compress each signal separately. We should note that the batch size does not influence the results. Posteriors of signals within one batch are optimized in parallel, and their gradients are not crossed. The decoding speed is measured per signal on CPU.\nSimilar to COMBINER, our approach features a high encoding time complexity. However, the decoding process is remarkably fast, even on CPU, matching the speed of COIN and COMBINER. Note that the decoding time listed here encompasses the retrieval of samples for each block. In practical applications, this process can be implemented and parallelized using lower-level languages such as C++ or C, which can lead to further acceleration of execution."
        },
        {
            "heading": "E THINGS WE TRIED THAT DID NOT WORK",
            "text": "\u2022 in RECOMBINER, we apply linear reparameterization on INR weights, which transfers the weights linearly into a transformed space. Perhaps a natural extension is to apply more complex transformations, e.g., neural networks, or flows. We experimented with this idea, but it did not provide gains over the linear transformation.\n\u2022 in RECOMBINER, we propose a hierarchical Bayesian model, equivalent to assigning hierarchical hyper-priors and inferring the hierarchical posteriors over the means of the INR weights. A natural extension can be assigning hyper-priors/posteriors to both means and variances. But we did not find any gain by this.\n\u2022 in RECOMBINER, the hierarchical Bayesian model is only applied to the latent INR weights hw. It is natural to apply the same hierarchical structure to the latent positional encodings hz. However, we found it does not provide visible gain."
        },
        {
            "heading": "F MORE RD CURVES",
            "text": "Here, we show the full-resolution RD curves for image compression in Figures 11 and 12. Besides, we also provide a further comparison between RECOMBINER with COMBINER on 24 test audio clips from LibriSpeech in Figure 13."
        },
        {
            "heading": "G RD VALUES",
            "text": "CIFAR-10:\nrate = [0.297, 0.719, 0.938, 1.531, 1.922, 3.344, 4.391]\nPSNR = [23.592, 27.222, 28.505, 30.911, 32.168, 35.732, 38.139]"
        },
        {
            "heading": "Kodak:",
            "text": "rate = [0.074, 0.130, 0.178, 0.316, 0.488, 0.972, 1.567, 3.320]\nPSNR = [26.158, 27.653, 28.594, 30.439, 31.953, 34.540, 36.547, 40.426]"
        },
        {
            "heading": "Audio:",
            "text": "On full test set:\nrate = [5.685, 10.661, 22.112, 43.637]\nPSNR = [42.612, 47.101, 52.196, 58.195]\nOn 24 test examples (to compare with COMBINER):\nrate = [5.168, 10.805, 22.112, 43.637]\nPSNR = [42.789, 47.106, 52.206, 58.327]"
        },
        {
            "heading": "Video:",
            "text": "rate = [0.115, 0.244, 0.605, 1.183]\nPSNR = [28.722, 31.494, 35.717, 39.171]"
        },
        {
            "heading": "Protein:",
            "text": "rate = [11.17, 35.17, 60.67, 83.83, 106.17]\nRMSD = [0.9242, 0.1388, 0.0709, 0.0506, 0.0436]"
        },
        {
            "heading": "H MORE DECODED EXAMPLES",
            "text": "H.1 CIFAR-10"
        },
        {
            "heading": "H.2 KODAK",
            "text": "H.4 VIDEO"
        },
        {
            "heading": "H.5 PROTEIN STRUCTURE",
            "text": ""
        }
    ],
    "title": "RECOMBINER: ROBUST AND ENHANCED COMPRESSION WITH BAYESIAN IMPLICIT NEURAL REPRESENTATIONS",
    "year": 2024
}