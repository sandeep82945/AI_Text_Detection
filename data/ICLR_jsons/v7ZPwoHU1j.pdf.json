{
    "abstractText": "K-means clustering is a widely used machine learning method for identifying patterns in large datasets. Semidefinite programming (SDP) relaxations have recently been proposed for solving the K-means optimization problem that enjoy strong statistical optimality guarantees, but the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. By contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm that is widely used by machine learning practitioners, but without a solid statistical underpinning nor rigorous guarantees. In this paper, we describe an NMF-like algorithm that works by solving a nonnegative low-rank restriction of the SDP relaxed K-means formulation using a nonconvex Burer\u2013Monteiro factorization approach. The resulting algorithm is just as simple and scalable as state-of-theart NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the SDP. In our experiments, we observe that our algorithm achieves substantially smaller mis-clustering errors compared to the existing state-of-the-art.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yubo Zhuang"
        },
        {
            "affiliations": [],
            "name": "Xiaohui Chen"
        },
        {
            "affiliations": [],
            "name": "Yun Yang"
        },
        {
            "affiliations": [],
            "name": "Richard Y. Zhang"
        }
    ],
    "id": "SP:0c761f1eed1271e89458feece2f4d391d92734f8",
    "references": [
        {
            "authors": [
                "Daniel Aloise",
                "Amit Deshpande",
                "Pierre Hansen",
                "Preyas Popat"
            ],
            "title": "Np-hardness of euclidean sum-ofsquares clustering",
            "venue": "Machine learning,",
            "year": 2009
        },
        {
            "authors": [
                "David Arthur",
                "Sergei Vassilvitskii"
            ],
            "title": "K-means++ the advantages of careful seeding",
            "venue": "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,",
            "year": 2007
        },
        {
            "authors": [
                "Dimitri P Bertsekas"
            ],
            "title": "Constrained optimization and Lagrange multiplier methods",
            "venue": "Academic press,",
            "year": 2014
        },
        {
            "authors": [
                "Samuel Burer",
                "Renato D.C. Monteiro"
            ],
            "title": "A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization",
            "venue": "Mathematical Programming,",
            "year": 2003
        },
        {
            "authors": [
                "Xiaohui Chen",
                "Yun Yang"
            ],
            "title": "Cutoff for exact recovery of gaussian mixture models",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Yuejie Chi",
                "Yue M Lu",
                "Yuxin Chen"
            ],
            "title": "Nonconvex optimization meets low-rank matrix factorization: An overview",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Andrzej Cichocki",
                "Anh-Huy Phan"
            ],
            "title": "Fast Local Algorithms for Large Scale Nonnegative Matrix and Tensor Factorizations",
            "venue": "IEICE Transactions on Fundamentals of Electronics Communications and Computer Sciences,",
            "year": 2009
        },
        {
            "authors": [
                "Sanjoy Dasgupta"
            ],
            "title": "The hardness of k-means clustering",
            "venue": "Technical Report CS2007-0890,",
            "year": 2007
        },
        {
            "authors": [
                "Chris Ding",
                "Xiaofeng He",
                "Horst D Simon"
            ],
            "title": "On the equivalence of nonnegative matrix factorization and spectral clustering",
            "venue": "In Proceedings of the 2005 SIAM international conference on data mining,",
            "year": 2005
        },
        {
            "authors": [
                "Dheeru Dua",
                "Casey Graff"
            ],
            "title": "UCI machine learning repository, 2017",
            "venue": "URL http://archive. ics.uci.edu/ml",
            "year": 2017
        },
        {
            "authors": [
                "Yingjie Fei",
                "Yudong Chen"
            ],
            "title": "Hidden integrality of sdp relaxations for sub-gaussian mixture models",
            "venue": "Proceedings of the 31st Conference On Learning Theory,",
            "year": 2018
        },
        {
            "authors": [
                "Dami\u00e1n Fern\u00e1ndez",
                "Mikhail V Solodov"
            ],
            "title": "Local convergence of exact and inexact augmented lagrangian methods under the second-order sufficient optimality condition",
            "venue": "SIAM Journal on Optimization,",
            "year": 2012
        },
        {
            "authors": [
                "Rong Ge",
                "Chi Jin",
                "Yi Zheng"
            ],
            "title": "No spurious local minima in nonconvex low rank problems: A unified geometric analysis",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Christophe Giraud",
                "Nicolas Verzelen"
            ],
            "title": "Partial recovery bounds for clustering with the relaxed kmeans",
            "venue": "Math. Stat. Learn.,",
            "year": 2018
        },
        {
            "authors": [
                "Xiao Han",
                "Xin Tong",
                "Yingying Fan"
            ],
            "title": "Eigen selection in spectral clustering: A theory-guided practice",
            "venue": "Journal of the American Statistical Association,",
            "year": 2023
        },
        {
            "authors": [
                "Zhaoshui He",
                "Shengli Xie",
                "Rafal Zdunek",
                "Guoxu Zhou",
                "Andrzej Cichocki"
            ],
            "title": "Symmetric nonnegative matrix factorization: Algorithms and applications to probabilistic clustering",
            "venue": "IEEE Transactions on Neural Networks,",
            "year": 2011
        },
        {
            "authors": [
                "Jingu Kim",
                "Yunlong He",
                "Haesun Park"
            ],
            "title": "Algorithms for nonnegative matrix and tensor factorizations: a unified view based on block coordinate descent framework",
            "venue": "Journal of Global Optimization,",
            "year": 2014
        },
        {
            "authors": [
                "Da Kuang",
                "Sangwoon Yun",
                "Haesun Park"
            ],
            "title": "Symnmf: nonnegative low-rank approximation of a similarity matrix for graph clustering",
            "venue": "Journal of Global Optimization,",
            "year": 2015
        },
        {
            "authors": [
                "Brian Kulis",
                "Arun C. Surendran",
                "John C. Platt"
            ],
            "title": "Fast low-rank semidefinite programming for embedding and clustering",
            "venue": "Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,",
            "year": 2007
        },
        {
            "authors": [
                "Stuart Lloyd"
            ],
            "title": "Least squares quantization in pcm",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 1982
        },
        {
            "authors": [
                "Matthias L\u00f6ffler",
                "Anderson Y. Zhang",
                "Harrison H. Zhou"
            ],
            "title": "Optimality of spectral clustering in the Gaussian mixture model",
            "venue": "The Annals of Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Yu Lu",
                "Harrison H Zhou"
            ],
            "title": "Statistical and computational guarantees of lloyd\u2019s algorithm and its variants",
            "venue": "arXiv preprint arXiv:1612.02099,",
            "year": 2016
        },
        {
            "authors": [
                "J.B. MacQueen"
            ],
            "title": "Some methods for classification and analysis of multivariate observations",
            "venue": "Proc. Fifth Berkeley Sympos. Math. Statist. and Probability,",
            "year": 1967
        },
        {
            "authors": [
                "Dustin G Mixon",
                "Soledad Villar",
                "Rachel Ward"
            ],
            "title": "Clustering subgaussian mixtures by semidefinite programming",
            "venue": "Information and Inference: A Journal of the IMA, 6(4):389\u2013415,",
            "year": 2017
        },
        {
            "authors": [
                "Andrew Y. Ng",
                "Michael I. Jordan",
                "Yair Weiss"
            ],
            "title": "On spectral clustering: Analysis and an algorithm",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2001
        },
        {
            "authors": [
                "Jorge Nocedal",
                "Stephen J. Wright"
            ],
            "title": "Numerical optimization. Springer series in operations research and financial engineering",
            "venue": "ed. edition,",
            "year": 2006
        },
        {
            "authors": [
                "Jiming Peng",
                "Yu Wei"
            ],
            "title": "Approximating k-means-type clustering via semidefinite programming",
            "venue": "SIAM J. OPTIM,",
            "year": 2007
        },
        {
            "authors": [
                "Martin Royer"
            ],
            "title": "Adaptive clustering through semidefinite programming",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2017
        },
        {
            "authors": [
                "Ulrike von Luxburg"
            ],
            "title": "A tutorial on spectral clustering",
            "venue": "Statistics and Computing,",
            "year": 2007
        },
        {
            "authors": [
                "Yu-Xiong Wang",
                "Yu-Jin Zhang"
            ],
            "title": "Nonnegative matrix factorization: A comprehensive review",
            "venue": "IEEE Transactions on knowledge and data engineering,",
            "year": 2012
        },
        {
            "authors": [
                "Wei Xu",
                "Xin Liu",
                "Yihong Gong"
            ],
            "title": "Document clustering based on non-negative matrix factorization",
            "venue": "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval,",
            "year": 2003
        },
        {
            "authors": [
                "Liuqin Yang",
                "Defeng Sun",
                "Kim-Chuan Toh"
            ],
            "title": "Sdpnal+: a majorized semismooth newton-cg augmented lagrangian method for semidefinite programming with nonnegative constraints",
            "venue": "Mathematical Programming Computation,",
            "year": 2015
        },
        {
            "authors": [
                "Zhihui Zhu",
                "Xiao Li",
                "Kai Liu",
                "Qiuwei Li"
            ],
            "title": "Dropping symmetry for fast symmetric nonnegative matrix factorization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Yubo Zhuang",
                "Xiaohui Chen",
                "Yun Yang"
            ],
            "title": "Wasserstein k-means for clustering probability distributions",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yubo Zhuang",
                "Xiaohui Chen",
                "Yun Yang"
            ],
            "title": "Likelihood adjusted semidefinite programs for clustering heterogeneous data",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [],
            "title": "K], and y\u2217 equals \u03b1 in (Chen and Yang, 2021). Theorem 3 (Convergence of projected gradient descent at y\u2217)",
            "year": 2021
        },
        {
            "authors": [
                "Chen",
                "Yang"
            ],
            "title": "2021) is defined to be [BGl,Gk1nk ]j and Bi,j",
            "year": 2021
        },
        {
            "authors": [
                "\u2211 j\u2208Gl"
            ],
            "title": "Dk,l(j),\u2200i \u2208 Gk, j \u2208 Gl (refer to the paragraph right below (22) in (Chen and Yang, 2021))",
            "venue": "On the other hand,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Clustering remains a common unsupervised learning technique, for which the basic objective is to assign similar data points to the same group. Given data in the Euclidean space, a widely used clustering method is K-means clustering, which quantifies \u201csimilarity\u201d in terms of distances between the given data and learned clustering centers, known as centroids (MacQueen, 1967). In order to divide the data points X1, . . . , Xn \u2208 Rp into K groups, K-means clustering aims to minimize the following cost function:\nmin \u03b21,...,\u03b2K\u2208Rp n\u2211 i=1 min k\u2208[K] \u2225Xi \u2212 \u03b2k\u222522, (1)\nwhere \u03b2k is the centroid of the k-th cluster for k \u2208 [K] := {1, . . . ,K}. It is well-known that exactly solving problem (1) is NP hard in the worst-case (Dasgupta, 2007; Aloise et al., 2009), so computationally tractable approximation algorithms and relaxed formulations have been extensively studied in the literature. Notable examples include Lloyd\u2019s algorithm (Lloyd, 1982), spectral clustering (von Luxburg, 2007; Ng et al., 2001), nonnegative matrix factorization (NMF) (He et al., 2011; Kuang et al., 2015; Wang and Zhang, 2012), and semidefinite programming (SDP) (Peng and Wei, 2007; Mixon et al., 2017; Royer, 2017; Fei and Chen, 2018; Giraud and Verzelen, 2018).\nAmong those popular relaxations, the SDP approach enjoys the strongest statistical guarantees under the standard Gaussian mixture model in that it achieves an information-theoretic sharp threshold for the purpose for exact recovery of the true cluster partition (Chen and Yang, 2021). Unfortunately, the SDP and its strong statistical guarantees remain completely inaccessible to real-world datasets, owing to the prohibitively high costs of solving the resulting SDP relaxation. Given n data points, the SDP is a matrix optimization problem, over a dense n\u00d7 n membership matrix Z, that is constrained to be both positive semidefinite Z \u2ab0 0 as well as elementwise nonnegative Z \u2265 0. Even ignoring the constraints, a basic but fundamental difficulty is the need to store and optimize over the n2 individual elements of the matrix. Even a small dataset with n \u2248 1000, such as the banknote authentication dataset (Dua and Graff, 2017), translates into an SDP with n2 \u2248 106 optimization variables, which is right at the very limit of state-of-the-art SDP solvers like SDPNAL+ (Yang et al., 2015).\nOn the other hand, NMF remains one of the simplest and practically useful approaches to clustering due to its scalability (He et al., 2011; Kuang et al., 2015). When the clustering problem at hand exhibits an appropriate low-dimensional structure, NMF gains significant computational savings by imposing elementwise nonnegativity over an n \u00d7 r low-rank factor matrix U \u2265 0, in order to imply positive semidefiniteness Z \u2ab0 0 and elementwise nonnegativity Z \u2265 0 over the n \u00d7 n membership matrix Z = UUT . While highly scalable, there remains unfortunately very little statistical underpinning behind NMF-based algorithms.\nOur contributions. In this paper, we propose an efficient, large-scale, NMF-like algorithm for the Kmeans clustering problem, that meanwhile enjoys the same sharp exact recovery guarantees provided by SDP relaxations. We are motivated by the fact that the three classical approaches to K-means clustering, namely spectral clustering, NMF, and SDP, can all be interpreted as techniques for solving slightly different relaxations of the same underlying K-means mixed integer linear program (MILP); see our exposition in Section 2. This gives us hope to break the existing computational and statistical bottleneck by investigating the intersection of these three classical approaches.\nAt its core, our proposed algorithm is a primal-dual gradient descent-ascent algorithm to optimize over a nonnegative factor matrix, inside an augmented Lagrangian method (ALM) solution of the SDP. The resulting iterations closely resemble the projected gradient descent algorithms widely used for NMF and spectral clustering in the existing literature; in fact, we show that the latter can be recovered from our algorithm by relaxing suitable constraints. We prove that the new algorithm enjoys local linear convergence within a primal-dual neighborhood of the SDP solution, which is unique whenever the centroids satisfy a well-separation condition from (Chen and Yang, 2021). In practice, we observe that the algorithm converges globally at a linear rate. As shown in Figure 1, our algorithm achieves substantially smaller mis-clustering errors compared to the existing state-of-the-art.\nThe main novelty of our algorithm is the use of projected gradient descent to solve the difficult primal update inside the ALM. Indeed, this primal update had been the main critical challenge faced by prior work; while a similar nonnegative low-rank ALM was previously proposed by Kulis et al. (2007), their inability to solve the primal update to high accuracy resulted in a substantial slow-down to the overall algorithm, which behaved more like an inexact ALM. In contrast, our projected gradient descent method is able to solve the primal update at a rapid linear rate to machine precision (see Theorem 1), so our overall algorithm is able to enjoy the rapid primal-dual linear convergence that is predicted and justified by classical theory for exact ALMs. As shown in Figure 4 in Appendix B, our algorithm is the first ALM that can solve the SDP relaxation to arbitrarily high accuracy.\nOrganization. The rest of the paper is organized as follows. In Section 2, we present some background on several equivalent and relaxed formulations of the K-means clustering problem. In\nSection 3, we introduce a nonnegative low-rank SDP for solving the Burer\u2013Monteiro formulation of K-means problem. In Section 4, we establish the linear convergence guarantee for our proposed primal-dual gradient descent-ascent algorithm in the exact recovery regime. Numerical experiments are reported in Section 5. Proof details are deferred to the Supplementary Material.\n2 BACKGROUND ON K-MEANS AND RELATED COMPUTATIONAL METHODS\nDue to the parallelogram law in Euclidean space, the centroid-based formulation of the K-means clustering problem in (1) has an equivalent partition-based formulation (cf. Zhuang et al. (2022)) as\nmax G1,...,GK { K\u2211 k=1 1 |Gk| \u2211 i,j\u2208Gk \u27e8Xi, Xj\u27e9 : K\u2294 k=1 Gk = [n] } , (2)\nwhere the Euclidean inner product \u27e8Xi, Xj\u27e9 = XTi Xj represents the similarity between the vectors Xi and Xj , the clusters (Gk)Kk=1 form a partition of the data index [n], |Gk| denotes the cardinality of Gk and \u2294 denotes disjoint union. The objective function in (2) is the log-likelihood function of the cluster labels (modulo constant) by profiling out the centroids in (1) as nuisance parameters under the standard Gaussian mixture model with a common isotropic covariance matrix across all the K components (Zhuang et al., 2023). Using the one-hot encoding of the partition G1, . . . , GK , we can uniquely associate it (up to label permutation) with a binary assignment matrix H = (hik) \u2208 {0, 1}n\u00d7K such that hik = 1 if i \u2208 Gk and hik = 0 otherwise. Then the K-means clustering problem in (2) can be written as a mixed-integer linear program (MILP) as follows:\nmin H\n{ \u27e8A,HBHT \u27e9 : H \u2208 {0, 1}n\u00d7K , H1k = 1n } , (3)\nwhere A = \u2212XTX is the n \u00d7 n negative Gram matrix of the data Xp\u00d7n = (X1, . . . , Xn), B = diag(|G1|\u22121, . . . , |GK |\u22121) is a normalization matrix and the constraint H1k = 1n with 1n being the n-dimensional vector of all ones reflects the row sum constraint of assignment, i.e., each row of H contains exactly nonzero entry with value one.\nIt is well-known that the problem in (3) is NP-hard in the worst-case (Dasgupta, 2007; Aloise et al., 2009), so various relaxations for the K-means constraint are formulated in the literature to tractably approach problem (3) with the same objective function. Popular methods include: (i) spectral clustering (von Luxburg, 2007; Ng et al., 2001) which only maintains the weakened orthonormal constraint H\u0303T H\u0303 = IK for H\u0303 := HB1/2; (ii) nonnegative matrix factorization (NMF) (He et al., 2011; Kuang et al., 2015) which only enforces the elementwise nonnegativity constraint on H\u0303 \u2265 0; (iii) semidefinite programming (SDP) (Peng and Wei, 2007; Royer, 2017; Giraud and Verzelen, 2018) which reparameterizes the assignment matrix H as the positive semidefinite (psd) membership matrix Z := HBHT \u2ab0 0 and additionally preserves the constraints tr(Z) = K, Z1n = 1n and Z \u2265 0, namely we solve\nmin Z\u2208Sn+\n{ \u27e8A,Z\u27e9 : tr(Z) = K, Z1n = 1n, Z \u2265 0 } , (4)\nwhere Sn+ stands for the convex cone of the n\u00d7 n real symmetric psd matrices. Among those popular relaxations, the SDP approach enjoys the strongest statistical guarantees under the Gaussian mixture model (15). It is shown by Chen and Yang (2021) that the above SDP achieves an information-theoretic limit for the purpose for exact recovery of the true cluster partition. Precisely, the sharp threshold on the centroid-separation for phase transition is given by\n\u0398 2 = 4\u03c32 ( 1 + \u221a 1 + Kp\nn log n\n) log n (5)\nin the sense that for any \u03b1 > 0 and K = O(log(n)/ log log(n)), if the minimal centroid-separation \u0398min := min1\u2264j \u0338=k\u2264K \u2225\u00b5j \u2212 \u00b5k\u22252 \u2265 (1 + \u03b1)\u0398, then with high probability solution of (4) is unique and it perfectly recovers the cluster partition structure; while if \u0398 \u2264 (1 \u2212 \u03b1)\u0398, then the maximum likelihood estimator (and thus any other estimator) fails to exactly recover the cluster partition. In simpler words, the SDP relaxed K-means optimally solves clustering problem with zero mis-clustering error as soon as it is possible to do so, i.e., there is no relaxation gap.\nIt is important to point out that \u0398min \u2192 \u221e is necessarily needed to achieve exact recovery with high probability. When the centroid-separation diverges, spectral clustering is shown to achieve the minimax mis-clustering error rate exp(\u2212(1 + o(1))\u03982min/8) under the condition p = o(n\u0398min) (or p = o(n\u03982min) with extra assumptions on singular values of the population matrix E[X]) (L\u00f6ffler et al., 2021). This result implies that, in the low-dimensional setting, spectral clustering asymptotically attains the sharp threshold (5) as lim infn\u2192\u221e \u03982min 8 logn > 1. However, it remains unclear how spectral cluster performs in theory under the high-dimensional regime when n\u03982min = O(p) because running K-means on the eigen-embedded data would also depend on the structure of the population singular values (Han et al., 2023). Thus, in practice, spectral clustering is less appealing for high dimensional data, in view of more robust alternatives such as the SDP relaxation in (4).\nDespite the K-means clustering problem solved via SDP enjoys statistical optimality, it optimizes over an n \u00d7 n dense psd matrix to make it completely inaccessible to practical datasets with even moderate size of a few thousands data points. By contrast, the NMF approach using a typical workstation hardware can handle much larger scale datasets as in the documents and images clustering applications (Cichocki and Phan, 2009; Kim et al., 2014; Xu et al., 2003). Nonetheless, little is known in literature about the provable statistical guarantee for a general NMF approach. For the clustering problem, our goal is to break the computational and statistical bottleneck by simultaneously leverage the implicit psd structure of the membership matrix and nonnegativity constraint. In Section 3, we propose a novel NMF-like algorithm that achieves the statistical optimality by solving a nonnegative low-rank restricted SDP formulation.\n3 OUR PROPOSAL: K-MEANS VIA NONNEGATIVE LOW-RANK SDP\nThe Burer\u2013Monteiro (BM) is a nonconvex approach for solving a SDP problem when its solution is expected to be low rank (Burer and Monteiro, 2003). Standard SDP problem with only equality constraints on the psd matrix variable Z \u2208 Sn+ can be factorized as Z = UUT where U is an n\u00d7 r rectangular matrix with r \u226a n. In our SDP relaxed K-means formulation (4), the true membership matrix Z\u2217 is a block diagonal matrix containing K blocks, where each block has size nk \u00d7 nk with all entries equal to n\u22121k and nk = |G\u2217k| is the size of the true cluster G\u2217k. Thus we can exploit this low-rank structure and recast the SDP in (4) as a nonconvex optimization problem over U \u2208 Rn\u00d7r:\nmin U\u2208Rn\u00d7r\n{ \u27e8A,UUT \u27e9 : \u2225U\u22252F = K, UUT1n = 1n, U \u2265 0 } , (6)\nwhere we replaced the constraint UUT \u2265 0 with the stronger constraint U \u2265 0 that is easier to enforce (Kulis et al., 2007) as in the NMF setting. Note that the BM reformulation (6) can be viewed as a restriction of the rank-constrained SDP formulation since U \u2265 0 implies Z = UUT \u2265 0. Even though the BM method turns a convex SDP to a nonconvex problem, it leverages the low-rank structure to achieve substantial computational savings. In addition, the solution U\u0302 has a natural statistical interpretation: rows of U\u0302 can be interpreted as latent positions of n data points when projected into Rr, and thus can be used to conduct other downstream data analyses in reduced dimensions. Under this latent embedding perspective, formulation (6) can be viewed as a constrained PCA respecting the clustering structure through constraint \u2225U\u22252F = K. The BM formulation (6) also has a close connection to a class of clustering algorithms based on NMF, which instead solve the following optimization problem\nmin U\u2208Rn\u00d7r\n{ \u2225A+ UUT \u22252F : U \u2265 0 } . (7)\nThe NMF formulation (7) is advocated in a number of papers (Ding et al., 2005; Kuang et al., 2015; Zhu et al., 2018) for data clustering in the NMF literature due to its remarkable compuational scalability, and is equivalent to a simpler version of (6) by dropping the two equality constraints. Our empirical results in Section 5 and Figure 1 show that keeping these two equality constraints are beneficial: the resulting method significantly improves the classification accuracy while maintaining comparable computational scalability. More importantly, we have the theoretical certification that the resulting method from (6) achieves the information-theoretic limit for exact recovery of the true cluster labels (see Section 4).\nWe will now derive a simple primal-dual gradient descent-ascent algorithm to solve (6). To begin, note that we can first turn the nonsmooth inequality constraint U \u2265 0 together with the trace constraint to\nAlgorithm 1 Primal-dual algorithm for solving BM formulation (6) of K-means clustering Input. Dissimilarity matrix A = \u2212XXT , clustering parameter K \u2265 1, rank parameter r \u2265 K, augmentation parameter \u03b2 > 0, step size \u03b1 > 0. Output. A second-order locally optimal point U . Algorithm. Initialize y = 0. Do the following:\n1. (Primal descent; Projected GD steps) Recursively run the following until convergence: U0 = U ; for t \u2208 N,\nU t+1 = \u03a0\u2126 ( U t\u2212\u03b1\u2207UL\u03b2(U t, y) ) with \u03a0\u2126(V ) = \u221a K\u00b7(V )+/\u2225(V )+\u2225F , \u2200V \u2208 Rn\u00d7r,\nwhere\u2207UL\u03b2(U t, y) = (2A+ 2L \u00b7 Id + 1nyT + y1Tn )U t and y = y+ \u03b2(U tU t T 1n \u2212 1n).\nUpon convergence at iteration count t0, set Unew = U t0 . 2. (Dual ascent; Augmented Lagrangian step) Update dual variable via\nynew = y + \u03b2(UnewU T new1n \u2212 1n).\n3. (Stopping criterion) If max{\u2225Unew \u2212 U\u2225F , \u2225UnewUTnew1n \u2212 1n\u2225F } falls below some tolerance threshold, then return Unew. Otherwise, set U \u2190 Unew and y \u2190 ynew, and repeat Steps 1 and 2.\na manifold-like subset \u2126 := {U \u2208 Rn\u00d7r : \u2225U\u22252F = K, U \u2265 0}. (8) The projection operator to \u2126 can be easily computed as\n\u03a0\u2126(V ) := arg min U\u2208\u2126 \u2225U \u2212 V \u2225F = \u221a K \u00b7 (V )+ \u2225(V )+\u2225F , (9)\nwhere (V )+ = max{V, 0} denotes elementwise positive part of V . Then the BM can be converted to the equality-constrained problem over \u2126:\nmin U\u2208\u2126\n{ \u27e8A,UUT \u27e9 : UUT1n = 1n } . (10)\nUsing the standard augmented Lagrangian method (Nocedal and Wright, 2006, Chapter 17), we see that the above (6) is also equivalent to\nmin U\u2208\u2126\n{ \u27e8L \u00b7 Idn +A,UUT \u27e9+ \u03b2\n2 \u2225UUT1n \u2212 1n\u222522 : UUT1n = 1n\n} , (11)\nwhere \u03b2 > 0 is a penalty parameter and L \u2208 (0, \u03bb) for some proper choice of \u03bb motivated from the optimal dual variable for the trace constraint \u2225U\u22252F = tr(Z) = K. The augmented Lagrangian for problem (11) is\nL\u03b2(U, y) := \u27e8L \u00b7 Idn +A,UUT \u27e9+ \u27e8y, UUT1n \u2212 1n\u27e9+ \u03b2\n2 \u2225UUT1n \u2212 1n\u222522. (12)\nTherefore, we consider the augmented Lagrangian iterations Unew = arg min\nU\u2208\u2126 L\u03b2(U, y), ynew = y + \u03b2(UnewUTnew1n \u2212 1n). (13)\nNow, we consider minimizing L\u03b2(U, y) over U , with a fixed \u03b2 and y. Here, we observe that L\u03b2(U, y) is simply a quadratic polynomial over U , and therefore its gradient can be easily derived as \u2207UL\u03b2(U, y) = (2A+ 2L \u00b7 Id + 1nyT + y1Tn )U where y = y + \u03b2(UUT1n \u2212 1n). Combining this with the insight that it is easy to project onto \u2126, we can perform the following projected gradient descent iterations\nUnew = \u03a0\u2126(U \u2212 \u03b1\u2207UL\u03b2(U, y)) until L\u03b2(U, y) is minimized. The complete algorithm is summarized in Algorithm 1, whose per iteration time and space complexity are both O(nr). We point out that the NMF formulation (7) is a simpler version of our BM formulation (6), a projected gradient descent algorithm for the solving the former corresponds to the primal decent step of Algorithm 1 with y = 0, \u03b2 = 0 and a simpler projection operator \u03a0Rn\u00d7r+ (V ) = (V )+. Upon obtaining the optimal solution U , a rounding procedure will be applied. More details about the rounding procedure and the choice of tuning parameters are mentioned in Appendix A."
        },
        {
            "heading": "4 THEORETICAL ANALYSIS",
            "text": "In this section, we establish the local linear convergence rate of the BM algorithm (Algorithm 1) for solving the K-means clustering. We shall work with the standard Gaussian mixture model (GMM) that assumes the data X1, . . . , Xn are generated from the following mechanism: if i \u2208 G\u2217k, then\nXi = \u00b5k + \u03b5i, (14)\nwhere G\u22171, . . . , G \u2217 K is a true (unknown) partition of [n] we wish to recover, \u00b51, . . . , \u00b5K \u2208 Rp are the cluster centers and \u03b5i \u223c N(0, Ip) are i.i.d. standard Gaussian noises. Let nk = |G\u2217k| denote the size of G\u2217k and by convention n0 = 0. Since in the exactly recovery regime (Assumption A below), there is no relaxation gap \u2014 any global optimum U\u2217 of the BM problem (4) at r = K corresponds to the unique global optimum Z\u2217 of the SDP problem (6) through the relation Z\u2217 = U\u2217U\u2217T , it is sufficient to focus our analysis on the r = K case. We note that the r = K case corresponds to an exact parameterization of the matrix rank, which is standard in the analysis of algorithms based on the BM factorization (Ge et al., 2017; Chi et al., 2019).\nAlgorithm 1 is formulated as an exact augmented Lagrangian method, in which the primal subproblem minU\u2208\u2126 L\u03b2(U, y) in Step 1 is solved to sufficiently high accuracy (i.e. machine precision) as to be viewed as an exact solution for Step 2.1 This contrasts with inexact methods, in which the primal subproblem over U is only solved to coarse accuracy (i.e. 1-2 digits), usually owing to slow convergence in Step 1. We will soon prove in this section that projected gradient descent enjoys rapid linear convergence within a local neighborhood of the primal-dual solution, and it is therefore very reasonable to run Step 1 until it reaches the numerical floor.\nUnder exact minimization in Step 1, it is a standard result that the dual multipliers converge at a linear rate in Step 2 under second-order sufficiency conditions, and that this in turn implies linear convergence of the primal minimizers in Step 1 towards the true solution. The wording for the following are taken directly from Proposition 1 and 2 of Bertsekas (1976), though a more modern proof without constraint qualification assumptions is given in Fern\u00e1ndez and Solodov (2012). (See also Bertsekas (2014, Proposition 2.7)) More details and explanations can be found in Appendix A. Proposition 1 (Existence and quality of primal minimizer). Let U\u2217 denote a local minimizing point that satisfies the second-order sufficient conditions for an isolated local minimum with respect to multipliers y\u2217. Then, there exists a scalar \u03b2\u2217 \u2265 0 such that, for every \u03b2 > \u03b2\u2217 the augmented Lagrangian L\u03b2(U, y) has a unique minimizing point U(y, \u03b2) within an open ball centered at U\u2217. Furthermore, there exists some scalar M > 0 such that \u2225U(y, \u03b2)\u2212 U\u2217\u2225F \u2264 (M/\u03b2)\u2225y \u2212 y\u2217\u2225. Proposition 2 (Linear convergence of dual multipliers). Under the same assumptions as Proposition 1, define the sequence\nyk+1 = yk + \u03b2(UkU T k 1n \u2212 1n) where Uk = U(yk, \u03b2) \u2261 argmin U L\u03b2(U, yk).\nThen, there exists a radius R > 0 such that, if \u2225y0 \u2212 y\u2217\u2225 \u2264 R, then yk \u2192 y\u2217 converges linearly\nlim sup k\u2192\u221e \u2225yk+1 \u2212 y\u2217\u2225 \u2225yk \u2212 y\u2217\u2225 \u2264 M \u03b2 .\nPropositions 1 and 2 are directly applicable to Algorithm 1 when r = K, because the global minimum U\u2217 is made isolated by the nonnegativity constraint U \u2265 0. In fact, it is possible to generalize both results to all r \u2265 K, even when the global minimum U\u2217 is no longer isolated, although this would require further specializing Propositions 1 and 2 to our specific problem. The key insight is that, in the r > K case, the closest global minimum U\u2217 to the current iterate U continues to satisfy all the properties satisfied by the isolated global minimum when r = K. This is essentially the same idea as (Ge et al., 2017, Definition 6) and (Chi et al., 2019, Lemma 4). Nevertheless, we leave the extension to the r > K case as future work, and focus on the r = K case in this paper.\nIn view of Propositions 1 and 2, all of the difficulty that remains is to show that projected GD is able to solve the primal subproblem minU\u2208\u2126 L\u03b2(U, y) efficiently. Below is our main theorem for establishing the local linear rate of convergence of the proposed BM solution for K-means clustering under the GMM model. In fact, this local linear convergence result holds for all r \u2265 K.\n1Note that with finite precision arithmetic, any \u201cexact solution\u201d is exact only up to numerical precision. Our use of \u201cexact\u201d in this context is consistent with the classical literature, e.g., Bertsekas (1976).\nAssumption A (Exact recovery regime). For notation simplicity, we consider the equal cluster size case, where the minimal separation \u0398min := min1\u2264j \u0338=k\u2264K \u2225\u00b5j \u2212 \u00b5k\u22252 satisfies \u0398min \u2265 (1 + \u03b1\u0303)\u0398 for some \u03b1\u0303 > 0, where \u0398 is the information-theoretic optimal threshold given in (5).\nIn Appendix D, we provide the theorem (Theorem 9 in Appendix D.4) for the general case where the cluster sizes are unbalanced (Assumption 1 in Appendix D.3). Analogous to the minimal separation, we define the maximal separation \u0398max := max1\u2264j \u0338=k\u2264K \u2225\u00b5j \u2212 \u00b5k\u22252. For any block partition sizes m = (m1,m2, . . . ,mK) satisfying mk \u2265 1, \u2211 k mk = r, we let Gm denote the set of all m-block diagonal matrices with nonnegative entries (see Appendix D.1 for a precise definition). For any within block weights ak = (ak,1, ak,2, . . . , ak,mk), we also let U\na,\u2217 denote the associated optimal solution, defined as (17) in Appendix D.1. Let a = mink\u2208[K] min\u2113\u2208[mk]{ak,\u2113}, a\u0304 = maxk\u2208[K] max\u2113\u2208[mk]{ak,\u2113}, and Or be the orthogonal transformation group on Rr. Theorem 1 (Local convergence of projected gradient descent). Suppose y\u2217 is the optimum dual for the SDP problem (see Assumption 2 in Appendix D.3) and Assumption A holds. Assume p = O( \u221a n log n), \u0398max \u2264 C\u0398min, for some C > 0, and there exists some block partition sizes m and an associated optimal solution Ua,\u2217 satisfying a\u0304 \u2264 ca, c > 0. We denote the initialization of the algorithm as U0, and let U\u0303 denote the unique (whose existence is also guaranteed) stationary point of the projected gradient descent updating formula under dual value y\u0303 close to y\u2217, i.e. U\u0303 satisfies\nU\u0303 = \u03a0\u2126 ( U\u0303 \u2212 \u03b1\u2207UL\u03b2(U\u0303 , y\u0303) ) .\nIf \u2225y\u0303 \u2212 y\u2217\u2225 \u2264 \u03b4 for some \u03b4 > 0, and the initialization discrepancy \u22060 = U0 \u2212 Ua,\u2217 satisfies\n\u2225\u22060S(a)c\u2225\u221e \u2264 O(K/ \u221a nr) and \u2225\u22060\u2225F \u2264 O ( r\u22120.5K\u22125.5 min{1, K\u22122.5\u03982min/ log n)} ) ,\nthen it holds with probability at least 1\u2212 c1n\u2212c2 that, for any t \u2265 I = O(K3),\nU t \u2208 Gm and inf Q\u2208Or;U\u0303Q\u2208Gm \u2225U t+1 \u2212 U\u0303Q\u2225F \u2264 \u03b3 inf Q\u2208Or;U\u0303Q\u2208Gm \u2225U t \u2212 U\u0303Q\u2225F ,\nwhere \u03b3 = 1\u2212O(K\u22126), provided that \u03b2 = O(\u03982min/K3), L = O(n\u03982min/K) in (12) and step size \u03b1 is chosen such that \u03b1\u22121 = O(K2n\u03982min). c1 and c2 are some constants.\nProof sketches. The proof strategy is to divide the convergence of the projected gradient descent for solving the primal subproblem into two phases. In phase one, we show that after at most I iterations, the iterates will become block diagonal, that is, falls into set Gm provided that the initialization is close to certain optimum point with some block diagonal form in Gm. We then show that once the iterate becomes m-block diagonal, the algorithm enters phase two, where the iterate remains m-block diagonal. Moreover, the projected gradient descent attains a linear convergence rate, since the objective function L( \u00b7 , y\u0303) is restricted strongly convex at U\u0303 within Gm. More precisely, there exists some \u03b2\u0303 > 0, such that for any U \u2208 Gm, we have\n\u27e8\u22072UL(U\u0303 , y\u0303)[\u2206],\u2206\u27e9 \u2265 \u03b2\u0303 \u2225\u2206\u22252F , for \u2206 = UQTU \u2212 U\u0303 ,\nwhere QU = argminQ\u2208Or;U\u0303Q\u2208Gm \u2225U \u2212 U\u0303Q\u2225F . More details can be found in Appendix D.\nImplications of the theorem: According to Theorem 1, if we choose L = O(n\u03982min/K), \u03b2 = O(\u03982min/K 3) and \u03b1\u22121 = O(K2n\u03982min), then phase one will last at most I = O(K 3) iterations; and after entering phase two, the contraction rate of the projected gradient descent is \u03b3 = 1\u2212O(K\u22126). These choices make the iteration complexity of the projected GD for solving the primal subproblem of order O(K6) (modulo logarithmic factors), and the overall time complexity of the primal-dual algorithm becomes of order O(K6nr). In addition, given that the stationary point U\u0303 satisfies \u2225U\u0303\u2225\u221e = O( \u221a K/n) and \u2225U\u0303\u2225F = O( \u221a K), the initialization condition only demands a constant relative error with respect to n, making it a reasonable requirement. For example, under mild conditions, rapid K-means algorithms like Lloyd\u2019s algorithm (Lloyd, 1982; Lu and Zhou, 2016) can be employed to construct an initialization that meets this condition with high probability. \u25a0"
        },
        {
            "heading": "5 NUMERICAL EXPERIMENTS",
            "text": "We present numerical results to assess the effectiveness of the proposed BM method. We first conduct two simulation experiments using GMM to evaluate the convergence and compare the performance\nof BM with other methods. Then, we perform a comparison using two real datasets. One of the competing methods in our comparison is clustering based on solving the NMF formulation (7). Specifically, we employ the projected gradient descent algorithm, which is a simplified version of Algorithm 1 discussed in Section 3, to implement this method. We adopt random initialization and set the same r for both NMF and BM to ensure a fair comparison. Finally, we conduct further experiments on three datasets in UCI.\nPerformance for BM for GMM. Our goal is to compare the statistical performance and time complexity of BM, NMF, the SDPNAL+ solver (Yang et al., 2015) for solving SDP, spectral clustering (SC), and K-means++ (KM) under Gaussian mixture models (GMM). In our setting, we choose cluster numbers K = 4 and place the centers of Gaussian distributions at the vertices of a simplex such that \u03982min = \u03b3\u0398 2 with \u03b3 = 0.64, where \u0398 is the sharp threshold defined in Equation (5). The sample size ranges from n = 400 to n = 57, 600, the dimension is p = 20, and we set the rank of the BM relaxation to be r = 2K. The results are summarized in the first two plots of Figure 2, with each case repeated 50 times. From the first plot, we observe that the mis-clustering error of SDP and BM coincides; the error remains stable and decreases as the sample size n increases. However, NMF, KM, and SC exhibit large variance and are far from achieving exact recovery. The second plot indicates that SDP and SC have super-linear time complexity, while the log-scale curves of our BM approach, KM, and NMF are nearly parallel, indicating that they all achieve linear time complexity. In particular, SDP becomes time-consuming when n is larger than 2000; when n = 40, 000, the space complexity issue makes SC infeasible. Further comparisons of statistical performance and time complexity with increasing dimension p or varying number of clusters K can be found in Appendix B.\nLinear convergence of BM. To analyze the convergence of Algorithm 1, we maintain the same setting as described earlier. However, we now fix n = 1000 and consider minimum distance between centers \u03982min = \u03b3\u0398 2 with \u03b3 = 1.44. We explore three cases: r = K, r = 2K, and r = 20K, and set the number of iterations for Step 1 (primal update) to be 100. The convergence of our BM approach is shown in the third plot of Figure 2, with each result based on 30 replicates. We measure the relative difference of U compared to optimal solution U\u2217 using the Frobenius\nnorm \u2225UUT \u2212 U\u2217(U\u2217)T \u2225F /\u2225U\u2217(U\u2217)T \u2225F . From the third plot, we observe that our BM approach achieves linear convergence in this setting. The curves for r = K, r = 2K, and r = 20K overlap closely, indicating that the choice of r does not significantly affect the convergence rate. Additional experiment for the comparison of the convergence rate between our algorithm (based on projected GD) and the conventional algorithm (which lifts all constraints into the Lagrangian function) can be found in Appendix B.\nCyTOF dataset. This mass cytometry (CyTOF) dataset consists of protein expression levels for N = 265, 627 cells with p = 32 protein markers. The dataset contains 14 clusters or gated cell populations. In our experiment, we select the labeled data for individual H1. From these labeled data, we uniformly sample n data points from K = 4 unbalanced clusters, specifically clusters 2, 7, 8, and 9, which together contain a total of 46, 258 samples. The results are presented in Figure 3, where we conduct all methods for 100 replicates for each value of n. We consider three cases: case 1 corresponds to n = 1800, case 2 corresponds to n = 20, 000, and case 3 corresponds to n = 46, 258. From the plot, we observe that BM and SDP remain stable, while KM, SC exhibit significant variance, and NMF produces many outliers. In case 1, the mis-clustering error of BM is comparable to that of SDP, indicating that the performance of BM can be as good as SDP. However, when n is on the order of 1000, the time complexity of SDP becomes dramatically high, which is at least O(n3.5). Additionally, when n = 46, 258, the space complexity of at least O(n2) becomes an issue for SC.\nCIFAR-10 dataset. We conduct a comparison of all methods on the CIFAR-10 dataset, which comprises 60,000 colored images of size 32\u00d7 32\u00d7 3, divided into 10 clusters. Firstly, we apply the Inception v3 model with default settings and perform PCA to reduce the dimensionality to p = 50. The test set consists of 10,000 samples with 10 clusters. In our experiment, we focus on four clusters (\"bird\", \"deer\", \"ship\", and \"truck\") with K = 4. We consider two cases: Case 1 involves random sub-sampling with n = 1800 to compare BM with SDP, and Case 2 uses n = 4000. The results are displayed in the second plot of Figure 3, based on 100 replicates. Similar to the previous experiment, we observe that BM and SDP exhibit similar behavior and achieve superior and more consistent performance compared to KM, SC, and NMF. SC and NMF display significant variance, while KM produces many outliers.\nUCI datasets. To empirically illustrate the advantages and robustness of our method against the GMM assumption, we conduct further experiments on three datasets in UCI: Msplice, Heart and DNA. The results of misclustering errors (with standard deviation) are summarized in Table 1, where we randomly sample n = 1000 (n = 300 for Heart dataset) many data points for total 10 replicates. From the table we can observe the superior behavior of BM (our algorithm) and SDP over the rest competitors. DNA1 (DNA2) stands for the results of DNA dataset after perturbation with t-distribution (skewed normal distribution) random noise. In both experiments, each data point xi is perturbed with xi +0.2\u03f5i. In the first experiment (DNA1), the injected noise \u03f5i is i.i.d. random vector with i.i.d. entries following t-distribution with 5 degree of freedom, which has much heavier tail than the normal. In the other experiment (DNA2), the injected noise follows a skewed normal distribution with variance 1 and skewness 0.2. From the table we can observe that the performance of all methods for DNA1 and DNA2 got impacted comparing to DNA; but BM performs better around all cases, which is comparable with the original SDP as expected. Moreover, from the QQ-plots of Figure 5 in Appendix B, the GMMs assumption is also clearly violated for the Heart dataset; however, we can still observe the superior behavior of BM (our algorithm) and SDP over the rest competitors."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We thank the reviewers for their valuable comments. X.C. acknowledges support from NSF CAREER Award DMS-2347760. Y.Y was supported by NSF DMS-2210717. R.Y.Z was supported by NSF CAREER Award ECCS-2047462 and the C3.ai Digital Transformation Institute."
        },
        {
            "heading": "SUPPLEMENTARY MATERIAL",
            "text": "This supplementary material contains additional details of the algorithm and propositions, more experimental results, a discussion and the proof of the main theoretical results (Theorem 1) in the paper."
        },
        {
            "heading": "A ADDITIONAL DETAILS OF THE ALGORITHM AND PROPOSITIONS",
            "text": "Choices of tuning parameters and rounding procedure for Algorithm 1. In practice, we start with small step size \u03b1 = 10\u22126 and increase \u03b1 until \u03b1 = 10\u22123. Similarly, we start with small augmentation term \u03b2 = 1 and increase \u03b2 until \u03b2 = 103. The second experiment shows that the choice of r does not significantly affect the convergence rate. In practice, we found that r = K will result in many local minima for small separation. Therefore, we slightly increase r and choose r = 2K for all applications, which turns out to provide desirable statistical performance that is comparable or slightly better than SDP.\nAfter getting the second-order locally optimal point U in Algorithm 1 (BM), a rounding procedure is applied, where we extract first K eigen vectors of U as columns of new matrix V and then use K-means to cluster the rows of V to get the final assignments. The same rounding procedure is applied to the results from both SDP and NMF.\nAdditional explanations to the propositions. Proposition 1 indicates that if U\u2217 is the local minimum of problem (6) that satisfies constraint qualification assumptions (Assumption (S) in Bertsekas (1976)), then we can get unique minimizing point around U\u2217 of augmented Lagrangian L\u03b2(U, y) for any y provided that we have large augmented coefficient \u03b2. A more modern proof without constraint qualification assumptions is given in Fern\u00e1ndez and Solodov (2012). Proposition 2 indicates that the dual variable yk in Algorithm 1 will converge to y\u2217 at exponential rate locally provided that Uk solves minU L\u03b2(U, yk). Therefore, Algorithm 1 can achieve local exponential rate provided that we can solve the local min of L\u03b2(U, y) for y \u2248 y\u2217 with exponential rate, which is proved by Theorem 1."
        },
        {
            "heading": "B ADDITIONAL EXPERIMENTAL RESULTS",
            "text": "Comparison of different ways to solve BM. We conduct an experiment under the same settings as described in Section 5\u2019s \u201cLinear convergence of BM\" except that here we consider a smaller scale with n = 300 and p = 4. We aimed to compare the convergence rate towards the optimum per dual update between our algorithm (based on projected GD) and the conventional algorithm (which lifts all constraints into the Lagrangian function). As seen in Figure 4, our algorithm successfully converges to the SDP solution, whereas the conventional algorithm\u2019s convergence halts at a specific step. Furthermore, the average time required to compute the minimum of the augmented Lagrangian (as denoted by (12) in the main paper) is 0.002 seconds for our algorithm. In contrast, the conventional algorithm takes an average of 8 seconds, even when employing the state-of-the-art limited memory BFGS method.\nComparision of computational complexity w.r.t. K. Here we want to compare the time complexity of BM, NMF, the SDPNAL+ solver (Yang et al., 2015) for solving SDP, spectral clustering (SC), and K-means++ (KM) under Gaussian mixture models (GMM) when the number of cluster K is moderate in practice. Under the same setting as our second experiment (\"Performance for BM for GMM\") in Section 5, except that now we consider fixed sample size n = 1000, dimension p = 50, and different number of clusters K = 5, 10, 20, 40, 50. SC is not considered as it would fail for large K in our experiments. The results are summarized in the first plot of Figure 6, where we can observe that the log-scale curve of BM is nearly parallel to the log-scale curve of KM and NMF, indicating that the growth of CPU time cost for BM with respect to K is reasonable (nearly O(K)) and would not achieve the loose upper bound O(K6) derived from the analysis. The curve of computational time cost for SDP is relative stable for different K since the dominant term of computational complexity for SDP is the sample size n, which is of as large as order O(n3.5).\nPerformance of BM under different p. Our goal is to compare the statistical performance and time complexity of BM, NMF, the SDPNAL+ solver (Yang et al., 2015) for solving SDP, and Kmeans++ (KM) with increasing dimension p. Here we consider the same setting as our second\nexperiment \"Performance for BM for GMM\" in Section 5 except that now we consider fixed sample size n = 2500. The dimension ranges from p = 125 to p = 1000. SC is not considered as it would fail for high dimensional cases in our experiments. The results are summarized in Table 2 and the second plot of Figure 2, with each case repeated 10 times. From Table 2 we observe that the mis-clustering errors for both SDP and BM coincide and stay optimal when the dimension p increases, while K-means++ has large variance and NMF would fail when the dimension is as large as p = 1000. The second plot in Figure 2 shows that the log-scale curve for BM is nearly parallel to the log-scale curves for both KM and NMF. This indicates the same order of CUP time cost with respect to the dimension p for BM, KM and NMF, which are nearly of order O(p). Similar to the case when K changes, the curve of computational time cost for SDP is relative stable for different p since the dominant term of computational complexity for SDP is the sample size n."
        },
        {
            "heading": "C DISCUSSION",
            "text": "One limitation of our derivation in Theorem 1 is the separation assumption. Theorem 1 is based on the fact that the optimum solutions of SDP and BM coincide, where the separation assumption serves as the sufficient condition. In practice, we can observe linear convergence of BM for small separation as well. Therefore, we anticipate similar derivation of convergence analysis for weak separation assumption, which will be our future research. On the other hand, in practice, if we apply our algorithm to the datasets where the separations (signal-to-noise ratio) are very small, our algorithm would fail to provide informative clustering results, similar issues occur for both SDP and NMF. We expect that this problem can be solved if different rounding procedures are applied. In other words, for the small separation cases where the solutions to BM, SDP and NMF are no longer represent exact membership assignments, it would be important to consider and compare different rounding processes to extract informative membership assignments."
        },
        {
            "heading": "D PROOF OF THEOREM 1",
            "text": "In this section, we prove the theorem for the general case with explicit constants, which extends Theorem 1 from the equal cluster size case to the unequal cluster size case (Theorem 9). The section is organized as follows. First, we will introduce the notations that will be used in the proof (Appendix D.1). Then, we characterize the optimum feasible solutions of the BM algorithm (Theorem 2 in Appendix D.2). Next, we present the convergence results at the optimum dual y = y\u2217 (Appendix D.3). After that, we extend the proof of convergence at the optimum dual to y \u2248 y\u2217 (Appendix D.4). The proofs of all technical lemmas are placed in the end of the section (Appendix D.5)."
        },
        {
            "heading": "D.1 NOTATIONS",
            "text": "We shall work with the standard Gaussian mixture model (GMM) that assumes the data X1, . . . , Xn are generated from the following mechanism: if i \u2208 Gk, then\nXi = \u00b5k + \u03b5i, (15) where G1, . . . , GK is a true (unknown) partition of [n] we wish to recover, \u00b51, . . . , \u00b5K \u2208 Rp are the cluster centers and \u03b5i \u223c N(0, Ip) are i.i.d. standard Gaussian noises. Let nk = |Gk| denote the size of Gk. Let Z\u2217 = U\u2217(U\u2217)T , where\nU\u2217 =  1\u221a n1 1n1 0 \u00b7 \u00b7 \u00b7 0 0 \u00b7 \u00b7 \u00b7 0 0 1\u221an21n2 \u00b7 \u00b7 \u00b7 0 0 \u00b7 \u00b7 \u00b7 0 ... ... . . . ... 0 ...\n... 0 0 \u00b7 \u00b7 \u00b7 1\u221anK 1nK 0 \u00b7 \u00b7 \u00b7 0  \u2208 Rn\u00d7r. (16) Define the minimum of cluster size n := mink\u2208[K]{nk}, the maximum of cluster size n\u0304 := maxk\u2208[K]{nk} and m = mink \u0338=l 2nknlnk+nl . Define the minimal separation \u0398min := min1\u2264j \u0338=k\u2264K \u2225\u00b5j \u2212 \u00b5k\u22252, and the maximal separation \u0398max := max1\u2264j \u0338=k\u2264K \u2225\u00b5j \u2212 \u00b5k\u22252. For any block partition sizes m = (m1,m2, . . . ,mK) satisfying mk \u2265 1, \u2211 k mk = r, we define the set Fm as consisting of all (orthogonal transformation) matrices Q such that U = U\u2217Q has the form a1,11n1 , . . . , a1,m11n1 0 \u00b7 \u00b7 \u00b7 0 0 a2,11n2 , . . . , a2,m21n2 \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 aK,11nK , . . . , aK,mK1nK  , (17) for some \u2211 l a 2 k,l = 1 nk ,\u2200k \u2208 [K], \u2211\nk mk = r. We will call any matrix U whose sparsity/nonzero pattern coincides with (17) (up to column permutations) as an m-block diagonal matrix. Let Gm denote the set of all m-block diagonal matrices with nonnegative entries. Note that Q exists for any U of the form since UUT = (U\u2217)(U\u2217)T . We denote the set of all such Q as Fm. Now we denote ak = (ak,1, ak,2, . . . , ak,mk) and U\na,\u2217 as (17). Define a := mink,l{ak,l}, a\u0304 := maxk,l{ak,l}. For any matrix A \u2208 Rn\u00d7r, we use AS(a) to denote the matrix of the same size as the restriction of A onto the support S(a) of Ua,\u2217, that is, [AS(a)]ij = Aij if [Ua,\u2217]ij \u0338= 0 and [AS(a)]ij = 0 otherwise; and let AS(a)c = A\u2212AS(a). Let Or be the orthogonal transformation group on Rr. Let \u03a0\u2126 be the projection map on to the set \u2126. i.e.,\nv = \u03a0\u2126(u)\u21d0\u21d2 v \u2208 \u2126, \u2225v \u2212 u\u2225 \u2264 \u2225u\u0303\u2212 u\u2225,\u2200u\u0303 \u2208 \u2126. Let C(\u2126) to be the convex hull of \u2126. Recall that \u2126 := {U \u2208 Rn\u00d7r : \u2225U\u22252F = K,U \u2265 0}. Then C(\u2126) = {U \u2208 Rn\u00d7r : \u2225U\u22252F \u2264 K,U \u2265 0}. We define V = \u03a0\u2126(W ), W = U \u2212 \u03b1\u2207f(U), \u03b1 > 0. In particular, we define \u03a0+(U) to be the positive part of U. Let U0 be the initialization. For t \u2208 N+, we define U t recursively by\nU t+1 = \u03a0\u2126 ( U t \u2212 \u03b1\u2207UL\u03b2(U t, y) ) .\nDenote Q,Qt \u2208 Fm as Q = argminQ\u0303\u2208Fm\u2225U \u2212 U \u2217Q\u0303\u2225F , Qt = argminQ\u0303\u2208Fm\u2225U t \u2212 U\u2217Q\u0303\u2225F , \u2200t \u2208 N."
        },
        {
            "heading": "D.2 CHARACTERIZATION OF GLOBAL OPTIMA OF BM PROBLEM",
            "text": "Theorem 2 (Feasible solutions). For any U \u2208 Rn\u00d7r such that UUT = Z\u2217 and U \u2265 0, U must take the following block form (up to column permutations), due to the nonnegative constraint U \u2265 0:\nU =  a1,11n1 , . . . , a1,m11n1 0 \u00b7 \u00b7 \u00b7 0 0 a2,11n2 , . . . , a2,m21n2 \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 aK,11nK , . . . , aK,mK1nK  , (18) for some block partition sizes m = (m1,m2, . . . ,mK) satisfying mk \u2265 1, \u2211 k mk = r and within\nblock weights ak = (ak,1, ak,2, . . . , ak,mk) satisfying ak,\u2113 \u2265 0, \u2211 \u2113 a 2 k,\u2113 = 1 nk for all k \u2208 [K].\nProof. Recall\nU\u2217 =  1\u221a n1 1n1 0 \u00b7 \u00b7 \u00b7 0 0 \u00b7 \u00b7 \u00b7 0 0 1\u221an21n2 \u00b7 \u00b7 \u00b7 0 0 \u00b7 \u00b7 \u00b7 0 ... ... . . . ... 0 ...\n... 0 0 \u00b7 \u00b7 \u00b7 1\u221anK 1nK 0 \u00b7 \u00b7 \u00b7 0  . Then for any Q = [q1, . . . , qr] \u2208 Or, the orthogonal transformation group on Rr, we have\nU := U\u2217QT = [ 1 \u221a n1 q11 T n1 , . . . , 1 \u221a nK qK1 T nK ]T .\nU is feasible hence U\u2217QT \u2265 0 =\u21d2 qi \u2265 0,\u2200i. Note that \u27e8qi, qi\u27e9 = 1, \u27e8qi, qj\u27e9 = 0,\u2200i \u0338= j, then U must have the form (18) (up to column permutations). \u25a0\nIn the rest of the proof, we first show the convergence of projected gradient descent at y\u2217 as Theorem 3 below, and then extend the theorem to a general dual variable y\u0303 in a neighborhood of y\u2217 in Appendix D.4.\nD.3 PROOF OF LINEAR CONVERGENCE WITH y = y\u2217\nWe list below the assumption on the minimal separation for the general unequal cluster size case. This is a generalized version of Assumption A, where cluster sizes are equal:\nAssumption 1 (Assumptions for Theorem II.1 in (Chen and Yang, 2021)). If there exist constants C1 \u2208 (0, 1), C3 > 0 such that\nlog n \u2265 (1\u2212 C1) 2\nC21\nD1n\nm , C3 \u2264 C21 (1\u2212 C1)2 D2 K , m \u2265 4(1 + C3) 2 C23 ,\n\u03982min \u2265 \u0398 2 a := 4\u03c32(1 + 2C3)\n(1\u2212 C1)2 1 +\u221a1 + (1\u2212 C1)2 1 + C3\np\nm log n +D3Rn  log n, with\nRn = (1\u2212 C1)2\n(1 + C3) log n\n(\u221a p log n\nn +\nlog n\nn\n) ,\nwhere n := mink\u2208[K]{nk} and m = mink \u0338=l 2nknlnk+nl . Here D1, D2, D3 are universal constants.\nThis assumption requires the minimum squared separation \u03982min to be of order O(log n). Recall that we use n\u0304 := maxk\u2208[K]{nk}, and \u03982max to denote the maximum squared separation. Assumption 1 will reduce to Assumption A in the main paper when the cluster size are equal. This is due to the fact that for any \u03b1\u0303 > 0, we can find C1 and C3 s.t.\n(1+2C3) (1\u2212C1)2 < (1 + \u03b1\u0303). Moreover,\nm = n/K in this case.\nAssumption 2 (The optimum dual y\u2217). Choose (\u03bb, y\u2217, B) to be the dual variables of the SDP defined by equations (20), (21), (22) and (25) in (Chen and Yang, 2021). In particular, \u03bb = p+ C14 m\u0398 2 min; B \u2265 0, BGk,Gk = 0, \u2200k \u2208 [K], and y\u2217 equals \u03b1 in (Chen and Yang, 2021). Theorem 3 (Convergence of projected gradient descent at y\u2217). Suppose Assumption 1 & 2 hold. Consider the projected gradient descent updating formula at the dual y\u2217:\nU t = \u03a0\u2126 ( U t\u22121 \u2212 \u03b1\u2207UL\u03b2(U t\u22121, y\u2217) ) , t \u2208 Z+.\nIf there exists some block partition sizes m and an associated optimal solution Ua,\u2217 with some within block weights ak = (ak,1, ak,2, . . . , ak,mk) satisfying a\u0304 \u2264 ca, c > 0, such that the initialization discrepancy \u22060 = U0 \u2212 Ua,\u2217 satisfies\n\u2225\u22060S(a)c\u2225\u221e \u2264 C1an\np/\u03982min +m and \u2225\u22060\u2225F \u2264 C3 [1 + C2\u03b1(L+ n\u03b2 +\u03982max)] I\n\u00b7 min { n\nn , an\u03982min rn2a\u03043(L+ n\u03b2 + n\u03982max) , (1\u2212 \u03b3)an\u03982min/K \u0398max \u221a K log n+ p+ log2 n)\n, (1\u2212 \u03b3)a\u221an } ,\nwhere m = mink \u0338=l 2nknlnk+nl , n = min{nk}, \u0398max = max1\u2264j \u0338=k\u2264K \u2225\u00b5j\u2212\u00b5k\u22252, and I = (4\u03b1) \u22121(p+ C1m\u0398 2 min) \u22121. If we take \u03b2 = C1an\u0398 2 min 16rn2a\u03043 , L = p + C1\u0398 2 min 4 (m \u2212 an2 8rn2a\u03043 ) in (12) and choose step size \u03b1 = C4an 2\u03982min\nrn2a\u03043(L+n\u03b2)2 , then it holds with probability at least 1\u2212 n \u2212C5 that for any t \u2265 I ,\nU t \u2208 Gm and inf Q\u2208Fm \u2225U t+1 \u2212 U\u2217Q\u2225F \u2264 \u03b3 inf Q\u2208Fm \u2225U t \u2212 U\u2217Q\u2225F ,\nwhere \u03b32 = 1\u2212 C6(an 2\u03982min) 2\n(rn2a\u03043)2(L+n\u03b2)2 and C1, . . . , C6 are universal constants.\nIn scenarios where the dimension p is moderate (p = O( \u221a n log n)) and the cluster sizes are equal, the parameters and initialization criteria can be simplified to the expression presented in Theorem 1 once we substitute p = O( \u221a n log n) and n\u0304 = n = n/K. The proof of Theorem 3 consists of four parts (Appendix D.3.1, D.3.2, D.3.3 and D.3.4). First we will prove some key propositions which are derived from the theoretical analysis of K-means SDP (Chen and Yang, 2021) in Appendix D.3.1, and some important properties of the BM formulation of the SDP in Appendix D.3.2. Then we will analyze the behavior of convergence in two phases. For the Phase 1 (D.3.3), we will prove a constant speed convergence (Theorem 4) of the algorithm until it reaches to the same block form as the optimum point, where we call it Phase 2. We then show that the algorithm converges exponentially quickly to the optimum solution in Phase 2 (Theorem 5, which combined with Phase 1 result leads to the overall convergence of Projected-GD (Theorem 3). The proof of all intermediate lemmas are placed in Appendix D.5."
        },
        {
            "heading": "D.3.1 PROPERTIES FOR SDP",
            "text": "First we list our main assumptions and propositions from (Chen and Yang, 2021) below. We will assume Assumption 1 and Assumption 2 to hold throughout the rest of the proof. Recall that Assumption 1 (general version of Assumption A for the case of unbalanced cluster sizes) requires the minimal separation \u0398min to be larger than certain threshold (with order O( \u221a log n)). The minimal separation can be interpreted as the signal-to-noise ratio of the clustering problem. Under such an assumption, the solution of SDP can achieve exact recovery under Gaussian mixture models (GMM) (Chen and Yang, 2021).\nThe following propositions are derived from the original SDP problem (Chen and Yang, 2021), which characterize the bounds for eigenvalues of the matrices related to the dual construction of the original SDP (Proposition 3, 4 and Corollary 1) that will be used mainly to describe the smoothness of augmented Lagrangian function (ALF) (Lemma 1) and Hessian of the ALF (Lemma 5). Proposition 5 will be used for characterizing the behavior of the gradient of ALF in Phase 1 at some optimum point (Lemma 2 and Lemma 3) Proposition 3. For any v \u2208 Rn, define S(v) := vTAv, \u0393K := span{1Gk : k \u2208 [K]}\u22a5, which is the orthogonal complement of the linear subspace of Rn spanned by the vectors 1G1 , . . . ,1GK . Then Lemma VI.1 (Chen and Yang, 2021) implies\nP (\nmax v\u2208\u0393K , \u2225v\u2225=1\n|S(v)| \u2265 ( \u221a n+ \u221a p+ \u221a 2t)2 ) \u2264 e\u2212t, \u2200t > 0.\nProof. Refer to the argument after Lemma IV.1 in (Chen and Yang, 2021) (right below equation (26) in (Chen and Yang, 2021)) The exact formula is showed right above Lemma IV.2 in (Chen and Yang, 2021). Proposition 4 (Theorem II.1 in (Chen and Yang, 2021)). Define\nWn := \u03bbId + 1\n2 (y\u22171Tn + 1n(y \u2217)T )\u2212B +A"
        },
        {
            "heading": "If Assumption 1 and 2 hold, then with high probability the following equation holds:",
            "text": "\u2225Wn\u2225op \u2264 \u03bb+ C2(n+ \u221a mp log n), Wn \u227b 0, Wn1Gk = 0,\u2200k \u2208 [K]\nfor some constant C2 > 0. Furthermore, SDP achieves exact recovery with high probability at least 1\u2212D4K2n\u2212C3 , for some constant D4.\nProof. Wn \u227b 0, Wn1Gk = 0,\u2200k \u2208 [K] and SDP achieves exact recovery have been shown from the proof of Theorem II.1 in (Chen and Yang, 2021). From the argument after Lemma IV.1 in (Chen and Yang, 2021) we have the upper bound vTWnv \u2264 \u03bb\u2225v\u22252 \u2212 T (v). Then from the bound of |T (v)| (Lemma IV.2 in (Chen and Yang, 2021)) we have\n\u2225Wn\u2225op \u2264 \u03bb+ C2(n+ \u221a mp log n),\nfor some constant C2 > 0 with high probability. \u25a0\nProposition 5. Recall the minimal separation \u0398min := min1\u2264j \u0338=k\u2264K \u2225\u00b5j \u2212 \u00b5k\u22252, and the maximal separation \u0398max := max1\u2264j \u0338=k\u2264K \u2225\u00b5j \u2212 \u00b5k\u22252. If Assumption 1 and 2 hold, then we have \u2200j \u2208 Gl, k \u0338= l, (k, l) \u2208 [K]2,\nDk,l(j) := [(2A+ 1n(y \u2217)T + y\u22171Tn )Gl,Gk1nk ]j \u2208 [C1n\u03982min, n\u0304\u03982max],\nwith probability at least 1\u2212 12n\u2212C3 . In particular, Bij \u2264 2/C1\u03982max, \u2200i, j \u2208 [n].\nProof. From equation (19) (21) (26) in (Chen and Yang, 2021), we know that\nDk,l(j) = [BGl,Gk1nk ]j \u2265 C1/2nk\u2225\u00b5l \u2212 \u00b5k\u2225 \u2265 C1/2n\u03982min.\nIn particular, c(k,l)j in (Chen and Yang, 2021) is defined to be [BGl,Gk1nk ]j and Bi,j := Dk,l(j)Dl,k(i)/ \u2211 j\u2208Gl Dk,l(j),\u2200i \u2208 Gk, j \u2208 Gl (refer to the paragraph right below (22) in (Chen and Yang, 2021)). On the other hand, from (21) in (Chen and Yang, 2021) we know\nDk,l(j) = \u2212 nl + nk 2nl \u03bb+ nk 2 (\u2225X\u0304k \u2212Xj\u22252 \u2212 \u2225X\u0304l \u2212Xj\u22252),\nwhere X\u0304k = n\u22121k \u2211\ni\u2208Gk Xi is the empirical mean of points in cluster k. From the proof of Lemma IV.1 in (Chen and Yang, 2021), recall Xj = \u00b5l + \u03b5j and denote \u03b8 = \u00b5l \u2212 \u00b5k we can write\n\u2225X\u0304k \u2212Xj\u22252 \u2212 \u2225X\u0304l \u2212Xj\u22252 = \u2225\u03b8 + \u03b5j \u2212 \u03b5\u0304k\u22252 \u2212 \u2225\u03b5j \u2212 \u03b5\u0304l\u22252 \u2264 \u2225\u03b8 + \u03b5\u0304l \u2212 \u03b5\u0304k\u22252.\nHence Dk,l(j) \u2264\nnk 2 (\u2225X\u0304k \u2212Xj\u22252 \u2212 \u2225X\u0304l \u2212Xj\u22252) \u2264 nk 2 \u2225\u03b8 + \u03b5\u0304l \u2212 \u03b5\u0304k\u22252.\nThen we can get the high dimensional upper bound nk\u2225\u03b8\u22252 for Dk,l(j) by bounding the Gaussian vectors \u03b5i using the same way as the proof of Lemma IV.1 in (Chen and Yang, 2021). Thus,\nDk,l(j) \u2264 nk\u2225\u03b8\u22252 \u2264 n\u0304\u03982max, and Bi,j = Dk,l(j)Dl,k(i)/ \u2211\nj\u2208Gl Dk,l(j) \u2264 2/C1nk\u2225\u00b5l \u2212 \u00b5k\u2225 \u2264 2/C1\u0398 2 max. \u25a0\nCorollary 1. Let w \u2208 Rn, S to be the set of non-zero positions for w. If S \u2286 Gk, for some k \u2208 [K], then \u22122\u03bb\u2225w\u22252 \u2264 \u27e82A+ y\u22171Tn + 1n(y\u2217)T , wwT \u27e9 \u2264 2C2(n+ \u221a mp log n))\u2225w\u22252.\nFurthermore, for general w \u2208 Rn, \u22122\u03bb\u2225w\u22252 \u2264 \u27e82A+ y\u22171Tn + 1n(y\u2217)T , wwT \u27e9 \u2264 2(C2(n+ \u221a mp log n) + 1/C1\u0398 2 max)\u2225w\u22252,\nfor some constant C1, C2.\nProof. From Proposition 4 we have\nWn := \u03bbId + 1\n2 (y\u22171Tn + 1n(y \u2217)T )\u2212B +A \u227b 0.\nHence\n\u27e81 2 (y\u22171Tn + 1n(y \u2217)T ) +A,wwT \u27e9 \u2265 \u27e8B \u2212 \u03bbId, wwT \u27e9.\nRecall B \u2265 0, BGk,Gk = 0, \u2200k \u2208 [K], then we have S \u2286 Gk =\u21d2 \u27e8B,wwT \u27e9 = 0. Thus,\n\u27e82A+ (y\u22171Tn + 1n(y\u2217)T ), wwT \u27e9 \u2265 \u22122\u03bb\u2225w\u22252.\nOn the other hand,\n\u27e81 2 (y\u22171Tn + 1n(y \u2217)T ) +A,wwT \u27e9 = \u27e8Wn +B \u2212 \u03bbId, wwT \u27e9\n\u2264 \u2225Wn\u2225op\u2225w\u22252 \u2212 \u03bb\u2225w\u22252 \u2264 C2(n+ \u221a mp log n)\u2225w\u22252F .\nSince \u2225Wn\u2225op \u2264 2C2(n + \u221a mp log n) from Proposition 4. Furthermore, if S \u0338\u2286 Gk, \u2200k \u2208 [K], then\n\u27e81 2 (y\u22171Tn + 1n(y \u2217)T ) +A,wwT \u27e9 = \u27e8Wn +B \u2212 \u03bbId, wwT \u27e9\n\u2264 C2(n+ \u221a mp log n)\u2225w\u22252 + 1/2max\nij Bij\u2225w\u22252.\nBy Proposition 5, Bij \u2264 2/C1\u03982max, \u2200i, j \u2208 [n], where universal constant C1 \u2208 (0, 1), \u0398max is the maximum of separation. Thus,\n\u27e81 2 (y\u22171Tn + 1n(y\n\u2217)T ) +A,wwT \u27e9 \u2264 (C2(n+ \u221a mp log n) + 1/C1\u0398 2 max)\u2225w\u22252.\n\u25a0"
        },
        {
            "heading": "D.3.2 THE BM FORMULATION AND SOME PROPERTIES",
            "text": "Before diving into the proofs of Phases 1 and 2, let us recall the Augmented Lagrangian Function (ALF) to which we would apply Projected Gradient Descent (PGD), and present two lemmas for characterizing the smoothness of ALF and the properties of the projection operator.\nConsider the y = y\u2217 defined Assumption 2. Recall the ALF is given by\nf(U) := L(U, y) = \u27e8L \u00b7 Idn +A,UUT \u27e9+ \u27e8y, UUT1n \u2212 1n\u27e9+ \u03b2\n2 \u2225UUT1n \u2212 1n\u22252F , (19)\nwhere L \u2208 (0, \u03bb), \u03b2 > 0. Then we can get the gradient of f as \u2207f(U) = (2A+ 2L \u00b7 Id + y1Tn + 1nyT )U + \u03b2[1n(UUT1n \u2212 1n)T + (UUT1n \u2212 1n)1Tn ]U. A direct implication from Proposition 4 is [\u2207f(U\u2217Q))]S = \u22122(\u03bb\u2212 L)[U\u2217Q]S , \u2200Q \u2208 Fm, where [U ]S stands for the matrix that keeps positive entries of U\u2217Q and set the non-positive ones to zero. This is due to the fact that Wn1Gk = 0,\u2200k \u2208 [K] and Wn = \u03bbId + 12 (y \u22171Tn + 1n(y \u2217)T )\u2212B +A. Recall that Q,Qt \u2208 Fm are defined to be Q = argminQ\u0303\u2208Fm\u2225U \u2212 U \u2217Q\u0303\u2225F , Qt = argminQ\u0303\u2208Fm\u2225U t \u2212 U\u2217Q\u0303\u2225F , \u2200t \u2265 0. Now we will consider the smoothness of the ALF. Here we consider two cases of U : the first case is for general U in the feasible set \u2126; the second case is for the U that has the same block form as some optimum point U\u2217Q\u0303. As we will see, the bound of Lipschitz constant of the later one will be smaller. Recall that \u2126 := {U \u2208 Rn\u00d7r : \u2225U\u22252F = K,U \u2265 0}. Lemma 1 (Smoothness condition). If \u2225U \u2212 U\u2217Q\u0303\u2225F \u2264 1, for some Q\u0303 \u2208 Fm, then\n\u2225\u2207f(U)\u2212\u2207f(U\u2217Q\u0303)\u2225F \u2264 R1\u2225U \u2212 U\u2217Q\u0303\u2225F , where R1 = 2C2(n + \u221a mp log n) + 2L + 12n\u03b2 if U \u2208 Gm; R1 = 2C2(n + \u221a mp log n) + 2/C1\u0398 2 max+2L+12n\u03b2, for some constant C1, C2 and general U \u2208 \u2126, where \u03982max is the maximum squared separation.\nThe next lemma shows that locally, the projection of PGD is equivalent to the projection to a convex set, and therefore is a local contraction map. Lemma 2 (Local projection to the ball). Denote \u03f5c := (\u03bb\u2212L) \u221a K\n2[ \u221a KR1+2(\u03bb\u2212L) \u221a K+n\u03982max] . Here R1 =\n2C2(n+ \u221a mp log n) + 2L+ 2/C1\u0398 2 max + 12n\u03b2. Then \u2225U \u2212 U\u2217Q\u2225F \u2264 \u03f5c =\u21d2 \u27e8\u2207f(U), U\u27e9 < \u2212(\u03bb\u2212 L)K/2 < 0."
        },
        {
            "heading": "Furthermore, if \u03b1 > 0,",
            "text": "\u2225\u03a0+(U \u2212 \u03b1\u2207f(U))\u22252F > K, \u03a0\u2126(U \u2212 \u03b1\u2207f(U)) = \u03a0C(\u2126)(U \u2212 \u03b1\u2207f(U)) for some constant C1, C2."
        },
        {
            "heading": "D.3.3 LINEAR CONVERGENCE NEAR OPTIMAL POINT (PHASE 1)",
            "text": "Here we will prove the convergence of U t to the block form in Phase 2 (Theorem 4), which will be implied by Lemma 3 and Lemma 4. The first lemma shows that after at most I iterations, the iterate will become the block form and fall into set Gm, which then enters the phase two. The second lemma controls the distance to the optimum within Phase 1, which will help us set appropriate neighborhood of initialization to ensure the convergence of Phase 1. Lemma 3 (One-step shrinkage towards block form). Denote \u2206 = U \u2212Ua,\u2217, where Ua,\u2217 is defind as (17). Choose (L, \u03b2) such that:\n2(\u03bb\u2212 L) n \u2264 \u03b2 \u2264 C1an\u0398 2 min 12rn2a\u03043 .\nSuppose \u2225\u2206\u2225F \u2264 \u03f5c, where \u03f5c is defined in Lemma 2. Assume\n\u2225\u2206S(a)c\u2225\u221e \u2264 C1an\u0398\n2 min\n32\u03bb , \u2225\u2206\u2225F \u2264 min\n{ \u03f51, 3\n4 a \u221a n\n} , (20)\nwhere \u03f51 :=\nmin  C1an\u03982min64K(\u0398max\u221a2K log n+ C5(p+ log2 n)) , \u221a C1an\u03982min 96\u03b2ra\u0304n\u0304 , C1an\u0398 2 min 96\u03b2ra\u03042n\u0304 \u221a n  , Then we have\n[U ]i,\u03c4 \u2212 \u03b1[\u2207f(U)]i,\u03c4 \u2264 [U ]i,\u03c4 \u2212 \u03b1 C1an\u0398\n2 min\n8 ,\u2200i \u2208 Gl, s(k \u2212 1) < \u03c4 \u2264 sk, l \u0338= k."
        },
        {
            "heading": "Furthermore,",
            "text": "[V ]i,\u03c4 \u2264 max{[U ]i,\u03c4 \u2212 \u03b1 C1an\u0398\n2 min\n8 , 0},\u2200i \u2208 Gl, s(k \u2212 1) < \u03c4 \u2264 sk, l \u0338= k.\nwhere V = \u03a0\u2126(W ), W = U \u2212 \u03b1\u2207f(U), for some constant C1, C5. Consequently, after at most I = 14\u03b1\u03bb iterations, U\nt will enter Phase 2, i.e.,U I \u2208 Gm if for every step t, U t meets the above conditions (20).\nThe next lemma calculates the upper bound of the Inflation of \u2225U \u2212 U\u2217Q\u2225F after Projected-GD. This will be used when we have the assumptions of neighborhood for the initialization of Phase 2. Then we can trace the assumptions back to the initialization of Phase 1 as we know the inflation rate at each step of Phase 1. Lemma 4 (Inflation of distance to Ua,\u2217 for Phase 1). If \u2225U \u2212 U\u2217Q\u2225F \u2264 \u03f5c, then\n\u2225V \u2212 Ua,\u2217\u2225F \u2264 \u03b7\u2225U \u2212 Ua,\u2217\u2225F , where \u03b7 = 1 + \u03b1R1, R1 = 2C2(n + \u221a mp log n) + 2/C1\u0398 2 max + 2L + 12n\u03b2, for some constant C1, C2.\nNow, we will establish our condition on the initialization, given our knowledge of the inflation of \u2225\u2206t\u2225F for each step t in Phase 1. This is summarized in the following theorem. Theorem 4 (Convergence of Phase 1). Recall that \u03bb = p+ C14 m\u0398 2 min, m = mink \u0338=l 2nknl nk+nl\n. Define"
        },
        {
            "heading": "U0 to be the initialization of BM method and define \u2206t := U t\u2212Ua,\u2217. Let I = 1/(4\u03b1\u03bb), \u03b7 = 1+\u03b1R1,",
            "text": "where R1 = 2C2(n+ \u221a mp log n) + 2L+2/C1\u0398 2 max +12n\u03b2. Suppose I is an integer and a\u0304 \u2264 ca, for some constant c > 0. If we choose (L, \u03b2) in (19) such that:\n2(\u03bb\u2212 L) n \u2264 \u03b2 \u2264 C1an\u0398 2 min 12rn2a\u03043 .\nThen with probability at least 1 \u2212 C4n\u2212C3 , we have U I \u2208 Gm, \u2225U t \u2212 Ua,\u2217\u2225F \u2264 \u03f5c,\u2200t \u2264 I, provided that\n\u2225\u22060S(a)c\u2225\u221e \u2264 C1an\u0398\n2 min\n32\u03bb , \u2225\u22060\u2225F \u2264\n1\n\u03b7I min\n{ \u03f5c, \u03f51, 3\n4 a \u221a n\n} .\nHere \u03f51 :=\nmin  C1an\u03982min64K(\u0398max\u221a2K log n+ C5(p+ log2 n)) , \u221a C1an\u03982min 96\u03b2ra\u0304n\u0304 , C1an\u0398 2 min 96\u03b2ra\u03042n\u0304 \u221a n  , \u03f5c := (\u03bb\u2212 L) \u221a K\n2[(1 + \u221a K)R1 + 2(\u03bb\u2212 L) \u221a K + \u221a nr \u00b7 a\u0304n\u0304\u03982max] .\nC1, C2, C3, C4, C5 are some constants.\nProof. This can be implied from Lemma 3 and Lemma 4 by assuming the neighborhood requirement in Lemma 3 multiplied by the factor 1/\u03b7I for the neighborhood of the initialization \u2225\u22060\u2225F . Note that if we know the total step of Phase 1, which is I , then we only need to assume \u2225\u22060\u2225F \u2264 1 \u03b7I min { \u03f5c, \u03f51, 3 4a \u221a n } , if we want \u2225\u2206t\u2225F \u2264 min { \u03f5c, \u03f51, 3 4a \u221a n } ,\u2200t \u2264 I. \u25a0"
        },
        {
            "heading": "D.3.4 LINEAR CONVERGENCE NEAR OPTIMAL POINT (PHASE 2)",
            "text": "Here our goal is to prove the linear convergence of Phase 2 when U \u2208 Gm (Theorem 5). The main idea is to show the local strongly convexity at optimal points (Lemma 5) followed by standard argument of linear convergence (Lemma 6). Moreover, we also need to ensure that the iterations continue in block form during Phase 2 (Lemma 7). Lemma 5 (Local strong convexity). When U \u2208 Gm, if we define \u2206 := UQT \u2212 U\u2217, then\n\u27e8\u22072f(U\u2217)[\u2206],\u2206\u27e9 \u2265 \u03b2\u0303\u2225\u2206\u22252F ,\nwhere \u03b2\u0303 = min{2[L \u2212 ( \u221a n + \u221a p + \u221a 2 log n)2],\u22122(\u03bb \u2212 L) + \u03b2n} > 0, provided that L \u2208 (( \u221a n+ \u221a p+ \u221a 2 log n)2, \u03bb), \u03b2 > 2(\u03bb\u2212L)/n. Furthermore, for every U satisfying \u2225U\u2212U\u2217\u2225F < \u03f5s, we have \u27e8\u22072f(U)[\u2206],\u2206\u27e9 \u2265 \u03b2\u0303/2\u2225\u2206\u22252F ,\nwhere \u03f5s = \u03b2\u030336\u03b2n .\nAfter establishing the local strong convexity, we can prove the linear (exponential) convergence of the algorithm through a standard analysis for PGD as below. Lemma 6 (Local exponential convergence). When U \u2208 Gm, if we define \u2206 := UQT \u2212 U\u2217, then there exists \u03b3 \u2208 (0, 1), \u03f5 > 0, s.t.,\n\u2225V \u2212 U\u2217Q\u2225F \u2264 \u03b3\u2225U \u2212 U\u2217Q\u2225F ,\nwhere Q = argminQ\u0303\u2208Fm\u2225U \u2212 U\n\u2217Q\u0303\u2225F , provided that \u2225U \u2212 U\u2217Q\u2225F < \u03f5, where \u03f5 = min{\u03f5c, \u03f5s}. Recall that \u03f5c is defined in Lemma 2, \u03f5s is defined in Lemma 5. In particular, if we we choose the step size \u03b1 \u2264 \u03b2\u0303/(2R22), where R2 = 2C2(n+ \u221a mp log n) + 2L+ 12n\u03b2, for some constant C2. Then the contraction factor would be \u03b32 = (1\u2212 \u03b1\u03b2\u0303/2).\nThe convergence for Phase 2 has not yet been proved since the above lemma only demonstrates the convergence of PGD once U t has reached Phase 2. However, the subsequent update U t+1 might not remain in Phase 2. The following lemma ensures that U t stays in Phase 2, provided that the initial point U0 in Phase 2 is located within some neighborhood of an optimum point. Lemma 7 (Iterations remain staying in Phase 2). Suppose Lemma 6 holds and U0 \u2208 Gm. Let \u03f5\u0304 := min { \u03f51, 3 4a \u221a n } . Then we have\nU t \u2208 Gm, \u2200t \u2265 1,\nwhere U t+1 = \u03a0\u2126(U\nt \u2212 \u03b1\u2207f(U t)), \u2200t \u2265 1, as long as \u2225U0 \u2212 Ua,\u2217\u2225F \u2264 min{(1\u2212 \u03b3)/2\u03f5\u0304, \u03f5}, where recall that \u03f5 = min{\u03f5c, \u03f5s}.\nNow, we can ensure the the overall exponential convergence in Phase 2 by combining above lemmas. Theorem 5 (Linear convergence in Phase 2). Suppose U0 \u2208 Gm. Let the step size: \u03b1 < \u03b2\u0303 2(2C2(n+ \u221a mp logn)+2L+12n\u03b2)2 , \u03b2\u0303 = min{2[L \u2212 ( \u221a n + \u221a p + \u221a 2 log n)2],\u22122(\u03bb \u2212 L) + \u03b2n}. Then with probability at least 1\u2212 C4n\u2212C3 , we have\n\u2225U t+1 \u2212 U\u2217Qt+1\u2225F \u2264 \u03b3\u2225U t \u2212 U\u2217Qt\u2225F , \u2200k,\ngiven \u2225U0 \u2212 U0Q0\u2225F \u2264 \u03f5, \u2225U0 \u2212 Ua,\u2217\u2225F \u2264 \u03f50, where \u03b32 = (1\u2212 \u03b1\u03b2\u0303/2),\n\u03f5 = min\n{ \u03b2\u0303\n36\u03b2n ,\n(\u03bb\u2212 L) \u221a K\n2[ \u221a KR1 + 2(\u03bb\u2212 L) \u221a K + n\u03982max]\n} , \u03f50 :=\n1\u2212 \u03b3 2 min\n{ \u03f51, 3\n4 a \u221a n\n} ,\nfor some constant C2, C3, C4.\nProof. This can be implied from Lemma 6 and Lemma 7. Lemma 6 indicates linear convergence if the former step is in Phase 2. Lemma 7 guarantees that every step of Projected-GD will stay in Phase 2. Therefore we only need to combine the conditions for both lemmas. \u25a0\nCombining Phase 1 and Phase 2 (Theorem 4 and Theorem 5) and lifting the assumptions of initialization for Phase 2 to the initialization of Phase 1 (by multiplying the factor 1/\u03b7I ), we obtain the following theorem. This theorem represents a stronger version of Theorem 3, presented at the beginning of Appendix D.3. This theorem is stronger in the sense that we can get the exact relationships between parameters. However, the presentation of the theorem would sacrifice the conciseness of the statement. Hence we present the simpler version as Theorem 3 at the beginning of Appendix D.3. Theorem 6 (Local linear rate of convergence). Recall \u2206t := U t \u2212 Ua,\u2217. Suppose a\u0304 \u2264 ca, c > 0. Let I = (4\u03b1\u03bb)\u22121, \u03b7 = 1+ \u03b1R1, R1 = 2C2(n+ \u221a mp log n) + 2L+ 2/C1\u0398 2 max + 12n\u03b2. Choose (L, \u03b2) such that:\nL > ( \u221a n+ \u221a p+ \u221a 2 log n)2,\n2(\u03bb\u2212 L) n \u2264 \u03b2 \u2264 C1an\u0398 2 min 12rn2a\u03043 ,\nand the step size \u03b1 < \u03b2\u0303 2(2C2(n+ \u221a mp logn)+2L+12n\u03b2)2\n, \u03b2\u0303 = min{2[L \u2212 ( \u221a n + \u221a p +\n\u221a 2 log n)2],\u22122(\u03bb \u2212 L) + \u03b2n}. Then with probability at least 1 \u2212 C4n\u2212C3 , we have that for any t \u2a7e I , U t \u2208 Gm, and \u2225U t+1 \u2212 U\u2217Qt+1\u2225F \u2264 \u03b3\u2225U t \u2212 U\u2217Qt\u2225F ,\nprovided that\n\u2225\u22060S(a)c\u2225\u221e \u2264 C1an\u0398\n2 min\n32\u03bb , \u2225\u22060\u2225F \u2264\n1\n\u03b7I min { \u03f5, 1\u2212 \u03b3 2 \u03f51, 1\u2212 \u03b3 2 3 4 a \u221a n } ,\nwhere \u03b32 = (1\u2212 \u03b1\u03b2\u0303/2), \u03f5 = min {\n\u03b2\u0303 36\u03b2n ,\n(\u03bb\u2212L) \u221a K\n2[ \u221a KR1+2(\u03bb\u2212L) \u221a K+n\u03982max]\n} , \u03f51 :=\nmin  C1an\u03982min64K(\u0398max\u221a2K log n+ C5(p+ log2 n)) , \u221a C1an\u03982min 96\u03b2ra\u0304n\u0304 , C1an\u0398 2 min 96\u03b2ra\u03042n\u0304 \u221a n  . C1, C2, C3, C4, C5 are some constants.\nD.4 PROOF OF LINEAR CONVERGENCE WITH y \u2248 y\u2217\nThe following theorem extends the results of Theorem 3 from dual variable y = y\u2217 to any value in a neighborhood of the optimum dual y\u2217, which completes the primal-dual local convergence analysis by demonstrating that the primal subproblem minU\u2208\u2126 L\u03b2(U, y) can be efficiently solved around the optimum.\nTheorem 1 (Convergence of projected gradient descent). Suppose y\u2217 is the optimum dual for the SDP problem (Assumption 2 in Appendix D.3) and Assumption A hold. Assume p = O(n log n), \u0398max \u2264 C\u0398min for some C > 0, and there exists some block partition sizes m and an associated optimal solution Ua,\u2217 with some within block weights ak = (ak,1, ak,2, . . . , ak,mk) satisfying\na\u0304 \u2264 ca, c > 0 and a = mink\u2208[K] min\u2113\u2208[mk]{ak,\u2113} and a\u0304 = maxk\u2208[K] max\u2113\u2208[mk]{ak,\u2113}. We denote the initialization of the algorithm as U0; let U\u0303 denote the unique (whose existence is also guaranteed) stationary point of the projected gradient descent updating formula under dual value y\u0303 close to y\u2217, i.e. U\u0303 satisfies\nU\u0303 = \u03a0\u2126 ( U\u0303 \u2212 \u03b1\u2207UL\u03b2(U\u0303 , y\u0303) ) .\nIf \u2225y\u0303 \u2212 y\u2217\u2225 \u2264 \u03b4 for some \u03b4 > 0, and the initialization discrepancy \u22060 = U0 \u2212 Ua,\u2217 satisfies\n\u2225\u22060S(a)c\u2225\u221e \u2264 O(K/ \u221a nr) and \u2225\u22060\u2225F \u2264 O ( r\u22120.5K\u22125.5 min{1, K\u22122.5\u03982min/ log n)} ) ,\nthen it holds with probability at least 1\u2212 c1n\u2212c2 that, for any t \u2265 I = O(K3),\nU t \u2208 Gm and inf Q\u2208Or;U\u0303Q\u2208Gm \u2225U t+1 \u2212 U\u0303Q\u2225F \u2264 \u03b3 inf Q\u2208Or;U\u0303Q\u2208Gm \u2225U t \u2212 U\u0303Q\u2225F ,\nwhere \u03b3 = 1\u2212O(K\u22126), provided that \u03b2 = O(\u03982min/K3), L = O(n\u03982min/K) in (12) and step size \u03b1 is chosen such that \u03b1\u22121 = O(K2n\u03982min). c1 and c2 are some constants.\nProof sketches. The proof slightly modifies that of Theorem 3, where we divide the convergence of the projected gradient descent for solving the primal subproblem into two phases. In phase one, we demonstrate that, for the same reason as in the previous proof, the iterate will become block diagonal Gm after at most I iterations. In phase two, due to the strong convexity of L( \u00b7 , y\u0303) around any U\u0303 within Gm, the projected gradient descent will achieve a linear convergence rate. Here, the strong convexity is implied by the strong convexity of L( \u00b7 , y\u2217) at U\u2217 within Gm and the continuity of the Hessian (with respect to U ) of the Augmented Lagrangian function L(U, y) with respect to U and y. Precise statements regarding the two phases are provided below. \u25a0\nBy employing the same argument used for Phase 1 and Phase 2 regarding convergence at the optimum dual y\u2217 (as per Theorem 4 and 5), we derive the following analogues for y\u0303.\nTheorem 7 (Convergence of Phase 1). Recall that \u03bb = p+ C14 m\u0398 2 min, m = mink \u0338=l 2nknl nk+nl . Define"
        },
        {
            "heading": "U0 to be the initialization of BM method and define \u2206t := U t \u2212 Ua,\u2217. Let I = 1/(4\u03b1\u03bb), \u03b7 =",
            "text": "1 + \u03b1R1, where R1 = 2C2(n+ \u221a mp log n) + 2L+ 2/C1\u0398 2 max + 12n\u03b2. Suppose I is an integer and a\u0304 \u2264 ca, c > 0. If we choose (L, \u03b2) in (19) such that:\n2(\u03bb\u2212 L) n \u2264 \u03b2 \u2264 C1an\u0398 2 min 12rn2a\u03043 ,\nthen with probability at least 1\u2212 C4n\u2212C3 (the high probability argument comes from Assumption 1), we have U I \u2208 Gm; \u2225U t \u2212 Ua,\u2217\u2225F \u2264 \u03f5c,\u2200t \u2264 I, provided that\n\u2225\u22060S(a)c\u2225\u221e \u2264 C1an\u0398\n2 min\n32\u03bb , \u2225\u22060\u2225F \u2264\n1\n\u03b7I min\n{ \u03f5c, \u03f51, 3\n4 a \u221a n\n} , \u2225y\u2217 \u2212 y\u0303\u2225 \u2264 \u03b4,\nfor some small \u03b4 > 0, where \u03f51 :=\nmin  C1an\u03982min64K(\u0398max\u221a2K log n+ C5(p+ log2 n)) , \u221a C1an\u03982min 96\u03b2ra\u0304n\u0304 , C1an\u0398 2 min 96\u03b2ra\u03042n\u0304 \u221a n  , \u03f5c := (\u03bb\u2212 L) \u221a K\n2[(1 + \u221a K)R1 + 2(\u03bb\u2212 L) \u221a K + \u221a nr \u00b7 a\u0304n\u0304\u03982max] .\nC1, C2, C3, C4, C5 are some constants.\nTheorem 8 (Linear convergence of Phase 2). Suppose U0 \u2208 Gm, then there exists a unique U\u0303 , such that it is the stationary point of the projected gradient descent updating formula for any dual y\u0303 close to y\u2217, that is,\nU\u0303 = \u03a0\u2126 ( U\u0303 \u2212 \u03b1\u2207UL\u03b2(U\u0303 , y\u0303) ) .\nFurthermore, we define\nQt = argminQ\u2208Or;U\u0303Q\u2208Gm \u2225U t \u2212 U\u0303Q\u2225F , \u2200t \u2265 0.\nIf \u2225y\u2217 \u2212 y\u0303\u2225 \u2264 \u03b4, for some \u03b4 > 0; the step size \u03b1 < \u03b2\u0303 2(2C2(n+ \u221a p logn)+2L+12n\u03b2)2 , \u03b2\u0303 = min{2[L\u2212 ( \u221a n+ \u221a p+ \u221a 2 log n)2],\u22122(\u03bb\u2212 L) + \u03b2n}, then with probability at least 1\u2212 C4n\u2212C3 , we have\n\u2225U t+1 \u2212 U\u0303Qt+1\u2225F \u2264 \u03b3\u2225U t \u2212 U\u0303Qt\u2225F , \u2200k,\ngiven \u2225U0 \u2212 U0Q0\u2225F \u2264 \u03f5, \u2225U0 \u2212 Ua,\u2217\u2225F \u2264 \u03f50, where \u03b32 = (1\u2212 \u03b1\u03b2\u0303/2),\n\u03f5 = min\n{ \u03b2\u0303\n36\u03b2n ,\n(\u03bb\u2212 L) \u221a K\n2[ \u221a KR1 + 2(\u03bb\u2212 L) \u221a K + n\u03982max]\n} , \u03f50 :=\n1\u2212 \u03b3 2 min\n{ \u03f51, 3\n4 a \u221a n\n} ,\nfor some constant C2, C3, C4.\nProof sketches. Lemma 5 indicates that \u27e8\u22072f(U\u2217)[\u2206],\u2206\u27e9 \u2265 \u03b2\u0303\u2225\u2206\u22252F , for some \u03b2\u0303 > 0, where \u2206 = U \u2212V, \u2200U, V \u2208 Gm. This is due to the fact that \u2206 = U \u2212V = (U \u2212V \u2212U\u2217Q)+U\u2217Q, (U \u2212 V \u2212 U\u2217Q) \u2208 Gm. Hence under the condition of the local projection lemma (Lemma 2) we have\n\u2225\u03a0\u2126(U \u2212 \u03b1\u2207f(U))\u2212\u03a0\u2126(U \u2212 \u03b1\u2207f(U))\u22252\n\u2264 \u2225(U \u2212 \u03b1\u2207f(U))\u2212 (U \u2212 \u03b1\u2207f(U))\u22252\n\u2264 \u2225U \u2212 V \u22252 \u2212 2\u03b1\u27e8\u2207f(U)\u2212\u2207f(V ), U \u2212 V \u27e9+ \u03b12\u2225\u2207f(U)\u2212\u2207f(V )\u22252\n\u2264 \u2225U \u2212 V \u22252 \u2212 \u03b1\u03b2\u0303/2\u2225U \u2212 V \u22252\n= (1\u2212 \u03b1\u03b2\u0303/2)\u2225U \u2212 V \u22252.\nTherefore the map U 7\u2192 \u03a0\u2126(U \u2212\u03b1\u2207f(U)) is a (strict) contraction map. Hence there exists a unique stationary point U\u0303 within Gm by the contraction mapping theorem. The rest of proof is the same as the proof of linear convergence in Phase 2 at optimum dual y\u2217 (Theorem 5). \u25a0\nIf we combine Phase 1 and Phase 2 together (Theorem 7 and Theorem 8) and lift the assumptions of initialization for Phase 2 to the initialization of Phase 1 (by multiplying the factor 1/\u03b7I ), we will get the following theorem, which is a stronger version of Theorem 1 in the sense that we do not require moderate p = O( \u221a n log n) and the constraint on the maximal separation \u0398max \u2264 C\u0398min. What is more, we can get the exact relationships between parameters from this theorem. However, the presentation of the theorem would sacrifice the conciseness of the statement. Hence we present the simpler version as Theorem 1 in the main paper. Theorem 9 (Local convergence of projected gradient descent). Recall \u2206t := U t \u2212Ua,\u2217. Suppose Assumption 1 and 2 hold, and assume \u2225y\u0303 \u2212 y\u2217\u2225 \u2264 \u03b4 for some \u03b4 > 0. Let I = (4\u03b1\u03bb)\u22121, \u03b7 = 1 + \u03b1R1, R1 = 2C2(n+ \u221a mp log n) + 2L+ 2/C1\u0398 2 max + 12n\u03b2. Choose (L, \u03b2) such that:\nL > ( \u221a n+ \u221a p+ \u221a 2 log n)2,\n2(\u03bb\u2212 L) n \u2264 \u03b2 \u2264 C1an\u0398 2 min 12rn2a\u03043 ,\nand the step size \u03b1 < \u03b2\u0303 2(2C2(n+ \u221a mp logn)+2L+12n\u03b2)2\n, \u03b2\u0303 = min{2[L \u2212 ( \u221a n + \u221a p +\n\u221a 2 log n)2],\u22122(\u03bb \u2212 L) + \u03b2n}. If a\u0304 \u2264 ca, c > 0, then with probability at least 1 \u2212 C4n\u2212C3 , we have that for any t \u2265 I ,\nU t \u2208 Gm and \u2225U t+1 \u2212 U\u0303Qt+1\u2225F \u2264 \u03b3\u2225U t \u2212 U\u0303Qt\u2225F ,\nprovided that\n\u2225\u22060S(a)c\u2225\u221e \u2264 C1an\u0398\n2 min\n32\u03bb , \u2225\u22060\u2225F \u2264\n1\n\u03b7I min { \u03f5, 1\u2212 \u03b3 2 \u03f51, 1\u2212 \u03b3 2 3 4 a \u221a n } ,\nwhere \u03b32 = (1\u2212 \u03b1\u03b2\u0303/2), \u03f5 = min {\n\u03b2\u0303 36\u03b2n ,\n(\u03bb\u2212L) \u221a K\n2[ \u221a KR1+2(\u03bb\u2212L) \u221a K+n\u03982max]\n} , \u03f51 :=\nmin  C1an\u03982min64K(\u0398max\u221a2K log n+ C5(p+ log2 n)) , \u221a C1an\u03982min 96\u03b2ra\u0304n\u0304 , C1an\u0398 2 min 96\u03b2ra\u03042n\u0304 \u221a n  . C1, C2, C3, C4, C5 are some constants."
        },
        {
            "heading": "D.5 PROOF OF LEMMAS",
            "text": "Lemma 1 (Smoothness condition).\nIf \u2225U \u2212 U\u2217Q\u0303\u2225F \u2264 1, for some Q\u0303 \u2208 Fm, then\n\u2225\u2207f(U)\u2212\u2207f(U\u2217Q\u0303)\u2225F \u2264 R1\u2225U \u2212 U\u2217Q\u0303\u2225F ,\nwhere R1 = 2C2(n + \u221a mp log n) + 2L + 12n\u03b2 if U \u2208 Gm; R1 = 2C2(n + \u221a mp log n) + 2/C1\u0398 2 max+2L+12n\u03b2, for some constant C1, C2 and general U \u2208 \u2126, where \u03982max is the maximum squared separation.\nProof. By definition we have \u2207f(U)\u2212\u2207f(U\u2217Q\u0303) =\n(2A+2L\u00b7Id+y1Tn+1nyT )(U\u2212U\u2217Q\u0303)+\u03b2[1n1Tn (UUT\u2212U\u2217Q\u0303(U\u2217Q\u0303)T )+(UUT\u2212U\u2217Q\u0303(U\u2217Q\u0303)T )1n1Tn ]U.\nNote that\n\u2225U\u2225op \u2264 \u2225U \u2212 U\u2217Q\u0303\u2225op + \u2225U\u2217Q\u0303\u2225op \u2264 \u2225U \u2212 U\u2217Q\u0303\u2225F + \u2225U\u2217Q\u0303\u2225op \u2264 1 + 1 = 2.\nHence\n\u2225(UUT \u2212 U\u2217Q\u0303(U\u2217Q\u0303)T )1n1TnU\u2225F \u2264 \u2225(UUT \u2212 U\u2217Q\u0303(U\u2217Q\u0303)T )\u2225op\u2225U\u2225op\u22251n1Tn\u2225F \u2264 2n\u2225U(U \u2212 U\u2217Q\u0303)T + (U \u2212 U\u2217Q\u0303)(U\u2217Q\u0303)T \u2225F \u2264 6n\u2225U \u2212 U\u2217Q\u0303\u2225F .\nFrom Corollary 1 we have for U \u2208 \u2126,\n\u2225\u2207f(U)\u2212\u2207f(U\u2217Q\u0303)\u2225F \u2264 (\u2225(2A+ y1Tn + 1nyT )\u2225F + 2L)\u2225(U \u2212 U\u2217Q\u0303)\u2225F + 2\u03b2\u2225(UUT \u2212 U\u2217Q\u0303(U\u2217Q\u0303)T )\u2225op\u2225U\u2225op\u22251n1Tn\u2225F \u2264 (2C2(n+ \u221a mp log n) + 2/C1\u0398 2 max + 2L)\u2225U \u2212 U\u2217Q\u0303\u2225F + 12n\u03b2\u2225U \u2212 U\u2217Q\u0303\u2225F\n\u2264 (2C2(n+ \u221a mp log n) + 2/C1\u0398 2 max + 2L+ 12n\u03b2)\u2225U \u2212 U\u2217Q\u0303\u2225F .\nFor U \u2208 Gm,\n\u2225\u2207f(U)\u2212\u2207f(U\u2217Q\u0303)\u2225F \u2264 (\u2225(2A+ y1Tn + 1nyT )\u2225F + 2L)\u2225(U \u2212 U\u2217Q\u0303)\u2225F + 2\u03b2\u2225(UUT \u2212 U\u2217Q\u0303(U\u2217Q\u0303)T )\u2225op\u2225U\u2225op\u22251n1Tn\u2225F \u2264 (2C2(n+ \u221a mp log n) + 2L)\u2225U \u2212 U\u2217Q\u0303\u2225F + 12n\u03b2\u2225U \u2212 U\u2217Q\u0303\u2225F\n\u2264 (2C2(n+ \u221a mp log n) + 2L+ 12n\u03b2)\u2225U \u2212 U\u2217Q\u0303\u2225F .\n\u25a0\nLemma 2 (Local contraction to the ball). Denote \u03f5c := (\u03bb\u2212L) \u221a K\n2[ \u221a KR1+2(\u03bb\u2212L) \u221a K+n\u03982max] . Here R1 =\n2C2(n+ \u221a p log n) + 2L+ 2/C1\u0398 2 max + 12n\u03b2. Then\n\u2225U \u2212 U\u2217Q\u2225F \u2264 \u03f5c =\u21d2 \u27e8\u2207f(U), U\u27e9 < \u2212(\u03bb\u2212 L)K/2 < 0.\nFurthermore, if \u03b1 > 0,\n\u2225\u03a0+(U \u2212 \u03b1\u2207f(U))\u22252F > K, \u03a0\u2126(U \u2212 \u03b1\u2207f(U)) = \u03a0C(\u2126)(U \u2212 \u03b1\u2207f(U)),\nfor some constant C1, C2.\nProof. Let [U ]S to be the matrix that keeps positive entries of U and set the nonpositive ones to zero, i.e.,\n\u2207f(U) = [\u2207f(U)]S + [\u2207f(U)]Sc .\nNote that\n\u2225[\u2207f(U\u2217))]S\u2225F = \u2225 \u2212 2(\u03bb\u2212 L)[U\u2217]S\u2225F = 2(\u03bb\u2212 L) \u221a K, \u27e8\u2207f(U\u2217Q), U\u2217Q\u27e9 = \u22122(\u03bb\u2212 L)K.\nFrom Proposition 5, we have \u2225[\u2207f(U\u2217))]Sc\u2225F \u2264 n\u03982max.\nThen by Lemma 1 we have\n|\u27e8\u2207f(U), U\u27e9 \u2212 \u27e8\u2207f(U\u2217Q), U\u2217Q\u27e9| = |\u27e8\u2207f(U)\u2212\u2207f(U\u2217Q), U\u27e9+ \u27e8\u2207f(U\u2217Q), U \u2212 U\u2217Q\u27e9| \u2264 \u2225\u2207f(U)\u2212\u2207f(U\u2217Q)\u2225F \u2225U\u2225F + \u2225\u2207f(U\u2217Q)\u2225F \u2225U \u2212 U\u2217Q\u2225F = \u2225\u2207f(U)\u2212\u2207f(U\u2217Q)\u2225F \u2225U\u2225F + \u2225\u2207f(U\u2217)Q\u2225F \u2225U \u2212 U\u2217Q\u2225F \u2264 [ \u221a KR1 + 2(\u03bb\u2212 L) \u221a K + n\u03982max]\u2225U \u2212 U\u2217Q\u2225F .\nFinally we have \u27e8\u2207f(U), U\u27e9 < \u2212(\u03bb\u2212 L)K < 0, given \u03f5c \u2264 (\u03bb\u2212L) \u221a K\n2[ \u221a KR1+2(\u03bb\u2212L) \u221a K+n\u03982max]\n. Here R1 = 2C2(n+ \u221a p log n)+2L+2/C1\u0398 2 max+12n\u03b2.\nNote the fact\n\u03a0\u2126(U) = \u03a0Snr\u22121( \u221a K)(\u03a0+(U)), \u03a0C(\u2126)(U) = \u03a0Bnr( \u221a K)(\u03a0+(U)),\nwhere \u03a0+(U) stands for the projection of U on to the space of matrices with positive entries, Snr\u22121( \u221a K) stands for the sphere with radius \u221a K, Bnr( \u221a K) stands for the ball with radius \u221a K. Thus we only need to show that\n\u2225\u03a0+(U \u2212 \u03b1\u2207f(U))\u22252F \u2265 K.\nDefine U\u0303i,j = \u03b1[\u2207f(U)]i,j , if [U \u2212 \u03b1\u2207f(U)]i,j > 0. And U\u0303i,j = Ui,j otherwise. Then we have\nU \u2212 U\u0303 = \u03a0+(U \u2212 U\u0303) = \u03a0+(U \u2212 \u03b1\u2207f(U))."
        },
        {
            "heading": "Moreover, if [U \u2212 \u03b1\u2207f(U)]i,j \u2264 0, then",
            "text": "[\u03b1\u2207f(U)]i,j \u2265 Ui,j \u2265 0. Hence \u27e8U, U\u0303\u27e9 \u2264 \u03b1\u27e8\u2207f(U), U\u27e9 < 0, which implies that\n\u2225\u03a0+(U \u2212 \u03b1\u2207f(U))\u22252F = \u2225U \u2212 U\u0303\u22252F > \u2225U\u22252F = K. \u25a0\nLemma 3 (One-step shrinkage towards block form). Denote \u2206 = U \u2212Ua,\u2217, where Ua,\u2217 is defind as (17). Choose (L, \u03b2) such that:\n2(\u03bb\u2212 L) n \u2264 \u03b2 \u2264 C1an\u0398 2 min 12rn2a\u03043 .\nSuppose \u2225\u2206\u2225F \u2264 \u03f5c, where \u03f5c is defined in Lemma 2. Assume\n\u2225\u2206S(a)c\u2225\u221e \u2264 C1an\u0398\n2 min\n32\u03bb , \u2225\u2206\u2225F \u2264 min\n{ \u03f51, 3\n4 a \u221a n\n} , (21)\nwhere \u03f51 :=\nmin  C1an\u03982min64K(\u0398max\u221a2K log n+ C5(p+ log2 n)) , \u221a C1an\u03982min 96\u03b2ra\u0304n\u0304 , C1an\u0398 2 min 96\u03b2ra\u03042n\u0304 \u221a n  , Then we have\n[U ]i,\u03c4 \u2212 \u03b1[\u2207f(U)]i,\u03c4 \u2264 [U ]i,\u03c4 \u2212 \u03b1 C1an\u0398\n2 min\n8 ,\u2200i \u2208 Gl, s(k \u2212 1) < \u03c4 \u2264 sk, l \u0338= k.\nFurthermore,\n[V ]i,\u03c4 \u2264 max{[U ]i,\u03c4 \u2212 \u03b1 C1an\u0398\n2 min\n8 , 0},\u2200i \u2208 Gl, s(k \u2212 1) < \u03c4 \u2264 sk, l \u0338= k.\nwhere V = \u03a0\u2126(W ), W = U \u2212 \u03b1\u2207f(U), for some constant C1, C5. Consequently, after at most I = 14\u03b1\u03bb iterations, U\nt will enter Phase 2, i.e.,U I \u2208 Gm if for every step t, U t meets the above conditions (21).\nProof. Without loss of generality and for notation simplicity, we assume m1 = \u00b7 \u00b7 \u00b7 = mK = s = r/K, ak,1 = \u00b7 \u00b7 \u00b7 = ak,mk = ak,\u2200k. By definition we have\n\u2207f(U) = (2A+ 2L \u00b7 Id + y1Tn + 1nyT )U + \u03b2[1n(UUT1n \u2212 1n)T + (UUT1n \u2212 1n)1Tn ]U.\nNow suppose \u2206 = U \u2212 Ua,\u2217,\u2206 = [d1, . . . dr]. Then \u2200i \u2208 Gl, l \u0338= 1.\n[\u2207f(U)]i,1 = 2L \u00b7 (d1)i + eTi (2A+ y1Tn + 1nyT )a1,11G1 + eTi (2A+ y1Tn + 1nyT )d1.\nFrom Proposition 5 we know\neTi (2A+ y1 T n + 1ny T )a1,11G1 = a1,1D1,l(i) \u2265 C1a1,1n\u03982min.\nThe goal of the rest of the proof is to show that eTi (2A+ y1 T n + 1ny T )a1,11G1 is the dominant term in [\u2207f(U)]i,1; the rest of the terms can be absorbed given that U is located within some neighborhood of the optimum point Ua,\u2217. From calculation we have\n[\u2207f(U)]i,1 = 2L \u00b7 (d1)i + eTi (2A+ y1Tn + 1nyT )a1,11G1 + eTi (2A+ y1Tn + 1nyT )d1 + \u03b2eTi [1n(UU\nT1n \u2212 1n)T + (UUT1n \u2212 1n)1Tn ]a1,11G1 + r1 = 2L \u00b7 (d1)i + a1,1D1,l(i) + eTi (2A+ y1Tn + 1nyT )d1 + \u03b2eTi [1n(UU T1n \u2212 1n)T + (UUT1n \u2212 1n)1Tn ]a1,11G1 + r1,\nwhere\nr1 = \u03b2e T i [1n(UU T1n \u2212 1n)T + (UUT1n \u2212 1n)1Tn ]d1.\nFurther orthogonally decompose d\u03c4 = \u2211\nk xk,\u03c41Gk + w\u03c4 , for xk,\u03c4 \u2208 R, \u03c4 = 1, . . . , r, w\u03c4 \u2208 \u0393\u22a5K . Then we have\neTi (2A+ y1 T n + 1ny T )d1 = \u2211 k xk,1Dk,l(i) + e T i (2A+ y1 T n + 1ny T )w1\n\u2265 x1,1D1,l(i)\u2212 2\u03bb \u00b7 xl,1 + eTi (2A+ y1Tn + 1nyT )w1,\nsince xk,1 \u2265 0, \u2200k \u0338= 1, Dl,l = \u22122\u03bbnl. Recall X = [X1, . . . , Xn], Xi = \u03b5i + \u00b5l, \u2200i \u2208 Gl. Then from high dimensional bound of Gaussian distributions we have\n|eTi (2A+ y1Tn + 1nyT )w1| = |2[Aw1]i \u2212 \u2211 k 2 nk 1TnkAGk,GkwGk |\n= \u2223\u2223\u2223\u2223\u2223\u22232 \u2211 k [(\u00b5l \u2212 \u00b5k) + (\u03b5i \u2212 \u03b5\u0304k)]T \u2211 j\u2208Gk \u03b5j(w1)j \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 2K(\u0398max \u221a 2K log n+ C5(p+ log 2 n))\u2225w1\u2225,\nwith probability \u2265 1\u2212 C4/n, for some constant C4, C5 > 0. On the other hand, \u03b2eTi [1n(UU\nT1n \u2212 1n)T + (UUT1n \u2212 1n)1Tn ]a11G1 = \u03b2a1e T i [1n1 T n ((U a,\u2217)\u2206T +\u2206(Ua,\u2217)T ) + ((Ua,\u2217)\u2206T +\u2206(Ua,\u2217)T )1n1 T n ]1G1\n= \u03b2a1[1 T n (U a,\u2217\u2206T +\u2206(Ua,\u2217)T )1G1 + n1e T i (U a,\u2217\u2206T +\u2206(Ua,\u2217)T )1n]\n= \u03b2a1 r\u2211 \u03c4=1 [1Tn (u 0 \u03c4d T \u03c4 + d\u03c4 (u 0 \u03c4 ) T )1G1 + n1e T i (u 0 \u03c4d T \u03c4 + d\u03c4 (u 0 \u03c4 ) T )1n]\n= \u03b2a1[( K\u2211 g=1 ng sg\u2211 \u03c4=s(g\u22121)+1 agd T \u03c4 1G1) + a1n1 s\u2211 \u03c4=1 1Tnd\u03c4\n+ n1 sl\u2211 \u03c4=s(l\u22121)+1 al1 T nd\u03c4 + n1( K\u2211 g=1 ng sg\u2211 \u03c4=s(g\u22121)+1 ag(d\u03c4 )i)]\n\u2265 \u03b2a1[2ra1n1 \u221a n(\u2212\u2225\u2206\u2225F ) + rn1a\u0304 \u221a nl(\u2212\u2225\u2206\u2225F )\n+ n1(a1n1(d1)i + a\u0304nl sl\u2211 \u03c4=s(l\u22121)+1 (d\u03c4 )i))]\n\u2265 \u03b2a1,1[3ra\u0304n1 \u221a n(\u2212\u2225\u2206\u2225F ) + a1n21(d1)i \u2212 a\u0304n1nlra\u0304].\nSimilarly we can show that |r1| \u2264 \u03b2ra\u0304(3n2a\u03042 + n\u0304\u2225\u2206\u22252F ). Hence [\u2207f(U)]i,1 \u2265 2L \u00b7 (d1)i + (a1,1 + x1,1)D1,l(i)\u2212 2\u03bb\u2225\u2206Sc\u2225\u221e\n\u2212 2K(\u0398max \u221a 2K log n+ C5(p+ log 2 n))\u2225w1\u2225 + \u03b2a1,1[3ra\u0304n1 \u221a n(\u2212\u2225\u2206\u2225F ) + a1,1n21(d1)i \u2212 a\u0304n1nlra\u0304] + r1\n\u2265 (2L+ \u03b2a21,1n21)(d1)i + C1an\u0398\n2 min\n8 ,\nprovided that \u2206 : x1,1 \u2265 \u22123/4a1,\n\u2225\u2206S(a)c\u2225\u221e \u2264 C1an\u0398\n2 min\n32\u03bb := \u03f5f1 , \u2225\u2206\u2225F \u2264 \u03f51.\nHere \u03f51 :=\nmin  C1an\u03982min64K(\u0398max\u221a2K log n+ C5(p+ log2 n)) , \u221a C1an\u03982min 96\u03b2ra\u0304n\u0304 , C1an\u0398 2 min 96\u03b2ra\u03042n\u0304 \u221a n  . It is also sufficient to assume\n\u2225\u2206S(a)c\u2225\u221e \u2264 \u03f5f1 , \u2225\u2206\u2225F \u2264 min { \u03f51, 3\n4 a \u221a n\n} .\nThe (L, \u03b2) pair need to satisfy:\n2(\u03bb\u2212 L) n \u2264 \u03b2 \u2264 C1an\u0398 2 min 12rn2a\u03043 .\nThus,\n[U ]i,1 \u2212 \u03b1[\u2207f(U)]i,1 \u2264 [U ]i,1 \u2212 \u03b1 C1an\u0398\n2 min\n8 ,\u2200i \u2208 Gl, l \u0338= 1.\nFrom Lemma 2 we know that \u2225W\u2225 = \u2225U \u2212 \u03b1\u2207f(U)\u2225 \u2265 \u221a K. Hence \u2200i \u2208 Gl, s(k \u2212 1) < \u03c4 \u2264 sk, l \u0338= k,\n[V ]i,1 =\n\u221a K \u2225W\u2225 \u03a0+([U ]i,1 \u2212 \u03b1[\u2207f(U)]i,1) \u2264 max{[U ]i,\u03c4 \u2212 \u03b1 C1an\u0398 2 min 8 , 0}.\nNote that the initialization U0 needs to satisfy \u2225\u22060S(a)c\u2225\u221e \u2264 C1an\u0398\n2 min\n32\u03bb from the above argument, where \u22060 = U0 \u2212 Ua,\u2217. Then it takes at most I total steps for U t to converge to the block form Gm, where\nI = ( C1an\u0398\n2 min\n32\u03bb )/(\u03b1\nC1an\u0398 2 min\n8 ) = 1/(4\u03b1\u03bb).\n\u25a0\nLemma 4 (Inflation of distance to Ua,\u2217 for Phase 1). If \u2225U \u2212 U\u2217Q\u2225F \u2264 \u03f5c, then\n\u2225V \u2212 Ua,\u2217\u2225F \u2264 \u03b7\u2225U \u2212 Ua,\u2217\u2225F ,\nwhere \u03b7 = 1 + \u03b1R1, R1 = 2C2(n + \u221a mp log n) + 2/C1\u0398 2 max + 2L + 12n\u03b2, for some constant C1, C2.\nProof. Suppose \u2225U \u2212 U\u2217Q\u2225F \u2264 \u03f5c, then from Lemma 2 we have\n\u2225V \u2212 Ua,\u2217\u2225F = \u2225\u03a0\u2126(U \u2212 \u03b1\u2207f(U))\u2212\u03a0\u2126(Ua,\u2217 \u2212 \u03b1\u2207f(Ua,\u2217))\u2225F = \u2225\u03a0C(\u2126)(U \u2212 \u03b1\u2207f(U))\u2212\u03a0C(\u2126)(Ua,\u2217 \u2212 \u03b1\u2207f(Ua,\u2217))\u2225F \u2264 \u2225(U \u2212 \u03b1\u2207f(U))\u2212 (Ua,\u2217 \u2212 \u03b1\u2207f(Ua,\u2217))\u2225F \u2264 (1 + \u03b1R1)\u2225U \u2212 Ua,\u2217\u2225F ,\nwhere R1 = 2C2(n+ \u221a mp log n) + 2/C1\u0398 2 max + 2L+ 12n\u03b2. \u25a0\nLemma 5 (Local strong convexity).\nIf U \u2208 Gm, define \u2206 := UQT \u2212 U\u2217, then\n\u27e8\u22072f(U\u2217)[\u2206],\u2206\u27e9 \u2265 \u03b2\u0303\u2225\u2206\u22252F ,\nwhere \u03b2\u0303 = min{2[L \u2212 ( \u221a n + \u221a p + \u221a 2 log n)2],\u22122(\u03bb \u2212 L) + \u03b2n} > 0, provided that L \u2208 (( \u221a n+ \u221a p+ \u221a 2 log n)2, \u03bb), \u03b2 > 2(\u03bb\u2212 L)/n. Furthermore, \u2200U s.t. \u2225U \u2212 U\u2217\u2225F < \u03f5s, we have\n\u27e8\u22072f(U)[\u2206],\u2206\u27e9 \u2265 \u03b2\u0303/2\u2225\u2206\u22252F ,\nwhere \u03f5s = \u03b2\u030336\u03b2n .\nProof. U \u2208 Gm, Q \u2208 Fm =\u21d2 U = W1 +W2, with\nW1 :=  d1,11n1 , . . . , d1,s1n1 0 \u00b7 \u00b7 \u00b7 0 0 d2,11n2 , . . . , d2,s1n2 \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 dK,11nK , . . . , dK,s1nK\n ,\nW2 :=  w1,1, . . . ,w1,s 0 \u00b7 \u00b7 \u00b7 0 0 w2,1, . . . ,w2,s \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 wK,1, . . . ,wK,s  , for some dk,i \u2208 R,wk,i \u2208 Rnk , \u27e8wk,i,1nk\u27e9 = 0,\u2200k \u2208 [K], i \u2208 [s]. Note that\nU\u2217Q :=  a\u03031,11n1 , . . . , a\u03031,s1n1 0 \u00b7 \u00b7 \u00b7 0 0 a\u03032,11n2 , . . . , a\u03032,s1n2 \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 a\u0303K,11nK , . . . , a\u0303K,s1nK  , for some a\u0303k,i \u2208 R, k \u2208 [K], i \u2208 [s]. Recall\nQ = argminQ\u0303\u2208Fm\u2225U \u2212 U \u2217Q\u0303\u2225F ,\nthen there exist ck > 0, k \u2208 [K], s.t.\ndk,i = cka\u0303k,i, \u2200k \u2208 [K], i \u2208 [s].\nHence W1QT = [W 11 |On\u00d7(r\u2212K)], where\nW 11 :=  c1\u221a n1 1n1 0 \u00b7 \u00b7 \u00b7 0 0 c2\u221an21n2 \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 cK\u221anK 1nK  . And W2QT = [W 12 |W 22 ], where\nW 12 :=  w1 0 \u00b7 \u00b7 \u00b7 0 0 w2 \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 wK  , for some wk \u2208 Rnk , k \u2208 [K]. And every column of W 22 belongs to the space \u0393K := span{1Gk : k \u2208 [K]}\u22a5, which is the orthogonal complement of the linear subspace of Rn spanned by the vectors 1G1 , . . . ,1GK . i.e.,\nUQT \u2212 U\u2217 = [W 11 +W 12 \u2212 U\u2217|W 22 ].\nDenote \u22061 = W 11 +W 1 2 \u2212 U\u2217, \u22062 = W 22 , \u2206 = [\u22061|\u22062]. First we will show that\n\u2225\u22061(U\u2217)T1n + U\u2217\u2206T1 1n\u22252F \u2265 n\u2225\u22061\u22252F .\nDenote\nU\u22171 =  1\u221a n1 1n1 0 \u00b7 \u00b7 \u00b7 0 0 1\u221an21n2 \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 1\u221anK 1nK\n , \u22061 =  d1 0 \u00b7 \u00b7 \u00b7 0 0 d2 \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 dK  , where dk \u2208 Rnk . Then we have\n\u22061(U \u2217 1 ) T1n + U \u2217 1\u2206 T 1 1n =  d\u03031 d\u03032 ...\nd\u0303K  , where\nd\u0303k = \u221a nkdk + 1 \u221a nk 1nk1 T nk dk.\nNote that\n\u2225d\u0303k\u22252 \u2265 \u03bb2min ( \u221a nkIdnk +\n1 \u221a nk 1nk1 T nk\n) \u2225dk\u22252 \u2265 nk\u2225dk\u22252.\nHence\n\u2225\u22061(U\u22171 )T1n + U\u22171\u2206T1 1n\u22252F = \u2211 k \u2225d\u0303k\u22252\n\u2265 min k {nk} \u2211 k \u2225dk\u22252.\nBy calculating the Hessian at U\u2217 we get\n\u27e8\u22072f(U\u2217)[\u2206],\u2206\u27e9 = \u27e82(L \u00b7 Idn +A) + y1Tn + 1nyT ,\u2206\u2206T \u27e9+ \u03b2\u2225\u2206(U\u2217)T1n + U\u2217\u2206T1n\u22252F = \u27e82(L \u00b7 Idn +A) + y1Tn + 1nyT ,\u22062\u2206T2 \u27e9 + \u27e82(L \u00b7 Idn +A) + y1Tn + 1nyT ,\u22061\u2206T1 \u27e9+ \u03b2\u2225\u22061(U\u22171 )T1n + U\u22171\u2206T1 1n\u22252F = 2\u27e8(L \u00b7 Idn +A),\u22062\u2206T2 \u27e9+ \u27e82(L \u00b7 Idn +A) + y1Tn + 1nyT ,\u22061\u2206T1 \u27e9 + \u03b2\u2225\u22061(U\u2217)T1n + U\u2217\u2206T1 1n\u22252F \u2265 2[L\u2212 ( \u221a n+ \u221a p+ \u221a 2 log n)2]\u2225\u22062\u22252F +R2\u2225\u22061\u22252F + \u03b2n\u2225\u22061\u22252F\n\u2265 min{2[L\u2212 ( \u221a n+ \u221a p+ \u221a 2 log n)2], R2 + \u03b2n}\u2225\u2206\u22252F\n= \u03b2\u0303\u2225\u2206\u22252F ,\nwith probability\u2265 1\u22121/n, where \u27e8A,\u22062\u2206T2 \u27e9 \u2265 \u2212( \u221a n+ \u221a p+ \u221a 2 log n)2\u2225\u22062\u22252F by Proposition 3;\nR2 = \u22122(\u03bb\u2212L) by Proposition 1. In particular, by choosing L \u2208 (( \u221a n+ \u221a p+ \u221a 2 log n)2, \u03bb), \u03b2 > \u2212R2/n, we have \u03b2\u0303 > 0. Recall the Hessian of f at U\u2217 and U\n\u27e8\u22072f(U\u2217)[\u2206],\u2206\u27e9 = \u27e82(L \u00b7 Idn +A) + y1Tn + 1nyT ,\u2206\u2206T \u27e9+ \u03b2\u2225\u2206(U\u2217)T1n + U\u2217\u2206T1n\u22252F ,\n\u27e8\u22072f(U)[\u2206],\u2206\u27e9 = \u27e82(L \u00b7 Idn +A) + y1Tn + 1nyT ,\u2206\u2206T \u27e9+ \u03b2\u2225\u2206(U)T1n + U\u2206T1n\u22252F + \u03b2\u27e81n1Tn (UUT \u2212 (U\u2217)(U\u2217)T ) + (UUT \u2212 (U\u2217)(U\u2217)T )1n1Tn ,\u2206\u2206T \u27e9.\nThen we have\n|\u27e8\u22072f(U\u2217)[\u2206],\u2206\u27e9 \u2212 \u27e8\u22072f(U)[\u2206],\u2206\u27e9| \u2264 \u03b2(\u2225\u2206(U)T1n + U\u2206T1n\u22252F \u2212 \u2225\u2206(U\u2217)T1n + U\u2217\u2206T1n\u22252F ) + \u03b2|\u27e81n1Tn (UUT \u2212 (U\u2217)(U\u2217)T ) + (UUT \u2212 (U\u2217)(U\u2217)T )1n1Tn ,\u2206\u2206T \u27e9| \u2264 \u03b2(12n\u2225U \u2212 U\u2217\u2225F + 6n\u2225U \u2212 U\u2217\u2225F ) = 18\u03b2n\u2225U \u2212 U\u2217\u2225F .\nHence \u27e8\u22072f(U)[\u2206],\u2206\u27e9 \u2265 \u03b2\u0303/2\u2225\u2206\u22252F ,\nprovided that \u2225U \u2212 U\u2217\u2225F \u2264 \u03b2\u030336\u03b2n . \u25a0\nLemma 6 (Local exponential convergence).\nIf U \u2208 Gm, define \u2206 := UQT \u2212 U\u2217, Then \u2203\u03b3 \u2208 (0, 1), \u03f5 > 0, s.t.,\n\u2225V \u2212 U\u2217Q\u2225F \u2264 \u03b3\u2225U \u2212 U\u2217Q\u2225F ,\nwhere Q = argminQ\u0303\u2208Fm\u2225U \u2212 U \u2217Q\u0303\u2225F ,\nprovided that \u2225U \u2212 U\u2217Q\u2225F < \u03f5, where \u03f5 = min{\u03f5c, \u03f5s}. Recall that \u03f5c is defined in Lemma 2, \u03f5s is defined in Lemma 5. In particular, if we we choose the step size \u03b1 \u2264 \u03b2\u0303/(2R22),, where R2 = 2C2(n+ \u221a mp log n) + 2L+ 12n\u03b2 for some constant C2. Then the contraction factor would be \u03b32 = (1\u2212 \u03b1\u03b2\u0303/2). Proof. By Lemma 2 we have\n\u03a0\u2126(U \u2212 \u03b1\u2207f(U)) = \u03a0C(\u2126)(U \u2212 \u03b1\u2207f(U)),\nwhere C(\u2126) stands for the convex hall of \u2126. It is known that\n\u2225\u03a0C(v)\u2212\u03a0C(u)\u2225 \u2264 \u2225v \u2212 u\u2225,\nfor any convex set C. Note \u2200Q\u0303 \u2208 Fm, U\u2217Q\u0303 is a stationary point for \u03a0\u2126. Then we have\n\u2225V \u2212 U\u2217Q\u22252F = \u2225\u03a0\u2126(U \u2212 \u03b1\u2207f(U))\u2212\u03a0\u2126(U\u2217Q\u2212 \u03b1\u2207f(U\u2217Q))\u22252F = \u2225\u03a0C(\u2126)(U \u2212 \u03b1\u2207f(U))\u2212\u03a0C(\u2126)(U\u2217Q\u2212 \u03b1\u2207f(U\u2217Q))\u22252F \u2264 \u2225(U \u2212 \u03b1\u2207f(U))\u2212 (U\u2217Q\u2212 \u03b1\u2207f(U\u2217Q))\u22252F = \u2225(UQT \u2212 \u03b1\u2207f(UQT ))\u2212 (U\u2217 \u2212 \u03b1\u2207f(U\u2217))\u22252F = \u2225UQT \u2212 U\u2217\u22252F + \u03b12\u2225\u2207f(UQT ))\u2212\u2207f(U\u2217))\u22252F \u2212 2\u03b1\u27e8\u2207f(UQT )\u2212\u2207f(U\u2217), UQT \u2212 U\u2217\u27e9,\nsince \u2207f(U\u0303Q\u0303) = \u2207f(U\u0303)Q\u0303, \u2200Q\u0303 \u2208 Or\u00d7r, U\u0303 \u2208 Rn\u00d7r. Note that \u2225UQT \u2212 U\u2217\u22252F = \u2225U \u2212 U\u2217Q\u22252F , \u2225\u2207f(UQT ) \u2212 \u2207f(U\u2217))\u2225F \u2264 R2\u2225UQT \u2212 U\u2217\u2225F , for some constant R2, so we only need to analyze the last term. And by MVT we have\n\u27e8\u2207f(UQT )\u2212\u2207f(U\u2217), UQT \u2212U\u2217\u27e9 = \u222b 1 0 \u27e8\u22072f(U\u2217+\u03c4(UQT \u2212U\u2217))[UQT \u2212U\u2217], UQT \u2212U\u2217\u27e9d\u03c4.\nNotice \u2225UQT \u2212 U\u2217\u2225F < \u03f5s, from Lemma 5 we have \u2200\u03c4 \u2208 (0, 1),\n\u27e8\u22072f(U\u2217 + \u03c4(UQT \u2212 U\u2217))[\u2206],\u2206\u27e9 \u2265 \u03b2\u0303/2\u2225\u2206\u22252F .\nHence\n\u27e8\u2207f(UQT )\u2212\u2207f(U\u2217), UQT \u2212 U\u2217\u27e9\n= \u222b 1 0 \u27e8\u22072f(U\u2217 + \u03c4(UQT \u2212 U\u2217))[UQT \u2212 U\u2217], UQT \u2212 U\u2217\u27e9d\u03c4 \u2265 \u03b2\u0303/2\u2225UQT \u2212 U\u2217\u22252F .\nThen we have\n\u2225V \u2212 U\u2217Q\u22252F = \u2225UQT \u2212 U\u2217\u22252F + \u03b12\u2225\u2207f(UQT ))\u2212\u2207f(U\u2217))\u22252F \u2212 2\u03b1\u27e8\u2207f(UQT )\u2212\u2207f(U\u2217), UQT \u2212 U\u2217\u27e9 \u2264 \u2225UQT \u2212 U\u2217\u22252F +R22\u03b12\u2225UQT \u2212 U\u2217\u22252F \u2212 2\u03b1\u03b2\u0303/2\u2225UQT \u2212 U\u2217\u22252F \u2264 (1\u2212 \u03b1\u03b2\u0303/2)\u2225UQT \u2212 U\u2217\u22252F = \u03b32\u2225U \u2212 U\u2217Q\u22252F ,\nby choosing \u03b1 \u2264 \u03b2\u0303/(2R22), \u03b32 = (1\u2212 \u03b1\u03b2\u0303/2). \u25a0\nLemma 7 (Iterations remain staying in Phase 2). Suppose Lemma 6 holds and U0 \u2208 Gm. Denote \u03f5\u0304 := min { \u03f51, 3 4a \u221a n } , we have U t \u2208 Gm, \u2200t \u2265 1, where\nU t+1 = \u03a0\u2126(U t \u2212 \u03b1\u2207f(U t)), \u2200t \u2265 1,\ngiven \u2225U0 \u2212 Ua,\u2217\u2225F \u2264 min{(1\u2212 \u03b3)/2\u03f5\u0304, \u03f5}, recall \u03f5 = min{\u03f5c, \u03f5s}. Proof. From the convergence of Phase 1 (Theorem 4) we know U t+1 \u2208 Gm if U t \u2208 Gm and \u2225U t \u2212 Ua,\u2217\u2225F \u2264 \u03f5\u0304. Thus from definition we know U1 \u2208 Gm. Recall that we define Qt \u2208 Fm as\nQt = argminQ\u2208Fm\u2225U t \u2212 U\u2217Q\u2225F .\nDefine \u03f50 := (1\u2212 \u03b3)/2 \u00b7 \u03f5\u0304. Note that U0 \u2208 Gm, \u2225U0 \u2212Ua,\u2217\u2225F \u2264 min{(1\u2212 \u03b3)/2 \u00b7 \u03f5\u0304, \u03f5}, then from the previous lemma (Lemma 6) we have\n\u2225U1 \u2212 U\u2217Q0\u2225F \u2264 \u03b3\u2225U0 \u2212 U\u2217Q0\u2225F .\nNote U\u2217Q0 is the projection of U0 on to Fm that located on a sphere, which implies that\n\u2225U\u2217Q0 \u2212 Ua,\u2217\u2225F < 2\u2225U0 \u2212 Ua,\u2217\u2225F ,\ngiven \u2225U0 \u2212 Ua,\u2217\u2225F < 1/2\u2225Ua,\u2217\u2225F . Thus,\n\u2225U1 \u2212 Ua,\u2217\u2225F \u2264 \u2225U1 \u2212 U\u2217Q0\u2225F + \u2225U\u2217Q0 \u2212 Ua,\u2217\u2225F \u2264 \u03b3\u2225U0 \u2212 U\u2217Q0\u2225F + 2\u2225U0 \u2212 Ua,\u2217\u2225F \u2264 \u03b3\u2225U0 \u2212 Ua,\u2217\u2225F + 2\u2225U0 \u2212 Ua,\u2217\u2225F \u2264 \u03b3\u03f50 + 2\u03f50 \u2264 \u03f5\u0304.\nThen we will finish the proof by induction. Suppose U l \u2208 Gm, \u2225U l\u22121 \u2212 Ua,\u2217\u2225F \u2264 \u03f5\u0304, \u2200l \u2264 t, we are going to show that U t+1 \u2208 Gm. By assumption we only need to show \u2225U t \u2212 Ua,\u2217\u2225F \u2264 \u03f5\u0304. From Lemma 6 we have\n\u2225U l \u2212 U\u2217Ql\u2225F \u2264 \u2225U l \u2212 U\u2217Ql\u22121\u2225F \u2264 \u03b3\u2225U l\u22121 \u2212 U\u2217Ql\u22121\u2225F \u2264 \u03f5, \u2200l \u2264 t.\nHence\n\u2225U t \u2212 Ua,\u2217\u2225F \u2264 \u2225U t \u2212 U\u2217Qt\u2225F + \u2225U\u2217Qt \u2212 U\u2217Qt\u22121\u2225F + \u00b7 \u00b7 \u00b7+ \u2225U\u2217Q0 \u2212 Ua,\u2217\u2225F\n\u2264 2 t\u2211\nl=0\n\u2225U l \u2212 U\u2217Ql\u22121\u2225F + 2\u2225U0 \u2212 Ua,\u2217\u2225F\n\u2264 2 t\u2211\nl=0\n\u03b3l\u22121\u2225U0 \u2212 Ua,\u2217\u2225F\n\u2264 \u03f5\u0304.\n\u25a0"
        }
    ],
    "title": "NONNEGATIVE LOW-RANK SEMIDEFINITE PROGRAM- MING",
    "year": 2024
}