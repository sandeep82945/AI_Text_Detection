{
    "abstractText": "Sorting is a fundamental operation of all computer systems, having been a longstanding significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds a non-decreasing condition and differentiability. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jungtaek Kim"
        },
        {
            "affiliations": [],
            "name": "Jeongbeen Yoon"
        },
        {
            "affiliations": [],
            "name": "Minsu Cho"
        }
    ],
    "id": "SP:ee941339616efd2fd1e21f7a576e378eddab228b",
    "references": [
        {
            "authors": [
                "M. Ajtai",
                "J. Koml\u00f3s",
                "E. Szemer\u00e9di"
            ],
            "title": "An O(n log n) sorting network",
            "venue": "In Proceedings of the Annual ACM Symposium on Theory of Computing (STOC),",
            "year": 1983
        },
        {
            "authors": [
                "Y. Bengio",
                "N. L\u00e9onard",
                "A. Courville"
            ],
            "title": "Estimating or propagating gradients through stochastic neurons for conditional computation",
            "venue": "arXiv preprint arXiv:1308.3432,",
            "year": 2013
        },
        {
            "authors": [
                "L. Berrada",
                "A. Zisserman",
                "M.P. Mudigonda"
            ],
            "title": "Smooth loss functions for deep top-k classification",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "M. Blondel",
                "O. Teboul",
                "Q. Berthet",
                "J. Djolonga"
            ],
            "title": "Fast differentiable sorting and ranking",
            "venue": "In Proceedings of the International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Z. Cao",
                "T. Qin",
                "T.-Y. Liu",
                "M.-F. Tsai",
                "H. Li"
            ],
            "title": "Learning to rank: from pairwise approach to listwise approach",
            "venue": "In Proceedings of the International Conference on Machine Learning (ICML),",
            "year": 2007
        },
        {
            "authors": [
                "T.H. Cormen",
                "C.E. Leiserson",
                "R.L. Rivest",
                "C. Stein"
            ],
            "title": "Introduction to algorithms",
            "year": 2022
        },
        {
            "authors": [
                "M. Cuturi"
            ],
            "title": "Sinkhorn distances: lightspeed computation of optimal transport",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2013
        },
        {
            "authors": [
                "M. Cuturi",
                "O. Teboul",
                "J.-P. Vert"
            ],
            "title": "Differentiable ranking and sorting using optimal transport",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "J. Devlin",
                "M.-W. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S. Gelly",
                "J. Uszkoreit",
                "N. Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "A. Grover",
                "E. Wang",
                "A. Zweig",
                "S. Ermon"
            ],
            "title": "Stochastic optimization of sorting networks via continuous relaxations",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Computation,",
            "year": 1997
        },
        {
            "authors": [
                "D.E. Knuth"
            ],
            "title": "The art of computer programming, volume 3. Addison-Wesley Professional, 2 edition",
            "year": 1998
        },
        {
            "authors": [
                "A. Krizhevsky",
                "G.E. Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Technical report,",
            "year": 2009
        },
        {
            "authors": [
                "Y. LeCun",
                "C. Cortes",
                "C.J.C. Burges"
            ],
            "title": "The MNIST database of handwritten digits",
            "venue": "http: //yann.lecun.com/exdb/mnist/,",
            "year": 1998
        },
        {
            "authors": [
                "J. Lee",
                "Y. Lee",
                "J. Kim",
                "A.R. Kosiorek",
                "S. Choi",
                "Y.W. Teh"
            ],
            "title": "Set Transformer: A framework for attention-based permutation-invariant neural networks",
            "venue": "In Proceedings of the International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "T.-Y. Liu"
            ],
            "title": "Learning to rank for information retrieval",
            "venue": "Foundations and Trends\u00ae in Information Retrieval,",
            "year": 2009
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Lin",
                "Y. Cao",
                "H. Hu",
                "Y. Wei",
                "Z. Zhang",
                "S. Lin",
                "B. Guo"
            ],
            "title": "Swin Transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "G.E. Mena",
                "D. Belanger",
                "S. Linderman",
                "J. Snoek"
            ],
            "title": "Learning latent permutations with GumbelSinkhorn networks",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "C. Nash",
                "Y. Ganin",
                "S.M.A. Eslami",
                "P.W. Battaglia"
            ],
            "title": "PolyGen: An autoregressive generative model of 3D meshes",
            "venue": "In Proceedings of the International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Y. Netzer",
                "T. Wang",
                "A. Coates",
                "A. Bissacco",
                "B. Wu",
                "A.Y. Ng"
            ],
            "title": "Reading digits in natural images with unsupervised feature learning",
            "venue": "In Neural Information Processing Systems Workshop on Deep Learning and Unsupervised Feature Learning,",
            "year": 2011
        },
        {
            "authors": [
                "F. Petersen",
                "C. Borgelt",
                "H. Kuehne",
                "O. Deussen"
            ],
            "title": "Differentiable sorting networks for scalable sorting and ranking supervision",
            "venue": "In Proceedings of the International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "F. Petersen",
                "C. Borgelt",
                "H. Kuehne",
                "O. Deussen"
            ],
            "title": "Monotonic differentiable sorting networks",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "I. Sutskever",
                "O. Vinyals",
                "Q.V. Le"
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2014
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2017
        },
        {
            "authors": [
                "O. Vinyals",
                "S. Bengio",
                "M. Kudlur"
            ],
            "title": "Order matters: Sequence to sequence for sets",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR), San Juan,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Traditional sorting algorithms (Cormen et al., 2022), e.g., bubble sort, insertion sort, and quick sort, are a well-established approach to arranging given instances in computer science. Since such a sorting algorithm is a basic component to build diverse computer systems, it has been a longstanding significant research area in science and engineering. Moreover, sorting networks (Knuth, 1998; Ajtai et al., 1983), which are structurally designed as an abstract device with a fixed number of wires, have been widely used to perform a sorting algorithm on computing hardware, where each wire corresponds to a connection for a single swap operation.\nGiven an unordered sequence of n elements s = [s1, . . . , sn] \u2208 Rn, the problem of sorting is defined to find a permutation matrix P \u2208 {0, 1}n\u00d7n that transforms s into an ordered sequence so:\nso = P \u22a4s, (1)\nwhere a sorting algorithm is a function f of s that predicts a permutation matrix P:\nP = f(s). (2)\nWe generalize the formulation of traditional sorting problems to handle more diverse and expressive types of inputs, e.g., multi-digit images and image fragments, which can contain ordinal information semantically. To this end, we extend the sequence of scalars s to the sequence of vectors X = [x1, . . . ,xn] \u22a4 \u2208 Rn\u00d7d, where d \u226b 1 is an input dimensionality, and consider the following:\nXo = P \u22a4X, (3)\nwhere Xo and X are ordered and unordered inputs, respectively. This generalized sorting problem can be reduced to (1) if we are given a proper mapping g from an input x \u2208 Rd to an ordinal value s \u2208 R. Without such a mapping g, predicting P in (3) remains more challenging than in (1) because x is often a highly implicative high-dimensional input. We address this generalized sorting problem by learning a neural sorting network together with a mapping g in an end-to-end manner, given training data {(X(i),P(i)gt )}Ni=1. The main challenge is to make the whole network f([g(x1), . . . , g(xn)]) with mapping and sorting components differentiable in order to effectively train the network with a gradient-based learning scheme, which is not the case in general. To tackle\nthe differentiability issue for such a composite function, there has been recent research (Grover et al., 2019; Cuturi et al., 2019; Blondel et al., 2020; Petersen et al., 2021; 2022).\nIn this paper, following a sorting network-based sorting algorithm with differentiable swap functions (DSFs) (Petersen et al., 2021; 2022), we first define a softening error by a sorting network, which indicates a difference between original and smoothed elements. Then, we propose an errorfree DSF that resolves an error accumulation problem induced by a soft DSF; this allows us to guarantee a zero error in mapping X to proper ordinal values. Based on this, we develop the sorting network with error-free DSFs where we adopt a permutation-equivariant Transformer architecture with multi-head attention (Vaswani et al., 2017) to capture dependency between high-dimensional inputs and also leverage the model capacity of the neural network with a self-attention scheme.\nOur contributions can be summarized as follows: (i) We define a softening error that measures a difference between original and smoothed values; (ii) We propose an error-free DSF that resolves the error accumulation problem of conventional DSFs and is still differentiable; (iii) We adopt a permutation-equivariant network with multi-head attention as a mapping from inputs to ordinal variables g(X), unlike g(x); (iv) We demonstrate that our proposed methods are effective in diverse sorting benchmarks, compared to existing baseline methods."
        },
        {
            "heading": "2 SORTING NETWORKS WITH DIFFERENTIABLE SWAP FUNCTIONS",
            "text": "Following traditional sorting algorithms such as bubble sort, quick sort, and merge sort (Cormen et al., 2022) and sorting networks that are constructed by a fixed number of wires (Knuth, 1998; Ajtai et al., 1983), a swap function is a key ingredient of sorting algorithms and sorting networks:\n(x\u2032, y\u2032) = swap(x, y), (4)\nwhere x\u2032 = min(x, y) and y\u2032 = max(x, y), which makes the order of x and y correct. For example, if x > y, then x\u2032 = y and y\u2032 = x. Without loss of generality, we can express min(\u00b7, \u00b7) and max(\u00b7, \u00b7) with the following equations:\nmin(x, y) = x\u230a\u03c3(y \u2212 x)\u2309+ y\u230a\u03c3(x\u2212 y)\u2309 and max(x, y) = x\u230a\u03c3(x\u2212 y)\u2309+ y\u230a\u03c3(y \u2212 x)\u2309, (5) where \u230a\u00b7\u2309 rounds to the nearest integer and \u03c3(\u00b7) \u2208 [0, 1] transforms an input to a bounded value, i.e., a probability over inputs. Computing (5) is straightforward, but they are not differentiable. To enable us to differentiate a swap function, the soft versions of min and max can be defined:\nmin(x, y) = x\u03c3(y \u2212 x) + y\u03c3(x\u2212 y) and max(x, y) = x\u03c3(x\u2212 y) + y\u03c3(y \u2212 x), (6) where \u03c3(\u00b7) is differentiable. In addition to its differentiability, either (5) or (6) can be achieved with a sigmoid function \u03c3(x), i.e., a s-shaped function, which satisfies the following properties that (i) \u03c3(x) is non-decreasing, (ii) \u03c3(x) = 1 if x \u2192 \u221e, (iii) \u03c3(x) = 0 if x \u2192 \u2212\u221e, (iv) \u03c3(0) = 0.5, and (v) \u03c3(x) = 1\u2212 \u03c3(\u2212x). Also, as discussed by Petersen et al. (2022), the choice of \u03c3 affects the performance of neural network-based sorting network in theory as well as in practice. For example, an optimal monotonic sigmoid function, which is visualized in Figure 4, is defined as the following:\n\u03c3O(x) =  \u2212 116 (\u03b2x)\u22121 if \u03b2x < \u22120.25, 1\u2212 116 (\u03b2x)\u22121 if \u03b2x > 0.25, \u03b2x+ 0.5 otherwise,\n(7)\nwhere \u03b2 is steepness; see the work (Petersen et al., 2022) for the details of these numerical and theoretical analyses. Here, we would like to emphasize that the important point of such monotonic sigmoid functions is strict monotonicity. However, as will be discussed in Section 3, it induces an error accumulation problem, which can degrade the performance of the sorting network.\nBy either (5) or (6), the permutation matrix P (henceforth, denoted as Phard and Psoft for (5) and (6), respectively) is calculated by the following procedure of a sorting network: (i) Building a predefined sorting network with a fixed number of wires \u2013 a wire is a component for comparing and swapping two elements; (ii) Feeding an unordered sequence s into the pre-defined sorting network and calculating a wire-wise permutation matrix Pi for each wire i iteratively; (iii) Calculating the permutation matrix P by multiplying all wire-wise permutation matrices.\nAs shown in Figure 1, a set of wires represents a set of swap operations that are operated simultaneously, so that each set produces an intermediate permutation matrix Pi at the ith step. Consequently,\nP\u22a4 = P\u22a41 P \u22a4 2 \u00b7 \u00b7 \u00b7P\u22a4k = (Pk \u00b7 \u00b7 \u00b7P2P1)\u22a4, where k is the number of wire sets. For example, in Figure 1, k = 5.\nThe doubly-stochastic matrix property of P is shown by the following proposition: Proposition 1 (Modification of Lemma 3 in the work (Petersen et al., 2022)). A permutation matrix P \u2208 Rn\u00d7n is doubly-stochastic, which implies that \u2211ni=1[P]ij = 1 and \u2211nj=1[P]ij = 1. In particular, regardless of the definition of a swap function with min, max, min, and max, hard and soft permutation matrices, i.e., Phard and Psoft, are doubly-stochastic.\nProof. The proof of Proposition 1 is provided in Section D.\nIn Sections 3 and 4, we present an error-free DSF and a neural sorting network with error-free DSFs."
        },
        {
            "heading": "3 ERROR-FREE DIFFERENTIABLE SWAP FUNCTIONS",
            "text": "Before introducing our error-free DSF, we start by describing the motivation of the error-free DSF.\nDue to the nature of min and max, which is described in (6), the monotonic DSF changes original input values. For example, if x < y, then x < min(x, y) and max(x, y) < y after applying the swap function. It can be a serious problem because changes by the DSF are accumulated as the DSF applies iteratively, called an error accumulation problem in this paper. The results of sigmoid functions such as the logistic, logistic with ART, reciprocal, Cauchy, and optimal monotonic functions, and also our error-free DSF are presented in Figure 2, where a swap function is applied once; see the work (Petersen et al., 2022) for the respective sigmoid functions. All DSFs except for our error-free DSF change two values, so that they can make two values not distinguishable. In particular, if a difference between two values is small, the consequence of softening is more significant than a case with a large difference. Moreover, if we apply a swap function repeatedly, they eventually become identical; see Figure 5 in Section B. While a swap function is not applied as many as it is tested in the synthetic example shown in Figure 5, it can still cause the error accumulation problem with a few operations. Here we formally define a softening error, which has been mentioned in this paragraph: Definition 1. Suppose that we are given x and y where x < y. By (6), these values x and y are softened by a monotonic DSF and they satisfy the following inequalities:\nx < x\u2032 = min(x, y) \u2264 y\u2032 = max(x, y) < y. (8) Therefore, we define a difference between the original and softened values, x\u2032 \u2212 x or y \u2212 y\u2032:\ny \u2212 y\u2032 = y \u2212max(x, y) = x\u2032 \u2212 x = min(x, y)\u2212 x > 0, (9) which is called a softening error in this paper. Without loss of generality, the softening error is min(x, y)\u2212min(x, y) or max(x, y)\u2212max(x, y) for any x, y.\nNote that (9) is satisfied by y \u2212max(x, y) = y(1\u2212 \u03c3(y \u2212 x))\u2212 x\u03c3(x\u2212 y) = y\u03c3(x\u2212 y)\u2212 x(1\u2212 \u03c3(y \u2212 x)) = min(x, y)\u2212 x, using (6) and \u03c3(x\u2212 y) = 1\u2212 \u03c3(y \u2212 x). With Definition 1, we are able to specify the seriousness of the error accumulation problem: Proposition 2. Suppose that x and y are given and a DSF is applied k times. Assuming an extreme scenario that k \u2192 \u221e, error accumulation becomes (max(x, y) \u2212 min(x, y))/2, under the assumption that \u2207x\u03c3(x) > 0.\nProof. The proof of this proposition can be found in Section E.\nAs mentioned in the proof of Proposition 2 and empirically shown in Figure 2, a swap function with relatively large \u2207x\u03c3(x) changes the original values x, y significantly compared to a swap function with relatively small \u2207x\u03c3(x) \u2013 they tend to become identical with the small number of operations in the case of large \u2207x\u03c3(x). In addition to the error accumulation problem, such a DSF depends on the scale of |x\u2212 y| as shown in Figure 2. If x < y but x and y are close enough, \u03c3(y \u2212 x) is between 0.5 and 1, which implies that the error can be induced by the scale of |x\u2212 y| as well. To tackle the aforementioned problem of error accumulation, we propose an error-free DSF:\n(x\u2032, y\u2032) = swaperror-free(x, y), (10)\nwhere x\u2032 = ( min(x, y)\u2212min(x, y) ) sg +min(x, y) and y\u2032 = (max(x, y)\u2212max(x, y))sg+max(x, y). (11) Note that sg indicates that gradients are stopped amid backward propagation, inspired by a straightthrough estimator (Bengio et al., 2013). At a step for forward propagation, the error-free DSF produces x\u2032 = min(x, y) and y\u2032 = max(x, y). On the contrary, at a step for backward propagation, the gradients of min and max are used to update learnable parameters. Consequently, our error-free DSF does not smooth the original elements as shown in Figure 2 and our DSF shows 100% accuracy for accem and accew (see Section 5 for their definitions) as shown in Figure 6. Compared to our DSF, the existing DSFs do not correspond the original elements to the elements that have been compared and fail to achieve reasonable performance as a sequence length increases, in the cases of Figure 6.\nBy (5), (6), and (11), we obtain the following:\nx\u2032=((x\u230a\u03c3(y \u2212 x)\u2309+y\u230a\u03c3(x\u2212 y)\u2309)\u2212(x\u03c3(y \u2212 x)+y\u03c3(x\u2212 y)))sg+(x\u03c3(y \u2212 x)+y\u03c3(x\u2212 y)) =x ((\u230a\u03c3(y \u2212 x)\u2309\u2212\u03c3(y \u2212 x))sg+\u03c3(y \u2212 x))+y ((\u230a\u03c3(x\u2212 y)\u2309\u2212\u03c3(x\u2212 y))sg+\u03c3(x\u2212 y)) , (12)\ny\u2032=x ((\u230a\u03c3(x\u2212 y)\u2309\u2212\u03c3(x\u2212 y))sg+\u03c3(x\u2212 y))+y ((\u230a\u03c3(y \u2212 x)\u2309\u2212\u03c3(y \u2212 x))sg+\u03c3(y \u2212 x)) , (13) which can be used to define a permutation matrix with the error-free DSF. For example, if n = 2, a permutation matrix P over [x, y] is\nP = [ (\u230a\u03c3(y \u2212 x)\u2309 \u2212 \u03c3(y \u2212 x))sg + \u03c3(y \u2212 x) (\u230a\u03c3(x\u2212 y)\u2309 \u2212 \u03c3(x\u2212 y))sg + \u03c3(x\u2212 y) (\u230a\u03c3(x\u2212 y)\u2309 \u2212 \u03c3(x\u2212 y))sg + \u03c3(x\u2212 y) (\u230a\u03c3(y \u2212 x)\u2309 \u2212 \u03c3(y \u2212 x))sg + \u03c3(y \u2212 x) ] . (14)\nTo sum up, we can describe the following proposition on our error-free DSF, swaperror-free(\u00b7, \u00b7): Proposition 3. By (11), the softening error x\u2032 \u2212min(x, y) or max(x, y)\u2212 y\u2032 for an error-free DSF is zero.\nProof. The proof of this proposition is presented in Section F."
        },
        {
            "heading": "4 NEURAL SORTING NETWORKS WITH ERROR-FREE DIFFERENTIABLE SWAP FUNCTIONS",
            "text": "In this section we propose a generalized neural network-based sorting network with an error-free DSF and a permutation-equivariant neural network, considering the properties covered in Section 3.\nFirst, we describe a procedure for transforming high-dimensional inputs to ordinal scores. Such a mapping g : Rd \u2192 R, which consists of a set of learnable parameters, has to satisfy a permutationequivariant property:\n[g(x\u03c01), g(x\u03c02), . . . , g(x\u03c0n)] = \u03c0([g(x1), g(x2), . . . , g(xn)]), (15) where \u03c0i = [\u03c0([1, 2, . . . , n])]i \u2200i \u2208 [n], for any permutation function \u03c0. Typically, an instancewise neural network, which is applied to each element in a sequence given, is permutationequivariant (Zaheer et al., 2017). Based on this property, instance-wise CNNs are employed in differentiable sorting algorithms (Grover et al., 2019; Cuturi et al., 2019; Petersen et al., 2021; 2022). However, such an instance-wise architecture is limited since it is ineffective for capturing essential features from a sequence. Some types of neural networks such as long short-term memory (Hochreiter & Schmidhuber, 1997) and the standard Transformer architecture (Vaswani et al., 2017) are capable of modeling a sequence of instances, utilizing recurrent connections, scaled dot-product attention, or parameter sharing across elements. While they are powerful for modeling a sequence, they are not obviously permutation-equivariant. Instead of such permutation-sensitive models, we adopt a robust Transformer-based network that satisfies the permutation-equivariant property, which is inspired by the recent work (Vaswani et al., 2017; Lee et al., 2019).\nTo explain our network, we briefly introduce scaled dot-product attention and multi-head attention:\natt(Q,K,V) = softmax ( QK\u22a4\u221a dm ) V and mha(Q,K,V) = [head1,head2, . . . ,headh]Wo, (16) where headi = att(QW (i) q ,KW (i) k ,VW (i) v ), Q,K,V\u2208Rn\u00d7hdm , W(i)q ,W(i)k ,W (i) v \u2208Rhdm\u00d7dm , and Wo \u2208Rhdm\u00d7hdm . Similar to the Transformer network, a series of mha blocks is stacked with layer normalization (Ba et al., 2016) and residual connections (He et al., 2016), and in this paper X is processed by mha(Z,Z,Z) where Z = g\u2032(X) or Z is the output of a previous layer; see Section I for the details of the architectures. Note that g\u2032(\u00b7) is an instance-wise embedding layer, e.g., a simple fully-connected network or a simple CNN. Importantly, compared to the standard Transformer model, our network does not include a positional embedding, in order to satisfy the permutation-equivariant property; mha(Z,Z,Z) satisfies (15) for the permutation of z1, z2, . . . , zn where Z = [z1, z2, . . . , zn]\u22a4. The output of our network is s, followed by the last instance-wise fully-connected layer. Finally, as shown in Figure 3, our sorting network is able to produce differentiable permutation matrices over s, i.e., Phard and Psoft, by utilizing (11) and (6), respectively. Note that Phard and Psoft are doubly-stochastic by Proposition 1. In addition, the details of the permutation-equivariant network with multi-head attention are briefly visualized in Figure 7.\nTo learn the permutation-equivariant network g, we define both objectives for Psoft and Phard: Lsoft = \u2212 n\u2211\ni=1 n\u2211 j=1 [Pgt logPsoft + (1\u2212Pgt) log(1\u2212Psoft)]ij , (17)\nLhard = \u2225Xo,hard \u2212Xo,gt\u22252F = \u2225P\u22a4hardX\u2212P\u22a4gtX\u22252F , (18)\nwhere Pgt is a ground-truth permutation matrix. Note that all the operations in Lsoft are entry-wise. Similar to (17), the objective (18) for Phard should be designed as the form of binary cross-entropy, which tends to be generally robust for training deep neural networks. However, we struggle to apply the binary cross-entropy for Phard into our problem formulation, due to discretized loss values. In particular, the form of cross-entropy for Phard can be used to train the sorting network, but degrades its performance in our preliminary experiments. Thus, we choose the objective for Phard as \u2225Xo,hard \u2212Xo,gt\u22252F with the Frobenius norm, which helps to train the network more robustly. In addition, using a proposition on splitting Phard, which is discussed in Section H, the objective (18) for Phard can be modified by splitting Phard, Pgt, and X, which is able to reduce the number of possible permutations; see the associated section for details. Eventually, our network g is trained by the combined loss L = Lsoft + \u03bbLhard, where \u03bb is a balancing hyperparameter; an analysis on \u03bb can be found in the appendices. As mentioned above, a landscape of Lhard is not smooth due to the property of a straight-through estimator, even though we use Lhard. Thus, we combine both objectives to the form of a single loss, which is widely adopted in the deep learning community."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We demonstrate experimental results to show the validity of our methods. Our neural networkbased sorting network aims to solve two benchmarks: sorting (i) multi-digit images and (ii) image fragments. Unless otherwise specified, an odd-even sorting network is used in the experiments. We measure the performance of each method in accem and accew:\naccem =\n\u2211N i=1 \u22c2n j=1 1 ( [ s\u0302(i) ] j = [ s\u0303(i) ] j ) N and accew = \u2211N i=1 \u2211n j=1 1 ( [ s\u0302(i) ] j = [ s\u0303(i) ] j ) Nn ,\n(19) where argsort returns indices to sort a given vector and 1(\u00b7) is an indicator function. Note that\ns\u0302(i) = argsort ( P (i)\u22a4 gt s (i) ) and s\u0303(i) = argsort ( P(i)\u22a4s(i) ) . (20)\nWe attempt to match the capacities of the Transformer-based models to the conventional CNNs. As described in Tables 1, 2, and 3, the capacities of the Transformer-Small models are smaller than or similar to the capacities of the CNNs in terms of FLOPs and the number of parameters."
        },
        {
            "heading": "5.1 SORTING MULTI-DIGIT IMAGES",
            "text": "Datasets. As steadily utilized in the previous work (Grover et al., 2019; Cuturi et al., 2019; Blondel et al., 2020; Petersen et al., 2021; 2022), we create a four-digit dataset by concatenating four images from the MNIST dataset (LeCun et al., 1998); see Figure 3 for some examples of the dataset. On the other hand, the SVHN dataset (Netzer et al., 2011) contains multi-digit numbers extracted from street view images and is therefore suitable for sorting.\nExperimental Details. We conduct the experiments 5 times by varying random seeds to report the average of accem and accew, and use the optimal monotonic sigmoid function as DSFs. The performance of each model is measured by a test dataset. We use the AdamW optimizer (Loshchilov & Hutter, 2018), and train each model for 200,000 steps on the four-digit MNIST dataset and 300,000 steps on the SVHN dataset. Unless otherwise noted, we follow the same settings of the work (Petersen et al., 2022) for fair comparisons. Missing details are described in Section J.\nResults. Tables 1 and 2 show the results of the previous work such as NeuralSort (Grover et al., 2019), Sinkhorn Sort (Cuturi et al., 2019), Fast Sort & Rank (Blondel et al., 2020), and Diffsort (Petersen et al., 2021; 2022), and our methods on the MNIST and SVHN datasets, respectively. When we use the conventional CNN as a permutation-equivariant network, our method shows better than or comparable to the previous methods. As we exploit more powerful models, i.e., the TransformerSmall and Transformer-Large permutation-equivariant models, our approaches show better results compared to other existing methods including our method with the conventional CNN.1"
        },
        {
            "heading": "5.2 SORTING IMAGE FRAGMENTS",
            "text": "Datasets. For experiments on sorting image fragments, we use two datasets: the MNIST dataset (LeCun et al., 1998) and the CIFAR-10 dataset (Krizhevsky & Hinton, 2009). Similar to the work (Mena et al., 2018), we create multiple fragments or patches from a single-digit image of the MNIST dataset to utilize themselves as inputs \u2013 for example, 4 fragments of size 14 \u00d7 14 or 9 fragments of size 9 \u00d7 9 are created from a single image. Similarly, the image included in the CIFAR-10 dataset, which contains one of various objects, e.g., birds and cats, is split to multiple patches, and then is used to the experiments on sorting image fragments. See Table 3 for the details of the image fragments and their sizes.\n1Thanks to many open-source projects, we can easily run the baseline methods. However, it is difficult to reproduce some results due to unknown random seeds. For this reason, we bring the results from the work (Petersen et al., 2022), and use fixed random seeds, i.e., 42, 84, 126, 168, 210, for our methods.\nExperimental Details. Similar to the experiments on sorting multi-digit images, an optimal monotonic sigmoid function is used as DSFs. Since the size of inputs is much smaller than the experiments on sorting multi-digit images, shown in Section 5.1, we modify the architectures of the CNNs and the Transformer-based models. We reduce the kernel size of convolution layers from 5 to 3 and make strides 2. Due to the small input sizes, we omit the results by the Transformer-Large model for these experiments. Additionally, max-pooling operations are removed. Similar to the experiments in Section 5.1, we use the AdamW optimizer (Loshchilov & Hutter, 2018). Moreover, each model is trained for 50,000 steps when the number of fragments is 2 \u00d7 2, i.e., when a sequence length is 4, and 100,000 steps for 3\u00d7 3 fragments, i.e., when a sequence length is 9. Additional information including the details of neural architectures can be found in Sections I and J.\nResults. Table 3 represents the experimental results on both datasets of image fragments, which are created from the MNIST and CIFAR-10 datasets. Similar to the experiments on sorting multidigit images, the more powerful architecture improves performance in this task.\nAccording to the experimental results, we achieve satisfactory performance by applying the errorfree DSFs, combined loss, and Transformer-based models with multi-head attention. We provide detailed discussion on how they contribute to the performance gains in Section 7, and empirical studies on steepness, learning rate, and a balancing hyperparameter in Section 7 and the appendices."
        },
        {
            "heading": "6 RELATED WORK",
            "text": "Differentiable Sorting Algorithms. To allow us to differentiate a sorting algorithm, Grover et al. (2019) have proposed the continuous relaxation of argsort operator, which is named NeuralSort. In this work, the output of NeuralSort only satisfies the row-stochastic matrix property, although Grover et al. (2019) attempt to employ a gradient-based optimization strategy in learning a neural sorting algorithm. Cuturi et al. (2019) propose a smoothed ranking and sorting operator using optimal transport, which is the natural relaxation for assignments. To reduce the cost of the optimal transport, the Sinkhorn algorithm (Cuturi, 2013) is used. Then, Blondel et al. (2020) have proposed a differentiable sorting and ranking operator with O(n log n) time and O(n) space complexities, which is named Fast Rank & Sort, by constructing differentiable operators as projections on permutahedron. Petersen et al. (2021) have suggested a differentiable sorting network with relaxed conditional swap functions. Recently, the same authors analyze the characteristics of the relaxation of monotonic conditional swap functions, and propose several monotonic swap functions, e.g., the Cauchy and optimal monotonic functions (Petersen et al., 2022).\nPermutation-Equivariant Networks. A seminal architecture, long short-term memory (Hochreiter & Schmidhuber, 1997) can be used in modeling a sequence without any difficulty, and a sequence-to-sequence model (Sutskever et al., 2014) can be employed to cope with a sequence. However, as discussed in the work by Vinyals et al. (2016), an unordered sequence can have good orderings, by analyzing the effects of permutation thoroughly. Zaheer et al. (2017) propose a permutation-invariant or permutation-equivariant network, named Deep Sets, and prove the permutation invariance and permutation equivariance of the proposed models. By utilizing the Transformer network (Vaswani et al., 2017), Lee et al. (2019) have proposed a permutation-equivariant network."
        },
        {
            "heading": "7 DISCUSSION",
            "text": "Numerical Analysis on Our Methods and Their Hyperparameters. We carry out numerical analyses on the effects of our methods, compared to a baseline method, i.e., Diffsort with the optimal monotonic sigmoid function. As reported in Table 4, we demonstrate that our methods better sorting performance compared to the baseline, which implies that our suggestions are effective in the sorting tasks. In these experiments, we follow the settings of the experiments described in Section 5.1. Moreover, we present numerical analyses on a balancing hyperparameter, steepness, and a learning rate in Sections K and L.\nAnalysis on Performance Gains. According to the results in Sections 5 and 7, we can argue that the error-free DSFs, our proposed loss, and the Transformer-based models contribute to better\nperformance considerably compared to the baseline methods. As shown in Tables 1 and 4, the performance gains by the Transformer-based models are more substantial than the gains by the errorfree DSFs and our loss, since multi-head attention is effective for capturing long-term dependency (or dependency between multiple instances in our case) and reducing inductive biases. However, as will be discussed in the following, the hard permutation matrices can be used in the case that does not allow us to mix instances in X, e.g., sorting image fragments in Section 5.2.\nUtilization of Hard Permutation Matrices. While the use of a soft permutation matrix Psoft makes given instances mixed, a hard permutation matrix Phard is instrumental in applying Phard in a problem that requires swapping given instances exactly. More precisely, each row of P\u22a4softX is a linear combination of some column of Psoft and X, but one of P\u22a4hardX corresponds to an exact row in X. This property can be used to preserve input instances from sorting operations. The experiments in Section 5.2 can be considered as one of such cases, and it exhibits the strength of our method, not only the performance in accem and accew.\nEffects of Multi-Head Attention in the Problem (3). We follow the model architecture used in the previous work (Grover et al., 2019; Cuturi et al., 2019; Petersen et al., 2021; 2022) for the CNNs. However, as shown in Tables 1, 2, and 3, the model is not enough to show the best performance. In particular, whereas the model capacity, i.e., FLOPs and the number of parameters, of the Transformer-Small models is almost matched to or less than the capacity of the CNNs, the results by the Transformer-Small models outperform the results by the CNNs. We presume that these performance gains are derived from a multi-head attention\u2019s ability to capture long-term dependency and reduce inductive biases, as widely stated in many recent studies in diverse fields such as natural language processing (Vaswani et al., 2017; Devlin et al., 2018; Brown et al., 2020), computer vision (Dosovitskiy et al., 2021; Liu et al., 2021), and 3D vision (Nash et al., 2020; Zhao et al., 2021). Especially, unlike the instance-wise CNNs, our permutation-equivariant Transformer architecture utilizes self-attention for given instances, so that our model can productively compare instances in a sequence and effectively learn the relative relationship between them.\nFurther Study of Differentiable Sorting Algorithms. Differentiable sorting encourages us to train a mapping from an abstract input to an ordinal score using supervision on permutation matrices. However, this line of studies is limited to a sorting problem of high-dimensional data with clear ordering information, e.g., multi-digit numbers. As the further study of differentiable sorting, we can expand this framework to sort more ambiguous data, which contains implicitly ordinal information."
        },
        {
            "heading": "8 CONCLUSION",
            "text": "In this paper, we defined a softening error, induced by a monotonic DSF, and demonstrated several evidences of the error accumulation problem. To resolve the error accumulation problem, an errorfree DSF is proposed, inspired by a straight-through estimator. Moreover, we provided the simple theoretical and empirical analyses that our error-free DSF successfully achieves a zero error and also holds a non-decreasing condition and differentiability. By combining all components, we suggested a generalized neural sorting network with the error-free DSF and multi-head attention. Finally, we showed that our methods are better than or comparable to other algorithms in diverse benchmarks.\nETHICS STATEMENT\nAs discussed in Section 7, the hard permutation matrices produced by our methods allow us to swap instances exactly, not the linear combination of instances. This characteristic is required when we are given the final outcomes of sorting as supervision. This scenario is tested by the experiments presented in Section 5.2. In these experiments, we are supposed that original images are provided as supervision. Building on the advantages of neural network-based sorting networks, we expand their practical significance to the cases that need hard permutation matrices. On the other hand, the nature of neural sorting networks may yield a potential negative societal impact. If this line of research including our approaches is employed to sort controversial high-dimensional data such as beauty and intelligence, it can be considered as the unethical use cases of artificial intelligence."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported by the IITP grants (2022-0-00290: Visual Intelligence for Space-Time Understanding and Generation based on Multi-layered Visual Common Sense, 2022-0-00264: Comprehensive Video Understanding and Generation with Knowledge-based Deep Logic Neural Network) funded by Ministry of Science and ICT, Republic of Korea."
        },
        {
            "heading": "A OPTIMAL MONOTONIC SIGMOID FUNCTIONS",
            "text": "We visualize an optimal monotonic sigmoid function in Figure 4."
        },
        {
            "heading": "B COMPARISONS OF DIFFERENTIABLE SWAP FUNCTIONS",
            "text": "As depicted in Figure 5, some sigmoid functions such as the logistic, logistic with ART, reciprocal, Cauchy, and optimal monotonic functions suffer from the error accumulation problem; see the work (Petersen et al., 2022) for the details of such sigmoid functions. For the case of the Cauchy function, two values are close enough at the 9th step in the left panel of Figure 5 and the 15th step in the right panel of Figure 5; we calculate the corresponding steps where a difference between two values becomes smaller than 0.001."
        },
        {
            "heading": "C COMPARISONS OF DIFFERENT SORTING NETWORKS",
            "text": "Figure 6 shows the comparisons of different sorting networks by varying sequence lengths. accem and accew are measured to assess the sorting networks."
        },
        {
            "heading": "D PROOF OF PROPOSITION 1",
            "text": "Proof. If two elements at indices i and j are swapped by a single swap function, [P]kk = 1 for k \u2208 [n]\\{i, j}, [P]kl = 0 for k \u0338= l, k, l \u2208 [n]\\{i, j}, [P]ii = [P]jj = p, and [P]ij = [P]ji = 1\u2212p, where p is the output of a sigmoid function, i.e., p = \u03c3(y \u2212 x) or p = \u230a\u03c3(y \u2212 x)\u2309. Since the multiplication of doubly-stochastic matrices is still doubly-stochastic, Proposition 1 is true."
        },
        {
            "heading": "E PROOF OF PROPOSITION 2",
            "text": "Proof. Let min k (x, y) and min k (x, y) be minimum and maximum values where swap with min and max is applied k times repeatedly. By Definition 1 and min i < min i+1 and maxi+1 < maxi, the following inequalities are satisfied:\nmin(x, y) < min 1 (x, y) < min 2 (x, y) < \u00b7 \u00b7 \u00b7 < mink(x, y)\n\u2264 maxk(x, y) < \u00b7 \u00b7 \u00b7 < max2(x, y) < max1(x, y) < max(x, y), (21) under the assumption that \u2207x\u03c3(x) > 0. By (21) and \u2207x\u03c3(x) > 0, we can obtain the following inequality:\n0 \u2264 maxk+1(x, y)\u2212mink+1(x, y) < maxk(x, y)\u2212mink(x, y) < maxk\u22121(x, y)\u2212mink\u22121(x, y). (22) Therefore, limk\u2192\u221e maxk(x, y) \u2212 mink(x, y) = 0, and mink(x, y) = maxk(x, y) if k \u2192 \u221e. To sum up, a softening error for k \u2192 \u221e is (max(x, y) \u2212 min(x, y))/2 since maxk(x, y) = (min(x, y) + max(x, y))/2 by (9). Note that the assumption \u2207x\u03c3(x) > 0 implies that \u03c3(\u00b7) is a strictly monotonic sigmoid function."
        },
        {
            "heading": "F PROOF OF PROPOSITION 3",
            "text": "Proof. According to Definition 1, given x and y, the softening error x\u2032 \u2212min(x, y) is expressed as the following:\nx\u2032 \u2212min(x, y) = ( min(x, y)\u2212min(x, y) ) sg +min(x, y)\u2212min(x, y)\n= min(x, y)\u2212min(x, y) + min(x, y)\u2212min(x, y) = 0, (23)\nwhile a forward pass is applied. The proof for max(x, y)\u2212 y\u2032 is omitted because it is obvious."
        },
        {
            "heading": "G DETAILS OF PERMUTATION-EQUIVARIANT NETWORKS WITH MULTI-HEAD ATTENTION",
            "text": "Figure 7 illustrates the Transformer-based permutation-equivariant network, which is implemented with multi-head attention (Vaswani et al., 2017). For the sake of brevity, this illustration briefly depicts our permutation-equivariant network without detailing the specifics of the Transformer network. Each instance in a sequence is first processed by a feature extractor, i.e., a convolutional neural network. Then, a sequence of latent vectors is provided into the Transformer network without positional encoding. At each multi-head attention module, each latent vector is updated by the aggregation of the latent vectors given where the aggregation is determined by the operations explained in Section 4 and (16). After passing through multiple layers of multi-head attention and the\ncorresponding components such as layer normalization and feed-forward neural networks, the final fully-connected layer is applied to transform the outputs of the Transformer network into a score vector s. The details of the Transformer network can be found in the work by Vaswani et al. (2017)."
        },
        {
            "heading": "H SPLIT STRATEGY TO REDUCE THE NUMBER OF POSSIBLE PERMUTATIONS",
            "text": "As presented in (14) and Proposition 1, the permutation matrix for the error-free DSF is a discretized doubly-stochastic matrix, which is denoted as Phard, in a forward pass, and is differentiable in a backward pass. Here, we show an interesting proposition of Phard: Proposition 4. Let s \u2208 Rn and Phard \u2208 Rn\u00d7n be an unordered sequence and the corresponding permutation matrix to transform it to so, respectively. We are able to split s to two subsequences s1 \u2208 Rn1 and s2 \u2208 Rn2 where s1 = [s]1:n1 and s2 = [s]n1+1:n1+n2 . Then, Phard is also split to P1 \u2208 Rn1\u00d7n1 and P2 \u2208 Rn2\u00d7n2 , so that P1 and P2 are (discretized) doubly-stochastic.\nProof. A split does not change the relative order of elements in the same split and each entry in the permutation matrix is zero or one, so that a permutation matrix can be split as shown in Figure 8. Moreover, multiple splits are straightforwardly doable.\nIn contrast to Phard, it is impossible to split Psoft to sub-block matrices since such sub-block matrices cannot satisfy the property of doubly-stochastic matrix, which is discussed in Proposition 1. Importantly, Proposition 4 does not show a possibility of the recoverable decomposition of the permutation matrix, which implies that we cannot guarantee the recovery of decomposed matrices to\nthe original matrix. Regardless of the existence of recoverable decomposition, we attempt to reduce the number of possible permutations with sub-block matrices, rather than holding the large number of possible permutations with the original permutation matrix. Therefore, by Proposition 4, relative relationships between instances with a smaller number of possible permutations are more distinctively learnable than the relationships with a larger number of possible permutations, preventing a sparse correct permutation among a large number of possible permutations."
        },
        {
            "heading": "I DETAILS OF ARCHITECTURES",
            "text": "We describe the details of the neural architectures used in our paper, as shown in Tables 5, 6, 7, 8, 9, 10, 11, 12, 13, and 14. For the experiments on sorting image fragments, we omit some of the architectures employed for particular fragmentation, since they follow the same architectures presented in Tables 11, 12, 13, and 14. Only differences are the sizes of inputs, and therefore the respective sizes of the first fully-connected layers change."
        },
        {
            "heading": "J DETAILS OF EXPERIMENTS",
            "text": "As described in the main article, we use three public datasets: MNIST (LeCun et al., 1998), SVHN (Netzer et al., 2011), and CIFAR-10 (Krizhevsky & Hinton, 2009). Unless otherwise spec-\nified, a learning rate 10\u22123.5 is used for the CNN architectures and a learning rate 10\u22124 is used for the Transformer-based architectures; see our implementation for the exact learning rates we utilize in the experiments. Learning rate decay is applied by multiplying 0.5 in every 50,000 steps for the experiments on sorting multi-digit images and every 20,000 steps for the experiments on sorting image fragments. Moreover, we balance two objectives for Phard and Psoft by multiplying 1, 0.1, 0.01, or 0.001; see our implementation for the respective values for all the experiments. For random seeds, we pick five random seeds 42, 84, 126, 168, and 210 for all the experiments; these values are picked without any trials. Other missing details can be found in our implementation. Furthermore, we employ several commercial NVIDIA GPUs, i.e., GeForce GTX Titan Xp, GeForce RTX 2080, and GeForce RTX 3090, in the experiments."
        },
        {
            "heading": "K STUDY ON BALANCING HYPERPARAMETER",
            "text": "We conduct a study on a balancing hyperparameter in the experiments on sorting the four-digit MNIST dataset, as shown in Table 15. For these experiments, we use steepness 2, 14, 23, 38, 25, and 124 for sequence lengths 3, 5, 7, 9, 15, and 32, respectively. Also, we use a learning rate 10\u22123 and 5 random seeds 42, 84, 126, 168, and 210."
        },
        {
            "heading": "L STUDY ON STEEPNESS AND LEARNING RATE",
            "text": "We present studies on steepness and learning rate for the experiments on sorting the multi-digit MNIST dataset, as shown in Tables 16, 17, 18, 19, 20, and 21. For these experiments, a random seed 42 is only used due to numerous experimental settings. Also, we use balancing hyperparameters \u03bb as 1.0, 1.0, 0.1, 0.1, 0.1, and 0.1 for sequence lengths 3, 5, 7, 9, 15, and 32, respectively. Since there are many configurations of steepness, learning rate, and a balancing hyperparameter, we cannot include all the configurations here. The final configurations we use in the experiments are described in our implementation. As widely known in the deep learning community, a learning rate should be set as a value around 10\u22123. Moreover, according to our empirical analyses, steepness should generally be higher as a sequence length is longer."
        },
        {
            "heading": "M LIMITATIONS",
            "text": "While a sorting task is one of the most significant problems in computer science and mathematics (Cormen et al., 2022), our ideas, which are built on sorting networks (Knuth, 1998; Ajtai et al., 1983), can be limited to sorting algorithms. It implies that it is not easy to devise neural networkbased approaches to solving general problems in computer science, e.g., combinatorial optimization, which are inspired by our ideas.\nIn addition, while our proposed methods show the superior performance compared to the baseline methods, this line of research suffers from performance degradation for longer sequences as shown in Tables 1, 2, and 3. More precisely, for longer sequences, the element-wise accuracy does not decline dramatically, but the sequence-wise accuracy significantly drops due to the nature of sequences. Incorporating our contributions such as the error-free DSFs and the Transformer-based networks, we expect that the further progress of neural network-based sorting networks can be achieved. In particular, the consideration of more sophisticated neural networks, which are capable of handling longer sequences, might help improve performance. This will be left for future work.\nOur frameworks successfully learn relationships between high-dimensional data with ordinal contents as shown in Section 5. However, we suppose that our methods might fail in sorting data without ordinal information; the elaborate discussion on this topic can be found in Section 7. In order to sort more ambiguous high-dimensional data, we can combine our work with part-based or segmentation-based approaches."
        },
        {
            "heading": "N ADDITIONAL DISCUSSION",
            "text": "It is challenging to directly sort a sequence of generic data instances without using auxiliary networks and explicit supervision. Unlike earlier sorting methods, this sorting network-based research (Petersen et al., 2021; 2022) including our work ensures that we can train a neural network that predicts numerical scores and eventually sorts them, even though we do not necessitate accessing explicit supervision such as exact numerical values of the contents in high-dimensional data. In this sense, the practical significance of our proposed methods can be highlighted by offering this possibility of solving a sorting problem with high-dimensional inputs. For example, as shown in Section 5, we can compare images of street view house numbers using the sorting network where our neural network is trained without exact house numbers.\nMoreover, instead of using costly supervision, our networks allow us to sort high-dimensional instances in a sequence where information on comparisons between instances is only given. This scenario often occurs when we cannot obtain complete supervision. For example, if we would sort four-digit MNIST images, ordinary neural networks are designed to solve a classification task by predicting class probabilities each of which indicates one of all labels from \u201c0000\u201d to \u201c9999\u201d. If some labels are missing and further we do not know the exact number of labels, they might fail in predicting unseen data corresponding to those labels. Unlike these methods, it is possible to solve sorting problems using our networks in such a scenario.\nFurthermore, this study can be applied in diverse deep learning tasks for learning to sort generic high-dimensional data, such as information retrieval (Cao et al., 2007; Liu, 2009) and top-k classification (Berrada et al., 2018)."
        }
    ],
    "title": "ERROR-FREE DIFFERENTIABLE SWAP FUNCTIONS",
    "year": 2024
}