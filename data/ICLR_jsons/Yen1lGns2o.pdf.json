{
    "abstractText": "Self-supervised learning has unlocked the potential of scaling up pretraining to billions of images, since annotation is unnecessary. But are we making the best use of data? How more economical can we be? In this work, we attempt to answer this question by making two contributions. First, we investigate first-person videos and introduce a \u201cWalking Tours\u201d dataset. These videos are high-resolution, hourslong, captured in a single uninterrupted take, depicting a large number of objects and actions with natural scene transitions. They are unlabeled and uncurated, thus realistic for self-supervision and comparable with human learning. Second, we introduce a novel self-supervised image pretraining method tailored for learning from continuous videos. Existing methods typically adapt imagebased pretraining approaches to incorporate more frames. Instead, we advocate a \u201ctracking to learn to recognize\u201d approach. Our method called DORA, leads to attention maps that Discover and tRAck objects over time in an end-to-end manner, using transformer cross-attention. We derive multiple views from the tracks and use them in a classical self-supervised distillation loss. Using our novel approach, a single Walking Tours video remarkably becomes a strong competitor to ImageNet for several image and video downstream tasks.",
    "authors": [],
    "id": "SP:d8e4b5dc6b3301bc610eb3a766fe30685cd0e437",
    "references": [
        {
            "authors": [
                "Russell J Adams"
            ],
            "title": "An evaluation of color preference in early infancy",
            "venue": "Infant Behavior and Development,",
            "year": 1987
        },
        {
            "authors": [
                "Pulkit Agrawal",
                "Joao Carreira",
                "Jitendra Malik"
            ],
            "title": "Learning to see by moving",
            "venue": "In ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "Yuki M. Asano",
                "Christian Rupprecht",
                "Andrea Vedaldi"
            ],
            "title": "Self-labelling via simultaneous clustering and representation learning",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Max Bain",
                "Arsha Nagrani",
                "G\u00fcl Varol",
                "Andrew Zisserman"
            ],
            "title": "Frozen in time: A joint video and image encoder for end-to-end retrieval",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Adrien Bardes",
                "Jean Ponce",
                "Yann LeCun"
            ],
            "title": "Vicreg: Variance-invariance-covariance regularization for selfsupervised learning",
            "venue": "arXiv preprint arXiv:2105.04906,",
            "year": 2021
        },
        {
            "authors": [
                "Paul C Bomba",
                "Einar R Siqueland"
            ],
            "title": "The nature and structure of infant form categories",
            "venue": "Journal of Experimental Child Psychology,",
            "year": 1983
        },
        {
            "authors": [
                "Zhaowei Cai",
                "Nuno Vasconcelos"
            ],
            "title": "Cascade r-cnn: High quality object detection and instance segmentation",
            "venue": "IEEE TPAMI, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Joseph J Campos",
                "Susan Hiatt",
                "Douglas Ramsay",
                "Charlotte Henderson",
                "Marilyn Svejda"
            ],
            "title": "The emergence of fear on the visual cliff",
            "venue": "The development of affect,",
            "year": 1978
        },
        {
            "authors": [
                "Mathilde Caron",
                "Ishan Misra",
                "Julien Mairal",
                "Priya Goyal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Unsupervised learning of visual features by contrasting cluster assignments",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Prabuddha Chakraborty",
                "Vinay P. Namboodiri"
            ],
            "title": "Learning to estimate pose by watching videos",
            "venue": "arXiv preprint arXiv:1704.04081,",
            "year": 2017
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Xin Chen",
                "Houwen Peng",
                "Dong Wang",
                "Huchuan Lu",
                "Han Hu"
            ],
            "title": "Seqtrack: Sequence to sequence learning for visual object tracking",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Ioana Croitoru",
                "Simion-Vlad Bogolin",
                "Marius Leordeanu"
            ],
            "title": "Unsupervised learning from video to detect foreground objects in single images",
            "venue": "In ICCV,",
            "year": 2017
        },
        {
            "authors": [
                "Marco Cuturi"
            ],
            "title": "Sinkhorn distances: Lightspeed computation of optimal transport",
            "venue": "NeurIPS,",
            "year": 2013
        },
        {
            "authors": [
                "Enrico Fini",
                "Moin Nabi",
                "Nicu Sebe",
                "Elisa Ricci"
            ],
            "title": "solo-learn: A library of self-supervised methods for visual representation learning",
            "year": 2022
        },
        {
            "authors": [
                "Dima Damen",
                "Hazel Doughty",
                "Giovanni Maria Farinella",
                "Antonino Furnari",
                "Jian Ma",
                "Evangelos Kazakos",
                "Davide Moltisanti",
                "Jonathan Munro",
                "Toby Perrett",
                "Will Price",
                "Michael Wray"
            ],
            "title": "Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100",
            "venue": "IJCV, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Michelle de Haan",
                "Mark H Johnson",
                "Daphne Maurer",
                "David I Perrett"
            ],
            "title": "Recognition of individual faces and average face prototypes by 1-and 3-month-old infants",
            "venue": "Cognitive development,",
            "year": 2001
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Kristen Grauman",
                "Andrew Westbury",
                "Eugene Byrne",
                "Zachary Chavis",
                "Antonino Furnari",
                "Rohit Girdhar",
                "Jackson Hamburger",
                "Hao Jiang",
                "Miao Liu",
                "Xingyu Liu"
            ],
            "title": "Ego4d: Around the world in 3,000 hours of egocentric video",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Chunhui Gu",
                "Chen Sun",
                "David A Ross",
                "Carl Vondrick",
                "Caroline Pantofaru",
                "Yeqing Li",
                "Sudheendra Vijayanarasimhan",
                "George Toderici",
                "Susanna Ricco",
                "Rahul Sukthankar"
            ],
            "title": "Ava: A video dataset of spatiotemporally localized atomic visual actions",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Lianghua Huang",
                "Xin Zhao",
                "Kaiqi Huang"
            ],
            "title": "Got-10k: A large high-diversity benchmark for generic object tracking in the wild",
            "venue": "IEEE TPAMI,",
            "year": 2019
        },
        {
            "authors": [
                "Lianghua Huang",
                "Xin Zhao",
                "Kaiqi Huang"
            ],
            "title": "Got-10k: A large high-diversity benchmark for generic object tracking in the wild",
            "venue": "IEEE TPAMI,",
            "year": 2021
        },
        {
            "authors": [
                "Dinesh Jayaraman",
                "Kristen Grauman"
            ],
            "title": "Learning image representations tied to ego-motion",
            "venue": "In ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "Dinesh Jayaraman",
                "Kristen Grauman"
            ],
            "title": "Look-ahead before you leap: end-to-end active recognition by forecasting the effect of motion",
            "venue": "In ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "Ioannis Kakogeorgiou",
                "Spyros Gidaris",
                "Bill Psomas",
                "Yannis Avrithis",
                "Andrei Bursuc",
                "Konstantinos Karantzalos",
                "Nikos Komodakis"
            ],
            "title": "What to hide from your students: Attention-guided masked image modeling",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Will Kay",
                "Joao Carreira",
                "Karen Simonyan",
                "Brian Zhang",
                "Chloe Hillier",
                "Sudheendra Vijayanarasimhan",
                "Fabio Viola",
                "Tim Green",
                "Trevor Back",
                "Paul Natsev"
            ],
            "title": "The kinetics human action video dataset",
            "venue": "arXiv preprint arXiv:1705.06950,",
            "year": 2017
        },
        {
            "authors": [
                "Abdullah Aman Khan",
                "Jie Shao",
                "Waqar Ali",
                "Saifullah Tumrani"
            ],
            "title": "Content-aware summarization of broadcast sports videos: an audio\u2013visual feature extraction approach",
            "venue": "Neural Processing Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Xueting Li",
                "Sifei Liu",
                "Shalini De Mello",
                "Xiaolong Wang",
                "Jan Kautz",
                "Ming-Hsuan Yang"
            ],
            "title": "Joint-task selfsupervised learning for temporal correspondence",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Aravindh Mahendran",
                "James Thewlis",
                "Andrea Vedaldi"
            ],
            "title": "Cross pixel optical-flow similarity for selfsupervised learning",
            "venue": "In ACCV,",
            "year": 2018
        },
        {
            "authors": [
                "Antoine Miech",
                "Dimitri Zhukov",
                "Jean-Baptiste Alayrac",
                "Makarand Tapaswi",
                "Ivan Laptev",
                "Josef Sivic"
            ],
            "title": "Howto100m: Learning a text-video embedding by watching hundred million narrated video clips",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Ishan Misra",
                "C Lawrence Zitnick",
                "Martial Hebert"
            ],
            "title": "Shuffle and learn: unsupervised learning using temporal order verification",
            "venue": "In ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "Emin Orhan",
                "Vaibhav Gupta",
                "Brenden M Lake"
            ],
            "title": "Self-supervised learning through the eyes of a child",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Nikhil Parthasarathy",
                "SM Eslami",
                "Jo\u00e3o Carreira",
                "Olivier J H\u00e9naff"
            ],
            "title": "Self-supervised video pretraining yields strong image representations",
            "venue": "arXiv preprint arXiv:2210.06433,",
            "year": 2022
        },
        {
            "authors": [
                "Deepak Pathak",
                "Ross Girshick",
                "Piotr Doll\u00e1r",
                "Trevor Darrell",
                "Bharath Hariharan"
            ],
            "title": "Learning features by watching objects move",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "S\u00f6ren Pirk",
                "Mohi Khansari",
                "Yunfei Bai",
                "Corey Lynch",
                "Pierre Sermanet"
            ],
            "title": "Online learning of object representations by appearance space feature alignment",
            "venue": "In ICRA,",
            "year": 2020
        },
        {
            "authors": [
                "Jordi Pont-Tuset",
                "Federico Perazzi",
                "Sergi Caelles",
                "Pablo Arbel\u00e1ez",
                "Alex Sorkine-Hornung",
                "Luc Van Gool"
            ],
            "title": "The 2017 davis challenge on video object segmentation",
            "venue": "arXiv preprint arXiv:1704.00675,",
            "year": 2017
        },
        {
            "authors": [
                "Paul C Quinn",
                "Peter D Eimas",
                "Stacey L Rosenkrantz"
            ],
            "title": "Evidence for representations of perceptually similar natural categories by 3-month-old and 4-month-old infants",
            "year": 1993
        },
        {
            "authors": [
                "Francesco Ragusa",
                "Antonino Furnari",
                "Giovanni Maria Farinella"
            ],
            "title": "Meccano: A multimodal egocentric dataset for humans behavior understanding in the industrial-like",
            "year": 2023
        },
        {
            "authors": [
                "Mohammadreza Salehi",
                "Efstratios Gavves",
                "Cees G.M. Snoek",
                "Yuki M. Asano"
            ],
            "title": "Time does tell: Selfsupervised time-tuning of dense image representations",
            "year": 2023
        },
        {
            "authors": [
                "F. Sener",
                "D. Chatterjee",
                "D. Shelepov",
                "K. He",
                "D. Singhania",
                "R. Wang",
                "A. Yao"
            ],
            "title": "Assembly101: A large-scale multi-view video dataset for understanding procedural activities",
            "venue": "CVPR, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Oriane Sim\u00e9oni",
                "Gilles Puy",
                "Huy V Vo",
                "Simon Roburin",
                "Spyros Gidaris",
                "Andrei Bursuc",
                "Patrick P\u00e9rez",
                "Renaud Marlet",
                "Jean Ponce"
            ],
            "title": "Localizing objects with self-supervised transformers and no labels",
            "venue": "In BMVC,",
            "year": 2021
        },
        {
            "authors": [
                "Samuel Sokol"
            ],
            "title": "Measurement of infant visual acuity from pattern reversal evoked potentials",
            "venue": "Vision research,",
            "year": 1978
        },
        {
            "authors": [
                "Michael Tschannen",
                "Josip Djolonga",
                "Marvin Ritter",
                "Aravindh Mahendran",
                "Neil Houlsby",
                "Sylvain Gelly",
                "Mario Lucic"
            ],
            "title": "Self-supervised learning of video-induced visual invariances",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaolong Wang",
                "Abhinav Gupta"
            ],
            "title": "Unsupervised learning of visual representations using videos",
            "venue": "In ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "Xiaolong Wang",
                "Allan Jabri",
                "Alexei A Efros"
            ],
            "title": "Learning correspondence from the cycle-consistency of time",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Olivia Wiles",
                "Joao Carreira",
                "Iain Barr",
                "Andrew Zisserman",
                "Mateusz Malinowski"
            ],
            "title": "Compressed vision for efficient video understanding",
            "venue": "In ACCV, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Laurenz Wiskott",
                "Terrence J Sejnowski"
            ],
            "title": "Slow feature analysis: Unsupervised learning of invariances",
            "venue": "Neural computation,",
            "year": 2002
        },
        {
            "authors": [
                "Tete Xiao",
                "Yingcheng Liu",
                "Bolei Zhou",
                "Yuning Jiang",
                "Jian Sun"
            ],
            "title": "Unified perceptual parsing for scene understanding",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Yuwen Xiong",
                "Mengye Ren",
                "Wenyuan Zeng",
                "Raquel Urtasun"
            ],
            "title": "Self-supervised representation learning from flow equivariance",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Bolei Zhou",
                "Hang Zhao",
                "Xavier Puig",
                "Sanja Fidler",
                "Adela Barriuso",
                "Antonio Torralba"
            ],
            "title": "Scene parsing through ade20k dataset",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Jinghao Zhou",
                "Chen Wei",
                "Huiyu Wang",
                "Wei Shen",
                "Cihang Xie",
                "Alan Yuille",
                "Tao Kong"
            ],
            "title": "ibot: Image bert pre-training with online tokenizer",
            "venue": "ICLR, 2022a. 8,",
            "year": 2022
        },
        {
            "authors": [
                "Xingyi Zhou",
                "Rohit Girdhar",
                "Armand Joulin",
                "Philipp Kr\u00e4henb\u00fchl",
                "Ishan Misra"
            ],
            "title": "Detecting twenty-thousand classes using image-level supervision",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Caron"
            ],
            "title": "2021) and use the frozen features of the transformer backbone to train a linear classifier in a supervised setting. We use global batch size of 1024 on the training set and evaluate on the validation set of ImageNet-1k",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "1 INTRODUCTION (To the question \u201cHave you read all the books in here?\u201d) No, only four of them. But I read those very, very carefully.\nJacques Derrida\nLearning from large scale datasets has been at the core of great progress. In particular, the field of self-supervised learning has allowed pretraining of neural networks to scale beyond the size of labelled datasets. By avoiding costly annotation, strong performance has been demonstrated by increasing the training dataset sizes into billions of images.\nBut how well are those images really used? At a rate of one image per second, a dataset of 1B images would take 317 years to watch. Yet, humans develop functioning visual systems much faster.* Besides potential genetic visual priors in humans, one stark difference is the type of data. Humans observe their visual surroundings in one continuous stream, only interrupted by sleep. Indeed, learning visual representations of images from videos is not new. However, previous works have found significant gaps in performance to image-pretrained models. They have mostly used object-centric videos scraped from the internet, and adapted image-based pretraining methods to use different frames as an extra form of data augmentation (Gordon et al., 2020; Parthasarathy et al., 2022).\nIn this work, we investigate two directions. First, in the direction of data, we introduce a new dataset of open-source first-person videos, recorded for the purpose of virtual \u201cwalking tours\u201d, inspired by (Wiles et al., 2022). These videos have several advantages. Not only are the individual frames dense in semantic categories \u2013 much more so than movies, as we analyze \u2013 but these videos also directly represent the viewpoint of a human, contain few or no shot cuts nor special effects and are long (1-3h). Another benefit is their transparency: indeed, one can watch the whole dataset in one setting. The dataset we create contains 10 Walking Tours (WT) videos with CC-BY license.\n*Humans develop face recognition (de Haan et al., 2001) and color sensitivity (Adams, 1987) in three months, depth perception in five months (Campos et al., 1978) and visual acuity in six months (Sokol, 1978).\nSecond, in the direction of the method, we develop a new self-supervised image-pretraining method that is uniquely suited for learning from natural, non-object-centric videos. Our approach is inspired by observing toddlers first learn to track objects and animals, then to recognize and differentiate them (Bomba & Siqueland, 1983; Quinn et al., 1993; Spelke & Kinzler, 2007). Our method, called DORA, is an end-to-end training approach that \u201ctracks to learn to recognize\u201d: given a video clip, objects in an initial frame are implicitly Discovered and tRAcked across time. The tracked objects are incentivized to be diverse by introducing a Sinkhorn-Knopp clustering of patch embeddings; the tracked instances are used as a learning signal for a classical multi-view SSL loss.\nSurprisingly, contrary to previous works, we find that our novel method obtains ImageNet-level performances by training on a single WT video, as evidenced by performances on segmentation and object detection downstream tasks. While humorously intentioned, Derrida\u2019s quote rings true to this finding and our results give some hope for alternative directions in SSL that depart from blind dataset scaling towards more efficient and smarter use of existing video data.\nTo summarize, our key contributions in this work are as follows:\n1. We introduce a new dataset of 10 WT videos, with single-video and mixed-video splits. The latter is conveniently equal in size to ImageNet. We analyze their usefulness compared to existing video and image datasets.\n2. We propose a new end-to-end self-supervised visual pretraining method called DORA. It builds upon DINO but is tailored to promote tracking of multiple objects across frames. We use it to learn strong image encoders and trace the source of its improvements through extensive ablations.\n3. We obtain strong performance on ADE20k segmentation and MS COCO detection, outperforming ImageNet-pretrained DINO, while instead pretraining on a single long video."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Self-supervised learning of image encoders from video data is a very active area of research. Video, and more generally temporal streams, have long been theorized to be ideal signals for unsupervised learning (Wiskott & Sejnowski, 2002). In computer vision, early methods have been very diverse and included pretext tasks such as egomotion prediction (Agrawal et al., 2015; Jayaraman & Grauman, 2015), active recognition (Jayaraman & Grauman, 2016), pose estimation (Chakraborty & Namboodiri, 2017), unsupervised object discovery (Croitoru et al., 2017), dense prediction (Pathak et al., 2017; Li et al., 2019), optical flow (Mahendran et al., 2018; Xiong et al., 2021), frame order prediction (Misra et al., 2016), view-point matching (Sermanet et al., 2018; Pirk et al., 2020) or learning visual correspondences (Wang et al., 2019).\nMore recently, there have been considerable advances in self-supervised learning using ImageNet, with the main theme being extracting multiple augmentations of an image (Chen et al., 2020; Caron et al., 2021) and training models to pull them together/apart. These methods have since percolated to learning from video frames (Gordon et al., 2020; Parthasarathy et al., 2022; Tschannen et al., 2020; Wang & Gupta, 2015; Orhan et al., 2020). Similar to this work, TimeTuning (Salehi et al., 2023) leverages the passage of time in videos by not treating it as simple augmentations. However, in contrast to our work, it requires an already image-pretrained backbone. VITO (Parthasarathy et al., 2022) improves performance relative to ImageNet, by using VideoNet, a large YouTube dataset of 10s videos from a similar class distribution and the same number of examples as ImageNet. In this\npaper, we show that it is possible to obtain strong results from a single long video, with a very different visual distribution compared to ImageNet / VideoNet."
        },
        {
            "heading": "3 WALKING TOURS DATASET",
            "text": ""
        },
        {
            "heading": "3.1 DATASET COLLECTION AND PROPERTIES",
            "text": "We collect from YouTube a new dataset of urban scenes called \u201cWalking Tours\u201d (WTours, or WT) comprising 10 egocentric videos of a person walking in different cities in Europe and Asia. The cities include Amsterdam, Bangkok, Chiang Mai, Istanbul, Kuala Lampur, Singapore, Stockholm, Venice, and Zurich. We also include a video from a Wildlife safari. Examples are shown in Figure 1. These videos are captured in 4K resolution (3840 \u00d7 2160 pixels) at 60 frames-per-second and are under Creative Commons License (CC-BY). The minimum video duration is 59 minutes (Wildlife safari), the maximum is 2 hours 55 minutes (Bangkok) and the average is 1 hour 38 minutes. Such videos are particularly interesting for visual learning because of the following properties:\n1. Large number of objects and actions. Each frame or clip taken from a video depicts several objects and actions, e.g. walking, riding a bike, sitting, drinking etc.\n2. Natural transition in lighting conditions. In some videos, the lighting gradually transitions from bright (late afternoon) to dim (dusk) then to dark (post sunset).\n3. Natural transition in scenes. The videos depict transitions between places, e.g. from city center to market place to residential areas to parks to water fronts etc.\n4. Natural object augmentations. Continuous variation e.g. of pose, deformation, viewpoint, perspective distortion, relative object position, occlusion, background clutter.\nThe abundance of information within these videos, encompassing a multitude of objects and complex scenes, presents a formidable challenge for manual annotation or curation, making it appropriate for unsupervised pretraining. To the best of our knowledge, we are the first to propose an egocentric video dataset for pretraining and evaluate it on a wealth of downstream tasks."
        },
        {
            "heading": "3.2 COMPARISON WITH OTHER VIDEO DATASETS",
            "text": "In Table 1, we compare WTours with existing video datasets. In summary, self-supervised pretraining on videos has been mostly limited to short, low-resolution datasets that rely on weak annotation in the form of video-text pairs (Bain et al., 2021; Miech et al., 2019) or are curated, e.g. their class balance is controlled, even if their annotation is unused (Kay et al., 2017). ImageNetaligned datasets (Gordon et al., 2020; Parthasarathy et al., 2022) contain short videos that are (semiautomatically) curated and annotated with the same distribution and classes as ImageNet. Egocentric video datasets (Damen et al., 2022; Grauman et al., 2022; Sener et al., 2022) have long, high-quality\nvideos, but are the result of significant manual work. In this paper we aim to learn from videos publicly available online.\nWTours videos are continuous, longer and higher-resolution than even other egocentric datasets. Using object detectors, we find that the average number of object classes is close to that of ImageNet and there is a high number of objects per frame, making WTours appropriate for representation learning. WTours is not curated and does not rely on search terms. It is data-first and more openended, thus well suited for the self-supervised setting. It is scalable since it requires no human labeling effort and more videos can be easily downloaded or even made. We are inspired by a 10k walking tours videos created by Wiles et al. (2022), which however is not publicly released and not studied for self-supervised learning. A more detailed discussion is given in subsection A.1."
        },
        {
            "heading": "3.3 DATASET ANALYSIS",
            "text": "In Figure 2, we analyse the properties of a single WTours video compared with videos of the same length from two other datasets, as well as two movie videos. In summary, our findings are as follows. From Figure 2(a), WT may exhibit gradual shifts in lightness, transitioning from bright to dim to dark, while Epic-Kitchens and AVA videos exhibit random brightness fluctuations. Lightness variations are not well expored in self-supervised pretraining. From Figure 2(b,c), unique classes appear more frequently and there are more unique objects per frame in WTours than in the other datasets. This makes WTours semantically richer. From Figure 2(d), WTours and Epic-Kitchens videos contain only one or two shots per entire video on average, while the other datasets contain hundreds. In subsection 5.2 and in Appendix C, we show that WTours significantly outperforms movies in downstream tasks, which is partially attributed to the absence of cuts. More detailed discussion of dataset analysis is given in subsection A.2."
        },
        {
            "heading": "4 ATTENTION-BASED MULTI-OBJECT TRACKING",
            "text": "High-level idea We introduce DORA, based on multi-object Discovery and tRAcking. As shown in Figure 3, it leverages the attention from the [CLS] token of distinct heads in a vision transformer to identify and consistently track multiple objects within a given frame across temporal sequences. On these, a teacher-student distillation loss is then applied. Importantly, we do not use any off-theshelf object tracker or optical flow network. This keeps our pipeline simple and does not require any additional data or training. It also ensures that the learned representation is robust.\nPreliminaries We are given a video clip consisting of T frames Xt \u2208 Rh\u00d7w\u00d7c for t \u2208 {1, . . . , T}, where h \u00d7 w is the spatial resolution and c is the number of channels. Each frame is split into n = hw/p2 non-overlapping patches of resolution p \u00d7 p. The patches are linearly projected into embeddings of dimension d and a [CLS] token embedding is prepended. This representation is input to a transformer encoder (Dosovitskiy et al., 2020). The output embeddings are Zt = g\u03b8(Xt) \u2208 R(n+1)\u00d7d, where mapping g\u03b8 includes the tokenizer and encoder, while \u03b8 denotes its learnable parameters. Given an embedding Z \u2208 R(n+1)\u00d7d, we write Z = [Z [CLS]; Z\u0303], where Z [CLS] \u2208 R1\u00d7d is the [CLS] token embedding and Z\u0303 \u2208 Rn\u00d7d are the patch embeddings.\nFollowing DINO (Caron et al., 2021), there is a student network with parameters \u03b8 and a teacher network with identical architecture and parameters \u03b8\u2032 obtained as the exponential moving average (EMA) of \u03b8 according to \u03b8\u2032 \u2190 \u03b1\u03b8\u2032 + (1\u2212 \u03b1)\u03b8. The encoder is followed by a head that includes an MLP and a scaled softmax, such that the output token embeddings can be interpreted as probabilities. We denote by f\u03b8 the mapping that includes the tokenizer, encoder and head.\nDiscovering objects with multi-head attention Starting at a first frame Xt0 , we obtain the query and key embeddings Q,K \u2208 R(n+1)\u00d7d from the last transformer layer of the teacher network\u2020. According to multi-head attention, these embeddings are partitioned as Q = [Q1, . . . , Qh], K = [K1, . . . ,Kh], where Qi,Ki \u2208 R(n+1)\u00d7d/h for i = 1, . . . , h and h is the number of heads. For each head i, the self-attention matrix Ai \u2208 R(n+1)\u00d7(n+1) is based on the dot-product similarity between the query and key embeddings:\nAi := softmax ( Qi(Ki)\u22a4/ \u221a d ) \u2208 R(n+1)\u00d7(n+1). (1)\nGiven an attention matrix A \u2208 R(n+1)\u00d7(n+1), let A[CLS] := [a1,2, . . . , a1,n] \u2208 R1\u00d7n be the [CLS]attention vector between the [CLS] and patch embeddings, where ai,j is the element (i, j) of A. We draw at random a subset I := {i1, . . . , ik} of k < h heads and collect their [CLS]-attention vectors into AI := [(Ai1)[CLS]; . . . ; (Aik)[CLS]] \u2208 Rk\u00d7n. Intuitively, as expressed in rows of matrix AI , the different heads attend to different objects in the frame (Caron et al., 2021).\nTo represent the k objects in the embedding space, we use matrix AI \u2208 Rk\u00d7n to form linear combinations of patch embeddings Q\u0303 \u2208 Rn\u00d7d, obtaining object prototypes\nP := AIQ\u0303 \u2208 Rk\u00d7d. (2)\nThis can be seen as the representation of k different [CLS] tokens in the full embedding space, capturing k objects at frame t0. Then, given the key embeddings Kt \u2208 R(n+1)\u00d7d at another frame t, we could track the objects by cross-attention\nTt := softmax ( PK\u0303\u22a4t / \u221a d ) \u2208 Rk\u00d7n, (3)\nwhere K\u0303t \u2208 Rn\u00d7d. Unfortunately, we observe in Figure 4 that the k attention maps obtained this way are spatially overlapping, meaning that each attention map is not delineating a single object.\n\u2020For simplicity, we drop t0 from the notation.\nEstablishing object-patch correspondences To discover spatially distinct objects, we propose to establish correspondences between prototypes and patch tokens. Let Z = g\u03b8\u2032(Xt0) \u2208 R(n+1)\u00d7d be the output embeddings of the teacher network, still at frame t0. We seek a correspondence between the rows of P \u2208 Rk\u00d7d and Z\u0303 \u2208 Rn\u00d7d, where Z\u0303 are the patch token embeddings. The goal is to find a transport plan M \u2208 Rk\u00d7n that minimizes the expected pairwise cost C := \u2212PZ\u0303\u22a4 \u2208 Rk\u00d7n between prototypes and patches, while incorporating an entropic regularizer with coefficient \u03f5. Matrix M is non-negative with row-wise sum 1/k and column-wise sum 1/n, representing a joint probability over P and Z\u0303 with uniform marginals. The minimal solution M\u2217 is unique and can be found by forming the matrix e\u2212C/\u03f5 and then applying the Sinkhorn-Knopp (SK) algorithm (Cuturi, 2013), i.e., iteratively normalizing its rows and columns:\nM\u2217 = SK ( exp ( PZ\u0303\u22a4/\u03f5 )) \u2208 Rk\u00d7n, (4)\nObserve the similarity with (1) and (3), where scaling is by \u221a d rather than \u03f5, exp is included in softmax and normalization is on rows only rather than iterative. Then, similarly with (2), we use the optimal transport plan M\u2217 \u2208 Rk\u00d7n to form linear combinations of patch embeddings Z\u0303 \u2208 Rn\u00d7d, obtaining the refined object prototypes\nP \u2032 = M\u2217Z\u0303 \u2208 Rk\u00d7d. (5)\nNow, given the key embeddings Kt \u2208 R(n+1)\u00d7d at another frame t, we track the objects by the refined cross-attention, similarly with (3):\nT \u2032t := softmax ( P \u2032K\u0303\u22a4t / \u221a d ) \u2208 Rk\u00d7n, (6)\nwhere K\u0303t \u2208 Rn\u00d7d. Indeed, Figure 4 confirms that each of the k resulting attention maps is associated with a spatially distinct object, thanks to the established correspondences.\nIn contrast to previous works that use SK in the context of self-supervised learning to force an equipartitioning of images to cluster labels (Asano et al., 2020; Caron et al., 2020; Oquab et al., 2023), we rather use optimal transport to re-balance spatial correspondences to different objects.\nMulti-object masking We use the cross-attention (6) to mask the input video clip for the student network, such that each masked clip can be considered as a multi-object crop. This crop plays a similar role with local crops in DINO (Caron et al., 2021), but it has arbitrary shape and tracks an object over video frames. In particular, given an input frame X \u2208 Rh\u00d7w\u00d7c with cross-attention matrix T \u2032 \u2208 Rk\u00d7n (6) and an object i \u2208 {1, . . . , k}, we reshape the i-th row of T \u2032 as h/p \u00d7 w/p and upsample to a h \u00d7 w attention map to match the spatial resolution of X, as shown in Figure 4. We repeat along the channel dimension to form tensor Ti \u2208 Rh\u00d7w\u00d7c and we mask X as\nXoi := X\u2299Ti, (7)\nwhere \u2299 is the Hadamard product. Following DINO (Caron et al., 2021), given an input frame Xt, we generate two standard resolution augmented global views Xat ,X b t . We introduce a multi-object loss LOt for frame t, applied to the [CLS] token between the teacher f\u03b8\u2032 output for one global view Xut and the student f\u03b8 output for the masked version X v,oi t of the other view X v t for i \u2208 {1, . . . , k}, where u, v \u2208 V = {a, b} and u \u0338= v:\nLOt := \u2211\nu,v\u2208V 1u \u0338=v k\u2211 i=1 f\u03b8\u2032(X u t ) [CLS] log ( f\u03b8(X v,oi t ) [CLS]) . (8) In addition, as detailed in subsection B.1, we apply a local loss, following multi-crop (Caron et al., 2020). The overall loss L is the sum of the two losses, averaged over all T frames."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "5.1 SETUP",
            "text": "Tasks and methods We perform self-supervised pretraining on a single WT tour video in Venice (referred to as WTVenice) or all 10 WT videos (referred to as WTall) and compare with other image and video datasets. To evaluate the quality of the learned representations, we use frozen features for classification, unsupervised object discovery and video object segmentation. We fine-tune for semantic segmentation, object detection and object tracking. We compare DORA with SoTA SSL methods (da Costa et al., 2022) using our settings. We provide more details in individual sections per task. Implementation details and hyperparameters are given in Appendix B."
        },
        {
            "heading": "5.2 ABLATIONS",
            "text": "We examine the effect of using different pretraining video dataset and different options and parameters for DORA, measuring performance of classification on ImageNet-1k (Deng et al., 2009) by linear probing (LP) accuracy and unsupervised object discovery on Pascal-VOC 2012 (Everingham et al.) by correct localization (CorLoc) (Sime\u0301oni et al., 2021).\nPretraining video dataset We study the impact of pretraining on diverse video datasets, encompassing object-centric videos such as Kinetics-400 (K-400) (Kay et al., 2017), egocentric videos like Epic-Kitchens (EK) (Damen et al., 2022) and a single movie, Movierom (Central). To maintain uniformity in terms of the number of frames, we curate a subset of videos from K-400 and EK, such that their total duration is the same as a single WT video. In Table 2a, we observe that although K-400 is object-centric, pretraining on WTours videos yields superior performance on ImageNet and Pascal-VOC 2012. Pretraining on a single movie yields is inferior to both WTours and K-400 by a large margin. This is possibly due to the presence of cuts, as studied in Appendix C.\nNumber of tracked objects We study the impact of the number k of objects. Objects are discovered using attention heads, where the total number of heads is in ViT-S/16 is h = 6. For k > h, we modify the MSA block as described in subsection B.2. In Table 2b, we observe that k = 3 works best. We hypothesize that this is a compromise between the number of objects that can be tracked and the multi-object loss (8) attempting to match small objects with the global crop.\nChoice of masking and Sinkhorn-Knopp We explore the effect of using a multi-object mask (7) vs. random block-wise (Zhou et al., 2022a) and the effect of improving object-patch correspondence through SK in refined cross-attention (6) vs. (3). In Table 2c, we observe that a multi-object mask leads to a remarkable performance improvement even in the absence of SK. In fact, random blockwise mask undermines object-patch correspondence, making the effect of SK negative. By contrast, SK improves performance in the presence of multi-object mask."
        },
        {
            "heading": "5.3 COMPARISON WITH STATE-OF-THE-ART",
            "text": "Dense scene understanding Table 3(a) shows semantic segmentation by fine-tuning on ADE20k (Zhou et al., 2017) using UperNet (Xiao et al., 2018). DORA outperforms DINO by 3% mIoU, and 1.8% Accm. It is interesting to note that DORA pretrained on 200k frames of a single WTours video outperforms DINO pretrained on 1.3M images of ImageNet-1k by 1.5% mIoU. A more comparable setting is DORA pretrained on 1.5M frames of WTall, which outperforms DINO pretrained on ImageNet by 3% mIoU. Table 3(b) shows object detection and instance segmentation by fine-tuning on MS-COCO (Lin et al., 2014) using Cascade RCNN (Cai & Vasconcelos, 2019). DORA outperforms DINO by 2.4% mAP and 2.6% mIoU. DORA pretrained on WTall outperforms DINO pretrained on ImageNet by 0.8% mIoU and 1.2% mAP. This shows that pretraining on WTours videos significantly improves the generality of DORA to dense prediction tasks, requiring only one tenth of the total images.\nVideo understanding Table 4(a) shows video object segmentation by using frozen features on DAVIS-2017 (Pont-Tuset et al., 2017), which assesses the ability to segment an object over its dynamic temporal changes. DORA captures detailed temporal deformations and outperforms baseline DINO by 3.4% Jm and 4.2%Fm. Using only a single video for pretraining, DORA achieves almost the same performance of DINO pretrained on ImageNet (56.4% vs. 57.4% Jm). Table 4(b) shows multi-object tracking by fine-tuning on GOT-10k (Huang et al., 2021) using SeqTrack (Chen et al., 2023). GOT-10k assesses the ability to track extremely fast moving objects, objects with illumination variation and low resolution. DORA achieves significant gains between 4-6% over DINO.\nImage classification and unsupervised object discovery We pretrain DORA on WTours and then we keep it frozen on the downstream task, indicating the quality of the pretrained features. Table 5(a) shows image classification on ImageNet-1k, measuring accuracy for linear probing and k-nearest neighbor. Table 5(b) shows unsupervised object discovery on Pascal-VOC 2012, using attention maps as segmentation masks to measure Jaccard similarity and CorLoc.\nOn both tasks, non-contrastive methods (DINO, iBOT, VICReg) outperform contrastive methods (SimCLR, SwAV), when pretrained on a single WT video. Importantly, non-contrastive methods are also more efficient to train, since no negative pairs are used. Also on both tasks, DORA outperforms DINO by a large margin, e.g. 11.6% LP and 3.9% k-NN on classification, when trained on a single WT video. Comparing DORA on WTVenice with the WTall dataset, the improvement brought by the full dataset is small when using DORA, although it is 10 times larger.\nIn Appendix D, we show that DORA outperforms SoTA methods on all tasks on ImageNet-1k."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "We have introduced a dataset of 10 walking tour videos \u2013 first-person videos taken by people touring a city, with no cuts, high resolution and that are hours long. We show that learning from clips taken from these videos is surprisingly powerful: with an appropriately tailored self-supervised learning method for videos, we obtain representations that rival those obtained on ImageNet when transferring to popular downstream image and video tasks. This differs from previous state-of-theart approaches to learning image encoders from video, which also obtain such results but require large video datasets, following closely the ImageNet blueprint.\nOur proposed learning method DORA is inspired by DINO, generalizing it to video by incorporating implicit multi-object tracking across video clips. We observe that the method leads to interesting emergent attention masks within the transformer model, that seem to latch on to particular objects, even through occlusions. This makes it uniquely suited to our newly introduced dataset."
        },
        {
            "heading": "A More on Walking Tours 1",
            "text": "A.1 Comparison with other video datasets . . . . . . . . . . . . . . . . . . . . . . . 1 A.2 Dataset analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2"
        },
        {
            "heading": "B More on experimental setup 2",
            "text": "B.1 Multi-crop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 B.2 Implementation details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 B.3 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3"
        },
        {
            "heading": "C More ablations 4",
            "text": ""
        },
        {
            "heading": "D More comparisons with state of the art 5",
            "text": ""
        },
        {
            "heading": "E More visualizations 6",
            "text": ""
        },
        {
            "heading": "A MORE ON WALKING TOURS",
            "text": "A.1 COMPARISON WITH OTHER VIDEO DATASETS\nIn Table 1, we compare WTours with different types of existing video datasets. Self-supervised pretraining on videos has been mostly limited to video datasets that rely on weak annotation in the form of video-text pairs (Bain et al., 2021; Miech et al., 2019) or even are curated, e.g. their class balance is controlled, even if their annotation is unused (Kay et al., 2017). Their average clip duration is small, e.g. less than 20 sec, and their resolution is also small, limiting the capacity to detect objects at a greater distance. By contrast, WTours videos are continuous, hours-long at high resolution and provide natural transitions of scenes and viewing conditions. They are not curated and thus better suited for the self-supervised setting.\nImageNet-aligned datasets such as R2V2 (Gordon et al., 2020) and VideoNet (Parthasarathy et al., 2022) contain videos that are curated and annotated with the same distribution and classes as ImageNet, meant for pretraining image encoders. These videos are short, i.e. 10 seconds on average. By contrast, WTours consists of a continuous stream of egocentric video, where the average number of classes is close to that of ImageNet, as shown in subsection 3.3. The rich information contained in 4K resolution, together with a high number of objects in a frame, makes it appropriate for representation learning. Importantly, the continuity and absence of curation make it more realistic and more comparable with human learning. Our dataset does not rely on a set of objects, human activities or other search terms but instead is data-first and more open-ended.\nDespite the large number of high-quality videos, egocentric video datasets (Damen et al., 2022; Grauman et al., 2022; Sener et al., 2022) have been used only for downstream tasks and thus come with extensive annotation. In comparison, WTours has 4-10 times longer average duration and twice the frame resolution. While WTours is smaller in terms of total duration and number of videos, it is scalable under the self-supervised setting since it requires no human labeling effort and more videos\ncan be easily found, downloaded or even made. This makes collecting more data as simple as a walk in the park.\nVery long video datasets. A large dataset of 10k WTours videos was created recently by (Wiles et al., 2022) but was not publicly released and not studied for self-supervised learning. Another dataset having hour-long videos is introduced in (Khan et al., 2020), in the context of sports analytics; it has not been explored for self-supervised learning either.\nA.2 DATASET ANALYSIS\nHere we present a more detailed discussion on the dataset analysis of subsection 3.3. We refer to Figure 2, where we analyse the properties of a single WTours video compared with videos of the same length from Epic-Kitchens (Damen et al., 2022) and AVA (Gu et al., 2018) datasets, as well as two movie videos, an action movie and a romantic movie.\nVariation in lightness We measure the change in perceived brightness using the lightness value (L) across consecutive frames. From Figure 2(a), we observe a gradual shift at roughly 150 min into the WTours video, transitioning from bright to dim to dark. By contrast, Epic-Kitchens and AVA videos exhibit random brightness fluctuations, alternating between dim and bright conditions. Typically, self-supervised pretraining happens on datasets with uniform brightness levels. Datasets featuring such brightness variations are less expored.\nVariation in number of objects Using Detic (Zhou et al., 2022b), a DETR-style object detector trained on ImageNet-21k, we detect objects in each frame. Figure 2(b) shows the number of unique objects per frame and Figure 2(c) shows their frequency in the entire video. We observe that WTours contains 703 unique objects, while Epic-Kitchens has 373, AVA has 663 and Movie-2 has 259. The unique objects appear more frequently and there are more unique objects per frame in WTours than in the other datasets. This makes WTours semantically richer, despite coming from one continuous stream of video. Using videos with a large number of objects can encourage the model to capture complex relations and variations in the data.\nVariation in shots Egocentric videos are typically captured in a single uninterrupted take, with exceptions being post-processed special effects or cuts. In Figure 2(d), we find that, on average, WTours and Epic-Kitchens videos contain only one or two shots per entire video, while AVA contains 406, an action movie (Movieact) (Skiptrace) contains 2000 and a romantic movie (Movierom) (Central) contains 667. The substantial number of shots in movies and AVA poses challenges for representation learning methods that rely on object tracking or optical flow. In subsection 5.2, we show that WTours significantly outperforms movies in downstream tasks, which may be attributed to the absence of cuts."
        },
        {
            "heading": "B MORE ON EXPERIMENTAL SETUP",
            "text": "B.1 MULTI-CROP\nFollowing DINO (Caron et al., 2021) and iBOT (Zhou et al., 2022a), we apply the multi-crop strategy (Caron et al., 2020). In particular, we generate m local crops X\u2113it of smaller resolution for i \u2208 {1, . . . ,m}. The local loss LLCt for frame t is applied to the [CLS] token between the teacher f\u03b8\u2032 output for a global view Xut and the student f\u03b8 output for the local crop X \u2113i t for i \u2208 {1, . . . ,m}:\nLLCt := \u2211 v\u2208V m\u2211 i=1 f\u03b8\u2032(X v t ) [CLS] log ( f\u03b8(X \u2113i t ) [CLS] )\n(9)\nThe overall loss L is the sum of the multi-object loss LOt (8) and the local loss L LC t (9), averaged over all T frames:\nL := 1\nT T\u2211 t=1 (LOt + L LC t ). (10)\nB.2 IMPLEMENTATION DETAILS\nCode will be published as open-source code. We use ViT-S/16 (Dosovitskiy et al., 2020) as the backbone in all our experiments. For each mini-batch, we randomly sample clips from the video, consisting of T = 8 frames temporally separated by 1 second i.e. we sample one frame every 30. Objects discovered in the first frame are tracked over the following 7 frames. Since each frame contains several different objects, applying the standard multi-crop augmentation (Caron et al., 2020) to the entire frame would result in crops with very different visual content or noisy positive pairs. Instead, we apply multi-crop to a 300 \u00d7 300 crop that we first take from the frame. Following DINO (Caron et al., 2021), we obtain two global crops and six local crops. Masking (7) is applied to the global crops seen by the student for the multi-object loss (8), while local crops are seen directly by the student for the local loss (9). We train for 100 epochs by default.\nObjects are discovered using attention heads, where the total number of heads is in ViT-S/16 is limited to h = 6. For the purpose of the ablation of the number k of objects for k > h in Table 2b, we modify the MSA block in the final layer, resulting in configurations of 16 and 32 heads. Consequently, we can identify and track up to 16 and 32 objects within the video clip. To accomplish this, we decompose the query and key embeddings of dimension d = 768 into 16 and 32 subvectors, resulting in new feature dimensions of 24 and 12 respectively, as opposed to 64 for 6 heads. In Table 2b, we observe that tracking 16 or 32 objects results in overall poor performance possibly due to the small feature dimension, which encodes poor representations.\nB.3 HYPERPARAMETERS\nImageNet-1k: Linear probing and k-NN We pretrain DORA in a self-supervised setting with ViT-S/16 using DINO for 100 and 300 epochs. We use two global and six local crops for each clip and train on 8 A100 GPUs with a global batch size of 16\u00d7 8 = 128. We use LARS with a learning rate of 5\u00d7 10\u22124, minimum learning rate of 1\u00d7 10\u22126, global crop scale of [0.4, 1.0] and local crop scale [0.05, 0.4].\nFor linear probing, we follow (Caron et al., 2021) and use the frozen features of the transformer backbone to train a linear classifier in a supervised setting. We use global batch size of 1024 on the training set and evaluate on the validation set of ImageNet-1k. We use top-1 accuracy (%) as our evaluation metric. For k-NN, we freeze the backbone and extract features of training images, then use a k-nearest neighbour classifier with k = 20.\nPascal-VOC 2012: Object discovery We use the validation set of Pascal VOC 2012 (Everingham et al.), which comprises a total of 1449 images. Following LOST (Sime\u0301oni et al., 2021), we use the averaged self-attention map, extracted from the final layer of a our pretrained ViT-S/16, to retain 80% of the mass. We use the Jaccard similarity J measured as overlap between predicted mask P and the ground truth mask G as J(P,G) = G\u2229PG\u222aP . We also use CorLoc, which measures the number of correct predicted boxes, where a predicted box is said to be correct if its IoU \u2265 0.5.\nADE20k: Semantic segmentation We evaluate DORA on ADE20k (Zhou et al., 2017) for semantic segmentation. The dataset includes 20,000 images in the training set and 2,000 images in the validation set. We use UperNet (Xiao et al., 2018) as the segmentation model and use DORA pretrained on WT to initialize the backbone. Following the experimental settings in iBOT (Zhou et al., 2022a), we use AdamW (Loshchilov & Hutter, 2019) with an initial learning rate of 6\u00d710\u22125, weight decay of 1\u00d710\u22122, and linear warmup of 1,500 iterations. We fine-tune for 160,000 iterations with a batch size of 4.\nMS-COCO: Object detection We evaluate DORA for object detection and instance segmentation on MS-COCO. We use Cascade Mask R-CNN (Cai & Vasconcelos, 2019), which produces bounding boxes and instance masks simultaneously on the COCO dataset. We use a multi-scale training strategy, where we resize images to have a shorter side ranging between 480 and 800, ensuring that the longer side does not exceed 1,333 pixels. The learning rate is 1 \u00d7 10\u22124 and the weight decay is 0.05. During training, we fine-tune the entire network using a 1\u00d7 schedule, which involves 12 epochs with learning rate reductions by a factor of 10 at epochs 9 and 11. We explore different layer decay rates, specifically 0.65, 0.75, 0.8, 0.9, with a rate of 1.0 indicating no decay.\nTo generate hierarchical feature maps, we utilize the features produced by layers 4, 6, 8, and 12 of our network and apply two deconvolutions for layer 4, one deconvolution for layer 6, identity mapping for layer 8, and max-pooling for layer 12. These post-processing steps enable the creation of hierarchical feature representations. It is important to note that we do not employ multi-scale testing in our experiments.\nDAVIS-2017: Video object segmentation We assess the performance of DORA for video object segmentation on DAVIS 2017 dataset (Pont-Tuset et al., 2017), which involves segmenting between 2 to 4 objects within the video frames. We follow DINO (Caron et al., 2021) and evaluate on video frames with a resolution of 480p. We apply label propagation on the attention map from our pretrained model and use mean region-based similarity Jm and mean contour-based accuracy Fm as our evaluation metrics.\nGOT-10k: Object tracking We evaluate the object-tracking performance of DORA on the GOT10k dataset (Huang et al., 2019). This is a large-scale benchmark for object tracking that contains 563 categories of common moving objects. The training set contains around 10,000 videos and the test set contains 180 videos. Another challenging aspect of this dataset is that the object classes in the training and test set are non-overlapping. We use the SeqTrack (Chen et al., 2023) codebase to evaluate the performance of different methods on this dataset. In particular, we initialize the encoder weights of SeqTrack with the self-supervised weights and keep them frozen during training. While training, we only update the parameters of the lightweight decoder which consists of 2 transformer blocks. We use all the default hyperparameters. We report mean average overlap (mAO) and success rate (SR) at different thresholds. The mAO measures the class-balanced average overlap between the ground truth and predicted bounding boxes whereas SR indicates the percentage of accurately tracked ground truth bounding boxes where the overlap crosses a certain threshold."
        },
        {
            "heading": "C MORE ABLATIONS",
            "text": "Pretraining WT video We study the effect of pretraining on different videos of WTours. In Table 6a, we observe that the effect is minimal on both image classification and unsupervised object discovery. Notably, the fluctuation in illumination conditions within the Bangkok video influences the performance on image classification. It is also interesting to note that, while pretraining on Amsterdam is best on image classification, pretraining on Venice is best on object discovery. This could be due to the large overlap of objects in these videos with respect to the downstream datasets. However, the consistency of our method across diverse videos indicates that DORA is robust to variations in scenes, number of objects and lighting conditions.\nPresence of cuts We now analyse the effect of cuts in representation learning. Cuts are defined as instant transitions from one shot to the next, which is frequent in movies. In action movies, a single shot lasts around 4 seconds, while in romance movies, around 12 seconds on average\u2021. To\n\u2021https://stephenfollows.com/many-shots-average-movie/\nunderstand the effect of cuts, we compare pretraining on WTours videos and a romance movie. We use PySceneDetect (Castellano) to extract the cut timestamps in the movie and we pretrain DORA by sampling clips that do not intersect cuts; cuts naturally do not exist in WT videos. In Table 6b, we observe that the performance improves significantly in the absence of cuts, as tracking in DORA will fail across a cut.\nLonger pretraining We investigate the impact of pretraining DoRA on a single WT video and all WT videos over 300 epochs. The results presented in Table 7 indicate a notable enhancement in performance when DoRA is pretrained for 300 epochs in general. Furthermore, we observe that DoRA pretrained on WTall surpasses WTVenice in VOS pretraining performance, in contrast to the situation where DoRA was pretrained for 100 epochs. It is worth highlighting that the extended pretraining duration also enables DoRA to achieve comparable results to DINO pretrained on ImageNet-1K, while demonstrating superior performance in dense scene understanding tasks."
        },
        {
            "heading": "D MORE COMPARISONS WITH STATE OF THE ART",
            "text": "Pretraining on ImageNet-1k We pretrain DORA on ImageNet-1k and compare with SoTA methods on multiple tasks. Unlike videos, we discover objects but do not track them. Instead, images in a mini-batch are processed independently. Given an input image X, we obtain refined object prototypes as usual (5), but the refined cross-attention (6) is with K\u0303t replaced by K\u0303 of the same image X. The same image X is masked for the student (7). The loss is given again by (8) and (9) with Xt replaced by X, averaged over the mini-batch. We refer to this version as DORA without tracking or DORA\u22c6.\nDINO (Caron et al., 2021) and iBOT (Zhou et al., 2022a) use only one global crop for the student, while DORA uses k object crops. To compensate, we perform an experiment where we pretrain DORA\u22c6 for 60 epochs and the competitors for 100, thus all methods having the same training time.\nFrom Table 8, we observe that DORA outperforms state-of-the-art self-supervised learning (SSL) methods like DINO and iBOT on image downstream tasks. This demonstrates that the multi-object loss not only enhances performance when pretrained on WTours videos but also achieves superior results when pretrained on ImageNet-1k images."
        },
        {
            "heading": "E MORE VISUALIZATIONS",
            "text": "Figures 5 and 6 show example attention maps obtained using SK on different clips. These figures show that SK (6) leads to attention maps that exhibit spatial locality and are well aligned with objects in the scene. Remarkably, the masks seem to be even robust to occlusions, as shown in the sequence with a bicycle moving behind traffic lights."
        }
    ],
    "year": 2023
}