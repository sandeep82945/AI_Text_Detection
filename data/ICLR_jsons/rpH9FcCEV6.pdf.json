{
    "abstractText": "Recently, diffusion models have achieved remarkable success in generating tasks, including image and audio generation. However, like other generative models, diffusion models are prone to privacy issues. In this paper, we propose an efficient query-based membership inference attack (MIA), namely Proximal Initialization Attack (PIA), which utilizes groundtruth trajectory obtained by \u03b5 initialized in t = 0 and predicted point to infer memberships. Experimental results indicate that the proposed method can achieve competitive performance with only two queries that achieve at least 6\u00d7 efficiency than the previous SOTA baseline on both discrete-time and continuous-time diffusion models. Moreover, previous works on the privacy of diffusion models have focused on vision tasks without considering audio tasks. Therefore, we also explore the robustness of diffusion models to MIA in the text-tospeech (TTS) task, which is an audio generation task. To the best of our knowledge, this work is the first to study the robustness of diffusion models to MIA in the TTS task. Experimental results indicate that models with mel-spectrogram (image-like) output are vulnerable to MIA, while models with audio output are relatively robust to MIA. Code is available at https://github.com/kong13661/PIA.",
    "authors": [
        {
            "affiliations": [],
            "name": "Fei Kong"
        },
        {
            "affiliations": [],
            "name": "Jinhao Duan"
        },
        {
            "affiliations": [],
            "name": "Ruipeng Ma"
        },
        {
            "affiliations": [],
            "name": "Hengtao Shen"
        },
        {
            "affiliations": [],
            "name": "Xiaoshuang Shi"
        },
        {
            "affiliations": [],
            "name": "Xiaofeng Zhu"
        },
        {
            "affiliations": [],
            "name": "Kaidi Xu"
        }
    ],
    "id": "SP:bcbb3d578ba0681de7d6f2b350e6f0b90ce01c21",
    "references": [
        {
            "authors": [
                "Rishi Bommasani",
                "Drew A Hudson",
                "Ehsan Adeli",
                "Russ Altman",
                "Simran Arora",
                "Sydney von Arx",
                "Michael S Bernstein",
                "Jeannette Bohg",
                "Antoine Bosselut",
                "Emma Brunskill"
            ],
            "title": "On the opportunities and risks of foundation models",
            "venue": "arXiv preprint arXiv:2108.07258,",
            "year": 2021
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Steve Chien",
                "Milad Nasr",
                "Shuang Song",
                "Andreas Terzis",
                "Florian Tramer"
            ],
            "title": "Membership inference attacks from first principles",
            "venue": "In IEEE Symposium on Security and Privacy,",
            "year": 2022
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Jamie Hayes",
                "Milad Nasr",
                "Matthew Jagielski",
                "Vikash Sehwag",
                "Florian Tramer",
                "Borja Balle",
                "Daphne Ippolito",
                "Eric Wallace"
            ],
            "title": "Extracting training data from diffusion models",
            "venue": "arXiv preprint arXiv:2301.13188,",
            "year": 2023
        },
        {
            "authors": [
                "Dingfan Chen",
                "Ning Yu",
                "Yang Zhang",
                "Mario Fritz"
            ],
            "title": "Gan-leaks: A taxonomy of membership inference attacks against generative models",
            "venue": "In SIGSAC Conference on Computer and Communications Security,",
            "year": 2020
        },
        {
            "authors": [
                "Nanxin Chen",
                "Yu Zhang",
                "Heiga Zen",
                "Ron J. Weiss",
                "Mohammad Norouzi",
                "William Chan"
            ],
            "title": "Wavegrad: Estimating gradients for waveform generation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Tim Dockhorn",
                "Arash Vahdat",
                "Karsten Kreis"
            ],
            "title": "Score-based generative modeling with criticallydamped langevin diffusion",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Jeff Donahue",
                "Philipp Kr\u00e4henb\u00fchl",
                "Trevor Darrell"
            ],
            "title": "Adversarial feature learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Jinhao Duan",
                "Fei Kong",
                "Shiqi Wang",
                "Xiaoshuang Shi",
                "Kaidi Xu"
            ],
            "title": "Are diffusion models vulnerable to membership inference attacks",
            "venue": "International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Vincent Dumoulin",
                "Ishmael Belghazi",
                "Ben Poole",
                "Alex Lamb",
                "Mart\u00edn Arjovsky",
                "Olivier Mastropietro",
                "Aaron C. Courville"
            ],
            "title": "Adversarially learned inference",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "Communications of the ACM,",
            "year": 2020
        },
        {
            "authors": [
                "Benjamin Hilprecht",
                "Martin H\u00e4rterich",
                "Daniel Bernau"
            ],
            "title": "Monte carlo and reconstruction membership inference attacks against generative models",
            "venue": "Proceedings on Privacy Enhancing Technologies,",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans",
                "Alexey Gritsenko",
                "William Chan",
                "Mohammad Norouzi",
                "David J. Fleet"
            ],
            "title": "Video diffusion models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Kalin Hristov"
            ],
            "title": "Artificial intelligence and the copyright",
            "venue": "dilemma. Idea,",
            "year": 2016
        },
        {
            "authors": [
                "Hailong Hu",
                "Jun Pang"
            ],
            "title": "Membership inference of diffusion models",
            "venue": "arXiv preprint arXiv:2301.09956,",
            "year": 2023
        },
        {
            "authors": [
                "Rongjie Huang",
                "Max W.Y. Lam",
                "Jun Wang",
                "Dan Su",
                "Dong Yu",
                "Yi Ren",
                "Zhou Zhao"
            ],
            "title": "Fastdiff: A fast conditional diffusion model for high-quality speech synthesis",
            "venue": "In European Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Keith Ito",
                "Linda Johnson"
            ],
            "title": "The lj speech dataset. https://keithito.com/ LJ-Speech-Dataset/, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Yamagishi Junichi",
                "Veaux Christophe",
                "MacDonald Kirsten"
            ],
            "title": "Cstr vctk corpus: English multispeaker corpus for cstr voice cloning",
            "venue": "toolkit. https://datashare.ed.ac.uk/handle/",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Zhifeng Kong",
                "Wei Ping",
                "Jiaji Huang",
                "Kexin Zhao",
                "Bryan Catanzaro"
            ],
            "title": "Diffwave: A versatile diffusion model for audio synthesis",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny",
            "year": 2009
        },
        {
            "authors": [
                "Alexander C Li",
                "Mihir Prabhudesai",
                "Shivam Duggal",
                "Ellis Brown",
                "Deepak Pathak"
            ],
            "title": "Your diffusion model is secretly a zero-shot classifier",
            "venue": "arXiv preprint arXiv:2303.16203,",
            "year": 2023
        },
        {
            "authors": [
                "Andreas Lugmayr",
                "Martin Danelljan",
                "Andres Romero",
                "Fisher Yu",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Repaint: Inpainting using denoising diffusion probabilistic models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Tomoya Matsumoto",
                "Takayuki Miura",
                "Naoto Yanai"
            ],
            "title": "Membership inference attacks against diffusion models",
            "venue": "arXiv preprint arXiv:2302.03262,",
            "year": 2023
        },
        {
            "authors": [
                "Dang Pham",
                "Tuan M.V. Le"
            ],
            "title": "Auto-encoding variational bayes for inferring topics and visualization",
            "venue": "In International Conference on Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Vadim Popov",
                "Ivan Vovk",
                "Vladimir Gogoryan",
                "Tasnima Sadekova",
                "Mikhail Kudinov"
            ],
            "title": "Grad-tts: A diffusion probabilistic model for text-to-speech",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical textconditional image generation with clip latents",
            "year": 2022
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L Denton",
                "Kamyar Ghasemipour",
                "Raphael Gontijo Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans"
            ],
            "title": "Photorealistic textto-image diffusion models with deep language understanding",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tim Salimans",
                "Jonathan Ho"
            ],
            "title": "Progressive distillation for fast sampling of diffusion models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Tim Salimans",
                "Ian Goodfellow",
                "Wojciech Zaremba",
                "Vicki Cheung",
                "Alec Radford",
                "Xi Chen"
            ],
            "title": "Improved techniques for training gans",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Romain Beaumont",
                "Richard Vencu",
                "Cade Gordon",
                "Ross Wightman",
                "Mehdi Cherti",
                "Theo Coombes",
                "Aarush Katta",
                "Clayton Mullis",
                "Mitchell Wortsman",
                "Patrick Schramowski",
                "Srivatsa Kundurthy",
                "Katherine Crowson",
                "Ludwig Schmidt",
                "Robert Kaczmarczyk",
                "Jenia Jitsev"
            ],
            "title": "Laion-5b: An open large-scale dataset for training next generation image-text models, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Reza Shokri",
                "Marco Stronati",
                "Congzheng Song",
                "Vitaly Shmatikov"
            ],
            "title": "Membership inference attacks against machine learning models",
            "venue": "In IEEE Symposium on Security and Privacy,",
            "year": 2017
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P. Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Zhisheng Xiao",
                "Karsten Kreis",
                "Arash Vahdat"
            ],
            "title": "Tackling the generative learning trilemma with denoising diffusion gans",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Jie Xu",
                "Yazhou Ren",
                "Huayi Tang",
                "Xiaorong Pu",
                "Xiaofeng Zhu",
                "Ming Zeng",
                "Lifang He"
            ],
            "title": "MultiVAE: Learning disentangled view-common and view-peculiar visual representations for multi-view clustering",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Kaidi Xu",
                "Sijia Liu",
                "Pu Zhao",
                "Pin-Yu Chen",
                "Huan Zhang",
                "Quanfu Fan",
                "Deniz Erdogmus",
                "Yanzhi Wang",
                "Xue Lin"
            ],
            "title": "Structured adversarial attack: Towards general implementation and better interpretability",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Kaidi Xu",
                "Gaoyuan Zhang",
                "Sijia Liu",
                "Quanfu Fan",
                "Mengshu Sun",
                "Hongge Chen",
                "Pin-Yu Chen",
                "Yanzhi Wang",
                "Xue Lin"
            ],
            "title": "Adversarial t-shirt! evading person detectors in a physical world",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Ruihan Yang",
                "Prakhar Srivastava",
                "Stephan Mandt"
            ],
            "title": "Diffusion probabilistic modeling for video generation",
            "venue": "arXiv preprint arXiv:2203.09481,",
            "year": 2022
        },
        {
            "authors": [
                "Samuel Yeom",
                "Irene Giacomelli",
                "Matt Fredrikson",
                "Somesh Jha"
            ],
            "title": "Privacy risk in machine learning: Analyzing the connection to overfitting",
            "venue": "In IEEE Computer Security Foundations Symposium,",
            "year": 2018
        },
        {
            "authors": [
                "Chenxi Yuan",
                "Mohsen Moghaddam"
            ],
            "title": "Attribute-aware generative design with generative adversarial networks",
            "venue": "IEEE Access,",
            "year": 2020
        },
        {
            "authors": [
                "Chenxi Yuan",
                "Jinhao Duan",
                "Nicholas J Tustison",
                "Kaidi Xu",
                "Rebecca A Hubbard",
                "Kristin A Linn"
            ],
            "title": "Remind: Recovery of missing neuroimaging using diffusion models with application to alzheimer\u2019s disease",
            "venue": "medRxiv,",
            "year": 2023
        },
        {
            "authors": [
                "Chenxi Yuan",
                "Tucker Marion",
                "Mohsen Moghaddam"
            ],
            "title": "Dde-gan: Integrating a data-driven design evaluator into generative adversarial networks for desirable and diverse concept generation",
            "venue": "Journal of Mechanical Design,",
            "year": 2023
        },
        {
            "authors": [
                "H. Zen",
                "V. Dang",
                "R. Clark",
                "Y. Zhang",
                "R.J. Weiss",
                "Y. Jia",
                "Z. Chen",
                "Y. Wu"
            ],
            "title": "Libritts: A corpus derived from librispeech for text-to-speech",
            "venue": "In Proceedings of Interspeech,",
            "year": 2019
        },
        {
            "authors": [
                "Chenshuang Zhang",
                "Chaoning Zhang",
                "Sheng Zheng",
                "Mengchun Zhang",
                "Maryam Qamar",
                "Sung-Ho Bae",
                "In So Kweon"
            ],
            "title": "A survey on audio diffusion models: Text to speech synthesis and enhancement in generative ai",
            "venue": "CoRR, abs/2303.13336,",
            "year": 2023
        },
        {
            "authors": [
                "Huan Zhang",
                "Shiqi Wang",
                "Kaidi Xu",
                "Yihan Wang",
                "Suman Jana",
                "Cho-Jui Hsieh",
                "Zico Kolter"
            ],
            "title": "A branch and bound framework for stronger adversarial attacks of relu networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Yuheng Zhang",
                "Ruoxi Jia",
                "Hengzhi Pei",
                "Wenxiao Wang",
                "Bo Li",
                "Dawn Song"
            ],
            "title": "The secret revealer",
            "year": 2024
        },
        {
            "authors": [
                "Duan"
            ],
            "title": "2023), and Laion5, COCO for stable diffusion Rombach et al. (2022). Unless otherwise specified, we randomly select half of the samples as a training set and the other half as the hold-out set. B.1 IMPLEMENTATIONS DETAILS For the audio generation",
            "year": 2022
        },
        {
            "authors": [
                "Carlini"
            ],
            "title": "2022), Fig. 8 and Fig. 9 display the log-scaled ROC curves. These curves demonstrate that the proposed method outperforms NA and SecMI at most of times",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Recently, the diffusion model Ho et al. (2020); Song et al. (2021b); Song & Ermon (2019) has emerged as a powerful approach in the field of generative tasks, achieving notable success in image generation Rombach et al. (2022); Saharia et al. (2022), audio generation Popov et al. (2021); Kong et al. (2021), video generation Yang et al. (2022); Ho et al. (2022), and other domains. However, like other generative models such as GANs Goodfellow et al. (2020) and VAEs Kingma & Welling (2013); Xu et al. (2021), the diffusion model may also be exposed to privacy risks Bommasani et al. (2021) and copyright disputes Hristov (2016). Dangers such as privacy leaks Pham & Le (2020) and data reconstruction Zhang et al. (2020) may compromise the model. Recently, some researchers have explored this topic Duan et al. (2023); Matsumoto et al. (2023); Hu & Pang (2023); Carlini et al. (2023), demonstrating that diffusion models are also vulnerable to privacy issues.\nMembership Inference Attacks (MIAs) are the most common privacy risks Shokri et al. (2017). MIAs can cause privacy concerns directly and can also contribute to privacy issues indirectly as part of data reconstruction. Given a pre-trained model, MIA aims to determine whether a sample is in the training set or not.\nGenerally speaking, MIA relies on the assumption that a model fits the training data better Yeom et al. (2018); Shokri et al. (2017), resulting in a smaller training loss. Recently, several MIA techniques have been proposed for diffusion models Duan et al. (2023); Matsumoto et al. (2023); Hu & Pang (2023). We refer to the query-based methods proposed in Matsumoto et al. (2023); Hu & Pang (2023) as Naive Attacks because they directly employ the training loss for the attack. However, unlike GANs or VAEs, the training loss for diffusion models is not deterministic because it requires the generation of Gaussian noise. The random Gaussian noise may not be the one in which diffusion models fit best.\n\u2217Equal corresponding author\nThis can negatively impact the performance of the MIA attack. To address this issue, the concurrent work SecMI Duan et al. (2023) adopts an iterative approach to obtain the deterministic x at a specific time t, but this requires more queries, resulting in longer attack times. As models grow larger, the time required for the attack also increases, making time an important metric to consider.\nTo reduce the time consumption, inspired by DDIM and SecMI, we proposed a Proximal Initialization Attack (PIA) method, which derives its name from the fact that we utilize the diffusion model\u2019s output at time t = 0 as the noise \u03f5. PIA is a query-based MIA that relies solely on the inference results and can be applied not only to discrete time diffusion models Ho et al. (2020); Rombach et al. (2022) but also to continuous time diffusion models Song et al. (2021b). We evaluate the effectiveness of our method on three image datasets, CIFAR10 Krizhevsky et al. (2009), CIFAR100 and TinyImageNet for DDPM and on two images dataset, COCO2017 Lin et al. (2014) and Laion5B Schuhmann et al. (2022) for Stable DIffuion, as well as three audio datasets, LJSpeech Ito & Johnson (2017), VCTK Junichi et al. (2019), and LibriTTS Zen et al. (2019).\nTo our knowledge, recent research on MIA of diffusion models has only focused on image data, and there has been no exploration of diffusion models in the audio domain. However, audio, such as music, encounters similar copyright and privacy concerns as those in the image domain CNN (2022); WashingtonPost (2022). Therefore, it is essential to conduct privacy research in the audio domain to determine whether audio data is also vulnerable to attacks and to identify which types of diffusion models are more robust against privacy attacks. To investigate the robustness of MIA on audio data, we conduct experiments using Naive Attack, SecMI Duan et al. (2023), and our proposed method on three audio models: Grad-TTS Popov et al. (2021), DiffWave Kong et al. (2021), and FastDiff Huang et al. (2022). The results suggest that the robustness of MIA on audio depends on the output type of the model.\nOur contributions can be summarized as follows:\n\u2022 We propose a query-based MIA method called PIA. Our method employs the output at t = 0 as the initial noise and the errors between the forward and backward processes as the attack metric. We generalize the PIA on both discrete-time and continuous-time diffusion models.\n\u2022 Our study is the first to evaluate the robustness of MIA on audio data. We evaluate the robustness of MIA on three TTS models (Grad-TTS, DiffWave, FastDiff) and three TTS datasets (LJSpeech, VCTK, Libritts) using Naive Attack, SecMI, and our proposed method.\n\u2022 Our evaluations show that PIA matches SecMI\u2019s AUC performance and outperforms it in TPR @ 1% FPR, while being 5-10 times faster. Moreover, our data imply that in text-tospeech tasks, models producing audio are more resistant to MIA attacks than those generating image-like mel-spectrograms. We therefore suggest using audio-output generation models to minimize privacy risks in audio creation tasks."
        },
        {
            "heading": "2 RELATED WORKS AND BACKGROUND",
            "text": "Generative Diffusion Models Generative diffusion models have recently achieved significant success in both image Ramesh et al. (2022); Rombach et al. (2022); Yuan et al. (2023a) and audio generation tasks Huang et al. (2022); Chen et al. (2021); Popov et al. (2021). Unlike GANs Goodfellow et al. (2020); Yuan & Moghaddam (2020); Yuan et al. (2023b), which consist of a generator and a discriminator, diffusion models generate samples by fitting the inverse process of a diffusion process from Gaussian noise. Compared to GANs, diffusion models typically produce higher quality samples and avoid issues such as checkerboard artifacts Salimans et al. (2016); Donahue et al. (2017); Dumoulin et al. (2017). A diffusion process is defined as xt = \u221a \u03b1txt\u22121 + \u221a \u03b2t\u03f5t, \u03f5t \u223c N (0, I), where \u03b1t + \u03b2t = 1 and \u03b2t increases gradually as t increases, so that eventually, xt approximates a random Gaussian noise. In the reverse diffusion process, x\u2032t still follows a Gaussian distribution, assuming the variance remains the same as in the forward diffusion process, and the mean is defined as \u00b5\u0303t = 1\u221aat ( xt \u2212 \u03b2t\u221a1\u2212a\u0304t \u03f5\u0304\u03b8(xt, t) ) , where \u03b1\u0304t = \u220ft k=0 \u03b1k and \u03b1\u0304t + \u03b2\u0304t = 1. The reverse diffusion process becomes xt\u22121 = \u00b5\u0303t + \u221a \u03b2t\u03f5, \u03f5 \u223c N (0, I). One can obtain a loss function Eq. (1) by minimizing the distance between the predicted and groundtruth distributions. Song et al. (2021b) transforms the discrete-time diffusion process into a continuous-time process and uses SDE ( Stochastic Differential Equation) to express the diffusion process. To accelerate the generation process, several methods have been proposed, such as Salimans & Ho (2022); Dockhorn et al. (2022); Xiao et al. (2022). DDIM Song et al. (2021a) is another popular method that proposes a forward process different from diffusion process with the same loss function as DDPM, allowing it to reuse the model trained by DDPM while achieving higher generation speed.\nL = Ex0,\u03f5\u0304t [\u2225\u2225\u03f5\u0304t \u2212 \u03f5\u03b8 (\u221a\u03b1\u0304tx0 +\u221a1\u2212 \u03b1\u0304t\u03f5\u0304t, t)\u2225\u22252] . (1)\nMembership Inference Privacy Different from conventional adversarial attacks Xu et al. (2018; 2020); Zhang et al. (2022), Membership inference attack (MIA) Shokri et al. (2017) aims to determine whether a sample is part of the training data. It can be formally described as follows: given two sets, the training set Dt and the hold-out set Dh, a target model m, and a sample x that either belongs to Dt or Dh, the goal of MIA is to find a classifier or function f(x,m) that determines which set x belongs to, with f(x,m) \u2208 {0, 1} and f(x,m) = 1 indicating that x \u2208 Dt and f(x,m) = 0 indicating that x \u2208 Dh. If a membership inference attack method utilizes a model\u2019s output obtained through queries to attack the model, it is called query-based attackDuan et al. (2023); Matsumoto et al. (2023); Hu & Pang (2023). Typically, MIA is based on the assumption that training data has a smaller loss compared to hold-out data. MIA for generation tasks, such as GANs Pham & Le (2020) and VAEs Hilprecht et al. (2019); Chen et al. (2020), has also been extensively researched.\nRecently, several MIA methods designed for diffusion models have been proposed. Matsumoto et al. (2023) proposed a method that directly employs the training loss Eq. (1) and find a specific t with maximum distinguishability. Because they directly use the training loss, we refer to this method as Naive Attack. SecMI Duan et al. (2023) improves the attack effectiveness by iteratively computing the t-error, which is the error between the DDIM sampling process and the inverse sampling process at a certain moment t.\nThreat model We follow the same threat model as Duan et al. (2023), which needs to access intermediate outputs of diffusion models. This is a query-based attack without the knowledge of model parameters but not fully end-to-end black-box. In scenarios such as inpainting Lugmayr et al. (2022), and classification Li et al. (2023), they also employ the intermediate output of the diffusion model. These works utilize a pre-trained model on a huge dataset to do other tasks, such as inpainting, and classification without fine-tuning. To meet these requirements, future service providers might consider opening up APIs for intermediate outputs. Our work is applicable to such scenarios."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "In this section, we introduce DDIM, a variant of DDPM, and provide a proof that if we know any two points in the DDIM framework, xk and x0, we can determine any other point xt. We then propose a new MIA method that utilizes this property to efficiently obtain xt\u2212t\u2032 and its corresponding predicted sample x\u2032t\u2212t\u2032 . We compute the difference between these two points and use it to determine if a sample\nis in the training set. Specifically, samples with small differences are more likely to belong to the training set. An overview of this proposed method is shown in Fig. 1."
        },
        {
            "heading": "3.1 PRELIMINARY",
            "text": "Denoising Diffusion Implicit Models To accelerate the inference process of diffusion models, DDIM defines a new process that shares the same loss function as DDPM. Unlike the DDPM process, which adds noise from x0 to xT , DDIM defines a diffusion process from xT to x1 by using x0. The process is described in Eq. (2) and Eq. (3). The distribution q\u03c3 (xT | x0) is the same as in DDPM.\nq\u03c3 (x1:T | x0) := q\u03c3 (xT | x0) T\u220f\nt=2\nq\u03c3 (xt\u22121 | xt,x0) , (2)\nq\u03c3 (xt\u22121 | xt,x0) = N ( \u221a \u03b1\u0304t\u22121x0 + \u221a 1\u2212 \u03b1\u0304t\u22121 \u2212 \u03c32t \u00b7 xt \u2212 \u221a \u03b1\u0304tx0\u221a\n1\u2212 \u03b1\u0304t , \u03c32t I\n) . (3)\nThe denoising process defined by DDIM is described below:\np(xt\u2032 | xt) = p (xt\u2032 | xt,x0 = \u00b5 (xt)) = N ( xt\u2032 ;\n\u221a \u03b1\u0304t\u2032\u221a \u03b1\u0304t\n( xt \u2212 (\u221a 1\u2212 \u03b1\u0304t \u2212\n\u221a \u03b1\u0304t\u221a \u03b1\u0304t\u2032\n\u221a 1\u2212 \u03b1\u0304t\u2032 \u2212 \u03c32t ) \u03f5\u03b8 (xt, t) ) , \u03c32t I ) (4)"
        },
        {
            "heading": "3.2 FINDING GROUNDTRUTH TRAJECTORY",
            "text": "In this section, we will first demonstrate that if we know xk and x0, we can determine any other xt. Then, we will provide the method for obtaining xk. Theorem 1 The trajectory of {xt} is determined if we know x0 and any other point xk when \u03c3t = 0 under DDIM framework.\nProof In DDIM definition, if standard deviation \u03c3t = 0, the process adding noise becomes determined. So Eq. (3) can be rewritten to Eq. (5).\nxt\u22121 = \u221a \u03b1\u0304t\u22121x0 + \u221a 1\u2212 \u03b1\u0304t\u22121 \u00b7 xt \u2212 \u221a \u03b1\u0304tx0\u221a\n1\u2212 \u03b1\u0304t . (5)\nAssuming that we know any point xk. Eq. (5) can be rewritten as xt\u22121\u2212\n\u221a \u03b1\u0304t\u22121x0\u221a\n1\u2212\u03b1\u0304t\u22121 = xt\u2212\n\u221a \u03b1\u0304tx0\u221a\n1\u2212\u03b1\u0304t . By\napplying this equation recurrently, we can obtain Eq. (6). In other words, we can obtain any point xt except xk.\nxt = \u221a \u03b1\u0304tx0 + \u221a 1\u2212 \u03b1\u0304t \u00b7 xk \u2212 \u221a \u03b1\u0304kx0\u221a\n1\u2212 \u03b1\u0304k . (6)\nWe call the trajectory obtained from xk groundtruth trajectory. Assuming that the point is xk = \u221a a\u0304kx0 + \u221a 1\u2212 a\u0304k\u03f5k, to find a better groundtruth trajectory, we choose k = 0 since the choice of k is arbitrary, and approximate \u03f5\u03040 using Eq. (7).\n\u03f5\u03b8 (\u221a a\u03040x0 + \u221a 1\u2212 a\u03040\u03f50, 0 ) \u2248 \u03f5\u03b8 (x0, 0) . (7)\nThis choice is intuitive. First, \u03b1\u03040 is very close to 1, making the approximation in Eq. (7) valid. Second, the time t = 0 is the closest timing to the original sample, so the model is likely to fit it better."
        },
        {
            "heading": "3.3 EXPOSING MEMBERSHIP VIA GROUNDTRUTH TRAJECTORY AND PREDICTED POINT",
            "text": "Our approach assumes that the training set\u2019s samples have a smaller loss, similar to many other MIAs, meaning that the training samples align more closely with the groundtruth trajectory. We measure the distance between any groundtruth point xt\u2212t\u2032 and the predicted point x\u2032t\u2212t\u2032 using the \u2113p-norm, which can be expressed by Eq. (8). Here, x\u2032t\u2212t\u2032 denotes the point predicted by the model from xt. To apply this attack, we need to select a specific time t\u2212 t\u2032, and we choose the time t\u2032 = t\u2212 1 since it is\nthe closest. However, we will demonstrate later that the choice of t\u2032 is not significant in discrete-time diffusion. dt\u2212t\u2032 = \u2225\u2225xt\u2212t\u2032 \u2212 x\u2032t\u2212t\u2032\u2225\u2225p . (8)\nTo predict x\u2032t\u2212t\u2032 from the groundtruth point xt, we apply the deterministic version (\u03c3t = 0) of the DDIM denoising process Eq. (4).\nWe use method described in Section 3.2 to obtain the groundtruth point xt and xt\u2212t\u2032 . We then insert these points into Eq. (8), giving us a simpler formula:\n\u221a 1\u2212 \u03b1\u0304t\u2212t\u2032 \u221a \u03b1\u0304t \u2212 \u221a 1\u2212 \u03b1\u0304t \u221a \u03b1\u0304t\u2212t\u2032\u221a\n\u03b1\u0304t \u2225\u2225\u03f5\u03040 \u2212 \u03f5\u03b8 (\u221aa\u0304tx0 +\u221a1\u2212 a\u0304t\u03f50, t)\u2225\u2225p . If we ignore the coefficient, t\u2032 disappears. Finally, the metric ignoring the coefficient reduces to Eq. (9), where samples with smaller Rt,p are more likely to be training samples.\nRt,p = \u2225\u2225\u03f5\u03b8 (x0, 0)\u2212 \u03f5\u03b8 (\u221aa\u0304tx0 +\u221a1\u2212 a\u0304t\u03f5\u03b8 (x0, 0) , t)\u2225\u2225p . (9)\nSince \u03f5 is initialized in time t = 0, we call our method Proximal Initialization Attack (PIA).\nNormalization The values of \u03f5\u03b8 (x0, 0) may not conform to a standard normal distribution, so we use Eq. (10) to normalize them. N represents the number of elements in the sample, such as h\u00d7 w for an image. We refer to this method as PIAN (PIA Normalized). Although this normalization cannot guarantee that \u03f5\u0302\u03b8 (x0, 0) \u223c N (0, I), we deem it reasonable since each element of \u03f5\u0304t in the training loss Eq. (1) is identically and independently distributed.\n\u03f5\u0302\u03b8 (x0, 0) = \u03f5\u03b8(x0, 0)\nEx\u223cN (0,1)(|x|)\u2225\u03f5\u03b8(x0,0)\u22251N = N\n\u221a \u03c0\n2\n\u03f5\u03b8(x0, 0)\n\u2225\u03f5\u03b8(x0, 0)\u22251 . (10)\nTo apply our attack, we first evaluate the value of Rt,p on a sample, and use an indicator function:\nf(x,m) = 1[Rt,p < \u03c4 ]. (11)\nThis indicator means we consider whether a sample is in the training set if Rt,p is smaller than a threshold \u03c4 . Rt,p is obtained from \u03f5\u03b8 (x0, 0) (PIA) or \u03f5\u0302\u03b8 (x0, 0) (PIAN)."
        },
        {
            "heading": "3.4 FOR CONTINUOUS-TIME DIFFUSION MODEL",
            "text": "Recently, some diffusion models are trained with continuous time. As demonstrated in Song et al. (2021b), the diffusion process with continuous time can be defined by a stochastic differential equation (SDE) as dxt = ft(xt)dt+ gtdwt, where wt is a Brownian process. One of the reverse processes is dxt = ( ft(xt)\u2212 12 ( g2t + \u03c3 2 t ) \u2207xt log pt(xt) ) dt+ \u03c3tdw. When \u03c3t = 0, this formula becomes\nan ordinary differential equation (ODE): dxt = ( ft(xt)\u2212 12g 2 t\u2207xt log pt(xt) ) dt. Continuous-time diffusion model train an s\u03b8 to approximate \u2207xt log pt(xt), so the loss function will be:\nL = Ex0,xt\u223cp(xt|x0)p\u0304(x0) [ \u2225s\u03b8 (xt, t)\u2212\u2207xt log p (xt | x0)\u2225 2 ] .\nReplacing \u2207xt log pt(xt) with s\u03b8 (xt, t), the inference procedure become the following equation:\ndxt = ( ft(xt)\u2212 1\n2 g2t s\u03b8(xt, t)\n) dt. (12)\nThe distribution p(xt|x0) is typically set to be the same as in DDPM for continuous-time diffusion models. Therefore, the loss of the continuous-time diffusion model and the loss of the concretediffusion model Eq. (1) are similar. Since DDPM and the diffusion model described by SDE share a similar loss, our method can be applied to continuous-time diffusion models. However, due to the different diffusion process, Rt,p differs from Eq. (9). From Eq. (12), we obtain the following equation: xt\u2212t\u2032 \u2212 xt \u2248 dxt = ( ft(xt)\u2212 12g 2 t s\u03b8 (xt, t) ) dt. By substituting this equation into Eq. (8), we obtain the following equation:\n\u2225xt\u2212t\u2032 \u2212 x\u2032t\u2212t\u2032\u2225p \u2248 \u2225\u2225\u2225\u2225(ft(xt)\u2212 12g2t s\u03b8 (xt, t) ) dt+ xt \u2212 x\u2032t\u2212t\u2032 \u2225\u2225\u2225\u2225 p . (13)\nSolving this ODE incurs a truncation error that is positively correlated with \u2206t. Therefore, we take the limit as t\u2032 \u2192 0. In this case, higher-order infinitesimals can be neglected in Eq. (13). so, we can obtain \u2225xt\u2212t\u2032 \u2212 x\u2032t\u2212t\u2032\u2225p \u2248 \u2225\u2225(ft(xt)\u2212 12g2t s\u03b8 (xt, t))\u2225\u2225p dt. Since the t\u2032 is same when comparing two sampling, we can neglect dt and use the following attack metric:\nRt,p = \u2225\u2225\u2225\u2225ft(xt)\u2212 12g2t s\u03b8 (xt, t) \u2225\u2225\u2225\u2225 p , (14)\nwhere xt is obtained from the output of s\u03b8(x0, 0), similar to the discrete-time diffusion case."
        },
        {
            "heading": "4 EXPERIMENT",
            "text": "In this section, we evaluate the performance of PIA and PIAN and robustness of TTS models across various datasets and settings. The detailed experimental settings, including datasets, models, and hyper-parameter settings can be found in Appendix B."
        },
        {
            "heading": "4.1 EVALUATION METRICS",
            "text": "We follow the most convincing metrics used in MIAs Carlini et al. (2023), including AUC, the True Positive Rate (TPR) when the False Positive Rate (FPR) is 1%, i.e., TPR @ 1% FPR, and TPR @ 0.1% FPR."
        },
        {
            "heading": "4.2 PROXIMAL INITIALIZATION ATTACK PERFORMANCE",
            "text": "We train TTS models on the LJSpeech, VCTK, and LibriTTS datasets. We summarize the AUC and TPR @ 1% FPR results on GradTTS, a continuous-time diffusion model, in Table 1. We employ NA to denote Naive Attack. Compared to SecMI, PIA and PIAN achieve slightly better AUC performance, and significantly higher TPR @ 1% FPR performance, i.e., 5.4% higher for PIA and 10.5% higher for PIAN on average. However, our proposed method only requires 1 + 1 queries, just one more query\nthan Naive Attack, and has a computational consumption of only 3.2% of SecMI. Both methods outperform SecMI and Naive Attack.\nFor DDPM, a discrete-time diffusion model, we present the results in Table 2. For this model, PIA performs slightly better than SecMI in terms of AUC but has a distinctly higher TPR @ 1% FPR than SecMI, i.e. 5.8% higher on average than SecMI. For PIAN, the AUC performance is slightly lower than PIA, but higher than SecMI, and the TPR @ 1% FPR performance is significantly better than SecMI, i.e. 17.8% higher on average than SecMI. Similar to the previous case, our attack only requires two queries on DDPM and the computational consumption is 17% of SecMI. Both methods outperform SecMI and Naive Attack.\nFor stable diffusion, we present the results in Table 3. We evaluated stable diffusion on Laion5 (training dataset) and COCO (evaluation dataset). Details are put into A.2. We tested three scenarios: knowing the ground truth text (Laion5), not knowing the ground truth text (Laion5 w/o text), and generating text through blip (Laion5 Blip text). PIA achieved the best results. PIA performs slightly better than SecMI in terms of AUC, i.e. 1.8% higher on average, but has a distinctly higher TPR @ 1% FPR than SecMI, i.e. 3.2% higher on average. Besides, our attack only requires two queries on DDPM and the computational consumption is 17% of SecMI.\nHowever, PIAN does not work well in stable diffusion. PIAN based on the fact that we added noise that follows a normal distribution during training, and we use Eq. (10) to rescale the \u03f5 to normal distribution. However, rescaling is a rough operation and may not always transform into a normal distribution. Thus, some other transforms might have better performance. Additionally, the model\u2019s output might be more accurate before the rescaling.\nWe highly recommend using PIA as the preferred method for conducting attacks, because it is directly derived. It will always yield the desired results. But PIAN can be another choice, since it has better performance at TPR @ 1% FPR metric than PIA on some models."
        },
        {
            "heading": "4.3 ABLATION STUDY",
            "text": "Our proposed method has three hyper-parameters: t and the \u2113p-norm used in the attack metrics Rt,p presented in Eqs. (9) and (14). The threshold \u03c4 presented in Eq. (11).\nImpact of t To evaluate the impact of t, we attack the target model at intervals of 0.01\u00d7 T from 0 to T and report the results across different models and datasets.\nWe demonstrate the performance of our proposed method on two different models: GradTTS, a continuous-time diffusion model used for audio; and DDPM, a discrete-time diffusion model employed for images. The results indicate that our method produces a consistent pattern in the same model across different datasets, whether PIA or PIAN. Specifically, for DDPM, both AUC and TPR @ 1% FPR exhibit a rapid increase at the beginning as t increases followed by a decline around t = 200 from Fig. 2a. In Fig. 2b, we randomly partition the CIFAR10 dataset four times and compare the performance of each partition. Consistent with the previous results, our method exhibits a similar\ntrend across the different splits. For GradTTS, a similar phenomenon can be observed in Appendix Fig. 6.\nImpact of \u2113p-norm In Fig. 3, we compare the results obtained on \u2113p-norm using the p = 1 to 7, with the choice of t being the same as in Section 4.2. The results indicate an increase in performance at \u21131-norm, followed by a decline after the p = 5. It reveals that the combined effect of both large and small differences exhibits a synergistic influence when present in an appropriate ratio.\nDetermining the value of \u03c4 In Table 4, we present the variation of Attack Success Rate (ASR) and TPR/FPR on the victim model with the \u03c4 determined by the surrogate model. Specifically, we will randomly split the corresponding dataset into two halves four times, resulting in four different train-test splits. We will train four models using these splits. One of the models will be selected as the surrogate model, from which we will obtain the threshold. We will then use this \u03c4 to attack the other three victim models and record the average values. The results indicate that our method achieves promising results when using the \u03c4 selected from the surrogate model."
        },
        {
            "heading": "4.4 WHICH TYPE OF MODEL OUTPUT IS MORE ROBUST?",
            "text": "According to Zhang et al. (2023), the TTS pipeline consists of three stages: text to mel-spectrogram, text to audio, and mel-spectrogram to audio. Our experiments tested models for each stage: Grad-TTS for text to mel-spectrogram, DiffWave for mel-spectrogram to audio, and FastDiff for text to audio. There are generally two forms of output: mel-spectrograms and audio. In Table 5, we summarize the model details and best results of our proposed method on three TTS models using the LJSpeech\ndataset and the DDPM model on the TinyImageNet dataset. We only report the results of our method since it achieves better performance most of the time.\nAs shown in Table 5, with the same training and hold-out data, GradTTS achieves an AUC close to 100, while DiffWave and FastDiff only achieve the performance slightly above 50, which is close to random guessing. However, DiffWave has a similar size to DDPM and GradTTS, and FastDiff has similar T with DDPM. Additionally, FastDiff has similar segmentation length to GradTTS. Thus, we believe that these hyperparameters are not the decisive parameters for the model\u2019s robustness. It is obvious that the output of GradTTS and DDPM is image-like. Fig. 4 provides an example of mel-spectrogram. The deep reasons why these models exhibit robustness can be further explored. We report these results hoping that they may inspire the design of models with MIA robustness.\nWe also explore the attack performance with various training and evaluation sample numbers. We select 10%, 30%, 50%, and 100% of the samples from the complete dataset. In each split, half of all samples are used for training, and the other half are utilized as a hold-out set. The results are presented in Fig. 5. As we can see, when only 10% of the data is used, relatively high AUC and TPR @ 1% FPR can be achieved. Additionally, we find that the AUC and TPR @ 1% FPR decrease as the proportion of selected samples in the total dataset increases. However, for GradTTS and DDPM, the decrease is relatively gentle, while for DiffWave and FastDiff, the decrease is rapid. In other words, the robustness increases rapidly with the increase of training samples."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we propose an efficient membership inference attack method for diffusion models, namely Proximal Initialization Attack (PIA) and its normalized version, PIAN. We demonstrate its effectiveness on a continuous-time diffusion model, GradTTS, and two discrete-time diffusion models, DDPM and Stable Diffusion. Experimental results indicate that our proposed method can achieve similar AUC performance to SecMI and significantly higher TPR @ 1% FPR with the cost of only 2 queries, which is much faster than the 12~62 queries required for SecMI in this paper. Additionally, we analyze the vulnerability of models in TTS, an audio generation task. The results suggest that diffusion models with the image-like output (mel-spectrogram) are more vulnerable than those with the audio output. Therefore, for privacy concerns, we recommend employing models with audio outputs in text-to-speech tasks."
        },
        {
            "heading": "6 ACKNOWLEDGMENT",
            "text": "Xiaofeng Zhu was supported in part by the National Key Research & Development Program of China under Grant (No. 2022YFA1004100). Fei Kong and Xiaoshuang Shi were supported by the National Natural Science Foundation of China (No.62276052)."
        },
        {
            "heading": "A LIMITATION AND BROADER IMPACTS",
            "text": "The purpose of our method is to identify whether a given sample is part of the training set. This capability can be leveraged to safeguard privacy rights by detecting instances of personal information being unlawfully used for training purposes. However, it is important to note that our method could also potentially result in privacy leaking. For instance, this could occur when anonymous data is labeled by determining whether a sample is part of the training set or as a part of data reconstruction attack. It is worth mentioning that our method solely relies on the diffusion model\u2019s output as we discussed in the threat model, but it does require the intermediate output. This dependency on the intermediate output may pose a limitation to our method."
        },
        {
            "heading": "B DATASETS AND DIFFUSION MODELS",
            "text": "For TTS, we evaluate three commonly used datasets: LJSpeech, VCTK, and a subset of LibriTTS called libritts-lean-100. We test three models: GradTTS 1, FastDiff 2, and DiffWave 3. For image generation, we evaluate the CIFAR10, CIFAR100 and TinyImageNet datasets using the same DDPM model as Duan et al. (2023), and Laion5, COCO for stable diffusion Rombach et al. (2022). Unless otherwise specified, we randomly select half of the samples as a training set and the other half as the hold-out set.\nB.1 IMPLEMENTATIONS DETAILS\nFor the audio generation models, we use their codes from the official repositories and apply the default hyperparameters for all models except for the hyperparameters we mentioned. The training iterations were set to 1,000,000, due to the default value for the three audio generative models are all around this. For DDPM, all settings are the same as those in Duan et al. (2023). For various experiments, due to the absence of corresponding trials by the baseline, we employ a grid-search approach to identify the optimal parameters attainable by the method.\nTable 6 demonstrate the setting for different attacks. On GradTTS, because SecMI is not designed for continuous-time diffusion, we discretize [0, 1] into 1000 steps and then apply SecMI. We chose \u21134-norm to compute Rt,p.\nTo conduct the experiment on stable diffusion, we download the stable-diffusion-v1-5 from 4, without any further fine-tuning or any other modification. We select 2500 sample from 600M laion-aestheticsv2-5plus as the member set, since stable-diffusion-v1-5 is trained on this dataset as mentioned by HuggingFace. We randomly select 2500 images from the COCO2017-val as the hold-out set, since COCO2017-val is one of the official validation set to examine the performance of stable diffusion. The prompt to generate Laion5 Blip text in BLIP is \"A picture of \".\nC PIA AND PIAN ON GRAD-TTS ACROSS VARIOUS t\nFig. 6 demonstrates the results of PIA and PIAN on Grad-TTS for different values of t and different datasets. For GradTTS, consistent with the DDPM, both AUC and TPR @ 1% FPR exhibit a rapid increase at the beginning as t increases followed by a decline around t = 0.5.\n1https://github.com/huawei-noah/Speech-Backbones/tree/main/Grad-TTS 2https://github.com/Rongjiehuang/FastDiff 3https://github.com/lmnt-com/diffwave 4https://huggingface.co/runwayml/stable-diffusion-v1-5"
        },
        {
            "heading": "D MORE EXPERIMENTAL RESULTS",
            "text": "D.1 ROBUSTNESS ON FASTDIFF AND DIFFWAVE\nTable 7 shows the AUC of different methods at FastDiff and DiffWave model on three datasets. The performance of all three MIA methods is very poor.\nD.2 DISTRIBUTION FOR SAMPLES FROM TRAINING SET AND HOLD-OUT SET.\nFig. 7 shows the Rt=0.3,p=4 distribution for samples from training set and hold-out set at GradTTS on different datasets of PIAN.\nD.3 LOG-SCALED ROC CURVE\nAs suggested by Carlini et al. (2022), Fig. 8 and Fig. 9 display the log-scaled ROC curves. These curves demonstrate that the proposed method outperforms NA and SecMI at most of times.\nD.4 VISUALIZATION OF RECONSTRUCTION\nNote Eq. (9) is equal to the distance between \u03f5\u03b8 (x0, 0) and the predicted one \u03f5\u2032 = \u03f5\u03b8 (xt, t) , where xt = \u221a a\u0304tx0 + \u221a 1\u2212 a\u0304t\u03f5\u03b8 (x0, 0). Fig. 10 and Fig. 11 show the reconstructed sample x\u20320 = xt\u2212 \u221a 1\u2212a\u0304t\u03f5\u2032\u221a a\u0304t\nfrom xt using the predicted \u03f5\u2032 at DDPM on CIFAR10 of PIAN. The reconstructed samples from t = 100 are clear for both the training set and the hold-out set. The reconstructed samples from t = 400 are blurry for both sets. However, for t = 200, the reconstructed samples are clear for the training set but blurry for the hold-out set.\nFor GradTTS, we use Eq. (12) to reconstruct samples from xt. This reconstruction is not rigorous, but we just use it to give a visualization. Fig. 12 and Fig. 13 show the reconstructed samples on LJSpeech from PIA. The observed pattern is consistent with DDPM."
        }
    ],
    "title": "AN EFFICIENT MEMBERSHIP INFERENCE ATTACK FOR THE DIFFUSION MODEL BY PROXIMAL INITIALIZATION",
    "year": 2024
}