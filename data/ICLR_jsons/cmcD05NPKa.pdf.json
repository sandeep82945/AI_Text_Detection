{
    "abstractText": "The predictions of small transformers, trained to calculate the greatest common divisor (GCD) of two positive integers, can be fully characterized by looking at model inputs and outputs. As training proceeds, the model learns a listD of integers, products of divisors of the base used to represent integers and small primes, and predicts the largest element of D that divides both inputs. Training distributions impact performance. Models trained from uniform operands only learn a handful of GCD (up to 38 GCD \u2264 100). Log-uniform operands boost performance to 73 GCD \u2264 100, and a log-uniform distribution of outcomes (i.e. GCD) to 91. However, training from uniform (balanced) GCD breaks explainability.",
    "authors": [
        {
            "affiliations": [],
            "name": "Fran\u00e7ois Charton"
        }
    ],
    "id": "SP:705b2ad37464730140b5482fc030923f84d60580",
    "references": [
        {
            "authors": [
                "Fran\u00e7ois Charton"
            ],
            "title": "Linear algebra with transformers",
            "venue": "arXiv preprint arXiv:2112.01898,",
            "year": 2021
        },
        {
            "authors": [
                "Fran\u00e7ois Charton",
                "Amaury Hayat",
                "Guillaume Lample"
            ],
            "title": "Learning advanced mathematical computations from examples",
            "venue": "arXiv preprint arXiv:2006.06462,",
            "year": 2020
        },
        {
            "authors": [
                "Fran\u00e7ois Charton"
            ],
            "title": "What is my math transformer doing? \u2013 three results on interpretability and generalization",
            "venue": "arXiv preprint arXiv:2211.00170,",
            "year": 2022
        },
        {
            "authors": [
                "Fran\u00e7ois Charton"
            ],
            "title": "Computing the roots of polynomials, 2022b. https://f-charton.github. io/polynomial-roots",
            "year": 2022
        },
        {
            "authors": [
                "Kyunghyun Cho",
                "Bart Van Merri\u00ebnboer",
                "Caglar Gulcehre",
                "Dzmitry Bahdanau",
                "Fethi Bougares",
                "Holger Schwenk",
                "Yoshua Bengio"
            ],
            "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
            "venue": "arXiv preprint arXiv:1406.1078,",
            "year": 2014
        },
        {
            "authors": [
                "Ernest Davis"
            ],
            "title": "Mathematics, word problems, common sense, and artificial intelligence, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Nouha Dziri",
                "Ximing Lu",
                "Melanie Sclar",
                "Xiang Lorraine Li",
                "Liwei Jiang",
                "Bill Yuchen Lin",
                "Peter West",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Jena D. Hwang",
                "Soumya Sanyal",
                "Sean Welleck",
                "Xiang Ren",
                "Allyson Ettinger",
                "Zaid Harchaoui",
                "Yejin Choi"
            ],
            "title": "Faith and fate",
            "venue": "Limits of transformers on compositionality,",
            "year": 2023
        },
        {
            "authors": [
                "Kaden Griffith",
                "Jugal Kalita"
            ],
            "title": "Solving arithmetic word problems with transformers and preprocessing of problem text",
            "venue": "arXiv preprint arXiv:2106.00893,",
            "year": 2021
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation,",
            "year": 1997
        },
        {
            "authors": [
                "\u0141ukasz Kaiser",
                "Ilya Sutskever"
            ],
            "title": "Neural gpus learn algorithms",
            "venue": "arXiv preprint arXiv:1511.08228,",
            "year": 2015
        },
        {
            "authors": [
                "Nal Kalchbrenner",
                "Ivo Danihelka",
                "Alex Graves"
            ],
            "title": "Grid long short-term memory",
            "venue": "arXiv preprint arxiv:1507.01526,",
            "year": 2015
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Guillaume Lample",
                "Fran\u00e7ois Charton"
            ],
            "title": "Deep learning for symbolic mathematics",
            "venue": "arXiv preprint arXiv:1912.01412,",
            "year": 2019
        },
        {
            "authors": [
                "Nayoung Lee",
                "Kartik Sreenivasan",
                "Jason D. Lee",
                "Kangwook Lee",
                "Dimitris Papailiopoulos"
            ],
            "title": "Teaching arithmetic to small transformers",
            "venue": "arXiv preprint arXiv:2307.03381,",
            "year": 2023
        },
        {
            "authors": [
                "Ziming Liu",
                "Ouail Kitouni",
                "Niklas Nolte",
                "Eric J. Michaud",
                "Max Tegmark",
                "Mike Williams"
            ],
            "title": "Towards understanding grokking: An effective theory of representation learning, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Yuanliang Meng",
                "Anna Rumshisky"
            ],
            "title": "Solving math word problems with double-decoder transformer",
            "venue": "arXiv preprint arXiv:1908.10924,",
            "year": 2019
        },
        {
            "authors": [
                "Bhumika Mistry"
            ],
            "title": "An investigation into neural arithmetic logic modules",
            "venue": "PhD thesis, University of Southampton,",
            "year": 2023
        },
        {
            "authors": [
                "Neel Nanda",
                "Lawrence Chan",
                "Tom Lieberum",
                "Jess Smith",
                "Jacob Steinhardt"
            ],
            "title": "Progress measures for grokking via mechanistic interpretability",
            "year": 2023
        },
        {
            "authors": [
                "Rodrigo Nogueira",
                "Zhiying Jiang",
                "Jimmy Lin"
            ],
            "title": "Investigating the limitations of transformers with simple arithmetic tasks",
            "venue": "arXiv preprint arXiv:2102.13019,",
            "year": 2021
        },
        {
            "authors": [
                "Maxwell Nye",
                "Anders Johan Andreassen",
                "Guy Gur-Ari",
                "Henryk Michalewski",
                "Jacob Austin",
                "David Bieber",
                "David Dohan",
                "Aitor Lewkowycz",
                "Maarten Bosma",
                "David Luan",
                "Charles Sutton",
                "Augustus Odena"
            ],
            "title": "Show your work: Scratchpads for intermediate computation with language models",
            "venue": "arXiv preprint arXiv:2112.00114,",
            "year": 2021
        },
        {
            "authors": [
                "Theodoros Palamas"
            ],
            "title": "Investigating the ability of neural networks to learn simple modular arithmetic",
            "year": 2017
        },
        {
            "authors": [
                "Alethea Power",
                "Yuri Burda",
                "Harri Edwards",
                "Igor Babuschkin",
                "Vedant Misra"
            ],
            "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
            "venue": "arXiv preprint arXiv:2201.02177,",
            "year": 2022
        },
        {
            "authors": [
                "David Saxton",
                "Edward Grefenstette",
                "Felix Hill",
                "Pushmeet Kohli"
            ],
            "title": "Analysing mathematical reasoning abilities of neural models, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Feng Shi",
                "Chonghan Lee",
                "Mohammad Khairul Bashar",
                "Nikhil Shukla",
                "Song-Chun Zhu",
                "Vijaykrishnan Narayanan"
            ],
            "title": "Transformer-based Machine Learning for Fast SAT Solvers and Logic Synthesis",
            "venue": "arXiv preprint arXiv:2107.07116,",
            "year": 2021
        },
        {
            "authors": [
                "Kai-Yeung Siu",
                "Vwani Roychowdhury"
            ],
            "title": "Optimal depth neural networks for multiplication and related problems",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 1992
        },
        {
            "authors": [
                "Andrew Trask",
                "Felix Hill",
                "Scott Reed",
                "Jack Rae",
                "Chris Dyer",
                "Phil Blunsom"
            ],
            "title": "Neural arithmetic logic units",
            "venue": "arXiv preprint arXiv:1808.00508,",
            "year": 2018
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Sean Welleck",
                "Peter West",
                "Jize Cao",
                "Yejin Choi"
            ],
            "title": "Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Wojciech Zaremba",
                "Tomas Mikolov",
                "Armand Joulin",
                "Rob Fergus"
            ],
            "title": "Learning simple algorithms from examples",
            "year": 2015
        },
        {
            "authors": [
                "Ziqian Zhong",
                "Ziming Liu",
                "Max Tegmark",
                "Jacob Andreas"
            ],
            "title": "The clock and the pizza: Two stories in mechanistic explanation of neural networks, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Hattie Zhou",
                "Azade Nova",
                "Hugo Larochelle",
                "Aaron Courville",
                "Behnam Neyshabur",
                "Hanie Sedghi"
            ],
            "title": "Teaching algorithmic reasoning via in-context learning",
            "venue": "arXiv preprint arXiv:2211.09066,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Transformers (Vaswani et al., 2017) have been applied to problems of mathematics, both symbolic (Lample & Charton, 2019; Charton et al., 2020; Shi et al., 2021) and numerical (Charton, 2021). Yet, they struggle with basic arithmetic (Lee et al., 2023; Nogueira et al., 2021). Large language models (LLM) can learn addition or multiplication by a small prefactor, and generalize beyond their training range when fine-tuned using scratchpad (Nye et al., 2021), chain-of-thought (Wei et al., 2023) or algorithmic prompting (Zhou et al., 2022), but these techniques require bespoke data and do not extend to complex tasks (Dziri et al., 2023). Math transformers were also found to be brittle (Welleck et al., 2021), to fail on simple tasks (Davis, 2023), and to be hard to interpret, except in the simplest cases (Nanda et al., 2023). Yet, small transformers can learn advanced calculations, such as eigen-decomposition (Charton, 2021) and polynomial roots (Charton, 2022b).\nIn this paper, I train 4-layer transformers to compute the greatest common divisor (GCD) of two positive integers, an important operation for rational arithmetic and number theory, and observe that:\n1. Transformers learn to cluster input pairs with the same GCD. All pairs of integers (a, b) with the same GCD k are predicted the same. 2. Transformer predictions can be fully characterized. During training, the model learns a set of integers D, and predicts, for any input pair (a, b), the largest element in D that divides a and b. 3. Early during training, transformers learn to predict products of divisors of the base used to represent integers. Small primes are \u201cgrokked\u201d (Power et al., 2022) after extended training. 4. Models trained from log-uniform operands and outcomes achieve better performance. They correctly predict up to 91 GCD \u2264 100. Model predictions remain fully explainable. 5. An unbalanced distribution of outcomes in the training set is required for full explainability: explainability partially fails once models are trained from uniformly distributed GCD.\nThese results demonstrate how transformers can be trained to perform exact calculations involving integer divisibility, a central task in integer arithmetic and number theory. Beyond GCD calculations, the broader potential impact of this research extends in three directions. First, it presents a new approach to model explainability: fully characterizing black-box model predictions by experimenting with selected inputs and leveraging our theoretical understanding of the underlying mathematics. Second, the results on log-uniform training distributions of operands and outcomes \u2013 faster learning and better performance \u2013 may extend to other arithmetic tasks, e.g. fine tuning LLM. Finally, mathematical tasks play a central role for Foundational Models for Science \u2013 large language models pre-trained on mathematics, and fine-tuned on specific fields, such as high energy physics, computational biology or astrophysics. Before they can do science, transformers must learn maths.\nRELATED WORK\nNeural networks for arithmetic were first proposed by Siu & Roychowdhury (1992), and recurrent models by Kalchbrenner et al. (2015), Zaremba et al. (2015) and Kaiser & Sutskever (2015). Recent research mostly focuses on fine-tuning LLM on arithmetic tasks, to solve math word problems (Meng & Rumshisky, 2019; Griffith & Kalita, 2021). See Lee et al. (2023) for a summary. As an alternative, Neural Arithmetic Logical Units (Trask et al., 2018; Mistry, 2023) learn exact computations that can generalize to any input, by constraining the weights of linear models to be close to 0, 1 or \u22121. The difficulty of learning arithmetic tasks was discussed by many authors. Saxton et al. (2019), benchmarking mathematical tasks, observe that number theoretic operations, like factorization, are hard. Palamas (2017) further investigates the hardness of modular arithmetic. Dziri et al. (2023) note the difficulty of extending the promising results obtained by Lee et al. (2023) on the four operations to complex mathematical calculations or algorithms \u2013 GCD and Euclid\u2019s algorithm, here.\nThe role of number representation was discussed by Nogueira et al. (2021) and Charton (2021). Grokking was first described by Power et al. (2022). Liu et al. (2022) propose metrics to characterize it. Gromov (2023) provides an insightful analysis of grokking in feed-forward networks. Most prior work on explainability in arithmetic transformers tries to interpret model weights (Nanda et al., 2023; Zhong et al., 2023). Charton (2022a) conducts similar experiments for linear algebra."
        },
        {
            "heading": "2 EXPERIMENTAL SETTINGS",
            "text": "GCD calculations are framed as a supervised translation task. Problems (pairs of integers) are randomly sampled, represented as sequences of tokens, and used to train sequence-to-sequence transformers to translate input pairs into their GCD, by minimizing the cross-entropy between model predictions and the sequences representing correct solutions. Integers are encoded as sequences of digits in base B, preceded by a sign which also serves as a separator (Table 1). In base 10, the model translates (8, 12), encoded as the sequence \u2018+ 8 + 1 2\u2019, into its GCD, 4, encoded as \u2018+ 4\u2019. The choice of B is a trade-off. Small bases result in longer sequences that are harder to learn, but use a small vocabulary that is easier to memorize. Composite bases allow for simple tests of divisibility: in base 10, divisibility by 2, 5 and 10 is decided by looking at the rightmost token in the sequence.\nTransformers with 4 layers, 512 dimensions and 8 attention heads, using Adam (Kingma & Ba, 2014) are trained with a learning rate of 10\u22125 (no scheduling is needed) on batches of 256 examples. All inputs pairs are sampled uniformly between 1 and M = 106. All data is generated on the fly: different training epochs use different examples for the train and test set. After each epoch (300,000 examples), the models are evaluated on two test sets of 100,000 examples: a natural test set of uniformly sampled pairs (a, b), and a stratified test set with GCD uniformly distributed between 1 and 100. In the natural set, small GCD are more common \u2013 we have P (gcd(a, b) = k) = 6\u03c02k2 (Ces\u00e0ro, 1883). The stratified set has about 1000 examples with GCD k for 1 \u2264 k \u2264 100, and is generated by:\n\u2022 sampling k, uniformly between 1 and 100, \u2022 sampling a and b, uniformly between 1 and Mk , such that gcd(a, b) = 1, using rejection sampling, \u2022 adding (ka, kb) to the stratified test set.\nThese two test sets provide two measures of accuracy. Model accuracy, measured on the natural set, is the probability that the GCD of two random integers from 1 to M is correctly predicted. Accuracy on the stratified test set is the number of GCD correctly predicted between 1 and 100. The size of the problem space (1012 possible input pairs) guarantees minimal duplication between train and test set. All experiments are run on one NVIDIA V100 GPU with 32 GB of memory."
        },
        {
            "heading": "3 LEARNING THE GREATEST COMMON DIVISOR - BASE EXPERIMENTS",
            "text": "A model trained on pairs of positive integers under one million, encoded in base B = 10, correctly predicts 84.7% of the examples in the natural test set, and 13 correct GCD under 100 (accuracy on the stratified test set). Performances vary with the encoding base: from 61.8% accuracy and 2 correct GCD for base 11, to 96.8% and 38 GCD for base 420 (Table 2). The best performances are achieved for composite bases (30, 60, 210 and 420), the worst for large primes. Learning is very fast: for base 30, the model achieves 90% accuracy after 2 epochs (600,000 examples), and 93% after 6. Model size has little impact on performance (Appendix B). For base 30, 1-layer transformers with 32 dimensions (less than 300,000 parameters) achieve 93.3% accuracy. 24-layer models with 1024 dimensions (714 million parameters) achieve 93.4%. For base 31, accuracy is 61% for all models.\nThese variations in model performance can be understood by looking at model predictions. Table 3 presents, for bases 2 and 10 and GCD up to 36, the most frequent model prediction for pairs with a given GCD (Pred), and its frequency in the stratified test set (%) \u2013 detailed results for 6 bases and GCD up to 100 are in Appendix E.3). All frequencies are very close to 100%: for every test pair with GCD k, the model makes the same prediction f(k). In other words, the model can tell whether two input pairs have the same GCD. Correct model predictions (f(k) = k) only happen for products of divisors of the base. In fact, all model predictions can be summarized in three rules:\n(R1) Predictions are deterministic. The model predicts a unique value f(k) for almost all (99.9%) pairs of integers with GCD k. Predictions are correct when f(k) = k. (R2) Correct predictions are products of primes dividing B. For base 2, they are 1, 2, 4, 8, 16, 32 and 64. For base 31, 1 and 31. For base 10, all products of elements from {1, 2, 4, 8, 16} and {1, 5, 25}. For base 30, all products of {1, 2, 4, 8}, {1, 3, 9, 27} and {1, 5, 25}. (R3) f(k) is the largest correct prediction that divides k. For instance, f(8) = 8, and f(7) = 1, for base 2 and 10, but f(15) = 5 for base 10 and f(15) = 1 for base 2.\nThese results can be interpreted as follows. For prime bases, such as B = 2, an integer is divisible by Bk iff its representation ends in k zeroes. The model learns to \u201cpredict\u201d GCD by counting the rightmost zeroes in its operands, za and zb, and predicting Bz with z = min(za, zb). This accounts for all observed results. For instance, it will correctly predict the GCD of a = 8 = 10002 and b = 12 = 11002 to be 22 = 4, and incorrectly predict the GCD of 7 = 1112 and 14 = 11102 to be\n1. For composite bases, such as B = 10, an integer a is divisible by f , such that kf = Bn, iff its n rightmost digits are in {0, f, 2f . . . (k\u2212 1)f}. The model learns to test the divisibility of its operands by comparing their n rightmost digits with the k possible values, and predict the largest f that divides both operands. In practice, only divisibilities that can be tested by considering the two last digits in the representation are learned. For B = 210 divisibility by 4 is learned, but divisibility by 8 is not. For B = 420 divisibility by 16 is learned, but not by 32. The three rules also account for variations in model accuracy (computed on the natural test set) for different bases (see Appendix C).\nLearning GCD one prime power at a time. Learning curves have a step-like shape (Figure 1), and GCD are learned in sudden batches. When the model learns a new power of a prime divisor of B, it also learns its products with already known GCD. For instance, for base 30, the model initially predicts {1, 2, 4}, {1, 3, 9}, {1, 5} and their products: 17 GCD under 100. A first step happens around epoch 50, when the model learns 25 and the three associated multiples 50, 75 and 100 (21 GCD), a second around epoch 220, learning 8, 24, 40 and 72, and a third at epoch 660, learning 27 and 54, for a grand total of 27 correct GCD. The three rules hold at all times during training.\nAccelerating learning by balancing the distribution of GCD. The distribution of GCD verifies P (gcd(a, b) = k) = 6\u03c02k2 (Ces\u00e0ro, 1883). As a result, large GCD are very rare in the training set, and learning them is very slow. This can be mitigated, and training accelerated, by adding a small proportion (5%) of uniformly sampled GCD to the training set: for B = 30, the model learns 25 GCD in 30 epochs, and 27 GCD in 175, vs 250 and 660 in the original experiments (Figure 2).\nIn these experiments, models only correctly calculate GCD that are products of divisors of the base, and the best accuracies are achieved for bases divisible by many small primes, e.g. 30, 210 or 420. Still, all models learn to cluster pairs of input integers according to their GCD, and output a unique prediction f(k) for all pairs with GCD k. This is a non-trivial result and a significant achievement."
        },
        {
            "heading": "4 LARGE COMPOSITE BASES B - GROKKING SMALL PRIMES",
            "text": "For large basesB, non-divisors ofB are sometimes learned after extended training. In one experiment with base 1000, the model predicts 13 GCD \u2264 100 after 84 epochs: all products of {1, 2, 4, 8, 16} and {1, 5, 25}. Then, the training loss is flat during 100 epochs, and it seems that the model is no longer learning anything. But then, the model starts predicting GCD 3, with an accuracy of 0.2% at epoch 188, and 93% at epoch 193 (despite only seeing 100,000 input pairs with GCD 3 during these 5 epochs). Multiples of 3 are then learned, and by epoch 220, the model predicts 22 GCD: all products of {1, 2, 4, 8, 16}, {1, 5, 25} and {1, 3}. Model predictions still respect rules R1 and R3 (Appendix E.1 Table 20), and the three rules can be updated as follows:\n(G1) Prediction is deterministic. All pairs with the same GCD are predicted the same, as f(k). (G2) Correct predictions are products of primes divisors of B and small primes. (G3) f(k) is the largest correct prediction that divides k.\nThis phenomenon is related to grokking (Power et al., 2022). Table 4 presents results for 16 large bases, with models trained up to 1300 epochs. Grokking usually sets in late during training: for bases 625 and 4000, all products of divisors of B are learned in 5 and 15 epochs, but it take 600 epochs for grokking (of 2 and 3) to happen. Primes and powers of primes are roughly grokked in order.\nLearning curves (Appendix E.1 Figure 5) retain their usual step-like shape: long periods of stagnation followed by sudden drops in the loss, and rises in accuracy, as new GCD are learned. Because it helps learn small GCD, grokking boosts model accuracy (from 63% to 91% for B = 2023), but overall the number of correct GCD remains low (under 30 for all large bases).\nBalancing outcomes. The technique proposed in section 3 to accelerate learning (adding a small amount of uniformly distributed GCD to the training set) does not apply to larger bases (Appendix E.3 Table 21). However, the unbalanced distribution of GCD can be corrected by sampling from a loguniform distribution \u2013 so that P (gcd(a, b) = k) = Ck instead of C k2 \u2013 as follows:\n\u2022 Sample k between 1 and 100, with probability P (k) = Ck , with 1 C = \u2211100 i=1 1 i . \u2022 Sample a and b uniformly from 1 to Mk , such that gcd(a, b) = 1. \u2022 Add (ak, bk) to the training set.\nA log-uniform training distribution of GCD helps the model learn new non-divisors of B for 9 bases out of 35 (Table 5). For B = 211, primes up to 7 are learned. For B = 10000, 7, 9, 13 and 27 are learned, bringing the number of correct GCD to 62, our best result so far. For B = 30, a counter-intuitive situation prevails: instead of small primes, the model learns B \u2212 1 and B + 1."
        },
        {
            "heading": "5 LEARNING FROM LOG-UNIFORM OPERANDS",
            "text": "In all experiments so far, all pairs in the training sets are uniformly sampled between 1 and 106. As a result, models are mostly trained from examples with large operands. 90% of operands are larger than 100,000, and small instances, like gcd(6, 9), are almost never encountered. This contrast with the way we are taught, and teach, arithmetic. We usually insist that small examples should be mastered, and sometimes memorized, before larger instances, like gcd(102370, 102372) can be tackled.\nIn this section, I sample training pairs from a log-uniform distribution, by uniformly sampling real numbers 0 \u2264 x \u2264 logM , computing ex and rounding to the nearest integer. In this setting, the training set has as many 1-digit as 6-digit operands. In 3% of training example, both operands are smaller than 10, and in 11% of examples, both are smaller than 100. This presents the model with many simple examples that it can memorize, just like children rote learn multiplication and addition tables. This is different from curriculum learning: the distribution of operands does not change during training. Also, the log-uniform sampling only applies to the training set (the test sets are unaffected), and it has no impact on the distribution of outcomes.\nTraining from log-uniform operands greatly improves performance (Table 6). Accuracy for all bases is between 94 and 99%, compared to 61 and 97% with uniform operands. For base 2401, the number of correct GCD is 73, our best result so far. For base 10, the number of correct GCD is 48 (vs 13). Learning is accelerated: for base 10, GCD 1, 2, 4 and 5 are learned as early as epoch 3, 3 and 8 by epoch 25, 7 and 9 by epoch 220 and 11 by epoch 750.\nAs before, large bases perform better. All models with B \u2264 420 have an accuracy over 98% and correctly predict more than 55 GCD under 100. The divisors orB are learned first, then, small powers of primes are grokked, roughly in order. After training, models have learned to predict all primes up to a certain value, some of their small powers, and all associated products. All primes up to 5 are learned for base 2, up to 11 for base 10, up to 17 for base 100, and up to 23 for base 1024. For base 1024, 2401, and 2744, only 27 GCD are incorrectly predicted:\n\u2022 the 16 primes from 29 and 97, all predicted as 1, \u2022 small multiples of these primes: products of 2 and 29, 31, 37, 41, 43 and 47, predicted as 2, and\nproducts of 3 and 29 and 31, predicted as 3, \u2022 powers of small primes: 49 = 72, predicted as 7, and 81 = 34, predicted as 27. \u2022 small multiples of these: 98 = 49 \u2217 2, predicted as 14.\nThe three rules with grokking (G1 to G3) still apply: predictions are deterministic, for a pair (a, b) with GCD k, the model predicts the largest correctly predicted GCD that divides k.\nLearning curves retain their step-like shape, but they are more noisy, and smoother (see Appendix E.2): transitions now span several epochs, and each new prime takes more examples to be fully learned. While the model During training, while the model learns a new divisor, rules G1 and G3 are temporarily violated. During a few epochs, model predictions are split between the old and the new value (e.g. between 7 and 49 when the model is learning 49). This situation, rarely observed in previous experiments, is common with log-uniform operands.\nLog-uniform outcomes. Balancing the distribution of GCD by making it log-uniform, as described in section 4, together with log-uniform operands, brings another large improvement in performance (Table 7). After 1000 epochs, all models with B larger than 1000 predict 87 to 91 GCD: all primes up to 53 and all composite numbers up to 100. These are our best results. They can be marginally improved by training models from an inverse square root distribution of outcomes (Appendix D.1). Note the low accuracy for base 2: with log-uniform outcomes, the model fails to learn GCD 1, for lack of examples."
        },
        {
            "heading": "6 LEARNING FROM UNIFORM OUTCOMES",
            "text": "Log-uniform distributions of outcomes improve model performance by reducing the imbalance between small and large GCD in the training set. It is therefore tempting to push this logic further, and train models from a uniform distribution of GCD and operands, i.e. sample the training set like the stratified test set from Section 2. Figure 3 presents learning curves for three models using base 10. Model accuracy (measured on the natural test set) seems to vary randomly, and the test loss is flat. Yet, the number of correct GCD is stable over time, and increases in steps, from 10 to 17, in line with the results from section 3 (13 GCD are learned). Something is learned despite the flat loss.\nTable 8 presents the most common model predictions, and their frequencies, for all GCD up to 20. At first glance, predictions seem chaotic. At epoch 266, the model achieves 81% accuracy, and correctly predicts 14 GCD: 1, 2, 5, 8, 20, 32, 40, 44, 48, 50, 64, 75, 80 and 100. One epoch later, accuracy is down to 6%, the model still predicts 14 GCD: 4, 8, 10, 16, 40, 50, 55, 60, 64, 66, 75, 80, 95 and 100, half of the correct GCD have changed! After another epoch, accuracy is 4% and the model predicts 4, 20, 25, 26, 30, 32, 40, 48, 50, 55, 64, 73, 80, 88 and 100. Again, half the correct GCD have changed.\nAs in previous experiments, frequencies are close to 100%: the model makes a unique prediction f(k) for all pairs with GCD k, with the notable exception of epoch 267 where model predictions for 1, 3 ... are split (almost evenly) between 11 and 19. Model predictions cluster by classes of GCD: all elements in class C1 = {1, 3, 7, 9, 11, 13, 17, 19} are predicted as 1 at epoch 266, 19 at epoch 267, 73 at epoch 268, and so on. The same pattern appears for classes C2 = {2, 6, 14, 18}, C4 = {4, 12}\nand C5 = {5, 15}, i.e. pairs of integers both divisible by 2, 4, and 5, that would have been predicted as 2, 4, and 5 by the base 10 model from section 3. In other words, the model learns to cluster input pairs into classes having a common divisor (a product of divisors of 10), just like it did in section 3, but instead of predicting the smallest (and most common) element in each class, it predict a different element at every epoch. We can summarize this into the three rules with uniform outcomes:\n(U1) Predictions are mostly deterministic. At a given epoch, the model usually predicts a unique value f(k) for a given GCD k. In rare cases, the model makes 2 or 3 predictions. (U2) Classes of multiples of products of prime divisors of B are predicted the same. For base 10, some classes are C1 = {1, 3, 7, 9, 11, 13, 17, 19 . . . }, C2 = {2, 6, 14, 18, 22, 26, 34, 38 . . . }, C4 = {4, 12, 24, 36, 44, 52, . . . } and C5 = {5, 15, 35, 55 . . . }. (U3) For each class, the model prediction is an element of the class. Prediction varies from one epoch to the next, but the number of correct GCD is stable over time: it is the number of classes, which increases as the model learns new divisors of B.\nThe three rules explain the variations in the accuracy curve: since 61% of examples in the natural test set have GCD 1, accuracy jumps by 61% every time class C1 is predicted as 1. Rule U3, on the other hand, accounts for the step-shaped learning curve for correct GCD.\nThese results shed light on the learning process and the role of the distribution of outcomes. During training, all models, regardless of outcome distribution, learn to partition their input pairs into classes, with GCD multiples of a product of divisors of the base (or small primes when grokking happens), i.e. for base 10, multiples of 2, 4, 5, 10, 20, and a default class associated to 1. The model makes a unique prediction for all pairs in a class. When the distribution of outcomes is unbalanced, this prediction is the smallest element in the class, which happens to be the most common. When outcomes are uniformly distributed, a different element of the class is predicted at every epoch, somewhat randomly: the model becomes less explainable.\nBase 1000, grokking and loss of determinism. Models with base 1000, trained on uniform operands and outcomes, undergo a similar learning process (see Appendix D.3) during the first 400 training epochs. Grokking sets in around epoch 200. Multiples of 11, 22, 44, 55 and 88 are learned around epoch 220, then multiples of 3 by epoch 260 and of 7 by epoch 400. At this point, 41 GCD are correctly predicted. Note that grokking no longer happens in order: 11 is learned before 3.\nDuring the grokking phase, a new phenomenon develops. As new primes are grokked and more classes are created, model predictions for each class become less deterministic. Instead of predicting a unique value for each class at each epoch, the model now \u201chesitates\u201d between several values, and the frequency of the most common prediction goes down. By epoch 400, for the class C1, the model makes 18 different predictions with frequencies ranging from 2% to 13% (Table 15 in Appendix D.3). Model predictions are no longer explainable, and the three rules are not respected.\nInterestingly, GCD continue to be learned under this new regime, starting with the largest (i.e. the smallest classes of multiples). By epoch 740, 95 GCD under 100 are correctly predicted. The worst performance is achieved for small GCD: 43, 74 and 85% correct predictions for GCD 1, 2 and 3. Appendix D.4 presents results for larger bases, where up to 99 GCD under 100 are learned."
        },
        {
            "heading": "7 DISCUSSION",
            "text": "Can transformers learn the greatest common divisor? With enough examples and appropriate adjustment of their training distribution, they can. Models leveraging large composite bases, and trained on log-uniform operands and outcomes predict over 90 of the 100 first GCD. Models trained on uniform outcomes predict 95 GCD. However, the experiments from section 3 show the limits of naive, benchmark-based evaluations on arithmetic tasks: high accuracies (95%) can be achieved, on held-out test sets of of random examples, by models that only predict a handful of GCD.\nThe approach to explainability presented in this paper differs from most works on the subject. Instead of looking at model parameters, I engineer experiments that reveal the algorithms that the model is implementing. It is often repeated that transformers are incomprehensible black-boxes, that sometimes confabulate and often fail in unpredictable ways. Here, model predictions can be fully characterized by a small number of rules. This is a promising direction for future research.\nExperiments indicate that transformers learn a sieve algorithm for computing GCD. The model first learns divisibility by products of divisors of the base, which can be tested by looking at the last digits of a number, or counting its rightmost zeroes. Using these rules, the model clusters its input pairs into classes of multiples of divisors of the base, and predicts the GCD as the minimum for the class. All GCD corresponding to products of divisors of B2 are learned this way. At the end of this phase, in base 2, the model correctly predicts 1, 2, 4, 8, 16 and 32.\nAs training proceeds, new prime divisors are learned (grokked) in order. They are all prime because multiples of previous divisors were learned already, i.e. the model functions like a sieve. Every time a new divisor p is learned, all existing classes are split between multiples and non-multiples of p. In base 2, once the model learns divisibility by 3, six new classes are created: multiples of 3, 6, 12, 24, 48 and 96 (splitted from 1, 2, 4, 8, 16 and 32. This accounts for the steps observed in the learning curves. A GCD is correctly predicted once all the powers of primes dividing it are learned. Eventually, all GCD will be learned this way.\nExperiments with uniform outcomes suggest that an unbalanced training distribution of GCD is needed for this algorithm to succeed, because it causes each class to be predicted by its smallest, and most common, member (the correct GCD), and it guarantees that primes are learned in order. Interestingly, this algorithm is not related to Euclid\u2019s algorithm. Note also that it is not specific to transformers: Appendix D.5 shows that similar results can be achieved with LSTM and GRU.\nAnother important finding is the role of training distributions. All models are tested on sets with uniform operands, but the best results are achieved with a log-uniform distribution of operands and outcomes in the training set. This may come as a surprise, since many authors observed that evaluating a model out of its training distribution has a negative impact on performance. The existence of special training distributions, that allow for faster learning and more robust models (with respect to out-of-distribution generalization) was already observed for linear algebra (Charton, 2022a).\nA log-uniform distribution of operands strikes a balance between memorization and generalization, and helps models learn hard instances by memorizing easier cases. This is related to curriculum learning, but avoids catastrophic forgetting, because the training distribution never changes. These observations may apply to other arithmetic tasks. On the other hand, a log-uniform distribution of outcomes helps learning by enforcing a better representation of large GCD in the training set, a classical recipe in machine learning (calssifiers are often trained on balanced datasets). The counterintuitive result is that a perfectly balanced, uniform training distribution set degrades performance by preventing the model from learning small GCD, and breaking model explainability.\nIs it really grokking? Power et al. (2022) define grokking as \u201cgeneralization far after overfitting.\u201d In our experiments, training and test data are generated on the fly from a very large problem space. No overfitting can happen, and the classical pattern of grokking, train accuracy dropping, and validation accuracy catching up after a long time, will not occur. The similarity with grokking lies in the sudden change in accuracy after a long stagnation of the training loss."
        },
        {
            "heading": "A RATIONAL ARITHMETIC WITH TRANSFORMERS",
            "text": "In these experiments, transformers are trained to perform five arithmetic operations on positive rational numbers:\n\u2022 comparison: given four positive integers a, b, c and d, predict whether ab < c d . \u2022 Integer division: given two integers a and b, predict the integer bab c. \u2022 Addition: given four integers a, b, c and d, predict the sum ab + c d , in lowest terms. \u2022 Multiplication: given four integers a, b, c and d, predict the product ab \u00d7 c d , in lowest terms. \u2022 Simplification: given two integers a and b, predict the lowest term representation of ab , i.e. c d with c = a gcd(a,b) and d = b gcd(a,b) .\nFor the comparison, addition and multiplication tasks, all integers a, b, c and d are uniformly sampled between 1 and M (M=100,000 or 1,000,000).\nFor the simplification task, 3 integers m,n, p are uniformly sampled between 1 and M , we let a = pmgcd(m,n) and b = pn gcd(m,n) and the model is tasked to predict a and b.\nFor the integer division task, 3 integersm,n, p are uniformly sampled between 1 andM , withm < n, we let a = pn+m and b = n, and the model is tasked to predict p = bab c. All integers are encoded as sequences of digits in base B (see section 2). Sequence to sequence transformers with 4 layers, 512 dimensions and 8 attention heads are trained to minimize a crossentropy loss, using Adam with learning rate 10\u22124, inverse square root scheduling, linear warmup over 10, 000 optimization steps, and a batch size of 256. After each epoch (300,000 examples), models are tested on 100,000 random examples.\nComparison is learned to very high accuracy, and integer division to some extent. On the other hand, the three tasks involving GCD calculations (simplification, addition and multiplication) are not learned (Table 9)."
        },
        {
            "heading": "B MODEL SCALING FOR THE BASE EXPERIMENTS",
            "text": "Section 3 presents results for 4-layer transformers with 512 dimensions and 8 attention heads. In this section, we experiment with very small models (down to 1 layer and 32 dimensions), and very large ones (up to 24 layers and 1024 dimensions). Note: in Tables 10 and 11, the number of trainable parameters are indicated for base 10, they will be larger for larger bases, because larger vocabularies increase the number of parameters in the embedding and decoding layers.\nTable 10 presents accuracies for models with one layer, 8 attention heads, and 32 to 512 dimensions. These models have 3 to 100 times less parameters that the 4-layer baseline, but there is no significant change in trained model accuracy for 12 different bases.\nTable 11 presents results for models from 6 to 24 layers, symmetric (same number of layers in the encoder and decoder), or asymmetric (using a one-layer encoder or decoder). The dimensions are 512, 640, 768 and 1024 for 6, 8, 12, and 24 layers, and the dimension-to-attention-heads ratio is kept constant at 64 (i.e.there are 8, 10, 12 and 24 attention heads respectively). Again, model size has no significant impact on accuracy.\nOverall, these scaling experiments suggest that trained model performance is stable over a wide range of model size (300 thousands to 700 millions parameters). These results are strikingly different from what is commonly observed in Natural Language Processing, where very small transformers (under a few million parameters) cannot learn, and accuracy improves with model size."
        },
        {
            "heading": "C THEORETICAL VALUES OF ACCURACY",
            "text": "In this section, we compute a theoretical accuracy for models from section 3 that follow the three rules, assuming that all products of prime divisors of B are correctly predicted. The distribution of the GCD of random uniform positive integers verifies: P (gcd(a, b) = k) = 6\u03c02k2 (Ces\u00e0ro, 1883).\nTherefore, if B = pk, with p prime, theoretical model accuracy is\nA(pk) = A(p) = 6 \u03c02 \u221e\u2211 i=0 1 p2i = 6 \u03c02 p2 p2 \u2212 1 ,\nif B = pkql, A(B) = 1\u2212 \u03c0 2\n6 (1\u2212A(p))(1\u2212A(q)),\nif B = pkqlrm, A(B) = 1\u2212 \u03c0 4\n36 (1\u2212A(p))(1\u2212A(q))(1\u2212A(r)), and so on.\nTable 12 compares theoretical accuracies with empirical observations. Best model performances may be higher than theory, because of sampling errors in the test set, or lower than theory when some powers of prime divisors of B have not been learned."
        },
        {
            "heading": "D ADDITIONAL EXPERIMENTS",
            "text": "D.1 EXPERIMENTS WITH OUTCOME DISTRIBUTIONS\nThe results at the end of section 5 demonstrate that training from a log-uniform distribution of GCD (P (gcd = k) = Ck ) improves model performance compared to the natural, inverse square distribution (P (gcd = k) = Ck2 ) . In this section, we experiment with three alternative distributions of outcomes:\n\u2022 a \u201clong-tail\u201d log-uniform distribution: instead of sampling GCD between 1 and 100, they are sampled between 1 and 200, \u2022 an inverse square root distribution of outcomes: P (gcd = k) = C\u221a k , \u2022 an inverse power 1.5 distribution: P (gcd = k) = C k \u221a k ."
        },
        {
            "heading": "1000 71 71 91 90 91",
            "text": ""
        },
        {
            "heading": "1024 71 72 90 85 91",
            "text": ""
        },
        {
            "heading": "2017 63 64 88 87 88",
            "text": ""
        },
        {
            "heading": "2021 66 71 89 87 92",
            "text": ""
        },
        {
            "heading": "2023 65 67 88 85 90",
            "text": ""
        },
        {
            "heading": "2025 70 71 91 88 92",
            "text": ""
        },
        {
            "heading": "2187 66 70 91 86 91",
            "text": ""
        },
        {
            "heading": "2197 68 65 90 85 91",
            "text": ""
        },
        {
            "heading": "2209 65 68 87 85 90",
            "text": ""
        },
        {
            "heading": "2401 73 69 89 85 92",
            "text": ""
        },
        {
            "heading": "2744 72 72 91 88 89",
            "text": ""
        },
        {
            "heading": "3125 65 67 91 87 92",
            "text": ""
        },
        {
            "heading": "3375 67 68 91 87 92",
            "text": ""
        },
        {
            "heading": "4000 66 60 90 85 90",
            "text": ""
        },
        {
            "heading": "4913 57 60 88 90 92",
            "text": ""
        },
        {
            "heading": "5000 64 65 89 90 91",
            "text": ""
        },
        {
            "heading": "10000 56 55 88 90 91",
            "text": "Table 13 presents results for 17 bases between 1000 and 10000, for models trained with log-uniform operands and five distributions of outcomes. As we observed in section 5, a log-uniform distribution of outcomes achieves better performances than the natural (inverse square) distribution. The inverse power 1.5 distribution of outcomes only brings marginal improvement over the natural distribution. With log-uniform outcomes, sampling GCD up to 200 instead of 100 has a negative impact on the number of correct GCD, except for the largest bases. On the other hand, training from an inverse square root distribution of outcomes improves performance for all bases. For 6 bases, 92 GCD under 100 are predicted correctly.\nD.2 LEARNING WITH SMALLER BATCHES\nA common advice, when training transformers on natural language processing tasks, is to use the largest possible batches (i.e. as many as will fit in GPU memory). Large batches have two advantages, they avoid extreme gradients by averaging them over many samples, and they accelerate training by reducing the number of optimization steps. All models in this paper were trained with batches of 256 examples. In this section, we experiment with batches of 64, training models with log-uniform operands (and various outcome distributions) for about 800 epochs.\nTable 14 compares models with batches of 64 to batches of 256, trained for a week (about 800 epochs for batch 64, 1300 for batch 256), on 11 different bases. For the same training time, batch size seems to have little impact on performance. This suggests that models could be trained on machines with less GPU memory, at no penalty.\nD.3 UNIFORM OPERANDS AND OUTCOMES - BASE 1000\nIn this section, we provide detailed results for models using base 1000, and trained on uniform operands and outcomes. Learning curves (Figure 4) are similar to those for base 10 (Figure 3) during the first 200 epochs: loss curves are flat, accuracy varies wildly, and the number of correct GCD has the characteristic step-like shape observed throughout this paper. Grokking, characterized by steep drops in the loss and increases in the number of correct GCD, happens between epochs 200 and 400. Then, the accuracy and the number of correct GCD, rise steadily. After 800 epochs 95 (out of 100) GCD are correctly predicted.\nMore precisely, by epoch 180, the model has learned to classify all examples into 14 sets: multiples of 1, 2, 4, 5, 8, 10, 16, 20, 25, 32, 40, 50, 80 and 100. At each epoch, the model selects one element in each class, which is its unique prediction for all pairs of integers with GCD in the class: the rules U1 to U3 are respected.\nGrokking sets in around epoch 200, and by epoch 220, 5 new classes have been learned: multiples of 11 (11, 33, 77 and 99), 22, 44, 55 and 88, created by \u201csplitting away\u201d the multiples of 11 from the classes of multiples of 1, 2, 4, 5 and 8. Because of uniform outcomes, grokking does not happen in increasing order: 11 is learned before 3. By epoch 260, multiples of 3 are learned and the model predicts 31 different outcomes (splitting 12 classes, from 1 to 32). By epoch 400, multiples of 7 are learned, and 41 GCD are predicted.\nDuring the grokking phase, a new phenomenon develops. As new primes are grokked and more classes are created, model predictions for each class become less deterministic. Instead of predicting a unique value for each class at each epoch, the model now \u201chesitates\u201d between several values, and the frequency of the most common prediction goes down. By epoch 400, for the class of multiples of 1, the model makes 18 different predictions with frequencies ranging from 2% to 13% (Table 15).\nAt this point, model predictions are neither deterministic nor interpretable, and the three rules are no longer respected. Classes have as many predictions as there are elements, and the model begins learning individual GCD, beginning with the largest ones (i.e. the smallest classes). By epoch 740, 95 of the 100 first GCD are correctly predicted, the worst performance being achieved on the smallest values (GCD 1, 2 and 3, correctly predicted 43, 74 and 85% of the time).\nD.4 UNIFORM OUTCOMES - LARGER BASES\nIn these experiments, models are trained for 1200 epochs, from uniform and log-uniform operands, and uniform outcomes. As previously, large bases achieve the best results. Models trained on uniform operands also seem to perform better.\nD.5 EXPERIMENTS WITH DIFFERENT ARCHITECTURES\nIn this section, we experiment with two popular recurrent architectures: long short-term memories (LSTM) (Hochreiter & Schmidhuber, 1997), and gated recurrent units (GRU) (Cho et al., 2014). We train models with 1024 and 2048 dimensions, and four layers, on uniform operands, log-uniform operands and log-uniform operands and outcomes, for 10 different bases: 10, 30, 31, 210, 420, 1000, 2021, 2023, 2025 and 2401.\nWe note that, by and large, the results for our experiments with transformers extend to other recurrent networks. After 500 epochs, models trained on uniform operaands (table 17 achieve performances similar to those obtained in sections 3 and 4. Composite bases like 210 and 420 achieve the best results (35 and 38 GCD), and large bases allow for grokking small primes. We observe no clear advantage of LSTM over GRU, or 2048 over 124 dimensions. Models trained on log-uniform operands (table 18) and outcomes (table 19) perform better, but results are lower (after comparable training time) than with trasnformers."
        },
        {
            "heading": "E ADDITIONAL RESULTS",
            "text": "E.1 GROKKING\nE.2 LEARNING CURVES - LOG-UNIFORM OPERANDS\nE.3 DETAILED MODEL PREDICTIONS - BASE EXPERIMENTS"
        }
    ],
    "title": "LEARNING THE GREATEST COMMON DIVISOR: EXPLAINING TRANSFORMER PREDICTIONS",
    "year": 2024
}