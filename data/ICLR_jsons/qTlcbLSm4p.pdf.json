{
    "abstractText": "Diffusion models achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find the main reason is that the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or lowresolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256\u00d7256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at https://github.com/THUDM/RelayDiffusion. Figure 1: (left): Generated Samples by RDM on ImageNet 256\u00d7256 and CelebA-HQ 256\u00d7256. (right): Benchmarking recent diffusion models on class-conditional ImageNet 256\u00d7256 generation without any guidance. RDM can achieve a FID of 1.99 (and a class-balanced FID of 1.87) if with classifier-free guidance.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiayan Teng"
        },
        {
            "affiliations": [],
            "name": "Wendi Zheng"
        },
        {
            "affiliations": [],
            "name": "Ming Ding"
        },
        {
            "affiliations": [],
            "name": "Wenyi Hong"
        },
        {
            "affiliations": [],
            "name": "Jianqiao Wangni"
        },
        {
            "affiliations": [],
            "name": "Zhuoyi Yang"
        },
        {
            "affiliations": [],
            "name": "Jie Tang"
        }
    ],
    "id": "SP:3812cc75979782132ad9a012b9eaede852f1b211",
    "references": [
        {
            "authors": [
                "Yogesh Balaji",
                "Seungjun Nah",
                "Xun Huang",
                "Arash Vahdat",
                "Jiaming Song",
                "Karsten Kreis",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine",
                "Bryan Catanzaro"
            ],
            "title": "ediffi: Text-to-image diffusion models with an ensemble of expert denoisers",
            "venue": "arXiv preprint arXiv:2211.01324,",
            "year": 2022
        },
        {
            "authors": [
                "Christopher M Bishop",
                "Nasser M Nasrabadi"
            ],
            "title": "Pattern recognition and machine learning, volume",
            "year": 2006
        },
        {
            "authors": [
                "Andrew Brock",
                "Jeff Donahue",
                "Karen Simonyan"
            ],
            "title": "Large scale gan training for high fidelity natural image synthesis",
            "venue": "arXiv preprint arXiv:1809.11096,",
            "year": 2018
        },
        {
            "authors": [
                "Ting Chen"
            ],
            "title": "On the importance of noise scheduling for diffusion models",
            "venue": "arXiv preprint arXiv:2301.10972,",
            "year": 2023
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ming Ding",
                "Zhuoyi Yang",
                "Wenyi Hong",
                "Wendi Zheng",
                "Chang Zhou",
                "Da Yin",
                "Junyang Lin",
                "Xu Zou",
                "Zhou Shao",
                "Hongxia Yang"
            ],
            "title": "Cogview: Mastering text-to-image generation via transformers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Shanghua Gao",
                "Pan Zhou",
                "Ming-Ming Cheng",
                "Shuicheng Yan"
            ],
            "title": "Masked diffusion transformer is a strong image synthesizer",
            "venue": "arXiv preprint arXiv:2303.14389,",
            "year": 2023
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "Communications of the ACM,",
            "year": 2020
        },
        {
            "authors": [
                "Jiatao Gu",
                "Shuangfei Zhai",
                "Yizhe Zhang",
                "Miguel Angel Bautista",
                "Josh Susskind"
            ],
            "title": "f-dm: A multistage diffusion model via progressive signal transformation",
            "venue": "arXiv preprint arXiv:2210.04955,",
            "year": 2022
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "arXiv preprint arXiv:2207.12598,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Chitwan Saharia",
                "William Chan",
                "David J Fleet",
                "Mohammad Norouzi",
                "Tim Salimans"
            ],
            "title": "Cascaded diffusion models for high fidelity image generation",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Emiel Hoogeboom",
                "Tim Salimans"
            ],
            "title": "Blurring diffusion models",
            "venue": "arXiv preprint arXiv:2209.05557,",
            "year": 2022
        },
        {
            "authors": [
                "Emiel Hoogeboom",
                "Jonathan Heek",
                "Tim"
            ],
            "title": "Salimans. simple diffusion: End-to-end diffusion for high resolution images",
            "venue": "arXiv preprint arXiv:2301.11093,",
            "year": 2023
        },
        {
            "authors": [
                "Ahmed Imtiaz Humayun",
                "Randall Balestriero",
                "Richard Baraniuk"
            ],
            "title": "Polarity sampling: Quality and diversity control of pre-trained generative networks via singular values",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Tero Karras",
                "Timo Aila",
                "Samuli Laine",
                "Jaakko Lehtinen"
            ],
            "title": "Progressive growing of gans for improved quality, stability, and variation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine"
            ],
            "title": "Elucidating the design space of diffusionbased generative models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tuomas Kynk\u00e4\u00e4nniemi",
                "Tero Karras",
                "Samuli Laine",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Improved precision and recall metric for assessing generative models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Ziwei Liu",
                "Ping Luo",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Charlie Nash",
                "Jacob Menick",
                "Sander Dieleman",
                "Peter W Battaglia"
            ],
            "title": "Generating images with sparse representations",
            "venue": "arXiv preprint arXiv:2103.03841,",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "William Peebles",
                "Saining Xie"
            ],
            "title": "Scalable diffusion models with transformers",
            "venue": "arXiv preprint arXiv:2212.09748,",
            "year": 2022
        },
        {
            "authors": [
                "Hao Phung",
                "Quan Dao",
                "Anh Tran"
            ],
            "title": "Wavelet diffusion models are fast and scalable image generators",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical textconditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Severi Rissanen",
                "Markus Heinonen",
                "Arno Solin"
            ],
            "title": "Generative modelling with inverse heat dissipation",
            "venue": "arXiv preprint arXiv:2206.13397,",
            "year": 2022
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "MICCAI 2015: 18th International Conference,",
            "year": 2015
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L Denton",
                "Kamyar Ghasemipour",
                "Raphael Gontijo Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tim Salimans",
                "Ian Goodfellow",
                "Wojciech Zaremba",
                "Vicki Cheung",
                "Alec Radford",
                "Xi Chen"
            ],
            "title": "Improved techniques for training gans",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Axel Sauer",
                "Katja Schwarz",
                "Andreas Geiger"
            ],
            "title": "Stylegan-xl: Scaling stylegan to large diverse datasets",
            "venue": "In ACM SIGGRAPH 2022 conference proceedings,",
            "year": 2022
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "arXiv preprint arXiv:2010.02502,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "arXiv preprint arXiv:2011.13456,",
            "year": 2020
        },
        {
            "authors": [
                "Arash Vahdat",
                "Karsten Kreis",
                "Jan Kautz"
            ],
            "title": "Score-based generative modeling in latent space",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Bowen Zhang",
                "Shuyang Gu",
                "Bo Zhang",
                "Jianmin Bao",
                "Dong Chen",
                "Fang Wen",
                "Yong Wang",
                "Baining Guo"
            ],
            "title": "Styleswin: Transformer-based gan for high-resolution image generation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Long Zhao",
                "Zizhao Zhang",
                "Ting Chen",
                "Dimitris Metaxas",
                "Han Zhang"
            ],
            "title": "Improved transformer for high-resolution gans",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Diffusion models (Ho et al., 2020; Rombach et al., 2022) succeeded GANs (Goodfellow et al., 2020) and autoregressive models (Ramesh et al., 2021; Ding et al., 2021) to become the most prevalent\ngenerative models in recent years. However, challenges still exist in the training of diffusion models for high-resolution images. More specifically, there are two main obstacles:\nTraining Efficiency. Although equipped with UNet to balance the memory and computation cost across different resolutions, diffusion models still require a large amount of resources to train on high-resolution images. One popular solution is to train the diffusion model on a latent (usually 4\u00d7 compression rate in resolution) space and map the result back as pixels (Rombach et al., 2022), which is fast but inevitably suffers from some low-level artifacts. The cascaded method (Ho et al., 2022; Saharia et al., 2022) trains a series of varying-size super-resolution diffusion models, which is effective but needs a complete sampling for each stage separately.\nNoise Schedule. Diffusion models need a noise schedule to control the amount of the isotropic Gaussian noise at each step. The setting of the noise schedule shows great influence over the performance, and most current models follow the linear (Ho et al., 2020) or cosine (Nichol & Dhariwal, 2021) schedule. However, an ideal noise schedule should be resolution-dependent (See Figure 2 or Chen (2023)), resulting in suboptimal performance to train high-resolution models directly with common schedules designed for resolutions of 32\u00d732 or 64\u00d764 pixels. These obstacles hindered previous researchers from establishing an effective end-to-end diffusion model for high-resolution image generation. Dhariwal & Nichol (2021) attempted to directly train a 256\u00d7256 ADM but found that it performs much worse than the cascaded pipeline. Chen (2023) and Hoogeboom et al. (2023) carefully adjusted the hyperparameters of the noise schedule and architecture for high-resolution cases, but the quality is still not comparable to the state-of-the-art cascaded method (Saharia et al., 2022).\nIn our opinion, the cascaded method contributes in both training efficiency and noise schedule: (1) It provides flexibility to adjust the model size and architecture for each stage to find the most efficient combination. (2) The existence of low-resolution condition makes the early sampling steps easy, so that the common noise schedules (optimized for low-resolution models) can be applied as a feasible baseline to the super-resolution models. Moreover, (3) high-resolution images are more difficult to obtain on the Internet than low-resolution images. The cascaded method leverages the knowledge from low-resolution samples, meanwhile keeps the capability to generate high-resolution images. Therefore, it might not be a promising direction to completely replace the cascaded method with an end-to-end one at the current stage.\nThe disadvantages of the cascaded method are also obvious: (1) Although the low-resolution part is determined, a complete diffusion model starting from pure noise is still trained and sampled for super-resolution, which is time-consuming. (2) The distribution mismatch between ground-truth and the generated low-resolution condition will hurt the performance, so that tricks like conditioning augmentation (Ho et al., 2022) become vitally important to mitigate the gap. Besides, the noise schedule of high-resolution stages are still not well studied.\nPresent Work. Here we present the Relay Diffusion Model (RDM), a new cascaded framework to improve the shortcomings of the previous cascaded methods. In each stage, the model starts diffusion from the result of the last stage, instead of conditioning on it and starting from pure noise. Our method is named as the cascaded models work together like a \u201crelay race\u201d. The contributions of this paper can be summarized as follows:\n\u2022 We analyze the reasons of the difficulty of noise scheduling in high-resolution diffusion models in frequency domain. Previous works like LDM (Rombach et al., 2022) assume all image signals from the same distribution when analyzing the SNR, neglecting the difference in frequency domain between low-resolution and high-resolution images. Our analysis successfully accounts for phenomenon that the same noise level shows different perceptual effects on different resolutions, and introduce the block noise to bridge the gap.\n\u2022 We propose RDM to disentangle the diffusion process and the underlying neural networks in the cascaded pipeline. RDM gets rid of the low-resolution conditioning and its distribution mismatch problem. Since RDM starts diffusion from the low-resolution result instead of pure noise, the training and sampling steps can also be reduced.\n\u2022 We evaluate the effectiveness of RDM on unconditional CelebA-HQ 256\u00d7256 and conditional ImageNet 256\u00d7256 datasets. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet."
        },
        {
            "heading": "2 PRELIMINARY",
            "text": ""
        },
        {
            "heading": "2.1 DIFFUSION MODELS",
            "text": "To model the data distribution pdata(x0), denoising diffusion probabilistic models (DDPMs, Ho et al. (2020)) define the generation process as a Markov chain of learned Gaussian transitions. DDPMs first assume a forward diffusion process, corrupting real data x0 by progressively adding Gaussian noise from time steps 0 to T , whose variance {\u03b2t} is called the noise schedule:\nq(xt|xt\u22121) = N (xt; \u221a 1\u2212 \u03b2txt\u22121, \u03b2tI). (1)\nThe reverse diffusion process is learned by a time-dependent neural network to predict denoised results at each time step, by optimizing the variational lower bound (ELBO).\nMany other formulations for diffusion models include stochastic differential equations (SDE, Song et al. (2020b)), denoising diffusion implicit models (DDIM, Song et al. (2020a)), etc. Karras et al. (2022) summarizes these different formulations into the EDM framework. In this paper, we generally follow the EDM formulation and implementation. The training objective of EDM is defined as L2 error terms:\nEx\u223cpdata,\u03c3\u223cp(\u03c3)E\u03f5\u223cN (0,I)\u2225D(x+ \u03c3\u03f5, \u03c3)\u2212 x\u2225 2, (2)\nwhere p(\u03c3) represents the distribution of a continuous noise schedule. D(x + \u03c3\u03f5, \u03c3) represents the denoiser function depending on the noise scale. We also follow the EDM precondition for D(x+ \u03c3\u03f5, \u03c3) with \u03c3-dependent skip connection (Karras et al., 2022).\nCascaded diffusion model (CDM, Ho et al. (2022)) is proposed for high-resolution generation. CDM divides the generation into multiple stages, where the first stage generates low-resolution images and the following stages perform super-resolution conditioning on the outputs of the previous stage. fDM (Gu et al., 2022) unifies multiple resolutions of image generation with a linear interpolation in a single model. Cascaded models are extensively adopted in recent works of text-to-image generation, e.g. Imagen (Saharia et al., 2022), DALL-E-2 (Ramesh et al., 2022) and eDiff-I (Balaji et al., 2022)."
        },
        {
            "heading": "2.2 BLURRING DIFFUSION",
            "text": "The Inverse Heat Dissipation Model (IHDM) (Rissanen et al., 2022) generates images by reversing the heat dissipation process. The heat dissipation is a thermodynamic process describing how the temperature u(x, y, t) at location (x, y) changes in a (2D) space with respect to the time t. The dynamics can be denoted by a PDE \u2202u\u2202t = \u22022u \u2202x2 + \u22022u \u2202y2 .\nBlurring diffusion (Hoogeboom & Salimans, 2022) is further derived by augmenting the Gaussian noise with heat dissipation for image corruption. Since simulating the heat equation up to time t is equivalent to a convolution with a Gaussian kernel with variance \u03c32 = 2t in an infinite plane (Bredies et al., 2018), the intermediate states xt become blurry, instead of noisy in the standard diffusion. If Neumann boundary conditions are assumed, blurring diffusion in discrete 2D pixel space can be transformed to the frequency space by Discrete Cosine Transformation (DCT) conveniently as:\nq(ut|u0) = N (ut|Dtu0, \u03c32t I), (3)\nwhere ut = DCT(xt) , and Dt = e\u039bt is a diagonal matrix with \u039bi\u00d7W+j = \u2212\u03c02( i 2 H2 + j2\nW 2 ) for coordinate (i, j). Here Gaussian noise with variance \u03c32t is mixed into the blurring diffusion process to transform the deterministic dissipation process to a stochastic one for diverse generation."
        },
        {
            "heading": "3 METHOD",
            "text": ""
        },
        {
            "heading": "3.1 MOTIVATION",
            "text": "The noise schedule is vitally important to the diffusion models and is resolution-dependent. A certain noise level appropriately corrupting the 64\u00d7 64 images, could fail to corrupt the 256\u00d7 256 (or a higher resolution) images, which is shown in the first row of Figure 2(a)(b). Chen (2023) and Hoogeboom et al. (2023) attributed this to the lack of schedule-tuning, but we found that an analysis from the perspective of frequency spectrum can help us better understand this phenomenon.\nFrequency spectrum analysis of the diffusion process. The natural images with different resolutions can be viewed as the result of visual signals sampled at varying frequencies. To compare the frequency features of a 64 \u00d7 64 image and a 256 \u00d7 256 image, we can upsample the 64 \u00d7 64 one to 256 \u00d7 256, perform DCT and compare them in the 256-point DCT spectrum. The second row of Figure 2(a) shows the signal noise ratio (SNR) at different frequencies and diffusion steps. In Figure 2(b), we clearly find that the same noise level on a higher resolution results in a higher SNR in (the low-frequency part of) the frequency domain. Detailed frequency spectrum analysis are included in Appendix D.\nAt a certain diffusion step, a higher SNR means that during training the neural network presumes the input image more accurate, but the early steps may not be able to generate such accurate images after the increase in SNR. This training-inference mismatch will accumulate over step by step during sampling, leading to the degradation of performance.\nBlock noise as the equivalence at high resolution. After the upsampling from 64\u00d764 to 256\u00d7256, the independent Gaussian noise on 64 \u00d7 64 becomes noise on 4 \u00d7 4 grids, thus greatly changes its frequency representation. To find a variant of the s\u00d7 s-grid noise without deterministic boundaries, we propose Block noise, where the Gaussian noises are correlated for nearby positions. More specifically, the covariance between noise \u03f5x0,y0 and \u03f5x1,y1 is defined as\nCov(\u03f5x0,y0 , \u03f5x1,y1) = \u03c32\ns2 max\n( 0, s\u2212 dis(x0, x1) ) max ( 0, s\u2212 dis(y0, y1) ) , (4)\nwhere \u03c32 is the noise variance, and s is a hyperparameter kernel size. The dis(\u00b7, \u00b7) function here is the Manhattan distance. For simplicity, we \u201cconnect\u201d the top and bottom edges and the left and right edges of the image, resulting in\ndis(x0, x1) = min (|x0 \u2212 x1|, xmax \u2212 |x0 \u2212 x1|) . (5)\nThe block noise with kernel size s can be generated by averaging s\u00d7 s independent Gaussian noise. Suppose we have an independent Gaussian noise matrix \u03f5, the block noise construction function Block[s](\u00b7) is defined as\nBlock[s](\u03f5)x,y = 1\ns s\u22121\u2211 i=0 s\u22121\u2211 j=0 \u03f5x\u2212i,y\u2212j , (6)\nwhere Block[s](\u03f5)x,y is the block noise at the position (x, y), and \u03f5\u2212x = \u03f5xmax\u2212x. Figure 2(c) shows that the block noise with kernel size s = 4 on 256\u00d7 256 has a similar frequency spectrum as the independent Gaussian noise on 64\u00d7 64 images.\nThe analysis above seems to indicate that we can design an end-to-end model for high-resolution images by introducing block noise in early diffusion steps, while cascaded models already achieves great success. Therefore, a revisit of the cascaded models is necessary.\nWhy does the cascaded models alleviate this issue? Experiments in previous works (Nichol & Dhariwal, 2021; Dhariwal & Nichol, 2021) have already shown that cascaded models perform better than end-to-end models under a fair setting. These models usually use the same noise schedule in all stages, so why are the cascaded models not affected by the increase of SNR? The reason is that in the super-resolution stages, the low-resolution condition greatly eases the difficulty of the early steps, so that although the higher SNR requires a more accurate input, the accuracy is within the capability of the model.\nA natural idea is that since the low-frequency information in the high-resolution stage has already been determined by the low-resolution condition, we can continue generating directly from the upsampled result to reduce both the training and sampling steps. However, the generation of lowresolution images is not perfect, and thus the solution of the distribution mismatch between groundtruth and generated low-resolution images is a priority to \u201ccontinue\u201d the diffusion process."
        },
        {
            "heading": "3.2 RELAY DIFFUSION",
            "text": "We propose relay diffusion model (RDM), a cascaded pipeline connecting the stages with block noise and (patch-level) blurring diffusion. Different from CDM, RDM considers the equivalence of the low-resolution generated images when upsampled to high resolution. Suppose that the generated 64\u00d7 64 low-resolution image xL0 = xL + \u03f5L can be decomposed into a sample in real distribution xL and a remaining noise \u03f5L \u223c N (0, \u03b220I). As mentioned in section 3.1, the 256\u00d7 256 equivalence of \u03f5L is Block[4] noise with variance \u03b220 , denoted by \u03f5H . After (nearest) upsampling, x\nL becomes xH , where each 4 \u00d7 4 grid share the same pixel values. We can define it as the starting state of a patch-wise blurring diffusion.\nUnlike blurring diffusion models (Rissanen et al., 2022) (Hoogeboom & Salimans, 2022) that perform the heat dissipation on the entire space of images, we propose to implement the heat dissipation on each 4\u00d74 patch independently, which is of the same size as the upsampling scale. We first define a series of patch-wise blurring matrices {Dpt }, which is introduced in detail in Appendix A.1. The forward process would have a similar representation with equation 3:\nq(xt|x0) = N (xt|V DptV Tx0, \u03c3t2I), t \u2208 {0, .., T}, (7) where V T is the projection matrix of DCT and \u03c3t is the variance of noise. Here the D p T is chosen to guarantee V DpTV Tx0 in the same distribution as xH , meaning that the blurring process ultimately makes the pixel value in each 4\u00d7 4 patch the same.\nThe training objective of the high-resolution stage of RDM generally follows EDM (Karras et al., 2022) framework in our implementation. We replace the Gaussian noise in equation 7 with a mixture of Gaussian noise and block noise in section 3.1. The loss function is defined on the prediction of denoiser function D to fit with true data x, which is written as:\nEx\u223cpdata,t\u223cU(0,1),\u03f5\u223cN (0,I),\u03f5\u2032\u223cN (0,I)\u2225D(xt, \u03c3t)\u2212 x\u2225 2,\nwhere xt = V D p tV T\ufe38 \ufe37\ufe37 \ufe38 blurring x+ \u03c3\u221a 1 + \u03b12\n( \u03f5+ \u03b1 \u00b7 Block[s](\u03f5\u2032)\ufe38 \ufe37\ufe37 \ufe38\nblock noise\n) , (8)\nwhere \u03f5 and \u03f5\u2032 are two independent Gaussian noise. The main difference in training between RDM and EDM is that the corrupted sample xt is not simply xt = x + \u03f5, but a mixture of the blurred image, block noise and independent Gaussian noise. Ideally, the noise should gradually transfer from block noise to high-resolution independent Gaussian noise, but we find that a weighting average strategy perform well enough, because the low-frequency component of the block noise is much larger than the independent Gaussian noise, and vice versa for high-frequency component. \u03b1 is a hyperparameter and the normalizer 1\u221a\n1+\u03b12 is used to keep the variance of the noise, \u03c32 unchanged.\nThe advantages of RDM compared to CDM includes:\n\u2022 RDM is more efficient, because RDM skips the re-generation of low-frequency information in the high-resolution stages, and reduce the number of training and sampling steps.\n\u2022 RDM is simpler, because it gets rid of the low-resolution conditioning and conditioning augmentation tricks. The consumption from cross-attention with the low-resolution condition is also spared.\n\u2022 RDM is more potential in performance, because RDM is a Markovian denoising process (if with DDPM sampler). Artifacts in the low-resolution images could be corrected in the high-resolution stage, while CDM is trained to correspond to the low-resolution condition.\nCompared to end-to-end models (Chen, 2023; Hoogeboom et al., 2023),\n\u2022 RDM is more flexible to adjust the model size and leverage more low-resolution data."
        },
        {
            "heading": "3.3 STOCHASTIC SAMPLER",
            "text": "Since RDM differs from traditional diffusion models in the forward process, we also need to adapt the sampling algorithms. In this section, we focus on the EDM sampler (Karras et al., 2022) due to its flexibility to switch between the first and second order (Heun\u2019s) samplers.\nHeun\u2019s method introduces an additional step for the correction of the first-order sampling. The updating direction of a first-order sampling step is controlled by the gradient term dn = xn\u2212x\u03b8(xn,\u03c3tn )\n\u03c3tn . The correction step updates current states with an averaged gradient term dn+dn\u221212 .\nHeun\u2019s method takes account of the change of gradient term dxdt between tn and tn\u22121. Therefore, it achieves higher quality while allowing for fewer steps of sampling.\nWe adapt the EDM sampler to the blurring diffusion of RDM\u2019s super-resolution stage following the derivation of DDIM (Song et al., 2020a). We define the indices of sampling steps as {ti}Ni=0, in corresponding to the noisy states of images {xi}Ni=0. To apply blurring diffusion, images are transformed into frequency space by DCT as ui = V Txi. Song et al. (2020a) uses a family of inference distributions to describe the diffusion process. We can write it for blurring diffusion as:\nq\u03b4(u1:N |u0) = q\u03b4(uN |u0) N\u220f\nn=2\nq\u03b4(un\u22121|un,u0), (9)\nwhere \u03b4 \u2208 RN\u22650 denotes the index vector for the distribution. For all n > 1, the backward process is: q\u03b4(un\u22121|un,u0) = N ( un\u22121| 1 \u03c3tn ( \u221a \u03c32tn\u22121 \u2212 \u03b42nun + (\u03c3tnD p tn\u22121 \u2212 \u221a \u03c32tn\u22121 \u2212 \u03b42nD p tn)u0), \u03b4 2 nI ) .\n(10)\nThe mean of the normal distribution ensures the forward process to be consistent with the formulation of blurring diffusion in Section 3.2, which is q(un|u0) = N (un|Dptnu0, \u03c3 2 tnI). We provide a detailed proof of the consistency between our sampler and the formulation of blurring diffusion in Appendix A.3. When the index vector \u03b4 is 0, the sampler degenerates into an ODE sampler. We set \u03b4n = \u03b7\u03c3tn\u22121 for our sampler, where \u03b7 \u2208 [0, 1) is a fixed scalar controlling the scale of randomness injected during sampling. We substitute the definition into equation 10 to obtain our sampler function as:\nun\u22121 = (D p tn\u22121 + \u03b3n(I \u2212D p tn))un + \u03c3tn(\u03b3nD p tn \u2212D p tn\u22121) un \u2212 u\u03030 \u03c3tn + \u03b7\u03c3tn\u22121\u03f5, (11)\nwhere \u03b3n \u225c \u221a 1\u2212 \u03b72 \u03c3tn\u22121\u03c3tn . As in the section 3.1, we also need to consider block noise besides blurring diffusion. The adaptation is just to replace isotropic Gaussian noise \u03f5 with \u03f5\u0303, which is a weighted sum of the block noise and isotropic Gaussian noise. u\u03030 = u\u03b8(un, \u03c3tn) is predicted by the neural network.\nFinally, a stochastic sampler for the super-resolution stage of RDM is summaried in Appendix A.4."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL SETTING",
            "text": "Dataset. We use CelebA-HQ and ImageNet in our experiments. CelebA-HQ (Karras et al., 2018) is a high-quality subset of CelebA (Liu et al., 2015) which consists of 30,000 images of faces from human celebrities. ImageNet (Deng et al., 2009) contains 1,281,167 images spanning 1000 classes and is a widely-used dataset for generation and other vision tasks. We train RDM on these datasets to generate 256\u00d7 256 images. See Appendix C.1 for further experiments on higher resolutions. Architecture and Training. RDM adopts UNet (Ronneberger et al., 2015) as the backbone of diffusion models for all stages. The detailed architectures largely follow ADM (Dhariwal & Nichol, 2021) for fair comparison. We train unconditional models on CelebA-HQ and class-conditional models on ImageNet respectively. Since we follow the EDM implementation, we directly use the released checkpoint from EDM in ImageNet in the 64\u00d764 stage. We calculate the training consumption by the number of training samples at 256\u00d7256 resolution, while also including the training cost of the 64\u00d7 64 stage in the total calculation. According to Appendix B.1, the FLOPs of the 64\u00d7 64 model are less than 1/10 that of the 256 \u00d7 256 model. So we add 1/10 of the first stage\u2019s number of training samples to the 256\u00d7 256 stage\u2019s to be the total training consumption. See Appendix B.1 for more information about the architecture and hyperparameters.\nEvaluation. We use metrics including FID (Heusel et al., 2017), sFID (Nash et al., 2021), IS (Salimans et al., 2016), Precision and Recall (Kynka\u0308a\u0308nniemi et al., 2019) for a comprehensive evaluation of the results. FID measures the difference between the features of model generations and real images, which is extracted by a pretrained Inception network. sFID differs from FID by using intermediate features, which better measures the similarity of spatial distribution. IS and Precision both measure the fidelity of the samples, while Recall indicates the diversity. We compute metrics with 50,000 and 30,000 generated samples for ImageNet and CelebA-HQ respectively.\nTable 2: Effect of stochasticity in the sampler on ImageNet 256\u00d7256 (top) and CelebA-HQ 256\u00d7256 (bottom). We explored different values of \u03b7 in Eq. 11.\n\u03b7 0 0.10 0.15 0.20 0.25 0.30 0.40 0.50\nFID\u2193 5.65 5.44 5.31 5.27 5.48 5.91 6.91 9.17\n\u03b7 0 0.10 0.15 0.20 0.25 0.30 0.40 0.50\nFID\u2193 4.11 3.74 3.43 3.15 3.23 3.52 4.79 6.41"
        },
        {
            "heading": "4.2 RESULTS",
            "text": "CelebA-HQ We compare RDM with the existing methods on CelebA-HQ 256 \u00d7 256 in Table 1, 512 \u00d7 512 in Table 6 and 1024 \u00d7 1024 in Table 7. RDM outperforms the state-of-the-art model\n1class-balance means making the number of images generated for each class same among 50,000 images.\nStyleSwin (Zhang et al., 2022) with a remarkably fewer training iterations (50M versus 820M trained images). We also achieve the best precision and recall among the existing works.\nImageNet Table 3 shows the performance of class-conditional generative models on ImageNet 256\u00d7 256. We report the best results as possible of the existing methods with classifier-free guidance (CFG) (Ho & Salimans, 2022). RDM achieves the best sFID and outperforms all the other methods by FID except MDT-XL/2 (Gao et al., 2023) with a dynamic CFG scale. If with a fixed but bestpicked CFG scale2, MDT-XL/2 can only achieve an FID of 2.26. While achieving competitive results, RDM is trained with only 70% of the iterations of MDT-XL/2 (1.2B versus 1.7B trained images), indicating that the longer training and a more granular CFG strategy are potential directions to further optimize the FID of RDM.\nTraining Efficiency We also compare the performance of RDM with existing methods along with the training cost in Figure 1. When CFG is disabled, RDM achieves a better FID than previous state-of-the-art diffusion models including DiT (Peebles & Xie, 2022) and MDT (Gao et al., 2023). RDM outperforms them even with only about 1/3 training iterations."
        },
        {
            "heading": "4.3 ABLATION STUDY",
            "text": "In this section, we conduct ablation experiments on the designs of RDM to verify their effectiveness. Unless otherwise stated, we report results of RDM on 256\u00d7 256 generation without CFG. The Effectiveness of block noise. We compare the performance of RDM with and without adding block noise in Figure 4. With a sufficient phase of training, RDM with block noise outperforms the model without block noise by a remarkable margin on both ImageNet and CelebA-HQ. This demonstrates the effectiveness of the block noise. The addition of block noise introduces higher modeling complexity of the noise pattern, which contributes to a slower convergence of training in the initial stage, as illustrated by Figure 4(a). We assume that training on a significantly smaller scale of samples leads to a fast convergence of the model, which obliterates such a feature, therefore a similar phenomenon cannot be observed in the training of CelebA-HQ.\nThe scale of stochasticity. As previous works (Song et al., 2020b) have shown, SDE samplers usually perform better than ODE samplers. We want to quantitatively measure how the scale of the stochaticity affects the performance in the RDM sampler (Algorithm 1). Table 2 shows results with \u03b7 varying from 0 to 0.50. For both CelebA-HQ and ImageNet, the optimal FID is achieved by \u03b7 = 0.2. We hypothesize a small \u03b7 is insufficient for the noise addition to cover the bias formed\n2The best CFG scale is 1.325 with a hyperparameter sweep from 1.0 to 1.8. We observed the FID increases greatly if CFG scale > 1.5 for MDT-XL/2.\nin earlier sampling steps, while a large \u03b7 introduces excessive noise into the process of sampling, which makes a moderate \u03b7 to be the best choice. Within a reasonable scale of stochasticity, an SDE sampler always outperforms the ODE sampler by a significant margin.\nSampling steps. To show the efficiency of our model, we compare the performance of RDM and other methods with fewer sampling steps. Number of Function Evaluations (NFE), i.e., the number that a neural network is called during sampling, is used as the index of the comparison for fairness. For RDM, the NFE consists of the NFE in the second stage and 1/10 the NFE in the first stage, according to the proportion of the FLOPs. As shown in Figure 5, the performance of DiT-XL/2 and MDT-XL/2 both drop significantly with a lower NFE, while RDM barely declines. Considering that the steps in different stages may contribute differently in FID, we demonstrates three FLOPs allocation strategies in Figure 5. With more NFE allocated in the first stage, RDM achieves a better FID. In all settings, RDM performs better than MDT-XL/2 and DiT-XL/2 if NFE < 200."
        },
        {
            "heading": "5 CONCLUSION AND DISCUSSION",
            "text": "In this paper, we propose relay diffusion to optimize the cascaded pipeline. The diffusion process can now continue when changing the image resolution or model architectures. We anticipate that our method can reduce the cost of training and inference, and help create more advanced text-to-image model in the future.\nThe frequency analysis in the paper reveals the relation between noise and image resolution, which might be helpful to design a better noise schedule. However, our numerous attempts to theoretically derive the optimal noise schedule on the dataset from a frequency perspective did not yield good results. The reason might be that the optimal noise schedule is also related to the size of the model, inductive bias, and the nuanced distribution characteristics of the data. Further investigation is left for future work."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work is supported by Technology and Innovation Major Project of the Ministry of Science and Technology of China under Grant 2022ZD0118600, the NSFC for Distinguished Young Scholar 61825602, Tsinghua University Initiative Scientific Research Program 20233080067 and the New Cornerstone Science Foundation through the XPLORER PRIZE. The authors also thank Ting Chen from Google DeepMind and Junbo Zhao from Zhejiang University for their valuable talks and comments."
        },
        {
            "heading": "AUTHOR CONTRIBUTIONS",
            "text": "Ming Ding proposes the methods and leads the project. Jiayan Teng and Wendi Zheng conduct most of the experiments. Wenyi Hong works together on early experiments. Jianqiao Wangni, Wenyi Hong and Zhuoyi Yang contribute to the writing of the paper. Jie Tang provides guidance and supervision.\nThe work is partly done during the internship of Jiayan Teng and Wendi Zheng at Zhipu AI."
        },
        {
            "heading": "A DERIVATION",
            "text": ""
        },
        {
            "heading": "A.1 PATCH-WISE BLURRING",
            "text": "The forward process of blurring diffusion is defined as Eq. 3, where u0 = V Tx0 denotes the representation of the image x0 in the frequency space. The diagonal matrix Dt = e\u039bt defines a non-isotropic blurring projection, where \u039b(i\u00d7W + j, i\u00d7W + j) = \u2212\u03c02( i 2\nH2 + j2\nW 2 ) corresponds to the coordinate (i, j) in the 2D frequency space. In the equation q(ut|u0) = N (ut|Dtu0, \u03c32t I), we can utilize the dot product of matrices to transform Dt and u0 into 2D matrices, D\u0303t and u\u03030, in the shape of H \u00d7W for calculation:\nDtu0 \u21d2 D\u0303t \u00b7 u\u03030 (12)\nIn the super-resolution stage of RDM, we apply blurring on each k \u00d7 k patch independently. We name it as patch-wise blurring and define the diagonal blurring matrix in the shape of k\u00d7 k for each patch as:\nD\u0303t,k\u00d7k = exp(\u039b\u0303k\u00d7kt), \u039b\u0303k\u00d7k(i, j) = \u2212\u03c02( i2\nk2 +\nj2 k2 ), (13)\nwhere i \u2208 [0, k), j \u2208 [0, k). For any patch, D\u0303t,k\u00d7k remains the same. The blurring matrix D\u0303pt of the patch-wise blurring is a combination of all the independent blurring matrices D\u0303t,k\u00d7k. The relationship between the elements of D\u0303pt and D\u0303t,k\u00d7k can be expressed as:\nD\u0303pt (i, j) = D\u0303t,k\u00d7k(i mod k, j mod k), (14)\nwhere (i, j) corresponds to the coordinate in the 2D frequency space. Finally, Dpt in Eq. 7 can be formulated as:\nDpt = diag(unfold(D\u0303 p t )), (15)\nwhere unfold(D\u0303pt ) means unfolding the H\u00d7W matrix into a vector of HW dimensions and diag(v) denotes the diagonal matrix with vector v as its diagonal line."
        },
        {
            "heading": "A.2 COMBINATION OF SCHEDULE",
            "text": "We follow Karras et al. (2022) to set the noise schedule for standard diffusion as ln(\u03c3) \u223c N (Pmean, P 2std). We use FD and F \u22121 D to denote the cumulative distribution function (CDF) and the inverse distribution function (IDF) for distribution D in the following description. With t sampled from uniform distribution U(0, 1), the noise scale is formulated as:\n\u03c3(t) = exp(F\u22121N (Pmean,P 2std)(t)). (16)\nFor the super-resolution stage of RDM, we apply a truncated version of diffusion noise schedule \u03c3\u2032(t), t \u223c U(0, 1). If we set ts as the starting point of the truncation, the new noise schedule can be formally expressed as:\n\u03c3\u2032(t) = \u03c3(F\u22121U(0,1)(FU(0,1)(ts)FU(0,1)(t))), (17)\nwhich means we only sample the noise scale \u03c3\u2032 from positions of the normal distribution N (Pmean, P 2std) where its CDF is less than ts. For the process of blurring, we set its schedule following the setting of Hoogeboom & Salimans (2022). They found that the heat dissipation is equivalent to a Gaussian blur with the variance of its kernel as \u03c32B,t = 2\u03c4t. They set the blurring scale \u03c3B,t as:\n\u03c3B,t = \u03c3B,max sin 2( t\u03c0\n2 ), (18)\nwhere t is also sampled from the uniform distribution U(0, 1) and \u03c3B,max denotes a fixed hyperparameter. Empirically, we set \u03c3B,max = 3 for ImageNet 256\u00d7 256 and \u03c3B,max = 2 for CelebA-HQ 256 \u00d7 256. The blurring matrix is formulated as Dt = e\u039b\u03c4t , where \u03c4t = \u03c32B,t 2 . As illustrated in Section 2.2, \u039b is a diagonal matrix and \u039bi\u00d7W+j = \u2212\u03c02( i 2 H2 + j2 W 2 ) for coordinate (i, j)."
        },
        {
            "heading": "A.3 SAMPLER DERIVATION",
            "text": "In this section, we prove the consistency between the design of our sampler and the formulation of blurring diffusion. We need to prove that the jointly distribution q\u03b4(un\u22121|un,u0) we define in Eq. 10 matches with the marginal distribution\nq\u03b4(un|u0) = N (un|Dptnu0, \u03c3 2 tnI) (19)\nunder the condition of Eq. 9.\nproof. Given that q\u03b4(uN |u0) = N (uN |DptNu0, \u03c3 2 tN I), we proceed with a mathematical induction approach. Assuming that for any n \u2264 N , q\u03b4(un|u0) = N (un|Dptnu0, \u03c3 2 tnI) holds. We only need to prove q\u03b4(un\u22121|u0) = N (un\u22121|Dptn\u22121u0, \u03c3 2 tn\u22121I), and then the conclusion above will be proved based on the induction hypothesis.\nFirstly, based on\nq\u03b4(un\u22121|u0) = \u222b q\u03b4(un\u22121|un,u0)q(un|u0)dun, (20)\nwe introduce\nq\u03b4(un\u22121|un,u0) = N ( un\u22121| 1 \u03c3tn ( \u221a \u03c32tn\u22121 \u2212 \u03b42nun + (\u03c3tnD p tn\u22121 \u2212 \u221a \u03c32tn\u22121 \u2212 \u03b42nD p tn)u0), \u03b4 2 nI ) (21) and\nq\u03b4(un|u0) = N (un|Dptnu0, \u03c3 2 tnI). (22)\nThen according to Bishop & Nasrabadi (2006), q\u03b4(un\u22121|u0) is also a Gaussian distribution:\nq\u03b4(un|u0) = N (un|\u00b5n\u22121,\u03a3n\u22121). (23)\nTherefore, from Eq. 20, we can derive that\n\u00b5n\u22121 = 1 \u03c3tn ( \u221a \u03c32tn\u22121 \u2212 \u03b42nD p tnu0 + (\u03c3tnD p tn\u22121 \u2212 \u221a \u03c32tn\u22121 \u2212 \u03b42nD p tn)u0) = D p tn\u22121u0 (24)\nand\n\u03a3n\u22121 = \u03c32tn\u22121 \u2212 \u03b4 2 n\n\u03c32tn \u03c32tnI + \u03b4 2 nI = \u03c3 2 tn\u22121I. (25)\nSumming up, q\u03b4(un\u22121|u0) = N (un\u22121|Dptn\u22121u0, \u03c3 2 tn\u22121I). The inductive proof is complete."
        },
        {
            "heading": "A.4 STOCHASTIC SAMPLER",
            "text": "Algorithm 1 the RDM second-order stochastic sampler sample x\u0302N from results of the first stage \u25b7 i.e. images at the resolution of 64\u00d7 64 xN = interpolate(x\u0302N , 256,mode = \u201dnearest\u201d) + \u03c3tn \u03f5\u0303 \u25b7 upsample 64px images to blurry 256px images and apply the truncated schedule in Appendix A.2 uN = V\nTxN \u25b7 transformed into the frequency domain for n \u2208 {N, . . . , 1} do\n\u03b3n = \u221a 1\u2212 \u03b72 \u03c3tn\u22121\u03c3tn , \u03b4n = \u03b7\u03c3tn\u22121 \u25b7 coefficient of the random term u\u03030 = u\u03b8(un, \u03c3tn) \u25b7 model prediction at tn dn =\nun\u2212u\u03030 \u03c3tn \u25b7 first-order gradient term at tn un\u22121 = (D p tn\u22121 + \u03b3n(I \u2212D p tn))un + \u03c3tn(\u03b3nD p tn \u2212D p tn\u22121)dn + \u03b4n\u03f5\u0303\n\u25b7 from tn to tn\u22121 using Euler\u2019s method\nif n \u0338= 1 then \u25b7 the second-order part u\u0303\u20320 = u\u03b8(un\u22121, \u03c3tn\u22121) \u25b7 model prediction at tn\u22121 dn\u22121 =\nun\u22121\u2212u\u0303\u20320 \u03c3tn\u22121\n\u25b7 gradient term at tn\u22121\nd\u2032n = dn+dn\u22121\n2 \u25b7 second-order gradient term u\u2032n\u22121 = (D p tn\u22121 + \u03b3n(I \u2212D p tn))un + \u03c3tn(\u03b3nD p tn \u2212D p tn\u22121)d \u2032 n + \u03b4n\u03f5\u0303 \u25b7 correction\nend if\nun\u22121 = u \u2032 n\u22121\nend for x0 = V u0\nAs for the sampler of the first stage, we follow the EDM sampler (Karras et al., 2022). Of course, samplers such as DDPM are also capable. After all, the first stage is just a standard diffusion model."
        },
        {
            "heading": "B MODEL DETAILS",
            "text": ""
        },
        {
            "heading": "B.1 HYPERPARAMETERS",
            "text": "Hyperparameters we use for the training of RDM are presented in Table 4. We set the architecture hyperparameters for diffusion models following Dhariwal & Nichol (2021), in corresponding to the input resolutions. For the experiments on CelebA-HQ, we set the model dropout to be larger (0.15 and 0.2 for two stages respectively), and enable sample augmentation to prevent RDM from overfitting."
        },
        {
            "heading": "B.2 TRAINING COST",
            "text": "On ImageNet, the first stage model was trained on 32 V100 for 13 days according to EDM (Karras et al., 2022) and the second stage model (64 \u2192 256) was trained on 64 40G-A100 for 12.5 days. On CelebA-HQ, we trained the first stage model on 32 40G-A100 for 16 hours and the second stage model (64 \u2192 256) on 32 40G-A100 for 25.5 hours."
        },
        {
            "heading": "C ADDITIONAL EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "C.1 FURTHER EXPERIMENTS ON HIGHER RESOLUTIONS",
            "text": "To further show that RDM can easily scale to high-resolution image generation without carefully adjusting the hyperparameters of noise schedule and architecture. We take dataset CelebA-HQ as an example and conduct experiments on higher resolutions: 512 and 1024. Except for the different hyperparameters shown in Table 5, other settings remain the same as the 64\u2192256 model in the second stage.\nThe cost in Table 6 and Table 7 contains the training of three stages, with the same equivalent conversion as Table 1 and Table 3, according to GFLOPs. As shown in the table, RDM achieves state-of-the-art FID at the resolution of both 512 and 1024, and only requires a small amount of training in the third stage according to Table 5. This demonstrates that RDM can be easily extended from two stages to three stages and higher resolutions. Examples are shown in Figure 12."
        },
        {
            "heading": "C.2 ADD BLOCK NOISE TO END-TO-END DIFFUSION MODEL",
            "text": "To further demonstrate the effectiveness of block noise, we conduct ablation experiments on an endto-end model followed by the setting of ADM (Dhariwal & Nichol, 2021). We use a mixture of\nfixed ratio block noise and Gaussian noise as illustrated in section 3.2. As shown in Figure 6, ADM with block noise outperforms the model without block noise by a remarkable margin on CelebA-HQ 256\u00d7 256."
        },
        {
            "heading": "C.3 CORRECT ARTIFACTS IN THE LOW-RESOLUTION IMAGES",
            "text": "As illustrated in section 3.2, the super-resolution generation of RDM is a Markovian process, in comparison of CDM and ADM-U using low-resolution conditioning all along the generation process. This could improve the robustness of RDM on handling artifacts from low-resolution stages. Figure 7 shows the comparison of super-resolution generation between RDM and ADM-U. We use 64 \u00d7 64 samples from ImageNet as low-resolution inputs, adding 0.05 scale of Gaussian Noise to introduce artifacts. While RDM successfully handles the noise to generate clean 256\u00d7256 samples, ADM-U preserves the noise to the 256\u00d7 256 samples."
        },
        {
            "heading": "D DETAILS ABOUT THE POWER SPECTRAL DENSITY",
            "text": ""
        },
        {
            "heading": "D.1 CALCULATION PROCEDURE OF THE PSD",
            "text": "We follow the setting of Rissanen et al. (2022) to calculate the PSD in the frequency space. The PSD at a certain frequency is defined as the square of the DCT coefficient at that frequency. Firstly,\nwe transform the image into the 2D frequency space by DCT and set the frequency range to [0, \u03c0]. To obtain the 1D curve of the PSD, we calculate the distance from each point (x, y) to the origin in the frequency space, i.e. \u221a x2 + y2, considering it as a 1D frequency value. Subsequently, we uniformly divide the frequency domain into N intervals, and take the midpoint of each interval as its representative frequency value. Finally, we take the mean of the PSD values for all points within the interval as the PSD value for that interval, in order to get N coordinate pairs for plotting. The SNR curve in Figure 2 can be obtained in a similar approach, while the only difference is that the vertical axis values are replaced with the absolute value of the ratio between the DCT coefficients for the image and noise in the frequency space."
        },
        {
            "heading": "D.2 ANALYSIS OF THE PSD",
            "text": "As shown in Figure 8, the PSD of real images gradually decreases from low frequency to high frequency. And the intensity of Gaussian noise components across all frequency bands is generally equal. Therefore, when corrupting real images, Gaussian noise initially drowns out high-frequency components until the noise intensity becomes high enough to drown out the low-frequency components of real images. And it is demonstrated in Figure 2 that, as the resolution of images increases, less information is corrupted under the same noise intensity. Correspondingly, as shown in Figure 8(a) and Figure 8(b), the low-frequency portion of the PSD gets drowned out more slowly as the resolution increases. It is indicated that we will introduce excessive high-frequency components of noise when corrupting the low-frequency information of real images, especially for high-resolution images.\nDifferently, the low-frequency portion of the PSD from block noise is notably higher than that of Gaussian noise with the same intensity. Furthermore, the PSD of block noise exhibits a decreasing trend as frequency increases, and its curve is quite similar to the PSD curve of Gaussian noise at the resolution of 64 upsampled to the resolution of 256. This leads to the PSD curves of high-resolution images with added block noise and that of low-resolution images with added Gaussian noise also\nbeing quite similar. As a result, the low-frequency portion of the PSD from images with added block noise gets drowned out more quickly than that from images with added Gaussian noise. We can conclude that block noise can corrupt the low-frequency components of images more easily."
        },
        {
            "heading": "E ADDITIONAL SAMPLES",
            "text": "Section 4.3 quantitatively compares the performance of RDM with other models under the same NFE and demonstrates the superiority of RDM with fewer sampling steps. Figure 9 shows qualitative comparison results. While other models achieve competitive quality of generation with sufficient NFE, their performances degenerate noticeably with the decrease of NFE. In contrast, RDM still maintains comparable generation quality with a low NFE.\nFigure 10 compares visualized samples generated by the best settings of StyleGAN-XL (Sauer et al., 2022), DiT (Peebles & Xie, 2022) and RDM. StyleGAN-XL is in the framework of GAN, while DiT and RDM are diffusion models. RDM achieves the best quality of images synthesis. Figure 11 exhibits more examples generated by our model RDM on ImageNet 256\u00d7 256."
        }
    ],
    "title": "RELAY DIFFUSION: UNIFYING DIFFUSION PROCESS ACROSS RESOLUTIONS FOR IMAGE SYNTHESIS",
    "year": 2024
}