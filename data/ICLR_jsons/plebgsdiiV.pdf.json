{
    "abstractText": "We consider off-policy evaluation (OPE) of deterministic target policies for reinforcement learning (RL) in environments with continuous action spaces. While it is common to use importance sampling for OPE, it suffers from high variance when the behavior policy deviates significantly from the target policy. In order to address this issue, some recent works on OPE proposed in-sample learning with importance resampling. Yet, these approaches are not applicable to deterministic target policies for continuous action spaces. To address this limitation, we propose to relax the deterministic target policy using a kernel and learn the kernel metrics that minimize the overall mean squared error of the estimated temporal difference update vector of an action value function, where the action value function is used for policy evaluation. We derive the bias and variance of the estimation error due to this relaxation and provide analytic solutions for the optimal kernel metric. In empirical studies using various test domains, we show that the OPE with in-sample learning using the kernel with optimized metric achieves significantly improved accuracy than other baselines.",
    "authors": [],
    "id": "SP:9ed17d6ca3d6c713fd9fea4591efacd6f9ad063d",
    "references": [
        {
            "authors": [
                "Roy De Maesschalck",
                "Delphine Jouan-Rimbaud",
                "D\u00e9sir\u00e9 L Massart"
            ],
            "title": "The mahalanobis distance",
            "venue": "Chemometrics and Intelligent Laboratory Systems,",
            "year": 2000
        },
        {
            "authors": [
                "Justin Fu",
                "Aviral Kumar",
                "Ofir Nachum",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "D4rl: Datasets for deep data-driven reinforcement learning",
            "venue": "arXiv preprint arXiv:2004.07219,",
            "year": 2020
        },
        {
            "authors": [
                "Scott Fujimoto",
                "Herke van Hoof",
                "David Meger"
            ],
            "title": "Addressing function approximation error in actor-critic methods",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Scott Fujimoto",
                "David Meger",
                "Doina Precup"
            ],
            "title": "A deep reinforcement learning approach to marginalized importance sampling with the successor representation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Seungyul Han",
                "Youngchul Sung"
            ],
            "title": "Dimension-wise importance sampling weight clipping for sample-efficient reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Josiah Hanna",
                "Scott Niekum",
                "Peter Stone"
            ],
            "title": "Importance sampling policy evaluation with an estimated behavior policy",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Nathan Kallus",
                "Angela Zhou"
            ],
            "title": "Policy evaluation and optimization with continuous treatments",
            "venue": "In Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics,",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Ksenia Konyushova",
                "Yutian Chen",
                "Thomas Paine",
                "Caglar Gulcehre",
                "Cosmin Paduraru",
                "Daniel J Mankowitz",
                "Misha Denil",
                "Nando de Freitas"
            ],
            "title": "Active offline policy selection",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hoang Le",
                "Cameron Voloshin",
                "Yisong Yue"
            ],
            "title": "Batch policy learning under constraints",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Haanvid Lee",
                "Jongmin Lee",
                "Yunseon Choi",
                "Wonseok Jeon",
                "Byung-Jun Lee",
                "Yung-Kyun Noh",
                "KeeEung Kim"
            ],
            "title": "Local metric learning for off-policy evaluation in contextual bandits with continuous actions",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Sergey Levine",
                "Aviral Kumar",
                "George Tucker",
                "Justin Fu"
            ],
            "title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems",
            "venue": "arXiv preprint arXiv:2005.01643,",
            "year": 2020
        },
        {
            "authors": [
                "Qiang Liu",
                "Lihong Li",
                "Ziyang Tang",
                "Dengyong Zhou"
            ],
            "title": "Breaking the curse of horizon: Infinitehorizon off-policy estimation",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Prasanta Chandra Mahalanobis"
            ],
            "title": "On the generalized distance in statistics",
            "venue": "In Proceedings of the National Institute of Sciences of India,",
            "year": 1936
        },
        {
            "authors": [
                "Travis Mandel",
                "Yun-En Liu",
                "Sergey Levine",
                "Emma Brunskill",
                "Zoran Popovic"
            ],
            "title": "Offline policy evaluation across representations with applications to educational games",
            "venue": "In Autonomous Agents and Multiagent Systems,",
            "year": 2014
        },
        {
            "authors": [
                "R\u00e9mi Munos",
                "Tom Stepleton",
                "Anna Harutyunyan",
                "Marc Bellemare"
            ],
            "title": "Safe and efficient off-policy reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Ofir Nachum",
                "Yinlam Chow",
                "Bo Dai",
                "Lihong Li"
            ],
            "title": "Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yung-Kyun Noh",
                "Byoung-Tak Zhang",
                "Daniel D Lee"
            ],
            "title": "Generative local metric learning for nearest neighbor classification",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2010
        },
        {
            "authors": [
                "Yung-Kyun Noh",
                "Masashi Sugiyama",
                "Kee-Eung Kim",
                "Frank Park",
                "Daniel D Lee"
            ],
            "title": "Generative local metric learning for kernel regression",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Doina Precup",
                "Richard S Sutton",
                "Sanjoy Dasgupta"
            ],
            "title": "Off-policy temporal-difference learning with function approximation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2001
        },
        {
            "authors": [
                "Matthew Schlegel",
                "Wesley Chung",
                "Daniel Graves",
                "Jian Qian",
                "Martha White"
            ],
            "title": "Importance resampling for off-policy prediction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "David Silver",
                "Guy Lever",
                "Nicolas Heess",
                "Thomas Degris",
                "Daan Wierstra",
                "Martin Riedmiller"
            ],
            "title": "Deterministic policy gradient algorithms",
            "venue": "In International conference on machine learning,",
            "year": 2014
        },
        {
            "authors": [
                "Yi Su",
                "Pavithra Srinath",
                "Akshay Krishnamurthy"
            ],
            "title": "Adaptive estimator selection for off-policy evaluation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Adith Swaminathan",
                "Akshay Krishnamurthy",
                "Alekh Agarwal",
                "Miro Dudik",
                "John Langford",
                "Damien Jose",
                "Imed Zitouni"
            ],
            "title": "Off-policy evaluation for slate recommendation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Emanuel Todorov",
                "Tom Erez",
                "Yuval Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,",
            "year": 2012
        },
        {
            "authors": [
                "Cameron Voloshin",
                "Hoang M Le",
                "Nan Jiang",
                "Yisong Yue"
            ],
            "title": "Empirical study of off-policy policy evaluation for reinforcement learning",
            "year": 1911
        },
        {
            "authors": [
                "Tengyang Xie",
                "Yifei Ma",
                "Yu-Xiang Wang"
            ],
            "title": "Towards optimal off-policy evaluation for reinforcement learning with marginalized importance sampling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Mengjiao Yang",
                "Ofir Nachum",
                "Bo Dai",
                "Lihong Li",
                "Dale Schuurmans"
            ],
            "title": "Off-policy evaluation via the regularized lagrangian",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tianhe Yu",
                "Garrett Thomas",
                "Lantao Yu",
                "Stefano Ermon",
                "James Y Zou",
                "Sergey Levine",
                "Chelsea Finn",
                "Tengyu Ma"
            ],
            "title": "Mopo: Model-based offline policy optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Hongchang Zhang",
                "Yixiu Mao",
                "Boyuan Wang",
                "Shuncheng He",
                "Yi Xu",
                "Xiangyang Ji"
            ],
            "title": "Insample actor critic for offline reinforcement learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Shangtong Zhang",
                "Bo Liu",
                "Shimon Whiteson"
            ],
            "title": "Gradientdice: Rethinking generalized offline estimation of stationary values",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Yufan Zhao",
                "Michael R Kosorok",
                "Donglin Zeng"
            ],
            "title": "Reinforcement learning design for cancer clinical trials",
            "venue": "Statistics in Medicine,",
            "year": 2009
        },
        {
            "authors": [
                "Noh"
            ],
            "title": "leading-order bias dominates over the leading-order variance in LOMSE when d\u226b 4. By plugging in h\u2217 (Eq. (10)) to the LOMSE in Eq. (9), and taking limit d\u2192\u221e, we derive similar relation between LOMSE and the bias as in the work",
            "year": 2017
        },
        {
            "authors": [
                "Fujimoto"
            ],
            "title": "\u2212amax, amax] for ith action dimension is denoted as U(ai| \u2212 amax, amax). For the first action dimension, the mixture of 80% Gaussian density and 20% uniform density is used. For the other dimensions, actions are sampled from the uniform densities. The behavior policy is similar to the one used in the work",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Off-policy evaluation (OPE) aims to assess the performance of a target policy by using offline data sampled from a separate behavior policy, without the target policy interacting with the environment (Fujimoto et al., 2021; Su et al., 2020). OPE has gained considerable significance due to the potential costs and risks associated with a reinforcement learning (RL) agent interacting with real-world environments when evaluating a policy in domains such as healthcare (Murphy et al., 2001; Zhao et al., 2009), education (Mandel et al., 2014), robotics (Yu et al., 2020), and recommendation systems (Swaminathan et al., 2017). In the context of offline RL, OPE plays a vital role since the RL agent lacks access to the environment. Moreover, OPE can be leveraged to evaluate policies prior to their deployment in real-world settings and policy selection (Konyushova et al., 2021).\nWhile OPE has been actively researched, OPE regarding deterministic target policies has not been extensively studied, and existing OPE algorithms either do not work or show low performance on evaluating deterministic policies (Schlegel et al., 2019; Fujimoto et al., 2021). However, there are many cases where deterministic policies are required. For example, safety-critical systems such as industrial robot control and drug prescription require precise and consistent control on sequences of actions without introducing variability (Silver et al., 2014; Kallus & Zhou, 2018). Therefore, the need for OPE of deterministic policies emerges. In this paper, we consider evaluating a deterministic policy with offline data.\nMost of the recent works on off-policy evaluation (OPE) use marginalized importance sampling (MIS) approach (Xie et al., 2019; Nachum et al., 2019; Zhang et al., 2020). This approach learns stationary distribution correction ratios of states and actions, which are then utilized to reweight the rewards in the data. Since MIS methods use a single correction ratio, they have relatively lower variance than vanilla importance sampling (IS) methods, which use products of IS ratios of actions in trajectories (Precup et al., 2001; Levine et al., 2020). However, these methods are vulnerable to training instability due to optimizing the minimax objective. SR-DICE (Fujimoto et al., 2021) avoided optimizing the minimax objective by adopting a successor representation learning scheme. However, SR-DICE may use out-of-distribution samples in learning successor representation and thus may suffer from an extrapolation error.\nIn-sample learning methods learn Q-functions using only the samples in the data, avoiding extrapolation error (Schlegel et al., 2019; Zhang et al., 2023). The methods employ an IS ratio to correct the distribution of actions used for querying the Q-values in a temporal difference (TD) target to learn the Q-function. Since a single IS ratio is used, the method has lower variance than the traditional IS approaches (Precup et al., 2001; Levine et al., 2020). However, a limitation of these methods is their inapplicability to deterministic target policies, as the IS ratio for the action is almost surely zero. To overcome this limitation, we aim to extend the use of IR to learn the Q-function in FQE (Voloshin et al., 2019; Le et al., 2019) style specifically tailored for deterministic target policies.\nIn this study, we introduce Kernel Metric learning for In-sample Fitted Q Evaluation (KMIFQE), a novel approach that enables in-sample FQE using offline data. Our contribution is twofold: 1) enabling in-sample OPE of deterministic policies by kernel relaxation and metric learning that has not been used in MDP settings (Kallus & Zhou, 2018; Lee et al., 2022), 2) providing the theoretical guarantee of the proposed method. In theoretical studies, we first derive the MSE of our estimator with kernel relaxation applied on a target policy to avoid having zero IS ratios. From the MSE, we derive the optimal metric scale, referred to as bandwidth, which balances between bias and variance of the estimation and reduces the derived MSE. Then, we derive the metric shape, referred to as the metric, which minimizes the bias induced by the relaxation. Lastly, we show the error bound of the estimated Q-function compared to the true Q-function of a target policy. For empirical studies, we evaluate KMIFQE using a modified classic control domain sourced from OpenAI Gym (Brockman et al., 2016). This evaluation serves to verify if the metrics and bandwidths are learned as intended. Furthermore, we conduct experiments on a more complex MuJoCo domain (Todorov et al., 2012). The experimental results demonstrate the effectiveness of our metric learning approach."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Marginalized importance sampling for OPE in RL Marginalized importance sampling (MIS) OPE method was first introduced by Liu et al. (2018). They pointed out that the importance sampling (IS) OPE estimation in RL suffers from the \"curse of horizon\" due to products of IS ratios used for the IS. They avoided the problem by learning a correction ratio for the marginalized state distribution, which corrects the marginalized state distribution induced by a behavior policy to that of a target policy. Their work was limited in that it required a known behavior policy. Subsequent studies in MIS learned marginalized state-action distribution correction ratios and estimated policy values in a behavior-agnostic manner (Nachum et al., 2019; Zhang et al., 2020; Yang et al., 2020). However, the methods required optimization of minimax learning objectives, which caused instability in learning. SR-DICE (Fujimoto et al., 2021) avoided the minimax learning objective by using the concept of successor representation and made the learning process more stable. However, their result showed that FQE (Voloshin et al., 2019; Le et al., 2019) outperforms all MIS methods in a MuJoCo domain (Todorov et al., 2012) when the target policy is deterministic.\nIn-Sample Learning for OPE in RL In-sample learning methods (Schlegel et al., 2019; Zhang et al., 2023) learn a Q-function of a target policy by using the semi-gradients from expected SARSA temporal-difference (TD) learning loss computed on samples in the data. With importance resampling (IR), the methods correct the distribution of TD update vectors before updating the Q-function rather than reweighting the TD update vectors with IS ratios. They show that IR estimation of the TD update vector has a lower variance than the IS reweighted TD update vectors, resulting in more accurate Q-function estimation. However, these methods cannot be applied to the case where the target policy is deterministic since the IS ratios are zeros almost surely.\nKernel metric learning for OPE in contextual bandits The work of Kallus & Zhou (2018) first enabled IS to estimate the expected rewards for a deterministic target policy. They derived the MSE of the IS estimation when a kernel relaxes the target policy distribution. Then, they derived the optimal bandwidth (scale of the kernel metric) that minimizes it. Subsequent work by Lee et al. (2022) learned a shape of the kernel as a Mahalanobis distance metric (Mahalanobis, 1936; De Maesschalck et al., 2000) that further reduces MSE by reducing the bias of the estimate. These works cannot be directly applied to OPE in RL since that would result in IS estimations using products of IS ratios to correct the distributions of action sequences in trajectories sampled by a behavior policy. As the episode length becomes longer, more IS ratios are multiplied, and this causes the curse of horizon problem (Liu et al., 2018)."
        },
        {
            "heading": "3 BACKGROUND",
            "text": "We seek to evaluate a deterministic target policy that operates in continuous action spaces given offline data sampled with a separate behavior policy. Specifically, we consider the MDP with M = \u27e8S,A, R, P, p0, \u03b3\u27e9 composed of a set of states S \u2208 Rq , set of actions S \u2208 Rd, reward function R : S \u00d7A \u2192 R, state transition probability P : S \u00d7A \u2192 \u2206(S), initial state distribution p0 \u2208 \u2206(S), and a discount factor \u03b3 \u2208 [0, 1). We assume that the offline data D = {(si,ai, ri, s\u2032i,a\u2032i)} n i=1 is sampled from the MDP M by using a stochastic policy \u00b5 : S \u2192 \u2206(A). And D is used to evaluate the deterministic target policy \u03c0\u0303 : S \u2192 A. The distribution of the target policy is denoted as \u03c0 : S \u2192 \u2206(A). The goal is to evaluate the target policy using the offline data D in a normalized discounted return V (\u03c0) = (1\u2212 \u03b3)E\u03c0 [ \u2211\u221e t=0 \u03b3\ntR (st,at)], where E\u03c0[\u00b7] is the expectation using the data sampled from M with \u03c0. The target policy value can be represented with an Qfunction V (\u03c0) = (1 \u2212 \u03b3)Es0\u223cp0(s0),a0\u223c\u03c0(a0 | s0) [Q\u03c0 (s0,a0)], where the Q-function is defined as Q\u03c0(s,a) := E\u03c0 [ \u2211\u221e t=0 \u03b3\ntR (st,at) | s0 = s,a0 = a]. This work estimates a Q-function to evaluate the target policy value."
        },
        {
            "heading": "3.1 IN-SAMPLE TD LEARNING",
            "text": "Temporal difference (TD) learning can be used for estimating Q\u03c0 (Zhang et al., 2023). FQE learns a Q-function by minimizing the following TD loss LTD(\u03b8) (Voloshin et al., 2019; Le et al., 2019):\nLTD(\u03b8) = E(s,a,s\u2032)\u223cp\u00b5 [( R(s,a) + \u03b3Ea\u2032\u223c\u03c0(a\u2032|s\u2032) [ Q\u03b8\u0304 ( s\u2032,a\u2032 )] \u2212Q\u03b8(s,a) )2] , (1)\nwhere Q\u03b8 is Q-function parameterized by \u03b8, Q\u03b8\u0304 is a frozen target network, a transition data sampling distribution is defined as p\u00b5 := d\u00b5(s)\u00b5(a | s)P (s\u2032 | s,a)p(r| s,a)\u00b5(a\u2032 | s\u2032), stationary distribution of state induced by \u00b5 is defined as d\u00b5(s) := (1 \u2212 \u03b3) \u2211\u221e t=0 \u03b3 t \u222b s0 p (s0) p\u03c0 (s0 \u2192 s, t)\u03c0(a | s)d s0, p\u03c0(s0 \u2192 s, t) is the probability of visiting state s after t time steps when the agent\u2019s initial state is s0. Q\u03b8 (or Q\u03b8\u0304) is trained only on (s,a) \u223c d\u00b5(s)\u00b5(a | s) while Q\u03b8\u0304 needs to be evaluated on out-of-distribution (OOD) samples (s\u2032,a\u2032) \u223c d\u00b5(s\u2032)\u03c0(a\u2032 | s\u2032) to compute the loss. Therefore, it may suffer from distributional shift and may produce inaccurate Q-values (Levine et al., 2020).\nTo avoid using OOD samples while computing the update vector for Q\u03b8, gradient of LTD(\u03b8) w.r.t. \u03b8, or the TD update vector \u2206TD, can be represented with an IS ratio w(s,a) := \u03c0(a | s) \u00b5(a | s) :\n\u2206TD = Ex\u223cp\u00b5 [ w ( s\u2032,a\u2032 ) ( R(s,a) + \u03b3Q\u03b8\u0304 ( s\u2032,a\u2032 ) \u2212Q\u03b8(s,a) ) \u2207\u03b8Q\u03b8(s,a)\ufe38 \ufe37\ufe37 \ufe38\n=:\u2206(x)\n] , (2)\nwhere a transition in the data is x := {s,a, r, s\u2032,a\u2032}, \u2206(x) is the semi-gradient of the transition x. By the importance ratio w, the data sampling distribution of p\u00b5 is corrected to the distribution of p\u03c0 := d \u00b5(s)\u00b5(a | s)P (s\u2032 | s,a)p(r| s,a)\u03c0(a\u2032 | s\u2032).\nIn-sample TD learning (Schlegel et al., 2019; Zhang et al., 2023) uses IR to estimate \u2206TD as \u2206\u0302IR:\n\u2206\u0302IR = w\u0304\nk k\u2211 j=1 \u2206(x\u0304j) , x\u0304j \u03c1\u223c {x1, . . . , xn} with probability \u03c1j =\nw ( s\u2032j ,a \u2032 j )\u2211n i=1 w (s \u2032 i,a \u2032 i) , (3)\nwhere w\u0304 := 1n \u2211n i=1 w(s \u2032 i,a \u2032 i) is the bias correction term that corrects the bias induced by resampling\nfrom a finite data D with size n."
        },
        {
            "heading": "3.2 KERNEL METRIC LEARNING",
            "text": "As the IS ratios used for in-sample learning (Schlegel et al., 2019; Zhang et al., 2023) are almost surely zeros for a deterministic policy, it cannot be directly applied to the evaluation of a deterministic target policy. In this work, we enable in-sample learning to evaluate a deterministic target policy by relaxing the density of the target policy in an IS ratio, which can be seen as a Dirac delta function \u03c0(a | s) = \u03b4(a\u2212\u03c0\u0303(s)), by a Gaussian kernel (Eq. (4)). We assume that the support of \u00b5(a | s) covers the support of 1\nhd K\n( L(s)\u22a4(a\u2212\u03c0\u0303(s))\nh\n) so the value of IS ratio is bounded. We seek to minimize the\nMSE between the estimated TD update vector and the true TD update vector \u2206TD in Eq. (2) by learning Mahalanobis metrics A(s)h2 in their scales (hereafter referred to as bandwidths) h and metrics\nin their shapes (hereafter referred to as metrics) A(s) (Mahalanobis, 1936; De Maesschalck et al., 2000), which is locally learned at each state s (Eq. (5)). The metrics are locally learned at each state s to reflect the Q-value landscape of the target policy near the target actions \u03c0\u0303(s).\nw(s,a) = \u03b4(a\u2212\u03c0\u0303(s)) \u00b5(a | s) \u2248\n1 hd\u00b5(a | s)K ( L(s)\u22a4 (a\u2212\u03c0\u0303(s)) h ) , (4)\n1\nhd K\n( L(s)\u22a4(a\u2212\u03c0\u0303(s))\nh\n) =\n1\nhd(2\u03c0) d 2\nexp ( \u2212 (a\u2212\u03c0\u0303(s))\n\u22a4A(s)(a\u2212\u03c0\u0303(s)) h2\n) , (5)\nwhere we assume that |A(s)| = 1, A(s) \u227b 0, A(s)\u22a4 = A(s). Applying Mahalanobis metric (A(s) = L(s)L(s)\u22a4) to a Gaussian kernels can be viewed as linearly transforming the inputs with the transformation matrix L(s) as in Eq. (5) (Noh et al., 2010; 2017)."
        },
        {
            "heading": "4 KERNEL METRIC LEARNING FOR IN-SAMPLE TD LEARNING",
            "text": "Our work enables the in-sample estimation of TD update vector \u2206\u0302KIR with kernel relaxation and metric learning for estimating Q\u03c0 which is used to evaluate a deterministic target policy. The insample estimation is enabled by relaxing the density of a deterministic target policy in an importance sampling (IS) ratio in Eq. (3) by a Gaussian kernel as in Eq. (4).\n\u2206\u0302KIR = w\u0304K\nk k\u2211 j=1 \u2206(x\u0304j) , x\u0304j \u03c1K\u223c x1, . . . , xn with probability \u03c1Kj = wK(s\u2032j ,a \u2032 j)\u2211n i=1 w K(s\u2032i,a \u2032 i) , (6)\nwhere wK(s\u2032i,a \u2032 i) := 1 hd\u00b5(a\u2032i | s\u2032i)\nK ( L(s\u2032i) \u22a4(a\u2032i \u2212\u03c0\u0303(s \u2032 i))\nh\n) , and the bias correction term with the kernel\nrelaxation is denoted as w\u0304K := 1n \u2211n i=1 w K(s\u2032i,a \u2032 i). The relaxation reduces variance but increases bias. Next, we analyze the bias and variance that together compose MSE of the proposed estimate and aim to find kernel bandwidths and metrics that best balance the bias and the variance to minimize the MSE of a TD update vector estimation."
        },
        {
            "heading": "4.1 MSE DERIVATION",
            "text": "To derive the MSE of the estimated TD update vector \u2206\u0302KIR while assuming an isotropic kernel (A(s\u2032) = I) is used, The bias and variance of \u2206\u0302KIR are derived.\nAssumption 1. Support of the behavior policy \u00b5(a | s) contains the actions determined by the target policy \u03c0\u0303(s) and the support of its kernel relaxation 1\nhd K\n( L(s)\u22a4(a\u2212\u03c0\u0303(s))\nh\n) .\nAssumption 2. The target Q network Q\u03b8\u0304(s,a) is twice differentiable w.r.t. an action a. Assumption 3. The datasize n is large enough to assume that bandwidth h\u226a 1. Theorem 1. Under Assumption 1-2, the bias and variance of \u2206\u0302KIR are:\nBias[\u2206\u0302KIR] = h 2 \u03b3\n2 ED\u223cp\u00b5\n[ \u22072a\u2032Q\u03b8\u0304(s \u2032,a\u2032)|a\u2032=\u03c0\u0303(s\u2032)\u2207\u03b8Q\u03b8(s,a) ]\n\ufe38 \ufe37\ufe37 \ufe38 =:b +O(h4), (7)\nVar[\u2206\u0302KIR] = 1\nnhd C(K)Ep\u00b5\n[ (r + \u03b3Q\u03b8\u0304 (s\n\u2032, \u03c0\u0303 (s\u2032))\u2212Q\u03b8(s,a))2 \u2225\u2207\u03b8Q\u03b8(s,a)\u222522 \u00b5 (\u03c0\u0303 (s\u2032) | s\u2032) ] \ufe38 \ufe37\ufe37 \ufe38\n=:v\n+O ( 1\nnhd\u22122\n) , (8)\nwhere \u22072a\u2032 is a Laplacian operator w.r.t. a\u2032, Var[z] := tr[Cov(z, z)] for a vector z, b is the bias constant vector, v is the variance constant, and C(K) := \u222b K(z)2dz. Under the Assumption 3, h2 b is the leading-order bias and v nhd is the leading-order variance.\nIn Theorem 1, as the bandwidth h increases, the leading-order bias increases, and the leading-order variance decreases as a bias-variance trade-off. For the leading-order variance, it increases when the L2 norm of the TD update vector (\u2206(x) in Eq. (8) with x = {s,a, r, s\u2032, \u03c0\u0303(s\u2032)}) is large, and this is understandable since the variance of a random variable (estimated TD update vector) would be increased if the scale of the random variable is increased. The leading-order bias increases when the\nsecond-order derivative of Q-function is large. This is reasonable since the estimation bias would increase when the difference between Q\u03b8\u0304(s \u2032,a\u2032) and Q\u03b8\u0304(s \u2032, \u03c0\u0303(s\u2032)) grows.\nThe bias and variance of the TD update vector estimation \u2206\u0302KIR are derived by applying the law of total expectation and variance since the samples used for estimating \u2206\u0302KIR are first sampled with p\u00b5 and then resampled with \u03c1K . Taylor expansion is also used for the derivation on \u00b5(a\u2032 | s\u2032) and target network Q\u03b8\u0304(s\n\u2032,a\u2032) at a\u2032 = \u03c0\u0303(s\u2032) and the odd terms of Taylor expansion are canceled out due to the symmetric property of K (proof in Appendix A.1).\nOur derivation of bias and variance differs from the kernel metric learning methods for OPE in contextual bandits (Kallus & Zhou, 2018; Lee et al., 2022) in that it is the bias and variance of the update vector of the OPE estimate rather than the bias and variance of the OPE estimate. The derivation also differs from the derivations in previous in-sample learning methods (Schlegel et al., 2019; Zhang et al., 2023) in that we derive the bias in terms of h, n, d to analyze the bias and variance w.r.t. these variables while the derivations in the previous methods are made to mainly compare bias and variance of IR estimate to the IS estimate.\nFrom the bias and variance in Theorem 1, MSE can be derived (proof in Appendix A.2).\nCorollary 1. Under Assumption 1-3, MSE between the TD update vector estimate \u2206\u0302KIR and the on-policy TD update vector \u2206TD is:\nMSE(h, n, k, d) = h4\u2225b \u222522 + v\nnhd\ufe38 \ufe37\ufe37 \ufe38 =:LOMSE(h,n,d)\n+O ( h6 ) +O\n( 1\nnhd\u22122\n) , (9)\nwhere LOMSE (h, n, d) is the leading-order MSE."
        },
        {
            "heading": "4.2 OPTIMAL BANDWIDTH AND METRIC",
            "text": "This section derives the optimal bandwidth and metric from the leading order MSE (LOMSE) in Eq. (9). The optimal bandwidth should minimize the LOMSE by balancing the bias-variance trade-off. Proposition 1. Under Assumption 1-3, the optimal bandwidth h\u2217 that minimizes the LOMSE(h, n, d) is:\nh\u2217 =\n( vd\n4n\u2225b \u222522\n) 1 d+4\n. (10)\nThe optimal bandwidth h\u2217 is derived by taking the derivative on LOMSE(h, n, d) w.r.t. the bandwidth h similarly to the work of Kallus & Zhou (2018) since LOMSE(h, n, d) is convex w.r.t. h when h > 0 (derivation in Appendix A). In Eq. (10), notice that the optimal bandwidth h\u2217 \u2192 0 as n\u2192\u221e and satisfies Assumption 3. Furthermore, the h\u2217 increases when the variance constant v is large, and the L2 norm of the bias constant vector b is small, and vice versa, to balance between bias and variance and minimize LOMSE (derivation in Appendix A.3).\nNext, we analyze how the LOMSE is affected by d when h\u2217 is applied to our algorithm.\nProposition 2. Under Assumption 1-2 and given optimal bandwidth h\u2217 (Eq. (10)), in highdimensional action space, the squared L2-norm of leading-order bias h4\u2225b \u222522 dominates over the leading-order variance v\nnhd in LOMSE (Eq. (9)). Furthermore, LOMSE(h\u2217, n,D) approximates\nto \u2225b \u222522.\nThe proof involves plugging in the optimal bandwidth h\u2217 in Eq. (10) to the LOMSE in Eq. (9) and taking the limit d \u2192 \u221e (derivation in Appendix A). Proposition 2 suggests that the MSE of the estimate \u2206\u0302KIR is dominated by the bias in high-dimensional action space when the optimal bandwidth h\u2217 is applied. Therefore, in a high-dimensional action space, a significant amount of MSE can be reduced by reducing the bias. We seek to further minimize the bias that dominates the MSE in high-dimensional action space with metric learning.\nSince applying the Mahalanobis metric A(s) = L(s)L(s)\u22a4 to the kernel is equivalent to linearly transforming the kernel inputs with the linear transformation matrix L(s) (Section 3.2) we analyze the effect of the metric on the LOMSE with the linearly transformed kernel inputs by replacing a\u2032 in\nEq. (7) with L(s\u2032)\u22a4 a\u2032. We first analyze the effect of the metric on the squared L2-norm of the bias constant vector \u2225bA\u222522:\n\u2225bA\u222522 \u2248 \u03b32\n4 \u2225ED\u223cp\u00b5\n[ tr [ A(s\u2032)\u22121 Ha\u2032 Q\u03b8\u0304(s \u2032,a\u2032)|a\u2032=\u03c0\u0303(s\u2032) ] \u2207\u03b8Q(s,a) ] \u222522, (11)\nwhere Ha\u2032 is the Hessian operator w.r.t. a\u2032. The derivation involves a change-of-variable on conditional distribution and derivatives (derivation in Appendix A.4). The approximation in Eq. (11) becomes equality when Q\u03b8\u0304 equals Q \u03c0 .\nMinimizing Eq. (11) is challenging as it necessitates a comprehensive examination of the collective impact exerted by every metric matrix A(s\u2032) inside the expectation. Therefore, an alternative approach is adopted wherein the focus shifts towards minimizing the subsequent upper bound U(A) in Eq. (12). This alternative strategy enables the derivation of a closed-form metric matrix for each state, employing a nonparametric methodology while being bandwidth-agnostic.\nmin A: A(s\u2032)\u227b0,\nA(s\u2032)=A(s\u2032)\u22a4, |A(s\u2032)|=1 \u2200s\u2032\nU(A) = \u03b32\n4 ED\u223cp\u00b5\n[ tr ( A(s\u2032)\u22121 Ha\u2032 Q\u03b8\u0304(s \u2032,a\u2032) \u2223\u2223 a\u2032=\u03c0\u0303(s\u2032) )2 \u2225\u2207\u03b8Q(s,a)\u222522 ] . (12)\nProposition 3. Under Assumption 2, define \u039b+(s\u2032) \u2208 Rd+(s \u2032)\u00d7d+(s\u2032) and \u039b\u2212(s\u2032) \u2208 Rd\u2212(s \u2032)\u00d7d\u2212(s\u2032) as diagonal matrices of positive and negative eigenvalues from the Hessian Ha\u2032Q\u03b8\u0304(s \u2032,a\u2032)|a\u2032=\u03c0\u0303(s\u2032), and define U+(s\u2032) and U\u2212(s\u2032) as the matrices of eigenvectors corresponding to \u039b+(s\u2032) and \u039b\u2212(s\u2032) respectively. Then the U(A) minimizing metric A\u2217(s\u2032) is:\nA\u2217(s\u2032) = \u03b1(s\u2032) [ U+(s \u2032)U\u2212(s \u2032) ]( d+(s\u2032)\u039b+(s\u2032) 0\n0 \u2212d\u2212(s\u2032)\u039b\u2212(s\u2032) ) \ufe38 \ufe37\ufe37 \ufe38\n=:M(s\u2032)\n[ U+(s \u2032)U\u2212(s \u2032) ]\u22a4 , (13)\nwhere \u03b1(s\u2032) := |M(s\u2032)|\u22121/(d+(s \u2032)+d\u2212(s \u2032)).\nThe proof entails solving Eq. (12) for A(s\u2032) for each s\u2032 (thus locally learn metrics for each s\u2032) by means of the Lagrangian equation. The closed-form solution to the minimization objective involving squared trace term of two matrix products in Eq. (12) is reported in the work of Noh et al. (2010) that learns kernel metric for kernel regression, and the solution had been used in the context of kernel metric learning for OPE in contextual bandits (Lee et al., 2022). We use the same solution for acquiring kernel metrics that minimize the squared trace term.\nNotice that when the Hessian Ha\u2032 Q\u03b8\u0304(s \u2032,a\u2032)|a\u2032=\u03c0\u0303(s\u2032) contains both positive and negative eigenvalues, the trace in Eq. (12) becomes zero. The optimal metric A\u2217(s\u2032) regards the next action a\u2032 in data is similar to the next target action \u03c0\u0303(s\u2032) when the eigenvalue of the Hessian in that direction is small, and decreases the Mahalanobis distance between a\u2032 and \u03c0\u0303(s\u2032). Consequently, as the Mahalanobis distance decreases, kernel value in the IS ratio wK(s\u2032,a\u2032) increases. Finally, the associated resampling probability increases, leading to a higher resampling probability.\nUtilizing the derived optimal bandwidth h\u2217 and optimal metric A\u2217, TD update vectors for \u03b8 can be estimated in an in-sample learning manner by using importance resampling (IR). The TD update vectors estimated by IR are then employed in the learning process of Q\u03b8. Finally, Q\u03b8 is used for evaluating the deterministic target policy \u03c0. The detailed procedure is in Algorithm 1 in Appendix B. Although we assumed a known behavior policy, our method can be applied to offline data that is sampled from unknown multiple behavior policies by estimating a behavior policy by maximum likelihood estimation. The work by Hanna et al. (2019) shows that the IS estimation of a target policy value is more accurate when an MLE behavior policy is used instead of the true behavior policy."
        },
        {
            "heading": "4.3 ERROR BOUND ANALYSIS",
            "text": "In this section, we analyze the error bound of the estimated Q-function with KMIFQE. The difference between the on-policy TD update vector \u2206TD and the KMIFQE estimated TD update vector \u2206\u0302KIR is that \u2206\u0302KIR is obtained from the TD loss in Eq. (1) with K instead of \u03c0. In other words, updating \u03b8 using \u2206\u0302KIR can be understood as evaluating the stochastic target policy relaxed by the bandwidth h and the matric A. We will show the evaluation gap resulting from using the relaxed stochastic policy instead of a deterministic policy. To this end, we first define Bellman operators T and TK for\nthe deterministic target policy \u03c0\u0303 and the stochastic target policy \u03c0K(a | s) = 1hdK ( L(s)\u22a4(a\u2212\u03c0\u0303(s)) h ) respectively.\nTQ(s,a) := R(s,a) + \u03b3Es\u2032\u223cP (s\u2032|s,a) [ Q ( s\u2032, \u03c0\u0303 ( s\u2032 ))] , (14)\nTKQ(s,a) := R(s,a) + \u03b3Es\u2032\u223cP (s\u2032|s,a),a\u2032\u223c\u03c0K(a\u2032 | s\u2032) [ Q ( s\u2032,a\u2032 )] . (15)\nWe then analyze the difference in fixed points for each Bellman operator.\nAssumption 4. For all {s,a} \u2208 S \u00d7 A, assume that the second-order derivatives of an arbitrary Q(s,a) w.r.t. actions are bounded. Theorem 2. Under Assumption 4, denoting m iterative applications of T and TK to an arbitrary Q as TmQ and TmK Q respectively, and h and A(s\n\u2032) as an arbitrary bandwidth and metric respectively, the following holds:\n\u2225Q\u03c0 \u2212 lim m\u2192\u221e\nTmK Q\u2225\u221e \u2264 \u03b3\u03be\n1\u2212 \u03b3 , (16)\n\u03be := max m,{s,a}\u2208S\u00d7A\nh2\n2 Es\u2032\u223cP (s\u2032 | s,a) [\u2223\u2223tr (A(s\u2032)\u22121 Ha\u2032 TmK Q(s\u2032,a\u2032)|a\u2032=\u03c0\u0303(s\u2032))\u2223\u2223]+ |O(h4)|. (17) The proof involves deriving \u03be that upperbounds \u2225TQ \u2212 TKQ\u2225\u221e/\u03b3 by using Taylor expansion at a\u2032 = \u03c0\u0303(s\u2032), and showing \u2225T\u221eQ \u2212 T\u221eK Q\u2225\u221e \u2264 \u2211\u221e i=1 \u03b3\ni\u03be using mathematical induction (proof in Appendix A.5).\nTheorem 2 shows that the evaluation gap is bounded by the degree of stochastic relaxation h and metric matrix A(s\u2032). It is noteworthy that for any given h, we can always reduce the gap by applying optimal metric A\u2217(s\u2032) in Proposition 3, which will also be shown empirically in the experiments (Figure 1b). While h = 0 seems to be always preferred in Theorem 2, it is because it only considers the exact policy evaluation without any estimation error by using finite samples. When we update Q using finite samples, the bias-variance trade-off by choice of h should be considered, as shown in Theorem 1."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we empirically validate the theoretical findings related to the learned metrics and bandwidths in Section 4.2 and compare the performance of KMIFQE with the baselines. The baselines include SR-DICE (Fujimoto et al., 2021) and FQE (Voloshin et al., 2019), which are state-of-the-art model-free OPE algorithms for evaluating deterministic target policies (Fujimoto et al., 2021) but vulnerable to extrapolation errors due to the usage of OOD samples. KMIFQE is evaluated on three test domains. First, we evaluate KMIFQE on OpenAI gym Pendulum-v0 environment (Brockman et al., 2016) with dummy action dimensions to see if the metrics and bandwidths are learned as intended. Then, we compare KMIFQE with the baselines on more complex MuJoCo environments (Todorov et al., 2012). Lastly, KMIFQE and baselines are evaluated on D4RL (Fu et al., 2020) datasets sampled from unknown multiple behavior policies since the baselines assume unknown multiple behavior policies. To apply KMIFQE, which assumes a known behavior policy, on the D4RL datasets, maximum likelihood estimated (MLE) behavior policies are used. For the hyperparameters in the baselines, we used the hyperparameters in the work of Fujimoto et al. (2021) as our test data is the same or similar to theirs. For KMIFQE, we used the same hyperparameter settings to the baselines for those overlap and IS ratio clipping in the range of [1e-3, 2] selected by grid search (see Appnedix C)."
        },
        {
            "heading": "5.1 PENDULUM WITH DUMMY ACTION DIMENSIONS",
            "text": "Pendulum-v0 environment (Brockman et al., 2016) is modified by adding dummy action dimensions that are unrelated to the estimation of target policy values for imposing large OPE estimation bias when an OPE algorithm fails to ignore the dummy action dimensions. The domain is prepared to observe if the KMIFQE metrics are learned to reduce the bias of \u2206\u0302KIR as intended in Proposition 3, leading to less error in the estimation of Q\u03c0 as stated in Theorem 2. For the dataset, actions in the dummy action dimensions are uniformly sampled from the original action range [\u2212amax, amax], and the original action is sampled from the mixture of 20% uniform random distribution and 80% Gaussian\ndensity of N(\u00b5\u03031(s), (0.5amax)2) where \u00b5\u03031 is a TD3 policy showing medium-level performance. The target policy \u03c0\u0303(s) outputs action of \u03c0\u03031(s) for the original action dimension and zeros for the dummy action dimensions. \u03c0\u03031(s) is a TD3 policy showing expert-level performance. The dataset contains 0.5 million transitions (more details in Appendix C).\nWe examine if our theoretical analysis made in Section 4.2 is supported by empirical results. Due to the unavailability of the ground truth TD update vectors, we analyze the impact of metric and bandwidth learning on the MSE of the estimated target policy values instead of evaluating its effect on the estimated TD update vector. We hypothesize that an estimation error in the TD update vector would lead to an estimation error in the target policy values. Firstly, Proposition 2 is empirically validated in Figure 1a. In the figure, empirical bias becomes much larger than the variance with KMIFQE learned bandwidth as the number of dummy action dimensions increases. Furthermore, the bias is reduced by the KMIFQE learned metrics (Proposition 3). Secondly, in Figure 1b, we show that the learned metrics reduce biases in all bandwidths as claimed in Proposition 3. Furthermore, the \"U\" shaped curve shows the biasvariance trade-off on varying bandwidths. Notably, the learned bandwidth balances between bias and variance as intended in Proposition 1. Finally, KMIFQE learned metrics and Q-value\nlandscapes are presented in Figure 2 showing that the metrics are learned to ignore the dummy action dimensions and the estimated Q-value landscape changes a little along the dummy action dimension compared to the one estimated by FQE. Moreover, the KMIFQE correctly estimates high Q-values for target actions that are optimal, while FQE estimates low Q-values for the actions.\nThe performance of KMIFQE with learned metric A(s\u2032) and bandwidth h, as well as baselines on the pendulum with one dummy action dimension are reported as root mean squared errors (RMSEs) in the first row of Table 1. KMIFQE outperforms the baselines that use OOD samples, and when metric learning is applied, the RMSEs are further reduced. The result supports the statement in Theorem 2 that the metric learning reduces the error bound."
        },
        {
            "heading": "5.2 CONTINUOUS CONTROL TASKS WITH A KNOWN BEHAVIOR POLICY",
            "text": "We compare the performance of KMIFQE with baselines on more complex continuous control tasks of MuJoCo (Todorov et al., 2012) with a known behavior policy. Deterministic target policies are trained by TD3 Fujimoto et al. (2018). For behavior policies, deterministic policies \u00b5\u0303 are trained with TD3 to achieve 70% \u223c 80% performance (in undiscounted returns) of the target policy and are used for making the stochastic behavior policies \u00b5(a | s) = N(a; \u00b5\u0303(s), (0.3amax)2I). One million transitions are collected with the behavior policy as the dataset for each environment.\nThe performance of OPE methods on five MuJoCo environments reported in Table 1 shows that KMIFQE outperforms other baselines in all environments except HalfCheetah-v2 where FQE performs the best. Since HalfCheetah-v2 does not have an episode termination condition, the Q\u03c0 that is being estimated may be relatively slowly changing w.r.t. changes in actions compared to the other environments, inducing low extrapolation error on the TD targets of FQE with OOD samples."
        },
        {
            "heading": "5.3 CONTINUOUS CONTROL TASKS WITH UNKNOWN MULTIPLE BEHAVIOR POLICIES",
            "text": "KMIFQE can be applied to a dataset sampled with unknown multiple behavior policies with a maximum likelihood estimated behavior policy. We evaluate KMIFQE on a D4RL (1) medium-expert dataset sampled from medium and expert level performance policies, and (2) medium-replay dataset collected from a single policy while it is being trained to achieve medium-level performance (Fu et al., 2020). The behavior policies used by KMIFQE are estimated as tanh-squashed mixture-of-Gaussians. For target policies, we use the means of stochastic expert policies provided by D4RL.\nThe last six rows of Table 1 show that KMIFQE outperforms the baselines except in halfcheetahmedium-expert-v2. The dataset may induce small extrapolation errors for the OPE methods that use OOD samples. First, half of the dataset is sampled with a stochastic expert policy, in which the mean value is the action selected by the target policy. Secondly, as discussed in Section 5.2, the Q\u03c0 being estimated in the halfcheetah environment may vary slowly w.r.t. changes in actions, making the extrapolation error due to using OOD samples small for FQE and SR-DICE."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We presented KMIFQE, an off-policy evaluation (OPE) algorithm for deterministic reinforcement learning (RL) policies in continuous action spaces. KMIFQE enables in-sample OPE of deterministic policies through kernel relaxation and metric learning. We first derived the MSE of a TD update vector for estimating the Q-function of a target policy. Then, to balance the bias and variance, we derived an optimal bandwidth. Upon our observation that the bias is dominant in the leading-order MSE when the action dimension is high, the bandwidth-agnostic optimal metric matrix for bias reduction was derived as a closed-form solution. The metric matrix is computed using the Hessian of the learned target Q-function. Lastly, we analyzed the error bound of the learned Q-function by KMIFQE estimated TD update vectors. Empirical studies show our KMIFQE outperforms baselines on the offline data sampled with known or unknown behavior policies, in which data distribution is quite different from the distribution induced by the target policy. As future work, exploring offline RL algorithms with actor-critic structures that utilize our policy evaluation method would be interesting."
        },
        {
            "heading": "A DERIVATIONS AND PROOFS",
            "text": ""
        },
        {
            "heading": "A.1 PROOF OF THEOREM 1",
            "text": "Theorem 1. Under Assumption 1-2, the bias and variance of \u2206\u0302KIR are:\nBias[\u2206\u0302KIR] = h 2 \u03b3\n2 ED\u223cp\u00b5\n[ \u22072a\u2032Q\u03b8\u0304(s \u2032,a\u2032)|a\u2032=\u03c0\u0303(s\u2032)\u2207\u03b8Q\u03b8(s,a) ]\n\ufe38 \ufe37\ufe37 \ufe38 =:b +O(h4), (18)\nVar[\u2206\u0302KIR] = 1\nnhd C(K)Ep\u00b5\n[ (R(s,a) + \u03b3Q\u03b8\u0304 (s\n\u2032, \u03c0\u0303 (s\u2032))\u2212Q\u03b8(s,a))2 \u2225\u2207\u03b8Q\u03b8(s,a)\u222522 \u00b5 (\u03c0\u0303 (s\u2032) | s\u2032) ] \ufe38 \ufe37\ufe37 \ufe38\n=:v\n+O ( 1\nnhd\u22122\n) ,\n(19)\nwhere Var[z] := tr[Cov(z, z)] for a vector z, b is the bias constant vector, v is the variance constant, h2 b is the leading-order bias, v\nnhd is the leading-order variance, and C(K) :=\n\u222b K(z)2dz.\nProof. With Assumptions 1-2,"
        },
        {
            "heading": "Derivation of the bias in Eq. (7).",
            "text": "Bias[\u2206\u0302KIR]\n= ED\u223cp\u00b5 [ w\u0304KEx\u0304\u223c\u03c1K [ 1\nk k\u2211 i=1 \u2206(x\u0304i) \u2223\u2223\u2223\u2223\u2223D ]] \u2212 Ex\u223cp\u03c0 [\u2206(x)] (20)\n= ED\u223cp\u00b5\n[ w\u0304K\nn\u2211 j=1 wK(s\u2032j ,a \u2032 j)\u2211n i=1 w K(s\u2032i,a \u2032 i) \u2206(x) ] \u2212 Ex\u223cp\u03c0 [\u2206(x)] (21)\n= ED\u223cp\u00b5\n[ 1\nn n\u2211 j=1 wK(s\u2032j ,a \u2032 j)\u2206(x) ] \u2212 Ex\u223cp\u03c0 [\u2206(x)] (22)\n= ED\u223cp\u00b5 [ wK(s\u2032,a\u2032)\u2206(x) ] \u2212 Ex\u223cp\u03c0 [\u2206(x)] (23)\n= E(s,a,r,s\u2032)\u223cp\u00b5 [\u222b 1\nhd K\n( a\u2032 \u2212\u03c0\u0303(s\u2032)\nh\n)( r + \u03b3Q\u03b8\u0304(s \u2032,a\u2032)\u2212Q\u03b8(s,a) ) \u2207\u03b8Q\u03b8(s,a)da\u2032 ] (24)\n\u2212 Ex\u223cp\u03c0 [\u2206(x)], = E(s,a,r,s\u2032)\u223cp\u00b5 [( r + \u03b3 { Q\u03b8\u0304(s \u2032, \u03c0\u0303(s\u2032)) + h2\n2\n\u222b K(z\u2032)(z\u2032)\u22a4 Ha\u2032 Q\u03b8\u0304(s \u2032,a\u2032)|a\u2032=\u03c0\u0303(s\u2032)z\u2032dz\u2032 } (25)\n\u2212Q\u03b8(s,a) ) \u2207\u03b8Q\u03b8(s,a) ] +O(h4)\u2212 Ex\u223cp\u03c0 [\u2206(x)]\n= E(s,a,r,s\u2032)\u223cp\u00b5 [( r + \u03b3 { Q\u03b8\u0304(s \u2032, \u03c0\u0303(s\u2032)) + h2\n2 tr\n[\u222b K(z\u2032)z\u2032(z\u2032)\u22a4dz\u2032 Ha\u2032 Q\u03b8\u0304(s \u2032,a\u2032)|a\u2032=\u03c0\u0303(s\u2032) ]} (26)\n\u2212Q\u03b8(s,a) ) \u2207\u03b8Q\u03b8(s,a) ] +O(h4)\u2212 Ex\u223cp\u03c0 [\u2206(x)]\n= \u03b3h2\n2 E(s,a,r,s\u2032)\u223cp\u00b5\n[ \u22072a\u2032Q\u03b8\u0304(s \u2032,a\u2032)|a\u2032=\u03c0\u0303(s\u2032)\u2207\u03b8Q\u03b8(s,a) ] +O(h4), (27)\nwhere \u22072a\u2032 and Ha\u2032 are the Laplacian and Hessian operator w.r.t. the next action a\u2032 respectively, equality on Eq. (25) is obtained by applying Taylor expansion in Eq. (24), and change-of-variable z\u2032 = a\n\u2032\u2212\u03c0\u0303(s\u2032) h . The Taylor expansion is applied on Q\u03b8\u0304(s \u2032,a\u2032) at a\u2032 = \u03c0\u0303(s\u2032). The odd terms of the Taylor expansion are canceled out due to the symmetric property of the Gaussian kernel. For the equality in Eq. (27), we used the property of a Gaussian kernel \u222b K(z\u2032)z\u2032 (z\u2032) \u22a4 dz\u2032 = I ."
        },
        {
            "heading": "Derivation of the variance in Eq. (8)",
            "text": "Var[\u2206\u0302KIR] = ED\u223cp\u00b5 [ Varx\u0304\u223c\u03c1K [\u2206\u0302 K IR | D] ] \ufe38 \ufe37\ufe37 \ufe38\n=:A\n+VarD\u223cp\u00b5 [ Ex\u0304\u223c\u03c1K [\u2206\u0302 K IR | D] ] \ufe38 \ufe37\ufe37 \ufe38\n=:B\n, (28)\nA = ED\u223cp\u00b5 [ (w\u0304K)2 Varx\u0304\u223c\u03c1K [ 1\nk k\u2211 j=1 \u2206(x\u0304j) | D\n]] (29)\n= ED\u223cp\u00b5\n[( w\u0304K )2\nk Varx\u0304\u223c\u03c1K [\u2206(x\u0304)|D]\n] (30)\n= ED\u223cp\u00b5\n[( w\u0304K )2\nk\n{ Ex\u0304\u223c\u03c1K [ \u2225\u2206(x\u0304)\u222522|D ] \u2212 \u2225Ex\u0304\u223cp\u00b5 [\u2206(x\u0304)|D] \u2225 2 2 }] (31)\n= 1\nn2k ED\u223cp\u00b5\n[( n\u2211\ni=1\nwK(s\u2032i,a \u2032 i) ) 2 n\u2211 j=1 ( wK(s\u2032j ,a \u2032 j) (((( (((\u2211n i=1 w K(s\u2032i,a \u2032 i) ) \u2225\u2206(xj)\u222522 (32)\n\u2212\n( n\u2211\ni=1\nwK(s\u2032i,a \u2032 i) )2\u2225\u2225\u2225 n\u2211 j=1 wK(s\u2032j ,a \u2032 j) ((( ((((\u2211n i=1 w K(s\u2032i,a \u2032 i) \u2206(xj) \u2225\u2225\u22252 2 ]\n= 1\nn2k ED\u223cp\u00b5\n\nn\u2211 j=1 (wK(s\u2032j ,a \u2032 j)) 2\u2225\u2206(xj)\u222522 + n,n\u2211 i \u0338=j wK(s\u2032i,a \u2032 i)w K(s\u2032j ,a \u2032 j)\u2225\u2206(xj)\u222522 (33)\n\u2212\n\nn\u2211 j=1 (wK(s\u2032j ,a \u2032 j)) 2\u2225\u2206(xj)\u222522 + n,n\u2211 i \u0338=j wK(s\u2032i,a \u2032 i)w K(s\u2032j ,a \u2032 j)\u2206(xi) \u22a4\u2206(xj)  =\nn2 \u2212 n n2k\n{\nEx\u223cp\u00b5\n[ wK(s\u2032,a\u2032) ] Ex\u223cp\u00b5 [ wK(s\u2032,a\u2032)\u2225\u2206(x)\u222522 ] (34)\n\u2212Ex\u223cp\u00b5 [w K(s\u2032,a\u2032)\u2206(x)]\u22a4Ex\u223cp\u00b5 [w K(s\u2032,a\u2032)\u2206(x)] }\n= n2 \u2212 n n2k\n{ Ex\u223cp\u00b5 [ wK(s\u2032,a\u2032)\u2225\u2206(x)\u222522 ] (35)\n\u2212Ex\u223cp\u00b5 [w K(s\u2032,a\u2032)\u2206(x)]\u22a4Ex\u223cp\u00b5 [w K(s\u2032,a\u2032)\u2206(x)] }\n= n2 \u2212 n n2k Var (s,a,r,s\u2032)\u223cp\u00b5,a\u2032\u223c 1\nhd K\n( a\u2032\u2212\u03c0\u0303(s\u2032)\nh ) [\u2206(x)] (36) = O ( 1\nk\n) , (37)\nB = VarD\u223cp\u00b5 [Ex\u0304\u223c\u03c1K [\u2206\u0302 K IR|D]] (38)\n= VarD\u223cp\u00b5 w\u0304KEx\u0304\u223c\u03c1K  1\nk k\u2211 j=1 \u2206(x\u0304j) \u2223\u2223\u2223\u2223\u2223D  (39)\n= VarD\u223cp\u00b5\n[ 1\nn\nn\u2211\ni=1\nwK(s\u2032i,a \u2032 i)\n[ n\u2211\nj=1\nwK(s\u2032j ,a \u2032 j) ((( ((((\u2211n i=1 w K(s\u2032i,a \u2032 i) \u2206(xj)\n]] (40)\n= 1\nn Varx\u223cp\u00b5\n[ wK(s\u2032,a\u2032)\u2206(x) ] (41)\n= 1\nn Ex\u223cp\u00b5 [(wK(s\u2032,a\u2032))2\u2225\u2206(x)\u222522]\ufe38 \ufe37\ufe37 \ufe38 =:X \u2212\u2225Ex\u223cp\u00b5 [w K(s\u2032,a\u2032)\u2206(x)]\u222522\ufe38 \ufe37\ufe37 \ufe38\n=\u2225Bias[\u2206\u0302IR]+\u2206TD\u222522=O(1)  (42) = X\nn +O\n( 1\nn\n) , (43)\nX = E(s,a,r,s\u2032)\u223cp\u00b5\n[\u222b 1\nh2d K\n( a\u2032 \u2212 \u03c0\u0303(s\u2032)\nh\n)2 (R(s,a) + \u03b3Q\u03b8\u0304(s \u2032,a\u2032)\u2212Q\u03b8(s,a))2\n\u00b5(a\u2032|s\u2032)\ufe38 \ufe37\ufe37 \ufe38 =:g(s,a,r,s\u2032,a\u2032)\n\u2225\u2207\u03b8Q\u03b8(s,a)\u222522da\u2032 ]\n(44)\n= E(s,a,r,s\u2032)\u223cp\u00b5\n[\u222b K(z\u2032)2\nh 2d g(s,a, r, s\u2032, hz\u2032 + \u03c0(s\u2032))\u2225\u2207\u03b8Q\u03b8(s,a)\u222522 hddz\u2032\n] (45)\n= C(K)\nhD E(s,a,r,s\u2032)\u223cp\u00b5\n[ g(s,a, r, s\u2032, \u03c0(s\u2032))\u2225\u2207\u03b8Q\u03b8(s,a)\u222522 ] +O ( 1\nhd\u22122\n) , (46)\nwhere C(K) := \u222b K(z)2dz, the equality in Eq. (45) is made by applying change-of-variable z\u2032 = a \u2032 \u2212\u03c0\u0303(s\u2032)\nh in Eq. (44), and the equality in Eq. (46) is made by applying Taylor expansion on g(s,a, r, s\u2032,a\u2032) at a\u2032 = \u03c0\u0303(s\u2032) in Eq. (45).\nBy plugging in Eq. (46) to Eq. (43),\nB = C(K)\nnhd E(s,a,r,s\u2032)\u223cp\u00b5\n[ (R(s,a) + \u03b3Q\u03b8\u0304(s\n\u2032, \u03c0(s\u2032))\u2212Q\u03b8(s,a))2\u2225\u2207\u03b8Q\u03b8(s,a)\u222522 \u00b5(\u03c0\u0303(s\u2032)| s\u2032)\n] +O ( 1\nnhd\u22122\n) .\n(47)\nBy substituting Eq. (37) and Eq. (47) into Eq. (28), and since the mini-batch size k is a constant multiple of n,\n\u2234 Var[\u2206\u0302KIR] = C(K)\nnhd Ep\u00b5\n[ (R(s,a) + \u03b3Q\u03b8\u0304(s\n\u2032, \u03c0\u0303(s\u2032))\u2212Q\u03b8(s,a))2\u2225\u2207\u03b8Q\u03b8(s,a)\u222522 \u00b5(\u03c0\u0303(s\u2032)| s\u2032)\n] +O ( 1\nnhd\u22122 ) (48)"
        },
        {
            "heading": "A.2 PROOF OF COROLLOARY 1",
            "text": "Corollary 1. Under Assumption 1-2, MSE between the TD update vector estimate \u2206\u0302KIR and the on-policy TD update vector \u2206TD is:\nMSE(h, n, k, d) = h4\u2225b \u222522 + v\nnhd\ufe38 \ufe37\ufe37 \ufe38 =:LOMSE(h,n,d)\n+O ( h6 ) +O\n( 1\nnhd\u22122\n) , (49)\nwhere LOMSE (h, n, d) is the leading-order MSE."
        },
        {
            "heading": "Proof.",
            "text": "MSE(h, n, k, d) = \u2225\u2225\u2225Bias [\u2206\u0302KIR]\u2225\u2225\u22252\n2 +Var\n[ \u2206\u0302KIR ] (50)\n= h4\u2225b \u222522 + v\nnhd\ufe38 \ufe37\ufe37 \ufe38 =:LOMSE(h,n,d)\n+O ( h6 ) +O\n( 1\nnhd\u22122\n) . (51)"
        },
        {
            "heading": "A.3 PROOF OF PROPOSITION 1",
            "text": "Proposition 1. Under Assumption 1-2, the optimal bandwidth h\u2217 that minimizes the LOMSE(h, n, d) is:\nh\u2217 =\n( vd\n4n\u2225b \u222522\n) 1 d+4\n. (52)\nProof. The LOMSE(h, n, d) (Eq. (9)) minimizing optimal bandwidth h\u2217 is:\nd\ndh (LOMSE (h, n, d)) = 4h3\u2225b \u222522 \u2212\nvd\nnhd+1 , (53)\n\u2234 h\u2217 =\n( vd\n4n\u2225b \u222522\n) 1 d+4\n. (54)"
        },
        {
            "heading": "A.4 PROOF OF PROPOSITION 2",
            "text": "Proposition 2. Under Assumption 1-2 and given optimal bandwidth h\u2217 (Eq. (10)), in highdimensional action space, the squared L2-norm of leading-order bias h4\u2225b \u222522 dominates over the leading-order variance v\nnhd in LOMSE (Eq. (9)). Furthermore, LOMSE(h\u2217, n, d) approximates\nto \u2225b \u222522.\nProof. LOMSE(h, n, d) in Eq. (9) is composed of leading-order bias LoBias(h) and leading-order variance LoVar(h, n, d):\nLoBias(h) : = h2 b, (55)\nLoVar(h, n, d) := v\nnhd . (56)\nThe ratio of \u2225LoBias(h\u2217)\u222522 and LoVar(h\u2217, n, d) with h\u2217 in Eq. (10) that together compose LOMSE(h\u2217, n, d) as d\u2192\u221e is:\nlim d\u2192\u221e \u2225LoBias(h\u2217)\u222522 LoVar(h\u2217, n, d) = lim d\u2192\u221e d 4 = \u221e. (57)\nFrom Eq. (57), it can be observed that the leading-order bias dominates over the leading-order variance in LOMSE when d\u226b 4. By plugging in h\u2217 (Eq. (10)) to the LOMSE in Eq. (9), and taking limit d\u2192\u221e, we derive similar relation between LOMSE and the bias as in the work of Noh et al. (2017) that learned metric for kernel regression:\nLOMSE(h\u2217, n, d) = n\u2212 4 d+4\n(( d\n4\n) 4 d+4\n+\n( 4\nd\n) d d+4 ) \u2225b \u2225 2d d+4 v 4 d+4 , (58)\n\u2234 lim d\u2192\u221e\nLOMSE(h\u2217, n, d) = \u2225b \u222522. (59)\nFrom Eq. (58) and Eq. (59), it can be observed that for a high-dimensional action space with d\u226b 4 and when the optimal bandwidth h\u2217 is used, the LOMSE can be approximated to \u2225b \u222522."
        },
        {
            "heading": "A.5 PROOF OF THEOREM 2",
            "text": "Theorem 2. Under Assumption 4, denoting m iterative applications of T and TK to an arbitrary Q as TmQ and TmK Q respectively, and h and A(s\n\u2032) as an arbitrary bandwidth and metric respectively, the following holds:\n\u2225Q\u03c0 \u2212 lim m\u2192\u221e\nTmK Q\u2225\u221e \u2264 \u03b3\u03be\n1\u2212 \u03b3 , (60)\n\u03be := max m,{s,a}\u2208S\u00d7A\nh2\n2 Es\u2032\u223cP (s\u2032 | s,a)\n[\u2223\u2223tr (A(s\u2032)\u22121 Ha\u2032 TmK Q(s\u2032,a\u2032)|a\u2032=\u03c0\u0303(s\u2032))\u2223\u2223]+ |O((h)4)|. (61)\nProof. Derive \u2225TQ\u2212 TKQ\u2225\u221e/\u03b3 w.r.t. h and A(s\u2032) by using Taylor expansion at a\u2032 = \u03c0\u0303(s\u2032):\n\u2225TQ\u2212 TKQ\u2225\u221e/\u03b3\n= max {s,a}\u2208S\u00d7A \u2223\u2223\u2223\u2223\u2223Es\u2032\u223cP (s\u2032 | s,a) [\u222b K ( L\u2217Q(s \u2032)\u22a4(a\u2032 \u2212\u03c0\u0303(s\u2032)) h\u2217Q ) Q(s\u2032,a\u2032)da\u2032 \u2212Q(s\u2032, \u03c0\u0303(s\u2032)) ]\u2223\u2223\u2223\u2223\u2223 = max\n{s,a}\u2208S\u00d7A \u2223\u2223\u2223\u2223Es\u2032\u223cP (s\u2032 | s,a) [\u222b K(z\u2032)( (h\u2217Q)22 (z\u2032)\u22a4L\u2217Q(s\u2032)\u22121Ha\u2032Q(s\u2032,a\u2032)|a\u2032=\u03c0\u0303(s\u2032)L\u2217Q(s\u2032)\u2212\u22a4 z\u2032 +O((h\u2217Q)4) ) d z\u2032 ]\u2223\u2223\n= max {s,a}\u2208S\u00d7A \u2223\u2223\u2223\u2223Es\u2032\u223cP (s\u2032 | s,a) [ (h\u2217Q)22 tr (A\u2217Q(s\u2032)\u22121Ha\u2032Q(s\u2032,a\u2032)|a\u2032=\u03c0\u0303(s\u2032))+O((h\u2217Q)4) ]\u2223\u2223\u2223\u2223\n\u2264 max {s,a}\u2208S\u00d7A\n(h\u2217Q) 2\n2 Es\u2032\u223cP (s\u2032 | s,a) [\u2223\u2223tr (A\u2217Q(s\u2032)\u22121Ha\u2032Q(s\u2032,a\u2032)|a\u2032=\u03c0\u0303(s\u2032))\u2223\u2223]+ |O((h\u2217Q)4)|\ufe38 \ufe37\ufe37 \ufe38 =:\u03be (62)\nwhere we used z\u2032 := L(s \u2032)\u22a4(a\u2032 \u2212\u03c0\u0303(s\u2032)) h , symmetricity of kernel K(z \u2032) = K(\u2212 z\u2032), and\u222b\nK(z\u2032) z\u2032 z\u2032 \u22a4 d z\u2032 = I .\nNext, we conjecture that \u2225TmQ\u2212 TmK Q\u2225\u221e \u2264 \u2211m i=1 \u03b3 i\u03be,\n(i) For m = 1, the conjecture is true by the definition of \u03be in Eq. (62).\n(ii) Assuming that the conjecture is true for an arbitrary m = j,\n\u2225T jQ\u2212 T jKQ\u2225\u221e \u2264 j\u2211\ni=1\n\u03b3i\u03be, (63)\n\u2225T j+1Q\u2212 T j+1K Q\u2225\u221e \u2264 \u2225(T \u2212 TK)T j KQ\u2225\u221e + \u2225T (T jQ\u2212 T jKQ)\u2225\u221e (64)\n\u2264 \u03b3\u03be + j\u2211\ni=1\n\u03b3i+1\u03be (65)\n= j+1\u2211 i=1 \u03b3i\u03be (66)\n(67)\n\u2234From (i) and (ii), the conjecture is proved to be true. Using the relation T\u221eQ = Q\u03c0 , Eq.(5) can be derived. Eq.(6) can be derived similarly."
        },
        {
            "heading": "A.6 DERIVATION OF THE BIAS WITH METRIC",
            "text": "From Eq. (7), the bias of our proposed estimator is:\nBias[\u2206\u0302KIR] = h 2 \u03b3\n2 E(s,a,r,s\u2032)\u223cp\u00b5\n[ tr [ Ha\u2032 Q\u03b8\u0304(s \u2032,a\u2032)|a\u2032=\u03c0\u0303(s\u2032) ] \u2207\u03b8Q(s,a) ] \ufe38 \ufe37\ufe37 \ufe38\n=:b\n+O(h4), (68)\nwhere b is the bias-related constant. Since applying a kernel metric is the same as linearly transforming the next action vector a\u2032 with the metric as mentioned in Section 3.2, we replace a\u2032 in Eq. (68) with u\u2032 = L(s\u2032)\u22a4 a\u2032 (where A(s\u2032) = L(s\u2032)L(s\u2032)\u22a4) to see the effect of a metric on the bias. The bias constant with the metric (bA) is:\nbA = \u03b3\n2 E(s,a,r,s\u2032)\u223cp\u00b5\n[ tr [ Hu\u2032Q\u03b8\u0304(s \u2032,u\u2032)|u\u2032=L(s\u2032)\u22a4\u03c0\u0303(s\u2032) ] \u2207\u03b8Q(s,a) ] . (69)\nQ\u03b8\u0304 can be understood as the approximation of Q \u03c0 ,\nQ\u03c0(s\u2032,a\u2032) \u2248 Q\u03b8\u0304(s \u2032,a\u2032) (70)\nQ\u03c0(s\u2032,a\u2032) = E\u03c0 [ \u221e\u2211 t=0 \u03b3trt | s0 = s\u2032,a0 = a\u2032 ]\n(71)\n= \u221e\u2211 t=0 \u03b3t \u222b rtp(rt | s0 = s\u2032,a0 = a\u2032)drt (72)\n= \u221e\u2211 t=0 \u03b3t \u222b rtp(rt | s0 = s\u2032,u0 = u\u2032)drt (73) = Q\u03c0U (s \u2032,u\u2032), (74)\nwhere Q\u03c0U is the Q-function defined in the linearly transformed action space U . The Q \u03c0 U is only used for derivation and the actual metrics are learned with Q\u03b8\u0304 defined in the original action space, as we will show in the followings. p(rt|s0,a0) = p(rt|s0,u0) since \u2223\u2223\u2223\u2202u0\u2202a0 \u2223\u2223\u2223 = |L(s0)| = 1. Using the invariant property in Eq. (74), Hu\u2032Q\u03c0U (s\n\u2032,u\u2032) can be presented w.r.t. a\u2032 and L(s\u2032) similar to the work of Lee et al. (2022):\nHu\u2032Q \u03c0(s\u2032,u\u2032) =\n\u22022\n\u2202 u\u2032 \u2202u\u2032\u22a4 Q\u03c0U (s \u2032,u\u2032) (75)\n= \u2202\n\u2202 u\u2032\n( \u2202\n\u2202 u\u2032 Q\u03c0U (s \u2032,u\u2032) )\u22a4 = \u2202\n\u2202 u\u2032\n( L(s)\u22121 \u2202\n\u2202 a\u2032 Q\u03c0(s\u2032,a\u2032) )\u22a4 = \u2202 a\u2032\n\u2202 u\u2032 \u2202 \u2202 a\u2032\n(( \u2202\n\u2202 a\u2032 Q\u03c0(s\u2032,a\u2032)\n)\u22a4 L(s)\u2212\u22a4 )\n= L(s)\u22121 \u22022\n\u2202 a\u2032 \u2202a\u22a4 Q\u03c0(s\u2032,a\u2032)L(s)\u2212\u22a4\n= L(s)\u22121 Ha\u2032 Q \u03c0(s\u2032,a\u2032) \u2223\u2223 a\u2032=\u03c0\u0303(s) L\u2212\u22a4(s). (76)\nUsing Eq. (70) and Eq. (76), bA in Eq. (69) can be approximated:\nbA \u2248 \u03b3\n2 E(s,a,r,s\u2032)\u223cp\u00b5\n[ tr [ A(s\u2032)\u22121 Ha\u2032 Q\u03b8\u0304(s \u2032,a\u2032)|a\u2032=\u03c0\u0303(s\u2032) ] \u2207\u03b8Q(s,a) ] . (77)\nFrom Eq. (77), Eq. (11) is obtained:\n\u2234 \u2225bA\u222522 \u2248 \u03b32\n4 \u2225ED\u223cp\u00b5\n[ tr [ A(s\u2032)\u22121 Ha\u2032 Q\u03b8\u0304(s \u2032,a\u2032)|a\u2032=\u03c0\u0303(s\u2032) ] \u2207\u03b8Q(s,a) ] \u222522,\nwhere the approximation in Eq. (11) becomes equality when Q\u03b8\u0304 = Q \u03c0 ."
        },
        {
            "heading": "B PSEUDO CODE FOR KMIFQE",
            "text": "Algorithm 1 Kernel Metric Learning for In-Sample Fitted Q Evaluation\nInput: Offline data D = {si,ai, ri, s\u2032i,a\u2032i}ni=1, stochastic behavior policy \u00b5, deterministic target policy \u03c0\u0303, Q-function Q\u03b8 with parameters \u03b8, target network Q\u03b8\u0304 parameterized by \u03b8\u0304, kernel K, initial linear transformation matrix L(si) = I for all i, learning rate \u03b1, number of TD update iterations N , mini-batch size k.\nOutput: Estimated target policy value V\u0302 (\u03c0).\n1: for until convergence do 2: for N steps do\n3: Update the bandwidth h = (\nvd 4n\u2225b \u222522\n) 1 d+4\n(Eq. (10)),\nwhere b := \u03b3\n2k k\u2211 i=1 \u22072a\u2032Q\u03b8\u0304(s\u2032i,a\u2032)|a\u2032=\u03c0\u0303(s\u2032i)\u2207\u03b8Q\u03b8(si,ai),\nv := C(K)\nk k\u2211 i=1\n[ (ri + \u03b3Q\u03b8\u0304 (s \u2032 i, \u03c0\u0303 (s \u2032 i))\u2212Q\u03b8(si,ai))\n2 \u2225\u2207\u03b8Q\u03b8(si,ai)\u222522 \u00b5 (\u03c0\u0303 (s\u2032i) | s\u2032i)\n] ,\nC(K) := \u222b K(z)2dz.\n4: Compute IS ratio for each sample i: wK(s\u2032i,a \u2032 i) := 1 hd\u00b5(a\u2032i | s\u2032i)\nK ( L(s\u2032i) \u22a4(a\u2032i \u2212\u03c0\u0303(s \u2032 i))\nh ) 5: Compute resampling probability for each sample: \u03c1Ki = wK(s\u2032i,a \u2032 i)\u2211n\nj=1 w K(s\u2032j ,a \u2032 j)\n6: Sample k transitions with \u03c1K : x\u0304j \u03c1K\u223c {x1, . . . , xn}, where xi := {si,ai, ri, s\u2032i,a\u2032i} 7: Compute the estimated expectation of the update vector \u2206\u0302KIR = w\u0304K k \u2211k j=1 \u2206(x\u0304j) in Eq. (6) 8: Update \u03b8\n\u03b8 \u2190 \u03b8 + \u03b1\u2206\u0302KIR 9: end for\n10: Update \u03b8\u0304\n\u03b8\u0304 \u2190 \u03b8\n11: Update linear transformation matrix L(s\u2032i) (A(s \u2032 i) = L(s \u2032 i)L(s \u2032 i) \u22a4)) for each sample\nL(si) = \u03b1(s \u2032 i) 1 2 [U+(s \u2032 i)U\u2212(s \u2032 i)]\n( d+(s \u2032 i)\u039b+(s \u2032 i) 0\n0 \u2212d\u2212(s\u2032i)\u039b\u2212(s\u2032i)\n) 1 2\n,\nwhere \u03b1(s\u2032i) := \u2223\u2223\u2223\u2223( d+(s\u2032i)\u039b+(s\u2032i) 00 \u2212d\u2212(s\u2032i)\u039b\u2212(s\u2032i) )\u2223\u2223\u2223\u2223\u22121/(d+(s\u2032i)+d\u2212(s\u2032i)) . 12: end for 13: Estimate V\u0302 (\u03c0) = (1\u2212 \u03b3) 1m \u2211m j=1 Q\u03b8(s0,j , \u03c0\u0303(s0,j)),\nwhere {s0,j}mj=1 is the set of initial states in D."
        },
        {
            "heading": "C EXPERIMENT DETAILS",
            "text": ""
        },
        {
            "heading": "C.1 PENDULUM WITH DUMMY ACTION DIMENSIONS",
            "text": "Environment The Pendulum-v0 environment in OpenAI Gym (Brockman et al., 2016) is modified to have d-dimensional actions. Among the d-dimensional actions, only the first dimension, which is the original action dimension in the Pendulum-v0 environment, is used for computing the next state transitions and rewards. The other (d - 1) action dimensions are unrelated to next state transitions and rewards. All of the dimensions have an action range equal to that of the original action space, which is [\u2212amax, amax], where amax = 2. The episode length is 200 steps. The discount factor of 0.95 is used.\nTarget policy We train a TD3 policy \u03c0\u03031 to be near-optimal on the original Pendulum-v0 environment which has only one action dimension and use it to make a target policy. The target policy \u03c0\u0303 is made with \u03c0\u03031(s):\n\u03c0\u0303(s) =  \u03c0\u03031(s) 0 ... 0  . The policy value of \u03c00 estimated from the rollout of 1500 episodes is \u22123.791. Since the dummy\naction dimensions do not have an effect on next state transitions and rewards, the policy values of \u03c0\u03031 and \u03c0\u0303 are the same.\nBehavior policy For the behavior policy, a TD3 policy \u00b5\u03031 is deliberately trained to be inferior to \u03c0\u03031 and is used to make a stochastic behavior policy. The behavior policy \u00b5 made with \u00b5\u03031 is as follows:\n\u00b5(a | s) = { 0.8N(a1|\u00b5\u03031(s), (0.5amax)2) + 0.2U(a1| \u2212 amax, amax) } d\u220f i=2 U(ai| \u2212 amax, amax), (78)\nwhere a uniform density with a range of [\u2212amax, amax] for ith action dimension is denoted as U(ai| \u2212 amax, amax). For the first action dimension, the mixture of 80% Gaussian density and 20% uniform density is used. For the other dimensions, actions are sampled from the uniform densities. The behavior policy is similar to the one used in the work of Fujimoto et al. (2021).\nThe policy values of \u00b5 and \u00b5\u03031 are estimated from the rollout of 1500 episodes and reported in Table 3. Since the dummy action dimensions are unrelated to next state transitions and rewards, the behavior policy value does not change as the number of dummy action dimensions changes.\nDataset Half a million transitions are sampled with the behavior policy \u00b5 for all experiments using the modified pendulum environment with dummy action dimensions.\nTarget policy value evaluation with OPE methods For KMIFQE and FQE, target policy values are estimated as V\u0302 (\u03c0) = (1\u2212\u03b3) 1m \u2211m i=1 Q\u03b8(s0,i, \u03c0\u0303(s0,i)), where m is the number of episodes in the dataset. For SR-DICE, the target policy values are estimated with 10k transitions randomly sampled from the data.\nNetwork architecture Our algorithm and FQE use the same architecture of a network of 2 hidden layers with 256 hidden units. For SR-DICE, we use the network architecture used in the work of Fujimoto et al. (2021). The encoder network of SR-DICE uses one hidden layer with 256 hidden units which outputs a feature vector of 256. The decoder network of SR-DICE uses one hidden layer with 256 hidden units for both next state and action decoders and uses a linear function of the feature vector for the reward decoder. The successor representation network of SR-DICE is composed of 2 hidden layers with 256 hidden units, and SR-DICE also uses 256 hidden units for the density ratio weights. We use the SR-DICE and FQE implementations in https://github.com/sfujim/SR-DICE.\nHyperparameters Following the hyperparameter settings used in the work (Fujimoto et al., 2021), all networks are trained with Adam optimizer (Kingma & Ba, 2014) with the learning rate of 3e\u2212 4. For mini-batch sizes, the encoder-decoder network, and successor representation network of SRDICE, as well as FQE, use a mini-batch size of 256. For the learning of density ratio in SR-DICE and our algorithm, we use a mini-batch size of 1024. FQE and SR-DICE use update rate \u03c4 = 0.005 for the soft update of the target critic network and target successor representation network respectively. For our proposed method, target critic network is hard updated every 1000 iterations. We clip the behavior policy density value at the target action by 1e-5 when the density value is below the clipping value for numerical stability when evaluating v defined in Eq. (8). The IS ratios are clipped to be in the range of [0.001, 2] to lower the variance of its estimations (Kallus & Zhou, 2018; Munos et al., 2016; Han & Sung, 2019). The clipping range is selected by grid search on Hopper-v2 domain from the cartesian product of minimum IS ratio clip values {1e-5, 1e-3, 1e-1} and maximum IS ratio clip values {1, 2, 10}.\nComputational resources used and KMIFQE train time One i7 CPU with one NVIDIA Titan Xp GPU runs KMIFQE for two million train steps in 5 hours."
        },
        {
            "heading": "C.2 CONTINUOUS CONTROL TASKS WITH A KNOWN BEHAVIOR POLICY",
            "text": "Environment MuJoCo environments (Todorov et al., 2012) of Hopper-v2 (d=3), HalfCheetah-v2 (d=6), and Walker2D-v2 (d=6), Ant-v2 (d=8), and Humanoid-v2 (d=17) are used. The maximum episode length of the environments is 1000 steps. HalfCheetah-v2 does not have a termination condition that terminates an episode before it reaches 1000 steps. But the other two environments may terminate before reaching the maximum episode length when the agent falls. The discount factor of 0.99 is used. The action range for all environments and for all action dimensions is in [\u2212amax, amax], where amax = 1 except Humanoid-v2 which is amax = 0.4.\nTarget policy We train a TD3 policy \u03c0 to be near-optimal and use it as a target policy. The target policy values estimated with the rollout of 1000 episodes are presented in Table 3.\nBehavior policy For the behavior policy of each environment of HalfCheetah-v2, Hopper-v2, and Walker2D-v2, a TD3 policies \u00b5\u0303 are deliberately trained to be inferior to a target policy, to achieve about 70%\u223c80% of the undiscounted return (R) of \u03c0. Then, \u00b5\u0303 is used to make the Gaussian behavior policy \u00b5(a | s) = N(a |\u00b5\u0303(s), (0.3amax)2I). The policy values of the behavior policy \u00b5 and \u00b5\u0303 for each environment are estimated with the rollout of 1000 episodes, and the policy values are reported in Table 3.\nTarget policy value evaluation with OPE methods For KMIFQE and FQE, target policy values are estimated as V\u0302 (\u03c0) = (1\u2212\u03b3) 1m \u2211m i=1 Q\u03b8(s0,i, \u03c0\u0303(s0,i)), where m is the number of episodes in the dataset. For SR-DICE, the target policy values are estimated with 10k transitions randomly sampled from the data.\nDataset One million transitions are sampled with the behavior policy \u00b5 for all experiments.\nTarget policy value evaluation with OPE methods For KMIFQE and FQE, target policy values are estimated as V\u0302 (\u03c0) = (1\u2212 \u03b3) 1m \u2211m i=1 Q(s0,i, \u03c0\u0303(s0,i)), where m is the number of episodes in the dataset. For SR-DICE, the target policy values are estimated with 10k transitions randomly sampled from the data.\nNetwork architecture The network architectures used for the MuJoCo experiment are the same as that of the architectures used in the experiments on pendulum environment with dummy action dimensions.\nHyperparameters The hyperparameters used for the MuJoCo experiments are the same as that of the experiments on the pendulum environment with dummy action dimensions except the mini-batch size used for the learning of density ratio in SR-DICE and our algorithm is 2048 as in the work of Fujimoto et al. (2021). For our proposed method, the IS ratios are dimension-wise clipped in the range of [0.001, 2] to lower the variance of its estimations (Han & Sung, 2019).\nComputational resources used and KMIFQE train time One i7 CPU with one NVIDIA Titan Xp GPU runs KMIFQE for one million train steps in 5 hours."
        },
        {
            "heading": "C.3 CONTINUOUS CONTROL TASKS WITH UNKNOWN MULTIPLE BEHAVIOR POLICIES",
            "text": "Dataset Among the D4RL dataset (Fu et al., 2020) we used halfcheetah-medium-expert-v2, hoppermedium-expert-v2, walker2d-medium-expert-v2, halfcheetah-medium-replay-v2, hopper-mediumreplay-v2, walker2d-medium-replay-v2. The discount factor of 0.99 is used. The action range for all environments and for all action dimensions is in [\u2212amax, amax], where amax = 1.\nTarget policy We use the mean values of the stochastic expert policies provided by D4RL dataset (Fu et al., 2020) as deterministic target policies.\nTarget policy value evaluation with OPE methods For KMIFQE and FQE, target policy values are estimated as V\u0302 (\u03c0) = (1\u2212\u03b3) 1m \u2211m i=1 Q\u03b8(s0,i, \u03c0\u0303(s0,i)), where m is the number of episodes in the dataset. For SR-DICE, the target policy values are estimated with 10k transitions randomly sampled from the data.\nMaximum likelihood estimation of behavior policies for KMIFQE Behavior policies are maximum likelihood estimated tanh-squashed mixture-of-Gaussian (MoG) models. The MoGs are squashed by tanh to restrict the support of the behavior policy to the action range of [\u2212amax, amax] for each action dimension. We fit tanh-squashed MoGs with mixture number of 10, 20, 30, 40 and select a mixture number by cross-validation for each dataset. The validation set is the 10% of the data, and rest of the data is used for training. The validation log-likelihood for each mixture number is reported in Table 4 with one standard error.\nNetwork architecture The network architectures used for the D4RL experiment are the same as that of the architectures used in the experiments on pendulum environment with dummy action dimensions.\nHyperparameters The hyperparameters used for the MuJoCo experiments are the same as those of the experiments on the MuJoCo domain (Todorov et al., 2012) with a known behavior policy except that since the dimension-wise clipping is not possible, we clip the overall IS ratios. Furthermore, we clip the behavior policy density value at the target action by 1e-3 when the density value is below the clipping value for numerical stability when evaluating v.\nComputational resources used and KMIFQE train time One i7 CPU with one NVIDIA Titan Xp GPU runs KMIFQE for one million train steps in 5 hours."
        }
    ],
    "year": 2023
}