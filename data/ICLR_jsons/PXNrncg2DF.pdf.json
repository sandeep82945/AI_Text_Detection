{
    "abstractText": "Open-world entity segmentation, as an emerging computer vision task, aims at segmenting entities in images without being restricted by pre-defined classes, offering impressive generalization capabilities on unseen images and concepts. Despite its promise, existing entity segmentation methods like Segment Anything Model (SAM) rely heavily on costly expert annotators. This work presents Self-supervised Open-world Hierarchical Entity Segmentation (SOHES), a novel approach that sidesteps the need for human annotations. SOHES operates in three phases: self-exploration, self-instruction, and self-correction. Given a pretrained self-supervised representation, we produce abundant high-quality pseudolabels through visual feature clustering. Then, we train a segmentation model on the pseudo-labels, and rectify the noises in pseudo-labels via a teacher-student mutual-learning procedure. Beyond segmenting entities, SOHES also captures their constituent parts, providing a hierarchical understanding of visual entities. Using raw images as the sole training data, our method achieves unprecedented performance in self-supervised open-world segmentation, marking a significant milestone towards high-quality open-world entity segmentation in the absence of human-annotated masks.",
    "authors": [],
    "id": "SP:8b1bb2fa66ee331383db40d190b20c44ee357937",
    "references": [
        {
            "authors": [
                "Ankan Bansal",
                "Karan Sikka",
                "Gaurav Sharma",
                "Rama Chellappa",
                "Ajay Divakaran"
            ],
            "title": "Zero-shot object detection",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Abhijit Bendale",
                "Terrance Boult"
            ],
            "title": "Towards open world recognition",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "Abhijit Bendale",
                "Terrance E Boult"
            ],
            "title": "Towards open set deep networks",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "George Papandreou",
                "Iasonas Kokkinos",
                "Kevin Murphy",
                "Alan L Yuille"
            ],
            "title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs",
            "year": 2017
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Zhe Chen",
                "Yuchen Duan",
                "Wenhai Wang",
                "Junjun He",
                "Tong Lu",
                "Jifeng Dai",
                "Yu Qiao"
            ],
            "title": "Vision transformer adapter for dense predictions",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Bowen Cheng",
                "Ishan Misra",
                "Alexander G Schwing",
                "Alexander Kirillov",
                "Rohit Girdhar"
            ],
            "title": "Maskedattention mask transformer for universal image segmentation",
            "year": 2022
        },
        {
            "authors": [
                "Ho Kei Cheng",
                "Jihoon Chung",
                "Yu-Wing Tai",
                "Chi-Keung Tang"
            ],
            "title": "CascadePSP: Toward classagnostic and very high-resolution segmentation via global and local refinement",
            "year": 2020
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "ImageNet: A large-scale hierarchical image database",
            "venue": "In CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Akshay Dhamija",
                "Manuel Gunther",
                "Jonathan Ventura",
                "Terrance Boult"
            ],
            "title": "The overlooked elephant of object detection: Open set",
            "venue": "In WACV,",
            "year": 2020
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Golnaz Ghiasi",
                "Yin Cui",
                "Aravind Srinivas",
                "Rui Qian",
                "Tsung-Yi Lin",
                "Ekin D Cubuk",
                "Quoc V Le",
                "Barret Zoph"
            ],
            "title": "Simple copy-paste is a strong data augmentation method for instance segmentation",
            "year": 2021
        },
        {
            "authors": [
                "Agrim Gupta",
                "Piotr Dollar",
                "Ross Girshick"
            ],
            "title": "LVIS: A dataset for large vocabulary instance segmentation",
            "year": 2019
        },
        {
            "authors": [
                "Ju He",
                "Shuo Yang",
                "Shaokang Yang",
                "Adam Kortylewski",
                "Xiaoding Yuan",
                "Jie-Neng Chen",
                "Shuai Liu",
                "Cheng Yang",
                "Qihang Yu",
                "Alan Yuille"
            ],
            "title": "PartImageNet: A large, high-quality dataset of parts",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Ronghang Hu",
                "Piotr Doll\u00e1r",
                "Kaiming He",
                "Trevor Darrell",
                "Ross Girshick"
            ],
            "title": "Learning to segment every thing",
            "year": 2018
        },
        {
            "authors": [
                "Ayush Jaiswal",
                "Yue Wu",
                "Pradeep Natarajan",
                "Premkumar Natarajan"
            ],
            "title": "Class-agnostic object detection",
            "venue": "In WACV,",
            "year": 2021
        },
        {
            "authors": [
                "KJ Joseph",
                "Salman Khan",
                "Fahad Shahbaz Khan",
                "Vineeth N Balasubramanian"
            ],
            "title": "Towards open world object detection",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Tarun Kalluri",
                "Weiyao Wang",
                "Heng Wang",
                "Manmohan Chandraker",
                "Lorenzo Torresani",
                "Du Tran"
            ],
            "title": "Open-world instance segmentation: Top-down learning with bottom-up supervision",
            "venue": "arXiv preprint arXiv:2303.05503,",
            "year": 2023
        },
        {
            "authors": [
                "Dahun Kim",
                "Tsung-Yi Lin",
                "Anelia Angelova",
                "In So Kweon",
                "Weicheng Kuo"
            ],
            "title": "Learning openworld object proposals without learning to classify",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Kirillov",
                "Eric Mintun",
                "Nikhila Ravi",
                "Hanzi Mao",
                "Chloe Rolland",
                "Laura Gustafson",
                "Tete Xiao",
                "Spencer Whitehead",
                "Alexander C. Berg",
                "Wan-Yen Lo",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Segment anything",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Philipp Kr\u00e4henb\u00fchl",
                "Vladlen Koltun"
            ],
            "title": "Efficient inference in fully connected CRFs with gaussian edge potentials",
            "venue": "In NeurIPS,",
            "year": 2011
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft COCO: Common objects in context",
            "venue": "In ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "In ICCV,",
            "year": 2017
        },
        {
            "authors": [
                "Yang Liu",
                "Idil Esen Zulfikar",
                "Jonathon Luiten",
                "Achal Dave",
                "Deva Ramanan",
                "Bastian Leibe",
                "Aljo\u0161a O\u0161ep",
                "Laura Leal-Taix\u00e9"
            ],
            "title": "Opening up open world tracking",
            "year": 2022
        },
        {
            "authors": [
                "Yen-Cheng Liu",
                "Chih-Yao Ma",
                "Zijian He",
                "Chia-Wen Kuo",
                "Kan Chen",
                "Peizhao Zhang",
                "Bichen Wu",
                "Zsolt Kira",
                "Peter Vajda"
            ],
            "title": "Unbiased teacher for semi-supervised object detection",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Long",
                "Evan Shelhamer",
                "Trevor Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "Lu Qi",
                "Jason Kuen",
                "Tiancheng Shen",
                "Jiuxiang Gu",
                "Weidong Guo",
                "Jiaya Jia",
                "Zhe Lin",
                "MingHsuan Yang"
            ],
            "title": "High-quality entity segmentation",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Vignesh Ramanathan",
                "Anmol Kalia",
                "Vladan Petrovic",
                "Yi Wen",
                "Baixue Zheng",
                "Baishan Guo",
                "Rui Wang",
                "Aaron Marquez",
                "Rama Kovvuri",
                "Abhishek Kadian",
                "Amir Mousavi",
                "Yiwen Song",
                "Abhimanyu Dubey",
                "Dhruv Mahajan"
            ],
            "title": "PACO: Parts and attributes of common objects",
            "year": 2023
        },
        {
            "authors": [
                "Walter J Scheirer",
                "Anderson de Rezende Rocha",
                "Archana Sapkota",
                "Terrance E Boult"
            ],
            "title": "Toward open set recognition",
            "venue": "TPAMI, 35(7):1757\u20131772,",
            "year": 2012
        },
        {
            "authors": [
                "Tiancheng Shen",
                "Yuechen Zhang",
                "Lu Qi",
                "Jason Kuen",
                "Xingyu Xie",
                "Jianlong Wu",
                "Zhe Lin",
                "Jiaya Jia"
            ],
            "title": "High quality segmentation for ultra high-resolution images",
            "year": 2022
        },
        {
            "authors": [
                "Jianbo Shi",
                "Jitendra Malik"
            ],
            "title": "Normalized cuts and image",
            "venue": "segmentation. TPAMI,",
            "year": 2000
        },
        {
            "authors": [
                "Oriane Sim\u00e9oni",
                "Gilles Puy",
                "Huy V Vo",
                "Simon Roburin",
                "Spyros Gidaris",
                "Andrei Bursuc",
                "Patrick P\u00e9rez",
                "Renaud Marlet",
                "Jean Ponce"
            ],
            "title": "Localizing objects with self-supervised transformers and no labels",
            "venue": "In BMVC,",
            "year": 2021
        },
        {
            "authors": [
                "Oriane Sim\u00e9oni",
                "Chlo\u00e9 Sekkat",
                "Gilles Puy",
                "Anton\u0131\u0301n Vobeck\u1ef3",
                "\u00c9loi Zablocki",
                "Patrick P\u00e9rez"
            ],
            "title": "Unsupervised object localization: Observing the background to discover objects",
            "year": 2023
        },
        {
            "authors": [
                "Antti Tarvainen",
                "Harri Valpola"
            ],
            "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Huy V Vo",
                "Francis Bach",
                "Minsu Cho",
                "Kai Han",
                "Yann LeCun",
                "Patrick P\u00e9rez",
                "Jean Ponce"
            ],
            "title": "Unsupervised image matching and object discovery as optimization",
            "year": 2019
        },
        {
            "authors": [
                "Huy V Vo",
                "Patrick P\u00e9rez",
                "Jean Ponce"
            ],
            "title": "Toward unsupervised, multi-object discovery in largescale image collections",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Van Huy Vo",
                "Elena Sizikova",
                "Cordelia Schmid",
                "Patrick P\u00e9rez",
                "Jean Ponce"
            ],
            "title": "Large-scale unsupervised object discovery",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Weiyao Wang",
                "Matt Feiszli",
                "Heng Wang",
                "Du Tran"
            ],
            "title": "Unidentified video objects: A benchmark for dense, open-world segmentation",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Weiyao Wang",
                "Matt Feiszli",
                "Heng Wang",
                "Jitendra Malik",
                "Du Tran"
            ],
            "title": "Open-world instance segmentation: Exploiting pseudo ground truth from learned pairwise affinity",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Xinlong Wang",
                "Zhiding Yu",
                "Shalini De Mello",
                "Jan Kautz",
                "Anima Anandkumar",
                "Chunhua Shen",
                "Jose M Alvarez"
            ],
            "title": "FreeSOLO: Learning to segment objects without annotations",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Xudong Wang",
                "Rohit Girdhar",
                "Stella X Yu",
                "Ishan Misra"
            ],
            "title": "Cut and learn for unsupervised object detection and instance segmentation",
            "year": 2023
        },
        {
            "authors": [
                "Yangtao Wang",
                "Xi Shen",
                "Yuan Yuan",
                "Yuming Du",
                "Maomao Li",
                "Shell Xu Hu",
                "James L Crowley",
                "Dominique Vaufreydaz"
            ],
            "title": "TokenCut: Segmenting objects in images and videos with self-supervised transformer and normalized cut",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Xingyu Xie",
                "Pan Zhou",
                "Huan Li",
                "Zhouchen Lin",
                "Shuicheng Yan"
            ],
            "title": "Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models",
            "venue": "arXiv preprint arXiv:2208.06677,",
            "year": 2022
        },
        {
            "authors": [
                "Bolei Zhou",
                "Hang Zhao",
                "Xavier Puig",
                "Sanja Fidler",
                "Adela Barriuso",
                "Antonio Torralba"
            ],
            "title": "Scene parsing through ADE20K dataset",
            "year": 2017
        },
        {
            "authors": [
                "S\u2032 \u00d7 S"
            ],
            "title": "clustered. In the next step, we use the off-the-shelf CascadePSP (Cheng et al., 2020) model to refine the masks. In the final step of hierarchy analysis, the coverage threshold is set to \u03b8cover% = 90%. Self-instruction. We learn a segmentation model composed of DINO (Caron et al., 2021) pretrained ViT-B/8, ViT-Adapter",
            "venue": "(Chen et al., 2022),",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Open-world entity segmentation (Qi et al., 2022; 2023) is an emerging vision task for localizing semantically coherent visual entities without the constraints of pre-defined classes. This task, in contrast to traditional segmentation (Long et al., 2015; Chen et al., 2017; He et al., 2017; Kirillov et al., 2019), aims at creating segmentation masks for visual entities inclusive of both \u201cthings\u201d (countable objects such as persons and cars) and \u201cstuff\u201d (amorphous regions such as sea and sky) (Kirillov et al., 2019; Qi et al., 2022), without regard for class labels. The inherent inclusivity and classagnostic nature enable open-world entity segmentation to perform strongly on unfamiliar entities\nfrom unseen image domains, a frequent real-world challenge in applications such as image editing and robotics. A prominent model for this task is Segment Anything Model (SAM) (Kirillov et al., 2023), which has garnered enthusiastic attention for its impressive performance in open-world segmentation. However, the efficacy of models like SAM depends on the avilability of extensively annotated datasets. To illustrate, SAM is trained on SA-1B (Kirillov et al., 2023), a vast dataset comprising 11 million images and a staggering amount of 1 billion segmentation masks. While automated segmentation plays a central role in building SA-1B, human expertise and manual labor are similarly important, where it takes 14 to 34 seconds to annotate a mask. Meanwhile, it is challenging for human annotators to produce segmentation masks at a consistent granularity, because there is no universally agreed definition of objects and parts. This reliance on intricately annotated datasets and considerable human effort raises a compelling question: Can we develop a high-quality openworld segmentation model using pure self-supervision? The prospect of learning from unlabeled raw images without the need for expert annotations is highly appealing.\nIn fact, self-supervised visual representation learning (Chen et al., 2020; He et al., 2020; Caron et al., 2021; He et al., 2022b) has already shown promise. Self-supervised models can effectively exploit useful training signals from purely unlabeled images, resulting in high-quality visual representations that are comparable with those achieved via supervised learning. However, mainstream self-supervised representation learning approaches typically learn holistic representations for whole images, without distinguishing individual entities nor understanding region-level structures. As a result, they cannot be directly used to achieve open-world entity segmentation. Our key insight to bridge this gap is that an intelligent model can not only learn representations from observations, but can also self-evolve to explore the open world, instruct and generalize itself, and continuously refine and correct its predictions in a self-supervised manner, and ultimately achieve open-world segmentation without extensive human supervision.\nFollowing this key insight, we propose Self-supervised Open-world Hierarchical Entity Segmentation (SOHES), a novel approach consisting of three phases \u2013 1) Self-exploration: Starting from a pre-trained self-supervised representation DINO (Caron et al., 2021), we generate initial pseudolabels to learn from. By clustering visual features based on similarity and locality, we can discern semantically coherent continuous regions that likely represent visually meaningful entities. 2) Self-instruction: Our initial pseudo-labels are constrained by the fixed visual representation. To refine the segmentation, we train a Mask2Former (Cheng et al., 2022) segmentation model on the initial pseudo-labels. Even though the initial pseudo-labels are noisy, learning a segmentation model on them can \u201caverage-out\u201d the noise and the model can predict masks which are more accurate. 3) Self-correction: Building upon the more accurate predictions generated by the model from self-instruction phase, we employ a teacher-student mutual-learning framework (Tarvainen & Valpola, 2017; Liu et al., 2020) to further reduce the early-stage noise and adapt the model for openworld segmentation. Throughout, we rely solely on the raw images, without any human annotations. Equally significantly, due to the compositional nature of things and stuff in natural scenes, our model learns not just to segment entities but also their constituent parts and finer subparts of these parts. During the self-exploration phase, we generate a hierarchical structure of each visual entity from individual parts to the whole. This hierarchical segmentation approach enriches our understanding of visual elements in an open-world context, ensuring a more comprehensive and versatile application.\nTo summarize, our key contributions include:\n\u2022 We propose Self-supervised Open-world Hierarchical Entity Segmentation (SOHES) to address the challenges of open-world segmentation. By gradually adapting self-supervised visual representations, we demonstrate the potential of high-quality open-world segmentation, by learning solely from unlabeled data. \u2022 We develop a method to generate over 100 segmentation masks per image as high-quality pseudolabels, solely by clustering self-supervised features. \u2022 We learn to segment entities and their constituent parts and perform hierarchical association between visual entities. This hierarchical segmentation approach provides a multi-granularity analysis of visual entities in complex scenes. \u2022 We achieve new state-of-the-art performance in self-supervised open-world segmentation, enhancing mask average recall (AR) on various datasets (e.g., improving AR on SA-1B from 17.0 to 33.3), using only 2% unlabeled image data as SAM."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Open-world visual recognition. Open-world recognition (Scheirer et al., 2012; Bendale & Boult, 2015; 2016) aims to classify and recognize visual concepts in an evolving environment where the model encounters unfamiliar objects, which challenges traditional models trained to recognize a fixed set of classes. The task has been extended from classification to detection (Bansal et al., 2018; Dhamija et al., 2020; Joseph et al., 2021; Jaiswal et al., 2021; Kim et al., 2022), segmentation (Hu et al., 2018; Wang et al., 2021; 2022a; Kalluri et al., 2023), and tracking (Liu et al., 2022). In particular, open-world entity segmentation (Qi et al., 2022; 2023) segments entities into semantically meaningful regions without regard for class labels. In this work, we further expand the scope to whole entities and their constituent parts.\nSelf-supervised object localization/discovery. Localizing objects from images in a self-supervised manner requires learning the concept of objects from visual data without any human annotations. Early explorations (Vo et al., 2019; 2020; 2021) formulate an optimization problem on a graph where the nodes are object proposals (e.g., by selective search), and the edges are constructed based on visual similarities. Following the observation that the segmentation of the most prominent object can emerge from DINO (Caron et al., 2021), Sime\u0301oni et al. (2021; 2023); Wang et al. (2022b) learn object detectors from saliency-based pseudo-labels. Meanwhile, Wang et al. (2022c; 2023) generate pseudo-labels by extending NormCut (Shi & Malik, 2000). However, both saliency-based and NormCut-based region proposals can only focus on the prominent objects in each image, and are constrained to detect one or few objects per image. In contrast, our clustering-based self-exploration can produce over 100 high-quality pseudo-labels per image, allowing our open-world segmentation model to recognize small, under-represented entities."
        },
        {
            "heading": "3 APPROACH",
            "text": "In this section, we first provide an overview of Self-supervised Open-world Hierarchical Entity Segmentation (SOHES) and then present its three learning phases in the following subsections. SOHES consists of three phases: self-exploration, self-instruction, and self-correction, as shown in Figure 2. 1) In Phase 1 self-exploration, we start from a pre-trained self-supervised representation DINO (Caron et al., 2021) with a ViT-B/8 (Dosovitskiy et al., 2020) architecture, and initiate our exploration on unlabeled raw images. Our strategy is based on agglomerative clustering, and organizes image patches into semantically consistent regions automatically. 2) With these pseudo-labels, we can begin Phase 2 self-instruction. We train a segmentation model composed by a DINO pre-trained ViT backbone (Caron et al., 2021), ViT-Adapter (Chen et al., 2022) (for generating multi-scale features from ViT), and Mask2Former (Cheng et al., 2022) (for the final mask prediction). Through self-instruction, our segmentation model can learn from common visual entities in different images and generalize better than the initial pseudo-labels produced by the frozen ViT backbone. 3) In the final Phase 3 self-correction, we exploit more self-supervision signals to lift the limit induced by noises in the initial pseudo-labels. Inspired by semi-supervised learning (Tarvainen & Valpola, 2017; Liu et al., 2020), we employ a teacher-student mutual-learning framework, allowing the student to learn from the improved pseudo-labels generated by the teacher."
        },
        {
            "heading": "3.1 SELF-EXPLORATION: GENERATE INITIAL PSEUDO-LABELS",
            "text": "In the self-exploration phase, we generate initial pseudo-labels with several steps delicately designed to include potential entities and their constituent parts of diverse categories. We take a globalto-local perspective to first create candidate regions at the global level, and then investigate local images to accurately discover small entities. In particular, we begin by clustering patch-level selfsupervised features to generate a pool of candidate regions, then refine and filter such candidates into initial pseudo-labeled masks, and finally analyze the hierarchical structure among them. Figure 3 depicts this process with visual examples.\nStep 1 is a global clustering procedure, which merges image patches into semantically meaningful regions. Given an unlabeled image with resolution S \u00d7 S, we use DINO ViT-B/8 to extract its visual features {f1, . . . , fS\n8 \u00d7 S 8 } corresponding to each 8 \u00d7 8 patch. Then, we merge these patches\nin a bottom-up, iterative manner. The initial seed regions are exactly these 8 \u00d7 8 patches. In each iteration, we find the pair of adjacent regions (i, j) with the highest cosine feature similarity (fi \u00b7 fj)/(\u2225fi\u22252 \u00b7 \u2225fj\u22252). These two regions i and j are merged into a new region k. The visual feature is computed as fk = fi + fj . After replacing regions i and j with the new merged region k, we continue with the next iteration.\nWe set a series of merging thresholds \u03b8merge,1 > \u00b7 \u00b7 \u00b7 > \u03b8merge,m as criterion for stopping the merging procedure. In general, the highest cosine feature similarity (among all unmerged region pairs) decreases as more regions are merged. When the highest cosine feature similarity goes below one threshold \u03b8merge,t (t \u2208 {1, . . . ,m}), we record the merging results that have been obtained so far. Consequently, we can generate m sets of regions, covering various granularity levels. We mix these sets into a pool of regions that may overlap with each other. Non-maximal suppression (NMS) is applied to remove duplicate regions. The thresholds {\u03b8merge,t}mt=1 can be determined based on the desired number of pseudo-labels per image (see Appendix C).\nStep 2 is local re-clustering. In the first step, we have generated a large pool of image regions that may correspond to valid entities. However, many of them are noisy and meaningless, especially the smaller ones. We take a global-to-local perspective to re-examine the regions that are smaller than \u03b8small% of the whole image. For each small candidate region, a local image around it is cropped from the full image, and the local image is then resized to S\u2032 \u00d7 S\u2032 before undergoing the same clustering procedure as in Step 1 to obtain subregions. We only consider the subregions that are strictly inside the candidate region. Both subregions (Step 2) and regions (Step 1) that have been gathered are used as our initial pseudo-labels. By \u201czooming in\u201d on the small candidate regions and repeat the clustering procedure at a finer scale, we can better remove noisy pseudo-labels and improve the quality of the remaining ones.\nIn Step 3, we leverage the off-the-shelf mask refinement model CascadePSP (Cheng et al., 2020) to further improve the mask quality of pseudo-labels. We compute the mask IoUs (intersection-overunion) between the pseudo-labels before and after undergoing the refinement step, and remove the ones that have poor IoUs because they are likely noisy samples.\nFinally, Step 4 focuses on identifying the hierarchical structure embedded within the set of pseudolabels, which is represented as a forest structure (i.e., set of trees) where the roots are whole entities, and their descendants are parts and subparts, etc. We test each pair of pseudo-labels i and j to determine their hierarchical relation: If 1) over \u03b8cover% pixels of pseudo-label i are also in pseudolabel j (meaning that i is covered by j), and 2) less than \u03b8cover% pixels of pseudo-label j are in pseudo-label i (meaning that j is larger than i), then pseudo-label j is an ancestor of i in the hierarchy forest. The smallest ancestor of i is the direct parent of i. By testing the pixel coverage between pseudo-labels, we can figure out the hierarchical structure of our pseudo-labels."
        },
        {
            "heading": "3.2 SELF-INSTRUCTION: LEARN FROM INITIAL PSEUDO-LABELS",
            "text": "In the self-instruction phase, we need to address two problems: 1) The initial pseudo-labels from the previous self-exploration phase contain noises. How to leverage self-supervised learning signals to \u201caverage-out\u201d the noises? 2) Existing segmentation models cannot predict the hierarchical relations among masks. How to learn the hierarchy forest from the previous phase?\nTo address the first problem, we train a segmentation model to learn and generalize from the initial pseudo-labels. Through this procedure, the segmentation model can observe valid entities from pseudo-labels which are more frequent than noises, and thus accurately segment unseen images. The model is composed of a ViT-based backbone and a Mask2Former (Cheng et al., 2022) segmentation head. In particular, the backbone is constructed by the same DINO (Caron et al., 2021) pre-trained ViT, and ViT-Adapter (Chen et al., 2022) for producing multi-scale visual feature maps. The ViT backbone is not fixed, and thus we can adapt its features for the segmentation task.\nTo accomplish the hierarchical segmentation task, we attach a novel ancestor prediction head to Mask2Former, which predicts the hierarchical relations among the predicted masks. In parallel to the existing mask/class prediction heads, our ancestor prediction head operates on the query features Q \u2208 RN\u00d7C , where N is the number of queries and C is the query feature dimension. As shown in Figure 4, the learning target of the ancestor prediction is a non-symmetric binary matrix representing the ancestor relations P \u2208 {0, 1}N\u00d7N , where Pi,j = 1 represents that mask i is an ancestor of mask j, and Pi,j = 0 otherwise. It is worth noting that a mask i may have no ancestors (as a root in the hierarchy forest), if mask i is a whole entity; mask i may also have more than one ancestors (as a deep node in the forest), if mask i is a part of another part. The ancestor prediction is formulated as:\nP\u0302 = sigmoid ( (QW1)(QW2) \u22a4/ \u221a C ) \u2208 RN\u00d7N , (1)\nwhere W1,W2 \u2208 RC\u00d7C are learnable weights for two linear transformations. We use two different linear mappings since the ancestor relations are non-symmetric. They are optimized via a binary cross-entropy (BCE) loss Lancestor = BCE(P\u0302 , P ). At inference time, we can employ topological sorting to reconstruct the forest structure from the binary ancestor relation predictions."
        },
        {
            "heading": "3.3 SELF-CORRECTION: IMPROVE OVER INITIAL PSEUDO-LABELS",
            "text": "Although we have elaborately built a pseudo-labeling process in the self-exploration phase, it is still based on a fixed self-supervised visual representation that is not optimized for image segmentation. Consequently, there may still be initial pseudo-labels that are noisy and can negatively affect our model. Meanwhile, we observe that the segmentation model learned through the self-instruction phase can predict masks that are more reliable and accurate than the clustering results from the selfexploration phase. Motivated by this observation, we further bootstrap our model by learning from itself and mitigates the impact of noise in the initial pseudo-labels, in the final self-correction phase. To achieve self-correction, we adopt a semi-supervised approach that is based on teacher-student mutual-learning (Tarvainen & Valpola, 2017; Liu et al., 2020).\nThe self-correction phase starts off by initializing two separate segmentation models which are exact clones of the segmentation model produced by the self-instruction phase. We denote one segmentation model as the student modelM(\u00b7,\u0398student), which is actively updated through gradient descent; the other segmentation model is the teacher modelM(\u00b7,\u0398teacher), which is updated every iteration as an exponential moving average (EMA) of the student: \u0398teacher \u2190 m\u0398teacher+(1\u2212m)\u0398student, where m \u2208 (0, 1) is the momentum. The student receives supervision from both the initial pseudo-labels and the teacher\u2019s pseudo-labels. Thus, the total loss is computed as:\nLtotal = Lseg(M (Tstrong(I1),\u0398student), Yinitial) + Lseg(M (Tstrong(I2),\u0398student), Yteacher) , (2) where I1 and I2 are two image batches, Yinitial is the initial pseudo-labels on I1, Yteacher is the teacher\u2019s pseudo-labels by thresholding the predictionsM(Tweak(I2),\u0398teacher), Tstrong and Tweak denote strong and weak data augmentations respectively, and Lseg is the standard Mask2Former segmentation loss, composed of a classification loss Lcls, a mask loss Lmask, and our ancestor prediction loss Lancestor. Figure 5 illustrate the steps in our teacher-student learning approach.\nTo get more reliable supervision from teacher\u2019s predictionsM(Tweak(I2),\u0398teacher), we keep only the masks with confidence scores higher than \u03b8score to form the pseudo-labels Yteacher. We observe that the teacher model is less confident when segmenting smaller entities. If the threshold \u03b8score is fixed for all masks, the ratio of small pseudo-labels would be too low, and consequently, the student\u2019s small entity segmentation performance and overall performance would be impaired. Therefore, we leverage a dynamic threshold:\n\u03b8score = (1\u2212 (1\u2212 a)\u03b3) (\u03b8score, large \u2212 \u03b8score, small) + \u03b8score, small, (3) where a \u2208 (0, 1) is the area ratio between the predicted mask and the whole image, \u03b3 > 1 is a hyperparameter, and \u03b8score, small < \u03b8score, large are pre-set thresholds for the smallest mask and the largest mask respectively. With this dynamic threshold changing for masks of different scales, we can better balance the distribution in the teacher\u2019s pseudo-labels and encourage the student to accurately segment small entities."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we thoroughly evaluate SOHES on various datasets and examine the ViT-based backbone improvement for down-stream tasks. We perform a series of ablation study experiments to demonstrate the efficacy of modules and steps in SOHES. Additional qualitative results are shown in Appendix D. We also discuss limitations of SOHES in Appendix E."
        },
        {
            "heading": "4.1 TRAINING AND EVALUATION DATA",
            "text": "We train our SOHES model on the SA-1B (Kirillov et al., 2023) dataset. In SA-1B, there are 11 million images equally split into 1,000 packs. Unless otherwise specified, we use 20 packs of raw images (2%) for training, and 1 different pack (0.1%) for evaluation.\nFor evaluation purposes, we test SOHES on various image datasets with segmentation mask annotations in a zero-shot manner (i.e., no further training on evaluation datasets). The diversity in the evaluation datasets can simulate the challenge of unseen entity classes and image domains in an open-world setting. Since the annotations in each dataset may only cover entities from a pre-defined list of classes, the commonly used MS-COCO style average precision (AP) metric for closed-world detection/segmentation would incorrectly penalize open-world predictions that cannot be matched with ground truths in known classes. Following prior work (Kim et al., 2022; Wang et al., 2022a; Liu et al., 2022), we mainly consider the average recall (AR) metric for up to 1,000 predictions per image when comparing different methods. Other implementation details are in Appendix A."
        },
        {
            "heading": "4.2 OPEN-WORLD ENTITY SEGMENTATION",
            "text": "We evaluate SOHES on a variety of datasets, including MS-COCO (Lin et al., 2014), LVIS (Gupta et al., 2019), ADE20K (Zhou et al., 2017), EntitySeg (Qi et al., 2023), and SA-1B (Kirillov et al., 2023). These datasets include natural images of complex scenes, in which multiple visual entities of diverse classes present and are labeled with segmentation masks. Thus, the collection of such evaluation datasets can faithfully reflect the performance of an open-world segmentation model. We compare with recent self-supervised baselines FreeSOLO (Wang et al., 2022b) and CutLER (Wang et al., 2023). We also aim to close the gap between self-supervised methods and the supervised state-of-the-art model SAM (Kirillov et al., 2023). Notably, we use only 2% images as SAM for training SOHES, and we do not require any human annotations on these images.\nAs summarized in Table 1 and Table 4, SOHES consistently outperforms the prior state-of-the-art CutLER by large margins (e.g., +16.3 AR on SA-1B, +10.4 AR on EntitySeg). Meanwhile, SOHES significantly closes the gap between self-supervised methods and supervised methods. For instance, using only 2% unlabeled data in SA-1B, SOHES already achieves over half AR of SAM. SOHES also reduces the gap between self-supervised methods and SAM on SA-1B relatively by 37%."
        },
        {
            "heading": "4.3 PART SEGMENTATION",
            "text": "In additional to whole entities, SOHES also learns to segment their constituent parts and subparts. To evaluate our hierarchical segmentation results, we compare them with the ground-truth mask an-\nnotations of object parts (e.g., heads and tails of animals) in two datasets PartImageNet (He et al., 2022a) and PACO-LVIS (Ramanathan et al., 2023) and summarize the results in Table 1 and Table 5. Compared with prior self-supervised baselines, SOHES can more accurately localize meaningful parts of entities, and almost doubles CutLER\u2019s performance on PACO (8.9 AR\u2192 17.1 AR). Impressively, SOHES outperforms SAM on PartImageNet and is on par with SAM on PACO. The reason is that SOHES can predict more parts and subparts than SAM (see Figure 13), which are the focus of the two datasets\u2019 annotations."
        },
        {
            "heading": "4.4 IMPROVED BACKBONE FEATURES",
            "text": "Through our self-instruction and correction phases, we adapt self-supervised representation DINO to an open-world segmentation model. Consequently, our fine-tuned visual backbone can better fit into other dense prediction tasks. To test such abilities, we compare a) the ViT-B/8 backbone pretrained by DINO and b) the backbone further tuned in SOHES, in down-stream tasks of semantic segmentation on ADE20K and object detection on MS-COCO. The down-stream fine-tuning is performed in a minimalistic style, mimicking the linear probing (Chen et al., 2020; He et al., 2020) in self-supervised representation learning. For semantic segmentation, we directly attach a linear classifier on the feature maps from ViT-Adapter; for object detection, we attach the simplest RetinaNet (Lin et al., 2017) detection head on the ViT-Adapter. We keep the ViT parameters frozen during the supervised fine-tuning. Table 6 summarizes the results, demonstrating that SOHES can adapt the ViT-based backbone to generate better features for dense prediction down-stream tasks."
        },
        {
            "heading": "4.5 ABLATION STUDY",
            "text": "In this subsection, we ablate the design choices in SOHES on SA-1B, and provide our insights for future research in self-supervised open-world segmentation.\nDINO backbone. In recent self-supervised object localization/discovery work (Sime\u0301oni et al., 2023; Wang et al., 2023), researchers prefer the ViT backbone pre-trained by DINO, in particular ViT-B with patch size 8. We have also observed that a ViT backbone with patch size 8 leads to better mask quality in SOHES. To investigate this, we use DINO to pre-train ViT-B backbones with varying patch size and input resolution configurations, with a shorter 100-epoch training schedule (DINO originally pre-trains on ImageNet (Deng et al., 2009) for 300 epochs). Then, we repeat our selfexploration phase with these backbones, and the resulting mask quality comparison is summarized in Figure 7 and Table 6. From this comparison, we can observe that the small patch size is positively correlated with the mask quality. When the patch size decreases from 32 to 8, the AR significantly improves. Meanwhile, the input resolution does not influence the mask quality as much. The small patch size may better support the ViT to capture pixel-aligned details for localizing entites, and thus is more suited in our self-supervised segmentation task. It is worth noting that we cannot further reduce the patch size due to computational constraints. Whenever the patch size is halved, ViT needs to process 4\u00d7 patches, and perform 16\u00d7 computation in self-attention. Therefore, the off-the-shelf DINO ViT-B/8 is the best choice in our task.\nSteps in self-exploration. In our self-exploration phase, we have delicately designed a series of steps to generate, select, and refine the pseudo-labels. We summarize the impact of the design choices in Table 2. In the first global clustering step, if we use one fixed merging threshold \u03b8merge, a larger \u03b8merge leads to more masks per image and better coverage of entities (increasing AR), and also\nintroduces noises (oscillating AP). We choose to mix the results together and remove duplicates, which provides the best AR and only slightly increases the number of masks compared with the largest \u03b8merge. In the second local re-clustering step, we significantly improve the recall for small entities, relatively by 168%. This step ensures that our model receives adequate supervision from small entities. We also improve the overall recall by 2.5 AR. Finally, in the third refinement step, we adopt CascadePSP (Cheng et al., 2020) because it can best boost the overall mask quality. The other two options DenseCRF (Kra\u0308henbu\u0308hl & Koltun, 2011) and CRM (Shen et al., 2022) are also viable, but they change the pseudo-labels more aggressively, leading to the removal of many potential entities. Overall, each step in our self-exploration phase contributes to the high-quality initial pseudo-labels for SOHES. Notably, we can parallelize the processing for each image and accelerate self-exploration with more compute nodes.\nSelf-correction. In our self-correction phase, we adopt a teacher-student mutual-learning framework from semi-supervised learning, to continuously improve the segmentation model by itself. However, as shown in Table 3, the initial attempt of the mutual-learning with a fixed confidence threshold leads to worse performance. In fact, the imbalanced distribution is reinforced during this procedure, as indicated by the decreased AR for small and medium entities and increased AR for larger entities. Therefore, we need a dynamic threshold that allows more small and medium pseudolabels from the teacher model, and balance the student\u2019s prediction for entities of different scales. With the dynamic threshold, we can improve ARS and ARM relatively by 7.5% and 4.5%, with an acceptable cost of 2.3% ARL. Consequently, the overall AR is improved by 0.7."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We present SOHES, a self-supervised approach towards open-world entity segmentation with hierarchical structures. Through three phases of self-evolution, a self-supervised learner is adapted to an open-world segmentation model. By recognizing and localizing entities and their constituent parts in an open world with superior mask quality, SOHES substantially closes the gap between self-supervised and supervised methods, and sets the new state of the art on various datasets."
        },
        {
            "heading": "B ADDITIONAL EVALUATION RESULTS",
            "text": "We provide additional evaluation results with more metrics in Table 4 and Table 5, as a supplement to Table 1 in the main paper."
        },
        {
            "heading": "C ADDITIONAL ABLATION STUDY RESULTS",
            "text": "Merging thresholds. In the first step of our self-exploration phase, we cluster patches into coherent regions and stop at pre-set merging thresholds \u03b8merge,1 > \u00b7 \u00b7 \u00b7 > \u03b8merge,m. We choose these thresholds based on a practical computation constraint: pseudo-labels per image. The average image resolution in SA-1B is about 1500\u00d72250. When there are too many pseudo-labels, data loading, augmentation, and pre-processing would become a bottleneck in model training. Therefore, we control the number of pseudo-labels per image to be lower than 200. As shown in Figure 8, the quantity of pseudolabels grows significantly when the merging threshold is larger. In order not to exceed 200 masks per image, we set the merging thresholds as 0.6, 0.5, 0.4, 0.3, 0.2, 0.1.\nDINO backbone. As a supplement to Figure 7 in the main paper, Table 6 summarizes the detailed statistics of the mask quality of initial pseudo-labels produced by DINO backboned pre-trained with different configurations. It also includes the mask quality after each self-exploration step."
        },
        {
            "heading": "D QUALITATIVE RESULTS",
            "text": "In Figures 9, 10, 11, 12, 13, we visualize the segmentation results of SOHES, and qualitatively compare SOHES with the supervised model SAM on the evaluation datasets we have used in the main paper."
        },
        {
            "heading": "E LIMITATIONS",
            "text": "In Figure 14, we visualize some failure cases of SOHES: 1) When there are discontinuous entities or occlusion (e.g., sky separated by foreground objects), SOHES may not correctly associate the disconnected segments of the same entity. The reason is that in the self-exploration phase, we only merge adjacent regions. The model rarely observes one entity separated in multiple disconnected\nregions. We observe that Copy-Paste data augmentation (Ghiasi et al., 2021) can simulate occlusion and mitigate this issue to some extent. 2) SOHES often produces imprecise segmentation masks for letters and characters. The boundary is not perfectly aligned with strokes and the mask is often larger than the letter. 3) When there are blurry backgrounds, SOHES tends not to predict a mask for the background. We aim to resolve these issues with improved pseudo-labeling strategies in future work."
        }
    ],
    "title": "SOHES: SELF-SUPERVISED OPEN-WORLD HIERARCHICAL ENTITY SEGMENTATION",
    "year": 2023
}