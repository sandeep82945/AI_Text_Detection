{
    "abstractText": "Mixture models arise in many regression problems, but most methods have seen limited adoption partly due to these algorithms\u2019 highly-tailored and model-specific nature. On the other hand, transformers are flexible, neural sequence models that present the intriguing possibility of providing general-purpose prediction methods, even in this mixture setting. In this work, we investigate the hypothesis that transformers can learn an optimal predictor for mixtures of regressions. We construct a generative process for a mixture of linear regressions for which the decision-theoretic optimal procedure is given by data-driven exponential weights on a finite set of parameters. We observe that transformers achieve low meansquared error on data generated via this process. By probing the transformer\u2019s output at inference time, we also show that transformers typically make predictions that are close to the optimal predictor. Our experiments also demonstrate that transformers can learn mixtures of regressions in a sample-efficient fashion and are somewhat robust to distribution shifts. We complement our experimental observations by proving constructively that the decision-theoretic optimal procedure is indeed implementable by a transformer.",
    "authors": [
        {
            "affiliations": [],
            "name": "Reese Pathak"
        }
    ],
    "id": "SP:d9baa0d2592c75dedd4f820a39f04422e5f7cdab",
    "references": [
        {
            "authors": [
                "Kabir Ahuja",
                "Madhur Panwar",
                "Navin Goyal"
            ],
            "title": "In-context learning through the bayesian prism",
            "venue": "arXiv preprint arXiv:2306.04891,",
            "year": 2023
        },
        {
            "authors": [
                "Ekin Aky\u00fcrek",
                "Dale Schuurmans",
                "Jacob Andreas",
                "Tengyu Ma",
                "Denny Zhou"
            ],
            "title": "What learning algorithm is in-context learning? investigations with linear models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Yu Bai",
                "Fan Chen",
                "Huan Wang",
                "Caiming Xiong",
                "Song Mei"
            ],
            "title": "Transformers as statisticians: Provable in-context learning with in-context algorithm selection",
            "venue": "arXiv preprint arXiv:2306.04637,",
            "year": 2023
        },
        {
            "authors": [
                "Christopher M. Bishop"
            ],
            "title": "Pattern recognition and machine learning",
            "venue": "Information Science and Statistics. Springer, New York,",
            "year": 2006
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Sitan Chen",
                "Jerry Li",
                "Zhao Song"
            ],
            "title": "Learning mixtures of linear regressions in subexponential time via Fourier moments",
            "venue": "In STOC. https://arxiv.org/pdf/1912.07629.pdf,",
            "year": 2020
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Lee H. Dicker"
            ],
            "title": "Ridge regression and asymptotic minimax estimation over spheres of growing",
            "venue": "dimension. Bernoulli,",
            "year": 2016
        },
        {
            "authors": [
                "Shivam Garg",
                "Dimitris Tsipras",
                "Percy S Liang",
                "Gregory Valiant"
            ],
            "title": "What can transformers learn in-context? a case study of simple function classes",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Kai Han",
                "Yunhe Wang",
                "Hanting Chen",
                "Xinghao Chen",
                "Jianyuan Guo",
                "Zhenhua Liu",
                "Yehui Tang",
                "An Xiao",
                "Chunjing Xu",
                "Yixing Xu"
            ],
            "title": "A survey on vision transformer",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "Bridging nonlinearities and stochastic regularizers with gaussian error linear units",
            "venue": "CoRR, abs/1606.08415,",
            "year": 2016
        },
        {
            "authors": [
                "Ayush Jain",
                "Rajat Sen",
                "Weihao Kong",
                "Abhimanyu Das",
                "Alon Orlitsky"
            ],
            "title": "Linear regression using heterogeneous data batches",
            "venue": "arXiv preprint arXiv:2309.01973,",
            "year": 2023
        },
        {
            "authors": [
                "Jon Kleinberg",
                "Mark Sandler"
            ],
            "title": "Using mixture models for collaborative filtering",
            "venue": "In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing,",
            "year": 2004
        },
        {
            "authors": [
                "Weihao Kong",
                "Raghav Somani",
                "Zhao Song",
                "Sham Kakade",
                "Sewoong Oh"
            ],
            "title": "Meta-learning for mixed linear regression",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Yingcong Li",
                "Muhammed Emrullah Ildiz",
                "Dimitris Papailiopoulos",
                "Samet Oymak"
            ],
            "title": "Transformers as algorithms: Generalization and stability in in-context learning",
            "year": 2023
        },
        {
            "authors": [
                "Yuanzhi Li",
                "Yingyu Liang"
            ],
            "title": "Learning mixtures of linear regressions with nearly optimal complexity",
            "venue": "In COLT. arXiv preprint arXiv:1802.07895,",
            "year": 2018
        },
        {
            "authors": [
                "Samuel M\u00fcller",
                "Noah Hollmann",
                "Sebastian Pineda Arango",
                "Josif Grabocka",
                "Frank Hutter"
            ],
            "title": "Transformers can do bayesian inference",
            "venue": "arXiv preprint arXiv:2112.10510,",
            "year": 2021
        },
        {
            "authors": [
                "Reese Pathak",
                "Martin J. Wainwright",
                "Lin Xiao"
            ],
            "title": "Noisy recovery from random linear observations: Sharp minimax rates under elliptical constraints, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Steinhardt",
                "Gregory Valiant",
                "Moses Charikar"
            ],
            "title": "Avoiding imposters and delinquents: Adversarial crowdsourcing and peer prediction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Kai Ming Ting",
                "Boon Toh Low",
                "Ian H Witten"
            ],
            "title": "Learning from batched data: Model combination versus data combination",
            "venue": "Knowledge and Information Systems,",
            "year": 1999
        },
        {
            "authors": [
                "Santosh Vempala",
                "Grant Wang"
            ],
            "title": "A spectral algorithm for learning mixture models",
            "venue": "Journal of Computer and System Sciences,",
            "year": 2004
        },
        {
            "authors": [
                "Johannes Von Oswald",
                "Eyvind Niklasson",
                "Ettore Randazzo",
                "Jo\u00e3o Sacramento",
                "Alexander Mordvintsev",
                "Andrey Zhmoginov",
                "Max Vladymyrov"
            ],
            "title": "Transformers learn in-context by gradient descent",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Jianyu Wang",
                "Zachary Charles",
                "Zheng Xu",
                "Gauri Joshi",
                "H Brendan McMahan",
                "Maruan Al-Shedivat",
                "Galen Andrew",
                "Salman Avestimehr",
                "Katharine Daly",
                "Deepesh Data"
            ],
            "title": "A field guide to federated optimization",
            "venue": "arXiv preprint arXiv:2107.06917,",
            "year": 2021
        },
        {
            "authors": [
                "Jun Wang",
                "Arjen P De Vries",
                "Marcel JT Reinders"
            ],
            "title": "Unifying user-based and item-based collaborative filtering approaches by similarity fusion",
            "venue": "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,",
            "year": 2006
        },
        {
            "authors": [
                "Xinyang Yi",
                "Constantine Caramanis",
                "Sujay Sanghavi"
            ],
            "title": "Alternating minimization for mixed linear regression",
            "venue": "In International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Xinyang Yi",
                "Constantine Caramanis",
                "Sujay Sanghavi"
            ],
            "title": "Solving a mixture of many random linear equations by tensor decomposition and alternating minimization",
            "venue": "arXiv preprint arXiv:1608.05749,",
            "year": 2016
        },
        {
            "authors": [
                "Ruiqi Zhang",
                "Spencer Frei",
                "Peter L Bartlett"
            ],
            "title": "Trained transformers learn linear models in-context",
            "venue": "arXiv preprint arXiv:2306.09927,",
            "year": 2023
        },
        {
            "authors": [
                "Kai Zhong",
                "Prateek Jain",
                "Inderjit S Dhillon"
            ],
            "title": "Mixed linear regression with multiple components",
            "venue": "In Advances in neural information processing systems (NIPS),",
            "year": 2016
        },
        {
            "authors": [
                "Chen et al",
                "Li",
                "Liang",
                "Zhong"
            ],
            "title": "problem with a batch size of 1 (Vempala",
            "year": 2004
        },
        {
            "authors": [
                "impractical. Kong"
            ],
            "title": "2020) pioneered the study of the problem in batch setting where they were motivated by meta-learning multiple tasks. They showed that they can recover well separated mixture of linear models from batched data with polynomial dependence on d,m and the inverse of the fraction of the smallest mixture component",
            "year": 2020
        },
        {
            "authors": [
                "Ahuja"
            ],
            "title": "in-context learning. Note that none of these prior works imply ability of transformers to learn mixture models from batch or non batch setting",
            "venue": "(Ahuja et al.,",
            "year": 2023
        },
        {
            "authors": [
                "Jain"
            ],
            "title": "Discussion on the construction: Note that in the operation aff used above, we do assume that the transformer stores the weights W\u2039. While at first glance this may seem like a strong assumption, we note that in our setting it is actually information-theoretically possible to learn the weights consistently as the number of prompts tends to infinity while keeping the prompt length size fixed.(For",
            "year": 2023
        },
        {
            "authors": [
                "\u0398J P Rr\u02c6|J",
                "\u0398K P R|K|\u02c6r. In Aky\u00fcrek"
            ],
            "title": "2023), they show that the RAW operator can be implemented in one autoregressive transformer layer.5 They also argue that (with a slight change in parameterization) that the mul and aff operators are transformer-implementable",
            "year": 2023
        },
        {
            "authors": [
                "Aky\u00fcrek"
            ],
            "title": "pHk,i, 1q, respectively, we can ensure that 5Note that in that paper, they more or less ignore the aggregate approximation error. Following this approach, we also ignore the lower order terms arising from the approximation of the underlying operations",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In several machine learning applications\u2014federated learning (Wang et al., 2021), crowdsourcing (Steinhardt et al., 2016) and recommendations systems (Wang et al., 2006)\u2014data is collected from multiple sources. Each source generally provides a small batch of data: for instance in recommendation systems, a user can provide a source of rating data on a subset of items that she has encountered. Such batches, on their own, are often too small to learn an effective model for the desired application. On the other hand, by pooling many batches together, improvements can typically be made in the quality of the predictors that can be learned.\nAn issue with this \u201cpooling\u201d approach is that if it is done carelessly, then the models which are learned may lack personalization (Ting et al., 1999). For instance, in a recommendation system, such an approach could yield a model that selects similar actions for dissimilar users. A better approach, however, is to model the problem as a mixture of distributions: for instance, we can model the sources as arising from m subpopulations, assuming that sources arising from the subpopulation have similar underlying distributions (Kleinberg & Sandler, 2004). The sources from a single subpopulation can then be aggregated for the purposes of learning. For instance, in the recommendation systems example, users in the same subpopulation might be identified as having similar preferences and tastes for item genres.\nA supervised learning formulation of the above setup is that the sources arise from a subpopulation indexed by an integer i P rms \u2013 t1, 2, \u00a8 \u00a8 \u00a8 ,mu. Additionally, assume that within each subpopulation the input-output pair px, yq follows a relation of the form y \u201c f\u2039i pxq ` \u03b7, where \u03b7 is a\nzero-mean noise, and x P Rd. A batch of i.i.d data from such a source can be represented as tpxj , yjqukj\u201c1 where k is the batch size. Given many such batches, each having examples only from one source, the task is to learn the functions tf\u2039i umi\u201c1 well enough to make good predictions on another input, sometimes also referred to as a query, xk`1. For instance, given the past ratings of an user, we should be able to determine their subpopulation well enough to infer their preferences on an unseen item.\nThe simplest version of the formulation above additionally imposes the assumption that the functions f\u2039i are linear: f \u2039 i pxq \u201c xw\u2039i , xiy. This setting has been studied theoretically in (Kong et al., 2020; Jain et al., 2023). Kong et al. (2020) introduced the problem and designed an algorithm in the setting where there are as many as Opdq batches with size k \u201c Op1q, and fewer medium-sized batched of size k \u201c Op ? mq. However, that work imposed strong assumptions on the covariate distribution, which lead to the paper (Jain et al., 2023), where these assumptions were relaxed. This latter work proposes a different algorithm that even allows covariate distributions to vary among subpopulations. Nonetheless, their algorithm needs to know problem parameters, such as a L2 \u00b4L4 hypercontractivity, a spectral norm bound on the covariance matrix, the noise level, and the number of subpopulations m. It is not clear how these algorithms will fare with model misspecification, or if they could be extended to applications like federated learning where it is unlikely that the correct model is linear, and distributed algorithms are required.\nIn this work, we ask the question: Is there a deep learning architecture that can be trained using standard gradient decent, yet learns mixture models from batched data and can leverage small batches from a source to make predictions for its appropriate subpopulation? If so, this would allow us to solve these type of mixture problem without needing highly specialized algorithms that could potentially be brittle with respect to knowing the correct form of the mixture model. Moreover, standard (stochastic) gradient descent would naturally extend to distributed training, using standard techniques from federated learning (Wang et al., 2006).\nA natural candidate to address this question is the widely used transformer architecture (Vaswani et al., 2017). Motivated by their immense success in NLP (Radford et al., 2019), computer vision (Han et al., 2022) and in context learning abilities demonstrated by large models (Chowdhery et al., 2022), several recent works have been aimed to analyze whether transformers can learn algorithms (Aky\u00fcrek et al., 2023; Garg et al., 2022; Von Oswald et al., 2023). These papers train decoder only transformers using prompts of the form px1, fpx1q, \u00a8 \u00a8 \u00a8 , xj , fpxjq, \u00a8 \u00a8 \u00a8xk, fpxkq, xk`1q where the task is to predict fpxk`1q after seeing the portion of the prompt preceding it. These papers show empirically that when f is sampled from a class of linear functions, then transformers learn to perform linear regression in-context. Aky\u00fcrek et al. (2023) also show that transformers can represent gradient descent in the case of linear regression provided through a construction.\nAnother important motivation for studying mixture models in the context of transformers is the question of data mixtures for pretraining large language models (LLM\u2019s). It is well known that for LLM\u2019s to succeed in various in-context learning tasks the pretraining dataset needs to be diverse, covering various languages, tasks, programming languages and concepts. This is evident from the complex datasheets for training popular LLMs (Chowdhery et al., 2022). A natural question then is to investigate whether transformers can zero in on relevant parts of the pretraining data given a new prompt and model select between algorithms (Bai et al., 2023). The mixture model studied in this paper can be seen as a simple linear or nonlinear analogue of the smae model."
        },
        {
            "heading": "Our contributions:",
            "text": "\u2022 We demonstrate that transformers can learn mixtures of linear regressions by training on such mixture data and exhibiting near-Bayes-optimal error at inference time.\n\u2022 We strengthen this observation by proving constructively that transformers can implement the optimal method for the mixture distribution on which the transformer was trained.\n\u2022 Our experiments show that transformers are sample-efficient: the transformers\u2019 performance is similar (or better) than model-specific methods, when fixing the training set size.\n\u2022 We evaluate certain inference-time metrics that capture the nearness of predictions made by the transformer versus another predictor. We show these metrics are smallest when taking the comparator to be the decision-theoretic optimal method, thereby further corroborating the hypothesis that transformers optimally learn mixtures of regressions.\n\u2022 We also show that the above empirical observations carry over to settings where f\u02dai \u2019s are non-linear specifically when they are polynomials or multi-layer perceptron (MLP)\u2019s with two hidden layers.\n\u2022 We suggest that transformers tolerate \u201csmall\u201d distribution shifts by investigating transformers\u2019 performance on both covariate and label shifts to the mixture model.\nThese contributions, taken together, are evidence that transformers can optimally, efficiently, and robustly learn mixtures of linear regressions.\nRelated work: The related work can be broadly divided into a thread that studies the theoretical properties of algorithms for estimation or prediction in a regression mixture model as well as another thread that studies the empirical and theoretical properties of transformers on learning regression models. Due to space considerations, we present a more detailed overview in Appendix A."
        },
        {
            "heading": "1.1 GENERATIVE MODEL FOR DATA",
            "text": "Throughout this paper we consider mixture of linear regression, except in Appendix E, where an extension to nonlinear models is considered. Underlying the mixture of linear regressions, we consider the discrete mixture\n\u03c0 \u2013 1\nm\nm \u00ff i\u201c1 \u03b4w\u2039i , (1)\nwhere tw\u2039i umi\u201c1 P Rd are normalized such that \u2225w\u2039i \u22252 \u201c ? d for each i P rms. We consider prompts or batches, denoted P \u201c px1, y1, . . . , xk, yk, xk`1q. Here, for noise level \u03c3 \u011b 0, we have\nw \u201e \u03c0, xi i.i.d.\u201e Np0, Idq, and yi | xi \u201e Npxw, xiy, \u03c32q.1 (2)\nThe goal is then to predict yk`1, the label for the query xk`1."
        },
        {
            "heading": "1.2 TRANSFORMERS",
            "text": "Transformers are deep neural networks that map sequences to sequences (Vaswani et al., 2017). In this work, we specifically focus on decoder-only, autoregressive transformers. These models are comprised of multiple layers that map an input matrix H P Rp\u02c6q to an output matrix H 1 P Rp\u02c6q . Here p denotes the hidden dimension, and q is corresponds to the number of input tokens. The output is then fed successively to more such layers. Since the computation in each layer is the same (apart from parameters), we describe the computation occurring in a single layer. Write hj \u201c pHijq1\u010fi\u010fp for the jth column of H , and h1j for the jth column of H\n1. Additionally, the prefix matrix H:i is the p \u02c6 pi \u00b4 1q submatrix of H obtained by concatenating the first i \u00b4 1 columns of H .2\nA layer is parameterized by a sequence of weights. Let nheads denote the number of attention heads and datt denote the hidden dimension for the attention layer and dff denote the hidden dimension for the feedforward (i.e., dense) layer. Then, a layer has the following weights:\ntWQi u nheads i\u201c1 , tW V i u nheads i\u201c1 , tW K i u nheads i\u201c1 ,\u0102 R datt\u02c6p,\ntWCi u nheads i\u201c1 \u0102 R\np\u02c6datt W in P Rdff\u02c6p, and W out P Rp\u02c6dff (3)\nfor each column i P rps, the computation proceeds in the following fashion.\nSelf-attention: The layer begins with computing the attention vector, ai P Rp, by\nsij \u2013 softmax \u00b4 ` WKj H:i \u02d8\u22ba WQj hi \u00af , for j P rnheadss, and,\nai \u2013 nheads \u00ff\nj\u201c1 WCj W V j H:isij\nAbove, with a slight abuse of notation, we define for any integer \u2113 \u0105 0, softmax: R\u2113 \u00d1 R\u2113 by the formula softmaxpvq \u201c pevt{\n\u0159\u2113 t1\u201c1 e vt1 q\u2113t\u201c1. Note that, above, sij P Ri\u00b41` .3\n1When \u03c3 \u201c 0, by Npv, 0q we mean the point mass \u03b4v . 2In the case i \u201c 1, the submatrix can be interpreted as 0. 3When i \u201c 1, sij \u201c 0.\nFeedforward network: The layer then continues by passing the attention vector (along with the original input column hi) through a nonlinear dense layer. This is defined by h1i \u2013 ai ` hi ` W out \u03c3\u201apW in \u03bbpai ` hiqq Above the notation \u03c3\u201a indicates that the map \u03c3 : R \u00d1 R is applied componentwise to its argument. In this work we take the nonlinearity to be the Gaussian error linear unit (GeLU) (Hendrycks & Gimpel, 2016) which is defined by\n\u03c3puq \u201c u 2 \u00b4 1 ` erf \u00b4 u? 2 \u00af\u00af , for any u P R.\nAbove, erf denotes the Gauss error function. The function \u03bb : Rp \u00d1 Rp denotes layer normalization (Ba et al., 2016), and is given by\n\u03bbpvq \u201c ?p v \u00b4 v1p \u2225v \u00b4 v1p\u22252 , where v \u201c 1 p\np \u00ff i\u201c1 vi.\nThis is a form of standardization where v is interpreted as the mean (averaging the components) and }v \u00b4 v1p}22{p is interpreted as the variance (averaging the squared deviation to the mean)."
        },
        {
            "heading": "2 REPRESENTATION",
            "text": "In this section, we prove that transformers can actually represent the minimum mean squared error procedure. Indeed, let f : P \u00de\u00d1 y\u0302 P R, by any procedure which takes a prompt P and outputs an estimate y\u0302 on the query, and define the mean squared error (MSE) by\nMSEpfq \u2013 EP \u201d pfpP q \u00b4 yk`1q2 \u0131 .\nThen by standard Bayesian decision theory, under the observational model described in Section 1.1, it follows that the mean squared error is minimized at the posterior mean f\u2039\u03c0 , which is given by\nf\u2039\u03c0pP q \u201c xw\u0302pP q, xk`1y where w\u0302pP q \u2013\n\u0159m j\u201c1 w \u2039 j exp\n\u00b4\n\u00b4 12\u03c32 \u0159k i\u201c1pxw\u2039j , xiy \u00b4 yiq2 \u00af\n\u0159m \u2113\u201c1 exp\n\u00b4\n\u00b4 12\u03c32 \u0159k i\u201c1pxw\u2039\u2113 , xiy \u00b4 yiq2 \u00af . (4)\nFormally, MSEpfq \u011b MSEpf\u2039\u03c0q, for all (measurable) f . Note above that w\u0302 does not depend on xk`1.\nThen our main result is that the function f\u2039\u03c0 can be computed by a transformer. Theorem 1. There is an autoregressive transformer which implements the function f\u2039\u03c0 in (4).\nSee Section B for a proof of this claim.\nFor an illustration of the underlying idea behind Theorem 1, see Figure 1 for an arithmetic circuit that computes the function f\u2039\u03c0 , in the case m \u201c 3, k \u201c 2. The objects rij are residuals, defined as rij \u201c xw\u2039j , xiy \u00b4 yi1ti \u2030 k ` 1u, for i P rks, j P rms. (5) The first layer computes the values triju, the second layer computes the squares of these values, the third layer computes the (scaled) sum of these values over the index i, which runs over the samples in the prompt, excluding the query. The fourth layer, computes the softmax of these sums,\npj \u2013 exp\n\u00b4\n\u00b4 12\u03c32 \u0159k i\u201c1pxw\u2039j , xiy \u00b4 yiq2 \u00af\n\u0159m \u2113\u201c1 exp\n\u00b4\n\u00b4 12\u03c32 \u0159k i\u201c1pxw\u2039\u2113 , xiy \u00b4 yiq2 \u00af , for j P rms (6)\nAnd the final layer computes m \u00ff\nj\u201c1 pjrpk`1q,j \u201c\nA m \u00ff\nj\u201c1 pjw\n\u2039 j , xk`1\nE\n\u201c xw\u0302pP q, xk`1y \u201c f\u2039\u03c0pP q\nwhere the last equation follows from the definitions in display (4). Therefore, the circuit depicted in Figure 1 is able to compute the posterior mean f\u2039\u03c0 , at least for the choices k \u201c 2,m \u201c 3. Generalizing the circuit to general pk,mq is straightforward; therefore, our proof amounts to exploiting the circuit and demonstrating that each operation: linear transforms in the first and final layers, squaring in the second layer, summation in the third layer, softmax in the fourth layers are all implementable by a transformer."
        },
        {
            "heading": "3 EXPERIMENTAL RESULTS",
            "text": "In this section, we present results of training transformers on batches as described in Section 1.1. Our methodology closely follows the training procedure described in (Garg et al., 2022). In the notation of Section 1.2, our transformer models set the hidden dimension as p \u201c 256, feedforward network dimension as dff \u201c 4p \u201c 1024, and the number of attention heads as nheads \u201c 8. Our models have 12 layers. Additional details on the training methodology can be found in Appendix C. We also release our training and simulation code along with this paper."
        },
        {
            "heading": "3.1 TRANSFORMERS CAN LEARN MIXTURES OF LINEAR REGRESSIONS",
            "text": "To begin with, we investigate the performance of transformers on mixture models with various numbers of components and varying noise levels. We plot the performance of the transformer when prompted with a prompt P of length k, for 1 \u010f k \u010f 60. The normalized MSE is the mean-squared error between the true labels and the estimated labels, divided by the dimension d \u201c 20. Above, the algorithms that we compare against are:4\n\u2022 Ordinary least squares (OLS). For a prompt of length k, computes an element w\u0302 P argminwPRd \u0159 j\u010fkpw \u22baxj \u00b4 yjq2. Outputs y\u0302k`1 \u201c w\u0302\u22baxk`1.\n\u2022 Posterior mean. This is an oracle algorithm. Given a prompt P of length k, computes the posterior mean y\u0302k`1 \u201c f\u2039\u03c0pP q, as defined in display (4).\n\u2022 Argmin. This is an oracle algorithm. Given a prompt P of length k, computes\nw\u0302 \u201c argminwPtw\u2039j umj\u201c1 \u00ff\nj\u010fk\n\u00ff j\u010fk pw\u22baxj \u00b4 yjq2. (7)\nThe prediction is then y\u0302k`1 \u201c w\u0302\u22baxk`1\nStrikingly, we see that the transformer prodictions are as good as\u2014or nearly as good as\u2014the oracle procedures which have knowledge of the true mixture components tw\u2039j u. It is important to note that\n4For interpretability of the figures, we omit the oracle algorithms above in the noiseless case (\u03c3 \u201c 0) as the error is multiple orders of magnitude smaller than the data-driven procedures.\nOLS is suboptimal in general for mixtures of linear regressions. Nonetheless, the transformer is performing much better than OLS, indicating the trained transformer implements a better predictor which is adapted to the mixtures of linear regressions setting.\nExtension to non-linear models: In Appendix E, we also present result for the extension to nonlinear mixtures of regressions. Qualitatively, we observe similar behavior where the trained transformer is competitive with oracle procedures which know the exact mixture model."
        },
        {
            "heading": "3.2 COMPARISON OF PERFORMANCE FOR FIXED TRAINING SET SIZE",
            "text": "Next, we investigate whether or not transformers learn mixtures of linear regressions in a sample efficient way. To do this, we depart slightly from the training methodology in (Garg et al., 2022). We first sample a fixed training set of size n P t15000, 30000, 45000, 60000u. Then\u2014with some hyperparameter tuning to avoid overfitting, as well as a modification to the curriculum training, described in Appendix C.1\u2014we train the transformer as in that paper. We then compare the inference time performance by computing the mean-squared error on prompts of length k P r1, 60s. The results of our simulation are shown below in Figure 3 We compared against two other procedures, which have the form of \u201cplug-in\u201d procedures:\n\u2022 Posterior mean, EM weights. Here, we first estimate the component means w\u2039j using batch expectation-maximiation (see Appendix C.2 and Algorithm 1 therein for details). Then, we form \u03c0\u0302, the uniform distribution over the estimated weights, and then predict yk`1 by f\u2039\u03c0\u0302pP q.\n\u2022 Posterior mean, SA weights. We follow the approach above, but estimate the weights by using the subspace algorithm (SA), which is Alg. 1 in (Jain et al., 2023).\nNote that the \u2018oracle prediction error\u2019 quantities appearing in Figure 3 are essentially the best possible error achievable using the weights estimated by the set of weights xW estimated by an algorithm.\nBefore normalization by the dimension, it is the noise level plus\n1\nm\nm \u00ff j\u201c1 min wP xW }w\u2039j \u00b4 w}22,\nwhich is easily verified to be the prediction error with oracle knowledge of the nearest element in W\u0302 to the true component mean w\u2039j , under our observational model (2). The main take-away from this simulation is that the transformer is able to get very close to the performance of the state-of-the-art model-specific algorithms, even when keeping the sample size the same."
        },
        {
            "heading": "3.3 WHAT IS THE TRANSFORMER ACTUALLY LEARNING?",
            "text": "In this section, we try to understand somewhat better, what algorithm the transformer is implementing at inference time. To do this, we define the squared distance, for two algorithms f, g that map a prompt P of length k to a prediction y\u0302k`1 of xk`1:\ndsqk pf, gq \u2013 EP \u201d pfpP q \u00b4 gpP qq2 \u0131 , where k \u011b 1.\nFigure 4 depicts k versus dsqk pf, gq, taking f to be the transformer, and g to be a candidate algorithm listed below, as k varies between 1 and 60. The compared algorithms are:\n\u2022 Posterior mean, oracle weights. Outputs f\u2039\u03c0pP q on prompt P . \u2022 Posterior mean, SA weights. Outputs f\u2039\u03c0\u0302pP q, with \u03c0\u0302 from the subspace algorithm (SA). \u2022 Posterior mean, EM weights. Same as above, but \u03c0\u0302 from expectation-maximization (EM). \u2022 Argmin, oracle weights. Outputs w\u0302pP q\u22baxk`1 where w\u0302pP q follows display (7). \u2022 Argmin, SA weights. Outputs w\u0302pP q\u22baxk`1 where w\u0302pP q follows display (7), with w\u2039j re-\nplaced by SA-estimated weights. \u2022 Argmin, EM weights. Same as above, but with EM-estimated weights.\nAs seen from Figure 4, in all of the simulated settings, the algorithm closest to the transformer at inference time is the posterior mean procedure, with the oracle weights. Impressively, this observation holds regardless of our choice of the number of mixture components."
        },
        {
            "heading": "3.4 EVALUATION ON COVARIATE SHIFT AND LABEL SHIFT",
            "text": "In this section, we evaluate transformers on distribution shift settings. The experimental results are presented in Figures 5 and 6. The distribution shift settings are described below, where we studied one setting of covariate shift and two settings of label shift.\nCovariate scaling: We evaluate the transformer on prompts of length k where the covariates (including the query) are sampled as xi \u201e Np0, \u03ba2Idq for i P rk ` 1s. This is a shift from the training distribution when \u03ba \u2030 1. Figure 5 shows the results when taking \u03ba P t0.33, 0.5, 1, 2, 3u. As we see from the figure, the transformer is able to handle, to some extent, small shifts, such as \u03ba P t0.33, 0.5, 2u, but not shifts much larger than this.\nWeight scaling: We evaluate the transformer on prompts sampled from the mixture distribution\n\u03c0scale\u03b1 \u2013 1\nm\nm \u00ff i\u201c1 \u03b4\u03b1w\u2039j , where \u03b1 \u0105 0.\nSo, the weights w\u2039j are scaled up or down by the factor \u03b1. Note that \u03c0 scale 1 \u201c \u03c0, meaning that \u03b1 \u201c 1 is no shift. The left panels of Figure 6 depict results for \u03b1 P t0.33, 0.5, 1.0, 2, 3u.\nWeight shift: We evaluate the transformer on weights sampled from the mixture distribution\n\u03c0add\u03b5 \u2013 1\nm\nm \u00ff i\u201c1 \u03b4wjp\u03b5q, where wjp\u03b5q \u2013 w \u2039 j ` \u03b5? d 1d.\nThus, \u03c0add\u03b5 shifts each component by an additive perturbation of norm \u03b5. The right panels of Figure 6 depict the results for \u03b5 P t0, 0.25, 0.5, 0.75, 1.0u. Note that \u03b5 \u201c 0 is no shift: \u03c0add0 \u201c \u03c0.\nAs seen from above, the transformer is fairly sensitive to weight scaling, as seen from the left panels in Figure 6. On the other hand, the transformer can handle small additive weight shifts, such as \u03b5 \u201c 0.25, as depicted in the right panels in Figure 6. In Appendix F we also evaluate the minimum and maximum MSE components within the mixture distribution to show the sensitivity of the model to different inference-time mixture proportions.\nComparison to posterior mean procedure: In Appendix D, we replicate the figures above, with the change that in place of the transformer, we evaluate the performance of the posterior mean procedure, f\u2039\u03c0 , defined in display (4). At a high-level, the posterior mean algorithm is less sensitive to covariate scaling, but exhibits similar behavior to the transformer on the two label shift settings."
        },
        {
            "heading": "4 DISCUSSION",
            "text": "In this work, we studied the behavior of transformers on mixtures of linear regressions, and showed they can learn these mixture models near-optimally, sample-efficiently, and robustly. The fact that general purpose predictors such as transformers can do well in this statistically-complex mixture setting should be practically useful, especially given its flexibility in the settings considered here.\nAdditionally, it would be interesting to study the in-context problem as was done in (Garg et al., 2022), but in the mixture setting. Here, the mixture distribution would be sampled from a distribution over mixture models for each prompt. In general, the decision-theoretic optimal method could be more complicated to compute, as implementing the posterior mean would require computing a highdimensional integral. Nonetheless, is it possible to approximate the optimal method with a trained transformer? We view this as an intriguing direction for future work."
        },
        {
            "heading": "A RELATED WORK",
            "text": "The related work can be broadly divided into two categories: (i) theoretical works on learning mixture models and (ii) analyzing theoretically and empirically the learning abilities of transformers.\nIn the context of (i), there are numerous works that study the well known mixed linear regression problem with a batch size of 1 (Vempala & Wang, 2004; Yi et al., 2014; 2016; Chen et al., 2020; Li & Liang, 2018; Zhong et al., 2016). In general the problem is NP-Hard as shown in (Yi et al., 2016). Therefore most of the above works with the exception of (Li & Liang, 2018) makes the assumption that the covariates of all the mixture components are isotropic Gaussians. However, even with this strong assumption the time-complexity of all these algorithms are at least super-polynomical in m rendering them impractical.\nKong et al. (2020) pioneered the study of the problem in batch setting where they were motivated by meta-learning multiple tasks. They showed that they can recover well separated mixture of linear models from batched data with polynomial dependence on d,m and the inverse of the fraction of the smallest mixture component. However, this work still had the isotropic covariate assumption. Recent work (Jain et al., 2023) removed this assumption and further improved the sample complexity and the length of the medium size batches that is required for learning. We compare the training sample complexity of learning using transformers with that of the latter, as well as the popular EM method (Zhong et al., 2016), modified to work with batched data.\nIn the context of (ii), following the emergence of several hundred billion parameter large language models (LLM)\u2019s like (Radford et al., 2019; Chowdhery et al., 2022), it has been observed that such models can learn from few examples supplied in a prompt during inference (Brown et al., 2020). This ability to learn in-context has been studied in simpler settings in many recent works (Garg et al., 2022; Von Oswald et al., 2023; Aky\u00fcrek et al., 2023; Zhang et al., 2023). (Garg et al., 2022) showed empirically that transformers can learn to perform linear regression in context. (Aky\u00fcrek et al., 2023) then showed that transformers can represent gradient decent for linear regression in context. A similar result was shown in (Von Oswald et al., 2023) but using linear self attention. Zhang et al. (2023) go one-step further by showing that gradient flow in linear self-attention based transformers can learn to do population gradient decent for linear regression. More general algorithm learning behavior has been demonstrated in (Li et al., 2023) and they also provide stability bounds for in-context learning.\nNote that none of these prior works imply ability of transformers to learn mixture models from batch or non batch setting. M\u00fcller et al. (2021); Ahuja et al. (2023) look at in context learning from a Bayesian perspective. M\u00fcller et al. (2021) show that transformers fitted on the respective prior can emulate Gaussian processes. (Ahuja et al., 2023) has a section on learning multiple function classes in-context where they empirically study gaussian mixture models with two mixture components. However, they do not study the representation learning problem and training sample complexity is not investigated in depth.\nThe work in (Bai et al., 2023) is also extremely relevant to this work. They generalize the results in (Garg et al., 2022; Aky\u00fcrek et al., 2023; Von Oswald et al., 2023) to show that transformers can represent a general version of in-context gradient descent which lets them show constructions for several in-context algorithms like linear, ridge regression and LASSO, gradient descent in 2-layer neural networks, in-context learning of GLMs. Perhaps most related is the part of the paper that deals with model selection. In particular the paper shows that tranformers can choose between two models classes either by a Post-ICL train-test split within context or a Pre-ICL distribution testing in-context. Note that these results do not generalize to our setting as (i) they do not yield our construction of the Bayes Opt in the mixture setting in Section 2 (ii) their experiments include learning with a mixture of different noise settings or distinguishing between a mixture of classification and regression, which is very different from having different f\u02dai \u2019s."
        },
        {
            "heading": "B PROOF OF THEOREM 1",
            "text": "In this section, we present the proof of Theorem 1. We begin, in Section B.1 by stating some preliminaries, such as the necessary operators we need to show that the transformer can implement. We then present the proof, assuming that these operators are transformer-representable in Section B.2.\nFinally, the proof of the representation capacity of these operators by transformers is provided in Section B.3."
        },
        {
            "heading": "B.1 OPERATORS THAT A TRANSFORMER CAN IMPLEMENT",
            "text": "We now list some operators, for a matrix H P Rp\u02c6q that output a matrix H 1 P Rp\u02c6q . The following list includes all the operators we need.\n\u2022 copy_downpH; k, k1, \u2113, Iq: For columns with index i P I, outputs H 1 where H 1k1:\u21131,i \u201c Hk:\u2113,i, and the remaining entries are unchanged. Here, \u21131 \u201c k1 `p\u2113\u00b4kq and k1 \u011b k, so that entries are copied \u201cdown\" within columns i P I. Note, we assume \u2113 \u011b k and that k1 \u010f q so that the operator is well-defined.\n\u2022 copy_overpH; k, k1, \u2113, Iq: For columns with index i P I, outputs H 1 with H 1k1:\u21131,i \u201c Hk:\u2113,i\u00b41. The remaining entries stay the same. Here entries from column i \u00b4 1 are copied \u201cover\u201d to column i.\n\u2022 mulpH; k, k1, k2, \u2113, Iq: For columns with index i P I, outputs H 1 where H 1k2`t,i \u201c Hk`t,iHk1`t,i, for t P t0, . . . , \u2113 \u00b4 ku.\nfor t P rk, \u2113s. The remaining entries stay the same. \u2022 affpH; k, k1, k2, \u2113, \u21131, \u21132,W,W 1, b, Iq: For columns with index i P I, outputs H 1 where\nH 1k2:\u21132,i \u201c WHk:\u2113,i ` W 1Hk1:\u21131,i ` b.\nNote that \u21132 \u201c k2 ` \u03b42 where W P R\u03b42\u02c6\u03b4 , W 1 P R\u03b42\u02c6\u03b41 and \u2113 \u201c k ` \u03b4, \u21131 \u201c k1 ` \u03b41. We assume \u03b4, \u03b41, \u03b42 \u011b 0. The remaining entries of H are copied over to H 1, unchanged.\n\u2022 scaled_aggpH;\u03b1, k, \u2113, k1, i, Iq: Outputs a matrix H 1 with entries\nHk1`t,i \u201c \u03b1 \u00ff\njPI Hk`t,j for t P t0, 1, . . . , \u2113 \u00b4 ku.\nThe set I is causal, so that I \u0102 ri \u00b4 1s. The remaining entries of H are copied over to H 1, unchanged.\n\u2022 softpH; k, \u2113, k1q: For the final column q, outputs a matrix H 1 with entries\nH 1k1`t,q \u201c eHk`t,q\n\u0159\u2113\u00b4k t1\u201c0 e\nHk`t1,q , for t P t0, 1, . . . , \u2113 \u00b4 ku.\nThe remaining entries of H are copied over to H 1, unchanged.\nThe important property of the above list of operators is that can all be implemented in a single layer of a autoregressive transformer. Proposition 1. Each of the operators copy_down, copy_over,mul, aff, scaled_agg, and soft, can be implemented by a single layer of an autoregressive transformer.\nSee Section B.3 for a proof of this claim."
        },
        {
            "heading": "B.2 PROOF OF THEOREM 1",
            "text": "In this section, we present the proof of Theorem 1, assuming Proposition 1. We need to introduce a bit of notation:\nIevenpkq \u2013 t2j : j P rksu and Ioddpkq \u2013 t2j \u00b4 1 : j P rk ` 1su. Additionally, we define W\u2039 P Rm\u02c6d to have rows w\u2039i P Rd, which as we recall from (10), are the true mixture weights.\nWe begin by assuming that the input prompt P is provided as Hp0q P Rp2d`4m`2q\u02c6p2k`1q. This matrix is such that the only nonzero entries are Hp0q1:d,2j\u00b41 \u201c xj P Rd for each j P rk ` 1s. Additionally, Hp0q1,2j \u201c yj for each j P rks Then, by leveraging the operators described above, we can see that f\u2039pP q \u201c Hp9q2d`4m`2,2k`1, where the matrix Hp8q is constructed by the following process:\n\u2022 Hp1q \u201c copy_downpHp0q; 1, d ` 1, d, Ioddpkqq\n\u2022 Hp2q \u201c copy_overpHp1q; d ` 1, d ` 1, 2d, Ievenpkqq\n\u2022 Hp3q \u201c copy_downpHp2q; 1, 2d ` 1, 1, Ioddpkqq\n\u2022 Hp4q \u201c affpHp3q; d ` 1, 2d ` 1, 2d ` 2, 2d, 2d ` 1, 2d ` m ` 1,W\u2039,1d\u02c61, 0, Ievenpkq Y t2k ` 1uq\n\u2022 Hp5q \u201c mulpHp4q; 2d ` 2, 2d ` 2, 2d ` m ` 2, 2d ` m ` 1, Ievenpkqq\n\u2022 Hp6q \u201c scaled_aggpHp5q;\u00b4 12\u03c32 , 2d`m` 2, 2d` 2m` 1, 2d`m` 2, 2k ` 1, Ievenpkqq\n\u2022 Hp7q \u201c softpHp6q; 2d ` m ` 2, 2d ` 2m ` 1, 2d ` 2m ` 2q\n\u2022 Hp8q \u201c mulpHp7q; 2d ` 2, 2d ` 2m ` 2, 2d ` 3m ` 2, 2d ` m ` 1t2k ` 1uq\n\u2022 Hp9q \u201c affpHp8q; 2d`3m`2, d`1, 2d`4m`1, 2d`2m, d`1, 2d`4m`2,1m, 0, 0, t2k` 1uqq\nThe process above is illustrated in Section B.2.1. By Proposition 1, each operation above is implementable by a layer of an autoregressive transformer. Therefore, this completes the proof.\nDiscussion on the construction: Note that in the operation aff used above, we do assume that the transformer stores the weights W\u2039. While at first glance this may seem like a strong assumption, we note that in our setting it is actually information-theoretically possible to learn the weights consistently as the number of prompts tends to infinity while keeping the prompt length size fixed.(For instance, see the paper Jain et al. (2023) and references in section 1.2 therein for more discussion.) We view it as an interesting line of future work to investigate to what extent the transformer stores approximate versions of the true mixture parameters w\u2039i explicitly. The main goal of the above construction is simply to show that it is indeed possible to implement the optimal method viz-a-viz a transformer.\nB.2.1 ILLUSTRATION OF PROOF OF THEOREM 1\nWe illustrate the steps taken by the transformer to implement the softmax operation. To begin with, the matrix input to the transformer is modelled as below, in the case where k \u201c 2. Below, y\u0303i \u201c py1, 0, . . . , 0q P Rd. Throughout we only show the nonzero entries (i.e., , missing rows and columns are always assumed 0). Then, our input is\nHp0q \u201c rx1 y\u03031 x2 y\u03032 x3s .\nAfter the copy_down operation, we have\nHp1q \u201c \u201e\nx1 y\u03031 x2 y\u03032 x3 x1 0 x2 0 x3\n\u0237\n.\nAfter the copy_over operation, we have\nHp2q \u201c \u201e\nx1 y\u03031 x2 y\u03032 x3 x1 x1 x2 x2 x3\n\u0237\n.\nAfter another copy_down operation, we have\nHp3q \u201c \u00ab x1 y\u03031 x2 y\u03032 x3 x1 x1 x2 x2 x3 0 y1 0 y2 0 ff .\nAfter the aff operation, we have,\nHp4q \u201c\n\u00bb\n\u2014\n\u2013 x1 y\u03031 x2 y\u03032 x3 x1 x1 x2 x2 x3 0 y1 0 y2 0 0 r1 0 r2 r3\nfi\nffi\nfl\n.\nNote that ri \u201c W\u2039xi \u00b4 yi1 for i \u2030 k ` 1 and otherwise rk`1 \u201c W\u2039xk`1. After the mul operation, we obtain\nHp5q \u201c\n\u00bb\n\u2014\n\u2014\n\u2014\n\u2013 x1 y\u03031 x2 y\u03032 x3 x1 x1 x2 x2 x3 0 y1 0 y2 0 0 r1 0 r2 r3 0 r21 0 r 2 2 0\nfi\nffi\nffi\nffi\nfl\nAbove the square should be interpreted element wise on the vectors ri. Then, after the scaled_agg operation, we obtain\nHp6q \u201c\n\u00bb\n\u2014\n\u2014\n\u2014\n\u2013 x1 y\u03031 x2 y\u03032 x3 x1 x1 x2 x2 x3 0 y1 0 y2 0 0 r1 0 r2 r3 0 r21 0 r 2 2 \u00b4 12\u03c32 pr 2 1 ` r22q\nfi\nffi\nffi\nffi\nfl\n.\nThen, after the softmax operation, we obtain\nHp7q \u201c\n\u00bb\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2013 x1 y\u03031 x2 y\u03032 x3 x1 x1 x2 x2 x3 0 y1 0 y2 0 0 r1 0 r2 r3 0 r21 0 r 2 2 \u00b4 12\u03c32 pr 2 1 ` r22q\n0 0 0 0 p\nfi\nffi\nffi\nffi\nffi\nffi\nfl\n.\nHere, p \u201c softmaxp\u00b4 12\u03c32 pr 2 1 ` r22qq. Finally, after yet another mul, we obtain\nHp8q \u201c\n\u00bb\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2013\nx1 y\u03031 x2 y\u03032 x3 x1 x1 x2 x2 x3 0 y1 0 y2 0 0 r1 0 r2 r3 0 r21 0 r 2 2 \u00b4 12\u03c32 pr 2 1 ` r22q\n0 0 0 0 p 0 0 0 0 p \u02dd r3\nfi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nfl\n.\nAbove, \u02dd denotes elementwise multiplication. Finally, after an aff operation, we obtain\nHp9q \u201c\n\u00bb\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2013\nx1 y\u03031 x2 y\u03032 x3 x1 x1 x2 x2 x3 0 y1 0 y2 0 0 r1 0 r2 r3 0 r21 0 r 2 2 \u00b4 12\u03c32 pr 2 1 ` r22q\n0 0 0 0 p 0 0 0 0 p \u02dd r3 0 0 0 0 f\u2039\u03c0pP q\nfi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nfl\n.\nNote that the bottom-right entry of Hp9q contains the output f\u2039\u03c0pP q. In other words, the desired result if Hp9q2d`4m`2,2k`1. Note that H P Rp\u02c6q where p \u201c 2d ` 4m ` 2, and q \u201c 2k ` 1."
        },
        {
            "heading": "B.3 PROOF OF PROPOSITION 1",
            "text": "To begin with, we recall a few definitions, introduced in recent work (Aky\u00fcrek et al., 2023). Definition 1 (RAW operator). The Read-Arithmetic-Write (RAW) operators are maps on matrices, Rp\u02c6q \u00d1 Rp\u02c6q ,\nRAW\u201apH; I,J ,K,\u0398I ,\u0398J ,\u0398K, \u03c0q \u201c H 1, where \u201a P tb,\u2018u. Here \u03c0 is a causal set-valued map, with \u03c0piq \u0102 ri \u00b4 1s. The operators b,\u2018 denote elementwise multiplication and addition, respectively. The entries of H 1 are given by\nH 1K,i \u2013 \u0398K \u02c6 \u0398JHJ ,i \u201a \u00b4 \u0398I maxt|\u03c0piq|, 1u \u00ff\ni1P\u03c0piq HI,i1\n\u00af\n\u02d9\n, and (8a)\nH 1Kc,i \u201c HKc,i, (8b)\nfor each i P rqs. Note that above Kc \u201c rps z K, and for some positive integer r, \u0398I P Rr\u02c6|I|,\u0398J P Rr\u02c6|J |, and \u0398K P R|K|\u02c6r.\nIn Aky\u00fcrek et al. (2023), they show that the RAW operator can be implemented in one autoregressive transformer layer.5 They also argue that (with a slight change in parameterization) that the mul and aff operators are transformer-implementable. Therefore, we simply need to argue that the operators soft, copy_down, copy_over, and scaled_agg are all implementable by a transformer.\nTo begin with, note that, by inspection, we have, with \u03b4 \u201c \u2113 \u00b4 k,\ncopy_overpH; k, k1, \u2113, Iq \u201c RAW\u2018pH; rk, \u2113s,H, rk1, k1 ` \u03b4s, I\u03b4`1, 0, I\u03b4`1, \u03c0Iq (9a) copy_downpH; k, k1, \u2113, Iq \u201c RAW\u2018pH; rk, \u2113s,H, rk1, k1 ` \u03b4s, I\u03b4`1, 0, I\u03b4`1, \u03c01Iq (9b) scaled_aggpH;\u03b1, k, \u2113, k1, i, Iq \u201c RAW\u2018pH; rk, \u2113s,H, rk1, k1 ` \u03b4s, I\u03b4`1, 0, \u03b1I\u03b4`1, \u03c02I,iq (9c)\nAbove, note that the intervals ra, bs are just the integers between a and b (inclusive) and that we have defined\n\u03c0Ipiq \u201c \" ti \u00b4 1u i \u011b 2 H otherwise , \u03c0 1 Ipiq \u201c \" tiu i P I H otherwise , and \u03c0 2 I,ipjq \u201c \" I j \u201c i H otherwise .\nTherefore, the displays (9) establish the following result. Lemma 1. The operators copy_over, copy_down, and scaled_agg are all implementable via the RAW operator in a single autoregressive transformer layer.\nFinally, in Section B.3.1 we demonstrate the following result. Lemma 2. The softmax operation is implementable by an autoregressive transformer.\nThis completes the proof of Proposition 1."
        },
        {
            "heading": "B.3.1 PROOF OF LEMMA 2",
            "text": "In order to implement the softmax operation, we need to introduce a few other operations:\n\u2022 divpH; j, k, k1, \u2113, Iq: For columns with index i P I, outputs H 1 where H 1k1`t,i \u201c Hk`t,i{Hj,i for all t P t0, . . . , \u2113\u00b4 ku. The remaining entries of H are copies as is into H 1. \u2022 movpH; k, k1, \u2113, Iq: For columns with index i P I, outputs H 1 where H 1k1`t,i \u201c Hk`t,i for all t P t0, . . . , \u2113 \u00b4 ku. The remaining entries of H are copies as is into H 1.\n\u2022 sigmoidpH; k, k1q : In the final column q, outputs H 1 with H 1k1,q \u201c 11`e\u00b4Hk,q . The remaining entries of H are copies as is into H 1.\nThe operations div,mov are special cases of the same operations as introduced in the paper Aky\u00fcrek et al. (2023). Thus, we only need to demonstrate that sigmoid is transformer-implementable. Assuming this for the moment, note that the softmax operation softmax is then implementable by the following sequence of operations. Let H denote the input to the softmax layer, and let s \u201c Hk:\u2113,q . Using the affine operation (aff) together with the softmax operation (soft) we can compute the values 1{p1 ` esiq. Using the affine operation (aff) together with the div operation, we can invert these values to compute esi . Finally, we can compute the sum of these values S \u201c \u0159\ni e si with an affine\noperation (aff) and we can divide by this sum using another div operation. The result values are esi{ \u0159\nj e sj , which is the softmax of the vector s. A move operation (mov) then can move these\nvalues into the correct locations, Hk1:pk1`\u2113\u00b4kq,q , as required.\nThus, to complete the proof, we need to show how to implement the sigmoid operation. For this, we can begin by using the affine operation to insert a value of 1 in the final column, and another affine operation to insert a 2 \u02c6 2 identity matrix in the first 2 columns of H . Then by selecting WK and WQ to select the identity matrix and to select pHk,i, 1q, respectively, we can ensure that\n5Note that in that paper, they more or less ignore the aggregate approximation error. Following this approach, we also ignore the lower order terms arising from the approximation of the underlying operations such as multiplication, as was done in Aky\u00fcrek et al. (2023).\nWKH:i \u201c I2\u02c62 and WQhi \u201c pHk,i, 1q, and the corresponding softmax values in the self-attention layer are si \u201c peHk`t,i{p1 ` Hk,iq, 1{peHk,i ` 1qq. By selecting WV to select the identity matrix in H:i, and WC to select the first value of si and place it in position k1 ` t, we can ensure that a \u201c 1{pe\u00b4Hk,i ` 1qek1 , where ej denotes the jth standard basis vector. This value is precisely the sigmoid, as needed. To place this value in the correct location, we simply set the feedforward network matrices W in,W out \u201c 0. Then, to preserve the output, we need to delete the value 1, and identity matrices placed into H at the beginning; this can clearly be done by two affine operations."
        },
        {
            "heading": "C ADDITIONAL DETAILS ON TRAINING METHODOLOGY",
            "text": "Our training approach closely follows that of Garg et al. (2022) and Aky\u00fcrek et al. (2023). After some hyperparameter optimization, we settled on the choice of hidden dimension of 256, 8 attention heads, and 12 layers. We trained our transformers using Adam, with a constant step size of 0.1. We used curriculum training, as in Garg et al. (2022), with the exception of Figure 3, where the sample size was fixed. Our curriculum phases were 2000 steps each, with a batch size of 64. The final stage of training had 250000 steps with 64 batches. All of our figures presented mean squared errors computed over batch sizes of 256. The dimension of the original covariates was d \u201c 20 throughout this paper."
        },
        {
            "heading": "C.1 DETAILS ON FIXED SAMPLE-SIZE TRAINING",
            "text": "In this setting, we used hyperparameter tuning over the dropout parameter, \u03c1 P t0, 0.05, 0.1u, and found the following choices to be best, for Figure 3:\n\u2022 for n \u201c 15000, we took \u03c1 \u201c 0.0.\n\u2022 for n \u201c 30000, we took \u03c1 \u201c 0.1.\n\u2022 for n \u201c 45000, we took \u03c1 \u201c 0.0.\n\u2022 for n \u201c 60000, we took \u03c1 \u201c 0.1.\nWe also used curriculum training in this setup, but obtained the samples by subsampling the fixed dataset. This was done by first randomly sampling a batch from the full dataset, and then randomly dropping and shuffling the prefix of each prompt so as to obtain a prompt of the shorter, desired length. Otherwise, the entire procedure was the same as the other figures, as described above."
        },
        {
            "heading": "C.2 BATCH EXPECTATION MAXIMIZATION (EM) ALGORITHM",
            "text": "Batch expectation-maximization is a variant of the standard expectation-maximization method (see, for instance, Section 14.5.1 in Bishop (2006)). For completeness, we describe the algorithm formally here. Note that \u03d5 denotes the standard univariate Gaussian pdf below. For notation, we also denote the prompts as\nP piq \u2013 ` x piq 1 , y piq 1 , . . . , x piq k , y piq k , x piq k`1 \u02d8 , for i P rns.\nThe algorithm is then stated below as Algorithm 1\nAlgorithm 1 Batch expectation-maximization for a discrete mixture of linear regressions with Gaussian noise Require: Length k prompts tP piquni\u201c1 noise variance \u03c3 \u0105 0, number of components m \u0105 0.\nInitialize \u03c0p0q P r0, 1sm, drawn uniformly on the probability simplex. Initialize wp0qj P Rd, drawn uniformly on the sphere of radius ? d for j P rms. Initialize \u03b3p0qij \u201c 0 for all i P rns, j P rms. while have not converged do\nupdate prompt-component assignment probabilities,\n\u03b3 pt`1q ij \u201c\n\u03c0 ptq j \u015bk l\u201c1 \u03d5\n\u00b4 y piq l \u00b4px piq l q \u22ba w ptq j\n\u03c3\n\u00af\n\u0159m j1\u201c1 \u03c0 ptq j1 \u015bk l\u201c1 \u03d5\n\u00b4 y piq l \u00b4px piq l q \u22baw ptq j1\n\u03c3\n\u00af\n, for all i P rns, j P rms.\nupdate the marginal component probabilities by the formula\n\u03c0 pt`1q j \u201c\n1\nn\nn \u00ff i\u201c1 \u03b3 pt`1q ij , for all j P rms\nupdate the parameter estimates by solving,\nw pt`1q j \u201c argminwPRd\n! n \u00ff\ni\u201c1\nk \u00ff l\u201c1 \u03b3 pt`1q ij ` y piq l \u00b4 w \u22ba x piq l \u02d82 ) , for all j P rms.\nupdate the iteration counter, t \u00d0 t ` 1. end while return final set of component centers, twptqj umj\u201c1\nIn our implementation we stop (or declare the algorithm converged) if t \u0105 tmax, or if\nmax j min j1\n}wptqj \u00b4 w pt\u00b41q j1 }2 \u010f \u03b5.\nIn our experiments we took tmax \u201c 20000 and \u03b5 \u201c 0.001."
        },
        {
            "heading": "D COMPARISON TO DISTRIBUTION SHIFT WITH THE POSTERIOR MEAN ESTIMATOR",
            "text": "In this section, we replicate the figures presented in Section 3.4, except we evaluate the distribution shift settings on the posterior mean procedure, f\u2039\u03c0 as defined in display (4)."
        },
        {
            "heading": "E EXTENSIONS TO NON-LINEAR MIXTURES OF REGRESSIONS",
            "text": "In this section, we consider nonlinear extensions to the model described in Section 1.1.\nWe consider discrete mixtures of the form\n\u03c0 \u2013 1\nm\nm \u00ff i\u201c1 \u03b4f\u2039i , (10)\nwhere tf\u2039i umi\u201c1 are functions lying a nonlinear function class F , mapping Rd to R. We consider prompts or batches, denoted P \u201c px1, y1, . . . , xk, yk, xk`1q. Here, for noise level \u03c3 \u011b 0, we have\nf \u201e \u03c0, xi i.i.d.\u201e Np0, Idq, and yi | xi \u201e Npfpxiq, \u03c32q. (11)\nThe goal is then to predict yk`1, the label for the query xk`1.\nWe train the transformer models here as described in the main text, with the exception that the data is generated according to the distribution above. We also compare, at inference time, to the following generalizations of the posterior mean algorithm, denoted f\u0302PMA, and the argmin procedure, denoted f\u0302AM. These are maps from the observed prompt P to a function f P F , given by\nf\u0302PMApP q \u2013\n\u0159m j\u201c1 f \u2039 j exp\n\u00b4\n\u00b4 12\u03c32 \u0159k i\u201c1pf\u2039j pxiq \u00b4 yiq2 \u00af\n\u0159m \u2113\u201c1 exp\n\u00b4\n\u00b4 12\u03c32 \u0159k i\u201c1pf\u2039\u2113 pxiq \u00b4 yiq2 \u00af , and,\nf\u0302AMpP q \u2013 argminfPtf\u2039j umj\u201c1 !\nk \u00ff i\u201c1 pfpxiq \u00b4 yiq2 ) .\nThus, in order to specify the setting fully, we need only define our particular choice of function class, F . We do this in each of the subsequent sections."
        },
        {
            "heading": "E.1 MULTIVARIATE POLYNOMIALS",
            "text": "In this section, we consider degree-2 polynomials in Rd given by\nF \u201c ! fpxq \u201c d \u00ff\ni,j\u201c1 \u03b1ijxixj : \u03b1ij P R\n)\nTo compute these efficiently, we select \u03b1ij \u201c wiwj where the weights wi P R are such that p \u0159d\ni\u201c1 w 2 i q2 \u201c d, which ensure that Epy2q \u201c 3d ` 1. When plotting the mean squared error (MSE) in the figures below, we divide by 3d ` 1. The interpretation is that when the normalized MSE is equal to 1, we are doing no better than predicting 0 for each x; MSE significantly lower (on the normalized scale) indicates a substantial improvement.\nIn addition to the comparison with the posterior mean and argmin procedures, we also compare against polynomial regression, where y is regressed on to the degree-2 monomials of the form xixj , where i, j P rds. We additionally present results with a ridge penalty at the noise level.\nE.2 2 LAYER FULLY-CONNECTED NEURAL NETWORKS\nWe consider the case where f\u02dai \u2019s are 2 hidden layer MLP\u2019s similar. Note that (Garg et al., 2022) also had non-linear experiments with one hidden layer MLP\u2019s but not in the mixture setting. In particular our functions take the form,\nf\u02dai pxq \u201c \u03c3pWi\u03c3pUix ` b p1q i q ` b p2q i q. (13)\nThere are m such fixed MLP\u2019s making the m mixture components for generating the data. Each prompt is generated from one component after adding additive noise with a fixed variance. In Figure 11 we show the performance of our method against posterior mean and argmin procedure. We also include the result from regressing a single MLP (of the same size as data generating functions) per prompt \u2013 called 2 layer NN in the figure. The f\u02dai \u2019s used in this experiment have 10 neurons in both hidden layers. We set m \u201c 5 and d \u201c 10. The noise variance is set to 0.1 which is roughly 10 percent of the signal for the networks chosen. We can clearly see that the trained transformer behaves very similar to the posterior mean algorithm which is optimal in this setting. Moreover, as expected the MLP regression does not work well since it trains one model per prompt."
        },
        {
            "heading": "F SENSITIVITY TO DIFFERENT MIXTURE PROPORTIONS",
            "text": "In this section, we extend our simulation results by considering what happens if the inference-time mixture propotions change. Consider a mixture distribution of the form\n\u03c0 \u201c m \u00ff\ni\u201c1 pi\u03b4w\u2039i ,\nwhere the weights w\u2039i are the same as in the model (10). Note that the mean squared error of any algorithm under \u03c0 is given by the average\nMSEp\u03c0q \u201c m \u00ff\ni\u201c1 pi Ew\u2039i\n\u201d pfpP q \u00b4 yk`1q2 \u0131 ,\nwhere the expectation under w\u2039i indicates the distribution of prompt P under the generative model (2) with the prompt weight fixed to be w\u2039i .\nEvidently, by considering the maximum and minimum choice of mixture distribution \u03c0, we have\nmin iPrms Ew\u2039i\n\u201d pfpP q \u00b4 yk`1q2 \u0131\n\u010f MSEp\u03c0q \u010f max iPrms Ew\u2039i\n\u201d pfpP q \u00b4 yk`1q2 \u0131 .\nThis holds for any choice of mixture distribution \u03c0 where the weights are w\u2039i but the mixture proportions pi are arbitrary.\nTherefore, motivated by this inequality, we plot the maximum and minimum MSE for each algorithm we considered in the main text, for each choice of prompt length. The results are presented in Figure 12. The main takeaway is that the models are fairly insensitive to changing the mixture proportions, as indicated by the minimum and maximum of the MSE for each model being quite close to each other.\nG COMPARISON TO LINEAR REGRESSION METHODS AS m GROWS LARGE\nWhen the number of components m grows large, the optimal method, when \u03c3 \u0105 0, is a ridge regression estimator (asymptotically, as the prompt length grows large); this is justified by Corollary 3 in Dicker (2016) and the result in Section 3.1.1 of Pathak et al. (2023).\nThus, we investigate to what extent this is true in our experiments, with transformers. In Figure 13 we plot the mean squared errors of ridge regression divided by the mean squared error of our trained transformer. The training of the transformer follows the main text of the paper, and the ridge regression estimates were computed for each k and m on a batch of 2048 samples. The qualitative takeaway is that as m grows large, this ratio of risks is getting smaller. Since when m is large the optimal estimator the optimal estimator should be closer to ridge regression, this can be interpreted as an indication that the same architecture of transformers used in our paper is able to be adapted to any structure of mixture distribution with essentially no modifications."
        }
    ],
    "title": "TRANSFORMERS CAN OPTIMALLY LEARN REGRESSION MIXTURE MODELS",
    "year": 2024
}