{
    "abstractText": "The Segment Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM\u2019s local prior assumption. Notably, Conv-LoRA not only preserves SAM\u2019s extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM\u2019s foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores ConvLoRA\u2019s superiority in adapting SAM to real-world semantic segmentation tasks.1",
    "authors": [
        {
            "affiliations": [],
            "name": "CIENT FINETUNING"
        },
        {
            "affiliations": [],
            "name": "ANYTHING MODEL"
        },
        {
            "affiliations": [],
            "name": "Zihan Zhong"
        },
        {
            "affiliations": [],
            "name": "Zhiqiang Tang"
        },
        {
            "affiliations": [],
            "name": "Haoyang Fang"
        },
        {
            "affiliations": [],
            "name": "Chun Yuan"
        }
    ],
    "id": "SP:3c2609d7cb2e91171175b7a7e7616e0c675e97e4",
    "references": [
        {
            "authors": [
                "Hangbo Bao",
                "Wenhui Wang",
                "Li Dong",
                "Qiang Liu",
                "Owais Khan Mohammed",
                "Kriti Aggarwal",
                "Subhojit Som",
                "Songhao Piao",
                "Furu Wei"
            ],
            "title": "Vlmo: Unified vision-language pre-training with mixture-of-modality-experts",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Anil Batra",
                "Suriya Singh",
                "Guan Pang",
                "Saikat Basu",
                "CV Jawahar",
                "Manohar Paluri"
            ],
            "title": "Improved road connectivity by joint learning of orientation and segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Jorge Bernal",
                "F Javier S\u00e1nchez",
                "Gloria Fern\u00e1ndez-Esparrach",
                "Debora Gil",
                "Cristina Rodr\u0131\u0301guez",
                "Fernando Vilari\u00f1o"
            ],
            "title": "Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians",
            "venue": "Computerized medical imaging and graphics,",
            "year": 2015
        },
        {
            "authors": [
                "Daniel Bolya",
                "Cheng-Yang Fu",
                "Xiaoliang Dai",
                "Peizhao Zhang",
                "Christoph Feichtenhofer",
                "Judy Hoffman"
            ],
            "title": "Token merging: Your vit but faster",
            "venue": "arXiv preprint arXiv:2210.09461,",
            "year": 2022
        },
        {
            "authors": [
                "Ali Borji",
                "Ming-Ming Cheng",
                "Qibin Hou",
                "Huaizu Jiang",
                "Jia Li"
            ],
            "title": "Salient object detection: A survey",
            "venue": "Computational visual media,",
            "year": 2019
        },
        {
            "authors": [
                "Shurong Chai",
                "Rahul Kumar Jain",
                "Shiyu Teng",
                "Jiaqing Liu",
                "Yinhao Li",
                "Tomoko Tateyama",
                "Yen-wei Chen"
            ],
            "title": "Ladder fine-tuning approach for sam integrating complementary network",
            "venue": "arXiv preprint arXiv:2306.12737,",
            "year": 2023
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "George Papandreou",
                "Iasonas Kokkinos",
                "Kevin Murphy",
                "Alan L Yuille"
            ],
            "title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "George Papandreou",
                "Florian Schroff",
                "Hartwig Adam"
            ],
            "title": "Rethinking atrous convolution for semantic image segmentation",
            "venue": "arXiv preprint arXiv:1706.05587,",
            "year": 2017
        },
        {
            "authors": [
                "Tianrun Chen",
                "Lanyun Zhu",
                "Chaotao Ding",
                "Runlong Cao",
                "Shangzhan Zhang",
                "Yan Wang",
                "Zejian Li",
                "Lingyun Sun",
                "Papa Mao",
                "Ying Zang"
            ],
            "title": "Sam fails to segment anything?\u2013sam-adapter: Adapting sam in underperformed scenes: Camouflage, shadow, and more",
            "venue": "arXiv preprint arXiv:2304.09148,",
            "year": 2023
        },
        {
            "authors": [
                "Zhe Chen",
                "Yuchen Duan",
                "Wenhai Wang",
                "Junjun He",
                "Tong Lu",
                "Jifeng Dai",
                "Yu Qiao"
            ],
            "title": "Vision transformer adapter for dense predictions",
            "venue": "arXiv preprint arXiv:2205.08534,",
            "year": 2022
        },
        {
            "authors": [
                "Bowen Cheng",
                "Alex Schwing",
                "Alexander Kirillov"
            ],
            "title": "Per-pixel classification is not all you need for semantic segmentation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Bowen Cheng",
                "Ishan Misra",
                "Alexander G Schwing",
                "Alexander Kirillov",
                "Rohit Girdhar"
            ],
            "title": "Maskedattention mask transformer for universal image segmentation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Noel CF Codella",
                "David Gutman",
                "M Emre Celebi",
                "Brian Helba",
                "Michael A Marchetti",
                "Stephen W Dusza",
                "Aadi Kalloo",
                "Konstantinos Liopyris",
                "Nabin Mishra",
                "Harald Kittler"
            ],
            "title": "Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)",
            "venue": "IEEE 15th international symposium on biomedical imaging (ISBI",
            "year": 2018
        },
        {
            "authors": [
                "Mostafa Dehghani",
                "Josip Djolonga",
                "Basil Mustafa",
                "Piotr Padlewski",
                "Jonathan Heek",
                "Justin Gilmer",
                "Andreas Peter Steiner",
                "Mathilde Caron",
                "Robert Geirhos",
                "Ibrahim Alabdulmohsin"
            ],
            "title": "Scaling vision transformers to 22 billion parameters",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Nan Du",
                "Yanping Huang",
                "Andrew M Dai",
                "Simon Tong",
                "Dmitry Lepikhin",
                "Yuanzhong Xu",
                "Maxim Krikun",
                "Yanqi Zhou",
                "Adams Wei Yu",
                "Orhan Firat"
            ],
            "title": "Glam: Efficient scaling of language models with mixture-of-experts",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Deng-Ping Fan",
                "Ge-Peng Ji",
                "Guolei Sun",
                "Ming-Ming Cheng",
                "Jianbing Shen",
                "Ling Shao"
            ],
            "title": "Camouflaged object detection",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Deng-Ping Fan",
                "Ge-Peng Ji",
                "Tao Zhou",
                "Geng Chen",
                "Huazhu Fu",
                "Jianbing Shen",
                "Ling Shao"
            ],
            "title": "Pranet: Parallel reverse attention network for polyp segmentation",
            "venue": "In International conference on medical image computing and computer-assisted intervention,",
            "year": 2020
        },
        {
            "authors": [
                "Deng-Ping Fan",
                "Ge-Peng Ji",
                "Ming-Ming Cheng",
                "Ling Shao"
            ],
            "title": "Concealed object detection",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "William Fedus",
                "Barret Zoph",
                "Noam Shazeer"
            ],
            "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Jun Fu",
                "Jing Liu",
                "Haijie Tian",
                "Yong Li",
                "Yongjun Bao",
                "Zhiwei Fang",
                "Hanqing Lu"
            ],
            "title": "Dual attention network for scene segmentation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Benjamin Graham",
                "Alaaeldin El-Nouby",
                "Hugo Touvron",
                "Pierre Stock",
                "Armand Joulin",
                "Herv\u00e9 J\u00e9gou",
                "Matthijs Douze"
            ],
            "title": "Levit: a vision transformer in convnet\u2019s clothing for faster inference",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Demi Guo",
                "Alexander M Rush",
                "Yoon Kim"
            ],
            "title": "Parameter-efficient transfer learning with diff pruning",
            "venue": "arXiv preprint arXiv:2012.07463,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Xinrong Hu",
                "Xiaowei Xu",
                "Yiyu Shi"
            ],
            "title": "How to efficiently adapt large segmentation model (sam) to medical images",
            "venue": "arXiv preprint arXiv:2306.13731,",
            "year": 2023
        },
        {
            "authors": [
                "Debesh Jha",
                "Pia H Smedsrud",
                "Michael A Riegler",
                "P\u00e5l Halvorsen",
                "Thomas de Lange",
                "Dag Johansen",
                "H\u00e5vard D Johansen"
            ],
            "title": "Kvasir-seg: A segmented polyp dataset",
            "venue": "In MultiMedia Modeling: 26th International Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Wei Ji",
                "Jingjing Li",
                "Qi Bi",
                "Wenbo Li",
                "Li Cheng"
            ],
            "title": "Segment anything is not always perfect: An investigation of sam on different real-world applications",
            "venue": "arXiv preprint arXiv:2304.05750,",
            "year": 2023
        },
        {
            "authors": [
                "Menglin Jia",
                "Luming Tang",
                "Bor-Chun Chen",
                "Claire Cardie",
                "Serge Belongie",
                "Bharath Hariharan",
                "Ser-Nam Lim"
            ],
            "title": "Visual prompt tuning",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Shibo Jie",
                "Zhi-Hong Deng"
            ],
            "title": "Convolutional bypasses are better vision transformer adapters",
            "venue": "arXiv preprint arXiv:2207.07039,",
            "year": 2022
        },
        {
            "authors": [
                "Trung-Nghia Le",
                "Tam V Nguyen",
                "Zhongliang Nie",
                "Minh-Triet Tran",
                "Akihiro Sugimoto"
            ],
            "title": "Anabranch network for camouflaged object segmentation",
            "venue": "Computer vision and image understanding,",
            "year": 2019
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant"
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "arXiv preprint arXiv:2104.08691,",
            "year": 2021
        },
        {
            "authors": [
                "Xia Li",
                "Zhisheng Zhong",
                "Jianlong Wu",
                "Yibo Yang",
                "Zhouchen Lin",
                "Hong Liu"
            ],
            "title": "Expectationmaximization attention networks for semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang"
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "arXiv preprint arXiv:2101.00190,",
            "year": 2021
        },
        {
            "authors": [
                "Dongze Lian",
                "Daquan Zhou",
                "Jiashi Feng",
                "Xinchao Wang"
            ],
            "title": "Scaling & shifting your features: A new baseline for efficient model tuning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Weicong Liang",
                "Yuhui Yuan",
                "Henghui Ding",
                "Xiao Luo",
                "Weihong Lin",
                "Ding Jia",
                "Zheng Zhang",
                "Chao Zhang",
                "Han Hu"
            ],
            "title": "Expediting large-scale vision transformer for dense prediction without finetuning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Long",
                "Evan Shelhamer",
                "Trevor Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Volodymyr Mnih"
            ],
            "title": "Machine learning for aerial image labeling",
            "venue": "University of Toronto (Canada),",
            "year": 2013
        },
        {
            "authors": [
                "Namuk Park",
                "Songkuk Kim"
            ],
            "title": "How do vision transformers work",
            "venue": "arXiv preprint arXiv:2202.06709,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Maithra Raghu",
                "Thomas Unterthiner",
                "Simon Kornblith",
                "Chiyuan Zhang",
                "Alexey Dosovitskiy"
            ],
            "title": "Do vision transformers see like convolutional neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Carlos Riquelme",
                "Joan Puigcerver",
                "Basil Mustafa",
                "Maxim Neumann",
                "Rodolphe Jenatton",
                "Andr\u00e9 Susano Pinto",
                "Daniel Keysers",
                "Neil Houlsby"
            ],
            "title": "Scaling vision with sparse mixture of experts",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "MICCAI 2015: 18th International Conference,",
            "year": 2015
        },
        {
            "authors": [
                "Tal Shaharabany",
                "Aviad Dahan",
                "Raja Giryes",
                "Lior Wolf"
            ],
            "title": "Autosam: Adapting sam to medical images by overloading the prompt encoder",
            "venue": "arXiv preprint arXiv:2306.06370,",
            "year": 2023
        },
        {
            "authors": [
                "Noam Shazeer",
                "Azalia Mirhoseini",
                "Krzysztof Maziarz",
                "Andy Davis",
                "Quoc Le",
                "Geoffrey Hinton",
                "Jeff Dean"
            ],
            "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
            "venue": "arXiv preprint arXiv:1701.06538,",
            "year": 2017
        },
        {
            "authors": [
                "Juan Silva",
                "Aymeric Histace",
                "Olivier Romain",
                "Xavier Dray",
                "Bertrand Granado"
            ],
            "title": "Toward embedded detection of polyps in wce images for early diagnosis of colorectal cancer",
            "venue": "International journal of computer assisted radiology and surgery,",
            "year": 2014
        },
        {
            "authors": [
                "Przemys\u0142aw Skurowski",
                "Hassan Abdulameer",
                "J B\u0142aszczyk",
                "Tomasz Depta",
                "Adam Kornacki",
                "P Kozie\u0142"
            ],
            "title": "Animal camouflage analysis: Chameleon database",
            "venue": "Unpublished manuscript,",
            "year": 2018
        },
        {
            "authors": [
                "Apurva Sriwastwa",
                "Shikha Prakash",
                "Swati Swarit",
                "Khushboo Kumari",
                "Sitanshu Sekhar Sahu"
            ],
            "title": "Detection of pests using color based image segmentation",
            "venue": "Second International Conference on Inventive Communication and Computational Technologies (ICICCT),",
            "year": 2018
        },
        {
            "authors": [
                "Yi-Lin Sung",
                "Jaemin Cho",
                "Mohit Bansal"
            ],
            "title": "Lst: Ladder side-tuning for parameter and memory efficient transfer learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wei Liu",
                "Yangqing Jia",
                "Pierre Sermanet",
                "Scott Reed",
                "Dragomir Anguelov",
                "Dumitru Erhan",
                "Vincent Vanhoucke",
                "Andrew Rabinovich"
            ],
            "title": "Going deeper with convolutions",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Nima Tajbakhsh",
                "Suryakanth R Gurudu",
                "Jianming Liang"
            ],
            "title": "Automated polyp detection in colonoscopy videos using shape and context information",
            "venue": "IEEE transactions on medical imaging,",
            "year": 2015
        },
        {
            "authors": [
                "Lv Tang",
                "Haoke Xiao",
                "Bo Li"
            ],
            "title": "Can sam segment anything? when sam meets camouflaged object detection",
            "venue": "arXiv preprint arXiv:2304.04709,",
            "year": 2023
        },
        {
            "authors": [
                "David V\u00e1zquez",
                "Jorge Bernal",
                "F Javier S\u00e1nchez",
                "Gloria Fern\u00e1ndez-Esparrach",
                "Antonio M L\u00f3pez",
                "Adriana Romero",
                "Michal Drozdzal",
                "Aaron Courville"
            ],
            "title": "A benchmark for endoluminal scene segmentation of colonoscopy",
            "venue": "images. Journal of healthcare engineering,",
            "year": 2017
        },
        {
            "authors": [
                "Tom\u00e1s F Yago Vicente",
                "Le Hou",
                "Chen-Ping Yu",
                "Minh Hoai",
                "Dimitris Samaras"
            ],
            "title": "Large-scale training of shadow detectors with noisily-annotated shadow examples",
            "venue": "In Computer Vision\u2013 ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "An Wang",
                "Mobarakol Islam",
                "Mengya Xu",
                "Yang Zhang",
                "Hongliang Ren"
            ],
            "title": "Sam meets robotic surgery: An empirical study on generalization, robustness and adaptation",
            "venue": "arXiv preprint arXiv:2308.07156,",
            "year": 2023
        },
        {
            "authors": [
                "Wenhai Wang",
                "Enze Xie",
                "Xiang Li",
                "Deng-Ping Fan",
                "Kaitao Song",
                "Ding Liang",
                "Tong Lu",
                "Ping Luo",
                "Ling Shao"
            ],
            "title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Wenhai Wang",
                "Enze Xie",
                "Xiang Li",
                "Deng-Ping Fan",
                "Kaitao Song",
                "Ding Liang",
                "Tong Lu",
                "Ping Luo",
                "Ling Shao"
            ],
            "title": "Pvt v2: Improved baselines with pyramid vision transformer",
            "venue": "Computational Visual Media,",
            "year": 2022
        },
        {
            "authors": [
                "Haiping Wu",
                "Bin Xiao",
                "Noel Codella",
                "Mengchen Liu",
                "Xiyang Dai",
                "Lu Yuan",
                "Lei Zhang"
            ],
            "title": "Cvt: Introducing convolutions to vision transformers",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Junde Wu",
                "Rao Fu",
                "Huihui Fang",
                "Yuanpei Liu",
                "Zhaowei Wang",
                "Yanwu Xu",
                "Yueming Jin",
                "Tal Arbel"
            ],
            "title": "Medical sam adapter: Adapting segment anything model for medical image segmentation",
            "venue": "arXiv preprint arXiv:2304.12620,",
            "year": 2023
        },
        {
            "authors": [
                "Enze Xie",
                "Wenjia Wang",
                "Wenhai Wang",
                "Mingyu Ding",
                "Chunhua Shen",
                "Ping Luo"
            ],
            "title": "Segmenting transparent objects in the wild",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Enze Xie",
                "Wenhai Wang",
                "Zhiding Yu",
                "Anima Anandkumar",
                "Jose M Alvarez",
                "Ping Luo"
            ],
            "title": "Segformer: Simple and efficient design for semantic segmentation with transformers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Enze Xie",
                "Wenjia Wang",
                "Wenhai Wang",
                "Peize Sun",
                "Hang Xu",
                "Ding Liang",
                "Ping Luo"
            ],
            "title": "Segmenting transparent object in the wild with transformer",
            "venue": "arXiv preprint arXiv:2101.08461,",
            "year": 2021
        },
        {
            "authors": [
                "Mengde Xu",
                "Zheng Zhang",
                "Fangyun Wei",
                "Han Hu",
                "Xiang Bai"
            ],
            "title": "Side adapter network for openvocabulary semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Weijian Xu",
                "Yifan Xu",
                "Tyler Chang",
                "Zhuowen Tu"
            ],
            "title": "Co-scale conv-attentional image transformers",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yongyang Xu",
                "Zhong Xie",
                "Yaxing Feng",
                "Zhanlong Chen"
            ],
            "title": "Road extraction from high-resolution remote sensing imagery using deep learning",
            "venue": "Remote Sensing,",
            "year": 2018
        },
        {
            "authors": [
                "Elad Ben Zaken",
                "Shauli Ravfogel",
                "Yoav Goldberg"
            ],
            "title": "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
            "venue": "arXiv preprint arXiv:2106.10199,",
            "year": 2021
        },
        {
            "authors": [
                "Kaidong Zhang",
                "Dong Liu"
            ],
            "title": "Customized segment anything model for medical image segmentation",
            "venue": "arXiv preprint arXiv:2304.13785,",
            "year": 2023
        },
        {
            "authors": [
                "Yundong Zhang",
                "Huiye Liu",
                "Qiang Hu"
            ],
            "title": "Transfuse: Fusing transformers and cnns for medical image segmentation",
            "venue": "24th International Conference,",
            "year": 2021
        },
        {
            "authors": [
                "Hengshuang Zhao",
                "Jianping Shi",
                "Xiaojuan Qi",
                "Xiaogang Wang",
                "Jiaya Jia"
            ],
            "title": "Pyramid scene parsing network",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Zilong Zhong",
                "Zhong Qiu Lin",
                "Rene Bidart",
                "Xiaodan Hu",
                "Ibrahim Ben Daya",
                "Zhifeng Li",
                "Wei-Shi Zheng",
                "Jonathan Li",
                "Alexander Wong"
            ],
            "title": "Squeeze-and-attention networks for semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Tao Zhou",
                "Yizhe Zhang",
                "Yi Zhou",
                "Ye Wu",
                "Chen Gong"
            ],
            "title": "Can sam segment polyps",
            "venue": "arXiv preprint arXiv:2304.07583,",
            "year": 2023
        },
        {
            "authors": [
                "Yanqi Zhou",
                "Tao Lei",
                "Hanxiao Liu",
                "Nan Du",
                "Yanping Huang",
                "Vincent Zhao",
                "Andrew M Dai",
                "Quoc V Le",
                "James Laudon"
            ],
            "title": "Mixture-of-experts with expert choice routing",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Lei Zhu",
                "Ke Xu",
                "Zhanghan Ke",
                "Rynson WH Lau"
            ],
            "title": "Mitigating intensity bias in shadow detection via feature decomposition and reweighting",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Simiao Zuo",
                "Xiaodong Liu",
                "Jian Jiao",
                "Young Jin Kim",
                "Hany Hassan",
                "Ruofei Zhang",
                "Tuo Zhao",
                "Jianfeng Gao"
            ],
            "title": "Taming sparsely activated transformer with stochastic experts",
            "venue": "arXiv preprint arXiv:2110.04260,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The AI community have witnessed the explosion development of a series of foundation models in recent years, such as CLIP (Radford et al., 2021), GPT-4 (OpenAI, 2023) and ViT-22B (Dehghani et al., 2023). Recently, Segment Anything (SAM) (Kirillov et al., 2023), a promptable model pretrained on over 1 billion masks and 11 million images, emerged as a foundation model for image segmentation. Despite its impressive zero-shot performance on generic object segmentation, it doesn\u2019t perform well on many real-world segmentation tasks in certain domains (Tang et al., 2023; Ji et al., 2023; Zhou et al., 2023), such as natural images (Borji et al., 2019; Fan et al., 2020a), agriculture (Sriwastwa et al., 2018), remote sensing (Xu et al., 2018) and medical images (Fan et al., 2020b).\nFollowing the pretraining-finetuing paradigm (Dosovitskiy et al., 2020; He et al., 2022; Liu et al., 2021a), it is natural to finetune SAM on downstream tasks to enhance its performance. However, existing works (Zhang & Liu, 2023; Chen et al., 2023; Shaharabany et al., 2023) have failed to either analyze or address certain limitations inherent in SAM. 1) SAM\u2019s image encoder is a plain ViT, which is known to lack of vision-specific inductive biases (Chen et al., 2022) that are useful for dense predictions. 2) SAM\u2019s pretraining is essentially a binary mask prediction task that, where, given one prompt, it separates foreground object from background. The low-level mask prediction pretraining hinders SAM\u2019s ability to capture high-level image semantic information crucial for tasks like multi-class semantic segmentation.\nTo tackle the above limitations and still retain SAM\u2019s valuable segmentation knowledge acquired during pretraining, we finetune a small set of (extra) model parameters while freezing most of SAM\u2019s pretrained weights, hence parameter efficient finetuning (PEFT). This raises the question: Can PEFT\n\u2217Work done while interning at Amazon Web Services. 1Our code is public available at https://github.com/autogluon/autogluon/tree/\nmaster/examples/automm/Conv-LoRA\nenhance SAM encoder with image-related local prior and facilitate the acquisition of high-level semantic information?\nIn this paper, we propose a new PEFT method named Conv-LoRA by diving into Low-Rank Adaptation (LoRA) (Hu et al., 2021). LoRA introduces slim trainable linear projection layers into each transformer layer of SAM\u2019s encoder, thereby helping recover its capacity to extract high-level semantic information. Our experiments demonstrate that LoRA surpasses the widely-adopted visual prompt tuning (VPT) (Jia et al., 2022), particularly in the multi-class semantic segmentation tasks. On top of LoRA, Conv-LoRA integrates lightweight convolution layers within its bottleneck structure. Convolution can introduce the image-related local prior (i.e. a pixel exhibits stronger correlation with its neighbors than the distant pixels) (Chen et al., 2022) through the local spatial operations.\nFurthermore, it is essential to inject the local prior into the appropriate scale(s) of image features, considering the potential variations in object scales. To this end, Conv-LoRA draws inspiration from the concept of Mixture-of-Experts (MoE) (Shazeer et al., 2017) and incorporates multiple parallel convolutional experts, each specializing in a distinct feature scale. Given that ViT processes image features at a fixed scale, typically downsampling them by a factor of 16 from the original resolution, each expert in Conv-LoRA initially recovers image features at a specific scale, applies convolutional operations, and then reverts the features to the default scale. Compared to ViT-adaptor (Chen et al., 2022) and vision-specific transformers like Swin Transformer (Liu et al., 2021a), Conv-LoRA provides an implicit way to enforce multi-scale local priors, assuming it can leverage image features at the default scale to reconstruct feature information at higher scales. Fortunately, SAM\u2019s supervised pretraining, which involves masks of various scales, enables the ViT to acquire knowledge of image features beyond the default scale.\nIn the spirit of PEFT, we also remove the prompt encoder and add lightweight MLPs in the mask decoder for multi-class prediction. This simple modification has transformed SAM into an end-toend model that can be finetuned on both binary and multi-class semantic segmentation applications. Overall, our contribution can be summarized as follows:\n\u2022 We present an innovative PEFT technique Conv-LoRA. By incorporating supplementary convolution operations, Conv-LoRA reinforces the local prior of SAM from the perspective of handling the limitation of plain ViT.\n\u2022 Conv-LoRA uses MoE to model the process of dynamically selecting the proper feature scale to inject the vision-specific inductive biases.\n\u2022 Our investigations reveal that SAM\u2019s pretraining has impeded its ViT encoder\u2019s capacity to learn high-level image semantic information. However, LoRA demonstrates the potential to help SAM recover this crucial ability.\n\u2022 We conduct an extensive benchmark encompassing diverse domains, including natural images, agriculture, remote sensing, and healthcare. Conv-LoRA consistently exhibits superior performance over other PEFT techniques in various downstream tasks."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Parameter Efficient Fine-Tuning (PEFT). Parameter Efficient Fine-Tuning (PEFT) minimizes computational and storage requirements by selectively fine-tuning a small subset of model parameters, while keeping the majority fixed. PEFT encompasses methods such as adapter-based techniques, selective parameter tuning, prompt-driven fine-tuning, and Low-Rank Adaptation (LoRA) emerging from Natural Language Processing (NLP). In the adapter paradigm (Houlsby et al., 2019; Hu et al., 2021; Sung et al., 2022), compact modules are inserted within transformer layers, and other approaches (Guo et al., 2020; Zaken et al., 2021) involve fine-tuning a small fraction of parameters from pre-trained backbones. Prompt tuning (Lester et al., 2021; Li & Liang, 2021) adds adaptable tokens to input or intermediate sequences, and LoRA (Hu et al., 2021) introduces trainable low-rank matrices into transformer layers for weight updates.\nPEFT techniques have also proven effective in the Computer Vision (CV) domain. Visual Prompt Tuning (VPT) (Jia et al., 2022) applies prompt tuning concepts (Lester et al., 2021) to image classification, while Scale and Shift Feature Modulation (SSF) (Lian et al., 2022) uses scale and shift parameters for modulating visual features in image classifiers. Convpass (Jie & Deng, 2022) introduces a convolutional bottleneck to enhance ViT\u2019s performance in image classification. In our study, we focus on developing PEFT for SAM in semantic segmentation tasks, specifically enforcing multi-scale local priors beyond the default scale, distinguishing our approach from Convpass.\nSegmentation Models. FCN (Long et al., 2015) is a key deep image segmentation model that directly generates pixel-wise segmentation maps from images. U-Net (Ronneberger et al., 2015) employs an encoder-decoder structure with skip connections to preserve fine-grained spatial information. Deeplab (Chen et al., 2017a) integrates atrous (dilated) convolutions for multi-scale context, while PSPNet (Zhao et al., 2017) uses a pyramid pooling module. DANet (Fu et al., 2019), SANet (Zhong et al., 2020), and EMA (Li et al., 2019) utilize attention mechanisms for contextual dependencies. Transformer architectures like PVT (Wang et al., 2021), Swin (Liu et al., 2021b), CvT (Wu et al., 2021), CoaT (Xu et al., 2021), LeViT (Graham et al., 2021), Segformer (Xie et al., 2021a), and PVT v2 (Wang et al., 2022) bring various improvements. SAM (Ji et al., 2023), a recent breakthrough in segmentation, offers a universal approach for segmenting diverse objects and regions in images. Fine-tuning SAM on downstream tasks is recommended due to a lack of high-level semantic information and potential domain bias in the pre-training dataset.\nFine-tuning SAM. Some prior works (Chen et al., 2023; Zhang & Liu, 2023; Wu et al., 2023; Chai et al., 2023; Shaharabany et al., 2023; Hu et al., 2023; Wang et al., 2023) explore fine-tuning SAM for downstream tasks. These methods include tuning SAM\u2019s mask decoder or integrating parameterefficient tuning methods with SAM\u2019s image encoder. Some of them (e.g.,(Chen et al., 2023; Zhang & Liu, 2023; Shaharabany et al., 2023)) provide end-to-end solutions to automate SAM. Our method further addresses the structural limitation of SAM\u2019s image encoder for capturing visual-specific inductive biases by introducing convolution operations. And we unveil that SAM\u2019s pretraining hampers its ViT encoder\u2019s ability to learn high-level semantic information. We also transform SAM into an end-to-end semantic segmentation model with minor architectural adjustments.\nMixture-of-Experts. Mixture-of-Expert (MoE) is designed to expand model capacity while introducing small computational overhead. An MoE layer leverages multiple experts to enhance model capacity, while using the gating network to regulate sparsity for computational savings. FeedForward Networks (FFN) are commonly employed as the default choice for experts (Shazeer et al., 2017; Riquelme et al., 2021; Bao et al., 2022; Du et al., 2022; Zhou et al., 2022; Fedus et al., 2022). Some efforts (Zuo et al., 2021; Zhou et al., 2022) focus on more efficient gating mechanisms.\nIn our work, we utilize the concept of MoE, not aiming at improving it. We compare MoE used in our work with original MoE in three aspects: 1) The original goal of MoE is to expand model capacity without excessively increasing computational overhead, whereas ours is to dynamically inject the local prior into the feature maps of different scales. 2) The structures of experts in MoE are typically the same, whereas ours are not. Each expert specializes in a specific scaling operation\nin our method. 3) While MoE is mostly employed during pre-training, we employ MoE as a part for parameter-efficient tuning on the downstream tasks."
        },
        {
            "heading": "3 METHOD",
            "text": ""
        },
        {
            "heading": "3.1 CONV-LORA",
            "text": "LoRA. First, let\u2019s briefly recap the design of LoRA (Hu et al., 2021), which uses an encoder-decoder structure to impose a low-rank constraint on the weight updates (fig. 2 (a)). It freezes the pretrained model weights and injects small trainable rank decomposition matrices into each layer of the transformer architecture. Specifically, given a pre-trained weight matrix W0 \u2208 Rb\u00d7a, LoRA adds a pair of linear encoder We and decoder Wd, i.e., trainable rank decomposition matrices, along its side. We and Wd satisfy the low rank constraints We \u2208 Rr\u00d7a, Wd \u2208 Rb\u00d7r, and r \u226a min(a, b). With LoRA, the forward pass changes from h = W0x to:\nh = W0x+WdWex (1)\nConv-LoRA aims to incorporate convolution operations between the encoder and decoder components of LoRA (fig. 2 (b)). On one hand, convolution can inject the image-related local prior, addressing fundamental limitation of the vanilla ViT. On the other hand, the low-rank constraint ensures that the convolution layers remain exceedingly lightweight, preserving the PEFT nature of Conv-LoRA.\nA pivotal consideration in designing Conv-LoRA is determining the scale of feature maps at which to introduce the local prior. While the feature maps in ViT are uniform in scale, object masks typically encompass a wide range of scales. Therefore, it is crucial to apply convolution operations at the right scale. To tackle this challenge, we draw inspiration from the concept of Mixture of Experts (MoE) (Shazeer et al., 2017). MoE comprises multiple expert networks and a gating module that dynamically selects which expert(s) to activate during the forward pass (fig. 3). Adapting this concept to Conv-LoRA, each expert specializes in convolution at a specific scale of feature maps, and a compact gating module learns to dynamically choose the expert(s) based on the input data. Mathematically, with Conv-LoRA, eq. (1) changes to:\nh = W0x+Wd( n\u2211 i G(Wex)iEi(Wex)) (2)\nwhere W0 \u2208 RCout\u00d7Cin , We \u2208 Rr\u00d7Cin , Wd \u2208 RCout\u00d7r, x \u2208 RB\u00d7Cin\u00d7H\u00d7W . B is batch size, Cin/Cout is the number of input / output channels, H and W correspond to the height and width. Ei is the ith expert of all n experts. G is the gating network with only top-k (default 1) values activated. Refer to appendix A for more details of gating.\nInside each expert, three key operations are arranged in sequence: an interpolation function that reconstructs feature maps at a specific scale, a 3 \u00d7 3 convolutional layer, and a subsequent interpo-\nlation operation to map the feature maps back to the default feature scale of ViT. Assume expert Ei is in charge of scale si, we can formulate it as:\nEi(x) = Interpolate(Conv3\u00d73(Interpolate(x, si)), 1/si) (3)\nFor instance, if si = 4, expert Ei would initially upscale the feature maps by a factor of 4x, apply the Conv3\u00d73 operation, and finally, downscale the feature maps by 4x.\nMoE vs. Multi-scale. In contrast to MoE, another method to address diverse scales is employing a multi-scale strategy. This approach utilizes multiple branches to concurrently inject local priors at various scales and aggregates the results. Although seemingly more straightforward, this method comes at a higher computational cost when compared to MoE. The efficiency of MoE stems from its capacity to selectively activate sparse experts, thereby minimizing computational overhead. Given our priority on efficient finetuning, we favor MoE as a discerning choice."
        },
        {
            "heading": "3.2 END-TO-END MULTI-CLASS SEGMENTATION WITH SAM",
            "text": "SAM comprises three essential components: an image encoder, a prompt encoder, and a mask decoder. When provided with an image and a prompt, which can take the form of a point, box, mask, or text, the mask decoder generates a mask of the object associated with the given prompt. While this prompt-based approach renders SAM flexible for integration into larger systems, such as interactive segmentation or a combination of detection and subsequent segmentation, it does pose challenges in making SAM an end-to-end model in practical applications. To automate SAM, we freeze the prompt encoder, thus always constant prompt tokens to mask decoder, when finetuning it on downstream tasks. Moreover, the original mask decoder is designed to predict binary masks, distinguishing between foreground and background based on the given prompt. To adapt SAM for multi-class semantic segmentation tasks, we introduce a straightforward classification branch (depicted as the red dashed box in fig. 4) within the mask decoder. This extra branch is responsible for predicting classification scores. Additionally, we apply full fine-tuning to the mask decoder as it is a lightweight module. For more comprehensive information, refer to appendix B."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "Settings. We perform several experiments using SAM on four real-world scenarios, including medical images, natural images, agriculture and remote sensing. We use the batch size of 4 and Adam optimizer with learning rate of 1\u00d710\u22124 as default, with a weight decay of 1\u00d710\u22124. A larger learning rate of 3 \u00d7 10\u22124 is found useful for the datasets we use in agriculture and remote sensing. The random horizontal flip is applied during training as data augmentation. All the methods are trained for 30 epochs with structure loss (i.e., the combination of weighted IoU loss and binary cross entropy loss) unless otherwise specified. Additionally, our Conv-LoRA follows Shazeer et al. (2017) to introduce extra loss for balancing the utilization among the experts. The weight of the extra loss is set to 1.0 and 2.0 for binary-class and multi-class semantic segmentation respectively. We set the number of experts to be 8 by default, with each expert specializing in a scaling ratio within the\ncontinuous range from 1 to 8. And we apply Conv-LoRA to the query, key and value matrices in self-attention layers, same as how LoRA does.\nDatasets. Our experiments encompass semantic segmentation datasets from various domains, spanning natural images, medical images, agriculture, and remote sensing. In the natural image domain, we explore two specific tasks: camouflaged object segmentation (Fan et al., 2020a; Skurowski et al., 2018; Le et al., 2019) and shadow detection (Vicente et al., 2016). Within medical segmentation, we investigate polyp segmentation (Jha et al., 2020; Bernal et al., 2015; Tajbakhsh et al., 2015; Va\u0301zquez et al., 2017; Silva et al., 2014) and skin lesion segmentation (Codella et al., 2018). For agriculture and remote sensing, we employ the leaf disease segmentation (Rath, 2023) and road segmentation (Mnih, 2013) datasets as representative examples, respectively. We also explore multi-class transparent object segmentation using Trans10K-v1 (Xie et al., 2020) with 3 classes and Trans10K-v2 (Xie et al., 2021b) with 12 fine-grained classes. Further details about each dataset can be found in appendix C.\nBaselines. We compare our method with the following methods: 1) Fine-tune SAM\u2019s mask decoder only. 2) BitFit (Zaken et al., 2021), which only fine-tunes bias terms in the pre-trained model. 3) Adapter (Houlsby et al., 2019), which inserts the trainable bottleneck layers between the transformer layers. 4) SAM-Adapter (Chen et al., 2023), which further tunes the patch embedded features and learns an extra embedding for high-frequency components for low-level semantic segmentation tasks. And it is one of the pioneer works that apply PEFT method to SAM. 5) VPT (Jia et al., 2022), which inserts learnable tokens to hidden states for each transformer layer. 6) LST (Sung et al., 2022), which inserts a trainable side network parallel to the frozen backbone. To control the number of trainable parameters, the side adapter network employs a pre-trained ViT-Tiny model, similar to SAN (Xu et al., 2023). The features from the frozen backbone and the side adapter network are fused at the global attention layers of SAM. 7) SSF (Lian et al., 2022), which inserts learnable scale and shift parameters to modulate visual features during training. 8) LoRA (Hu et al., 2021) inserts trainable bottleneck layers parallel to the frozen linear weight.\nTo adhere to the page limit, we select representative datasets of different domains to report the main experiment results. Refer to appendix D for the full experiment results. All the experiments for PEFT methods are run for three times to ease the randomness. The average values and the standard error are reported in table 1."
        },
        {
            "heading": "4.1 BINARY-CLASS SEMANTIC SEGMENTATION",
            "text": "In table 1, all PEFT methods consistently outperform fine-tuning the mask decoder alone, underscoring the importance of finetuning the image encoder of SAM. Furthermore, Conv-LoRA surpasses other PEFT techniques across diverse datasets from different domains. Compared to other PEFT methods, Conv-LoRA uses lightweight convolution operations to strengthen the vision-specific local prior, which turns out effective in boosting the segmentation performance. The substantial performance gaps between SAM trained from scratch and Conv-LoRA also underscore the considerable assistance provided by SAM\u2019s pretraining knowledge in enhancing downstream task performance.\nAdditionally, while Conv-LoRA outperforms certain domain-specific methods that are having more trainable parameters, it may still fall short on specific datasets. It\u2019s important to note that ConvLoRA aims to be a general-purpose PEFT method for adapting SAM to various domains rather than competing with these domain-specific models. Tailoring SAM for specific domain applications with more intricate adjustments might yield superior performance compared to specialized model designs."
        },
        {
            "heading": "4.2 MULTI-CLASS SEMANTIC SEGMENTATION",
            "text": "In table 2, while decoder-only fine-tuning approaches achieve comparable segmentation accuracy with domain-specific methods, they exhibit a substantial gap in terms of mIoU metrics. While accuracy measures pixel-level segmentation performance, mIoU takes mask class information into account. We suspect that SAM\u2019s image encoder encounters challenges in extracting high-level semantic information that is valuable for classification tasks. Moreover, fine-tuning the image encoder using PEFT methods results in a significant boost in mIoU, indicating a restoration of its capability to learn high-level image semantics.\nAdditionally, we conduct linear probing experiments on SAM\u2019s image encoder. Specifically, we freeze SAM\u2019s ViT-B encoder and train only a linear head on ImageNet-1K. Similarly, we perform linear probing for the ViT-B pretrained with MAE (He et al., 2022). The results reveal that SAM\u2019s image encoder exhibits significantly lower ImageNet-1K accuracy compared to the MAE encoder (54.2% vs. 67.7%). Given that SAM\u2019s image encoder is initialized using an MAE encoder, we\nhypothesize the pre-training focused on low-level foreground-background mask prediction adversely affects the ViT encoder\u2019s ability to capture high-level semantic information for classification.\nTo be specific, SAM was trained on a dataset that exclusively comprises segmentation masks without explicit semantic information. In theory, to minimize loss, the fundamental objective for its encoder is to project pixels into a metric space where pixels from the same object are in close proximity, while those from distinct objects are distantly positioned. This projection requires an implicit understanding of \u2018objectness\u2019, focusing on proximity within an image rather than preserving consistent representations of the same-type object across different images. This introduces a potential challenge in aligning representations with semantics across diverse images."
        },
        {
            "heading": "4.3 ABLATION STUDY",
            "text": "SAM\u2019s local prior assumption. SAM\u2019s local prior is grounded in its extensive segmentation pretraining. Through supervised training on a vast dataset encompassing 1 billion high-quality masks and 11 million images, SAM has honed a robust capability to discern and capture local features within images. Notably, SAM\u2019s encoder retains the ViT architecture, which inherently lacks a dedicated local prior. However, this deficiency is effectively compensated by the significant local prior acquired through segmentation pretraining.\nWe analyze using the mean attention distance as a metric. In fig. 5, SAM exhibits numerous heads in the deep blocks with short mean attention distances, suggesting its heightened focus on local information during the later stages of the encoder. In contrast, the MAE pretrained ViT, representing SAM\u2019s initialization, displays consistently long mean attention distances among attention heads in the later stages. Consequently, SAM\u2019s segmentation pretraining induces a transformative shift in ViT\u2019s attention heads, steering them from a global-oriented to a local-oriented configuration. This transformation underscores the efficacy of SAM\u2019s approach in imbuing the model with a distinctive local prior, enhancing its ability to capture fine-grained details within images.\nMoE vs. Multi-scale. Conv-LoRA leverages MoE to dynamically inject the local prior into feature maps of different scales (fig. 6 (a)), as the optimal scale remains unclear. Here we explore the impact of a multi-scale strategy, which fuses the features from all scales simultaneously (fig. 6 (b)).\nWe compare performance and training costs in table 3. Dynamic MoE outperforms multi-scale direct addition in terms of performance. This could be attributed to the preference for specific scales\u2019 feature maps based on different inputs. When injecting the local prior into feature maps of all scales simultaneously, the discrepancy of the importance diminishes as the information from critical feature maps is smoothed out. However, with dynamic selection of the top-1 experts for each forward pass, the information from these crucial scales takes precedence and isn\u2019t diluted by other\nscales. Dynamic MoE also provides a 1.54x speedup and reduces memory usage by 1.7GB during training. In summary, the comparison underscores the effectiveness and efficiency of MoE.\nThe \u2018Optimal\u2019 Scale of Feature Map for Different Datasets. While we are unable to definitively determine the optimal scale for introducing the local prior, we could check whether the \u2018optimal\u2019 scale within a given range varies indeed across different datasets.\nSpecifically, we simply modify Conv-LoRA: use only one expert and a specific scaling ratio. We set the scaling ratio to 1, 2, 4 respectively. The experiments are conducted on Leaf Disease Segmentation dataset and ISIC 2017 dataset.\nIn table 4, the \u2018optimal\u2019 scale indeed varies across the different datasets. The scaling ratio set to 4 is optimal for Leaf Disease Segmentation dataset, whereas it is set to 2 for ISIC 2017 dataset. These results further confirms our assumption and the necessity for our dynamic local prior injection based on different inputs.\nFor more ablation experiments, analyses (e.g., further analyses of local prior and MoE) and visualization, refer to appendix E through appendix I."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "Parameter efficient finetuning (PEFT) is a popular way when adapting foundation models to various downstream tasks. We present Conv-LoRA, a novel PEFT approach for applying SAM to downstream segmentation applications. Conv-LoRA is simple, generic, and obtains promising results over multiple domains including natural images, agriculture, remote sensing, and healthcare. Moreover, ours shed light on several aspects of SAM: 1) although the large-scale supervised segmentation pretraining can provide image-related local prior knowledge from the data perspective, injecting lightweight convolution operations in the ViT encoder can further boost the exploitation of local prior from another perspective of architecture; 2) the foreground-background segmentation pretraining prevents the image encoder from learning high-level semantic information, which can be alleviated through finetuning relatively few parameters in the encoder.\nOur efforts primarily focus on developing a general PEFT method for SAM, showing stronger performance than existing PEFT methods in a broad spectrum of benchmarks, other than directly competing with state-of-the-art (SOTA) models in specialized domains. Given that SAM fine-tuned with Conv-LoRA may not yet consistently outperform domain-specific SOTA models, we believe that tailoring the mask decoder and prompt encoder beyond image encoder finetuning, and combining Conv-LoRA with other PEFT methods can be promising directions for domain-specific applications."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "We thank all the anonymous reviewers for their helpful comments. This work was supported by the National Key R&D Program of China (2022YFB4701400/4701402), SSTIC Grant (KJZD20230923115106012), Shenzhen Key Laboratory (ZDSYS20210623092001004), and Beijing Key Lab of Networked Multimedia."
        },
        {
            "heading": "A GATING NETWORK OF CONV-LORA",
            "text": "In order to calculate the gating scores G(x) \u2208 RB\u00d7k, i.e., the scores for B samples in a batch assigned to k experts, we need to calculate the values H(x) \u2208 RB\u00d7n of n experts first. As the size of input x is B \u00d7 r\u00d7H \u00d7W (we use x to represent Wex in eq. (2) for simplicity), we apply global average pooling (denoted as \u2018AvgPool\u2019) followed by a reshaping operation (denoted as \u2018Reshape\u2019), to obtain xh \u2208 RB\u00d7r. Specifically, the size of x is changed to B \u00d7 r \u00d7 1 \u00d7 1 after \u2018AvgPool\u2019, and then changed to B\u00d7r after \u2018Reshape\u2019. Then, following (Shazeer et al., 2017), we apply the trainable gating weight Wg \u2208 Rr\u00d7n, and the noise term Wnoise \u2208 Rr\u00d7n to calculate H(x):\nxh = Reshape(AvgPool(x), (B, r)) H(x)i = (xh \u00b7Wg)i+ StandardNormal() \u00b7 Softplus((xh \u00b7Wnoise)i) (4)\nWe keep only the top k values (denoted as \u2018KeepTopK\u2019) based on H(x)i of each expert Ei, setting the rest to \u2212\u221e, whose corresponding gate values is equal to 0 after a \u2018Softmax\u2019 operation (If G(x)j is 0, we need not compute Ej(x)):\nG(x) = Softmax(KeepTopK(H(x), k)) where KeepTopK(v, k)i = { vi if vi is in the top k elements of v. \u2212\u221e otherwise.\n(5)"
        },
        {
            "heading": "B MASK DECODER FOR MULTI-CLASS SEGMENTATION",
            "text": "SAM\u2019s mask decoder follows the design in mask-based segmentation, wherein the image is grouped into N (i.e., the number of output tokens in fig. 4) regions represented by binary masks. However, unlike other mask-based methods such as Maskformer (Cheng et al., 2021) and Mask2former (Cheng et al., 2022), SAM does not incorporate a classification module; it solely identifies the foreground, lacking the ability for multi-class segmentation. What we need to do is straightforward: incorporate a classification branch to obtain class predictions for the corresponding N masks.\nDuring training, we need to match the set of N mask predictions and the set of ground truth segments. We follow the design in Mask2Former (Cheng et al., 2022), which proposes a memoryfriendly way for bipartite matching between the predictions and ground truths. It efficiently alleviates the memory demands in semantic segmentation tasks that necessitate high-resolution mask prediction, particularly for SAM involving the processing of high-resolution input images (1024 \u00d7 1024). For the masks that are not allocated to any ground truths, an extra \u2018no object\u2019 category is introduced to ensure the one-to-one matching. Hence, the classification branch should produce K + 1 class predictions, assuming there are originally K categories. For semantic inference, we can exclude the \u2018no object\u2019 category and perform a straightforward matrix multiplication between the masks and the classification predictions to obtain pixel-wise predictions.\nFollowing Mask2Former, we use binary cross-entropy loss Lce and dice loss Ldice for mask loss, cross entropy loss Lcls for classification predictions:\nLmulti\u2212class = \u03bbmaskLmask + \u03bbclsLcls + \u03bbMoELMoE (6)\nwhere Lmask = \u03bbceLce + \u03bbdiceLdice."
        },
        {
            "heading": "C DATASET DETAILS",
            "text": "Polyp Segmentation (medical images) Polyp Segmentation, the task to identify abnormal growths known as polyps within gastrointestinal endoscopic images, plays a critical role in early colorectal cancer diagnosis and treatment planning, and it presents a formidable challenge due to the considerable diversity in polyp shapes and sizes. We choose two polyp segmentation datasets: Kvasir (Jha\net al., 2020) and CVC-ClinicDB/CVC-612 (Bernal et al., 2015). Kvasir contains 1000 images and CVC-ClinicDB, also called CVC-612, includes 612 open-access images. Fan et al. (2020b) divides the images into a 9:1 ratio for training and testing. Additionally, we randomly divide a validation set comprising 20% of the images from the training set, for validation during training. Furthermore, we also use images from CVC-ColonDB (Tajbakhsh et al., 2015), EndoScene (Va\u0301zquez et al., 2017) and ETIS (Silva et al., 2014) for testing following the setting in (Fan et al., 2020b). We consistently train all methods for 30 epochs, unless otherwise specified for specific datasets.\nSkin Lesion Segmentation (medical images) Skin Lesion Segmentation involves the segmentation of various types of skin lesions within medical images, serving a crucial role in the early diagnosis and treatment of skin disorders, notably skin cancer. However, this task is particularly challenging due to ambiguous boundaries and color variations. We choose ISIC 2017 (Codella et al., 2018) for skin lesion segmentation. ISIC 2017 provides 2000 images for training, 150 images for validation and 600 images for testing.\nCamouflaged Object Segmentation (natural images) Camouflaged Object Segmentation focuses on identifying objects that are concealed within complex or visually cluttered backgrounds, which is more challenging comparing to traditional object segmentation. We choose three camouflaged object detection datasets: COD10K (Fan et al., 2020a), CHAMELEON (Skurowski et al., 2018), and CAMO (Le et al., 2019). COD10K contains 3040 training and 2026 testing samples. CHAMELEON includes 76 images collected from the Internet for testing. And CAMO provides 1000 images for training and 250 for testing. Following (Fan et al., 2020a), we train on the combined dataset consists of the training images from COD10K and CAMO for 20 epochs, and test on the three datasets. Addtionally, we randomly split 10% of the images from the training set for validation.\nShadow Detection (natural images) Shadow Detection focuses on the recognition of shadow regions within a scene and can facilitate the estimation of lighting conditions or the removal of shadows. We choose SBU (Vicente et al., 2016), which is the largest annotated shadow dataset. SBU contains 4085 and 638 images for training and testing. We randomly split 10% of the images from the training set for validation and we train the methods for 10 epochs with balanced binary cross entropy loss.\nLeaf Segmentation (agriculture) Leaf segmentation involves the identification of individual plant leaves within agricultural images and plays a crucial role in advancing automation for plant diseases control and high quality food production. We choose a Leaf Disease Segmentation dataset (Rath, 2023), which contains 498 images for training and 90 images for testing. We randomly split the training images into 80% for training, 20% for validation.\nRoad Segmentation (remote sensing) Road segmentation detects road or street regions within images or video frames, and is crucial for applications in autonomous driving, traffic analysis, and urban planning. We choose Massachusetts Roads Dataset (Mnih, 2013), which contains 1107 images for training, 13 images for validation and 48 images for testing. And we train the methods for 20 epochs.\nMulticlass Semantic Segmentation (natural images) We choose Trans10K-v1 (Xie et al., 2020) and Trans10K-v2 (Xie et al., 2021b) dataset for multi-class transparent object segmentation. Trans10Kv1 dataset contains 10428 images, with background as one category and two more categories of transparent objects: Transparent Things (e.g., cups, bottles) and Stuff (e.g., windows). Trans10Kv2 dataset is based on Trans10K-v1 dataset, with more fine-grained categories annotations. The dataset contains background plus two main categories divided into 11 fine-grained categories: 1) Transparent Things containing cup, bottle, jar, bowl and eyeglass. 2) Transparent Stuff containing windows, shelf, box, freezer, glass walls and glass doors. In respect to fine-grained categories and high diversity, Trans10K-v2 is more challenging than Trans10K-v1. All the datasets use 5000, 1000 and 4428 images for training, validation and testing, respectively."
        },
        {
            "heading": "D FULL EXPERIMENT RESULTS",
            "text": "Here are the full experiment results for binary semantic segmentation. Our Conv-LoRA demonstrates superiority across diverse datasets from various domains compared to other PEFT methods. Noted that models indicated in italics in the following tables are specifically designed for the corresponding tasks. We re-run the experiments on all the datasets using the model code provided by\nthe authors. We choose methods that are either widely established or relatively novel within their respective domains. With the exception of the Leaf Segmentation dataset, which is sourced from Kaggle, we utilize the author\u2019s provided sample code to conduct our experiments.\nPolyp Segmentation:\nMethod # Params (M) Kvasir CVC-612/ Ratio(%) S\u03b1 \u2191 E\u03d5 \u2191 F\u03c9\u03b2 \u2191 MAE \u2193 S\u03b1 \u2191 E\u03d5 \u2191 F\u03c9\u03b2 \u2191 MAE \u2193 PraNet (Fan et al., 2020b) 32.55 / 100% 90.9 94.4 88.7 2.9 92.6 95.5 88.7 1.0 decoder-only 3.51 / 0.55% 86.5 89.5 77.9 5.1 85.5 89.9 74.7 3.0 BitFit 3.96 / 0.62% 90.8 \u00b1 0.57 93.8 \u00b1 0.98 87.4 \u00b1 0.50 3.2 \u00b1 0.43 89.0 \u00b1 0.40 91.6 \u00b1 0.98 81.7 \u00b1 0.39 2.3 \u00b1 0.15 Adapter 3.92 / 0.61% 91.2 \u00b1 0.23 94.0 \u00b1 0.16 88.2 \u00b1 1.28 3.1 \u00b1 0.06 89.3 \u00b1 0.43 92.0 \u00b1 0.63 82.5 \u00b1 0.41 2.2 \u00b1 0.10 VPT 4.00 / 0.62% 91.5 \u00b1 0.23 94.3 \u00b1 0.06 90.0 \u00b1 0.54 2.8 \u00b1 0.07 91.0 \u00b1 0.94 93.7 \u00b1 1.41 84.8 \u00b1 2.16 2.1 \u00b1 0.28 LST 11.49 / 1.77% 89.7 \u00b1 0.25 93.3 \u00b1 0.37 86.9 \u00b1 0.28 3.7 \u00b1 0.10 89.4 \u00b1 0.37 92.4 \u00b1 0.54 83.7 \u00b1 0.59 2.4 \u00b1 0.17 SAM-Adapter 3.98 / 0.62% 89.6 \u00b1 0.24 92.5 \u00b1 0.10 86.9 \u00b1 0.31 3.6 \u00b1 0.09 89.6 \u00b1 0.22 92.4 \u00b1 1.06 82.2 \u00b1 0.51 2.2 \u00b1 0.07 SSF 4.42 / 0.69% 91.3 \u00b1 0.87 93.9 \u00b1 1.49 88.3 \u00b1 1.42 3.0 \u00b1 0.57 89.6 \u00b1 0.37 91.9 \u00b1 0.79 82.0 \u00b1 0.74 2.2 \u00b1 0.18 LoRA 4.00 / 0.62% 91.2 \u00b1 0.28 93.8 \u00b1 0.22 88.4 \u00b1 0.46 3.1 \u00b1 0.17 90.7 \u00b1 0.04 92.5 \u00b1 0.41 84.5 \u00b1 1.03 2.2 \u00b1 0.19 Conv-LoRA 4.02 / 0.63% 92.0 \u00b1 0.15 94.7 \u00b1 0.16 89.7 \u00b1 0.60 2.6 \u00b1 0.06 91.3 \u00b1 0.69 94.0 \u00b1 0.78 85.5 \u00b1 0.97 1.9 \u00b1 0.17\nMethod # Params (M) CVC-ColonDB ETIS CVC-T/ Ratio(%) S\u03b1 \u2191 E\u03d5 \u2191 F\u03c9\u03b2 \u2191 MAE \u2193 S\u03b1 \u2191 E\u03d5 \u2191 F\u03c9\u03b2 \u2191 MAE \u2193 S\u03b1 \u2191 E\u03d5 \u2191 F\u03c9\u03b2 \u2191 MAE \u2193 PraNet (Fan et al., 2020b) 32.55 / 100% 82.0 84.5 70.9 4.0 79.3 80.6 58.2 2.3 93.9 97.1 85.2 0.8 decoder-only 3.51 / 0.55% 76.7 80.7 59.5 5.2 67.9 71.4 41.0 7.4 86.4 88.4 67.5 2.3 BitFit 3.96 / 0.62% 83.8 \u00b1 0.25 86.8 \u00b1 0.38 72.7 \u00b1 0.19 3.9 \u00b1 0.12 84.7 \u00b1 0.37 87.1 \u00b1 0.73 67.4 \u00b1 1.37 1.7 \u00b1 0.13 91.5 \u00b1 0.55 94.2 \u00b1 0.18 81.3 \u00b1 1.86 1.4 \u00b1 0.12 Adapter 3.92 / 0.61% 83.6 \u00b1 0.24 86.3 \u00b1 0.32 71.7 \u00b1 0.37 3.6 \u00b1 0.24 85.3 \u00b1 0.64 86.9 \u00b1 0.26 67.0 \u00b1 0.98 1.8 \u00b1 0.26 92.9 \u00b1 0.19 94.6 \u00b1 0.56 84.1 \u00b1 0.76 1.2 \u00b1 0.15 VPT 4.00 / 0.62% 83.9 \u00b1 0.23 87.3 \u00b1 0.54 72.9 \u00b1 0.47 3.5 \u00b1 0.02 86.3 \u00b1 0.51 88.0 \u00b1 0.0 69.4 \u00b1 0.0 1.8 \u00b1 0.09 94.6 \u00b1 0.14 97.7 \u00b1 0.04 88.2 \u00b1 0.57 0.6 \u00b1 0.01 LST 11.49 / 1.77% 82.5 \u00b1 0.23 86.6 \u00b1 0.41 72.2 \u00b1 0.65 4.3 \u00b1 0.09 81.5 \u00b1 0.88 84.3 \u00b1 1.03 62.2 \u00b1 2.04 3.4 \u00b1 0.28 92.0 \u00b1 0.49 93.7 \u00b1 0.78 82.5 \u00b1 0.75 1.2 \u00b1 0.16 SAM-Adapter 3.98 / 0.62% 83.1 \u00b1 0.66 86.3 \u00b1 0.50 70.8 \u00b1 1.15 3.8 \u00b1 0.21 83.2 \u00b1 0.75 85.3 \u00b1 1.75 63.5 \u00b1 1.36 2.2 \u00b1 0.22 92.1 \u00b1 0.28 94.2 \u00b1 0.91 81.8 \u00b1 0.52 1.2 \u00b1 0.20 SSF 4.42 / 0.69% 83.9 \u00b1 0.15 86.9 \u00b1 0.33 72.1 \u00b1 0.72 3.9 \u00b1 0.10 84.7 \u00b1 0.42 87.4 \u00b1 0.48 66.7 \u00b1 0.65 1.7 \u00b1 0.05 92.1 \u00b1 0.07 93.9 \u00b1 0.66 83.6 \u00b1 0.24 1.4 \u00b1 0.23 LoRA 4.00 / 0.62% 84.4 \u00b1 0.26 87.2 \u00b1 0.04 73.5 \u00b1 0.67 4.1 \u00b1 0.35 85.5 \u00b1 0.59 86.5 \u00b1 0.83 68.4 \u00b1 1.48 1.8 \u00b1 0.37 93.5 \u00b1 0.33 95.9 \u00b1 0.54 85.7 \u00b1 1.33 0.9 \u00b1 0.17 Conv-LoRA 4.02 / 0.63% 84.7 \u00b1 0.69 88.0 \u00b1 1.08 75.3 \u00b1 0.89 3.4 \u00b1 0.10 87.1 \u00b1 1.70 88.8 \u00b1 1.43 71.6 \u00b1 3.87 1.5 \u00b1 0.43 93.7 \u00b1 0.18 96.5 \u00b1 0.15 87.2 \u00b1 0.12 0.9 \u00b1 0.18\nTable 5: Quantitative results for Polyp Segmentation.\nSkin Lesion Segmentation:\nCamouflaged Object Segmentation:\nShadow Detection:\nLeaf Segmentation:\nMethod # Params (M) / LeafRatio (%) IoU \u2191 Dice \u2191 Acc \u2191 DeepLabv3 (Chen et al., 2017b) 41.99 / 100% 62.3 74.1 94.0\ndecoder-only 3.51 / 0.55% 50.8 63.8 89.6\nBitFit 3.96 / 0.62% 71.4 \u00b1 1.15 81.7 \u00b1 1.01 95.5 \u00b1 0.30 Adapter 3.92 / 0.61% 72.1 \u00b1 0.47 82.4 \u00b1 0.36 95.3 \u00b1 0.18 VPT 4.00 / 0.62% 73.6 \u00b1 0.26 83.8 \u00b1 0.26 95.9 \u00b1 0.14 LST 11.49 / 1.77% 70.2 \u00b1 0.87 81.1 \u00b1 0.82 95.3 \u00b1 0.35 SAM-Adapter 3.98 / 0.62% 71.4 \u00b1 0.20 82.1 \u00b1 0.10 95.3 \u00b1 0.05 SSF 4.42 / 0.69% 71.5 \u00b1 0.63 81.8 \u00b1 0.44 95.5 \u00b1 0.26 LoRA 4.00 / 0.62% 73.7 \u00b1 0.20 83.6 \u00b1 0.13 95.7 \u00b1 0.07 Conv-LoRA 4.02 / 0.63% 74.5 \u00b1 0.39 84.3 \u00b1 0.34 96.0 \u00b1 0.07\nTable 9: Quantitative results for Leaf Segmentation.\nRoad Segmentation:\nMethod # Params (M) / RoadRatio (%) IoU \u2191 Dice \u2191 Acc \u2191 LinkNet34MTL(Batra et al., 2019) 22.00 / 100% 59.1 73.0 97.7\ndecoder-only 3.51 / 0.55% 48.6 65.1 96.4\nBitFit 3.96 / 0.62% 60.6 \u00b1 0.15 75.2 \u00b1 0.11 97.5 \u00b1 0.02 Adapter 3.92 / 0.61% 61.5 \u00b1 0.11 75.9 \u00b1 0.12 97.6 \u00b1 0.01 VPT 4.00 / 0.62% 60.2 \u00b1 1.87 74.9 \u00b1 1.50 97.4 \u00b1 0.22 LST 11.49 / 1.77% 60.2 \u00b1 0.26 74.9 \u00b1 0.22 97.5 \u00b1 0.01 SAM-Adapter 3.98 / 0.62% 60.6 \u00b1 0.06 75.2 \u00b1 0.04 97.5 \u00b1 0.01 SSF 4.42 / 0.69% 61.6 \u00b1 0.03 76.0 \u00b1 0.02 97.6 \u00b1 0.01 LoRA 4.00 / 0.62% 62.2 \u00b1 0.21 76.5 \u00b1 0.18 97.6 \u00b1 0.02 Conv-LoRA 4.02 / 0.63% 62.6 \u00b1 0.36 76.8 \u00b1 0.27 97.7 \u00b1 0.05\nTable 10: Quantitative results for Road Segmentation."
        },
        {
            "heading": "E ADDITIONAL ABLATION",
            "text": "The Rank of LoRA. The performance of SAM for twelve-class segmentation could be improved by introducing more extra parameters (table 11). However, this may disobey the rule of \u2018parameter efficiency\u2019 of PEFT. Exploring the design of a more efficient way for introducing \u2018classification prior\u2019 for SAM is a worthwhile endeavor for the future.\nCombined with Other PEFT Methods. We also evaluate the combination of Conv-LoRA and VPT on Poly Segmentation in table 12. This combination achieves superior or competitive performance, demonstrating the potential of introducing Conv-LoRA in addition to other PEFT methods. As a future work, such a combination might motivate techniques that further reduce the number of trainable parameters while ensuring the enhanced performance.\nMoE-Conv vs. Blocks with Various Kernel sizes. We further compare the design of our MoE-Conv and convolutional blocks with various kernel sizes. Convolutional blocks with various kernel sizes are commonly used in multi-scale feature fusion. Here we follow the Inception module proposed by (Szegedy et al., 2015), which consists of multiple kernel sizes with 1 \u00d7 1, 3 \u00d7 3, 5 \u00d7 5. To ensure a fair relative comparison, we opt for four experts in MoE-Conv to align with the branch number of the Inception structure. We conduct experiments on Leaf Dataset and ISIC 2017 Dataset twice for evaluation.\nIn table 13, MoE-Conv outperforms the design of convolutional blocks with various kernel sizes. We attribute this to convolutions with various kernel sizes only operate on the default feature scale. Maybe a larger kernel size can inject some approximate local prior in features of a smaller scale, but it probably can\u2019t bring local prior into features of larger scales than the default. The features in ViT are downscaled 16x from the original image, so injecting priors in larger scale features are generally more useful, as evidenced in table 4, where the optimal scale is usually larger than the default scale.\nComputational Cost. In table 14, we compare the training / inference speed and per epoch training time with the ISIC 2017 dataset and a single V100 GPU. While Conv-LoRA is slower in speed than others, it achieves robust performance gains across various semantic segmentation tasks (table 1). We find that the main cost comes from the upscale and downscale operations. A possible future direction is exploring how to inject local prior without explicitly scaling up and down features. We also notice that there are some other orthogonal works, like token merging (Liang et al., 2022; Bolya et al., 2022), that may accelerate Conv-LoRA."
        },
        {
            "heading": "F LOCAL PRIOR ANALYSIS",
            "text": "We elaborate on \u2018SAM\u2019s local prior assumption\u2019 mentioned in our abstract in section 4.3. Additionally, as our Conv-LoRA is designed to reinforce existing inductive biases within the features, we investigate the presence of these biases in the features.\nWe use two metrics: 1) Mean attention distance, which reflects the extent of local or global information that a self-attention layer is aggregating (Dosovitskiy et al., 2020; Raghu et al., 2021). We calculate the attention distance for each attention head, defined as the average distance between the position of the query patch and the locations to which it attends, weighted by the attention weights. Then we calculate the average attention distance for each layer by computing the mean across 500 randomly sampled images from downstream semantic segmentation tasks. 2) Relative log amplitudes (Park & Kim, 2022), which reflects whether the model tends to reduce or amplify high-frequency signals (e.g., edges, textures) in feature map. We compute the relative log amplitudes of the Fourier-transformed feature map in each layer and average them across layers. Then we calculate the mean of the relative log amplitudes over 100 randomly sampled segmentation images.\nInductive biases of the features. Extensive pre-training on large-scale datasets has been demonstrated to equip ViTs with inductive biases (Dosovitskiy et al., 2020; Raghu et al., 2021). SAM\u2019s large-scale segmentation pretraining further amplifies these inductive biases within the features.\nTo evaluate the inductive biases of the features, we conduct comprehensive comparisons among randomly initialized ViT, MAE pretrained ViT, and SAM ViT. We use mean attention distance and relative log amplitudes for evaluation. In fig. 7, as the training progresses from randomly initialized ViT to MAE ViT and ultimately to SAM ViT, we observe a consistent trend: the mean attention distance decreases, while the high-frequency signals in the features increase, indicating a focus on local information. These findings further confirm the presence of inductive biases in features following large-scale pre-training. Furthermore, the findings affirm the rationality of ConvLoRA in reinforcing the inherent inductive biases within the features."
        },
        {
            "heading": "G MOE ANALYSIS",
            "text": "What does MoE learn? MoE learns to dynamically select an appropriate scale for injecting local priors based on input features. To illuminate its functionality, we conduct an analysis by\ntracking the frequency of each expert\u2019s selection across different datasets during inference. The detailed results are presented in fig. 8.\nNotably, distinct datasets exhibit preferences for different experts. For instance, in Leaf Segmentation, MoE tends to favor experts with upsampling ratios of 3 and 4, while on the ISIC 2017 dataset, it tends to select the expert with an upsampling ratio of 2. Connecting these insights with the optimal scale ablation results in table 4, it become evident that MoE selects the expert adaptively for each sample, and consequently, the distributions of MoE selection across datasets reflect their different data distributions. This observation supports that MoE behaves adaptively and effectively with respect to each sample\u2019s property. This adaptability reinforces the significance of MoE in tailoring its selection to the unique characteristics of diverse datasets, enhancing its effectiveness in local prior injection."
        },
        {
            "heading": "H LOW-DATA REGIME EXPERIMENTS",
            "text": "Convolutional layers could introduce inductive biases, thereby enhancing data efficiency during finetuning. Therefore, we undertake experiments in a low-data regime to validate whether Conv-LoRA can indeed improve data efficiency in semantic segmentation.\nSpecifically, we conduct experiments under the few-shot setting, wherein acquiring data for downstream tasks is challenging, and only a limited number of training samples per task are available. Our experiments are performed on the Trans10K-v2 dataset, which is used for twelve-class transparent object segmentation. We randomly select the training samples, to meet the settings of 1, 2, 4, 8, and 16 shots (i.e., the number of labeled training examples per class), and the experiments are run for 100 epochs.\nIn table 15, the performance improvement of Conv-LoRA compared to LoRA, is particularly pronounced in an extremely low-data setting (e.g., 1-shot). The result demenstrates that the introduction of inductive biases in Conv-LoRA contribute to improved data efficiency."
        },
        {
            "heading": "I MORE VISUALIZATION RESULTS",
            "text": "Conv-LoRA vs. LoRA Feature. We visualize the feature maps from the fine-tuned SAM\u2019s image encoder, which incorporates LoRA and our Conv-LoRA respectively. In fig. 9, the image features from SAM\u2019s image encoder equipped with Conv-LoRA could provide more fine-grained information, e.g., slim edges, which is beneficial to later mask prediction. This further demonstrates the effectiveness of reinforcing the image-related local priors with network architecture.\nMask prediction. We also provide more visualization results for the mask prediction across various datasets when applying different PEFT methods (VPT, LoRA and Conv-LoRA). This further confirms the superiority of our Conv-LoRA."
        }
    ],
    "title": "CONVOLUTION MEETS LORA: PARAMETER EFFI-",
    "year": 2024
}