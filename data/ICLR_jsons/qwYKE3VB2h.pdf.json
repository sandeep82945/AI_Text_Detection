{
    "abstractText": "We study the implications of the modeling choice to use a graph, instead of a hypergraph, to represent real-world interconnected systems whose constituent relationships are of higher order by nature. Such a modeling choice typically involves an underlying projection process that maps the original hypergraph onto a graph, and is common in graph-based analysis. While hypergraph projection can potentially lead to loss of higher-order relations, there exists very limited studies on the consequences of doing so, as well as its remediation. This work fills this gap by doing two things: (1) we develop analysis based on graph and set theory, showing two ubiquitous patterns of hyperedges that are root to structural information loss in all hypergraph projections; we also quantify the combinatorial impossibility of recovering the lost higher-order structures if no extra help is provided; (2) we still seek to recover the lost higher-order structures in hypergraph projection, and in light of (1)\u2019s findings we propose to relax the problem into a learning-based setting. Under this setting, we develop a learning-based hypergraph reconstruction method based on an important statistic of hyperedge distributions that we find. Our reconstruction method is evaluated on 8 real-world datasets under different settings, and exhibits consistently good performance. We also demonstrate benefits of the reconstructed hypergraphs via use cases of protein rankings and link predictions.",
    "authors": [],
    "id": "SP:4e67a972979bbca9e42a8231ee71f8193891d38c",
    "references": [
        {
            "authors": [
                "Austin R Benson",
                "Rediet Abebe",
                "Michael T Schaub",
                "Ali Jadbabaie",
                "Jon Kleinberg"
            ],
            "title": "Simplicial closure and higher-order link prediction",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2018
        },
        {
            "authors": [
                "Austin R Benson",
                "Ravi Kumar",
                "Andrew Tomkins"
            ],
            "title": "Sequences of sets",
            "venue": "In Proceedings of the 24th ACM SIGKDD International Conference,",
            "year": 2018
        },
        {
            "authors": [
                "Claude Berge"
            ],
            "title": "Graphs and hypergraphs",
            "year": 1973
        },
        {
            "authors": [
                "Claude Berge",
                "Pierre Duchet"
            ],
            "title": "A generalization of gilmore\u2019s theorem",
            "venue": "Recent advances in graph theory, pp",
            "year": 1975
        },
        {
            "authors": [
                "Sonja Blasche",
                "Manfred Koegl"
            ],
            "title": "Analysis of protein\u2013protein interactions using lumier assays",
            "venue": "In Virus-Host Interactions,",
            "year": 2013
        },
        {
            "authors": [
                "Coen Bron",
                "Joep Kerbosch"
            ],
            "title": "Algorithm 457: finding all cliques of an undirected graph",
            "venue": "Communications of the ACM,",
            "year": 1973
        },
        {
            "authors": [
                "Andries E Brouwer",
                "CF Mills",
                "WH Mills",
                "A Verbeek"
            ],
            "title": "Counting families of mutually intersecting sets. the electronic journal of combinatorics",
            "venue": "pp. P8\u2013P8,",
            "year": 2013
        },
        {
            "authors": [
                "Anna Br\u00fcckner",
                "C\u00e9cile Polge",
                "Nicolas Lentze",
                "Daniel Auerbach",
                "Uwe Schlattner"
            ],
            "title": "Yeast twohybrid, a powerful tool for systems biology",
            "venue": "International journal of molecular sciences,",
            "year": 2009
        },
        {
            "authors": [
                "Pierre Colomb",
                "Lhouari Nourine"
            ],
            "title": "About keys of formal context and conformal hypergraph",
            "venue": "In Formal Concept Analysis: 6th International Conference,",
            "year": 2008
        },
        {
            "authors": [
                "Alessio Conte",
                "Roberto Grossi",
                "Andrea Marino"
            ],
            "title": "Clique covering of large real-world networks",
            "venue": "In 31st ACM Symposium on Applied Computing,",
            "year": 2016
        },
        {
            "authors": [
                "Michele Coscia",
                "Giulio Rossetti",
                "Fosca Giannotti",
                "Dino Pedreschi"
            ],
            "title": "Demon: a local-first discovery method for overlapping communities",
            "venue": "In Proceedings of the 18th ACM SIGKDD international conference,",
            "year": 2012
        },
        {
            "authors": [
                "David Croft",
                "Gavin O\u2019kelly",
                "Guanming Wu",
                "Robin Haw",
                "Marc Gillespie",
                "Lisa Matthews",
                "Michael Caudy",
                "Phani Garapati",
                "Gopal Gopinath",
                "Bijay Jassal"
            ],
            "title": "Reactome: a database of reactions, pathways and biological processes",
            "venue": "Nucleic acids research,",
            "year": 2010
        },
        {
            "authors": [
                "Sicheng Dai",
                "H\u00e9l\u00e8ne Bouchet",
                "Aur\u00e9lie Nardy",
                "Eric Fleury",
                "Jean-Pierre Chevrot",
                "M\u00e1rton Karsai"
            ],
            "title": "Temporal social network reconstruction using wireless proximity sensors: model selection and consequences",
            "venue": "EPJ Data Science,",
            "year": 2020
        },
        {
            "authors": [
                "Yihe Dong",
                "Will Sawin",
                "Yoshua Bengio"
            ],
            "title": "Hnhn: Hypergraph networks with hyperedge neurons",
            "venue": "arXiv preprint arXiv:2006.12278,",
            "year": 2020
        },
        {
            "authors": [
                "Kevin Drew",
                "Chanjae Lee",
                "Ryan L Huizar",
                "Fan Tu",
                "Blake Borgeson",
                "Claire D McWhite",
                "Yun Ma",
                "John B Wallingford",
                "Edward M Marcotte"
            ],
            "title": "Integration of over 9,000 mass spectrometry experiments builds a global map of human protein complexes",
            "venue": "Molecular systems biology,",
            "year": 2017
        },
        {
            "authors": [
                "David Eppstein",
                "Maarten L\u00f6ffler",
                "Darren Strash"
            ],
            "title": "Listing all maximal cliques in sparse graphs in near-optimal time",
            "venue": "In Algorithms and Computation: 21st International Symposium, ISAAC 2010, Jeju Island,",
            "year": 2010
        },
        {
            "authors": [
                "Paul Erd\u00f6s",
                "Adolph W Goodman",
                "Louis P\u00f3sa"
            ],
            "title": "The representation of a graph by set intersections",
            "venue": "Canadian Journal of Mathematics,",
            "year": 1966
        },
        {
            "authors": [
                "Ernesto Estrada"
            ],
            "title": "Virtual identification of essential proteins within the protein interaction network of yeast. Proteomics",
            "year": 2006
        },
        {
            "authors": [
                "Jacob Fox",
                "Tim Roughgarden",
                "C Seshadhri",
                "Fan Wei",
                "Nicole Wein"
            ],
            "title": "Finding cliques in social networks: A new distribution-free model",
            "venue": "SIAM journal on computing,",
            "year": 2020
        },
        {
            "authors": [
                "Curtis Greene",
                "Daniel J Kleitman"
            ],
            "title": "Strong versions of sperner\u2019s theorem",
            "venue": "Journal of Combinatorial Theory, Series A,",
            "year": 1976
        },
        {
            "authors": [
                "Keith Henderson",
                "Brian Gallagher",
                "Tina Eliassi-Rad",
                "Hanghang Tong",
                "Sugato Basu",
                "Leman Akoglu",
                "Danai Koutra",
                "Christos Faloutsos",
                "Lei Li"
            ],
            "title": "Rolx: structural role extraction & mining in large graphs",
            "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2012
        },
        {
            "authors": [
                "Ren\u00e9e V Hoch",
                "Philippe Soriano"
            ],
            "title": "Generating diversity and specificity through developmental cell signaling",
            "venue": "In Principles of Developmental Genetics,",
            "year": 2015
        },
        {
            "authors": [
                "Weihua Hu",
                "Matthias Fey",
                "Marinka Zitnik",
                "Yuxiao Dong",
                "Hongyu Ren",
                "Bowen Liu",
                "Michele Catasta",
                "Jure Leskovec"
            ],
            "title": "Open graph benchmark: Datasets for machine learning on graphs",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Massih khorvash"
            ],
            "title": "On uniform sampling of cliques",
            "venue": "PhD thesis, University of British Columbia,",
            "year": 2009
        },
        {
            "authors": [
                "Florian Klimm",
                "Charlotte M Deane",
                "Gesine Reinert"
            ],
            "title": "Hypergraphs for predicting essential genes using multiprotein complex data",
            "venue": "Journal of Complex Networks,",
            "year": 2021
        },
        {
            "authors": [
                "Yunbum Kook",
                "Jihoon Ko",
                "Kijung Shin"
            ],
            "title": "Evolution of real-world hypergraphs: Patterns and models without oracles",
            "venue": "IEEE International Conference on Data Mining (ICDM),",
            "year": 2020
        },
        {
            "authors": [
                "Tarun Kumar",
                "K Darwin",
                "Srinivasan Parthasarathy",
                "Balaraman Ravindran"
            ],
            "title": "Hpra: Hyperedge prediction using resource allocation",
            "venue": "In Proceedings of the 12th ACM Conference on Web Science,",
            "year": 2020
        },
        {
            "authors": [
                "Tarun Kumar",
                "Sankaran Vaidyanathan",
                "Harini Ananthapadmanabhan",
                "Srinivasan Parthasarathy",
                "Balaraman Ravindran"
            ],
            "title": "Hypergraph clustering by iteratively reweighted modularity maximization",
            "venue": "Applied Network Science,",
            "year": 2020
        },
        {
            "authors": [
                "Geon Lee",
                "Jaemin Yoo",
                "Kijung Shin"
            ],
            "title": "Mining of real-world hypergraphs: Patterns, tools, and generators",
            "venue": "In Proceedings of the 31st ACM International Conference on Information & Knowledge Management,",
            "year": 2022
        },
        {
            "authors": [
                "Jure Leskovec",
                "Jon Kleinberg",
                "Christos Faloutsos"
            ],
            "title": "Graphs over time: densification laws, shrinking diameters and possible explanations",
            "venue": "In SIGKDD,",
            "year": 2005
        },
        {
            "authors": [
                "Pan Li",
                "Yanbang Wang",
                "Hongwei Wang",
                "Jure Leskovec"
            ],
            "title": "Distance encoding: Design provably more powerful neural networks for graph representation learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaoli Li",
                "Min Wu",
                "Chee-Keong Kwoh",
                "See-Kiong Ng"
            ],
            "title": "Computational approaches for detecting protein complexes from protein interaction networks: a survey",
            "venue": "BMC genomics,",
            "year": 2010
        },
        {
            "authors": [
                "Anmol Madan",
                "Manuel Cebrian",
                "Sai Moturu",
                "Katayoun Farrahi"
            ],
            "title": "Sensing the\" health state\" of a community",
            "venue": "IEEE Pervasive Computing,",
            "year": 2011
        },
        {
            "authors": [
                "Ron Milo",
                "Shai Shen-Orr",
                "Shalev Itzkovitz",
                "Nadav Kashtan",
                "Dmitri Chklovskii",
                "Uri Alon"
            ],
            "title": "Network motifs: simple building blocks of complex",
            "year": 2002
        },
        {
            "authors": [
                "John Mullane",
                "Ba-Ngu Vo",
                "Martin D Adams",
                "Ba-Tuong Vo"
            ],
            "title": "A random-finite-set approach to bayesian slam",
            "venue": "IEEE transactions on robotics,",
            "year": 2011
        },
        {
            "authors": [
                "Mark EJ Newman"
            ],
            "title": "Coauthorship networks and patterns of scientific collaboration",
            "venue": "Proceedings of the national academy of sciences,",
            "year": 2004
        },
        {
            "authors": [
                "Laura Ozella",
                "Daniela Paolotti",
                "Guilherme Lichand",
                "Jorge P Rodr\u00edguez",
                "Simon Haenni",
                "John Phuka",
                "Onicio B Leal-Neto",
                "Ciro Cattuto"
            ],
            "title": "Using wearable proximity sensors to characterize social contact patterns in a village of rural malawi",
            "venue": "EPJ Data Science,",
            "year": 2021
        },
        {
            "authors": [
                "Gergely Palla",
                "Imre Der\u00e9nyi",
                "Ill\u00e9s Farkas",
                "Tam\u00e1s Vicsek"
            ],
            "title": "Uncovering the overlapping community structure of complex networks in nature and society",
            "year": 2005
        },
        {
            "authors": [
                "Xinyu Que",
                "Fabio Checconi",
                "Fabrizio Petrini",
                "John A Gunnels"
            ],
            "title": "Scalable community detection with the louvain algorithm",
            "venue": "In 2015 IEEE IPDPS",
            "year": 2015
        },
        {
            "authors": [
                "Emad Ramadan",
                "Arijit Tarafdar",
                "Alex Pothen"
            ],
            "title": "A hypergraph model for the yeast protein complex network",
            "venue": "In 18th International Parallel and Distributed Processing Symposium,",
            "year": 2004
        },
        {
            "authors": [
                "Guillaume Rigaut",
                "Anna Shevchenko",
                "Berthold Rutz",
                "Matthias Wilm",
                "Matthias Mann",
                "Bertrand S\u00e9raphin"
            ],
            "title": "A generic protein purification method for protein complex characterization and proteome exploration",
            "venue": "Nature biotechnology,",
            "year": 1999
        },
        {
            "authors": [
                "Emre Sarig\u00f6l",
                "Ren\u00e9 Pfitzner",
                "Ingo Scholtes",
                "Antonios Garas",
                "Frank Schweitzer"
            ],
            "title": "Predicting scientific success based on coauthorship networks",
            "venue": "EPJ Data Science,",
            "year": 2014
        },
        {
            "authors": [
                "Arnab Sinha",
                "Zhihong Shen",
                "Yang Song",
                "Hao Ma",
                "Darrin Eide",
                "Bo-June Hsu",
                "Kuansan Wang"
            ],
            "title": "An overview of microsoft academic service (mas) and applications",
            "venue": "In Proceedings of the 24th international conference on world wide web,",
            "year": 2015
        },
        {
            "authors": [
                "Victor Spirin",
                "Leonid A Mirny"
            ],
            "title": "Protein complexes and functional modules in molecular networks",
            "venue": "Proceedings of the national Academy of sciences,",
            "year": 2003
        },
        {
            "authors": [
                "Etsuji Tomita",
                "Akira Tanaka",
                "Haruhisa Takahashi"
            ],
            "title": "The worst-case time complexity for generating all maximal cliques and computational experiments",
            "venue": "Theoretical computer science,",
            "year": 2006
        },
        {
            "authors": [
                "Leo Torres",
                "Ann S Blevins",
                "Danielle Bassett",
                "Tina Eliassi-Rad"
            ],
            "title": "The why, how, and when of representations for complex systems",
            "venue": "SIAM Review,",
            "year": 2021
        },
        {
            "authors": [
                "Vincent A Traag",
                "Ludo Waltman",
                "Nees Jan Van Eck"
            ],
            "title": "From louvain to leiden: guaranteeing well-connected communities",
            "venue": "Scientific reports,",
            "year": 2019
        },
        {
            "authors": [
                "Michael M Wolf",
                "Alicia M Klinvex",
                "Daniel M Dunlavy"
            ],
            "title": "Advantages to modeling relational data using hypergraphs versus graphs",
            "venue": "IEEE High Performance Extreme Computing Conference (HPEC),",
            "year": 2016
        },
        {
            "authors": [
                "Qianghua Xiao",
                "Jianxin Wang",
                "Xiaoqing Peng",
                "Fang-xiang Wu",
                "Yi Pan"
            ],
            "title": "Identifying essential proteins from active ppi networks constructed with dynamic gene expression",
            "venue": "In BMC genomics,",
            "year": 2015
        },
        {
            "authors": [
                "Keyulu Xu",
                "Weihua Hu",
                "Jure Leskovec",
                "Stefanie Jegelka"
            ],
            "title": "How powerful are graph neural networks",
            "venue": "arXiv preprint arXiv:1810.00826,",
            "year": 2018
        },
        {
            "authors": [
                "Ye Xu",
                "Dan Rockmore",
                "Adam M. Kleinbaum"
            ],
            "title": "Hyperlink prediction in hypernetworks using latent social features",
            "venue": "In Discovery Science,",
            "year": 2013
        },
        {
            "authors": [
                "Naganand Yadati",
                "Vikram Nitin",
                "Madhav Nimishakavi",
                "Prateek Yadav",
                "Anand Louis",
                "Partha Talukdar"
            ],
            "title": "NHP: Neural Hypergraph Link Prediction, pp. 1705\u20131714",
            "year": 2020
        },
        {
            "authors": [
                "Se-eun Yoon",
                "Hyungseok Song",
                "Kijung Shin",
                "Yung Yi"
            ],
            "title": "How much and when do we need higher-order information in hypergraphs? a case study on hyperedge prediction",
            "venue": "In Proceedings of The Web Conference",
            "year": 2020
        },
        {
            "authors": [
                "Jean-Gabriel Young",
                "Giovanni Petri",
                "Tiago P Peixoto"
            ],
            "title": "Hypergraph reconstruction from network data",
            "venue": "Communications Physics,",
            "year": 2021
        },
        {
            "authors": [
                "KH Young"
            ],
            "title": "Yeast two-hybrid: so many interactions,(in) so little time",
            "venue": "Biology of reproduction,",
            "year": 1998
        },
        {
            "authors": [
                "Yang Yu",
                "Dezhou Kong"
            ],
            "title": "Protein complexes detection based on node local properties and gene expression in ppi weighted networks",
            "venue": "BMC bioinformatics,",
            "year": 2022
        },
        {
            "authors": [
                "Muhan Zhang",
                "Zhicheng Cui",
                "Shali Jiang",
                "Yixin Chen"
            ],
            "title": "Beyond link prediction: Predicting hyperlinks in adjacency space",
            "venue": "In AAAI",
            "year": 2018
        },
        {
            "authors": [
                "Ruochi Zhang",
                "Yuesong Zou",
                "Jian Ma"
            ],
            "title": "Hyper-sagnn: a self-attention based graph neural network for hypergraphs",
            "venue": "ICLR",
            "year": 2019
        },
        {
            "authors": [
                "Dengyong Zhou",
                "Jiayuan Huang",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Learning with hypergraphs: Clustering, classification, and embedding",
            "venue": "Advances in neural information processing systems,",
            "year": 2006
        },
        {
            "authors": [
                "Torres"
            ],
            "title": "2021) provides a unified overview of various representations for complex systems: it notes the mathematical relationship between graphs and hypergraphs, warning against unthoughtful usage of graphs to model higher-order relationships",
            "year": 2020
        },
        {
            "authors": [
                "Yoon"
            ],
            "title": "2020) further empirically compares the performance of various methods in the hyperedge prediction task when the methods are executed on different lower-order approximations of hypergraphs, including order-2 approximation (clique expansion), order-3 approximations (3-uniform projected hypergraphs), etc.; their experiments show that lower-order approximations especially struggle on more complex datasets or more challenging versions of tasks",
            "year": 2020
        },
        {
            "authors": [
                "Zhang"
            ],
            "title": "Hyperedge Prediction is to identify missing hyperedges of an incomplete hypergraph from a pool of given candidates. Existing works focus on characterizing a node set\u2019s structural features. The methods span proximity measures Benson et al. (2018a), deep learning",
            "year": 2018
        },
        {
            "authors": [
                "Fox"
            ],
            "title": "2020) shows that any (weakly) c-closed graph on",
            "year": 2020
        },
        {
            "authors": [
                "Benson"
            ],
            "title": "features\u2019 average rankings, shown in Fig.7. More imporant features have smaller ranking numbers. Interestingly we found that the most important feature has a very concrete physical meaning: it indicates whether each edge of the clique has existed in at least two maximal cliques of the projected graph",
            "year": 2018
        },
        {
            "authors": [
                "on H"
            ],
            "title": "For hyperedge prediction, we ask that they cannot rely on hypergraphs for prediction, and can only use the projection. Based on that we use the two recent SOTAs",
            "venue": "We implemented the best heuristic in Conte et al",
            "year": 2018
        },
        {
            "authors": [
                "Lee"
            ],
            "title": "2022), we further compare some of the advanced structural properties between the original hypergraphs and the reconstructed hypergraphs. The advanced structural properties include simplicial closure (Benson et",
            "year": 2018
        },
        {
            "authors": [
                "Kumar"
            ],
            "title": "2020b), node-degree preserving projection is a novel projection method proposed in recent years that can preserve node degrees in the original hypergraph. It innovatively achieves this by scaling down the weight of each projected edge by a factor of (\u2223E\u2223 \u2212 1), where \u2223E\u2223 is the size of the corresponding hyperedge",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Graphs are an abstraction of many real-world complex systems, recording pairs of related entities by nodes and edges. Hypergraphs further this idea by extending edges from node pairs to node sets of arbitrary sizes, admitting a more expressive form of encoding for higher-order relationships. In graph-based analysis, a fundamental and enduring issue exists with the modeling choice of using a graph, instead of a hypergraph, to represent complex systems whose constituent relationships are of higher order by nature. For example, coauthorship networks and social networks, as popular subjects in graph-based analysis, often use edges to denote two-author collaborations or two-person conversations, respectively. Protein interaction networks do the same for protein co-occurrence in biological processes. However, coauthorships, conversations, and biological processes all typically involve more than just two authors, people, and proteins. Reviewing more concrete cases in previous research, we find that this issue usually arises in one of the following two scenarios: \u2022 \u201cUnobservable\u201d: In some key scenarios, the most available technology for data collection can\nonly detect pairwise relations, as is common in social science and biological science. For example, in the study of social interactions (Madan et al., 2011; Ozella et al., 2021; Dai et al., 2020), sensing methodologies to record physical proximity can be used to build networks of face-to-face interaction: an interaction between two people is recorded by thresholding distances between their body-worn sensors. There is no way to directly record the multiple participants in each conversation by sensors only. In the study of protein interactions (Li et al., 2010; Yu & Kong, 2022; Spirin & Mirny, 2003), detecting protein components of a multiprotein complex all at once poses way more technical barriers than identifying pairs of interacting proteins. Methods for the latter are often regarded as being more economic, high-throughput, reliable, and thus more commonly used.\n\u2022 \u201cUnpublished\u201d: Even when they are technically observable, in practice the source hypergraph datasets of many studies are never released. For example, many influential studies analyzing coauthorships do not make available a hypergraph version of their dataset (Newman, 2004; Sarig\u00f6l et al., 2014). Many graph benchmarks, including arXiv-hepth (Leskovec et al., 2005), ogbl-collab (Hu et al., 2020), and ogbn-product(Hu et al., 2020), also do not provide their hypergraph originals. Yet the underlying, unobserved hypergraphs contain important information about the domain.\nIn both cases above, there is often an underlying hypergraph projection process that maps the original hypergraph onto a graph on the same set of nodes, and that each hyperedge in the hypergraph is mapped to a clique (i.e. a complete subset where all nodes are pairwise connected) in the graph. In other words, two nodes are connected in the projected graph iff they coexist within a hyperedge.\nWhile it\u2019s easy to perceive the loss of some higher-order relations during hypergraph projection, to this date we still have many crucial unanswered questions regarding this issue\u2019s detailed cause, consequences, and potential remediation: (Q1) what connection patterns of hyperedges in the original hypergraph are combinatorically impossible to recover after the projection? (Q2) What are the theoretical worst cases that these connection patterns can create, and how frequent do they occur in real-world hypergraph datasets? (Q3) Given a projected graph, is it possible to reconstruct a hypergraph out of it if some reasonable extra help is allowed, and if so, how? (Q4) How might the reconstructed hypergraph offer advantages over the projected graph?\nHypergraph reconsruction. We note that all the questions above essentially point to a common problem structure which is the reversal of the hypergraph projection process: there is an underlying hypergraph H1 that we can\u2019t directly observe; instead we can only access its projected graph (or projection) G1. Our goal is to reconstruct H1 as accurately as possible from G1. The first two questions above assume no extra help (input) should be given in the reconstruction, other than the projected graph G1 itself. The latter two questions permit some extra input, which we will elaborate in Sec.4.\nFor broad applicability, we assume no multiplicities for G1\u2019s edges: they just say whether two nodes co-exist in at least one hyperedge. Appx. F.9 addresses the simpler case with edge multiplicities.\nPrevious work. Very limited work investigated implications of hypergraph projection or its reversal (i.e. hypergraph reconstruction). For implications, the only work to our knowledge is Wolf et al. (2016), which compares hypergraph and its projected graph for computational efficiency on spectral clustering; the former was found to be more efficient. For reconstruction, Young et al. (2021) is by far the closest, which however studies how to use least number of cliques to cover the projected graph, whose principle does not really apply to real-world hypergraphs (see experiment in Sec.5). In graph mining, two relevant problems are hyperedge prediction and community detection, yet both are still vastly different in setup. For hyperedge prediction (Benson et al., 2018a;b; Yadati et al., 2020; Xu et al., 2013), its input is a hypergraph, rather than a projected graph. Methods for hyperedge prediction also only identify hyperedges from a given set of candidates, rather than the large implicit spaces of all possible hyperedges. Community detection, on the other hand, looks for densely connected regions of a graph under various definitions, but not for hyperedges. Both tasks are very different from the goal of searching and inferring hyperedges over the projected edges. We include discussion of other related work in Appendix C. Contributions. Our work has the following contributions:s (1) We propose and pioneer the exploration on the crucial problem of hypergraph projection and its reconstruction. (2) We conduct theoretical analysis, showing two critical patterns of hyperedges that cause information loss in all hypergrpah projections, and derive their error bound. (3) We propose a new paradigm of learning-based hypergraph reconstruction, and design a novel method to effectively reconstruct hypergraphs."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "Hypergraph. A hypergraph H is a tuple (V,E): V is a set of nodes, and E = {E1,E2, ...,Em} is a set of sets with Ei \u2286 V for all 1 \u2264 i \u2264 m. For the purpose of reconstruction, we assume the hyperedges are distinct, i.e. \u22001 \u2264 i, j \u2264m, Ei \u2260 Ej . Projected Graph. H\u2019s projected graph (i.e. projection, clique expansion), G, is a graph with the same node set V , and (undirected) edge set E \u2032, i.e. G = (V,E \u2032), where two nodes are joined by an edge in E \u2032 iff they belong to a common hyperedge in E . That is, E \u2032 = {(vi, vj)\u2223vi, vj \u2208 E,E \u2208 E}. Maximal Cliques. A clique C is a fully connected subgraph. We use C to also denote the set of nodes in the clique. A maximal clique is a clique that cannot become larger by including more nodes.\nDataset \u2223E \u2223 \u2223E \u2032\u2223 \u2223M\u2223 Err.I Err.II DBLP (Benson et al., 2018a) 197,067 194,598 166,571 2.02% 18.9% Enron (Benson et al., 2018a) 756 300 362 42.5% 53.3% Foursquare (Young et al., 2021) 1,019 874 8,135 1.74% 88.6% HostsVirus(Young et al., 2021) 218 126 361 19.5% 58.1% H. School (Benson et al., 2018a) 3,909 2864 3,279 14.9% 82.7%\nTable 1: E is the set of hyperedges; E \u2032 is the set of hyperedges not nested in any other hyperedges; M is the set of maximal cliques in G. Error I, II result from the violation of Conditions I, II, respectively. Error I = \u2223E/E \u2032 \u2223\n\u2223E\u222aM\u2223 ,\nError II = \u2223M/E \u2032 \u2223+\u2223E\u2032/M\u2223 \u2223E\u222aM\u2223 . Errors caused by violation of both conditions are counted as Error I.\nThe maximal clique algorithm returns all maximal cliquesM in a graph, and its time complexity is linear to \u2223M\u2223 (Tomita et al., 2006). A maximum clique is the largest maximal clique in a graph."
        },
        {
            "heading": "3 ANALYSIS OF HYPERGRAPH PROJECTION AND RECONSTRUCTION",
            "text": "We first analyze hypergraph projection and its reversal from the lens of graph theory and set theory, addressing (Q1) and (Q2) raised above.\nHyperedge patterns that are hard to recover after projection (Q1) In principle, any clique in a projection can be a true hyperedge. Therefore, toward perfect reconstruction we should consider U , the universe of all cliques in G, including single nodes. To enumerate U , a helpful view is the union of all maximal clique\u2019s power set: U = \u22c3C\u2208MP(C) \u2216 \u2205. In that sense, maximal clique algorithm is a critical first step for hypergraph reconstruction, as also applied by Young et al. (2021) to initialize its MCMC solver. In the extreme case, it\u2019s easy to see that if H\u2019s hyperedges are all disjoint, G\u2019s maximal cliques would be exactlyH\u2019s hyperedges. It is impossible to find all hyperedges without finding all maximal cliques. Therefore, we consider the reconstruction accuracy of maximal clique algorithm a good measure of the reconstruction\u2019s difficulty. Theorem 1. The maximal cliques of G are exactly all hyperedges ofH, i.e.M = E , if and only if the following two conditions hold:\nI. for every hyperedge E \u2208 E there does not exist a hyperedge E\u2032 \u2208 E s.t. E \u2282 E\u2032; II. every maximal clique in G is a hyperedge inH, i.e.M \u2286 E .\nTheorem 1 gives the two necessary and sufficient conditions that characterize whenH is \u201ceasy\u201d to reconstruct. Note that Condition I is the famous Sperner property (Greene & Kleitman, 1976), and Condition II is the definition of \u201cconformal\u201d as in Colomb & Nourine (2008). In terms of their implications on hyperedge patterns, Condition I is quite self-explanatory, which simply forbids the pattern of \u201cnested\u201d hyperedges. In comparison, Condition II is much less intelligible. Our following theorem further interprets Condition II by the hyperedge pattern of \"uncovered triangle\". Theorem 2. A hypergraphH = (V,E) is conformal iff for every three hyperedges there always exists some hyperedge E such that all pairwise intersections of the three hyperedges are subsets of E, i.e.: \u2200Ei,Ej ,Eq \u2208 E , \u2203 E \u2208 E , s.t. (Ei \u2229Ej) \u222a (Ej \u2229Eq) \u222a (Eq \u2229Ei) \u2286 E Theorem 2 shows that for a hypergraph to satisfy Condition II, it can\u2019t allow any three hyperedges in itself to form a \u201ctriangle\u201d whose three vertices are \u201cuncovered\u201d. Intuitively, the triangle in this pattern would induce a 3-clique among the \"vertices\" of the triangle, which would not be part of a hyperedge if the triangle is uncovered. The value of Theorem 2 is that it gives a nontrivial interpretation of Condition II in Theorem 1 by showing how to check a hypergraph\u2019s conformity just based on its hyperedge patterns, without computing any maximal cliques. Based on the two conditions, we can now further define the two types of errors made by maximal cliques. Definition 1. Every error made due to treating all maximal cliques in G as true hyperedges inH can be attributed toH\u2019s violation of at least one of the conditions in Theorem 1. An error is defined to be Error I (Error II) if it is caused by theH\u2019s violation of Condition I (Condition II).\nFig.2 illustrates the two error-triggering hyperedge patterns (i.e. \u201cerror patterns\u201d for brevity hereafter) corresponding to the two errors, as well as their relationship to other important concepts. Also note that here the Errors I & II are different from (but related to) the well-known Type I (false positive) and Type II (false negative) errors in statistics. See Appx. B.5 for more discussion. Empirical frequency of the hyperedge patterns, and their theoretical worst case (Q2) Both error patterns in Fig.2 have simple construct, so it is perceivable that they are common in real-world hypergraphs. Table 1shows the frequency of the error patterns and their resulted error rate in reconstruction. We see that Error I patterns are common in hypergraphs of emails and social interactions. In the worst case, a hypergraph contains one large hyperedge and many nested hyperedges as proper subsets. That said, one can argue that there may be many real-world hypergraphs that (almost) satisfy Condition I. It turns out that the Error II patterns caused by violating Condition II is also disastrous.\nTheorem 3. Let H = (V,E) be a hypergraph that only satisfies Condition I in Theorem 1, with m = \u2223E \u2223. Denote by p(H) the accuracy of maximal clique algorithm for reconstructingH. Then,\nminH p(H) \u2264 2\u2212( m\u22121 [m/2]\u22121) \u226a 2\u2212m\nTheorem 3 shows that our Error II pattern can also give rise to (super-)exponentially many hyperedges that are almost impossible to be combinatorically recovered, if we only rely on the projected graph."
        },
        {
            "heading": "4 LEARNING-BASED HYPERGRAPH RECONSTRUCTION",
            "text": "Overview. This section will introduce a new learning-based hypergraph reconstruction paradigm, including the problem setup and the proposed method. The idea is that in addition to the projected graph as input, we also assume to know a hypergraph (or part of it) from the same distribution as the reconstruction target. Sec.4.1 will formulate the problem and explain why this setup is meaningful in practice. Sec.4.2.1 - 4.2.3 will present details of the reconstruction method."
        },
        {
            "heading": "4.1 PROBLEM DESCRIPTION",
            "text": "We\u2019ve identified the inherent difficulty of reconstructing a hypergraph from its projection without additional information. However, as presented at the beginning of this paper, in practice, there are many cases where it would be highly desirable if we can recover the lost higher-order relations from projection. How do we reconcile this theoretical \u201cimpossibility\u201d with practical necessities? The key observations are two. First, the noted \u201cimpossibility\u201d pertains specifically to the aim of flawlessly reconstructing a hypergraph solely based on its projection, especially in theoretical worst case. However, this doesn\u2019t preclude our option to approximate a hypergraph within a specific application domain, particularly if we have insights about the domain\u2019s typical hypergraph structures. Secondly, in the context of a specific application domain with one or more projected graphs to be reconstructed, collecting just a single \"hypergraph sample\" (or even a portion of it) within the domain can be immensely beneficial. This sample aids in understanding the typical structure and patterns of hypergraphs in that particular domain. For instance, to reconstruct the coauthorship\nhypergraphs from graphs claimed to be built from an earlier DBLP dataset, accessing and learning from a sample of another coauthorship hypergraph, perhaps from DBLP of a more recent time period, or a different database like Microsoft Academic Graph (MAG, Sinha et al. (2015)), can be invaluable. In addition, resources like human-labeled or crowd-sourced data, such as surveys (Ozella et al., 2021), can also be potential source of the hypergraph sample. In the case of protein interaction networks, expert-provided labels for a distinct yet related species or organ could be particularly helpful \u2014 for example, using one of the following databases to reconstruct the other: Reactome (Croft et al., 2010) and hu.MAP 2.0 (Drew et al., 2017). We also note that the hypergraph sample\u2019s size doesn\u2019t necessarily need to match that of the reconstruction target. Sec.5 will provide a detailed illustration of all the scenarios mentioned above. The crucial observations above point us to a relaxed version of the problem, which involves the usage of another hypergraph from the same application domain as training data. The new learning-based paradigm is shown Fig.3a, as an update to Fig.1, with the following the problem statement. Learning-based hypergraph reconstruction. (1) input X: a projected graph G; (2) output y: the original hypergraphH; (3) split: as in Fig.3a, (Xtrain, ytrain) = (G0,H0), (Xquery, yquery) = (G1,H1), givenH0,H1 \u223c D. (4) metric: following Young et al. (2021) we use Jaccard score to evaluate reconstruction accuracy: \u2223E1\u2229R1\u2223\u2223E1\u222aR1\u2223 , where E1 is the true hyperedges;R1 is the reconstructed hyperedges.\nThe reconstructed hypergraphs in this learning-based setting offer two advantages. Most importantly, they still serve to crucially identify interactive node groups in a projected graph, irrespective of whether supervised signals are used. Also, they can potentially enhance various downstream tasks, such as node ranking and link prediction (see Sec.5.4), as they aggregate information from both the projected graph and the application domain.\nIt\u2019s important to note that, in the second advantage above, our goal of hypergraph reconstruction isn\u2019t to outperform SOTA methods in downstream tasks. In fact, SOTA methods for a specific downstream task are often end-to-end customized, in which cases reconstructed hypergraphs would not be necessary. Rather, we highlight the value of reconstructed hypergraphs as an informative and handy intermediate representation, especially when specific downstream tasks are undetermined at the time of reconstruction, which is similar to the role word embeddings play in language tasks."
        },
        {
            "heading": "4.2 PROPOSED METHOD FOR LEARNING-BASED HYPERGRAPH RECONSTRUCTION",
            "text": "We are now ready to present our method for the problem. Apparently, even with the training hypergraph, the greatest challenge here is still the enormous search space of potential hyperedges (i.e. \u201cclique space\u201d in Fig.2). The novel idea here is that we will use a clique sampler to narrow the search space of hyperedges, and then use a hyperedge classifier to identify hyperedges from the narrowed space. Both modules are optimized using the training data. Fig.3b gives a more detailed 4-step view.\n4.2.1 \u03c1(n, k)-ALIGNMENT\nWe now start by introducing an important statistic that we found, which is the foundation of our proposed method: \u03c1(n, k) is a statistic that describes distributions of hyperedges inside maximal cliques. Given a hypergraph H = (V,E), its projection G, maximal cliques M: \u03c1(n, k) is the probability that we find a unique hyperedge by randomly sampling a size-k subset of nodes from an arbitrary size-n maximal clique. A (n, k) is called valid if 1 \u2264 k \u2264 n \u2264 N ; N is the maximum clique\u2019s size. \u03c1(n, k) can be empirically estimated via the unbiased estimator \u03c1\u0302(n, k):\n\u03c1\u0302(n, k) = \u2223En,k \u2223\n\u2223Qn,k \u2223 , where{\nEn,k = {S \u2208 E \u2223S \u2286 C, \u2223S\u2223 = k,C \u2208M, \u2223C \u2223 = n}\nQn,k = {(S,C)\u2223S \u2286 C, \u2223S\u2223 = k,C \u2208M, \u2223C \u2223 = n}\nEn,k is all size-k hyperedges in size-n maximal cliques; Qn,k = \u2223{C \u2223C \u2208M, \u2223C \u2223 = n}\u2223( n k ) is all ways to sample a size-n maximal clique and then a size-k subset (i.e. k-clique) from the maximal clique. Our key observation is that if two hypergraphs e.g. H0, H1, are generated from the same source (application domain), they should have similar \u03c1(n, k)\u2019s, which we call \u03c1(n, k)-alignment. Fig.4a uses heatmaps to visualize \u03c1(n, k)-alignment on a famous email dataset, Enron (Benson et al., 2018a), whereH0 andH1 are split based on a middle timestamp of the emails (hyperedges). n is plot on the y-axis, k on the x-axis. Fig.4b plots \u03c1(n, k)\u2019s for 5 other datasets. Both figures show the distributions of \u03c1(n, k) to align well between training and query splits; in contrast, the distributions of \u03c1(n, k) across datasets are much different. Appendix Fig.17 confirms this visual observation quantitatively.\nAlso note Fig.4a\u2019s second column and diagonal cells are darkest, implying that the (n k ) term in Qn,k can\u2019t dominate \u03c1\u0302(n, k): (n k ) is smallest at k = n or 1, growing exponentially as k \u2192 n/2 regardless of data. \u03c1\u0302(n, k) peaking at k = 2 shows that the data term \u2223En,k \u2223 plays a numerically meaningful role. Complexity. The complexity for computing \u03c1(n, k) is O(\u2223M\u2223). See Appx. D.1 for details."
        },
        {
            "heading": "4.2.2 CLIQUE SAMPLER",
            "text": "Given a query graph, we can\u2019t afford to take all its cliques U1 as candidates for hyperedges. Therefore, we create a clique sampler. Assume a limited sampling budget \u03b2 \u226a \u2223U1\u2223, our goal is to collect as many hyperedges as possible by sampling \u03b2 cliques from U . Any hyperedge missed in sampling will never get identified by the hyperedge classifier later on, so this step is crucial. Query G1 is not enough to locate hyperedges in the enormous search space of U1. Fortunately, we can get hints from G0 andH0. The idea is that we use G0 andH0 to optimize a clique sampler that can provably collect many hyperedges. The optimization (Fig.3b step 1) is a process to learn \u201cwhere to sample\u201d. Then in G1, we use the optimized clique sampler to sample cliques (Fig.3b step 2). The clique sampler takes the following form:\n(\u22c6) For each valid (n, k), we sample a total of rn,k \u2223Qn,k \u2223 size-k subsets (i.e. k-cliques) from size-n maximal cliques in the query graph, subject to the sampling budget: \u2211n,k rn,k \u2223Qn,k \u2223 = \u03b2.\nrn,k \u2208 [0,1] is the sampling ratio of the (n, k) cell, \u2223Qn,k \u2223 is the size of that cell\u2019s sample space in G0. To instantiate a sampler, a rn,k should be specified for every valid (n, k). How to determine the rn,k\u2019s? We optimize rn,k\u2019s towards collecting the most hyperedges from G0, with the objective:\n{rn,k} = argmax {rn,k} E(\u2223 \u22c3 (n,k) rn,k \u2299 En,k \u2223)\n\u2299 is a set sampling operator that yields a uniformly downsampled subset of En,k at downsampling rate rn,k. \u2299 is essentially a generator for random finite set (Mullane et al., 2011) (See Appx. B.4). E(\u2223 \u22c5 \u2223) is expected cardinality. Given \u03c1(n, k)-alignment and objective fullfilled, the optimized sampler should also collect many hyperedges when applied to G1. See Appx. F.8 for empirical validation.\nOptimization. To collect more hyperedges from G0, a heuristic is to allocate all budget to the darkest cells of the training data\u2019s heamap (Fig.4a-left), where hyperedges most densely populate. However, a caveat is that the set of hyperedges En,k in each (n, k) cell of the same column are not disjoint. In other words, a size-k clique can appear in multiple maximal cliques of different sizes.\nAlgorithm 1 Optimize Clique Sampler Require: \u03b2; N ; En,k, Qn,k for all 1 \u2264 k \u2264 N,k \u2264 n \u2264 N\n1: for k = 1 to N do \u25b7 traverse k to initialize state variables 2: \u0393k \u2190 \u2205 \u25b7 union of En,k\u2019s picked from column k 3: \u03c9k \u2190 {k, k + 1, ...,N} \u25b7 available column-k cells 4: ri,k \u2190 0 for i \u2208 \u03c9k \u25b7 sampling ratios for column-k cells 5: \u2206k, nk \u2190 UPDATE (k, \u03c9k, \u0393k, E\u22c5,k,Q\u22c5,k) 6: end for 7: while \u03b2 > 0 do \u25b7 the greedy selection starts 8: k \u2190 argmaxi \u2206i \u25b7 selects the next best k 9: rnk,k \u2190min{1,\n\u03b2 \u2223Qnk,k \u2223 } \u25b7 samples cell by ratio 10: \u0393k \u2190 \u0393k \u222a Enk,k \u25b7 updates state variables 11: \u03c9k \u2190 \u03c9k/{nk} 12: \u03b2 \u2190 \u03b2 \u2212 \u2223Qnk,k \u2223 13: \u2206k, nk \u2190 UPDATE (k, \u03c9k, \u0393k, E\u22c5,k,Q\u22c5,k) 14: if maxk \u2206k = 0 then break \u25b7 breaks if all cells sampled 15: end while 16: return rn,k for all 1 \u2264 k \u2264 N,k \u2264 n \u2264 N\nTherefore, taking the darkest cells may not give best result. In fact, optimizing the objective above involves maximizing a monotone submodular function under budget constraint, which is NP-hard. In light of this, we design Algorithm 1 to greedily approach the optimal solution with worst-case guarantee. It takes four inputs: sampling budget \u03b2, size of the maximum clique N , En,k and Qn,k for all 1 \u2264 k \u2264 n \u2264 N . Lines 1-6 initialize state variables. Lines 7-15 run greedy selection iteratively. Subroutine 1 UPDATE in Algorithm1 Require: k; \u03c9k; \u0393k; E\u22c5,k; Q\u22c5,k\nif \u03c9k \u2260 \u2205 then \u2206\u2032 \u2190maxn\u2208\u03c9k \u2223\u0393k\u222aEn,k \u2223\u2212\u2223\u0393k \u2223 \u2223Qn,k \u2223\nn\u2032 \u2190 argmaxn\u2208\u03c9k \u2223\u0393k\u222aEn,k \u2223\u2212\u2223\u0393k \u2223\n\u2223Qn,k \u2223 else\n\u2206\u2032 \u2190 0; n\u2032 \u2190 0; end if return \u2206\u2032, n\u2032\nThe initialization is done column-wise. In each column k, \u0393k stores the union of all En,k selected from column k so far; \u03c9k stores the row indices of all cells in column k that haven\u2019t been selected; ri,k is the sampling ratio of each valid cell; line 5 calls the subroutine UPDATE to compute \u03b4k, the best sampling efficiency among all available cells, and nk, the row index of that most efficient cell. Lines 7-15 run the greedy selection. In each iteration, we take the next most efficient cell among the best of all columns, store the selection in the corresponding r, and up-\ndate (\u0393, \u03c9, \u03b4k, nk); k is the column index of the selected cell. Only column k needs to be updated as En,k\u2019s with different k\u2019s are independent. We stop when reaching budget or having traversed all cells. Appx. F.8 visualizes Algorithm 1\u2019s iterations and does ablation studies on its necessity. Theorem 4. Let q be the expected number of hyperedges inH0 drawn by the clique sampler optimized by Algo. 1; let q\u2217 be the expected number of hyperedges in H0 drawn by the best-possible clique sampler, under the same \u03b2. Then, q > (1 \u2212 1\ne )q\u2217 \u2248 0.63q\u2217.\nTheorem 4 bounds the optimality of Algo. 1; q q\u2217 in practice can be much higher than 0.63. Also notice that Algo. 1 leaves at most one (n, k) cell partially sampled. Is that a good design? In fact, there always exists an optimal clique sampler that leaves at most one cell partially sampled. Otherwise, we can always relocate all our budget from one partially sampled cell to another to achieve a higher q. Appx.D.3 further discusses Algo. 1\u2019s relationship with the standard submodular optimization, how it eliminates Errors I & II, and the tuning of \u03b2 from the perspective of precision-recall tradeoff. Complexity. Algo.1\u2019s average complexity is O(\u2223E \u2223), worst complexity is O(N \u2223E \u2223). See Appx. D.3."
        },
        {
            "heading": "4.2.3 HYPEREDGE CLASSIFIER",
            "text": "A hyperedge classifier is a binary classification model that takes a target clique in the projection as input, and outputs a 0/1 label indicating whether the target clique is a hyperedge. We train the hyperedge classifier on (H0,G0) which has ground-truth labels (step 3, Fig.3b), then use it to identify hyperedges from G1 (step 4, Fig.3b). To serve this purpose, a hyperedge classifier should contain two parts (1) a feature extractor that extracts expressive features for characterizing a target clique, and (2) a binary classifier that transforms a feature vector into a 0/1 label. (2) is a standard task, so we use a MLP with 100 hidden neurons. (1) requires more careful design, as discussed below.\nDataset \u2223V \u2223 \u2223E \u2223 \u00b5(E) \u03c3(E) d\u0304(V ) \u2223M\u2223 Enron (Benson et al., 2018a) 142 756 3.0 2.0 16 362 DBLP (Benson et al., 2018a) 319,916 197,067 3.0 1.7 1.8 166,571 P. School (Benson et al., 2018a) 242 6,352 2.4 0.6 64 15,017 H. School (Benson et al., 2018a) 327 3,909 2.3 0.5 28 3,279 Foursquare (Young et al., 2021) 2,334 1,019 6.4 6.5 2.8 8,135 Hosts-Virus (Young et al., 2021) 466 218 5.6 9.0 2.6 361 Directors (Young et al., 2021) 522 102 5.4 2.2 1.2 102 Crimes (Young et al., 2021) 510 256 3.0 2.3 1.5 207\nTable 2: Summary of the datasets (query split). \u00b5(E) and \u03c3(E) stand for mean and std. of hyperedge\u2019s size. d\u0304(V ): average node degrees w.r.t. hyperedges. M: maximal cliques. See Table 6 for the full version.\nDesign Principles. Creating an effective feature extractor necessitates identifying the essential information about a target clique in the projection. Since our setting doesn\u2019t assume attributed nodes or edges, leveraging the structural features both within and around the target clique is crucial. Also, positional embeddings like Deepwalk are not applicable here due to unaligned node IDs. In principle, any structural learning model for characterizing connectivities can be a potentially good choice \u2014 and there are many of them operating on individual nodes (Henderson et al., 2012; Li et al., 2020; Xu et al., 2018). However, since this is a new task involving complex clique structures, we want to have a learning model as interpretable as possible in such a\u201cclique-rich\u201d context. Here, we introduce two feature extractors that achieve this via interpretable features, though alternative options exist.\n\u201cCount\u201d Feature Extractor. Many powerful graph structural learning models use different notions of \u201ccount\u201d to characterize local connectivity patterns. For example, GNNs typically use node degrees as initial features when node attributes are unavailable; the Weisfeiler-Lehman Test also updates a node\u2019s color based on the count of different colors in its neighborhood. Viewing a target clique as a subgraph in a projection, the notion of \u201ccount\u201d can be especially rich in meaning: a target clique can be characterized by the count of its [own nodes / neighboring nodes / neighboring edges / attached maximal cliques] in many different ways. We create a total of 8 types of generalized count-based features, elaborated in Appx. D.4. Despite technical simplicity, these count features works surprisingly well and can be easily interpreted. See Appx. D.4 also.\n\u201cMotif\u201d Feature Extractor. As a second attempt we use maximal cliques as intermediaries to bridge the gap between projection and hyperedges. The maximal cliques serve as initial estimations of the high-order structures, encompassing full projection information and offering partially refined insights into higher-order structures. Also, the interaction between maximal cliques and nodes in the target clique form rich connectivity patterns, generalizing the notion of motif (Milo et al., 2002). Fig.5 lists all 13 connectivity patterns involving the target clique\u2019s 1 or 2 nodes and maximal cliques. Clique motifs, compared to count features, more systematically extract structural properties, with two component types (node, maximal clique) and three relation types explained in the legend. A clique motif is attached to a target clique C if the clique motif contains at least one node of C. We further use \u03a6(C)i to denote the set of type-i (1 \u2264 i \u2264 13) clique motifs attached to C. Given a target clique C, how to use clique motifs attached to C to characterize structures around C? We define C\u2019s structural features as a concatenation of 13 vectors: [u(C)1 ;u (C) 2 ; ..., u (C) 13 ]. u (C) i is a vector of statistics describing the vectorized distribution of type-i clique motifs attached to C:\nu(C)i = SUMMARIZE(P (C) i ), P (C) i = { [COUNT(C, i,{v}) for v in C], if 1 \u2264 i \u2264 3; [COUNT(C, i,{v1, v2}) for v1, v2 in C], if 4 \u2264 i \u2264 13;\nP (C)i is a vectorized distribution in the form of an array of counts regarding i and C. COUNT(C, i, \u03c7) = \u2223{\u03d5 \u2208 \u03a6(C)i \u2223\u03c7 \u2286 \u03d5}\u2223. Finally, SUMMARIZE(P (C) i ) is a function that transforms a vectorized distribution P (C)i into a vector of statistical descriptors. Here we simply define the statistical descriptors as:\nSUMMARIZE(P (C)i ) = [mean(P (C) i ), std(P (C) i ),min(P (C) i ), max(P (C) i )]\nAs we have 13 clique motifs, these amount to 52 structural features. On the high level, clique motifs extend the well-tested motif methods on graphs to hypergraph projections with clique structures. Compared to count features, clique motifs capture structural features in a more principled manner.\nDBLP Hosts-Virus Enron Best Baseline (full) 79.13 41.00 6.61 SHyRe-motif (full) 81.19\u00b10.02 45.16\u00b10.55 16.02\u00b10.35 SHyRe-count (20%) 81.17\u00b10.01 44.02\u00b10.39 6.43\u00b10.18 SHyRe-motif (20%) 81.17\u00b10.01 44.48\u00b10.21 10.56\u00b10.92\nTable 4: Performance of semi-supervised reconstruction using 20% training hyperedges, measured in Jaccard Score. 50 60 70 80 90 100 Transfer Learning Performance (%)\nDBLP2007\nDBLP2008\nDBLP2009\nDBLP2012\nDBLP2013\nDBLP2014\nMAG-History\nMAG-Geology\nMAG-TopCS\nSHyRe-motif SHyRe-count\nFigure 6: Transfer learning performance: trained on DBLP2011, tested on various coauthorship datasets."
        },
        {
            "heading": "5 EXPERIMENTS AND FINDINGS",
            "text": ""
        },
        {
            "heading": "5.1 EXPERIMENTAL SETTINGS",
            "text": "Baselines. We adapt 7 methods from four task domains for their best relevance or state-of-the-art performance: (1) Community Detection: Demon (Coscia et al., 2012), CFinder (Palla et al., 2005); (2) Clique Decomposition: MaxClique (Bron & Kerbosch, 1973), Clique Covering (Conte et al., 2016); (3) Hyperedge Prediction: Hyper-SAGNN (Zhang et al., 2019), CMM (Zhang et al., 2018), HPRA (Kumar et al., 2020a); (4) Probabilistic Models: Bayesian-MDL(Young et al., 2021). See more in Appendix C. Data & Training. We use 8 real-world datasets from various application domains. For each dataset, we split the hyperedges to generateH0,H1, G0, G1. The properties ofH1 are summarized in Table 2. See Appx. F.1 for more details of dataset split, baselines selection and adaptation, and tuning. Reproducibility. Our code and data are available here."
        },
        {
            "heading": "5.2 EVALUATING QUALITY OF RECONSTRUCTION",
            "text": "We name our approach SHyRe (Supervised Hypergraph Reconstruction) whose performance is shown in Table 3. SHyRe variants significantly outperform all baselines in most datasets (7/8), with best improvement in hard datasets such as P. School, H.School, and Enron.This success is attributed to SHyRe\u2019s innovative use of the training graph and its ability to capture strong, interpretable features, as detailed in Appx. D.4. Although Clique Covering and Bayesian-MDL perform relatively well, they struggle with dense hypergraphs. Hyperedge prediction methods have similar issues despite being also learning-based methods, a topic elaborated in Appx.C.3. More metrics. Fig.17 in Appendix further shows fine-grained performance measured by partitioned errors (Def.1): SHyRe variants significantly reduce more Errors I and II than other baselines do. Besides, we also study various topological properties of the reconstructed hypergraphs and compare them to those of the original hypergraphs. We find that on these new measurements SHyRe variants also produce more faithful reconstructions than baselines. See Appx.F.2 for more details."
        },
        {
            "heading": "5.3 SEMI-SUPERVISED LEARNING AND TRANSFER LEARNING",
            "text": "We study more constrained scenarios where we only have access to a small subset of hyperedges, or a a training hypergraph from a nearby domain. These settings correspond to semi-supervised learning and transfer learning. For semi-supervised setting, we choose three datasets of different difficulties: DBLP, Hosts-Virus, and Enron. For each dataset, we randomly discard 80% hyperedges in the training split. Table 4 shows the result: the reconstruction accuracy drops on all datasets, but SHyRe trained on 20% data still outperforms the best baseline on full data. For transfer learning, we train SHyRe on DBLP 2011, and test on various other DBLP slices and Microsoft Academic Graphs (Sinha et al., 2015), shown in Fig.6. We can see that SHyRe remains robust to the distribution shift as its performance on DBLP 2011 is 81.19% according to Table 3."
        },
        {
            "heading": "5.4 USE CASES OF RECONSTRUCTED HYPERGRAPHS IN DOWNSTREAM TASKS",
            "text": "We evaluate advantages of using reconstructed hypergraphs as informative intermediate representations for downstream tasks in comparison to relying on projected graphs, through two use cases: node ranking in protein-protein interaction (PPI) network and link prediction. The details are presented in Appx.F.4 and F.5, respectively. For the former, we apply our method to recover multiprotein complexes from pairwise interaction data, which are then used to rank the essentiality of proteins based on their node degrees. This produces a ranking list that is much better aligned with the ground truth than results based on the projected version of PPI network, as shown in Appendix Table 8. In the second use case, link prediction, we demonstrate that reconstructed hypergraphs enhance performance over projected graphs across multiple datasets measured by AUC and Recall. These use cases show the efficacy of our hypergraph reconstruction technique in deriving richer, more informative data representations that can benefit downstream analytical tasks. More Experiment. We report more experiment in Appx.F.7-F.12, including ablation studies on clique sampler, optimal sampling coefficients (rm,k\u2019s), and running time and storage comparison."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We studied hypergraph projection and its reversal. We identified specific hyperedge patterns that trigger errors, and introduced a novel learning-based approach for hypergraph reconstruction. For future studies, considering different projection expansions like \"star\" or \"line\" can be highly promising. We also discuss the Broader Impacts of our work in Appx. E."
        },
        {
            "heading": "A REPRODUCIBILITY",
            "text": "Our code and data can be downloaded from https://anonymous.4open.science/r/ supervised_hypergraph_reconstruction-FD0B/README.md."
        },
        {
            "heading": "B PROOFS AND ADDITIONAL DISCUSSIONS FOR SEC.3",
            "text": ""
        },
        {
            "heading": "B.1 PROOF OF THEOREM 1",
            "text": "Proof. \u201cOnly if\u201d direction: Condition I holds because every hyperedge is a maximal clique, and two maximal cliques cannot be proper subset of each other. Therefore, it is impossible for any two hyperedges E,E\u2032 \u2208 E , which are both maximal cliques, to still satisfy E \u2282 E\u2032. Condition II holds trivially becauseM = E \u21d2M \u2286 E .\n\u201cIf\u201d direction: starting from Condition II, it only remains to show that every hyperedge inH is also a maximal clique in G. We prove by contradiction. If there exists a hyperedge E that is not a maximal clique, by definition of maximal clique and because E itself is a clique, E must be a proper subset of some maximal clique E\u2032. However, based on Condition II, E\u2032 is also a hyperedge. This leads to the relationship E \u2208 E\u2032, a contradiction with Condition I."
        },
        {
            "heading": "B.2 PROOF OF THEOREM 2",
            "text": "Proof. The \u201cif\u201d direction: Suppose thatH is not conformal. According to Def.2, we know that there exists a maximal clique C \u2209 E . Clearly for every C with \u2223C \u2223 \u2264 2, \u2223C \u2223 \u2208 E . For a C \u2209 E and \u2223C \u2223 \u2265 3, pick any two nodes v1, v2 \u2208 C. Because they are connected, v1, v2 must be in some hyperedge Ei. Now pick a third node v3 \u2209 Ei. Likewise, there exists some different Ej such that v1, v3 \u2208 E2, and some different Eq such that v2, v3 \u2208 E3. Notice that Ej \u2260 Eq because otherwise the three nodes would be in the same hyperedge. Now we have {v1, v2, v3} \u2286 (Ei \u2229Ej) \u222a (Ej \u2229Eq) \u222a (Eq \u2229Ei). Because {v1, v2, v3} is not in the same hyperedge, (Ei \u2229Ej) \u222a (Ej \u2229Eq) \u222a (Eq \u2229Ei) is also not in the same hyperedge.\nThe \u201conly if\u201d direction: Because every two of the three intersections share a common hyperedge, their union is a clique. The clique must be contained by some hyperedge, because otherwise the maximal clique containing the clique is not contained by any hyperedge.\nAlternatively, there is a less intuitive proof that builds upon results from existing work in a detour: It can be proved that H being conformal is equivalent to its dual H\u2032 being Helly Berge (1973). According to an equivalence to Helly property mentioned in Berge & Duchet (1975), for every set A of 3 nodes in H\u2032, the intersection of the edges Ei with \u2223Ei \u2229A\u2223 \u2265 k is non-empty. Upon a dual transformation, this result can be translated into the statement of Theorem 2. We refer the interested readers to the original text."
        },
        {
            "heading": "B.3 PROOF OF THEOREM 3",
            "text": "Given a set X = {1,2, ...,m} and a hypergraphH = (V,{E1, ...Em}), we define f \u2236 2X \u2192 2V to be:\nf(S) = (\u22c2 i\u2208S\nEi)\u22c2 \u239b\n\u239d \u22c2 i\u2208X/S E\u0304i \u239e \u23a0 , S \u2286X\nwhere E\u0304i = V /Ei. Lemma 5. {f(S)\u2223S \u2208 2X} is a partition of V .\nProof. Clearly f(Si) \u2229 f(Sj) = \u2205 for all Si \u2260 Sj , so elements in {f(S)\u2223S \u2208 2X} are disjoint. Meanwhile, for every node v \u2208 V , we can construct a S = {i\u2223v \u2208 Ei} so that v \u2208 f(S). Therefore, the union of all elements in {f(S)\u2223S \u2208 2X} spans V .\nBecause Lemma 5 holds, for any v \u2208 V we can define the reverse function f\u22121(v) = S\u21d4 v \u2208 f(S). Here f\u22121 is a signature function that represents a node in V by a subset of X , whose physical meaning is the intersection of hyperedges inH. Lemma 6. If S1 \u2229 S2 \u2260 \u2205, S1, S2 \u2286 X , then for every v1 \u2208 f(S1) and v2 \u2208 f(S2), (v1, v2) is an edge in H\u2019s projection G. Reversely, if (v1, v2) is an edge in G, f\u22121(v1) \u2229 f\u22121(v2) \u2260 \u2205.\nProof. According to the definition of f(S), \u2200i \u2208 S1 \u2229 S2, v1, v2 \u2208 Ei. Appearing in the same hyperedge means that they are connected in G, so the first part is proved. If (v1, v2) is an edge in G, there exists an Ei that contains both nodes, so f\u22121(v1) \u2229 f\u22121(v2) \u2287 {i} \u2260 \u2205.\nAn intersecting family F is a set of non-empty sets with non-empty pairwise intersection, i.e. Si \u2229Sj \u2260 \u2205, \u2200Si, Sj \u2208 F . Given a set X , a maximal intersecting family of subsets, is an intersecting family of set F that satisfies two additional conditions: (1) Each element of F is a subset of X; (2) No other subset of X can be added to F . Lemma 7. Given a a hypergraphH = (V,{E1,E2, ...Em}), its projection G, and X = {1,2, ...,m}, the two statements below are true:\n\u2022 If a node set C \u2286 V is a maximal clique in G, then {f\u22121(v)\u2223v \u2208 C} is a maximal intersecting family of subsets of X .\n\u2022 Reversely, if F is a maximal intersecting family of subsets of X , then \u222aS\u2208Ff(S) is a maximal clique in G.\nProof. For the first statement, clearly \u2200v \u2208 V, f\u22121(v) \u2286 X . Because C is a clique, every pair of nodes in C is an edge in G. According to Lemma 6, \u2200v1, v2 \u2208 C, f\u22121(v1) \u2229 f\u22121(v2) \u2260 \u2205. Finally, because C is maximal, there does not exist a node v \u2208 V that can be added to C. Equivalently there does not exist a S = f\u22121(v) that can be added to f\u22121(C). Therefore, f\u22121(C) is maximal.\nFor the second statement, because F is an intersecting family, \u2200S1, S2 \u2286 F , S1 \u2229S2 \u2260 \u2205. According to Lemma 6, \u2200v1, v2 \u2208 f(F), (v1, v2) is an edge in G. Therefore, f(F) is a clique. Also, no other node v can be added to f(F). Otherwise, f\u22121(v) \u222aF is still an intersecting family while f\u22121(v) is not in F , which makes F strictly larger \u2014 a contradiction. Therefore, \u222aS\u2208Ff(S) is a maximal clique.\nLemma 7 shows there is a bijective mapping between a maximal clique and a maximal intersecting family of subsets. GivenH, G and X , Counting the former is equivalent to counting the latter. The result is denoted as \u03bb(m) in the main text. Lemma 2.1 of Brouwer et al. (2013) gives an lower bound: \u03bb(m) \u2265 2( m\u22121 [m/2]\u22121)."
        },
        {
            "heading": "B.4 PROOF OF THEOREM 4",
            "text": "We start with some definitions. A random finite set, or RFS, is defined as a random variable whose value is a finite set. Given a RFS A, we use S(A) to denote A\u2019s sample space; for a set a \u2208 S(A) we use PA(a) to denote the probability that A takes on value a. One way to generate a RFS is by defining the set sampling operator \u2299 on two operands r and X , where r \u2208 [0,1] and X is a finite set: r \u2299X is a RFS obtained by uniformly sampling elements from X at sampling rate r, i.e. each element x \u2208X has probability r to be kept. Also, notice that the finite set X itself can also be viewed as a RFS with only one possible value to be taken. Now, we generalize two operations, union and difference, to RFS as the following:\n\u2022 Union A \u222aB: S(A \u222aB) = {x\u2223x = a \u222a b, a \u2208 A, b \u2208 B}\nPA\u222aB(x) = \u2211 x=a\u222ab,a\u2208A,b\u2208B PA(a)PB(b)\n\u2022 Difference A/B: S(A/B) = {x\u2223x = a/b, a \u2208 A, b \u2208 B}\nPA\u222aB(x) = \u2211 x=a/b,a\u2208A,b\u2208B PA(a)PB(b)\nWith these ready, we have the following propositions that hold true for RFS A and B:\n(i) E(\u2223A \u222aB\u2223) = E(\u2223B \u222aA\u2223) (ii) E(\u2223A \u222aB\u2223) = E(\u2223A/B\u2223) +E(\u2223B\u2223);\n(iii) E(\u2223A \u222aB\u2223) \u2265 E(\u2223A\u2223), E(\u2223A \u222aB\u2223) \u2265 E(\u2223B\u2223); (iv) E(\u2223(r \u2299X)/Y \u2223) = r\u2223X/Y \u2223 = E(\u2223X/Y \u2223); (X , Y are both set)\nLemma 8. At iteration (i + 1) when Algorithm 1 samples a cell (n, k) (line 8), it reduces the gap between q\u2217 and the expected number of hyperedges it already collects, qi, by a fraction of at least rn,k \u2223Qn,k \u2223\n\u03b2 :\nq\u2217 \u2212 qi+1 q\u2217 \u2212 qi \u2264 1 \u2212 ri+1\u2223Qi+1\u2223\n\u03b2Proof. q\u2217 \u2212 qi\n= E(\u2223 \u222azj=1 r \u2217 j \u2299 Ej \u2223) \u2212E(\u2223 \u222a i j=1 rj \u2299 Ej \u2223) (Thm.4 setup) \u2264 E(\u2223(\u222azj=1r \u2217 j \u2299 Ej) \u222a (\u222a i j=1rj \u2299 Ej)] \u2212E(\u2223 \u222a i j=1 rj \u2299 Ej \u2223) (Prop.iii) = E(\u2223(\u222azj=1r \u2217 j \u2299 Ej)/(\u222a i j=1rj \u2299 Ej)\u2223) (Prop.ii)\n= z\n\u2211 t=1\nE(\u2223(r\u2217t \u2299 Et)/((\u222a t\u22121 j=1r \u2217 t \u2299 Et) \u222a (\u222a i j=1rj \u2299 Ej))\u2223) (Prop.ii)\n\u2264 z\n\u2211 t=1\nE(\u2223(r\u2217t \u2299 Et)/(\u222a i j=1rj \u2299 Ej)\u2223) (Prop.iii)\n= z\n\u2211 t=1\nr\u2217t E(\u2223Et/(\u222a i j=1rj \u2299 Ej)\u2223) (Prop.iv)\n= z\n\u2211 t=1\nr\u2217t \u2223Qt\u2223 E(\u2223Et/(\u222aij=1rj \u2299 Ej)\u2223)\n\u2223Qt\u2223\n\u2264 z\n\u2211 t=1\nr\u2217t \u2223Qt\u2223 E(\u2223Ei+1/(\u222aij=1rj \u2299 Ej)\u2223)\n\u2223Qi+1\u2223 (Alg.1, line 7)\n= \u03b2 \u22c5 ri+1E(\u2223Ei+1/(\u222aij=1rj \u2299 Ej)\u2223)\nri+1\u2223Qi+1\u2223 (Def. of \u03b2)\n= \u03b2\nri+1\u2223Qi+1\u2223 E(\u2223(ri+1 \u2299 Ei+1)/(\u222aij=1rj \u2299 Ej)\u2223) (Prop.iv)\n= \u03b2\nri+1\u2223Qi+1\u2223 (qi+1 \u2212 qi) (Thm.4 setup)\nTherefore, q \u2217\u2212qi+1 q\u2217\u2212qi \u2264 1 \u2212 ri+1\u2223Qi+1\u2223 \u03b2\nNow, according to our budget constraint we have\n\u2211 n,k\n(1 \u2212 rn,k \u2223Qn,k \u2223\n\u03b2 ) = z \u2212 1\nz is the total number of (n, k) pairs where rn,k > 0, which is a constant. Finally, we have\nq\u2217 \u2212 q\nq\u2217 = z\u22121 \u220f i=0 q\u2217 \u2212 qi+1 q\u2217 \u2212 qi \u2264\u220f n,k (1 \u2212 rn,k \u2223Qn,k \u2223 \u03b2 ) \u2264 (1 \u2212 1 z ) z < 1 e\nTherefore q > (1 \u2212 1 e )q\u2217."
        },
        {
            "heading": "B.5 ERRORS I & II VS. \u201cTYPE I & II\u201d ERRORS",
            "text": "Note that here Errors I and II are different from the well-known Type I (false positive) and Type II (false negative) Error in statistics. In fact, Error I\u2019s are hyperedges that nest inside some other hyperedges, so they are indeed false negatives; Error II\u2019s can be either false positives or negatives. For example, in Fig.2 \u201cError II Pattern\u201d : {v1, v3, v5} is a false positive, {v1, v5} is a false negative."
        },
        {
            "heading": "C ADDITIONAL RELATED WORK",
            "text": ""
        },
        {
            "heading": "C.1 COMPARISON BETWEEN GRAPHS AND HYPERGRAPHS",
            "text": "Previous works have compared the graph representations (e.g., clique expansion) and hypergraph representations in various contexts, highlighting the concerning loss of higher-order information in graph representations. For example, Torres et al. (2021) provides a unified overview of various representations for complex systems: it notes the mathematical relationship between graphs and hypergraphs, warning against unthoughtful usage of graphs to model higher-order relationships. Similarly, Dong et al. (2020) also raises concerns about information loss when designing learning methods for hypergraphs based on their clique expansions.\nYoon et al. (2020) further empirically compares the performance of various methods in the hyperedge prediction task when the methods are executed on different lower-order approximations of hypergraphs, including order-2 approximation (clique expansion), order-3 approximations (3-uniform projected hypergraphs), etc.; their experiments show that lower-order approximations especially struggle on more complex datasets or more challenging versions of tasks.\nOur work significantly extends the notions in these previous works, formulating the central problem of interest (i.e. comparing graphs and hypergraphs, and mitigating the information loss) and giving it a systematical, rigorous study."
        },
        {
            "heading": "C.2 RELATED METHODS TO HYPERGRAPH RECONSTRUCTION TASK",
            "text": "Besides the ones in Introduction, three lines of works are pertinent to the hypergraph reconstruction task discussed in this paper. Edge Clique Cover is to find a minimal set of cliques that cover all the graph\u2019s edges. Erd\u00f6s et al. (1966) proves that any graph can be covered by at most [\u2223V \u22232/4] cliques. Conte et al. (2016) finds a fast heuristic for approximating the solution. Young et al. (2021) creates a probabilistic to solve the task. However, this line of work shares the \u201cprinciple of parsimony\u201d, which is often impractical in real-world datasets. Hyperedge Prediction is to identify missing hyperedges of an incomplete hypergraph from a pool of given candidates. Existing works focus on characterizing a node set\u2019s structural features. The methods span proximity measures Benson et al. (2018a), deep learning Li et al. (2020); Zhang et al. (2019), and matrix factorization Zhang et al. (2018). Despite the relevance, the task has a very different setting and focus from ours as mentioned in introduction.\nCommunity Detection finds node clusters in which edge density is unusually high. Existing works roughly comes in two categories by the community types: disjoint Que et al. (2015); Traag et al. (2019), and overlapping Coscia et al. (2012); Palla et al. (2005). As mentioned, however, their focus on \u201crelative density\u201d is different from ours on cliques."
        },
        {
            "heading": "C.3 COMPARING LEARNING-BASED HYPERGRAPH RECONSTRUCTION WITH HYPERGRAPH PREDICTION",
            "text": "As we observe in Table 3, the two hyperedge prediction methods have very poor performance. This is resulted from the incompatibility of the hypergraph prediction task with our task of hypergraph reconstruction, in particular:\n\u2022 Incompatibility of Input: all hyperedge prediction methods, including the two baselines, require a (at least partially observed) hypergraph as input, and they must run on hyperedges. In hypergraph reconstruction task, we don\u2019t have this as input. We only have a projected graph.\n\u2022 Incompatibility of output: hyperedge prediction methods can only tell us whether a potential hyperedge can really exist. They cannot tell where a hyperedge is, given only a projected graph as input. In other words, they only do classification, rather than generation. This is also the key issue that our work has gone great lengths to address.\nIn order to run hyperedge prediction methods on hypergraph reconstruction, we have to make significant adaptations. First, we must treat all edges in the input projected graph as existing hyperedges. Second, it is only fair that we sample cliques from projected graph as \u201cpotential hyperedge\u201d for classification completely at random, until we reach our computing capacity. This is because none of the hyperedge prediction methods mentions or concerns about this procedure. These two adaptations enable hyperedge prediction methods to run on hypergraph reconstruction tasks, but at the cost of vastly degraded performance."
        },
        {
            "heading": "D MORE DISCUSSIONS ON THE SUPERVISED HYPERGRAPH RECONSTRUCTION APPROACH",
            "text": "D.1 COMPLEXITY OF \u03c1(n, k)\nThe main complexity of \u03c1(n, k) involves two parts: (a)M; (b) En,k for all valid (n, k).\n(a)\u2019s complexity is O(\u2223M\u2223) as mentioned in Sec.2. Though in worst case \u2223M\u2223 can be exponential to \u2223V \u2223, in practice we often observe \u2223M\u2223 on the same magnitude order as \u2223E \u2223 (see Table 1), which is an interesting phenomenon. Please see our discussion below for more details.\n(b) requires matching size-n maximal cliques with size-k hyperedges for each valid (n, k) pair. The key is that in real-world data, the average number of hyperedges incident to a node is usually a constant independent from the growth of \u2223V \u2223 or \u2223M\u2223 (see also d\u0304(V ) in Table 2), known as the sparsity of hypergraphs Kook et al. (2020). This property greatly reduces the average size of search space for all size-k hyperedges in a size-n maximal clique from \u2223E \u2223 to nd\u0304(V ). As we see both n and d\u0304(V ) are typically under 50 in practice. b\u2019s complexity can still be viewed as O(\u2223M\u2223). Therefore, the total complexity for computing \u03c1(n, k) is O(\u2223M\u2223). Sec.5.2 provides more empirical evidence.\nMore on the complexity of extracting maximal cliques: As discussed above, the complexity of extracting data inputs for Algorithm1 is bounded by the number of maximal cliques in the projected graph, and for thinking about this, we believe it\u2019s useful to distinguish between the worst case and the cases we encounter in practice. For the worst case, the number of maximal cliques indeed can be (super-)exponential to the number of hyperedges, as our Theorem 3 proved. We agree with the reviewer on this point.\nThe interesting observation here is that in practice we don\u2019t typically witness such an explosion when we try to enumerate all maximal cliques in various datasets, see Table 5 in Appendix for example. This contrast between the worst case and the cases encountered in practice is of course a common theme in network analysis, where research has often tried to provide theoretical or heuristic reasons why the worst-case behavior of certain methods doesn\u2019t seem to generally occur on the kinds of real-world network that arise in practice. This theme has been explored in a number of lines of work for problems that require the enumeration of maximal cliques.\nIn particular, there are multiple lines of work that try to give theoretical explanations for why realworld graphs generally have a tractable number of maximal cliques, thereby making algorithms that use maximal clique enumeration feasible. The reviewer\u2019s point is correct that some of these are related to the sparsity of the input graph, but they also include other structural features typically exhibited by real-world network structures. Here we include brief discussions on two metrics relevant to this:\n\u2022 k-degeneracy. The degeneracy of an n-vertex graph G is the smallest number k such that every subgraph of G contains a vertex of degree at most k. The k here is often used as a measure of how sparse a graph is, in a way that captures subtler structural information than simply the average degree, and with a smaller k indicating a sparser graph. Eppstein et al. (2010) found that a graph of n nodes and degeneracy k can have at most (n \u2212 k)3k/3 maximal cliques. This result explains as it restricts the size of hyperedges.\n\u2022 c-closure. An undirected graph G is c-closed if, whenever two distinct vertices u, v have at least c common neighbors, (u, v) is an edge of G. The number c here measures the strength of triadic closure of this graph. Fox et al. (2020) shows that any (weakly) c-closed graph on n vertices has at most 3(c\u22121)/3n2 maximal cliques. Our paper is benefiting from the tractable\nnumber of maximal cliques on real-world networks, on which there\u2019s been a lot of progress in the theoretical underpinnings via these and follow-up papers.\nD.2 NUMERICAL STABILITY OF \u03c1(n, k)\nA desirable property of \u03c1(n, k) to be a hypergraph statistic is its robustness to small distribution shifts of the hypergraphs. Here we report a study of \u03c1(n, k)\u2019s numerical stability based on simulation.\nFirst, we introduce a simple generative model for hypergraphs:\nH \u223c P (n, k,m)\nwhere n is the total number of nodes, k is the size of the largest hyperedge (measured by number of nodes in that hyperedge), and m is a vector of length (k \u2212 1) whose i-th element denotes the number of hyperedges of size (i + 1). This model generates a random hypergraph by sampling from n nodes m[1] hyperedges of size 2, m[2] hyperedges of size 3, . . . m[k \u2212 1] hyperedges of size k.\nNext, we study how a small perturbation to vector m results in the faction of change in the distribution of \u03c1(n, k)\u2019s. Fixing n and k, for each m we add a random noise (i.e. a vector \u2206m) at the strength of 5%, i.e. \u2223\u2206m\u2223/\u2223m\u2223 = 0.05. As a result of the added noise, \u03c1(n, k) would become \u03c1\u0302(n, k)We can therefore quantify the instability of \u03c1(n, k) with respect to m as\ninstability = \u2211n,k(\u03c1(n, k) \u2212 \u03c1\u0302(n, k))\n2/\u2211n,k(\u03c1(n, k)) 2\n\u2223\u2206m\u2223/\u2223m\u2223\nIn our experiment, for each m we repeat the measurement for this stability quantifier for 10 times.We study a simple case where k = 5, n ranges from 10 to 60. In order to mimic the sparsity of the hypergraphs in real world, we further set each element in m to be n (so that the hypergraph\u2019s density is on the order of O(1)). Here is the result of the instability test:\nWe observe that the instabilities are smaller that 1. This means that the relative change in the distribution of rho(n,k) is actually smaller than the relative change in the distribution of hyperedges captured by hyperedge numbers m. And as n goes large the influence of the disturbance decades. Finally, we also acknowledge that the result of this simulation may not apply universally, and that a fully theoretical analysis of the numerical stability of rho(n,k) is unavailable to us at this point."
        },
        {
            "heading": "D.3 MORE DISCUSSION ON ALGORITHM 1",
            "text": "Relating to Errors I & II. The effectiveness of the clique sampler can also be interpreted by the reduction of Errors I and II. Taking Fig.4a as an example: by learning which non-diagonal cells to sample, the clique sampler essentially reduces Error I as well as the false negative part of Error II; by learning which diagonal cells to sample, it further reduces the false positive part of Error II.\nRelating to Standard Submodular Optimization. There are two distinctions between our clique sampler and the standard greedy algorithm for submodular optimization.\n\u2022 The standard greedy algorithm runs deterministically on a set function whose form is already known. In comparison, our clique sampler runs on a function defined over Random Finite Sets (RFS) whose form can only be statistically estimated from the data.\n\u2022 The standard submodular optimization problem forbids picking a set fractionally. Our problem allows fractional sampling from an RFS (i.e. rn,k \u2208 [0,1]).\nWe can see from the Proof of Theorem 4 above that it is harder to prove the optimality of our clique sampler than to prove for the greedy algorithm for Standard Submodular Optimization.\nPrecision-Recall Tradeoff. For each dataset, \u03b2 should be specified manually. What\u2019s the best \u03b2? Clearly a larger \u03b2 yields a larger q, thus a higher recall q\u2223E \u2223 in samples. On the other hand, a larger\n\u03b2 also yields a lower precision q \u03b2 , as sparser regions get sampled. q \u03b2 being too low harms sampling quality and later the training. Such tradeoff necessitates more calibration of \u03b2. We empirically found it often good to search \u03b2 in a range that makes q\u2223E \u2223 \u2208 [0.6,0.95], with more tuning details in Appendix.\nComplexity. The bottleneck of Algo. 1 is UPDATE. In each iteration after a k is picked, UPDATE recomputes (\u2223\u0393k \u222a En,k \u2223 \u2212 \u2223\u0393k \u2223) for all n \u2208 \u03c9k, which is O( \u2223E \u2223 N ). Empirically we found the number of iterations under the best \u03b2 always O(N). N is the size of the maximum clique, and mostly falls in [10,40] (see Fig.4b). Therefore, on expectation we would traverse O(N)O( \u2223E \u2223\nN ) = O(\u2223E \u2223)\nhyperedges if \u2223En,k \u2223 distributes evenly among different k\u2019s. In the worst case where \u2223En,k \u2223\u2019s are deadly skewed, this degenerates to O(N \u2223E \u2223)."
        },
        {
            "heading": "D.4 COUNT FEATURES",
            "text": "We define a target clique C = {v1, v2, ..., v\u2223C\u2223}. The 8 features are:\n1. size of the clique: \u2223C \u2223; 2. avg. node degree: 1\u2223C\u2223 \u2211v\u2208C d(v);\n3. avg. node degree (recursive): 1\u2223C\u2223 \u2211v\u2208C 1 \u2223N(v)\u2223 \u2211v\u2032\u2208N(v) d(v \u2032);\n4. avg. node degree w.r.t. max cliques: 1\u2223C\u2223 \u2211v\u2208C \u2223{M \u2208M\u2223v \u2208M}\u2223;\n5. avg. edge degree w.r.t. max cliques: 1\u2223C\u2223 \u2211v1,v2\u2208C \u2223{M \u2208M\u2223v1, v2 \u2208M}\u2223;\n6. binarized \u201cedge degree\u201d (w.r.t. max cliques):\u220fv1,v2\u2208C 1[e], where e = \u2211v1,v2\u2208C \u2223{M \u2208M\u2223v1, v2 \u2208M}\u2223 > 1; 7. avg. clustering coefficient: 1\u2223C\u2223 \u2211v\u2208C cc(v), where cc is the clustering coefficient of node v in the projection;\n8. avg. size of encompassing maximal cliques: 1\u2223MC \u2223 \u2211M\u2208MC \u2223M \u2223, whereM C = {M \u2208M\u2223C \u2286M};\nNotice that avg. clustering coefficient is essentially a normalized count of the edges between direct neighbors.\nFeature Rankings. We study the relative importance of the 8 features by an ablation study. For each dataset, we ablate the 8 features one at a time, record the performance drops, and use those values to rank the 8 features. We repeat this for all datasets, obtaining the 8 features\u2019 average rankings, shown in Fig.7. More imporant features have smaller ranking numbers. Interestingly we found that the most important feature has a very concrete physical meaning: it indicates whether each edge of the clique has existed in at least two maximal cliques of the projected graph. Remarkably, this is very similar to the simplicial closure phenomenon we\u2019ve seen in in Benson et al. (2018a)."
        },
        {
            "heading": "E BROADER IMPACTS",
            "text": "Extra care should be taken when the hypergraph to be reconstructed involves human subjects. For example, in Fig.8a, the reconstruction of DBLP coauthorship hypergraph has higher accuracy on larger hyperedges, missing more hyperedges of sizes 1 and 2. In other words, papers with fewer authors seem to be harder to recover in this case. The technical challenge in solving this problem resides in the Error I patterns as discussed in the main text. Meanwhile, small hyperedges also contain less structural information to be used by the classifier. In the broader sense, this issue could lead to concerns that marginalized groups of people are not given equal amount of attention as other groups.\nTo mitigate the risk of this issue, we propose improvement to three places in the reconstruction pipeline, and encourage follow-up research into those directions. First, the training hypergraphH0\n\u2223E\u222aM\u2223 , Error II = \u2223M/E\u2032 \u2223+\u2223E\u2032/M\u2223 \u2223E\u222aM\u2223 .\ncan be improved towards more emphasis on smaller hyperedges. In practice, for example, this can be achieved by collecting more data instances from marginalized social groups. Second, we can also adjust the clique sampler so that it allocates more sampling budget to small hyperedges. Technically speaking, we can manually assign larger values to the rn,1\u2019s and the rn,2\u2019s, which originally are parameters to be learned. Third, the hyperedge classifier may also be improved towards better characterization of small hyperedges. One promising direction to achieve this is to utilize node or edge attributes, which, similar to the first measure, also boils down to more data collected on marginalized groups."
        },
        {
            "heading": "F EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "F.1 EXPERIMENTAL SETUP - ADDITIONAL DETAILS",
            "text": "Selection Criteria and Adaptation of Baselines. For community detection, the criteria are: 1. community number must be automatically found; 2. the output is overlapping communities. Based on them, we choose the most representative two. We tested Demon and found it always work best with min community size = 1 and \u03f5 = 1. To adapt CFinder we search the best k between [0.1,0.5] quantile of hyperedge sizes on H0. For hyperedge prediction, we ask that they cannot rely on hypergraphs for prediction, and can only use the projection. Based on that we use the two recent SOTAs, Zhang et al. (2018; 2019). We use their default hyperparameters for training. For Bayesian-MDL we use its official library in graph-tools with default hyperparameters. We implemented the best heuristic in Conte et al. (2016) for clique covering. Both our method and Bayesian-MDL use the same maximal clique algorithm (i.e. the Max Clique baseline) as a preprocessing step.\nDatasets. The first 4 datasets in Table 2 are from Benson et al. (2018a); the rest are from Young et al. (2021). All source links and data can be found in submitted code.\nGenerating training & query set. To generate a training set and a query set, we follow two common standards to split the collection of hyperedges in each dataset: (1) For datasets that come in natural segments, such as DBLP and Enron whose hyperedges are timestamped, we follow their segments so that training and query contain two disjoint and roughly equal-sized sets of hyperedges. For DBLP, we constructH0 from year 2011 andH1 from year 2010; for Enron, we use 02/27/2001, 23:59 as a median timestamp to split all emails intoH0 (first half) andH1 (second half). (2) For all the other datasets that lack natural segments, we randomly split the set of hyperedges into halves.\nWe note that the first standard has less restrictions on the data generation process than the second standard, which follows a more ideal setting and is most widely seen as the standard train-test split in ML evaluations. We therefore also introduce the settings of semi-supervised learning and transfer learning in Sec.5.3 to compensate. We consider these settings together as a relatively comprehensive suite for validating both the proposed learning-based reconstruction problem and its solution pipeline.\nTo enforce inductiveness, we also randomly re-index node IDs in each split. Finally, we projectH0 andH1 to get G0 and G1 respectively.\nHyperparameter Tuning. For Demon, we tested all combinations of its two hyperparameters (min_com_size, epsilon), and found that on all datasets the best combination is (1, 1). For CFinder,\nwe tuned its hyperparameter k by search through 10%,20%, . . . ,50% quantiles of distribution of hyperedge sizes in the dataset. For CMM, we set the number of latent factors to 30. For HyperSAGNN, we set the representation size to 64, window size to 10, walk length to 40, the number of walks per vertex to 10. We compare the two variants of Hyper-SAGNN: the encoder variant and the random walk variant, and chose the latter which consistently yields better performance. The baselines of Bayesian-MDL, Maximal Cliques and Clique Covering do not have hyperparameters to tune.\nTraining Configuration. For models requiring back propagation, we use cross entropy loss and optimize using Adam for 2000 epochs and learning rate 0.0001. Those with randomized modules are repeated 10 times with different seeds.Regarding the tuning of \u03b2, we found the best \u03b2 by training our model on 90% training data and evaluated on the rest 10% training data. The best values are reported in our code instructions.\nMachine Specs. All experiments including model training are run on Intel Xeon Gold 6254 CPU @ 3.15GHz with 1.6TB Memory."
        },
        {
            "heading": "F.2 ANALYSIS OF RECONSTRUCTIONS",
            "text": ""
        },
        {
            "heading": "F.2.1 BASIC PROPERTIES",
            "text": "We characterize the topological properties of the reconstruction using the middle four columns of Table 2: [\u2223E \u2223, \u00b5(E), \u03c3(E), d\u0304(V )]. \u2223V \u2223 is excluded as it is known from the input. For each (dataset, method) combination, we analyze the reconstruction and obtain a unique property vector. We use PCA to project all (normalized) property vectors into 2D space, visualized in Fig.8a.\nIn Fig.8a, colors encode datasets, and marker styles encode methods. Compared with baselines, SHyRe variants ( # and \u00d7) produce reconstructions more similar to the ground truth (\u220e). The reasons are two-fold: (1) SHyRe variants have better accuracy, which encourages a more aligned property space; (2) This is a bonus of our greedy Algo. 1, which tends to pick a cell from a different column in each iteration. Cells in the same column has diminishing returns due to overlapping of En,k with same k, whereas cells in different columns remain unaffected as they have hyperedges of different sizes. The inclination of having diverse hyperedge sizes reduces the chance of a skewed distribution.\nMarkers of the same colors are cluttered, meaning that most baselines work to some extent despite low accuracy sometimes. Fig.8a also embeds a histogram for the size distribution of the reconstructed hyperedges on DBLP. SHyRe\u2019s distribution aligns decently with the ground truth, especially on large hyperedges. Some errors are made on sizes 1 and 2, which are mostly the nested cases in Fig.2.\nFig.8b visualizes a portion of the actual reconstructed hypergraph by SHyRe-count on DBLP dataset. The caption includes more explanation and analysis."
        },
        {
            "heading": "F.2.2 ADVANCED PROPERTIES",
            "text": "Inspired by Lee et al. (2022), we further compare some of the advanced structural properties between the original hypergraphs and the reconstructed hypergraphs. The advanced structural properties include simplicial closure (Benson et al., 2018a), degree distribution, singular-value distribution, density, and diameter.\nFor simplicial closure, density, and diameter, we quantify the alignment by: \u2223x1 \u2212 x2\u2223/max(x1, x2), so that it is a continuous value in [0, 1]; x1, x2 are the values of ground truths and reconstructions, respectively. This gives us a unified measurement that can be compared across different datasets. A larger value here indicates better alignment. For the alignment degree and singular values, we treat each of them as a probability density function and report the cross entropy between the two distributions. A smaller value indicates better alignment.\nThe result is presented in Table 7. We observe that our method works well at recovering the density and diameter. We also find that the degree of alignment on simplicial closure between ground truth and reconstruction seems to be correlated well with the Jaccard scores reported in our main Table 3. It is not easy though to obtain an intuitive sense of the alignment between two distributions measured by cross entropy. However, the ordering of the cross entropy on all datasets seems to decently correlate with the ordering of the Jaccard scores."
        },
        {
            "heading": "F.3 CORRELATION ANALYSIS BETWEEN HYPERGRAPH PROPERTIES AND RECONSTRUCTION PERFORMANCE",
            "text": "On some datasets, our proposed method significantly outperforms baselines; on some other datasets, our improvement is less prominent. To understand what properties of the dataset makes our proposed method most suitable, we conduct a mini-study to interpret Table 3.\nIn this mini-study, we fit a linear regression model to the performance gap (y) between our method and the best baseline, using the four basic properties of hypergraphs (X) listed in Table 6\n\u2022 average hyperedge size (\u00b5(E)); \u2022 standard deviation of hyperedge size (\u03c3(E)); \u2022 node degree in hypergraph (d(V\u0304 ) = \u2223E \u2223/\u2223V \u2223); \u2022 number of maximal cliques in projection (\u2223M\u2223);\nThe performance gap is computed by:\nbest performance of SHyRe variants \u2212 best performance of baselines (1)\nbased on Table 3. For example, for DBLP the performance gap is 81.19 \u2212 73.08 = 8.11.\nThe resultant four coefficients are shown in Figure 9. The bar plot shows that:\n\u2022 Our proposed method has more advantage on datasets with large average node degree, such as Enron, P.School, and H.School. Notice that large average node degree means that the hypergraph has densely overlapping hyperedges. For example, in P.School dataset, each node appears in an average of 64 hyperedges. The dense overlapping of hyperedges, as we have analyzed in Section 3,create highly challenging cases for reconstruction. This is why our proposed method stands out,as it can effectively leverages information embedded in the training hypergraphs.\n\u2022 Our proposed method has generally less advantage on datasets with large average hyperedge size, such as Foursquare and Directors. A possible explanation is that, compared with small hyperedge, projections from large hyperedges are less likely to mix up with each other, making the reconstruction easier. Meanwhile, baselines like max clique, clique covering, and Bayesian-MDL, all favor large cliques in reconstruction by their design. Therefore, we observe more competitive baselines on datasets with large hyperedges."
        },
        {
            "heading": "F.4 USE CASE 1: NODE RANKING IN PROTEIN-PROTEIN INTERACTION (PPI) NETWORKS",
            "text": "Background. We applied our method to PPI networks, recovering multiprotein complexes from pairwise protein interaction data. More than half of the proteins in nature form multiprotein complexes, which perform many fundamental functions such as DNA transcription, energy supply, etc. (Spirin & Mirny, 2003). Mainstream laboratory-based methods for directly detecting proteins in multiprotein complexes, such as TAP-MS (Rigaut et al., 1999) or LUMIER (Blasche & Koegl, 2013), are known to be expensive and error-prone (Br\u00fcckner et al., 2009; Hoch & Soriano, 2015). Alternatively, wellstudied high-throughput methods such as Yeast 2-hybrid screening (Y2H) (Young, 1998) captures pairwise protein interactions. Br\u00fcckner et al. (2009) mentions that \u201cMS is less accessible than Y2H due to the expensive large equipment needed. Thus, a large amount of the data so far generated from protein interaction studies have come from Y2H screening. \u201d However, many studies (Klimm et al., 2021; Spirin & Mirny, 2003; Ramadan et al., 2004) found that the pairwise PPI network produced by Y2H obscures much information compared to the hypergraph modeling of multiprotein complexes. This is a natural setting for the hypergraph reconstruction task.\nOur experiment follows the transfer learning\u2019s setting. We seek to recover multiprotein complexes from a dataset of pairwise protein interactions, Reactome Croft et al. (2010), given access to a different but similar dataset of multiprotein complexes, hu.MAP 2.0 Drew et al. (2017). After preprocessing, the training dataset has 6,292 nodes (proteins), 5119 ground truth hyperedges (multiprotein complexes); the query dataset has 8,243 nodes, 6,688 hyperedges. Our SHyRe-motif algorithm achieves a Jaccard accuracy of 0.43 (precision 0.89, recall 0.45). This means that we sucessfully recover around half of all multiproteins complexes in the query dataset, and around 90% of those recovered are correct.\nNode degrees for ranking protein essentiality. Many studies Xiao et al. (2015); Estrada (2006); Klimm et al. (2021) found that the number of interactions that one protein participates in highly correlate to the protein\u2019s essentiality in the system. The best measure for this is hypergraph node degrees. In practice, node degrees in pairwise PPI networks are often used instead \u2014 a compromise due to the technical constraints mentioned.\nRanking the proteins with highest node degrees produces Table 8, the top-10 lists for their corresponding upstream gene names in the original multiproteins hypergraph (H), the projected pairwise PPI network(G), and the recovered multiprotein hypergraph (H\u0303). The recovered hypergraph produces a list much closer to the ground truth than the pairwise graph does. Also, notice the different positions of UBB and GRB2 in all lists: GRB2 encodes the protein that binds the epidermal (skin) growth factor receptor; UBB is one of the most conserved proteins, playing major roles in many critical functions including abnormal protein degradation, gene expression, and maintenance of chromatin structure. Therefore, UBB is arguably more essential than GRB2 in cellular functions, despite interacting with fewer proteins in total. The middle list produced by our algorithm precisely captures this subtlety.\nWe further use node degrees in the ground truth hypergraph as a reference, and compare their correlation with node degrees in the projected graph and the recovered hypergraph, respectively. The latter was found to have a Pearson correlation of 0.93, higher than the former of 0.88 (p < 0.01). This means that our method helps recover protein essentiality information that is closer to the ground truth."
        },
        {
            "heading": "F.5 USE CASE 2: LINK PREDICTION",
            "text": "As the second use case, we conduct a mini-study to compare the performance of link prediction (as a downstream task) on the projected graph G, the reconstructed hypergraph H\u0302, and the original hypergraph H. To ensure fair comparison, all results are obtained using the same base model of a two-layered Graph Convolutional Network (GCN), including those on hypergraphs: The initial node features are initialized using one-hot encodings; to utilize hypergraph features when training on H\u0302 andH, we append several structural features of hyperedges to the final link embeddings, including: the number of common neighbors of the two end nodes, the number of hyperedges associated with each of the two end nodes and the average size of the hyperedges associated with each of the two end nodes, and the shortest distance (minimum number of hyperedges to traverse) between the two end nodes.\nThe results are shown in Table 9. We observe that the numerical results show that the reconstructed hypergraphs are indeed helpful to link prediction task compared to the projection graph: 3 out 4 datasets if measured by AUC, and 4 out of 4 datasets if measured by Recall. Cross referencing our Table 3, we can also see the general trend that a better-reconstructed hypergraph would lead to more boost in link prediction performance. We also note that these results are not to demonstrate hypergraph reconstruction as a state-of-the-art method for link prediction. It, however, provides a piece of evidence that the reconstructed hypergraphs are a good form of intermediate representation which, compared to their projected graphs, is more informative and helpful in downstream tasks.\nAgain, we want to emphasize that the goal of this mini-study here isn\u2019t to demonstrate the superiority of learning-based hypergraph reconstruction against the current state-of-the-art method for link prediction. In fact, SOTA methods for link prediction is often end-to-end customized, in which case the reconstructed hypergraph is not necessary. We reconstruct hypergraphs, instead of end-to-end training a model for each downstream task with reconstructing hypergraphs, because we do not know the exact downstream task when we do reconstruction, and we may not even be the person to do the downstream task. Reconstructed hypergraphs can be viewed as an intermediate product for more general purpose and versatile usage, which is similar to the role word embeddings play in language tasks."
        },
        {
            "heading": "F.6 USE CASE III: NODE CLUSTERING",
            "text": "In this third use case, we demonstrate the benefits of reconstructed hypergraphs via another classical task: node clustering. The dataset we use is P.School, which is also used in our main experiment. Its statistics can be found in Table 6. In this dataset, each node represents a student, and each hyperedge represents a face-to-face interaction among multiple students, recorded by their body-worn sensors\nduring a period. Meanwhile, each node is labeled as one of the 11 classrooms to which the student belongs. We treat these node labels as the ground truth for clustering.\nSimilar to the first two use cases, we conduct clustering on the original hypergraph, the projected graph, and the reconstructed hypergraph. To ensure fairness, we stick to spectral clustering as the clustering method. Notice that spectral clustering can be easily generalized from graphs to hypergraphs, and has been used as a classical clustering method for hypergraphs for a long time (Zhou et al., 2006). We use normalized mutual information (NMI) and confusion matrix to evaluate the performance. A larger NMI indicates better performance.\nFigure 10 shows the result. We see that our reconstructed hypergraph has much better NMI than the projected graph does. We can also visually examine the confusion matrix and derive the same conclusion. This experiment shows that the reconstructed hypergraph crucially recovers a great amount of higher-order cluster structures in the original hypergraph."
        },
        {
            "heading": "F.7 RUNNING TIME ANALYSIS",
            "text": "We have claimed that the clique sampler\u2019s complexity is close to O(\u2223M\u2223) + O(\u2223E \u2223) = O(\u2223M\u2223) in practice. Here we check this by asymptotic running time. Both the clique sampler and the hyperedge classifier are tested. For p \u2208 [30,100], We sample p% hyperedges from DBLP and record the CPU time for running both modules of SHyRe-motif. The result is plot in Fig.11. It shows that both the total CPU time and the number of maximal cliques are roughly linear to the data usage (size), which verifies our claim. Fig.12 reports statistics for all methods.\nNotice that among all baselines, only HyperSAGNN is suitable for running on GPUs. Other baselines run on CPUs. We can see that HyperSAGNN still runs slower than or on par with SHyRe in general. There are two main reasons for this. First, the original HyperSAGNN does not have any procedure for generating hyperedge candidates. Therefore, we have to adapt it so that it actually shares the same maximal clique algorithm with SHyRe. It also took a portion of time to sample the cliques from maximal cliques. Second, similar to DeepWalk, HyperSAGNN\u2019s best-performed variant relies on random walks sampling to generate initial node features, which also takes extensive time."
        },
        {
            "heading": "F.8 ABLATION STUDY ON CLIQUE SAMPLER",
            "text": "One might argue that the optimization of the clique sampler (Sec.4.2.2) appears complex: can we adopt simpler heuristics for sampling, abandoning the notion of rn,k\u2019s? We study this via ablations.\nWe test three sampling heuristics to replace the clique sampler. 1. \u201crandom\u201d: we sample \u03b2 cliques from the projection as candidates. While it is hard to achieve strict uniformness khorvash (2009), we approximate this by growing a clique from a random node and stopping when the clique reaches a random size; 2.\u201csmall\u201d: we sample \u03b2 cliques of sizes 1 and 2 (i.e. nodes and edges); 3.\u201chead & tail\u201d: we sample \u03b2 cliques from all cliques of sizes 1 and 2 as well as maximal cliques.\nFig.13 compares the efficacy in the sampling stage on Enron dataset. It shows that our clique sampler significantly outperforms all heuristics and so it cannot be replaced. Also, the the great alignment\nbetween the training curve and query curve means our clique sampler generalizes well. We further report reconstruction performance on 3 datasets in Table 10, which also confirms this point."
        },
        {
            "heading": "F.9 TASK EXTENSION I: USING EDGE MULTIPLICITIES",
            "text": "Throughout this work, we do not assume that the projected graph has edge multiplicities. Relying on edge multiplicities addresses a simpler version of the problem which might limit its applicability. That said, some applications may come with edge multiplicity information, and it is important to understand what is possible in this more tractable case. Here we provide an effective unsupervised method as a foundation for further work.\nThe multiplicity of an edge (u, v) is the number of hyperedges containing both u and v. It is not hard to show that knowledge of the edge multiplicities does not suffice to allow perfect reconstruction, and so we still must choose from among a set of available cliques to form hyperedges. In doing this with multiplicity information, we need to ensure that the cliques we select add up to the given edges multiplicities. We do this by repeatedly finding maximal cliques, removing them, and reducing the multiplicities of their edges by 1. We find that an effective heuristic is to select maximal cliques that have large size and small average edge multiplicities (combining these for example using a weighted sum).\nTable 11 gives the performance on the datasets we study. We can see that with edge multiplicities our unsupervised baseline outperforms all the methods not using edge multiplicities on most datasets, showing the power of this additional information. The performance, however, is still far from perfect, and we leave the study of this interesting extension to future work."
        },
        {
            "heading": "F.10 TASK EXTENSION II: RECONSTRUCTING FROM NODE-DEGREE-PRESERVING PROJECTION",
            "text": "Formulated in Eq.(3) of Kumar et al. (2020b), node-degree preserving projection is a novel projection method proposed in recent years that can preserve node degrees in the original hypergraph. It innovatively achieves this by scaling down the weight of each projected edge by a factor of (\u2223E\u2223 \u2212 1), where \u2223E\u2223 is the size of the corresponding hyperedge.\nOur reconstruction method can naturally extend to this novel type of projection. The reason is that our method has solved a strictly harder version of the reconstruction problem: our problem setting does not require the projected graph to have edge multiplicities (weight). In comparison, node-degree preserving projection not only produces the same set of edges as our projection does, but also provides edge weights. More importantly, node-degree-preserving projection (as its name suggests) provides the degree of each node in the original hypergraph, which is a useful piece of information in reconstruction. This information can be seamlessly integrated into our framework, as follows.\nFor each target clique:\n1. We obtain the degree of each of its node in the original hypergraph (by computing that node\u2019s degree in the node-degree-preserving projection).\n2. We compute the min and average of this new type of \u201cnode degree\u201d for all nodes in the target clique.\n3. We append the two features obtained in the last step to the \u201ccount\u201d feature vector used in SHyRe-count.\nThis gives us a customized SHyRe-count model for node-degree-preserving projection. We train and test this customized SHyRe-count model on all the 8 datasets we used. The results are presented in Table 12. We observe that SHyRe\u2019s reconstruction performance on every dataset\u2019s node-degreepreserving projection is better than its performance on the regular projection used throughout this paper. These results validate our claim that our reconstruction method can naturally extend to the novel node-degree-preserving projection."
        },
        {
            "heading": "F.11 STORAGE COMPARISON",
            "text": "A side bonus of having a reconstructed hypergraph versus a projected graph is that the former typically requires much less storage space. As a mini-study, we compare the storage of each hypergraph, its projected graph, and its reconstruction generated by SHyRe-count. We use the unified data structure of a nested array to represent the list of edges/hyperedges. Each node is indexed by an int64. Fig.14 visualizes the results. We see that the reconstructions take 1 to 2 orders of magnitude less storage space than the projected graphs and are closest to the originals."
        },
        {
            "heading": "F.12 MORE EXPERIMENTAL RESULTS",
            "text": ""
        },
        {
            "heading": "G MORE DISCUSSION ON LIMITATIONS AND FUTURE WORK",
            "text": ""
        },
        {
            "heading": "G.1 DIRECTION I: OPTIMIZING SAMPLING RATES USING DOWNSTREAM CLASSIFICATION LOSS",
            "text": "One limitation in the presented reconstruction method is that it does not adjust the sampling rate (i.e., the rn,k\u2019s) based on the downstream classifier\u2019s performance on cliques sampled at different (n, k)\u2019s. In other words, because we have a constraint budget, ideally we should allocate more sampling budget to sample the types of cliques on which the downstream classifier excels at making judgement. However, under the current framework, our framework is not optimizable in an end-to-end fashion using gradient descent. Formally speaking, the current reconstruction framework is essentially optimizing the following objective f(r, \u03b8):\nf(r, \u03b8) =max r min \u03b8 ED\u223cp(D\u2223r,H)L(\u03b8,D) (2)\nwhere r is the sampling rates of all cells, i.e., the vector of all rn,k\u2019s as defined under Sec.4.2.1; \u03b8 is the parameters of our hyperedge sampler; D is the distribution of all possible hyperedge candidate sets that we can construct based on given sample rates r and the training hypergraph H; D \u2208 D is the specific instance of hyperedge candidate set that we end up sampling and constructing.\nIt would obviously be highly desirable if we can optimize f with respect to both r and \u03b8 using gradient descent. Fixing r, to optimize f with respect to \u03b8 via gradient descent is straightforward, which is exactly the job done by our current hyperedge classifier. However, optimizing f with respect to both r is much trickier. Notice that f depends on r through D which essentially is a discrete data\nrepresentation, i.e., a set of cliques. This blocks the gradient flow and can could be very challenging to address.\nWe propose a simple workaround here which is to alternating the optimization of r and \u03b8, in two phases:\n\u2022 In phase 1, we fix the parameters \u03b8 of our hyperedge classifier, and optimize the parameter r of our clique sampler using Algorithm 1 in the paper;\n\u2022 In phase 2, we fix the parameter r of our clique sampler, and optimize the parameters \u03b8 of our hyperedge classifier by standard supervised learning.\nThe proposed solution above works in a similar manner as k-means. However, it is not guaranteed that the global minimum will be reached. We consider this as a very intriguing direction to explore in the future."
        },
        {
            "heading": "G.2 USING ATTRIBUTES IN RECONSTRUCTION",
            "text": "Another direction we may consider is the usage of node attributes in the learning-based framework. Our current method targets the most difficult version of the reconstruction problem: we don\u2019t assume nodes or edges to have attributes, and the reconstruction is purely based on leveraging structural information in the projected graph. It would be interesting to think about how we can effectively integrating node attributes into the picture, since in practice we may be able to collect some information about nodes in the projected graph. This is a nontrivial problem to research though, because the projected graph has special clique structures (and with max cliques that we have preprocessed). Therefore, how to conduct graph learning most effectively on these type of clique structures could be something worth exploring in the future."
        }
    ],
    "year": 2023
}