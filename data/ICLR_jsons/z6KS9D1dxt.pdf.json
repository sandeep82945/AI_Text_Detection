{
    "abstractText": "In this study, we explore the robustness of cooperative multi-agent reinforcement learning (c-MARL) against Byzantine failures, where any agent can enact arbitrary, worst-case actions due to malfunction or adversarial attack. To address the uncertainty that any agent can be adversarial, we propose a Bayesian Adversarial Robust Dec-POMDP (BARDec-POMDP) framework, which views Byzantine adversaries as nature-dictated types, represented by a separate transition. This allows agents to learn policies grounded on their posterior beliefs about the type of other agents, fostering collaboration with identified allies and minimizing vulnerability to adversarial manipulation. We define the optimal solution to the BARDec-POMDP as an ex interim robust Markov perfect Bayesian equilibrium, which we proof to exist and the corresponding policy weakly dominates previous approaches as time goes to infinity. To realize this equilibrium, we put forward a two-timescale actor-critic algorithm with almost sure convergence under specific conditions. Experiments on matrix game, Level-based Foraging and StarCraft II indicate that, our method successfully acquires intricate micromanagement skills and adaptively aligns with allies under worst-case perturbations, showing resilience against non-oblivious adversaries, random allies, observation-based attacks, and transfer-based attacks.",
    "authors": [
        {
            "affiliations": [],
            "name": "BAYESIAN GAME"
        },
        {
            "affiliations": [],
            "name": "Simin Li"
        },
        {
            "affiliations": [],
            "name": "Jun Guo"
        },
        {
            "affiliations": [],
            "name": "Jingqiao Xiu"
        },
        {
            "affiliations": [],
            "name": "Ruixiao Xu"
        },
        {
            "affiliations": [],
            "name": "Xin Yu"
        },
        {
            "affiliations": [],
            "name": "Jiakai Wang"
        },
        {
            "affiliations": [],
            "name": "Aishan Liu"
        },
        {
            "affiliations": [],
            "name": "Yaodong Yang"
        },
        {
            "affiliations": [],
            "name": "Xianglong Liu"
        }
    ],
    "id": "SP:ddbf84d1550ef65c50469cc7cfcfe913608df09d",
    "references": [
        {
            "authors": [
                "Stefano V Albrecht",
                "Subramanian Ramamoorthy"
            ],
            "title": "A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems",
            "venue": "arXiv preprint arXiv:1506.01170,",
            "year": 2015
        },
        {
            "authors": [
                "Stefano V Albrecht",
                "Jacob W Crandall",
                "Subramanian Ramamoorthy"
            ],
            "title": "Belief and truth in hypothesised behaviours",
            "venue": "Artificial Intelligence,",
            "year": 2016
        },
        {
            "authors": [
                "Karl J Astrom"
            ],
            "title": "Optimal control of markov processes with incomplete state information",
            "venue": "Journal of mathematical analysis and applications,",
            "year": 1965
        },
        {
            "authors": [
                "Samuel Barrett",
                "Avi Rosenfeld",
                "Sarit Kraus",
                "Peter Stone"
            ],
            "title": "Making friends on the fly: Cooperating with new teammates",
            "venue": "Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Christopher Berner",
                "Greg Brockman",
                "Brooke Chan",
                "Vicki Cheung",
                "Przemys\u0142aw D\u0119biak",
                "Christy Dennison",
                "David Farhi",
                "Quirin Fischer",
                "Shariq Hashme",
                "Chris Hesse"
            ],
            "title": "Dota 2 with large scale deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1912.06680,",
            "year": 2019
        },
        {
            "authors": [
                "Daniel S Bernstein",
                "Robert Givan",
                "Neil Immerman",
                "Shlomo Zilberstein"
            ],
            "title": "The complexity of decentralized control of markov decision processes",
            "venue": "Mathematics of operations research,",
            "year": 2002
        },
        {
            "authors": [
                "Vivek S Borkar"
            ],
            "title": "Stochastic approximation with two time scales",
            "venue": "Systems & Control Letters,",
            "year": 1997
        },
        {
            "authors": [
                "Vivek S Borkar"
            ],
            "title": "Stochastic approximation: a dynamical systems viewpoint, volume 48",
            "year": 2009
        },
        {
            "authors": [
                "Vivek S Borkar",
                "Sean P Meyn"
            ],
            "title": "The ode method for convergence of stochastic approximation and reinforcement learning",
            "venue": "SIAM Journal on Control and Optimization,",
            "year": 2000
        },
        {
            "authors": [
                "Tianshu Chu",
                "Jie Wang",
                "Lara Codec\u00e0",
                "Zhaojian Li"
            ],
            "title": "Multi-agent deep reinforcement learning for large-scale traffic signal control",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Junyoung Chung",
                "Caglar Gulcehre",
                "KyungHyun Cho",
                "Yoshua Bengio"
            ],
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "venue": "arXiv preprint arXiv:1412.3555,",
            "year": 2014
        },
        {
            "authors": [
                "Constantinos Daskalakis",
                "Dylan J Foster",
                "Noah Golowich"
            ],
            "title": "Independent policy gradient methods for competitive reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Christian Schroeder de Witt",
                "Tarun Gupta",
                "Denys Makoviichuk",
                "Viktor Makoviychuk",
                "Philip HS Torr",
                "Mingfei Sun",
                "Shimon Whiteson"
            ],
            "title": "Is independent learning all you need in the starcraft multi-agent challenge",
            "year": 2011
        },
        {
            "authors": [
                "Persi Diaconis",
                "David Freedman"
            ],
            "title": "On the consistency of bayes estimates",
            "venue": "The Annals of Statistics, pp",
            "year": 1986
        },
        {
            "authors": [
                "Le Cong Dinh",
                "David Henry Mguni",
                "Long Tran-Thanh",
                "Jun Wang",
                "Yaodong Yang"
            ],
            "title": "Online markov decision processes with non-oblivious strategic adversary",
            "venue": "Autonomous Agents and Multi-Agent Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Arlington M Fink"
            ],
            "title": "Equilibrium in a stochastic n-person game. Journal of science of the hiroshima university, series ai (mathematics)",
            "year": 1964
        },
        {
            "authors": [
                "Adam Gleave",
                "Michael Dennis",
                "Cody Wild",
                "Neel Kant",
                "Sergey Levine",
                "Stuart Russell"
            ],
            "title": "Adversarial policies: Attacking deep reinforcement learning",
            "year": 1905
        },
        {
            "authors": [
                "Barbara J Grosz",
                "Sarit Kraus"
            ],
            "title": "The evolution of sharedplans",
            "venue": "Foundations of rational agency,",
            "year": 1999
        },
        {
            "authors": [
                "Wenbo Guo",
                "Xian Wu",
                "Sui Huang",
                "Xinyu Xing"
            ],
            "title": "Adversarial policy learning in two-player competitive games",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Songyang Han",
                "Sanbao Su",
                "Sihong He",
                "Shuo Han",
                "Haizhao Yang",
                "Fei Miao"
            ],
            "title": "What is the solution for state adversarial multi-agent reinforcement learning",
            "venue": "arXiv preprint arXiv:2212.02705,",
            "year": 2022
        },
        {
            "authors": [
                "John C Harsanyi"
            ],
            "title": "Games with incomplete information played by \u201cbayesian",
            "venue": "players, i\u2013iii part i. the basic model. Management science,",
            "year": 1967
        },
        {
            "authors": [
                "Sihong He",
                "Songyang Han",
                "Sanbao Su",
                "Shuo Han",
                "Shaofeng Zou",
                "Fei Miao"
            ],
            "title": "Robust multi-agent reinforcement learning with state uncertainty",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Johannes Heinrich",
                "David Silver"
            ],
            "title": "Deep reinforcement learning from self-play in imperfectinformation games",
            "venue": "arXiv preprint arXiv:1603.01121,",
            "year": 2016
        },
        {
            "authors": [
                "Maximilian H\u00fcttenrauch",
                "Sosic Adrian",
                "Gerhard Neumann"
            ],
            "title": "Deep reinforcement learning for swarm systems",
            "venue": "Journal of Machine Learning Research,",
            "year": 2019
        },
        {
            "authors": [
                "Garud N Iyengar"
            ],
            "title": "Robust dynamic programming",
            "venue": "Mathematics of Operations Research,",
            "year": 2005
        },
        {
            "authors": [
                "Fivos Kalogiannis",
                "Ioannis Anagnostides",
                "Ioannis Panageas",
                "Vaggos Chatziafratis",
                "Stelios Stavroulakis"
            ],
            "title": "Efficiently computing nash equilibria in adversarial team markov games",
            "venue": "arXiv preprint arXiv:2208.02204,",
            "year": 2022
        },
        {
            "authors": [
                "Erim Karde\u015f",
                "Fernando Ord\u00f3\u00f1ez",
                "Randolph W Hall"
            ],
            "title": "Discounted robust stochastic games and an application to queueing control",
            "venue": "Operations research,",
            "year": 2011
        },
        {
            "authors": [
                "Jakub Grudzien Kuba",
                "Ruiqing Chen",
                "Muning Wen",
                "Ying Wen",
                "Fanglei Sun",
                "Jun Wang",
                "Yaodong Yang"
            ],
            "title": "Trust region policy optimisation in multi-agent reinforcement learning",
            "venue": "arXiv preprint arXiv:2109.11251,",
            "year": 2021
        },
        {
            "authors": [
                "Shihui Li",
                "Yi Wu",
                "Xinyue Cui",
                "Honghua Dong",
                "Fei Fang",
                "Stuart Russell"
            ],
            "title": "Robust multi-agent reinforcement learning via minimax deep deterministic policy gradient",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Jieyu Lin",
                "Kristina Dzeparoska",
                "Sai Qian Zhang",
                "Alberto Leon-Garcia",
                "Nicolas Papernot"
            ],
            "title": "On the robustness of cooperative multi-agent reinforcement learning",
            "venue": "IEEE Security and Privacy Workshops (SPW),",
            "year": 2020
        },
        {
            "authors": [
                "Yen-Chen Lin",
                "Zhang-Wei Hong",
                "Yuan-Hong Liao",
                "Meng-Li Shih",
                "Ming-Yu Liu",
                "Min Sun"
            ],
            "title": "Tactics of adversarial attack on deep reinforcement learning agents",
            "venue": "arXiv preprint arXiv:1703.06748,",
            "year": 2017
        },
        {
            "authors": [
                "Aishan Liu",
                "Xianglong Liu",
                "Jiaxin Fan",
                "Yuqing Ma",
                "Anlan Zhang",
                "Huiyuan Xie",
                "Dacheng Tao"
            ],
            "title": "Perceptual-sensitive gan for generating adversarial patches",
            "venue": "In AAAI,",
            "year": 2019
        },
        {
            "authors": [
                "Aishan Liu",
                "Tairan Huang",
                "Xianglong Liu",
                "Yitao Xu",
                "Yuqing Ma",
                "Xinyun Chen",
                "Stephen J Maybank",
                "Dacheng Tao"
            ],
            "title": "Spatiotemporal attacks for embodied agents",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Aishan Liu",
                "Jiakai Wang",
                "Xianglong Liu",
                "Bowen Cao",
                "Chongzhi Zhang",
                "Hang Yu"
            ],
            "title": "Bias-based universal adversarial patch attack for automatic check-out",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Aishan Liu",
                "Jun Guo",
                "Jiakai Wang",
                "Siyuan Liang",
                "Renshuai Tao",
                "Wenbo Zhou",
                "Cong Liu",
                "Xianglong Liu",
                "Dacheng Tao"
            ],
            "title": "X-adv: Physical adversarial object attacks against x-ray prohibited item detection",
            "venue": "In USENIX Security Symposium,",
            "year": 2023
        },
        {
            "authors": [
                "Ryan Lowe",
                "Yi Wu",
                "Aviv Tamar",
                "Jean Harb",
                "OpenAI Pieter Abbeel",
                "Igor Mordatch"
            ],
            "title": "Multi-agent actor-critic for mixed cooperative-competitive environments",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Daniel J Mankowitz",
                "Nir Levine",
                "Rae Jeong",
                "Yuanyuan Shi",
                "Jackie Kay",
                "Abbas Abdolmaleki",
                "Jost Tobias Springenberg",
                "Timothy Mann",
                "Todd Hester",
                "Martin Riedmiller"
            ],
            "title": "Robust reinforcement learning for continuous control with model misspecification",
            "venue": "arXiv preprint arXiv:1906.07516,",
            "year": 2019
        },
        {
            "authors": [
                "Reuth Mirsky",
                "Ignacio Carlucho",
                "Arrasy Rahman",
                "Elliot Fosong",
                "William Macke",
                "Mohan Sridharan",
                "Peter Stone",
                "Stefano V Albrecht"
            ],
            "title": "A survey of ad hoc teamwork research",
            "venue": "In Multi-Agent Systems: 19th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Arnab Nilim",
                "Laurent El Ghaoui"
            ],
            "title": "Robust control of markov decision processes with uncertain transition matrices",
            "venue": "Operations Research,",
            "year": 2005
        },
        {
            "authors": [
                "Eleni Nisioti",
                "Daan Bloembergen",
                "Michael Kaisers"
            ],
            "title": "Robust multi-agent q-learning in cooperative games with adversaries",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Frans A Oliehoek",
                "Christopher Amato"
            ],
            "title": "A concise introduction to decentralized POMDPs",
            "year": 2016
        },
        {
            "authors": [
                "Frans A Oliehoek",
                "Matthijs TJ Spaan",
                "Nikos Vlassis"
            ],
            "title": "Optimal and approximate q-value functions for decentralized pomdps",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2008
        },
        {
            "authors": [
                "Georgios Papoudakis",
                "Filippos Christianos",
                "Lukas Sch\u00e4fer",
                "Stefano V Albrecht"
            ],
            "title": "Benchmarking multi-agent deep reinforcement learning algorithms in cooperative tasks",
            "venue": "arXiv preprint arXiv:2006.07869,",
            "year": 2020
        },
        {
            "authors": [
                "Bei Peng",
                "Tabish Rashid",
                "Christian Schroeder de Witt",
                "Pierre-Alexandre Kamienny",
                "Philip Torr",
                "Wendelin B\u00f6hmer",
                "Shimon Whiteson"
            ],
            "title": "Facmac: Factored multi-agent centralised policy gradients",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Thomy Phan",
                "Thomas Gabor",
                "Andreas Sedlmeier",
                "Fabian Ritz",
                "Bernhard Kempter",
                "Cornel Klein",
                "Horst Sauer",
                "Reiner Schmid",
                "Jan Wieghardt",
                "Marc Zeller"
            ],
            "title": "Learning and testing resilience in cooperative multi-agent systems",
            "venue": "In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Thomy Phan",
                "Lenz Belzner",
                "Thomas Gabor",
                "Andreas Sedlmeier",
                "Fabian Ritz",
                "Claudia LinnhoffPopien"
            ],
            "title": "Resilient multi-agent reinforcement learning with adversarial value decomposition",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Lerrel Pinto",
                "James Davidson",
                "Rahul Sukthankar",
                "Abhinav Gupta"
            ],
            "title": "Robust adversarial reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Arrasy Rahman",
                "Ignacio Carlucho",
                "Niklas H\u00f6pner",
                "Stefano V Albrecht"
            ],
            "title": "A general learning framework for open ad hoc teamwork using graph-based policy learning",
            "venue": "arXiv preprint arXiv:2210.05448,",
            "year": 2022
        },
        {
            "authors": [
                "Tabish Rashid",
                "Mikayel Samvelyan",
                "Christian Schroeder",
                "Gregory Farquhar",
                "Jakob Foerster",
                "Shimon Whiteson"
            ],
            "title": "Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Manish Chandra Reddy Ravula"
            ],
            "title": "Ad-hoc teamwork with behavior-switching agents",
            "venue": "PhD thesis,",
            "year": 2019
        },
        {
            "authors": [
                "Mikayel Samvelyan",
                "Tabish Rashid",
                "Christian Schroeder De Witt",
                "Gregory Farquhar",
                "Nantas Nardelli",
                "Tim GJ Rudner",
                "Chia-Man Hung",
                "Philip HS Torr",
                "Jakob Foerster",
                "Shimon Whiteson"
            ],
            "title": "The starcraft multi-agent challenge",
            "year": 1902
        },
        {
            "authors": [
                "John Schulman",
                "Philipp Moritz",
                "Sergey Levine",
                "Michael Jordan",
                "Pieter Abbeel"
            ],
            "title": "High-dimensional continuous control using generalized advantage estimation",
            "venue": "arXiv preprint arXiv:1506.02438,",
            "year": 2015
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Yoav Shoham",
                "Kevin Leyton-Brown"
            ],
            "title": "Multiagent systems: Algorithmic, game-theoretic, and logical foundations",
            "year": 2008
        },
        {
            "authors": [
                "Peter Stone",
                "Gal Kaminka",
                "Sarit Kraus",
                "Jeffrey Rosenschein"
            ],
            "title": "Ad hoc autonomous agent teams: Collaboration without pre-coordination",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2010
        },
        {
            "authors": [
                "DJ Strouse",
                "Kevin McKee",
                "Matt Botvinick",
                "Edward Hughes",
                "Richard Everett"
            ],
            "title": "Collaborating with humans without human data",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Chuangchuang Sun",
                "Dong-Ki Kim",
                "Jonathan P How"
            ],
            "title": "Romax: Certifiably robust deep multiagent reinforcement learning via convex relaxation",
            "venue": "In 2022 International Conference on Robotics and Automation (ICRA),",
            "year": 2022
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Aviv Tamar",
                "Huan Xu",
                "Shie Mannor"
            ],
            "title": "Scaling up robust mdps by reinforcement learning",
            "venue": "arXiv preprint arXiv:1306.6189,",
            "year": 2013
        },
        {
            "authors": [
                "Chen Tessler",
                "Yonathan Efroni",
                "Shie Mannor"
            ],
            "title": "Action robust reinforcement learning and applications in continuous control",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Paul Tylkin",
                "Goran Radanovic",
                "David C Parkes"
            ],
            "title": "Learning robust helpful behaviors in twoplayer cooperative atari environments",
            "venue": "In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Igor Babuschkin",
                "Wojciech M Czarnecki",
                "Micha\u00ebl Mathieu",
                "Andrew Dudzik",
                "Junyoung Chung",
                "David H Choi",
                "Richard Powell",
                "Timo Ewalds",
                "Petko Georgiev"
            ],
            "title": "Grandmaster level in starcraft ii using multi-agent reinforcement learning",
            "year": 2019
        },
        {
            "authors": [
                "Jiakai Wang",
                "Aishan Liu",
                "Zixin Yin",
                "Shunchang Liu",
                "Shiyu Tang",
                "Xianglong Liu"
            ],
            "title": "Dual attention suppression attack: Generate adversarial camouflage in physical world",
            "year": 2021
        },
        {
            "authors": [
                "Jingkang Wang",
                "Yang Liu",
                "Bo Li"
            ],
            "title": "Reinforcement learning with perturbed rewards",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Muning Wen",
                "Jakub Kuba",
                "Runji Lin",
                "Weinan Zhang",
                "Ying Wen",
                "Jun Wang",
                "Yaodong Yang"
            ],
            "title": "Multiagent reinforcement learning is a sequence modeling problem",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Wolfram Wiesemann",
                "Daniel Kuhn",
                "Ber\u00e7 Rustem"
            ],
            "title": "Robust markov decision processes",
            "venue": "Mathematics of Operations Research,",
            "year": 2013
        },
        {
            "authors": [
                "Xian Wu",
                "Wenbo Guo",
                "Hua Wei",
                "Xinyu Xing"
            ],
            "title": "Adversarial policy training against deep reinforcement learning",
            "venue": "In 30th USENIX Security Symposium (USENIX Security 21),",
            "year": 2021
        },
        {
            "authors": [
                "Lei Xi",
                "Jianfeng Chen",
                "Yuehua Huang",
                "Yanchun Xu",
                "Lang Liu",
                "Yimin Zhou",
                "Yudan Li"
            ],
            "title": "Smart generation control based on multi-agent reinforcement learning with the idea of the time",
            "venue": "tunnel. Energy,",
            "year": 2018
        },
        {
            "authors": [
                "Annie Xie",
                "Shagun Sodhani",
                "Chelsea Finn",
                "Joelle Pineau",
                "Amy Zhang"
            ],
            "title": "Robust policy learning over multiple uncertainty sets",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Wanqi Xue",
                "Wei Qiu",
                "Bo An",
                "Zinovi Rabinovich",
                "Svetlana Obraztsova",
                "Chai Kiat Yeo"
            ],
            "title": "Mis-spoke or mis-lead: Achieving robustness in multi-agent communicative reinforcement learning",
            "venue": "arXiv preprint arXiv:2108.03803,",
            "year": 2021
        },
        {
            "authors": [
                "Dong Yin",
                "Yudong Chen",
                "Ramchandran Kannan",
                "Peter Bartlett"
            ],
            "title": "Byzantine-robust distributed learning: Towards optimal statistical rates",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Chao Yu",
                "Akash Velu",
                "Eugene Vinitsky",
                "Yu Wang",
                "Alexandre Bayen",
                "Yi Wu"
            ],
            "title": "The surprising effectiveness of ppo in cooperative, multi-agent games",
            "venue": "arXiv preprint arXiv:2103.01955,",
            "year": 2021
        },
        {
            "authors": [
                "Huan Zhang",
                "Hongge Chen",
                "Chaowei Xiao",
                "Bo Li",
                "Mingyan Liu",
                "Duane Boning",
                "Cho-Jui Hsieh"
            ],
            "title": "Robust deep reinforcement learning against adversarial perturbations on state observations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Huan Zhang",
                "Hongge Chen",
                "Duane Boning",
                "Cho-Jui Hsieh"
            ],
            "title": "Robust reinforcement learning on state observations with learned optimal adversary",
            "venue": "arXiv preprint arXiv:2101.08452,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiqing Zhang",
                "Bin Hu",
                "Tamer Basar"
            ],
            "title": "On the stability and convergence of robust adversarial reinforcement learning: A case study on linear quadratic systems",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiqing Zhang",
                "Tao Sun",
                "Yunzhe Tao",
                "Sahika Genc",
                "Sunil Mallya",
                "Tamer Basar"
            ],
            "title": "Robust multiagent reinforcement learning with model uncertainty",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Eric Zhao",
                "Alexander R Trott",
                "Caiming Xiong",
                "Stephan Zheng"
            ],
            "title": "Ermas: Learning policies robust to reality gaps in multi-agent",
            "year": 2020
        },
        {
            "authors": [
                "Yifan Zhong",
                "Jakub Grudzien Kuba",
                "Siyi Hu",
                "Jiaming Ji",
                "Yaodong Yang"
            ],
            "title": "Heterogeneous-agent reinforcement learning",
            "venue": "arXiv preprint arXiv:2304.09870,",
            "year": 2023
        },
        {
            "authors": [
                "Ziyuan Zhou",
                "Guanjun Liu"
            ],
            "title": "Robustness testing for multi-agent reinforcement learning: State perturbations on critical agents",
            "venue": "arXiv preprint arXiv:2306.06136,",
            "year": 2023
        },
        {
            "authors": [
                "equicontinuous. Lemma A"
            ],
            "title": "\u03c6(\u03c0\u2212i) is a convex set. Proof. The proof follows Theorem 4 of Karde\u015f et al",
            "year": 2011
        },
        {
            "authors": [
                "H a"
            ],
            "title": "i). Thus, the update rule of \u03c0 and \u03c0\u0302 follows the general update rule of stochastic optimization (Borkar",
            "year": 2009
        },
        {
            "authors": [
                "Samvelyan"
            ],
            "title": "2019) is the most commly used testbed for c-MARL, which agents control a team of red agents and seeks to win a team of blue agents with built-in AIs. We adapt the map 3m proposed in SMAC testbed to 4m vs 3m, with one agent as adversary. Thus, the map can still be viewed as 3m, albeit with one adversary agents trying to fool its teammates. Note that the adversary cannot attack its allies by design of SMAC environment",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Cooperative multi-agent reinforcement learning (c-MARL) (Rashid et al., 2018; Yu et al., 2021; Kuba et al., 2021) has shown remarkable efficacy in managing groups of agents with aligned interests in complex tasks (Vinyals et al., 2019; Berner et al., 2019). Nevertheless, real-world applications often deviates from the presumption of full cooperation. In robot swarm control (H\u00fcttenrauch et al., 2019), individual robots may act unpredictably due to hardware or software malfunctions, or even display worst-case adversarial actions if compromised by a non-oblivious adversary (Gleave et al., 2019; Lin et al., 2020; Dinh et al., 2023; Liu et al., 2019; 2020a;b; 2023; Wang et al., 2021). Such uncertainty of allies undermine the cooperative premise of c-MARL, rendering the learned policy non-robust.\nIn single-agent reinforcement learning (RL), robustness under uncertainty is addressed through a maximin optimization between an uncertainty set and a robust agent within the framework of robust Markov Decision Processes (MDPs) (Nilim & El Ghaoui, 2005; Iyengar, 2005; Wiesemann et al., 2013; Pinto et al., 2017; Tessler et al., 2019; Zhang et al., 2020a). However, ensuring robustness in c-MARL when dealing with uncertain allies presents a greater challenge. This is largely due to the potential for Byzantine failure (Yin et al., 2018; Xue et al., 2021), situations where defenders are left in the dark regarding which ally may be compromised and what their resulting actions might be.\nTo address Byzantine failures, we employ a Bayesian game approach, which treats Byzantine adversaries as types assigned by nature, with each agent operating unaware of others\u2019 type. We formalize robust c-MARL as a Bayesian Adversarial Robust Dec-POMDP (BARDec-POMDP), where existing robust MARL researches (Li et al., 2019; Sun et al., 2022; Phan et al., 2020; 2021) can be reinterpreted as pursuing an ex ante equilibrium (Shoham & Leyton-Brown, 2008), viewing\n\u2217Corresponding Authors. E-mails: yaodong.yang@pku.edu.cn, xlliu@buaa.edu.cn.\nall other agents as potential adversaries. However, these methods might not yield optimal outcomes as they can mask the trade-offs between the equilibria that cooperative and robustness-focused agents aim for. Moreover, this approach can result in overly conservative strategies (Li et al., 2019; Sun et al., 2022), given the low likelihood of adversaries taking control of all agents.\nInstead, we seek an ex interim mixed-strategy robust Markov perfect Bayesian equilibrium, which weakly dominates the policy of ex ante equilibrium in previous robust MARL studies as time goes to infinity. Agents in our ex interim equilibrium makes decisions based on its inferred posterior belief over other agents, enhancing cooperation with allies and defense against adversaries concurrently. To realize this equilibrium, we derive a robust Harsanyi-Bellman equation for value function update and introduce a two-timescale actor-critic algorithm, with almost sure convergence under certain assumptions. Experiments in matrix game, Level-based Foraging and StarCraft II shows our defense exhibits intricate micromanagement skills and adaptively aligns with allies under worst-case perturbations. Consequently, our defense outperforms existing baselines under non-oblivious adversaries, random allies, observation-based attacks and transfer-based attacks by large margins.\nContribution. Our contributions are two-fold: first, we theoretically formulate Byzantine adversaries in c-MARL as a BARDec-POMDP, and concurrently pursues robustness and cooperation by targeting an ex interim equilibrium. Secondly, to achieve this equilibrium, we devise an actor-critic algorithm that ensures almost sure convergence under certain conditions. Empirically, our method exhibits greater resilience against a broad spectrum of adversaries on three c-MARL environments.\nRelated Work. Our research belongs to the field of robust RL, theoretically framed as robust MDPs (Nilim & El Ghaoui, 2005; Iyengar, 2005; Tamar et al., 2013; Wiesemann et al., 2013). This framework trains a defender to counteract a worst-case adversary amid uncertainty, which can stem from environment transitions (Pinto et al., 2017; Mankowitz et al., 2019), actions (Tessler et al., 2019), states (Zhang et al., 2020a; 2021) and rewards (Wang et al., 2020). In robust MARL, action uncertainty has been a central focus. M3DDPG (Li et al., 2019) enhances robustness in MARL through agents taking jointly worst-case actions under a small perturbation budget. Evaluation was done via one agent consistently introducing worst-case perturbations. This is later known as adversarial policy (Gleave et al., 2019) or non-oblivious adversary (Dinh et al., 2023), a practical and detrimental form of attack. Follow-up works either enhanced M3DDPG (Sun et al., 2022) or defended against uncertain adversaries by presupposing each agent as potentially adversarial (Nisioti et al., 2021; Phan et al., 2020; 2021), which our BARDec-POMDP formulation interprets as seeking a conservative ex ante equilibrium. Another approach by Kalogiannis et al. (2022) studies a special case that the adversary is known. Besides action perturbation, studies have also explored robust MARL under uncertainties in reward (Zhang et al., 2020c), environmental dynamics (Zhao et al., 2020), and observations (Han et al., 2022; He et al., 2023; Zhou & Liu, 2023).\nBayesian games and their MARL applications represents another relevant field. With roots in Harsanyi\u2019s pioneering work (Harsanyi, 1967), Bayesian games have been used to analyze games with incomplete information by transforming them into complete information games featuring chance moves made by nature. Within MARL, Bayesian games have been utilized to coordinate varying agent types, a concept known as ad hoc coordination (Albrecht & Ramamoorthy, 2015; Albrecht et al., 2016; Stone et al., 2010; Barrett et al., 2017). This problem was theoretically framed as a stochastic Bayesian game and solved using the Harsanyi-Bellman ad hoc coordination algorithm (Albrecht & Ramamoorthy, 2015). Subsequent research has concentrated on agents with varying types (Ravula, 2019), open ad hoc teamwork (Rahman et al., 2022), and human coordination (Tylkin et al., 2021; Strouse et al., 2021). Our work differs from these works by assuming a worst-case, non-oblivious adversary with conflicting goals, whereas in ad hoc coordination, agents have common goals and non-conflicting secondary objectives (Grosz & Kraus, 1999; Mirsky et al., 2022)."
        },
        {
            "heading": "2 PROBLEM FORMULATION",
            "text": ""
        },
        {
            "heading": "2.1 COOPERATIVE MARL AND ITS FORMULATION",
            "text": "The problem of c-MARL can be formulated as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Oliehoek & Amato, 2016), defined as a tuple:\nG := \u27e8N ,S,O, O,A,P, R, \u03b3\u27e9,\nwhere N = {1, ..., N} is the set of N agents, S is the global state space, O = \u00d7i\u2208NOi is the observation space, with O the observation emission function. A = \u00d7i\u2208NAi is the joint action space, P : S \u00d7A \u2192 \u2206(S) is the state transition probability, mapping from current state and joint actions to a probability distribution over the state space. R : S \u00d7A \u2192 R is the shared reward function for cooperative agents and \u03b3 \u2208 [0, 1) is the discount factor. At time t and global state st \u2208 S, each agent i adds current observation oit to its history and gets Hit = [o i 0, a i 0, ...o i t]. Then, each agent i selects its action a i t \u2208 Ai using its policy \u03c0i(\u00b7|Hit), which maps current history to its action space. The global state then transitions to st+1 according to transition probability P(st+1|st,at), with at = {a1t , ..., aNt } the joint actions. Each agent receives a shared reward rt = R(st,at). The goal of all agents is to learn a joint policy \u03c0 = \u220f i\u2208N \u03c0\ni that maximize the long-term return J(\u03c0) = E [ \u2211\u221e t=0 \u03b3 trt|s0,at \u223c \u03c0(\u00b7|Ht)]."
        },
        {
            "heading": "2.2 BAYESIAN ADVERSARIAL ROBUST DEC-POMDP",
            "text": "In numerous real-world situations, some allies may experience Byzantine failure (Yin et al., 2018; Xue et al., 2021) and thus, not perform cooperative actions as expected. This includes random actions due to hardware/software error and adversarial actions if being controlled by an adversary, which violates the fully cooperative assumption in Dec-POMDP. We propose Bayesian Adversarial Robust Dec-POMDP (BARDec-POMDP) to cope with uncertainties in agent actions, defined as follows:\nG\u0302 := \u27e8N ,S,\u0398,O, O,A,P\u03b1,P, R, \u03b3\u27e9, where N , S, O, O, A, \u03b3 represent the number of agents, global state space, observation space, observation emission function, joint action space and discount factor, following Dec-POMDP.\nAs depicted in Fig. 1, BARDec-POMDP views the Byzantine adversary as an uncertain transition characterized by type \u03b8 and adversarial policy \u03c0\u0302. At the start of each episode, a type \u03b8 is selected from the type space \u0398 = \u00d7i\u2208N\u0398i, with \u03b8i = {0, 1}. \u03b8i = 0 indicates the agent is cooperative and \u0398i = 1 signifies adversaries. At time t, if agent i is assigned \u03b8i = 1, the action ait taken by cooperative agent i with policy \u03c0i(\u00b7|Hit) is replaced by action a\u0302it sampled from an adversary with policy \u03c0\u0302i(\u00b7|Hit , \u03b8). The attack process is characterized by action perturbation probability P\u03b1(at|at, \u03c0\u0302, \u03b8) = \u220f i\u2208N \u03c0\u0302\ni(\u00b7|Hit , \u03b8) \u00b7 \u03b8i+ \u03b4(ait\u2212ait) \u00b7 (1\u2212 \u03b8i) that maps joint actions at to joint actions with perturbations at, where \u03b4(\u00b7) is the Dirac delta function. Note that the actions of cooperative agents and adversaries are taken simultaneously. Finally, the state transition probability P(st+1|st,at) takes the perturbed actions and output the state of next timestep. The shared reward rt = R(st,at) for (cooperative) agents is defined over perturbed actions. Given type \u03b8, the value function can be defined as V\u03b8(s) = E [ \u2211\u221e t=0 \u03b3\ntrt|s0 = s,at \u223c \u03c0(\u00b7|Ht), a\u0302t \u223c \u03c0\u0302(\u00b7|Ht, \u03b8)]. We leave the goal of adversary and robust agents to Section. 2.3 and 2.4 below.\nOur BARDec-POMDP formulation is flexible and draws close connection with current literature. Regarding type space, Dec-POMDP can be viewed as a BARDec-POMDP with \u0398 = 0N . Robust MARL approaches, such as M3DDPG (Li et al., 2019) and ROMAX (Sun et al., 2022) assumes agents are entirely adversarial, which refers to type space \u0398 = 1N . Subsequent robust MARL researchs (Phan et al., 2020; 2021; Nisioti et al., 2021), though not explicitly defining the type space, can be integrated in our BARDec-POMDP. Our formulation also draws inspiration from state-adversarial MDP (Zhang et al., 2020a) which considers adversary as a part of decision making process, and probabilistic action robust MDP (Tessler et al., 2019) by their formulation of action perturbation."
        },
        {
            "heading": "2.3 THREAT MODEL",
            "text": "The robustness towards action perturbations in both single and multi-agent RL has gained prominence since the pioneering works of (Tessler et al., 2019; Li et al., 2019). Action uncertainties, formulated as a type of adversarial attack known as adversarial policy (Gleave et al., 2019; Wu et al., 2021; Guo et al., 2021), or non-oblivious adversary (Dinh et al., 2023), represent a pragmatic and destructive\nform of attack that is challenging to counter. In line with these works, we propose a practical threat model with certain assumptions on attackers and defenders.\nAssumption 2.1 (Attacker\u2019s capability and limitations). At the onset of an episode, the attacker can select \u03b8 and arbitrarily manipulate the actions of agents with type \u03b8i = 1. Within an episode, the type cannot be altered and we assume there is only one attacker.\nOur main focus in this paper is to model action uncertainties of unknown agents in c-MARL as types shaped by nature and to advance corresponding solution concept. In line with (Li et al., 2019), we assume one agent is vulnerable to action perturbations in each episode. In real-world, the type space can be more complicated, potentially involving adversaries controlling multiple agents (Nisioti et al., 2021), perturbing actions intermittently (Lin et al., 2017), or featuring a non-binary type space (Xie et al., 2022). These variations can be viewed as straightforward extensions of our work.\nProposition 2.1 (Existence of worst-case adversary). For any robust c-MARL with fixed agent policy, a worst-case (i.e., most harmful) adversary exists.\nProof sketch. Since defender policies are fixed, they can be considered part of the environment transitions for attackers. Thus, the attackers solve an RL problem. See full proof in Appendix. A.1.\nAssumption 2.2 (Defender\u2019s capability and limitations). The defender can use all available information during training stage, including global state and information of other agents. However, during testing, the defender relies solely on partial observations and is agnostic of the type of other agents. The defender\u2019s policy is fixed during an attack and must resist the worst-case adversary \u03c0\u0302\u2217.\n2.4 SOLUTION CONCEPT"
        },
        {
            "heading": "Expectation",
            "text": "In this section, we first introduce the non-optimal solution concept seek by existing robust c-MARL methods, then pose our optimal solution concept for BARDec-POMDP. Specifically, existing robust c-MARL methods blindly maximize reward without considering the type of others. This is akin to an ex ante equilibrium in Bayesian game (Shoham & LeytonBrown, 2008), where agents make decisions based on the prior belief about the types of other agents.\nDefinition 2.1 (ex ante robustness). A joint cooperative policy \u03c0EA\u2217 = (\u03c0 EA,i \u2217 )i\u2208N and adversarial policy \u03c0\u0302EA\u2217 = (\u03c0\u0302 EA,i \u2217 )i\u2208N forms an ex ante robust Markov perfect Bayesian equilibrium (RMPBE), if for all p(\u03b8), s \u2208 S, H \u2208 (O \u00d7A)\u2217,\n(\u03c0EA\u2217 (\u00b7|H), \u03c0\u0302EA\u2217 (\u00b7|H, \u03b8)) \u2208 argmax \u03c0(\u00b7|H)\nEp(\u03b8) [\nmin \u03c0\u0302(\u00b7|H,\u03b8)\nV\u03b8(s) ] , (1)\nwith V\u03b8(s) = \u2211 a\u2208A P\u03b1(a|a, \u03c0\u0302, \u03b8) \u220f i\u2208N \u03c0 i(ai|Hi)(R(s,a) + \u03b3 \u2211 s\u2032\u2208S P(s\u2032|s,a)V\u03b8(s\u2032)).\nBy maximizing the expected value under prior p(\u03b8) over types, as illustrated in Fig. 2, the policy might struggle to balance the different equilibrium corresponding to cooperation and robustness against different agents as (Byzantine) adversaries. In contrast, we propose a more refined ex interim robustness concept, such that under current history, each agent make optimal decisions to maximize their expected value from their posterior belief of current type, with following assumptions:\nAssumption 2.3. Assume belief and policy are updated under following conditions: (1) Consistency. At each timestep t, each agent updates its belief bit = p(\u03b8|Hit) of the current type by Bayes\u2019 rule. (2) Sequential rationality. Each policy maximizes the expected value function under belief bi.\nHere we use \u03c0i(\u00b7|Hi, bi) to denote the explicit dependence on belief bi. The two conditions are common in dynamic games with incomplete information (Shoham & Leyton-Brown, 2008).\nDefinition 2.2 (ex interim robustness). Under Assumption 2.3 and let b = (bi)i\u2208N , a joint cooperative policy \u03c0EI\u2217 = (\u03c0 EI,i \u2217 )i\u2208N and adversarial policy \u03c0\u0302EI\u2217 = (\u03c0\u0302 EI,i \u2217 )i\u2208N forms an ex interim robust Markov perfect Bayesian equilibrium, if \u2200s \u2208 S, H \u2208 (O \u00d7A)\u2217,\n(\u03c0EI\u2217 (\u00b7|H, b), \u03c0\u0302EI\u2217 (\u00b7|H, \u03b8)) \u2208 argmax \u03c0(\u00b7|H,b)\nEp(\u03b8|H) [\nmin \u03c0\u0302(\u00b7|H,\u03b8)\nV\u03b8(s)) ] . (2)\nNote that both ex ante and ex interim RMPBE requires optimality of each individual agent i. To reduce notation complexity, we use joint policy and belief instead. Proposition 2.2 (Existence of RMPBE). Assume a BARDec-POMDP of finite agents, finite set of state, observation and action space, agents use stationary policies, the type space \u0398 is a compact set, then ex ante and ex interim mixed strategy robust Markov perfect Bayesian equilibrium exists.\nProof sketch. The proof is done by first showing the policy and its corresponding value function, with uncertainties of the current type and presence of the adversaries, satisfy the requirements of Kakutani\u2019s fixed point theorem. Next, by Kakutani\u2019s fixed point theorem, there always exists an optimal fixed point corresponding to a mixed strategy RMPBE. See full proof in Appendix. A.2.\nUnlike c-MARL with optimal deterministic policies (Oliehoek et al., 2008), in robust c-MARL, a pure-strategy equilibrium is not guaranteed to exist. This is intuitive since zero-sum games do not always have a pure-strategy equilibrium. The finding suggests the optimal policies for robust c-MARL are stochastic. Next, we show the relation between ex ante and ex interim equilibrium. Proposition 2.3. Under Assumption 2.3, given finite type space and the prior of each type is not zero, as t\u2192\u221e, \u03c0EI\u2217 (\u00b7|Ht, bt) weakly dominates \u03c0EA\u2217 (\u00b7|Ht) under the worst-case adversary.\nProof sketch. As t \u2192 \u221e, by the consistency of Bayes\u2019 rule, the belief converges to the true type. Thus, ex interim policies that maximize the value function under the true type are guaranteed to weakly dominate (i.e., have value higher or equal to) ex ante policies. See full proof in Appendix A.3."
        },
        {
            "heading": "3 ALGORITHM",
            "text": "In this section, we explain how to find the optimal solution in Definition 2.2. We start by defining the robust Harsanyi-Bellman equation, an update rule of value function which converges to a fixed point. Then, we develop a two-timescale actor-critic algorithm that considers belief of others\u2019 type, which ensures almost sure convergence under assumptions in stochastic approximation theory."
        },
        {
            "heading": "3.1 ROBUST HARSANYI-BELLMAN EQUATION",
            "text": "We first define the Bellman-type update of value functions for our ex interim equilibrium. Considering the Q function before and after action perturbation, we can formulate the Q function via cumulative reward, with posterior belief bi = p(\u03b8|Hi) over type:\nQi(s,a, bi) =Ep(\u03b8|Hi) [ E [ \u221e\u2211 t=0 \u03b3trt \u2223\u2223\u2223\u2223s0 = s,a0 = a,at \u223c \u03c0(\u00b7|Ht, bt), a\u0302t \u223c \u03c0\u0302(\u00b7|Ht, \u03b8) ]] , (3)\nQi(s,a, bi) =Ep(\u03b8|Hi) [ E [ \u221e\u2211 t=0 \u03b3trt \u2223\u2223\u2223\u2223s0 = s,a0 = a,at \u223c \u03c0(\u00b7|Ht, bt), a\u0302t \u223c \u03c0\u0302(\u00b7|Ht, \u03b8) ]] , (4)\nThe two Q functions are defined with different purpose. Qi(s,a, bi) is the expected Q function before action perturbation, suitable for decision making of defenders, such as fictitious self-play (Heinrich & Silver, 2016) and soft actor-critic (Haarnoja et al., 2018). In this way, the action perturbation can be viewed as part of the environment transition, resulting in P(s\u2032|s,a, \u03c0\u0302, \u03b8) = P(s\u2032|s,a) \u00b7P\u03b1(a|a, \u03c0\u0302, \u03b8). On the other hand, Qi(s,a, bi) is the Q function with actions taken by the adversary, suitable for policy gradients (Sutton & Barto, 2018) and decision making of the adversary. For Qi(s,a, Hi), the action perturbation is integrated into the policy of robust agents, resulting in a mixed policy \u03c0(a\u0302|H, b, \u03b8) = P\u03b1(a|a, \u03c0\u0302, \u03b8) \u00b7 \u03c0(a|H, b) = (1 \u2212 \u03b8) \u00b7 \u03c0(a|H, b) + \u03b8 \u00b7 \u03c0\u0302(a\u0302|H, \u03b8). The relationship between the two Q functions is as follows:\nQi(s,a, bi) = \u2211 \u03b8\u2208\u0398 p(\u03b8|Hi) \u2211 a\u0302\u2208A P\u03b1(at|at, \u03c0\u0302, \u03b8)Qi(s,a, bi). (5)\nNext, we formulate the Bellman-type equation for the two Q functions, which we call the robust Harsanyi-Bellman equation. This update differs from the conventional approach by considering the posterior belief over other agents and the worst-case adversary. Definition 3.1. We define the robust Harsanyi-Bellman equation for Q function as:\nQi\u2217(s,a, b i) = max\n\u03c0(\u00b7|H,b) min \u03c0\u0302(\u00b7|H,\u03b8) \u2211 \u03b8\u2208\u0398 p(\u03b8|Hi) \u2211 s\u2032\u2208S \u2211 a\u0302\u2208A P(s\u2032|s,a, \u03c0\u0302, \u03b8) [\nR(s,a) + \u03b3 \u2211 a\u2032\u2208A \u03c0(a\u2032|H \u2032, b\u2032)Qi\u2217(s\u2032,a\u2032, b\u2032i) ] , (6)\nQi\u2217(s,a, b i) = max\n\u03c0(\u00b7|H,b) min \u03c0\u0302(\u00b7|H,\u03b8) R(s,a) + \u03b3 \u2211 s\u2032\u2208S P(s\u2032|s,a) \u2211 \u03b8\u2208\u0398\np(\u03b8|H \u2032i)\u2211 a\u2032\u2208A \u03c0(a\u2032|H \u2032, b\u2032, \u03b8)Qi\u2217(s\u2032,a\u2032, b\u2032i). (7)\nThis Q function can be estimated via Temporal Difference (TD) loss. Proposition 3.1 (Convergence). Assume the belief is updated via Bayes\u2019 rule, the space of state, actions and belief are finite, updating value functions by robust Harsanyi-Bellman equation converge to the optimal value Qi\u2217(s,a, b i) and Qi\u2217(s,a, b i).\nProof sketch. The proof is done by combining the standard convergence proof of Q function with adversaries and Bayesian belief update, and showing our Q function forms a contraction mapping. Next, applying Banach\u2019s fixed point theorem completes the proof. See full proof in Appendix. A.4.\n3.2 ACTOR-CRITIC ALGORITHM FOR ex interim ROBUST EQUILIBRIUM\nArmed with the robust Harsanyi-Bellman equation, we propose an actor-critic algorithm to achieve our proposed ex interim equilibrium with almost sure convergence under certain assumptions. We first derive the policy gradient theorem for robust c-MARL. Assume policies of robust agents and adversaries are parameterized by \u03c0\u03d5 := (\u03c0i\u03d5i)i\u2208N and \u03c0\u0302\u03d5\u0302 := (\u03c0\u0302 i \u03d5\u0302i )i\u2208N respectively, forming a mixed policy \u03c0\u03d5,\u03d5\u0302 = (1\u2212 \u03b8) \u00b7 \u03c0\u03d5 + \u03b8 \u00b7 \u03c0\u0302\u03d5\u0302. Define the performance for a robust agent i in the episodic case as J i(\u03d5) = Es\u223c\u03c1\u03c0(s)[V i(s, bi)] and (zero-sum) adversary as J i(\u03d5\u0302) = Es\u223c\u03c1\u03c0(s)[\u2212V i(s, bi)], where \u03c1\u03c0(s) is the state visitation frequency. The policy gradients for \u03c0\u03d5 and \u03c0\u0302\u03d5\u0302 are then defined as:\nTheorem 3.1. The policy gradient theorem for robust agent and adversary i is: \u2207\u03d5iJ i(\u03d5i) = Es\u223c\u03c1\u03c0(s),a\u223c\u03c0\u03d5,\u03d5\u0302(a|H,b,\u03b8) [ (1\u2212 \u03b8i)\u2207 log \u03c0\u03d5i(ai|Hi, bi)Qi(s,a, bi) ] , (8)\n\u2207\u03d5\u0302iJ i(\u03d5\u0302i) = Es\u223c\u03c1\u03c0(s),a\u223c\u03c0\u03d5,\u03d5\u0302(a|H,b,\u03b8) [ \u2212\u03b8i\u2207 log \u03c0\u0302\u03d5\u0302i(a\u0302 i|Hi, \u03b8)Qi(s,a, bi) ] . (9)\nThe policy gradient naturally depends on Qi(s,a, bi), which is related to policy gradient and decision of adversaries. Specifically, \u03b8 cuts off the gradient of robust agents \u03c0\u03d5i with \u03b8i = 1 and cut off the gradient of adversary \u03c0\u0302\u03d5\u0302i with \u03b8 i = 0. The detailed derivation is deferred to Appendix. A.5.\nConvergence. In zero-sum Markov games, achieving convergence through policy gradients remains challenging, with current theoretical results being dependent on specific conditions (Daskalakis et al., 2020; Zhang et al., 2020b; Kalogiannis et al., 2022). In this paper, we prove that, under certain assumptions in stochastic approximation theory (Borkar, 1997; Borkar & Meyn, 2000; Borkar, 2009), applying a two-timescale update for both adversaries and defenders, as stated in Theorem 3.1, guarantees almost sure convergence (i.e., converge with probability 1) to an ex interim RMPBE. A detailed proof is provided in Appendix A.6 as an application of stochastic approximation. With this two-timescale update, the adversary\u2019s policy is updated on a faster timescale and is essentially equilibrated, while the defender\u2019s policy is updated on a slower timescale and remains quasi-static. Despite these advances, establishing finite sample, global convergence guarantees without restrictive assumptions remains an open problem, warranting future research.\nFinally, we suggest update rules for critic and belief networks. Assuming the critic Qi\u03c8(s,a, b i) is parameterized by \u03c8. As for belief bi, the calculation of bi via Bayes\u2019 rule requires assess to the policy\nof other agents, which is not possible during deployment. To remedy this, we approximate the belief bi = max\u03be p\u03be(\u03b8|Hi) using a neural network parameterized by \u03be. The objectives to update critic and belief network are:\nmin \u03c8\n( R(s,a)\u2212 \u03b3Qi\u03c8(s\u2032,a\u2032, b\u2032i) +Qi\u03c8(s,a, bi) )2 , (10)\nmin \u03be \u2212\u03b8 log\n( p\u03be(\u03b8|Hi) ) \u2212 (1\u2212 \u03b8) log ( 1\u2212 p\u03be(\u03b8|Hi) ) , (11)\nwith critic trained via TD loss and belief network trained by binary cross entropy loss. See the pseudo-code for our algorithm in Appendix. B."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "Environments. To validate the efficacy of our proposed approach, we conducted experiments on three benchmark cooperative MARL environments, as shown in Fig. 7. Environments include a toy iterative matrix game proposed by (Han et al., 2022), rewarding XNOR or XOR actions at different state, 12x12-4p-3f-c of Level-Based Foraging (LBF) (Papoudakis et al., 2020) and 4m vs 3m of the StarCraft Multi-agent Challenge (SMAC) (Samvelyan et al., 2019), which reduce to 3m in the presence of an adversary.\nBaselines. Our comparative study includes MADDPG (Lowe et al., 2017), M3DDPG (Li et al., 2019), MAPPO (Yu et al., 2021), RMAAC He et al. (2023), ex ante robust MAPPO (EAR-MAPPO), a MAPPO variant of (Phan et al., 2020; Zhang et al., 2020c) that considers ex ante equilibrium, which is also an ablation of our approach without belief. We dubbed our method ex interim robust MAPPO (EIR-MAPPO) and add an ideal case which grants access to true type, labelled \u201cTrue Type\u201d. It\u2019s worth noting that we couldn\u2019t directly adapt M3DDPG onto the MAPPO framework due to its reliance on Q(s,a), a component not compatible with MAPPO\u2019s use of V (s) as a critic. More experiment details are given in Appendix. C. For fair comparison, all methods use the same codebase, network structure and hyperparameters. Code and demo videos available at https: //github.com/DIG-Beihang/EIR-MAPPO.\nEvaluation protocol. In each environment with N cooperative agents, the robust policy was trained using five random seeds. Attack results were compiled by launching attacks on each of the N agents using the same five seeds, yielding a total of 5\u00d7N attacks per environment. We plot all results with 95% confidence interval.\nEvaluated attacks. We consider four types of threats. (1) Non-oblivious adversaries (Gleave et al., 2019): we fix the trained policy and deployed a zero-sum, worst-case adversarial policy to attack each agents separately. (2) Random agents: an agent perform random actions from a uniform distribution, possibly via hardware or software failure (labelled as \u201crandom\u201d). (3) Noisy observations: we add \u2113\u221e bounded adversarial noise (Lin et al., 2020) with perturbation budgets \u03f5 \u2208 {0.2, 0.5, 1.0} to the observation of an agent (denoted as \u201c\u03f5 =\u201d). (4) Transferred adversaries: attackers initially train a policy on a surrogate algorithm, then directly transfer the attack to target other algorithms."
        },
        {
            "heading": "4.1 ROBUSTNESS OVER NON-OBLIVIOUS ATTACKS",
            "text": "We first evaluate our performance under the most arduous non-oblivious attack, where an adversary can manipulate any agent in cooperative tasks and execute arbitrary learned worst-case policy. The cooperation and attack performance on three environments are given in Fig. 4. Across all environments, our EIR-MAPPO consistently delivers robust performance close to the maximum\nreward achievable in each environment under attack (50 for Toy, 1.0 for LBF, 20 for SMAC), outperforming baselines by large margins and displays robustness equalling the ideal True Type defense. Concurrently, EIR-MAPPO maintains cooperative performance on par with MAPPO.\nAs illustrated in Fig. 5, a detailed examination of each method\u2019s behaviour under attack enriches our comprehension of robustness, with adversaries marked by a red square. First, MADDPG and M3DDPG can be easily swayed by adversaries. In Fig. 5a, a downward-moving adversary easily diverts two victims from the battle, resulting in a single victim facing three opponents. As for MAPPO and RMAAC, agents fail to master useful micromanagement strategies, such as kiting or focused fire during combat1. Consequently, the agent do not exihibit any cooperation skills under attack and are not skillful enough to win the game. As for EAR-MAPPO, agents occasionally demonstrate kiting but fall short in executing focused fire. They spread fire over two enemies instead of concentrating fire on one. Furthermore, even successful kiting can be compromised. In Fig. 5c, the adversary advances, causing two half-health agents to mistakenly believe that an ally is coming to its aid, and thus retreats to kite the enemy. This, however, leaves another low-health ally vulnerable to enemy fire and immediately being eliminated. Finally, we find that both EIR-MAPPO and True Type demonstrate focused fire and kiting, proving resistant to adversarial agents. Illustrated in Fig. 5d, two low-health agents retreat to avoid being eliminated, while an agent with high health advances to shield its allies, showcasing classic kiting behaviour. Moreover, allies coordinate to eliminate enemies, leaving one enemy nearly unscathed and another at half health."
        },
        {
            "heading": "4.2 ROBUSTNESS OVER VARIOUS TYPE OF ATTACKS",
            "text": "Apart from the worst-case oblivious adversary, c-MARL can encounter various uncertainties in real world, ranging from allies taking random actions, having uncertainties in observations, or a transferred\n1Micromanagements are granular control strategies for agents to win in StarCraft II (Samvelyan et al., 2019). Kiting enables agents to evade enemy fire by stepping outside the enemy\u2019s attack range, thereby compelling the enemy to give chase rather than attack; focused fire requires taking enemies down one after another.\nadversary trained on alternate algorithms. We examine the robust performance of all methods under such diverse uncertainties in Fig. 6. The rows signify uncertainties while the columns represent the evaluated methods. Diagonal entries (i.e., blocks with the same uncertainty and evaluated method) denote non-oblivious attacks. For each uncertainty (column), the method of highest reward was marked by a red square. Furthermore, since the True Type represents an ideal scenario, if it secures the highest reward, we mark the method that gained the second-highest reward as well.\nWe first emphasize the effectiveness of our EIR-MAPPO approach. Considering the average reward gained under a broad range of uncertainties, our EIR-MAPPO surpasses baselines by 5.81% on Toy, 5.88% on LBF, and 25.45% on SMAC. Notably, EIR-MAPPO and True Type yields highest reward in almost all LBF and SMAC environments. In toy environment, owing to the existence of two pure-strategy equilibria, algorithms and attacks deploying deterministic strategies can occasionally yield higher rewards by chance. However, given its superior worst-case robustness, our EIR-MAPPO consistently delivers commendable results under all uncertainties.\nOur second observation focuses on the relationship between action and observation perturbations. As an algorithm designed to counteract observation uncertainties, RMAAC is robust against observation perturbations, but fails to counter unseen action perturbations. In contrast, our EIR-MAPPO maintains its robustness against observation perturbations, even though it has not encountered the attack before. This resilience is due to the fact that observational attacks ultimately affect agents\u2019 choices of actions, which reduces observation uncertainty to a form of action uncertainty."
        },
        {
            "heading": "5 CONCLUSIONS",
            "text": "In this paper, we study robust c-MARL against Byzantine threat, where agents in this system to fail, or being compromised by an adversary. We frame the problem as a Dec-POMDP and define its solution as an ex interim RMPBE, such that the equilibrated policy weakly dominates ex ante solutions in prior research, when time goes to infinity. To actualize this equilibrium, we introduce Harsanyi-Bellman equation for value function updates, and an actor-critic algorithm with almost sure convergence under specific conditions. Experimental results show that under worst-case adversarial perturbation, our method can learn intricate and adaptive cooperation skills, and can withstand non-oblivious, random, observation-based, and transferred adversaries. As future work, we plan to apply our method to c-MARL applications, including robot swarm control (H\u00fcttenrauch et al., 2019), traffic light management (Chu et al., 2019), and power grid maintenance (Xi et al., 2018)."
        },
        {
            "heading": "6 ACKNOWLEDGEMENTS",
            "text": "This work was supported by the National Key Research and Development Plan of China (2022ZD0116405) and National Natural Science Foundation of China (62306025, 62022009, 62132010 and 62206009)."
        },
        {
            "heading": "APPENDIX FOR \"BYZANTINE ROBUST COOPERATIVE MULTI-AGENT REINFORCEMENT LEARNING AS A",
            "text": ""
        },
        {
            "heading": "BAYESIAN GAME\"",
            "text": ""
        },
        {
            "heading": "A PROOFS AND DERIVATIONS",
            "text": ""
        },
        {
            "heading": "A.1 PROOF OF PROPOSITION 2.1",
            "text": "In this section, we proof a worst-case adversary always exist for any BARDec-POMDP with fixed defenders. Considering different numbers of adversaries (i.e., \u2211 i\u2208N ), with fixed defenders, the attackers can be seen as solving a POMDP or Dec-POMDP, where optimal solution exists.\nFirst, if \u2211 i\u2208N \u03b8i = 0, no adversary exists and the environment is reduced to a fully cooperative setting. Since adversary do not even exist, the problem becomes vacuous, and any \u03c0\u0302 achieves the same (optimal) result.\nSecond, if \u2211 i\u2208N \u03b8i = 1, the problem is reduced to a POMDP G\u03b1 := \u27e8S,O, O,A, P\u0302,P\u03b1, R\u03b1, \u03b3\u27e9 for adversary, where S the global state space,O the observation space for adversary, O is the observation emission function, A is the action space of adversary, R\u03b1 : S \u00d7 A \u2192 R is the reward function for adversary. Actions taken by adversary i are sampled via adversarial policy a\u0302it \u223c \u03c0\u0302i(\u00b7|Hit , \u03b8). The environment transition for adversary is defined as P\u0302(st+1|st, a\u0302it) = P(st+1|st,at) \u00b7 P\u03b1(at|\u03c0, \u03b8t, a\u0302it), where at is the mixed joint actions after perturbation in BARDec-POMDP. Here, P(st+1|st,at) is the environment transition in BARDec-POMDP, which combines with the transition P\u03b1(at|\u03c0, \u03b8, a\u0302it) =\u220f i\u2208N \u03b4(a i t \u2212 a\u0302it) \u00b7 \u03b8i + \u03c0i(\u00b7|Hit) \u00b7 (1\u2212 \u03b8i) represents the decision of \u03c0, which is fixed and treated as a part of environment transition. Following the proof of Astrom et al. (1965), there always exists an optimal policy for POMDP. Thus, there exists an optimal adversary.\nThird, with \u2211 i\u2208N \u03b8i > 1, the problem is reduced to a Dec-POMDP \u27e8N ,S,O\u03b1, O,A, P\u0302, R\u03b1, \u03b3\u27e9, with S the global state space, O\u03b1 = \u00d7i\u2208{\u03b8i=1}Oi is the observation space of adversaries, O is the observation emission function, A = \u00d7i\u2208{\u03b8i=1}Ai action space of adversaries, R\u03b1 : S \u00d7 A\u03b1 \u2192 R is the reward function for adversary. Actions taken by adversary are sampled via adversarial policy a\u0302t \u223c \u03c0\u0302(\u00b7|Ht, \u03b8). The environment transition for adversary is defined as P\u0302(st+1|st, a\u0302t) = P(st+1|st,at) \u00b7 P\u03b1(at|\u03c0, \u03b8, a\u0302t). Here, P(st+1|st, a\u0302t) is the environment transition for BARDecPOMDP, with the effect of the policy of \u03c0 merged in transition P\u03b1(at|\u03c0, \u03b8, a\u0302t) = \u220f i\u2208N \u03b4(a i t \u2212 a\u0302it) \u00b7 \u03b8i + \u03c0i(\u00b7|Hit) \u00b7 (1 \u2212 \u03b8i), which is fixed and treated as a part of transition function. Following the proof of Bernstein et al. (2002), there always exists an optimal policy for Dec-POMDP. Thus, there exists an optimal adversary."
        },
        {
            "heading": "A.2 PROOF OF PROPOSITION 2.2",
            "text": "We proof this by showing our BARDec-POMDP satisfies the requirement of Kakutani\u2019s fixed point theorem. Our proof is an extension of Kardes\u0327 et al. (2011), which shows equilibrium exists in robust stochastic games. In the following proof, we will first state existing results, then shows the existence of our ex interim RMPBE as a generalization of the proof of (Kardes\u0327 et al., 2011) considering Bayesian update and transforming action uncertainty as a kind of environment uncertainty. The existence proof of ex ante RMPBE rise as a corollary."
        },
        {
            "heading": "A.2.1 PRELIMINARIES",
            "text": "We first introduce existing results.\nTheorem A.1 (Kakutani\u2019s fixed point theorem). If X is a closed, bounded, and convex set in a Euclidean space, and \u03d5 is an upper semicontinuous correspondence mapping X into the family of closed, convex subsets of X , then \u2203x \u2208 X , s.t. x \u2208 \u03d5(x).\nNext, we brief the definition of robust stochastic game and its equilibrium concept before we introduce the main result of Kardes\u0327 et al. (2011).\nIn a stochastic gameG =< N ,S,A,P, R, \u03b3 > with finite state and actions, whereN is the indicator of agents, S is the state space, A = \u00d7i\u2208NAi is the joint action space, P : S \u00d7 A \u2192 \u2206(S) is the state transition probability, R : S \u00d7 A \u00d7N \u2192 RN is the reward function that maps state and actions to reward of each agent, \u03b3 is the discount factor. Let s \u2208 S, ai \u2208 Ai, with a the joint actions, \u03c0 : S \u2192 \u2206(A) is the policy for agent i with \u03c0i(ai|s), ri = R(s, a, i) is the reward for agent i. A robust stochastic game is a stochastic game with perturbed reward r\u0302t \u2208 R\u03b1 and environment transition P\u0302(s\u2032|s,a) \u2208 P\u03b1, withR\u03b1 and P\u03b1 are the bounded perturbation range. Define the expected value function V i\n\u03c0,r\u0302,P\u0302(s) under perturbed reward and environment transition recursively through the following Bellman equation:\nV i \u03c0,r\u0302,P\u0302(s) = \u2211 a\u2208A \u03c0i(ai|s) \u220f j \u0338=i \u03c0j(aj |s)[r\u0302i + \u2211 s\u2032\u2208S P\u0302(s\u2032|s,a)V i\u03c0(s\u2032)],\nthe worst-case value function V i \u03c0,r\u0302\u2217,P\u0302\u2217 (s) is defined as V i \u03c0,r\u0302,P\u0302(s) = minr\u0302,P\u0302 V i \u03c0,r\u0302\u2217,P\u0302\u2217 (s). The equilibrium policy, if exists, is thus \u2200i \u2208 N , s \u2208 S, a \u2208 A, \u03c0i\u2217(\u00b7|s) \u2208 argmax\u03c0i(\u00b7|s) V i\u03c0(s), with corresponding value as V i{\u03c0i\u2217,\u03c0\u2212i},r\u0302\u2217,P\u0302\u2217 (s) = max\u03c0i V i \u03c0,r\u0302\u2217,P\u0302\u2217 (s).\nNext, let \u03c0(\u00b7|s) = \u220f i\u2208N \u03c0\ni(\u00b7|s) be the joint policy, and the set of value functions for each agent as V\u03c0,r\u0302,P\u0302(s) = (V 1 \u03c0,r\u0302,P\u0302(s), V 2 \u03c0,r\u0302,P\u0302(s), ..., V N \u03c0,r\u0302,P\u0302(s)). Define mappings \u03b2 and \u03d5 by \u03b2(\u03c0 \u2212i) = {V i|V i = max\u03c0i minr\u0302,P\u0302 V i{\u03c0i,\u03c0\u2212i},r\u0302,P\u0302(s)} and \u03d5(\u03c0 \u2212i) = {\u03c0i|\u03b2(\u03c0\u2212i) = V i{\u03c0i\u2217,\u03c0\u2212i},r\u0302\u2217,P\u0302\u2217(s)} to be the set of value functions and optimal policies. Theorem A.2 (Kardes\u0327 et al. (2011)). Assume uncertainties in transition probabilities and payoffs belongs to compact sets, all agents use stationary strategies, then the set of functions {V\u03c0,r\u0302,P\u0302(s), r\u0302 \u2208 R\u03b1, P\u0302 \u2208 P\u03b1} is equicontinuous and V\u03c0,r\u0302\u2217,P\u0302\u2217(s) continuous on all its variables. \u03d5(\u03c0\n\u2212i) is an upper semicontinuous correspondence mapping \u2206(A) in a convex and closed subsets of \u2206(A), which satisfies the assumptions of Kakutani\u2019s fixed point theorem.\nWe are now ready to construct our proof."
        },
        {
            "heading": "A.2.2 EXISTENCE OF EX INTERIM RMPBE",
            "text": "We first state the existence of ex interim RMPBE. The proof is conducted by transforming BARDecPOMDP to a Dec-POMDP with environmental uncertainties, and applying Bayesian update to value functions. Note that we include some results of Kardes\u0327 et al. (2011) for a self-contained proof.\nFirst, by definition of our BARDec-POMDP, it can be transformed to a robust Dec-POMDP with adversary in environmental dynamics. This is done by combining environment transition P(st+1|st,a) with action perturbation probability P\u03b1(a\u0302|a, \u03c0\u0302, \u03b8), resulting in P(st+1|st,a, \u03c0\u0302, \u03b8) = P(st+1|st, a\u0302) \u00b7 P\u03b1(a\u0302|a, \u03c0\u0302, \u03b8). Thus, the robust Dec-POMDP can be seen as a particular case of stochastic game with shared reward, partial observations and uncertainties in perturbations. Thus, some results of Kardes\u0327 et al. (2011) can be taken for our proof. Before that, let us redefine some notations for clarity of our proof.\nLet us redefine the expected value function V\u03b8(s) using the following Bellman equation: V i\u03c0,\u03c0\u0302,\u03b8(s) = \u2211 a\u2208A \u03c0i(ai|Hi, bi) \u220f j \u0338=i \u03c0j(aj |Hj , bj)[ri + \u2211 s\u2032\u2208S \u2211 a\u0302\u2208A P(s\u2032|s,a, \u03c0\u0302, \u03b8)V\u03c0,\u03c0\u0302,\u03b8(s\u2032)].\nNote that V i\u03c0,\u03c0\u0302,\u03b8(s) assumes the type \u03b8 is known, thus there is no uncertainty over \u03b8 and the function is not updated by robust Harsanyi-Bellman equation. The expected value function with belief, V i\u03c0,\u03c0\u0302,bi(s), is defined by V i \u03c0,\u03c0\u0302,bi(s) = Ep(\u03b8|H)[V i \u03c0,\u03c0\u0302,\u03b8(s)]. The worst-case value function V i\u03c0,\u03c0\u0302\u2217,\u03b8(s) and V i \u03c0,\u03c0\u0302\u2217,bi (s) are defined as V i\u03c0,\u03c0\u0302\u2217,\u03b8(s) = min\u03c0\u0302 V i \u03c0,\u03c0\u0302,\u03b8(s) and V i \u03c0,\u03c0\u0302\u2217,bi (s) = min\u03c0\u0302 V i \u03c0,\u03c0\u0302,bi(s). With optimal (equilibrium) policy defined as \u03c0\u2217, the corresponding value is V i{\u03c0i\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,bi (s) = max\u03c0i V i \u03c0,\u03c0\u0302\u2217,bi\n(s). The mappings \u03b2 and \u03d5 are similarly defined \u03b2(\u03c0\u2212i) = {V i|V i = max\u03c0i min\u03c0\u0302 V i{\u03c0i,\u03c0\u2212i},\u03c0\u0302,bi(s)} and \u03d5(\u03c0\n\u2212i) = {\u03c0i|\u03b2(\u03c0\u2212i) = V i{\u03c0i\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,bi(s)} to be the set of value functions and optimal policies. By Proposition. 3.1, the optimal value at each s \u2208 S is unique, which we denote it as vis.\nLemma A.1 (Equicontinuity of {V i\u03c0,\u03c0\u0302,bi(s), \u03c0\u0302 \u2208 \u2206(A)}). For every \u03f5 > 0, \u2203\u03b4 > 0, for any (\u03c01, V\u03c01,\u03c0\u0302,bi1(s \u2032 1), b i 2) and (\u03c02, V\u03c02,\u03c0\u0302,bi2(s \u2032 2), b i 2), |\u03c01\u2212\u03c02|+|V\u03c01,\u03c0\u0302,bi(s\u20321)\u2212V\u03c02,\u03c0\u0302,bi(s\u20322)|+|b1\u2212b2| < \u03b4, then, \u2200\u03c0\u0302 \u2208 \u2206(A), |V\u03c01,\u03c0\u0302,bi(s1)\u2212 V\u03c02,\u03c0\u0302,bi(s2)| < \u03f5.\nProof. By Lemma 1 of Kardes\u0327 et al. (2011), the equicontinuity of {V i\u03c0,\u03c0\u0302,\u03b8(s), \u03c0\u0302 \u2208 \u2206(A)} holds, if we consider \u03c0\u0302 as uncertainties in P(s\u2032|s,a, \u03c0\u0302, \u03b8). All we need now is to extend the proof considering belief bi.\n|V\u03c01,\u03c0\u0302,bi(s1)\u2212 V\u03c02,\u03c0\u0302,bi(s2)| =|Ep(\u03b8|Hi1)[V\u03c01,\u03c0\u0302,\u03b8(s1)]\u2212 Ep(\u03b8|Hi2)[V\u03c02,\u03c0\u0302,\u03b8(s2)]|\n= \u2223\u2223\u2223\u2223\u2223\u2211 \u03b8\u2208\u0398 [p(\u03b8|Hi1) \u00b7 V\u03c01,\u03c0\u0302,\u03b8(s1)\u2212 p(\u03b8|Hi2) \u00b7 V\u03c02,\u03c0\u0302,\u03b8(s2)] \u2223\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223\u2211 \u03b8\u2208\u0398 [p(\u03b8|Hi1) \u00b7 (V\u03c01,\u03c0\u0302,\u03b8(s1)\u2212 V\u03c02,\u03c0\u0302,\u03b8(s2)) + (p(\u03b8|Hi1)\u2212 p(\u03b8|Hi2)) \u00b7 V\u03c02,\u03c0\u0302,\u03b8(s2) \u2223\u2223\u2223\u2223\u2223 \u2264 \u2211 \u03b8\u2208\u0398 [ |p(\u03b8|Hi1) \u00b7 (V\u03c01,\u03c0\u0302,\u03b8(s1)\u2212 V\u03c02,\u03c0\u0302,\u03b8(s2))|+ |(p(\u03b8|Hi1)\u2212 p(\u03b8|Hi2)) \u00b7 V\u03c02,\u03c0\u0302,\u03b8(s2)|\n] \u2264 \u2211 \u03b8\u2208\u0398 [|p(\u03b8|Hi1)| \u00b7 |V\u03c01,\u03c0\u0302,\u03b8(s1)\u2212 V\u03c02,\u03c0\u0302,\u03b8(s2)|+ |(p(\u03b8|Hi1)\u2212 p(\u03b8|Hi2)| \u00b7 |V\u03c02,\u03c0\u0302,\u03b8(s2)|]\nSince p(\u03b8|Hi1) is a probability function, we have p(\u03b8|Hi1) \u2264 1. Since reward is finite, and |V\u03c0i,\u03c0\u0302,\u03b8(si)|, i \u2208 1, 2 is defined by discount factor \u03b3, |V\u03c0i,\u03c0\u0302,\u03b8(si)| is also bounded, i.e., |V\u03c0i,\u03c0\u0302,\u03b8(si)| \u2264 K. Now, let\n|V\u03c01,\u03c0\u0302,\u03b8(s1)\u2212 V\u03c02,\u03c0\u0302,\u03b8(s2)| < \u03b41 = min{\u03f5, 1} 2 \u00b7 |\u0398| ,\n|(p(\u03b8|Hi1)\u2212 p(\u03b8|Hi2)| < \u03b42 = min{\u03f5, 1} 2 \u00b7K ,\nand let \u03b4 = min{\u03b41, \u03b42}, we have:\n|V\u03c01,\u03c0\u0302,bi(s1)\u2212 V\u03c02,\u03c0\u0302,bi(s2)| \u2264 \u2211 \u03b8\u2208\u0398 [|p(\u03b8|Hi1)| \u00b7 |V\u03c01,\u03c0\u0302,\u03b8(s1)\u2212 V\u03c02,\u03c0\u0302,\u03b8(s2)|+ |(p(\u03b8|Hi1)\u2212 p(\u03b8|Hi2)| \u00b7 |V\u03c02,\u03c0\u0302,\u03b8(s2)|]\n= \u2211 \u03b8\u2208\u0398 [|p(\u03b8|Hi1)| \u00b7 \u03b41] + \u2211 \u03b8\u2208\u0398 [\u03b42 \u00b7 |V\u03c02,\u03c0\u0302,\u03b8(s2)|]\n\u2264\u03f5/2 + \u03f5/2 = \u03f5.\nThus, the set of functions {V i\u03c0,\u03c0\u0302,bi(s), \u03c0\u0302 \u2208 \u2206(A)} is equicontinuous.\nLemma A.2. \u03d5(\u03c0\u2212i) is a convex set.\nProof. The proof follows Theorem 4 of Kardes\u0327 et al. (2011). By Lemma 2 of Kardes\u0327 et al. (2011), the pointwise minimum of an equicontinuous set of function is continuous, V i\u03c0,\u03c0\u0302\u2217,bi(s) = min\u03c0\u0302 V i \u03c0,\u03c0\u0302,bi(s) is continuous on all its variables. Besides, V i\u03c0,\u03c0\u0302\u2217,bi(s) is defined by a discounted factor and is bounded. Thus, the maxima of V i\u03c0,\u03c0\u0302\u2217,bi(s) exists.\nSecond, in Proposition 3.1, we have proof that updating Q function by robust Harsanyi-Bellman equation yields an optimal robust Q value. It rise as a simple corollary that the optimal V value, V i{\u03c0i\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,bi(s) exists. By equality V i {\u03c0i\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,bi\n(s) = max\u03c0i(ai|Hi,bi) min\u03c0\u0302\u2211 a\u2208A \u03c0 i(ai|Hi, bi) \u220f j \u0338=i \u03c0 j(aj |Hj , bj) [ri+ \u2211 s\u2032\u2208S \u2211 a\u0302\u2208A P(s\u2032|s,a, \u03c0\u0302, \u03b8) V i{\u03c0i\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,bi(s\n\u2032), \u03d5(\u03c0\u2212i) \u0338= \u2205.\nThird, by Lemma 3 of Kardes\u0327 et al. (2011), the value function considering worstcase adversary V i\u03c0,\u03c0\u0302\u2217,bi(s) = min\u03c0\u0302 \u2211 a\u2208A \u03c0 i(ai|Hi, bj) \u220f j \u0338=i \u03c0\nj(aj |Hj , bj)[ri +\u2211 s\u2032\u2208S \u2211 a\u0302\u2208A \u2211 \u03b8\u2208\u0398 p(\u03b8|Hi)P(s\u2032|s,a, \u03c0\u0302, \u03b8)V\u03c0,\u03c0\u0302,bi(s\u2032)] is concave in \u03c0i with fixed \u03c0\u2212i and V i\u03c0,\u03c0\u0302\u2217,bi(s \u2032) 2.\nFinally, we show the convexity of \u03d5(\u03c0\u2212i). Let \u03c0i1,\u2217, \u03c0 i 2,\u2217 \u2208 \u03d5(\u03c0\u2212i), with fixed \u03c0\u2212i. By definition of V i{\u03c0i\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,bi(s), \u2200\u03c0, s \u2208 S, b \u2208 \u2206(\u0398), i \u2208 N , V i {\u03c0i1,\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,bi (s) = V i{\u03c0i2,\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,bi (s) \u2265 V i\u03c0,\u03c0\u0302\u2217,bi(s). Thus, \u2200\u03bb \u2208 [0, 1], we also have \u03bbV i {\u03c0i1,\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,bi\n(s) + (1 \u2212 \u03bb)V i{\u03c02,\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,bi(s) \u2265 V i\u03c0,\u03c0\u0302\u2217,bi(s). By concavity of V i \u03c0,\u03c0\u0302\u2217,bi (s), V i{\u03c0i\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,bi(s) = \u03bbV i {\u03c0i1,\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,bi\n(s) + (1 \u2212 \u03bb)V i{\u03c0i2,\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,bi (s) \u2264 V i{\u03bb\u03c0i1,\u2217+(1\u2212\u03bb)\u03c0i2,\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,bi(s). Since by Proposition 3.1, V i \u03c0,\u03c0\u0302\u2217,bi (s) is optimal and unique. Thus, \u03bb\u03c0i1,\u2217 + (1\u2212 \u03bb)\u03c0i2,\u2217 \u2208 \u03d5(\u03c0\u2212i), \u03d5(\u03c0\u2212i) is a convex set.\nNext, we first introduce several lemmas, then show \u03d5(x) is an upper semicontinuous correspondence.\nLemma A.3. Let T be the robust Harsanyi-Bellman operator defined in Appendix. A.6. T V i{\u03c0i\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,bi(s) is continuous in \u03c0 \u2212i. The set {T V{\u03c0i\u2217,\u03c0\u2212i},\u03c0\u0302,bi(s)| V i {\u03c0i\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,bi (s) is bounded} is equicontinuous.\nProof. By Lemma 4 of Kardes\u0327 et al. (2011), T V i{\u03c0i\u2217\u03c0\u2212i},\u03c0\u0302\u2217,\u03b8(s) is continuous and the set {T V{\u03c0i\u2217,\u03c0\u2212i},\u03c0\u0302,bi(s)|V i {\u03c0i\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,\u03b8\n(s) is bounded} is equicontinuous. Since T V i{\u03c0i\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,bi(s) = T Ep(\u03b8|H))[V i{\u03c0i\u2217,\u03c0\u2212i},\u03c0\u0302\u2217,\u03b8(s)], the expectation of a continuous function is still continuous, and the expectation over a equicontinuous set is still equicontinuous. This completes the proof.\nLemma A.4. Define the optimal value as vi = {vi1, vi2, ...viS}. If \u03c0in \u2192 \u03c0i, \u03c0\u2212in \u2192 \u03c0\u2212i, \u03b2(\u03c0\u2212in )\u2192 vi and \u03c0in \u2208 \u03d5(\u03c0\u2212in ), then \u03c0i \u2208 \u03d5(\u03c0\u2212i), i.e., \u03d5(\u03c0\u2212i) is an upper semicontinuous correspondence.\nProof. The proof is by Lemma 5 of Fink (1964). We re-write it here using our notation to make the proof self-contained. Specifically, \u2200s \u2208 S, H \u2208 (O \u00d7 A)\u2217, b \u2208 \u2206(\u0398). Define function f(\u00b7) as f(V\u03c0,\u03c0\u0302\u2217,bi(s)) = min\u03c0\u0302(a\u0302|H,\u03b8) \u2211 a\u2208A \u03c0 i(ai|Hi, bi) \u220f j \u0338=i \u03c0\nj(aj |Hj , bj)[ri +\u2211 s\u2032\u2208S \u2211 a\u0302\u2208A \u2211 \u03b8\u2208\u0398 p(\u03b8|Hi)P(s\u2032|s,a, \u03c0\u0302, bi)V\u03c0,\u03c0\u0302\u2217,bi(s\u2032)]. Recall vis is the fixed point. Let \u03c0i\u2217 \u2208 \u03d5(\u03c0\u2212i), |f(vis) \u2212 vis| \u2264 |f(vis) \u2212 f(\u03b2(\u03c0\u2212in |s))| + |f(\u03b2(\u03c0\u2212in |s)) \u2212 vis| = |f(vis) \u2212 f(\u03b2(\u03c0\u2212in |s))| + |\u03b2(\u03c0\u2212in |s)\u2212 vis| \u2192 0 as n\u2192\u221e. Now we need to show when \u03c0\u2212in \u2192 \u03c0\u2212i and \u03b2(\u03c0\u2212in )\u2192 vi, \u03b2(\u03c0\u2212i|s) = vis. We have |vis \u2212 T vis| \u2264 |vis\u2212\u03b2(\u03c0\u2212in |s)|+ |\u03b2(\u03c0\u2212in |s)\u2212T \u03b2(\u03c0\u2212in |s)|+ |T \u03b2(\u03c0\u2212in |s)\u2212T vis|. By our assumption, \u03b2(\u03c0\u2212in )\u2192 vi and \u03c0in \u2208 \u03d5(\u03c0\u2212in ) as n \u2192 \u221e, |vis \u2212 \u03b2(\u03c0\u2212in |s)| \u2192 0, |T \u03b2(\u03c0\u2212in |s) \u2212 T vis| \u2192 0. By Lemma A.3, |\u03b2(\u03c0\u2212in |s) \u2212 T \u03b2(\u03c0\u2212in |s)| \u2192 0. Thus, |vis \u2212 T vis| \u2192 0 as n \u2192 \u221e. As \u03b2(\u03c0\u2212i) = {V i|V i = max\u03c0i min\u03c0\u0302 V i {\u03c0i,\u03c0\u2212i},\u03c0\u0302,bi(s) = v i s}, \u03b2(\u03c0\u2212i|s) = vis.\nAs we have \u03b2(\u03c0\u2212i|s) = vis, we have vis = T vis and is a fixed point. Thus, vis = f(vis) = \u03b2(\u03c0\u2212i|s) = min\u03c0i V\ni {\u03c0i,\u03c0\u2212i},\u03c0\u0302\u2217,bi(s). As a consequence, \u03c0 i \u2208 \u03d5(\u03c0\u2212i), \u03d5(\u03c0\u2212i) is an upper semicontinuous correspondence.\nNow, we have proofed \u03d5 is an upper semicontinuous correspondence (Lemma. A.4), mapping \u2206(A) into the family of convex subsets of \u2206(A) (Lemma. A.2). Since \u03d5(\u03c0\u2212i) is an upper semicontinuous correspondence, it is also a closed set for any \u03c0. Thus, the result satisfies the requirement of Kakutani\u2019s fixed point theorem, with equilibrium policy \u03d5(\u03c0\u2212i)."
        },
        {
            "heading": "A.2.3 EXISTENCE OF EX INTERIM RMPBE",
            "text": "The existence of ex ante RMPBE follows the proof of ex interim RMPBE, but having a prior belief p(\u03b8) that is never updated. Since the expectation over p(\u03b8) is a linear combination and p(\u03b8) is bounded, the addition of p(\u03b8) do not violate the convergence and continuity of value functions. The proof thus follows the result of ex interim RMPBE.\n2Note that Lemma 3 of Kardes\u0327 et al. (2011) considers cost, which is the negative of reward. While in their proof, the function considering cost is convex. Taking the negative of a convex function is thus concave."
        },
        {
            "heading": "A.3 PROOF OF PROPOSITION 3.2",
            "text": "For convenience of notations, Let us redefine the value function under worst-case adversary and type \u03b8 at time t, but additionally adding \u03c0, \u03c0\u0302 into the notation of V\u03b8(s) for clarity, resulting in\nV \u03c0,\u03c0\u0302\u2217\u03b8 (s) = min \u03c0\u0302(\u00b7|H,\u03b8) \u2211 a\u2208A P\u03b1(a|a, \u03c0\u0302, \u03b8) \u220f i\u2208N \u03c0i(ai|Hi)(R(s,a) + \u03b3 \u2211 s\u2032\u2208S P(s\u2032|s,a)V \u03c0,\u03c0\u0302\u2217\u03b8 (s \u2032)).\nWe can then redefine the value function for ex ante RMPBE as V \u03c0,\u03c0\u0302\u2217p (\u03b8)(s) = Ep(\u03b8)[V \u03c0,\u03c0\u0302\u2217 \u03b8 (s)] and the value function for ex interim RMPBE as V \u03c0,\u03c0\u0302\u2217b (s) = Ep(\u03b8|H)[V \u03c0,\u03c0\u0302\u2217 \u03b8 (s)].\nNext, for each \u03b8 \u2208 \u0398, with time t\u2192\u221e, we have bt = p(\u03b8|Ht)\u2192 \u03b8 by consistency of Bayes\u2019 rule Diaconis & Freedman (1986), if \u2200\u03b8 \u2208 \u0398, p(\u03b8) \u0338= 0 and with finite type space \u0398. The resulting value function at t\u2192\u221e is thus:\nV \u03c0EI\u2217 ,\u03c0\u0302 EI \u2217\nbt\u2192\u221e (s) = V\n\u03c0EI\u2217 ,\u03c0\u0302 EI \u2217\n\u03b8 (s) \u2265 V \u03c0,\u03c0\u0302\u2217 \u03b8 (s),\nwhere \u03c0\u0302\u2217 is always the optimal adversarial policy for current \u03c0. The rest follows. As for the expected return of ex ante robustness for \u03b8, we get:\nV \u03c0EA\u2217 ,\u03c0\u0302 EA \u2217 p(\u03b8) (s) = Ep(\u03b8) [ V \u03c0EA\u2217 ,\u03c0\u0302 EA \u2217 \u03b8 (s)) ] .\nDuring evaluation, \u2200p(\u03b8), at t \u2192 \u221e, since current type \u03b8 is known and do not change, the return conditions on \u03b8, instead of expectations on \u03b8. As a consequence, the value function of ex ante and ex interim equilibrium is V \u03c0 EA \u2217 ,\u03c0\u0302 EA \u2217\n\u03b8 (s) and V \u03c0EI\u2217 ,\u03c0\u0302 EI \u2217\n\u03b8 (s), respectively. At t\u2192\u221e and \u2200\u03b8 \u2208 \u0398, we have bt = p(\u03b8|Ht)\u2192 \u03b8, and following inequality holds:\nV \u03c0EI\u2217 ,\u03c0\u0302 EI \u2217 \u03b8 (s) \u2265 V \u03c0EA\u2217 ,\u03c0\u0302 EA \u2217 \u03b8 (s),\nwhich use the fact V \u03c0 EI \u2217 ,\u03c0\u0302 EI \u2217\n\u03b8 (s) \u2265 V \u03c0,\u03c0\u0302\u2217 \u03b8 (s). Considering to the gap between V \u03c0,\u03c0\u0302\u2217 \u03b8 (s) and V \u03c0,\u03c0\u0302\u2217 p(\u03b8) (s),\na sufficient condition for this equality to hold is when the type space \u0398 contains one type only. Note that even at t\u2192\u221e, the relation between expected value function V \u03c0 EA \u2217 ,\u03c0\u0302 EA \u2217,\u03b8\np(\u03b8) (s) of ex ante equilibrium\nand V \u03c0 EI \u2217 ,\u03c0\u0302 EI \u2217\n\u03b8 (s) of ex interim equilibrium is still not known. This is because the ex ante equilibrium can get high expected values by simply \u201cbelieving\u201d it in some prior p(\u03b8) that yields high value. However, since the belief is not correct, the resulting policy is non-optimal in any type at t\u2192\u221e."
        },
        {
            "heading": "A.4 PROOF OF PROPOSITION 3.3",
            "text": "Overview. We first proof the contraction mapping of Q(s,a, bi) by combining standard proof of the contraction mapping of Q function and Bayesian belief update. Afterwards, applying Banach\u2019s fixed point theorem completes the proof. The convergence of Q(s,a, bi) follows the same vein.\nWe first show the proof for Q-function for Qi\u2217(s,a, b i). The proof incorporates our robust HarsanyiBellman equation in the contraction mapping proof of robust MDP (Iyengar, 2005). Based on Bellman equation in Definition 3.1, let \u2206(\u0398) be a probability over \u0398, the optimal Q-function of a contraction operator T , defined from a generic function Q : S \u00d7A\u00d7\u2206(\u0398)\u2192 R can be defined as:\n(T Qi)(s,a, bi) = max \u03c0(\u00b7|o,b) min \u03c0\u0302(\u00b7|o,\u03b8) R(s,a) + \u03b3 [\u2211 s\u2032\u2208S P(s\u2032|s,a) \u2211 \u03b8\u2208\u0398 p(\u03b8|Hi)\n\u2211 a\u0302\u2032\u2208A \u03c0(a\u2032|H \u2032, b\u2032, \u03b8)Qi\u2217(s\u2032,a\u2032, b\u2032i)\n] .\nNext, we show T forms a contraction operator, such that for any two Q functionQi1 andQi2, assuming T Qi1(s,a, bi) \u2265 T Qi2(s,a, bi), the following holds:\n||T Qi1 \u2212 T Qi2||\u221e \u2264 \u03b3||Qi1 \u2212Qi2||\u221e,\nSpecifically, for \u03f5 > 0 and with fixed a \u2208 A, s \u2208 S, b \u2208 \u2206(\u0398), H \u2208 (O \u00d7A)\u2217 for Q1 and Q2,\nmin \u03c0\u0302(\u00b7|H,\u03b8) R(s, a\u0302) + \u03b3 [\u2211 s\u2032\u2208S P(s\u2032|s,a) \u2211 \u03b8\u2208\u0398 p(\u03b8|Hi) \u2211 a\u2032\u2208A \u03c0(a\u0302\u2032|H \u2032, b\u2032, \u03b8)Qi1(s\u2032,a\u2032, b\u2032i) ] \u2265 T Qi1 \u2212 \u03f5.\nNote that updating belief by Bayes\u2019 rule is required for consistency of b\u2032i. We require fixed Hi as well since the calculation of belief and Q function depends on Hi. Now we can also choose a conditional policy measure of the adversary \u03c0\u0302s, such that:\nE\u03c0\u0302s\n[ R(s, a\u0302) + \u03b3 [\u2211 s\u2032\u2208S P(s\u2032|s, a\u0302) \u2211 \u03b8\u2208\u0398 p(\u03b8|Hi) \u2211 a\u0302\u2032\u2208A \u03c0(a\u0302\u2032|H \u2032, b\u2032, \u03b8)Qi2(s\u2032, a\u0302\u2032, b\u2032i) ]] \u2264\nmin \u03c0\u0302(\u00b7|o,\u03b8) R(s, a\u0302) + \u03b3 [\u2211 s\u2032\u2208S P(s\u2032|s, a\u0302) \u2211 \u03b8\u2208\u0398 p(\u03b8|Hi) \u2211 a\u0302\u2032\u2208A \u03c0(a\u0302\u2032|H \u2032, b\u2032, \u03b8)Qi2(s\u2032, a\u0302\u2032, b\u2032i) ] +\u03f5.\nThen,\n0 \u2264 T Qi1 \u2212 T Qi2\n\u2264 ( min\n\u03c0\u0302(\u00b7|H,\u03b8) R(s, a\u0302) + \u03b3 [\u2211 s\u2032\u2208S P(s\u2032|s,a) \u2211 \u03b8\u2208\u0398 p(\u03b8|Hi) \u2211 a\u0302\u2032\u2208A \u03c0(a\u0302\u2032|H \u2032, b\u2032, \u03b8)Qi1(s\u2032, a\u0302\u2032, b\u2032i) ] +\u03f5 ) \u2212(\nmin \u03c0\u0302(\u00b7|H,\u03b8) R(s,a) + \u03b3 [\u2211 s\u2032\u2208S P(s\u2032|s,a) \u2211 \u03b8\u2208\u0398 p(\u03b8|Hi) \u2211 a\u2032\u2208A \u03c0(a\u2032|H \u2032, b\u2032, \u03b8)Qi2(s\u2032,a\u2032, b\u2032i) ])\n\u2264 ( E\u03c0\u0302s [ R(s,a) + \u03b3 [\u2211 s\u2032\u2208S P(s\u2032|s,a) \u2211 \u03b8\u2208\u0398 p(\u03b8|Hi) \u2211 a\u2032\u2208A \u03c0(a\u2032|H \u2032, b\u2032, \u03b8)Qi1(s\u2032,a\u2032, b\u2032i) ]] + \u03f5 ) \u2212(\nE\u03c0\u0302s\n[ R(s,a) + \u03b3 [\u2211 s\u2032\u2208S P(s\u2032|s,a) \u2211 \u03b8\u2208\u0398 p(\u03b8|Hi) \u2211 a\u2032\u2208A \u03c0(a\u2032|H \u2032, b\u2032, \u03b8)Qi2(s\u2032,a\u2032, b\u2032i) ]] \u2212 \u03f5 ) = \u03b3E\u03c0\u0302s [ Qi1 \u2212Qi2 ] + 2\u03f5 \u2264 \u03b3E\u03c0\u0302s\n\u2223\u2223Qi1 \u2212Qi2\u2223\u2223+ 2\u03f5 \u2264 \u03b3E\u03c0\u0302s ||Qi1 \u2212Qi2||\u221e + 2\u03f5. Thus, we have:\n||T Qi1 \u2212 T Qi2||\u221e \u2264 \u03b3||Qi1 \u2212Qi2||\u221e + 2\u03f5,\nand since by definition, \u03f5 is arbitrary, then we have ||T Qi1 \u2212 T Qi2||\u221e \u2264 \u03b3||Qi1 \u2212Qi2||\u221e. Finally, since T is a contraction operator on a Banach space, by Banach\u2019s fixed point theorem, updating Qi(s,a, bi) by Bellman operator T will converge to the optimal value function Qi\u2217(s,a, bi). In the same way, the convergence of Qi(s,a, bi) is again done by robust Harsanyi-Bellman equation, Qi\u2217(s,a, b\ni) = max \u03c0(\u00b7|H,b) min \u03c0\u0302(\u00b7|H,\u03b8)\n\u2211 \u03b8\u2208\u0398 p(\u03b8|Hi) \u2211 s\u2032\u2208S \u2211 a\u0302\u2208A P(s\u2032|s,a, \u03c0\u0302, \u03b8)[R(s,a) +\n\u03b3 \u2211\na\u2032\u2208A \u03c0(a \u2032|H \u2032, b\u2032)Qi\u2217(s\u2032,a\u2032, b\u2032i)]. Expanding the function in the same way above completes\nthe proof."
        },
        {
            "heading": "A.5 PROOF OF THEOREM 4.1",
            "text": "We first discuss the policy gradient with \u03c0\u03d5i(ai|Hi, bi):\n\u2207\u03d5iV i(s, bi) =\u2207\u03d5i [\u2211 a\u2208A \u03c0\u03d5,\u03d5\u0302(a|H, b, \u03b8)Q i(s,a, bi) ]\n=\u2207\u03d5i [\u2211 a\u2208A ( (1\u2212 \u03b8) \u00b7 \u03c0\u03d5(a|H, b) + \u03b8 \u00b7 \u03c0\u0302\u03d5\u0302(a\u0302|H, \u03b8) ) Qi(s,a, bi) ] = \u2211 a\u2208A [ (1\u2212 \u03b8i)\u2207\u03d5i\u03c0\u03d5i(ai|Hi, bi) \u00b7Qi(s,a, bi) + \u03c0\u03d5,\u03d5\u0302(a|H, b, \u03b8)\u2207\u03d5iQ i(s,a, bi) ]\n= \u2211 a\u2208A [ (1\u2212 \u03b8i)\u2207\u03d5i\u03c0\u03d5i(ai|Hi, bi) \u00b7Qi(s,a, bi) + \u03c0\u03d5,\u03d5\u0302(a\u0302|H, b, \u03b8)\u2207\u03d5i [ R(s,a)\n+ \u03b3 \u2211 s\u2032\u2208S \u2211 a\u2032\u2208A P(s\u2032|s,a) \u2211 \u03b8\u2208\u0398 p(\u03b8|H)\u03c0\u03d5,\u03d5\u0302(a \u2032|H \u2032, b\u2032, \u03b8)Qi(s\u2032,a\u2032, b\u2032i) ] ,\n= \u2211 a\u2208A [ (1\u2212 \u03b8i)\u2207\u03d5i\u03c0\u03d5i(ai|oi, bi) \u00b7Qi(s,a, bi) + \u03c0\u03d5,\u03d5\u0302(a|H, b, \u03b8) [ \u03b3(1\u2212 \u03b8i)\u2207\u03d5i\n\u03c0\u03d5i(a \u2032|H \u2032, b\u2032) + \u03b3 \u2211 s\u2032\u2208S \u2211 a\u2032\u2208A P(s\u2032|s,a)\u03c0\u03d5,\u03d5\u0302(a \u2032|H \u2032, b\u2032, \u03b8) \u2211 \u03b8\u2208\u0398 p(\u03b8|H)\u2207\u03d5iQi(s\u2032,a\u2032, b\u2032i) ] ,\n= \u2211 s\u2032\u2208S \u221e\u2211 t=0 Pr(s\u2192 s\u2032, t, \u03c0) \u2211 a\u0302\u2208A (1\u2212 \u03b8i)\u2207\u03d5i\u03c0\u03d5i(ai|Hi, bi) \u00b7Qi(s,a, bi).\nConsidering\u2207\u03d5iJ i(\u03d5i), we have\n\u2207\u03d5iJ i(\u03d5i) =\u2207\u03d5iV i(s0, bi)\n= \u2211 s\u2208S \u221e\u2211 t=0 Pr(s0 \u2192 s, t, \u03c0) \u2211 a\u2208A (1\u2212 \u03b8i)\u2207\u03d5i\u03c0\u03d5i(ai|Hi, bi) \u00b7Qi(s,a, bi)\n= \u2211 s\u2208S \u03b7(s) \u2211 a\u2208A (1\u2212 \u03b8i)\u2207\u03d5i\u03c0\u03d5i(ai|Hi, bi) \u00b7Qi(s,a, bi)\n= \u2211 s\u2032\u2208S \u03b7(s\u2032) \u2211 s\u2208S \u03b7(s)\u2211 s\u2032\u2208S \u03b7(s \u2032) \u2211 a\u2208A (1\u2212 \u03b8i)\u2207\u03d5i\u03c0\u03d5i(ai|Hi, bi) \u00b7Qi(s,a, bi)\n= \u2211 s\u2032\u2208S \u03b7(s\u2032) \u2211 s\u2208S \u03c1\u03c0(s) \u2211 a\u2208A (1\u2212 \u03b8i)\u03c0\u03d5i(ai|Hi, bi) \u00b7Qi(s,a, bi)\n\u221d \u2211 s\u2208S \u03c1\u03c0(s) \u2211 a\u2208A (1\u2212 \u03b8i)\u2207\u03d5i\u03c0\u03d5i(ai|Hi, bi) \u00b7Qi(s,a, bi).\nUsing the log-derivative trick, we have:\n\u2207\u03d5iJ i(\u03d5i) \u221d \u2211 s\u2208S \u03c1\u03c0(s) \u2211 a\u2208A (1\u2212 \u03b8i)\u2207\u03d5i\u03c0\u03d5i(ai|Hi, bi) \u00b7Qi(s,a, bi)\n= \u2211 s\u2208S \u03c1\u03c0(s) \u2211 a\u2208A (1\u2212 \u03b8i)\u03c0\u03d5i(ai|Hi, bi) \u00b7Qi(s,a, bi) \u2207\u03d5i\u03c0\u03d5i(ai|Hi, bi) \u03c0\u03d5i(ai|Hi, bi)\n=Es\u223c\u03c1\u03c0(s),a\u223c\u03c0(\u00b7|H,b,\u03b8)[(1\u2212 \u03b8i)\u2207\u03d5i ln\u03c0\u03d5i(ai|Hi, bi) \u00b7Qi(s,a, bi)].\nThe proof of \u03c0\u03d5\u0302i(a\u0302 i|oi, \u03b8) basically follows the same method.\n\u2207\u03d5\u0302iV i(s, bi) =\u2207\u03d5\u0302i [\u2211 a\u2208A \u03c0\u03d5,\u03d5\u0302(a|H, b, \u03b8)Q i(s,a, bi) ]\n=\u2207\u03d5\u0302i [\u2211 a\u2208A ( (1\u2212 \u03b8) \u00b7 \u03c0\u03d5(a|H, b) + \u03b8 \u00b7 \u03c0\u0302\u03d5\u0302(a\u0302|H, \u03b8) ) Qi(s,a, bi) ] = \u2211 a\u2208A [ \u03b8i \u00b7 \u2207\u03d5\u0302i \u03c0\u0302\u03d5\u0302i(a\u0302 i|Hi, \u03b8) \u00b7Qi(s,a, bi) + \u03c0\u03d5,\u03d5\u0302(a\u0302|H, b, \u03b8)\u2207\u03d5\u0302iQ i(s,a, bi)\n] = \u2211 a\u2208A [ \u03b8i \u00b7 \u2207\u03d5\u0302i \u03c0\u0302\u03d5\u0302i(a\u0302 i|Hi, \u03b8) \u00b7Qi(s,a, bi) + \u03c0\u03d5,\u03d5\u0302(a\u0302|H, b, \u03b8)\u2207\u03d5\u0302i [ R(s,a)\n+ \u03b3 \u2211 s\u2032\u2208S \u2211 a\u2032\u2208A P(s\u2032|s,a) \u2211 \u03b8\u2208\u0398 p(\u03b8|H)\u03c0\u03d5,\u03d5\u0302(a \u2032|H \u2032, b\u2032, \u03b8)Qi(s\u2032, a\u0302\u2032, b\u2032i) ] ,\n= \u2211 a\u2208A [ \u03b8i \u00b7 \u2207\u03d5\u0302i \u03c0\u0302\u03d5\u0302i(a\u0302 i|Hi, \u03b8) \u00b7Qi(s,a, bi) + \u03c0\u03d5,\u03d5\u0302(a|H, b, \u03b8) [ \u03b3 \u00b7 \u03b8i \u00b7 \u2207\u03d5\u0302i\n\u03c0\u0302\u03d5\u0302i(a\u0302 \u2032i|H \u2032i, \u03b8) + \u03b3 \u2211 s\u2032\u2208S \u2211 a\u2032\u2208A P(s\u2032|s,a) \u2211 \u03b8\u2208\u0398 p(\u03b8|H)\u03c0\u03d5,\u03d5\u0302(a\u0302 \u2032|H \u2032, b\u2032, \u03b8)\u2207\u03d5iQi(s\u2032,a\u2032, b\u2032i) ] ,\n= \u2211 s\u2032\u2208S \u221e\u2211 t=0 Pr(s\u2192 s\u2032, t, \u03c0) \u2211 a\u2208A \u03b8i \u00b7 \u2207\u03d5\u0302i \u03c0\u0302\u03d5\u0302i(a\u0302 i|Hi, \u03b8) \u00b7Qi(s,a, bi).\nConsidering\u2207\u03d5iJ i(\u03d5i) and use the log-derivative trick same as above, we get:\n\u2207\u03d5\u0302iJ i(\u03d5\u0302i) \u221d \u2211 s\u2208S \u03c1\u03c0(s) \u2211 a\u2208A \u03b8i \u00b7 \u2207\u03d5\u0302i \u03c0\u0302\u03d5\u0302i(a\u0302 i|Hi, \u03b8) \u00b7Qi(s,a, bi)\n= \u2211 s\u2208S \u03c1\u03c0(s) \u2211 a\u2208A \u03b8i\u03c0\u0302\u03d5\u0302(a\u0302 i|Hi, \u03b8) \u00b7Qi(s, a\u0302, bi) \u2207\u03d5\u0302i \u03c0\u0302\u03d5\u0302i(a\u0302 i|Hi, \u03b8) \u03c0\u0302\u03d5\u0302i(a\u0302 i|Hi, \u03b8)\n=Es\u223c\u03c1\u03c0(s),a\u223c\u03c0(\u00b7|H,b,\u03b8)[\u03b8i\u2207\u03d5\u0302i ln \u03c0\u0302\u03d5\u0302i(a\u0302 i|Hi, \u03b8) \u00b7Qi(s,a, bi)].\nThis completes the proof."
        },
        {
            "heading": "A.6 CONVERGENCE PROOF OF THEOREM 3.1",
            "text": "We proof this via stochastic approximation theory of Borkar (Borkar, 1997; Borkar & Meyn, 2000; Borkar, 2009), where the robust agent is quasi-static and the adversary is essentially equilibrated (Borkar & Meyn, 2000). One notable difference is, since the type in our work was sampled from prior distribution \u03b8 \u223c p(\u03b8), we take the expectation with respect to p(\u03b8) to follow the notation of stochastic approximation theory.\nProposition A.1 (Convergence). Theorem 3.1 in main paper converge to robust Bayesian Markov Perfect equilibrium a.s. if the following assumption holds.\nAssumption A.1. Given step n, learning rate of \u03c0 and \u03c0\u0302 as \u03b1(n) and \u03b2(n) with \u03b1(n), \u03b2(n) \u2208 (0, 1), denote the probability of having an adversary as p\u03b8 i\n= p(\u03b8i = 1), such that \u2200i \u2208 N ,\u2211 t \u03b1(n)(1\u2212p\u03b8 i ) = \u2211 n \u03b2(n)p \u03b8i =\u221e, \u2211 t(\u03b1(n)(1\u2212p\u03b8 i ))2+(\u03b2(n)p\u03b8 i )2 <\u221e, \u03b1(n)(1\u2212p \u03b8i ) \u03b2(n)p\u03b8i ) \u2192 0.\nNote that the assumption slightly differs from standard stochastic approximation, since adversaries and robust agents are not uniformly explored.\nAssumption A.2. \u2200i \u2208 N , \u03b8 \u2208 \u0398, Qi(s,a, \u03b8) is Lipshitz continuous. As a corollary, Qi(s,a, bi) = Ep(\u03b8|Hi)[Qi(s,a, \u03b8)] is Lipshitz continuous.\nAssumption A.3. Let \u03bd(s, a, \u03b8) denote the number of visit to state s and action a under \u03b8i. \u2200s, a, \u03b8, \u03bd(s, a, \u03b8)\u2192\u221e.\nAssumption A.4. The error in stochastic approximation (i.e., inaccuracy in critic value, environment noise, beliefetc.) constitutes martingale difference sequences with respect to the increasing \u03c3-fields.\nAssumption A.5. \u2200\u03b8 \u2208 \u0398, a global asymptotically stable ex interim equilibrium (\u03c0EI\u2217 , \u03c0\u0302EI\u2217 ) exists. Assumption A.6. \u2200\u03b8 \u2208 \u0398, supt(||\u03c0EIn ||+ ||\u03c0\u0302EIn ||) <\u221e.\nproof. We can write the update rule of \u03c0 and \u03c0\u0302 in their Ordinary Differential Equation (ODE) form:\nE\u03b8i\u223cp(\u03b8i)[\u03c0in+1] =E\u03b8i\u223cp(\u03b8i)[\u03c0in] + \u03b1(\u03bd(s, a))(1\u2212 p\u03b8 i )E\u03b8i\u223cp(\u03b8i)[\u2207 log \u03c0in(R(s,a) + \u2211 s\u2032\u2208S P(s\u2032|s,a) \u2211 \u03b8\u2208\u0398 p(\u03b8|H \u2032i) \u2211 a\u2208A \u03c0n(a \u2032|H \u2032, b\u2032, \u03b8)Qin(s\u2032,a\u2032, b\u2032i))],\nE\u03b8i\u223cp(\u03b8i)[\u03c0\u0302in+1] =E\u03b8i\u223cp(\u03b8i)[\u03c0\u0302in] + \u03b2(\u03bd(s, a))p\u03b8 i E\u03b8i\u223cp(\u03b8i)[\u2207 log \u03c0\u0302in(R(s,a) + \u2211 s\u2032\u2208S P(s\u2032|s,a) \u2211 \u03b8\u2208\u0398 p(\u03b8|H \u2032i) \u2211 a\u0302\u2208A \u03c0n(a \u2032|H \u2032, b\u2032, \u03b8)Qin(s\u2032,a\u2032, b\u2032i))].\nwhere we assume Qin(s \u2032,a\u2032, b\u2032i) is the learned critic, updated at a faster timescale than \u03b1(n) and \u03b2(n), following Assumption 1.1-1.5, such that Qi is essentially equilibrated. The error terms are embedded in Qin(s\n\u2032,a\u2032, H \u2032i). Thus, the update rule of \u03c0i and \u03c0\u0302i follows the general update rule of stochastic optimization (Borkar, 2009):\nxn+1 =xn + \u03b1(n)[h(xn, yn) +M (1) n+1],\nyn+1 =yn + \u03b1(n)[g(xn, yn) +M (2) n+1].\nThus, by Theorem 4.1 in (Borkar & Meyn, 2000) or Theorem 2 in (Borkar, 2009), \u03c0n and \u03c0\u0302n converge to equilibrium."
        },
        {
            "heading": "B ADDITIONAL DETAILS ON ALGORITHM",
            "text": "We implement our algorithm on top of MAPPO (Yu et al., 2021). MAPPO is an widely used multi-agent extension of PPO and consistently achieves strong performance on many benchmark environments. Note that our method do not include algorithm-specific structures, which means it can easily be applied to other actor-critic based algorithms, such as IPPO (de Witt et al., 2020), HATRPO (Kuba et al., 2021), MAT (Wen et al., 2022) etc. easily. However, while technically possible, we do not suggest a MADDPG implementation (Lowe et al., 2017) of our algorithm. This is because MADDPG provide deterministic output, but pure-strategy robust Markov perfect Perfect Bayesian equilibrium is not guaranteed to exist, as we have shown in Appedix. ?? by a counterexample.\nOne important thing to notice is that we empirically find using larger learning rate for adversaries during training of EIR-MAPPO do not always work well in all environments. For example, the learning dynamics of adversary in our toy environment can be unstable even with learning rate 5e\u2212 4, and get worse if the learning rate further increase, which is much smaller than the convergence rate suggested by previous papers (Daskalakis et al., 2020). We empirically find adversaries using slightly higher learning rates than robust agents works well, but requires extensive tuning. To tackle this problem, we maintain the central assumption of two-timescale updates (i.e., updating the adversary faster and the robust agent slower), but instead update the adversary for more rollouts (denoted by interval in our algorithm), while update the victim for less rollouts. This do not violate our proof in Appendix A.6, since taking expectations to policy update brings the same result.\nWe closely follow the implementation details of MAPPO and PPO (Schulman et al., 2017), including parameter sharing, generalized advantage estimation (GAE) (Schulman et al., 2015), and other tricks in the codebase of MAPPO, available at https://github.com/marlbenchmark/ on-policy. Note that we use fixed learning rate for both adversary and robust agents. p\u03be(\u03b8|Hi) is a GRU (Chung et al., 2014) with input p\u03be(bi|oi, hi), where hi is the hidden state that summarize observations in previous timesteps and oi is observation of current timestep.\nAlgorithm 1 ex interim robust c-MARL (EIR-MAPPO). Input: Policy network of robust agents \u03c0\u03d5, adversary \u03c0\u0302\u03d5\u0302i , value function V\u03c8 , belief network p\u03be. Output: Trained policy network of robust agents \u03c0\u03d5.\n1: for k = 0, 1, 2, ... K do 2: Sample \u03b8 \u223c p(\u03b8). Initialize \u03c4 = []. 3: for t = 0, 1, 2, ... T do 4: \u2200i \u2208 N , perform rollout under bit = p\u03be(\u03b8|Hit), at \u223c \u03c0\u03d5(\u00b7|Ht, bt), a\u0302t \u223c \u03c0\u0302\u03d5\u0302i(\u00b7|Ht, \u03b8) and st+1 \u223c P(st+1|st,at, \u03b8, \u03c0\u0302), receive Hit+1 and rt. 5: \u03c4 \u2190 \u03c4 \u222a (bt,at, a\u0302t, st+1, Ht+1, rt, Ht, \u03b8). 6: end for 7: for i = 0, 1, 2, ... N do 8: if k % interval == 0 then 9: Using \u03c4 , calculate Ai\u03c8(s,a, b\ni) by GAE; calculate bi by p\u03be. 10: \u03d5\u2190 \u03d5i + \u03b1\u03d5(1\u2212 \u03b8i)\u2207 log \u03c0\u03d5(ai|Hi, bi)Ai\u03c8(s,a, bi). // Shared parameters 11: \u03d5\u0302\u2190 \u03d5i \u2212 \u03b1\u03d5\u0302\u03b8 i\u2207 log \u03c0\u0302\u03d5\u0302(a i|Hi, \u03b8)Ai\u03c8(s,a, bi). 12: \u03c8 \u2190 \u03c8 + \u03b1\u03c8\u2207\u03c8(r \u2212 \u03b3Qi\u03c8(s\u2032,a\u2032, b\u2032i) +Qi\u03c8(s,a, bi))2\n13: \u03be \u2190 \u03be \u2212 \u03b1\u03be\u2207\u03be(\u03b8 log ( p\u03be(\u03b8|Hi) ) + (1\u2212 \u03b8) log(1\u2212 p\u03be(\u03b8|Hi)) 14: else 15: \u03d5\u0302 \u2190 \u03d5i \u2212 \u03b1\u03d5\u0302\u03b8 i\u2207 log \u03c0\u0302\u03d5\u0302(a i|Hi, \u03b8)Ai\u03c8(s,a, bi). // Update adversaries for\nmore rollouts, with others fixed. Empirically stabilize training.\n16: end if 17: end for 18: end for"
        },
        {
            "heading": "C ADDITIONAL DETAILS ON EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "C.1 ENVIRONMENT DETAILS",
            "text": "In this section, we introduce more details on environment. Again, we add the figure of environments in Fig. 7. Next, we introduce the tasks, actions and reward of each environments as follows.\nToy environment. The toy environment was first proposed by (Han et al., 2022) to study the effect of state-based attacks on c-MARL. In this game, two agents play simultaneously for 100 iterations to achieve maximum coordination. Specifically, in state s1, two agents seeks same actions (XNOR gate), while state s2 seeks two agents seeks different actions (XOR gate). During attack, the adversary can take over each agent, and perform actions to maximally attack another agent. The attack requires the robust agent to simultaneously identify other agents as adversary or allies, while taking cooperative actions if other agent is an ally, and take randomized action if other agent is an adversary.\nLevel-Based Foraging environment. Level-Based Foraging environment (Papoudakis et al., 2020) aims at a set of agents to cooperatively achieve maximum reward in food collection process. Each\nagents are assigned different \u201clevels\u201d, while the food can only be collected via agents with level higher than the food. Note that we use the cooperative setting in LBF, which majority agents (except the adversary) have to collaborate jointly to achieve the goal. While there do not exist a commonly used testbed in LBF, we use 12x12-4p-4f in our experiment.\nWe also need to notice that, in LBF environment, an adversary is capable of physically interfere with other agents, such as blocking the way of others or intentionally colliding with other agents, resulting in a deadlock ever since. While Gleave et al. (2019) cited an important aspect of adversarial policy as \u201cnot to physically interfere with others\u201d, we turn the collision in LBF off to represent this.\nStarCraft Multi-Agent Challenge environment. StarCraft Multi-Agent Challenge environment (Samvelyan et al., 2019) is the most commly used testbed for c-MARL, which agents control a team of red agents and seeks to win a team of blue agents with built-in AIs. We adapt the map 3m proposed in SMAC testbed to 4m vs 3m, with one agent as adversary. Thus, the map can still be viewed as 3m, albeit with one adversary agents trying to fool its teammates. Note that the adversary cannot attack its allies by design of SMAC environment.\nC.2 IMPLEMENTATION DETAILS\nThe implementation of MAPPO, RMAAC, EAR-MAPPO and EIR-MAPPO are based on the original codebase of MAPPO (https://github.com/marlbenchmark/on-policy). The implementation of MADDPG and M3DDPG resembles the code of FACMAC (Peng et al., 2021) (https://github.com/oxwhirl/facmac) and Heterogeneous-Agent Reinforcement Learning (HARL) codebase (https://github.com/PKU-MARL/HARL). Our code are available in supplementary files, and will be open sourced after this paper is accepted.\nFor all environments, we set p(\u03b8 = 0N ) = 0.5 and p(\u03b8 = 1i) = 0.5/N , where 1i denotes the one-hot vector with \u03b8i = 1 and others 0. This probability of selecting p(\u03b8) remains fixed throughout training process. During training, we store the model of robust agents with improved robustness without decreasing cooperation reward, evaluated on the adversary during training. While testing, we held all parameters in robust agents fixed, including policy and belief network. Then, a black-box adversary was trained following the approach of adversarial policy. The adversary also follows a CTDE approach, assuming assess to state, reward and local observation during training, and use local observation only in testing. For fair comparison, we attack all baselines by PPO (Schulman et al., 2017).\nAs for M3DDPG, note that the original version of M3DDPG (Li et al., 2019) are designed for continuous control only, where actions are continuous and can be perturbed by a small value, while in discrete control, one will have to completely change the action, or not changing the action at all. To solve that, we add the noise perturbation to the action probability of MADDPG and send it to Q function instead. We also find using large \u03f5 for M3DDPG will make the policy impossible to converge in fully cooperative settings: since M3DDPG add perturbations directly to each agents, resulting in an overly challenging setting. As such, we select the largest \u03f5 which enables maximum cooperation result in each setting.\nAs for RMAAC, the perturbation in training is set to \u03f5 = 0.5 following their original paper, except for \u03f5 = 0.05 in toy environment since otherwise the policy will not converge in normal training.\nNext, we present all hyperparameters of each environment in the table below. These hyparameters follows the default in previous papers, including MAPPO (Yu et al., 2021), HARL (Zhong et al., 2023) and FACMAC (Peng et al., 2021)."
        }
    ],
    "year": 2024
}